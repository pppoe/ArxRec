<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250617.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos", "author": "Kaifeng Zhang and Baoyu Li and Kris Hauser and Yunzhu Li", "abstract": "  Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .\n", "link": "http://arxiv.org/abs/2506.15680v1", "date": "2025-06-18", "relevancy": 3.2309, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6697}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6393}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Particle-Grid%20Neural%20Dynamics%20for%20Learning%20Deformable%20Object%20Models%20from%0A%20%20RGB-D%20Videos&body=Title%3A%20Particle-Grid%20Neural%20Dynamics%20for%20Learning%20Deformable%20Object%20Models%20from%0A%20%20RGB-D%20Videos%0AAuthor%3A%20Kaifeng%20Zhang%20and%20Baoyu%20Li%20and%20Kris%20Hauser%20and%20Yunzhu%20Li%0AAbstract%3A%20%20%20Modeling%20the%20dynamics%20of%20deformable%20objects%20is%20challenging%20due%20to%20their%0Adiverse%20physical%20properties%20and%20the%20difficulty%20of%20estimating%20states%20from%0Alimited%20visual%20information.%20We%20address%20these%20challenges%20with%20a%20neural%20dynamics%0Aframework%20that%20combines%20object%20particles%20and%20spatial%20grids%20in%20a%20hybrid%0Arepresentation.%20Our%20particle-grid%20model%20captures%20global%20shape%20and%20motion%0Ainformation%20while%20predicting%20dense%20particle%20movements%2C%20enabling%20the%20modeling%20of%0Aobjects%20with%20varied%20shapes%20and%20materials.%20Particles%20represent%20object%20shapes%2C%0Awhile%20the%20spatial%20grid%20discretizes%20the%203D%20space%20to%20ensure%20spatial%20continuity%0Aand%20enhance%20learning%20efficiency.%20Coupled%20with%20Gaussian%20Splattings%20for%20visual%0Arendering%2C%20our%20framework%20achieves%20a%20fully%20learning-based%20digital%20twin%20of%0Adeformable%20objects%20and%20generates%203D%20action-conditioned%20videos.%20Through%0Aexperiments%2C%20we%20demonstrate%20that%20our%20model%20learns%20the%20dynamics%20of%20diverse%0Aobjects%20--%20such%20as%20ropes%2C%20cloths%2C%20stuffed%20animals%2C%20and%20paper%20bags%20--%20from%0Asparse-view%20RGB-D%20recordings%20of%20robot-object%20interactions%2C%20while%20also%0Ageneralizing%20at%20the%20category%20level%20to%20unseen%20instances.%20Our%20approach%0Aoutperforms%20state-of-the-art%20learning-based%20and%20physics-based%20simulators%2C%0Aparticularly%20in%20scenarios%20with%20limited%20camera%20views.%20Furthermore%2C%20we%20showcase%0Athe%20utility%20of%20our%20learned%20models%20in%20model-based%20planning%2C%20enabling%0Agoal-conditioned%20object%20manipulation%20across%20a%20range%20of%20tasks.%20The%20project%20page%0Ais%20available%20at%20https%3A//kywind.github.io/pgnd%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParticle-Grid%2520Neural%2520Dynamics%2520for%2520Learning%2520Deformable%2520Object%2520Models%2520from%250A%2520%2520RGB-D%2520Videos%26entry.906535625%3DKaifeng%2520Zhang%2520and%2520Baoyu%2520Li%2520and%2520Kris%2520Hauser%2520and%2520Yunzhu%2520Li%26entry.1292438233%3D%2520%2520Modeling%2520the%2520dynamics%2520of%2520deformable%2520objects%2520is%2520challenging%2520due%2520to%2520their%250Adiverse%2520physical%2520properties%2520and%2520the%2520difficulty%2520of%2520estimating%2520states%2520from%250Alimited%2520visual%2520information.%2520We%2520address%2520these%2520challenges%2520with%2520a%2520neural%2520dynamics%250Aframework%2520that%2520combines%2520object%2520particles%2520and%2520spatial%2520grids%2520in%2520a%2520hybrid%250Arepresentation.%2520Our%2520particle-grid%2520model%2520captures%2520global%2520shape%2520and%2520motion%250Ainformation%2520while%2520predicting%2520dense%2520particle%2520movements%252C%2520enabling%2520the%2520modeling%2520of%250Aobjects%2520with%2520varied%2520shapes%2520and%2520materials.%2520Particles%2520represent%2520object%2520shapes%252C%250Awhile%2520the%2520spatial%2520grid%2520discretizes%2520the%25203D%2520space%2520to%2520ensure%2520spatial%2520continuity%250Aand%2520enhance%2520learning%2520efficiency.%2520Coupled%2520with%2520Gaussian%2520Splattings%2520for%2520visual%250Arendering%252C%2520our%2520framework%2520achieves%2520a%2520fully%2520learning-based%2520digital%2520twin%2520of%250Adeformable%2520objects%2520and%2520generates%25203D%2520action-conditioned%2520videos.%2520Through%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520our%2520model%2520learns%2520the%2520dynamics%2520of%2520diverse%250Aobjects%2520--%2520such%2520as%2520ropes%252C%2520cloths%252C%2520stuffed%2520animals%252C%2520and%2520paper%2520bags%2520--%2520from%250Asparse-view%2520RGB-D%2520recordings%2520of%2520robot-object%2520interactions%252C%2520while%2520also%250Ageneralizing%2520at%2520the%2520category%2520level%2520to%2520unseen%2520instances.%2520Our%2520approach%250Aoutperforms%2520state-of-the-art%2520learning-based%2520and%2520physics-based%2520simulators%252C%250Aparticularly%2520in%2520scenarios%2520with%2520limited%2520camera%2520views.%2520Furthermore%252C%2520we%2520showcase%250Athe%2520utility%2520of%2520our%2520learned%2520models%2520in%2520model-based%2520planning%252C%2520enabling%250Agoal-conditioned%2520object%2520manipulation%2520across%2520a%2520range%2520of%2520tasks.%2520The%2520project%2520page%250Ais%2520available%2520at%2520https%253A//kywind.github.io/pgnd%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Particle-Grid%20Neural%20Dynamics%20for%20Learning%20Deformable%20Object%20Models%20from%0A%20%20RGB-D%20Videos&entry.906535625=Kaifeng%20Zhang%20and%20Baoyu%20Li%20and%20Kris%20Hauser%20and%20Yunzhu%20Li&entry.1292438233=%20%20Modeling%20the%20dynamics%20of%20deformable%20objects%20is%20challenging%20due%20to%20their%0Adiverse%20physical%20properties%20and%20the%20difficulty%20of%20estimating%20states%20from%0Alimited%20visual%20information.%20We%20address%20these%20challenges%20with%20a%20neural%20dynamics%0Aframework%20that%20combines%20object%20particles%20and%20spatial%20grids%20in%20a%20hybrid%0Arepresentation.%20Our%20particle-grid%20model%20captures%20global%20shape%20and%20motion%0Ainformation%20while%20predicting%20dense%20particle%20movements%2C%20enabling%20the%20modeling%20of%0Aobjects%20with%20varied%20shapes%20and%20materials.%20Particles%20represent%20object%20shapes%2C%0Awhile%20the%20spatial%20grid%20discretizes%20the%203D%20space%20to%20ensure%20spatial%20continuity%0Aand%20enhance%20learning%20efficiency.%20Coupled%20with%20Gaussian%20Splattings%20for%20visual%0Arendering%2C%20our%20framework%20achieves%20a%20fully%20learning-based%20digital%20twin%20of%0Adeformable%20objects%20and%20generates%203D%20action-conditioned%20videos.%20Through%0Aexperiments%2C%20we%20demonstrate%20that%20our%20model%20learns%20the%20dynamics%20of%20diverse%0Aobjects%20--%20such%20as%20ropes%2C%20cloths%2C%20stuffed%20animals%2C%20and%20paper%20bags%20--%20from%0Asparse-view%20RGB-D%20recordings%20of%20robot-object%20interactions%2C%20while%20also%0Ageneralizing%20at%20the%20category%20level%20to%20unseen%20instances.%20Our%20approach%0Aoutperforms%20state-of-the-art%20learning-based%20and%20physics-based%20simulators%2C%0Aparticularly%20in%20scenarios%20with%20limited%20camera%20views.%20Furthermore%2C%20we%20showcase%0Athe%20utility%20of%20our%20learned%20models%20in%20model-based%20planning%2C%20enabling%0Agoal-conditioned%20object%20manipulation%20across%20a%20range%20of%20tasks.%20The%20project%20page%0Ais%20available%20at%20https%3A//kywind.github.io/pgnd%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15680v1&entry.124074799=Read"},
{"title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized\n  Semantic Segmentation", "author": "Siyu Chen and Ting Han and Chengzheng Fu and Changshe Zhang and Chaolei Wang and Jinhe Su and Guorong Cai and Meiliu Wu", "abstract": "  Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.\n", "link": "http://arxiv.org/abs/2506.09881v2", "date": "2025-06-18", "relevancy": 3.0838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6399}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%0A%20%20Semantic%20Segmentation&body=Title%3A%20Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Siyu%20Chen%20and%20Ting%20Han%20and%20Chengzheng%20Fu%20and%20Changshe%20Zhang%20and%20Chaolei%20Wang%20and%20Jinhe%20Su%20and%20Guorong%20Cai%20and%20Meiliu%20Wu%0AAbstract%3A%20%20%20Open-Vocabulary%20semantic%20segmentation%20%28OVSS%29%20and%20domain%20generalization%20in%0Asemantic%20segmentation%20%28DGSS%29%20highlight%20a%20subtle%20complementarity%20that%20motivates%0AOpen-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation%20%28OV-DGSS%29.%20OV-DGSS%0Aaims%20to%20generate%20pixel-level%20masks%20for%20unseen%20categories%20while%20maintaining%0Arobustness%20across%20unseen%20domains%2C%20a%20critical%20capability%20for%20real-world%0Ascenarios%20such%20as%20autonomous%20driving%20in%20adverse%20conditions.%20We%20introduce%20Vireo%2C%0Aa%20novel%20single-stage%20framework%20for%20OV-DGSS%20that%20unifies%20the%20strengths%20of%20OVSS%0Aand%20DGSS%20for%20the%20first%20time.%20Vireo%20builds%20upon%20the%20frozen%20Visual%20Foundation%0AModels%20%28VFMs%29%20and%20incorporates%20scene%20geometry%20via%20Depth%20VFMs%20to%20extract%0Adomain-invariant%20structural%20features.%20To%20bridge%20the%20gap%20between%20visual%20and%0Atextual%20modalities%20under%20domain%20shift%2C%20we%20propose%20three%20key%20components%3A%20%281%29%0AGeoText%20Prompts%2C%20which%20align%20geometric%20features%20with%20language%20cues%20and%0Aprogressively%20refine%20VFM%20encoder%20representations%3B%20%282%29%20Coarse%20Mask%20Prior%0AEmbedding%20%28CMPE%29%20for%20enhancing%20gradient%20flow%20for%20faster%20convergence%20and%0Astronger%20textual%20influence%3B%20and%20%283%29%20the%20Domain-Open-Vocabulary%20Vector%20Embedding%0AHead%20%28DOV-VEH%29%2C%20which%20fuses%20refined%20structural%20and%20semantic%20features%20for%20robust%0Aprediction.%20Comprehensive%20evaluation%20on%20these%20components%20demonstrates%20the%0Aeffectiveness%20of%20our%20designs.%20Our%20proposed%20Vireo%20achieves%20the%20state-of-the-art%0Aperformance%20and%20surpasses%20existing%20methods%20by%20a%20large%20margin%20in%20both%20domain%0Ageneralization%20and%20open-vocabulary%20recognition%2C%20offering%20a%20unified%20and%20scalable%0Asolution%20for%20robust%20visual%20understanding%20in%20diverse%20and%20dynamic%20environments.%0ACode%20is%20available%20at%20https%3A//github.com/anonymouse-9c53tp182bvz/Vireo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09881v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Depth%2520and%2520Language%2520for%2520Open-Vocabulary%2520Domain-Generalized%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DSiyu%2520Chen%2520and%2520Ting%2520Han%2520and%2520Chengzheng%2520Fu%2520and%2520Changshe%2520Zhang%2520and%2520Chaolei%2520Wang%2520and%2520Jinhe%2520Su%2520and%2520Guorong%2520Cai%2520and%2520Meiliu%2520Wu%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520semantic%2520segmentation%2520%2528OVSS%2529%2520and%2520domain%2520generalization%2520in%250Asemantic%2520segmentation%2520%2528DGSS%2529%2520highlight%2520a%2520subtle%2520complementarity%2520that%2520motivates%250AOpen-Vocabulary%2520Domain-Generalized%2520Semantic%2520Segmentation%2520%2528OV-DGSS%2529.%2520OV-DGSS%250Aaims%2520to%2520generate%2520pixel-level%2520masks%2520for%2520unseen%2520categories%2520while%2520maintaining%250Arobustness%2520across%2520unseen%2520domains%252C%2520a%2520critical%2520capability%2520for%2520real-world%250Ascenarios%2520such%2520as%2520autonomous%2520driving%2520in%2520adverse%2520conditions.%2520We%2520introduce%2520Vireo%252C%250Aa%2520novel%2520single-stage%2520framework%2520for%2520OV-DGSS%2520that%2520unifies%2520the%2520strengths%2520of%2520OVSS%250Aand%2520DGSS%2520for%2520the%2520first%2520time.%2520Vireo%2520builds%2520upon%2520the%2520frozen%2520Visual%2520Foundation%250AModels%2520%2528VFMs%2529%2520and%2520incorporates%2520scene%2520geometry%2520via%2520Depth%2520VFMs%2520to%2520extract%250Adomain-invariant%2520structural%2520features.%2520To%2520bridge%2520the%2520gap%2520between%2520visual%2520and%250Atextual%2520modalities%2520under%2520domain%2520shift%252C%2520we%2520propose%2520three%2520key%2520components%253A%2520%25281%2529%250AGeoText%2520Prompts%252C%2520which%2520align%2520geometric%2520features%2520with%2520language%2520cues%2520and%250Aprogressively%2520refine%2520VFM%2520encoder%2520representations%253B%2520%25282%2529%2520Coarse%2520Mask%2520Prior%250AEmbedding%2520%2528CMPE%2529%2520for%2520enhancing%2520gradient%2520flow%2520for%2520faster%2520convergence%2520and%250Astronger%2520textual%2520influence%253B%2520and%2520%25283%2529%2520the%2520Domain-Open-Vocabulary%2520Vector%2520Embedding%250AHead%2520%2528DOV-VEH%2529%252C%2520which%2520fuses%2520refined%2520structural%2520and%2520semantic%2520features%2520for%2520robust%250Aprediction.%2520Comprehensive%2520evaluation%2520on%2520these%2520components%2520demonstrates%2520the%250Aeffectiveness%2520of%2520our%2520designs.%2520Our%2520proposed%2520Vireo%2520achieves%2520the%2520state-of-the-art%250Aperformance%2520and%2520surpasses%2520existing%2520methods%2520by%2520a%2520large%2520margin%2520in%2520both%2520domain%250Ageneralization%2520and%2520open-vocabulary%2520recognition%252C%2520offering%2520a%2520unified%2520and%2520scalable%250Asolution%2520for%2520robust%2520visual%2520understanding%2520in%2520diverse%2520and%2520dynamic%2520environments.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/anonymouse-9c53tp182bvz/Vireo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09881v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Depth%20and%20Language%20for%20Open-Vocabulary%20Domain-Generalized%0A%20%20Semantic%20Segmentation&entry.906535625=Siyu%20Chen%20and%20Ting%20Han%20and%20Chengzheng%20Fu%20and%20Changshe%20Zhang%20and%20Chaolei%20Wang%20and%20Jinhe%20Su%20and%20Guorong%20Cai%20and%20Meiliu%20Wu&entry.1292438233=%20%20Open-Vocabulary%20semantic%20segmentation%20%28OVSS%29%20and%20domain%20generalization%20in%0Asemantic%20segmentation%20%28DGSS%29%20highlight%20a%20subtle%20complementarity%20that%20motivates%0AOpen-Vocabulary%20Domain-Generalized%20Semantic%20Segmentation%20%28OV-DGSS%29.%20OV-DGSS%0Aaims%20to%20generate%20pixel-level%20masks%20for%20unseen%20categories%20while%20maintaining%0Arobustness%20across%20unseen%20domains%2C%20a%20critical%20capability%20for%20real-world%0Ascenarios%20such%20as%20autonomous%20driving%20in%20adverse%20conditions.%20We%20introduce%20Vireo%2C%0Aa%20novel%20single-stage%20framework%20for%20OV-DGSS%20that%20unifies%20the%20strengths%20of%20OVSS%0Aand%20DGSS%20for%20the%20first%20time.%20Vireo%20builds%20upon%20the%20frozen%20Visual%20Foundation%0AModels%20%28VFMs%29%20and%20incorporates%20scene%20geometry%20via%20Depth%20VFMs%20to%20extract%0Adomain-invariant%20structural%20features.%20To%20bridge%20the%20gap%20between%20visual%20and%0Atextual%20modalities%20under%20domain%20shift%2C%20we%20propose%20three%20key%20components%3A%20%281%29%0AGeoText%20Prompts%2C%20which%20align%20geometric%20features%20with%20language%20cues%20and%0Aprogressively%20refine%20VFM%20encoder%20representations%3B%20%282%29%20Coarse%20Mask%20Prior%0AEmbedding%20%28CMPE%29%20for%20enhancing%20gradient%20flow%20for%20faster%20convergence%20and%0Astronger%20textual%20influence%3B%20and%20%283%29%20the%20Domain-Open-Vocabulary%20Vector%20Embedding%0AHead%20%28DOV-VEH%29%2C%20which%20fuses%20refined%20structural%20and%20semantic%20features%20for%20robust%0Aprediction.%20Comprehensive%20evaluation%20on%20these%20components%20demonstrates%20the%0Aeffectiveness%20of%20our%20designs.%20Our%20proposed%20Vireo%20achieves%20the%20state-of-the-art%0Aperformance%20and%20surpasses%20existing%20methods%20by%20a%20large%20margin%20in%20both%20domain%0Ageneralization%20and%20open-vocabulary%20recognition%2C%20offering%20a%20unified%20and%20scalable%0Asolution%20for%20robust%20visual%20understanding%20in%20diverse%20and%20dynamic%20environments.%0ACode%20is%20available%20at%20https%3A//github.com/anonymouse-9c53tp182bvz/Vireo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09881v2&entry.124074799=Read"},
{"title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer", "author": "Gonglin Chen and Tianwen Fu and Haiwei Chen and Wenbin Teng and Hanyuan Xiao and Yajie Zhao", "abstract": "  As a core step in structure-from-motion and SLAM, robust feature detection\nand description under challenging scenarios such as significant viewpoint\nchanges remain unresolved despite their ubiquity. While recent works have\nidentified the importance of local features in modeling geometric\ntransformations, these methods fail to learn the visual cues present in\nlong-range relationships. We present Robust Deformable Detector (RDD), a novel\nand robust keypoint detector/descriptor leveraging the deformable transformer,\nwhich captures global context and geometric invariance through deformable\nself-attention mechanisms. Specifically, we observed that deformable attention\nfocuses on key locations, effectively reducing the search space complexity and\nmodeling the geometric invariance. Furthermore, we collected an Air-to-Ground\ndataset for training in addition to the standard MegaDepth dataset. Our\nproposed method outperforms all state-of-the-art keypoint detection/description\nmethods in sparse matching tasks and is also capable of semi-dense matching. To\nensure comprehensive evaluation, we introduce two challenging benchmarks: one\nemphasizing large viewpoint and scale variations, and the other being an\nAir-to-Ground benchmark -- an evaluation setting that has recently gaining\npopularity for 3D reconstruction across different altitudes.\n", "link": "http://arxiv.org/abs/2505.08013v2", "date": "2025-06-18", "relevancy": 3.0144, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6234}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6063}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RDD%3A%20Robust%20Feature%20Detector%20and%20Descriptor%20using%20Deformable%20Transformer&body=Title%3A%20RDD%3A%20Robust%20Feature%20Detector%20and%20Descriptor%20using%20Deformable%20Transformer%0AAuthor%3A%20Gonglin%20Chen%20and%20Tianwen%20Fu%20and%20Haiwei%20Chen%20and%20Wenbin%20Teng%20and%20Hanyuan%20Xiao%20and%20Yajie%20Zhao%0AAbstract%3A%20%20%20As%20a%20core%20step%20in%20structure-from-motion%20and%20SLAM%2C%20robust%20feature%20detection%0Aand%20description%20under%20challenging%20scenarios%20such%20as%20significant%20viewpoint%0Achanges%20remain%20unresolved%20despite%20their%20ubiquity.%20While%20recent%20works%20have%0Aidentified%20the%20importance%20of%20local%20features%20in%20modeling%20geometric%0Atransformations%2C%20these%20methods%20fail%20to%20learn%20the%20visual%20cues%20present%20in%0Along-range%20relationships.%20We%20present%20Robust%20Deformable%20Detector%20%28RDD%29%2C%20a%20novel%0Aand%20robust%20keypoint%20detector/descriptor%20leveraging%20the%20deformable%20transformer%2C%0Awhich%20captures%20global%20context%20and%20geometric%20invariance%20through%20deformable%0Aself-attention%20mechanisms.%20Specifically%2C%20we%20observed%20that%20deformable%20attention%0Afocuses%20on%20key%20locations%2C%20effectively%20reducing%20the%20search%20space%20complexity%20and%0Amodeling%20the%20geometric%20invariance.%20Furthermore%2C%20we%20collected%20an%20Air-to-Ground%0Adataset%20for%20training%20in%20addition%20to%20the%20standard%20MegaDepth%20dataset.%20Our%0Aproposed%20method%20outperforms%20all%20state-of-the-art%20keypoint%20detection/description%0Amethods%20in%20sparse%20matching%20tasks%20and%20is%20also%20capable%20of%20semi-dense%20matching.%20To%0Aensure%20comprehensive%20evaluation%2C%20we%20introduce%20two%20challenging%20benchmarks%3A%20one%0Aemphasizing%20large%20viewpoint%20and%20scale%20variations%2C%20and%20the%20other%20being%20an%0AAir-to-Ground%20benchmark%20--%20an%20evaluation%20setting%20that%20has%20recently%20gaining%0Apopularity%20for%203D%20reconstruction%20across%20different%20altitudes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRDD%253A%2520Robust%2520Feature%2520Detector%2520and%2520Descriptor%2520using%2520Deformable%2520Transformer%26entry.906535625%3DGonglin%2520Chen%2520and%2520Tianwen%2520Fu%2520and%2520Haiwei%2520Chen%2520and%2520Wenbin%2520Teng%2520and%2520Hanyuan%2520Xiao%2520and%2520Yajie%2520Zhao%26entry.1292438233%3D%2520%2520As%2520a%2520core%2520step%2520in%2520structure-from-motion%2520and%2520SLAM%252C%2520robust%2520feature%2520detection%250Aand%2520description%2520under%2520challenging%2520scenarios%2520such%2520as%2520significant%2520viewpoint%250Achanges%2520remain%2520unresolved%2520despite%2520their%2520ubiquity.%2520While%2520recent%2520works%2520have%250Aidentified%2520the%2520importance%2520of%2520local%2520features%2520in%2520modeling%2520geometric%250Atransformations%252C%2520these%2520methods%2520fail%2520to%2520learn%2520the%2520visual%2520cues%2520present%2520in%250Along-range%2520relationships.%2520We%2520present%2520Robust%2520Deformable%2520Detector%2520%2528RDD%2529%252C%2520a%2520novel%250Aand%2520robust%2520keypoint%2520detector/descriptor%2520leveraging%2520the%2520deformable%2520transformer%252C%250Awhich%2520captures%2520global%2520context%2520and%2520geometric%2520invariance%2520through%2520deformable%250Aself-attention%2520mechanisms.%2520Specifically%252C%2520we%2520observed%2520that%2520deformable%2520attention%250Afocuses%2520on%2520key%2520locations%252C%2520effectively%2520reducing%2520the%2520search%2520space%2520complexity%2520and%250Amodeling%2520the%2520geometric%2520invariance.%2520Furthermore%252C%2520we%2520collected%2520an%2520Air-to-Ground%250Adataset%2520for%2520training%2520in%2520addition%2520to%2520the%2520standard%2520MegaDepth%2520dataset.%2520Our%250Aproposed%2520method%2520outperforms%2520all%2520state-of-the-art%2520keypoint%2520detection/description%250Amethods%2520in%2520sparse%2520matching%2520tasks%2520and%2520is%2520also%2520capable%2520of%2520semi-dense%2520matching.%2520To%250Aensure%2520comprehensive%2520evaluation%252C%2520we%2520introduce%2520two%2520challenging%2520benchmarks%253A%2520one%250Aemphasizing%2520large%2520viewpoint%2520and%2520scale%2520variations%252C%2520and%2520the%2520other%2520being%2520an%250AAir-to-Ground%2520benchmark%2520--%2520an%2520evaluation%2520setting%2520that%2520has%2520recently%2520gaining%250Apopularity%2520for%25203D%2520reconstruction%2520across%2520different%2520altitudes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RDD%3A%20Robust%20Feature%20Detector%20and%20Descriptor%20using%20Deformable%20Transformer&entry.906535625=Gonglin%20Chen%20and%20Tianwen%20Fu%20and%20Haiwei%20Chen%20and%20Wenbin%20Teng%20and%20Hanyuan%20Xiao%20and%20Yajie%20Zhao&entry.1292438233=%20%20As%20a%20core%20step%20in%20structure-from-motion%20and%20SLAM%2C%20robust%20feature%20detection%0Aand%20description%20under%20challenging%20scenarios%20such%20as%20significant%20viewpoint%0Achanges%20remain%20unresolved%20despite%20their%20ubiquity.%20While%20recent%20works%20have%0Aidentified%20the%20importance%20of%20local%20features%20in%20modeling%20geometric%0Atransformations%2C%20these%20methods%20fail%20to%20learn%20the%20visual%20cues%20present%20in%0Along-range%20relationships.%20We%20present%20Robust%20Deformable%20Detector%20%28RDD%29%2C%20a%20novel%0Aand%20robust%20keypoint%20detector/descriptor%20leveraging%20the%20deformable%20transformer%2C%0Awhich%20captures%20global%20context%20and%20geometric%20invariance%20through%20deformable%0Aself-attention%20mechanisms.%20Specifically%2C%20we%20observed%20that%20deformable%20attention%0Afocuses%20on%20key%20locations%2C%20effectively%20reducing%20the%20search%20space%20complexity%20and%0Amodeling%20the%20geometric%20invariance.%20Furthermore%2C%20we%20collected%20an%20Air-to-Ground%0Adataset%20for%20training%20in%20addition%20to%20the%20standard%20MegaDepth%20dataset.%20Our%0Aproposed%20method%20outperforms%20all%20state-of-the-art%20keypoint%20detection/description%0Amethods%20in%20sparse%20matching%20tasks%20and%20is%20also%20capable%20of%20semi-dense%20matching.%20To%0Aensure%20comprehensive%20evaluation%2C%20we%20introduce%20two%20challenging%20benchmarks%3A%20one%0Aemphasizing%20large%20viewpoint%20and%20scale%20variations%2C%20and%20the%20other%20being%20an%0AAir-to-Ground%20benchmark%20--%20an%20evaluation%20setting%20that%20has%20recently%20gaining%0Apopularity%20for%203D%20reconstruction%20across%20different%20altitudes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08013v2&entry.124074799=Read"},
{"title": "HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization", "author": "Roey Ron and Guy Tevet and Haim Sawdayee and Amit H. Bermano", "abstract": "  We present HOIDiNi, a text-driven diffusion framework for synthesizing\nrealistic and plausible human-object interaction (HOI). HOI generation is\nextremely challenging since it induces strict contact accuracies alongside a\ndiverse motion manifold. While current literature trades off between realism\nand physical correctness, HOIDiNi optimizes directly in the noise space of a\npretrained diffusion model using Diffusion Noise Optimization (DNO), achieving\nboth. This is made feasible thanks to our observation that the problem can be\nseparated into two phases: an object-centric phase, primarily making discrete\nchoices of hand-object contact locations, and a human-centric phase that\nrefines the full-body motion to realize this blueprint. This structured\napproach allows for precise hand-object contact without compromising motion\nnaturalness. Quantitative, qualitative, and subjective evaluations on the GRAB\ndataset alone clearly indicate HOIDiNi outperforms prior works and baselines in\ncontact accuracy, physical validity, and overall quality. Our results\ndemonstrate the ability to generate complex, controllable interactions,\nincluding grasping, placing, and full-body coordination, driven solely by\ntextual prompts. https://hoidini.github.io.\n", "link": "http://arxiv.org/abs/2506.15625v1", "date": "2025-06-18", "relevancy": 2.9742, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6292}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.601}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOIDiNi%3A%20Human-Object%20Interaction%20through%20Diffusion%20Noise%20Optimization&body=Title%3A%20HOIDiNi%3A%20Human-Object%20Interaction%20through%20Diffusion%20Noise%20Optimization%0AAuthor%3A%20Roey%20Ron%20and%20Guy%20Tevet%20and%20Haim%20Sawdayee%20and%20Amit%20H.%20Bermano%0AAbstract%3A%20%20%20We%20present%20HOIDiNi%2C%20a%20text-driven%20diffusion%20framework%20for%20synthesizing%0Arealistic%20and%20plausible%20human-object%20interaction%20%28HOI%29.%20HOI%20generation%20is%0Aextremely%20challenging%20since%20it%20induces%20strict%20contact%20accuracies%20alongside%20a%0Adiverse%20motion%20manifold.%20While%20current%20literature%20trades%20off%20between%20realism%0Aand%20physical%20correctness%2C%20HOIDiNi%20optimizes%20directly%20in%20the%20noise%20space%20of%20a%0Apretrained%20diffusion%20model%20using%20Diffusion%20Noise%20Optimization%20%28DNO%29%2C%20achieving%0Aboth.%20This%20is%20made%20feasible%20thanks%20to%20our%20observation%20that%20the%20problem%20can%20be%0Aseparated%20into%20two%20phases%3A%20an%20object-centric%20phase%2C%20primarily%20making%20discrete%0Achoices%20of%20hand-object%20contact%20locations%2C%20and%20a%20human-centric%20phase%20that%0Arefines%20the%20full-body%20motion%20to%20realize%20this%20blueprint.%20This%20structured%0Aapproach%20allows%20for%20precise%20hand-object%20contact%20without%20compromising%20motion%0Anaturalness.%20Quantitative%2C%20qualitative%2C%20and%20subjective%20evaluations%20on%20the%20GRAB%0Adataset%20alone%20clearly%20indicate%20HOIDiNi%20outperforms%20prior%20works%20and%20baselines%20in%0Acontact%20accuracy%2C%20physical%20validity%2C%20and%20overall%20quality.%20Our%20results%0Ademonstrate%20the%20ability%20to%20generate%20complex%2C%20controllable%20interactions%2C%0Aincluding%20grasping%2C%20placing%2C%20and%20full-body%20coordination%2C%20driven%20solely%20by%0Atextual%20prompts.%20https%3A//hoidini.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOIDiNi%253A%2520Human-Object%2520Interaction%2520through%2520Diffusion%2520Noise%2520Optimization%26entry.906535625%3DRoey%2520Ron%2520and%2520Guy%2520Tevet%2520and%2520Haim%2520Sawdayee%2520and%2520Amit%2520H.%2520Bermano%26entry.1292438233%3D%2520%2520We%2520present%2520HOIDiNi%252C%2520a%2520text-driven%2520diffusion%2520framework%2520for%2520synthesizing%250Arealistic%2520and%2520plausible%2520human-object%2520interaction%2520%2528HOI%2529.%2520HOI%2520generation%2520is%250Aextremely%2520challenging%2520since%2520it%2520induces%2520strict%2520contact%2520accuracies%2520alongside%2520a%250Adiverse%2520motion%2520manifold.%2520While%2520current%2520literature%2520trades%2520off%2520between%2520realism%250Aand%2520physical%2520correctness%252C%2520HOIDiNi%2520optimizes%2520directly%2520in%2520the%2520noise%2520space%2520of%2520a%250Apretrained%2520diffusion%2520model%2520using%2520Diffusion%2520Noise%2520Optimization%2520%2528DNO%2529%252C%2520achieving%250Aboth.%2520This%2520is%2520made%2520feasible%2520thanks%2520to%2520our%2520observation%2520that%2520the%2520problem%2520can%2520be%250Aseparated%2520into%2520two%2520phases%253A%2520an%2520object-centric%2520phase%252C%2520primarily%2520making%2520discrete%250Achoices%2520of%2520hand-object%2520contact%2520locations%252C%2520and%2520a%2520human-centric%2520phase%2520that%250Arefines%2520the%2520full-body%2520motion%2520to%2520realize%2520this%2520blueprint.%2520This%2520structured%250Aapproach%2520allows%2520for%2520precise%2520hand-object%2520contact%2520without%2520compromising%2520motion%250Anaturalness.%2520Quantitative%252C%2520qualitative%252C%2520and%2520subjective%2520evaluations%2520on%2520the%2520GRAB%250Adataset%2520alone%2520clearly%2520indicate%2520HOIDiNi%2520outperforms%2520prior%2520works%2520and%2520baselines%2520in%250Acontact%2520accuracy%252C%2520physical%2520validity%252C%2520and%2520overall%2520quality.%2520Our%2520results%250Ademonstrate%2520the%2520ability%2520to%2520generate%2520complex%252C%2520controllable%2520interactions%252C%250Aincluding%2520grasping%252C%2520placing%252C%2520and%2520full-body%2520coordination%252C%2520driven%2520solely%2520by%250Atextual%2520prompts.%2520https%253A//hoidini.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOIDiNi%3A%20Human-Object%20Interaction%20through%20Diffusion%20Noise%20Optimization&entry.906535625=Roey%20Ron%20and%20Guy%20Tevet%20and%20Haim%20Sawdayee%20and%20Amit%20H.%20Bermano&entry.1292438233=%20%20We%20present%20HOIDiNi%2C%20a%20text-driven%20diffusion%20framework%20for%20synthesizing%0Arealistic%20and%20plausible%20human-object%20interaction%20%28HOI%29.%20HOI%20generation%20is%0Aextremely%20challenging%20since%20it%20induces%20strict%20contact%20accuracies%20alongside%20a%0Adiverse%20motion%20manifold.%20While%20current%20literature%20trades%20off%20between%20realism%0Aand%20physical%20correctness%2C%20HOIDiNi%20optimizes%20directly%20in%20the%20noise%20space%20of%20a%0Apretrained%20diffusion%20model%20using%20Diffusion%20Noise%20Optimization%20%28DNO%29%2C%20achieving%0Aboth.%20This%20is%20made%20feasible%20thanks%20to%20our%20observation%20that%20the%20problem%20can%20be%0Aseparated%20into%20two%20phases%3A%20an%20object-centric%20phase%2C%20primarily%20making%20discrete%0Achoices%20of%20hand-object%20contact%20locations%2C%20and%20a%20human-centric%20phase%20that%0Arefines%20the%20full-body%20motion%20to%20realize%20this%20blueprint.%20This%20structured%0Aapproach%20allows%20for%20precise%20hand-object%20contact%20without%20compromising%20motion%0Anaturalness.%20Quantitative%2C%20qualitative%2C%20and%20subjective%20evaluations%20on%20the%20GRAB%0Adataset%20alone%20clearly%20indicate%20HOIDiNi%20outperforms%20prior%20works%20and%20baselines%20in%0Acontact%20accuracy%2C%20physical%20validity%2C%20and%20overall%20quality.%20Our%20results%0Ademonstrate%20the%20ability%20to%20generate%20complex%2C%20controllable%20interactions%2C%0Aincluding%20grasping%2C%20placing%2C%20and%20full-body%20coordination%2C%20driven%20solely%20by%0Atextual%20prompts.%20https%3A//hoidini.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15625v1&entry.124074799=Read"},
{"title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with\n  Production-Ready PBR Material", "author": "Team Hunyuan3D and Shuhui Yang and Mingxin Yang and Yifei Feng and Xin Huang and Sheng Zhang and Zebin He and Di Luo and Haolin Liu and Yunfei Zhao and Qingxiang Lin and Zeqiang Lai and Xianghui Yang and Huiwen Shi and Zibo Zhao and Bowen Zhang and Hongyu Yan and Lifu Wang and Sicong Liu and Jihong Zhang and Meng Chen and Liang Dong and Yiwen Jia and Yulin Cai and Jiaao Yu and Yixuan Tang and Dongyuan Guo and Junlin Yu and Hao Zhang and Zheng Ye and Peng He and Runzhou Wu and Shida Wei and Chao Zhang and Yonghao Tan and Yifu Sun and Lin Niu and Shirui Huang and Bojian Zheng and Shu Liu and Shilin Chen and Xiang Yuan and Xiaofeng Yang and Kai Liu and Jianchen Zhu and Peng Chen and Tian Liu and Di Wang and Yuhong Liu and  Linus and Jie Jiang and Jingwei Huang and Chunchao Guo", "abstract": "  3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.\n", "link": "http://arxiv.org/abs/2506.15442v1", "date": "2025-06-18", "relevancy": 2.9652, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6041}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6041}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunyuan3D%202.1%3A%20From%20Images%20to%20High-Fidelity%203D%20Assets%20with%0A%20%20Production-Ready%20PBR%20Material&body=Title%3A%20Hunyuan3D%202.1%3A%20From%20Images%20to%20High-Fidelity%203D%20Assets%20with%0A%20%20Production-Ready%20PBR%20Material%0AAuthor%3A%20Team%20Hunyuan3D%20and%20Shuhui%20Yang%20and%20Mingxin%20Yang%20and%20Yifei%20Feng%20and%20Xin%20Huang%20and%20Sheng%20Zhang%20and%20Zebin%20He%20and%20Di%20Luo%20and%20Haolin%20Liu%20and%20Yunfei%20Zhao%20and%20Qingxiang%20Lin%20and%20Zeqiang%20Lai%20and%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Zibo%20Zhao%20and%20Bowen%20Zhang%20and%20Hongyu%20Yan%20and%20Lifu%20Wang%20and%20Sicong%20Liu%20and%20Jihong%20Zhang%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Yiwen%20Jia%20and%20Yulin%20Cai%20and%20Jiaao%20Yu%20and%20Yixuan%20Tang%20and%20Dongyuan%20Guo%20and%20Junlin%20Yu%20and%20Hao%20Zhang%20and%20Zheng%20Ye%20and%20Peng%20He%20and%20Runzhou%20Wu%20and%20Shida%20Wei%20and%20Chao%20Zhang%20and%20Yonghao%20Tan%20and%20Yifu%20Sun%20and%20Lin%20Niu%20and%20Shirui%20Huang%20and%20Bojian%20Zheng%20and%20Shu%20Liu%20and%20Shilin%20Chen%20and%20Xiang%20Yuan%20and%20Xiaofeng%20Yang%20and%20Kai%20Liu%20and%20Jianchen%20Zhu%20and%20Peng%20Chen%20and%20Tian%20Liu%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20%20Linus%20and%20Jie%20Jiang%20and%20Jingwei%20Huang%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%203D%20AI-generated%20content%20%28AIGC%29%20is%20a%20passionate%20field%20that%20has%20significantly%0Aaccelerated%20the%20creation%20of%203D%20models%20in%20gaming%2C%20film%2C%20and%20design.%20Despite%20the%0Adevelopment%20of%20several%20groundbreaking%20models%20that%20have%20revolutionized%203D%0Ageneration%2C%20the%20field%20remains%20largely%20accessible%20only%20to%20researchers%2C%0Adevelopers%2C%20and%20designers%20due%20to%20the%20complexities%20involved%20in%20collecting%2C%0Aprocessing%2C%20and%20training%203D%20models.%20To%20address%20these%20challenges%2C%20we%20introduce%0AHunyuan3D%202.1%20as%20a%20case%20study%20in%20this%20tutorial.%20This%20tutorial%20offers%20a%0Acomprehensive%2C%20step-by-step%20guide%20on%20processing%203D%20data%2C%20training%20a%203D%0Agenerative%20model%2C%20and%20evaluating%20its%20performance%20using%20Hunyuan3D%202.1%2C%20an%0Aadvanced%20system%20for%20producing%20high-resolution%2C%20textured%203D%20assets.%20The%20system%0Acomprises%20two%20core%20components%3A%20the%20Hunyuan3D-DiT%20for%20shape%20generation%20and%20the%0AHunyuan3D-Paint%20for%20texture%20synthesis.%20We%20will%20explore%20the%20entire%20workflow%2C%0Aincluding%20data%20preparation%2C%20model%20architecture%2C%20training%20strategies%2C%20evaluation%0Ametrics%2C%20and%20deployment.%20By%20the%20conclusion%20of%20this%20tutorial%2C%20you%20will%20have%20the%0Aknowledge%20to%20finetune%20or%20develop%20a%20robust%203D%20generative%20model%20suitable%20for%0Aapplications%20in%20gaming%2C%20virtual%20reality%2C%20and%20industrial%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuan3D%25202.1%253A%2520From%2520Images%2520to%2520High-Fidelity%25203D%2520Assets%2520with%250A%2520%2520Production-Ready%2520PBR%2520Material%26entry.906535625%3DTeam%2520Hunyuan3D%2520and%2520Shuhui%2520Yang%2520and%2520Mingxin%2520Yang%2520and%2520Yifei%2520Feng%2520and%2520Xin%2520Huang%2520and%2520Sheng%2520Zhang%2520and%2520Zebin%2520He%2520and%2520Di%2520Luo%2520and%2520Haolin%2520Liu%2520and%2520Yunfei%2520Zhao%2520and%2520Qingxiang%2520Lin%2520and%2520Zeqiang%2520Lai%2520and%2520Xianghui%2520Yang%2520and%2520Huiwen%2520Shi%2520and%2520Zibo%2520Zhao%2520and%2520Bowen%2520Zhang%2520and%2520Hongyu%2520Yan%2520and%2520Lifu%2520Wang%2520and%2520Sicong%2520Liu%2520and%2520Jihong%2520Zhang%2520and%2520Meng%2520Chen%2520and%2520Liang%2520Dong%2520and%2520Yiwen%2520Jia%2520and%2520Yulin%2520Cai%2520and%2520Jiaao%2520Yu%2520and%2520Yixuan%2520Tang%2520and%2520Dongyuan%2520Guo%2520and%2520Junlin%2520Yu%2520and%2520Hao%2520Zhang%2520and%2520Zheng%2520Ye%2520and%2520Peng%2520He%2520and%2520Runzhou%2520Wu%2520and%2520Shida%2520Wei%2520and%2520Chao%2520Zhang%2520and%2520Yonghao%2520Tan%2520and%2520Yifu%2520Sun%2520and%2520Lin%2520Niu%2520and%2520Shirui%2520Huang%2520and%2520Bojian%2520Zheng%2520and%2520Shu%2520Liu%2520and%2520Shilin%2520Chen%2520and%2520Xiang%2520Yuan%2520and%2520Xiaofeng%2520Yang%2520and%2520Kai%2520Liu%2520and%2520Jianchen%2520Zhu%2520and%2520Peng%2520Chen%2520and%2520Tian%2520Liu%2520and%2520Di%2520Wang%2520and%2520Yuhong%2520Liu%2520and%2520%2520Linus%2520and%2520Jie%2520Jiang%2520and%2520Jingwei%2520Huang%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%25203D%2520AI-generated%2520content%2520%2528AIGC%2529%2520is%2520a%2520passionate%2520field%2520that%2520has%2520significantly%250Aaccelerated%2520the%2520creation%2520of%25203D%2520models%2520in%2520gaming%252C%2520film%252C%2520and%2520design.%2520Despite%2520the%250Adevelopment%2520of%2520several%2520groundbreaking%2520models%2520that%2520have%2520revolutionized%25203D%250Ageneration%252C%2520the%2520field%2520remains%2520largely%2520accessible%2520only%2520to%2520researchers%252C%250Adevelopers%252C%2520and%2520designers%2520due%2520to%2520the%2520complexities%2520involved%2520in%2520collecting%252C%250Aprocessing%252C%2520and%2520training%25203D%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AHunyuan3D%25202.1%2520as%2520a%2520case%2520study%2520in%2520this%2520tutorial.%2520This%2520tutorial%2520offers%2520a%250Acomprehensive%252C%2520step-by-step%2520guide%2520on%2520processing%25203D%2520data%252C%2520training%2520a%25203D%250Agenerative%2520model%252C%2520and%2520evaluating%2520its%2520performance%2520using%2520Hunyuan3D%25202.1%252C%2520an%250Aadvanced%2520system%2520for%2520producing%2520high-resolution%252C%2520textured%25203D%2520assets.%2520The%2520system%250Acomprises%2520two%2520core%2520components%253A%2520the%2520Hunyuan3D-DiT%2520for%2520shape%2520generation%2520and%2520the%250AHunyuan3D-Paint%2520for%2520texture%2520synthesis.%2520We%2520will%2520explore%2520the%2520entire%2520workflow%252C%250Aincluding%2520data%2520preparation%252C%2520model%2520architecture%252C%2520training%2520strategies%252C%2520evaluation%250Ametrics%252C%2520and%2520deployment.%2520By%2520the%2520conclusion%2520of%2520this%2520tutorial%252C%2520you%2520will%2520have%2520the%250Aknowledge%2520to%2520finetune%2520or%2520develop%2520a%2520robust%25203D%2520generative%2520model%2520suitable%2520for%250Aapplications%2520in%2520gaming%252C%2520virtual%2520reality%252C%2520and%2520industrial%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunyuan3D%202.1%3A%20From%20Images%20to%20High-Fidelity%203D%20Assets%20with%0A%20%20Production-Ready%20PBR%20Material&entry.906535625=Team%20Hunyuan3D%20and%20Shuhui%20Yang%20and%20Mingxin%20Yang%20and%20Yifei%20Feng%20and%20Xin%20Huang%20and%20Sheng%20Zhang%20and%20Zebin%20He%20and%20Di%20Luo%20and%20Haolin%20Liu%20and%20Yunfei%20Zhao%20and%20Qingxiang%20Lin%20and%20Zeqiang%20Lai%20and%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Zibo%20Zhao%20and%20Bowen%20Zhang%20and%20Hongyu%20Yan%20and%20Lifu%20Wang%20and%20Sicong%20Liu%20and%20Jihong%20Zhang%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Yiwen%20Jia%20and%20Yulin%20Cai%20and%20Jiaao%20Yu%20and%20Yixuan%20Tang%20and%20Dongyuan%20Guo%20and%20Junlin%20Yu%20and%20Hao%20Zhang%20and%20Zheng%20Ye%20and%20Peng%20He%20and%20Runzhou%20Wu%20and%20Shida%20Wei%20and%20Chao%20Zhang%20and%20Yonghao%20Tan%20and%20Yifu%20Sun%20and%20Lin%20Niu%20and%20Shirui%20Huang%20and%20Bojian%20Zheng%20and%20Shu%20Liu%20and%20Shilin%20Chen%20and%20Xiang%20Yuan%20and%20Xiaofeng%20Yang%20and%20Kai%20Liu%20and%20Jianchen%20Zhu%20and%20Peng%20Chen%20and%20Tian%20Liu%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20%20Linus%20and%20Jie%20Jiang%20and%20Jingwei%20Huang%20and%20Chunchao%20Guo&entry.1292438233=%20%203D%20AI-generated%20content%20%28AIGC%29%20is%20a%20passionate%20field%20that%20has%20significantly%0Aaccelerated%20the%20creation%20of%203D%20models%20in%20gaming%2C%20film%2C%20and%20design.%20Despite%20the%0Adevelopment%20of%20several%20groundbreaking%20models%20that%20have%20revolutionized%203D%0Ageneration%2C%20the%20field%20remains%20largely%20accessible%20only%20to%20researchers%2C%0Adevelopers%2C%20and%20designers%20due%20to%20the%20complexities%20involved%20in%20collecting%2C%0Aprocessing%2C%20and%20training%203D%20models.%20To%20address%20these%20challenges%2C%20we%20introduce%0AHunyuan3D%202.1%20as%20a%20case%20study%20in%20this%20tutorial.%20This%20tutorial%20offers%20a%0Acomprehensive%2C%20step-by-step%20guide%20on%20processing%203D%20data%2C%20training%20a%203D%0Agenerative%20model%2C%20and%20evaluating%20its%20performance%20using%20Hunyuan3D%202.1%2C%20an%0Aadvanced%20system%20for%20producing%20high-resolution%2C%20textured%203D%20assets.%20The%20system%0Acomprises%20two%20core%20components%3A%20the%20Hunyuan3D-DiT%20for%20shape%20generation%20and%20the%0AHunyuan3D-Paint%20for%20texture%20synthesis.%20We%20will%20explore%20the%20entire%20workflow%2C%0Aincluding%20data%20preparation%2C%20model%20architecture%2C%20training%20strategies%2C%20evaluation%0Ametrics%2C%20and%20deployment.%20By%20the%20conclusion%20of%20this%20tutorial%2C%20you%20will%20have%20the%0Aknowledge%20to%20finetune%20or%20develop%20a%20robust%203D%20generative%20model%20suitable%20for%0Aapplications%20in%20gaming%2C%20virtual%20reality%2C%20and%20industrial%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15442v1&entry.124074799=Read"},
{"title": "Instance-Adaptive Keypoint Learning with Local-to-Global Geometric\n  Aggregation for Category-Level Object Pose Estimation", "author": "Xiao Zhang and Lu Zou and Tao Lu and Yuan Yao and Zhangjin Huang and Guoping Wang", "abstract": "  Category-level object pose estimation aims to predict the 6D pose and size of\npreviously unseen instances from predefined categories, requiring strong\ngeneralization across diverse object instances. Although many previous methods\nattempt to mitigate intra-class variations, they often struggle with instances\nexhibiting complex geometries or significant deviations from canonical shapes.\nTo address this issue, we propose INKL-Pose, a novel category-level object pose\nestimation framework that enables INstance-adaptive Keypoint Learning with\nlocal-to-global geometric aggregation. Specifically, our method first predicts\nsemantically consistent and geometrically informative keypoints using an\nInstance-Adaptive Keypoint Detector, then refines them: (1) a Local Keypoint\nFeature Aggregator capturing fine-grained geometries, and (2) a Global Keypoint\nFeature Aggregator using bidirectional Mamba for structural consistency. To\nenable bidirectional modeling in Mamba, we introduce a simple yet effective\nFeature Sequence Flipping strategy that preserves spatial coherence while\nconstructing backward feature sequence. Additionally, we design a surface loss\nand a separation loss to encourage uniform coverage and spatial diversity in\nkeypoint distribution. The resulting keypoints are mapped to a canonical space\nfor 6D pose and size regression. Extensive experiments on CAMERA25, REAL275,\nand HouseCat6D show that INKL-Pose achieves state-of-the-art performance with\n16.7M parameters and runs at 36 FPS on an NVIDIA RTX 4090D GPU.\n", "link": "http://arxiv.org/abs/2504.15134v3", "date": "2025-06-18", "relevancy": 2.9342, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6317}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Adaptive%20Keypoint%20Learning%20with%20Local-to-Global%20Geometric%0A%20%20Aggregation%20for%20Category-Level%20Object%20Pose%20Estimation&body=Title%3A%20Instance-Adaptive%20Keypoint%20Learning%20with%20Local-to-Global%20Geometric%0A%20%20Aggregation%20for%20Category-Level%20Object%20Pose%20Estimation%0AAuthor%3A%20Xiao%20Zhang%20and%20Lu%20Zou%20and%20Tao%20Lu%20and%20Yuan%20Yao%20and%20Zhangjin%20Huang%20and%20Guoping%20Wang%0AAbstract%3A%20%20%20Category-level%20object%20pose%20estimation%20aims%20to%20predict%20the%206D%20pose%20and%20size%20of%0Apreviously%20unseen%20instances%20from%20predefined%20categories%2C%20requiring%20strong%0Ageneralization%20across%20diverse%20object%20instances.%20Although%20many%20previous%20methods%0Aattempt%20to%20mitigate%20intra-class%20variations%2C%20they%20often%20struggle%20with%20instances%0Aexhibiting%20complex%20geometries%20or%20significant%20deviations%20from%20canonical%20shapes.%0ATo%20address%20this%20issue%2C%20we%20propose%20INKL-Pose%2C%20a%20novel%20category-level%20object%20pose%0Aestimation%20framework%20that%20enables%20INstance-adaptive%20Keypoint%20Learning%20with%0Alocal-to-global%20geometric%20aggregation.%20Specifically%2C%20our%20method%20first%20predicts%0Asemantically%20consistent%20and%20geometrically%20informative%20keypoints%20using%20an%0AInstance-Adaptive%20Keypoint%20Detector%2C%20then%20refines%20them%3A%20%281%29%20a%20Local%20Keypoint%0AFeature%20Aggregator%20capturing%20fine-grained%20geometries%2C%20and%20%282%29%20a%20Global%20Keypoint%0AFeature%20Aggregator%20using%20bidirectional%20Mamba%20for%20structural%20consistency.%20To%0Aenable%20bidirectional%20modeling%20in%20Mamba%2C%20we%20introduce%20a%20simple%20yet%20effective%0AFeature%20Sequence%20Flipping%20strategy%20that%20preserves%20spatial%20coherence%20while%0Aconstructing%20backward%20feature%20sequence.%20Additionally%2C%20we%20design%20a%20surface%20loss%0Aand%20a%20separation%20loss%20to%20encourage%20uniform%20coverage%20and%20spatial%20diversity%20in%0Akeypoint%20distribution.%20The%20resulting%20keypoints%20are%20mapped%20to%20a%20canonical%20space%0Afor%206D%20pose%20and%20size%20regression.%20Extensive%20experiments%20on%20CAMERA25%2C%20REAL275%2C%0Aand%20HouseCat6D%20show%20that%20INKL-Pose%20achieves%20state-of-the-art%20performance%20with%0A16.7M%20parameters%20and%20runs%20at%2036%20FPS%20on%20an%20NVIDIA%20RTX%204090D%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15134v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Adaptive%2520Keypoint%2520Learning%2520with%2520Local-to-Global%2520Geometric%250A%2520%2520Aggregation%2520for%2520Category-Level%2520Object%2520Pose%2520Estimation%26entry.906535625%3DXiao%2520Zhang%2520and%2520Lu%2520Zou%2520and%2520Tao%2520Lu%2520and%2520Yuan%2520Yao%2520and%2520Zhangjin%2520Huang%2520and%2520Guoping%2520Wang%26entry.1292438233%3D%2520%2520Category-level%2520object%2520pose%2520estimation%2520aims%2520to%2520predict%2520the%25206D%2520pose%2520and%2520size%2520of%250Apreviously%2520unseen%2520instances%2520from%2520predefined%2520categories%252C%2520requiring%2520strong%250Ageneralization%2520across%2520diverse%2520object%2520instances.%2520Although%2520many%2520previous%2520methods%250Aattempt%2520to%2520mitigate%2520intra-class%2520variations%252C%2520they%2520often%2520struggle%2520with%2520instances%250Aexhibiting%2520complex%2520geometries%2520or%2520significant%2520deviations%2520from%2520canonical%2520shapes.%250ATo%2520address%2520this%2520issue%252C%2520we%2520propose%2520INKL-Pose%252C%2520a%2520novel%2520category-level%2520object%2520pose%250Aestimation%2520framework%2520that%2520enables%2520INstance-adaptive%2520Keypoint%2520Learning%2520with%250Alocal-to-global%2520geometric%2520aggregation.%2520Specifically%252C%2520our%2520method%2520first%2520predicts%250Asemantically%2520consistent%2520and%2520geometrically%2520informative%2520keypoints%2520using%2520an%250AInstance-Adaptive%2520Keypoint%2520Detector%252C%2520then%2520refines%2520them%253A%2520%25281%2529%2520a%2520Local%2520Keypoint%250AFeature%2520Aggregator%2520capturing%2520fine-grained%2520geometries%252C%2520and%2520%25282%2529%2520a%2520Global%2520Keypoint%250AFeature%2520Aggregator%2520using%2520bidirectional%2520Mamba%2520for%2520structural%2520consistency.%2520To%250Aenable%2520bidirectional%2520modeling%2520in%2520Mamba%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%250AFeature%2520Sequence%2520Flipping%2520strategy%2520that%2520preserves%2520spatial%2520coherence%2520while%250Aconstructing%2520backward%2520feature%2520sequence.%2520Additionally%252C%2520we%2520design%2520a%2520surface%2520loss%250Aand%2520a%2520separation%2520loss%2520to%2520encourage%2520uniform%2520coverage%2520and%2520spatial%2520diversity%2520in%250Akeypoint%2520distribution.%2520The%2520resulting%2520keypoints%2520are%2520mapped%2520to%2520a%2520canonical%2520space%250Afor%25206D%2520pose%2520and%2520size%2520regression.%2520Extensive%2520experiments%2520on%2520CAMERA25%252C%2520REAL275%252C%250Aand%2520HouseCat6D%2520show%2520that%2520INKL-Pose%2520achieves%2520state-of-the-art%2520performance%2520with%250A16.7M%2520parameters%2520and%2520runs%2520at%252036%2520FPS%2520on%2520an%2520NVIDIA%2520RTX%25204090D%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15134v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Adaptive%20Keypoint%20Learning%20with%20Local-to-Global%20Geometric%0A%20%20Aggregation%20for%20Category-Level%20Object%20Pose%20Estimation&entry.906535625=Xiao%20Zhang%20and%20Lu%20Zou%20and%20Tao%20Lu%20and%20Yuan%20Yao%20and%20Zhangjin%20Huang%20and%20Guoping%20Wang&entry.1292438233=%20%20Category-level%20object%20pose%20estimation%20aims%20to%20predict%20the%206D%20pose%20and%20size%20of%0Apreviously%20unseen%20instances%20from%20predefined%20categories%2C%20requiring%20strong%0Ageneralization%20across%20diverse%20object%20instances.%20Although%20many%20previous%20methods%0Aattempt%20to%20mitigate%20intra-class%20variations%2C%20they%20often%20struggle%20with%20instances%0Aexhibiting%20complex%20geometries%20or%20significant%20deviations%20from%20canonical%20shapes.%0ATo%20address%20this%20issue%2C%20we%20propose%20INKL-Pose%2C%20a%20novel%20category-level%20object%20pose%0Aestimation%20framework%20that%20enables%20INstance-adaptive%20Keypoint%20Learning%20with%0Alocal-to-global%20geometric%20aggregation.%20Specifically%2C%20our%20method%20first%20predicts%0Asemantically%20consistent%20and%20geometrically%20informative%20keypoints%20using%20an%0AInstance-Adaptive%20Keypoint%20Detector%2C%20then%20refines%20them%3A%20%281%29%20a%20Local%20Keypoint%0AFeature%20Aggregator%20capturing%20fine-grained%20geometries%2C%20and%20%282%29%20a%20Global%20Keypoint%0AFeature%20Aggregator%20using%20bidirectional%20Mamba%20for%20structural%20consistency.%20To%0Aenable%20bidirectional%20modeling%20in%20Mamba%2C%20we%20introduce%20a%20simple%20yet%20effective%0AFeature%20Sequence%20Flipping%20strategy%20that%20preserves%20spatial%20coherence%20while%0Aconstructing%20backward%20feature%20sequence.%20Additionally%2C%20we%20design%20a%20surface%20loss%0Aand%20a%20separation%20loss%20to%20encourage%20uniform%20coverage%20and%20spatial%20diversity%20in%0Akeypoint%20distribution.%20The%20resulting%20keypoints%20are%20mapped%20to%20a%20canonical%20space%0Afor%206D%20pose%20and%20size%20regression.%20Extensive%20experiments%20on%20CAMERA25%2C%20REAL275%2C%0Aand%20HouseCat6D%20show%20that%20INKL-Pose%20achieves%20state-of-the-art%20performance%20with%0A16.7M%20parameters%20and%20runs%20at%2036%20FPS%20on%20an%20NVIDIA%20RTX%204090D%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15134v3&entry.124074799=Read"},
{"title": "Creating User-steerable Projections with Interactive Semantic Mapping", "author": "Artur Andr\u00e9 Oliveira and Mateus Espadoto and Roberto Hirata Jr. and Roberto M. Cesar Jr. and Alex C. Telea", "abstract": "  Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.\n", "link": "http://arxiv.org/abs/2506.15479v1", "date": "2025-06-18", "relevancy": 2.8227, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creating%20User-steerable%20Projections%20with%20Interactive%20Semantic%20Mapping&body=Title%3A%20Creating%20User-steerable%20Projections%20with%20Interactive%20Semantic%20Mapping%0AAuthor%3A%20Artur%20Andr%C3%A9%20Oliveira%20and%20Mateus%20Espadoto%20and%20Roberto%20Hirata%20Jr.%20and%20Roberto%20M.%20Cesar%20Jr.%20and%20Alex%20C.%20Telea%0AAbstract%3A%20%20%20Dimensionality%20reduction%20%28DR%29%20techniques%20map%20high-dimensional%20data%20into%0Alower-dimensional%20spaces.%20Yet%2C%20current%20DR%20techniques%20are%20not%20designed%20to%0Aexplore%20semantic%20structure%20that%20is%20not%20directly%20available%20in%20the%20form%20of%0Avariables%20or%20class%20labels.%20We%20introduce%20a%20novel%20user-guided%20projection%0Aframework%20for%20image%20and%20text%20data%20that%20enables%20customizable%2C%20interpretable%2C%0Adata%20visualizations%20via%20zero-shot%20classification%20with%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20We%20enable%20users%20to%20steer%20projections%20dynamically%20via%0Anatural-language%20guiding%20prompts%2C%20to%20specify%20high-level%20semantic%20relationships%0Aof%20interest%20to%20the%20users%20which%20are%20not%20explicitly%20present%20in%20the%20data%0Adimensions.%20We%20evaluate%20our%20method%20across%20several%20datasets%20and%20show%20that%20it%20not%0Aonly%20enhances%20cluster%20separation%2C%20but%20also%20transforms%20DR%20into%20an%20interactive%2C%0Auser-driven%20process.%20Our%20approach%20bridges%20the%20gap%20between%20fully%20automated%20DR%0Atechniques%20and%20human-centered%20data%20exploration%2C%20offering%20a%20flexible%20and%0Aadaptive%20way%20to%20tailor%20projections%20to%20specific%20analytical%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreating%2520User-steerable%2520Projections%2520with%2520Interactive%2520Semantic%2520Mapping%26entry.906535625%3DArtur%2520Andr%25C3%25A9%2520Oliveira%2520and%2520Mateus%2520Espadoto%2520and%2520Roberto%2520Hirata%2520Jr.%2520and%2520Roberto%2520M.%2520Cesar%2520Jr.%2520and%2520Alex%2520C.%2520Telea%26entry.1292438233%3D%2520%2520Dimensionality%2520reduction%2520%2528DR%2529%2520techniques%2520map%2520high-dimensional%2520data%2520into%250Alower-dimensional%2520spaces.%2520Yet%252C%2520current%2520DR%2520techniques%2520are%2520not%2520designed%2520to%250Aexplore%2520semantic%2520structure%2520that%2520is%2520not%2520directly%2520available%2520in%2520the%2520form%2520of%250Avariables%2520or%2520class%2520labels.%2520We%2520introduce%2520a%2520novel%2520user-guided%2520projection%250Aframework%2520for%2520image%2520and%2520text%2520data%2520that%2520enables%2520customizable%252C%2520interpretable%252C%250Adata%2520visualizations%2520via%2520zero-shot%2520classification%2520with%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529.%2520We%2520enable%2520users%2520to%2520steer%2520projections%2520dynamically%2520via%250Anatural-language%2520guiding%2520prompts%252C%2520to%2520specify%2520high-level%2520semantic%2520relationships%250Aof%2520interest%2520to%2520the%2520users%2520which%2520are%2520not%2520explicitly%2520present%2520in%2520the%2520data%250Adimensions.%2520We%2520evaluate%2520our%2520method%2520across%2520several%2520datasets%2520and%2520show%2520that%2520it%2520not%250Aonly%2520enhances%2520cluster%2520separation%252C%2520but%2520also%2520transforms%2520DR%2520into%2520an%2520interactive%252C%250Auser-driven%2520process.%2520Our%2520approach%2520bridges%2520the%2520gap%2520between%2520fully%2520automated%2520DR%250Atechniques%2520and%2520human-centered%2520data%2520exploration%252C%2520offering%2520a%2520flexible%2520and%250Aadaptive%2520way%2520to%2520tailor%2520projections%2520to%2520specific%2520analytical%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20User-steerable%20Projections%20with%20Interactive%20Semantic%20Mapping&entry.906535625=Artur%20Andr%C3%A9%20Oliveira%20and%20Mateus%20Espadoto%20and%20Roberto%20Hirata%20Jr.%20and%20Roberto%20M.%20Cesar%20Jr.%20and%20Alex%20C.%20Telea&entry.1292438233=%20%20Dimensionality%20reduction%20%28DR%29%20techniques%20map%20high-dimensional%20data%20into%0Alower-dimensional%20spaces.%20Yet%2C%20current%20DR%20techniques%20are%20not%20designed%20to%0Aexplore%20semantic%20structure%20that%20is%20not%20directly%20available%20in%20the%20form%20of%0Avariables%20or%20class%20labels.%20We%20introduce%20a%20novel%20user-guided%20projection%0Aframework%20for%20image%20and%20text%20data%20that%20enables%20customizable%2C%20interpretable%2C%0Adata%20visualizations%20via%20zero-shot%20classification%20with%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20We%20enable%20users%20to%20steer%20projections%20dynamically%20via%0Anatural-language%20guiding%20prompts%2C%20to%20specify%20high-level%20semantic%20relationships%0Aof%20interest%20to%20the%20users%20which%20are%20not%20explicitly%20present%20in%20the%20data%0Adimensions.%20We%20evaluate%20our%20method%20across%20several%20datasets%20and%20show%20that%20it%20not%0Aonly%20enhances%20cluster%20separation%2C%20but%20also%20transforms%20DR%20into%20an%20interactive%2C%0Auser-driven%20process.%20Our%20approach%20bridges%20the%20gap%20between%20fully%20automated%20DR%0Atechniques%20and%20human-centered%20data%20exploration%2C%20offering%20a%20flexible%20and%0Aadaptive%20way%20to%20tailor%20projections%20to%20specific%20analytical%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15479v1&entry.124074799=Read"},
{"title": "RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth\n  Estimation", "author": "Xingrui Qin and Wentao Zhao and Chuan Cao and Yihe Niu and Houcheng Jiang and Jingchuan Wang", "abstract": "  Dense metric depth estimation using millimeter-wave radar typically requires\ndense LiDAR supervision, generated via multi-frame projection and\ninterpolation, to guide the learning of accurate depth from sparse radar\nmeasurements and RGB images. However, this paradigm is both costly and\ndata-intensive. To address this, we propose RaCalNet, a novel framework that\neliminates the need for dense supervision by using sparse LiDAR to supervise\nthe learning of refined radar measurements, resulting in a supervision density\nof merely around 1% compared to dense-supervised methods. Unlike previous\napproaches that associate radar points with broad image regions and rely\nheavily on dense labels, RaCalNet first recalibrates and refines sparse radar\npoints to construct accurate depth priors. These priors then serve as reliable\nanchors to guide monocular depth prediction, enabling metric-scale estimation\nwithout resorting to dense supervision. This design improves structural\nconsistency and preserves fine details. Despite relying solely on sparse\nsupervision, RaCalNet surpasses state-of-the-art dense-supervised methods,\nproducing depth maps with clear object contours and fine-grained textures.\nExtensive experiments on the ZJU-4DRadarCam dataset and real-world deployment\nscenarios demonstrate its effectiveness, reducing RMSE by 35.30% and 34.89%,\nrespectively.\n", "link": "http://arxiv.org/abs/2506.15560v1", "date": "2025-06-18", "relevancy": 2.7969, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5799}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5633}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaCalNet%3A%20Radar%20Calibration%20Network%20for%20Sparse-Supervised%20Metric%20Depth%0A%20%20Estimation&body=Title%3A%20RaCalNet%3A%20Radar%20Calibration%20Network%20for%20Sparse-Supervised%20Metric%20Depth%0A%20%20Estimation%0AAuthor%3A%20Xingrui%20Qin%20and%20Wentao%20Zhao%20and%20Chuan%20Cao%20and%20Yihe%20Niu%20and%20Houcheng%20Jiang%20and%20Jingchuan%20Wang%0AAbstract%3A%20%20%20Dense%20metric%20depth%20estimation%20using%20millimeter-wave%20radar%20typically%20requires%0Adense%20LiDAR%20supervision%2C%20generated%20via%20multi-frame%20projection%20and%0Ainterpolation%2C%20to%20guide%20the%20learning%20of%20accurate%20depth%20from%20sparse%20radar%0Ameasurements%20and%20RGB%20images.%20However%2C%20this%20paradigm%20is%20both%20costly%20and%0Adata-intensive.%20To%20address%20this%2C%20we%20propose%20RaCalNet%2C%20a%20novel%20framework%20that%0Aeliminates%20the%20need%20for%20dense%20supervision%20by%20using%20sparse%20LiDAR%20to%20supervise%0Athe%20learning%20of%20refined%20radar%20measurements%2C%20resulting%20in%20a%20supervision%20density%0Aof%20merely%20around%201%25%20compared%20to%20dense-supervised%20methods.%20Unlike%20previous%0Aapproaches%20that%20associate%20radar%20points%20with%20broad%20image%20regions%20and%20rely%0Aheavily%20on%20dense%20labels%2C%20RaCalNet%20first%20recalibrates%20and%20refines%20sparse%20radar%0Apoints%20to%20construct%20accurate%20depth%20priors.%20These%20priors%20then%20serve%20as%20reliable%0Aanchors%20to%20guide%20monocular%20depth%20prediction%2C%20enabling%20metric-scale%20estimation%0Awithout%20resorting%20to%20dense%20supervision.%20This%20design%20improves%20structural%0Aconsistency%20and%20preserves%20fine%20details.%20Despite%20relying%20solely%20on%20sparse%0Asupervision%2C%20RaCalNet%20surpasses%20state-of-the-art%20dense-supervised%20methods%2C%0Aproducing%20depth%20maps%20with%20clear%20object%20contours%20and%20fine-grained%20textures.%0AExtensive%20experiments%20on%20the%20ZJU-4DRadarCam%20dataset%20and%20real-world%20deployment%0Ascenarios%20demonstrate%20its%20effectiveness%2C%20reducing%20RMSE%20by%2035.30%25%20and%2034.89%25%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaCalNet%253A%2520Radar%2520Calibration%2520Network%2520for%2520Sparse-Supervised%2520Metric%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DXingrui%2520Qin%2520and%2520Wentao%2520Zhao%2520and%2520Chuan%2520Cao%2520and%2520Yihe%2520Niu%2520and%2520Houcheng%2520Jiang%2520and%2520Jingchuan%2520Wang%26entry.1292438233%3D%2520%2520Dense%2520metric%2520depth%2520estimation%2520using%2520millimeter-wave%2520radar%2520typically%2520requires%250Adense%2520LiDAR%2520supervision%252C%2520generated%2520via%2520multi-frame%2520projection%2520and%250Ainterpolation%252C%2520to%2520guide%2520the%2520learning%2520of%2520accurate%2520depth%2520from%2520sparse%2520radar%250Ameasurements%2520and%2520RGB%2520images.%2520However%252C%2520this%2520paradigm%2520is%2520both%2520costly%2520and%250Adata-intensive.%2520To%2520address%2520this%252C%2520we%2520propose%2520RaCalNet%252C%2520a%2520novel%2520framework%2520that%250Aeliminates%2520the%2520need%2520for%2520dense%2520supervision%2520by%2520using%2520sparse%2520LiDAR%2520to%2520supervise%250Athe%2520learning%2520of%2520refined%2520radar%2520measurements%252C%2520resulting%2520in%2520a%2520supervision%2520density%250Aof%2520merely%2520around%25201%2525%2520compared%2520to%2520dense-supervised%2520methods.%2520Unlike%2520previous%250Aapproaches%2520that%2520associate%2520radar%2520points%2520with%2520broad%2520image%2520regions%2520and%2520rely%250Aheavily%2520on%2520dense%2520labels%252C%2520RaCalNet%2520first%2520recalibrates%2520and%2520refines%2520sparse%2520radar%250Apoints%2520to%2520construct%2520accurate%2520depth%2520priors.%2520These%2520priors%2520then%2520serve%2520as%2520reliable%250Aanchors%2520to%2520guide%2520monocular%2520depth%2520prediction%252C%2520enabling%2520metric-scale%2520estimation%250Awithout%2520resorting%2520to%2520dense%2520supervision.%2520This%2520design%2520improves%2520structural%250Aconsistency%2520and%2520preserves%2520fine%2520details.%2520Despite%2520relying%2520solely%2520on%2520sparse%250Asupervision%252C%2520RaCalNet%2520surpasses%2520state-of-the-art%2520dense-supervised%2520methods%252C%250Aproducing%2520depth%2520maps%2520with%2520clear%2520object%2520contours%2520and%2520fine-grained%2520textures.%250AExtensive%2520experiments%2520on%2520the%2520ZJU-4DRadarCam%2520dataset%2520and%2520real-world%2520deployment%250Ascenarios%2520demonstrate%2520its%2520effectiveness%252C%2520reducing%2520RMSE%2520by%252035.30%2525%2520and%252034.89%2525%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaCalNet%3A%20Radar%20Calibration%20Network%20for%20Sparse-Supervised%20Metric%20Depth%0A%20%20Estimation&entry.906535625=Xingrui%20Qin%20and%20Wentao%20Zhao%20and%20Chuan%20Cao%20and%20Yihe%20Niu%20and%20Houcheng%20Jiang%20and%20Jingchuan%20Wang&entry.1292438233=%20%20Dense%20metric%20depth%20estimation%20using%20millimeter-wave%20radar%20typically%20requires%0Adense%20LiDAR%20supervision%2C%20generated%20via%20multi-frame%20projection%20and%0Ainterpolation%2C%20to%20guide%20the%20learning%20of%20accurate%20depth%20from%20sparse%20radar%0Ameasurements%20and%20RGB%20images.%20However%2C%20this%20paradigm%20is%20both%20costly%20and%0Adata-intensive.%20To%20address%20this%2C%20we%20propose%20RaCalNet%2C%20a%20novel%20framework%20that%0Aeliminates%20the%20need%20for%20dense%20supervision%20by%20using%20sparse%20LiDAR%20to%20supervise%0Athe%20learning%20of%20refined%20radar%20measurements%2C%20resulting%20in%20a%20supervision%20density%0Aof%20merely%20around%201%25%20compared%20to%20dense-supervised%20methods.%20Unlike%20previous%0Aapproaches%20that%20associate%20radar%20points%20with%20broad%20image%20regions%20and%20rely%0Aheavily%20on%20dense%20labels%2C%20RaCalNet%20first%20recalibrates%20and%20refines%20sparse%20radar%0Apoints%20to%20construct%20accurate%20depth%20priors.%20These%20priors%20then%20serve%20as%20reliable%0Aanchors%20to%20guide%20monocular%20depth%20prediction%2C%20enabling%20metric-scale%20estimation%0Awithout%20resorting%20to%20dense%20supervision.%20This%20design%20improves%20structural%0Aconsistency%20and%20preserves%20fine%20details.%20Despite%20relying%20solely%20on%20sparse%0Asupervision%2C%20RaCalNet%20surpasses%20state-of-the-art%20dense-supervised%20methods%2C%0Aproducing%20depth%20maps%20with%20clear%20object%20contours%20and%20fine-grained%20textures.%0AExtensive%20experiments%20on%20the%20ZJU-4DRadarCam%20dataset%20and%20real-world%20deployment%0Ascenarios%20demonstrate%20its%20effectiveness%2C%20reducing%20RMSE%20by%2035.30%25%20and%2034.89%25%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15560v1&entry.124074799=Read"},
{"title": "A Unified Graph-based Framework for Scalable 3D Tree Reconstruction and\n  Non-Destructive Biomass Estimation from Point Clouds", "author": "Di Wang and Shi Li", "abstract": "  Estimating forest above-ground biomass (AGB) is crucial for assessing carbon\nstorage and supporting sustainable forest management. Quantitative Structural\nModel (QSM) offers a non-destructive approach to AGB estimation through 3D tree\nstructural reconstruction. However, current QSM methods face significant\nlimitations, as they are primarily designed for individual trees,depend on\nhigh-quality point cloud data from terrestrial laser scanning (TLS), and also\nrequire multiple pre-processing steps that hinder scalability and practical\ndeployment. This study presents a novel unified framework that enables\nend-to-end processing of large-scale point clouds using an innovative\ngraph-based pipeline. The proposed approach seamlessly integrates tree\nsegmentation,leaf-wood separation and 3D skeletal reconstruction through\ndedicated graph operations including pathing and abstracting for tree topology\nreasoning. Comprehensive validation was conducted on datasets with varying leaf\nconditions (leaf-on and leaf-off), spatial scales (tree- and plot-level), and\ndata sources (TLS and UAV-based laser scanning, ULS). Experimental results\ndemonstrate strong performance under challenging conditions, particularly in\nleaf-on scenarios (~20% relative error) and low-density ULS datasets with\npartial coverage (~30% relative error). These findings indicate that the\nproposed framework provides a robust and scalable solution for large-scale,\nnon-destructive AGB estimation. It significantly reduces dependency on\nspecialized pre-processing tools and establishes ULS as a viable alternative to\nTLS. To our knowledge, this is the first method capable of enabling seamless,\nend-to-end 3D tree reconstruction at operational scales. This advancement\nsubstantially improves the feasibility of QSM-based AGB estimation, paving the\nway for broader applications in forest inventory and climate change research.\n", "link": "http://arxiv.org/abs/2506.15577v1", "date": "2025-06-18", "relevancy": 2.7369, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5817}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5335}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Graph-based%20Framework%20for%20Scalable%203D%20Tree%20Reconstruction%20and%0A%20%20Non-Destructive%20Biomass%20Estimation%20from%20Point%20Clouds&body=Title%3A%20A%20Unified%20Graph-based%20Framework%20for%20Scalable%203D%20Tree%20Reconstruction%20and%0A%20%20Non-Destructive%20Biomass%20Estimation%20from%20Point%20Clouds%0AAuthor%3A%20Di%20Wang%20and%20Shi%20Li%0AAbstract%3A%20%20%20Estimating%20forest%20above-ground%20biomass%20%28AGB%29%20is%20crucial%20for%20assessing%20carbon%0Astorage%20and%20supporting%20sustainable%20forest%20management.%20Quantitative%20Structural%0AModel%20%28QSM%29%20offers%20a%20non-destructive%20approach%20to%20AGB%20estimation%20through%203D%20tree%0Astructural%20reconstruction.%20However%2C%20current%20QSM%20methods%20face%20significant%0Alimitations%2C%20as%20they%20are%20primarily%20designed%20for%20individual%20trees%2Cdepend%20on%0Ahigh-quality%20point%20cloud%20data%20from%20terrestrial%20laser%20scanning%20%28TLS%29%2C%20and%20also%0Arequire%20multiple%20pre-processing%20steps%20that%20hinder%20scalability%20and%20practical%0Adeployment.%20This%20study%20presents%20a%20novel%20unified%20framework%20that%20enables%0Aend-to-end%20processing%20of%20large-scale%20point%20clouds%20using%20an%20innovative%0Agraph-based%20pipeline.%20The%20proposed%20approach%20seamlessly%20integrates%20tree%0Asegmentation%2Cleaf-wood%20separation%20and%203D%20skeletal%20reconstruction%20through%0Adedicated%20graph%20operations%20including%20pathing%20and%20abstracting%20for%20tree%20topology%0Areasoning.%20Comprehensive%20validation%20was%20conducted%20on%20datasets%20with%20varying%20leaf%0Aconditions%20%28leaf-on%20and%20leaf-off%29%2C%20spatial%20scales%20%28tree-%20and%20plot-level%29%2C%20and%0Adata%20sources%20%28TLS%20and%20UAV-based%20laser%20scanning%2C%20ULS%29.%20Experimental%20results%0Ademonstrate%20strong%20performance%20under%20challenging%20conditions%2C%20particularly%20in%0Aleaf-on%20scenarios%20%28~20%25%20relative%20error%29%20and%20low-density%20ULS%20datasets%20with%0Apartial%20coverage%20%28~30%25%20relative%20error%29.%20These%20findings%20indicate%20that%20the%0Aproposed%20framework%20provides%20a%20robust%20and%20scalable%20solution%20for%20large-scale%2C%0Anon-destructive%20AGB%20estimation.%20It%20significantly%20reduces%20dependency%20on%0Aspecialized%20pre-processing%20tools%20and%20establishes%20ULS%20as%20a%20viable%20alternative%20to%0ATLS.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20capable%20of%20enabling%20seamless%2C%0Aend-to-end%203D%20tree%20reconstruction%20at%20operational%20scales.%20This%20advancement%0Asubstantially%20improves%20the%20feasibility%20of%20QSM-based%20AGB%20estimation%2C%20paving%20the%0Away%20for%20broader%20applications%20in%20forest%20inventory%20and%20climate%20change%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Graph-based%2520Framework%2520for%2520Scalable%25203D%2520Tree%2520Reconstruction%2520and%250A%2520%2520Non-Destructive%2520Biomass%2520Estimation%2520from%2520Point%2520Clouds%26entry.906535625%3DDi%2520Wang%2520and%2520Shi%2520Li%26entry.1292438233%3D%2520%2520Estimating%2520forest%2520above-ground%2520biomass%2520%2528AGB%2529%2520is%2520crucial%2520for%2520assessing%2520carbon%250Astorage%2520and%2520supporting%2520sustainable%2520forest%2520management.%2520Quantitative%2520Structural%250AModel%2520%2528QSM%2529%2520offers%2520a%2520non-destructive%2520approach%2520to%2520AGB%2520estimation%2520through%25203D%2520tree%250Astructural%2520reconstruction.%2520However%252C%2520current%2520QSM%2520methods%2520face%2520significant%250Alimitations%252C%2520as%2520they%2520are%2520primarily%2520designed%2520for%2520individual%2520trees%252Cdepend%2520on%250Ahigh-quality%2520point%2520cloud%2520data%2520from%2520terrestrial%2520laser%2520scanning%2520%2528TLS%2529%252C%2520and%2520also%250Arequire%2520multiple%2520pre-processing%2520steps%2520that%2520hinder%2520scalability%2520and%2520practical%250Adeployment.%2520This%2520study%2520presents%2520a%2520novel%2520unified%2520framework%2520that%2520enables%250Aend-to-end%2520processing%2520of%2520large-scale%2520point%2520clouds%2520using%2520an%2520innovative%250Agraph-based%2520pipeline.%2520The%2520proposed%2520approach%2520seamlessly%2520integrates%2520tree%250Asegmentation%252Cleaf-wood%2520separation%2520and%25203D%2520skeletal%2520reconstruction%2520through%250Adedicated%2520graph%2520operations%2520including%2520pathing%2520and%2520abstracting%2520for%2520tree%2520topology%250Areasoning.%2520Comprehensive%2520validation%2520was%2520conducted%2520on%2520datasets%2520with%2520varying%2520leaf%250Aconditions%2520%2528leaf-on%2520and%2520leaf-off%2529%252C%2520spatial%2520scales%2520%2528tree-%2520and%2520plot-level%2529%252C%2520and%250Adata%2520sources%2520%2528TLS%2520and%2520UAV-based%2520laser%2520scanning%252C%2520ULS%2529.%2520Experimental%2520results%250Ademonstrate%2520strong%2520performance%2520under%2520challenging%2520conditions%252C%2520particularly%2520in%250Aleaf-on%2520scenarios%2520%2528~20%2525%2520relative%2520error%2529%2520and%2520low-density%2520ULS%2520datasets%2520with%250Apartial%2520coverage%2520%2528~30%2525%2520relative%2520error%2529.%2520These%2520findings%2520indicate%2520that%2520the%250Aproposed%2520framework%2520provides%2520a%2520robust%2520and%2520scalable%2520solution%2520for%2520large-scale%252C%250Anon-destructive%2520AGB%2520estimation.%2520It%2520significantly%2520reduces%2520dependency%2520on%250Aspecialized%2520pre-processing%2520tools%2520and%2520establishes%2520ULS%2520as%2520a%2520viable%2520alternative%2520to%250ATLS.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520method%2520capable%2520of%2520enabling%2520seamless%252C%250Aend-to-end%25203D%2520tree%2520reconstruction%2520at%2520operational%2520scales.%2520This%2520advancement%250Asubstantially%2520improves%2520the%2520feasibility%2520of%2520QSM-based%2520AGB%2520estimation%252C%2520paving%2520the%250Away%2520for%2520broader%2520applications%2520in%2520forest%2520inventory%2520and%2520climate%2520change%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Graph-based%20Framework%20for%20Scalable%203D%20Tree%20Reconstruction%20and%0A%20%20Non-Destructive%20Biomass%20Estimation%20from%20Point%20Clouds&entry.906535625=Di%20Wang%20and%20Shi%20Li&entry.1292438233=%20%20Estimating%20forest%20above-ground%20biomass%20%28AGB%29%20is%20crucial%20for%20assessing%20carbon%0Astorage%20and%20supporting%20sustainable%20forest%20management.%20Quantitative%20Structural%0AModel%20%28QSM%29%20offers%20a%20non-destructive%20approach%20to%20AGB%20estimation%20through%203D%20tree%0Astructural%20reconstruction.%20However%2C%20current%20QSM%20methods%20face%20significant%0Alimitations%2C%20as%20they%20are%20primarily%20designed%20for%20individual%20trees%2Cdepend%20on%0Ahigh-quality%20point%20cloud%20data%20from%20terrestrial%20laser%20scanning%20%28TLS%29%2C%20and%20also%0Arequire%20multiple%20pre-processing%20steps%20that%20hinder%20scalability%20and%20practical%0Adeployment.%20This%20study%20presents%20a%20novel%20unified%20framework%20that%20enables%0Aend-to-end%20processing%20of%20large-scale%20point%20clouds%20using%20an%20innovative%0Agraph-based%20pipeline.%20The%20proposed%20approach%20seamlessly%20integrates%20tree%0Asegmentation%2Cleaf-wood%20separation%20and%203D%20skeletal%20reconstruction%20through%0Adedicated%20graph%20operations%20including%20pathing%20and%20abstracting%20for%20tree%20topology%0Areasoning.%20Comprehensive%20validation%20was%20conducted%20on%20datasets%20with%20varying%20leaf%0Aconditions%20%28leaf-on%20and%20leaf-off%29%2C%20spatial%20scales%20%28tree-%20and%20plot-level%29%2C%20and%0Adata%20sources%20%28TLS%20and%20UAV-based%20laser%20scanning%2C%20ULS%29.%20Experimental%20results%0Ademonstrate%20strong%20performance%20under%20challenging%20conditions%2C%20particularly%20in%0Aleaf-on%20scenarios%20%28~20%25%20relative%20error%29%20and%20low-density%20ULS%20datasets%20with%0Apartial%20coverage%20%28~30%25%20relative%20error%29.%20These%20findings%20indicate%20that%20the%0Aproposed%20framework%20provides%20a%20robust%20and%20scalable%20solution%20for%20large-scale%2C%0Anon-destructive%20AGB%20estimation.%20It%20significantly%20reduces%20dependency%20on%0Aspecialized%20pre-processing%20tools%20and%20establishes%20ULS%20as%20a%20viable%20alternative%20to%0ATLS.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20capable%20of%20enabling%20seamless%2C%0Aend-to-end%203D%20tree%20reconstruction%20at%20operational%20scales.%20This%20advancement%0Asubstantially%20improves%20the%20feasibility%20of%20QSM-based%20AGB%20estimation%2C%20paving%20the%0Away%20for%20broader%20applications%20in%20forest%20inventory%20and%20climate%20change%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15577v1&entry.124074799=Read"},
{"title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language\n  Model Era: A Survey", "author": "Atsuyuki Miyai and Jingkang Yang and Jingyang Zhang and Yifei Ming and Yueqian Lin and Qing Yu and Go Irie and Shafiq Joty and Yixuan Li and Hai Li and Ziwei Liu and Toshihiko Yamasaki and Kiyoharu Aizawa", "abstract": "  Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of these fields in\nthe VLM era. Our framework reveals that, with some field inactivity and\nintegration, the demanding challenges have become OOD detection and AD. Then,\nwe highlight the significant shift in the definition, problem settings, and\nbenchmarks; we thus feature a comprehensive review of the methodology for OOD\ndetection and related tasks to clarify their relationship to OOD detection.\nFinally, we explore the advancements in the emerging Large Vision Language\nModel (LVLM) era, such as GPT-4V. We conclude with open challenges and future\ndirections. The resource is available at\nhttps://github.com/AtsuMiyai/Awesome-OOD-VLM.\n", "link": "http://arxiv.org/abs/2407.21794v2", "date": "2025-06-18", "relevancy": 2.7368, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.562}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Out-of-Distribution%20Detection%20and%20Beyond%20in%20Vision%20Language%0A%20%20Model%20Era%3A%20A%20Survey&body=Title%3A%20Generalized%20Out-of-Distribution%20Detection%20and%20Beyond%20in%20Vision%20Language%0A%20%20Model%20Era%3A%20A%20Survey%0AAuthor%3A%20Atsuyuki%20Miyai%20and%20Jingkang%20Yang%20and%20Jingyang%20Zhang%20and%20Yifei%20Ming%20and%20Yueqian%20Lin%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Shafiq%20Joty%20and%20Yixuan%20Li%20and%20Hai%20Li%20and%20Ziwei%20Liu%20and%20Toshihiko%20Yamasaki%20and%20Kiyoharu%20Aizawa%0AAbstract%3A%20%20%20Detecting%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20for%20ensuring%20the%0Asafety%20of%20machine%20learning%20systems%20and%20has%20shaped%20the%20field%20of%20OOD%20detection.%0AMeanwhile%2C%20several%20other%20problems%20are%20closely%20related%20to%20OOD%20detection%2C%0Aincluding%20anomaly%20detection%20%28AD%29%2C%20novelty%20detection%20%28ND%29%2C%20open%20set%20recognition%0A%28OSR%29%2C%20and%20outlier%20detection%20%28OD%29.%20To%20unify%20these%20problems%2C%20a%20generalized%20OOD%0Adetection%20framework%20was%20proposed%2C%20taxonomically%20categorizing%20these%20five%0Aproblems.%20However%2C%20Vision%20Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20have%0Asignificantly%20changed%20the%20paradigm%20and%20blurred%20the%20boundaries%20between%20these%0Afields%2C%20again%20confusing%20researchers.%20In%20this%20survey%2C%20we%20first%20present%20a%0Ageneralized%20OOD%20detection%20v2%2C%20encapsulating%20the%20evolution%20of%20these%20fields%20in%0Athe%20VLM%20era.%20Our%20framework%20reveals%20that%2C%20with%20some%20field%20inactivity%20and%0Aintegration%2C%20the%20demanding%20challenges%20have%20become%20OOD%20detection%20and%20AD.%20Then%2C%0Awe%20highlight%20the%20significant%20shift%20in%20the%20definition%2C%20problem%20settings%2C%20and%0Abenchmarks%3B%20we%20thus%20feature%20a%20comprehensive%20review%20of%20the%20methodology%20for%20OOD%0Adetection%20and%20related%20tasks%20to%20clarify%20their%20relationship%20to%20OOD%20detection.%0AFinally%2C%20we%20explore%20the%20advancements%20in%20the%20emerging%20Large%20Vision%20Language%0AModel%20%28LVLM%29%20era%2C%20such%20as%20GPT-4V.%20We%20conclude%20with%20open%20challenges%20and%20future%0Adirections.%20The%20resource%20is%20available%20at%0Ahttps%3A//github.com/AtsuMiyai/Awesome-OOD-VLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Out-of-Distribution%2520Detection%2520and%2520Beyond%2520in%2520Vision%2520Language%250A%2520%2520Model%2520Era%253A%2520A%2520Survey%26entry.906535625%3DAtsuyuki%2520Miyai%2520and%2520Jingkang%2520Yang%2520and%2520Jingyang%2520Zhang%2520and%2520Yifei%2520Ming%2520and%2520Yueqian%2520Lin%2520and%2520Qing%2520Yu%2520and%2520Go%2520Irie%2520and%2520Shafiq%2520Joty%2520and%2520Yixuan%2520Li%2520and%2520Hai%2520Li%2520and%2520Ziwei%2520Liu%2520and%2520Toshihiko%2520Yamasaki%2520and%2520Kiyoharu%2520Aizawa%26entry.1292438233%3D%2520%2520Detecting%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520is%2520crucial%2520for%2520ensuring%2520the%250Asafety%2520of%2520machine%2520learning%2520systems%2520and%2520has%2520shaped%2520the%2520field%2520of%2520OOD%2520detection.%250AMeanwhile%252C%2520several%2520other%2520problems%2520are%2520closely%2520related%2520to%2520OOD%2520detection%252C%250Aincluding%2520anomaly%2520detection%2520%2528AD%2529%252C%2520novelty%2520detection%2520%2528ND%2529%252C%2520open%2520set%2520recognition%250A%2528OSR%2529%252C%2520and%2520outlier%2520detection%2520%2528OD%2529.%2520To%2520unify%2520these%2520problems%252C%2520a%2520generalized%2520OOD%250Adetection%2520framework%2520was%2520proposed%252C%2520taxonomically%2520categorizing%2520these%2520five%250Aproblems.%2520However%252C%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520have%250Asignificantly%2520changed%2520the%2520paradigm%2520and%2520blurred%2520the%2520boundaries%2520between%2520these%250Afields%252C%2520again%2520confusing%2520researchers.%2520In%2520this%2520survey%252C%2520we%2520first%2520present%2520a%250Ageneralized%2520OOD%2520detection%2520v2%252C%2520encapsulating%2520the%2520evolution%2520of%2520these%2520fields%2520in%250Athe%2520VLM%2520era.%2520Our%2520framework%2520reveals%2520that%252C%2520with%2520some%2520field%2520inactivity%2520and%250Aintegration%252C%2520the%2520demanding%2520challenges%2520have%2520become%2520OOD%2520detection%2520and%2520AD.%2520Then%252C%250Awe%2520highlight%2520the%2520significant%2520shift%2520in%2520the%2520definition%252C%2520problem%2520settings%252C%2520and%250Abenchmarks%253B%2520we%2520thus%2520feature%2520a%2520comprehensive%2520review%2520of%2520the%2520methodology%2520for%2520OOD%250Adetection%2520and%2520related%2520tasks%2520to%2520clarify%2520their%2520relationship%2520to%2520OOD%2520detection.%250AFinally%252C%2520we%2520explore%2520the%2520advancements%2520in%2520the%2520emerging%2520Large%2520Vision%2520Language%250AModel%2520%2528LVLM%2529%2520era%252C%2520such%2520as%2520GPT-4V.%2520We%2520conclude%2520with%2520open%2520challenges%2520and%2520future%250Adirections.%2520The%2520resource%2520is%2520available%2520at%250Ahttps%253A//github.com/AtsuMiyai/Awesome-OOD-VLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Out-of-Distribution%20Detection%20and%20Beyond%20in%20Vision%20Language%0A%20%20Model%20Era%3A%20A%20Survey&entry.906535625=Atsuyuki%20Miyai%20and%20Jingkang%20Yang%20and%20Jingyang%20Zhang%20and%20Yifei%20Ming%20and%20Yueqian%20Lin%20and%20Qing%20Yu%20and%20Go%20Irie%20and%20Shafiq%20Joty%20and%20Yixuan%20Li%20and%20Hai%20Li%20and%20Ziwei%20Liu%20and%20Toshihiko%20Yamasaki%20and%20Kiyoharu%20Aizawa&entry.1292438233=%20%20Detecting%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20for%20ensuring%20the%0Asafety%20of%20machine%20learning%20systems%20and%20has%20shaped%20the%20field%20of%20OOD%20detection.%0AMeanwhile%2C%20several%20other%20problems%20are%20closely%20related%20to%20OOD%20detection%2C%0Aincluding%20anomaly%20detection%20%28AD%29%2C%20novelty%20detection%20%28ND%29%2C%20open%20set%20recognition%0A%28OSR%29%2C%20and%20outlier%20detection%20%28OD%29.%20To%20unify%20these%20problems%2C%20a%20generalized%20OOD%0Adetection%20framework%20was%20proposed%2C%20taxonomically%20categorizing%20these%20five%0Aproblems.%20However%2C%20Vision%20Language%20Models%20%28VLMs%29%20such%20as%20CLIP%20have%0Asignificantly%20changed%20the%20paradigm%20and%20blurred%20the%20boundaries%20between%20these%0Afields%2C%20again%20confusing%20researchers.%20In%20this%20survey%2C%20we%20first%20present%20a%0Ageneralized%20OOD%20detection%20v2%2C%20encapsulating%20the%20evolution%20of%20these%20fields%20in%0Athe%20VLM%20era.%20Our%20framework%20reveals%20that%2C%20with%20some%20field%20inactivity%20and%0Aintegration%2C%20the%20demanding%20challenges%20have%20become%20OOD%20detection%20and%20AD.%20Then%2C%0Awe%20highlight%20the%20significant%20shift%20in%20the%20definition%2C%20problem%20settings%2C%20and%0Abenchmarks%3B%20we%20thus%20feature%20a%20comprehensive%20review%20of%20the%20methodology%20for%20OOD%0Adetection%20and%20related%20tasks%20to%20clarify%20their%20relationship%20to%20OOD%20detection.%0AFinally%2C%20we%20explore%20the%20advancements%20in%20the%20emerging%20Large%20Vision%20Language%0AModel%20%28LVLM%29%20era%2C%20such%20as%20GPT-4V.%20We%20conclude%20with%20open%20challenges%20and%20future%0Adirections.%20The%20resource%20is%20available%20at%0Ahttps%3A//github.com/AtsuMiyai/Awesome-OOD-VLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21794v2&entry.124074799=Read"},
{"title": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis\n  for Unseen Objects", "author": "Shujia Li and Haiyu Zhang and Xinyuan Chen and Yaohui Wang and Yutong Ban", "abstract": "  While diffusion models and large-scale motion datasets have advanced\ntext-driven human motion synthesis, extending these advances to 4D human-object\ninteraction (HOI) remains challenging, mainly due to the limited availability\nof large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel\ntwo-stage framework aimed at achieving two key objectives: 1) generalization to\nunseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the\ninitial stage of our framework, we employ an Object-AnchorNet to reconstruct\nsparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI\ndatasets, thereby mitigating the dependence on large-scale 4D HOI datasets.\nSubsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the\nsecond stage to seamlessly interpolate sparse 3D HOI keyframes into densely\ntemporally coherent 4D HOI sequences. To enhance the quality of generated 4D\nHOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to\nextract human-object contact patterns and a novel Contact-Aware HOI Attention\nto effectively integrate the contact signals into diffusion models.\nExperimental results show that we achieve state-of-the-art results on the\npublicly available OMOMO and 3D-FUTURE datasets, demonstrating strong\ngeneralization abilities to unseen objects, while enabling high-fidelity 4D HOI\ngeneration.\n", "link": "http://arxiv.org/abs/2506.15483v1", "date": "2025-06-18", "relevancy": 2.7265, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.73}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6575}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenHOI%3A%20Generalizing%20Text-driven%204D%20Human-Object%20Interaction%20Synthesis%0A%20%20for%20Unseen%20Objects&body=Title%3A%20GenHOI%3A%20Generalizing%20Text-driven%204D%20Human-Object%20Interaction%20Synthesis%0A%20%20for%20Unseen%20Objects%0AAuthor%3A%20Shujia%20Li%20and%20Haiyu%20Zhang%20and%20Xinyuan%20Chen%20and%20Yaohui%20Wang%20and%20Yutong%20Ban%0AAbstract%3A%20%20%20While%20diffusion%20models%20and%20large-scale%20motion%20datasets%20have%20advanced%0Atext-driven%20human%20motion%20synthesis%2C%20extending%20these%20advances%20to%204D%20human-object%0Ainteraction%20%28HOI%29%20remains%20challenging%2C%20mainly%20due%20to%20the%20limited%20availability%0Aof%20large-scale%204D%20HOI%20datasets.%20In%20our%20study%2C%20we%20introduce%20GenHOI%2C%20a%20novel%0Atwo-stage%20framework%20aimed%20at%20achieving%20two%20key%20objectives%3A%201%29%20generalization%20to%0Aunseen%20objects%20and%202%29%20the%20synthesis%20of%20high-fidelity%204D%20HOI%20sequences.%20In%20the%0Ainitial%20stage%20of%20our%20framework%2C%20we%20employ%20an%20Object-AnchorNet%20to%20reconstruct%0Asparse%203D%20HOI%20keyframes%20for%20unseen%20objects%2C%20learning%20solely%20from%203D%20HOI%0Adatasets%2C%20thereby%20mitigating%20the%20dependence%20on%20large-scale%204D%20HOI%20datasets.%0ASubsequently%2C%20we%20introduce%20a%20Contact-Aware%20Diffusion%20Model%20%28ContactDM%29%20in%20the%0Asecond%20stage%20to%20seamlessly%20interpolate%20sparse%203D%20HOI%20keyframes%20into%20densely%0Atemporally%20coherent%204D%20HOI%20sequences.%20To%20enhance%20the%20quality%20of%20generated%204D%0AHOI%20sequences%2C%20we%20propose%20a%20novel%20Contact-Aware%20Encoder%20within%20ContactDM%20to%0Aextract%20human-object%20contact%20patterns%20and%20a%20novel%20Contact-Aware%20HOI%20Attention%0Ato%20effectively%20integrate%20the%20contact%20signals%20into%20diffusion%20models.%0AExperimental%20results%20show%20that%20we%20achieve%20state-of-the-art%20results%20on%20the%0Apublicly%20available%20OMOMO%20and%203D-FUTURE%20datasets%2C%20demonstrating%20strong%0Ageneralization%20abilities%20to%20unseen%20objects%2C%20while%20enabling%20high-fidelity%204D%20HOI%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenHOI%253A%2520Generalizing%2520Text-driven%25204D%2520Human-Object%2520Interaction%2520Synthesis%250A%2520%2520for%2520Unseen%2520Objects%26entry.906535625%3DShujia%2520Li%2520and%2520Haiyu%2520Zhang%2520and%2520Xinyuan%2520Chen%2520and%2520Yaohui%2520Wang%2520and%2520Yutong%2520Ban%26entry.1292438233%3D%2520%2520While%2520diffusion%2520models%2520and%2520large-scale%2520motion%2520datasets%2520have%2520advanced%250Atext-driven%2520human%2520motion%2520synthesis%252C%2520extending%2520these%2520advances%2520to%25204D%2520human-object%250Ainteraction%2520%2528HOI%2529%2520remains%2520challenging%252C%2520mainly%2520due%2520to%2520the%2520limited%2520availability%250Aof%2520large-scale%25204D%2520HOI%2520datasets.%2520In%2520our%2520study%252C%2520we%2520introduce%2520GenHOI%252C%2520a%2520novel%250Atwo-stage%2520framework%2520aimed%2520at%2520achieving%2520two%2520key%2520objectives%253A%25201%2529%2520generalization%2520to%250Aunseen%2520objects%2520and%25202%2529%2520the%2520synthesis%2520of%2520high-fidelity%25204D%2520HOI%2520sequences.%2520In%2520the%250Ainitial%2520stage%2520of%2520our%2520framework%252C%2520we%2520employ%2520an%2520Object-AnchorNet%2520to%2520reconstruct%250Asparse%25203D%2520HOI%2520keyframes%2520for%2520unseen%2520objects%252C%2520learning%2520solely%2520from%25203D%2520HOI%250Adatasets%252C%2520thereby%2520mitigating%2520the%2520dependence%2520on%2520large-scale%25204D%2520HOI%2520datasets.%250ASubsequently%252C%2520we%2520introduce%2520a%2520Contact-Aware%2520Diffusion%2520Model%2520%2528ContactDM%2529%2520in%2520the%250Asecond%2520stage%2520to%2520seamlessly%2520interpolate%2520sparse%25203D%2520HOI%2520keyframes%2520into%2520densely%250Atemporally%2520coherent%25204D%2520HOI%2520sequences.%2520To%2520enhance%2520the%2520quality%2520of%2520generated%25204D%250AHOI%2520sequences%252C%2520we%2520propose%2520a%2520novel%2520Contact-Aware%2520Encoder%2520within%2520ContactDM%2520to%250Aextract%2520human-object%2520contact%2520patterns%2520and%2520a%2520novel%2520Contact-Aware%2520HOI%2520Attention%250Ato%2520effectively%2520integrate%2520the%2520contact%2520signals%2520into%2520diffusion%2520models.%250AExperimental%2520results%2520show%2520that%2520we%2520achieve%2520state-of-the-art%2520results%2520on%2520the%250Apublicly%2520available%2520OMOMO%2520and%25203D-FUTURE%2520datasets%252C%2520demonstrating%2520strong%250Ageneralization%2520abilities%2520to%2520unseen%2520objects%252C%2520while%2520enabling%2520high-fidelity%25204D%2520HOI%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenHOI%3A%20Generalizing%20Text-driven%204D%20Human-Object%20Interaction%20Synthesis%0A%20%20for%20Unseen%20Objects&entry.906535625=Shujia%20Li%20and%20Haiyu%20Zhang%20and%20Xinyuan%20Chen%20and%20Yaohui%20Wang%20and%20Yutong%20Ban&entry.1292438233=%20%20While%20diffusion%20models%20and%20large-scale%20motion%20datasets%20have%20advanced%0Atext-driven%20human%20motion%20synthesis%2C%20extending%20these%20advances%20to%204D%20human-object%0Ainteraction%20%28HOI%29%20remains%20challenging%2C%20mainly%20due%20to%20the%20limited%20availability%0Aof%20large-scale%204D%20HOI%20datasets.%20In%20our%20study%2C%20we%20introduce%20GenHOI%2C%20a%20novel%0Atwo-stage%20framework%20aimed%20at%20achieving%20two%20key%20objectives%3A%201%29%20generalization%20to%0Aunseen%20objects%20and%202%29%20the%20synthesis%20of%20high-fidelity%204D%20HOI%20sequences.%20In%20the%0Ainitial%20stage%20of%20our%20framework%2C%20we%20employ%20an%20Object-AnchorNet%20to%20reconstruct%0Asparse%203D%20HOI%20keyframes%20for%20unseen%20objects%2C%20learning%20solely%20from%203D%20HOI%0Adatasets%2C%20thereby%20mitigating%20the%20dependence%20on%20large-scale%204D%20HOI%20datasets.%0ASubsequently%2C%20we%20introduce%20a%20Contact-Aware%20Diffusion%20Model%20%28ContactDM%29%20in%20the%0Asecond%20stage%20to%20seamlessly%20interpolate%20sparse%203D%20HOI%20keyframes%20into%20densely%0Atemporally%20coherent%204D%20HOI%20sequences.%20To%20enhance%20the%20quality%20of%20generated%204D%0AHOI%20sequences%2C%20we%20propose%20a%20novel%20Contact-Aware%20Encoder%20within%20ContactDM%20to%0Aextract%20human-object%20contact%20patterns%20and%20a%20novel%20Contact-Aware%20HOI%20Attention%0Ato%20effectively%20integrate%20the%20contact%20signals%20into%20diffusion%20models.%0AExperimental%20results%20show%20that%20we%20achieve%20state-of-the-art%20results%20on%20the%0Apublicly%20available%20OMOMO%20and%203D-FUTURE%20datasets%2C%20demonstrating%20strong%0Ageneralization%20abilities%20to%20unseen%20objects%2C%20while%20enabling%20high-fidelity%204D%20HOI%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15483v1&entry.124074799=Read"},
{"title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models", "author": "Justus Westerhoff and Erblina Purelku and Jakob Hackstein and Jonas Loos and Leo Pinetzki and Lorenz Hufe", "abstract": "  Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper along with the code for evaluations at\nwww.bliss.berlin/research/scam.\n", "link": "http://arxiv.org/abs/2504.04893v5", "date": "2025-06-18", "relevancy": 2.7253, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&body=Title%3A%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models%0AAuthor%3A%20Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Lorenz%20Hufe%0AAbstract%3A%20%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20However%2C%20existing%20datasets%20are%20limited%20in%20size%20and%0Adiversity%2C%20making%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%0Aintroduce%20SCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%0Aattack%20images%20to%20date%2C%20containing%201%2C162%20images%20across%20hundreds%20of%20object%0Acategories%20and%20attack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%0AModels%20%28VLMs%29%20on%20SCAM%2C%20we%20demonstrate%20that%20typographic%20attacks%20significantly%0Adegrade%20performance%2C%20and%20identify%20that%20training%20data%20and%20model%20architecture%0Ainfluence%20the%20susceptibility%20to%20these%20attacks.%20Our%20findings%20reveal%20that%0Atypographic%20attacks%20persist%20in%20state-of-the-art%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20due%20to%20the%20choice%20of%20their%20vision%20encoder%2C%20though%20larger%20Large%20Language%0AModels%20%28LLMs%29%20backbones%20help%20mitigate%20their%20vulnerability.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20We%20publicly%20release%20the%20datasets%20introduced%0Ain%20this%20paper%20along%20with%20the%20code%20for%20evaluations%20at%0Awww.bliss.berlin/research/scam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04893v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAM%253A%2520A%2520Real-World%2520Typographic%2520Robustness%2520Evaluation%2520for%2520Multimodal%250A%2520%2520Foundation%2520Models%26entry.906535625%3DJustus%2520Westerhoff%2520and%2520Erblina%2520Purelku%2520and%2520Jakob%2520Hackstein%2520and%2520Jonas%2520Loos%2520and%2520Leo%2520Pinetzki%2520and%2520Lorenz%2520Hufe%26entry.1292438233%3D%2520%2520Typographic%2520attacks%2520exploit%2520the%2520interplay%2520between%2520text%2520and%2520visual%2520content%2520in%250Amultimodal%2520foundation%2520models%252C%2520causing%2520misclassifications%2520when%2520misleading%2520text%250Ais%2520embedded%2520within%2520images.%2520However%252C%2520existing%2520datasets%2520are%2520limited%2520in%2520size%2520and%250Adiversity%252C%2520making%2520it%2520difficult%2520to%2520study%2520such%2520vulnerabilities.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520SCAM%252C%2520the%2520largest%2520and%2520most%2520diverse%2520dataset%2520of%2520real-world%2520typographic%250Aattack%2520images%2520to%2520date%252C%2520containing%25201%252C162%2520images%2520across%2520hundreds%2520of%2520object%250Acategories%2520and%2520attack%2520words.%2520Through%2520extensive%2520benchmarking%2520of%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520on%2520SCAM%252C%2520we%2520demonstrate%2520that%2520typographic%2520attacks%2520significantly%250Adegrade%2520performance%252C%2520and%2520identify%2520that%2520training%2520data%2520and%2520model%2520architecture%250Ainfluence%2520the%2520susceptibility%2520to%2520these%2520attacks.%2520Our%2520findings%2520reveal%2520that%250Atypographic%2520attacks%2520persist%2520in%2520state-of-the-art%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520due%2520to%2520the%2520choice%2520of%2520their%2520vision%2520encoder%252C%2520though%2520larger%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520backbones%2520help%2520mitigate%2520their%2520vulnerability.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520synthetic%2520attacks%2520closely%2520resemble%2520real-world%2520%2528handwritten%2529%250Aattacks%252C%2520validating%2520their%2520use%2520in%2520research.%2520Our%2520work%2520provides%2520a%2520comprehensive%250Aresource%2520and%2520empirical%2520insights%2520to%2520facilitate%2520future%2520research%2520toward%2520robust%2520and%250Atrustworthy%2520multimodal%2520AI%2520systems.%2520We%2520publicly%2520release%2520the%2520datasets%2520introduced%250Ain%2520this%2520paper%2520along%2520with%2520the%2520code%2520for%2520evaluations%2520at%250Awww.bliss.berlin/research/scam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04893v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&entry.906535625=Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Lorenz%20Hufe&entry.1292438233=%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20However%2C%20existing%20datasets%20are%20limited%20in%20size%20and%0Adiversity%2C%20making%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%0Aintroduce%20SCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%0Aattack%20images%20to%20date%2C%20containing%201%2C162%20images%20across%20hundreds%20of%20object%0Acategories%20and%20attack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%0AModels%20%28VLMs%29%20on%20SCAM%2C%20we%20demonstrate%20that%20typographic%20attacks%20significantly%0Adegrade%20performance%2C%20and%20identify%20that%20training%20data%20and%20model%20architecture%0Ainfluence%20the%20susceptibility%20to%20these%20attacks.%20Our%20findings%20reveal%20that%0Atypographic%20attacks%20persist%20in%20state-of-the-art%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20due%20to%20the%20choice%20of%20their%20vision%20encoder%2C%20though%20larger%20Large%20Language%0AModels%20%28LLMs%29%20backbones%20help%20mitigate%20their%20vulnerability.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20We%20publicly%20release%20the%20datasets%20introduced%0Ain%20this%20paper%20along%20with%20the%20code%20for%20evaluations%20at%0Awww.bliss.berlin/research/scam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04893v5&entry.124074799=Read"},
{"title": "The OCR Quest for Generalization: Learning to recognize low-resource\n  alphabets with model editing", "author": "Adri\u00e0 Molina Rodr\u00edguez and Oriol Ramos Terrades and Josep Llad\u00f3s", "abstract": "  Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.\n", "link": "http://arxiv.org/abs/2506.06761v2", "date": "2025-06-18", "relevancy": 2.7045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20OCR%20Quest%20for%20Generalization%3A%20Learning%20to%20recognize%20low-resource%0A%20%20alphabets%20with%20model%20editing&body=Title%3A%20The%20OCR%20Quest%20for%20Generalization%3A%20Learning%20to%20recognize%20low-resource%0A%20%20alphabets%20with%20model%20editing%0AAuthor%3A%20Adri%C3%A0%20Molina%20Rodr%C3%ADguez%20and%20Oriol%20Ramos%20Terrades%20and%20Josep%20Llad%C3%B3s%0AAbstract%3A%20%20%20Achieving%20robustness%20in%20recognition%20systems%20across%20diverse%20domains%20is%20crucial%0Afor%20their%20practical%20utility.%20While%20ample%20data%20availability%20is%20usually%20assumed%2C%0Alow-resource%20languages%2C%20such%20as%20ancient%20manuscripts%20and%20non-western%20languages%2C%0Atend%20to%20be%20kept%20out%20of%20the%20equations%20of%20massive%20pretraining%20and%20foundational%0Atechniques%20due%20to%20an%20under%20representation.%20In%20this%20work%2C%20we%20aim%20for%20building%0Amodels%20which%20can%20generalize%20to%20new%20distributions%20of%20data%2C%20such%20as%20alphabets%2C%0Afaster%20than%20centralized%20fine-tune%20strategies.%20For%20doing%20so%2C%20we%20take%20advantage%0Aof%20the%20recent%20advancements%20in%20model%20editing%20to%20enhance%20the%20incorporation%20of%0Aunseen%20scripts%20%28low-resource%20learning%29.%20In%20contrast%20to%20state-of-the-art%0Ameta-learning%2C%20we%20showcase%20the%20effectiveness%20of%20domain%20merging%20in%20sparse%0Adistributions%20of%20data%2C%20with%20agnosticity%20of%20its%20relation%20to%20the%20overall%0Adistribution%20or%20any%20other%20prototyping%20necessity.%20Even%20when%20using%20the%20same%20exact%0Atraining%20data%2C%20our%20experiments%20showcase%20significant%20performance%20boosts%20in%0A%5Ctextbf%7Btransfer%20learning%7D%20to%20new%20alphabets%20and%20%5Ctextbf%7Bout-of-domain%0Aevaluation%7D%20in%20challenging%20domain%20shifts%2C%20including%20historical%20ciphered%20texts%0Aand%20non-Latin%20scripts.%20This%20research%20contributes%20a%20novel%20approach%20into%20building%0Amodels%20that%20can%20easily%20adopt%20under-represented%20alphabets%20and%2C%20therefore%2C%20enable%0Adocument%20recognition%20to%20a%20wider%20set%20of%20contexts%20and%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520OCR%2520Quest%2520for%2520Generalization%253A%2520Learning%2520to%2520recognize%2520low-resource%250A%2520%2520alphabets%2520with%2520model%2520editing%26entry.906535625%3DAdri%25C3%25A0%2520Molina%2520Rodr%25C3%25ADguez%2520and%2520Oriol%2520Ramos%2520Terrades%2520and%2520Josep%2520Llad%25C3%25B3s%26entry.1292438233%3D%2520%2520Achieving%2520robustness%2520in%2520recognition%2520systems%2520across%2520diverse%2520domains%2520is%2520crucial%250Afor%2520their%2520practical%2520utility.%2520While%2520ample%2520data%2520availability%2520is%2520usually%2520assumed%252C%250Alow-resource%2520languages%252C%2520such%2520as%2520ancient%2520manuscripts%2520and%2520non-western%2520languages%252C%250Atend%2520to%2520be%2520kept%2520out%2520of%2520the%2520equations%2520of%2520massive%2520pretraining%2520and%2520foundational%250Atechniques%2520due%2520to%2520an%2520under%2520representation.%2520In%2520this%2520work%252C%2520we%2520aim%2520for%2520building%250Amodels%2520which%2520can%2520generalize%2520to%2520new%2520distributions%2520of%2520data%252C%2520such%2520as%2520alphabets%252C%250Afaster%2520than%2520centralized%2520fine-tune%2520strategies.%2520For%2520doing%2520so%252C%2520we%2520take%2520advantage%250Aof%2520the%2520recent%2520advancements%2520in%2520model%2520editing%2520to%2520enhance%2520the%2520incorporation%2520of%250Aunseen%2520scripts%2520%2528low-resource%2520learning%2529.%2520In%2520contrast%2520to%2520state-of-the-art%250Ameta-learning%252C%2520we%2520showcase%2520the%2520effectiveness%2520of%2520domain%2520merging%2520in%2520sparse%250Adistributions%2520of%2520data%252C%2520with%2520agnosticity%2520of%2520its%2520relation%2520to%2520the%2520overall%250Adistribution%2520or%2520any%2520other%2520prototyping%2520necessity.%2520Even%2520when%2520using%2520the%2520same%2520exact%250Atraining%2520data%252C%2520our%2520experiments%2520showcase%2520significant%2520performance%2520boosts%2520in%250A%255Ctextbf%257Btransfer%2520learning%257D%2520to%2520new%2520alphabets%2520and%2520%255Ctextbf%257Bout-of-domain%250Aevaluation%257D%2520in%2520challenging%2520domain%2520shifts%252C%2520including%2520historical%2520ciphered%2520texts%250Aand%2520non-Latin%2520scripts.%2520This%2520research%2520contributes%2520a%2520novel%2520approach%2520into%2520building%250Amodels%2520that%2520can%2520easily%2520adopt%2520under-represented%2520alphabets%2520and%252C%2520therefore%252C%2520enable%250Adocument%2520recognition%2520to%2520a%2520wider%2520set%2520of%2520contexts%2520and%2520cultures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20OCR%20Quest%20for%20Generalization%3A%20Learning%20to%20recognize%20low-resource%0A%20%20alphabets%20with%20model%20editing&entry.906535625=Adri%C3%A0%20Molina%20Rodr%C3%ADguez%20and%20Oriol%20Ramos%20Terrades%20and%20Josep%20Llad%C3%B3s&entry.1292438233=%20%20Achieving%20robustness%20in%20recognition%20systems%20across%20diverse%20domains%20is%20crucial%0Afor%20their%20practical%20utility.%20While%20ample%20data%20availability%20is%20usually%20assumed%2C%0Alow-resource%20languages%2C%20such%20as%20ancient%20manuscripts%20and%20non-western%20languages%2C%0Atend%20to%20be%20kept%20out%20of%20the%20equations%20of%20massive%20pretraining%20and%20foundational%0Atechniques%20due%20to%20an%20under%20representation.%20In%20this%20work%2C%20we%20aim%20for%20building%0Amodels%20which%20can%20generalize%20to%20new%20distributions%20of%20data%2C%20such%20as%20alphabets%2C%0Afaster%20than%20centralized%20fine-tune%20strategies.%20For%20doing%20so%2C%20we%20take%20advantage%0Aof%20the%20recent%20advancements%20in%20model%20editing%20to%20enhance%20the%20incorporation%20of%0Aunseen%20scripts%20%28low-resource%20learning%29.%20In%20contrast%20to%20state-of-the-art%0Ameta-learning%2C%20we%20showcase%20the%20effectiveness%20of%20domain%20merging%20in%20sparse%0Adistributions%20of%20data%2C%20with%20agnosticity%20of%20its%20relation%20to%20the%20overall%0Adistribution%20or%20any%20other%20prototyping%20necessity.%20Even%20when%20using%20the%20same%20exact%0Atraining%20data%2C%20our%20experiments%20showcase%20significant%20performance%20boosts%20in%0A%5Ctextbf%7Btransfer%20learning%7D%20to%20new%20alphabets%20and%20%5Ctextbf%7Bout-of-domain%0Aevaluation%7D%20in%20challenging%20domain%20shifts%2C%20including%20historical%20ciphered%20texts%0Aand%20non-Latin%20scripts.%20This%20research%20contributes%20a%20novel%20approach%20into%20building%0Amodels%20that%20can%20easily%20adopt%20under-represented%20alphabets%20and%2C%20therefore%2C%20enable%0Adocument%20recognition%20to%20a%20wider%20set%20of%20contexts%20and%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06761v2&entry.124074799=Read"},
{"title": "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention", "author": "Alexandru Dimofte and Glenn Anta Bucagu and Thorir Mar Ingolfsson and Xiaying Wang and Andrea Cossettini and Luca Benini and Yawei Li", "abstract": "  Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.\n", "link": "http://arxiv.org/abs/2501.10885v3", "date": "2025-06-18", "relevancy": 2.6172, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CEReBrO%3A%20Compact%20Encoder%20for%20Representations%20of%20Brain%20Oscillations%20Using%0A%20%20Efficient%20Alternating%20Attention&body=Title%3A%20CEReBrO%3A%20Compact%20Encoder%20for%20Representations%20of%20Brain%20Oscillations%20Using%0A%20%20Efficient%20Alternating%20Attention%0AAuthor%3A%20Alexandru%20Dimofte%20and%20Glenn%20Anta%20Bucagu%20and%20Thorir%20Mar%20Ingolfsson%20and%20Xiaying%20Wang%20and%20Andrea%20Cossettini%20and%20Luca%20Benini%20and%20Yawei%20Li%0AAbstract%3A%20%20%20Electroencephalograph%20%28EEG%29%20is%20a%20crucial%20tool%20for%20studying%20brain%20activity.%0ARecently%2C%20self-supervised%20learning%20methods%20leveraging%20large%20unlabeled%20datasets%0Ahave%20emerged%20as%20a%20potential%20solution%20to%20the%20scarcity%20of%20widely%20available%0Aannotated%20EEG%20data.%20However%2C%20current%20methods%20suffer%20from%20at%20least%20one%20of%20the%0Afollowing%20limitations%3A%20i%29%20sub-optimal%20EEG%20signal%20modeling%2C%20ii%29%20model%20sizes%20in%0Athe%20hundreds%20of%20millions%20of%20trainable%20parameters%2C%20and%20iii%29%20reliance%20on%20private%0Adatasets%20and/or%20inconsistent%20public%20benchmarks%2C%20hindering%20reproducibility.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20Compact%20Encoder%20for%20Representations%20of%0ABrain%20Oscillations%20using%20alternating%20attention%20%28CEReBrO%29%2C%20a%20new%20small%20EEG%0Afoundation%20model.%20Our%20tokenization%20scheme%20represents%20EEG%20signals%20at%20a%0Aper-channel%20patch%20granularity.%20We%20propose%20an%20alternating%20attention%20mechanism%0Athat%20jointly%20models%20intra-channel%20temporal%20dynamics%20and%20inter-channel%20spatial%0Acorrelations%2C%20achieving%202x%20speed%20improvement%20with%206x%20less%20memory%20required%0Acompared%20to%20standard%20self-attention.%20We%20present%20several%20model%20sizes%20ranging%0Afrom%203.6%20million%20to%2085%20million%20parameters.%20Pre-trained%20on%20over%2020%2C000%20hours%20of%0Apublicly%20available%20scalp%20EEG%20recordings%20with%20diverse%20channel%20configurations%2C%0Aour%20models%20set%20new%20benchmarks%20in%20emotion%20detection%20and%20seizure%20detection%20tasks%2C%0Awith%20competitive%20performance%20in%20anomaly%20classification%20and%20gait%20prediction.%0AThis%20validates%20our%20models%27%20effectiveness%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10885v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCEReBrO%253A%2520Compact%2520Encoder%2520for%2520Representations%2520of%2520Brain%2520Oscillations%2520Using%250A%2520%2520Efficient%2520Alternating%2520Attention%26entry.906535625%3DAlexandru%2520Dimofte%2520and%2520Glenn%2520Anta%2520Bucagu%2520and%2520Thorir%2520Mar%2520Ingolfsson%2520and%2520Xiaying%2520Wang%2520and%2520Andrea%2520Cossettini%2520and%2520Luca%2520Benini%2520and%2520Yawei%2520Li%26entry.1292438233%3D%2520%2520Electroencephalograph%2520%2528EEG%2529%2520is%2520a%2520crucial%2520tool%2520for%2520studying%2520brain%2520activity.%250ARecently%252C%2520self-supervised%2520learning%2520methods%2520leveraging%2520large%2520unlabeled%2520datasets%250Ahave%2520emerged%2520as%2520a%2520potential%2520solution%2520to%2520the%2520scarcity%2520of%2520widely%2520available%250Aannotated%2520EEG%2520data.%2520However%252C%2520current%2520methods%2520suffer%2520from%2520at%2520least%2520one%2520of%2520the%250Afollowing%2520limitations%253A%2520i%2529%2520sub-optimal%2520EEG%2520signal%2520modeling%252C%2520ii%2529%2520model%2520sizes%2520in%250Athe%2520hundreds%2520of%2520millions%2520of%2520trainable%2520parameters%252C%2520and%2520iii%2529%2520reliance%2520on%2520private%250Adatasets%2520and/or%2520inconsistent%2520public%2520benchmarks%252C%2520hindering%2520reproducibility.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520Compact%2520Encoder%2520for%2520Representations%2520of%250ABrain%2520Oscillations%2520using%2520alternating%2520attention%2520%2528CEReBrO%2529%252C%2520a%2520new%2520small%2520EEG%250Afoundation%2520model.%2520Our%2520tokenization%2520scheme%2520represents%2520EEG%2520signals%2520at%2520a%250Aper-channel%2520patch%2520granularity.%2520We%2520propose%2520an%2520alternating%2520attention%2520mechanism%250Athat%2520jointly%2520models%2520intra-channel%2520temporal%2520dynamics%2520and%2520inter-channel%2520spatial%250Acorrelations%252C%2520achieving%25202x%2520speed%2520improvement%2520with%25206x%2520less%2520memory%2520required%250Acompared%2520to%2520standard%2520self-attention.%2520We%2520present%2520several%2520model%2520sizes%2520ranging%250Afrom%25203.6%2520million%2520to%252085%2520million%2520parameters.%2520Pre-trained%2520on%2520over%252020%252C000%2520hours%2520of%250Apublicly%2520available%2520scalp%2520EEG%2520recordings%2520with%2520diverse%2520channel%2520configurations%252C%250Aour%2520models%2520set%2520new%2520benchmarks%2520in%2520emotion%2520detection%2520and%2520seizure%2520detection%2520tasks%252C%250Awith%2520competitive%2520performance%2520in%2520anomaly%2520classification%2520and%2520gait%2520prediction.%250AThis%2520validates%2520our%2520models%2527%2520effectiveness%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10885v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CEReBrO%3A%20Compact%20Encoder%20for%20Representations%20of%20Brain%20Oscillations%20Using%0A%20%20Efficient%20Alternating%20Attention&entry.906535625=Alexandru%20Dimofte%20and%20Glenn%20Anta%20Bucagu%20and%20Thorir%20Mar%20Ingolfsson%20and%20Xiaying%20Wang%20and%20Andrea%20Cossettini%20and%20Luca%20Benini%20and%20Yawei%20Li&entry.1292438233=%20%20Electroencephalograph%20%28EEG%29%20is%20a%20crucial%20tool%20for%20studying%20brain%20activity.%0ARecently%2C%20self-supervised%20learning%20methods%20leveraging%20large%20unlabeled%20datasets%0Ahave%20emerged%20as%20a%20potential%20solution%20to%20the%20scarcity%20of%20widely%20available%0Aannotated%20EEG%20data.%20However%2C%20current%20methods%20suffer%20from%20at%20least%20one%20of%20the%0Afollowing%20limitations%3A%20i%29%20sub-optimal%20EEG%20signal%20modeling%2C%20ii%29%20model%20sizes%20in%0Athe%20hundreds%20of%20millions%20of%20trainable%20parameters%2C%20and%20iii%29%20reliance%20on%20private%0Adatasets%20and/or%20inconsistent%20public%20benchmarks%2C%20hindering%20reproducibility.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20Compact%20Encoder%20for%20Representations%20of%0ABrain%20Oscillations%20using%20alternating%20attention%20%28CEReBrO%29%2C%20a%20new%20small%20EEG%0Afoundation%20model.%20Our%20tokenization%20scheme%20represents%20EEG%20signals%20at%20a%0Aper-channel%20patch%20granularity.%20We%20propose%20an%20alternating%20attention%20mechanism%0Athat%20jointly%20models%20intra-channel%20temporal%20dynamics%20and%20inter-channel%20spatial%0Acorrelations%2C%20achieving%202x%20speed%20improvement%20with%206x%20less%20memory%20required%0Acompared%20to%20standard%20self-attention.%20We%20present%20several%20model%20sizes%20ranging%0Afrom%203.6%20million%20to%2085%20million%20parameters.%20Pre-trained%20on%20over%2020%2C000%20hours%20of%0Apublicly%20available%20scalp%20EEG%20recordings%20with%20diverse%20channel%20configurations%2C%0Aour%20models%20set%20new%20benchmarks%20in%20emotion%20detection%20and%20seizure%20detection%20tasks%2C%0Awith%20competitive%20performance%20in%20anomaly%20classification%20and%20gait%20prediction.%0AThis%20validates%20our%20models%27%20effectiveness%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10885v3&entry.124074799=Read"},
{"title": "Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high\n  Spatial Resolution Land Cover Classification", "author": "Junhao Wu and Aboagye-Ntow Stephen and Chuyuan Wang and Gang Chen and Xin Huang", "abstract": "  Ultra-high Spatial Resolution Land Cover Classification is essential for\nfine-grained land cover analysis, yet it remains challenging due to the high\ncost of pixel-level annotations, significant scale variation, and the limited\nadaptability of large-scale vision models. Existing methods typically focus on\n1-meter spatial resolution imagery and rely heavily on annotated data, whereas\npractical applications often require processing higher-resolution imagery under\nweak supervision. To address this, we propose a parameter-efficient\nsemi-supervised segmentation framework for 0.3 m spatial resolution imagery,\nwhich leverages the knowledge of SAM2 and introduces a remote sensing-specific\nFreqWeaver Adapter to enhance fine-grained detail modeling while maintaining a\nlightweight design at only 5.96% of the total model parameters. By effectively\nleveraging unlabeled data and maintaining minimal parameter overhead, the\nproposed method delivers robust segmentation results with superior structural\nconsistency, achieving a 1.78% improvement over existing parameter-efficient\ntuning strategies and a 3.44% gain compared to state-of-the-art high-resolution\nremote sensing segmentation approaches.\n", "link": "http://arxiv.org/abs/2506.15565v1", "date": "2025-06-18", "relevancy": 2.6116, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5468}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5153}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Baltimore%20Atlas%3A%20FreqWeaver%20Adapter%20for%20Semi-supervised%20Ultra-high%0A%20%20Spatial%20Resolution%20Land%20Cover%20Classification&body=Title%3A%20Baltimore%20Atlas%3A%20FreqWeaver%20Adapter%20for%20Semi-supervised%20Ultra-high%0A%20%20Spatial%20Resolution%20Land%20Cover%20Classification%0AAuthor%3A%20Junhao%20Wu%20and%20Aboagye-Ntow%20Stephen%20and%20Chuyuan%20Wang%20and%20Gang%20Chen%20and%20Xin%20Huang%0AAbstract%3A%20%20%20Ultra-high%20Spatial%20Resolution%20Land%20Cover%20Classification%20is%20essential%20for%0Afine-grained%20land%20cover%20analysis%2C%20yet%20it%20remains%20challenging%20due%20to%20the%20high%0Acost%20of%20pixel-level%20annotations%2C%20significant%20scale%20variation%2C%20and%20the%20limited%0Aadaptability%20of%20large-scale%20vision%20models.%20Existing%20methods%20typically%20focus%20on%0A1-meter%20spatial%20resolution%20imagery%20and%20rely%20heavily%20on%20annotated%20data%2C%20whereas%0Apractical%20applications%20often%20require%20processing%20higher-resolution%20imagery%20under%0Aweak%20supervision.%20To%20address%20this%2C%20we%20propose%20a%20parameter-efficient%0Asemi-supervised%20segmentation%20framework%20for%200.3%20m%20spatial%20resolution%20imagery%2C%0Awhich%20leverages%20the%20knowledge%20of%20SAM2%20and%20introduces%20a%20remote%20sensing-specific%0AFreqWeaver%20Adapter%20to%20enhance%20fine-grained%20detail%20modeling%20while%20maintaining%20a%0Alightweight%20design%20at%20only%205.96%25%20of%20the%20total%20model%20parameters.%20By%20effectively%0Aleveraging%20unlabeled%20data%20and%20maintaining%20minimal%20parameter%20overhead%2C%20the%0Aproposed%20method%20delivers%20robust%20segmentation%20results%20with%20superior%20structural%0Aconsistency%2C%20achieving%20a%201.78%25%20improvement%20over%20existing%20parameter-efficient%0Atuning%20strategies%20and%20a%203.44%25%20gain%20compared%20to%20state-of-the-art%20high-resolution%0Aremote%20sensing%20segmentation%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBaltimore%2520Atlas%253A%2520FreqWeaver%2520Adapter%2520for%2520Semi-supervised%2520Ultra-high%250A%2520%2520Spatial%2520Resolution%2520Land%2520Cover%2520Classification%26entry.906535625%3DJunhao%2520Wu%2520and%2520Aboagye-Ntow%2520Stephen%2520and%2520Chuyuan%2520Wang%2520and%2520Gang%2520Chen%2520and%2520Xin%2520Huang%26entry.1292438233%3D%2520%2520Ultra-high%2520Spatial%2520Resolution%2520Land%2520Cover%2520Classification%2520is%2520essential%2520for%250Afine-grained%2520land%2520cover%2520analysis%252C%2520yet%2520it%2520remains%2520challenging%2520due%2520to%2520the%2520high%250Acost%2520of%2520pixel-level%2520annotations%252C%2520significant%2520scale%2520variation%252C%2520and%2520the%2520limited%250Aadaptability%2520of%2520large-scale%2520vision%2520models.%2520Existing%2520methods%2520typically%2520focus%2520on%250A1-meter%2520spatial%2520resolution%2520imagery%2520and%2520rely%2520heavily%2520on%2520annotated%2520data%252C%2520whereas%250Apractical%2520applications%2520often%2520require%2520processing%2520higher-resolution%2520imagery%2520under%250Aweak%2520supervision.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520parameter-efficient%250Asemi-supervised%2520segmentation%2520framework%2520for%25200.3%2520m%2520spatial%2520resolution%2520imagery%252C%250Awhich%2520leverages%2520the%2520knowledge%2520of%2520SAM2%2520and%2520introduces%2520a%2520remote%2520sensing-specific%250AFreqWeaver%2520Adapter%2520to%2520enhance%2520fine-grained%2520detail%2520modeling%2520while%2520maintaining%2520a%250Alightweight%2520design%2520at%2520only%25205.96%2525%2520of%2520the%2520total%2520model%2520parameters.%2520By%2520effectively%250Aleveraging%2520unlabeled%2520data%2520and%2520maintaining%2520minimal%2520parameter%2520overhead%252C%2520the%250Aproposed%2520method%2520delivers%2520robust%2520segmentation%2520results%2520with%2520superior%2520structural%250Aconsistency%252C%2520achieving%2520a%25201.78%2525%2520improvement%2520over%2520existing%2520parameter-efficient%250Atuning%2520strategies%2520and%2520a%25203.44%2525%2520gain%2520compared%2520to%2520state-of-the-art%2520high-resolution%250Aremote%2520sensing%2520segmentation%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Baltimore%20Atlas%3A%20FreqWeaver%20Adapter%20for%20Semi-supervised%20Ultra-high%0A%20%20Spatial%20Resolution%20Land%20Cover%20Classification&entry.906535625=Junhao%20Wu%20and%20Aboagye-Ntow%20Stephen%20and%20Chuyuan%20Wang%20and%20Gang%20Chen%20and%20Xin%20Huang&entry.1292438233=%20%20Ultra-high%20Spatial%20Resolution%20Land%20Cover%20Classification%20is%20essential%20for%0Afine-grained%20land%20cover%20analysis%2C%20yet%20it%20remains%20challenging%20due%20to%20the%20high%0Acost%20of%20pixel-level%20annotations%2C%20significant%20scale%20variation%2C%20and%20the%20limited%0Aadaptability%20of%20large-scale%20vision%20models.%20Existing%20methods%20typically%20focus%20on%0A1-meter%20spatial%20resolution%20imagery%20and%20rely%20heavily%20on%20annotated%20data%2C%20whereas%0Apractical%20applications%20often%20require%20processing%20higher-resolution%20imagery%20under%0Aweak%20supervision.%20To%20address%20this%2C%20we%20propose%20a%20parameter-efficient%0Asemi-supervised%20segmentation%20framework%20for%200.3%20m%20spatial%20resolution%20imagery%2C%0Awhich%20leverages%20the%20knowledge%20of%20SAM2%20and%20introduces%20a%20remote%20sensing-specific%0AFreqWeaver%20Adapter%20to%20enhance%20fine-grained%20detail%20modeling%20while%20maintaining%20a%0Alightweight%20design%20at%20only%205.96%25%20of%20the%20total%20model%20parameters.%20By%20effectively%0Aleveraging%20unlabeled%20data%20and%20maintaining%20minimal%20parameter%20overhead%2C%20the%0Aproposed%20method%20delivers%20robust%20segmentation%20results%20with%20superior%20structural%0Aconsistency%2C%20achieving%20a%201.78%25%20improvement%20over%20existing%20parameter-efficient%0Atuning%20strategies%20and%20a%203.44%25%20gain%20compared%20to%20state-of-the-art%20high-resolution%0Aremote%20sensing%20segmentation%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15565v1&entry.124074799=Read"},
{"title": "Context-Informed Grounding Supervision", "author": "Hyunji Lee and Seunghyun Yoon and Yunjae Won and Hanseok Oh and Geewook Kim and Trung Bui and Franck Dernoncourt and Elias Stengel-Eskin and Mohit Bansal and Minjoon Seo", "abstract": "  Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.\n", "link": "http://arxiv.org/abs/2506.15480v1", "date": "2025-06-18", "relevancy": 2.5922, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Informed%20Grounding%20Supervision&body=Title%3A%20Context-Informed%20Grounding%20Supervision%0AAuthor%3A%20Hyunji%20Lee%20and%20Seunghyun%20Yoon%20and%20Yunjae%20Won%20and%20Hanseok%20Oh%20and%20Geewook%20Kim%20and%20Trung%20Bui%20and%20Franck%20Dernoncourt%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%20and%20Minjoon%20Seo%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20often%20supplemented%20with%20external%20knowledge%0Ato%20provide%20information%20not%20encoded%20in%20their%20parameters%20or%20to%20reduce%0Ahallucination.%20In%20such%20cases%2C%20we%20expect%20the%20model%20to%20generate%20responses%20by%0Agrounding%20its%20response%20in%20the%20provided%20external%20context.%20However%2C%20prior%20work%0Ahas%20shown%20that%20simply%20appending%20context%20at%20inference%20time%20does%20not%20ensure%0Agrounded%20generation.%20To%20address%20this%2C%20we%20propose%20Context-INformed%20Grounding%0ASupervision%20%28CINGS%29%2C%20a%20post-training%20supervision%20in%20which%20the%20model%20is%20trained%0Awith%20relevant%20context%20prepended%20to%20the%20response%2C%20while%20computing%20the%20loss%20only%0Aover%20the%20response%20tokens%20and%20masking%20out%20the%20context.%20Our%20experiments%0Ademonstrate%20that%20models%20trained%20with%20CINGS%20exhibit%20stronger%20grounding%20in%20both%0Atextual%20and%20visual%20domains%20compared%20to%20standard%20instruction-tuned%20models.%20In%0Athe%20text%20domain%2C%20CINGS%20outperforms%20other%20training%20methods%20across%2011%0Ainformation-seeking%20datasets%20and%20is%20complementary%20to%20inference-time%20grounding%0Atechniques.%20In%20the%20vision-language%20domain%2C%20replacing%20a%20vision-language%20model%27s%0ALLM%20backbone%20with%20a%20CINGS-trained%20model%20reduces%20hallucinations%20across%20four%0Abenchmarks%20and%20maintains%20factual%20consistency%20throughout%20the%20generated%20response.%0AThis%20improved%20grounding%20comes%20without%20degradation%20in%20general%20downstream%0Aperformance.%20Finally%2C%20we%20analyze%20the%20mechanism%20underlying%20the%20enhanced%0Agrounding%20in%20CINGS%20and%20find%20that%20it%20induces%20a%20shift%20in%20the%20model%27s%20prior%0Aknowledge%20and%20behavior%2C%20implicitly%20encouraging%20greater%20reliance%20on%20the%20external%0Acontext.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Informed%2520Grounding%2520Supervision%26entry.906535625%3DHyunji%2520Lee%2520and%2520Seunghyun%2520Yoon%2520and%2520Yunjae%2520Won%2520and%2520Hanseok%2520Oh%2520and%2520Geewook%2520Kim%2520and%2520Trung%2520Bui%2520and%2520Franck%2520Dernoncourt%2520and%2520Elias%2520Stengel-Eskin%2520and%2520Mohit%2520Bansal%2520and%2520Minjoon%2520Seo%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520often%2520supplemented%2520with%2520external%2520knowledge%250Ato%2520provide%2520information%2520not%2520encoded%2520in%2520their%2520parameters%2520or%2520to%2520reduce%250Ahallucination.%2520In%2520such%2520cases%252C%2520we%2520expect%2520the%2520model%2520to%2520generate%2520responses%2520by%250Agrounding%2520its%2520response%2520in%2520the%2520provided%2520external%2520context.%2520However%252C%2520prior%2520work%250Ahas%2520shown%2520that%2520simply%2520appending%2520context%2520at%2520inference%2520time%2520does%2520not%2520ensure%250Agrounded%2520generation.%2520To%2520address%2520this%252C%2520we%2520propose%2520Context-INformed%2520Grounding%250ASupervision%2520%2528CINGS%2529%252C%2520a%2520post-training%2520supervision%2520in%2520which%2520the%2520model%2520is%2520trained%250Awith%2520relevant%2520context%2520prepended%2520to%2520the%2520response%252C%2520while%2520computing%2520the%2520loss%2520only%250Aover%2520the%2520response%2520tokens%2520and%2520masking%2520out%2520the%2520context.%2520Our%2520experiments%250Ademonstrate%2520that%2520models%2520trained%2520with%2520CINGS%2520exhibit%2520stronger%2520grounding%2520in%2520both%250Atextual%2520and%2520visual%2520domains%2520compared%2520to%2520standard%2520instruction-tuned%2520models.%2520In%250Athe%2520text%2520domain%252C%2520CINGS%2520outperforms%2520other%2520training%2520methods%2520across%252011%250Ainformation-seeking%2520datasets%2520and%2520is%2520complementary%2520to%2520inference-time%2520grounding%250Atechniques.%2520In%2520the%2520vision-language%2520domain%252C%2520replacing%2520a%2520vision-language%2520model%2527s%250ALLM%2520backbone%2520with%2520a%2520CINGS-trained%2520model%2520reduces%2520hallucinations%2520across%2520four%250Abenchmarks%2520and%2520maintains%2520factual%2520consistency%2520throughout%2520the%2520generated%2520response.%250AThis%2520improved%2520grounding%2520comes%2520without%2520degradation%2520in%2520general%2520downstream%250Aperformance.%2520Finally%252C%2520we%2520analyze%2520the%2520mechanism%2520underlying%2520the%2520enhanced%250Agrounding%2520in%2520CINGS%2520and%2520find%2520that%2520it%2520induces%2520a%2520shift%2520in%2520the%2520model%2527s%2520prior%250Aknowledge%2520and%2520behavior%252C%2520implicitly%2520encouraging%2520greater%2520reliance%2520on%2520the%2520external%250Acontext.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Informed%20Grounding%20Supervision&entry.906535625=Hyunji%20Lee%20and%20Seunghyun%20Yoon%20and%20Yunjae%20Won%20and%20Hanseok%20Oh%20and%20Geewook%20Kim%20and%20Trung%20Bui%20and%20Franck%20Dernoncourt%20and%20Elias%20Stengel-Eskin%20and%20Mohit%20Bansal%20and%20Minjoon%20Seo&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20often%20supplemented%20with%20external%20knowledge%0Ato%20provide%20information%20not%20encoded%20in%20their%20parameters%20or%20to%20reduce%0Ahallucination.%20In%20such%20cases%2C%20we%20expect%20the%20model%20to%20generate%20responses%20by%0Agrounding%20its%20response%20in%20the%20provided%20external%20context.%20However%2C%20prior%20work%0Ahas%20shown%20that%20simply%20appending%20context%20at%20inference%20time%20does%20not%20ensure%0Agrounded%20generation.%20To%20address%20this%2C%20we%20propose%20Context-INformed%20Grounding%0ASupervision%20%28CINGS%29%2C%20a%20post-training%20supervision%20in%20which%20the%20model%20is%20trained%0Awith%20relevant%20context%20prepended%20to%20the%20response%2C%20while%20computing%20the%20loss%20only%0Aover%20the%20response%20tokens%20and%20masking%20out%20the%20context.%20Our%20experiments%0Ademonstrate%20that%20models%20trained%20with%20CINGS%20exhibit%20stronger%20grounding%20in%20both%0Atextual%20and%20visual%20domains%20compared%20to%20standard%20instruction-tuned%20models.%20In%0Athe%20text%20domain%2C%20CINGS%20outperforms%20other%20training%20methods%20across%2011%0Ainformation-seeking%20datasets%20and%20is%20complementary%20to%20inference-time%20grounding%0Atechniques.%20In%20the%20vision-language%20domain%2C%20replacing%20a%20vision-language%20model%27s%0ALLM%20backbone%20with%20a%20CINGS-trained%20model%20reduces%20hallucinations%20across%20four%0Abenchmarks%20and%20maintains%20factual%20consistency%20throughout%20the%20generated%20response.%0AThis%20improved%20grounding%20comes%20without%20degradation%20in%20general%20downstream%0Aperformance.%20Finally%2C%20we%20analyze%20the%20mechanism%20underlying%20the%20enhanced%0Agrounding%20in%20CINGS%20and%20find%20that%20it%20induces%20a%20shift%20in%20the%20model%27s%20prior%0Aknowledge%20and%20behavior%2C%20implicitly%20encouraging%20greater%20reliance%20on%20the%20external%0Acontext.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15480v1&entry.124074799=Read"},
{"title": "AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation", "author": "Leah von der Heyde and Anna-Carolina Haensch and Bernd Wei\u00df and Jessica Daikeler", "abstract": "  The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.\n", "link": "http://arxiv.org/abs/2506.14634v2", "date": "2025-06-18", "relevancy": 2.5897, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIn%27t%20Nothing%20But%20a%20Survey%3F%20Using%20Large%20Language%20Models%20for%20Coding%0A%20%20German%20Open-Ended%20Survey%20Responses%20on%20Survey%20Motivation&body=Title%3A%20AIn%27t%20Nothing%20But%20a%20Survey%3F%20Using%20Large%20Language%20Models%20for%20Coding%0A%20%20German%20Open-Ended%20Survey%20Responses%20on%20Survey%20Motivation%0AAuthor%3A%20Leah%20von%20der%20Heyde%20and%20Anna-Carolina%20Haensch%20and%20Bernd%20Wei%C3%9F%20and%20Jessica%20Daikeler%0AAbstract%3A%20%20%20The%20recent%20development%20and%20wider%20accessibility%20of%20LLMs%20have%20spurred%0Adiscussions%20about%20how%20they%20can%20be%20used%20in%20survey%20research%2C%20including%0Aclassifying%20open-ended%20survey%20responses.%20Due%20to%20their%20linguistic%20capacities%2C%20it%0Ais%20possible%20that%20LLMs%20are%20an%20efficient%20alternative%20to%20time-consuming%20manual%0Acoding%20and%20the%20pre-training%20of%20supervised%20machine%20learning%20models.%20As%20most%0Aexisting%20research%20on%20this%20topic%20has%20focused%20on%20English-language%20responses%0Arelating%20to%20non-complex%20topics%20or%20on%20single%20LLMs%2C%20it%20is%20unclear%20whether%20its%0Afindings%20generalize%20and%20how%20the%20quality%20of%20these%20classifications%20compares%20to%0Aestablished%20methods.%20In%20this%20study%2C%20we%20investigate%20to%20what%20extent%20different%0ALLMs%20can%20be%20used%20to%20code%20open-ended%20survey%20responses%20in%20other%20contexts%2C%20using%0AGerman%20data%20on%20reasons%20for%20survey%20participation%20as%20an%20example.%20We%20compare%0Aseveral%20state-of-the-art%20LLMs%20and%20several%20prompting%20approaches%2C%20and%20evaluate%0Athe%20LLMs%27%20performance%20by%20using%20human%20expert%20codings.%20Overall%20performance%0Adiffers%20greatly%20between%20LLMs%2C%20and%20only%20a%20fine-tuned%20LLM%20achieves%20satisfactory%0Alevels%20of%20predictive%20performance.%20Performance%20differences%20between%20prompting%0Aapproaches%20are%20conditional%20on%20the%20LLM%20used.%20Finally%2C%20LLMs%27%20unequal%0Aclassification%20performance%20across%20different%20categories%20of%20reasons%20for%20survey%0Aparticipation%20results%20in%20different%20categorical%20distributions%20when%20not%20using%0Afine-tuning.%20We%20discuss%20the%20implications%20of%20these%20findings%2C%20both%20for%0Amethodological%20research%20on%20coding%20open-ended%20responses%20and%20for%20their%0Asubstantive%20analysis%2C%20and%20for%20practitioners%20processing%20or%20substantively%0Aanalyzing%20such%20data.%20Finally%2C%20we%20highlight%20the%20many%20trade-offs%20researchers%20need%0Ato%20consider%20when%20choosing%20automated%20methods%20for%20open-ended%20response%0Aclassification%20in%20the%20age%20of%20LLMs.%20In%20doing%20so%2C%20our%20study%20contributes%20to%20the%0Agrowing%20body%20of%20research%20about%20the%20conditions%20under%20which%20LLMs%20can%20be%0Aefficiently%2C%20accurately%2C%20and%20reliably%20leveraged%20in%20survey%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIn%2527t%2520Nothing%2520But%2520a%2520Survey%253F%2520Using%2520Large%2520Language%2520Models%2520for%2520Coding%250A%2520%2520German%2520Open-Ended%2520Survey%2520Responses%2520on%2520Survey%2520Motivation%26entry.906535625%3DLeah%2520von%2520der%2520Heyde%2520and%2520Anna-Carolina%2520Haensch%2520and%2520Bernd%2520Wei%25C3%259F%2520and%2520Jessica%2520Daikeler%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520and%2520wider%2520accessibility%2520of%2520LLMs%2520have%2520spurred%250Adiscussions%2520about%2520how%2520they%2520can%2520be%2520used%2520in%2520survey%2520research%252C%2520including%250Aclassifying%2520open-ended%2520survey%2520responses.%2520Due%2520to%2520their%2520linguistic%2520capacities%252C%2520it%250Ais%2520possible%2520that%2520LLMs%2520are%2520an%2520efficient%2520alternative%2520to%2520time-consuming%2520manual%250Acoding%2520and%2520the%2520pre-training%2520of%2520supervised%2520machine%2520learning%2520models.%2520As%2520most%250Aexisting%2520research%2520on%2520this%2520topic%2520has%2520focused%2520on%2520English-language%2520responses%250Arelating%2520to%2520non-complex%2520topics%2520or%2520on%2520single%2520LLMs%252C%2520it%2520is%2520unclear%2520whether%2520its%250Afindings%2520generalize%2520and%2520how%2520the%2520quality%2520of%2520these%2520classifications%2520compares%2520to%250Aestablished%2520methods.%2520In%2520this%2520study%252C%2520we%2520investigate%2520to%2520what%2520extent%2520different%250ALLMs%2520can%2520be%2520used%2520to%2520code%2520open-ended%2520survey%2520responses%2520in%2520other%2520contexts%252C%2520using%250AGerman%2520data%2520on%2520reasons%2520for%2520survey%2520participation%2520as%2520an%2520example.%2520We%2520compare%250Aseveral%2520state-of-the-art%2520LLMs%2520and%2520several%2520prompting%2520approaches%252C%2520and%2520evaluate%250Athe%2520LLMs%2527%2520performance%2520by%2520using%2520human%2520expert%2520codings.%2520Overall%2520performance%250Adiffers%2520greatly%2520between%2520LLMs%252C%2520and%2520only%2520a%2520fine-tuned%2520LLM%2520achieves%2520satisfactory%250Alevels%2520of%2520predictive%2520performance.%2520Performance%2520differences%2520between%2520prompting%250Aapproaches%2520are%2520conditional%2520on%2520the%2520LLM%2520used.%2520Finally%252C%2520LLMs%2527%2520unequal%250Aclassification%2520performance%2520across%2520different%2520categories%2520of%2520reasons%2520for%2520survey%250Aparticipation%2520results%2520in%2520different%2520categorical%2520distributions%2520when%2520not%2520using%250Afine-tuning.%2520We%2520discuss%2520the%2520implications%2520of%2520these%2520findings%252C%2520both%2520for%250Amethodological%2520research%2520on%2520coding%2520open-ended%2520responses%2520and%2520for%2520their%250Asubstantive%2520analysis%252C%2520and%2520for%2520practitioners%2520processing%2520or%2520substantively%250Aanalyzing%2520such%2520data.%2520Finally%252C%2520we%2520highlight%2520the%2520many%2520trade-offs%2520researchers%2520need%250Ato%2520consider%2520when%2520choosing%2520automated%2520methods%2520for%2520open-ended%2520response%250Aclassification%2520in%2520the%2520age%2520of%2520LLMs.%2520In%2520doing%2520so%252C%2520our%2520study%2520contributes%2520to%2520the%250Agrowing%2520body%2520of%2520research%2520about%2520the%2520conditions%2520under%2520which%2520LLMs%2520can%2520be%250Aefficiently%252C%2520accurately%252C%2520and%2520reliably%2520leveraged%2520in%2520survey%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIn%27t%20Nothing%20But%20a%20Survey%3F%20Using%20Large%20Language%20Models%20for%20Coding%0A%20%20German%20Open-Ended%20Survey%20Responses%20on%20Survey%20Motivation&entry.906535625=Leah%20von%20der%20Heyde%20and%20Anna-Carolina%20Haensch%20and%20Bernd%20Wei%C3%9F%20and%20Jessica%20Daikeler&entry.1292438233=%20%20The%20recent%20development%20and%20wider%20accessibility%20of%20LLMs%20have%20spurred%0Adiscussions%20about%20how%20they%20can%20be%20used%20in%20survey%20research%2C%20including%0Aclassifying%20open-ended%20survey%20responses.%20Due%20to%20their%20linguistic%20capacities%2C%20it%0Ais%20possible%20that%20LLMs%20are%20an%20efficient%20alternative%20to%20time-consuming%20manual%0Acoding%20and%20the%20pre-training%20of%20supervised%20machine%20learning%20models.%20As%20most%0Aexisting%20research%20on%20this%20topic%20has%20focused%20on%20English-language%20responses%0Arelating%20to%20non-complex%20topics%20or%20on%20single%20LLMs%2C%20it%20is%20unclear%20whether%20its%0Afindings%20generalize%20and%20how%20the%20quality%20of%20these%20classifications%20compares%20to%0Aestablished%20methods.%20In%20this%20study%2C%20we%20investigate%20to%20what%20extent%20different%0ALLMs%20can%20be%20used%20to%20code%20open-ended%20survey%20responses%20in%20other%20contexts%2C%20using%0AGerman%20data%20on%20reasons%20for%20survey%20participation%20as%20an%20example.%20We%20compare%0Aseveral%20state-of-the-art%20LLMs%20and%20several%20prompting%20approaches%2C%20and%20evaluate%0Athe%20LLMs%27%20performance%20by%20using%20human%20expert%20codings.%20Overall%20performance%0Adiffers%20greatly%20between%20LLMs%2C%20and%20only%20a%20fine-tuned%20LLM%20achieves%20satisfactory%0Alevels%20of%20predictive%20performance.%20Performance%20differences%20between%20prompting%0Aapproaches%20are%20conditional%20on%20the%20LLM%20used.%20Finally%2C%20LLMs%27%20unequal%0Aclassification%20performance%20across%20different%20categories%20of%20reasons%20for%20survey%0Aparticipation%20results%20in%20different%20categorical%20distributions%20when%20not%20using%0Afine-tuning.%20We%20discuss%20the%20implications%20of%20these%20findings%2C%20both%20for%0Amethodological%20research%20on%20coding%20open-ended%20responses%20and%20for%20their%0Asubstantive%20analysis%2C%20and%20for%20practitioners%20processing%20or%20substantively%0Aanalyzing%20such%20data.%20Finally%2C%20we%20highlight%20the%20many%20trade-offs%20researchers%20need%0Ato%20consider%20when%20choosing%20automated%20methods%20for%20open-ended%20response%0Aclassification%20in%20the%20age%20of%20LLMs.%20In%20doing%20so%2C%20our%20study%20contributes%20to%20the%0Agrowing%20body%20of%20research%20about%20the%20conditions%20under%20which%20LLMs%20can%20be%0Aefficiently%2C%20accurately%2C%20and%20reliably%20leveraged%20in%20survey%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14634v2&entry.124074799=Read"},
{"title": "RefChartQA: Grounding Visual Answer on Chart Images through Instruction\n  Tuning", "author": "Alexander Vogel and Omar Moured and Yufan Chen and Jiaming Zhang and Rainer Stiefelhagen", "abstract": "  Recently, Vision Language Models (VLMs) have increasingly emphasized document\nvisual grounding to achieve better human-computer interaction, accessibility,\nand detailed understanding. However, its application to visualizations such as\ncharts remains under-explored due to the inherent complexity of interleaved\nvisual-numerical relationships in chart images. Existing chart understanding\nmethods primarily focus on answering questions without explicitly identifying\nthe visual elements that support their predictions. To bridge this gap, we\nintroduce RefChartQA, a novel benchmark that integrates Chart Question\nAnswering (ChartQA) with visual grounding, enabling models to refer elements at\nmultiple granularities within chart images. Furthermore, we conduct a\ncomprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across\ndifferent categories. Our experiments demonstrate that incorporating spatial\nawareness via grounding improves response accuracy by over 15%, reducing\nhallucinations, and improving model reliability. Additionally, we identify key\nfactors influencing text-spatial alignment, such as architectural improvements\nin TinyChart, which leverages a token-merging module for enhanced feature\nfusion. Our dataset is open-sourced for community development and further\nadvancements. All models and code will be publicly available at\nhttps://github.com/moured/RefChartQA.\n", "link": "http://arxiv.org/abs/2503.23131v2", "date": "2025-06-18", "relevancy": 2.5574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5243}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefChartQA%3A%20Grounding%20Visual%20Answer%20on%20Chart%20Images%20through%20Instruction%0A%20%20Tuning&body=Title%3A%20RefChartQA%3A%20Grounding%20Visual%20Answer%20on%20Chart%20Images%20through%20Instruction%0A%20%20Tuning%0AAuthor%3A%20Alexander%20Vogel%20and%20Omar%20Moured%20and%20Yufan%20Chen%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20Recently%2C%20Vision%20Language%20Models%20%28VLMs%29%20have%20increasingly%20emphasized%20document%0Avisual%20grounding%20to%20achieve%20better%20human-computer%20interaction%2C%20accessibility%2C%0Aand%20detailed%20understanding.%20However%2C%20its%20application%20to%20visualizations%20such%20as%0Acharts%20remains%20under-explored%20due%20to%20the%20inherent%20complexity%20of%20interleaved%0Avisual-numerical%20relationships%20in%20chart%20images.%20Existing%20chart%20understanding%0Amethods%20primarily%20focus%20on%20answering%20questions%20without%20explicitly%20identifying%0Athe%20visual%20elements%20that%20support%20their%20predictions.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20RefChartQA%2C%20a%20novel%20benchmark%20that%20integrates%20Chart%20Question%0AAnswering%20%28ChartQA%29%20with%20visual%20grounding%2C%20enabling%20models%20to%20refer%20elements%20at%0Amultiple%20granularities%20within%20chart%20images.%20Furthermore%2C%20we%20conduct%20a%0Acomprehensive%20evaluation%20by%20instruction-tuning%205%20state-of-the-art%20VLMs%20across%0Adifferent%20categories.%20Our%20experiments%20demonstrate%20that%20incorporating%20spatial%0Aawareness%20via%20grounding%20improves%20response%20accuracy%20by%20over%2015%25%2C%20reducing%0Ahallucinations%2C%20and%20improving%20model%20reliability.%20Additionally%2C%20we%20identify%20key%0Afactors%20influencing%20text-spatial%20alignment%2C%20such%20as%20architectural%20improvements%0Ain%20TinyChart%2C%20which%20leverages%20a%20token-merging%20module%20for%20enhanced%20feature%0Afusion.%20Our%20dataset%20is%20open-sourced%20for%20community%20development%20and%20further%0Aadvancements.%20All%20models%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/moured/RefChartQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefChartQA%253A%2520Grounding%2520Visual%2520Answer%2520on%2520Chart%2520Images%2520through%2520Instruction%250A%2520%2520Tuning%26entry.906535625%3DAlexander%2520Vogel%2520and%2520Omar%2520Moured%2520and%2520Yufan%2520Chen%2520and%2520Jiaming%2520Zhang%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520Recently%252C%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520increasingly%2520emphasized%2520document%250Avisual%2520grounding%2520to%2520achieve%2520better%2520human-computer%2520interaction%252C%2520accessibility%252C%250Aand%2520detailed%2520understanding.%2520However%252C%2520its%2520application%2520to%2520visualizations%2520such%2520as%250Acharts%2520remains%2520under-explored%2520due%2520to%2520the%2520inherent%2520complexity%2520of%2520interleaved%250Avisual-numerical%2520relationships%2520in%2520chart%2520images.%2520Existing%2520chart%2520understanding%250Amethods%2520primarily%2520focus%2520on%2520answering%2520questions%2520without%2520explicitly%2520identifying%250Athe%2520visual%2520elements%2520that%2520support%2520their%2520predictions.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520RefChartQA%252C%2520a%2520novel%2520benchmark%2520that%2520integrates%2520Chart%2520Question%250AAnswering%2520%2528ChartQA%2529%2520with%2520visual%2520grounding%252C%2520enabling%2520models%2520to%2520refer%2520elements%2520at%250Amultiple%2520granularities%2520within%2520chart%2520images.%2520Furthermore%252C%2520we%2520conduct%2520a%250Acomprehensive%2520evaluation%2520by%2520instruction-tuning%25205%2520state-of-the-art%2520VLMs%2520across%250Adifferent%2520categories.%2520Our%2520experiments%2520demonstrate%2520that%2520incorporating%2520spatial%250Aawareness%2520via%2520grounding%2520improves%2520response%2520accuracy%2520by%2520over%252015%2525%252C%2520reducing%250Ahallucinations%252C%2520and%2520improving%2520model%2520reliability.%2520Additionally%252C%2520we%2520identify%2520key%250Afactors%2520influencing%2520text-spatial%2520alignment%252C%2520such%2520as%2520architectural%2520improvements%250Ain%2520TinyChart%252C%2520which%2520leverages%2520a%2520token-merging%2520module%2520for%2520enhanced%2520feature%250Afusion.%2520Our%2520dataset%2520is%2520open-sourced%2520for%2520community%2520development%2520and%2520further%250Aadvancements.%2520All%2520models%2520and%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/moured/RefChartQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefChartQA%3A%20Grounding%20Visual%20Answer%20on%20Chart%20Images%20through%20Instruction%0A%20%20Tuning&entry.906535625=Alexander%20Vogel%20and%20Omar%20Moured%20and%20Yufan%20Chen%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20Recently%2C%20Vision%20Language%20Models%20%28VLMs%29%20have%20increasingly%20emphasized%20document%0Avisual%20grounding%20to%20achieve%20better%20human-computer%20interaction%2C%20accessibility%2C%0Aand%20detailed%20understanding.%20However%2C%20its%20application%20to%20visualizations%20such%20as%0Acharts%20remains%20under-explored%20due%20to%20the%20inherent%20complexity%20of%20interleaved%0Avisual-numerical%20relationships%20in%20chart%20images.%20Existing%20chart%20understanding%0Amethods%20primarily%20focus%20on%20answering%20questions%20without%20explicitly%20identifying%0Athe%20visual%20elements%20that%20support%20their%20predictions.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20RefChartQA%2C%20a%20novel%20benchmark%20that%20integrates%20Chart%20Question%0AAnswering%20%28ChartQA%29%20with%20visual%20grounding%2C%20enabling%20models%20to%20refer%20elements%20at%0Amultiple%20granularities%20within%20chart%20images.%20Furthermore%2C%20we%20conduct%20a%0Acomprehensive%20evaluation%20by%20instruction-tuning%205%20state-of-the-art%20VLMs%20across%0Adifferent%20categories.%20Our%20experiments%20demonstrate%20that%20incorporating%20spatial%0Aawareness%20via%20grounding%20improves%20response%20accuracy%20by%20over%2015%25%2C%20reducing%0Ahallucinations%2C%20and%20improving%20model%20reliability.%20Additionally%2C%20we%20identify%20key%0Afactors%20influencing%20text-spatial%20alignment%2C%20such%20as%20architectural%20improvements%0Ain%20TinyChart%2C%20which%20leverages%20a%20token-merging%20module%20for%20enhanced%20feature%0Afusion.%20Our%20dataset%20is%20open-sourced%20for%20community%20development%20and%20further%0Aadvancements.%20All%20models%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/moured/RefChartQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23131v2&entry.124074799=Read"},
{"title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation", "author": "Xinnuo Xu and Rachel Lawrence and Kshitij Dubey and Atharva Pandey and Risa Ueno and Fabian Falck and Aditya V. Nori and Rahul Sharma and Amit Sharma and Javier Gonzalez", "abstract": "  Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.\n", "link": "http://arxiv.org/abs/2506.15455v1", "date": "2025-06-18", "relevancy": 2.543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5279}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RE-IMAGINE%3A%20Symbolic%20Benchmark%20Synthesis%20for%20Reasoning%20Evaluation&body=Title%3A%20RE-IMAGINE%3A%20Symbolic%20Benchmark%20Synthesis%20for%20Reasoning%20Evaluation%0AAuthor%3A%20Xinnuo%20Xu%20and%20Rachel%20Lawrence%20and%20Kshitij%20Dubey%20and%20Atharva%20Pandey%20and%20Risa%20Ueno%20and%20Fabian%20Falck%20and%20Aditya%20V.%20Nori%20and%20Rahul%20Sharma%20and%20Amit%20Sharma%20and%20Javier%20Gonzalez%0AAbstract%3A%20%20%20Recent%20Large%20Language%20Models%20%28LLMs%29%20have%20reported%20high%20accuracy%20on%20reasoning%0Abenchmarks.%20However%2C%20it%20is%20still%20unclear%20whether%20the%20observed%20results%20arise%0Afrom%20true%20reasoning%20or%20from%20statistical%20recall%20of%20the%20training%20set.%20Inspired%20by%0Athe%20ladder%20of%20causation%20%28Pearl%2C%202009%29%20and%20its%20three%20levels%20%28associations%2C%0Ainterventions%20and%20counterfactuals%29%2C%20this%20paper%20introduces%20RE-IMAGINE%2C%20a%0Aframework%20to%20characterize%20a%20hierarchy%20of%20reasoning%20ability%20in%20LLMs%2C%20alongside%0Aan%20automated%20pipeline%20to%20generate%20problem%20variations%20at%20different%20levels%20of%20the%0Ahierarchy.%20By%20altering%20problems%20in%20an%20intermediate%20symbolic%20representation%2C%0ARE-IMAGINE%20generates%20arbitrarily%20many%20problems%20that%20are%20not%20solvable%20using%0Amemorization%20alone.%20Moreover%2C%20the%20framework%20is%20general%20and%20can%20work%20across%0Areasoning%20domains%2C%20including%20math%2C%20code%2C%20and%20logic.%20We%20demonstrate%20our%0Aframework%20on%20four%20widely-used%20benchmarks%20to%20evaluate%20several%20families%20of%20LLMs%2C%0Aand%20observe%20reductions%20in%20performance%20when%20the%20models%20are%20queried%20with%20problem%0Avariations.%20These%20assessments%20indicate%20a%20degree%20of%20reliance%20on%20statistical%0Arecall%20for%20past%20performance%2C%20and%20open%20the%20door%20to%20further%20research%20targeting%0Askills%20across%20the%20reasoning%20hierarchy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRE-IMAGINE%253A%2520Symbolic%2520Benchmark%2520Synthesis%2520for%2520Reasoning%2520Evaluation%26entry.906535625%3DXinnuo%2520Xu%2520and%2520Rachel%2520Lawrence%2520and%2520Kshitij%2520Dubey%2520and%2520Atharva%2520Pandey%2520and%2520Risa%2520Ueno%2520and%2520Fabian%2520Falck%2520and%2520Aditya%2520V.%2520Nori%2520and%2520Rahul%2520Sharma%2520and%2520Amit%2520Sharma%2520and%2520Javier%2520Gonzalez%26entry.1292438233%3D%2520%2520Recent%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520reported%2520high%2520accuracy%2520on%2520reasoning%250Abenchmarks.%2520However%252C%2520it%2520is%2520still%2520unclear%2520whether%2520the%2520observed%2520results%2520arise%250Afrom%2520true%2520reasoning%2520or%2520from%2520statistical%2520recall%2520of%2520the%2520training%2520set.%2520Inspired%2520by%250Athe%2520ladder%2520of%2520causation%2520%2528Pearl%252C%25202009%2529%2520and%2520its%2520three%2520levels%2520%2528associations%252C%250Ainterventions%2520and%2520counterfactuals%2529%252C%2520this%2520paper%2520introduces%2520RE-IMAGINE%252C%2520a%250Aframework%2520to%2520characterize%2520a%2520hierarchy%2520of%2520reasoning%2520ability%2520in%2520LLMs%252C%2520alongside%250Aan%2520automated%2520pipeline%2520to%2520generate%2520problem%2520variations%2520at%2520different%2520levels%2520of%2520the%250Ahierarchy.%2520By%2520altering%2520problems%2520in%2520an%2520intermediate%2520symbolic%2520representation%252C%250ARE-IMAGINE%2520generates%2520arbitrarily%2520many%2520problems%2520that%2520are%2520not%2520solvable%2520using%250Amemorization%2520alone.%2520Moreover%252C%2520the%2520framework%2520is%2520general%2520and%2520can%2520work%2520across%250Areasoning%2520domains%252C%2520including%2520math%252C%2520code%252C%2520and%2520logic.%2520We%2520demonstrate%2520our%250Aframework%2520on%2520four%2520widely-used%2520benchmarks%2520to%2520evaluate%2520several%2520families%2520of%2520LLMs%252C%250Aand%2520observe%2520reductions%2520in%2520performance%2520when%2520the%2520models%2520are%2520queried%2520with%2520problem%250Avariations.%2520These%2520assessments%2520indicate%2520a%2520degree%2520of%2520reliance%2520on%2520statistical%250Arecall%2520for%2520past%2520performance%252C%2520and%2520open%2520the%2520door%2520to%2520further%2520research%2520targeting%250Askills%2520across%2520the%2520reasoning%2520hierarchy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RE-IMAGINE%3A%20Symbolic%20Benchmark%20Synthesis%20for%20Reasoning%20Evaluation&entry.906535625=Xinnuo%20Xu%20and%20Rachel%20Lawrence%20and%20Kshitij%20Dubey%20and%20Atharva%20Pandey%20and%20Risa%20Ueno%20and%20Fabian%20Falck%20and%20Aditya%20V.%20Nori%20and%20Rahul%20Sharma%20and%20Amit%20Sharma%20and%20Javier%20Gonzalez&entry.1292438233=%20%20Recent%20Large%20Language%20Models%20%28LLMs%29%20have%20reported%20high%20accuracy%20on%20reasoning%0Abenchmarks.%20However%2C%20it%20is%20still%20unclear%20whether%20the%20observed%20results%20arise%0Afrom%20true%20reasoning%20or%20from%20statistical%20recall%20of%20the%20training%20set.%20Inspired%20by%0Athe%20ladder%20of%20causation%20%28Pearl%2C%202009%29%20and%20its%20three%20levels%20%28associations%2C%0Ainterventions%20and%20counterfactuals%29%2C%20this%20paper%20introduces%20RE-IMAGINE%2C%20a%0Aframework%20to%20characterize%20a%20hierarchy%20of%20reasoning%20ability%20in%20LLMs%2C%20alongside%0Aan%20automated%20pipeline%20to%20generate%20problem%20variations%20at%20different%20levels%20of%20the%0Ahierarchy.%20By%20altering%20problems%20in%20an%20intermediate%20symbolic%20representation%2C%0ARE-IMAGINE%20generates%20arbitrarily%20many%20problems%20that%20are%20not%20solvable%20using%0Amemorization%20alone.%20Moreover%2C%20the%20framework%20is%20general%20and%20can%20work%20across%0Areasoning%20domains%2C%20including%20math%2C%20code%2C%20and%20logic.%20We%20demonstrate%20our%0Aframework%20on%20four%20widely-used%20benchmarks%20to%20evaluate%20several%20families%20of%20LLMs%2C%0Aand%20observe%20reductions%20in%20performance%20when%20the%20models%20are%20queried%20with%20problem%0Avariations.%20These%20assessments%20indicate%20a%20degree%20of%20reliance%20on%20statistical%0Arecall%20for%20past%20performance%2C%20and%20open%20the%20door%20to%20further%20research%20targeting%0Askills%20across%20the%20reasoning%20hierarchy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15455v1&entry.124074799=Read"},
{"title": "Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence", "author": "\u0130lker I\u015f\u0131k and Ramazan Gokberk Cinbis and Ebru Aydin Gol", "abstract": "  Language models lack the notion of interchangeable tokens: symbols that are\nsemantically equivalent yet distinct, such as bound variables in formal logic.\nThis limitation prevents generalization to larger vocabularies and hinders the\nmodel's ability to recognize alpha-equivalence, where renaming bound variables\npreserves meaning. We formalize this machine learning problem and introduce\nalpha-covariance, a metric for evaluating robustness to such transformations.\nTo tackle this task, we propose a dual-part token embedding strategy: a shared\ncomponent ensures semantic consistency, while a randomized component maintains\ntoken distinguishability. Compared to a baseline that relies on alpha-renaming\nfor data augmentation, our approach demonstrates improved generalization to\nunseen tokens in linear temporal logic solving, propositional logic assignment\nprediction, and copying with an extendable vocabulary, while introducing a\nfavorable inductive bias for alpha-equivalence. Our findings establish a\nfoundation for designing language models that can learn interchangeable token\nrepresentations, a crucial step toward more flexible and systematic reasoning\nin formal domains. Our code and project page are available at\nhttps://necrashter.github.io/interchangeable-token-embeddings\n", "link": "http://arxiv.org/abs/2410.17161v3", "date": "2025-06-18", "relevancy": 2.5424, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interchangeable%20Token%20Embeddings%20for%20Extendable%20Vocabulary%20and%0A%20%20Alpha-Equivalence&body=Title%3A%20Interchangeable%20Token%20Embeddings%20for%20Extendable%20Vocabulary%20and%0A%20%20Alpha-Equivalence%0AAuthor%3A%20%C4%B0lker%20I%C5%9F%C4%B1k%20and%20Ramazan%20Gokberk%20Cinbis%20and%20Ebru%20Aydin%20Gol%0AAbstract%3A%20%20%20Language%20models%20lack%20the%20notion%20of%20interchangeable%20tokens%3A%20symbols%20that%20are%0Asemantically%20equivalent%20yet%20distinct%2C%20such%20as%20bound%20variables%20in%20formal%20logic.%0AThis%20limitation%20prevents%20generalization%20to%20larger%20vocabularies%20and%20hinders%20the%0Amodel%27s%20ability%20to%20recognize%20alpha-equivalence%2C%20where%20renaming%20bound%20variables%0Apreserves%20meaning.%20We%20formalize%20this%20machine%20learning%20problem%20and%20introduce%0Aalpha-covariance%2C%20a%20metric%20for%20evaluating%20robustness%20to%20such%20transformations.%0ATo%20tackle%20this%20task%2C%20we%20propose%20a%20dual-part%20token%20embedding%20strategy%3A%20a%20shared%0Acomponent%20ensures%20semantic%20consistency%2C%20while%20a%20randomized%20component%20maintains%0Atoken%20distinguishability.%20Compared%20to%20a%20baseline%20that%20relies%20on%20alpha-renaming%0Afor%20data%20augmentation%2C%20our%20approach%20demonstrates%20improved%20generalization%20to%0Aunseen%20tokens%20in%20linear%20temporal%20logic%20solving%2C%20propositional%20logic%20assignment%0Aprediction%2C%20and%20copying%20with%20an%20extendable%20vocabulary%2C%20while%20introducing%20a%0Afavorable%20inductive%20bias%20for%20alpha-equivalence.%20Our%20findings%20establish%20a%0Afoundation%20for%20designing%20language%20models%20that%20can%20learn%20interchangeable%20token%0Arepresentations%2C%20a%20crucial%20step%20toward%20more%20flexible%20and%20systematic%20reasoning%0Ain%20formal%20domains.%20Our%20code%20and%20project%20page%20are%20available%20at%0Ahttps%3A//necrashter.github.io/interchangeable-token-embeddings%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17161v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterchangeable%2520Token%2520Embeddings%2520for%2520Extendable%2520Vocabulary%2520and%250A%2520%2520Alpha-Equivalence%26entry.906535625%3D%25C4%25B0lker%2520I%25C5%259F%25C4%25B1k%2520and%2520Ramazan%2520Gokberk%2520Cinbis%2520and%2520Ebru%2520Aydin%2520Gol%26entry.1292438233%3D%2520%2520Language%2520models%2520lack%2520the%2520notion%2520of%2520interchangeable%2520tokens%253A%2520symbols%2520that%2520are%250Asemantically%2520equivalent%2520yet%2520distinct%252C%2520such%2520as%2520bound%2520variables%2520in%2520formal%2520logic.%250AThis%2520limitation%2520prevents%2520generalization%2520to%2520larger%2520vocabularies%2520and%2520hinders%2520the%250Amodel%2527s%2520ability%2520to%2520recognize%2520alpha-equivalence%252C%2520where%2520renaming%2520bound%2520variables%250Apreserves%2520meaning.%2520We%2520formalize%2520this%2520machine%2520learning%2520problem%2520and%2520introduce%250Aalpha-covariance%252C%2520a%2520metric%2520for%2520evaluating%2520robustness%2520to%2520such%2520transformations.%250ATo%2520tackle%2520this%2520task%252C%2520we%2520propose%2520a%2520dual-part%2520token%2520embedding%2520strategy%253A%2520a%2520shared%250Acomponent%2520ensures%2520semantic%2520consistency%252C%2520while%2520a%2520randomized%2520component%2520maintains%250Atoken%2520distinguishability.%2520Compared%2520to%2520a%2520baseline%2520that%2520relies%2520on%2520alpha-renaming%250Afor%2520data%2520augmentation%252C%2520our%2520approach%2520demonstrates%2520improved%2520generalization%2520to%250Aunseen%2520tokens%2520in%2520linear%2520temporal%2520logic%2520solving%252C%2520propositional%2520logic%2520assignment%250Aprediction%252C%2520and%2520copying%2520with%2520an%2520extendable%2520vocabulary%252C%2520while%2520introducing%2520a%250Afavorable%2520inductive%2520bias%2520for%2520alpha-equivalence.%2520Our%2520findings%2520establish%2520a%250Afoundation%2520for%2520designing%2520language%2520models%2520that%2520can%2520learn%2520interchangeable%2520token%250Arepresentations%252C%2520a%2520crucial%2520step%2520toward%2520more%2520flexible%2520and%2520systematic%2520reasoning%250Ain%2520formal%2520domains.%2520Our%2520code%2520and%2520project%2520page%2520are%2520available%2520at%250Ahttps%253A//necrashter.github.io/interchangeable-token-embeddings%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17161v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interchangeable%20Token%20Embeddings%20for%20Extendable%20Vocabulary%20and%0A%20%20Alpha-Equivalence&entry.906535625=%C4%B0lker%20I%C5%9F%C4%B1k%20and%20Ramazan%20Gokberk%20Cinbis%20and%20Ebru%20Aydin%20Gol&entry.1292438233=%20%20Language%20models%20lack%20the%20notion%20of%20interchangeable%20tokens%3A%20symbols%20that%20are%0Asemantically%20equivalent%20yet%20distinct%2C%20such%20as%20bound%20variables%20in%20formal%20logic.%0AThis%20limitation%20prevents%20generalization%20to%20larger%20vocabularies%20and%20hinders%20the%0Amodel%27s%20ability%20to%20recognize%20alpha-equivalence%2C%20where%20renaming%20bound%20variables%0Apreserves%20meaning.%20We%20formalize%20this%20machine%20learning%20problem%20and%20introduce%0Aalpha-covariance%2C%20a%20metric%20for%20evaluating%20robustness%20to%20such%20transformations.%0ATo%20tackle%20this%20task%2C%20we%20propose%20a%20dual-part%20token%20embedding%20strategy%3A%20a%20shared%0Acomponent%20ensures%20semantic%20consistency%2C%20while%20a%20randomized%20component%20maintains%0Atoken%20distinguishability.%20Compared%20to%20a%20baseline%20that%20relies%20on%20alpha-renaming%0Afor%20data%20augmentation%2C%20our%20approach%20demonstrates%20improved%20generalization%20to%0Aunseen%20tokens%20in%20linear%20temporal%20logic%20solving%2C%20propositional%20logic%20assignment%0Aprediction%2C%20and%20copying%20with%20an%20extendable%20vocabulary%2C%20while%20introducing%20a%0Afavorable%20inductive%20bias%20for%20alpha-equivalence.%20Our%20findings%20establish%20a%0Afoundation%20for%20designing%20language%20models%20that%20can%20learn%20interchangeable%20token%0Arepresentations%2C%20a%20crucial%20step%20toward%20more%20flexible%20and%20systematic%20reasoning%0Ain%20formal%20domains.%20Our%20code%20and%20project%20page%20are%20available%20at%0Ahttps%3A//necrashter.github.io/interchangeable-token-embeddings%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17161v3&entry.124074799=Read"},
{"title": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System", "author": "Miaoxin Pan and Jinnan Li and Yaowen Zhang and Yi Yang and Yufeng Yue", "abstract": "  Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.\n", "link": "http://arxiv.org/abs/2506.15402v1", "date": "2025-06-18", "relevancy": 2.5152, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6459}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6245}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCOO-SLAM%3A%20A%20Multi-Camera%20Omnidirectional%20Object%20SLAM%20System&body=Title%3A%20MCOO-SLAM%3A%20A%20Multi-Camera%20Omnidirectional%20Object%20SLAM%20System%0AAuthor%3A%20Miaoxin%20Pan%20and%20Jinnan%20Li%20and%20Yaowen%20Zhang%20and%20Yi%20Yang%20and%20Yufeng%20Yue%0AAbstract%3A%20%20%20Object-level%20SLAM%20offers%20structured%20and%20semantically%20meaningful%20environment%0Arepresentations%2C%20making%20it%20more%20interpretable%20and%20suitable%20for%20high-level%0Arobotic%20tasks.%20However%2C%20most%20existing%20approaches%20rely%20on%20RGB-D%20sensors%20or%0Amonocular%20views%2C%20which%20suffer%20from%20narrow%20fields%20of%20view%2C%20occlusion%0Asensitivity%2C%20and%20limited%20depth%20perception-especially%20in%20large-scale%20or%20outdoor%0Aenvironments.%20These%20limitations%20often%20restrict%20the%20system%20to%20observing%20only%0Apartial%20views%20of%20objects%20from%20limited%20perspectives%2C%20leading%20to%20inaccurate%0Aobject%20modeling%20and%20unreliable%20data%20association.%20In%20this%20work%2C%20we%20propose%0AMCOO-SLAM%2C%20a%20novel%20Multi-Camera%20Omnidirectional%20Object%20SLAM%20system%20that%20fully%0Aleverages%20surround-view%20camera%20configurations%20to%20achieve%20robust%2C%20consistent%2C%0Aand%20semantically%20enriched%20mapping%20in%20complex%20outdoor%20scenarios.%20Our%20approach%0Aintegrates%20point%20features%20and%20object-level%20landmarks%20enhanced%20with%0Aopen-vocabulary%20semantics.%20A%20semantic-geometric-temporal%20fusion%20strategy%20is%0Aintroduced%20for%20robust%20object%20association%20across%20multiple%20views%2C%20leading%20to%0Aimproved%20consistency%20and%20accurate%20object%20modeling%2C%20and%20an%20omnidirectional%20loop%0Aclosure%20module%20is%20designed%20to%20enable%20viewpoint-invariant%20place%20recognition%0Ausing%20scene-level%20descriptors.%20Furthermore%2C%20the%20constructed%20map%20is%20abstracted%0Ainto%20a%20hierarchical%203D%20scene%20graph%20to%20support%20downstream%20reasoning%20tasks.%0AExtensive%20experiments%20in%20real-world%20demonstrate%20that%20MCOO-SLAM%20achieves%0Aaccurate%20localization%20and%20scalable%20object-level%20mapping%20with%20improved%0Arobustness%20to%20occlusion%2C%20pose%20variation%2C%20and%20environmental%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCOO-SLAM%253A%2520A%2520Multi-Camera%2520Omnidirectional%2520Object%2520SLAM%2520System%26entry.906535625%3DMiaoxin%2520Pan%2520and%2520Jinnan%2520Li%2520and%2520Yaowen%2520Zhang%2520and%2520Yi%2520Yang%2520and%2520Yufeng%2520Yue%26entry.1292438233%3D%2520%2520Object-level%2520SLAM%2520offers%2520structured%2520and%2520semantically%2520meaningful%2520environment%250Arepresentations%252C%2520making%2520it%2520more%2520interpretable%2520and%2520suitable%2520for%2520high-level%250Arobotic%2520tasks.%2520However%252C%2520most%2520existing%2520approaches%2520rely%2520on%2520RGB-D%2520sensors%2520or%250Amonocular%2520views%252C%2520which%2520suffer%2520from%2520narrow%2520fields%2520of%2520view%252C%2520occlusion%250Asensitivity%252C%2520and%2520limited%2520depth%2520perception-especially%2520in%2520large-scale%2520or%2520outdoor%250Aenvironments.%2520These%2520limitations%2520often%2520restrict%2520the%2520system%2520to%2520observing%2520only%250Apartial%2520views%2520of%2520objects%2520from%2520limited%2520perspectives%252C%2520leading%2520to%2520inaccurate%250Aobject%2520modeling%2520and%2520unreliable%2520data%2520association.%2520In%2520this%2520work%252C%2520we%2520propose%250AMCOO-SLAM%252C%2520a%2520novel%2520Multi-Camera%2520Omnidirectional%2520Object%2520SLAM%2520system%2520that%2520fully%250Aleverages%2520surround-view%2520camera%2520configurations%2520to%2520achieve%2520robust%252C%2520consistent%252C%250Aand%2520semantically%2520enriched%2520mapping%2520in%2520complex%2520outdoor%2520scenarios.%2520Our%2520approach%250Aintegrates%2520point%2520features%2520and%2520object-level%2520landmarks%2520enhanced%2520with%250Aopen-vocabulary%2520semantics.%2520A%2520semantic-geometric-temporal%2520fusion%2520strategy%2520is%250Aintroduced%2520for%2520robust%2520object%2520association%2520across%2520multiple%2520views%252C%2520leading%2520to%250Aimproved%2520consistency%2520and%2520accurate%2520object%2520modeling%252C%2520and%2520an%2520omnidirectional%2520loop%250Aclosure%2520module%2520is%2520designed%2520to%2520enable%2520viewpoint-invariant%2520place%2520recognition%250Ausing%2520scene-level%2520descriptors.%2520Furthermore%252C%2520the%2520constructed%2520map%2520is%2520abstracted%250Ainto%2520a%2520hierarchical%25203D%2520scene%2520graph%2520to%2520support%2520downstream%2520reasoning%2520tasks.%250AExtensive%2520experiments%2520in%2520real-world%2520demonstrate%2520that%2520MCOO-SLAM%2520achieves%250Aaccurate%2520localization%2520and%2520scalable%2520object-level%2520mapping%2520with%2520improved%250Arobustness%2520to%2520occlusion%252C%2520pose%2520variation%252C%2520and%2520environmental%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCOO-SLAM%3A%20A%20Multi-Camera%20Omnidirectional%20Object%20SLAM%20System&entry.906535625=Miaoxin%20Pan%20and%20Jinnan%20Li%20and%20Yaowen%20Zhang%20and%20Yi%20Yang%20and%20Yufeng%20Yue&entry.1292438233=%20%20Object-level%20SLAM%20offers%20structured%20and%20semantically%20meaningful%20environment%0Arepresentations%2C%20making%20it%20more%20interpretable%20and%20suitable%20for%20high-level%0Arobotic%20tasks.%20However%2C%20most%20existing%20approaches%20rely%20on%20RGB-D%20sensors%20or%0Amonocular%20views%2C%20which%20suffer%20from%20narrow%20fields%20of%20view%2C%20occlusion%0Asensitivity%2C%20and%20limited%20depth%20perception-especially%20in%20large-scale%20or%20outdoor%0Aenvironments.%20These%20limitations%20often%20restrict%20the%20system%20to%20observing%20only%0Apartial%20views%20of%20objects%20from%20limited%20perspectives%2C%20leading%20to%20inaccurate%0Aobject%20modeling%20and%20unreliable%20data%20association.%20In%20this%20work%2C%20we%20propose%0AMCOO-SLAM%2C%20a%20novel%20Multi-Camera%20Omnidirectional%20Object%20SLAM%20system%20that%20fully%0Aleverages%20surround-view%20camera%20configurations%20to%20achieve%20robust%2C%20consistent%2C%0Aand%20semantically%20enriched%20mapping%20in%20complex%20outdoor%20scenarios.%20Our%20approach%0Aintegrates%20point%20features%20and%20object-level%20landmarks%20enhanced%20with%0Aopen-vocabulary%20semantics.%20A%20semantic-geometric-temporal%20fusion%20strategy%20is%0Aintroduced%20for%20robust%20object%20association%20across%20multiple%20views%2C%20leading%20to%0Aimproved%20consistency%20and%20accurate%20object%20modeling%2C%20and%20an%20omnidirectional%20loop%0Aclosure%20module%20is%20designed%20to%20enable%20viewpoint-invariant%20place%20recognition%0Ausing%20scene-level%20descriptors.%20Furthermore%2C%20the%20constructed%20map%20is%20abstracted%0Ainto%20a%20hierarchical%203D%20scene%20graph%20to%20support%20downstream%20reasoning%20tasks.%0AExtensive%20experiments%20in%20real-world%20demonstrate%20that%20MCOO-SLAM%20achieves%0Aaccurate%20localization%20and%20scalable%20object-level%20mapping%20with%20improved%0Arobustness%20to%20occlusion%2C%20pose%20variation%2C%20and%20environmental%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15402v1&entry.124074799=Read"},
{"title": "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via\n  Real-Time Multi-View Box Fusion", "author": "Yuqing Lan and Chenyang Zhu and Zhirui Gao and Jiazhao Zhang and Yihan Cao and Renjiao Yi and Yijie Wang and Kai Xu", "abstract": "  Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.\n", "link": "http://arxiv.org/abs/2506.15610v1", "date": "2025-06-18", "relevancy": 2.5137, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6335}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6335}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoxFusion%3A%20Reconstruction-Free%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Real-Time%20Multi-View%20Box%20Fusion&body=Title%3A%20BoxFusion%3A%20Reconstruction-Free%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Real-Time%20Multi-View%20Box%20Fusion%0AAuthor%3A%20Yuqing%20Lan%20and%20Chenyang%20Zhu%20and%20Zhirui%20Gao%20and%20Jiazhao%20Zhang%20and%20Yihan%20Cao%20and%20Renjiao%20Yi%20and%20Yijie%20Wang%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Open-vocabulary%203D%20object%20detection%20has%20gained%20significant%20interest%20due%20to%0Aits%20critical%20applications%20in%20autonomous%20driving%20and%20embodied%20AI.%20Existing%0Adetection%20methods%2C%20whether%20offline%20or%20online%2C%20typically%20rely%20on%20dense%20point%0Acloud%20reconstruction%2C%20which%20imposes%20substantial%20computational%20overhead%20and%0Amemory%20constraints%2C%20hindering%20real-time%20deployment%20in%20downstream%20tasks.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20reconstruction-free%20online%20framework%20tailored%0Afor%20memory-efficient%20and%20real-time%203D%20detection.%20Specifically%2C%20given%20streaming%0Aposed%20RGB-D%20video%20input%2C%20we%20leverage%20Cubify%20Anything%20as%20a%20pre-trained%20visual%0Afoundation%20model%20%28VFM%29%20for%20single-view%203D%20object%20detection%20by%20bounding%20boxes%2C%0Acoupled%20with%20CLIP%20to%20capture%20open-vocabulary%20semantics%20of%20detected%20objects.%20To%0Afuse%20all%20detected%20bounding%20boxes%20across%20different%20views%20into%20a%20unified%20one%2C%20we%0Aemploy%20an%20association%20module%20for%20correspondences%20of%20multi-views%20and%20an%0Aoptimization%20module%20to%20fuse%20the%203D%20bounding%20boxes%20of%20the%20same%20instance%0Apredicted%20in%20multi-views.%20The%20association%20module%20utilizes%203D%20Non-Maximum%0ASuppression%20%28NMS%29%20and%20a%20box%20correspondence%20matching%20module%2C%20while%20the%0Aoptimization%20module%20uses%20an%20IoU-guided%20efficient%20random%20optimization%20technique%0Abased%20on%20particle%20filtering%20to%20enforce%20multi-view%20consistency%20of%20the%203D%0Abounding%20boxes%20while%20minimizing%20computational%20complexity.%20Extensive%20experiments%0Aon%20ScanNetV2%20and%20CA-1M%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20among%20online%20methods.%20Benefiting%20from%20this%20novel%0Areconstruction-free%20paradigm%20for%203D%20object%20detection%2C%20our%20method%20exhibits%20great%0Ageneralization%20abilities%20in%20various%20scenarios%2C%20enabling%20real-time%20perception%0Aeven%20in%20environments%20exceeding%201000%20square%20meters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoxFusion%253A%2520Reconstruction-Free%2520Open-Vocabulary%25203D%2520Object%2520Detection%2520via%250A%2520%2520Real-Time%2520Multi-View%2520Box%2520Fusion%26entry.906535625%3DYuqing%2520Lan%2520and%2520Chenyang%2520Zhu%2520and%2520Zhirui%2520Gao%2520and%2520Jiazhao%2520Zhang%2520and%2520Yihan%2520Cao%2520and%2520Renjiao%2520Yi%2520and%2520Yijie%2520Wang%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520object%2520detection%2520has%2520gained%2520significant%2520interest%2520due%2520to%250Aits%2520critical%2520applications%2520in%2520autonomous%2520driving%2520and%2520embodied%2520AI.%2520Existing%250Adetection%2520methods%252C%2520whether%2520offline%2520or%2520online%252C%2520typically%2520rely%2520on%2520dense%2520point%250Acloud%2520reconstruction%252C%2520which%2520imposes%2520substantial%2520computational%2520overhead%2520and%250Amemory%2520constraints%252C%2520hindering%2520real-time%2520deployment%2520in%2520downstream%2520tasks.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520novel%2520reconstruction-free%2520online%2520framework%2520tailored%250Afor%2520memory-efficient%2520and%2520real-time%25203D%2520detection.%2520Specifically%252C%2520given%2520streaming%250Aposed%2520RGB-D%2520video%2520input%252C%2520we%2520leverage%2520Cubify%2520Anything%2520as%2520a%2520pre-trained%2520visual%250Afoundation%2520model%2520%2528VFM%2529%2520for%2520single-view%25203D%2520object%2520detection%2520by%2520bounding%2520boxes%252C%250Acoupled%2520with%2520CLIP%2520to%2520capture%2520open-vocabulary%2520semantics%2520of%2520detected%2520objects.%2520To%250Afuse%2520all%2520detected%2520bounding%2520boxes%2520across%2520different%2520views%2520into%2520a%2520unified%2520one%252C%2520we%250Aemploy%2520an%2520association%2520module%2520for%2520correspondences%2520of%2520multi-views%2520and%2520an%250Aoptimization%2520module%2520to%2520fuse%2520the%25203D%2520bounding%2520boxes%2520of%2520the%2520same%2520instance%250Apredicted%2520in%2520multi-views.%2520The%2520association%2520module%2520utilizes%25203D%2520Non-Maximum%250ASuppression%2520%2528NMS%2529%2520and%2520a%2520box%2520correspondence%2520matching%2520module%252C%2520while%2520the%250Aoptimization%2520module%2520uses%2520an%2520IoU-guided%2520efficient%2520random%2520optimization%2520technique%250Abased%2520on%2520particle%2520filtering%2520to%2520enforce%2520multi-view%2520consistency%2520of%2520the%25203D%250Abounding%2520boxes%2520while%2520minimizing%2520computational%2520complexity.%2520Extensive%2520experiments%250Aon%2520ScanNetV2%2520and%2520CA-1M%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520among%2520online%2520methods.%2520Benefiting%2520from%2520this%2520novel%250Areconstruction-free%2520paradigm%2520for%25203D%2520object%2520detection%252C%2520our%2520method%2520exhibits%2520great%250Ageneralization%2520abilities%2520in%2520various%2520scenarios%252C%2520enabling%2520real-time%2520perception%250Aeven%2520in%2520environments%2520exceeding%25201000%2520square%2520meters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoxFusion%3A%20Reconstruction-Free%20Open-Vocabulary%203D%20Object%20Detection%20via%0A%20%20Real-Time%20Multi-View%20Box%20Fusion&entry.906535625=Yuqing%20Lan%20and%20Chenyang%20Zhu%20and%20Zhirui%20Gao%20and%20Jiazhao%20Zhang%20and%20Yihan%20Cao%20and%20Renjiao%20Yi%20and%20Yijie%20Wang%20and%20Kai%20Xu&entry.1292438233=%20%20Open-vocabulary%203D%20object%20detection%20has%20gained%20significant%20interest%20due%20to%0Aits%20critical%20applications%20in%20autonomous%20driving%20and%20embodied%20AI.%20Existing%0Adetection%20methods%2C%20whether%20offline%20or%20online%2C%20typically%20rely%20on%20dense%20point%0Acloud%20reconstruction%2C%20which%20imposes%20substantial%20computational%20overhead%20and%0Amemory%20constraints%2C%20hindering%20real-time%20deployment%20in%20downstream%20tasks.%20To%0Aaddress%20this%2C%20we%20propose%20a%20novel%20reconstruction-free%20online%20framework%20tailored%0Afor%20memory-efficient%20and%20real-time%203D%20detection.%20Specifically%2C%20given%20streaming%0Aposed%20RGB-D%20video%20input%2C%20we%20leverage%20Cubify%20Anything%20as%20a%20pre-trained%20visual%0Afoundation%20model%20%28VFM%29%20for%20single-view%203D%20object%20detection%20by%20bounding%20boxes%2C%0Acoupled%20with%20CLIP%20to%20capture%20open-vocabulary%20semantics%20of%20detected%20objects.%20To%0Afuse%20all%20detected%20bounding%20boxes%20across%20different%20views%20into%20a%20unified%20one%2C%20we%0Aemploy%20an%20association%20module%20for%20correspondences%20of%20multi-views%20and%20an%0Aoptimization%20module%20to%20fuse%20the%203D%20bounding%20boxes%20of%20the%20same%20instance%0Apredicted%20in%20multi-views.%20The%20association%20module%20utilizes%203D%20Non-Maximum%0ASuppression%20%28NMS%29%20and%20a%20box%20correspondence%20matching%20module%2C%20while%20the%0Aoptimization%20module%20uses%20an%20IoU-guided%20efficient%20random%20optimization%20technique%0Abased%20on%20particle%20filtering%20to%20enforce%20multi-view%20consistency%20of%20the%203D%0Abounding%20boxes%20while%20minimizing%20computational%20complexity.%20Extensive%20experiments%0Aon%20ScanNetV2%20and%20CA-1M%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20among%20online%20methods.%20Benefiting%20from%20this%20novel%0Areconstruction-free%20paradigm%20for%203D%20object%20detection%2C%20our%20method%20exhibits%20great%0Ageneralization%20abilities%20in%20various%20scenarios%2C%20enabling%20real-time%20perception%0Aeven%20in%20environments%20exceeding%201000%20square%20meters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15610v1&entry.124074799=Read"},
{"title": "Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition", "author": "Andi Chen and Hua-Lei Yin and Zeng-Bing Chen and Shengjun Wu", "abstract": "  In this paper, we propose two hybrid quantum-inspired neural networks with\nadaptive residual and dense connections respectively for pattern recognition.\nWe explain the frameworks of the symmetrical circuit models in the\nquantum-inspired layers in our hybrid models. We also illustrate the potential\nsuperiority of our hybrid models to prevent gradient explosion owing to the\nsine and cosine functions in the quantum-inspired layers. Groups of numerical\nexperiments on generalization power showcase that our hybrid models are\ncomparable to the pure classical models with different noisy datasets utilized.\nFurthermore, the comparison between our hybrid models and a state-of-the-art\nhybrid quantum-classical convolutional network demonstrates 3%-4% higher\naccuracy of our hybrid densely-connected model than the hybrid\nquantum-classical network. Additionally, compared with other two hybrid\nquantum-inspired residual networks, our hybrid models showcase a little higher\naccuracy on image datasets with asymmetrical noises. Simultaneously, in terms\nof groups of robustness experiments, the outcomes demonstrate that our two\nhybrid models outperform pure classical models notably in resistance to\nadversarial parameter attacks with various asymmetrical noises. They also\nindicate the slight superiority of our densely-connected hybrid model over the\nhybrid quantum-classical network to both symmetrical and asymmetrical attacks.\nMeanwhile, the accuracy of our two hybrid models is a little bit higher than\nthat of the two hybrid quantum-inspired residual networks. In addition, an\nablation study indicate that the recognition accuracy of our two hybrid models\nis 2%-3% higher than that of the traditional quantum-inspired neural network\nwithout residual or dense connection. Eventually, we discuss the application\nscenarios of our hybrid models by analyzing their computational complexity.\n", "link": "http://arxiv.org/abs/2403.05754v7", "date": "2025-06-18", "relevancy": 2.5109, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5278}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4896}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Quantum-inspired%20Resnet%20and%20Densenet%20for%20Pattern%20Recognition&body=Title%3A%20Hybrid%20Quantum-inspired%20Resnet%20and%20Densenet%20for%20Pattern%20Recognition%0AAuthor%3A%20Andi%20Chen%20and%20Hua-Lei%20Yin%20and%20Zeng-Bing%20Chen%20and%20Shengjun%20Wu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20two%20hybrid%20quantum-inspired%20neural%20networks%20with%0Aadaptive%20residual%20and%20dense%20connections%20respectively%20for%20pattern%20recognition.%0AWe%20explain%20the%20frameworks%20of%20the%20symmetrical%20circuit%20models%20in%20the%0Aquantum-inspired%20layers%20in%20our%20hybrid%20models.%20We%20also%20illustrate%20the%20potential%0Asuperiority%20of%20our%20hybrid%20models%20to%20prevent%20gradient%20explosion%20owing%20to%20the%0Asine%20and%20cosine%20functions%20in%20the%20quantum-inspired%20layers.%20Groups%20of%20numerical%0Aexperiments%20on%20generalization%20power%20showcase%20that%20our%20hybrid%20models%20are%0Acomparable%20to%20the%20pure%20classical%20models%20with%20different%20noisy%20datasets%20utilized.%0AFurthermore%2C%20the%20comparison%20between%20our%20hybrid%20models%20and%20a%20state-of-the-art%0Ahybrid%20quantum-classical%20convolutional%20network%20demonstrates%203%25-4%25%20higher%0Aaccuracy%20of%20our%20hybrid%20densely-connected%20model%20than%20the%20hybrid%0Aquantum-classical%20network.%20Additionally%2C%20compared%20with%20other%20two%20hybrid%0Aquantum-inspired%20residual%20networks%2C%20our%20hybrid%20models%20showcase%20a%20little%20higher%0Aaccuracy%20on%20image%20datasets%20with%20asymmetrical%20noises.%20Simultaneously%2C%20in%20terms%0Aof%20groups%20of%20robustness%20experiments%2C%20the%20outcomes%20demonstrate%20that%20our%20two%0Ahybrid%20models%20outperform%20pure%20classical%20models%20notably%20in%20resistance%20to%0Aadversarial%20parameter%20attacks%20with%20various%20asymmetrical%20noises.%20They%20also%0Aindicate%20the%20slight%20superiority%20of%20our%20densely-connected%20hybrid%20model%20over%20the%0Ahybrid%20quantum-classical%20network%20to%20both%20symmetrical%20and%20asymmetrical%20attacks.%0AMeanwhile%2C%20the%20accuracy%20of%20our%20two%20hybrid%20models%20is%20a%20little%20bit%20higher%20than%0Athat%20of%20the%20two%20hybrid%20quantum-inspired%20residual%20networks.%20In%20addition%2C%20an%0Aablation%20study%20indicate%20that%20the%20recognition%20accuracy%20of%20our%20two%20hybrid%20models%0Ais%202%25-3%25%20higher%20than%20that%20of%20the%20traditional%20quantum-inspired%20neural%20network%0Awithout%20residual%20or%20dense%20connection.%20Eventually%2C%20we%20discuss%20the%20application%0Ascenarios%20of%20our%20hybrid%20models%20by%20analyzing%20their%20computational%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05754v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Quantum-inspired%2520Resnet%2520and%2520Densenet%2520for%2520Pattern%2520Recognition%26entry.906535625%3DAndi%2520Chen%2520and%2520Hua-Lei%2520Yin%2520and%2520Zeng-Bing%2520Chen%2520and%2520Shengjun%2520Wu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520two%2520hybrid%2520quantum-inspired%2520neural%2520networks%2520with%250Aadaptive%2520residual%2520and%2520dense%2520connections%2520respectively%2520for%2520pattern%2520recognition.%250AWe%2520explain%2520the%2520frameworks%2520of%2520the%2520symmetrical%2520circuit%2520models%2520in%2520the%250Aquantum-inspired%2520layers%2520in%2520our%2520hybrid%2520models.%2520We%2520also%2520illustrate%2520the%2520potential%250Asuperiority%2520of%2520our%2520hybrid%2520models%2520to%2520prevent%2520gradient%2520explosion%2520owing%2520to%2520the%250Asine%2520and%2520cosine%2520functions%2520in%2520the%2520quantum-inspired%2520layers.%2520Groups%2520of%2520numerical%250Aexperiments%2520on%2520generalization%2520power%2520showcase%2520that%2520our%2520hybrid%2520models%2520are%250Acomparable%2520to%2520the%2520pure%2520classical%2520models%2520with%2520different%2520noisy%2520datasets%2520utilized.%250AFurthermore%252C%2520the%2520comparison%2520between%2520our%2520hybrid%2520models%2520and%2520a%2520state-of-the-art%250Ahybrid%2520quantum-classical%2520convolutional%2520network%2520demonstrates%25203%2525-4%2525%2520higher%250Aaccuracy%2520of%2520our%2520hybrid%2520densely-connected%2520model%2520than%2520the%2520hybrid%250Aquantum-classical%2520network.%2520Additionally%252C%2520compared%2520with%2520other%2520two%2520hybrid%250Aquantum-inspired%2520residual%2520networks%252C%2520our%2520hybrid%2520models%2520showcase%2520a%2520little%2520higher%250Aaccuracy%2520on%2520image%2520datasets%2520with%2520asymmetrical%2520noises.%2520Simultaneously%252C%2520in%2520terms%250Aof%2520groups%2520of%2520robustness%2520experiments%252C%2520the%2520outcomes%2520demonstrate%2520that%2520our%2520two%250Ahybrid%2520models%2520outperform%2520pure%2520classical%2520models%2520notably%2520in%2520resistance%2520to%250Aadversarial%2520parameter%2520attacks%2520with%2520various%2520asymmetrical%2520noises.%2520They%2520also%250Aindicate%2520the%2520slight%2520superiority%2520of%2520our%2520densely-connected%2520hybrid%2520model%2520over%2520the%250Ahybrid%2520quantum-classical%2520network%2520to%2520both%2520symmetrical%2520and%2520asymmetrical%2520attacks.%250AMeanwhile%252C%2520the%2520accuracy%2520of%2520our%2520two%2520hybrid%2520models%2520is%2520a%2520little%2520bit%2520higher%2520than%250Athat%2520of%2520the%2520two%2520hybrid%2520quantum-inspired%2520residual%2520networks.%2520In%2520addition%252C%2520an%250Aablation%2520study%2520indicate%2520that%2520the%2520recognition%2520accuracy%2520of%2520our%2520two%2520hybrid%2520models%250Ais%25202%2525-3%2525%2520higher%2520than%2520that%2520of%2520the%2520traditional%2520quantum-inspired%2520neural%2520network%250Awithout%2520residual%2520or%2520dense%2520connection.%2520Eventually%252C%2520we%2520discuss%2520the%2520application%250Ascenarios%2520of%2520our%2520hybrid%2520models%2520by%2520analyzing%2520their%2520computational%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05754v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Quantum-inspired%20Resnet%20and%20Densenet%20for%20Pattern%20Recognition&entry.906535625=Andi%20Chen%20and%20Hua-Lei%20Yin%20and%20Zeng-Bing%20Chen%20and%20Shengjun%20Wu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20two%20hybrid%20quantum-inspired%20neural%20networks%20with%0Aadaptive%20residual%20and%20dense%20connections%20respectively%20for%20pattern%20recognition.%0AWe%20explain%20the%20frameworks%20of%20the%20symmetrical%20circuit%20models%20in%20the%0Aquantum-inspired%20layers%20in%20our%20hybrid%20models.%20We%20also%20illustrate%20the%20potential%0Asuperiority%20of%20our%20hybrid%20models%20to%20prevent%20gradient%20explosion%20owing%20to%20the%0Asine%20and%20cosine%20functions%20in%20the%20quantum-inspired%20layers.%20Groups%20of%20numerical%0Aexperiments%20on%20generalization%20power%20showcase%20that%20our%20hybrid%20models%20are%0Acomparable%20to%20the%20pure%20classical%20models%20with%20different%20noisy%20datasets%20utilized.%0AFurthermore%2C%20the%20comparison%20between%20our%20hybrid%20models%20and%20a%20state-of-the-art%0Ahybrid%20quantum-classical%20convolutional%20network%20demonstrates%203%25-4%25%20higher%0Aaccuracy%20of%20our%20hybrid%20densely-connected%20model%20than%20the%20hybrid%0Aquantum-classical%20network.%20Additionally%2C%20compared%20with%20other%20two%20hybrid%0Aquantum-inspired%20residual%20networks%2C%20our%20hybrid%20models%20showcase%20a%20little%20higher%0Aaccuracy%20on%20image%20datasets%20with%20asymmetrical%20noises.%20Simultaneously%2C%20in%20terms%0Aof%20groups%20of%20robustness%20experiments%2C%20the%20outcomes%20demonstrate%20that%20our%20two%0Ahybrid%20models%20outperform%20pure%20classical%20models%20notably%20in%20resistance%20to%0Aadversarial%20parameter%20attacks%20with%20various%20asymmetrical%20noises.%20They%20also%0Aindicate%20the%20slight%20superiority%20of%20our%20densely-connected%20hybrid%20model%20over%20the%0Ahybrid%20quantum-classical%20network%20to%20both%20symmetrical%20and%20asymmetrical%20attacks.%0AMeanwhile%2C%20the%20accuracy%20of%20our%20two%20hybrid%20models%20is%20a%20little%20bit%20higher%20than%0Athat%20of%20the%20two%20hybrid%20quantum-inspired%20residual%20networks.%20In%20addition%2C%20an%0Aablation%20study%20indicate%20that%20the%20recognition%20accuracy%20of%20our%20two%20hybrid%20models%0Ais%202%25-3%25%20higher%20than%20that%20of%20the%20traditional%20quantum-inspired%20neural%20network%0Awithout%20residual%20or%20dense%20connection.%20Eventually%2C%20we%20discuss%20the%20application%0Ascenarios%20of%20our%20hybrid%20models%20by%20analyzing%20their%20computational%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05754v7&entry.124074799=Read"},
{"title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in\n  LangChain A CoT-Enhanced Prompt Engineering Approach", "author": "Wenqi Guan and Yang Fang", "abstract": "  Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.\n", "link": "http://arxiv.org/abs/2506.15512v1", "date": "2025-06-18", "relevancy": 2.496, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5032}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5001}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Web-Based%20AI%20Query%20Retrieval%20with%20GPT%20Integration%20in%0A%20%20LangChain%20A%20CoT-Enhanced%20Prompt%20Engineering%20Approach&body=Title%3A%20Optimizing%20Web-Based%20AI%20Query%20Retrieval%20with%20GPT%20Integration%20in%0A%20%20LangChain%20A%20CoT-Enhanced%20Prompt%20Engineering%20Approach%0AAuthor%3A%20Wenqi%20Guan%20and%20Yang%20Fang%0AAbstract%3A%20%20%20Large%20Language%20Models%20have%20brought%20a%20radical%20change%20in%20the%20process%20of%20remote%0Alearning%20students%2C%20among%20other%20aspects%20of%20educative%20activities.%20Current%0Aretrieval%20of%20remote%20learning%20resources%20lacks%20depth%20in%20contextual%20meaning%20that%0Aprovides%20comprehensive%20information%20on%20complex%20student%20queries.%20This%20work%0Aproposes%20a%20novel%20approach%20to%20enhancing%20remote%20learning%20retrieval%20by%20integrating%0AGPT-based%20models%20within%20the%20LangChain%20framework.%20We%20achieve%20this%20system%20in%20a%0Amore%20intuitive%20and%20productive%20manner%20using%20CoT%20reasoning%20and%20prompt%0Aengineering.%20The%20framework%20we%20propose%20puts%20much%20emphasis%20on%20increasing%20the%0Aprecision%20and%20relevance%20of%20the%20retrieval%20results%20to%20return%20comprehensive%20and%0Acontextually%20enriched%20explanations%20and%20resources%20that%20best%20suit%20each%20student%27s%0Aneeds.%20We%20also%20assess%20the%20effectiveness%20of%20our%20approach%20against%20paradigmatic%0ALLMs%20and%20report%20improvements%20in%20user%20satisfaction%20and%20learning%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Web-Based%2520AI%2520Query%2520Retrieval%2520with%2520GPT%2520Integration%2520in%250A%2520%2520LangChain%2520A%2520CoT-Enhanced%2520Prompt%2520Engineering%2520Approach%26entry.906535625%3DWenqi%2520Guan%2520and%2520Yang%2520Fang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520have%2520brought%2520a%2520radical%2520change%2520in%2520the%2520process%2520of%2520remote%250Alearning%2520students%252C%2520among%2520other%2520aspects%2520of%2520educative%2520activities.%2520Current%250Aretrieval%2520of%2520remote%2520learning%2520resources%2520lacks%2520depth%2520in%2520contextual%2520meaning%2520that%250Aprovides%2520comprehensive%2520information%2520on%2520complex%2520student%2520queries.%2520This%2520work%250Aproposes%2520a%2520novel%2520approach%2520to%2520enhancing%2520remote%2520learning%2520retrieval%2520by%2520integrating%250AGPT-based%2520models%2520within%2520the%2520LangChain%2520framework.%2520We%2520achieve%2520this%2520system%2520in%2520a%250Amore%2520intuitive%2520and%2520productive%2520manner%2520using%2520CoT%2520reasoning%2520and%2520prompt%250Aengineering.%2520The%2520framework%2520we%2520propose%2520puts%2520much%2520emphasis%2520on%2520increasing%2520the%250Aprecision%2520and%2520relevance%2520of%2520the%2520retrieval%2520results%2520to%2520return%2520comprehensive%2520and%250Acontextually%2520enriched%2520explanations%2520and%2520resources%2520that%2520best%2520suit%2520each%2520student%2527s%250Aneeds.%2520We%2520also%2520assess%2520the%2520effectiveness%2520of%2520our%2520approach%2520against%2520paradigmatic%250ALLMs%2520and%2520report%2520improvements%2520in%2520user%2520satisfaction%2520and%2520learning%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Web-Based%20AI%20Query%20Retrieval%20with%20GPT%20Integration%20in%0A%20%20LangChain%20A%20CoT-Enhanced%20Prompt%20Engineering%20Approach&entry.906535625=Wenqi%20Guan%20and%20Yang%20Fang&entry.1292438233=%20%20Large%20Language%20Models%20have%20brought%20a%20radical%20change%20in%20the%20process%20of%20remote%0Alearning%20students%2C%20among%20other%20aspects%20of%20educative%20activities.%20Current%0Aretrieval%20of%20remote%20learning%20resources%20lacks%20depth%20in%20contextual%20meaning%20that%0Aprovides%20comprehensive%20information%20on%20complex%20student%20queries.%20This%20work%0Aproposes%20a%20novel%20approach%20to%20enhancing%20remote%20learning%20retrieval%20by%20integrating%0AGPT-based%20models%20within%20the%20LangChain%20framework.%20We%20achieve%20this%20system%20in%20a%0Amore%20intuitive%20and%20productive%20manner%20using%20CoT%20reasoning%20and%20prompt%0Aengineering.%20The%20framework%20we%20propose%20puts%20much%20emphasis%20on%20increasing%20the%0Aprecision%20and%20relevance%20of%20the%20retrieval%20results%20to%20return%20comprehensive%20and%0Acontextually%20enriched%20explanations%20and%20resources%20that%20best%20suit%20each%20student%27s%0Aneeds.%20We%20also%20assess%20the%20effectiveness%20of%20our%20approach%20against%20paradigmatic%0ALLMs%20and%20report%20improvements%20in%20user%20satisfaction%20and%20learning%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15512v1&entry.124074799=Read"},
{"title": "Unsupervised Pelage Pattern Unwrapping for Animal Re-identification", "author": "Aleksandr Algasov and Ekaterina Nepovinnykh and Fedor Zolotarev and Tuomas Eerola and Heikki K\u00e4lvi\u00e4inen and Pavel Zem\u010d\u00edk and Charles V. Stewart", "abstract": "  Existing individual re-identification methods often struggle with the\ndeformable nature of animal fur or skin patterns which undergo geometric\ndistortions due to body movement and posture changes. In this paper, we propose\na geometry-aware texture mapping approach that unwarps pelage patterns, the\nunique markings found on an animal's skin or fur, into a canonical UV space,\nenabling more robust feature matching. Our method uses surface normal\nestimation to guide the unwrapping process while preserving the geometric\nconsistency between the 3D surface and the 2D texture space. We focus on two\nchallenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards\n(Panthera pardus). Both species have distinctive yet highly deformable fur\npatterns. By integrating our pattern-preserving UV mapping with existing\nre-identification techniques, we demonstrate improved accuracy across diverse\nposes and viewing angles. Our framework does not require ground truth UV\nannotations and can be trained in a self-supervised manner. Experiments on seal\nand leopard datasets show up to a 5.4% improvement in re-identification\naccuracy.\n", "link": "http://arxiv.org/abs/2506.15369v1", "date": "2025-06-18", "relevancy": 2.4648, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5067}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.491}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Pelage%20Pattern%20Unwrapping%20for%20Animal%20Re-identification&body=Title%3A%20Unsupervised%20Pelage%20Pattern%20Unwrapping%20for%20Animal%20Re-identification%0AAuthor%3A%20Aleksandr%20Algasov%20and%20Ekaterina%20Nepovinnykh%20and%20Fedor%20Zolotarev%20and%20Tuomas%20Eerola%20and%20Heikki%20K%C3%A4lvi%C3%A4inen%20and%20Pavel%20Zem%C4%8D%C3%ADk%20and%20Charles%20V.%20Stewart%0AAbstract%3A%20%20%20Existing%20individual%20re-identification%20methods%20often%20struggle%20with%20the%0Adeformable%20nature%20of%20animal%20fur%20or%20skin%20patterns%20which%20undergo%20geometric%0Adistortions%20due%20to%20body%20movement%20and%20posture%20changes.%20In%20this%20paper%2C%20we%20propose%0Aa%20geometry-aware%20texture%20mapping%20approach%20that%20unwarps%20pelage%20patterns%2C%20the%0Aunique%20markings%20found%20on%20an%20animal%27s%20skin%20or%20fur%2C%20into%20a%20canonical%20UV%20space%2C%0Aenabling%20more%20robust%20feature%20matching.%20Our%20method%20uses%20surface%20normal%0Aestimation%20to%20guide%20the%20unwrapping%20process%20while%20preserving%20the%20geometric%0Aconsistency%20between%20the%203D%20surface%20and%20the%202D%20texture%20space.%20We%20focus%20on%20two%0Achallenging%20species%3A%20Saimaa%20ringed%20seals%20%28Pusa%20hispida%20saimensis%29%20and%20leopards%0A%28Panthera%20pardus%29.%20Both%20species%20have%20distinctive%20yet%20highly%20deformable%20fur%0Apatterns.%20By%20integrating%20our%20pattern-preserving%20UV%20mapping%20with%20existing%0Are-identification%20techniques%2C%20we%20demonstrate%20improved%20accuracy%20across%20diverse%0Aposes%20and%20viewing%20angles.%20Our%20framework%20does%20not%20require%20ground%20truth%20UV%0Aannotations%20and%20can%20be%20trained%20in%20a%20self-supervised%20manner.%20Experiments%20on%20seal%0Aand%20leopard%20datasets%20show%20up%20to%20a%205.4%25%20improvement%20in%20re-identification%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Pelage%2520Pattern%2520Unwrapping%2520for%2520Animal%2520Re-identification%26entry.906535625%3DAleksandr%2520Algasov%2520and%2520Ekaterina%2520Nepovinnykh%2520and%2520Fedor%2520Zolotarev%2520and%2520Tuomas%2520Eerola%2520and%2520Heikki%2520K%25C3%25A4lvi%25C3%25A4inen%2520and%2520Pavel%2520Zem%25C4%258D%25C3%25ADk%2520and%2520Charles%2520V.%2520Stewart%26entry.1292438233%3D%2520%2520Existing%2520individual%2520re-identification%2520methods%2520often%2520struggle%2520with%2520the%250Adeformable%2520nature%2520of%2520animal%2520fur%2520or%2520skin%2520patterns%2520which%2520undergo%2520geometric%250Adistortions%2520due%2520to%2520body%2520movement%2520and%2520posture%2520changes.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520geometry-aware%2520texture%2520mapping%2520approach%2520that%2520unwarps%2520pelage%2520patterns%252C%2520the%250Aunique%2520markings%2520found%2520on%2520an%2520animal%2527s%2520skin%2520or%2520fur%252C%2520into%2520a%2520canonical%2520UV%2520space%252C%250Aenabling%2520more%2520robust%2520feature%2520matching.%2520Our%2520method%2520uses%2520surface%2520normal%250Aestimation%2520to%2520guide%2520the%2520unwrapping%2520process%2520while%2520preserving%2520the%2520geometric%250Aconsistency%2520between%2520the%25203D%2520surface%2520and%2520the%25202D%2520texture%2520space.%2520We%2520focus%2520on%2520two%250Achallenging%2520species%253A%2520Saimaa%2520ringed%2520seals%2520%2528Pusa%2520hispida%2520saimensis%2529%2520and%2520leopards%250A%2528Panthera%2520pardus%2529.%2520Both%2520species%2520have%2520distinctive%2520yet%2520highly%2520deformable%2520fur%250Apatterns.%2520By%2520integrating%2520our%2520pattern-preserving%2520UV%2520mapping%2520with%2520existing%250Are-identification%2520techniques%252C%2520we%2520demonstrate%2520improved%2520accuracy%2520across%2520diverse%250Aposes%2520and%2520viewing%2520angles.%2520Our%2520framework%2520does%2520not%2520require%2520ground%2520truth%2520UV%250Aannotations%2520and%2520can%2520be%2520trained%2520in%2520a%2520self-supervised%2520manner.%2520Experiments%2520on%2520seal%250Aand%2520leopard%2520datasets%2520show%2520up%2520to%2520a%25205.4%2525%2520improvement%2520in%2520re-identification%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Pelage%20Pattern%20Unwrapping%20for%20Animal%20Re-identification&entry.906535625=Aleksandr%20Algasov%20and%20Ekaterina%20Nepovinnykh%20and%20Fedor%20Zolotarev%20and%20Tuomas%20Eerola%20and%20Heikki%20K%C3%A4lvi%C3%A4inen%20and%20Pavel%20Zem%C4%8D%C3%ADk%20and%20Charles%20V.%20Stewart&entry.1292438233=%20%20Existing%20individual%20re-identification%20methods%20often%20struggle%20with%20the%0Adeformable%20nature%20of%20animal%20fur%20or%20skin%20patterns%20which%20undergo%20geometric%0Adistortions%20due%20to%20body%20movement%20and%20posture%20changes.%20In%20this%20paper%2C%20we%20propose%0Aa%20geometry-aware%20texture%20mapping%20approach%20that%20unwarps%20pelage%20patterns%2C%20the%0Aunique%20markings%20found%20on%20an%20animal%27s%20skin%20or%20fur%2C%20into%20a%20canonical%20UV%20space%2C%0Aenabling%20more%20robust%20feature%20matching.%20Our%20method%20uses%20surface%20normal%0Aestimation%20to%20guide%20the%20unwrapping%20process%20while%20preserving%20the%20geometric%0Aconsistency%20between%20the%203D%20surface%20and%20the%202D%20texture%20space.%20We%20focus%20on%20two%0Achallenging%20species%3A%20Saimaa%20ringed%20seals%20%28Pusa%20hispida%20saimensis%29%20and%20leopards%0A%28Panthera%20pardus%29.%20Both%20species%20have%20distinctive%20yet%20highly%20deformable%20fur%0Apatterns.%20By%20integrating%20our%20pattern-preserving%20UV%20mapping%20with%20existing%0Are-identification%20techniques%2C%20we%20demonstrate%20improved%20accuracy%20across%20diverse%0Aposes%20and%20viewing%20angles.%20Our%20framework%20does%20not%20require%20ground%20truth%20UV%0Aannotations%20and%20can%20be%20trained%20in%20a%20self-supervised%20manner.%20Experiments%20on%20seal%0Aand%20leopard%20datasets%20show%20up%20to%20a%205.4%25%20improvement%20in%20re-identification%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15369v1&entry.124074799=Read"},
{"title": "Graph Neural Networks for Jamming Source Localization", "author": "Dania Herzalla and Willian T. Lunardi and Martin Andreoni", "abstract": "  Graph-based learning provides a powerful framework for modeling complex\nrelational structures; however, its application within the domain of wireless\nsecurity remains significantly underexplored. In this work, we introduce the\nfirst application of graph-based learning for jamming source localization,\naddressing the imminent threat of jamming attacks in wireless networks. Unlike\ngeometric optimization techniques that struggle under environmental\nuncertainties and dense interference, we reformulate the localization as an\ninductive graph regression task. Our approach integrates structured node\nrepresentations that encode local and global signal aggregation, ensuring\nspatial coherence and adaptive signal fusion. To enhance robustness, we\nincorporate an attention-based \\ac{GNN} that adaptively refines neighborhood\ninfluence and introduces a confidence-guided estimation mechanism that\ndynamically balances learned predictions with domain-informed priors. We\nevaluate our approach under complex \\ac{RF} environments with various sampling\ndensities, network topologies, jammer characteristics, and signal propagation\nconditions, conducting comprehensive ablation studies on graph construction,\nfeature selection, and pooling strategies. Results demonstrate that our novel\ngraph-based learning framework significantly outperforms established\nlocalization baselines, particularly in challenging scenarios with sparse and\nobfuscated signal information. Our code is available at\nhttps://github.com/tiiuae/gnn-jamming-source-localization.\n", "link": "http://arxiv.org/abs/2506.03196v2", "date": "2025-06-18", "relevancy": 2.4485, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4891}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20for%20Jamming%20Source%20Localization&body=Title%3A%20Graph%20Neural%20Networks%20for%20Jamming%20Source%20Localization%0AAuthor%3A%20Dania%20Herzalla%20and%20Willian%20T.%20Lunardi%20and%20Martin%20Andreoni%0AAbstract%3A%20%20%20Graph-based%20learning%20provides%20a%20powerful%20framework%20for%20modeling%20complex%0Arelational%20structures%3B%20however%2C%20its%20application%20within%20the%20domain%20of%20wireless%0Asecurity%20remains%20significantly%20underexplored.%20In%20this%20work%2C%20we%20introduce%20the%0Afirst%20application%20of%20graph-based%20learning%20for%20jamming%20source%20localization%2C%0Aaddressing%20the%20imminent%20threat%20of%20jamming%20attacks%20in%20wireless%20networks.%20Unlike%0Ageometric%20optimization%20techniques%20that%20struggle%20under%20environmental%0Auncertainties%20and%20dense%20interference%2C%20we%20reformulate%20the%20localization%20as%20an%0Ainductive%20graph%20regression%20task.%20Our%20approach%20integrates%20structured%20node%0Arepresentations%20that%20encode%20local%20and%20global%20signal%20aggregation%2C%20ensuring%0Aspatial%20coherence%20and%20adaptive%20signal%20fusion.%20To%20enhance%20robustness%2C%20we%0Aincorporate%20an%20attention-based%20%5Cac%7BGNN%7D%20that%20adaptively%20refines%20neighborhood%0Ainfluence%20and%20introduces%20a%20confidence-guided%20estimation%20mechanism%20that%0Adynamically%20balances%20learned%20predictions%20with%20domain-informed%20priors.%20We%0Aevaluate%20our%20approach%20under%20complex%20%5Cac%7BRF%7D%20environments%20with%20various%20sampling%0Adensities%2C%20network%20topologies%2C%20jammer%20characteristics%2C%20and%20signal%20propagation%0Aconditions%2C%20conducting%20comprehensive%20ablation%20studies%20on%20graph%20construction%2C%0Afeature%20selection%2C%20and%20pooling%20strategies.%20Results%20demonstrate%20that%20our%20novel%0Agraph-based%20learning%20framework%20significantly%20outperforms%20established%0Alocalization%20baselines%2C%20particularly%20in%20challenging%20scenarios%20with%20sparse%20and%0Aobfuscated%20signal%20information.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tiiuae/gnn-jamming-source-localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03196v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520for%2520Jamming%2520Source%2520Localization%26entry.906535625%3DDania%2520Herzalla%2520and%2520Willian%2520T.%2520Lunardi%2520and%2520Martin%2520Andreoni%26entry.1292438233%3D%2520%2520Graph-based%2520learning%2520provides%2520a%2520powerful%2520framework%2520for%2520modeling%2520complex%250Arelational%2520structures%253B%2520however%252C%2520its%2520application%2520within%2520the%2520domain%2520of%2520wireless%250Asecurity%2520remains%2520significantly%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%250Afirst%2520application%2520of%2520graph-based%2520learning%2520for%2520jamming%2520source%2520localization%252C%250Aaddressing%2520the%2520imminent%2520threat%2520of%2520jamming%2520attacks%2520in%2520wireless%2520networks.%2520Unlike%250Ageometric%2520optimization%2520techniques%2520that%2520struggle%2520under%2520environmental%250Auncertainties%2520and%2520dense%2520interference%252C%2520we%2520reformulate%2520the%2520localization%2520as%2520an%250Ainductive%2520graph%2520regression%2520task.%2520Our%2520approach%2520integrates%2520structured%2520node%250Arepresentations%2520that%2520encode%2520local%2520and%2520global%2520signal%2520aggregation%252C%2520ensuring%250Aspatial%2520coherence%2520and%2520adaptive%2520signal%2520fusion.%2520To%2520enhance%2520robustness%252C%2520we%250Aincorporate%2520an%2520attention-based%2520%255Cac%257BGNN%257D%2520that%2520adaptively%2520refines%2520neighborhood%250Ainfluence%2520and%2520introduces%2520a%2520confidence-guided%2520estimation%2520mechanism%2520that%250Adynamically%2520balances%2520learned%2520predictions%2520with%2520domain-informed%2520priors.%2520We%250Aevaluate%2520our%2520approach%2520under%2520complex%2520%255Cac%257BRF%257D%2520environments%2520with%2520various%2520sampling%250Adensities%252C%2520network%2520topologies%252C%2520jammer%2520characteristics%252C%2520and%2520signal%2520propagation%250Aconditions%252C%2520conducting%2520comprehensive%2520ablation%2520studies%2520on%2520graph%2520construction%252C%250Afeature%2520selection%252C%2520and%2520pooling%2520strategies.%2520Results%2520demonstrate%2520that%2520our%2520novel%250Agraph-based%2520learning%2520framework%2520significantly%2520outperforms%2520established%250Alocalization%2520baselines%252C%2520particularly%2520in%2520challenging%2520scenarios%2520with%2520sparse%2520and%250Aobfuscated%2520signal%2520information.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/tiiuae/gnn-jamming-source-localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03196v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20for%20Jamming%20Source%20Localization&entry.906535625=Dania%20Herzalla%20and%20Willian%20T.%20Lunardi%20and%20Martin%20Andreoni&entry.1292438233=%20%20Graph-based%20learning%20provides%20a%20powerful%20framework%20for%20modeling%20complex%0Arelational%20structures%3B%20however%2C%20its%20application%20within%20the%20domain%20of%20wireless%0Asecurity%20remains%20significantly%20underexplored.%20In%20this%20work%2C%20we%20introduce%20the%0Afirst%20application%20of%20graph-based%20learning%20for%20jamming%20source%20localization%2C%0Aaddressing%20the%20imminent%20threat%20of%20jamming%20attacks%20in%20wireless%20networks.%20Unlike%0Ageometric%20optimization%20techniques%20that%20struggle%20under%20environmental%0Auncertainties%20and%20dense%20interference%2C%20we%20reformulate%20the%20localization%20as%20an%0Ainductive%20graph%20regression%20task.%20Our%20approach%20integrates%20structured%20node%0Arepresentations%20that%20encode%20local%20and%20global%20signal%20aggregation%2C%20ensuring%0Aspatial%20coherence%20and%20adaptive%20signal%20fusion.%20To%20enhance%20robustness%2C%20we%0Aincorporate%20an%20attention-based%20%5Cac%7BGNN%7D%20that%20adaptively%20refines%20neighborhood%0Ainfluence%20and%20introduces%20a%20confidence-guided%20estimation%20mechanism%20that%0Adynamically%20balances%20learned%20predictions%20with%20domain-informed%20priors.%20We%0Aevaluate%20our%20approach%20under%20complex%20%5Cac%7BRF%7D%20environments%20with%20various%20sampling%0Adensities%2C%20network%20topologies%2C%20jammer%20characteristics%2C%20and%20signal%20propagation%0Aconditions%2C%20conducting%20comprehensive%20ablation%20studies%20on%20graph%20construction%2C%0Afeature%20selection%2C%20and%20pooling%20strategies.%20Results%20demonstrate%20that%20our%20novel%0Agraph-based%20learning%20framework%20significantly%20outperforms%20established%0Alocalization%20baselines%2C%20particularly%20in%20challenging%20scenarios%20with%20sparse%20and%0Aobfuscated%20signal%20information.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tiiuae/gnn-jamming-source-localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03196v2&entry.124074799=Read"},
{"title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "author": "Shufan Li and Konstantinos Kallidromitis and Hritik Bansal and Akash Gokul and Yusuke Kato and Kazuki Kozuka and Jason Kuen and Zhe Lin and Kai-Wei Chang and Aditya Grover", "abstract": "  Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.\n", "link": "http://arxiv.org/abs/2505.16839v3", "date": "2025-06-18", "relevancy": 2.4365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.61}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.61}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaViDa%3A%20A%20Large%20Diffusion%20Language%20Model%20for%20Multimodal%20Understanding&body=Title%3A%20LaViDa%3A%20A%20Large%20Diffusion%20Language%20Model%20for%20Multimodal%20Understanding%0AAuthor%3A%20Shufan%20Li%20and%20Konstantinos%20Kallidromitis%20and%20Hritik%20Bansal%20and%20Akash%20Gokul%20and%20Yusuke%20Kato%20and%20Kazuki%20Kozuka%20and%20Jason%20Kuen%20and%20Zhe%20Lin%20and%20Kai-Wei%20Chang%20and%20Aditya%20Grover%0AAbstract%3A%20%20%20Modern%20Vision-Language%20Models%20%28VLMs%29%20can%20solve%20a%20wide%20range%20of%20tasks%0Arequiring%20visual%20reasoning.%20In%20real-world%20scenarios%2C%20desirable%20properties%20for%0AVLMs%20include%20fast%20inference%20and%20controllable%20generation%20%28e.g.%2C%20constraining%0Aoutputs%20to%20adhere%20to%20a%20desired%20format%29.%20However%2C%20existing%20autoregressive%20%28AR%29%0AVLMs%20like%20LLaVA%20struggle%20in%20these%20aspects.%20Discrete%20diffusion%20models%20%28DMs%29%0Aoffer%20a%20promising%20alternative%2C%20enabling%20parallel%20decoding%20for%20faster%20inference%0Aand%20bidirectional%20context%20for%20controllable%20generation%20through%20text-infilling.%0AWhile%20effective%20in%20language-only%20settings%2C%20DMs%27%20potential%20for%20multimodal%20tasks%0Ais%20underexplored.%20We%20introduce%20LaViDa%2C%20a%20family%20of%20VLMs%20built%20on%20DMs.%20We%20build%0ALaViDa%20by%20equipping%20DMs%20with%20a%20vision%20encoder%20and%20jointly%20fine-tune%20the%0Acombined%20parts%20for%20multimodal%20instruction%20following.%20To%20address%20challenges%0Aencountered%2C%20LaViDa%20incorporates%20novel%20techniques%20such%20as%20complementary%20masking%0Afor%20effective%20training%2C%20prefix%20KV%20cache%20for%20efficient%20inference%2C%20and%20timestep%0Ashifting%20for%20high-quality%20sampling.%20Experiments%20show%20that%20LaViDa%20achieves%0Acompetitive%20or%20superior%20performance%20to%20AR%20VLMs%20on%20multi-modal%20benchmarks%20such%0Aas%20MMMU%2C%20while%20offering%20unique%20advantages%20of%20DMs%2C%20including%20flexible%0Aspeed-quality%20tradeoff%2C%20controllability%2C%20and%20bidirectional%20reasoning.%20On%20COCO%0Acaptioning%2C%20LaViDa%20surpasses%20Open-LLaVa-Next-8B%20by%20%2B4.1%20CIDEr%20with%201.92x%0Aspeedup.%20On%20bidirectional%20tasks%2C%20it%20achieves%20%2B59%25%20improvement%20on%20Constrained%0APoem%20Completion.%20These%20results%20demonstrate%20LaViDa%20as%20a%20strong%20alternative%20to%20AR%0AVLMs.%20Code%20and%20models%20will%20be%20released%20in%20the%20camera-ready%20version.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16839v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaViDa%253A%2520A%2520Large%2520Diffusion%2520Language%2520Model%2520for%2520Multimodal%2520Understanding%26entry.906535625%3DShufan%2520Li%2520and%2520Konstantinos%2520Kallidromitis%2520and%2520Hritik%2520Bansal%2520and%2520Akash%2520Gokul%2520and%2520Yusuke%2520Kato%2520and%2520Kazuki%2520Kozuka%2520and%2520Jason%2520Kuen%2520and%2520Zhe%2520Lin%2520and%2520Kai-Wei%2520Chang%2520and%2520Aditya%2520Grover%26entry.1292438233%3D%2520%2520Modern%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520can%2520solve%2520a%2520wide%2520range%2520of%2520tasks%250Arequiring%2520visual%2520reasoning.%2520In%2520real-world%2520scenarios%252C%2520desirable%2520properties%2520for%250AVLMs%2520include%2520fast%2520inference%2520and%2520controllable%2520generation%2520%2528e.g.%252C%2520constraining%250Aoutputs%2520to%2520adhere%2520to%2520a%2520desired%2520format%2529.%2520However%252C%2520existing%2520autoregressive%2520%2528AR%2529%250AVLMs%2520like%2520LLaVA%2520struggle%2520in%2520these%2520aspects.%2520Discrete%2520diffusion%2520models%2520%2528DMs%2529%250Aoffer%2520a%2520promising%2520alternative%252C%2520enabling%2520parallel%2520decoding%2520for%2520faster%2520inference%250Aand%2520bidirectional%2520context%2520for%2520controllable%2520generation%2520through%2520text-infilling.%250AWhile%2520effective%2520in%2520language-only%2520settings%252C%2520DMs%2527%2520potential%2520for%2520multimodal%2520tasks%250Ais%2520underexplored.%2520We%2520introduce%2520LaViDa%252C%2520a%2520family%2520of%2520VLMs%2520built%2520on%2520DMs.%2520We%2520build%250ALaViDa%2520by%2520equipping%2520DMs%2520with%2520a%2520vision%2520encoder%2520and%2520jointly%2520fine-tune%2520the%250Acombined%2520parts%2520for%2520multimodal%2520instruction%2520following.%2520To%2520address%2520challenges%250Aencountered%252C%2520LaViDa%2520incorporates%2520novel%2520techniques%2520such%2520as%2520complementary%2520masking%250Afor%2520effective%2520training%252C%2520prefix%2520KV%2520cache%2520for%2520efficient%2520inference%252C%2520and%2520timestep%250Ashifting%2520for%2520high-quality%2520sampling.%2520Experiments%2520show%2520that%2520LaViDa%2520achieves%250Acompetitive%2520or%2520superior%2520performance%2520to%2520AR%2520VLMs%2520on%2520multi-modal%2520benchmarks%2520such%250Aas%2520MMMU%252C%2520while%2520offering%2520unique%2520advantages%2520of%2520DMs%252C%2520including%2520flexible%250Aspeed-quality%2520tradeoff%252C%2520controllability%252C%2520and%2520bidirectional%2520reasoning.%2520On%2520COCO%250Acaptioning%252C%2520LaViDa%2520surpasses%2520Open-LLaVa-Next-8B%2520by%2520%252B4.1%2520CIDEr%2520with%25201.92x%250Aspeedup.%2520On%2520bidirectional%2520tasks%252C%2520it%2520achieves%2520%252B59%2525%2520improvement%2520on%2520Constrained%250APoem%2520Completion.%2520These%2520results%2520demonstrate%2520LaViDa%2520as%2520a%2520strong%2520alternative%2520to%2520AR%250AVLMs.%2520Code%2520and%2520models%2520will%2520be%2520released%2520in%2520the%2520camera-ready%2520version.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16839v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaViDa%3A%20A%20Large%20Diffusion%20Language%20Model%20for%20Multimodal%20Understanding&entry.906535625=Shufan%20Li%20and%20Konstantinos%20Kallidromitis%20and%20Hritik%20Bansal%20and%20Akash%20Gokul%20and%20Yusuke%20Kato%20and%20Kazuki%20Kozuka%20and%20Jason%20Kuen%20and%20Zhe%20Lin%20and%20Kai-Wei%20Chang%20and%20Aditya%20Grover&entry.1292438233=%20%20Modern%20Vision-Language%20Models%20%28VLMs%29%20can%20solve%20a%20wide%20range%20of%20tasks%0Arequiring%20visual%20reasoning.%20In%20real-world%20scenarios%2C%20desirable%20properties%20for%0AVLMs%20include%20fast%20inference%20and%20controllable%20generation%20%28e.g.%2C%20constraining%0Aoutputs%20to%20adhere%20to%20a%20desired%20format%29.%20However%2C%20existing%20autoregressive%20%28AR%29%0AVLMs%20like%20LLaVA%20struggle%20in%20these%20aspects.%20Discrete%20diffusion%20models%20%28DMs%29%0Aoffer%20a%20promising%20alternative%2C%20enabling%20parallel%20decoding%20for%20faster%20inference%0Aand%20bidirectional%20context%20for%20controllable%20generation%20through%20text-infilling.%0AWhile%20effective%20in%20language-only%20settings%2C%20DMs%27%20potential%20for%20multimodal%20tasks%0Ais%20underexplored.%20We%20introduce%20LaViDa%2C%20a%20family%20of%20VLMs%20built%20on%20DMs.%20We%20build%0ALaViDa%20by%20equipping%20DMs%20with%20a%20vision%20encoder%20and%20jointly%20fine-tune%20the%0Acombined%20parts%20for%20multimodal%20instruction%20following.%20To%20address%20challenges%0Aencountered%2C%20LaViDa%20incorporates%20novel%20techniques%20such%20as%20complementary%20masking%0Afor%20effective%20training%2C%20prefix%20KV%20cache%20for%20efficient%20inference%2C%20and%20timestep%0Ashifting%20for%20high-quality%20sampling.%20Experiments%20show%20that%20LaViDa%20achieves%0Acompetitive%20or%20superior%20performance%20to%20AR%20VLMs%20on%20multi-modal%20benchmarks%20such%0Aas%20MMMU%2C%20while%20offering%20unique%20advantages%20of%20DMs%2C%20including%20flexible%0Aspeed-quality%20tradeoff%2C%20controllability%2C%20and%20bidirectional%20reasoning.%20On%20COCO%0Acaptioning%2C%20LaViDa%20surpasses%20Open-LLaVa-Next-8B%20by%20%2B4.1%20CIDEr%20with%201.92x%0Aspeedup.%20On%20bidirectional%20tasks%2C%20it%20achieves%20%2B59%25%20improvement%20on%20Constrained%0APoem%20Completion.%20These%20results%20demonstrate%20LaViDa%20as%20a%20strong%20alternative%20to%20AR%0AVLMs.%20Code%20and%20models%20will%20be%20released%20in%20the%20camera-ready%20version.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16839v3&entry.124074799=Read"},
{"title": "Dense SAE Latents Are Features, Not Bugs", "author": "Xiaoqing Sun and Alessandro Stolfo and Joshua Engels and Ben Wu and Senthooran Rajamanoharan and Mrinmaya Sachan and Max Tegmark", "abstract": "  Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.\n", "link": "http://arxiv.org/abs/2506.15679v1", "date": "2025-06-18", "relevancy": 2.426, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20SAE%20Latents%20Are%20Features%2C%20Not%20Bugs&body=Title%3A%20Dense%20SAE%20Latents%20Are%20Features%2C%20Not%20Bugs%0AAuthor%3A%20Xiaoqing%20Sun%20and%20Alessandro%20Stolfo%20and%20Joshua%20Engels%20and%20Ben%20Wu%20and%20Senthooran%20Rajamanoharan%20and%20Mrinmaya%20Sachan%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20designed%20to%20extract%20interpretable%20features%0Afrom%20language%20models%20by%20enforcing%20a%20sparsity%20constraint.%20Ideally%2C%20training%20an%0ASAE%20would%20yield%20latents%20that%20are%20both%20sparse%20and%20semantically%20meaningful.%0AHowever%2C%20many%20SAE%20latents%20activate%20frequently%20%28i.e.%2C%20are%20%5Cemph%7Bdense%7D%29%2C%20raising%0Aconcerns%20that%20they%20may%20be%20undesirable%20artifacts%20of%20the%20training%20procedure.%20In%0Athis%20work%2C%20we%20systematically%20investigate%20the%20geometry%2C%20function%2C%20and%20origin%20of%0Adense%20latents%20and%20show%20that%20they%20are%20not%20only%20persistent%20but%20often%20reflect%0Ameaningful%20model%20representations.%20We%20first%20demonstrate%20that%20dense%20latents%20tend%0Ato%20form%20antipodal%20pairs%20that%20reconstruct%20specific%20directions%20in%20the%20residual%0Astream%2C%20and%20that%20ablating%20their%20subspace%20suppresses%20the%20emergence%20of%20new%20dense%0Afeatures%20in%20retrained%20SAEs%20--%20suggesting%20that%20high%20density%20features%20are%20an%0Aintrinsic%20property%20of%20the%20residual%20space.%20We%20then%20introduce%20a%20taxonomy%20of%20dense%0Alatents%2C%20identifying%20classes%20tied%20to%20position%20tracking%2C%20context%20binding%2C%0Aentropy%20regulation%2C%20letter-specific%20output%20signals%2C%20part-of-speech%2C%20and%0Aprincipal%20component%20reconstruction.%20Finally%2C%20we%20analyze%20how%20these%20features%0Aevolve%20across%20layers%2C%20revealing%20a%20shift%20from%20structural%20features%20in%20early%0Alayers%2C%20to%20semantic%20features%20in%20mid%20layers%2C%20and%20finally%20to%20output-oriented%0Asignals%20in%20the%20last%20layers%20of%20the%20model.%20Our%20findings%20indicate%20that%20dense%0Alatents%20serve%20functional%20roles%20in%20language%20model%20computation%20and%20should%20not%20be%0Adismissed%20as%20training%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520SAE%2520Latents%2520Are%2520Features%252C%2520Not%2520Bugs%26entry.906535625%3DXiaoqing%2520Sun%2520and%2520Alessandro%2520Stolfo%2520and%2520Joshua%2520Engels%2520and%2520Ben%2520Wu%2520and%2520Senthooran%2520Rajamanoharan%2520and%2520Mrinmaya%2520Sachan%2520and%2520Max%2520Tegmark%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520designed%2520to%2520extract%2520interpretable%2520features%250Afrom%2520language%2520models%2520by%2520enforcing%2520a%2520sparsity%2520constraint.%2520Ideally%252C%2520training%2520an%250ASAE%2520would%2520yield%2520latents%2520that%2520are%2520both%2520sparse%2520and%2520semantically%2520meaningful.%250AHowever%252C%2520many%2520SAE%2520latents%2520activate%2520frequently%2520%2528i.e.%252C%2520are%2520%255Cemph%257Bdense%257D%2529%252C%2520raising%250Aconcerns%2520that%2520they%2520may%2520be%2520undesirable%2520artifacts%2520of%2520the%2520training%2520procedure.%2520In%250Athis%2520work%252C%2520we%2520systematically%2520investigate%2520the%2520geometry%252C%2520function%252C%2520and%2520origin%2520of%250Adense%2520latents%2520and%2520show%2520that%2520they%2520are%2520not%2520only%2520persistent%2520but%2520often%2520reflect%250Ameaningful%2520model%2520representations.%2520We%2520first%2520demonstrate%2520that%2520dense%2520latents%2520tend%250Ato%2520form%2520antipodal%2520pairs%2520that%2520reconstruct%2520specific%2520directions%2520in%2520the%2520residual%250Astream%252C%2520and%2520that%2520ablating%2520their%2520subspace%2520suppresses%2520the%2520emergence%2520of%2520new%2520dense%250Afeatures%2520in%2520retrained%2520SAEs%2520--%2520suggesting%2520that%2520high%2520density%2520features%2520are%2520an%250Aintrinsic%2520property%2520of%2520the%2520residual%2520space.%2520We%2520then%2520introduce%2520a%2520taxonomy%2520of%2520dense%250Alatents%252C%2520identifying%2520classes%2520tied%2520to%2520position%2520tracking%252C%2520context%2520binding%252C%250Aentropy%2520regulation%252C%2520letter-specific%2520output%2520signals%252C%2520part-of-speech%252C%2520and%250Aprincipal%2520component%2520reconstruction.%2520Finally%252C%2520we%2520analyze%2520how%2520these%2520features%250Aevolve%2520across%2520layers%252C%2520revealing%2520a%2520shift%2520from%2520structural%2520features%2520in%2520early%250Alayers%252C%2520to%2520semantic%2520features%2520in%2520mid%2520layers%252C%2520and%2520finally%2520to%2520output-oriented%250Asignals%2520in%2520the%2520last%2520layers%2520of%2520the%2520model.%2520Our%2520findings%2520indicate%2520that%2520dense%250Alatents%2520serve%2520functional%2520roles%2520in%2520language%2520model%2520computation%2520and%2520should%2520not%2520be%250Adismissed%2520as%2520training%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20SAE%20Latents%20Are%20Features%2C%20Not%20Bugs&entry.906535625=Xiaoqing%20Sun%20and%20Alessandro%20Stolfo%20and%20Joshua%20Engels%20and%20Ben%20Wu%20and%20Senthooran%20Rajamanoharan%20and%20Mrinmaya%20Sachan%20and%20Max%20Tegmark&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20designed%20to%20extract%20interpretable%20features%0Afrom%20language%20models%20by%20enforcing%20a%20sparsity%20constraint.%20Ideally%2C%20training%20an%0ASAE%20would%20yield%20latents%20that%20are%20both%20sparse%20and%20semantically%20meaningful.%0AHowever%2C%20many%20SAE%20latents%20activate%20frequently%20%28i.e.%2C%20are%20%5Cemph%7Bdense%7D%29%2C%20raising%0Aconcerns%20that%20they%20may%20be%20undesirable%20artifacts%20of%20the%20training%20procedure.%20In%0Athis%20work%2C%20we%20systematically%20investigate%20the%20geometry%2C%20function%2C%20and%20origin%20of%0Adense%20latents%20and%20show%20that%20they%20are%20not%20only%20persistent%20but%20often%20reflect%0Ameaningful%20model%20representations.%20We%20first%20demonstrate%20that%20dense%20latents%20tend%0Ato%20form%20antipodal%20pairs%20that%20reconstruct%20specific%20directions%20in%20the%20residual%0Astream%2C%20and%20that%20ablating%20their%20subspace%20suppresses%20the%20emergence%20of%20new%20dense%0Afeatures%20in%20retrained%20SAEs%20--%20suggesting%20that%20high%20density%20features%20are%20an%0Aintrinsic%20property%20of%20the%20residual%20space.%20We%20then%20introduce%20a%20taxonomy%20of%20dense%0Alatents%2C%20identifying%20classes%20tied%20to%20position%20tracking%2C%20context%20binding%2C%0Aentropy%20regulation%2C%20letter-specific%20output%20signals%2C%20part-of-speech%2C%20and%0Aprincipal%20component%20reconstruction.%20Finally%2C%20we%20analyze%20how%20these%20features%0Aevolve%20across%20layers%2C%20revealing%20a%20shift%20from%20structural%20features%20in%20early%0Alayers%2C%20to%20semantic%20features%20in%20mid%20layers%2C%20and%20finally%20to%20output-oriented%0Asignals%20in%20the%20last%20layers%20of%20the%20model.%20Our%20findings%20indicate%20that%20dense%0Alatents%20serve%20functional%20roles%20in%20language%20model%20computation%20and%20should%20not%20be%0Adismissed%20as%20training%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15679v1&entry.124074799=Read"},
{"title": "Sampling 3D Molecular Conformers with Diffusion Transformers", "author": "J. Thorben Frank and Winfried Ripken and Gregor Lied and Klaus-Robert M\u00fcller and Oliver T. Unke and Stefan Chmiela", "abstract": "  Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.\n", "link": "http://arxiv.org/abs/2506.15378v1", "date": "2025-06-18", "relevancy": 2.4231, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6088}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6088}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%203D%20Molecular%20Conformers%20with%20Diffusion%20Transformers&body=Title%3A%20Sampling%203D%20Molecular%20Conformers%20with%20Diffusion%20Transformers%0AAuthor%3A%20J.%20Thorben%20Frank%20and%20Winfried%20Ripken%20and%20Gregor%20Lied%20and%20Klaus-Robert%20M%C3%BCller%20and%20Oliver%20T.%20Unke%20and%20Stefan%20Chmiela%0AAbstract%3A%20%20%20Diffusion%20Transformers%20%28DiTs%29%20have%20demonstrated%20strong%20performance%20in%0Agenerative%20modeling%2C%20particularly%20in%20image%20synthesis%2C%20making%20them%20a%20compelling%0Achoice%20for%20molecular%20conformer%20generation.%20However%2C%20applying%20DiTs%20to%20molecules%0Aintroduces%20novel%20challenges%2C%20such%20as%20integrating%20discrete%20molecular%20graph%0Ainformation%20with%20continuous%203D%20geometry%2C%20handling%20Euclidean%20symmetries%2C%20and%0Adesigning%20conditioning%20mechanisms%20that%20generalize%20across%20molecules%20of%20varying%0Asizes%20and%20structures.%20We%20propose%20DiTMC%2C%20a%20framework%20that%20adapts%20DiTs%20to%20address%0Athese%20challenges%20through%20a%20modular%20architecture%20that%20separates%20the%20processing%0Aof%203D%20coordinates%20from%20conditioning%20on%20atomic%20connectivity.%20To%20this%20end%2C%20we%0Aintroduce%20two%20complementary%20graph-based%20conditioning%20strategies%20that%20integrate%0Aseamlessly%20with%20the%20DiT%20architecture.%20These%20are%20combined%20with%20different%0Aattention%20mechanisms%2C%20including%20both%20standard%20non-equivariant%20and%0ASO%283%29-equivariant%20formulations%2C%20enabling%20flexible%20control%20over%20the%20trade-off%0Abetween%20between%20accuracy%20and%20computational%20efficiency.%20Experiments%20on%20standard%0Aconformer%20generation%20benchmarks%20%28GEOM-QM9%2C%20-DRUGS%2C%20-XL%29%20demonstrate%20that%20DiTMC%0Aachieves%20state-of-the-art%20precision%20and%20physical%20validity.%20Our%20results%0Ahighlight%20how%20architectural%20choices%20and%20symmetry%20priors%20affect%20sample%20quality%0Aand%20efficiency%2C%20suggesting%20promising%20directions%20for%20large-scale%20generative%0Amodeling%20of%20molecular%20structures.%20Code%20available%20at%0Ahttps%3A//github.com/ML4MolSim/dit_mc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%25203D%2520Molecular%2520Conformers%2520with%2520Diffusion%2520Transformers%26entry.906535625%3DJ.%2520Thorben%2520Frank%2520and%2520Winfried%2520Ripken%2520and%2520Gregor%2520Lied%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Oliver%2520T.%2520Unke%2520and%2520Stefan%2520Chmiela%26entry.1292438233%3D%2520%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520have%2520demonstrated%2520strong%2520performance%2520in%250Agenerative%2520modeling%252C%2520particularly%2520in%2520image%2520synthesis%252C%2520making%2520them%2520a%2520compelling%250Achoice%2520for%2520molecular%2520conformer%2520generation.%2520However%252C%2520applying%2520DiTs%2520to%2520molecules%250Aintroduces%2520novel%2520challenges%252C%2520such%2520as%2520integrating%2520discrete%2520molecular%2520graph%250Ainformation%2520with%2520continuous%25203D%2520geometry%252C%2520handling%2520Euclidean%2520symmetries%252C%2520and%250Adesigning%2520conditioning%2520mechanisms%2520that%2520generalize%2520across%2520molecules%2520of%2520varying%250Asizes%2520and%2520structures.%2520We%2520propose%2520DiTMC%252C%2520a%2520framework%2520that%2520adapts%2520DiTs%2520to%2520address%250Athese%2520challenges%2520through%2520a%2520modular%2520architecture%2520that%2520separates%2520the%2520processing%250Aof%25203D%2520coordinates%2520from%2520conditioning%2520on%2520atomic%2520connectivity.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520two%2520complementary%2520graph-based%2520conditioning%2520strategies%2520that%2520integrate%250Aseamlessly%2520with%2520the%2520DiT%2520architecture.%2520These%2520are%2520combined%2520with%2520different%250Aattention%2520mechanisms%252C%2520including%2520both%2520standard%2520non-equivariant%2520and%250ASO%25283%2529-equivariant%2520formulations%252C%2520enabling%2520flexible%2520control%2520over%2520the%2520trade-off%250Abetween%2520between%2520accuracy%2520and%2520computational%2520efficiency.%2520Experiments%2520on%2520standard%250Aconformer%2520generation%2520benchmarks%2520%2528GEOM-QM9%252C%2520-DRUGS%252C%2520-XL%2529%2520demonstrate%2520that%2520DiTMC%250Aachieves%2520state-of-the-art%2520precision%2520and%2520physical%2520validity.%2520Our%2520results%250Ahighlight%2520how%2520architectural%2520choices%2520and%2520symmetry%2520priors%2520affect%2520sample%2520quality%250Aand%2520efficiency%252C%2520suggesting%2520promising%2520directions%2520for%2520large-scale%2520generative%250Amodeling%2520of%2520molecular%2520structures.%2520Code%2520available%2520at%250Ahttps%253A//github.com/ML4MolSim/dit_mc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%203D%20Molecular%20Conformers%20with%20Diffusion%20Transformers&entry.906535625=J.%20Thorben%20Frank%20and%20Winfried%20Ripken%20and%20Gregor%20Lied%20and%20Klaus-Robert%20M%C3%BCller%20and%20Oliver%20T.%20Unke%20and%20Stefan%20Chmiela&entry.1292438233=%20%20Diffusion%20Transformers%20%28DiTs%29%20have%20demonstrated%20strong%20performance%20in%0Agenerative%20modeling%2C%20particularly%20in%20image%20synthesis%2C%20making%20them%20a%20compelling%0Achoice%20for%20molecular%20conformer%20generation.%20However%2C%20applying%20DiTs%20to%20molecules%0Aintroduces%20novel%20challenges%2C%20such%20as%20integrating%20discrete%20molecular%20graph%0Ainformation%20with%20continuous%203D%20geometry%2C%20handling%20Euclidean%20symmetries%2C%20and%0Adesigning%20conditioning%20mechanisms%20that%20generalize%20across%20molecules%20of%20varying%0Asizes%20and%20structures.%20We%20propose%20DiTMC%2C%20a%20framework%20that%20adapts%20DiTs%20to%20address%0Athese%20challenges%20through%20a%20modular%20architecture%20that%20separates%20the%20processing%0Aof%203D%20coordinates%20from%20conditioning%20on%20atomic%20connectivity.%20To%20this%20end%2C%20we%0Aintroduce%20two%20complementary%20graph-based%20conditioning%20strategies%20that%20integrate%0Aseamlessly%20with%20the%20DiT%20architecture.%20These%20are%20combined%20with%20different%0Aattention%20mechanisms%2C%20including%20both%20standard%20non-equivariant%20and%0ASO%283%29-equivariant%20formulations%2C%20enabling%20flexible%20control%20over%20the%20trade-off%0Abetween%20between%20accuracy%20and%20computational%20efficiency.%20Experiments%20on%20standard%0Aconformer%20generation%20benchmarks%20%28GEOM-QM9%2C%20-DRUGS%2C%20-XL%29%20demonstrate%20that%20DiTMC%0Aachieves%20state-of-the-art%20precision%20and%20physical%20validity.%20Our%20results%0Ahighlight%20how%20architectural%20choices%20and%20symmetry%20priors%20affect%20sample%20quality%0Aand%20efficiency%2C%20suggesting%20promising%20directions%20for%20large-scale%20generative%0Amodeling%20of%20molecular%20structures.%20Code%20available%20at%0Ahttps%3A//github.com/ML4MolSim/dit_mc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15378v1&entry.124074799=Read"},
{"title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models", "author": "Shuo Xing and Lanqing Guo and Hongyuan Hua and Seoyoung Lee and Peiran Li and Yufei Wang and Zhangyang Wang and Zhengzhong Tu", "abstract": "  Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.\n", "link": "http://arxiv.org/abs/2506.15645v1", "date": "2025-06-18", "relevancy": 2.4133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6054}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20the%20Visual%20Quality%20Paradox%20in%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Demystifying%20the%20Visual%20Quality%20Paradox%20in%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Shuo%20Xing%20and%20Lanqing%20Guo%20and%20Hongyuan%20Hua%20and%20Seoyoung%20Lee%20and%20Peiran%20Li%20and%20Yufei%20Wang%20and%20Zhangyang%20Wang%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20on%20benchmark%0Avision-language%20tasks%2C%20yet%20little%20is%20known%20about%20how%20input%20visual%20quality%0Ashapes%20their%20responses.%20Does%20higher%20perceptual%20quality%20of%20images%20already%0Atranslate%20to%20better%20MLLM%20understanding%3F%20We%20conduct%20the%20first%20systematic%20study%0Aspanning%20leading%20MLLMs%20and%20a%20suite%20of%20vision-language%20benchmarks%2C%20applying%0Acontrolled%20degradations%20and%20stylistic%20shifts%20to%20each%20image.%20Surprisingly%2C%20we%0Auncover%20a%20visual-quality%20paradox%3A%20model%2C%20task%2C%20and%20even%20individual-instance%0Aperformance%20can%20improve%20when%20images%20deviate%20from%20human-perceived%20fidelity.%0AOff-the-shelf%20restoration%20pipelines%20fail%20to%20reconcile%20these%20idiosyncratic%0Apreferences.%20To%20close%20the%20gap%2C%20we%20introduce%20Visual-Quality%20Test-Time%20Tuning%0A%28VQ-TTT%29-a%20lightweight%20adaptation%20module%20that%3A%20%281%29%20inserts%20a%20learnable%2C%0Alow-rank%20kernel%20before%20the%20frozen%20vision%20encoder%20to%20modulate%20frequency%20content%3B%0Aand%20%282%29%20fine-tunes%20only%20shallow%20vision-encoder%20layers%20via%20LoRA.%20VQ-TTT%0Adynamically%20adjusts%20each%20input%20image%20in%20a%20single%20forward%20pass%2C%20aligning%20it%20with%0Atask-specific%20model%20preferences.%20Across%20the%20evaluated%20MLLMs%20and%20all%20datasets%2C%0AVQ-TTT%20lifts%20significant%20average%20accuracy%2C%20with%20no%20external%20models%2C%20cached%0Afeatures%2C%20or%20extra%20training%20data.%20These%20findings%20redefine%20%60%60better%27%27%20visual%0Ainputs%20for%20MLLMs%20and%20highlight%20the%20need%20for%20adaptive%2C%20rather%20than%20universally%0A%60%60clean%27%27%2C%20imagery%2C%20in%20the%20new%20era%20of%20AI%20being%20the%20main%20data%20customer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520the%2520Visual%2520Quality%2520Paradox%2520in%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DShuo%2520Xing%2520and%2520Lanqing%2520Guo%2520and%2520Hongyuan%2520Hua%2520and%2520Seoyoung%2520Lee%2520and%2520Peiran%2520Li%2520and%2520Yufei%2520Wang%2520and%2520Zhangyang%2520Wang%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520Recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520on%2520benchmark%250Avision-language%2520tasks%252C%2520yet%2520little%2520is%2520known%2520about%2520how%2520input%2520visual%2520quality%250Ashapes%2520their%2520responses.%2520Does%2520higher%2520perceptual%2520quality%2520of%2520images%2520already%250Atranslate%2520to%2520better%2520MLLM%2520understanding%253F%2520We%2520conduct%2520the%2520first%2520systematic%2520study%250Aspanning%2520leading%2520MLLMs%2520and%2520a%2520suite%2520of%2520vision-language%2520benchmarks%252C%2520applying%250Acontrolled%2520degradations%2520and%2520stylistic%2520shifts%2520to%2520each%2520image.%2520Surprisingly%252C%2520we%250Auncover%2520a%2520visual-quality%2520paradox%253A%2520model%252C%2520task%252C%2520and%2520even%2520individual-instance%250Aperformance%2520can%2520improve%2520when%2520images%2520deviate%2520from%2520human-perceived%2520fidelity.%250AOff-the-shelf%2520restoration%2520pipelines%2520fail%2520to%2520reconcile%2520these%2520idiosyncratic%250Apreferences.%2520To%2520close%2520the%2520gap%252C%2520we%2520introduce%2520Visual-Quality%2520Test-Time%2520Tuning%250A%2528VQ-TTT%2529-a%2520lightweight%2520adaptation%2520module%2520that%253A%2520%25281%2529%2520inserts%2520a%2520learnable%252C%250Alow-rank%2520kernel%2520before%2520the%2520frozen%2520vision%2520encoder%2520to%2520modulate%2520frequency%2520content%253B%250Aand%2520%25282%2529%2520fine-tunes%2520only%2520shallow%2520vision-encoder%2520layers%2520via%2520LoRA.%2520VQ-TTT%250Adynamically%2520adjusts%2520each%2520input%2520image%2520in%2520a%2520single%2520forward%2520pass%252C%2520aligning%2520it%2520with%250Atask-specific%2520model%2520preferences.%2520Across%2520the%2520evaluated%2520MLLMs%2520and%2520all%2520datasets%252C%250AVQ-TTT%2520lifts%2520significant%2520average%2520accuracy%252C%2520with%2520no%2520external%2520models%252C%2520cached%250Afeatures%252C%2520or%2520extra%2520training%2520data.%2520These%2520findings%2520redefine%2520%2560%2560better%2527%2527%2520visual%250Ainputs%2520for%2520MLLMs%2520and%2520highlight%2520the%2520need%2520for%2520adaptive%252C%2520rather%2520than%2520universally%250A%2560%2560clean%2527%2527%252C%2520imagery%252C%2520in%2520the%2520new%2520era%2520of%2520AI%2520being%2520the%2520main%2520data%2520customer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20the%20Visual%20Quality%20Paradox%20in%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Shuo%20Xing%20and%20Lanqing%20Guo%20and%20Hongyuan%20Hua%20and%20Seoyoung%20Lee%20and%20Peiran%20Li%20and%20Yufei%20Wang%20and%20Zhangyang%20Wang%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20Recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20on%20benchmark%0Avision-language%20tasks%2C%20yet%20little%20is%20known%20about%20how%20input%20visual%20quality%0Ashapes%20their%20responses.%20Does%20higher%20perceptual%20quality%20of%20images%20already%0Atranslate%20to%20better%20MLLM%20understanding%3F%20We%20conduct%20the%20first%20systematic%20study%0Aspanning%20leading%20MLLMs%20and%20a%20suite%20of%20vision-language%20benchmarks%2C%20applying%0Acontrolled%20degradations%20and%20stylistic%20shifts%20to%20each%20image.%20Surprisingly%2C%20we%0Auncover%20a%20visual-quality%20paradox%3A%20model%2C%20task%2C%20and%20even%20individual-instance%0Aperformance%20can%20improve%20when%20images%20deviate%20from%20human-perceived%20fidelity.%0AOff-the-shelf%20restoration%20pipelines%20fail%20to%20reconcile%20these%20idiosyncratic%0Apreferences.%20To%20close%20the%20gap%2C%20we%20introduce%20Visual-Quality%20Test-Time%20Tuning%0A%28VQ-TTT%29-a%20lightweight%20adaptation%20module%20that%3A%20%281%29%20inserts%20a%20learnable%2C%0Alow-rank%20kernel%20before%20the%20frozen%20vision%20encoder%20to%20modulate%20frequency%20content%3B%0Aand%20%282%29%20fine-tunes%20only%20shallow%20vision-encoder%20layers%20via%20LoRA.%20VQ-TTT%0Adynamically%20adjusts%20each%20input%20image%20in%20a%20single%20forward%20pass%2C%20aligning%20it%20with%0Atask-specific%20model%20preferences.%20Across%20the%20evaluated%20MLLMs%20and%20all%20datasets%2C%0AVQ-TTT%20lifts%20significant%20average%20accuracy%2C%20with%20no%20external%20models%2C%20cached%0Afeatures%2C%20or%20extra%20training%20data.%20These%20findings%20redefine%20%60%60better%27%27%20visual%0Ainputs%20for%20MLLMs%20and%20highlight%20the%20need%20for%20adaptive%2C%20rather%20than%20universally%0A%60%60clean%27%27%2C%20imagery%2C%20in%20the%20new%20era%20of%20AI%20being%20the%20main%20data%20customer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15645v1&entry.124074799=Read"},
{"title": "Control and Realism: Best of Both Worlds in Layout-to-Image without\n  Training", "author": "Bonan Li and Yinhan Hu and Songhua Liu and Xinchao Wang", "abstract": "  Layout-to-Image generation aims to create complex scenes with precise control\nover the placement and arrangement of subjects. Existing works have\ndemonstrated that pre-trained Text-to-Image diffusion models can achieve this\ngoal without training on any specific data; however, they often face challenges\nwith imprecise localization and unrealistic artifacts. Focusing on these\ndrawbacks, we propose a novel training-free method, WinWinLay. At its core,\nWinWinLay presents two key strategies, Non-local Attention Energy Function and\nAdaptive Update, that collaboratively enhance control precision and realism. On\none hand, we theoretically demonstrate that the commonly used attention energy\nfunction introduces inherent spatial distribution biases, hindering objects\nfrom being uniformly aligned with layout instructions. To overcome this issue,\nnon-local attention prior is explored to redistribute attention scores,\nfacilitating objects to better conform to the specified spatial conditions. On\nthe other hand, we identify that the vanilla backpropagation update rule can\ncause deviations from the pre-trained domain, leading to out-of-distribution\nartifacts. We accordingly introduce a Langevin dynamics-based adaptive update\nscheme as a remedy that promotes in-domain updating while respecting layout\nconstraints. Extensive experiments demonstrate that WinWinLay excels in\ncontrolling element placement and achieving photorealistic visual fidelity,\noutperforming the current state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2506.15563v1", "date": "2025-06-18", "relevancy": 2.4109, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6455}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5731}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Control%20and%20Realism%3A%20Best%20of%20Both%20Worlds%20in%20Layout-to-Image%20without%0A%20%20Training&body=Title%3A%20Control%20and%20Realism%3A%20Best%20of%20Both%20Worlds%20in%20Layout-to-Image%20without%0A%20%20Training%0AAuthor%3A%20Bonan%20Li%20and%20Yinhan%20Hu%20and%20Songhua%20Liu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Layout-to-Image%20generation%20aims%20to%20create%20complex%20scenes%20with%20precise%20control%0Aover%20the%20placement%20and%20arrangement%20of%20subjects.%20Existing%20works%20have%0Ademonstrated%20that%20pre-trained%20Text-to-Image%20diffusion%20models%20can%20achieve%20this%0Agoal%20without%20training%20on%20any%20specific%20data%3B%20however%2C%20they%20often%20face%20challenges%0Awith%20imprecise%20localization%20and%20unrealistic%20artifacts.%20Focusing%20on%20these%0Adrawbacks%2C%20we%20propose%20a%20novel%20training-free%20method%2C%20WinWinLay.%20At%20its%20core%2C%0AWinWinLay%20presents%20two%20key%20strategies%2C%20Non-local%20Attention%20Energy%20Function%20and%0AAdaptive%20Update%2C%20that%20collaboratively%20enhance%20control%20precision%20and%20realism.%20On%0Aone%20hand%2C%20we%20theoretically%20demonstrate%20that%20the%20commonly%20used%20attention%20energy%0Afunction%20introduces%20inherent%20spatial%20distribution%20biases%2C%20hindering%20objects%0Afrom%20being%20uniformly%20aligned%20with%20layout%20instructions.%20To%20overcome%20this%20issue%2C%0Anon-local%20attention%20prior%20is%20explored%20to%20redistribute%20attention%20scores%2C%0Afacilitating%20objects%20to%20better%20conform%20to%20the%20specified%20spatial%20conditions.%20On%0Athe%20other%20hand%2C%20we%20identify%20that%20the%20vanilla%20backpropagation%20update%20rule%20can%0Acause%20deviations%20from%20the%20pre-trained%20domain%2C%20leading%20to%20out-of-distribution%0Aartifacts.%20We%20accordingly%20introduce%20a%20Langevin%20dynamics-based%20adaptive%20update%0Ascheme%20as%20a%20remedy%20that%20promotes%20in-domain%20updating%20while%20respecting%20layout%0Aconstraints.%20Extensive%20experiments%20demonstrate%20that%20WinWinLay%20excels%20in%0Acontrolling%20element%20placement%20and%20achieving%20photorealistic%20visual%20fidelity%2C%0Aoutperforming%20the%20current%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControl%2520and%2520Realism%253A%2520Best%2520of%2520Both%2520Worlds%2520in%2520Layout-to-Image%2520without%250A%2520%2520Training%26entry.906535625%3DBonan%2520Li%2520and%2520Yinhan%2520Hu%2520and%2520Songhua%2520Liu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Layout-to-Image%2520generation%2520aims%2520to%2520create%2520complex%2520scenes%2520with%2520precise%2520control%250Aover%2520the%2520placement%2520and%2520arrangement%2520of%2520subjects.%2520Existing%2520works%2520have%250Ademonstrated%2520that%2520pre-trained%2520Text-to-Image%2520diffusion%2520models%2520can%2520achieve%2520this%250Agoal%2520without%2520training%2520on%2520any%2520specific%2520data%253B%2520however%252C%2520they%2520often%2520face%2520challenges%250Awith%2520imprecise%2520localization%2520and%2520unrealistic%2520artifacts.%2520Focusing%2520on%2520these%250Adrawbacks%252C%2520we%2520propose%2520a%2520novel%2520training-free%2520method%252C%2520WinWinLay.%2520At%2520its%2520core%252C%250AWinWinLay%2520presents%2520two%2520key%2520strategies%252C%2520Non-local%2520Attention%2520Energy%2520Function%2520and%250AAdaptive%2520Update%252C%2520that%2520collaboratively%2520enhance%2520control%2520precision%2520and%2520realism.%2520On%250Aone%2520hand%252C%2520we%2520theoretically%2520demonstrate%2520that%2520the%2520commonly%2520used%2520attention%2520energy%250Afunction%2520introduces%2520inherent%2520spatial%2520distribution%2520biases%252C%2520hindering%2520objects%250Afrom%2520being%2520uniformly%2520aligned%2520with%2520layout%2520instructions.%2520To%2520overcome%2520this%2520issue%252C%250Anon-local%2520attention%2520prior%2520is%2520explored%2520to%2520redistribute%2520attention%2520scores%252C%250Afacilitating%2520objects%2520to%2520better%2520conform%2520to%2520the%2520specified%2520spatial%2520conditions.%2520On%250Athe%2520other%2520hand%252C%2520we%2520identify%2520that%2520the%2520vanilla%2520backpropagation%2520update%2520rule%2520can%250Acause%2520deviations%2520from%2520the%2520pre-trained%2520domain%252C%2520leading%2520to%2520out-of-distribution%250Aartifacts.%2520We%2520accordingly%2520introduce%2520a%2520Langevin%2520dynamics-based%2520adaptive%2520update%250Ascheme%2520as%2520a%2520remedy%2520that%2520promotes%2520in-domain%2520updating%2520while%2520respecting%2520layout%250Aconstraints.%2520Extensive%2520experiments%2520demonstrate%2520that%2520WinWinLay%2520excels%2520in%250Acontrolling%2520element%2520placement%2520and%2520achieving%2520photorealistic%2520visual%2520fidelity%252C%250Aoutperforming%2520the%2520current%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Control%20and%20Realism%3A%20Best%20of%20Both%20Worlds%20in%20Layout-to-Image%20without%0A%20%20Training&entry.906535625=Bonan%20Li%20and%20Yinhan%20Hu%20and%20Songhua%20Liu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Layout-to-Image%20generation%20aims%20to%20create%20complex%20scenes%20with%20precise%20control%0Aover%20the%20placement%20and%20arrangement%20of%20subjects.%20Existing%20works%20have%0Ademonstrated%20that%20pre-trained%20Text-to-Image%20diffusion%20models%20can%20achieve%20this%0Agoal%20without%20training%20on%20any%20specific%20data%3B%20however%2C%20they%20often%20face%20challenges%0Awith%20imprecise%20localization%20and%20unrealistic%20artifacts.%20Focusing%20on%20these%0Adrawbacks%2C%20we%20propose%20a%20novel%20training-free%20method%2C%20WinWinLay.%20At%20its%20core%2C%0AWinWinLay%20presents%20two%20key%20strategies%2C%20Non-local%20Attention%20Energy%20Function%20and%0AAdaptive%20Update%2C%20that%20collaboratively%20enhance%20control%20precision%20and%20realism.%20On%0Aone%20hand%2C%20we%20theoretically%20demonstrate%20that%20the%20commonly%20used%20attention%20energy%0Afunction%20introduces%20inherent%20spatial%20distribution%20biases%2C%20hindering%20objects%0Afrom%20being%20uniformly%20aligned%20with%20layout%20instructions.%20To%20overcome%20this%20issue%2C%0Anon-local%20attention%20prior%20is%20explored%20to%20redistribute%20attention%20scores%2C%0Afacilitating%20objects%20to%20better%20conform%20to%20the%20specified%20spatial%20conditions.%20On%0Athe%20other%20hand%2C%20we%20identify%20that%20the%20vanilla%20backpropagation%20update%20rule%20can%0Acause%20deviations%20from%20the%20pre-trained%20domain%2C%20leading%20to%20out-of-distribution%0Aartifacts.%20We%20accordingly%20introduce%20a%20Langevin%20dynamics-based%20adaptive%20update%0Ascheme%20as%20a%20remedy%20that%20promotes%20in-domain%20updating%20while%20respecting%20layout%0Aconstraints.%20Extensive%20experiments%20demonstrate%20that%20WinWinLay%20excels%20in%0Acontrolling%20element%20placement%20and%20achieving%20photorealistic%20visual%20fidelity%2C%0Aoutperforming%20the%20current%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15563v1&entry.124074799=Read"},
{"title": "Contributions to Representation Learning with Graph Autoencoders and\n  Applications to Music Recommendation", "author": "Guillaume Salha-Galvan", "abstract": "  Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as\ntwo powerful groups of unsupervised node embedding methods, with various\napplications to graph-based machine learning problems such as link prediction\nand community detection. Nonetheless, at the beginning of this Ph.D. project,\nGAE and VGAE models were also suffering from key limitations, preventing them\nfrom being adopted in the industry. In this thesis, we present several\ncontributions to improve these models, with the general aim of facilitating\ntheir use to address industrial-level problems involving graph representations.\nFirstly, we propose two strategies to overcome the scalability issues of\nprevious GAE and VGAE models, permitting to effectively train these models on\nlarge graphs with millions of nodes and edges. These strategies leverage graph\ndegeneracy and stochastic subgraph decoding techniques, respectively. Besides,\nwe introduce Gravity-Inspired GAE and VGAE, providing the first extensions of\nthese models for directed graphs, that are ubiquitous in industrial\napplications. We also consider extensions of GAE and VGAE models for dynamic\ngraphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily\ncomplex, and we propose to simplify them by leveraging linear encoders. Lastly,\nwe introduce Modularity-Aware GAE and VGAE to improve community detection on\ngraphs, while jointly preserving good performances on link prediction. In the\nlast part of this thesis, we evaluate our methods on several graphs extracted\nfrom the music streaming service Deezer. We put the emphasis on graph-based\nmusic recommendation problems. In particular, we show that our methods can\nimprove the detection of communities of similar musical items to recommend to\nusers, that they can effectively rank similar artists in a cold start setting,\nand that they permit modeling the music genre perception across cultures.\n", "link": "http://arxiv.org/abs/2205.14651v3", "date": "2025-06-18", "relevancy": 2.3816, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4895}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4893}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contributions%20to%20Representation%20Learning%20with%20Graph%20Autoencoders%20and%0A%20%20Applications%20to%20Music%20Recommendation&body=Title%3A%20Contributions%20to%20Representation%20Learning%20with%20Graph%20Autoencoders%20and%0A%20%20Applications%20to%20Music%20Recommendation%0AAuthor%3A%20Guillaume%20Salha-Galvan%0AAbstract%3A%20%20%20Graph%20autoencoders%20%28GAE%29%20and%20variational%20graph%20autoencoders%20%28VGAE%29%20emerged%20as%0Atwo%20powerful%20groups%20of%20unsupervised%20node%20embedding%20methods%2C%20with%20various%0Aapplications%20to%20graph-based%20machine%20learning%20problems%20such%20as%20link%20prediction%0Aand%20community%20detection.%20Nonetheless%2C%20at%20the%20beginning%20of%20this%20Ph.D.%20project%2C%0AGAE%20and%20VGAE%20models%20were%20also%20suffering%20from%20key%20limitations%2C%20preventing%20them%0Afrom%20being%20adopted%20in%20the%20industry.%20In%20this%20thesis%2C%20we%20present%20several%0Acontributions%20to%20improve%20these%20models%2C%20with%20the%20general%20aim%20of%20facilitating%0Atheir%20use%20to%20address%20industrial-level%20problems%20involving%20graph%20representations.%0AFirstly%2C%20we%20propose%20two%20strategies%20to%20overcome%20the%20scalability%20issues%20of%0Aprevious%20GAE%20and%20VGAE%20models%2C%20permitting%20to%20effectively%20train%20these%20models%20on%0Alarge%20graphs%20with%20millions%20of%20nodes%20and%20edges.%20These%20strategies%20leverage%20graph%0Adegeneracy%20and%20stochastic%20subgraph%20decoding%20techniques%2C%20respectively.%20Besides%2C%0Awe%20introduce%20Gravity-Inspired%20GAE%20and%20VGAE%2C%20providing%20the%20first%20extensions%20of%0Athese%20models%20for%20directed%20graphs%2C%20that%20are%20ubiquitous%20in%20industrial%0Aapplications.%20We%20also%20consider%20extensions%20of%20GAE%20and%20VGAE%20models%20for%20dynamic%0Agraphs.%20Furthermore%2C%20we%20argue%20that%20GAE%20and%20VGAE%20models%20are%20often%20unnecessarily%0Acomplex%2C%20and%20we%20propose%20to%20simplify%20them%20by%20leveraging%20linear%20encoders.%20Lastly%2C%0Awe%20introduce%20Modularity-Aware%20GAE%20and%20VGAE%20to%20improve%20community%20detection%20on%0Agraphs%2C%20while%20jointly%20preserving%20good%20performances%20on%20link%20prediction.%20In%20the%0Alast%20part%20of%20this%20thesis%2C%20we%20evaluate%20our%20methods%20on%20several%20graphs%20extracted%0Afrom%20the%20music%20streaming%20service%20Deezer.%20We%20put%20the%20emphasis%20on%20graph-based%0Amusic%20recommendation%20problems.%20In%20particular%2C%20we%20show%20that%20our%20methods%20can%0Aimprove%20the%20detection%20of%20communities%20of%20similar%20musical%20items%20to%20recommend%20to%0Ausers%2C%20that%20they%20can%20effectively%20rank%20similar%20artists%20in%20a%20cold%20start%20setting%2C%0Aand%20that%20they%20permit%20modeling%20the%20music%20genre%20perception%20across%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.14651v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContributions%2520to%2520Representation%2520Learning%2520with%2520Graph%2520Autoencoders%2520and%250A%2520%2520Applications%2520to%2520Music%2520Recommendation%26entry.906535625%3DGuillaume%2520Salha-Galvan%26entry.1292438233%3D%2520%2520Graph%2520autoencoders%2520%2528GAE%2529%2520and%2520variational%2520graph%2520autoencoders%2520%2528VGAE%2529%2520emerged%2520as%250Atwo%2520powerful%2520groups%2520of%2520unsupervised%2520node%2520embedding%2520methods%252C%2520with%2520various%250Aapplications%2520to%2520graph-based%2520machine%2520learning%2520problems%2520such%2520as%2520link%2520prediction%250Aand%2520community%2520detection.%2520Nonetheless%252C%2520at%2520the%2520beginning%2520of%2520this%2520Ph.D.%2520project%252C%250AGAE%2520and%2520VGAE%2520models%2520were%2520also%2520suffering%2520from%2520key%2520limitations%252C%2520preventing%2520them%250Afrom%2520being%2520adopted%2520in%2520the%2520industry.%2520In%2520this%2520thesis%252C%2520we%2520present%2520several%250Acontributions%2520to%2520improve%2520these%2520models%252C%2520with%2520the%2520general%2520aim%2520of%2520facilitating%250Atheir%2520use%2520to%2520address%2520industrial-level%2520problems%2520involving%2520graph%2520representations.%250AFirstly%252C%2520we%2520propose%2520two%2520strategies%2520to%2520overcome%2520the%2520scalability%2520issues%2520of%250Aprevious%2520GAE%2520and%2520VGAE%2520models%252C%2520permitting%2520to%2520effectively%2520train%2520these%2520models%2520on%250Alarge%2520graphs%2520with%2520millions%2520of%2520nodes%2520and%2520edges.%2520These%2520strategies%2520leverage%2520graph%250Adegeneracy%2520and%2520stochastic%2520subgraph%2520decoding%2520techniques%252C%2520respectively.%2520Besides%252C%250Awe%2520introduce%2520Gravity-Inspired%2520GAE%2520and%2520VGAE%252C%2520providing%2520the%2520first%2520extensions%2520of%250Athese%2520models%2520for%2520directed%2520graphs%252C%2520that%2520are%2520ubiquitous%2520in%2520industrial%250Aapplications.%2520We%2520also%2520consider%2520extensions%2520of%2520GAE%2520and%2520VGAE%2520models%2520for%2520dynamic%250Agraphs.%2520Furthermore%252C%2520we%2520argue%2520that%2520GAE%2520and%2520VGAE%2520models%2520are%2520often%2520unnecessarily%250Acomplex%252C%2520and%2520we%2520propose%2520to%2520simplify%2520them%2520by%2520leveraging%2520linear%2520encoders.%2520Lastly%252C%250Awe%2520introduce%2520Modularity-Aware%2520GAE%2520and%2520VGAE%2520to%2520improve%2520community%2520detection%2520on%250Agraphs%252C%2520while%2520jointly%2520preserving%2520good%2520performances%2520on%2520link%2520prediction.%2520In%2520the%250Alast%2520part%2520of%2520this%2520thesis%252C%2520we%2520evaluate%2520our%2520methods%2520on%2520several%2520graphs%2520extracted%250Afrom%2520the%2520music%2520streaming%2520service%2520Deezer.%2520We%2520put%2520the%2520emphasis%2520on%2520graph-based%250Amusic%2520recommendation%2520problems.%2520In%2520particular%252C%2520we%2520show%2520that%2520our%2520methods%2520can%250Aimprove%2520the%2520detection%2520of%2520communities%2520of%2520similar%2520musical%2520items%2520to%2520recommend%2520to%250Ausers%252C%2520that%2520they%2520can%2520effectively%2520rank%2520similar%2520artists%2520in%2520a%2520cold%2520start%2520setting%252C%250Aand%2520that%2520they%2520permit%2520modeling%2520the%2520music%2520genre%2520perception%2520across%2520cultures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.14651v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contributions%20to%20Representation%20Learning%20with%20Graph%20Autoencoders%20and%0A%20%20Applications%20to%20Music%20Recommendation&entry.906535625=Guillaume%20Salha-Galvan&entry.1292438233=%20%20Graph%20autoencoders%20%28GAE%29%20and%20variational%20graph%20autoencoders%20%28VGAE%29%20emerged%20as%0Atwo%20powerful%20groups%20of%20unsupervised%20node%20embedding%20methods%2C%20with%20various%0Aapplications%20to%20graph-based%20machine%20learning%20problems%20such%20as%20link%20prediction%0Aand%20community%20detection.%20Nonetheless%2C%20at%20the%20beginning%20of%20this%20Ph.D.%20project%2C%0AGAE%20and%20VGAE%20models%20were%20also%20suffering%20from%20key%20limitations%2C%20preventing%20them%0Afrom%20being%20adopted%20in%20the%20industry.%20In%20this%20thesis%2C%20we%20present%20several%0Acontributions%20to%20improve%20these%20models%2C%20with%20the%20general%20aim%20of%20facilitating%0Atheir%20use%20to%20address%20industrial-level%20problems%20involving%20graph%20representations.%0AFirstly%2C%20we%20propose%20two%20strategies%20to%20overcome%20the%20scalability%20issues%20of%0Aprevious%20GAE%20and%20VGAE%20models%2C%20permitting%20to%20effectively%20train%20these%20models%20on%0Alarge%20graphs%20with%20millions%20of%20nodes%20and%20edges.%20These%20strategies%20leverage%20graph%0Adegeneracy%20and%20stochastic%20subgraph%20decoding%20techniques%2C%20respectively.%20Besides%2C%0Awe%20introduce%20Gravity-Inspired%20GAE%20and%20VGAE%2C%20providing%20the%20first%20extensions%20of%0Athese%20models%20for%20directed%20graphs%2C%20that%20are%20ubiquitous%20in%20industrial%0Aapplications.%20We%20also%20consider%20extensions%20of%20GAE%20and%20VGAE%20models%20for%20dynamic%0Agraphs.%20Furthermore%2C%20we%20argue%20that%20GAE%20and%20VGAE%20models%20are%20often%20unnecessarily%0Acomplex%2C%20and%20we%20propose%20to%20simplify%20them%20by%20leveraging%20linear%20encoders.%20Lastly%2C%0Awe%20introduce%20Modularity-Aware%20GAE%20and%20VGAE%20to%20improve%20community%20detection%20on%0Agraphs%2C%20while%20jointly%20preserving%20good%20performances%20on%20link%20prediction.%20In%20the%0Alast%20part%20of%20this%20thesis%2C%20we%20evaluate%20our%20methods%20on%20several%20graphs%20extracted%0Afrom%20the%20music%20streaming%20service%20Deezer.%20We%20put%20the%20emphasis%20on%20graph-based%0Amusic%20recommendation%20problems.%20In%20particular%2C%20we%20show%20that%20our%20methods%20can%0Aimprove%20the%20detection%20of%20communities%20of%20similar%20musical%20items%20to%20recommend%20to%0Ausers%2C%20that%20they%20can%20effectively%20rank%20similar%20artists%20in%20a%20cold%20start%20setting%2C%0Aand%20that%20they%20permit%20modeling%20the%20music%20genre%20perception%20across%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.14651v3&entry.124074799=Read"},
{"title": "Differentiable and accelerated spherical harmonic and Wigner transforms", "author": "Matthew A. Price and Jason D. McEwen", "abstract": "  Many areas of science and engineering encounter data defined on spherical\nmanifolds. Modelling and analysis of spherical data often necessitates\nspherical harmonic transforms, at high degrees, and increasingly requires\nefficient computation of gradients for machine learning or other differentiable\nprogramming tasks. We develop novel algorithmic structures for accelerated and\ndifferentiable computation of generalised Fourier transforms on the sphere\n$\\mathbb{S}^2$ and rotation group $\\text{SO}(3)$, i.e. spherical harmonic and\nWigner transforms, respectively. We present a recursive algorithm for the\ncalculation of Wigner $d$-functions that is both stable to high harmonic\ndegrees and extremely parallelisable. By tightly coupling this with separable\nspherical transforms, we obtain algorithms that exhibit an extremely\nparallelisable structure that is well-suited for the high throughput computing\nof modern hardware accelerators (e.g. GPUs). We also develop a hybrid automatic\nand manual differentiation approach so that gradients can be computed\nefficiently. Our algorithms are implemented within the JAX differentiable\nprogramming framework in the S2FFT software code. Numerous samplings of the\nsphere are supported, including equiangular and HEALPix sampling. Computational\nerrors are at the order of machine precision for spherical samplings that admit\na sampling theorem. When benchmarked against alternative C codes we observe up\nto a 400-fold acceleration. Furthermore, when distributing over multiple GPUs\nwe achieve very close to optimal linear scaling with increasing number of GPUs\ndue to the highly parallelised and balanced nature of our algorithms. Provided\naccess to sufficiently many GPUs our transforms thus exhibit an unprecedented\neffective linear time complexity.\n", "link": "http://arxiv.org/abs/2311.14670v3", "date": "2025-06-18", "relevancy": 2.3746, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4886}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4772}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20and%20accelerated%20spherical%20harmonic%20and%20Wigner%20transforms&body=Title%3A%20Differentiable%20and%20accelerated%20spherical%20harmonic%20and%20Wigner%20transforms%0AAuthor%3A%20Matthew%20A.%20Price%20and%20Jason%20D.%20McEwen%0AAbstract%3A%20%20%20Many%20areas%20of%20science%20and%20engineering%20encounter%20data%20defined%20on%20spherical%0Amanifolds.%20Modelling%20and%20analysis%20of%20spherical%20data%20often%20necessitates%0Aspherical%20harmonic%20transforms%2C%20at%20high%20degrees%2C%20and%20increasingly%20requires%0Aefficient%20computation%20of%20gradients%20for%20machine%20learning%20or%20other%20differentiable%0Aprogramming%20tasks.%20We%20develop%20novel%20algorithmic%20structures%20for%20accelerated%20and%0Adifferentiable%20computation%20of%20generalised%20Fourier%20transforms%20on%20the%20sphere%0A%24%5Cmathbb%7BS%7D%5E2%24%20and%20rotation%20group%20%24%5Ctext%7BSO%7D%283%29%24%2C%20i.e.%20spherical%20harmonic%20and%0AWigner%20transforms%2C%20respectively.%20We%20present%20a%20recursive%20algorithm%20for%20the%0Acalculation%20of%20Wigner%20%24d%24-functions%20that%20is%20both%20stable%20to%20high%20harmonic%0Adegrees%20and%20extremely%20parallelisable.%20By%20tightly%20coupling%20this%20with%20separable%0Aspherical%20transforms%2C%20we%20obtain%20algorithms%20that%20exhibit%20an%20extremely%0Aparallelisable%20structure%20that%20is%20well-suited%20for%20the%20high%20throughput%20computing%0Aof%20modern%20hardware%20accelerators%20%28e.g.%20GPUs%29.%20We%20also%20develop%20a%20hybrid%20automatic%0Aand%20manual%20differentiation%20approach%20so%20that%20gradients%20can%20be%20computed%0Aefficiently.%20Our%20algorithms%20are%20implemented%20within%20the%20JAX%20differentiable%0Aprogramming%20framework%20in%20the%20S2FFT%20software%20code.%20Numerous%20samplings%20of%20the%0Asphere%20are%20supported%2C%20including%20equiangular%20and%20HEALPix%20sampling.%20Computational%0Aerrors%20are%20at%20the%20order%20of%20machine%20precision%20for%20spherical%20samplings%20that%20admit%0Aa%20sampling%20theorem.%20When%20benchmarked%20against%20alternative%20C%20codes%20we%20observe%20up%0Ato%20a%20400-fold%20acceleration.%20Furthermore%2C%20when%20distributing%20over%20multiple%20GPUs%0Awe%20achieve%20very%20close%20to%20optimal%20linear%20scaling%20with%20increasing%20number%20of%20GPUs%0Adue%20to%20the%20highly%20parallelised%20and%20balanced%20nature%20of%20our%20algorithms.%20Provided%0Aaccess%20to%20sufficiently%20many%20GPUs%20our%20transforms%20thus%20exhibit%20an%20unprecedented%0Aeffective%20linear%20time%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14670v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520and%2520accelerated%2520spherical%2520harmonic%2520and%2520Wigner%2520transforms%26entry.906535625%3DMatthew%2520A.%2520Price%2520and%2520Jason%2520D.%2520McEwen%26entry.1292438233%3D%2520%2520Many%2520areas%2520of%2520science%2520and%2520engineering%2520encounter%2520data%2520defined%2520on%2520spherical%250Amanifolds.%2520Modelling%2520and%2520analysis%2520of%2520spherical%2520data%2520often%2520necessitates%250Aspherical%2520harmonic%2520transforms%252C%2520at%2520high%2520degrees%252C%2520and%2520increasingly%2520requires%250Aefficient%2520computation%2520of%2520gradients%2520for%2520machine%2520learning%2520or%2520other%2520differentiable%250Aprogramming%2520tasks.%2520We%2520develop%2520novel%2520algorithmic%2520structures%2520for%2520accelerated%2520and%250Adifferentiable%2520computation%2520of%2520generalised%2520Fourier%2520transforms%2520on%2520the%2520sphere%250A%2524%255Cmathbb%257BS%257D%255E2%2524%2520and%2520rotation%2520group%2520%2524%255Ctext%257BSO%257D%25283%2529%2524%252C%2520i.e.%2520spherical%2520harmonic%2520and%250AWigner%2520transforms%252C%2520respectively.%2520We%2520present%2520a%2520recursive%2520algorithm%2520for%2520the%250Acalculation%2520of%2520Wigner%2520%2524d%2524-functions%2520that%2520is%2520both%2520stable%2520to%2520high%2520harmonic%250Adegrees%2520and%2520extremely%2520parallelisable.%2520By%2520tightly%2520coupling%2520this%2520with%2520separable%250Aspherical%2520transforms%252C%2520we%2520obtain%2520algorithms%2520that%2520exhibit%2520an%2520extremely%250Aparallelisable%2520structure%2520that%2520is%2520well-suited%2520for%2520the%2520high%2520throughput%2520computing%250Aof%2520modern%2520hardware%2520accelerators%2520%2528e.g.%2520GPUs%2529.%2520We%2520also%2520develop%2520a%2520hybrid%2520automatic%250Aand%2520manual%2520differentiation%2520approach%2520so%2520that%2520gradients%2520can%2520be%2520computed%250Aefficiently.%2520Our%2520algorithms%2520are%2520implemented%2520within%2520the%2520JAX%2520differentiable%250Aprogramming%2520framework%2520in%2520the%2520S2FFT%2520software%2520code.%2520Numerous%2520samplings%2520of%2520the%250Asphere%2520are%2520supported%252C%2520including%2520equiangular%2520and%2520HEALPix%2520sampling.%2520Computational%250Aerrors%2520are%2520at%2520the%2520order%2520of%2520machine%2520precision%2520for%2520spherical%2520samplings%2520that%2520admit%250Aa%2520sampling%2520theorem.%2520When%2520benchmarked%2520against%2520alternative%2520C%2520codes%2520we%2520observe%2520up%250Ato%2520a%2520400-fold%2520acceleration.%2520Furthermore%252C%2520when%2520distributing%2520over%2520multiple%2520GPUs%250Awe%2520achieve%2520very%2520close%2520to%2520optimal%2520linear%2520scaling%2520with%2520increasing%2520number%2520of%2520GPUs%250Adue%2520to%2520the%2520highly%2520parallelised%2520and%2520balanced%2520nature%2520of%2520our%2520algorithms.%2520Provided%250Aaccess%2520to%2520sufficiently%2520many%2520GPUs%2520our%2520transforms%2520thus%2520exhibit%2520an%2520unprecedented%250Aeffective%2520linear%2520time%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14670v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20and%20accelerated%20spherical%20harmonic%20and%20Wigner%20transforms&entry.906535625=Matthew%20A.%20Price%20and%20Jason%20D.%20McEwen&entry.1292438233=%20%20Many%20areas%20of%20science%20and%20engineering%20encounter%20data%20defined%20on%20spherical%0Amanifolds.%20Modelling%20and%20analysis%20of%20spherical%20data%20often%20necessitates%0Aspherical%20harmonic%20transforms%2C%20at%20high%20degrees%2C%20and%20increasingly%20requires%0Aefficient%20computation%20of%20gradients%20for%20machine%20learning%20or%20other%20differentiable%0Aprogramming%20tasks.%20We%20develop%20novel%20algorithmic%20structures%20for%20accelerated%20and%0Adifferentiable%20computation%20of%20generalised%20Fourier%20transforms%20on%20the%20sphere%0A%24%5Cmathbb%7BS%7D%5E2%24%20and%20rotation%20group%20%24%5Ctext%7BSO%7D%283%29%24%2C%20i.e.%20spherical%20harmonic%20and%0AWigner%20transforms%2C%20respectively.%20We%20present%20a%20recursive%20algorithm%20for%20the%0Acalculation%20of%20Wigner%20%24d%24-functions%20that%20is%20both%20stable%20to%20high%20harmonic%0Adegrees%20and%20extremely%20parallelisable.%20By%20tightly%20coupling%20this%20with%20separable%0Aspherical%20transforms%2C%20we%20obtain%20algorithms%20that%20exhibit%20an%20extremely%0Aparallelisable%20structure%20that%20is%20well-suited%20for%20the%20high%20throughput%20computing%0Aof%20modern%20hardware%20accelerators%20%28e.g.%20GPUs%29.%20We%20also%20develop%20a%20hybrid%20automatic%0Aand%20manual%20differentiation%20approach%20so%20that%20gradients%20can%20be%20computed%0Aefficiently.%20Our%20algorithms%20are%20implemented%20within%20the%20JAX%20differentiable%0Aprogramming%20framework%20in%20the%20S2FFT%20software%20code.%20Numerous%20samplings%20of%20the%0Asphere%20are%20supported%2C%20including%20equiangular%20and%20HEALPix%20sampling.%20Computational%0Aerrors%20are%20at%20the%20order%20of%20machine%20precision%20for%20spherical%20samplings%20that%20admit%0Aa%20sampling%20theorem.%20When%20benchmarked%20against%20alternative%20C%20codes%20we%20observe%20up%0Ato%20a%20400-fold%20acceleration.%20Furthermore%2C%20when%20distributing%20over%20multiple%20GPUs%0Awe%20achieve%20very%20close%20to%20optimal%20linear%20scaling%20with%20increasing%20number%20of%20GPUs%0Adue%20to%20the%20highly%20parallelised%20and%20balanced%20nature%20of%20our%20algorithms.%20Provided%0Aaccess%20to%20sufficiently%20many%20GPUs%20our%20transforms%20thus%20exhibit%20an%20unprecedented%0Aeffective%20linear%20time%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14670v3&entry.124074799=Read"},
{"title": "Supervised Robustness-preserving Data-free Neural Network Pruning", "author": "Mark Huasong Meng and Guangdong Bai and Sin Gee Teo and Jin Song Dong", "abstract": "  When deploying pre-trained neural network models in real-world applications,\nmodel consumers often encounter resource-constraint platforms such as mobile\nand smart devices. They typically use the pruning technique to reduce the size\nand complexity of the model, generating a lighter one with less resource\nconsumption. Nonetheless, most existing pruning methods are proposed with the\npremise that the model after being pruned has a chance to be fine-tuned or even\nretrained based on the original training data. This may be unrealistic in\npractice, as the data controllers are often reluctant to provide their model\nconsumers with the original data. In this work, we study the neural network\npruning in the data-free context, aiming to yield lightweight models that are\nnot only accurate in prediction but also robust against undesired inputs in\nopen-world deployments. Considering the absence of the fine-tuning and\nretraining that can fix the mis-pruned units, we replace the traditional\naggressive one-shot strategy with a conservative one that treats the pruning as\na progressive process. We propose a pruning method based on stochastic\noptimization that uses robustness-related metrics to guide the pruning process.\nOur method is implemented as a Python program and evaluated with a series of\nexperiments on diverse neural network models. The experimental results show\nthat it significantly outperforms existing one-shot data-free pruning\napproaches in terms of robustness preservation and accuracy.\n", "link": "http://arxiv.org/abs/2204.00783v3", "date": "2025-06-18", "relevancy": 2.3579, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.478}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4699}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Supervised%20Robustness-preserving%20Data-free%20Neural%20Network%20Pruning&body=Title%3A%20Supervised%20Robustness-preserving%20Data-free%20Neural%20Network%20Pruning%0AAuthor%3A%20Mark%20Huasong%20Meng%20and%20Guangdong%20Bai%20and%20Sin%20Gee%20Teo%20and%20Jin%20Song%20Dong%0AAbstract%3A%20%20%20When%20deploying%20pre-trained%20neural%20network%20models%20in%20real-world%20applications%2C%0Amodel%20consumers%20often%20encounter%20resource-constraint%20platforms%20such%20as%20mobile%0Aand%20smart%20devices.%20They%20typically%20use%20the%20pruning%20technique%20to%20reduce%20the%20size%0Aand%20complexity%20of%20the%20model%2C%20generating%20a%20lighter%20one%20with%20less%20resource%0Aconsumption.%20Nonetheless%2C%20most%20existing%20pruning%20methods%20are%20proposed%20with%20the%0Apremise%20that%20the%20model%20after%20being%20pruned%20has%20a%20chance%20to%20be%20fine-tuned%20or%20even%0Aretrained%20based%20on%20the%20original%20training%20data.%20This%20may%20be%20unrealistic%20in%0Apractice%2C%20as%20the%20data%20controllers%20are%20often%20reluctant%20to%20provide%20their%20model%0Aconsumers%20with%20the%20original%20data.%20In%20this%20work%2C%20we%20study%20the%20neural%20network%0Apruning%20in%20the%20data-free%20context%2C%20aiming%20to%20yield%20lightweight%20models%20that%20are%0Anot%20only%20accurate%20in%20prediction%20but%20also%20robust%20against%20undesired%20inputs%20in%0Aopen-world%20deployments.%20Considering%20the%20absence%20of%20the%20fine-tuning%20and%0Aretraining%20that%20can%20fix%20the%20mis-pruned%20units%2C%20we%20replace%20the%20traditional%0Aaggressive%20one-shot%20strategy%20with%20a%20conservative%20one%20that%20treats%20the%20pruning%20as%0Aa%20progressive%20process.%20We%20propose%20a%20pruning%20method%20based%20on%20stochastic%0Aoptimization%20that%20uses%20robustness-related%20metrics%20to%20guide%20the%20pruning%20process.%0AOur%20method%20is%20implemented%20as%20a%20Python%20program%20and%20evaluated%20with%20a%20series%20of%0Aexperiments%20on%20diverse%20neural%20network%20models.%20The%20experimental%20results%20show%0Athat%20it%20significantly%20outperforms%20existing%20one-shot%20data-free%20pruning%0Aapproaches%20in%20terms%20of%20robustness%20preservation%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.00783v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupervised%2520Robustness-preserving%2520Data-free%2520Neural%2520Network%2520Pruning%26entry.906535625%3DMark%2520Huasong%2520Meng%2520and%2520Guangdong%2520Bai%2520and%2520Sin%2520Gee%2520Teo%2520and%2520Jin%2520Song%2520Dong%26entry.1292438233%3D%2520%2520When%2520deploying%2520pre-trained%2520neural%2520network%2520models%2520in%2520real-world%2520applications%252C%250Amodel%2520consumers%2520often%2520encounter%2520resource-constraint%2520platforms%2520such%2520as%2520mobile%250Aand%2520smart%2520devices.%2520They%2520typically%2520use%2520the%2520pruning%2520technique%2520to%2520reduce%2520the%2520size%250Aand%2520complexity%2520of%2520the%2520model%252C%2520generating%2520a%2520lighter%2520one%2520with%2520less%2520resource%250Aconsumption.%2520Nonetheless%252C%2520most%2520existing%2520pruning%2520methods%2520are%2520proposed%2520with%2520the%250Apremise%2520that%2520the%2520model%2520after%2520being%2520pruned%2520has%2520a%2520chance%2520to%2520be%2520fine-tuned%2520or%2520even%250Aretrained%2520based%2520on%2520the%2520original%2520training%2520data.%2520This%2520may%2520be%2520unrealistic%2520in%250Apractice%252C%2520as%2520the%2520data%2520controllers%2520are%2520often%2520reluctant%2520to%2520provide%2520their%2520model%250Aconsumers%2520with%2520the%2520original%2520data.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520neural%2520network%250Apruning%2520in%2520the%2520data-free%2520context%252C%2520aiming%2520to%2520yield%2520lightweight%2520models%2520that%2520are%250Anot%2520only%2520accurate%2520in%2520prediction%2520but%2520also%2520robust%2520against%2520undesired%2520inputs%2520in%250Aopen-world%2520deployments.%2520Considering%2520the%2520absence%2520of%2520the%2520fine-tuning%2520and%250Aretraining%2520that%2520can%2520fix%2520the%2520mis-pruned%2520units%252C%2520we%2520replace%2520the%2520traditional%250Aaggressive%2520one-shot%2520strategy%2520with%2520a%2520conservative%2520one%2520that%2520treats%2520the%2520pruning%2520as%250Aa%2520progressive%2520process.%2520We%2520propose%2520a%2520pruning%2520method%2520based%2520on%2520stochastic%250Aoptimization%2520that%2520uses%2520robustness-related%2520metrics%2520to%2520guide%2520the%2520pruning%2520process.%250AOur%2520method%2520is%2520implemented%2520as%2520a%2520Python%2520program%2520and%2520evaluated%2520with%2520a%2520series%2520of%250Aexperiments%2520on%2520diverse%2520neural%2520network%2520models.%2520The%2520experimental%2520results%2520show%250Athat%2520it%2520significantly%2520outperforms%2520existing%2520one-shot%2520data-free%2520pruning%250Aapproaches%2520in%2520terms%2520of%2520robustness%2520preservation%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.00783v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Supervised%20Robustness-preserving%20Data-free%20Neural%20Network%20Pruning&entry.906535625=Mark%20Huasong%20Meng%20and%20Guangdong%20Bai%20and%20Sin%20Gee%20Teo%20and%20Jin%20Song%20Dong&entry.1292438233=%20%20When%20deploying%20pre-trained%20neural%20network%20models%20in%20real-world%20applications%2C%0Amodel%20consumers%20often%20encounter%20resource-constraint%20platforms%20such%20as%20mobile%0Aand%20smart%20devices.%20They%20typically%20use%20the%20pruning%20technique%20to%20reduce%20the%20size%0Aand%20complexity%20of%20the%20model%2C%20generating%20a%20lighter%20one%20with%20less%20resource%0Aconsumption.%20Nonetheless%2C%20most%20existing%20pruning%20methods%20are%20proposed%20with%20the%0Apremise%20that%20the%20model%20after%20being%20pruned%20has%20a%20chance%20to%20be%20fine-tuned%20or%20even%0Aretrained%20based%20on%20the%20original%20training%20data.%20This%20may%20be%20unrealistic%20in%0Apractice%2C%20as%20the%20data%20controllers%20are%20often%20reluctant%20to%20provide%20their%20model%0Aconsumers%20with%20the%20original%20data.%20In%20this%20work%2C%20we%20study%20the%20neural%20network%0Apruning%20in%20the%20data-free%20context%2C%20aiming%20to%20yield%20lightweight%20models%20that%20are%0Anot%20only%20accurate%20in%20prediction%20but%20also%20robust%20against%20undesired%20inputs%20in%0Aopen-world%20deployments.%20Considering%20the%20absence%20of%20the%20fine-tuning%20and%0Aretraining%20that%20can%20fix%20the%20mis-pruned%20units%2C%20we%20replace%20the%20traditional%0Aaggressive%20one-shot%20strategy%20with%20a%20conservative%20one%20that%20treats%20the%20pruning%20as%0Aa%20progressive%20process.%20We%20propose%20a%20pruning%20method%20based%20on%20stochastic%0Aoptimization%20that%20uses%20robustness-related%20metrics%20to%20guide%20the%20pruning%20process.%0AOur%20method%20is%20implemented%20as%20a%20Python%20program%20and%20evaluated%20with%20a%20series%20of%0Aexperiments%20on%20diverse%20neural%20network%20models.%20The%20experimental%20results%20show%0Athat%20it%20significantly%20outperforms%20existing%20one-shot%20data-free%20pruning%0Aapproaches%20in%20terms%20of%20robustness%20preservation%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.00783v3&entry.124074799=Read"},
{"title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework", "author": "Laura Kopf and Nils Feldhus and Kirill Bykov and Philine Lou Bommer and Anna Hedstr\u00f6m and Marina M. -C. H\u00f6hne and Oliver Eberle", "abstract": "  Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).\n", "link": "http://arxiv.org/abs/2506.15538v1", "date": "2025-06-18", "relevancy": 2.3539, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5941}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capturing%20Polysemanticity%20with%20PRISM%3A%20A%20Multi-Concept%20Feature%0A%20%20Description%20Framework&body=Title%3A%20Capturing%20Polysemanticity%20with%20PRISM%3A%20A%20Multi-Concept%20Feature%0A%20%20Description%20Framework%0AAuthor%3A%20Laura%20Kopf%20and%20Nils%20Feldhus%20and%20Kirill%20Bykov%20and%20Philine%20Lou%20Bommer%20and%20Anna%20Hedstr%C3%B6m%20and%20Marina%20M.%20-C.%20H%C3%B6hne%20and%20Oliver%20Eberle%0AAbstract%3A%20%20%20Automated%20interpretability%20research%20aims%20to%20identify%20concepts%20encoded%20in%0Aneural%20network%20features%20to%20enhance%20human%20understanding%20of%20model%20behavior.%0ACurrent%20feature%20description%20methods%20face%20two%20critical%20challenges%3A%20limited%0Arobustness%20and%20the%20flawed%20assumption%20that%20each%20neuron%20encodes%20only%20a%20single%0Aconcept%20%28monosemanticity%29%2C%20despite%20growing%20evidence%20that%20neurons%20are%20often%0Apolysemantic.%20This%20assumption%20restricts%20the%20expressiveness%20of%20feature%0Adescriptions%20and%20limits%20their%20ability%20to%20capture%20the%20full%20range%20of%20behaviors%0Aencoded%20in%20model%20internals.%20To%20address%20this%2C%20we%20introduce%20Polysemantic%20FeatuRe%0AIdentification%20and%20Scoring%20Method%20%28PRISM%29%2C%20a%20novel%20framework%20that%20captures%20the%0Ainherent%20complexity%20of%20neural%20network%20features.%20Unlike%20prior%20approaches%20that%0Aassign%20a%20single%20description%20per%20feature%2C%20PRISM%20provides%20more%20nuanced%0Adescriptions%20for%20both%20polysemantic%20and%20monosemantic%20features.%20We%20apply%20PRISM%20to%0Alanguage%20models%20and%2C%20through%20extensive%20benchmarking%20against%20existing%20methods%2C%0Ademonstrate%20that%20our%20approach%20produces%20more%20accurate%20and%20faithful%20feature%0Adescriptions%2C%20improving%20both%20overall%20description%20quality%20%28via%20a%20description%0Ascore%29%20and%20the%20ability%20to%20capture%20distinct%20concepts%20when%20polysemanticity%20is%0Apresent%20%28via%20a%20polysemanticity%20score%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapturing%2520Polysemanticity%2520with%2520PRISM%253A%2520A%2520Multi-Concept%2520Feature%250A%2520%2520Description%2520Framework%26entry.906535625%3DLaura%2520Kopf%2520and%2520Nils%2520Feldhus%2520and%2520Kirill%2520Bykov%2520and%2520Philine%2520Lou%2520Bommer%2520and%2520Anna%2520Hedstr%25C3%25B6m%2520and%2520Marina%2520M.%2520-C.%2520H%25C3%25B6hne%2520and%2520Oliver%2520Eberle%26entry.1292438233%3D%2520%2520Automated%2520interpretability%2520research%2520aims%2520to%2520identify%2520concepts%2520encoded%2520in%250Aneural%2520network%2520features%2520to%2520enhance%2520human%2520understanding%2520of%2520model%2520behavior.%250ACurrent%2520feature%2520description%2520methods%2520face%2520two%2520critical%2520challenges%253A%2520limited%250Arobustness%2520and%2520the%2520flawed%2520assumption%2520that%2520each%2520neuron%2520encodes%2520only%2520a%2520single%250Aconcept%2520%2528monosemanticity%2529%252C%2520despite%2520growing%2520evidence%2520that%2520neurons%2520are%2520often%250Apolysemantic.%2520This%2520assumption%2520restricts%2520the%2520expressiveness%2520of%2520feature%250Adescriptions%2520and%2520limits%2520their%2520ability%2520to%2520capture%2520the%2520full%2520range%2520of%2520behaviors%250Aencoded%2520in%2520model%2520internals.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Polysemantic%2520FeatuRe%250AIdentification%2520and%2520Scoring%2520Method%2520%2528PRISM%2529%252C%2520a%2520novel%2520framework%2520that%2520captures%2520the%250Ainherent%2520complexity%2520of%2520neural%2520network%2520features.%2520Unlike%2520prior%2520approaches%2520that%250Aassign%2520a%2520single%2520description%2520per%2520feature%252C%2520PRISM%2520provides%2520more%2520nuanced%250Adescriptions%2520for%2520both%2520polysemantic%2520and%2520monosemantic%2520features.%2520We%2520apply%2520PRISM%2520to%250Alanguage%2520models%2520and%252C%2520through%2520extensive%2520benchmarking%2520against%2520existing%2520methods%252C%250Ademonstrate%2520that%2520our%2520approach%2520produces%2520more%2520accurate%2520and%2520faithful%2520feature%250Adescriptions%252C%2520improving%2520both%2520overall%2520description%2520quality%2520%2528via%2520a%2520description%250Ascore%2529%2520and%2520the%2520ability%2520to%2520capture%2520distinct%2520concepts%2520when%2520polysemanticity%2520is%250Apresent%2520%2528via%2520a%2520polysemanticity%2520score%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capturing%20Polysemanticity%20with%20PRISM%3A%20A%20Multi-Concept%20Feature%0A%20%20Description%20Framework&entry.906535625=Laura%20Kopf%20and%20Nils%20Feldhus%20and%20Kirill%20Bykov%20and%20Philine%20Lou%20Bommer%20and%20Anna%20Hedstr%C3%B6m%20and%20Marina%20M.%20-C.%20H%C3%B6hne%20and%20Oliver%20Eberle&entry.1292438233=%20%20Automated%20interpretability%20research%20aims%20to%20identify%20concepts%20encoded%20in%0Aneural%20network%20features%20to%20enhance%20human%20understanding%20of%20model%20behavior.%0ACurrent%20feature%20description%20methods%20face%20two%20critical%20challenges%3A%20limited%0Arobustness%20and%20the%20flawed%20assumption%20that%20each%20neuron%20encodes%20only%20a%20single%0Aconcept%20%28monosemanticity%29%2C%20despite%20growing%20evidence%20that%20neurons%20are%20often%0Apolysemantic.%20This%20assumption%20restricts%20the%20expressiveness%20of%20feature%0Adescriptions%20and%20limits%20their%20ability%20to%20capture%20the%20full%20range%20of%20behaviors%0Aencoded%20in%20model%20internals.%20To%20address%20this%2C%20we%20introduce%20Polysemantic%20FeatuRe%0AIdentification%20and%20Scoring%20Method%20%28PRISM%29%2C%20a%20novel%20framework%20that%20captures%20the%0Ainherent%20complexity%20of%20neural%20network%20features.%20Unlike%20prior%20approaches%20that%0Aassign%20a%20single%20description%20per%20feature%2C%20PRISM%20provides%20more%20nuanced%0Adescriptions%20for%20both%20polysemantic%20and%20monosemantic%20features.%20We%20apply%20PRISM%20to%0Alanguage%20models%20and%2C%20through%20extensive%20benchmarking%20against%20existing%20methods%2C%0Ademonstrate%20that%20our%20approach%20produces%20more%20accurate%20and%20faithful%20feature%0Adescriptions%2C%20improving%20both%20overall%20description%20quality%20%28via%20a%20description%0Ascore%29%20and%20the%20ability%20to%20capture%20distinct%20concepts%20when%20polysemanticity%20is%0Apresent%20%28via%20a%20polysemanticity%20score%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15538v1&entry.124074799=Read"},
{"title": "Revisiting Randomization in Greedy Model Search", "author": "Xin Chen and Jason M. Klusowski and Yan Shuo Tan and Chang Yu", "abstract": "  Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.\n", "link": "http://arxiv.org/abs/2506.15643v1", "date": "2025-06-18", "relevancy": 2.353, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4835}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4768}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Randomization%20in%20Greedy%20Model%20Search&body=Title%3A%20Revisiting%20Randomization%20in%20Greedy%20Model%20Search%0AAuthor%3A%20Xin%20Chen%20and%20Jason%20M.%20Klusowski%20and%20Yan%20Shuo%20Tan%20and%20Chang%20Yu%0AAbstract%3A%20%20%20Combining%20randomized%20estimators%20in%20an%20ensemble%2C%20such%20as%20via%20random%20forests%2C%0Ahas%20become%20a%20fundamental%20technique%20in%20modern%20data%20science%2C%20but%20can%20be%0Acomputationally%20expensive.%20Furthermore%2C%20the%20mechanism%20by%20which%20this%20improves%0Apredictive%20performance%20is%20poorly%20understood.%20We%20address%20these%20issues%20in%20the%0Acontext%20of%20sparse%20linear%20regression%20by%20proposing%20and%20analyzing%20an%20ensemble%20of%0Agreedy%20forward%20selection%20estimators%20that%20are%20randomized%20by%20feature%20subsampling%0A--%20at%20each%20iteration%2C%20the%20best%20feature%20is%20selected%20from%20within%20a%20random%20subset.%0AWe%20design%20a%20novel%20implementation%20based%20on%20dynamic%20programming%20that%20greatly%0Aimproves%20its%20computational%20efficiency.%20Furthermore%2C%20we%20show%20via%20careful%0Anumerical%20experiments%20that%20our%20method%20can%20outperform%20popular%20methods%20such%20as%0Alasso%20and%20elastic%20net%20across%20a%20wide%20range%20of%20settings.%20Next%2C%20contrary%20to%0Aprevailing%20belief%20that%20randomized%20ensembling%20is%20analogous%20to%20shrinkage%2C%20we%20show%0Avia%20numerical%20experiments%20that%20it%20can%20simultaneously%20reduce%20training%20error%20and%0Adegrees%20of%20freedom%2C%20thereby%20shifting%20the%20entire%20bias-variance%20trade-off%20curve%0Aof%20the%20base%20estimator.%20We%20prove%20this%20fact%20rigorously%20in%20the%20setting%20of%0Aorthogonal%20features%2C%20in%20which%20case%2C%20the%20ensemble%20estimator%20rescales%20the%0Aordinary%20least%20squares%20coefficients%20with%20a%20two-parameter%20family%20of%20logistic%0Aweights%2C%20thereby%20enlarging%20the%20model%20search%20space.%20These%20results%20enhance%20our%0Aunderstanding%20of%20random%20forests%20and%20suggest%20that%20implicit%20regularization%20in%0Ageneral%20may%20have%20more%20complicated%20effects%20than%20explicit%20regularization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Randomization%2520in%2520Greedy%2520Model%2520Search%26entry.906535625%3DXin%2520Chen%2520and%2520Jason%2520M.%2520Klusowski%2520and%2520Yan%2520Shuo%2520Tan%2520and%2520Chang%2520Yu%26entry.1292438233%3D%2520%2520Combining%2520randomized%2520estimators%2520in%2520an%2520ensemble%252C%2520such%2520as%2520via%2520random%2520forests%252C%250Ahas%2520become%2520a%2520fundamental%2520technique%2520in%2520modern%2520data%2520science%252C%2520but%2520can%2520be%250Acomputationally%2520expensive.%2520Furthermore%252C%2520the%2520mechanism%2520by%2520which%2520this%2520improves%250Apredictive%2520performance%2520is%2520poorly%2520understood.%2520We%2520address%2520these%2520issues%2520in%2520the%250Acontext%2520of%2520sparse%2520linear%2520regression%2520by%2520proposing%2520and%2520analyzing%2520an%2520ensemble%2520of%250Agreedy%2520forward%2520selection%2520estimators%2520that%2520are%2520randomized%2520by%2520feature%2520subsampling%250A--%2520at%2520each%2520iteration%252C%2520the%2520best%2520feature%2520is%2520selected%2520from%2520within%2520a%2520random%2520subset.%250AWe%2520design%2520a%2520novel%2520implementation%2520based%2520on%2520dynamic%2520programming%2520that%2520greatly%250Aimproves%2520its%2520computational%2520efficiency.%2520Furthermore%252C%2520we%2520show%2520via%2520careful%250Anumerical%2520experiments%2520that%2520our%2520method%2520can%2520outperform%2520popular%2520methods%2520such%2520as%250Alasso%2520and%2520elastic%2520net%2520across%2520a%2520wide%2520range%2520of%2520settings.%2520Next%252C%2520contrary%2520to%250Aprevailing%2520belief%2520that%2520randomized%2520ensembling%2520is%2520analogous%2520to%2520shrinkage%252C%2520we%2520show%250Avia%2520numerical%2520experiments%2520that%2520it%2520can%2520simultaneously%2520reduce%2520training%2520error%2520and%250Adegrees%2520of%2520freedom%252C%2520thereby%2520shifting%2520the%2520entire%2520bias-variance%2520trade-off%2520curve%250Aof%2520the%2520base%2520estimator.%2520We%2520prove%2520this%2520fact%2520rigorously%2520in%2520the%2520setting%2520of%250Aorthogonal%2520features%252C%2520in%2520which%2520case%252C%2520the%2520ensemble%2520estimator%2520rescales%2520the%250Aordinary%2520least%2520squares%2520coefficients%2520with%2520a%2520two-parameter%2520family%2520of%2520logistic%250Aweights%252C%2520thereby%2520enlarging%2520the%2520model%2520search%2520space.%2520These%2520results%2520enhance%2520our%250Aunderstanding%2520of%2520random%2520forests%2520and%2520suggest%2520that%2520implicit%2520regularization%2520in%250Ageneral%2520may%2520have%2520more%2520complicated%2520effects%2520than%2520explicit%2520regularization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Randomization%20in%20Greedy%20Model%20Search&entry.906535625=Xin%20Chen%20and%20Jason%20M.%20Klusowski%20and%20Yan%20Shuo%20Tan%20and%20Chang%20Yu&entry.1292438233=%20%20Combining%20randomized%20estimators%20in%20an%20ensemble%2C%20such%20as%20via%20random%20forests%2C%0Ahas%20become%20a%20fundamental%20technique%20in%20modern%20data%20science%2C%20but%20can%20be%0Acomputationally%20expensive.%20Furthermore%2C%20the%20mechanism%20by%20which%20this%20improves%0Apredictive%20performance%20is%20poorly%20understood.%20We%20address%20these%20issues%20in%20the%0Acontext%20of%20sparse%20linear%20regression%20by%20proposing%20and%20analyzing%20an%20ensemble%20of%0Agreedy%20forward%20selection%20estimators%20that%20are%20randomized%20by%20feature%20subsampling%0A--%20at%20each%20iteration%2C%20the%20best%20feature%20is%20selected%20from%20within%20a%20random%20subset.%0AWe%20design%20a%20novel%20implementation%20based%20on%20dynamic%20programming%20that%20greatly%0Aimproves%20its%20computational%20efficiency.%20Furthermore%2C%20we%20show%20via%20careful%0Anumerical%20experiments%20that%20our%20method%20can%20outperform%20popular%20methods%20such%20as%0Alasso%20and%20elastic%20net%20across%20a%20wide%20range%20of%20settings.%20Next%2C%20contrary%20to%0Aprevailing%20belief%20that%20randomized%20ensembling%20is%20analogous%20to%20shrinkage%2C%20we%20show%0Avia%20numerical%20experiments%20that%20it%20can%20simultaneously%20reduce%20training%20error%20and%0Adegrees%20of%20freedom%2C%20thereby%20shifting%20the%20entire%20bias-variance%20trade-off%20curve%0Aof%20the%20base%20estimator.%20We%20prove%20this%20fact%20rigorously%20in%20the%20setting%20of%0Aorthogonal%20features%2C%20in%20which%20case%2C%20the%20ensemble%20estimator%20rescales%20the%0Aordinary%20least%20squares%20coefficients%20with%20a%20two-parameter%20family%20of%20logistic%0Aweights%2C%20thereby%20enlarging%20the%20model%20search%20space.%20These%20results%20enhance%20our%0Aunderstanding%20of%20random%20forests%20and%20suggest%20that%20implicit%20regularization%20in%0Ageneral%20may%20have%20more%20complicated%20effects%20than%20explicit%20regularization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15643v1&entry.124074799=Read"},
{"title": "Deep Graph Anomaly Detection: A Survey and New Perspectives", "author": "Hezhe Qiao and Hanghang Tong and Bo An and Irwin King and Charu Aggarwal and Guansong Pang", "abstract": "  Graph anomaly detection (GAD), which aims to identify unusual graph instances\n(nodes, edges, subgraphs, or graphs), has attracted increasing attention in\nrecent years due to its significance in a wide range of applications. Deep\nlearning approaches, graph neural networks (GNNs) in particular, have been\nemerging as a promising paradigm for GAD, owing to its strong capability in\ncapturing complex structure and/or node attributes in graph data. Considering\nthe large number of methods proposed for GNN-based GAD, it is of paramount\nimportance to summarize the methodologies and findings in the existing GAD\nstudies, so that we can pinpoint effective model designs for tackling open GAD\nproblems. To this end, in this work we aim to present a comprehensive review of\ndeep learning approaches for GAD. Existing GAD surveys are focused on\ntask-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique\nchallenges in GAD. To fill this gap, we first discuss the problem complexities\nand their resulting challenges in GAD, and then provide a systematic review of\ncurrent deep GAD methods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD, and graph anomaly\nmeasures. To deepen the discussions, we further propose a taxonomy of 13\nfine-grained method categories under these three perspectives to provide more\nin-depth insights into the model designs and their capabilities. To facilitate\nthe experiments and validation, we also summarize a collection of widely-used\nGAD datasets and empirical comparison. We further discuss multiple open\nproblems to inspire more future high-quality research. A continuously updated\nrepository for datasets, links to the codes of algorithms, and empirical\ncomparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.\n", "link": "http://arxiv.org/abs/2409.09957v2", "date": "2025-06-18", "relevancy": 2.3367, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4854}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4657}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Graph%20Anomaly%20Detection%3A%20A%20Survey%20and%20New%20Perspectives&body=Title%3A%20Deep%20Graph%20Anomaly%20Detection%3A%20A%20Survey%20and%20New%20Perspectives%0AAuthor%3A%20Hezhe%20Qiao%20and%20Hanghang%20Tong%20and%20Bo%20An%20and%20Irwin%20King%20and%20Charu%20Aggarwal%20and%20Guansong%20Pang%0AAbstract%3A%20%20%20Graph%20anomaly%20detection%20%28GAD%29%2C%20which%20aims%20to%20identify%20unusual%20graph%20instances%0A%28nodes%2C%20edges%2C%20subgraphs%2C%20or%20graphs%29%2C%20has%20attracted%20increasing%20attention%20in%0Arecent%20years%20due%20to%20its%20significance%20in%20a%20wide%20range%20of%20applications.%20Deep%0Alearning%20approaches%2C%20graph%20neural%20networks%20%28GNNs%29%20in%20particular%2C%20have%20been%0Aemerging%20as%20a%20promising%20paradigm%20for%20GAD%2C%20owing%20to%20its%20strong%20capability%20in%0Acapturing%20complex%20structure%20and/or%20node%20attributes%20in%20graph%20data.%20Considering%0Athe%20large%20number%20of%20methods%20proposed%20for%20GNN-based%20GAD%2C%20it%20is%20of%20paramount%0Aimportance%20to%20summarize%20the%20methodologies%20and%20findings%20in%20the%20existing%20GAD%0Astudies%2C%20so%20that%20we%20can%20pinpoint%20effective%20model%20designs%20for%20tackling%20open%20GAD%0Aproblems.%20To%20this%20end%2C%20in%20this%20work%20we%20aim%20to%20present%20a%20comprehensive%20review%20of%0Adeep%20learning%20approaches%20for%20GAD.%20Existing%20GAD%20surveys%20are%20focused%20on%0Atask-specific%20discussions%2C%20making%20it%20difficult%20to%20understand%20the%20technical%0Ainsights%20of%20existing%20methods%20and%20their%20limitations%20in%20addressing%20some%20unique%0Achallenges%20in%20GAD.%20To%20fill%20this%20gap%2C%20we%20first%20discuss%20the%20problem%20complexities%0Aand%20their%20resulting%20challenges%20in%20GAD%2C%20and%20then%20provide%20a%20systematic%20review%20of%0Acurrent%20deep%20GAD%20methods%20from%20three%20novel%20perspectives%20of%20methodology%2C%0Aincluding%20GNN%20backbone%20design%2C%20proxy%20task%20design%20for%20GAD%2C%20and%20graph%20anomaly%0Ameasures.%20To%20deepen%20the%20discussions%2C%20we%20further%20propose%20a%20taxonomy%20of%2013%0Afine-grained%20method%20categories%20under%20these%20three%20perspectives%20to%20provide%20more%0Ain-depth%20insights%20into%20the%20model%20designs%20and%20their%20capabilities.%20To%20facilitate%0Athe%20experiments%20and%20validation%2C%20we%20also%20summarize%20a%20collection%20of%20widely-used%0AGAD%20datasets%20and%20empirical%20comparison.%20We%20further%20discuss%20multiple%20open%0Aproblems%20to%20inspire%20more%20future%20high-quality%20research.%20A%20continuously%20updated%0Arepository%20for%20datasets%2C%20links%20to%20the%20codes%20of%20algorithms%2C%20and%20empirical%0Acomparison%20is%20available%20at%0Ahttps%3A//github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Graph%2520Anomaly%2520Detection%253A%2520A%2520Survey%2520and%2520New%2520Perspectives%26entry.906535625%3DHezhe%2520Qiao%2520and%2520Hanghang%2520Tong%2520and%2520Bo%2520An%2520and%2520Irwin%2520King%2520and%2520Charu%2520Aggarwal%2520and%2520Guansong%2520Pang%26entry.1292438233%3D%2520%2520Graph%2520anomaly%2520detection%2520%2528GAD%2529%252C%2520which%2520aims%2520to%2520identify%2520unusual%2520graph%2520instances%250A%2528nodes%252C%2520edges%252C%2520subgraphs%252C%2520or%2520graphs%2529%252C%2520has%2520attracted%2520increasing%2520attention%2520in%250Arecent%2520years%2520due%2520to%2520its%2520significance%2520in%2520a%2520wide%2520range%2520of%2520applications.%2520Deep%250Alearning%2520approaches%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520in%2520particular%252C%2520have%2520been%250Aemerging%2520as%2520a%2520promising%2520paradigm%2520for%2520GAD%252C%2520owing%2520to%2520its%2520strong%2520capability%2520in%250Acapturing%2520complex%2520structure%2520and/or%2520node%2520attributes%2520in%2520graph%2520data.%2520Considering%250Athe%2520large%2520number%2520of%2520methods%2520proposed%2520for%2520GNN-based%2520GAD%252C%2520it%2520is%2520of%2520paramount%250Aimportance%2520to%2520summarize%2520the%2520methodologies%2520and%2520findings%2520in%2520the%2520existing%2520GAD%250Astudies%252C%2520so%2520that%2520we%2520can%2520pinpoint%2520effective%2520model%2520designs%2520for%2520tackling%2520open%2520GAD%250Aproblems.%2520To%2520this%2520end%252C%2520in%2520this%2520work%2520we%2520aim%2520to%2520present%2520a%2520comprehensive%2520review%2520of%250Adeep%2520learning%2520approaches%2520for%2520GAD.%2520Existing%2520GAD%2520surveys%2520are%2520focused%2520on%250Atask-specific%2520discussions%252C%2520making%2520it%2520difficult%2520to%2520understand%2520the%2520technical%250Ainsights%2520of%2520existing%2520methods%2520and%2520their%2520limitations%2520in%2520addressing%2520some%2520unique%250Achallenges%2520in%2520GAD.%2520To%2520fill%2520this%2520gap%252C%2520we%2520first%2520discuss%2520the%2520problem%2520complexities%250Aand%2520their%2520resulting%2520challenges%2520in%2520GAD%252C%2520and%2520then%2520provide%2520a%2520systematic%2520review%2520of%250Acurrent%2520deep%2520GAD%2520methods%2520from%2520three%2520novel%2520perspectives%2520of%2520methodology%252C%250Aincluding%2520GNN%2520backbone%2520design%252C%2520proxy%2520task%2520design%2520for%2520GAD%252C%2520and%2520graph%2520anomaly%250Ameasures.%2520To%2520deepen%2520the%2520discussions%252C%2520we%2520further%2520propose%2520a%2520taxonomy%2520of%252013%250Afine-grained%2520method%2520categories%2520under%2520these%2520three%2520perspectives%2520to%2520provide%2520more%250Ain-depth%2520insights%2520into%2520the%2520model%2520designs%2520and%2520their%2520capabilities.%2520To%2520facilitate%250Athe%2520experiments%2520and%2520validation%252C%2520we%2520also%2520summarize%2520a%2520collection%2520of%2520widely-used%250AGAD%2520datasets%2520and%2520empirical%2520comparison.%2520We%2520further%2520discuss%2520multiple%2520open%250Aproblems%2520to%2520inspire%2520more%2520future%2520high-quality%2520research.%2520A%2520continuously%2520updated%250Arepository%2520for%2520datasets%252C%2520links%2520to%2520the%2520codes%2520of%2520algorithms%252C%2520and%2520empirical%250Acomparison%2520is%2520available%2520at%250Ahttps%253A//github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Graph%20Anomaly%20Detection%3A%20A%20Survey%20and%20New%20Perspectives&entry.906535625=Hezhe%20Qiao%20and%20Hanghang%20Tong%20and%20Bo%20An%20and%20Irwin%20King%20and%20Charu%20Aggarwal%20and%20Guansong%20Pang&entry.1292438233=%20%20Graph%20anomaly%20detection%20%28GAD%29%2C%20which%20aims%20to%20identify%20unusual%20graph%20instances%0A%28nodes%2C%20edges%2C%20subgraphs%2C%20or%20graphs%29%2C%20has%20attracted%20increasing%20attention%20in%0Arecent%20years%20due%20to%20its%20significance%20in%20a%20wide%20range%20of%20applications.%20Deep%0Alearning%20approaches%2C%20graph%20neural%20networks%20%28GNNs%29%20in%20particular%2C%20have%20been%0Aemerging%20as%20a%20promising%20paradigm%20for%20GAD%2C%20owing%20to%20its%20strong%20capability%20in%0Acapturing%20complex%20structure%20and/or%20node%20attributes%20in%20graph%20data.%20Considering%0Athe%20large%20number%20of%20methods%20proposed%20for%20GNN-based%20GAD%2C%20it%20is%20of%20paramount%0Aimportance%20to%20summarize%20the%20methodologies%20and%20findings%20in%20the%20existing%20GAD%0Astudies%2C%20so%20that%20we%20can%20pinpoint%20effective%20model%20designs%20for%20tackling%20open%20GAD%0Aproblems.%20To%20this%20end%2C%20in%20this%20work%20we%20aim%20to%20present%20a%20comprehensive%20review%20of%0Adeep%20learning%20approaches%20for%20GAD.%20Existing%20GAD%20surveys%20are%20focused%20on%0Atask-specific%20discussions%2C%20making%20it%20difficult%20to%20understand%20the%20technical%0Ainsights%20of%20existing%20methods%20and%20their%20limitations%20in%20addressing%20some%20unique%0Achallenges%20in%20GAD.%20To%20fill%20this%20gap%2C%20we%20first%20discuss%20the%20problem%20complexities%0Aand%20their%20resulting%20challenges%20in%20GAD%2C%20and%20then%20provide%20a%20systematic%20review%20of%0Acurrent%20deep%20GAD%20methods%20from%20three%20novel%20perspectives%20of%20methodology%2C%0Aincluding%20GNN%20backbone%20design%2C%20proxy%20task%20design%20for%20GAD%2C%20and%20graph%20anomaly%0Ameasures.%20To%20deepen%20the%20discussions%2C%20we%20further%20propose%20a%20taxonomy%20of%2013%0Afine-grained%20method%20categories%20under%20these%20three%20perspectives%20to%20provide%20more%0Ain-depth%20insights%20into%20the%20model%20designs%20and%20their%20capabilities.%20To%20facilitate%0Athe%20experiments%20and%20validation%2C%20we%20also%20summarize%20a%20collection%20of%20widely-used%0AGAD%20datasets%20and%20empirical%20comparison.%20We%20further%20discuss%20multiple%20open%0Aproblems%20to%20inspire%20more%20future%20high-quality%20research.%20A%20continuously%20updated%0Arepository%20for%20datasets%2C%20links%20to%20the%20codes%20of%20algorithms%2C%20and%20empirical%0Acomparison%20is%20available%20at%0Ahttps%3A//github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09957v2&entry.124074799=Read"},
{"title": "A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction", "author": "Yi Wang and Zhenghong Wang and Fan Zhang and Chengling Tang and Chaogui Kang and Di Zhu and Zhongfu Ma and Sijie Ruan and Weiyu Zhang and Yu Zheng and Philip S. Yu and Yu Liu", "abstract": "  Human activity intensity prediction is a crucial to many location-based\nservices. Although tremendous progress has been made to model dynamic\nspatiotemporal patterns of human activity, most existing methods, including\nspatiotemporal graph neural networks (ST-GNNs), overlook physical constraints\nof spatial interactions and the over-smoothing phenomenon in spatial\ncorrelation modeling. To address these limitations, this work proposes a\nphysics-informed deep learning framework, namely Gravity-informed\nSpatiotemporal Transformer (Gravityformer) by refining transformer attention to\nintegrate the universal law of gravitation and explicitly incorporating\nconstraints from spatial interactions. Specifically, it (1) estimates two\nspatially explicit mass parameters based on inflow and outflow, (2) models the\nlikelihood of cross-unit interaction using closed-form solutions of spatial\ninteractions to constrain spatial modeling randomness, and (3) utilizes the\nlearned spatial interaction to guide and mitigate the over-smoothing phenomenon\nin transformer attention matrices. The underlying law of human activity can be\nexplicitly modeled by the proposed adaptive gravity model. Moreover, a parallel\nspatiotemporal graph convolution transformer structure is proposed for\nachieving a balance between coupled spatial and temporal learning. Systematic\nexperiments on six real-world large-scale activity datasets demonstrate the\nquantitative and qualitative superiority of our approach over state-of-the-art\nbenchmarks. Additionally, the learned gravity attention matrix can be\ndisentangled and interpreted based on geographical laws. This work provides a\nnovel insight into integrating physical laws with deep learning for\nspatiotemporal predictive learning.\n", "link": "http://arxiv.org/abs/2506.13678v2", "date": "2025-06-18", "relevancy": 2.3366, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6087}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5777}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Gravity-informed%20Spatiotemporal%20Transformer%20for%20Human%20Activity%0A%20%20Intensity%20Prediction&body=Title%3A%20A%20Gravity-informed%20Spatiotemporal%20Transformer%20for%20Human%20Activity%0A%20%20Intensity%20Prediction%0AAuthor%3A%20Yi%20Wang%20and%20Zhenghong%20Wang%20and%20Fan%20Zhang%20and%20Chengling%20Tang%20and%20Chaogui%20Kang%20and%20Di%20Zhu%20and%20Zhongfu%20Ma%20and%20Sijie%20Ruan%20and%20Weiyu%20Zhang%20and%20Yu%20Zheng%20and%20Philip%20S.%20Yu%20and%20Yu%20Liu%0AAbstract%3A%20%20%20Human%20activity%20intensity%20prediction%20is%20a%20crucial%20to%20many%20location-based%0Aservices.%20Although%20tremendous%20progress%20has%20been%20made%20to%20model%20dynamic%0Aspatiotemporal%20patterns%20of%20human%20activity%2C%20most%20existing%20methods%2C%20including%0Aspatiotemporal%20graph%20neural%20networks%20%28ST-GNNs%29%2C%20overlook%20physical%20constraints%0Aof%20spatial%20interactions%20and%20the%20over-smoothing%20phenomenon%20in%20spatial%0Acorrelation%20modeling.%20To%20address%20these%20limitations%2C%20this%20work%20proposes%20a%0Aphysics-informed%20deep%20learning%20framework%2C%20namely%20Gravity-informed%0ASpatiotemporal%20Transformer%20%28Gravityformer%29%20by%20refining%20transformer%20attention%20to%0Aintegrate%20the%20universal%20law%20of%20gravitation%20and%20explicitly%20incorporating%0Aconstraints%20from%20spatial%20interactions.%20Specifically%2C%20it%20%281%29%20estimates%20two%0Aspatially%20explicit%20mass%20parameters%20based%20on%20inflow%20and%20outflow%2C%20%282%29%20models%20the%0Alikelihood%20of%20cross-unit%20interaction%20using%20closed-form%20solutions%20of%20spatial%0Ainteractions%20to%20constrain%20spatial%20modeling%20randomness%2C%20and%20%283%29%20utilizes%20the%0Alearned%20spatial%20interaction%20to%20guide%20and%20mitigate%20the%20over-smoothing%20phenomenon%0Ain%20transformer%20attention%20matrices.%20The%20underlying%20law%20of%20human%20activity%20can%20be%0Aexplicitly%20modeled%20by%20the%20proposed%20adaptive%20gravity%20model.%20Moreover%2C%20a%20parallel%0Aspatiotemporal%20graph%20convolution%20transformer%20structure%20is%20proposed%20for%0Aachieving%20a%20balance%20between%20coupled%20spatial%20and%20temporal%20learning.%20Systematic%0Aexperiments%20on%20six%20real-world%20large-scale%20activity%20datasets%20demonstrate%20the%0Aquantitative%20and%20qualitative%20superiority%20of%20our%20approach%20over%20state-of-the-art%0Abenchmarks.%20Additionally%2C%20the%20learned%20gravity%20attention%20matrix%20can%20be%0Adisentangled%20and%20interpreted%20based%20on%20geographical%20laws.%20This%20work%20provides%20a%0Anovel%20insight%20into%20integrating%20physical%20laws%20with%20deep%20learning%20for%0Aspatiotemporal%20predictive%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Gravity-informed%2520Spatiotemporal%2520Transformer%2520for%2520Human%2520Activity%250A%2520%2520Intensity%2520Prediction%26entry.906535625%3DYi%2520Wang%2520and%2520Zhenghong%2520Wang%2520and%2520Fan%2520Zhang%2520and%2520Chengling%2520Tang%2520and%2520Chaogui%2520Kang%2520and%2520Di%2520Zhu%2520and%2520Zhongfu%2520Ma%2520and%2520Sijie%2520Ruan%2520and%2520Weiyu%2520Zhang%2520and%2520Yu%2520Zheng%2520and%2520Philip%2520S.%2520Yu%2520and%2520Yu%2520Liu%26entry.1292438233%3D%2520%2520Human%2520activity%2520intensity%2520prediction%2520is%2520a%2520crucial%2520to%2520many%2520location-based%250Aservices.%2520Although%2520tremendous%2520progress%2520has%2520been%2520made%2520to%2520model%2520dynamic%250Aspatiotemporal%2520patterns%2520of%2520human%2520activity%252C%2520most%2520existing%2520methods%252C%2520including%250Aspatiotemporal%2520graph%2520neural%2520networks%2520%2528ST-GNNs%2529%252C%2520overlook%2520physical%2520constraints%250Aof%2520spatial%2520interactions%2520and%2520the%2520over-smoothing%2520phenomenon%2520in%2520spatial%250Acorrelation%2520modeling.%2520To%2520address%2520these%2520limitations%252C%2520this%2520work%2520proposes%2520a%250Aphysics-informed%2520deep%2520learning%2520framework%252C%2520namely%2520Gravity-informed%250ASpatiotemporal%2520Transformer%2520%2528Gravityformer%2529%2520by%2520refining%2520transformer%2520attention%2520to%250Aintegrate%2520the%2520universal%2520law%2520of%2520gravitation%2520and%2520explicitly%2520incorporating%250Aconstraints%2520from%2520spatial%2520interactions.%2520Specifically%252C%2520it%2520%25281%2529%2520estimates%2520two%250Aspatially%2520explicit%2520mass%2520parameters%2520based%2520on%2520inflow%2520and%2520outflow%252C%2520%25282%2529%2520models%2520the%250Alikelihood%2520of%2520cross-unit%2520interaction%2520using%2520closed-form%2520solutions%2520of%2520spatial%250Ainteractions%2520to%2520constrain%2520spatial%2520modeling%2520randomness%252C%2520and%2520%25283%2529%2520utilizes%2520the%250Alearned%2520spatial%2520interaction%2520to%2520guide%2520and%2520mitigate%2520the%2520over-smoothing%2520phenomenon%250Ain%2520transformer%2520attention%2520matrices.%2520The%2520underlying%2520law%2520of%2520human%2520activity%2520can%2520be%250Aexplicitly%2520modeled%2520by%2520the%2520proposed%2520adaptive%2520gravity%2520model.%2520Moreover%252C%2520a%2520parallel%250Aspatiotemporal%2520graph%2520convolution%2520transformer%2520structure%2520is%2520proposed%2520for%250Aachieving%2520a%2520balance%2520between%2520coupled%2520spatial%2520and%2520temporal%2520learning.%2520Systematic%250Aexperiments%2520on%2520six%2520real-world%2520large-scale%2520activity%2520datasets%2520demonstrate%2520the%250Aquantitative%2520and%2520qualitative%2520superiority%2520of%2520our%2520approach%2520over%2520state-of-the-art%250Abenchmarks.%2520Additionally%252C%2520the%2520learned%2520gravity%2520attention%2520matrix%2520can%2520be%250Adisentangled%2520and%2520interpreted%2520based%2520on%2520geographical%2520laws.%2520This%2520work%2520provides%2520a%250Anovel%2520insight%2520into%2520integrating%2520physical%2520laws%2520with%2520deep%2520learning%2520for%250Aspatiotemporal%2520predictive%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Gravity-informed%20Spatiotemporal%20Transformer%20for%20Human%20Activity%0A%20%20Intensity%20Prediction&entry.906535625=Yi%20Wang%20and%20Zhenghong%20Wang%20and%20Fan%20Zhang%20and%20Chengling%20Tang%20and%20Chaogui%20Kang%20and%20Di%20Zhu%20and%20Zhongfu%20Ma%20and%20Sijie%20Ruan%20and%20Weiyu%20Zhang%20and%20Yu%20Zheng%20and%20Philip%20S.%20Yu%20and%20Yu%20Liu&entry.1292438233=%20%20Human%20activity%20intensity%20prediction%20is%20a%20crucial%20to%20many%20location-based%0Aservices.%20Although%20tremendous%20progress%20has%20been%20made%20to%20model%20dynamic%0Aspatiotemporal%20patterns%20of%20human%20activity%2C%20most%20existing%20methods%2C%20including%0Aspatiotemporal%20graph%20neural%20networks%20%28ST-GNNs%29%2C%20overlook%20physical%20constraints%0Aof%20spatial%20interactions%20and%20the%20over-smoothing%20phenomenon%20in%20spatial%0Acorrelation%20modeling.%20To%20address%20these%20limitations%2C%20this%20work%20proposes%20a%0Aphysics-informed%20deep%20learning%20framework%2C%20namely%20Gravity-informed%0ASpatiotemporal%20Transformer%20%28Gravityformer%29%20by%20refining%20transformer%20attention%20to%0Aintegrate%20the%20universal%20law%20of%20gravitation%20and%20explicitly%20incorporating%0Aconstraints%20from%20spatial%20interactions.%20Specifically%2C%20it%20%281%29%20estimates%20two%0Aspatially%20explicit%20mass%20parameters%20based%20on%20inflow%20and%20outflow%2C%20%282%29%20models%20the%0Alikelihood%20of%20cross-unit%20interaction%20using%20closed-form%20solutions%20of%20spatial%0Ainteractions%20to%20constrain%20spatial%20modeling%20randomness%2C%20and%20%283%29%20utilizes%20the%0Alearned%20spatial%20interaction%20to%20guide%20and%20mitigate%20the%20over-smoothing%20phenomenon%0Ain%20transformer%20attention%20matrices.%20The%20underlying%20law%20of%20human%20activity%20can%20be%0Aexplicitly%20modeled%20by%20the%20proposed%20adaptive%20gravity%20model.%20Moreover%2C%20a%20parallel%0Aspatiotemporal%20graph%20convolution%20transformer%20structure%20is%20proposed%20for%0Aachieving%20a%20balance%20between%20coupled%20spatial%20and%20temporal%20learning.%20Systematic%0Aexperiments%20on%20six%20real-world%20large-scale%20activity%20datasets%20demonstrate%20the%0Aquantitative%20and%20qualitative%20superiority%20of%20our%20approach%20over%20state-of-the-art%0Abenchmarks.%20Additionally%2C%20the%20learned%20gravity%20attention%20matrix%20can%20be%0Adisentangled%20and%20interpreted%20based%20on%20geographical%20laws.%20This%20work%20provides%20a%0Anovel%20insight%20into%20integrating%20physical%20laws%20with%20deep%20learning%20for%0Aspatiotemporal%20predictive%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13678v2&entry.124074799=Read"},
{"title": "A Real-time Endoscopic Image Denoising System", "author": "Yu Xing and Shishi Huang and Meng Lv and Guo Chen and Huailiang Wang and Lingzhi Sui", "abstract": "  Endoscopes featuring a miniaturized design have significantly enhanced\noperational flexibility, portability, and diagnostic capability while\nsubstantially reducing the invasiveness of medical procedures. Recently,\nsingle-use endoscopes equipped with an ultra-compact analogue image sensor\nmeasuring less than 1mm x 1mm bring revolutionary advancements to medical\ndiagnosis. They reduce the structural redundancy and large capital expenditures\nassociated with reusable devices, eliminate the risk of patient infections\ncaused by inadequate disinfection, and alleviate patient suffering. However,\nthe limited photosensitive area results in reduced photon capture per pixel,\nrequiring higher photon sensitivity settings to maintain adequate brightness.\nIn high-contrast medical imaging scenarios, the small-sized sensor exhibits a\nconstrained dynamic range, making it difficult to simultaneously capture\ndetails in both highlights and shadows, and additional localized digital gain\nis required to compensate. Moreover, the simplified circuit design and analog\nsignal transmission introduce additional noise sources. These factors\ncollectively contribute to significant noise issues in processed endoscopic\nimages. In this work, we developed a comprehensive noise model for analog image\nsensors in medical endoscopes, addressing three primary noise types:\nfixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.\nBuilding on this analysis, we propose a hybrid denoising system that\nsynergistically combines traditional image processing algorithms with advanced\nlearning-based techniques for captured raw frames from sensors. Experiments\ndemonstrate that our approach effectively reduces image noise without fine\ndetail loss or color distortion, while achieving real-time performance on FPGA\nplatforms and an average PSNR improvement from 21.16 to 33.05 on our test\ndataset.\n", "link": "http://arxiv.org/abs/2506.15395v1", "date": "2025-06-18", "relevancy": 2.329, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6194}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5821}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Real-time%20Endoscopic%20Image%20Denoising%20System&body=Title%3A%20A%20Real-time%20Endoscopic%20Image%20Denoising%20System%0AAuthor%3A%20Yu%20Xing%20and%20Shishi%20Huang%20and%20Meng%20Lv%20and%20Guo%20Chen%20and%20Huailiang%20Wang%20and%20Lingzhi%20Sui%0AAbstract%3A%20%20%20Endoscopes%20featuring%20a%20miniaturized%20design%20have%20significantly%20enhanced%0Aoperational%20flexibility%2C%20portability%2C%20and%20diagnostic%20capability%20while%0Asubstantially%20reducing%20the%20invasiveness%20of%20medical%20procedures.%20Recently%2C%0Asingle-use%20endoscopes%20equipped%20with%20an%20ultra-compact%20analogue%20image%20sensor%0Ameasuring%20less%20than%201mm%20x%201mm%20bring%20revolutionary%20advancements%20to%20medical%0Adiagnosis.%20They%20reduce%20the%20structural%20redundancy%20and%20large%20capital%20expenditures%0Aassociated%20with%20reusable%20devices%2C%20eliminate%20the%20risk%20of%20patient%20infections%0Acaused%20by%20inadequate%20disinfection%2C%20and%20alleviate%20patient%20suffering.%20However%2C%0Athe%20limited%20photosensitive%20area%20results%20in%20reduced%20photon%20capture%20per%20pixel%2C%0Arequiring%20higher%20photon%20sensitivity%20settings%20to%20maintain%20adequate%20brightness.%0AIn%20high-contrast%20medical%20imaging%20scenarios%2C%20the%20small-sized%20sensor%20exhibits%20a%0Aconstrained%20dynamic%20range%2C%20making%20it%20difficult%20to%20simultaneously%20capture%0Adetails%20in%20both%20highlights%20and%20shadows%2C%20and%20additional%20localized%20digital%20gain%0Ais%20required%20to%20compensate.%20Moreover%2C%20the%20simplified%20circuit%20design%20and%20analog%0Asignal%20transmission%20introduce%20additional%20noise%20sources.%20These%20factors%0Acollectively%20contribute%20to%20significant%20noise%20issues%20in%20processed%20endoscopic%0Aimages.%20In%20this%20work%2C%20we%20developed%20a%20comprehensive%20noise%20model%20for%20analog%20image%0Asensors%20in%20medical%20endoscopes%2C%20addressing%20three%20primary%20noise%20types%3A%0Afixed-pattern%20noise%2C%20periodic%20banding%20noise%2C%20and%20mixed%20Poisson-Gaussian%20noise.%0ABuilding%20on%20this%20analysis%2C%20we%20propose%20a%20hybrid%20denoising%20system%20that%0Asynergistically%20combines%20traditional%20image%20processing%20algorithms%20with%20advanced%0Alearning-based%20techniques%20for%20captured%20raw%20frames%20from%20sensors.%20Experiments%0Ademonstrate%20that%20our%20approach%20effectively%20reduces%20image%20noise%20without%20fine%0Adetail%20loss%20or%20color%20distortion%2C%20while%20achieving%20real-time%20performance%20on%20FPGA%0Aplatforms%20and%20an%20average%20PSNR%20improvement%20from%2021.16%20to%2033.05%20on%20our%20test%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15395v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Real-time%2520Endoscopic%2520Image%2520Denoising%2520System%26entry.906535625%3DYu%2520Xing%2520and%2520Shishi%2520Huang%2520and%2520Meng%2520Lv%2520and%2520Guo%2520Chen%2520and%2520Huailiang%2520Wang%2520and%2520Lingzhi%2520Sui%26entry.1292438233%3D%2520%2520Endoscopes%2520featuring%2520a%2520miniaturized%2520design%2520have%2520significantly%2520enhanced%250Aoperational%2520flexibility%252C%2520portability%252C%2520and%2520diagnostic%2520capability%2520while%250Asubstantially%2520reducing%2520the%2520invasiveness%2520of%2520medical%2520procedures.%2520Recently%252C%250Asingle-use%2520endoscopes%2520equipped%2520with%2520an%2520ultra-compact%2520analogue%2520image%2520sensor%250Ameasuring%2520less%2520than%25201mm%2520x%25201mm%2520bring%2520revolutionary%2520advancements%2520to%2520medical%250Adiagnosis.%2520They%2520reduce%2520the%2520structural%2520redundancy%2520and%2520large%2520capital%2520expenditures%250Aassociated%2520with%2520reusable%2520devices%252C%2520eliminate%2520the%2520risk%2520of%2520patient%2520infections%250Acaused%2520by%2520inadequate%2520disinfection%252C%2520and%2520alleviate%2520patient%2520suffering.%2520However%252C%250Athe%2520limited%2520photosensitive%2520area%2520results%2520in%2520reduced%2520photon%2520capture%2520per%2520pixel%252C%250Arequiring%2520higher%2520photon%2520sensitivity%2520settings%2520to%2520maintain%2520adequate%2520brightness.%250AIn%2520high-contrast%2520medical%2520imaging%2520scenarios%252C%2520the%2520small-sized%2520sensor%2520exhibits%2520a%250Aconstrained%2520dynamic%2520range%252C%2520making%2520it%2520difficult%2520to%2520simultaneously%2520capture%250Adetails%2520in%2520both%2520highlights%2520and%2520shadows%252C%2520and%2520additional%2520localized%2520digital%2520gain%250Ais%2520required%2520to%2520compensate.%2520Moreover%252C%2520the%2520simplified%2520circuit%2520design%2520and%2520analog%250Asignal%2520transmission%2520introduce%2520additional%2520noise%2520sources.%2520These%2520factors%250Acollectively%2520contribute%2520to%2520significant%2520noise%2520issues%2520in%2520processed%2520endoscopic%250Aimages.%2520In%2520this%2520work%252C%2520we%2520developed%2520a%2520comprehensive%2520noise%2520model%2520for%2520analog%2520image%250Asensors%2520in%2520medical%2520endoscopes%252C%2520addressing%2520three%2520primary%2520noise%2520types%253A%250Afixed-pattern%2520noise%252C%2520periodic%2520banding%2520noise%252C%2520and%2520mixed%2520Poisson-Gaussian%2520noise.%250ABuilding%2520on%2520this%2520analysis%252C%2520we%2520propose%2520a%2520hybrid%2520denoising%2520system%2520that%250Asynergistically%2520combines%2520traditional%2520image%2520processing%2520algorithms%2520with%2520advanced%250Alearning-based%2520techniques%2520for%2520captured%2520raw%2520frames%2520from%2520sensors.%2520Experiments%250Ademonstrate%2520that%2520our%2520approach%2520effectively%2520reduces%2520image%2520noise%2520without%2520fine%250Adetail%2520loss%2520or%2520color%2520distortion%252C%2520while%2520achieving%2520real-time%2520performance%2520on%2520FPGA%250Aplatforms%2520and%2520an%2520average%2520PSNR%2520improvement%2520from%252021.16%2520to%252033.05%2520on%2520our%2520test%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15395v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Real-time%20Endoscopic%20Image%20Denoising%20System&entry.906535625=Yu%20Xing%20and%20Shishi%20Huang%20and%20Meng%20Lv%20and%20Guo%20Chen%20and%20Huailiang%20Wang%20and%20Lingzhi%20Sui&entry.1292438233=%20%20Endoscopes%20featuring%20a%20miniaturized%20design%20have%20significantly%20enhanced%0Aoperational%20flexibility%2C%20portability%2C%20and%20diagnostic%20capability%20while%0Asubstantially%20reducing%20the%20invasiveness%20of%20medical%20procedures.%20Recently%2C%0Asingle-use%20endoscopes%20equipped%20with%20an%20ultra-compact%20analogue%20image%20sensor%0Ameasuring%20less%20than%201mm%20x%201mm%20bring%20revolutionary%20advancements%20to%20medical%0Adiagnosis.%20They%20reduce%20the%20structural%20redundancy%20and%20large%20capital%20expenditures%0Aassociated%20with%20reusable%20devices%2C%20eliminate%20the%20risk%20of%20patient%20infections%0Acaused%20by%20inadequate%20disinfection%2C%20and%20alleviate%20patient%20suffering.%20However%2C%0Athe%20limited%20photosensitive%20area%20results%20in%20reduced%20photon%20capture%20per%20pixel%2C%0Arequiring%20higher%20photon%20sensitivity%20settings%20to%20maintain%20adequate%20brightness.%0AIn%20high-contrast%20medical%20imaging%20scenarios%2C%20the%20small-sized%20sensor%20exhibits%20a%0Aconstrained%20dynamic%20range%2C%20making%20it%20difficult%20to%20simultaneously%20capture%0Adetails%20in%20both%20highlights%20and%20shadows%2C%20and%20additional%20localized%20digital%20gain%0Ais%20required%20to%20compensate.%20Moreover%2C%20the%20simplified%20circuit%20design%20and%20analog%0Asignal%20transmission%20introduce%20additional%20noise%20sources.%20These%20factors%0Acollectively%20contribute%20to%20significant%20noise%20issues%20in%20processed%20endoscopic%0Aimages.%20In%20this%20work%2C%20we%20developed%20a%20comprehensive%20noise%20model%20for%20analog%20image%0Asensors%20in%20medical%20endoscopes%2C%20addressing%20three%20primary%20noise%20types%3A%0Afixed-pattern%20noise%2C%20periodic%20banding%20noise%2C%20and%20mixed%20Poisson-Gaussian%20noise.%0ABuilding%20on%20this%20analysis%2C%20we%20propose%20a%20hybrid%20denoising%20system%20that%0Asynergistically%20combines%20traditional%20image%20processing%20algorithms%20with%20advanced%0Alearning-based%20techniques%20for%20captured%20raw%20frames%20from%20sensors.%20Experiments%0Ademonstrate%20that%20our%20approach%20effectively%20reduces%20image%20noise%20without%20fine%0Adetail%20loss%20or%20color%20distortion%2C%20while%20achieving%20real-time%20performance%20on%20FPGA%0Aplatforms%20and%20an%20average%20PSNR%20improvement%20from%2021.16%20to%2033.05%20on%20our%20test%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15395v1&entry.124074799=Read"},
{"title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning", "author": "Zhangcheng Qiang and Kerry Taylor and Weiqing Wang", "abstract": "  Due to the dynamic nature of the Semantic Web, version control is necessary\nto capture time-varying information, particularly for widely used ontologies.\nDespite the long-standing recognition of ontology versioning (OV) as a crucial\ncomponent for efficient ontology management, the growing size of ontologies and\naccumulating errors caused by manual labour overwhelm current OV approaches. In\nthis paper, we propose yet another approach to performing OV using existing\nontology matching (OM) techniques and systems. We introduce a unified OM4OV\npipeline. From an OM perspective, we reconstruct a new task formulation and\nmeasurement for OV tasks. Building upon the prior alignment(s) from OM, we\npropose a pipeline optimisation method called the cross-reference (CR)\nmechanism to enhance overall OV performance. We experimentally validate the\nOM4OV pipeline and the cross-reference mechanism in the OV tested originating\nfrom the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss insights into OM used for OV tasks, where some false mappings detected\nby OV systems are not actually untrue.\n", "link": "http://arxiv.org/abs/2409.20302v3", "date": "2025-06-18", "relevancy": 2.3162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4647}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&body=Title%3A%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning%0AAuthor%3A%20Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang%0AAbstract%3A%20%20%20Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%0Ato%20capture%20time-varying%20information%2C%20particularly%20for%20widely%20used%20ontologies.%0ADespite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%0Acomponent%20for%20efficient%20ontology%20management%2C%20the%20growing%20size%20of%20ontologies%20and%0Aaccumulating%20errors%20caused%20by%20manual%20labour%20overwhelm%20current%20OV%20approaches.%20In%0Athis%20paper%2C%20we%20propose%20yet%20another%20approach%20to%20performing%20OV%20using%20existing%0Aontology%20matching%20%28OM%29%20techniques%20and%20systems.%20We%20introduce%20a%20unified%20OM4OV%0Apipeline.%20From%20an%20OM%20perspective%2C%20we%20reconstruct%20a%20new%20task%20formulation%20and%0Ameasurement%20for%20OV%20tasks.%20Building%20upon%20the%20prior%20alignment%28s%29%20from%20OM%2C%20we%0Apropose%20a%20pipeline%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%0Amechanism%20to%20enhance%20overall%20OV%20performance.%20We%20experimentally%20validate%20the%0AOM4OV%20pipeline%20and%20the%20cross-reference%20mechanism%20in%20the%20OV%20tested%20originating%0Afrom%20the%20Ontology%20Alignment%20Evaluation%20Initiative%20%28OAEI%29%20datasets.%20We%20also%0Adiscuss%20insights%20into%20OM%20used%20for%20OV%20tasks%2C%20where%20some%20false%20mappings%20detected%0Aby%20OV%20systems%20are%20not%20actually%20untrue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20302v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOM4OV%253A%2520Leveraging%2520Ontology%2520Matching%2520for%2520Ontology%2520Versioning%26entry.906535625%3DZhangcheng%2520Qiang%2520and%2520Kerry%2520Taylor%2520and%2520Weiqing%2520Wang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520dynamic%2520nature%2520of%2520the%2520Semantic%2520Web%252C%2520version%2520control%2520is%2520necessary%250Ato%2520capture%2520time-varying%2520information%252C%2520particularly%2520for%2520widely%2520used%2520ontologies.%250ADespite%2520the%2520long-standing%2520recognition%2520of%2520ontology%2520versioning%2520%2528OV%2529%2520as%2520a%2520crucial%250Acomponent%2520for%2520efficient%2520ontology%2520management%252C%2520the%2520growing%2520size%2520of%2520ontologies%2520and%250Aaccumulating%2520errors%2520caused%2520by%2520manual%2520labour%2520overwhelm%2520current%2520OV%2520approaches.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520yet%2520another%2520approach%2520to%2520performing%2520OV%2520using%2520existing%250Aontology%2520matching%2520%2528OM%2529%2520techniques%2520and%2520systems.%2520We%2520introduce%2520a%2520unified%2520OM4OV%250Apipeline.%2520From%2520an%2520OM%2520perspective%252C%2520we%2520reconstruct%2520a%2520new%2520task%2520formulation%2520and%250Ameasurement%2520for%2520OV%2520tasks.%2520Building%2520upon%2520the%2520prior%2520alignment%2528s%2529%2520from%2520OM%252C%2520we%250Apropose%2520a%2520pipeline%2520optimisation%2520method%2520called%2520the%2520cross-reference%2520%2528CR%2529%250Amechanism%2520to%2520enhance%2520overall%2520OV%2520performance.%2520We%2520experimentally%2520validate%2520the%250AOM4OV%2520pipeline%2520and%2520the%2520cross-reference%2520mechanism%2520in%2520the%2520OV%2520tested%2520originating%250Afrom%2520the%2520Ontology%2520Alignment%2520Evaluation%2520Initiative%2520%2528OAEI%2529%2520datasets.%2520We%2520also%250Adiscuss%2520insights%2520into%2520OM%2520used%2520for%2520OV%2520tasks%252C%2520where%2520some%2520false%2520mappings%2520detected%250Aby%2520OV%2520systems%2520are%2520not%2520actually%2520untrue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20302v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&entry.906535625=Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang&entry.1292438233=%20%20Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%0Ato%20capture%20time-varying%20information%2C%20particularly%20for%20widely%20used%20ontologies.%0ADespite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%0Acomponent%20for%20efficient%20ontology%20management%2C%20the%20growing%20size%20of%20ontologies%20and%0Aaccumulating%20errors%20caused%20by%20manual%20labour%20overwhelm%20current%20OV%20approaches.%20In%0Athis%20paper%2C%20we%20propose%20yet%20another%20approach%20to%20performing%20OV%20using%20existing%0Aontology%20matching%20%28OM%29%20techniques%20and%20systems.%20We%20introduce%20a%20unified%20OM4OV%0Apipeline.%20From%20an%20OM%20perspective%2C%20we%20reconstruct%20a%20new%20task%20formulation%20and%0Ameasurement%20for%20OV%20tasks.%20Building%20upon%20the%20prior%20alignment%28s%29%20from%20OM%2C%20we%0Apropose%20a%20pipeline%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%0Amechanism%20to%20enhance%20overall%20OV%20performance.%20We%20experimentally%20validate%20the%0AOM4OV%20pipeline%20and%20the%20cross-reference%20mechanism%20in%20the%20OV%20tested%20originating%0Afrom%20the%20Ontology%20Alignment%20Evaluation%20Initiative%20%28OAEI%29%20datasets.%20We%20also%0Adiscuss%20insights%20into%20OM%20used%20for%20OV%20tasks%2C%20where%20some%20false%20mappings%20detected%0Aby%20OV%20systems%20are%20not%20actually%20untrue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20302v3&entry.124074799=Read"},
{"title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning", "author": "Guoguo Ai and Hezhe Qiao and Hui Yan and Guansong Pang", "abstract": "  Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.\n", "link": "http://arxiv.org/abs/2506.15448v1", "date": "2025-06-18", "relevancy": 2.3035, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4746}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4625}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-supervised%20Graph%20Anomaly%20Detection%20via%20Robust%20Homophily%20Learning&body=Title%3A%20Semi-supervised%20Graph%20Anomaly%20Detection%20via%20Robust%20Homophily%20Learning%0AAuthor%3A%20Guoguo%20Ai%20and%20Hezhe%20Qiao%20and%20Hui%20Yan%20and%20Guansong%20Pang%0AAbstract%3A%20%20%20Semi-supervised%20graph%20anomaly%20detection%20%28GAD%29%20utilizes%20a%20small%20set%20of%20labeled%0Anormal%20nodes%20to%20identify%20abnormal%20nodes%20from%20a%20large%20set%20of%20unlabeled%20nodes%20in%0Aa%20graph.%20Current%20methods%20in%20this%20line%20posit%20that%201%29%20normal%20nodes%20share%20a%0Asimilar%20level%20of%20homophily%20and%202%29%20the%20labeled%20normal%20nodes%20can%20well%20represent%0Athe%20homophily%20patterns%20in%20the%20normal%20class.%20However%2C%20this%20assumption%20often%20does%0Anot%20hold%20well%20since%20normal%20nodes%20in%20a%20graph%20can%20exhibit%20diverse%20homophily%20in%0Areal-world%20GAD%20datasets.%20In%20this%20paper%2C%20we%20propose%20RHO%2C%20namely%20Robust%20Homophily%0ALearning%2C%20to%20adaptively%20learn%20such%20homophily%20patterns.%20RHO%20consists%20of%20two%0Anovel%20modules%2C%20adaptive%20frequency%20response%20filters%20%28AdaFreq%29%20and%20graph%0Anormality%20alignment%20%28GNA%29.%20AdaFreq%20learns%20a%20set%20of%20adaptive%20spectral%20filters%0Athat%20capture%20different%20frequency%20components%20of%20the%20labeled%20normal%20nodes%20with%0Avarying%20homophily%20in%20the%20channel-wise%20and%20cross-channel%20views%20of%20node%0Aattributes.%20GNA%20is%20introduced%20to%20enforce%20consistency%20between%20the%20channel-wise%0Aand%20cross-channel%20homophily%20representations%20to%20robustify%20the%20normality%20learned%0Aby%20the%20filters%20in%20the%20two%20views.%20Experiments%20on%20eight%20real-world%20GAD%20datasets%0Ashow%20that%20RHO%20can%20effectively%20learn%20varying%2C%20often%20under-represented%2C%20homophily%0Ain%20the%20small%20normal%20node%20set%20and%20substantially%20outperforms%20state-of-the-art%0Acompeting%20methods.%20Code%20is%20available%20at%20https%3A//github.com/mala-lab/RHO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-supervised%2520Graph%2520Anomaly%2520Detection%2520via%2520Robust%2520Homophily%2520Learning%26entry.906535625%3DGuoguo%2520Ai%2520and%2520Hezhe%2520Qiao%2520and%2520Hui%2520Yan%2520and%2520Guansong%2520Pang%26entry.1292438233%3D%2520%2520Semi-supervised%2520graph%2520anomaly%2520detection%2520%2528GAD%2529%2520utilizes%2520a%2520small%2520set%2520of%2520labeled%250Anormal%2520nodes%2520to%2520identify%2520abnormal%2520nodes%2520from%2520a%2520large%2520set%2520of%2520unlabeled%2520nodes%2520in%250Aa%2520graph.%2520Current%2520methods%2520in%2520this%2520line%2520posit%2520that%25201%2529%2520normal%2520nodes%2520share%2520a%250Asimilar%2520level%2520of%2520homophily%2520and%25202%2529%2520the%2520labeled%2520normal%2520nodes%2520can%2520well%2520represent%250Athe%2520homophily%2520patterns%2520in%2520the%2520normal%2520class.%2520However%252C%2520this%2520assumption%2520often%2520does%250Anot%2520hold%2520well%2520since%2520normal%2520nodes%2520in%2520a%2520graph%2520can%2520exhibit%2520diverse%2520homophily%2520in%250Areal-world%2520GAD%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520RHO%252C%2520namely%2520Robust%2520Homophily%250ALearning%252C%2520to%2520adaptively%2520learn%2520such%2520homophily%2520patterns.%2520RHO%2520consists%2520of%2520two%250Anovel%2520modules%252C%2520adaptive%2520frequency%2520response%2520filters%2520%2528AdaFreq%2529%2520and%2520graph%250Anormality%2520alignment%2520%2528GNA%2529.%2520AdaFreq%2520learns%2520a%2520set%2520of%2520adaptive%2520spectral%2520filters%250Athat%2520capture%2520different%2520frequency%2520components%2520of%2520the%2520labeled%2520normal%2520nodes%2520with%250Avarying%2520homophily%2520in%2520the%2520channel-wise%2520and%2520cross-channel%2520views%2520of%2520node%250Aattributes.%2520GNA%2520is%2520introduced%2520to%2520enforce%2520consistency%2520between%2520the%2520channel-wise%250Aand%2520cross-channel%2520homophily%2520representations%2520to%2520robustify%2520the%2520normality%2520learned%250Aby%2520the%2520filters%2520in%2520the%2520two%2520views.%2520Experiments%2520on%2520eight%2520real-world%2520GAD%2520datasets%250Ashow%2520that%2520RHO%2520can%2520effectively%2520learn%2520varying%252C%2520often%2520under-represented%252C%2520homophily%250Ain%2520the%2520small%2520normal%2520node%2520set%2520and%2520substantially%2520outperforms%2520state-of-the-art%250Acompeting%2520methods.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/mala-lab/RHO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-supervised%20Graph%20Anomaly%20Detection%20via%20Robust%20Homophily%20Learning&entry.906535625=Guoguo%20Ai%20and%20Hezhe%20Qiao%20and%20Hui%20Yan%20and%20Guansong%20Pang&entry.1292438233=%20%20Semi-supervised%20graph%20anomaly%20detection%20%28GAD%29%20utilizes%20a%20small%20set%20of%20labeled%0Anormal%20nodes%20to%20identify%20abnormal%20nodes%20from%20a%20large%20set%20of%20unlabeled%20nodes%20in%0Aa%20graph.%20Current%20methods%20in%20this%20line%20posit%20that%201%29%20normal%20nodes%20share%20a%0Asimilar%20level%20of%20homophily%20and%202%29%20the%20labeled%20normal%20nodes%20can%20well%20represent%0Athe%20homophily%20patterns%20in%20the%20normal%20class.%20However%2C%20this%20assumption%20often%20does%0Anot%20hold%20well%20since%20normal%20nodes%20in%20a%20graph%20can%20exhibit%20diverse%20homophily%20in%0Areal-world%20GAD%20datasets.%20In%20this%20paper%2C%20we%20propose%20RHO%2C%20namely%20Robust%20Homophily%0ALearning%2C%20to%20adaptively%20learn%20such%20homophily%20patterns.%20RHO%20consists%20of%20two%0Anovel%20modules%2C%20adaptive%20frequency%20response%20filters%20%28AdaFreq%29%20and%20graph%0Anormality%20alignment%20%28GNA%29.%20AdaFreq%20learns%20a%20set%20of%20adaptive%20spectral%20filters%0Athat%20capture%20different%20frequency%20components%20of%20the%20labeled%20normal%20nodes%20with%0Avarying%20homophily%20in%20the%20channel-wise%20and%20cross-channel%20views%20of%20node%0Aattributes.%20GNA%20is%20introduced%20to%20enforce%20consistency%20between%20the%20channel-wise%0Aand%20cross-channel%20homophily%20representations%20to%20robustify%20the%20normality%20learned%0Aby%20the%20filters%20in%20the%20two%20views.%20Experiments%20on%20eight%20real-world%20GAD%20datasets%0Ashow%20that%20RHO%20can%20effectively%20learn%20varying%2C%20often%20under-represented%2C%20homophily%0Ain%20the%20small%20normal%20node%20set%20and%20substantially%20outperforms%20state-of-the-art%0Acompeting%20methods.%20Code%20is%20available%20at%20https%3A//github.com/mala-lab/RHO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15448v1&entry.124074799=Read"},
{"title": "Benchmarking Neural Network Training Algorithms", "author": "George E. Dahl and Frank Schneider and Zachary Nado and Naman Agarwal and Chandramouli Shama Sastry and Philipp Hennig and Sourabh Medapati and Runa Eschenhagen and Priya Kasimbeg and Daniel Suo and Juhan Bae and Justin Gilmer and Abel L. Peirson and Bilal Khan and Rohan Anil and Mike Rabbat and Shankar Krishnan and Daniel Snider and Ehsan Amid and Kongtao Chen and Chris J. Maddison and Rakshith Vasudev and Michal Badura and Ankush Garg and Peter Mattson", "abstract": "  Training algorithms, broadly construed, are an essential part of every deep\nlearning pipeline. Training algorithm improvements that speed up training\nacross a wide variety of workloads (e.g., better update rules, tuning\nprotocols, learning rate schedules, or data selection schemes) could save time,\nsave computational resources, and lead to better, more accurate, models.\nUnfortunately, as a community, we are currently unable to reliably identify\ntraining algorithm improvements, or even determine the state-of-the-art\ntraining algorithm. In this work, using concrete experiments, we argue that\nreal progress in speeding up training requires new benchmarks that resolve\nthree basic challenges faced by empirical comparisons of training algorithms:\n(1) how to decide when training is complete and precisely measure training\ntime, (2) how to handle the sensitivity of measurements to exact workload\ndetails, and (3) how to fairly compare algorithms that require hyperparameter\ntuning. In order to address these challenges, we introduce a new, competitive,\ntime-to-result benchmark using multiple workloads running on fixed hardware,\nthe AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of\nworkload variants that make it possible to detect benchmark submissions that\nare more robust to workload changes than current widely-used methods. Finally,\nwe evaluate baseline submissions constructed using various optimizers that\nrepresent current practice, as well as other optimizers that have recently\nreceived attention in the literature. These baseline results collectively\ndemonstrate the feasibility of our benchmark, show that non-trivial gaps\nbetween methods exist, and set a provisional state-of-the-art for future\nbenchmark submissions to try and surpass.\n", "link": "http://arxiv.org/abs/2306.07179v2", "date": "2025-06-18", "relevancy": 2.2975, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5201}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4321}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Neural%20Network%20Training%20Algorithms&body=Title%3A%20Benchmarking%20Neural%20Network%20Training%20Algorithms%0AAuthor%3A%20George%20E.%20Dahl%20and%20Frank%20Schneider%20and%20Zachary%20Nado%20and%20Naman%20Agarwal%20and%20Chandramouli%20Shama%20Sastry%20and%20Philipp%20Hennig%20and%20Sourabh%20Medapati%20and%20Runa%20Eschenhagen%20and%20Priya%20Kasimbeg%20and%20Daniel%20Suo%20and%20Juhan%20Bae%20and%20Justin%20Gilmer%20and%20Abel%20L.%20Peirson%20and%20Bilal%20Khan%20and%20Rohan%20Anil%20and%20Mike%20Rabbat%20and%20Shankar%20Krishnan%20and%20Daniel%20Snider%20and%20Ehsan%20Amid%20and%20Kongtao%20Chen%20and%20Chris%20J.%20Maddison%20and%20Rakshith%20Vasudev%20and%20Michal%20Badura%20and%20Ankush%20Garg%20and%20Peter%20Mattson%0AAbstract%3A%20%20%20Training%20algorithms%2C%20broadly%20construed%2C%20are%20an%20essential%20part%20of%20every%20deep%0Alearning%20pipeline.%20Training%20algorithm%20improvements%20that%20speed%20up%20training%0Aacross%20a%20wide%20variety%20of%20workloads%20%28e.g.%2C%20better%20update%20rules%2C%20tuning%0Aprotocols%2C%20learning%20rate%20schedules%2C%20or%20data%20selection%20schemes%29%20could%20save%20time%2C%0Asave%20computational%20resources%2C%20and%20lead%20to%20better%2C%20more%20accurate%2C%20models.%0AUnfortunately%2C%20as%20a%20community%2C%20we%20are%20currently%20unable%20to%20reliably%20identify%0Atraining%20algorithm%20improvements%2C%20or%20even%20determine%20the%20state-of-the-art%0Atraining%20algorithm.%20In%20this%20work%2C%20using%20concrete%20experiments%2C%20we%20argue%20that%0Areal%20progress%20in%20speeding%20up%20training%20requires%20new%20benchmarks%20that%20resolve%0Athree%20basic%20challenges%20faced%20by%20empirical%20comparisons%20of%20training%20algorithms%3A%0A%281%29%20how%20to%20decide%20when%20training%20is%20complete%20and%20precisely%20measure%20training%0Atime%2C%20%282%29%20how%20to%20handle%20the%20sensitivity%20of%20measurements%20to%20exact%20workload%0Adetails%2C%20and%20%283%29%20how%20to%20fairly%20compare%20algorithms%20that%20require%20hyperparameter%0Atuning.%20In%20order%20to%20address%20these%20challenges%2C%20we%20introduce%20a%20new%2C%20competitive%2C%0Atime-to-result%20benchmark%20using%20multiple%20workloads%20running%20on%20fixed%20hardware%2C%0Athe%20AlgoPerf%3A%20Training%20Algorithms%20benchmark.%20Our%20benchmark%20includes%20a%20set%20of%0Aworkload%20variants%20that%20make%20it%20possible%20to%20detect%20benchmark%20submissions%20that%0Aare%20more%20robust%20to%20workload%20changes%20than%20current%20widely-used%20methods.%20Finally%2C%0Awe%20evaluate%20baseline%20submissions%20constructed%20using%20various%20optimizers%20that%0Arepresent%20current%20practice%2C%20as%20well%20as%20other%20optimizers%20that%20have%20recently%0Areceived%20attention%20in%20the%20literature.%20These%20baseline%20results%20collectively%0Ademonstrate%20the%20feasibility%20of%20our%20benchmark%2C%20show%20that%20non-trivial%20gaps%0Abetween%20methods%20exist%2C%20and%20set%20a%20provisional%20state-of-the-art%20for%20future%0Abenchmark%20submissions%20to%20try%20and%20surpass.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07179v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Neural%2520Network%2520Training%2520Algorithms%26entry.906535625%3DGeorge%2520E.%2520Dahl%2520and%2520Frank%2520Schneider%2520and%2520Zachary%2520Nado%2520and%2520Naman%2520Agarwal%2520and%2520Chandramouli%2520Shama%2520Sastry%2520and%2520Philipp%2520Hennig%2520and%2520Sourabh%2520Medapati%2520and%2520Runa%2520Eschenhagen%2520and%2520Priya%2520Kasimbeg%2520and%2520Daniel%2520Suo%2520and%2520Juhan%2520Bae%2520and%2520Justin%2520Gilmer%2520and%2520Abel%2520L.%2520Peirson%2520and%2520Bilal%2520Khan%2520and%2520Rohan%2520Anil%2520and%2520Mike%2520Rabbat%2520and%2520Shankar%2520Krishnan%2520and%2520Daniel%2520Snider%2520and%2520Ehsan%2520Amid%2520and%2520Kongtao%2520Chen%2520and%2520Chris%2520J.%2520Maddison%2520and%2520Rakshith%2520Vasudev%2520and%2520Michal%2520Badura%2520and%2520Ankush%2520Garg%2520and%2520Peter%2520Mattson%26entry.1292438233%3D%2520%2520Training%2520algorithms%252C%2520broadly%2520construed%252C%2520are%2520an%2520essential%2520part%2520of%2520every%2520deep%250Alearning%2520pipeline.%2520Training%2520algorithm%2520improvements%2520that%2520speed%2520up%2520training%250Aacross%2520a%2520wide%2520variety%2520of%2520workloads%2520%2528e.g.%252C%2520better%2520update%2520rules%252C%2520tuning%250Aprotocols%252C%2520learning%2520rate%2520schedules%252C%2520or%2520data%2520selection%2520schemes%2529%2520could%2520save%2520time%252C%250Asave%2520computational%2520resources%252C%2520and%2520lead%2520to%2520better%252C%2520more%2520accurate%252C%2520models.%250AUnfortunately%252C%2520as%2520a%2520community%252C%2520we%2520are%2520currently%2520unable%2520to%2520reliably%2520identify%250Atraining%2520algorithm%2520improvements%252C%2520or%2520even%2520determine%2520the%2520state-of-the-art%250Atraining%2520algorithm.%2520In%2520this%2520work%252C%2520using%2520concrete%2520experiments%252C%2520we%2520argue%2520that%250Areal%2520progress%2520in%2520speeding%2520up%2520training%2520requires%2520new%2520benchmarks%2520that%2520resolve%250Athree%2520basic%2520challenges%2520faced%2520by%2520empirical%2520comparisons%2520of%2520training%2520algorithms%253A%250A%25281%2529%2520how%2520to%2520decide%2520when%2520training%2520is%2520complete%2520and%2520precisely%2520measure%2520training%250Atime%252C%2520%25282%2529%2520how%2520to%2520handle%2520the%2520sensitivity%2520of%2520measurements%2520to%2520exact%2520workload%250Adetails%252C%2520and%2520%25283%2529%2520how%2520to%2520fairly%2520compare%2520algorithms%2520that%2520require%2520hyperparameter%250Atuning.%2520In%2520order%2520to%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520new%252C%2520competitive%252C%250Atime-to-result%2520benchmark%2520using%2520multiple%2520workloads%2520running%2520on%2520fixed%2520hardware%252C%250Athe%2520AlgoPerf%253A%2520Training%2520Algorithms%2520benchmark.%2520Our%2520benchmark%2520includes%2520a%2520set%2520of%250Aworkload%2520variants%2520that%2520make%2520it%2520possible%2520to%2520detect%2520benchmark%2520submissions%2520that%250Aare%2520more%2520robust%2520to%2520workload%2520changes%2520than%2520current%2520widely-used%2520methods.%2520Finally%252C%250Awe%2520evaluate%2520baseline%2520submissions%2520constructed%2520using%2520various%2520optimizers%2520that%250Arepresent%2520current%2520practice%252C%2520as%2520well%2520as%2520other%2520optimizers%2520that%2520have%2520recently%250Areceived%2520attention%2520in%2520the%2520literature.%2520These%2520baseline%2520results%2520collectively%250Ademonstrate%2520the%2520feasibility%2520of%2520our%2520benchmark%252C%2520show%2520that%2520non-trivial%2520gaps%250Abetween%2520methods%2520exist%252C%2520and%2520set%2520a%2520provisional%2520state-of-the-art%2520for%2520future%250Abenchmark%2520submissions%2520to%2520try%2520and%2520surpass.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07179v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Neural%20Network%20Training%20Algorithms&entry.906535625=George%20E.%20Dahl%20and%20Frank%20Schneider%20and%20Zachary%20Nado%20and%20Naman%20Agarwal%20and%20Chandramouli%20Shama%20Sastry%20and%20Philipp%20Hennig%20and%20Sourabh%20Medapati%20and%20Runa%20Eschenhagen%20and%20Priya%20Kasimbeg%20and%20Daniel%20Suo%20and%20Juhan%20Bae%20and%20Justin%20Gilmer%20and%20Abel%20L.%20Peirson%20and%20Bilal%20Khan%20and%20Rohan%20Anil%20and%20Mike%20Rabbat%20and%20Shankar%20Krishnan%20and%20Daniel%20Snider%20and%20Ehsan%20Amid%20and%20Kongtao%20Chen%20and%20Chris%20J.%20Maddison%20and%20Rakshith%20Vasudev%20and%20Michal%20Badura%20and%20Ankush%20Garg%20and%20Peter%20Mattson&entry.1292438233=%20%20Training%20algorithms%2C%20broadly%20construed%2C%20are%20an%20essential%20part%20of%20every%20deep%0Alearning%20pipeline.%20Training%20algorithm%20improvements%20that%20speed%20up%20training%0Aacross%20a%20wide%20variety%20of%20workloads%20%28e.g.%2C%20better%20update%20rules%2C%20tuning%0Aprotocols%2C%20learning%20rate%20schedules%2C%20or%20data%20selection%20schemes%29%20could%20save%20time%2C%0Asave%20computational%20resources%2C%20and%20lead%20to%20better%2C%20more%20accurate%2C%20models.%0AUnfortunately%2C%20as%20a%20community%2C%20we%20are%20currently%20unable%20to%20reliably%20identify%0Atraining%20algorithm%20improvements%2C%20or%20even%20determine%20the%20state-of-the-art%0Atraining%20algorithm.%20In%20this%20work%2C%20using%20concrete%20experiments%2C%20we%20argue%20that%0Areal%20progress%20in%20speeding%20up%20training%20requires%20new%20benchmarks%20that%20resolve%0Athree%20basic%20challenges%20faced%20by%20empirical%20comparisons%20of%20training%20algorithms%3A%0A%281%29%20how%20to%20decide%20when%20training%20is%20complete%20and%20precisely%20measure%20training%0Atime%2C%20%282%29%20how%20to%20handle%20the%20sensitivity%20of%20measurements%20to%20exact%20workload%0Adetails%2C%20and%20%283%29%20how%20to%20fairly%20compare%20algorithms%20that%20require%20hyperparameter%0Atuning.%20In%20order%20to%20address%20these%20challenges%2C%20we%20introduce%20a%20new%2C%20competitive%2C%0Atime-to-result%20benchmark%20using%20multiple%20workloads%20running%20on%20fixed%20hardware%2C%0Athe%20AlgoPerf%3A%20Training%20Algorithms%20benchmark.%20Our%20benchmark%20includes%20a%20set%20of%0Aworkload%20variants%20that%20make%20it%20possible%20to%20detect%20benchmark%20submissions%20that%0Aare%20more%20robust%20to%20workload%20changes%20than%20current%20widely-used%20methods.%20Finally%2C%0Awe%20evaluate%20baseline%20submissions%20constructed%20using%20various%20optimizers%20that%0Arepresent%20current%20practice%2C%20as%20well%20as%20other%20optimizers%20that%20have%20recently%0Areceived%20attention%20in%20the%20literature.%20These%20baseline%20results%20collectively%0Ademonstrate%20the%20feasibility%20of%20our%20benchmark%2C%20show%20that%20non-trivial%20gaps%0Abetween%20methods%20exist%2C%20and%20set%20a%20provisional%20state-of-the-art%20for%20future%0Abenchmark%20submissions%20to%20try%20and%20surpass.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07179v2&entry.124074799=Read"},
{"title": "FindingDory: A Benchmark to Evaluate Memory in Embodied Agents", "author": "Karmesh Yadav and Yusuf Ali and Gunshi Gupta and Yarin Gal and Zsolt Kira", "abstract": "  Large vision-language models have recently demonstrated impressive\nperformance in planning and control tasks, driving interest in their\napplication to real-world robotics. However, deploying these models for\nreasoning in embodied contexts is limited by their ability to incorporate\nlong-term experience collected across multiple days and represented by vast\ncollections of images. Current VLMs typically struggle to process more than a\nfew hundred images concurrently, highlighting the need for more efficient\nmechanisms to handle long-term memory in embodied settings. To effectively\nevaluate these models for long-horizon control, a benchmark must specifically\ntarget scenarios where memory is crucial for success. Existing long-video QA\nbenchmarks overlook embodied challenges like object manipulation and\nnavigation, which demand low-level skills and fine-grained reasoning over past\ninteractions. Moreover, effective memory integration in embodied agents\ninvolves both recalling relevant historical information and executing actions\nbased on that information, making it essential to study these aspects together\nrather than in isolation. In this work, we introduce a new benchmark for\nlong-range embodied tasks in the Habitat simulator. This benchmark evaluates\nmemory-based capabilities across 60 tasks requiring sustained engagement and\ncontextual awareness in an environment. The tasks can also be procedurally\nextended to longer and more challenging versions, enabling scalable evaluation\nof memory and reasoning. We also present baselines that integrate\nstate-of-the-art VLMs with low level navigation policies, assessing their\nperformance on these memory-intensive tasks and highlight areas for\nimprovement.\n", "link": "http://arxiv.org/abs/2506.15635v1", "date": "2025-06-18", "relevancy": 2.2973, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5789}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5789}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FindingDory%3A%20A%20Benchmark%20to%20Evaluate%20Memory%20in%20Embodied%20Agents&body=Title%3A%20FindingDory%3A%20A%20Benchmark%20to%20Evaluate%20Memory%20in%20Embodied%20Agents%0AAuthor%3A%20Karmesh%20Yadav%20and%20Yusuf%20Ali%20and%20Gunshi%20Gupta%20and%20Yarin%20Gal%20and%20Zsolt%20Kira%0AAbstract%3A%20%20%20Large%20vision-language%20models%20have%20recently%20demonstrated%20impressive%0Aperformance%20in%20planning%20and%20control%20tasks%2C%20driving%20interest%20in%20their%0Aapplication%20to%20real-world%20robotics.%20However%2C%20deploying%20these%20models%20for%0Areasoning%20in%20embodied%20contexts%20is%20limited%20by%20their%20ability%20to%20incorporate%0Along-term%20experience%20collected%20across%20multiple%20days%20and%20represented%20by%20vast%0Acollections%20of%20images.%20Current%20VLMs%20typically%20struggle%20to%20process%20more%20than%20a%0Afew%20hundred%20images%20concurrently%2C%20highlighting%20the%20need%20for%20more%20efficient%0Amechanisms%20to%20handle%20long-term%20memory%20in%20embodied%20settings.%20To%20effectively%0Aevaluate%20these%20models%20for%20long-horizon%20control%2C%20a%20benchmark%20must%20specifically%0Atarget%20scenarios%20where%20memory%20is%20crucial%20for%20success.%20Existing%20long-video%20QA%0Abenchmarks%20overlook%20embodied%20challenges%20like%20object%20manipulation%20and%0Anavigation%2C%20which%20demand%20low-level%20skills%20and%20fine-grained%20reasoning%20over%20past%0Ainteractions.%20Moreover%2C%20effective%20memory%20integration%20in%20embodied%20agents%0Ainvolves%20both%20recalling%20relevant%20historical%20information%20and%20executing%20actions%0Abased%20on%20that%20information%2C%20making%20it%20essential%20to%20study%20these%20aspects%20together%0Arather%20than%20in%20isolation.%20In%20this%20work%2C%20we%20introduce%20a%20new%20benchmark%20for%0Along-range%20embodied%20tasks%20in%20the%20Habitat%20simulator.%20This%20benchmark%20evaluates%0Amemory-based%20capabilities%20across%2060%20tasks%20requiring%20sustained%20engagement%20and%0Acontextual%20awareness%20in%20an%20environment.%20The%20tasks%20can%20also%20be%20procedurally%0Aextended%20to%20longer%20and%20more%20challenging%20versions%2C%20enabling%20scalable%20evaluation%0Aof%20memory%20and%20reasoning.%20We%20also%20present%20baselines%20that%20integrate%0Astate-of-the-art%20VLMs%20with%20low%20level%20navigation%20policies%2C%20assessing%20their%0Aperformance%20on%20these%20memory-intensive%20tasks%20and%20highlight%20areas%20for%0Aimprovement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFindingDory%253A%2520A%2520Benchmark%2520to%2520Evaluate%2520Memory%2520in%2520Embodied%2520Agents%26entry.906535625%3DKarmesh%2520Yadav%2520and%2520Yusuf%2520Ali%2520and%2520Gunshi%2520Gupta%2520and%2520Yarin%2520Gal%2520and%2520Zsolt%2520Kira%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520have%2520recently%2520demonstrated%2520impressive%250Aperformance%2520in%2520planning%2520and%2520control%2520tasks%252C%2520driving%2520interest%2520in%2520their%250Aapplication%2520to%2520real-world%2520robotics.%2520However%252C%2520deploying%2520these%2520models%2520for%250Areasoning%2520in%2520embodied%2520contexts%2520is%2520limited%2520by%2520their%2520ability%2520to%2520incorporate%250Along-term%2520experience%2520collected%2520across%2520multiple%2520days%2520and%2520represented%2520by%2520vast%250Acollections%2520of%2520images.%2520Current%2520VLMs%2520typically%2520struggle%2520to%2520process%2520more%2520than%2520a%250Afew%2520hundred%2520images%2520concurrently%252C%2520highlighting%2520the%2520need%2520for%2520more%2520efficient%250Amechanisms%2520to%2520handle%2520long-term%2520memory%2520in%2520embodied%2520settings.%2520To%2520effectively%250Aevaluate%2520these%2520models%2520for%2520long-horizon%2520control%252C%2520a%2520benchmark%2520must%2520specifically%250Atarget%2520scenarios%2520where%2520memory%2520is%2520crucial%2520for%2520success.%2520Existing%2520long-video%2520QA%250Abenchmarks%2520overlook%2520embodied%2520challenges%2520like%2520object%2520manipulation%2520and%250Anavigation%252C%2520which%2520demand%2520low-level%2520skills%2520and%2520fine-grained%2520reasoning%2520over%2520past%250Ainteractions.%2520Moreover%252C%2520effective%2520memory%2520integration%2520in%2520embodied%2520agents%250Ainvolves%2520both%2520recalling%2520relevant%2520historical%2520information%2520and%2520executing%2520actions%250Abased%2520on%2520that%2520information%252C%2520making%2520it%2520essential%2520to%2520study%2520these%2520aspects%2520together%250Arather%2520than%2520in%2520isolation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520for%250Along-range%2520embodied%2520tasks%2520in%2520the%2520Habitat%2520simulator.%2520This%2520benchmark%2520evaluates%250Amemory-based%2520capabilities%2520across%252060%2520tasks%2520requiring%2520sustained%2520engagement%2520and%250Acontextual%2520awareness%2520in%2520an%2520environment.%2520The%2520tasks%2520can%2520also%2520be%2520procedurally%250Aextended%2520to%2520longer%2520and%2520more%2520challenging%2520versions%252C%2520enabling%2520scalable%2520evaluation%250Aof%2520memory%2520and%2520reasoning.%2520We%2520also%2520present%2520baselines%2520that%2520integrate%250Astate-of-the-art%2520VLMs%2520with%2520low%2520level%2520navigation%2520policies%252C%2520assessing%2520their%250Aperformance%2520on%2520these%2520memory-intensive%2520tasks%2520and%2520highlight%2520areas%2520for%250Aimprovement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FindingDory%3A%20A%20Benchmark%20to%20Evaluate%20Memory%20in%20Embodied%20Agents&entry.906535625=Karmesh%20Yadav%20and%20Yusuf%20Ali%20and%20Gunshi%20Gupta%20and%20Yarin%20Gal%20and%20Zsolt%20Kira&entry.1292438233=%20%20Large%20vision-language%20models%20have%20recently%20demonstrated%20impressive%0Aperformance%20in%20planning%20and%20control%20tasks%2C%20driving%20interest%20in%20their%0Aapplication%20to%20real-world%20robotics.%20However%2C%20deploying%20these%20models%20for%0Areasoning%20in%20embodied%20contexts%20is%20limited%20by%20their%20ability%20to%20incorporate%0Along-term%20experience%20collected%20across%20multiple%20days%20and%20represented%20by%20vast%0Acollections%20of%20images.%20Current%20VLMs%20typically%20struggle%20to%20process%20more%20than%20a%0Afew%20hundred%20images%20concurrently%2C%20highlighting%20the%20need%20for%20more%20efficient%0Amechanisms%20to%20handle%20long-term%20memory%20in%20embodied%20settings.%20To%20effectively%0Aevaluate%20these%20models%20for%20long-horizon%20control%2C%20a%20benchmark%20must%20specifically%0Atarget%20scenarios%20where%20memory%20is%20crucial%20for%20success.%20Existing%20long-video%20QA%0Abenchmarks%20overlook%20embodied%20challenges%20like%20object%20manipulation%20and%0Anavigation%2C%20which%20demand%20low-level%20skills%20and%20fine-grained%20reasoning%20over%20past%0Ainteractions.%20Moreover%2C%20effective%20memory%20integration%20in%20embodied%20agents%0Ainvolves%20both%20recalling%20relevant%20historical%20information%20and%20executing%20actions%0Abased%20on%20that%20information%2C%20making%20it%20essential%20to%20study%20these%20aspects%20together%0Arather%20than%20in%20isolation.%20In%20this%20work%2C%20we%20introduce%20a%20new%20benchmark%20for%0Along-range%20embodied%20tasks%20in%20the%20Habitat%20simulator.%20This%20benchmark%20evaluates%0Amemory-based%20capabilities%20across%2060%20tasks%20requiring%20sustained%20engagement%20and%0Acontextual%20awareness%20in%20an%20environment.%20The%20tasks%20can%20also%20be%20procedurally%0Aextended%20to%20longer%20and%20more%20challenging%20versions%2C%20enabling%20scalable%20evaluation%0Aof%20memory%20and%20reasoning.%20We%20also%20present%20baselines%20that%20integrate%0Astate-of-the-art%20VLMs%20with%20low%20level%20navigation%20policies%2C%20assessing%20their%0Aperformance%20on%20these%20memory-intensive%20tasks%20and%20highlight%20areas%20for%0Aimprovement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15635v1&entry.124074799=Read"},
{"title": "A Bird Song Detector for improving bird identification through Deep\n  Learning: a case study from Do\u00f1ana", "author": "Alba M\u00e1rquez-Rodr\u00edguez and Miguel \u00c1ngel Mohedano-Munoz and Manuel J. Mar\u00edn-Jim\u00e9nez and Eduardo Santamar\u00eda-Garc\u00eda and Giulia Bastianelli and Pedro Jordano and Irene Mendoza", "abstract": "  Passive Acoustic Monitoring is a key tool for biodiversity conservation, but\nthe large volumes of unsupervised audio it generates present major challenges\nfor extracting meaningful information. Deep Learning offers promising\nsolutions. BirdNET, a widely used bird identification model, has shown success\nin many study systems but is limited at local scale due to biases in its\ntraining data, which focus on specific locations and target sounds rather than\nentire soundscapes. A key challenge in bird species identification is that many\nrecordings either lack target species or contain overlapping vocalizations,\ncomplicating automatic identification. To address these problems, we developed\na multi-stage pipeline for automatic bird vocalization identification in\nDo\\~nana National Park (SW Spain), a wetland of high conservation concern. We\ndeployed AudioMoth recorders in three main habitats across nine locations and\nmanually annotated 461 minutes of audio, resulting in 3749 labeled segments\nspanning 34 classes. We first applied a Bird Song Detector to isolate bird\nvocalizations using spectrogram-based image processing. Then, species were\nclassified using custom models trained at the local scale. Applying the Bird\nSong Detector before classification improved species identification, as all\nmodels performed better when analyzing only the segments where birds were\ndetected. Specifically, the combination of detector and fine-tuned BirdNET\noutperformed the baseline without detection. This approach demonstrates the\neffectiveness of integrating a Bird Song Detector with local classification\nmodels. These findings highlight the need to adapt general-purpose tools to\nspecific ecological challenges. Automatically detecting bird species helps\ntrack the health of this threatened ecosystem, given birds sensitivity to\nenvironmental change, and supports conservation planning to reduce biodiversity\nloss.\n", "link": "http://arxiv.org/abs/2503.15576v2", "date": "2025-06-18", "relevancy": 2.2962, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4753}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bird%20Song%20Detector%20for%20improving%20bird%20identification%20through%20Deep%0A%20%20Learning%3A%20a%20case%20study%20from%20Do%C3%B1ana&body=Title%3A%20A%20Bird%20Song%20Detector%20for%20improving%20bird%20identification%20through%20Deep%0A%20%20Learning%3A%20a%20case%20study%20from%20Do%C3%B1ana%0AAuthor%3A%20Alba%20M%C3%A1rquez-Rodr%C3%ADguez%20and%20Miguel%20%C3%81ngel%20Mohedano-Munoz%20and%20Manuel%20J.%20Mar%C3%ADn-Jim%C3%A9nez%20and%20Eduardo%20Santamar%C3%ADa-Garc%C3%ADa%20and%20Giulia%20Bastianelli%20and%20Pedro%20Jordano%20and%20Irene%20Mendoza%0AAbstract%3A%20%20%20Passive%20Acoustic%20Monitoring%20is%20a%20key%20tool%20for%20biodiversity%20conservation%2C%20but%0Athe%20large%20volumes%20of%20unsupervised%20audio%20it%20generates%20present%20major%20challenges%0Afor%20extracting%20meaningful%20information.%20Deep%20Learning%20offers%20promising%0Asolutions.%20BirdNET%2C%20a%20widely%20used%20bird%20identification%20model%2C%20has%20shown%20success%0Ain%20many%20study%20systems%20but%20is%20limited%20at%20local%20scale%20due%20to%20biases%20in%20its%0Atraining%20data%2C%20which%20focus%20on%20specific%20locations%20and%20target%20sounds%20rather%20than%0Aentire%20soundscapes.%20A%20key%20challenge%20in%20bird%20species%20identification%20is%20that%20many%0Arecordings%20either%20lack%20target%20species%20or%20contain%20overlapping%20vocalizations%2C%0Acomplicating%20automatic%20identification.%20To%20address%20these%20problems%2C%20we%20developed%0Aa%20multi-stage%20pipeline%20for%20automatic%20bird%20vocalization%20identification%20in%0ADo%5C~nana%20National%20Park%20%28SW%20Spain%29%2C%20a%20wetland%20of%20high%20conservation%20concern.%20We%0Adeployed%20AudioMoth%20recorders%20in%20three%20main%20habitats%20across%20nine%20locations%20and%0Amanually%20annotated%20461%20minutes%20of%20audio%2C%20resulting%20in%203749%20labeled%20segments%0Aspanning%2034%20classes.%20We%20first%20applied%20a%20Bird%20Song%20Detector%20to%20isolate%20bird%0Avocalizations%20using%20spectrogram-based%20image%20processing.%20Then%2C%20species%20were%0Aclassified%20using%20custom%20models%20trained%20at%20the%20local%20scale.%20Applying%20the%20Bird%0ASong%20Detector%20before%20classification%20improved%20species%20identification%2C%20as%20all%0Amodels%20performed%20better%20when%20analyzing%20only%20the%20segments%20where%20birds%20were%0Adetected.%20Specifically%2C%20the%20combination%20of%20detector%20and%20fine-tuned%20BirdNET%0Aoutperformed%20the%20baseline%20without%20detection.%20This%20approach%20demonstrates%20the%0Aeffectiveness%20of%20integrating%20a%20Bird%20Song%20Detector%20with%20local%20classification%0Amodels.%20These%20findings%20highlight%20the%20need%20to%20adapt%20general-purpose%20tools%20to%0Aspecific%20ecological%20challenges.%20Automatically%20detecting%20bird%20species%20helps%0Atrack%20the%20health%20of%20this%20threatened%20ecosystem%2C%20given%20birds%20sensitivity%20to%0Aenvironmental%20change%2C%20and%20supports%20conservation%20planning%20to%20reduce%20biodiversity%0Aloss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bird%2520Song%2520Detector%2520for%2520improving%2520bird%2520identification%2520through%2520Deep%250A%2520%2520Learning%253A%2520a%2520case%2520study%2520from%2520Do%25C3%25B1ana%26entry.906535625%3DAlba%2520M%25C3%25A1rquez-Rodr%25C3%25ADguez%2520and%2520Miguel%2520%25C3%2581ngel%2520Mohedano-Munoz%2520and%2520Manuel%2520J.%2520Mar%25C3%25ADn-Jim%25C3%25A9nez%2520and%2520Eduardo%2520Santamar%25C3%25ADa-Garc%25C3%25ADa%2520and%2520Giulia%2520Bastianelli%2520and%2520Pedro%2520Jordano%2520and%2520Irene%2520Mendoza%26entry.1292438233%3D%2520%2520Passive%2520Acoustic%2520Monitoring%2520is%2520a%2520key%2520tool%2520for%2520biodiversity%2520conservation%252C%2520but%250Athe%2520large%2520volumes%2520of%2520unsupervised%2520audio%2520it%2520generates%2520present%2520major%2520challenges%250Afor%2520extracting%2520meaningful%2520information.%2520Deep%2520Learning%2520offers%2520promising%250Asolutions.%2520BirdNET%252C%2520a%2520widely%2520used%2520bird%2520identification%2520model%252C%2520has%2520shown%2520success%250Ain%2520many%2520study%2520systems%2520but%2520is%2520limited%2520at%2520local%2520scale%2520due%2520to%2520biases%2520in%2520its%250Atraining%2520data%252C%2520which%2520focus%2520on%2520specific%2520locations%2520and%2520target%2520sounds%2520rather%2520than%250Aentire%2520soundscapes.%2520A%2520key%2520challenge%2520in%2520bird%2520species%2520identification%2520is%2520that%2520many%250Arecordings%2520either%2520lack%2520target%2520species%2520or%2520contain%2520overlapping%2520vocalizations%252C%250Acomplicating%2520automatic%2520identification.%2520To%2520address%2520these%2520problems%252C%2520we%2520developed%250Aa%2520multi-stage%2520pipeline%2520for%2520automatic%2520bird%2520vocalization%2520identification%2520in%250ADo%255C~nana%2520National%2520Park%2520%2528SW%2520Spain%2529%252C%2520a%2520wetland%2520of%2520high%2520conservation%2520concern.%2520We%250Adeployed%2520AudioMoth%2520recorders%2520in%2520three%2520main%2520habitats%2520across%2520nine%2520locations%2520and%250Amanually%2520annotated%2520461%2520minutes%2520of%2520audio%252C%2520resulting%2520in%25203749%2520labeled%2520segments%250Aspanning%252034%2520classes.%2520We%2520first%2520applied%2520a%2520Bird%2520Song%2520Detector%2520to%2520isolate%2520bird%250Avocalizations%2520using%2520spectrogram-based%2520image%2520processing.%2520Then%252C%2520species%2520were%250Aclassified%2520using%2520custom%2520models%2520trained%2520at%2520the%2520local%2520scale.%2520Applying%2520the%2520Bird%250ASong%2520Detector%2520before%2520classification%2520improved%2520species%2520identification%252C%2520as%2520all%250Amodels%2520performed%2520better%2520when%2520analyzing%2520only%2520the%2520segments%2520where%2520birds%2520were%250Adetected.%2520Specifically%252C%2520the%2520combination%2520of%2520detector%2520and%2520fine-tuned%2520BirdNET%250Aoutperformed%2520the%2520baseline%2520without%2520detection.%2520This%2520approach%2520demonstrates%2520the%250Aeffectiveness%2520of%2520integrating%2520a%2520Bird%2520Song%2520Detector%2520with%2520local%2520classification%250Amodels.%2520These%2520findings%2520highlight%2520the%2520need%2520to%2520adapt%2520general-purpose%2520tools%2520to%250Aspecific%2520ecological%2520challenges.%2520Automatically%2520detecting%2520bird%2520species%2520helps%250Atrack%2520the%2520health%2520of%2520this%2520threatened%2520ecosystem%252C%2520given%2520birds%2520sensitivity%2520to%250Aenvironmental%2520change%252C%2520and%2520supports%2520conservation%2520planning%2520to%2520reduce%2520biodiversity%250Aloss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bird%20Song%20Detector%20for%20improving%20bird%20identification%20through%20Deep%0A%20%20Learning%3A%20a%20case%20study%20from%20Do%C3%B1ana&entry.906535625=Alba%20M%C3%A1rquez-Rodr%C3%ADguez%20and%20Miguel%20%C3%81ngel%20Mohedano-Munoz%20and%20Manuel%20J.%20Mar%C3%ADn-Jim%C3%A9nez%20and%20Eduardo%20Santamar%C3%ADa-Garc%C3%ADa%20and%20Giulia%20Bastianelli%20and%20Pedro%20Jordano%20and%20Irene%20Mendoza&entry.1292438233=%20%20Passive%20Acoustic%20Monitoring%20is%20a%20key%20tool%20for%20biodiversity%20conservation%2C%20but%0Athe%20large%20volumes%20of%20unsupervised%20audio%20it%20generates%20present%20major%20challenges%0Afor%20extracting%20meaningful%20information.%20Deep%20Learning%20offers%20promising%0Asolutions.%20BirdNET%2C%20a%20widely%20used%20bird%20identification%20model%2C%20has%20shown%20success%0Ain%20many%20study%20systems%20but%20is%20limited%20at%20local%20scale%20due%20to%20biases%20in%20its%0Atraining%20data%2C%20which%20focus%20on%20specific%20locations%20and%20target%20sounds%20rather%20than%0Aentire%20soundscapes.%20A%20key%20challenge%20in%20bird%20species%20identification%20is%20that%20many%0Arecordings%20either%20lack%20target%20species%20or%20contain%20overlapping%20vocalizations%2C%0Acomplicating%20automatic%20identification.%20To%20address%20these%20problems%2C%20we%20developed%0Aa%20multi-stage%20pipeline%20for%20automatic%20bird%20vocalization%20identification%20in%0ADo%5C~nana%20National%20Park%20%28SW%20Spain%29%2C%20a%20wetland%20of%20high%20conservation%20concern.%20We%0Adeployed%20AudioMoth%20recorders%20in%20three%20main%20habitats%20across%20nine%20locations%20and%0Amanually%20annotated%20461%20minutes%20of%20audio%2C%20resulting%20in%203749%20labeled%20segments%0Aspanning%2034%20classes.%20We%20first%20applied%20a%20Bird%20Song%20Detector%20to%20isolate%20bird%0Avocalizations%20using%20spectrogram-based%20image%20processing.%20Then%2C%20species%20were%0Aclassified%20using%20custom%20models%20trained%20at%20the%20local%20scale.%20Applying%20the%20Bird%0ASong%20Detector%20before%20classification%20improved%20species%20identification%2C%20as%20all%0Amodels%20performed%20better%20when%20analyzing%20only%20the%20segments%20where%20birds%20were%0Adetected.%20Specifically%2C%20the%20combination%20of%20detector%20and%20fine-tuned%20BirdNET%0Aoutperformed%20the%20baseline%20without%20detection.%20This%20approach%20demonstrates%20the%0Aeffectiveness%20of%20integrating%20a%20Bird%20Song%20Detector%20with%20local%20classification%0Amodels.%20These%20findings%20highlight%20the%20need%20to%20adapt%20general-purpose%20tools%20to%0Aspecific%20ecological%20challenges.%20Automatically%20detecting%20bird%20species%20helps%0Atrack%20the%20health%20of%20this%20threatened%20ecosystem%2C%20given%20birds%20sensitivity%20to%0Aenvironmental%20change%2C%20and%20supports%20conservation%20planning%20to%20reduce%20biodiversity%0Aloss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15576v2&entry.124074799=Read"},
{"title": "Blockchain-Enabled Variational Information Bottleneck for Data\n  Extraction Based on Mutual Information in Internet of Vehicles", "author": "Cui Zhang and Wenjun Zhang and Qiong Wu and Pingyi Fan and Nan Cheng and Wen Chen and Khaled B. Letaief", "abstract": "  The Internet of Vehicles (IoV) network can address the issue of limited\ncomputing resources and data processing capabilities of individual vehicles,\nbut it also brings the risk of privacy leakage to vehicle users. Applying\nblockchain technology can establish secure data links within the IoV, solving\nthe problems of insufficient computing resources for each vehicle and the\nsecurity of data transmission over the network. However, with the development\nof the IoV, the amount of data interaction between multiple vehicles and\nbetween vehicles and base stations, roadside units, etc., is continuously\nincreasing. There is a need to further reduce the interaction volume, and\nintelligent data compression is key to solving this problem. The VIB technique\nfacilitates the training of encoding and decoding models, substantially\ndiminishing the volume of data that needs to be transmitted. This paper\nintroduces an innovative approach that integrates blockchain with VIB, referred\nto as BVIB, designed to lighten computational workloads and reinforce the\nsecurity of the network. We first construct a new network framework by\nseparating the encoding and decoding networks to address the computational\nburden issue, and then propose a new algorithm to enhance the security of IoV\nnetworks. We also discuss the impact of the data extraction rate on system\nlatency to determine the most suitable data extraction rate. An experimental\nframework combining Python and C++ has been established to substantiate the\nefficacy of our BVIB approach. Comprehensive simulation studies indicate that\nthe BVIB consistently excels in comparison to alternative foundational\nmethodologies.\n", "link": "http://arxiv.org/abs/2409.17287v2", "date": "2025-06-18", "relevancy": 2.2897, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4724}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4557}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blockchain-Enabled%20Variational%20Information%20Bottleneck%20for%20Data%0A%20%20Extraction%20Based%20on%20Mutual%20Information%20in%20Internet%20of%20Vehicles&body=Title%3A%20Blockchain-Enabled%20Variational%20Information%20Bottleneck%20for%20Data%0A%20%20Extraction%20Based%20on%20Mutual%20Information%20in%20Internet%20of%20Vehicles%0AAuthor%3A%20Cui%20Zhang%20and%20Wenjun%20Zhang%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Nan%20Cheng%20and%20Wen%20Chen%20and%20Khaled%20B.%20Letaief%0AAbstract%3A%20%20%20The%20Internet%20of%20Vehicles%20%28IoV%29%20network%20can%20address%20the%20issue%20of%20limited%0Acomputing%20resources%20and%20data%20processing%20capabilities%20of%20individual%20vehicles%2C%0Abut%20it%20also%20brings%20the%20risk%20of%20privacy%20leakage%20to%20vehicle%20users.%20Applying%0Ablockchain%20technology%20can%20establish%20secure%20data%20links%20within%20the%20IoV%2C%20solving%0Athe%20problems%20of%20insufficient%20computing%20resources%20for%20each%20vehicle%20and%20the%0Asecurity%20of%20data%20transmission%20over%20the%20network.%20However%2C%20with%20the%20development%0Aof%20the%20IoV%2C%20the%20amount%20of%20data%20interaction%20between%20multiple%20vehicles%20and%0Abetween%20vehicles%20and%20base%20stations%2C%20roadside%20units%2C%20etc.%2C%20is%20continuously%0Aincreasing.%20There%20is%20a%20need%20to%20further%20reduce%20the%20interaction%20volume%2C%20and%0Aintelligent%20data%20compression%20is%20key%20to%20solving%20this%20problem.%20The%20VIB%20technique%0Afacilitates%20the%20training%20of%20encoding%20and%20decoding%20models%2C%20substantially%0Adiminishing%20the%20volume%20of%20data%20that%20needs%20to%20be%20transmitted.%20This%20paper%0Aintroduces%20an%20innovative%20approach%20that%20integrates%20blockchain%20with%20VIB%2C%20referred%0Ato%20as%20BVIB%2C%20designed%20to%20lighten%20computational%20workloads%20and%20reinforce%20the%0Asecurity%20of%20the%20network.%20We%20first%20construct%20a%20new%20network%20framework%20by%0Aseparating%20the%20encoding%20and%20decoding%20networks%20to%20address%20the%20computational%0Aburden%20issue%2C%20and%20then%20propose%20a%20new%20algorithm%20to%20enhance%20the%20security%20of%20IoV%0Anetworks.%20We%20also%20discuss%20the%20impact%20of%20the%20data%20extraction%20rate%20on%20system%0Alatency%20to%20determine%20the%20most%20suitable%20data%20extraction%20rate.%20An%20experimental%0Aframework%20combining%20Python%20and%20C%2B%2B%20has%20been%20established%20to%20substantiate%20the%0Aefficacy%20of%20our%20BVIB%20approach.%20Comprehensive%20simulation%20studies%20indicate%20that%0Athe%20BVIB%20consistently%20excels%20in%20comparison%20to%20alternative%20foundational%0Amethodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17287v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlockchain-Enabled%2520Variational%2520Information%2520Bottleneck%2520for%2520Data%250A%2520%2520Extraction%2520Based%2520on%2520Mutual%2520Information%2520in%2520Internet%2520of%2520Vehicles%26entry.906535625%3DCui%2520Zhang%2520and%2520Wenjun%2520Zhang%2520and%2520Qiong%2520Wu%2520and%2520Pingyi%2520Fan%2520and%2520Nan%2520Cheng%2520and%2520Wen%2520Chen%2520and%2520Khaled%2520B.%2520Letaief%26entry.1292438233%3D%2520%2520The%2520Internet%2520of%2520Vehicles%2520%2528IoV%2529%2520network%2520can%2520address%2520the%2520issue%2520of%2520limited%250Acomputing%2520resources%2520and%2520data%2520processing%2520capabilities%2520of%2520individual%2520vehicles%252C%250Abut%2520it%2520also%2520brings%2520the%2520risk%2520of%2520privacy%2520leakage%2520to%2520vehicle%2520users.%2520Applying%250Ablockchain%2520technology%2520can%2520establish%2520secure%2520data%2520links%2520within%2520the%2520IoV%252C%2520solving%250Athe%2520problems%2520of%2520insufficient%2520computing%2520resources%2520for%2520each%2520vehicle%2520and%2520the%250Asecurity%2520of%2520data%2520transmission%2520over%2520the%2520network.%2520However%252C%2520with%2520the%2520development%250Aof%2520the%2520IoV%252C%2520the%2520amount%2520of%2520data%2520interaction%2520between%2520multiple%2520vehicles%2520and%250Abetween%2520vehicles%2520and%2520base%2520stations%252C%2520roadside%2520units%252C%2520etc.%252C%2520is%2520continuously%250Aincreasing.%2520There%2520is%2520a%2520need%2520to%2520further%2520reduce%2520the%2520interaction%2520volume%252C%2520and%250Aintelligent%2520data%2520compression%2520is%2520key%2520to%2520solving%2520this%2520problem.%2520The%2520VIB%2520technique%250Afacilitates%2520the%2520training%2520of%2520encoding%2520and%2520decoding%2520models%252C%2520substantially%250Adiminishing%2520the%2520volume%2520of%2520data%2520that%2520needs%2520to%2520be%2520transmitted.%2520This%2520paper%250Aintroduces%2520an%2520innovative%2520approach%2520that%2520integrates%2520blockchain%2520with%2520VIB%252C%2520referred%250Ato%2520as%2520BVIB%252C%2520designed%2520to%2520lighten%2520computational%2520workloads%2520and%2520reinforce%2520the%250Asecurity%2520of%2520the%2520network.%2520We%2520first%2520construct%2520a%2520new%2520network%2520framework%2520by%250Aseparating%2520the%2520encoding%2520and%2520decoding%2520networks%2520to%2520address%2520the%2520computational%250Aburden%2520issue%252C%2520and%2520then%2520propose%2520a%2520new%2520algorithm%2520to%2520enhance%2520the%2520security%2520of%2520IoV%250Anetworks.%2520We%2520also%2520discuss%2520the%2520impact%2520of%2520the%2520data%2520extraction%2520rate%2520on%2520system%250Alatency%2520to%2520determine%2520the%2520most%2520suitable%2520data%2520extraction%2520rate.%2520An%2520experimental%250Aframework%2520combining%2520Python%2520and%2520C%252B%252B%2520has%2520been%2520established%2520to%2520substantiate%2520the%250Aefficacy%2520of%2520our%2520BVIB%2520approach.%2520Comprehensive%2520simulation%2520studies%2520indicate%2520that%250Athe%2520BVIB%2520consistently%2520excels%2520in%2520comparison%2520to%2520alternative%2520foundational%250Amethodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17287v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blockchain-Enabled%20Variational%20Information%20Bottleneck%20for%20Data%0A%20%20Extraction%20Based%20on%20Mutual%20Information%20in%20Internet%20of%20Vehicles&entry.906535625=Cui%20Zhang%20and%20Wenjun%20Zhang%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Nan%20Cheng%20and%20Wen%20Chen%20and%20Khaled%20B.%20Letaief&entry.1292438233=%20%20The%20Internet%20of%20Vehicles%20%28IoV%29%20network%20can%20address%20the%20issue%20of%20limited%0Acomputing%20resources%20and%20data%20processing%20capabilities%20of%20individual%20vehicles%2C%0Abut%20it%20also%20brings%20the%20risk%20of%20privacy%20leakage%20to%20vehicle%20users.%20Applying%0Ablockchain%20technology%20can%20establish%20secure%20data%20links%20within%20the%20IoV%2C%20solving%0Athe%20problems%20of%20insufficient%20computing%20resources%20for%20each%20vehicle%20and%20the%0Asecurity%20of%20data%20transmission%20over%20the%20network.%20However%2C%20with%20the%20development%0Aof%20the%20IoV%2C%20the%20amount%20of%20data%20interaction%20between%20multiple%20vehicles%20and%0Abetween%20vehicles%20and%20base%20stations%2C%20roadside%20units%2C%20etc.%2C%20is%20continuously%0Aincreasing.%20There%20is%20a%20need%20to%20further%20reduce%20the%20interaction%20volume%2C%20and%0Aintelligent%20data%20compression%20is%20key%20to%20solving%20this%20problem.%20The%20VIB%20technique%0Afacilitates%20the%20training%20of%20encoding%20and%20decoding%20models%2C%20substantially%0Adiminishing%20the%20volume%20of%20data%20that%20needs%20to%20be%20transmitted.%20This%20paper%0Aintroduces%20an%20innovative%20approach%20that%20integrates%20blockchain%20with%20VIB%2C%20referred%0Ato%20as%20BVIB%2C%20designed%20to%20lighten%20computational%20workloads%20and%20reinforce%20the%0Asecurity%20of%20the%20network.%20We%20first%20construct%20a%20new%20network%20framework%20by%0Aseparating%20the%20encoding%20and%20decoding%20networks%20to%20address%20the%20computational%0Aburden%20issue%2C%20and%20then%20propose%20a%20new%20algorithm%20to%20enhance%20the%20security%20of%20IoV%0Anetworks.%20We%20also%20discuss%20the%20impact%20of%20the%20data%20extraction%20rate%20on%20system%0Alatency%20to%20determine%20the%20most%20suitable%20data%20extraction%20rate.%20An%20experimental%0Aframework%20combining%20Python%20and%20C%2B%2B%20has%20been%20established%20to%20substantiate%20the%0Aefficacy%20of%20our%20BVIB%20approach.%20Comprehensive%20simulation%20studies%20indicate%20that%0Athe%20BVIB%20consistently%20excels%20in%20comparison%20to%20alternative%20foundational%0Amethodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17287v2&entry.124074799=Read"},
{"title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting", "author": "Mufan Liu and Cixiao Zhang and Qi Yang and Yujie Cao and Yiling Xu and Yin Xu and Shu Sun and Mingzeng Dai and Yunfeng Guan", "abstract": "  Modeling the wireless radiance field (WRF) is fundamental to modern\ncommunication systems, enabling key tasks such as localization, sensing, and\nchannel estimation. Traditional approaches, which rely on empirical formulas or\nphysical simulations, often suffer from limited accuracy or require strong\nscene priors. Recent neural radiance field (NeRF-based) methods improve\nreconstruction fidelity through differentiable volumetric rendering, but their\nreliance on computationally expensive multilayer perceptron (MLP) queries\nhinders real-time deployment. To overcome these challenges, we introduce\nGaussian splatting (GS) to the wireless domain, leveraging its efficiency in\nmodeling optical radiance fields to enable compact and accurate WRF\nreconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian\nsplatting framework that synthesizes WRF spectra at arbitrary positions under\nsingle-sided transceiver mobility. SwiftWRF employs CUDA-accelerated\nrasterization to render spectra at over 100000 fps and uses a lightweight MLP\nto model the deformation of 2D Gaussians, effectively capturing\nmobility-induced WRF variations. In addition to novel spectrum synthesis, the\nefficacy of SwiftWRF is further underscored in its applications in\nangle-of-arrival (AoA) and received signal strength indicator (RSSI)\nprediction. Experiments conducted on both real-world and synthetic indoor\nscenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster\nthan existing state-of-the-art methods, while significantly enhancing its\nsignal quality. The project page is https://evan-sudo.github.io/swiftwrf/.\n", "link": "http://arxiv.org/abs/2506.12787v2", "date": "2025-06-18", "relevancy": 2.2842, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6291}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5327}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rasterizing%20Wireless%20Radiance%20Field%20via%20Deformable%202D%20Gaussian%20Splatting&body=Title%3A%20Rasterizing%20Wireless%20Radiance%20Field%20via%20Deformable%202D%20Gaussian%20Splatting%0AAuthor%3A%20Mufan%20Liu%20and%20Cixiao%20Zhang%20and%20Qi%20Yang%20and%20Yujie%20Cao%20and%20Yiling%20Xu%20and%20Yin%20Xu%20and%20Shu%20Sun%20and%20Mingzeng%20Dai%20and%20Yunfeng%20Guan%0AAbstract%3A%20%20%20Modeling%20the%20wireless%20radiance%20field%20%28WRF%29%20is%20fundamental%20to%20modern%0Acommunication%20systems%2C%20enabling%20key%20tasks%20such%20as%20localization%2C%20sensing%2C%20and%0Achannel%20estimation.%20Traditional%20approaches%2C%20which%20rely%20on%20empirical%20formulas%20or%0Aphysical%20simulations%2C%20often%20suffer%20from%20limited%20accuracy%20or%20require%20strong%0Ascene%20priors.%20Recent%20neural%20radiance%20field%20%28NeRF-based%29%20methods%20improve%0Areconstruction%20fidelity%20through%20differentiable%20volumetric%20rendering%2C%20but%20their%0Areliance%20on%20computationally%20expensive%20multilayer%20perceptron%20%28MLP%29%20queries%0Ahinders%20real-time%20deployment.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0AGaussian%20splatting%20%28GS%29%20to%20the%20wireless%20domain%2C%20leveraging%20its%20efficiency%20in%0Amodeling%20optical%20radiance%20fields%20to%20enable%20compact%20and%20accurate%20WRF%0Areconstruction.%20Specifically%2C%20we%20propose%20SwiftWRF%2C%20a%20deformable%202D%20Gaussian%0Asplatting%20framework%20that%20synthesizes%20WRF%20spectra%20at%20arbitrary%20positions%20under%0Asingle-sided%20transceiver%20mobility.%20SwiftWRF%20employs%20CUDA-accelerated%0Arasterization%20to%20render%20spectra%20at%20over%20100000%20fps%20and%20uses%20a%20lightweight%20MLP%0Ato%20model%20the%20deformation%20of%202D%20Gaussians%2C%20effectively%20capturing%0Amobility-induced%20WRF%20variations.%20In%20addition%20to%20novel%20spectrum%20synthesis%2C%20the%0Aefficacy%20of%20SwiftWRF%20is%20further%20underscored%20in%20its%20applications%20in%0Aangle-of-arrival%20%28AoA%29%20and%20received%20signal%20strength%20indicator%20%28RSSI%29%0Aprediction.%20Experiments%20conducted%20on%20both%20real-world%20and%20synthetic%20indoor%0Ascenes%20demonstrate%20that%20SwiftWRF%20can%20reconstruct%20WRF%20spectra%20up%20to%20500x%20faster%0Athan%20existing%20state-of-the-art%20methods%2C%20while%20significantly%20enhancing%20its%0Asignal%20quality.%20The%20project%20page%20is%20https%3A//evan-sudo.github.io/swiftwrf/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.12787v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRasterizing%2520Wireless%2520Radiance%2520Field%2520via%2520Deformable%25202D%2520Gaussian%2520Splatting%26entry.906535625%3DMufan%2520Liu%2520and%2520Cixiao%2520Zhang%2520and%2520Qi%2520Yang%2520and%2520Yujie%2520Cao%2520and%2520Yiling%2520Xu%2520and%2520Yin%2520Xu%2520and%2520Shu%2520Sun%2520and%2520Mingzeng%2520Dai%2520and%2520Yunfeng%2520Guan%26entry.1292438233%3D%2520%2520Modeling%2520the%2520wireless%2520radiance%2520field%2520%2528WRF%2529%2520is%2520fundamental%2520to%2520modern%250Acommunication%2520systems%252C%2520enabling%2520key%2520tasks%2520such%2520as%2520localization%252C%2520sensing%252C%2520and%250Achannel%2520estimation.%2520Traditional%2520approaches%252C%2520which%2520rely%2520on%2520empirical%2520formulas%2520or%250Aphysical%2520simulations%252C%2520often%2520suffer%2520from%2520limited%2520accuracy%2520or%2520require%2520strong%250Ascene%2520priors.%2520Recent%2520neural%2520radiance%2520field%2520%2528NeRF-based%2529%2520methods%2520improve%250Areconstruction%2520fidelity%2520through%2520differentiable%2520volumetric%2520rendering%252C%2520but%2520their%250Areliance%2520on%2520computationally%2520expensive%2520multilayer%2520perceptron%2520%2528MLP%2529%2520queries%250Ahinders%2520real-time%2520deployment.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%250AGaussian%2520splatting%2520%2528GS%2529%2520to%2520the%2520wireless%2520domain%252C%2520leveraging%2520its%2520efficiency%2520in%250Amodeling%2520optical%2520radiance%2520fields%2520to%2520enable%2520compact%2520and%2520accurate%2520WRF%250Areconstruction.%2520Specifically%252C%2520we%2520propose%2520SwiftWRF%252C%2520a%2520deformable%25202D%2520Gaussian%250Asplatting%2520framework%2520that%2520synthesizes%2520WRF%2520spectra%2520at%2520arbitrary%2520positions%2520under%250Asingle-sided%2520transceiver%2520mobility.%2520SwiftWRF%2520employs%2520CUDA-accelerated%250Arasterization%2520to%2520render%2520spectra%2520at%2520over%2520100000%2520fps%2520and%2520uses%2520a%2520lightweight%2520MLP%250Ato%2520model%2520the%2520deformation%2520of%25202D%2520Gaussians%252C%2520effectively%2520capturing%250Amobility-induced%2520WRF%2520variations.%2520In%2520addition%2520to%2520novel%2520spectrum%2520synthesis%252C%2520the%250Aefficacy%2520of%2520SwiftWRF%2520is%2520further%2520underscored%2520in%2520its%2520applications%2520in%250Aangle-of-arrival%2520%2528AoA%2529%2520and%2520received%2520signal%2520strength%2520indicator%2520%2528RSSI%2529%250Aprediction.%2520Experiments%2520conducted%2520on%2520both%2520real-world%2520and%2520synthetic%2520indoor%250Ascenes%2520demonstrate%2520that%2520SwiftWRF%2520can%2520reconstruct%2520WRF%2520spectra%2520up%2520to%2520500x%2520faster%250Athan%2520existing%2520state-of-the-art%2520methods%252C%2520while%2520significantly%2520enhancing%2520its%250Asignal%2520quality.%2520The%2520project%2520page%2520is%2520https%253A//evan-sudo.github.io/swiftwrf/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12787v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rasterizing%20Wireless%20Radiance%20Field%20via%20Deformable%202D%20Gaussian%20Splatting&entry.906535625=Mufan%20Liu%20and%20Cixiao%20Zhang%20and%20Qi%20Yang%20and%20Yujie%20Cao%20and%20Yiling%20Xu%20and%20Yin%20Xu%20and%20Shu%20Sun%20and%20Mingzeng%20Dai%20and%20Yunfeng%20Guan&entry.1292438233=%20%20Modeling%20the%20wireless%20radiance%20field%20%28WRF%29%20is%20fundamental%20to%20modern%0Acommunication%20systems%2C%20enabling%20key%20tasks%20such%20as%20localization%2C%20sensing%2C%20and%0Achannel%20estimation.%20Traditional%20approaches%2C%20which%20rely%20on%20empirical%20formulas%20or%0Aphysical%20simulations%2C%20often%20suffer%20from%20limited%20accuracy%20or%20require%20strong%0Ascene%20priors.%20Recent%20neural%20radiance%20field%20%28NeRF-based%29%20methods%20improve%0Areconstruction%20fidelity%20through%20differentiable%20volumetric%20rendering%2C%20but%20their%0Areliance%20on%20computationally%20expensive%20multilayer%20perceptron%20%28MLP%29%20queries%0Ahinders%20real-time%20deployment.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0AGaussian%20splatting%20%28GS%29%20to%20the%20wireless%20domain%2C%20leveraging%20its%20efficiency%20in%0Amodeling%20optical%20radiance%20fields%20to%20enable%20compact%20and%20accurate%20WRF%0Areconstruction.%20Specifically%2C%20we%20propose%20SwiftWRF%2C%20a%20deformable%202D%20Gaussian%0Asplatting%20framework%20that%20synthesizes%20WRF%20spectra%20at%20arbitrary%20positions%20under%0Asingle-sided%20transceiver%20mobility.%20SwiftWRF%20employs%20CUDA-accelerated%0Arasterization%20to%20render%20spectra%20at%20over%20100000%20fps%20and%20uses%20a%20lightweight%20MLP%0Ato%20model%20the%20deformation%20of%202D%20Gaussians%2C%20effectively%20capturing%0Amobility-induced%20WRF%20variations.%20In%20addition%20to%20novel%20spectrum%20synthesis%2C%20the%0Aefficacy%20of%20SwiftWRF%20is%20further%20underscored%20in%20its%20applications%20in%0Aangle-of-arrival%20%28AoA%29%20and%20received%20signal%20strength%20indicator%20%28RSSI%29%0Aprediction.%20Experiments%20conducted%20on%20both%20real-world%20and%20synthetic%20indoor%0Ascenes%20demonstrate%20that%20SwiftWRF%20can%20reconstruct%20WRF%20spectra%20up%20to%20500x%20faster%0Athan%20existing%20state-of-the-art%20methods%2C%20while%20significantly%20enhancing%20its%0Asignal%20quality.%20The%20project%20page%20is%20https%3A//evan-sudo.github.io/swiftwrf/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.12787v2&entry.124074799=Read"},
{"title": "On Zero-Initialized Attention: Optimal Prompt and Gating Factor\n  Estimation", "author": "Nghiem T. Diep and Huy Nguyen and Chau Nguyen and Minh Le and Duy M. H. Nguyen and Daniel Sonntag and Mathias Niepert and Nhat Ho", "abstract": "  The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique\nfor LLaMA models, leveraging zero-initialized attention to stabilize training\nand enhance performance. However, despite its empirical success, the\ntheoretical foundations of zero-initialized attention remain largely\nunexplored. In this paper, we provide a rigorous theoretical analysis,\nestablishing a connection between zero-initialized attention and\nmixture-of-expert models. We prove that both linear and non-linear prompts,\nalong with gating functions, can be optimally estimated, with non-linear\nprompts offering greater flexibility for future applications. Empirically, we\nvalidate our findings on the open LLM benchmarks, demonstrating that non-linear\nprompts outperform linear ones. Notably, even with limited training data, both\nprompt types consistently surpass vanilla attention, highlighting the\nrobustness and adaptability of zero-initialized attention.\n", "link": "http://arxiv.org/abs/2502.03029v3", "date": "2025-06-18", "relevancy": 2.2822, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4798}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Zero-Initialized%20Attention%3A%20Optimal%20Prompt%20and%20Gating%20Factor%0A%20%20Estimation&body=Title%3A%20On%20Zero-Initialized%20Attention%3A%20Optimal%20Prompt%20and%20Gating%20Factor%0A%20%20Estimation%0AAuthor%3A%20Nghiem%20T.%20Diep%20and%20Huy%20Nguyen%20and%20Chau%20Nguyen%20and%20Minh%20Le%20and%20Duy%20M.%20H.%20Nguyen%20and%20Daniel%20Sonntag%20and%20Mathias%20Niepert%20and%20Nhat%20Ho%0AAbstract%3A%20%20%20The%20LLaMA-Adapter%20has%20recently%20emerged%20as%20an%20efficient%20fine-tuning%20technique%0Afor%20LLaMA%20models%2C%20leveraging%20zero-initialized%20attention%20to%20stabilize%20training%0Aand%20enhance%20performance.%20However%2C%20despite%20its%20empirical%20success%2C%20the%0Atheoretical%20foundations%20of%20zero-initialized%20attention%20remain%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20provide%20a%20rigorous%20theoretical%20analysis%2C%0Aestablishing%20a%20connection%20between%20zero-initialized%20attention%20and%0Amixture-of-expert%20models.%20We%20prove%20that%20both%20linear%20and%20non-linear%20prompts%2C%0Aalong%20with%20gating%20functions%2C%20can%20be%20optimally%20estimated%2C%20with%20non-linear%0Aprompts%20offering%20greater%20flexibility%20for%20future%20applications.%20Empirically%2C%20we%0Avalidate%20our%20findings%20on%20the%20open%20LLM%20benchmarks%2C%20demonstrating%20that%20non-linear%0Aprompts%20outperform%20linear%20ones.%20Notably%2C%20even%20with%20limited%20training%20data%2C%20both%0Aprompt%20types%20consistently%20surpass%20vanilla%20attention%2C%20highlighting%20the%0Arobustness%20and%20adaptability%20of%20zero-initialized%20attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03029v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Zero-Initialized%2520Attention%253A%2520Optimal%2520Prompt%2520and%2520Gating%2520Factor%250A%2520%2520Estimation%26entry.906535625%3DNghiem%2520T.%2520Diep%2520and%2520Huy%2520Nguyen%2520and%2520Chau%2520Nguyen%2520and%2520Minh%2520Le%2520and%2520Duy%2520M.%2520H.%2520Nguyen%2520and%2520Daniel%2520Sonntag%2520and%2520Mathias%2520Niepert%2520and%2520Nhat%2520Ho%26entry.1292438233%3D%2520%2520The%2520LLaMA-Adapter%2520has%2520recently%2520emerged%2520as%2520an%2520efficient%2520fine-tuning%2520technique%250Afor%2520LLaMA%2520models%252C%2520leveraging%2520zero-initialized%2520attention%2520to%2520stabilize%2520training%250Aand%2520enhance%2520performance.%2520However%252C%2520despite%2520its%2520empirical%2520success%252C%2520the%250Atheoretical%2520foundations%2520of%2520zero-initialized%2520attention%2520remain%2520largely%250Aunexplored.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520rigorous%2520theoretical%2520analysis%252C%250Aestablishing%2520a%2520connection%2520between%2520zero-initialized%2520attention%2520and%250Amixture-of-expert%2520models.%2520We%2520prove%2520that%2520both%2520linear%2520and%2520non-linear%2520prompts%252C%250Aalong%2520with%2520gating%2520functions%252C%2520can%2520be%2520optimally%2520estimated%252C%2520with%2520non-linear%250Aprompts%2520offering%2520greater%2520flexibility%2520for%2520future%2520applications.%2520Empirically%252C%2520we%250Avalidate%2520our%2520findings%2520on%2520the%2520open%2520LLM%2520benchmarks%252C%2520demonstrating%2520that%2520non-linear%250Aprompts%2520outperform%2520linear%2520ones.%2520Notably%252C%2520even%2520with%2520limited%2520training%2520data%252C%2520both%250Aprompt%2520types%2520consistently%2520surpass%2520vanilla%2520attention%252C%2520highlighting%2520the%250Arobustness%2520and%2520adaptability%2520of%2520zero-initialized%2520attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03029v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Zero-Initialized%20Attention%3A%20Optimal%20Prompt%20and%20Gating%20Factor%0A%20%20Estimation&entry.906535625=Nghiem%20T.%20Diep%20and%20Huy%20Nguyen%20and%20Chau%20Nguyen%20and%20Minh%20Le%20and%20Duy%20M.%20H.%20Nguyen%20and%20Daniel%20Sonntag%20and%20Mathias%20Niepert%20and%20Nhat%20Ho&entry.1292438233=%20%20The%20LLaMA-Adapter%20has%20recently%20emerged%20as%20an%20efficient%20fine-tuning%20technique%0Afor%20LLaMA%20models%2C%20leveraging%20zero-initialized%20attention%20to%20stabilize%20training%0Aand%20enhance%20performance.%20However%2C%20despite%20its%20empirical%20success%2C%20the%0Atheoretical%20foundations%20of%20zero-initialized%20attention%20remain%20largely%0Aunexplored.%20In%20this%20paper%2C%20we%20provide%20a%20rigorous%20theoretical%20analysis%2C%0Aestablishing%20a%20connection%20between%20zero-initialized%20attention%20and%0Amixture-of-expert%20models.%20We%20prove%20that%20both%20linear%20and%20non-linear%20prompts%2C%0Aalong%20with%20gating%20functions%2C%20can%20be%20optimally%20estimated%2C%20with%20non-linear%0Aprompts%20offering%20greater%20flexibility%20for%20future%20applications.%20Empirically%2C%20we%0Avalidate%20our%20findings%20on%20the%20open%20LLM%20benchmarks%2C%20demonstrating%20that%20non-linear%0Aprompts%20outperform%20linear%20ones.%20Notably%2C%20even%20with%20limited%20training%20data%2C%20both%0Aprompt%20types%20consistently%20surpass%20vanilla%20attention%2C%20highlighting%20the%0Arobustness%20and%20adaptability%20of%20zero-initialized%20attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03029v3&entry.124074799=Read"},
{"title": "GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for\n  Generalized Low-Rank Trace Regression", "author": "Junghyun Lee and Kyoungseok Jang and Kwang-Sung Jun and Milan Vojnovi\u0107 and Se-Young Yun", "abstract": "  We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized\nlow-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it\nemploys a two-stage approach: nuclear norm regularization followed by matrix\nCatoni estimation. We establish state-of-the-art estimation error bounds,\nsurpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and\nreveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key\ntechnical challenge is controlling bias from the nonlinear inverse link\nfunction, which we address by our two-stage approach. We prove a *local*\nminimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise\noptimality up to the condition number of the ground-truth Hessian. Applications\ninclude generalized linear matrix completion, where `GL-LowPopArt` achieves a\nstate-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a\nnovel setting inspired by general preference learning (Zhang et al., 2024). Our\nanalysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,\npotentially interesting problem-dependent quantity, along with improved Borda\nregret bound than vectorization (Wu et al., 2024).\n", "link": "http://arxiv.org/abs/2506.03074v3", "date": "2025-06-18", "relevancy": 2.279, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4637}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4538}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GL-LowPopArt%3A%20A%20Nearly%20Instance-Wise%20Minimax-Optimal%20Estimator%20for%0A%20%20Generalized%20Low-Rank%20Trace%20Regression&body=Title%3A%20GL-LowPopArt%3A%20A%20Nearly%20Instance-Wise%20Minimax-Optimal%20Estimator%20for%0A%20%20Generalized%20Low-Rank%20Trace%20Regression%0AAuthor%3A%20Junghyun%20Lee%20and%20Kyoungseok%20Jang%20and%20Kwang-Sung%20Jun%20and%20Milan%20Vojnovi%C4%87%20and%20Se-Young%20Yun%0AAbstract%3A%20%20%20We%20present%20%60GL-LowPopArt%60%2C%20a%20novel%20Catoni-style%20estimator%20for%20generalized%0Alow-rank%20trace%20regression.%20Building%20on%20%60LowPopArt%60%20%28Jang%20et%20al.%2C%202024%29%2C%20it%0Aemploys%20a%20two-stage%20approach%3A%20nuclear%20norm%20regularization%20followed%20by%20matrix%0ACatoni%20estimation.%20We%20establish%20state-of-the-art%20estimation%20error%20bounds%2C%0Asurpassing%20existing%20guarantees%20%28Fan%20et%20al.%2C%202019%3B%20Kang%20et%20al.%2C%202022%29%2C%20and%0Areveal%20a%20novel%20experimental%20design%20objective%2C%20%24%5Cmathrm%7BGL%7D%28%5Cpi%29%24.%20The%20key%0Atechnical%20challenge%20is%20controlling%20bias%20from%20the%20nonlinear%20inverse%20link%0Afunction%2C%20which%20we%20address%20by%20our%20two-stage%20approach.%20We%20prove%20a%20%2Alocal%2A%0Aminimax%20lower%20bound%2C%20showing%20that%20our%20%60GL-LowPopArt%60%20enjoys%20instance-wise%0Aoptimality%20up%20to%20the%20condition%20number%20of%20the%20ground-truth%20Hessian.%20Applications%0Ainclude%20generalized%20linear%20matrix%20completion%2C%20where%20%60GL-LowPopArt%60%20achieves%20a%0Astate-of-the-art%20Frobenius%20error%20guarantee%2C%20and%20%2A%2Abilinear%20dueling%20bandits%2A%2A%2C%20a%0Anovel%20setting%20inspired%20by%20general%20preference%20learning%20%28Zhang%20et%20al.%2C%202024%29.%20Our%0Aanalysis%20of%20a%20%60GL-LowPopArt%60-based%20explore-then-commit%20algorithm%20reveals%20a%20new%2C%0Apotentially%20interesting%20problem-dependent%20quantity%2C%20along%20with%20improved%20Borda%0Aregret%20bound%20than%20vectorization%20%28Wu%20et%20al.%2C%202024%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03074v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGL-LowPopArt%253A%2520A%2520Nearly%2520Instance-Wise%2520Minimax-Optimal%2520Estimator%2520for%250A%2520%2520Generalized%2520Low-Rank%2520Trace%2520Regression%26entry.906535625%3DJunghyun%2520Lee%2520and%2520Kyoungseok%2520Jang%2520and%2520Kwang-Sung%2520Jun%2520and%2520Milan%2520Vojnovi%25C4%2587%2520and%2520Se-Young%2520Yun%26entry.1292438233%3D%2520%2520We%2520present%2520%2560GL-LowPopArt%2560%252C%2520a%2520novel%2520Catoni-style%2520estimator%2520for%2520generalized%250Alow-rank%2520trace%2520regression.%2520Building%2520on%2520%2560LowPopArt%2560%2520%2528Jang%2520et%2520al.%252C%25202024%2529%252C%2520it%250Aemploys%2520a%2520two-stage%2520approach%253A%2520nuclear%2520norm%2520regularization%2520followed%2520by%2520matrix%250ACatoni%2520estimation.%2520We%2520establish%2520state-of-the-art%2520estimation%2520error%2520bounds%252C%250Asurpassing%2520existing%2520guarantees%2520%2528Fan%2520et%2520al.%252C%25202019%253B%2520Kang%2520et%2520al.%252C%25202022%2529%252C%2520and%250Areveal%2520a%2520novel%2520experimental%2520design%2520objective%252C%2520%2524%255Cmathrm%257BGL%257D%2528%255Cpi%2529%2524.%2520The%2520key%250Atechnical%2520challenge%2520is%2520controlling%2520bias%2520from%2520the%2520nonlinear%2520inverse%2520link%250Afunction%252C%2520which%2520we%2520address%2520by%2520our%2520two-stage%2520approach.%2520We%2520prove%2520a%2520%252Alocal%252A%250Aminimax%2520lower%2520bound%252C%2520showing%2520that%2520our%2520%2560GL-LowPopArt%2560%2520enjoys%2520instance-wise%250Aoptimality%2520up%2520to%2520the%2520condition%2520number%2520of%2520the%2520ground-truth%2520Hessian.%2520Applications%250Ainclude%2520generalized%2520linear%2520matrix%2520completion%252C%2520where%2520%2560GL-LowPopArt%2560%2520achieves%2520a%250Astate-of-the-art%2520Frobenius%2520error%2520guarantee%252C%2520and%2520%252A%252Abilinear%2520dueling%2520bandits%252A%252A%252C%2520a%250Anovel%2520setting%2520inspired%2520by%2520general%2520preference%2520learning%2520%2528Zhang%2520et%2520al.%252C%25202024%2529.%2520Our%250Aanalysis%2520of%2520a%2520%2560GL-LowPopArt%2560-based%2520explore-then-commit%2520algorithm%2520reveals%2520a%2520new%252C%250Apotentially%2520interesting%2520problem-dependent%2520quantity%252C%2520along%2520with%2520improved%2520Borda%250Aregret%2520bound%2520than%2520vectorization%2520%2528Wu%2520et%2520al.%252C%25202024%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03074v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GL-LowPopArt%3A%20A%20Nearly%20Instance-Wise%20Minimax-Optimal%20Estimator%20for%0A%20%20Generalized%20Low-Rank%20Trace%20Regression&entry.906535625=Junghyun%20Lee%20and%20Kyoungseok%20Jang%20and%20Kwang-Sung%20Jun%20and%20Milan%20Vojnovi%C4%87%20and%20Se-Young%20Yun&entry.1292438233=%20%20We%20present%20%60GL-LowPopArt%60%2C%20a%20novel%20Catoni-style%20estimator%20for%20generalized%0Alow-rank%20trace%20regression.%20Building%20on%20%60LowPopArt%60%20%28Jang%20et%20al.%2C%202024%29%2C%20it%0Aemploys%20a%20two-stage%20approach%3A%20nuclear%20norm%20regularization%20followed%20by%20matrix%0ACatoni%20estimation.%20We%20establish%20state-of-the-art%20estimation%20error%20bounds%2C%0Asurpassing%20existing%20guarantees%20%28Fan%20et%20al.%2C%202019%3B%20Kang%20et%20al.%2C%202022%29%2C%20and%0Areveal%20a%20novel%20experimental%20design%20objective%2C%20%24%5Cmathrm%7BGL%7D%28%5Cpi%29%24.%20The%20key%0Atechnical%20challenge%20is%20controlling%20bias%20from%20the%20nonlinear%20inverse%20link%0Afunction%2C%20which%20we%20address%20by%20our%20two-stage%20approach.%20We%20prove%20a%20%2Alocal%2A%0Aminimax%20lower%20bound%2C%20showing%20that%20our%20%60GL-LowPopArt%60%20enjoys%20instance-wise%0Aoptimality%20up%20to%20the%20condition%20number%20of%20the%20ground-truth%20Hessian.%20Applications%0Ainclude%20generalized%20linear%20matrix%20completion%2C%20where%20%60GL-LowPopArt%60%20achieves%20a%0Astate-of-the-art%20Frobenius%20error%20guarantee%2C%20and%20%2A%2Abilinear%20dueling%20bandits%2A%2A%2C%20a%0Anovel%20setting%20inspired%20by%20general%20preference%20learning%20%28Zhang%20et%20al.%2C%202024%29.%20Our%0Aanalysis%20of%20a%20%60GL-LowPopArt%60-based%20explore-then-commit%20algorithm%20reveals%20a%20new%2C%0Apotentially%20interesting%20problem-dependent%20quantity%2C%20along%20with%20improved%20Borda%0Aregret%20bound%20than%20vectorization%20%28Wu%20et%20al.%2C%202024%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03074v3&entry.124074799=Read"},
{"title": "Learning the Geometric Mechanics of Robot Motion Using Gaussian Mixtures", "author": "Ruizhen Hu and Shai Revzen", "abstract": "  Data-driven models of robot motion constructed using principles from\nGeometric Mechanics have been shown to produce useful predictions of robot\nmotion for a variety of robots. For robots with a useful number of DoF, these\ngeometric mechanics models can only be constructed in the neighborhood of a\ngait. Here we show how Gaussian Mixture Models (GMM) can be used as a form of\nmanifold learning that learns the structure of the Geometric Mechanics\n\"motility map\" and demonstrate: [i] a sizable improvement in prediction quality\nwhen compared to the previously published methods; [ii] a method that can be\napplied to any motion dataset and not only periodic gait data; [iii] a way to\npre-process the data-set to facilitate extrapolation in places where the\nmotility map is known to be linear. Our results can be applied anywhere a\ndata-driven geometric motion model might be useful.\n", "link": "http://arxiv.org/abs/2502.05309v2", "date": "2025-06-18", "relevancy": 2.2788, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6036}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.576}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Geometric%20Mechanics%20of%20Robot%20Motion%20Using%20Gaussian%20Mixtures&body=Title%3A%20Learning%20the%20Geometric%20Mechanics%20of%20Robot%20Motion%20Using%20Gaussian%20Mixtures%0AAuthor%3A%20Ruizhen%20Hu%20and%20Shai%20Revzen%0AAbstract%3A%20%20%20Data-driven%20models%20of%20robot%20motion%20constructed%20using%20principles%20from%0AGeometric%20Mechanics%20have%20been%20shown%20to%20produce%20useful%20predictions%20of%20robot%0Amotion%20for%20a%20variety%20of%20robots.%20For%20robots%20with%20a%20useful%20number%20of%20DoF%2C%20these%0Ageometric%20mechanics%20models%20can%20only%20be%20constructed%20in%20the%20neighborhood%20of%20a%0Agait.%20Here%20we%20show%20how%20Gaussian%20Mixture%20Models%20%28GMM%29%20can%20be%20used%20as%20a%20form%20of%0Amanifold%20learning%20that%20learns%20the%20structure%20of%20the%20Geometric%20Mechanics%0A%22motility%20map%22%20and%20demonstrate%3A%20%5Bi%5D%20a%20sizable%20improvement%20in%20prediction%20quality%0Awhen%20compared%20to%20the%20previously%20published%20methods%3B%20%5Bii%5D%20a%20method%20that%20can%20be%0Aapplied%20to%20any%20motion%20dataset%20and%20not%20only%20periodic%20gait%20data%3B%20%5Biii%5D%20a%20way%20to%0Apre-process%20the%20data-set%20to%20facilitate%20extrapolation%20in%20places%20where%20the%0Amotility%20map%20is%20known%20to%20be%20linear.%20Our%20results%20can%20be%20applied%20anywhere%20a%0Adata-driven%20geometric%20motion%20model%20might%20be%20useful.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520Geometric%2520Mechanics%2520of%2520Robot%2520Motion%2520Using%2520Gaussian%2520Mixtures%26entry.906535625%3DRuizhen%2520Hu%2520and%2520Shai%2520Revzen%26entry.1292438233%3D%2520%2520Data-driven%2520models%2520of%2520robot%2520motion%2520constructed%2520using%2520principles%2520from%250AGeometric%2520Mechanics%2520have%2520been%2520shown%2520to%2520produce%2520useful%2520predictions%2520of%2520robot%250Amotion%2520for%2520a%2520variety%2520of%2520robots.%2520For%2520robots%2520with%2520a%2520useful%2520number%2520of%2520DoF%252C%2520these%250Ageometric%2520mechanics%2520models%2520can%2520only%2520be%2520constructed%2520in%2520the%2520neighborhood%2520of%2520a%250Agait.%2520Here%2520we%2520show%2520how%2520Gaussian%2520Mixture%2520Models%2520%2528GMM%2529%2520can%2520be%2520used%2520as%2520a%2520form%2520of%250Amanifold%2520learning%2520that%2520learns%2520the%2520structure%2520of%2520the%2520Geometric%2520Mechanics%250A%2522motility%2520map%2522%2520and%2520demonstrate%253A%2520%255Bi%255D%2520a%2520sizable%2520improvement%2520in%2520prediction%2520quality%250Awhen%2520compared%2520to%2520the%2520previously%2520published%2520methods%253B%2520%255Bii%255D%2520a%2520method%2520that%2520can%2520be%250Aapplied%2520to%2520any%2520motion%2520dataset%2520and%2520not%2520only%2520periodic%2520gait%2520data%253B%2520%255Biii%255D%2520a%2520way%2520to%250Apre-process%2520the%2520data-set%2520to%2520facilitate%2520extrapolation%2520in%2520places%2520where%2520the%250Amotility%2520map%2520is%2520known%2520to%2520be%2520linear.%2520Our%2520results%2520can%2520be%2520applied%2520anywhere%2520a%250Adata-driven%2520geometric%2520motion%2520model%2520might%2520be%2520useful.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Geometric%20Mechanics%20of%20Robot%20Motion%20Using%20Gaussian%20Mixtures&entry.906535625=Ruizhen%20Hu%20and%20Shai%20Revzen&entry.1292438233=%20%20Data-driven%20models%20of%20robot%20motion%20constructed%20using%20principles%20from%0AGeometric%20Mechanics%20have%20been%20shown%20to%20produce%20useful%20predictions%20of%20robot%0Amotion%20for%20a%20variety%20of%20robots.%20For%20robots%20with%20a%20useful%20number%20of%20DoF%2C%20these%0Ageometric%20mechanics%20models%20can%20only%20be%20constructed%20in%20the%20neighborhood%20of%20a%0Agait.%20Here%20we%20show%20how%20Gaussian%20Mixture%20Models%20%28GMM%29%20can%20be%20used%20as%20a%20form%20of%0Amanifold%20learning%20that%20learns%20the%20structure%20of%20the%20Geometric%20Mechanics%0A%22motility%20map%22%20and%20demonstrate%3A%20%5Bi%5D%20a%20sizable%20improvement%20in%20prediction%20quality%0Awhen%20compared%20to%20the%20previously%20published%20methods%3B%20%5Bii%5D%20a%20method%20that%20can%20be%0Aapplied%20to%20any%20motion%20dataset%20and%20not%20only%20periodic%20gait%20data%3B%20%5Biii%5D%20a%20way%20to%0Apre-process%20the%20data-set%20to%20facilitate%20extrapolation%20in%20places%20where%20the%0Amotility%20map%20is%20known%20to%20be%20linear.%20Our%20results%20can%20be%20applied%20anywhere%20a%0Adata-driven%20geometric%20motion%20model%20might%20be%20useful.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05309v2&entry.124074799=Read"},
{"title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on\n  Synthetic Video Understanding", "author": "Zongxia Li and Xiyang Wu and Guangyao Shi and Yubin Qin and Hongyang Du and Tianyi Zhou and Dinesh Manocha and Jordan Lee Boyd-Graber", "abstract": "  Synthetic video generation has gained significant attention for its realism\nand broad applications, but remains prone to violations of common sense and\nphysical laws. This highlights the need for reliable abnormality detectors that\nunderstand such principles and are robust to hallucinations. To address this,\nwe introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from\nsynthetic videos generated by models like Veo2, Sora, and Kling, paired with\nexpert-crafted counterintuitive QA to evaluate the critical thinking abilities\nof Multi-modal Large Language Models (MLLMs) on abnormalities that are\nperceptually obvious to humans but often hallucinated due to language priors.\nVideoHallu evaluates MLLMs' abnormality detection abilities with examples\nacross alignment, consistency, commonsense, and physics. We benchmark SOTA\nMLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and\nVideoChat-R1. We observe that these models perform well on many real-world\nbenchmarks like MVBench and MovieChat, but still struggle with basic\nphysics-based and commonsense reasoning in synthetic videos. We further show\nthat post-training with Group Relative Policy Optimization (GRPO), using\ncurriculum learning on datasets combining video QA with counterintuitive\ncommonsense and physics reasoning over real and synthetic videos, improves\nMLLMs' abnormality detection and critical thinking, demonstrating the value of\ntargeted training for improving their understanding of commonsense and physical\nlaws. Our code is available at https://github.com/zli12321/VideoHallu.git.\n", "link": "http://arxiv.org/abs/2505.01481v3", "date": "2025-06-18", "relevancy": 2.267, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoHallu%3A%20Evaluating%20and%20Mitigating%20Multi-modal%20Hallucinations%20on%0A%20%20Synthetic%20Video%20Understanding&body=Title%3A%20VideoHallu%3A%20Evaluating%20and%20Mitigating%20Multi-modal%20Hallucinations%20on%0A%20%20Synthetic%20Video%20Understanding%0AAuthor%3A%20Zongxia%20Li%20and%20Xiyang%20Wu%20and%20Guangyao%20Shi%20and%20Yubin%20Qin%20and%20Hongyang%20Du%20and%20Tianyi%20Zhou%20and%20Dinesh%20Manocha%20and%20Jordan%20Lee%20Boyd-Graber%0AAbstract%3A%20%20%20Synthetic%20video%20generation%20has%20gained%20significant%20attention%20for%20its%20realism%0Aand%20broad%20applications%2C%20but%20remains%20prone%20to%20violations%20of%20common%20sense%20and%0Aphysical%20laws.%20This%20highlights%20the%20need%20for%20reliable%20abnormality%20detectors%20that%0Aunderstand%20such%20principles%20and%20are%20robust%20to%20hallucinations.%20To%20address%20this%2C%0Awe%20introduce%20VideoHallu%2C%20a%20benchmark%20of%20over%203%2C000%20video%20QA%20pairs%20built%20from%0Asynthetic%20videos%20generated%20by%20models%20like%20Veo2%2C%20Sora%2C%20and%20Kling%2C%20paired%20with%0Aexpert-crafted%20counterintuitive%20QA%20to%20evaluate%20the%20critical%20thinking%20abilities%0Aof%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20on%20abnormalities%20that%20are%0Aperceptually%20obvious%20to%20humans%20but%20often%20hallucinated%20due%20to%20language%20priors.%0AVideoHallu%20evaluates%20MLLMs%27%20abnormality%20detection%20abilities%20with%20examples%0Aacross%20alignment%2C%20consistency%2C%20commonsense%2C%20and%20physics.%20We%20benchmark%20SOTA%0AMLLMs%2C%20including%20GPT-4o%2C%20Gemini-2.5-Pro%2C%20Qwen2.5-VL%2C%20Video-R1%2C%20and%0AVideoChat-R1.%20We%20observe%20that%20these%20models%20perform%20well%20on%20many%20real-world%0Abenchmarks%20like%20MVBench%20and%20MovieChat%2C%20but%20still%20struggle%20with%20basic%0Aphysics-based%20and%20commonsense%20reasoning%20in%20synthetic%20videos.%20We%20further%20show%0Athat%20post-training%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20using%0Acurriculum%20learning%20on%20datasets%20combining%20video%20QA%20with%20counterintuitive%0Acommonsense%20and%20physics%20reasoning%20over%20real%20and%20synthetic%20videos%2C%20improves%0AMLLMs%27%20abnormality%20detection%20and%20critical%20thinking%2C%20demonstrating%20the%20value%20of%0Atargeted%20training%20for%20improving%20their%20understanding%20of%20commonsense%20and%20physical%0Alaws.%20Our%20code%20is%20available%20at%20https%3A//github.com/zli12321/VideoHallu.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01481v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoHallu%253A%2520Evaluating%2520and%2520Mitigating%2520Multi-modal%2520Hallucinations%2520on%250A%2520%2520Synthetic%2520Video%2520Understanding%26entry.906535625%3DZongxia%2520Li%2520and%2520Xiyang%2520Wu%2520and%2520Guangyao%2520Shi%2520and%2520Yubin%2520Qin%2520and%2520Hongyang%2520Du%2520and%2520Tianyi%2520Zhou%2520and%2520Dinesh%2520Manocha%2520and%2520Jordan%2520Lee%2520Boyd-Graber%26entry.1292438233%3D%2520%2520Synthetic%2520video%2520generation%2520has%2520gained%2520significant%2520attention%2520for%2520its%2520realism%250Aand%2520broad%2520applications%252C%2520but%2520remains%2520prone%2520to%2520violations%2520of%2520common%2520sense%2520and%250Aphysical%2520laws.%2520This%2520highlights%2520the%2520need%2520for%2520reliable%2520abnormality%2520detectors%2520that%250Aunderstand%2520such%2520principles%2520and%2520are%2520robust%2520to%2520hallucinations.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520VideoHallu%252C%2520a%2520benchmark%2520of%2520over%25203%252C000%2520video%2520QA%2520pairs%2520built%2520from%250Asynthetic%2520videos%2520generated%2520by%2520models%2520like%2520Veo2%252C%2520Sora%252C%2520and%2520Kling%252C%2520paired%2520with%250Aexpert-crafted%2520counterintuitive%2520QA%2520to%2520evaluate%2520the%2520critical%2520thinking%2520abilities%250Aof%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520on%2520abnormalities%2520that%2520are%250Aperceptually%2520obvious%2520to%2520humans%2520but%2520often%2520hallucinated%2520due%2520to%2520language%2520priors.%250AVideoHallu%2520evaluates%2520MLLMs%2527%2520abnormality%2520detection%2520abilities%2520with%2520examples%250Aacross%2520alignment%252C%2520consistency%252C%2520commonsense%252C%2520and%2520physics.%2520We%2520benchmark%2520SOTA%250AMLLMs%252C%2520including%2520GPT-4o%252C%2520Gemini-2.5-Pro%252C%2520Qwen2.5-VL%252C%2520Video-R1%252C%2520and%250AVideoChat-R1.%2520We%2520observe%2520that%2520these%2520models%2520perform%2520well%2520on%2520many%2520real-world%250Abenchmarks%2520like%2520MVBench%2520and%2520MovieChat%252C%2520but%2520still%2520struggle%2520with%2520basic%250Aphysics-based%2520and%2520commonsense%2520reasoning%2520in%2520synthetic%2520videos.%2520We%2520further%2520show%250Athat%2520post-training%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520using%250Acurriculum%2520learning%2520on%2520datasets%2520combining%2520video%2520QA%2520with%2520counterintuitive%250Acommonsense%2520and%2520physics%2520reasoning%2520over%2520real%2520and%2520synthetic%2520videos%252C%2520improves%250AMLLMs%2527%2520abnormality%2520detection%2520and%2520critical%2520thinking%252C%2520demonstrating%2520the%2520value%2520of%250Atargeted%2520training%2520for%2520improving%2520their%2520understanding%2520of%2520commonsense%2520and%2520physical%250Alaws.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/zli12321/VideoHallu.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01481v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoHallu%3A%20Evaluating%20and%20Mitigating%20Multi-modal%20Hallucinations%20on%0A%20%20Synthetic%20Video%20Understanding&entry.906535625=Zongxia%20Li%20and%20Xiyang%20Wu%20and%20Guangyao%20Shi%20and%20Yubin%20Qin%20and%20Hongyang%20Du%20and%20Tianyi%20Zhou%20and%20Dinesh%20Manocha%20and%20Jordan%20Lee%20Boyd-Graber&entry.1292438233=%20%20Synthetic%20video%20generation%20has%20gained%20significant%20attention%20for%20its%20realism%0Aand%20broad%20applications%2C%20but%20remains%20prone%20to%20violations%20of%20common%20sense%20and%0Aphysical%20laws.%20This%20highlights%20the%20need%20for%20reliable%20abnormality%20detectors%20that%0Aunderstand%20such%20principles%20and%20are%20robust%20to%20hallucinations.%20To%20address%20this%2C%0Awe%20introduce%20VideoHallu%2C%20a%20benchmark%20of%20over%203%2C000%20video%20QA%20pairs%20built%20from%0Asynthetic%20videos%20generated%20by%20models%20like%20Veo2%2C%20Sora%2C%20and%20Kling%2C%20paired%20with%0Aexpert-crafted%20counterintuitive%20QA%20to%20evaluate%20the%20critical%20thinking%20abilities%0Aof%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20on%20abnormalities%20that%20are%0Aperceptually%20obvious%20to%20humans%20but%20often%20hallucinated%20due%20to%20language%20priors.%0AVideoHallu%20evaluates%20MLLMs%27%20abnormality%20detection%20abilities%20with%20examples%0Aacross%20alignment%2C%20consistency%2C%20commonsense%2C%20and%20physics.%20We%20benchmark%20SOTA%0AMLLMs%2C%20including%20GPT-4o%2C%20Gemini-2.5-Pro%2C%20Qwen2.5-VL%2C%20Video-R1%2C%20and%0AVideoChat-R1.%20We%20observe%20that%20these%20models%20perform%20well%20on%20many%20real-world%0Abenchmarks%20like%20MVBench%20and%20MovieChat%2C%20but%20still%20struggle%20with%20basic%0Aphysics-based%20and%20commonsense%20reasoning%20in%20synthetic%20videos.%20We%20further%20show%0Athat%20post-training%20with%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%2C%20using%0Acurriculum%20learning%20on%20datasets%20combining%20video%20QA%20with%20counterintuitive%0Acommonsense%20and%20physics%20reasoning%20over%20real%20and%20synthetic%20videos%2C%20improves%0AMLLMs%27%20abnormality%20detection%20and%20critical%20thinking%2C%20demonstrating%20the%20value%20of%0Atargeted%20training%20for%20improving%20their%20understanding%20of%20commonsense%20and%20physical%0Alaws.%20Our%20code%20is%20available%20at%20https%3A//github.com/zli12321/VideoHallu.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01481v3&entry.124074799=Read"},
{"title": "Uncovering Intention through LLM-Driven Code Snippet Description\n  Generation", "author": "Yusuf Sulistyo Nugroho and Farah Danisha Salam and Brittany Reid and Raula Gaikovina Kula and Kazumasa Shimari and Kenichi Matsumoto", "abstract": "  Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.\n", "link": "http://arxiv.org/abs/2506.15453v1", "date": "2025-06-18", "relevancy": 2.264, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4647}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Intention%20through%20LLM-Driven%20Code%20Snippet%20Description%0A%20%20Generation&body=Title%3A%20Uncovering%20Intention%20through%20LLM-Driven%20Code%20Snippet%20Description%0A%20%20Generation%0AAuthor%3A%20Yusuf%20Sulistyo%20Nugroho%20and%20Farah%20Danisha%20Salam%20and%20Brittany%20Reid%20and%20Raula%20Gaikovina%20Kula%20and%20Kazumasa%20Shimari%20and%20Kenichi%20Matsumoto%0AAbstract%3A%20%20%20Documenting%20code%20snippets%20is%20essential%20to%20pinpoint%20key%20areas%20where%20both%0Adevelopers%20and%20users%20should%20pay%20attention.%20Examples%20include%20usage%20examples%20and%0Aother%20Application%20Programming%20Interfaces%20%28APIs%29%2C%20which%20are%20especially%20important%0Afor%20third-party%20libraries.%20With%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20the%0Akey%20goal%20is%20to%20investigate%20the%20kinds%20of%20description%20developers%20commonly%20use%20and%0Aevaluate%20how%20well%20an%20LLM%2C%20in%20this%20case%20Llama%2C%20can%20support%20description%0Ageneration.%20We%20use%20NPM%20Code%20Snippets%2C%20consisting%20of%20185%2C412%20packages%20with%0A1%2C024%2C579%20code%20snippets.%20From%20there%2C%20we%20use%20400%20code%20snippets%20%28and%20their%0Adescriptions%29%20as%20samples.%20First%2C%20our%20manual%20classification%20found%20that%20the%0Amajority%20of%20original%20descriptions%20%2855.5%25%29%20highlight%20example-based%20usage.%20This%0Afinding%20emphasizes%20the%20importance%20of%20clear%20documentation%2C%20as%20some%20descriptions%0Alacked%20sufficient%20detail%20to%20convey%20intent.%20Second%2C%20the%20LLM%20correctly%20identified%0Athe%20majority%20of%20original%20descriptions%20as%20%22Example%22%20%2879.75%25%29%2C%20which%20is%20identical%0Ato%20our%20manual%20finding%2C%20showing%20a%20propensity%20for%20generalization.%20Third%2C%20compared%0Ato%20the%20originals%2C%20the%20produced%20description%20had%20an%20average%20similarity%20score%20of%0A0.7173%2C%20suggesting%20relevance%20but%20room%20for%20improvement.%20Scores%20below%200.9%0Aindicate%20some%20irrelevance.%20Our%20results%20show%20that%20depending%20on%20the%20task%20of%20the%0Acode%20snippet%2C%20the%20intention%20of%20the%20document%20may%20differ%20from%20being%20instructions%0Afor%20usage%2C%20installations%2C%20or%20descriptive%20learning%20examples%20for%20any%20user%20of%20a%0Alibrary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Intention%2520through%2520LLM-Driven%2520Code%2520Snippet%2520Description%250A%2520%2520Generation%26entry.906535625%3DYusuf%2520Sulistyo%2520Nugroho%2520and%2520Farah%2520Danisha%2520Salam%2520and%2520Brittany%2520Reid%2520and%2520Raula%2520Gaikovina%2520Kula%2520and%2520Kazumasa%2520Shimari%2520and%2520Kenichi%2520Matsumoto%26entry.1292438233%3D%2520%2520Documenting%2520code%2520snippets%2520is%2520essential%2520to%2520pinpoint%2520key%2520areas%2520where%2520both%250Adevelopers%2520and%2520users%2520should%2520pay%2520attention.%2520Examples%2520include%2520usage%2520examples%2520and%250Aother%2520Application%2520Programming%2520Interfaces%2520%2528APIs%2529%252C%2520which%2520are%2520especially%2520important%250Afor%2520third-party%2520libraries.%2520With%2520the%2520rise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520the%250Akey%2520goal%2520is%2520to%2520investigate%2520the%2520kinds%2520of%2520description%2520developers%2520commonly%2520use%2520and%250Aevaluate%2520how%2520well%2520an%2520LLM%252C%2520in%2520this%2520case%2520Llama%252C%2520can%2520support%2520description%250Ageneration.%2520We%2520use%2520NPM%2520Code%2520Snippets%252C%2520consisting%2520of%2520185%252C412%2520packages%2520with%250A1%252C024%252C579%2520code%2520snippets.%2520From%2520there%252C%2520we%2520use%2520400%2520code%2520snippets%2520%2528and%2520their%250Adescriptions%2529%2520as%2520samples.%2520First%252C%2520our%2520manual%2520classification%2520found%2520that%2520the%250Amajority%2520of%2520original%2520descriptions%2520%252855.5%2525%2529%2520highlight%2520example-based%2520usage.%2520This%250Afinding%2520emphasizes%2520the%2520importance%2520of%2520clear%2520documentation%252C%2520as%2520some%2520descriptions%250Alacked%2520sufficient%2520detail%2520to%2520convey%2520intent.%2520Second%252C%2520the%2520LLM%2520correctly%2520identified%250Athe%2520majority%2520of%2520original%2520descriptions%2520as%2520%2522Example%2522%2520%252879.75%2525%2529%252C%2520which%2520is%2520identical%250Ato%2520our%2520manual%2520finding%252C%2520showing%2520a%2520propensity%2520for%2520generalization.%2520Third%252C%2520compared%250Ato%2520the%2520originals%252C%2520the%2520produced%2520description%2520had%2520an%2520average%2520similarity%2520score%2520of%250A0.7173%252C%2520suggesting%2520relevance%2520but%2520room%2520for%2520improvement.%2520Scores%2520below%25200.9%250Aindicate%2520some%2520irrelevance.%2520Our%2520results%2520show%2520that%2520depending%2520on%2520the%2520task%2520of%2520the%250Acode%2520snippet%252C%2520the%2520intention%2520of%2520the%2520document%2520may%2520differ%2520from%2520being%2520instructions%250Afor%2520usage%252C%2520installations%252C%2520or%2520descriptive%2520learning%2520examples%2520for%2520any%2520user%2520of%2520a%250Alibrary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Intention%20through%20LLM-Driven%20Code%20Snippet%20Description%0A%20%20Generation&entry.906535625=Yusuf%20Sulistyo%20Nugroho%20and%20Farah%20Danisha%20Salam%20and%20Brittany%20Reid%20and%20Raula%20Gaikovina%20Kula%20and%20Kazumasa%20Shimari%20and%20Kenichi%20Matsumoto&entry.1292438233=%20%20Documenting%20code%20snippets%20is%20essential%20to%20pinpoint%20key%20areas%20where%20both%0Adevelopers%20and%20users%20should%20pay%20attention.%20Examples%20include%20usage%20examples%20and%0Aother%20Application%20Programming%20Interfaces%20%28APIs%29%2C%20which%20are%20especially%20important%0Afor%20third-party%20libraries.%20With%20the%20rise%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20the%0Akey%20goal%20is%20to%20investigate%20the%20kinds%20of%20description%20developers%20commonly%20use%20and%0Aevaluate%20how%20well%20an%20LLM%2C%20in%20this%20case%20Llama%2C%20can%20support%20description%0Ageneration.%20We%20use%20NPM%20Code%20Snippets%2C%20consisting%20of%20185%2C412%20packages%20with%0A1%2C024%2C579%20code%20snippets.%20From%20there%2C%20we%20use%20400%20code%20snippets%20%28and%20their%0Adescriptions%29%20as%20samples.%20First%2C%20our%20manual%20classification%20found%20that%20the%0Amajority%20of%20original%20descriptions%20%2855.5%25%29%20highlight%20example-based%20usage.%20This%0Afinding%20emphasizes%20the%20importance%20of%20clear%20documentation%2C%20as%20some%20descriptions%0Alacked%20sufficient%20detail%20to%20convey%20intent.%20Second%2C%20the%20LLM%20correctly%20identified%0Athe%20majority%20of%20original%20descriptions%20as%20%22Example%22%20%2879.75%25%29%2C%20which%20is%20identical%0Ato%20our%20manual%20finding%2C%20showing%20a%20propensity%20for%20generalization.%20Third%2C%20compared%0Ato%20the%20originals%2C%20the%20produced%20description%20had%20an%20average%20similarity%20score%20of%0A0.7173%2C%20suggesting%20relevance%20but%20room%20for%20improvement.%20Scores%20below%200.9%0Aindicate%20some%20irrelevance.%20Our%20results%20show%20that%20depending%20on%20the%20task%20of%20the%0Acode%20snippet%2C%20the%20intention%20of%20the%20document%20may%20differ%20from%20being%20instructions%0Afor%20usage%2C%20installations%2C%20or%20descriptive%20learning%20examples%20for%20any%20user%20of%20a%0Alibrary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15453v1&entry.124074799=Read"},
{"title": "Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image\n  Registration", "author": "Kyobin Choo and Hyunkyung Han and Jinyeong Kim and Chanyong Yoon and Seong Jae Hwang", "abstract": "  In clinical practice, imaging modalities with functional characteristics,\nsuch as positron emission tomography (PET) and fractional anisotropy (FA), are\noften aligned with a structural reference (e.g., MRI, CT) for accurate\ninterpretation or group analysis, necessitating multi-modal deformable image\nregistration (DIR). However, due to the extreme heterogeneity of these\nmodalities compared to standard structural scans, conventional unsupervised DIR\nmethods struggle to learn reliable spatial mappings and often distort images.\nWe find that the similarity metrics guiding these models fail to capture\nalignment between highly disparate modalities. To address this, we propose\nM2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal\nDIR models using only mono-modal similarity while preserving the established\narchitectural paradigm for seamless integration into existing models. We also\nintroduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training\nscheme to promote diffeomorphism. Furthermore, our framework naturally extends\nto a semi-supervised setting, integrating pre-aligned and unaligned pairs only,\nwithout requiring ground-truth transformations or segmentation masks.\nExperiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\ndemonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for\nPET-MRI and FA-MRI registration, highlighting its effectiveness in handling\nhighly heterogeneous multi-modal DIR. Our code is available at\nhttps://github.com/MICV-yonsei/M2M-Reg.\n", "link": "http://arxiv.org/abs/2506.15596v1", "date": "2025-06-18", "relevancy": 2.2434, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5968}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5416}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mono-Modalizing%20Extremely%20Heterogeneous%20Multi-Modal%20Medical%20Image%0A%20%20Registration&body=Title%3A%20Mono-Modalizing%20Extremely%20Heterogeneous%20Multi-Modal%20Medical%20Image%0A%20%20Registration%0AAuthor%3A%20Kyobin%20Choo%20and%20Hyunkyung%20Han%20and%20Jinyeong%20Kim%20and%20Chanyong%20Yoon%20and%20Seong%20Jae%20Hwang%0AAbstract%3A%20%20%20In%20clinical%20practice%2C%20imaging%20modalities%20with%20functional%20characteristics%2C%0Asuch%20as%20positron%20emission%20tomography%20%28PET%29%20and%20fractional%20anisotropy%20%28FA%29%2C%20are%0Aoften%20aligned%20with%20a%20structural%20reference%20%28e.g.%2C%20MRI%2C%20CT%29%20for%20accurate%0Ainterpretation%20or%20group%20analysis%2C%20necessitating%20multi-modal%20deformable%20image%0Aregistration%20%28DIR%29.%20However%2C%20due%20to%20the%20extreme%20heterogeneity%20of%20these%0Amodalities%20compared%20to%20standard%20structural%20scans%2C%20conventional%20unsupervised%20DIR%0Amethods%20struggle%20to%20learn%20reliable%20spatial%20mappings%20and%20often%20distort%20images.%0AWe%20find%20that%20the%20similarity%20metrics%20guiding%20these%20models%20fail%20to%20capture%0Aalignment%20between%20highly%20disparate%20modalities.%20To%20address%20this%2C%20we%20propose%0AM2M-Reg%20%28Multi-to-Mono%20Registration%29%2C%20a%20novel%20framework%20that%20trains%20multi-modal%0ADIR%20models%20using%20only%20mono-modal%20similarity%20while%20preserving%20the%20established%0Aarchitectural%20paradigm%20for%20seamless%20integration%20into%20existing%20models.%20We%20also%0Aintroduce%20GradCyCon%2C%20a%20regularizer%20that%20leverages%20M2M-Reg%27s%20cyclic%20training%0Ascheme%20to%20promote%20diffeomorphism.%20Furthermore%2C%20our%20framework%20naturally%20extends%0Ato%20a%20semi-supervised%20setting%2C%20integrating%20pre-aligned%20and%20unaligned%20pairs%20only%2C%0Awithout%20requiring%20ground-truth%20transformations%20or%20segmentation%20masks.%0AExperiments%20on%20the%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%20dataset%0Ademonstrate%20that%20M2M-Reg%20achieves%20up%20to%202x%20higher%20DSC%20than%20prior%20methods%20for%0APET-MRI%20and%20FA-MRI%20registration%2C%20highlighting%20its%20effectiveness%20in%20handling%0Ahighly%20heterogeneous%20multi-modal%20DIR.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/MICV-yonsei/M2M-Reg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMono-Modalizing%2520Extremely%2520Heterogeneous%2520Multi-Modal%2520Medical%2520Image%250A%2520%2520Registration%26entry.906535625%3DKyobin%2520Choo%2520and%2520Hyunkyung%2520Han%2520and%2520Jinyeong%2520Kim%2520and%2520Chanyong%2520Yoon%2520and%2520Seong%2520Jae%2520Hwang%26entry.1292438233%3D%2520%2520In%2520clinical%2520practice%252C%2520imaging%2520modalities%2520with%2520functional%2520characteristics%252C%250Asuch%2520as%2520positron%2520emission%2520tomography%2520%2528PET%2529%2520and%2520fractional%2520anisotropy%2520%2528FA%2529%252C%2520are%250Aoften%2520aligned%2520with%2520a%2520structural%2520reference%2520%2528e.g.%252C%2520MRI%252C%2520CT%2529%2520for%2520accurate%250Ainterpretation%2520or%2520group%2520analysis%252C%2520necessitating%2520multi-modal%2520deformable%2520image%250Aregistration%2520%2528DIR%2529.%2520However%252C%2520due%2520to%2520the%2520extreme%2520heterogeneity%2520of%2520these%250Amodalities%2520compared%2520to%2520standard%2520structural%2520scans%252C%2520conventional%2520unsupervised%2520DIR%250Amethods%2520struggle%2520to%2520learn%2520reliable%2520spatial%2520mappings%2520and%2520often%2520distort%2520images.%250AWe%2520find%2520that%2520the%2520similarity%2520metrics%2520guiding%2520these%2520models%2520fail%2520to%2520capture%250Aalignment%2520between%2520highly%2520disparate%2520modalities.%2520To%2520address%2520this%252C%2520we%2520propose%250AM2M-Reg%2520%2528Multi-to-Mono%2520Registration%2529%252C%2520a%2520novel%2520framework%2520that%2520trains%2520multi-modal%250ADIR%2520models%2520using%2520only%2520mono-modal%2520similarity%2520while%2520preserving%2520the%2520established%250Aarchitectural%2520paradigm%2520for%2520seamless%2520integration%2520into%2520existing%2520models.%2520We%2520also%250Aintroduce%2520GradCyCon%252C%2520a%2520regularizer%2520that%2520leverages%2520M2M-Reg%2527s%2520cyclic%2520training%250Ascheme%2520to%2520promote%2520diffeomorphism.%2520Furthermore%252C%2520our%2520framework%2520naturally%2520extends%250Ato%2520a%2520semi-supervised%2520setting%252C%2520integrating%2520pre-aligned%2520and%2520unaligned%2520pairs%2520only%252C%250Awithout%2520requiring%2520ground-truth%2520transformations%2520or%2520segmentation%2520masks.%250AExperiments%2520on%2520the%2520Alzheimer%2527s%2520Disease%2520Neuroimaging%2520Initiative%2520%2528ADNI%2529%2520dataset%250Ademonstrate%2520that%2520M2M-Reg%2520achieves%2520up%2520to%25202x%2520higher%2520DSC%2520than%2520prior%2520methods%2520for%250APET-MRI%2520and%2520FA-MRI%2520registration%252C%2520highlighting%2520its%2520effectiveness%2520in%2520handling%250Ahighly%2520heterogeneous%2520multi-modal%2520DIR.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/MICV-yonsei/M2M-Reg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mono-Modalizing%20Extremely%20Heterogeneous%20Multi-Modal%20Medical%20Image%0A%20%20Registration&entry.906535625=Kyobin%20Choo%20and%20Hyunkyung%20Han%20and%20Jinyeong%20Kim%20and%20Chanyong%20Yoon%20and%20Seong%20Jae%20Hwang&entry.1292438233=%20%20In%20clinical%20practice%2C%20imaging%20modalities%20with%20functional%20characteristics%2C%0Asuch%20as%20positron%20emission%20tomography%20%28PET%29%20and%20fractional%20anisotropy%20%28FA%29%2C%20are%0Aoften%20aligned%20with%20a%20structural%20reference%20%28e.g.%2C%20MRI%2C%20CT%29%20for%20accurate%0Ainterpretation%20or%20group%20analysis%2C%20necessitating%20multi-modal%20deformable%20image%0Aregistration%20%28DIR%29.%20However%2C%20due%20to%20the%20extreme%20heterogeneity%20of%20these%0Amodalities%20compared%20to%20standard%20structural%20scans%2C%20conventional%20unsupervised%20DIR%0Amethods%20struggle%20to%20learn%20reliable%20spatial%20mappings%20and%20often%20distort%20images.%0AWe%20find%20that%20the%20similarity%20metrics%20guiding%20these%20models%20fail%20to%20capture%0Aalignment%20between%20highly%20disparate%20modalities.%20To%20address%20this%2C%20we%20propose%0AM2M-Reg%20%28Multi-to-Mono%20Registration%29%2C%20a%20novel%20framework%20that%20trains%20multi-modal%0ADIR%20models%20using%20only%20mono-modal%20similarity%20while%20preserving%20the%20established%0Aarchitectural%20paradigm%20for%20seamless%20integration%20into%20existing%20models.%20We%20also%0Aintroduce%20GradCyCon%2C%20a%20regularizer%20that%20leverages%20M2M-Reg%27s%20cyclic%20training%0Ascheme%20to%20promote%20diffeomorphism.%20Furthermore%2C%20our%20framework%20naturally%20extends%0Ato%20a%20semi-supervised%20setting%2C%20integrating%20pre-aligned%20and%20unaligned%20pairs%20only%2C%0Awithout%20requiring%20ground-truth%20transformations%20or%20segmentation%20masks.%0AExperiments%20on%20the%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%20dataset%0Ademonstrate%20that%20M2M-Reg%20achieves%20up%20to%202x%20higher%20DSC%20than%20prior%20methods%20for%0APET-MRI%20and%20FA-MRI%20registration%2C%20highlighting%20its%20effectiveness%20in%20handling%0Ahighly%20heterogeneous%20multi-modal%20DIR.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/MICV-yonsei/M2M-Reg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15596v1&entry.124074799=Read"},
{"title": "Style-Preserving Lip Sync via Audio-Aware Style Reference", "author": "Weizhi Zhong and Jichang Li and Yinqi Cai and Ming Li and Feng Gao and Liang Lin and Guanbin Li", "abstract": "  Audio-driven lip sync has recently drawn significant attention due to its\nwidespread application in the multimedia domain. Individuals exhibit distinct\nlip shapes when speaking the same utterance, attributed to the unique speaking\nstyles of individuals, posing a notable challenge for audio-driven lip sync.\nEarlier methods for such task often bypassed the modeling of personalized\nspeaking styles, resulting in sub-optimal lip sync conforming to the general\nstyles. Recent lip sync techniques attempt to guide the lip sync for arbitrary\naudio by aggregating information from a style reference video, yet they can not\npreserve the speaking styles well due to their inaccuracy in style aggregation.\nThis work proposes an innovative audio-aware style reference scheme that\neffectively leverages the relationships between input audio and reference audio\nfrom style reference video to address the style-preserving audio-driven lip\nsync. Specifically, we first develop an advanced Transformer-based model adept\nat predicting lip motion corresponding to the input audio, augmented by the\nstyle information aggregated through cross-attention layers from style\nreference video. Afterwards, to better render the lip motion into realistic\ntalking face video, we devise a conditional latent diffusion model, integrating\nlip motion through modulated convolutional layers and fusing reference facial\nimages via spatial cross-attention layers. Extensive experiments validate the\nefficacy of the proposed approach in achieving precise lip sync, preserving\nspeaking styles, and generating high-fidelity, realistic talking face videos.\n", "link": "http://arxiv.org/abs/2408.05412v2", "date": "2025-06-18", "relevancy": 2.2405, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6144}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5629}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Style-Preserving%20Lip%20Sync%20via%20Audio-Aware%20Style%20Reference&body=Title%3A%20Style-Preserving%20Lip%20Sync%20via%20Audio-Aware%20Style%20Reference%0AAuthor%3A%20Weizhi%20Zhong%20and%20Jichang%20Li%20and%20Yinqi%20Cai%20and%20Ming%20Li%20and%20Feng%20Gao%20and%20Liang%20Lin%20and%20Guanbin%20Li%0AAbstract%3A%20%20%20Audio-driven%20lip%20sync%20has%20recently%20drawn%20significant%20attention%20due%20to%20its%0Awidespread%20application%20in%20the%20multimedia%20domain.%20Individuals%20exhibit%20distinct%0Alip%20shapes%20when%20speaking%20the%20same%20utterance%2C%20attributed%20to%20the%20unique%20speaking%0Astyles%20of%20individuals%2C%20posing%20a%20notable%20challenge%20for%20audio-driven%20lip%20sync.%0AEarlier%20methods%20for%20such%20task%20often%20bypassed%20the%20modeling%20of%20personalized%0Aspeaking%20styles%2C%20resulting%20in%20sub-optimal%20lip%20sync%20conforming%20to%20the%20general%0Astyles.%20Recent%20lip%20sync%20techniques%20attempt%20to%20guide%20the%20lip%20sync%20for%20arbitrary%0Aaudio%20by%20aggregating%20information%20from%20a%20style%20reference%20video%2C%20yet%20they%20can%20not%0Apreserve%20the%20speaking%20styles%20well%20due%20to%20their%20inaccuracy%20in%20style%20aggregation.%0AThis%20work%20proposes%20an%20innovative%20audio-aware%20style%20reference%20scheme%20that%0Aeffectively%20leverages%20the%20relationships%20between%20input%20audio%20and%20reference%20audio%0Afrom%20style%20reference%20video%20to%20address%20the%20style-preserving%20audio-driven%20lip%0Async.%20Specifically%2C%20we%20first%20develop%20an%20advanced%20Transformer-based%20model%20adept%0Aat%20predicting%20lip%20motion%20corresponding%20to%20the%20input%20audio%2C%20augmented%20by%20the%0Astyle%20information%20aggregated%20through%20cross-attention%20layers%20from%20style%0Areference%20video.%20Afterwards%2C%20to%20better%20render%20the%20lip%20motion%20into%20realistic%0Atalking%20face%20video%2C%20we%20devise%20a%20conditional%20latent%20diffusion%20model%2C%20integrating%0Alip%20motion%20through%20modulated%20convolutional%20layers%20and%20fusing%20reference%20facial%0Aimages%20via%20spatial%20cross-attention%20layers.%20Extensive%20experiments%20validate%20the%0Aefficacy%20of%20the%20proposed%20approach%20in%20achieving%20precise%20lip%20sync%2C%20preserving%0Aspeaking%20styles%2C%20and%20generating%20high-fidelity%2C%20realistic%20talking%20face%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyle-Preserving%2520Lip%2520Sync%2520via%2520Audio-Aware%2520Style%2520Reference%26entry.906535625%3DWeizhi%2520Zhong%2520and%2520Jichang%2520Li%2520and%2520Yinqi%2520Cai%2520and%2520Ming%2520Li%2520and%2520Feng%2520Gao%2520and%2520Liang%2520Lin%2520and%2520Guanbin%2520Li%26entry.1292438233%3D%2520%2520Audio-driven%2520lip%2520sync%2520has%2520recently%2520drawn%2520significant%2520attention%2520due%2520to%2520its%250Awidespread%2520application%2520in%2520the%2520multimedia%2520domain.%2520Individuals%2520exhibit%2520distinct%250Alip%2520shapes%2520when%2520speaking%2520the%2520same%2520utterance%252C%2520attributed%2520to%2520the%2520unique%2520speaking%250Astyles%2520of%2520individuals%252C%2520posing%2520a%2520notable%2520challenge%2520for%2520audio-driven%2520lip%2520sync.%250AEarlier%2520methods%2520for%2520such%2520task%2520often%2520bypassed%2520the%2520modeling%2520of%2520personalized%250Aspeaking%2520styles%252C%2520resulting%2520in%2520sub-optimal%2520lip%2520sync%2520conforming%2520to%2520the%2520general%250Astyles.%2520Recent%2520lip%2520sync%2520techniques%2520attempt%2520to%2520guide%2520the%2520lip%2520sync%2520for%2520arbitrary%250Aaudio%2520by%2520aggregating%2520information%2520from%2520a%2520style%2520reference%2520video%252C%2520yet%2520they%2520can%2520not%250Apreserve%2520the%2520speaking%2520styles%2520well%2520due%2520to%2520their%2520inaccuracy%2520in%2520style%2520aggregation.%250AThis%2520work%2520proposes%2520an%2520innovative%2520audio-aware%2520style%2520reference%2520scheme%2520that%250Aeffectively%2520leverages%2520the%2520relationships%2520between%2520input%2520audio%2520and%2520reference%2520audio%250Afrom%2520style%2520reference%2520video%2520to%2520address%2520the%2520style-preserving%2520audio-driven%2520lip%250Async.%2520Specifically%252C%2520we%2520first%2520develop%2520an%2520advanced%2520Transformer-based%2520model%2520adept%250Aat%2520predicting%2520lip%2520motion%2520corresponding%2520to%2520the%2520input%2520audio%252C%2520augmented%2520by%2520the%250Astyle%2520information%2520aggregated%2520through%2520cross-attention%2520layers%2520from%2520style%250Areference%2520video.%2520Afterwards%252C%2520to%2520better%2520render%2520the%2520lip%2520motion%2520into%2520realistic%250Atalking%2520face%2520video%252C%2520we%2520devise%2520a%2520conditional%2520latent%2520diffusion%2520model%252C%2520integrating%250Alip%2520motion%2520through%2520modulated%2520convolutional%2520layers%2520and%2520fusing%2520reference%2520facial%250Aimages%2520via%2520spatial%2520cross-attention%2520layers.%2520Extensive%2520experiments%2520validate%2520the%250Aefficacy%2520of%2520the%2520proposed%2520approach%2520in%2520achieving%2520precise%2520lip%2520sync%252C%2520preserving%250Aspeaking%2520styles%252C%2520and%2520generating%2520high-fidelity%252C%2520realistic%2520talking%2520face%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Style-Preserving%20Lip%20Sync%20via%20Audio-Aware%20Style%20Reference&entry.906535625=Weizhi%20Zhong%20and%20Jichang%20Li%20and%20Yinqi%20Cai%20and%20Ming%20Li%20and%20Feng%20Gao%20and%20Liang%20Lin%20and%20Guanbin%20Li&entry.1292438233=%20%20Audio-driven%20lip%20sync%20has%20recently%20drawn%20significant%20attention%20due%20to%20its%0Awidespread%20application%20in%20the%20multimedia%20domain.%20Individuals%20exhibit%20distinct%0Alip%20shapes%20when%20speaking%20the%20same%20utterance%2C%20attributed%20to%20the%20unique%20speaking%0Astyles%20of%20individuals%2C%20posing%20a%20notable%20challenge%20for%20audio-driven%20lip%20sync.%0AEarlier%20methods%20for%20such%20task%20often%20bypassed%20the%20modeling%20of%20personalized%0Aspeaking%20styles%2C%20resulting%20in%20sub-optimal%20lip%20sync%20conforming%20to%20the%20general%0Astyles.%20Recent%20lip%20sync%20techniques%20attempt%20to%20guide%20the%20lip%20sync%20for%20arbitrary%0Aaudio%20by%20aggregating%20information%20from%20a%20style%20reference%20video%2C%20yet%20they%20can%20not%0Apreserve%20the%20speaking%20styles%20well%20due%20to%20their%20inaccuracy%20in%20style%20aggregation.%0AThis%20work%20proposes%20an%20innovative%20audio-aware%20style%20reference%20scheme%20that%0Aeffectively%20leverages%20the%20relationships%20between%20input%20audio%20and%20reference%20audio%0Afrom%20style%20reference%20video%20to%20address%20the%20style-preserving%20audio-driven%20lip%0Async.%20Specifically%2C%20we%20first%20develop%20an%20advanced%20Transformer-based%20model%20adept%0Aat%20predicting%20lip%20motion%20corresponding%20to%20the%20input%20audio%2C%20augmented%20by%20the%0Astyle%20information%20aggregated%20through%20cross-attention%20layers%20from%20style%0Areference%20video.%20Afterwards%2C%20to%20better%20render%20the%20lip%20motion%20into%20realistic%0Atalking%20face%20video%2C%20we%20devise%20a%20conditional%20latent%20diffusion%20model%2C%20integrating%0Alip%20motion%20through%20modulated%20convolutional%20layers%20and%20fusing%20reference%20facial%0Aimages%20via%20spatial%20cross-attention%20layers.%20Extensive%20experiments%20validate%20the%0Aefficacy%20of%20the%20proposed%20approach%20in%20achieving%20precise%20lip%20sync%2C%20preserving%0Aspeaking%20styles%2C%20and%20generating%20high-fidelity%2C%20realistic%20talking%20face%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05412v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey on Continual Learning in Generative Models", "author": "Haiyang Guo and Fanhu Zeng and Fei Zhu and Jiayi Wang and Xukai Wang and Jingang Zhou and Hongbo Zhao and Wenzhuo Liu and Shijie Ma and Da-Han Wang and Xu-Yao Zhang and Cheng-Lin Liu", "abstract": "  The rapid advancement of generative models has enabled modern AI systems to\ncomprehend and produce highly sophisticated content, even achieving human-level\nperformance in specific domains. However, these models remain fundamentally\nconstrained by catastrophic forgetting - a persistent challenge where adapting\nto new tasks typically leads to significant degradation in performance on\npreviously learned tasks. To address this practical limitation, numerous\napproaches have been proposed to enhance the adaptability and scalability of\ngenerative models in real-world applications. In this work, we present a\ncomprehensive survey of continual learning methods for mainstream generative\nmodels, including large language models, multimodal large language models,\nvision language action models, and diffusion models. Drawing inspiration from\nthe memory mechanisms of the human brain, we systematically categorize these\napproaches into three paradigms: architecture-based, regularization-based, and\nreplay-based methods, while elucidating their underlying methodologies and\nmotivations. We further analyze continual learning setups for different\ngenerative models, including training objectives, benchmarks, and core\nbackbones, offering deeper insights into the field. The project page of this\npaper is available at\nhttps://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.\n", "link": "http://arxiv.org/abs/2506.13045v2", "date": "2025-06-18", "relevancy": 2.2305, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5759}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5675}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20on%20Continual%20Learning%20in%20Generative%20Models&body=Title%3A%20A%20Comprehensive%20Survey%20on%20Continual%20Learning%20in%20Generative%20Models%0AAuthor%3A%20Haiyang%20Guo%20and%20Fanhu%20Zeng%20and%20Fei%20Zhu%20and%20Jiayi%20Wang%20and%20Xukai%20Wang%20and%20Jingang%20Zhou%20and%20Hongbo%20Zhao%20and%20Wenzhuo%20Liu%20and%20Shijie%20Ma%20and%20Da-Han%20Wang%20and%20Xu-Yao%20Zhang%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20generative%20models%20has%20enabled%20modern%20AI%20systems%20to%0Acomprehend%20and%20produce%20highly%20sophisticated%20content%2C%20even%20achieving%20human-level%0Aperformance%20in%20specific%20domains.%20However%2C%20these%20models%20remain%20fundamentally%0Aconstrained%20by%20catastrophic%20forgetting%20-%20a%20persistent%20challenge%20where%20adapting%0Ato%20new%20tasks%20typically%20leads%20to%20significant%20degradation%20in%20performance%20on%0Apreviously%20learned%20tasks.%20To%20address%20this%20practical%20limitation%2C%20numerous%0Aapproaches%20have%20been%20proposed%20to%20enhance%20the%20adaptability%20and%20scalability%20of%0Agenerative%20models%20in%20real-world%20applications.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20survey%20of%20continual%20learning%20methods%20for%20mainstream%20generative%0Amodels%2C%20including%20large%20language%20models%2C%20multimodal%20large%20language%20models%2C%0Avision%20language%20action%20models%2C%20and%20diffusion%20models.%20Drawing%20inspiration%20from%0Athe%20memory%20mechanisms%20of%20the%20human%20brain%2C%20we%20systematically%20categorize%20these%0Aapproaches%20into%20three%20paradigms%3A%20architecture-based%2C%20regularization-based%2C%20and%0Areplay-based%20methods%2C%20while%20elucidating%20their%20underlying%20methodologies%20and%0Amotivations.%20We%20further%20analyze%20continual%20learning%20setups%20for%20different%0Agenerative%20models%2C%20including%20training%20objectives%2C%20benchmarks%2C%20and%20core%0Abackbones%2C%20offering%20deeper%20insights%20into%20the%20field.%20The%20project%20page%20of%20this%0Apaper%20is%20available%20at%0Ahttps%3A//github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520on%2520Continual%2520Learning%2520in%2520Generative%2520Models%26entry.906535625%3DHaiyang%2520Guo%2520and%2520Fanhu%2520Zeng%2520and%2520Fei%2520Zhu%2520and%2520Jiayi%2520Wang%2520and%2520Xukai%2520Wang%2520and%2520Jingang%2520Zhou%2520and%2520Hongbo%2520Zhao%2520and%2520Wenzhuo%2520Liu%2520and%2520Shijie%2520Ma%2520and%2520Da-Han%2520Wang%2520and%2520Xu-Yao%2520Zhang%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520generative%2520models%2520has%2520enabled%2520modern%2520AI%2520systems%2520to%250Acomprehend%2520and%2520produce%2520highly%2520sophisticated%2520content%252C%2520even%2520achieving%2520human-level%250Aperformance%2520in%2520specific%2520domains.%2520However%252C%2520these%2520models%2520remain%2520fundamentally%250Aconstrained%2520by%2520catastrophic%2520forgetting%2520-%2520a%2520persistent%2520challenge%2520where%2520adapting%250Ato%2520new%2520tasks%2520typically%2520leads%2520to%2520significant%2520degradation%2520in%2520performance%2520on%250Apreviously%2520learned%2520tasks.%2520To%2520address%2520this%2520practical%2520limitation%252C%2520numerous%250Aapproaches%2520have%2520been%2520proposed%2520to%2520enhance%2520the%2520adaptability%2520and%2520scalability%2520of%250Agenerative%2520models%2520in%2520real-world%2520applications.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Acomprehensive%2520survey%2520of%2520continual%2520learning%2520methods%2520for%2520mainstream%2520generative%250Amodels%252C%2520including%2520large%2520language%2520models%252C%2520multimodal%2520large%2520language%2520models%252C%250Avision%2520language%2520action%2520models%252C%2520and%2520diffusion%2520models.%2520Drawing%2520inspiration%2520from%250Athe%2520memory%2520mechanisms%2520of%2520the%2520human%2520brain%252C%2520we%2520systematically%2520categorize%2520these%250Aapproaches%2520into%2520three%2520paradigms%253A%2520architecture-based%252C%2520regularization-based%252C%2520and%250Areplay-based%2520methods%252C%2520while%2520elucidating%2520their%2520underlying%2520methodologies%2520and%250Amotivations.%2520We%2520further%2520analyze%2520continual%2520learning%2520setups%2520for%2520different%250Agenerative%2520models%252C%2520including%2520training%2520objectives%252C%2520benchmarks%252C%2520and%2520core%250Abackbones%252C%2520offering%2520deeper%2520insights%2520into%2520the%2520field.%2520The%2520project%2520page%2520of%2520this%250Apaper%2520is%2520available%2520at%250Ahttps%253A//github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20on%20Continual%20Learning%20in%20Generative%20Models&entry.906535625=Haiyang%20Guo%20and%20Fanhu%20Zeng%20and%20Fei%20Zhu%20and%20Jiayi%20Wang%20and%20Xukai%20Wang%20and%20Jingang%20Zhou%20and%20Hongbo%20Zhao%20and%20Wenzhuo%20Liu%20and%20Shijie%20Ma%20and%20Da-Han%20Wang%20and%20Xu-Yao%20Zhang%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20generative%20models%20has%20enabled%20modern%20AI%20systems%20to%0Acomprehend%20and%20produce%20highly%20sophisticated%20content%2C%20even%20achieving%20human-level%0Aperformance%20in%20specific%20domains.%20However%2C%20these%20models%20remain%20fundamentally%0Aconstrained%20by%20catastrophic%20forgetting%20-%20a%20persistent%20challenge%20where%20adapting%0Ato%20new%20tasks%20typically%20leads%20to%20significant%20degradation%20in%20performance%20on%0Apreviously%20learned%20tasks.%20To%20address%20this%20practical%20limitation%2C%20numerous%0Aapproaches%20have%20been%20proposed%20to%20enhance%20the%20adaptability%20and%20scalability%20of%0Agenerative%20models%20in%20real-world%20applications.%20In%20this%20work%2C%20we%20present%20a%0Acomprehensive%20survey%20of%20continual%20learning%20methods%20for%20mainstream%20generative%0Amodels%2C%20including%20large%20language%20models%2C%20multimodal%20large%20language%20models%2C%0Avision%20language%20action%20models%2C%20and%20diffusion%20models.%20Drawing%20inspiration%20from%0Athe%20memory%20mechanisms%20of%20the%20human%20brain%2C%20we%20systematically%20categorize%20these%0Aapproaches%20into%20three%20paradigms%3A%20architecture-based%2C%20regularization-based%2C%20and%0Areplay-based%20methods%2C%20while%20elucidating%20their%20underlying%20methodologies%20and%0Amotivations.%20We%20further%20analyze%20continual%20learning%20setups%20for%20different%0Agenerative%20models%2C%20including%20training%20objectives%2C%20benchmarks%2C%20and%20core%0Abackbones%2C%20offering%20deeper%20insights%20into%20the%20field.%20The%20project%20page%20of%20this%0Apaper%20is%20available%20at%0Ahttps%3A//github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13045v2&entry.124074799=Read"},
{"title": "DAILOC: Domain-Incremental Learning for Indoor Localization using\n  Smartphones", "author": "Akhil Singampalli and Danish Gufran and Sudeep Pasricha", "abstract": "  Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.\n", "link": "http://arxiv.org/abs/2506.15554v1", "date": "2025-06-18", "relevancy": 2.225, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5851}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5475}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAILOC%3A%20Domain-Incremental%20Learning%20for%20Indoor%20Localization%20using%0A%20%20Smartphones&body=Title%3A%20DAILOC%3A%20Domain-Incremental%20Learning%20for%20Indoor%20Localization%20using%0A%20%20Smartphones%0AAuthor%3A%20Akhil%20Singampalli%20and%20Danish%20Gufran%20and%20Sudeep%20Pasricha%0AAbstract%3A%20%20%20Wi-Fi%20fingerprinting-based%20indoor%20localization%20faces%20significant%20challenges%0Ain%20real-world%20deployments%20due%20to%20domain%20shifts%20arising%20from%20device%0Aheterogeneity%20and%20temporal%20variations%20within%20indoor%20environments.%20Existing%0Aapproaches%20often%20address%20these%20issues%20independently%2C%20resulting%20in%20poor%0Ageneralization%20and%20susceptibility%20to%20catastrophic%20forgetting%20over%20time.%20In%20this%0Awork%2C%20we%20propose%20DAILOC%2C%20a%20novel%20domain-incremental%20learning%20framework%20that%0Ajointly%20addresses%20both%20temporal%20and%20device-induced%20domain%20shifts.%20DAILOC%0Aintroduces%20a%20novel%20disentanglement%20strategy%20that%20separates%20domain%20shifts%20from%0Alocation-relevant%20features%20using%20a%20multi-level%20variational%20autoencoder.%0AAdditionally%2C%20we%20introduce%20a%20novel%20memory-guided%20class%20latent%20alignment%0Amechanism%20to%20address%20the%20effects%20of%20catastrophic%20forgetting%20over%20time.%0AExperiments%20across%20multiple%20smartphones%2C%20buildings%2C%20and%20time%20instances%0Ademonstrate%20that%20DAILOC%20significantly%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20up%20to%202.74x%20lower%20average%20error%20and%204.6x%20lower%20worst-case%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAILOC%253A%2520Domain-Incremental%2520Learning%2520for%2520Indoor%2520Localization%2520using%250A%2520%2520Smartphones%26entry.906535625%3DAkhil%2520Singampalli%2520and%2520Danish%2520Gufran%2520and%2520Sudeep%2520Pasricha%26entry.1292438233%3D%2520%2520Wi-Fi%2520fingerprinting-based%2520indoor%2520localization%2520faces%2520significant%2520challenges%250Ain%2520real-world%2520deployments%2520due%2520to%2520domain%2520shifts%2520arising%2520from%2520device%250Aheterogeneity%2520and%2520temporal%2520variations%2520within%2520indoor%2520environments.%2520Existing%250Aapproaches%2520often%2520address%2520these%2520issues%2520independently%252C%2520resulting%2520in%2520poor%250Ageneralization%2520and%2520susceptibility%2520to%2520catastrophic%2520forgetting%2520over%2520time.%2520In%2520this%250Awork%252C%2520we%2520propose%2520DAILOC%252C%2520a%2520novel%2520domain-incremental%2520learning%2520framework%2520that%250Ajointly%2520addresses%2520both%2520temporal%2520and%2520device-induced%2520domain%2520shifts.%2520DAILOC%250Aintroduces%2520a%2520novel%2520disentanglement%2520strategy%2520that%2520separates%2520domain%2520shifts%2520from%250Alocation-relevant%2520features%2520using%2520a%2520multi-level%2520variational%2520autoencoder.%250AAdditionally%252C%2520we%2520introduce%2520a%2520novel%2520memory-guided%2520class%2520latent%2520alignment%250Amechanism%2520to%2520address%2520the%2520effects%2520of%2520catastrophic%2520forgetting%2520over%2520time.%250AExperiments%2520across%2520multiple%2520smartphones%252C%2520buildings%252C%2520and%2520time%2520instances%250Ademonstrate%2520that%2520DAILOC%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%250Aachieving%2520up%2520to%25202.74x%2520lower%2520average%2520error%2520and%25204.6x%2520lower%2520worst-case%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAILOC%3A%20Domain-Incremental%20Learning%20for%20Indoor%20Localization%20using%0A%20%20Smartphones&entry.906535625=Akhil%20Singampalli%20and%20Danish%20Gufran%20and%20Sudeep%20Pasricha&entry.1292438233=%20%20Wi-Fi%20fingerprinting-based%20indoor%20localization%20faces%20significant%20challenges%0Ain%20real-world%20deployments%20due%20to%20domain%20shifts%20arising%20from%20device%0Aheterogeneity%20and%20temporal%20variations%20within%20indoor%20environments.%20Existing%0Aapproaches%20often%20address%20these%20issues%20independently%2C%20resulting%20in%20poor%0Ageneralization%20and%20susceptibility%20to%20catastrophic%20forgetting%20over%20time.%20In%20this%0Awork%2C%20we%20propose%20DAILOC%2C%20a%20novel%20domain-incremental%20learning%20framework%20that%0Ajointly%20addresses%20both%20temporal%20and%20device-induced%20domain%20shifts.%20DAILOC%0Aintroduces%20a%20novel%20disentanglement%20strategy%20that%20separates%20domain%20shifts%20from%0Alocation-relevant%20features%20using%20a%20multi-level%20variational%20autoencoder.%0AAdditionally%2C%20we%20introduce%20a%20novel%20memory-guided%20class%20latent%20alignment%0Amechanism%20to%20address%20the%20effects%20of%20catastrophic%20forgetting%20over%20time.%0AExperiments%20across%20multiple%20smartphones%2C%20buildings%2C%20and%20time%20instances%0Ademonstrate%20that%20DAILOC%20significantly%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20up%20to%202.74x%20lower%20average%20error%20and%204.6x%20lower%20worst-case%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15554v1&entry.124074799=Read"},
{"title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with\n  World Foundation Models", "author": "Xuanchi Ren and Yifan Lu and Tianshi Cao and Ruiyuan Gao and Shengyu Huang and Amirmojtaba Sabour and Tianchang Shen and Tobias Pfaff and Jay Zhangjie Wu and Runjian Chen and Seung Wook Kim and Jun Gao and Laura Leal-Taixe and Mike Chen and Sanja Fidler and Huan Ling", "abstract": "  Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams\n", "link": "http://arxiv.org/abs/2506.09042v3", "date": "2025-06-18", "relevancy": 2.2234, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5623}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5546}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cosmos-Drive-Dreams%3A%20Scalable%20Synthetic%20Driving%20Data%20Generation%20with%0A%20%20World%20Foundation%20Models&body=Title%3A%20Cosmos-Drive-Dreams%3A%20Scalable%20Synthetic%20Driving%20Data%20Generation%20with%0A%20%20World%20Foundation%20Models%0AAuthor%3A%20Xuanchi%20Ren%20and%20Yifan%20Lu%20and%20Tianshi%20Cao%20and%20Ruiyuan%20Gao%20and%20Shengyu%20Huang%20and%20Amirmojtaba%20Sabour%20and%20Tianchang%20Shen%20and%20Tobias%20Pfaff%20and%20Jay%20Zhangjie%20Wu%20and%20Runjian%20Chen%20and%20Seung%20Wook%20Kim%20and%20Jun%20Gao%20and%20Laura%20Leal-Taixe%20and%20Mike%20Chen%20and%20Sanja%20Fidler%20and%20Huan%20Ling%0AAbstract%3A%20%20%20Collecting%20and%20annotating%20real-world%20data%20for%20safety-critical%20physical%20AI%0Asystems%2C%20such%20as%20Autonomous%20Vehicle%20%28AV%29%2C%20is%20time-consuming%20and%20costly.%20It%20is%0Aespecially%20challenging%20to%20capture%20rare%20edge%20cases%2C%20which%20play%20a%20critical%20role%0Ain%20training%20and%20testing%20of%20an%20AV%20system.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20the%20Cosmos-Drive-Dreams%20-%20a%20synthetic%20data%20generation%20%28SDG%29%20pipeline%0Athat%20aims%20to%20generate%20challenging%20scenarios%20to%20facilitate%20downstream%20tasks%20such%0Aas%20perception%20and%20driving%20policy%20training.%20Powering%20this%20pipeline%20is%0ACosmos-Drive%2C%20a%20suite%20of%20models%20specialized%20from%20NVIDIA%20Cosmos%20world%20foundation%0Amodel%20for%20the%20driving%20domain%20and%20are%20capable%20of%20controllable%2C%20high-fidelity%2C%0Amulti-view%2C%20and%20spatiotemporally%20consistent%20driving%20video%20generation.%20We%0Ashowcase%20the%20utility%20of%20these%20models%20by%20applying%20Cosmos-Drive-Dreams%20to%20scale%0Athe%20quantity%20and%20diversity%20of%20driving%20datasets%20with%20high-fidelity%20and%0Achallenging%20scenarios.%20Experimentally%2C%20we%20demonstrate%20that%20our%20generated%20data%0Ahelps%20in%20mitigating%20long-tail%20distribution%20problems%20and%20enhances%20generalization%0Ain%20downstream%20tasks%20such%20as%203D%20lane%20detection%2C%203D%20object%20detection%20and%20driving%0Apolicy%20learning.%20We%20open%20source%20our%20pipeline%20toolkit%2C%20dataset%20and%20model%20weights%0Athrough%20the%20NVIDIA%27s%20Cosmos%20platform.%0A%20%20Project%20page%3A%20https%3A//research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09042v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCosmos-Drive-Dreams%253A%2520Scalable%2520Synthetic%2520Driving%2520Data%2520Generation%2520with%250A%2520%2520World%2520Foundation%2520Models%26entry.906535625%3DXuanchi%2520Ren%2520and%2520Yifan%2520Lu%2520and%2520Tianshi%2520Cao%2520and%2520Ruiyuan%2520Gao%2520and%2520Shengyu%2520Huang%2520and%2520Amirmojtaba%2520Sabour%2520and%2520Tianchang%2520Shen%2520and%2520Tobias%2520Pfaff%2520and%2520Jay%2520Zhangjie%2520Wu%2520and%2520Runjian%2520Chen%2520and%2520Seung%2520Wook%2520Kim%2520and%2520Jun%2520Gao%2520and%2520Laura%2520Leal-Taixe%2520and%2520Mike%2520Chen%2520and%2520Sanja%2520Fidler%2520and%2520Huan%2520Ling%26entry.1292438233%3D%2520%2520Collecting%2520and%2520annotating%2520real-world%2520data%2520for%2520safety-critical%2520physical%2520AI%250Asystems%252C%2520such%2520as%2520Autonomous%2520Vehicle%2520%2528AV%2529%252C%2520is%2520time-consuming%2520and%2520costly.%2520It%2520is%250Aespecially%2520challenging%2520to%2520capture%2520rare%2520edge%2520cases%252C%2520which%2520play%2520a%2520critical%2520role%250Ain%2520training%2520and%2520testing%2520of%2520an%2520AV%2520system.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520the%2520Cosmos-Drive-Dreams%2520-%2520a%2520synthetic%2520data%2520generation%2520%2528SDG%2529%2520pipeline%250Athat%2520aims%2520to%2520generate%2520challenging%2520scenarios%2520to%2520facilitate%2520downstream%2520tasks%2520such%250Aas%2520perception%2520and%2520driving%2520policy%2520training.%2520Powering%2520this%2520pipeline%2520is%250ACosmos-Drive%252C%2520a%2520suite%2520of%2520models%2520specialized%2520from%2520NVIDIA%2520Cosmos%2520world%2520foundation%250Amodel%2520for%2520the%2520driving%2520domain%2520and%2520are%2520capable%2520of%2520controllable%252C%2520high-fidelity%252C%250Amulti-view%252C%2520and%2520spatiotemporally%2520consistent%2520driving%2520video%2520generation.%2520We%250Ashowcase%2520the%2520utility%2520of%2520these%2520models%2520by%2520applying%2520Cosmos-Drive-Dreams%2520to%2520scale%250Athe%2520quantity%2520and%2520diversity%2520of%2520driving%2520datasets%2520with%2520high-fidelity%2520and%250Achallenging%2520scenarios.%2520Experimentally%252C%2520we%2520demonstrate%2520that%2520our%2520generated%2520data%250Ahelps%2520in%2520mitigating%2520long-tail%2520distribution%2520problems%2520and%2520enhances%2520generalization%250Ain%2520downstream%2520tasks%2520such%2520as%25203D%2520lane%2520detection%252C%25203D%2520object%2520detection%2520and%2520driving%250Apolicy%2520learning.%2520We%2520open%2520source%2520our%2520pipeline%2520toolkit%252C%2520dataset%2520and%2520model%2520weights%250Athrough%2520the%2520NVIDIA%2527s%2520Cosmos%2520platform.%250A%2520%2520Project%2520page%253A%2520https%253A//research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09042v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cosmos-Drive-Dreams%3A%20Scalable%20Synthetic%20Driving%20Data%20Generation%20with%0A%20%20World%20Foundation%20Models&entry.906535625=Xuanchi%20Ren%20and%20Yifan%20Lu%20and%20Tianshi%20Cao%20and%20Ruiyuan%20Gao%20and%20Shengyu%20Huang%20and%20Amirmojtaba%20Sabour%20and%20Tianchang%20Shen%20and%20Tobias%20Pfaff%20and%20Jay%20Zhangjie%20Wu%20and%20Runjian%20Chen%20and%20Seung%20Wook%20Kim%20and%20Jun%20Gao%20and%20Laura%20Leal-Taixe%20and%20Mike%20Chen%20and%20Sanja%20Fidler%20and%20Huan%20Ling&entry.1292438233=%20%20Collecting%20and%20annotating%20real-world%20data%20for%20safety-critical%20physical%20AI%0Asystems%2C%20such%20as%20Autonomous%20Vehicle%20%28AV%29%2C%20is%20time-consuming%20and%20costly.%20It%20is%0Aespecially%20challenging%20to%20capture%20rare%20edge%20cases%2C%20which%20play%20a%20critical%20role%0Ain%20training%20and%20testing%20of%20an%20AV%20system.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20the%20Cosmos-Drive-Dreams%20-%20a%20synthetic%20data%20generation%20%28SDG%29%20pipeline%0Athat%20aims%20to%20generate%20challenging%20scenarios%20to%20facilitate%20downstream%20tasks%20such%0Aas%20perception%20and%20driving%20policy%20training.%20Powering%20this%20pipeline%20is%0ACosmos-Drive%2C%20a%20suite%20of%20models%20specialized%20from%20NVIDIA%20Cosmos%20world%20foundation%0Amodel%20for%20the%20driving%20domain%20and%20are%20capable%20of%20controllable%2C%20high-fidelity%2C%0Amulti-view%2C%20and%20spatiotemporally%20consistent%20driving%20video%20generation.%20We%0Ashowcase%20the%20utility%20of%20these%20models%20by%20applying%20Cosmos-Drive-Dreams%20to%20scale%0Athe%20quantity%20and%20diversity%20of%20driving%20datasets%20with%20high-fidelity%20and%0Achallenging%20scenarios.%20Experimentally%2C%20we%20demonstrate%20that%20our%20generated%20data%0Ahelps%20in%20mitigating%20long-tail%20distribution%20problems%20and%20enhances%20generalization%0Ain%20downstream%20tasks%20such%20as%203D%20lane%20detection%2C%203D%20object%20detection%20and%20driving%0Apolicy%20learning.%20We%20open%20source%20our%20pipeline%20toolkit%2C%20dataset%20and%20model%20weights%0Athrough%20the%20NVIDIA%27s%20Cosmos%20platform.%0A%20%20Project%20page%3A%20https%3A//research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09042v3&entry.124074799=Read"},
{"title": "Vision Transformers Don't Need Trained Registers", "author": "Nick Jiang and Amil Dravid and Alexei Efros and Yossi Gandelsman", "abstract": "  We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.\n", "link": "http://arxiv.org/abs/2506.08010v3", "date": "2025-06-18", "relevancy": 2.2226, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5856}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5494}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Transformers%20Don%27t%20Need%20Trained%20Registers&body=Title%3A%20Vision%20Transformers%20Don%27t%20Need%20Trained%20Registers%0AAuthor%3A%20Nick%20Jiang%20and%20Amil%20Dravid%20and%20Alexei%20Efros%20and%20Yossi%20Gandelsman%0AAbstract%3A%20%20%20We%20investigate%20the%20mechanism%20underlying%20a%20previously%20identified%20phenomenon%20in%0AVision%20Transformers%20--%20the%20emergence%20of%20high-norm%20tokens%20that%20lead%20to%20noisy%0Aattention%20maps.%20We%20observe%20that%20in%20multiple%20models%20%28e.g.%2C%20CLIP%2C%20DINOv2%29%2C%20a%0Asparse%20set%20of%20neurons%20is%20responsible%20for%20concentrating%20high-norm%20activations%20on%0Aoutlier%20tokens%2C%20leading%20to%20irregular%20attention%20patterns%20and%20degrading%0Adownstream%20visual%20processing.%20While%20the%20existing%20solution%20for%20removing%20these%0Aoutliers%20involves%20retraining%20models%20from%20scratch%20with%20additional%20learned%0Aregister%20tokens%2C%20we%20use%20our%20findings%20to%20create%20a%20training-free%20approach%20to%0Amitigate%20these%20artifacts.%20By%20shifting%20the%20high-norm%20activations%20from%20our%0Adiscovered%20register%20neurons%20into%20an%20additional%20untrained%20token%2C%20we%20can%20mimic%0Athe%20effect%20of%20register%20tokens%20on%20a%20model%20already%20trained%20without%20registers.%20We%0Ademonstrate%20that%20our%20method%20produces%20cleaner%20attention%20and%20feature%20maps%2C%0Aenhances%20performance%20over%20base%20models%20across%20multiple%20downstream%20visual%20tasks%2C%0Aand%20achieves%20results%20comparable%20to%20models%20explicitly%20trained%20with%20register%0Atokens.%20We%20then%20extend%20test-time%20registers%20to%20off-the-shelf%20vision-language%0Amodels%20to%20improve%20their%20interpretability.%20Our%20results%20suggest%20that%20test-time%0Aregisters%20effectively%20take%20on%20the%20role%20of%20register%20tokens%20at%20test-time%2C%0Aoffering%20a%20training-free%20solution%20for%20any%20pre-trained%20model%20released%20without%0Athem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08010v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Transformers%2520Don%2527t%2520Need%2520Trained%2520Registers%26entry.906535625%3DNick%2520Jiang%2520and%2520Amil%2520Dravid%2520and%2520Alexei%2520Efros%2520and%2520Yossi%2520Gandelsman%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520mechanism%2520underlying%2520a%2520previously%2520identified%2520phenomenon%2520in%250AVision%2520Transformers%2520--%2520the%2520emergence%2520of%2520high-norm%2520tokens%2520that%2520lead%2520to%2520noisy%250Aattention%2520maps.%2520We%2520observe%2520that%2520in%2520multiple%2520models%2520%2528e.g.%252C%2520CLIP%252C%2520DINOv2%2529%252C%2520a%250Asparse%2520set%2520of%2520neurons%2520is%2520responsible%2520for%2520concentrating%2520high-norm%2520activations%2520on%250Aoutlier%2520tokens%252C%2520leading%2520to%2520irregular%2520attention%2520patterns%2520and%2520degrading%250Adownstream%2520visual%2520processing.%2520While%2520the%2520existing%2520solution%2520for%2520removing%2520these%250Aoutliers%2520involves%2520retraining%2520models%2520from%2520scratch%2520with%2520additional%2520learned%250Aregister%2520tokens%252C%2520we%2520use%2520our%2520findings%2520to%2520create%2520a%2520training-free%2520approach%2520to%250Amitigate%2520these%2520artifacts.%2520By%2520shifting%2520the%2520high-norm%2520activations%2520from%2520our%250Adiscovered%2520register%2520neurons%2520into%2520an%2520additional%2520untrained%2520token%252C%2520we%2520can%2520mimic%250Athe%2520effect%2520of%2520register%2520tokens%2520on%2520a%2520model%2520already%2520trained%2520without%2520registers.%2520We%250Ademonstrate%2520that%2520our%2520method%2520produces%2520cleaner%2520attention%2520and%2520feature%2520maps%252C%250Aenhances%2520performance%2520over%2520base%2520models%2520across%2520multiple%2520downstream%2520visual%2520tasks%252C%250Aand%2520achieves%2520results%2520comparable%2520to%2520models%2520explicitly%2520trained%2520with%2520register%250Atokens.%2520We%2520then%2520extend%2520test-time%2520registers%2520to%2520off-the-shelf%2520vision-language%250Amodels%2520to%2520improve%2520their%2520interpretability.%2520Our%2520results%2520suggest%2520that%2520test-time%250Aregisters%2520effectively%2520take%2520on%2520the%2520role%2520of%2520register%2520tokens%2520at%2520test-time%252C%250Aoffering%2520a%2520training-free%2520solution%2520for%2520any%2520pre-trained%2520model%2520released%2520without%250Athem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08010v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Transformers%20Don%27t%20Need%20Trained%20Registers&entry.906535625=Nick%20Jiang%20and%20Amil%20Dravid%20and%20Alexei%20Efros%20and%20Yossi%20Gandelsman&entry.1292438233=%20%20We%20investigate%20the%20mechanism%20underlying%20a%20previously%20identified%20phenomenon%20in%0AVision%20Transformers%20--%20the%20emergence%20of%20high-norm%20tokens%20that%20lead%20to%20noisy%0Aattention%20maps.%20We%20observe%20that%20in%20multiple%20models%20%28e.g.%2C%20CLIP%2C%20DINOv2%29%2C%20a%0Asparse%20set%20of%20neurons%20is%20responsible%20for%20concentrating%20high-norm%20activations%20on%0Aoutlier%20tokens%2C%20leading%20to%20irregular%20attention%20patterns%20and%20degrading%0Adownstream%20visual%20processing.%20While%20the%20existing%20solution%20for%20removing%20these%0Aoutliers%20involves%20retraining%20models%20from%20scratch%20with%20additional%20learned%0Aregister%20tokens%2C%20we%20use%20our%20findings%20to%20create%20a%20training-free%20approach%20to%0Amitigate%20these%20artifacts.%20By%20shifting%20the%20high-norm%20activations%20from%20our%0Adiscovered%20register%20neurons%20into%20an%20additional%20untrained%20token%2C%20we%20can%20mimic%0Athe%20effect%20of%20register%20tokens%20on%20a%20model%20already%20trained%20without%20registers.%20We%0Ademonstrate%20that%20our%20method%20produces%20cleaner%20attention%20and%20feature%20maps%2C%0Aenhances%20performance%20over%20base%20models%20across%20multiple%20downstream%20visual%20tasks%2C%0Aand%20achieves%20results%20comparable%20to%20models%20explicitly%20trained%20with%20register%0Atokens.%20We%20then%20extend%20test-time%20registers%20to%20off-the-shelf%20vision-language%0Amodels%20to%20improve%20their%20interpretability.%20Our%20results%20suggest%20that%20test-time%0Aregisters%20effectively%20take%20on%20the%20role%20of%20register%20tokens%20at%20test-time%2C%0Aoffering%20a%20training-free%20solution%20for%20any%20pre-trained%20model%20released%20without%0Athem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08010v3&entry.124074799=Read"},
{"title": "Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video\n  Generation Model", "author": "Wei Li and Ming Hu and Guoan Wang and Lihao Liu and Kaijin Zhou and Junzhi Ning and Xin Guo and Zongyuan Ge and Lixu Gu and Junjun He", "abstract": "  In ophthalmic surgery, developing an AI system capable of interpreting\nsurgical videos and predicting subsequent operations requires numerous\nophthalmic surgical videos with high-quality annotations, which are difficult\nto collect due to privacy concerns and labor consumption. Text-guided video\ngeneration (T2V) emerges as a promising solution to overcome this issue by\ngenerating ophthalmic surgical videos based on surgeon instructions. In this\npaper, we present Ophora, a pioneering model that can generate ophthalmic\nsurgical videos following natural language instructions. To construct Ophora,\nwe first propose a Comprehensive Data Curation pipeline to convert narrative\nophthalmic surgical videos into a large-scale, high-quality dataset comprising\nover 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive\nVideo-Instruction Tuning scheme to transfer rich spatial-temporal knowledge\nfrom a T2V model pre-trained on natural video-text datasets for\nprivacy-preserved ophthalmic surgical video generation based on Ophora-160K.\nExperiments on video quality evaluation via quantitative analysis and\nophthalmologist feedback demonstrate that Ophora can generate realistic and\nreliable ophthalmic surgical videos based on surgeon instructions. We also\nvalidate the capability of Ophora for empowering downstream tasks of ophthalmic\nsurgical workflow understanding. Code is available at\nhttps://github.com/mar-cry/Ophora.\n", "link": "http://arxiv.org/abs/2505.07449v5", "date": "2025-06-18", "relevancy": 2.2187, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.556}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5547}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ophora%3A%20A%20Large-Scale%20Data-Driven%20Text-Guided%20Ophthalmic%20Surgical%20Video%0A%20%20Generation%20Model&body=Title%3A%20Ophora%3A%20A%20Large-Scale%20Data-Driven%20Text-Guided%20Ophthalmic%20Surgical%20Video%0A%20%20Generation%20Model%0AAuthor%3A%20Wei%20Li%20and%20Ming%20Hu%20and%20Guoan%20Wang%20and%20Lihao%20Liu%20and%20Kaijin%20Zhou%20and%20Junzhi%20Ning%20and%20Xin%20Guo%20and%20Zongyuan%20Ge%20and%20Lixu%20Gu%20and%20Junjun%20He%0AAbstract%3A%20%20%20In%20ophthalmic%20surgery%2C%20developing%20an%20AI%20system%20capable%20of%20interpreting%0Asurgical%20videos%20and%20predicting%20subsequent%20operations%20requires%20numerous%0Aophthalmic%20surgical%20videos%20with%20high-quality%20annotations%2C%20which%20are%20difficult%0Ato%20collect%20due%20to%20privacy%20concerns%20and%20labor%20consumption.%20Text-guided%20video%0Ageneration%20%28T2V%29%20emerges%20as%20a%20promising%20solution%20to%20overcome%20this%20issue%20by%0Agenerating%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20In%20this%0Apaper%2C%20we%20present%20Ophora%2C%20a%20pioneering%20model%20that%20can%20generate%20ophthalmic%0Asurgical%20videos%20following%20natural%20language%20instructions.%20To%20construct%20Ophora%2C%0Awe%20first%20propose%20a%20Comprehensive%20Data%20Curation%20pipeline%20to%20convert%20narrative%0Aophthalmic%20surgical%20videos%20into%20a%20large-scale%2C%20high-quality%20dataset%20comprising%0Aover%20160K%20video-instruction%20pairs%2C%20Ophora-160K.%20Then%2C%20we%20propose%20a%20Progressive%0AVideo-Instruction%20Tuning%20scheme%20to%20transfer%20rich%20spatial-temporal%20knowledge%0Afrom%20a%20T2V%20model%20pre-trained%20on%20natural%20video-text%20datasets%20for%0Aprivacy-preserved%20ophthalmic%20surgical%20video%20generation%20based%20on%20Ophora-160K.%0AExperiments%20on%20video%20quality%20evaluation%20via%20quantitative%20analysis%20and%0Aophthalmologist%20feedback%20demonstrate%20that%20Ophora%20can%20generate%20realistic%20and%0Areliable%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20We%20also%0Avalidate%20the%20capability%20of%20Ophora%20for%20empowering%20downstream%20tasks%20of%20ophthalmic%0Asurgical%20workflow%20understanding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mar-cry/Ophora.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07449v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOphora%253A%2520A%2520Large-Scale%2520Data-Driven%2520Text-Guided%2520Ophthalmic%2520Surgical%2520Video%250A%2520%2520Generation%2520Model%26entry.906535625%3DWei%2520Li%2520and%2520Ming%2520Hu%2520and%2520Guoan%2520Wang%2520and%2520Lihao%2520Liu%2520and%2520Kaijin%2520Zhou%2520and%2520Junzhi%2520Ning%2520and%2520Xin%2520Guo%2520and%2520Zongyuan%2520Ge%2520and%2520Lixu%2520Gu%2520and%2520Junjun%2520He%26entry.1292438233%3D%2520%2520In%2520ophthalmic%2520surgery%252C%2520developing%2520an%2520AI%2520system%2520capable%2520of%2520interpreting%250Asurgical%2520videos%2520and%2520predicting%2520subsequent%2520operations%2520requires%2520numerous%250Aophthalmic%2520surgical%2520videos%2520with%2520high-quality%2520annotations%252C%2520which%2520are%2520difficult%250Ato%2520collect%2520due%2520to%2520privacy%2520concerns%2520and%2520labor%2520consumption.%2520Text-guided%2520video%250Ageneration%2520%2528T2V%2529%2520emerges%2520as%2520a%2520promising%2520solution%2520to%2520overcome%2520this%2520issue%2520by%250Agenerating%2520ophthalmic%2520surgical%2520videos%2520based%2520on%2520surgeon%2520instructions.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Ophora%252C%2520a%2520pioneering%2520model%2520that%2520can%2520generate%2520ophthalmic%250Asurgical%2520videos%2520following%2520natural%2520language%2520instructions.%2520To%2520construct%2520Ophora%252C%250Awe%2520first%2520propose%2520a%2520Comprehensive%2520Data%2520Curation%2520pipeline%2520to%2520convert%2520narrative%250Aophthalmic%2520surgical%2520videos%2520into%2520a%2520large-scale%252C%2520high-quality%2520dataset%2520comprising%250Aover%2520160K%2520video-instruction%2520pairs%252C%2520Ophora-160K.%2520Then%252C%2520we%2520propose%2520a%2520Progressive%250AVideo-Instruction%2520Tuning%2520scheme%2520to%2520transfer%2520rich%2520spatial-temporal%2520knowledge%250Afrom%2520a%2520T2V%2520model%2520pre-trained%2520on%2520natural%2520video-text%2520datasets%2520for%250Aprivacy-preserved%2520ophthalmic%2520surgical%2520video%2520generation%2520based%2520on%2520Ophora-160K.%250AExperiments%2520on%2520video%2520quality%2520evaluation%2520via%2520quantitative%2520analysis%2520and%250Aophthalmologist%2520feedback%2520demonstrate%2520that%2520Ophora%2520can%2520generate%2520realistic%2520and%250Areliable%2520ophthalmic%2520surgical%2520videos%2520based%2520on%2520surgeon%2520instructions.%2520We%2520also%250Avalidate%2520the%2520capability%2520of%2520Ophora%2520for%2520empowering%2520downstream%2520tasks%2520of%2520ophthalmic%250Asurgical%2520workflow%2520understanding.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/mar-cry/Ophora.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07449v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ophora%3A%20A%20Large-Scale%20Data-Driven%20Text-Guided%20Ophthalmic%20Surgical%20Video%0A%20%20Generation%20Model&entry.906535625=Wei%20Li%20and%20Ming%20Hu%20and%20Guoan%20Wang%20and%20Lihao%20Liu%20and%20Kaijin%20Zhou%20and%20Junzhi%20Ning%20and%20Xin%20Guo%20and%20Zongyuan%20Ge%20and%20Lixu%20Gu%20and%20Junjun%20He&entry.1292438233=%20%20In%20ophthalmic%20surgery%2C%20developing%20an%20AI%20system%20capable%20of%20interpreting%0Asurgical%20videos%20and%20predicting%20subsequent%20operations%20requires%20numerous%0Aophthalmic%20surgical%20videos%20with%20high-quality%20annotations%2C%20which%20are%20difficult%0Ato%20collect%20due%20to%20privacy%20concerns%20and%20labor%20consumption.%20Text-guided%20video%0Ageneration%20%28T2V%29%20emerges%20as%20a%20promising%20solution%20to%20overcome%20this%20issue%20by%0Agenerating%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20In%20this%0Apaper%2C%20we%20present%20Ophora%2C%20a%20pioneering%20model%20that%20can%20generate%20ophthalmic%0Asurgical%20videos%20following%20natural%20language%20instructions.%20To%20construct%20Ophora%2C%0Awe%20first%20propose%20a%20Comprehensive%20Data%20Curation%20pipeline%20to%20convert%20narrative%0Aophthalmic%20surgical%20videos%20into%20a%20large-scale%2C%20high-quality%20dataset%20comprising%0Aover%20160K%20video-instruction%20pairs%2C%20Ophora-160K.%20Then%2C%20we%20propose%20a%20Progressive%0AVideo-Instruction%20Tuning%20scheme%20to%20transfer%20rich%20spatial-temporal%20knowledge%0Afrom%20a%20T2V%20model%20pre-trained%20on%20natural%20video-text%20datasets%20for%0Aprivacy-preserved%20ophthalmic%20surgical%20video%20generation%20based%20on%20Ophora-160K.%0AExperiments%20on%20video%20quality%20evaluation%20via%20quantitative%20analysis%20and%0Aophthalmologist%20feedback%20demonstrate%20that%20Ophora%20can%20generate%20realistic%20and%0Areliable%20ophthalmic%20surgical%20videos%20based%20on%20surgeon%20instructions.%20We%20also%0Avalidate%20the%20capability%20of%20Ophora%20for%20empowering%20downstream%20tasks%20of%20ophthalmic%0Asurgical%20workflow%20understanding.%20Code%20is%20available%20at%0Ahttps%3A//github.com/mar-cry/Ophora.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07449v5&entry.124074799=Read"},
{"title": "Detecting Neurocognitive Disorders through Analyses of Topic Evolution\n  and Cross-modal Consistency in Visual-Stimulated Narratives", "author": "Jinchao Li and Yuejiao Wang and Junan Li and Jiawen Kang and Bo Zheng and Simon Wong and Brian Mak and Helene Fung and Jean Woo and Man-Wai Mak and Timothy Kwok and Vincent Mok and Xianmin Gong and Xixin Wu and Xunying Liu and Patrick Wong and Helen Meng", "abstract": "  Early detection of neurocognitive disorders (NCDs) is crucial for timely\nintervention and disease management. Given that language impairments manifest\nearly in NCD progression, visual-stimulated narrative (VSN)-based analysis\noffers a promising avenue for NCD detection. Current VSN-based NCD detection\nmethods primarily focus on linguistic microstructures (e.g., pauses, lexical\ndiversity), which are potentially linked to bottom-up (stimulus-driven)\ncognitive processing. While these features illuminate basic language abilities,\nthe higher-order linguistic macrostructures (e.g., thematic or logical\ndevelopment), which may reflect top-down (concept-driven) cognitive abilities,\nremain underexplored. These patterns are crucial for NCD detection yet\nchallenging to quantify due to their abstract and complex nature. To bridge\nthis gap, we propose two novel dynamic macrostructural approaches: (1) Dynamic\nTopic Model (DTM) to track topic evolution over time, and (2) Text-Image\nTemporal Alignment Network (TITAN) to measure cross-modal consistency between\nspeech and visual stimuli. Experimental results validated the efficiency of\nproposed approaches in NCD detection, with TITAN achieving superior performance\nboth on the CU-MARVEL-RABBIT corpus (F1 = 0.7238) and the ADReSS corpus (F1 =\n0.8889). The feature contribution analysis revealed that macrostructural\nfeatures (e.g., topic variability, topic change rate, and topic consistency)\nconstituted the most significant contributors in the model's decision pathways,\noutperforming investigated microstructural features. These findings underscore\nthe critical role of macrostructural patterns in understanding cognitive\nimpairment mechanisms in NCDs.\n", "link": "http://arxiv.org/abs/2501.03727v2", "date": "2025-06-18", "relevancy": 2.2105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Neurocognitive%20Disorders%20through%20Analyses%20of%20Topic%20Evolution%0A%20%20and%20Cross-modal%20Consistency%20in%20Visual-Stimulated%20Narratives&body=Title%3A%20Detecting%20Neurocognitive%20Disorders%20through%20Analyses%20of%20Topic%20Evolution%0A%20%20and%20Cross-modal%20Consistency%20in%20Visual-Stimulated%20Narratives%0AAuthor%3A%20Jinchao%20Li%20and%20Yuejiao%20Wang%20and%20Junan%20Li%20and%20Jiawen%20Kang%20and%20Bo%20Zheng%20and%20Simon%20Wong%20and%20Brian%20Mak%20and%20Helene%20Fung%20and%20Jean%20Woo%20and%20Man-Wai%20Mak%20and%20Timothy%20Kwok%20and%20Vincent%20Mok%20and%20Xianmin%20Gong%20and%20Xixin%20Wu%20and%20Xunying%20Liu%20and%20Patrick%20Wong%20and%20Helen%20Meng%0AAbstract%3A%20%20%20Early%20detection%20of%20neurocognitive%20disorders%20%28NCDs%29%20is%20crucial%20for%20timely%0Aintervention%20and%20disease%20management.%20Given%20that%20language%20impairments%20manifest%0Aearly%20in%20NCD%20progression%2C%20visual-stimulated%20narrative%20%28VSN%29-based%20analysis%0Aoffers%20a%20promising%20avenue%20for%20NCD%20detection.%20Current%20VSN-based%20NCD%20detection%0Amethods%20primarily%20focus%20on%20linguistic%20microstructures%20%28e.g.%2C%20pauses%2C%20lexical%0Adiversity%29%2C%20which%20are%20potentially%20linked%20to%20bottom-up%20%28stimulus-driven%29%0Acognitive%20processing.%20While%20these%20features%20illuminate%20basic%20language%20abilities%2C%0Athe%20higher-order%20linguistic%20macrostructures%20%28e.g.%2C%20thematic%20or%20logical%0Adevelopment%29%2C%20which%20may%20reflect%20top-down%20%28concept-driven%29%20cognitive%20abilities%2C%0Aremain%20underexplored.%20These%20patterns%20are%20crucial%20for%20NCD%20detection%20yet%0Achallenging%20to%20quantify%20due%20to%20their%20abstract%20and%20complex%20nature.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20two%20novel%20dynamic%20macrostructural%20approaches%3A%20%281%29%20Dynamic%0ATopic%20Model%20%28DTM%29%20to%20track%20topic%20evolution%20over%20time%2C%20and%20%282%29%20Text-Image%0ATemporal%20Alignment%20Network%20%28TITAN%29%20to%20measure%20cross-modal%20consistency%20between%0Aspeech%20and%20visual%20stimuli.%20Experimental%20results%20validated%20the%20efficiency%20of%0Aproposed%20approaches%20in%20NCD%20detection%2C%20with%20TITAN%20achieving%20superior%20performance%0Aboth%20on%20the%20CU-MARVEL-RABBIT%20corpus%20%28F1%20%3D%200.7238%29%20and%20the%20ADReSS%20corpus%20%28F1%20%3D%0A0.8889%29.%20The%20feature%20contribution%20analysis%20revealed%20that%20macrostructural%0Afeatures%20%28e.g.%2C%20topic%20variability%2C%20topic%20change%20rate%2C%20and%20topic%20consistency%29%0Aconstituted%20the%20most%20significant%20contributors%20in%20the%20model%27s%20decision%20pathways%2C%0Aoutperforming%20investigated%20microstructural%20features.%20These%20findings%20underscore%0Athe%20critical%20role%20of%20macrostructural%20patterns%20in%20understanding%20cognitive%0Aimpairment%20mechanisms%20in%20NCDs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Neurocognitive%2520Disorders%2520through%2520Analyses%2520of%2520Topic%2520Evolution%250A%2520%2520and%2520Cross-modal%2520Consistency%2520in%2520Visual-Stimulated%2520Narratives%26entry.906535625%3DJinchao%2520Li%2520and%2520Yuejiao%2520Wang%2520and%2520Junan%2520Li%2520and%2520Jiawen%2520Kang%2520and%2520Bo%2520Zheng%2520and%2520Simon%2520Wong%2520and%2520Brian%2520Mak%2520and%2520Helene%2520Fung%2520and%2520Jean%2520Woo%2520and%2520Man-Wai%2520Mak%2520and%2520Timothy%2520Kwok%2520and%2520Vincent%2520Mok%2520and%2520Xianmin%2520Gong%2520and%2520Xixin%2520Wu%2520and%2520Xunying%2520Liu%2520and%2520Patrick%2520Wong%2520and%2520Helen%2520Meng%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520neurocognitive%2520disorders%2520%2528NCDs%2529%2520is%2520crucial%2520for%2520timely%250Aintervention%2520and%2520disease%2520management.%2520Given%2520that%2520language%2520impairments%2520manifest%250Aearly%2520in%2520NCD%2520progression%252C%2520visual-stimulated%2520narrative%2520%2528VSN%2529-based%2520analysis%250Aoffers%2520a%2520promising%2520avenue%2520for%2520NCD%2520detection.%2520Current%2520VSN-based%2520NCD%2520detection%250Amethods%2520primarily%2520focus%2520on%2520linguistic%2520microstructures%2520%2528e.g.%252C%2520pauses%252C%2520lexical%250Adiversity%2529%252C%2520which%2520are%2520potentially%2520linked%2520to%2520bottom-up%2520%2528stimulus-driven%2529%250Acognitive%2520processing.%2520While%2520these%2520features%2520illuminate%2520basic%2520language%2520abilities%252C%250Athe%2520higher-order%2520linguistic%2520macrostructures%2520%2528e.g.%252C%2520thematic%2520or%2520logical%250Adevelopment%2529%252C%2520which%2520may%2520reflect%2520top-down%2520%2528concept-driven%2529%2520cognitive%2520abilities%252C%250Aremain%2520underexplored.%2520These%2520patterns%2520are%2520crucial%2520for%2520NCD%2520detection%2520yet%250Achallenging%2520to%2520quantify%2520due%2520to%2520their%2520abstract%2520and%2520complex%2520nature.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520propose%2520two%2520novel%2520dynamic%2520macrostructural%2520approaches%253A%2520%25281%2529%2520Dynamic%250ATopic%2520Model%2520%2528DTM%2529%2520to%2520track%2520topic%2520evolution%2520over%2520time%252C%2520and%2520%25282%2529%2520Text-Image%250ATemporal%2520Alignment%2520Network%2520%2528TITAN%2529%2520to%2520measure%2520cross-modal%2520consistency%2520between%250Aspeech%2520and%2520visual%2520stimuli.%2520Experimental%2520results%2520validated%2520the%2520efficiency%2520of%250Aproposed%2520approaches%2520in%2520NCD%2520detection%252C%2520with%2520TITAN%2520achieving%2520superior%2520performance%250Aboth%2520on%2520the%2520CU-MARVEL-RABBIT%2520corpus%2520%2528F1%2520%253D%25200.7238%2529%2520and%2520the%2520ADReSS%2520corpus%2520%2528F1%2520%253D%250A0.8889%2529.%2520The%2520feature%2520contribution%2520analysis%2520revealed%2520that%2520macrostructural%250Afeatures%2520%2528e.g.%252C%2520topic%2520variability%252C%2520topic%2520change%2520rate%252C%2520and%2520topic%2520consistency%2529%250Aconstituted%2520the%2520most%2520significant%2520contributors%2520in%2520the%2520model%2527s%2520decision%2520pathways%252C%250Aoutperforming%2520investigated%2520microstructural%2520features.%2520These%2520findings%2520underscore%250Athe%2520critical%2520role%2520of%2520macrostructural%2520patterns%2520in%2520understanding%2520cognitive%250Aimpairment%2520mechanisms%2520in%2520NCDs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Neurocognitive%20Disorders%20through%20Analyses%20of%20Topic%20Evolution%0A%20%20and%20Cross-modal%20Consistency%20in%20Visual-Stimulated%20Narratives&entry.906535625=Jinchao%20Li%20and%20Yuejiao%20Wang%20and%20Junan%20Li%20and%20Jiawen%20Kang%20and%20Bo%20Zheng%20and%20Simon%20Wong%20and%20Brian%20Mak%20and%20Helene%20Fung%20and%20Jean%20Woo%20and%20Man-Wai%20Mak%20and%20Timothy%20Kwok%20and%20Vincent%20Mok%20and%20Xianmin%20Gong%20and%20Xixin%20Wu%20and%20Xunying%20Liu%20and%20Patrick%20Wong%20and%20Helen%20Meng&entry.1292438233=%20%20Early%20detection%20of%20neurocognitive%20disorders%20%28NCDs%29%20is%20crucial%20for%20timely%0Aintervention%20and%20disease%20management.%20Given%20that%20language%20impairments%20manifest%0Aearly%20in%20NCD%20progression%2C%20visual-stimulated%20narrative%20%28VSN%29-based%20analysis%0Aoffers%20a%20promising%20avenue%20for%20NCD%20detection.%20Current%20VSN-based%20NCD%20detection%0Amethods%20primarily%20focus%20on%20linguistic%20microstructures%20%28e.g.%2C%20pauses%2C%20lexical%0Adiversity%29%2C%20which%20are%20potentially%20linked%20to%20bottom-up%20%28stimulus-driven%29%0Acognitive%20processing.%20While%20these%20features%20illuminate%20basic%20language%20abilities%2C%0Athe%20higher-order%20linguistic%20macrostructures%20%28e.g.%2C%20thematic%20or%20logical%0Adevelopment%29%2C%20which%20may%20reflect%20top-down%20%28concept-driven%29%20cognitive%20abilities%2C%0Aremain%20underexplored.%20These%20patterns%20are%20crucial%20for%20NCD%20detection%20yet%0Achallenging%20to%20quantify%20due%20to%20their%20abstract%20and%20complex%20nature.%20To%20bridge%0Athis%20gap%2C%20we%20propose%20two%20novel%20dynamic%20macrostructural%20approaches%3A%20%281%29%20Dynamic%0ATopic%20Model%20%28DTM%29%20to%20track%20topic%20evolution%20over%20time%2C%20and%20%282%29%20Text-Image%0ATemporal%20Alignment%20Network%20%28TITAN%29%20to%20measure%20cross-modal%20consistency%20between%0Aspeech%20and%20visual%20stimuli.%20Experimental%20results%20validated%20the%20efficiency%20of%0Aproposed%20approaches%20in%20NCD%20detection%2C%20with%20TITAN%20achieving%20superior%20performance%0Aboth%20on%20the%20CU-MARVEL-RABBIT%20corpus%20%28F1%20%3D%200.7238%29%20and%20the%20ADReSS%20corpus%20%28F1%20%3D%0A0.8889%29.%20The%20feature%20contribution%20analysis%20revealed%20that%20macrostructural%0Afeatures%20%28e.g.%2C%20topic%20variability%2C%20topic%20change%20rate%2C%20and%20topic%20consistency%29%0Aconstituted%20the%20most%20significant%20contributors%20in%20the%20model%27s%20decision%20pathways%2C%0Aoutperforming%20investigated%20microstructural%20features.%20These%20findings%20underscore%0Athe%20critical%20role%20of%20macrostructural%20patterns%20in%20understanding%20cognitive%0Aimpairment%20mechanisms%20in%20NCDs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03727v2&entry.124074799=Read"},
{"title": "EgoBlind: Towards Egocentric Visual Assistance for the Blind", "author": "Junbin Xiao and Nanxin Huang and Hao Qiu and Zhulin Tao and Xun Yang and Richang Hong and Meng Wang and Angela Yao", "abstract": "  We present EgoBlind, the first egocentric VideoQA dataset collected from\nblind individuals to evaluate the assistive capabilities of contemporary\nmultimodal large language models (MLLMs). EgoBlind comprises 1,392 videos that\nrecord the daily lives of real blind users from a first-person perspective. It\nalso features 5,311 questions directly posed or generated and verified by blind\nindividuals to reflect their in-situation needs for visual assistance under\nvarious scenarios. We provide each question with an average of 3 reference\nanswers to alleviate subjective evaluation. Using EgoBlind, we comprehensively\nevaluate 16 advanced MLLMs and find that all models struggle, with the best\nperformers achieving accuracy near 60\\%, far behind human performance of\n87.4\\%. To guide future advancements, we identify and summarize major\nlimitations of existing MLLMs in egocentric visual assistance for the blind and\nexplore heuristic solutions for improvement. With these efforts, we hope\nEgoBlind can serve as a valuable foundation for developing more effective AI\nassistants to enhance the independence of the blind individuals' lives. Data\nand evaluation code are available at https://github.com/doc-doc/EgoBlind.\n", "link": "http://arxiv.org/abs/2503.08221v2", "date": "2025-06-18", "relevancy": 2.1861, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5936}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5268}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoBlind%3A%20Towards%20Egocentric%20Visual%20Assistance%20for%20the%20Blind&body=Title%3A%20EgoBlind%3A%20Towards%20Egocentric%20Visual%20Assistance%20for%20the%20Blind%0AAuthor%3A%20Junbin%20Xiao%20and%20Nanxin%20Huang%20and%20Hao%20Qiu%20and%20Zhulin%20Tao%20and%20Xun%20Yang%20and%20Richang%20Hong%20and%20Meng%20Wang%20and%20Angela%20Yao%0AAbstract%3A%20%20%20We%20present%20EgoBlind%2C%20the%20first%20egocentric%20VideoQA%20dataset%20collected%20from%0Ablind%20individuals%20to%20evaluate%20the%20assistive%20capabilities%20of%20contemporary%0Amultimodal%20large%20language%20models%20%28MLLMs%29.%20EgoBlind%20comprises%201%2C392%20videos%20that%0Arecord%20the%20daily%20lives%20of%20real%20blind%20users%20from%20a%20first-person%20perspective.%20It%0Aalso%20features%205%2C311%20questions%20directly%20posed%20or%20generated%20and%20verified%20by%20blind%0Aindividuals%20to%20reflect%20their%20in-situation%20needs%20for%20visual%20assistance%20under%0Avarious%20scenarios.%20We%20provide%20each%20question%20with%20an%20average%20of%203%20reference%0Aanswers%20to%20alleviate%20subjective%20evaluation.%20Using%20EgoBlind%2C%20we%20comprehensively%0Aevaluate%2016%20advanced%20MLLMs%20and%20find%20that%20all%20models%20struggle%2C%20with%20the%20best%0Aperformers%20achieving%20accuracy%20near%2060%5C%25%2C%20far%20behind%20human%20performance%20of%0A87.4%5C%25.%20To%20guide%20future%20advancements%2C%20we%20identify%20and%20summarize%20major%0Alimitations%20of%20existing%20MLLMs%20in%20egocentric%20visual%20assistance%20for%20the%20blind%20and%0Aexplore%20heuristic%20solutions%20for%20improvement.%20With%20these%20efforts%2C%20we%20hope%0AEgoBlind%20can%20serve%20as%20a%20valuable%20foundation%20for%20developing%20more%20effective%20AI%0Aassistants%20to%20enhance%20the%20independence%20of%20the%20blind%20individuals%27%20lives.%20Data%0Aand%20evaluation%20code%20are%20available%20at%20https%3A//github.com/doc-doc/EgoBlind.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoBlind%253A%2520Towards%2520Egocentric%2520Visual%2520Assistance%2520for%2520the%2520Blind%26entry.906535625%3DJunbin%2520Xiao%2520and%2520Nanxin%2520Huang%2520and%2520Hao%2520Qiu%2520and%2520Zhulin%2520Tao%2520and%2520Xun%2520Yang%2520and%2520Richang%2520Hong%2520and%2520Meng%2520Wang%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520We%2520present%2520EgoBlind%252C%2520the%2520first%2520egocentric%2520VideoQA%2520dataset%2520collected%2520from%250Ablind%2520individuals%2520to%2520evaluate%2520the%2520assistive%2520capabilities%2520of%2520contemporary%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520EgoBlind%2520comprises%25201%252C392%2520videos%2520that%250Arecord%2520the%2520daily%2520lives%2520of%2520real%2520blind%2520users%2520from%2520a%2520first-person%2520perspective.%2520It%250Aalso%2520features%25205%252C311%2520questions%2520directly%2520posed%2520or%2520generated%2520and%2520verified%2520by%2520blind%250Aindividuals%2520to%2520reflect%2520their%2520in-situation%2520needs%2520for%2520visual%2520assistance%2520under%250Avarious%2520scenarios.%2520We%2520provide%2520each%2520question%2520with%2520an%2520average%2520of%25203%2520reference%250Aanswers%2520to%2520alleviate%2520subjective%2520evaluation.%2520Using%2520EgoBlind%252C%2520we%2520comprehensively%250Aevaluate%252016%2520advanced%2520MLLMs%2520and%2520find%2520that%2520all%2520models%2520struggle%252C%2520with%2520the%2520best%250Aperformers%2520achieving%2520accuracy%2520near%252060%255C%2525%252C%2520far%2520behind%2520human%2520performance%2520of%250A87.4%255C%2525.%2520To%2520guide%2520future%2520advancements%252C%2520we%2520identify%2520and%2520summarize%2520major%250Alimitations%2520of%2520existing%2520MLLMs%2520in%2520egocentric%2520visual%2520assistance%2520for%2520the%2520blind%2520and%250Aexplore%2520heuristic%2520solutions%2520for%2520improvement.%2520With%2520these%2520efforts%252C%2520we%2520hope%250AEgoBlind%2520can%2520serve%2520as%2520a%2520valuable%2520foundation%2520for%2520developing%2520more%2520effective%2520AI%250Aassistants%2520to%2520enhance%2520the%2520independence%2520of%2520the%2520blind%2520individuals%2527%2520lives.%2520Data%250Aand%2520evaluation%2520code%2520are%2520available%2520at%2520https%253A//github.com/doc-doc/EgoBlind.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoBlind%3A%20Towards%20Egocentric%20Visual%20Assistance%20for%20the%20Blind&entry.906535625=Junbin%20Xiao%20and%20Nanxin%20Huang%20and%20Hao%20Qiu%20and%20Zhulin%20Tao%20and%20Xun%20Yang%20and%20Richang%20Hong%20and%20Meng%20Wang%20and%20Angela%20Yao&entry.1292438233=%20%20We%20present%20EgoBlind%2C%20the%20first%20egocentric%20VideoQA%20dataset%20collected%20from%0Ablind%20individuals%20to%20evaluate%20the%20assistive%20capabilities%20of%20contemporary%0Amultimodal%20large%20language%20models%20%28MLLMs%29.%20EgoBlind%20comprises%201%2C392%20videos%20that%0Arecord%20the%20daily%20lives%20of%20real%20blind%20users%20from%20a%20first-person%20perspective.%20It%0Aalso%20features%205%2C311%20questions%20directly%20posed%20or%20generated%20and%20verified%20by%20blind%0Aindividuals%20to%20reflect%20their%20in-situation%20needs%20for%20visual%20assistance%20under%0Avarious%20scenarios.%20We%20provide%20each%20question%20with%20an%20average%20of%203%20reference%0Aanswers%20to%20alleviate%20subjective%20evaluation.%20Using%20EgoBlind%2C%20we%20comprehensively%0Aevaluate%2016%20advanced%20MLLMs%20and%20find%20that%20all%20models%20struggle%2C%20with%20the%20best%0Aperformers%20achieving%20accuracy%20near%2060%5C%25%2C%20far%20behind%20human%20performance%20of%0A87.4%5C%25.%20To%20guide%20future%20advancements%2C%20we%20identify%20and%20summarize%20major%0Alimitations%20of%20existing%20MLLMs%20in%20egocentric%20visual%20assistance%20for%20the%20blind%20and%0Aexplore%20heuristic%20solutions%20for%20improvement.%20With%20these%20efforts%2C%20we%20hope%0AEgoBlind%20can%20serve%20as%20a%20valuable%20foundation%20for%20developing%20more%20effective%20AI%0Aassistants%20to%20enhance%20the%20independence%20of%20the%20blind%20individuals%27%20lives.%20Data%0Aand%20evaluation%20code%20are%20available%20at%20https%3A//github.com/doc-doc/EgoBlind.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08221v2&entry.124074799=Read"},
{"title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment\n  for Fast and Faithful VLM Captioning", "author": "Ankan Deria and Adinath Madhavrao Dukre and Feilong Tang and Sara Atito and Sudipta Roy and Muhammad Awais and Muhammad Haris Khan and Imran Razzak", "abstract": "  Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.\n", "link": "http://arxiv.org/abs/2506.15649v1", "date": "2025-06-18", "relevancy": 2.1858, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5516}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5439}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Stage%20Value-Guided%20Inference%20with%20Margin-Based%20Reward%20Adjustment%0A%20%20for%20Fast%20and%20Faithful%20VLM%20Captioning&body=Title%3A%20Dual-Stage%20Value-Guided%20Inference%20with%20Margin-Based%20Reward%20Adjustment%0A%20%20for%20Fast%20and%20Faithful%20VLM%20Captioning%0AAuthor%3A%20Ankan%20Deria%20and%20Adinath%20Madhavrao%20Dukre%20and%20Feilong%20Tang%20and%20Sara%20Atito%20and%20Sudipta%20Roy%20and%20Muhammad%20Awais%20and%20Muhammad%20Haris%20Khan%20and%20Imran%20Razzak%0AAbstract%3A%20%20%20Despite%20significant%20advances%20in%20inference-time%20search%20for%20vision-language%0Amodels%20%28VLMs%29%2C%20existing%20approaches%20remain%20both%20computationally%20expensive%20and%0Aprone%20to%20unpenalized%2C%20low-confidence%20generations%20which%20often%20lead%20to%20persistent%0Ahallucinations.%20We%20introduce%20%5Ctextbf%7BValue-guided%20Inference%20with%20Margin-based%0AReward%20%28ViMaR%29%7D%2C%20a%20two-stage%20inference%20framework%20that%20improves%20both%20efficiency%0Aand%20output%20fidelity%20by%20combining%20a%20temporal-difference%20value%20model%20with%20a%0Amargin-aware%20reward%20adjustment.%20In%20the%20first%20stage%2C%20we%20perform%20a%20single%20pass%20to%0Aidentify%20the%20highest-value%20caption%20among%20diverse%20candidates.%20In%20the%20second%0Astage%2C%20we%20selectively%20refine%20only%20those%20segments%20that%20were%20overlooked%20or%0Aexhibit%20weak%20visual%20grounding%2C%20thereby%20eliminating%20frequently%20rewarded%0Aevaluations.%20A%20calibrated%20margin-based%20penalty%20discourages%20low-confidence%0Acontinuations%20while%20preserving%20descriptive%20richness.%20Extensive%20experiments%0Aacross%20multiple%20VLM%20architectures%20demonstrate%20that%20ViMaR%20generates%20captions%0Athat%20are%20significantly%20more%20reliable%2C%20factually%20accurate%2C%20detailed%2C%20and%0Aexplanatory%2C%20while%20achieving%20over%204%24%5Ctimes%24%20speedup%20compared%20to%20existing%0Avalue-guided%20methods.%20Specifically%2C%20we%20show%20that%20ViMaR%20trained%20solely%20on%20LLaVA%0AMistral-7B%2C%20%5Ctextit%7Bgeneralizes%20effectively%20to%20guide%20decoding%20in%20a%20stronger%0Aunseen%20model%7D.%20To%20further%20validate%20this%2C%20we%20adapt%20the%20ViMaR%20to%20steer%20generation%0Ain%20LLaVA-OneVision-Qwen2-7B%2C%20leading%20to%20consistent%20improvements%20in%20caption%0Aquality%20and%20demonstrating%20robust%20cross-model%20guidance.%20This%20cross-model%0Ageneralization%20highlights%20ViMaR%27s%20flexibility%20and%20modularity%2C%20positioning%20it%20as%0Aa%20scalable%20and%20transferable%20inference-time%20decoding%20strategy.%20Furthermore%2C%20when%0AViMaR-generated%20captions%20are%20used%20for%20self-training%2C%20the%20underlying%20models%0Aachieve%20substantial%20gains%20across%20a%20broad%20suite%20of%20visual%20comprehension%0Abenchmarks%2C%20underscoring%20the%20potential%20of%20fast%2C%20accurate%2C%20and%20self-improving%0AVLM%20pipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Stage%2520Value-Guided%2520Inference%2520with%2520Margin-Based%2520Reward%2520Adjustment%250A%2520%2520for%2520Fast%2520and%2520Faithful%2520VLM%2520Captioning%26entry.906535625%3DAnkan%2520Deria%2520and%2520Adinath%2520Madhavrao%2520Dukre%2520and%2520Feilong%2520Tang%2520and%2520Sara%2520Atito%2520and%2520Sudipta%2520Roy%2520and%2520Muhammad%2520Awais%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Imran%2520Razzak%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advances%2520in%2520inference-time%2520search%2520for%2520vision-language%250Amodels%2520%2528VLMs%2529%252C%2520existing%2520approaches%2520remain%2520both%2520computationally%2520expensive%2520and%250Aprone%2520to%2520unpenalized%252C%2520low-confidence%2520generations%2520which%2520often%2520lead%2520to%2520persistent%250Ahallucinations.%2520We%2520introduce%2520%255Ctextbf%257BValue-guided%2520Inference%2520with%2520Margin-based%250AReward%2520%2528ViMaR%2529%257D%252C%2520a%2520two-stage%2520inference%2520framework%2520that%2520improves%2520both%2520efficiency%250Aand%2520output%2520fidelity%2520by%2520combining%2520a%2520temporal-difference%2520value%2520model%2520with%2520a%250Amargin-aware%2520reward%2520adjustment.%2520In%2520the%2520first%2520stage%252C%2520we%2520perform%2520a%2520single%2520pass%2520to%250Aidentify%2520the%2520highest-value%2520caption%2520among%2520diverse%2520candidates.%2520In%2520the%2520second%250Astage%252C%2520we%2520selectively%2520refine%2520only%2520those%2520segments%2520that%2520were%2520overlooked%2520or%250Aexhibit%2520weak%2520visual%2520grounding%252C%2520thereby%2520eliminating%2520frequently%2520rewarded%250Aevaluations.%2520A%2520calibrated%2520margin-based%2520penalty%2520discourages%2520low-confidence%250Acontinuations%2520while%2520preserving%2520descriptive%2520richness.%2520Extensive%2520experiments%250Aacross%2520multiple%2520VLM%2520architectures%2520demonstrate%2520that%2520ViMaR%2520generates%2520captions%250Athat%2520are%2520significantly%2520more%2520reliable%252C%2520factually%2520accurate%252C%2520detailed%252C%2520and%250Aexplanatory%252C%2520while%2520achieving%2520over%25204%2524%255Ctimes%2524%2520speedup%2520compared%2520to%2520existing%250Avalue-guided%2520methods.%2520Specifically%252C%2520we%2520show%2520that%2520ViMaR%2520trained%2520solely%2520on%2520LLaVA%250AMistral-7B%252C%2520%255Ctextit%257Bgeneralizes%2520effectively%2520to%2520guide%2520decoding%2520in%2520a%2520stronger%250Aunseen%2520model%257D.%2520To%2520further%2520validate%2520this%252C%2520we%2520adapt%2520the%2520ViMaR%2520to%2520steer%2520generation%250Ain%2520LLaVA-OneVision-Qwen2-7B%252C%2520leading%2520to%2520consistent%2520improvements%2520in%2520caption%250Aquality%2520and%2520demonstrating%2520robust%2520cross-model%2520guidance.%2520This%2520cross-model%250Ageneralization%2520highlights%2520ViMaR%2527s%2520flexibility%2520and%2520modularity%252C%2520positioning%2520it%2520as%250Aa%2520scalable%2520and%2520transferable%2520inference-time%2520decoding%2520strategy.%2520Furthermore%252C%2520when%250AViMaR-generated%2520captions%2520are%2520used%2520for%2520self-training%252C%2520the%2520underlying%2520models%250Aachieve%2520substantial%2520gains%2520across%2520a%2520broad%2520suite%2520of%2520visual%2520comprehension%250Abenchmarks%252C%2520underscoring%2520the%2520potential%2520of%2520fast%252C%2520accurate%252C%2520and%2520self-improving%250AVLM%2520pipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Stage%20Value-Guided%20Inference%20with%20Margin-Based%20Reward%20Adjustment%0A%20%20for%20Fast%20and%20Faithful%20VLM%20Captioning&entry.906535625=Ankan%20Deria%20and%20Adinath%20Madhavrao%20Dukre%20and%20Feilong%20Tang%20and%20Sara%20Atito%20and%20Sudipta%20Roy%20and%20Muhammad%20Awais%20and%20Muhammad%20Haris%20Khan%20and%20Imran%20Razzak&entry.1292438233=%20%20Despite%20significant%20advances%20in%20inference-time%20search%20for%20vision-language%0Amodels%20%28VLMs%29%2C%20existing%20approaches%20remain%20both%20computationally%20expensive%20and%0Aprone%20to%20unpenalized%2C%20low-confidence%20generations%20which%20often%20lead%20to%20persistent%0Ahallucinations.%20We%20introduce%20%5Ctextbf%7BValue-guided%20Inference%20with%20Margin-based%0AReward%20%28ViMaR%29%7D%2C%20a%20two-stage%20inference%20framework%20that%20improves%20both%20efficiency%0Aand%20output%20fidelity%20by%20combining%20a%20temporal-difference%20value%20model%20with%20a%0Amargin-aware%20reward%20adjustment.%20In%20the%20first%20stage%2C%20we%20perform%20a%20single%20pass%20to%0Aidentify%20the%20highest-value%20caption%20among%20diverse%20candidates.%20In%20the%20second%0Astage%2C%20we%20selectively%20refine%20only%20those%20segments%20that%20were%20overlooked%20or%0Aexhibit%20weak%20visual%20grounding%2C%20thereby%20eliminating%20frequently%20rewarded%0Aevaluations.%20A%20calibrated%20margin-based%20penalty%20discourages%20low-confidence%0Acontinuations%20while%20preserving%20descriptive%20richness.%20Extensive%20experiments%0Aacross%20multiple%20VLM%20architectures%20demonstrate%20that%20ViMaR%20generates%20captions%0Athat%20are%20significantly%20more%20reliable%2C%20factually%20accurate%2C%20detailed%2C%20and%0Aexplanatory%2C%20while%20achieving%20over%204%24%5Ctimes%24%20speedup%20compared%20to%20existing%0Avalue-guided%20methods.%20Specifically%2C%20we%20show%20that%20ViMaR%20trained%20solely%20on%20LLaVA%0AMistral-7B%2C%20%5Ctextit%7Bgeneralizes%20effectively%20to%20guide%20decoding%20in%20a%20stronger%0Aunseen%20model%7D.%20To%20further%20validate%20this%2C%20we%20adapt%20the%20ViMaR%20to%20steer%20generation%0Ain%20LLaVA-OneVision-Qwen2-7B%2C%20leading%20to%20consistent%20improvements%20in%20caption%0Aquality%20and%20demonstrating%20robust%20cross-model%20guidance.%20This%20cross-model%0Ageneralization%20highlights%20ViMaR%27s%20flexibility%20and%20modularity%2C%20positioning%20it%20as%0Aa%20scalable%20and%20transferable%20inference-time%20decoding%20strategy.%20Furthermore%2C%20when%0AViMaR-generated%20captions%20are%20used%20for%20self-training%2C%20the%20underlying%20models%0Aachieve%20substantial%20gains%20across%20a%20broad%20suite%20of%20visual%20comprehension%0Abenchmarks%2C%20underscoring%20the%20potential%20of%20fast%2C%20accurate%2C%20and%20self-improving%0AVLM%20pipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15649v1&entry.124074799=Read"},
{"title": "Real-Time Initialization of Unknown Anchors for UWB-aided Navigation", "author": "Giulio Delama and Igor Borowski and Roland Jung and Stephan Weiss", "abstract": "  This paper presents a framework for the real-time initialization of unknown\nUltra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is\ndesigned for localization solutions where UWB modules act as supplementary\nsensors. Our approach enables the automatic detection and calibration of\npreviously unknown anchors during operation, removing the need for manual\nsetup. By combining an online Positional Dilution of Precision (PDOP)\nestimation, a lightweight outlier detection method, and an adaptive robust\nkernel for non-linear optimization, our approach significantly improves\nrobustness and suitability for real-world applications compared to\nstate-of-the-art. In particular, we show that our metric which triggers an\ninitialization decision is more conservative than current ones commonly based\non initial linear or non-linear initialization guesses. This allows for better\ninitialization geometry and subsequently lower initialization errors. We\ndemonstrate the proposed approach on two different mobile robots: an autonomous\nforklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry\n(VIO) framework. The results highlight the effectiveness of the proposed method\nwith robust initialization and low positioning error. We open-source our code\nin a C++ library including a ROS wrapper.\n", "link": "http://arxiv.org/abs/2506.15518v1", "date": "2025-06-18", "relevancy": 2.1842, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5925}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5153}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Initialization%20of%20Unknown%20Anchors%20for%20UWB-aided%20Navigation&body=Title%3A%20Real-Time%20Initialization%20of%20Unknown%20Anchors%20for%20UWB-aided%20Navigation%0AAuthor%3A%20Giulio%20Delama%20and%20Igor%20Borowski%20and%20Roland%20Jung%20and%20Stephan%20Weiss%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20framework%20for%20the%20real-time%20initialization%20of%20unknown%0AUltra-Wideband%20%28UWB%29%20anchors%20in%20UWB-aided%20navigation%20systems.%20The%20method%20is%0Adesigned%20for%20localization%20solutions%20where%20UWB%20modules%20act%20as%20supplementary%0Asensors.%20Our%20approach%20enables%20the%20automatic%20detection%20and%20calibration%20of%0Apreviously%20unknown%20anchors%20during%20operation%2C%20removing%20the%20need%20for%20manual%0Asetup.%20By%20combining%20an%20online%20Positional%20Dilution%20of%20Precision%20%28PDOP%29%0Aestimation%2C%20a%20lightweight%20outlier%20detection%20method%2C%20and%20an%20adaptive%20robust%0Akernel%20for%20non-linear%20optimization%2C%20our%20approach%20significantly%20improves%0Arobustness%20and%20suitability%20for%20real-world%20applications%20compared%20to%0Astate-of-the-art.%20In%20particular%2C%20we%20show%20that%20our%20metric%20which%20triggers%20an%0Ainitialization%20decision%20is%20more%20conservative%20than%20current%20ones%20commonly%20based%0Aon%20initial%20linear%20or%20non-linear%20initialization%20guesses.%20This%20allows%20for%20better%0Ainitialization%20geometry%20and%20subsequently%20lower%20initialization%20errors.%20We%0Ademonstrate%20the%20proposed%20approach%20on%20two%20different%20mobile%20robots%3A%20an%20autonomous%0Aforklift%20and%20a%20quadcopter%20equipped%20with%20a%20UWB-aided%20Visual-Inertial%20Odometry%0A%28VIO%29%20framework.%20The%20results%20highlight%20the%20effectiveness%20of%20the%20proposed%20method%0Awith%20robust%20initialization%20and%20low%20positioning%20error.%20We%20open-source%20our%20code%0Ain%20a%20C%2B%2B%20library%20including%20a%20ROS%20wrapper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Initialization%2520of%2520Unknown%2520Anchors%2520for%2520UWB-aided%2520Navigation%26entry.906535625%3DGiulio%2520Delama%2520and%2520Igor%2520Borowski%2520and%2520Roland%2520Jung%2520and%2520Stephan%2520Weiss%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520framework%2520for%2520the%2520real-time%2520initialization%2520of%2520unknown%250AUltra-Wideband%2520%2528UWB%2529%2520anchors%2520in%2520UWB-aided%2520navigation%2520systems.%2520The%2520method%2520is%250Adesigned%2520for%2520localization%2520solutions%2520where%2520UWB%2520modules%2520act%2520as%2520supplementary%250Asensors.%2520Our%2520approach%2520enables%2520the%2520automatic%2520detection%2520and%2520calibration%2520of%250Apreviously%2520unknown%2520anchors%2520during%2520operation%252C%2520removing%2520the%2520need%2520for%2520manual%250Asetup.%2520By%2520combining%2520an%2520online%2520Positional%2520Dilution%2520of%2520Precision%2520%2528PDOP%2529%250Aestimation%252C%2520a%2520lightweight%2520outlier%2520detection%2520method%252C%2520and%2520an%2520adaptive%2520robust%250Akernel%2520for%2520non-linear%2520optimization%252C%2520our%2520approach%2520significantly%2520improves%250Arobustness%2520and%2520suitability%2520for%2520real-world%2520applications%2520compared%2520to%250Astate-of-the-art.%2520In%2520particular%252C%2520we%2520show%2520that%2520our%2520metric%2520which%2520triggers%2520an%250Ainitialization%2520decision%2520is%2520more%2520conservative%2520than%2520current%2520ones%2520commonly%2520based%250Aon%2520initial%2520linear%2520or%2520non-linear%2520initialization%2520guesses.%2520This%2520allows%2520for%2520better%250Ainitialization%2520geometry%2520and%2520subsequently%2520lower%2520initialization%2520errors.%2520We%250Ademonstrate%2520the%2520proposed%2520approach%2520on%2520two%2520different%2520mobile%2520robots%253A%2520an%2520autonomous%250Aforklift%2520and%2520a%2520quadcopter%2520equipped%2520with%2520a%2520UWB-aided%2520Visual-Inertial%2520Odometry%250A%2528VIO%2529%2520framework.%2520The%2520results%2520highlight%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%250Awith%2520robust%2520initialization%2520and%2520low%2520positioning%2520error.%2520We%2520open-source%2520our%2520code%250Ain%2520a%2520C%252B%252B%2520library%2520including%2520a%2520ROS%2520wrapper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Initialization%20of%20Unknown%20Anchors%20for%20UWB-aided%20Navigation&entry.906535625=Giulio%20Delama%20and%20Igor%20Borowski%20and%20Roland%20Jung%20and%20Stephan%20Weiss&entry.1292438233=%20%20This%20paper%20presents%20a%20framework%20for%20the%20real-time%20initialization%20of%20unknown%0AUltra-Wideband%20%28UWB%29%20anchors%20in%20UWB-aided%20navigation%20systems.%20The%20method%20is%0Adesigned%20for%20localization%20solutions%20where%20UWB%20modules%20act%20as%20supplementary%0Asensors.%20Our%20approach%20enables%20the%20automatic%20detection%20and%20calibration%20of%0Apreviously%20unknown%20anchors%20during%20operation%2C%20removing%20the%20need%20for%20manual%0Asetup.%20By%20combining%20an%20online%20Positional%20Dilution%20of%20Precision%20%28PDOP%29%0Aestimation%2C%20a%20lightweight%20outlier%20detection%20method%2C%20and%20an%20adaptive%20robust%0Akernel%20for%20non-linear%20optimization%2C%20our%20approach%20significantly%20improves%0Arobustness%20and%20suitability%20for%20real-world%20applications%20compared%20to%0Astate-of-the-art.%20In%20particular%2C%20we%20show%20that%20our%20metric%20which%20triggers%20an%0Ainitialization%20decision%20is%20more%20conservative%20than%20current%20ones%20commonly%20based%0Aon%20initial%20linear%20or%20non-linear%20initialization%20guesses.%20This%20allows%20for%20better%0Ainitialization%20geometry%20and%20subsequently%20lower%20initialization%20errors.%20We%0Ademonstrate%20the%20proposed%20approach%20on%20two%20different%20mobile%20robots%3A%20an%20autonomous%0Aforklift%20and%20a%20quadcopter%20equipped%20with%20a%20UWB-aided%20Visual-Inertial%20Odometry%0A%28VIO%29%20framework.%20The%20results%20highlight%20the%20effectiveness%20of%20the%20proposed%20method%0Awith%20robust%20initialization%20and%20low%20positioning%20error.%20We%20open-source%20our%20code%0Ain%20a%20C%2B%2B%20library%20including%20a%20ROS%20wrapper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15518v1&entry.124074799=Read"},
{"title": "Incorporating Pre-training Data Matters in Unsupervised Domain\n  Adaptation", "author": "Yinsong Xu and Aidong Men and Yang Liu and Xiahai Zhuang and Qingchao Chen", "abstract": "  In deep learning, initializing models with pre-trained weights has become the\nde facto practice for various downstream tasks. Many unsupervised domain\nadaptation (UDA) methods typically adopt a backbone pre-trained on ImageNet,\nand focus on reducing the source-target domain discrepancy. However, the impact\nof pre-training on adaptation received little attention. In this study, we\ndelve into UDA from the novel perspective of pre-training. We first demonstrate\nthe impact of pre-training by analyzing the dynamic distribution discrepancies\nbetween pre-training data domain and the source/ target domain during\nadaptation. Then, we reveal that the target error also stems from the\npre-training in the following two factors: 1) empirically, target error arises\nfrom the gradually degenerative pre-trained knowledge during adaptation; 2)\ntheoretically, the error bound depends on difference between the gradient of\nloss function, \\ie, on the target domain and pre-training data domain. To\naddress these two issues, we redefine UDA as a three-domain problem, \\ie,\nsource domain, target domain, and pre-training data domain; then we propose a\nnovel framework, named TriDA. We maintain the pre-trained knowledge and improve\nthe error bound by incorporating pre-training data into adaptation for both\nvanilla UDA and source-free UDA scenarios. For efficiency, we introduce a\nselection strategy for pre-training data, and offer a solution with synthesized\nimages when pre-training data is unavailable during adaptation. Notably, TriDA\nis effective even with a small amount of pre-training or synthesized images,\nand seamlessly complements the two scenario UDA methods, demonstrating\nstate-of-the-art performance across multiple benchmarks. We hope our work\nprovides new insights for better understanding and application of domain\nadaptation.\n", "link": "http://arxiv.org/abs/2308.03097v2", "date": "2025-06-18", "relevancy": 2.1797, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5785}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5212}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Pre-training%20Data%20Matters%20in%20Unsupervised%20Domain%0A%20%20Adaptation&body=Title%3A%20Incorporating%20Pre-training%20Data%20Matters%20in%20Unsupervised%20Domain%0A%20%20Adaptation%0AAuthor%3A%20Yinsong%20Xu%20and%20Aidong%20Men%20and%20Yang%20Liu%20and%20Xiahai%20Zhuang%20and%20Qingchao%20Chen%0AAbstract%3A%20%20%20In%20deep%20learning%2C%20initializing%20models%20with%20pre-trained%20weights%20has%20become%20the%0Ade%20facto%20practice%20for%20various%20downstream%20tasks.%20Many%20unsupervised%20domain%0Aadaptation%20%28UDA%29%20methods%20typically%20adopt%20a%20backbone%20pre-trained%20on%20ImageNet%2C%0Aand%20focus%20on%20reducing%20the%20source-target%20domain%20discrepancy.%20However%2C%20the%20impact%0Aof%20pre-training%20on%20adaptation%20received%20little%20attention.%20In%20this%20study%2C%20we%0Adelve%20into%20UDA%20from%20the%20novel%20perspective%20of%20pre-training.%20We%20first%20demonstrate%0Athe%20impact%20of%20pre-training%20by%20analyzing%20the%20dynamic%20distribution%20discrepancies%0Abetween%20pre-training%20data%20domain%20and%20the%20source/%20target%20domain%20during%0Aadaptation.%20Then%2C%20we%20reveal%20that%20the%20target%20error%20also%20stems%20from%20the%0Apre-training%20in%20the%20following%20two%20factors%3A%201%29%20empirically%2C%20target%20error%20arises%0Afrom%20the%20gradually%20degenerative%20pre-trained%20knowledge%20during%20adaptation%3B%202%29%0Atheoretically%2C%20the%20error%20bound%20depends%20on%20difference%20between%20the%20gradient%20of%0Aloss%20function%2C%20%5Cie%2C%20on%20the%20target%20domain%20and%20pre-training%20data%20domain.%20To%0Aaddress%20these%20two%20issues%2C%20we%20redefine%20UDA%20as%20a%20three-domain%20problem%2C%20%5Cie%2C%0Asource%20domain%2C%20target%20domain%2C%20and%20pre-training%20data%20domain%3B%20then%20we%20propose%20a%0Anovel%20framework%2C%20named%20TriDA.%20We%20maintain%20the%20pre-trained%20knowledge%20and%20improve%0Athe%20error%20bound%20by%20incorporating%20pre-training%20data%20into%20adaptation%20for%20both%0Avanilla%20UDA%20and%20source-free%20UDA%20scenarios.%20For%20efficiency%2C%20we%20introduce%20a%0Aselection%20strategy%20for%20pre-training%20data%2C%20and%20offer%20a%20solution%20with%20synthesized%0Aimages%20when%20pre-training%20data%20is%20unavailable%20during%20adaptation.%20Notably%2C%20TriDA%0Ais%20effective%20even%20with%20a%20small%20amount%20of%20pre-training%20or%20synthesized%20images%2C%0Aand%20seamlessly%20complements%20the%20two%20scenario%20UDA%20methods%2C%20demonstrating%0Astate-of-the-art%20performance%20across%20multiple%20benchmarks.%20We%20hope%20our%20work%0Aprovides%20new%20insights%20for%20better%20understanding%20and%20application%20of%20domain%0Aadaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.03097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Pre-training%2520Data%2520Matters%2520in%2520Unsupervised%2520Domain%250A%2520%2520Adaptation%26entry.906535625%3DYinsong%2520Xu%2520and%2520Aidong%2520Men%2520and%2520Yang%2520Liu%2520and%2520Xiahai%2520Zhuang%2520and%2520Qingchao%2520Chen%26entry.1292438233%3D%2520%2520In%2520deep%2520learning%252C%2520initializing%2520models%2520with%2520pre-trained%2520weights%2520has%2520become%2520the%250Ade%2520facto%2520practice%2520for%2520various%2520downstream%2520tasks.%2520Many%2520unsupervised%2520domain%250Aadaptation%2520%2528UDA%2529%2520methods%2520typically%2520adopt%2520a%2520backbone%2520pre-trained%2520on%2520ImageNet%252C%250Aand%2520focus%2520on%2520reducing%2520the%2520source-target%2520domain%2520discrepancy.%2520However%252C%2520the%2520impact%250Aof%2520pre-training%2520on%2520adaptation%2520received%2520little%2520attention.%2520In%2520this%2520study%252C%2520we%250Adelve%2520into%2520UDA%2520from%2520the%2520novel%2520perspective%2520of%2520pre-training.%2520We%2520first%2520demonstrate%250Athe%2520impact%2520of%2520pre-training%2520by%2520analyzing%2520the%2520dynamic%2520distribution%2520discrepancies%250Abetween%2520pre-training%2520data%2520domain%2520and%2520the%2520source/%2520target%2520domain%2520during%250Aadaptation.%2520Then%252C%2520we%2520reveal%2520that%2520the%2520target%2520error%2520also%2520stems%2520from%2520the%250Apre-training%2520in%2520the%2520following%2520two%2520factors%253A%25201%2529%2520empirically%252C%2520target%2520error%2520arises%250Afrom%2520the%2520gradually%2520degenerative%2520pre-trained%2520knowledge%2520during%2520adaptation%253B%25202%2529%250Atheoretically%252C%2520the%2520error%2520bound%2520depends%2520on%2520difference%2520between%2520the%2520gradient%2520of%250Aloss%2520function%252C%2520%255Cie%252C%2520on%2520the%2520target%2520domain%2520and%2520pre-training%2520data%2520domain.%2520To%250Aaddress%2520these%2520two%2520issues%252C%2520we%2520redefine%2520UDA%2520as%2520a%2520three-domain%2520problem%252C%2520%255Cie%252C%250Asource%2520domain%252C%2520target%2520domain%252C%2520and%2520pre-training%2520data%2520domain%253B%2520then%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520named%2520TriDA.%2520We%2520maintain%2520the%2520pre-trained%2520knowledge%2520and%2520improve%250Athe%2520error%2520bound%2520by%2520incorporating%2520pre-training%2520data%2520into%2520adaptation%2520for%2520both%250Avanilla%2520UDA%2520and%2520source-free%2520UDA%2520scenarios.%2520For%2520efficiency%252C%2520we%2520introduce%2520a%250Aselection%2520strategy%2520for%2520pre-training%2520data%252C%2520and%2520offer%2520a%2520solution%2520with%2520synthesized%250Aimages%2520when%2520pre-training%2520data%2520is%2520unavailable%2520during%2520adaptation.%2520Notably%252C%2520TriDA%250Ais%2520effective%2520even%2520with%2520a%2520small%2520amount%2520of%2520pre-training%2520or%2520synthesized%2520images%252C%250Aand%2520seamlessly%2520complements%2520the%2520two%2520scenario%2520UDA%2520methods%252C%2520demonstrating%250Astate-of-the-art%2520performance%2520across%2520multiple%2520benchmarks.%2520We%2520hope%2520our%2520work%250Aprovides%2520new%2520insights%2520for%2520better%2520understanding%2520and%2520application%2520of%2520domain%250Aadaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.03097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Pre-training%20Data%20Matters%20in%20Unsupervised%20Domain%0A%20%20Adaptation&entry.906535625=Yinsong%20Xu%20and%20Aidong%20Men%20and%20Yang%20Liu%20and%20Xiahai%20Zhuang%20and%20Qingchao%20Chen&entry.1292438233=%20%20In%20deep%20learning%2C%20initializing%20models%20with%20pre-trained%20weights%20has%20become%20the%0Ade%20facto%20practice%20for%20various%20downstream%20tasks.%20Many%20unsupervised%20domain%0Aadaptation%20%28UDA%29%20methods%20typically%20adopt%20a%20backbone%20pre-trained%20on%20ImageNet%2C%0Aand%20focus%20on%20reducing%20the%20source-target%20domain%20discrepancy.%20However%2C%20the%20impact%0Aof%20pre-training%20on%20adaptation%20received%20little%20attention.%20In%20this%20study%2C%20we%0Adelve%20into%20UDA%20from%20the%20novel%20perspective%20of%20pre-training.%20We%20first%20demonstrate%0Athe%20impact%20of%20pre-training%20by%20analyzing%20the%20dynamic%20distribution%20discrepancies%0Abetween%20pre-training%20data%20domain%20and%20the%20source/%20target%20domain%20during%0Aadaptation.%20Then%2C%20we%20reveal%20that%20the%20target%20error%20also%20stems%20from%20the%0Apre-training%20in%20the%20following%20two%20factors%3A%201%29%20empirically%2C%20target%20error%20arises%0Afrom%20the%20gradually%20degenerative%20pre-trained%20knowledge%20during%20adaptation%3B%202%29%0Atheoretically%2C%20the%20error%20bound%20depends%20on%20difference%20between%20the%20gradient%20of%0Aloss%20function%2C%20%5Cie%2C%20on%20the%20target%20domain%20and%20pre-training%20data%20domain.%20To%0Aaddress%20these%20two%20issues%2C%20we%20redefine%20UDA%20as%20a%20three-domain%20problem%2C%20%5Cie%2C%0Asource%20domain%2C%20target%20domain%2C%20and%20pre-training%20data%20domain%3B%20then%20we%20propose%20a%0Anovel%20framework%2C%20named%20TriDA.%20We%20maintain%20the%20pre-trained%20knowledge%20and%20improve%0Athe%20error%20bound%20by%20incorporating%20pre-training%20data%20into%20adaptation%20for%20both%0Avanilla%20UDA%20and%20source-free%20UDA%20scenarios.%20For%20efficiency%2C%20we%20introduce%20a%0Aselection%20strategy%20for%20pre-training%20data%2C%20and%20offer%20a%20solution%20with%20synthesized%0Aimages%20when%20pre-training%20data%20is%20unavailable%20during%20adaptation.%20Notably%2C%20TriDA%0Ais%20effective%20even%20with%20a%20small%20amount%20of%20pre-training%20or%20synthesized%20images%2C%0Aand%20seamlessly%20complements%20the%20two%20scenario%20UDA%20methods%2C%20demonstrating%0Astate-of-the-art%20performance%20across%20multiple%20benchmarks.%20We%20hope%20our%20work%0Aprovides%20new%20insights%20for%20better%20understanding%20and%20application%20of%20domain%0Aadaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.03097v2&entry.124074799=Read"},
{"title": "Local minima of the empirical risk in high dimension: General theorems\n  and convex examples", "author": "Kiana Asgari and Andrea Montanari and Basil Saeed", "abstract": "  We consider a general model for high-dimensional empirical risk minimization\nwhereby the data $\\mathbf{x}_i$ are $d$-dimensional isotropic Gaussian vectors,\nthe model is parametrized by $\\mathbf{\\Theta}\\in\\mathbb{R}^{d\\times k}$, and\nthe loss depends on the data via the projection\n$\\mathbf{\\Theta}^\\mathsf{T}\\mathbf{x}_i$. This setting covers as special cases\nclassical statistics methods (e.g. multinomial regression and other generalized\nlinear models), but also two-layer fully connected neural networks with $k$\nhidden neurons. We use the Kac-Rice formula from Gaussian process theory to\nderive a bound on the expected number of local minima of this empirical risk,\nunder the proportional asymptotics in which $n,d\\to\\infty$, with $n\\asymp d$.\nVia Markov's inequality, this bound allows to determine the positions of these\nminimizers (with exponential deviation bounds) and hence derive sharp\nasymptotics on the estimation and prediction error. In this paper, we apply our\ncharacterization to convex losses, where high-dimensional asymptotics were not\n(in general) rigorously established for $k\\ge 2$. We show that our approach is\ntight and allows to prove previously conjectured results. In addition, we\ncharacterize the spectrum of the Hessian at the minimizer. A companion paper\napplies our general result to non-convex examples.\n", "link": "http://arxiv.org/abs/2502.01953v2", "date": "2025-06-18", "relevancy": 2.1768, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4452}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4414}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20minima%20of%20the%20empirical%20risk%20in%20high%20dimension%3A%20General%20theorems%0A%20%20and%20convex%20examples&body=Title%3A%20Local%20minima%20of%20the%20empirical%20risk%20in%20high%20dimension%3A%20General%20theorems%0A%20%20and%20convex%20examples%0AAuthor%3A%20Kiana%20Asgari%20and%20Andrea%20Montanari%20and%20Basil%20Saeed%0AAbstract%3A%20%20%20We%20consider%20a%20general%20model%20for%20high-dimensional%20empirical%20risk%20minimization%0Awhereby%20the%20data%20%24%5Cmathbf%7Bx%7D_i%24%20are%20%24d%24-dimensional%20isotropic%20Gaussian%20vectors%2C%0Athe%20model%20is%20parametrized%20by%20%24%5Cmathbf%7B%5CTheta%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D%24%2C%20and%0Athe%20loss%20depends%20on%20the%20data%20via%20the%20projection%0A%24%5Cmathbf%7B%5CTheta%7D%5E%5Cmathsf%7BT%7D%5Cmathbf%7Bx%7D_i%24.%20This%20setting%20covers%20as%20special%20cases%0Aclassical%20statistics%20methods%20%28e.g.%20multinomial%20regression%20and%20other%20generalized%0Alinear%20models%29%2C%20but%20also%20two-layer%20fully%20connected%20neural%20networks%20with%20%24k%24%0Ahidden%20neurons.%20We%20use%20the%20Kac-Rice%20formula%20from%20Gaussian%20process%20theory%20to%0Aderive%20a%20bound%20on%20the%20expected%20number%20of%20local%20minima%20of%20this%20empirical%20risk%2C%0Aunder%20the%20proportional%20asymptotics%20in%20which%20%24n%2Cd%5Cto%5Cinfty%24%2C%20with%20%24n%5Casymp%20d%24.%0AVia%20Markov%27s%20inequality%2C%20this%20bound%20allows%20to%20determine%20the%20positions%20of%20these%0Aminimizers%20%28with%20exponential%20deviation%20bounds%29%20and%20hence%20derive%20sharp%0Aasymptotics%20on%20the%20estimation%20and%20prediction%20error.%20In%20this%20paper%2C%20we%20apply%20our%0Acharacterization%20to%20convex%20losses%2C%20where%20high-dimensional%20asymptotics%20were%20not%0A%28in%20general%29%20rigorously%20established%20for%20%24k%5Cge%202%24.%20We%20show%20that%20our%20approach%20is%0Atight%20and%20allows%20to%20prove%20previously%20conjectured%20results.%20In%20addition%2C%20we%0Acharacterize%20the%20spectrum%20of%20the%20Hessian%20at%20the%20minimizer.%20A%20companion%20paper%0Aapplies%20our%20general%20result%20to%20non-convex%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520minima%2520of%2520the%2520empirical%2520risk%2520in%2520high%2520dimension%253A%2520General%2520theorems%250A%2520%2520and%2520convex%2520examples%26entry.906535625%3DKiana%2520Asgari%2520and%2520Andrea%2520Montanari%2520and%2520Basil%2520Saeed%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520general%2520model%2520for%2520high-dimensional%2520empirical%2520risk%2520minimization%250Awhereby%2520the%2520data%2520%2524%255Cmathbf%257Bx%257D_i%2524%2520are%2520%2524d%2524-dimensional%2520isotropic%2520Gaussian%2520vectors%252C%250Athe%2520model%2520is%2520parametrized%2520by%2520%2524%255Cmathbf%257B%255CTheta%257D%255Cin%255Cmathbb%257BR%257D%255E%257Bd%255Ctimes%2520k%257D%2524%252C%2520and%250Athe%2520loss%2520depends%2520on%2520the%2520data%2520via%2520the%2520projection%250A%2524%255Cmathbf%257B%255CTheta%257D%255E%255Cmathsf%257BT%257D%255Cmathbf%257Bx%257D_i%2524.%2520This%2520setting%2520covers%2520as%2520special%2520cases%250Aclassical%2520statistics%2520methods%2520%2528e.g.%2520multinomial%2520regression%2520and%2520other%2520generalized%250Alinear%2520models%2529%252C%2520but%2520also%2520two-layer%2520fully%2520connected%2520neural%2520networks%2520with%2520%2524k%2524%250Ahidden%2520neurons.%2520We%2520use%2520the%2520Kac-Rice%2520formula%2520from%2520Gaussian%2520process%2520theory%2520to%250Aderive%2520a%2520bound%2520on%2520the%2520expected%2520number%2520of%2520local%2520minima%2520of%2520this%2520empirical%2520risk%252C%250Aunder%2520the%2520proportional%2520asymptotics%2520in%2520which%2520%2524n%252Cd%255Cto%255Cinfty%2524%252C%2520with%2520%2524n%255Casymp%2520d%2524.%250AVia%2520Markov%2527s%2520inequality%252C%2520this%2520bound%2520allows%2520to%2520determine%2520the%2520positions%2520of%2520these%250Aminimizers%2520%2528with%2520exponential%2520deviation%2520bounds%2529%2520and%2520hence%2520derive%2520sharp%250Aasymptotics%2520on%2520the%2520estimation%2520and%2520prediction%2520error.%2520In%2520this%2520paper%252C%2520we%2520apply%2520our%250Acharacterization%2520to%2520convex%2520losses%252C%2520where%2520high-dimensional%2520asymptotics%2520were%2520not%250A%2528in%2520general%2529%2520rigorously%2520established%2520for%2520%2524k%255Cge%25202%2524.%2520We%2520show%2520that%2520our%2520approach%2520is%250Atight%2520and%2520allows%2520to%2520prove%2520previously%2520conjectured%2520results.%2520In%2520addition%252C%2520we%250Acharacterize%2520the%2520spectrum%2520of%2520the%2520Hessian%2520at%2520the%2520minimizer.%2520A%2520companion%2520paper%250Aapplies%2520our%2520general%2520result%2520to%2520non-convex%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20minima%20of%20the%20empirical%20risk%20in%20high%20dimension%3A%20General%20theorems%0A%20%20and%20convex%20examples&entry.906535625=Kiana%20Asgari%20and%20Andrea%20Montanari%20and%20Basil%20Saeed&entry.1292438233=%20%20We%20consider%20a%20general%20model%20for%20high-dimensional%20empirical%20risk%20minimization%0Awhereby%20the%20data%20%24%5Cmathbf%7Bx%7D_i%24%20are%20%24d%24-dimensional%20isotropic%20Gaussian%20vectors%2C%0Athe%20model%20is%20parametrized%20by%20%24%5Cmathbf%7B%5CTheta%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes%20k%7D%24%2C%20and%0Athe%20loss%20depends%20on%20the%20data%20via%20the%20projection%0A%24%5Cmathbf%7B%5CTheta%7D%5E%5Cmathsf%7BT%7D%5Cmathbf%7Bx%7D_i%24.%20This%20setting%20covers%20as%20special%20cases%0Aclassical%20statistics%20methods%20%28e.g.%20multinomial%20regression%20and%20other%20generalized%0Alinear%20models%29%2C%20but%20also%20two-layer%20fully%20connected%20neural%20networks%20with%20%24k%24%0Ahidden%20neurons.%20We%20use%20the%20Kac-Rice%20formula%20from%20Gaussian%20process%20theory%20to%0Aderive%20a%20bound%20on%20the%20expected%20number%20of%20local%20minima%20of%20this%20empirical%20risk%2C%0Aunder%20the%20proportional%20asymptotics%20in%20which%20%24n%2Cd%5Cto%5Cinfty%24%2C%20with%20%24n%5Casymp%20d%24.%0AVia%20Markov%27s%20inequality%2C%20this%20bound%20allows%20to%20determine%20the%20positions%20of%20these%0Aminimizers%20%28with%20exponential%20deviation%20bounds%29%20and%20hence%20derive%20sharp%0Aasymptotics%20on%20the%20estimation%20and%20prediction%20error.%20In%20this%20paper%2C%20we%20apply%20our%0Acharacterization%20to%20convex%20losses%2C%20where%20high-dimensional%20asymptotics%20were%20not%0A%28in%20general%29%20rigorously%20established%20for%20%24k%5Cge%202%24.%20We%20show%20that%20our%20approach%20is%0Atight%20and%20allows%20to%20prove%20previously%20conjectured%20results.%20In%20addition%2C%20we%0Acharacterize%20the%20spectrum%20of%20the%20Hessian%20at%20the%20minimizer.%20A%20companion%20paper%0Aapplies%20our%20general%20result%20to%20non-convex%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01953v2&entry.124074799=Read"},
{"title": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh\n  Smoothing", "author": "Le Vu Anh and Nguyen Viet Anh and Mehmet Dik and Tu Nguyen Thi Ngoc", "abstract": "  Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.\n", "link": "http://arxiv.org/abs/2506.15571v1", "date": "2025-06-18", "relevancy": 2.1717, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5679}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5307}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MicroRicci%3A%20A%20Greedy%20and%20Local%20Ricci%20Flow%20Solver%20for%20Self-Tuning%20Mesh%0A%20%20Smoothing&body=Title%3A%20MicroRicci%3A%20A%20Greedy%20and%20Local%20Ricci%20Flow%20Solver%20for%20Self-Tuning%20Mesh%0A%20%20Smoothing%0AAuthor%3A%20Le%20Vu%20Anh%20and%20Nguyen%20Viet%20Anh%20and%20Mehmet%20Dik%20and%20Tu%20Nguyen%20Thi%20Ngoc%0AAbstract%3A%20%20%20Real-time%20mesh%20smoothing%20at%20scale%20remains%20a%20formidable%20challenge%3A%20classical%0ARicci-flow%20solvers%20demand%20costly%20global%20updates%2C%20while%20greedy%20heuristics%20suffer%0Afrom%20slow%20convergence%20or%20brittle%20tuning.%20We%20present%20MicroRicci%2C%20the%20first%20truly%0Aself-tuning%2C%20local%20Ricci-flow%20solver%20that%20borrows%20ideas%20from%20coding%20theory%20and%0Apacks%20them%20into%20just%201K%20%2B%20200%20parameters.%20Its%20primary%20core%20is%20a%20greedy%0Asyndrome-decoding%20step%20that%20pinpoints%20and%20corrects%20the%20largest%20curvature%20error%0Ain%20O%28E%29%20time%2C%20augmented%20by%20two%20tiny%20neural%20modules%20that%20adaptively%20choose%0Avertices%20and%20step%20sizes%20on%20the%20fly.%20On%20a%20diverse%20set%20of%20110%20SJTU-TMQA%20meshes%2C%0AMicroRicci%20slashes%20iteration%20counts%20from%20950%2B%3D140%20to%20400%2B%3D80%20%282.4x%20speedup%29%2C%0Atightens%20curvature%20spread%20from%200.19%20to%200.185%2C%20and%20achieves%20a%20remarkable%0AUV-distortion-to-MOS%20correlation%20of%20r%20%3D%20-0.93.%20It%20adds%20only%200.25%20ms%20per%0Aiteration%20%280.80%20to%201.05%20ms%29%2C%20yielding%20an%20end-to-end%201.8x%20runtime%20acceleration%0Aover%20state-of-the-art%20methods.%20MicroRicci%27s%20combination%20of%20linear-time%20updates%2C%0Aautomatic%20hyperparameter%20adaptation%2C%20and%20high-quality%20geometric%20and%20perceptual%0Aresults%20makes%20it%20well%20suited%20for%20real-time%2C%20resource-limited%20applications%20in%0Agraphics%2C%20simulation%2C%20and%20related%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicroRicci%253A%2520A%2520Greedy%2520and%2520Local%2520Ricci%2520Flow%2520Solver%2520for%2520Self-Tuning%2520Mesh%250A%2520%2520Smoothing%26entry.906535625%3DLe%2520Vu%2520Anh%2520and%2520Nguyen%2520Viet%2520Anh%2520and%2520Mehmet%2520Dik%2520and%2520Tu%2520Nguyen%2520Thi%2520Ngoc%26entry.1292438233%3D%2520%2520Real-time%2520mesh%2520smoothing%2520at%2520scale%2520remains%2520a%2520formidable%2520challenge%253A%2520classical%250ARicci-flow%2520solvers%2520demand%2520costly%2520global%2520updates%252C%2520while%2520greedy%2520heuristics%2520suffer%250Afrom%2520slow%2520convergence%2520or%2520brittle%2520tuning.%2520We%2520present%2520MicroRicci%252C%2520the%2520first%2520truly%250Aself-tuning%252C%2520local%2520Ricci-flow%2520solver%2520that%2520borrows%2520ideas%2520from%2520coding%2520theory%2520and%250Apacks%2520them%2520into%2520just%25201K%2520%252B%2520200%2520parameters.%2520Its%2520primary%2520core%2520is%2520a%2520greedy%250Asyndrome-decoding%2520step%2520that%2520pinpoints%2520and%2520corrects%2520the%2520largest%2520curvature%2520error%250Ain%2520O%2528E%2529%2520time%252C%2520augmented%2520by%2520two%2520tiny%2520neural%2520modules%2520that%2520adaptively%2520choose%250Avertices%2520and%2520step%2520sizes%2520on%2520the%2520fly.%2520On%2520a%2520diverse%2520set%2520of%2520110%2520SJTU-TMQA%2520meshes%252C%250AMicroRicci%2520slashes%2520iteration%2520counts%2520from%2520950%252B%253D140%2520to%2520400%252B%253D80%2520%25282.4x%2520speedup%2529%252C%250Atightens%2520curvature%2520spread%2520from%25200.19%2520to%25200.185%252C%2520and%2520achieves%2520a%2520remarkable%250AUV-distortion-to-MOS%2520correlation%2520of%2520r%2520%253D%2520-0.93.%2520It%2520adds%2520only%25200.25%2520ms%2520per%250Aiteration%2520%25280.80%2520to%25201.05%2520ms%2529%252C%2520yielding%2520an%2520end-to-end%25201.8x%2520runtime%2520acceleration%250Aover%2520state-of-the-art%2520methods.%2520MicroRicci%2527s%2520combination%2520of%2520linear-time%2520updates%252C%250Aautomatic%2520hyperparameter%2520adaptation%252C%2520and%2520high-quality%2520geometric%2520and%2520perceptual%250Aresults%2520makes%2520it%2520well%2520suited%2520for%2520real-time%252C%2520resource-limited%2520applications%2520in%250Agraphics%252C%2520simulation%252C%2520and%2520related%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MicroRicci%3A%20A%20Greedy%20and%20Local%20Ricci%20Flow%20Solver%20for%20Self-Tuning%20Mesh%0A%20%20Smoothing&entry.906535625=Le%20Vu%20Anh%20and%20Nguyen%20Viet%20Anh%20and%20Mehmet%20Dik%20and%20Tu%20Nguyen%20Thi%20Ngoc&entry.1292438233=%20%20Real-time%20mesh%20smoothing%20at%20scale%20remains%20a%20formidable%20challenge%3A%20classical%0ARicci-flow%20solvers%20demand%20costly%20global%20updates%2C%20while%20greedy%20heuristics%20suffer%0Afrom%20slow%20convergence%20or%20brittle%20tuning.%20We%20present%20MicroRicci%2C%20the%20first%20truly%0Aself-tuning%2C%20local%20Ricci-flow%20solver%20that%20borrows%20ideas%20from%20coding%20theory%20and%0Apacks%20them%20into%20just%201K%20%2B%20200%20parameters.%20Its%20primary%20core%20is%20a%20greedy%0Asyndrome-decoding%20step%20that%20pinpoints%20and%20corrects%20the%20largest%20curvature%20error%0Ain%20O%28E%29%20time%2C%20augmented%20by%20two%20tiny%20neural%20modules%20that%20adaptively%20choose%0Avertices%20and%20step%20sizes%20on%20the%20fly.%20On%20a%20diverse%20set%20of%20110%20SJTU-TMQA%20meshes%2C%0AMicroRicci%20slashes%20iteration%20counts%20from%20950%2B%3D140%20to%20400%2B%3D80%20%282.4x%20speedup%29%2C%0Atightens%20curvature%20spread%20from%200.19%20to%200.185%2C%20and%20achieves%20a%20remarkable%0AUV-distortion-to-MOS%20correlation%20of%20r%20%3D%20-0.93.%20It%20adds%20only%200.25%20ms%20per%0Aiteration%20%280.80%20to%201.05%20ms%29%2C%20yielding%20an%20end-to-end%201.8x%20runtime%20acceleration%0Aover%20state-of-the-art%20methods.%20MicroRicci%27s%20combination%20of%20linear-time%20updates%2C%0Aautomatic%20hyperparameter%20adaptation%2C%20and%20high-quality%20geometric%20and%20perceptual%0Aresults%20makes%20it%20well%20suited%20for%20real-time%2C%20resource-limited%20applications%20in%0Agraphics%2C%20simulation%2C%20and%20related%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15571v1&entry.124074799=Read"},
{"title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement\n  Learning", "author": "Roger Creus Castanyer and Johan Obando-Ceron and Lu Li and Pierre-Luc Bacon and Glen Berseth and Aaron Courville and Pablo Samuel Castro", "abstract": "  Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.\n", "link": "http://arxiv.org/abs/2506.15544v1", "date": "2025-06-18", "relevancy": 2.1536, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5632}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5338}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Gradients%20for%20Stable%20Learning%20at%20Scale%20in%20Deep%20Reinforcement%0A%20%20Learning&body=Title%3A%20Stable%20Gradients%20for%20Stable%20Learning%20at%20Scale%20in%20Deep%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Roger%20Creus%20Castanyer%20and%20Johan%20Obando-Ceron%20and%20Lu%20Li%20and%20Pierre-Luc%20Bacon%20and%20Glen%20Berseth%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro%0AAbstract%3A%20%20%20Scaling%20deep%20reinforcement%20learning%20networks%20is%20challenging%20and%20often%20results%0Ain%20degraded%20performance%2C%20yet%20the%20root%20causes%20of%20this%20failure%20mode%20remain%20poorly%0Aunderstood.%20Several%20recent%20works%20have%20proposed%20mechanisms%20to%20address%20this%2C%20but%0Athey%20are%20often%20complex%20and%20fail%20to%20highlight%20the%20causes%20underlying%20this%0Adifficulty.%20In%20this%20work%2C%20we%20conduct%20a%20series%20of%20empirical%20analyses%20which%0Asuggest%20that%20the%20combination%20of%20non-stationarity%20with%20gradient%20pathologies%2C%20due%0Ato%20suboptimal%20architectural%20choices%2C%20underlie%20the%20challenges%20of%20scale.%20We%0Apropose%20a%20series%20of%20direct%20interventions%20that%20stabilize%20gradient%20flow%2C%20enabling%0Arobust%20performance%20across%20a%20range%20of%20network%20depths%20and%20widths.%20Our%0Ainterventions%20are%20simple%20to%20implement%20and%20compatible%20with%20well-established%0Aalgorithms%2C%20and%20result%20in%20an%20effective%20mechanism%20that%20enables%20strong%0Aperformance%20even%20at%20large%20scales.%20We%20validate%20our%20findings%20on%20a%20variety%20of%0Aagents%20and%20suites%20of%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Gradients%2520for%2520Stable%2520Learning%2520at%2520Scale%2520in%2520Deep%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DRoger%2520Creus%2520Castanyer%2520and%2520Johan%2520Obando-Ceron%2520and%2520Lu%2520Li%2520and%2520Pierre-Luc%2520Bacon%2520and%2520Glen%2520Berseth%2520and%2520Aaron%2520Courville%2520and%2520Pablo%2520Samuel%2520Castro%26entry.1292438233%3D%2520%2520Scaling%2520deep%2520reinforcement%2520learning%2520networks%2520is%2520challenging%2520and%2520often%2520results%250Ain%2520degraded%2520performance%252C%2520yet%2520the%2520root%2520causes%2520of%2520this%2520failure%2520mode%2520remain%2520poorly%250Aunderstood.%2520Several%2520recent%2520works%2520have%2520proposed%2520mechanisms%2520to%2520address%2520this%252C%2520but%250Athey%2520are%2520often%2520complex%2520and%2520fail%2520to%2520highlight%2520the%2520causes%2520underlying%2520this%250Adifficulty.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520series%2520of%2520empirical%2520analyses%2520which%250Asuggest%2520that%2520the%2520combination%2520of%2520non-stationarity%2520with%2520gradient%2520pathologies%252C%2520due%250Ato%2520suboptimal%2520architectural%2520choices%252C%2520underlie%2520the%2520challenges%2520of%2520scale.%2520We%250Apropose%2520a%2520series%2520of%2520direct%2520interventions%2520that%2520stabilize%2520gradient%2520flow%252C%2520enabling%250Arobust%2520performance%2520across%2520a%2520range%2520of%2520network%2520depths%2520and%2520widths.%2520Our%250Ainterventions%2520are%2520simple%2520to%2520implement%2520and%2520compatible%2520with%2520well-established%250Aalgorithms%252C%2520and%2520result%2520in%2520an%2520effective%2520mechanism%2520that%2520enables%2520strong%250Aperformance%2520even%2520at%2520large%2520scales.%2520We%2520validate%2520our%2520findings%2520on%2520a%2520variety%2520of%250Aagents%2520and%2520suites%2520of%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Gradients%20for%20Stable%20Learning%20at%20Scale%20in%20Deep%20Reinforcement%0A%20%20Learning&entry.906535625=Roger%20Creus%20Castanyer%20and%20Johan%20Obando-Ceron%20and%20Lu%20Li%20and%20Pierre-Luc%20Bacon%20and%20Glen%20Berseth%20and%20Aaron%20Courville%20and%20Pablo%20Samuel%20Castro&entry.1292438233=%20%20Scaling%20deep%20reinforcement%20learning%20networks%20is%20challenging%20and%20often%20results%0Ain%20degraded%20performance%2C%20yet%20the%20root%20causes%20of%20this%20failure%20mode%20remain%20poorly%0Aunderstood.%20Several%20recent%20works%20have%20proposed%20mechanisms%20to%20address%20this%2C%20but%0Athey%20are%20often%20complex%20and%20fail%20to%20highlight%20the%20causes%20underlying%20this%0Adifficulty.%20In%20this%20work%2C%20we%20conduct%20a%20series%20of%20empirical%20analyses%20which%0Asuggest%20that%20the%20combination%20of%20non-stationarity%20with%20gradient%20pathologies%2C%20due%0Ato%20suboptimal%20architectural%20choices%2C%20underlie%20the%20challenges%20of%20scale.%20We%0Apropose%20a%20series%20of%20direct%20interventions%20that%20stabilize%20gradient%20flow%2C%20enabling%0Arobust%20performance%20across%20a%20range%20of%20network%20depths%20and%20widths.%20Our%0Ainterventions%20are%20simple%20to%20implement%20and%20compatible%20with%20well-established%0Aalgorithms%2C%20and%20result%20in%20an%20effective%20mechanism%20that%20enables%20strong%0Aperformance%20even%20at%20large%20scales.%20We%20validate%20our%20findings%20on%20a%20variety%20of%0Aagents%20and%20suites%20of%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15544v1&entry.124074799=Read"},
{"title": "When and How Unlabeled Data Provably Improve In-Context Learning", "author": "Yingcong Li and Xiangyu Chang and Muti Kara and Xiaofeng Liu and Amit Roy-Chowdhury and Samet Oymak", "abstract": "  Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.\n", "link": "http://arxiv.org/abs/2506.15329v1", "date": "2025-06-18", "relevancy": 2.15, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5561}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5428}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20and%20How%20Unlabeled%20Data%20Provably%20Improve%20In-Context%20Learning&body=Title%3A%20When%20and%20How%20Unlabeled%20Data%20Provably%20Improve%20In-Context%20Learning%0AAuthor%3A%20Yingcong%20Li%20and%20Xiangyu%20Chang%20and%20Muti%20Kara%20and%20Xiaofeng%20Liu%20and%20Amit%20Roy-Chowdhury%20and%20Samet%20Oymak%0AAbstract%3A%20%20%20Recent%20research%20shows%20that%20in-context%20learning%20%28ICL%29%20can%20be%20effective%20even%0Awhen%20demonstrations%20have%20missing%20or%20incorrect%20labels.%20To%20shed%20light%20on%20this%0Acapability%2C%20we%20examine%20a%20canonical%20setting%20where%20the%20demonstrations%20are%20drawn%0Aaccording%20to%20a%20binary%20Gaussian%20mixture%20model%20%28GMM%29%20and%20a%20certain%20fraction%20of%0Athe%20demonstrations%20have%20missing%20labels.%20We%20provide%20a%20comprehensive%20theoretical%0Astudy%20to%20show%20that%3A%20%281%29%20The%20loss%20landscape%20of%20one-layer%20linear%20attention%20models%0Arecover%20the%20optimal%20fully-supervised%20estimator%20but%20completely%20fail%20to%20exploit%0Aunlabeled%20data%3B%20%282%29%20In%20contrast%2C%20multilayer%20or%20looped%20transformers%20can%0Aeffectively%20leverage%20unlabeled%20data%20by%20implicitly%20constructing%20estimators%20of%0Athe%20form%20%24%5Csum_%7Bi%5Cge%200%7D%20a_i%20%28X%5E%5Ctop%20X%29%5EiX%5E%5Ctop%20y%24%20with%20%24X%24%20and%20%24y%24%20denoting%0Afeatures%20and%20partially-observed%20labels%20%28with%20missing%20entries%20set%20to%20zero%29.%20We%0Acharacterize%20the%20class%20of%20polynomials%20that%20can%20be%20expressed%20as%20a%20function%20of%0Adepth%20and%20draw%20connections%20to%20Expectation%20Maximization%2C%20an%20iterative%0Apseudo-labeling%20algorithm%20commonly%20used%20in%20semi-supervised%20learning.%0AImportantly%2C%20the%20leading%20polynomial%20power%20is%20exponential%20in%20depth%2C%20so%20mild%0Aamount%20of%20depth/looping%20suffices.%20As%20an%20application%20of%20theory%2C%20we%20propose%0Alooping%20off-the-shelf%20tabular%20foundation%20models%20to%20enhance%20their%0Asemi-supervision%20capabilities.%20Extensive%20evaluations%20on%20real-world%20datasets%0Ashow%20that%20our%20method%20significantly%20improves%20the%20semisupervised%20tabular%20learning%0Aperformance%20over%20the%20standard%20single%20pass%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520and%2520How%2520Unlabeled%2520Data%2520Provably%2520Improve%2520In-Context%2520Learning%26entry.906535625%3DYingcong%2520Li%2520and%2520Xiangyu%2520Chang%2520and%2520Muti%2520Kara%2520and%2520Xiaofeng%2520Liu%2520and%2520Amit%2520Roy-Chowdhury%2520and%2520Samet%2520Oymak%26entry.1292438233%3D%2520%2520Recent%2520research%2520shows%2520that%2520in-context%2520learning%2520%2528ICL%2529%2520can%2520be%2520effective%2520even%250Awhen%2520demonstrations%2520have%2520missing%2520or%2520incorrect%2520labels.%2520To%2520shed%2520light%2520on%2520this%250Acapability%252C%2520we%2520examine%2520a%2520canonical%2520setting%2520where%2520the%2520demonstrations%2520are%2520drawn%250Aaccording%2520to%2520a%2520binary%2520Gaussian%2520mixture%2520model%2520%2528GMM%2529%2520and%2520a%2520certain%2520fraction%2520of%250Athe%2520demonstrations%2520have%2520missing%2520labels.%2520We%2520provide%2520a%2520comprehensive%2520theoretical%250Astudy%2520to%2520show%2520that%253A%2520%25281%2529%2520The%2520loss%2520landscape%2520of%2520one-layer%2520linear%2520attention%2520models%250Arecover%2520the%2520optimal%2520fully-supervised%2520estimator%2520but%2520completely%2520fail%2520to%2520exploit%250Aunlabeled%2520data%253B%2520%25282%2529%2520In%2520contrast%252C%2520multilayer%2520or%2520looped%2520transformers%2520can%250Aeffectively%2520leverage%2520unlabeled%2520data%2520by%2520implicitly%2520constructing%2520estimators%2520of%250Athe%2520form%2520%2524%255Csum_%257Bi%255Cge%25200%257D%2520a_i%2520%2528X%255E%255Ctop%2520X%2529%255EiX%255E%255Ctop%2520y%2524%2520with%2520%2524X%2524%2520and%2520%2524y%2524%2520denoting%250Afeatures%2520and%2520partially-observed%2520labels%2520%2528with%2520missing%2520entries%2520set%2520to%2520zero%2529.%2520We%250Acharacterize%2520the%2520class%2520of%2520polynomials%2520that%2520can%2520be%2520expressed%2520as%2520a%2520function%2520of%250Adepth%2520and%2520draw%2520connections%2520to%2520Expectation%2520Maximization%252C%2520an%2520iterative%250Apseudo-labeling%2520algorithm%2520commonly%2520used%2520in%2520semi-supervised%2520learning.%250AImportantly%252C%2520the%2520leading%2520polynomial%2520power%2520is%2520exponential%2520in%2520depth%252C%2520so%2520mild%250Aamount%2520of%2520depth/looping%2520suffices.%2520As%2520an%2520application%2520of%2520theory%252C%2520we%2520propose%250Alooping%2520off-the-shelf%2520tabular%2520foundation%2520models%2520to%2520enhance%2520their%250Asemi-supervision%2520capabilities.%2520Extensive%2520evaluations%2520on%2520real-world%2520datasets%250Ashow%2520that%2520our%2520method%2520significantly%2520improves%2520the%2520semisupervised%2520tabular%2520learning%250Aperformance%2520over%2520the%2520standard%2520single%2520pass%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20and%20How%20Unlabeled%20Data%20Provably%20Improve%20In-Context%20Learning&entry.906535625=Yingcong%20Li%20and%20Xiangyu%20Chang%20and%20Muti%20Kara%20and%20Xiaofeng%20Liu%20and%20Amit%20Roy-Chowdhury%20and%20Samet%20Oymak&entry.1292438233=%20%20Recent%20research%20shows%20that%20in-context%20learning%20%28ICL%29%20can%20be%20effective%20even%0Awhen%20demonstrations%20have%20missing%20or%20incorrect%20labels.%20To%20shed%20light%20on%20this%0Acapability%2C%20we%20examine%20a%20canonical%20setting%20where%20the%20demonstrations%20are%20drawn%0Aaccording%20to%20a%20binary%20Gaussian%20mixture%20model%20%28GMM%29%20and%20a%20certain%20fraction%20of%0Athe%20demonstrations%20have%20missing%20labels.%20We%20provide%20a%20comprehensive%20theoretical%0Astudy%20to%20show%20that%3A%20%281%29%20The%20loss%20landscape%20of%20one-layer%20linear%20attention%20models%0Arecover%20the%20optimal%20fully-supervised%20estimator%20but%20completely%20fail%20to%20exploit%0Aunlabeled%20data%3B%20%282%29%20In%20contrast%2C%20multilayer%20or%20looped%20transformers%20can%0Aeffectively%20leverage%20unlabeled%20data%20by%20implicitly%20constructing%20estimators%20of%0Athe%20form%20%24%5Csum_%7Bi%5Cge%200%7D%20a_i%20%28X%5E%5Ctop%20X%29%5EiX%5E%5Ctop%20y%24%20with%20%24X%24%20and%20%24y%24%20denoting%0Afeatures%20and%20partially-observed%20labels%20%28with%20missing%20entries%20set%20to%20zero%29.%20We%0Acharacterize%20the%20class%20of%20polynomials%20that%20can%20be%20expressed%20as%20a%20function%20of%0Adepth%20and%20draw%20connections%20to%20Expectation%20Maximization%2C%20an%20iterative%0Apseudo-labeling%20algorithm%20commonly%20used%20in%20semi-supervised%20learning.%0AImportantly%2C%20the%20leading%20polynomial%20power%20is%20exponential%20in%20depth%2C%20so%20mild%0Aamount%20of%20depth/looping%20suffices.%20As%20an%20application%20of%20theory%2C%20we%20propose%0Alooping%20off-the-shelf%20tabular%20foundation%20models%20to%20enhance%20their%0Asemi-supervision%20capabilities.%20Extensive%20evaluations%20on%20real-world%20datasets%0Ashow%20that%20our%20method%20significantly%20improves%20the%20semisupervised%20tabular%20learning%0Aperformance%20over%20the%20standard%20single%20pass%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15329v1&entry.124074799=Read"},
{"title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence", "author": "Yao Zhang and Chenyang Lin and Shijie Tang and Haokun Chen and Shijie Zhou and Yunpu Ma and Volker Tresp", "abstract": "  The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.\n", "link": "http://arxiv.org/abs/2506.15672v1", "date": "2025-06-18", "relevancy": 2.1467, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5646}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5492}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SwarmAgentic%3A%20Towards%20Fully%20Automated%20Agentic%20System%20Generation%20via%0A%20%20Swarm%20Intelligence&body=Title%3A%20SwarmAgentic%3A%20Towards%20Fully%20Automated%20Agentic%20System%20Generation%20via%0A%20%20Swarm%20Intelligence%0AAuthor%3A%20Yao%20Zhang%20and%20Chenyang%20Lin%20and%20Shijie%20Tang%20and%20Haokun%20Chen%20and%20Shijie%20Zhou%20and%20Yunpu%20Ma%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20Large%20Language%20Models%20has%20advanced%20agentic%20systems%20in%0Adecision-making%2C%20coordination%2C%20and%20task%20execution.%20Yet%2C%20existing%20agentic%20system%0Ageneration%20frameworks%20lack%20full%20autonomy%2C%20missing%20from-scratch%20agent%0Ageneration%2C%20self-optimizing%20agent%20functionality%2C%20and%20collaboration%2C%20limiting%0Aadaptability%20and%20scalability.%20We%20propose%20SwarmAgentic%2C%20a%20framework%20for%20fully%0Aautomated%20agentic%20system%20generation%20that%20constructs%20agentic%20systems%20from%0Ascratch%20and%20jointly%20optimizes%20agent%20functionality%20and%20collaboration%20as%0Ainterdependent%20components%20through%20language-driven%20exploration.%20To%20enable%0Aefficient%20search%20over%20system-level%20structures%2C%20SwarmAgentic%20maintains%20a%0Apopulation%20of%20candidate%20systems%20and%20evolves%20them%20via%20feedback-guided%20updates%2C%0Adrawing%20inspiration%20from%20Particle%20Swarm%20Optimization%20%28PSO%29.%20We%20evaluate%20our%0Amethod%20on%20six%20real-world%2C%20open-ended%2C%20and%20exploratory%20tasks%20involving%0Ahigh-level%20planning%2C%20system-level%20coordination%2C%20and%20creative%20reasoning.%20Given%0Aonly%20a%20task%20description%20and%20an%20objective%20function%2C%20SwarmAgentic%20outperforms%20all%0Abaselines%2C%20achieving%20a%20%2B261.8%25%20relative%20improvement%20over%20ADAS%20on%20the%0ATravelPlanner%20benchmark%2C%20highlighting%20the%20effectiveness%20of%20full%20automation%20in%0Astructurally%20unconstrained%20tasks.%20This%20framework%20marks%20a%20significant%20step%0Atoward%20scalable%20and%20autonomous%20agentic%20system%20design%2C%20bridging%20swarm%0Aintelligence%20with%20fully%20automated%20system%20multi-agent%20generation.%20Our%20code%20is%0Apublicly%20released%20at%20https%3A//yaoz720.github.io/SwarmAgentic/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwarmAgentic%253A%2520Towards%2520Fully%2520Automated%2520Agentic%2520System%2520Generation%2520via%250A%2520%2520Swarm%2520Intelligence%26entry.906535625%3DYao%2520Zhang%2520and%2520Chenyang%2520Lin%2520and%2520Shijie%2520Tang%2520and%2520Haokun%2520Chen%2520and%2520Shijie%2520Zhou%2520and%2520Yunpu%2520Ma%2520and%2520Volker%2520Tresp%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520Large%2520Language%2520Models%2520has%2520advanced%2520agentic%2520systems%2520in%250Adecision-making%252C%2520coordination%252C%2520and%2520task%2520execution.%2520Yet%252C%2520existing%2520agentic%2520system%250Ageneration%2520frameworks%2520lack%2520full%2520autonomy%252C%2520missing%2520from-scratch%2520agent%250Ageneration%252C%2520self-optimizing%2520agent%2520functionality%252C%2520and%2520collaboration%252C%2520limiting%250Aadaptability%2520and%2520scalability.%2520We%2520propose%2520SwarmAgentic%252C%2520a%2520framework%2520for%2520fully%250Aautomated%2520agentic%2520system%2520generation%2520that%2520constructs%2520agentic%2520systems%2520from%250Ascratch%2520and%2520jointly%2520optimizes%2520agent%2520functionality%2520and%2520collaboration%2520as%250Ainterdependent%2520components%2520through%2520language-driven%2520exploration.%2520To%2520enable%250Aefficient%2520search%2520over%2520system-level%2520structures%252C%2520SwarmAgentic%2520maintains%2520a%250Apopulation%2520of%2520candidate%2520systems%2520and%2520evolves%2520them%2520via%2520feedback-guided%2520updates%252C%250Adrawing%2520inspiration%2520from%2520Particle%2520Swarm%2520Optimization%2520%2528PSO%2529.%2520We%2520evaluate%2520our%250Amethod%2520on%2520six%2520real-world%252C%2520open-ended%252C%2520and%2520exploratory%2520tasks%2520involving%250Ahigh-level%2520planning%252C%2520system-level%2520coordination%252C%2520and%2520creative%2520reasoning.%2520Given%250Aonly%2520a%2520task%2520description%2520and%2520an%2520objective%2520function%252C%2520SwarmAgentic%2520outperforms%2520all%250Abaselines%252C%2520achieving%2520a%2520%252B261.8%2525%2520relative%2520improvement%2520over%2520ADAS%2520on%2520the%250ATravelPlanner%2520benchmark%252C%2520highlighting%2520the%2520effectiveness%2520of%2520full%2520automation%2520in%250Astructurally%2520unconstrained%2520tasks.%2520This%2520framework%2520marks%2520a%2520significant%2520step%250Atoward%2520scalable%2520and%2520autonomous%2520agentic%2520system%2520design%252C%2520bridging%2520swarm%250Aintelligence%2520with%2520fully%2520automated%2520system%2520multi-agent%2520generation.%2520Our%2520code%2520is%250Apublicly%2520released%2520at%2520https%253A//yaoz720.github.io/SwarmAgentic/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwarmAgentic%3A%20Towards%20Fully%20Automated%20Agentic%20System%20Generation%20via%0A%20%20Swarm%20Intelligence&entry.906535625=Yao%20Zhang%20and%20Chenyang%20Lin%20and%20Shijie%20Tang%20and%20Haokun%20Chen%20and%20Shijie%20Zhou%20and%20Yunpu%20Ma%20and%20Volker%20Tresp&entry.1292438233=%20%20The%20rapid%20progress%20of%20Large%20Language%20Models%20has%20advanced%20agentic%20systems%20in%0Adecision-making%2C%20coordination%2C%20and%20task%20execution.%20Yet%2C%20existing%20agentic%20system%0Ageneration%20frameworks%20lack%20full%20autonomy%2C%20missing%20from-scratch%20agent%0Ageneration%2C%20self-optimizing%20agent%20functionality%2C%20and%20collaboration%2C%20limiting%0Aadaptability%20and%20scalability.%20We%20propose%20SwarmAgentic%2C%20a%20framework%20for%20fully%0Aautomated%20agentic%20system%20generation%20that%20constructs%20agentic%20systems%20from%0Ascratch%20and%20jointly%20optimizes%20agent%20functionality%20and%20collaboration%20as%0Ainterdependent%20components%20through%20language-driven%20exploration.%20To%20enable%0Aefficient%20search%20over%20system-level%20structures%2C%20SwarmAgentic%20maintains%20a%0Apopulation%20of%20candidate%20systems%20and%20evolves%20them%20via%20feedback-guided%20updates%2C%0Adrawing%20inspiration%20from%20Particle%20Swarm%20Optimization%20%28PSO%29.%20We%20evaluate%20our%0Amethod%20on%20six%20real-world%2C%20open-ended%2C%20and%20exploratory%20tasks%20involving%0Ahigh-level%20planning%2C%20system-level%20coordination%2C%20and%20creative%20reasoning.%20Given%0Aonly%20a%20task%20description%20and%20an%20objective%20function%2C%20SwarmAgentic%20outperforms%20all%0Abaselines%2C%20achieving%20a%20%2B261.8%25%20relative%20improvement%20over%20ADAS%20on%20the%0ATravelPlanner%20benchmark%2C%20highlighting%20the%20effectiveness%20of%20full%20automation%20in%0Astructurally%20unconstrained%20tasks.%20This%20framework%20marks%20a%20significant%20step%0Atoward%20scalable%20and%20autonomous%20agentic%20system%20design%2C%20bridging%20swarm%0Aintelligence%20with%20fully%20automated%20system%20multi-agent%20generation.%20Our%20code%20is%0Apublicly%20released%20at%20https%3A//yaoz720.github.io/SwarmAgentic/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15672v1&entry.124074799=Read"},
{"title": "Heterogeneous Relationships of Subjects and Shapelets for\n  Semi-supervised Multivariate Series Classification", "author": "Mingsen Du and Meng Chen and Yongjian Li and Cun Ji and Shoushui Wei", "abstract": "  Multivariate time series (MTS) classification is widely applied in fields\nsuch as industry, healthcare, and finance, aiming to extract key features from\ncomplex time series data for accurate decision-making and prediction. However,\nexisting methods for MTS often struggle due to the challenges of effectively\nmodeling high-dimensional data and the lack of labeled data, resulting in poor\nclassification performance. To address this issue, we propose a heterogeneous\nrelationships of subjects and shapelets method for semi-supervised MTS\nclassification. This method offers a novel perspective by integrating various\ntypes of additional information while capturing the relationships between them.\nSpecifically, we first utilize a contrast temporal self-attention module to\nobtain sparse MTS representations, and then model the similarities between\nthese representations using soft dynamic time warping to construct a similarity\ngraph. Secondly, we learn the shapelets for different subject types,\nincorporating both the subject features and their shapelets as additional\ninformation to further refine the similarity graph, ultimately generating a\nheterogeneous graph. Finally, we use a dual level graph attention network to\nget prediction. Through this method, we successfully transform dataset into a\nheterogeneous graph, integrating multiple additional information and achieving\nprecise semi-supervised node classification. Experiments on the Human Activity\nRecognition, sleep stage classification and University of East Anglia datasets\ndemonstrate that our method outperforms current state-of-the-art methods in MTS\nclassification tasks, validating its superiority.\n", "link": "http://arxiv.org/abs/2411.18043v2", "date": "2025-06-18", "relevancy": 2.1308, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneous%20Relationships%20of%20Subjects%20and%20Shapelets%20for%0A%20%20Semi-supervised%20Multivariate%20Series%20Classification&body=Title%3A%20Heterogeneous%20Relationships%20of%20Subjects%20and%20Shapelets%20for%0A%20%20Semi-supervised%20Multivariate%20Series%20Classification%0AAuthor%3A%20Mingsen%20Du%20and%20Meng%20Chen%20and%20Yongjian%20Li%20and%20Cun%20Ji%20and%20Shoushui%20Wei%0AAbstract%3A%20%20%20Multivariate%20time%20series%20%28MTS%29%20classification%20is%20widely%20applied%20in%20fields%0Asuch%20as%20industry%2C%20healthcare%2C%20and%20finance%2C%20aiming%20to%20extract%20key%20features%20from%0Acomplex%20time%20series%20data%20for%20accurate%20decision-making%20and%20prediction.%20However%2C%0Aexisting%20methods%20for%20MTS%20often%20struggle%20due%20to%20the%20challenges%20of%20effectively%0Amodeling%20high-dimensional%20data%20and%20the%20lack%20of%20labeled%20data%2C%20resulting%20in%20poor%0Aclassification%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20heterogeneous%0Arelationships%20of%20subjects%20and%20shapelets%20method%20for%20semi-supervised%20MTS%0Aclassification.%20This%20method%20offers%20a%20novel%20perspective%20by%20integrating%20various%0Atypes%20of%20additional%20information%20while%20capturing%20the%20relationships%20between%20them.%0ASpecifically%2C%20we%20first%20utilize%20a%20contrast%20temporal%20self-attention%20module%20to%0Aobtain%20sparse%20MTS%20representations%2C%20and%20then%20model%20the%20similarities%20between%0Athese%20representations%20using%20soft%20dynamic%20time%20warping%20to%20construct%20a%20similarity%0Agraph.%20Secondly%2C%20we%20learn%20the%20shapelets%20for%20different%20subject%20types%2C%0Aincorporating%20both%20the%20subject%20features%20and%20their%20shapelets%20as%20additional%0Ainformation%20to%20further%20refine%20the%20similarity%20graph%2C%20ultimately%20generating%20a%0Aheterogeneous%20graph.%20Finally%2C%20we%20use%20a%20dual%20level%20graph%20attention%20network%20to%0Aget%20prediction.%20Through%20this%20method%2C%20we%20successfully%20transform%20dataset%20into%20a%0Aheterogeneous%20graph%2C%20integrating%20multiple%20additional%20information%20and%20achieving%0Aprecise%20semi-supervised%20node%20classification.%20Experiments%20on%20the%20Human%20Activity%0ARecognition%2C%20sleep%20stage%20classification%20and%20University%20of%20East%20Anglia%20datasets%0Ademonstrate%20that%20our%20method%20outperforms%20current%20state-of-the-art%20methods%20in%20MTS%0Aclassification%20tasks%2C%20validating%20its%20superiority.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneous%2520Relationships%2520of%2520Subjects%2520and%2520Shapelets%2520for%250A%2520%2520Semi-supervised%2520Multivariate%2520Series%2520Classification%26entry.906535625%3DMingsen%2520Du%2520and%2520Meng%2520Chen%2520and%2520Yongjian%2520Li%2520and%2520Cun%2520Ji%2520and%2520Shoushui%2520Wei%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520%2528MTS%2529%2520classification%2520is%2520widely%2520applied%2520in%2520fields%250Asuch%2520as%2520industry%252C%2520healthcare%252C%2520and%2520finance%252C%2520aiming%2520to%2520extract%2520key%2520features%2520from%250Acomplex%2520time%2520series%2520data%2520for%2520accurate%2520decision-making%2520and%2520prediction.%2520However%252C%250Aexisting%2520methods%2520for%2520MTS%2520often%2520struggle%2520due%2520to%2520the%2520challenges%2520of%2520effectively%250Amodeling%2520high-dimensional%2520data%2520and%2520the%2520lack%2520of%2520labeled%2520data%252C%2520resulting%2520in%2520poor%250Aclassification%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520heterogeneous%250Arelationships%2520of%2520subjects%2520and%2520shapelets%2520method%2520for%2520semi-supervised%2520MTS%250Aclassification.%2520This%2520method%2520offers%2520a%2520novel%2520perspective%2520by%2520integrating%2520various%250Atypes%2520of%2520additional%2520information%2520while%2520capturing%2520the%2520relationships%2520between%2520them.%250ASpecifically%252C%2520we%2520first%2520utilize%2520a%2520contrast%2520temporal%2520self-attention%2520module%2520to%250Aobtain%2520sparse%2520MTS%2520representations%252C%2520and%2520then%2520model%2520the%2520similarities%2520between%250Athese%2520representations%2520using%2520soft%2520dynamic%2520time%2520warping%2520to%2520construct%2520a%2520similarity%250Agraph.%2520Secondly%252C%2520we%2520learn%2520the%2520shapelets%2520for%2520different%2520subject%2520types%252C%250Aincorporating%2520both%2520the%2520subject%2520features%2520and%2520their%2520shapelets%2520as%2520additional%250Ainformation%2520to%2520further%2520refine%2520the%2520similarity%2520graph%252C%2520ultimately%2520generating%2520a%250Aheterogeneous%2520graph.%2520Finally%252C%2520we%2520use%2520a%2520dual%2520level%2520graph%2520attention%2520network%2520to%250Aget%2520prediction.%2520Through%2520this%2520method%252C%2520we%2520successfully%2520transform%2520dataset%2520into%2520a%250Aheterogeneous%2520graph%252C%2520integrating%2520multiple%2520additional%2520information%2520and%2520achieving%250Aprecise%2520semi-supervised%2520node%2520classification.%2520Experiments%2520on%2520the%2520Human%2520Activity%250ARecognition%252C%2520sleep%2520stage%2520classification%2520and%2520University%2520of%2520East%2520Anglia%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520current%2520state-of-the-art%2520methods%2520in%2520MTS%250Aclassification%2520tasks%252C%2520validating%2520its%2520superiority.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneous%20Relationships%20of%20Subjects%20and%20Shapelets%20for%0A%20%20Semi-supervised%20Multivariate%20Series%20Classification&entry.906535625=Mingsen%20Du%20and%20Meng%20Chen%20and%20Yongjian%20Li%20and%20Cun%20Ji%20and%20Shoushui%20Wei&entry.1292438233=%20%20Multivariate%20time%20series%20%28MTS%29%20classification%20is%20widely%20applied%20in%20fields%0Asuch%20as%20industry%2C%20healthcare%2C%20and%20finance%2C%20aiming%20to%20extract%20key%20features%20from%0Acomplex%20time%20series%20data%20for%20accurate%20decision-making%20and%20prediction.%20However%2C%0Aexisting%20methods%20for%20MTS%20often%20struggle%20due%20to%20the%20challenges%20of%20effectively%0Amodeling%20high-dimensional%20data%20and%20the%20lack%20of%20labeled%20data%2C%20resulting%20in%20poor%0Aclassification%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20a%20heterogeneous%0Arelationships%20of%20subjects%20and%20shapelets%20method%20for%20semi-supervised%20MTS%0Aclassification.%20This%20method%20offers%20a%20novel%20perspective%20by%20integrating%20various%0Atypes%20of%20additional%20information%20while%20capturing%20the%20relationships%20between%20them.%0ASpecifically%2C%20we%20first%20utilize%20a%20contrast%20temporal%20self-attention%20module%20to%0Aobtain%20sparse%20MTS%20representations%2C%20and%20then%20model%20the%20similarities%20between%0Athese%20representations%20using%20soft%20dynamic%20time%20warping%20to%20construct%20a%20similarity%0Agraph.%20Secondly%2C%20we%20learn%20the%20shapelets%20for%20different%20subject%20types%2C%0Aincorporating%20both%20the%20subject%20features%20and%20their%20shapelets%20as%20additional%0Ainformation%20to%20further%20refine%20the%20similarity%20graph%2C%20ultimately%20generating%20a%0Aheterogeneous%20graph.%20Finally%2C%20we%20use%20a%20dual%20level%20graph%20attention%20network%20to%0Aget%20prediction.%20Through%20this%20method%2C%20we%20successfully%20transform%20dataset%20into%20a%0Aheterogeneous%20graph%2C%20integrating%20multiple%20additional%20information%20and%20achieving%0Aprecise%20semi-supervised%20node%20classification.%20Experiments%20on%20the%20Human%20Activity%0ARecognition%2C%20sleep%20stage%20classification%20and%20University%20of%20East%20Anglia%20datasets%0Ademonstrate%20that%20our%20method%20outperforms%20current%20state-of-the-art%20methods%20in%20MTS%0Aclassification%20tasks%2C%20validating%20its%20superiority.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18043v2&entry.124074799=Read"},
{"title": "Towards Explainable Indoor Localization: Interpreting Neural Network\n  Learning on Wi-Fi Fingerprints Using Logic Gates", "author": "Danish Gufran and Sudeep Pasricha", "abstract": "  Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.\n", "link": "http://arxiv.org/abs/2506.15559v1", "date": "2025-06-18", "relevancy": 2.1266, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5386}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5337}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Explainable%20Indoor%20Localization%3A%20Interpreting%20Neural%20Network%0A%20%20Learning%20on%20Wi-Fi%20Fingerprints%20Using%20Logic%20Gates&body=Title%3A%20Towards%20Explainable%20Indoor%20Localization%3A%20Interpreting%20Neural%20Network%0A%20%20Learning%20on%20Wi-Fi%20Fingerprints%20Using%20Logic%20Gates%0AAuthor%3A%20Danish%20Gufran%20and%20Sudeep%20Pasricha%0AAbstract%3A%20%20%20Indoor%20localization%20using%20deep%20learning%20%28DL%29%20has%20demonstrated%20strong%20accuracy%0Ain%20mapping%20Wi-Fi%20RSS%20fingerprints%20to%20physical%20locations%3B%20however%2C%20most%20existing%0ADL%20frameworks%20function%20as%20black-box%20models%2C%20offering%20limited%20insight%20into%20how%0Apredictions%20are%20made%20or%20how%20models%20respond%20to%20real-world%20noise%20over%20time.%20This%0Alack%20of%20interpretability%20hampers%20our%20ability%20to%20understand%20the%20impact%20of%0Atemporal%20variations%20-%20caused%20by%20environmental%20dynamics%20-%20and%20to%20adapt%20models%0Afor%20long-term%20reliability.%20To%20address%20this%2C%20we%20introduce%20LogNet%2C%20a%20novel%20logic%0Agate-based%20framework%20designed%20to%20interpret%20and%20enhance%20DL-based%20indoor%0Alocalization.%20LogNet%20enables%20transparent%20reasoning%20by%20identifying%20which%20access%0Apoints%20%28APs%29%20are%20most%20influential%20for%20each%20reference%20point%20%28RP%29%20and%20reveals%20how%0Aenvironmental%20noise%20disrupts%20DL-driven%20localization%20decisions.%20This%0Ainterpretability%20allows%20us%20to%20trace%20and%20diagnose%20model%20failures%20and%20adapt%20DL%0Asystems%20for%20more%20stable%20long-term%20deployments.%20Evaluations%20across%20multiple%0Areal-world%20building%20floorplans%20and%20over%20two%20years%20of%20temporal%20variation%20show%0Athat%20LogNet%20not%20only%20interprets%20the%20internal%20behavior%20of%20DL%20models%20but%20also%0Aimproves%20performance-achieving%20up%20to%201.1x%20to%202.8x%20lower%20localization%20error%2C%0A3.4x%20to%2043.3x%20smaller%20model%20size%2C%20and%201.5x%20to%203.6x%20lower%20latency%20compared%20to%0Aprior%20DL-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Explainable%2520Indoor%2520Localization%253A%2520Interpreting%2520Neural%2520Network%250A%2520%2520Learning%2520on%2520Wi-Fi%2520Fingerprints%2520Using%2520Logic%2520Gates%26entry.906535625%3DDanish%2520Gufran%2520and%2520Sudeep%2520Pasricha%26entry.1292438233%3D%2520%2520Indoor%2520localization%2520using%2520deep%2520learning%2520%2528DL%2529%2520has%2520demonstrated%2520strong%2520accuracy%250Ain%2520mapping%2520Wi-Fi%2520RSS%2520fingerprints%2520to%2520physical%2520locations%253B%2520however%252C%2520most%2520existing%250ADL%2520frameworks%2520function%2520as%2520black-box%2520models%252C%2520offering%2520limited%2520insight%2520into%2520how%250Apredictions%2520are%2520made%2520or%2520how%2520models%2520respond%2520to%2520real-world%2520noise%2520over%2520time.%2520This%250Alack%2520of%2520interpretability%2520hampers%2520our%2520ability%2520to%2520understand%2520the%2520impact%2520of%250Atemporal%2520variations%2520-%2520caused%2520by%2520environmental%2520dynamics%2520-%2520and%2520to%2520adapt%2520models%250Afor%2520long-term%2520reliability.%2520To%2520address%2520this%252C%2520we%2520introduce%2520LogNet%252C%2520a%2520novel%2520logic%250Agate-based%2520framework%2520designed%2520to%2520interpret%2520and%2520enhance%2520DL-based%2520indoor%250Alocalization.%2520LogNet%2520enables%2520transparent%2520reasoning%2520by%2520identifying%2520which%2520access%250Apoints%2520%2528APs%2529%2520are%2520most%2520influential%2520for%2520each%2520reference%2520point%2520%2528RP%2529%2520and%2520reveals%2520how%250Aenvironmental%2520noise%2520disrupts%2520DL-driven%2520localization%2520decisions.%2520This%250Ainterpretability%2520allows%2520us%2520to%2520trace%2520and%2520diagnose%2520model%2520failures%2520and%2520adapt%2520DL%250Asystems%2520for%2520more%2520stable%2520long-term%2520deployments.%2520Evaluations%2520across%2520multiple%250Areal-world%2520building%2520floorplans%2520and%2520over%2520two%2520years%2520of%2520temporal%2520variation%2520show%250Athat%2520LogNet%2520not%2520only%2520interprets%2520the%2520internal%2520behavior%2520of%2520DL%2520models%2520but%2520also%250Aimproves%2520performance-achieving%2520up%2520to%25201.1x%2520to%25202.8x%2520lower%2520localization%2520error%252C%250A3.4x%2520to%252043.3x%2520smaller%2520model%2520size%252C%2520and%25201.5x%2520to%25203.6x%2520lower%2520latency%2520compared%2520to%250Aprior%2520DL-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Explainable%20Indoor%20Localization%3A%20Interpreting%20Neural%20Network%0A%20%20Learning%20on%20Wi-Fi%20Fingerprints%20Using%20Logic%20Gates&entry.906535625=Danish%20Gufran%20and%20Sudeep%20Pasricha&entry.1292438233=%20%20Indoor%20localization%20using%20deep%20learning%20%28DL%29%20has%20demonstrated%20strong%20accuracy%0Ain%20mapping%20Wi-Fi%20RSS%20fingerprints%20to%20physical%20locations%3B%20however%2C%20most%20existing%0ADL%20frameworks%20function%20as%20black-box%20models%2C%20offering%20limited%20insight%20into%20how%0Apredictions%20are%20made%20or%20how%20models%20respond%20to%20real-world%20noise%20over%20time.%20This%0Alack%20of%20interpretability%20hampers%20our%20ability%20to%20understand%20the%20impact%20of%0Atemporal%20variations%20-%20caused%20by%20environmental%20dynamics%20-%20and%20to%20adapt%20models%0Afor%20long-term%20reliability.%20To%20address%20this%2C%20we%20introduce%20LogNet%2C%20a%20novel%20logic%0Agate-based%20framework%20designed%20to%20interpret%20and%20enhance%20DL-based%20indoor%0Alocalization.%20LogNet%20enables%20transparent%20reasoning%20by%20identifying%20which%20access%0Apoints%20%28APs%29%20are%20most%20influential%20for%20each%20reference%20point%20%28RP%29%20and%20reveals%20how%0Aenvironmental%20noise%20disrupts%20DL-driven%20localization%20decisions.%20This%0Ainterpretability%20allows%20us%20to%20trace%20and%20diagnose%20model%20failures%20and%20adapt%20DL%0Asystems%20for%20more%20stable%20long-term%20deployments.%20Evaluations%20across%20multiple%0Areal-world%20building%20floorplans%20and%20over%20two%20years%20of%20temporal%20variation%20show%0Athat%20LogNet%20not%20only%20interprets%20the%20internal%20behavior%20of%20DL%20models%20but%20also%0Aimproves%20performance-achieving%20up%20to%201.1x%20to%202.8x%20lower%20localization%20error%2C%0A3.4x%20to%2043.3x%20smaller%20model%20size%2C%20and%201.5x%20to%203.6x%20lower%20latency%20compared%20to%0Aprior%20DL-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15559v1&entry.124074799=Read"},
{"title": "Exploring and Exploiting the Inherent Efficiency within Large Reasoning\n  Models for Self-Guided Efficiency Enhancement", "author": "Weixiang Zhao and Jiahe Guo and Yang Deng and Xingyu Sui and Yulin Hu and Yanyan Zhao and Wanxiang Che and Bing Qin and Tat-Seng Chua and Ting Liu", "abstract": "  Recent advancements in large reasoning models (LRMs) have significantly\nenhanced language models' capabilities in complex problem-solving by emulating\nhuman-like deliberative thinking. However, these models often exhibit\noverthinking (i.e., the generation of unnecessarily verbose and redundant\ncontent), which hinders efficiency and inflates inference cost. In this work,\nwe explore the representational and behavioral origins of this inefficiency,\nrevealing that LRMs inherently possess the capacity for more concise reasoning.\nEmpirical analyses show that correct reasoning paths vary significantly in\nlength, and the shortest correct responses often suffice, indicating untapped\nefficiency potential. Exploiting these findings, we propose two lightweight\nmethods to enhance LRM efficiency. First, we introduce Efficiency Steering, a\ntraining-free activation steering technique that modulates reasoning behavior\nvia a single direction in the model's representation space. Second, we develop\nSelf-Rewarded Efficiency RL, a reinforcement learning framework that\ndynamically balances task accuracy and brevity by rewarding concise correct\nsolutions. Extensive experiments on seven LRM backbones across multiple\nmathematical reasoning benchmarks demonstrate that our methods significantly\nreduce reasoning length while preserving or improving task performance. Our\nresults highlight that reasoning efficiency can be improved by leveraging and\nguiding the intrinsic capabilities of existing models in a self-guided manner.\n", "link": "http://arxiv.org/abs/2506.15647v1", "date": "2025-06-18", "relevancy": 2.1254, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20and%20Exploiting%20the%20Inherent%20Efficiency%20within%20Large%20Reasoning%0A%20%20Models%20for%20Self-Guided%20Efficiency%20Enhancement&body=Title%3A%20Exploring%20and%20Exploiting%20the%20Inherent%20Efficiency%20within%20Large%20Reasoning%0A%20%20Models%20for%20Self-Guided%20Efficiency%20Enhancement%0AAuthor%3A%20Weixiang%20Zhao%20and%20Jiahe%20Guo%20and%20Yang%20Deng%20and%20Xingyu%20Sui%20and%20Yulin%20Hu%20and%20Yanyan%20Zhao%20and%20Wanxiang%20Che%20and%20Bing%20Qin%20and%20Tat-Seng%20Chua%20and%20Ting%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20reasoning%20models%20%28LRMs%29%20have%20significantly%0Aenhanced%20language%20models%27%20capabilities%20in%20complex%20problem-solving%20by%20emulating%0Ahuman-like%20deliberative%20thinking.%20However%2C%20these%20models%20often%20exhibit%0Aoverthinking%20%28i.e.%2C%20the%20generation%20of%20unnecessarily%20verbose%20and%20redundant%0Acontent%29%2C%20which%20hinders%20efficiency%20and%20inflates%20inference%20cost.%20In%20this%20work%2C%0Awe%20explore%20the%20representational%20and%20behavioral%20origins%20of%20this%20inefficiency%2C%0Arevealing%20that%20LRMs%20inherently%20possess%20the%20capacity%20for%20more%20concise%20reasoning.%0AEmpirical%20analyses%20show%20that%20correct%20reasoning%20paths%20vary%20significantly%20in%0Alength%2C%20and%20the%20shortest%20correct%20responses%20often%20suffice%2C%20indicating%20untapped%0Aefficiency%20potential.%20Exploiting%20these%20findings%2C%20we%20propose%20two%20lightweight%0Amethods%20to%20enhance%20LRM%20efficiency.%20First%2C%20we%20introduce%20Efficiency%20Steering%2C%20a%0Atraining-free%20activation%20steering%20technique%20that%20modulates%20reasoning%20behavior%0Avia%20a%20single%20direction%20in%20the%20model%27s%20representation%20space.%20Second%2C%20we%20develop%0ASelf-Rewarded%20Efficiency%20RL%2C%20a%20reinforcement%20learning%20framework%20that%0Adynamically%20balances%20task%20accuracy%20and%20brevity%20by%20rewarding%20concise%20correct%0Asolutions.%20Extensive%20experiments%20on%20seven%20LRM%20backbones%20across%20multiple%0Amathematical%20reasoning%20benchmarks%20demonstrate%20that%20our%20methods%20significantly%0Areduce%20reasoning%20length%20while%20preserving%20or%20improving%20task%20performance.%20Our%0Aresults%20highlight%20that%20reasoning%20efficiency%20can%20be%20improved%20by%20leveraging%20and%0Aguiding%20the%20intrinsic%20capabilities%20of%20existing%20models%20in%20a%20self-guided%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520and%2520Exploiting%2520the%2520Inherent%2520Efficiency%2520within%2520Large%2520Reasoning%250A%2520%2520Models%2520for%2520Self-Guided%2520Efficiency%2520Enhancement%26entry.906535625%3DWeixiang%2520Zhao%2520and%2520Jiahe%2520Guo%2520and%2520Yang%2520Deng%2520and%2520Xingyu%2520Sui%2520and%2520Yulin%2520Hu%2520and%2520Yanyan%2520Zhao%2520and%2520Wanxiang%2520Che%2520and%2520Bing%2520Qin%2520and%2520Tat-Seng%2520Chua%2520and%2520Ting%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520reasoning%2520models%2520%2528LRMs%2529%2520have%2520significantly%250Aenhanced%2520language%2520models%2527%2520capabilities%2520in%2520complex%2520problem-solving%2520by%2520emulating%250Ahuman-like%2520deliberative%2520thinking.%2520However%252C%2520these%2520models%2520often%2520exhibit%250Aoverthinking%2520%2528i.e.%252C%2520the%2520generation%2520of%2520unnecessarily%2520verbose%2520and%2520redundant%250Acontent%2529%252C%2520which%2520hinders%2520efficiency%2520and%2520inflates%2520inference%2520cost.%2520In%2520this%2520work%252C%250Awe%2520explore%2520the%2520representational%2520and%2520behavioral%2520origins%2520of%2520this%2520inefficiency%252C%250Arevealing%2520that%2520LRMs%2520inherently%2520possess%2520the%2520capacity%2520for%2520more%2520concise%2520reasoning.%250AEmpirical%2520analyses%2520show%2520that%2520correct%2520reasoning%2520paths%2520vary%2520significantly%2520in%250Alength%252C%2520and%2520the%2520shortest%2520correct%2520responses%2520often%2520suffice%252C%2520indicating%2520untapped%250Aefficiency%2520potential.%2520Exploiting%2520these%2520findings%252C%2520we%2520propose%2520two%2520lightweight%250Amethods%2520to%2520enhance%2520LRM%2520efficiency.%2520First%252C%2520we%2520introduce%2520Efficiency%2520Steering%252C%2520a%250Atraining-free%2520activation%2520steering%2520technique%2520that%2520modulates%2520reasoning%2520behavior%250Avia%2520a%2520single%2520direction%2520in%2520the%2520model%2527s%2520representation%2520space.%2520Second%252C%2520we%2520develop%250ASelf-Rewarded%2520Efficiency%2520RL%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%250Adynamically%2520balances%2520task%2520accuracy%2520and%2520brevity%2520by%2520rewarding%2520concise%2520correct%250Asolutions.%2520Extensive%2520experiments%2520on%2520seven%2520LRM%2520backbones%2520across%2520multiple%250Amathematical%2520reasoning%2520benchmarks%2520demonstrate%2520that%2520our%2520methods%2520significantly%250Areduce%2520reasoning%2520length%2520while%2520preserving%2520or%2520improving%2520task%2520performance.%2520Our%250Aresults%2520highlight%2520that%2520reasoning%2520efficiency%2520can%2520be%2520improved%2520by%2520leveraging%2520and%250Aguiding%2520the%2520intrinsic%2520capabilities%2520of%2520existing%2520models%2520in%2520a%2520self-guided%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20and%20Exploiting%20the%20Inherent%20Efficiency%20within%20Large%20Reasoning%0A%20%20Models%20for%20Self-Guided%20Efficiency%20Enhancement&entry.906535625=Weixiang%20Zhao%20and%20Jiahe%20Guo%20and%20Yang%20Deng%20and%20Xingyu%20Sui%20and%20Yulin%20Hu%20and%20Yanyan%20Zhao%20and%20Wanxiang%20Che%20and%20Bing%20Qin%20and%20Tat-Seng%20Chua%20and%20Ting%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20large%20reasoning%20models%20%28LRMs%29%20have%20significantly%0Aenhanced%20language%20models%27%20capabilities%20in%20complex%20problem-solving%20by%20emulating%0Ahuman-like%20deliberative%20thinking.%20However%2C%20these%20models%20often%20exhibit%0Aoverthinking%20%28i.e.%2C%20the%20generation%20of%20unnecessarily%20verbose%20and%20redundant%0Acontent%29%2C%20which%20hinders%20efficiency%20and%20inflates%20inference%20cost.%20In%20this%20work%2C%0Awe%20explore%20the%20representational%20and%20behavioral%20origins%20of%20this%20inefficiency%2C%0Arevealing%20that%20LRMs%20inherently%20possess%20the%20capacity%20for%20more%20concise%20reasoning.%0AEmpirical%20analyses%20show%20that%20correct%20reasoning%20paths%20vary%20significantly%20in%0Alength%2C%20and%20the%20shortest%20correct%20responses%20often%20suffice%2C%20indicating%20untapped%0Aefficiency%20potential.%20Exploiting%20these%20findings%2C%20we%20propose%20two%20lightweight%0Amethods%20to%20enhance%20LRM%20efficiency.%20First%2C%20we%20introduce%20Efficiency%20Steering%2C%20a%0Atraining-free%20activation%20steering%20technique%20that%20modulates%20reasoning%20behavior%0Avia%20a%20single%20direction%20in%20the%20model%27s%20representation%20space.%20Second%2C%20we%20develop%0ASelf-Rewarded%20Efficiency%20RL%2C%20a%20reinforcement%20learning%20framework%20that%0Adynamically%20balances%20task%20accuracy%20and%20brevity%20by%20rewarding%20concise%20correct%0Asolutions.%20Extensive%20experiments%20on%20seven%20LRM%20backbones%20across%20multiple%0Amathematical%20reasoning%20benchmarks%20demonstrate%20that%20our%20methods%20significantly%0Areduce%20reasoning%20length%20while%20preserving%20or%20improving%20task%20performance.%20Our%0Aresults%20highlight%20that%20reasoning%20efficiency%20can%20be%20improved%20by%20leveraging%20and%0Aguiding%20the%20intrinsic%20capabilities%20of%20existing%20models%20in%20a%20self-guided%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15647v1&entry.124074799=Read"},
{"title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts", "author": "Negar Foroutan and Angelika Romanou and Matin Ansaripour and Julian Martin Eisenschlos and Karl Aberer and R\u00e9mi Lebret", "abstract": "  Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.\n", "link": "http://arxiv.org/abs/2506.15594v1", "date": "2025-06-18", "relevancy": 2.1185, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WikiMixQA%3A%20A%20Multimodal%20Benchmark%20for%20Question%20Answering%20over%20Tables%20and%0A%20%20Charts&body=Title%3A%20WikiMixQA%3A%20A%20Multimodal%20Benchmark%20for%20Question%20Answering%20over%20Tables%20and%0A%20%20Charts%0AAuthor%3A%20Negar%20Foroutan%20and%20Angelika%20Romanou%20and%20Matin%20Ansaripour%20and%20Julian%20Martin%20Eisenschlos%20and%20Karl%20Aberer%20and%20R%C3%A9mi%20Lebret%0AAbstract%3A%20%20%20Documents%20are%20fundamental%20to%20preserving%20and%20disseminating%20information%2C%20often%0Aincorporating%20complex%20layouts%2C%20tables%2C%20and%20charts%20that%20pose%20significant%0Achallenges%20for%20automatic%20document%20understanding%20%28DU%29.%20While%20vision-language%0Alarge%20models%20%28VLLMs%29%20have%20demonstrated%20improvements%20across%20various%20tasks%2C%20their%0Aeffectiveness%20in%20processing%20long-context%20vision%20inputs%20remains%20unclear.%20This%0Apaper%20introduces%20WikiMixQA%2C%20a%20benchmark%20comprising%201%2C000%20multiple-choice%0Aquestions%20%28MCQs%29%20designed%20to%20evaluate%20cross-modal%20reasoning%20over%20tables%20and%0Acharts%20extracted%20from%204%2C000%20Wikipedia%20pages%20spanning%20seven%20distinct%20topics.%0AUnlike%20existing%20benchmarks%2C%20WikiMixQA%20emphasizes%20complex%20reasoning%20by%20requiring%0Amodels%20to%20synthesize%20information%20from%20multiple%20modalities.%20We%20evaluate%2012%0Astate-of-the-art%20vision-language%20models%2C%20revealing%20that%20while%20proprietary%0Amodels%20achieve%20~70%25%20accuracy%20when%20provided%20with%20direct%20context%2C%20their%0Aperformance%20deteriorates%20significantly%20when%20retrieval%20from%20long%20documents%20is%0Arequired.%20Among%20these%2C%20GPT-4-o%20is%20the%20only%20model%20exceeding%2050%25%20accuracy%20in%20this%0Asetting%2C%20whereas%20open-source%20models%20perform%20considerably%20worse%2C%20with%20a%20maximum%0Aaccuracy%20of%2027%25.%20These%20findings%20underscore%20the%20challenges%20of%20long-context%2C%0Amulti-modal%20reasoning%20and%20establish%20WikiMixQA%20as%20a%20crucial%20benchmark%20for%0Aadvancing%20document%20understanding%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWikiMixQA%253A%2520A%2520Multimodal%2520Benchmark%2520for%2520Question%2520Answering%2520over%2520Tables%2520and%250A%2520%2520Charts%26entry.906535625%3DNegar%2520Foroutan%2520and%2520Angelika%2520Romanou%2520and%2520Matin%2520Ansaripour%2520and%2520Julian%2520Martin%2520Eisenschlos%2520and%2520Karl%2520Aberer%2520and%2520R%25C3%25A9mi%2520Lebret%26entry.1292438233%3D%2520%2520Documents%2520are%2520fundamental%2520to%2520preserving%2520and%2520disseminating%2520information%252C%2520often%250Aincorporating%2520complex%2520layouts%252C%2520tables%252C%2520and%2520charts%2520that%2520pose%2520significant%250Achallenges%2520for%2520automatic%2520document%2520understanding%2520%2528DU%2529.%2520While%2520vision-language%250Alarge%2520models%2520%2528VLLMs%2529%2520have%2520demonstrated%2520improvements%2520across%2520various%2520tasks%252C%2520their%250Aeffectiveness%2520in%2520processing%2520long-context%2520vision%2520inputs%2520remains%2520unclear.%2520This%250Apaper%2520introduces%2520WikiMixQA%252C%2520a%2520benchmark%2520comprising%25201%252C000%2520multiple-choice%250Aquestions%2520%2528MCQs%2529%2520designed%2520to%2520evaluate%2520cross-modal%2520reasoning%2520over%2520tables%2520and%250Acharts%2520extracted%2520from%25204%252C000%2520Wikipedia%2520pages%2520spanning%2520seven%2520distinct%2520topics.%250AUnlike%2520existing%2520benchmarks%252C%2520WikiMixQA%2520emphasizes%2520complex%2520reasoning%2520by%2520requiring%250Amodels%2520to%2520synthesize%2520information%2520from%2520multiple%2520modalities.%2520We%2520evaluate%252012%250Astate-of-the-art%2520vision-language%2520models%252C%2520revealing%2520that%2520while%2520proprietary%250Amodels%2520achieve%2520~70%2525%2520accuracy%2520when%2520provided%2520with%2520direct%2520context%252C%2520their%250Aperformance%2520deteriorates%2520significantly%2520when%2520retrieval%2520from%2520long%2520documents%2520is%250Arequired.%2520Among%2520these%252C%2520GPT-4-o%2520is%2520the%2520only%2520model%2520exceeding%252050%2525%2520accuracy%2520in%2520this%250Asetting%252C%2520whereas%2520open-source%2520models%2520perform%2520considerably%2520worse%252C%2520with%2520a%2520maximum%250Aaccuracy%2520of%252027%2525.%2520These%2520findings%2520underscore%2520the%2520challenges%2520of%2520long-context%252C%250Amulti-modal%2520reasoning%2520and%2520establish%2520WikiMixQA%2520as%2520a%2520crucial%2520benchmark%2520for%250Aadvancing%2520document%2520understanding%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WikiMixQA%3A%20A%20Multimodal%20Benchmark%20for%20Question%20Answering%20over%20Tables%20and%0A%20%20Charts&entry.906535625=Negar%20Foroutan%20and%20Angelika%20Romanou%20and%20Matin%20Ansaripour%20and%20Julian%20Martin%20Eisenschlos%20and%20Karl%20Aberer%20and%20R%C3%A9mi%20Lebret&entry.1292438233=%20%20Documents%20are%20fundamental%20to%20preserving%20and%20disseminating%20information%2C%20often%0Aincorporating%20complex%20layouts%2C%20tables%2C%20and%20charts%20that%20pose%20significant%0Achallenges%20for%20automatic%20document%20understanding%20%28DU%29.%20While%20vision-language%0Alarge%20models%20%28VLLMs%29%20have%20demonstrated%20improvements%20across%20various%20tasks%2C%20their%0Aeffectiveness%20in%20processing%20long-context%20vision%20inputs%20remains%20unclear.%20This%0Apaper%20introduces%20WikiMixQA%2C%20a%20benchmark%20comprising%201%2C000%20multiple-choice%0Aquestions%20%28MCQs%29%20designed%20to%20evaluate%20cross-modal%20reasoning%20over%20tables%20and%0Acharts%20extracted%20from%204%2C000%20Wikipedia%20pages%20spanning%20seven%20distinct%20topics.%0AUnlike%20existing%20benchmarks%2C%20WikiMixQA%20emphasizes%20complex%20reasoning%20by%20requiring%0Amodels%20to%20synthesize%20information%20from%20multiple%20modalities.%20We%20evaluate%2012%0Astate-of-the-art%20vision-language%20models%2C%20revealing%20that%20while%20proprietary%0Amodels%20achieve%20~70%25%20accuracy%20when%20provided%20with%20direct%20context%2C%20their%0Aperformance%20deteriorates%20significantly%20when%20retrieval%20from%20long%20documents%20is%0Arequired.%20Among%20these%2C%20GPT-4-o%20is%20the%20only%20model%20exceeding%2050%25%20accuracy%20in%20this%0Asetting%2C%20whereas%20open-source%20models%20perform%20considerably%20worse%2C%20with%20a%20maximum%0Aaccuracy%20of%2027%25.%20These%20findings%20underscore%20the%20challenges%20of%20long-context%2C%0Amulti-modal%20reasoning%20and%20establish%20WikiMixQA%20as%20a%20crucial%20benchmark%20for%0Aadvancing%20document%20understanding%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15594v1&entry.124074799=Read"},
{"title": "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object\n  Detection Framework", "author": "Dahang Wan and Rongsheng Lu and Yang Fang and Xianli Lang and Shuangbao Shu and Jingjing Chen and Siyuan Shen and Ting Xu and Zecong Ye", "abstract": "  Multispectral object detection, which integrates information from multiple\nbands, can enhance detection accuracy and environmental adaptability, holding\ngreat application potential across various fields. Although existing methods\nhave made progress in cross-modal interaction, low-light conditions, and model\nlightweight, there are still challenges like the lack of a unified single-stage\nframework, difficulty in balancing performance and fusion strategy, and\nunreasonable modality weight allocation. To address these, based on the YOLOv11\nframework, we present YOLOv11-RGBT, a new comprehensive multimodal object\ndetection framework. We designed six multispectral fusion modes and\nsuccessfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After\nreevaluating the importance of the two modalities, we proposed a P3 mid-fusion\nstrategy and multispectral controllable fine-tuning (MCF) strategy for\nmultispectral models. These improvements optimize feature fusion, reduce\nredundancy and mismatches, and boost overall model performance. Experiments\nshow our framework excels on three major open-source multispectral object\ndetection datasets, like LLVIP and FLIR. Particularly, the multispectral\ncontrollable fine-tuning strategy significantly enhanced model adaptability and\nrobustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP\nby 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and\nstrategies' effectiveness. The code is available at:\nhttps://github.com/wandahangFY/YOLOv11-RGBT.\n", "link": "http://arxiv.org/abs/2506.14696v2", "date": "2025-06-18", "relevancy": 2.1176, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.532}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLOv11-RGBT%3A%20Towards%20a%20Comprehensive%20Single-Stage%20Multispectral%20Object%0A%20%20Detection%20Framework&body=Title%3A%20YOLOv11-RGBT%3A%20Towards%20a%20Comprehensive%20Single-Stage%20Multispectral%20Object%0A%20%20Detection%20Framework%0AAuthor%3A%20Dahang%20Wan%20and%20Rongsheng%20Lu%20and%20Yang%20Fang%20and%20Xianli%20Lang%20and%20Shuangbao%20Shu%20and%20Jingjing%20Chen%20and%20Siyuan%20Shen%20and%20Ting%20Xu%20and%20Zecong%20Ye%0AAbstract%3A%20%20%20Multispectral%20object%20detection%2C%20which%20integrates%20information%20from%20multiple%0Abands%2C%20can%20enhance%20detection%20accuracy%20and%20environmental%20adaptability%2C%20holding%0Agreat%20application%20potential%20across%20various%20fields.%20Although%20existing%20methods%0Ahave%20made%20progress%20in%20cross-modal%20interaction%2C%20low-light%20conditions%2C%20and%20model%0Alightweight%2C%20there%20are%20still%20challenges%20like%20the%20lack%20of%20a%20unified%20single-stage%0Aframework%2C%20difficulty%20in%20balancing%20performance%20and%20fusion%20strategy%2C%20and%0Aunreasonable%20modality%20weight%20allocation.%20To%20address%20these%2C%20based%20on%20the%20YOLOv11%0Aframework%2C%20we%20present%20YOLOv11-RGBT%2C%20a%20new%20comprehensive%20multimodal%20object%0Adetection%20framework.%20We%20designed%20six%20multispectral%20fusion%20modes%20and%0Asuccessfully%20applied%20them%20to%20models%20from%20YOLOv3%20to%20YOLOv12%20and%20RT-DETR.%20After%0Areevaluating%20the%20importance%20of%20the%20two%20modalities%2C%20we%20proposed%20a%20P3%20mid-fusion%0Astrategy%20and%20multispectral%20controllable%20fine-tuning%20%28MCF%29%20strategy%20for%0Amultispectral%20models.%20These%20improvements%20optimize%20feature%20fusion%2C%20reduce%0Aredundancy%20and%20mismatches%2C%20and%20boost%20overall%20model%20performance.%20Experiments%0Ashow%20our%20framework%20excels%20on%20three%20major%20open-source%20multispectral%20object%0Adetection%20datasets%2C%20like%20LLVIP%20and%20FLIR.%20Particularly%2C%20the%20multispectral%0Acontrollable%20fine-tuning%20strategy%20significantly%20enhanced%20model%20adaptability%20and%0Arobustness.%20On%20the%20FLIR%20dataset%2C%20it%20consistently%20improved%20YOLOv11%20models%27%20mAP%0Aby%203.41%25-5.65%25%2C%20reaching%20a%20maximum%20of%2047.61%25%2C%20verifying%20the%20framework%20and%0Astrategies%27%20effectiveness.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/wandahangFY/YOLOv11-RGBT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14696v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLOv11-RGBT%253A%2520Towards%2520a%2520Comprehensive%2520Single-Stage%2520Multispectral%2520Object%250A%2520%2520Detection%2520Framework%26entry.906535625%3DDahang%2520Wan%2520and%2520Rongsheng%2520Lu%2520and%2520Yang%2520Fang%2520and%2520Xianli%2520Lang%2520and%2520Shuangbao%2520Shu%2520and%2520Jingjing%2520Chen%2520and%2520Siyuan%2520Shen%2520and%2520Ting%2520Xu%2520and%2520Zecong%2520Ye%26entry.1292438233%3D%2520%2520Multispectral%2520object%2520detection%252C%2520which%2520integrates%2520information%2520from%2520multiple%250Abands%252C%2520can%2520enhance%2520detection%2520accuracy%2520and%2520environmental%2520adaptability%252C%2520holding%250Agreat%2520application%2520potential%2520across%2520various%2520fields.%2520Although%2520existing%2520methods%250Ahave%2520made%2520progress%2520in%2520cross-modal%2520interaction%252C%2520low-light%2520conditions%252C%2520and%2520model%250Alightweight%252C%2520there%2520are%2520still%2520challenges%2520like%2520the%2520lack%2520of%2520a%2520unified%2520single-stage%250Aframework%252C%2520difficulty%2520in%2520balancing%2520performance%2520and%2520fusion%2520strategy%252C%2520and%250Aunreasonable%2520modality%2520weight%2520allocation.%2520To%2520address%2520these%252C%2520based%2520on%2520the%2520YOLOv11%250Aframework%252C%2520we%2520present%2520YOLOv11-RGBT%252C%2520a%2520new%2520comprehensive%2520multimodal%2520object%250Adetection%2520framework.%2520We%2520designed%2520six%2520multispectral%2520fusion%2520modes%2520and%250Asuccessfully%2520applied%2520them%2520to%2520models%2520from%2520YOLOv3%2520to%2520YOLOv12%2520and%2520RT-DETR.%2520After%250Areevaluating%2520the%2520importance%2520of%2520the%2520two%2520modalities%252C%2520we%2520proposed%2520a%2520P3%2520mid-fusion%250Astrategy%2520and%2520multispectral%2520controllable%2520fine-tuning%2520%2528MCF%2529%2520strategy%2520for%250Amultispectral%2520models.%2520These%2520improvements%2520optimize%2520feature%2520fusion%252C%2520reduce%250Aredundancy%2520and%2520mismatches%252C%2520and%2520boost%2520overall%2520model%2520performance.%2520Experiments%250Ashow%2520our%2520framework%2520excels%2520on%2520three%2520major%2520open-source%2520multispectral%2520object%250Adetection%2520datasets%252C%2520like%2520LLVIP%2520and%2520FLIR.%2520Particularly%252C%2520the%2520multispectral%250Acontrollable%2520fine-tuning%2520strategy%2520significantly%2520enhanced%2520model%2520adaptability%2520and%250Arobustness.%2520On%2520the%2520FLIR%2520dataset%252C%2520it%2520consistently%2520improved%2520YOLOv11%2520models%2527%2520mAP%250Aby%25203.41%2525-5.65%2525%252C%2520reaching%2520a%2520maximum%2520of%252047.61%2525%252C%2520verifying%2520the%2520framework%2520and%250Astrategies%2527%2520effectiveness.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/wandahangFY/YOLOv11-RGBT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14696v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLOv11-RGBT%3A%20Towards%20a%20Comprehensive%20Single-Stage%20Multispectral%20Object%0A%20%20Detection%20Framework&entry.906535625=Dahang%20Wan%20and%20Rongsheng%20Lu%20and%20Yang%20Fang%20and%20Xianli%20Lang%20and%20Shuangbao%20Shu%20and%20Jingjing%20Chen%20and%20Siyuan%20Shen%20and%20Ting%20Xu%20and%20Zecong%20Ye&entry.1292438233=%20%20Multispectral%20object%20detection%2C%20which%20integrates%20information%20from%20multiple%0Abands%2C%20can%20enhance%20detection%20accuracy%20and%20environmental%20adaptability%2C%20holding%0Agreat%20application%20potential%20across%20various%20fields.%20Although%20existing%20methods%0Ahave%20made%20progress%20in%20cross-modal%20interaction%2C%20low-light%20conditions%2C%20and%20model%0Alightweight%2C%20there%20are%20still%20challenges%20like%20the%20lack%20of%20a%20unified%20single-stage%0Aframework%2C%20difficulty%20in%20balancing%20performance%20and%20fusion%20strategy%2C%20and%0Aunreasonable%20modality%20weight%20allocation.%20To%20address%20these%2C%20based%20on%20the%20YOLOv11%0Aframework%2C%20we%20present%20YOLOv11-RGBT%2C%20a%20new%20comprehensive%20multimodal%20object%0Adetection%20framework.%20We%20designed%20six%20multispectral%20fusion%20modes%20and%0Asuccessfully%20applied%20them%20to%20models%20from%20YOLOv3%20to%20YOLOv12%20and%20RT-DETR.%20After%0Areevaluating%20the%20importance%20of%20the%20two%20modalities%2C%20we%20proposed%20a%20P3%20mid-fusion%0Astrategy%20and%20multispectral%20controllable%20fine-tuning%20%28MCF%29%20strategy%20for%0Amultispectral%20models.%20These%20improvements%20optimize%20feature%20fusion%2C%20reduce%0Aredundancy%20and%20mismatches%2C%20and%20boost%20overall%20model%20performance.%20Experiments%0Ashow%20our%20framework%20excels%20on%20three%20major%20open-source%20multispectral%20object%0Adetection%20datasets%2C%20like%20LLVIP%20and%20FLIR.%20Particularly%2C%20the%20multispectral%0Acontrollable%20fine-tuning%20strategy%20significantly%20enhanced%20model%20adaptability%20and%0Arobustness.%20On%20the%20FLIR%20dataset%2C%20it%20consistently%20improved%20YOLOv11%20models%27%20mAP%0Aby%203.41%25-5.65%25%2C%20reaching%20a%20maximum%20of%2047.61%25%2C%20verifying%20the%20framework%20and%0Astrategies%27%20effectiveness.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/wandahangFY/YOLOv11-RGBT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14696v2&entry.124074799=Read"},
{"title": "SFDLA: Source-Free Document Layout Analysis", "author": "Sebastian Tewes and Yufan Chen and Omar Moured and Jiaming Zhang and Rainer Stiefelhagen", "abstract": "  Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps://github.com/s3setewe/sfdla-DLAdapter.\n", "link": "http://arxiv.org/abs/2503.18742v2", "date": "2025-06-18", "relevancy": 2.1108, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5583}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5155}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFDLA%3A%20Source-Free%20Document%20Layout%20Analysis&body=Title%3A%20SFDLA%3A%20Source-Free%20Document%20Layout%20Analysis%0AAuthor%3A%20Sebastian%20Tewes%20and%20Yufan%20Chen%20and%20Omar%20Moured%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20Document%20Layout%20Analysis%20%28DLA%29%20is%20a%20fundamental%20task%20in%20document%0Aunderstanding.%20However%2C%20existing%20DLA%20and%20adaptation%20methods%20often%20require%0Aaccess%20to%20large-scale%20source%20data%20and%20target%20labels.%20This%20requirements%20severely%0Alimiting%20their%20real-world%20applicability%2C%20particularly%20in%20privacy-sensitive%20and%0Aresource-constrained%20domains%2C%20such%20as%20financial%20statements%2C%20medical%20records%2C%0Aand%20proprietary%20business%20documents.%20According%20to%20our%20observation%2C%20directly%0Atransferring%20source-domain%20fine-tuned%20models%20on%20target%20domains%20often%20results%20in%0Aa%20significant%20performance%20drop%20%28Avg.%20-32.64%25%29.%20In%20this%20work%2C%20we%20introduce%0ASource-Free%20Document%20Layout%20Analysis%20%28SFDLA%29%2C%20aiming%20for%20adapting%20a%20pre-trained%0Asource%20DLA%20models%20to%20an%20unlabeled%20target%20domain%2C%20without%20access%20to%20any%20source%0Adata.%20To%20address%20this%20challenge%2C%20we%20establish%20the%20first%20SFDLA%20benchmark%2C%0Acovering%20three%20major%20DLA%20datasets%20for%20geometric-%20and%20content-aware%20adaptation.%0AFurthermore%2C%20we%20propose%20Document%20Layout%20Analysis%20Adapter%20%28DLAdapter%29%2C%20a%20novel%0Aframework%20that%20is%20designed%20to%20improve%20source-free%20adaptation%20across%20document%0Adomains.%20Our%20method%20achieves%20a%20%2B4.21%25%20improvement%20over%20the%20source-only%20baseline%0Aand%20a%20%2B2.26%25%20gain%20over%20existing%20source-free%20methods%20from%20PubLayNet%20to%0ADocLayNet.%20We%20believe%20this%20work%20will%20inspire%20the%20DLA%20community%20to%20further%0Ainvestigate%20source-free%20document%20understanding.%20To%20support%20future%20research%20of%0Athe%20community%2C%20the%20benchmark%2C%20models%2C%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/s3setewe/sfdla-DLAdapter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFDLA%253A%2520Source-Free%2520Document%2520Layout%2520Analysis%26entry.906535625%3DSebastian%2520Tewes%2520and%2520Yufan%2520Chen%2520and%2520Omar%2520Moured%2520and%2520Jiaming%2520Zhang%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520Document%2520Layout%2520Analysis%2520%2528DLA%2529%2520is%2520a%2520fundamental%2520task%2520in%2520document%250Aunderstanding.%2520However%252C%2520existing%2520DLA%2520and%2520adaptation%2520methods%2520often%2520require%250Aaccess%2520to%2520large-scale%2520source%2520data%2520and%2520target%2520labels.%2520This%2520requirements%2520severely%250Alimiting%2520their%2520real-world%2520applicability%252C%2520particularly%2520in%2520privacy-sensitive%2520and%250Aresource-constrained%2520domains%252C%2520such%2520as%2520financial%2520statements%252C%2520medical%2520records%252C%250Aand%2520proprietary%2520business%2520documents.%2520According%2520to%2520our%2520observation%252C%2520directly%250Atransferring%2520source-domain%2520fine-tuned%2520models%2520on%2520target%2520domains%2520often%2520results%2520in%250Aa%2520significant%2520performance%2520drop%2520%2528Avg.%2520-32.64%2525%2529.%2520In%2520this%2520work%252C%2520we%2520introduce%250ASource-Free%2520Document%2520Layout%2520Analysis%2520%2528SFDLA%2529%252C%2520aiming%2520for%2520adapting%2520a%2520pre-trained%250Asource%2520DLA%2520models%2520to%2520an%2520unlabeled%2520target%2520domain%252C%2520without%2520access%2520to%2520any%2520source%250Adata.%2520To%2520address%2520this%2520challenge%252C%2520we%2520establish%2520the%2520first%2520SFDLA%2520benchmark%252C%250Acovering%2520three%2520major%2520DLA%2520datasets%2520for%2520geometric-%2520and%2520content-aware%2520adaptation.%250AFurthermore%252C%2520we%2520propose%2520Document%2520Layout%2520Analysis%2520Adapter%2520%2528DLAdapter%2529%252C%2520a%2520novel%250Aframework%2520that%2520is%2520designed%2520to%2520improve%2520source-free%2520adaptation%2520across%2520document%250Adomains.%2520Our%2520method%2520achieves%2520a%2520%252B4.21%2525%2520improvement%2520over%2520the%2520source-only%2520baseline%250Aand%2520a%2520%252B2.26%2525%2520gain%2520over%2520existing%2520source-free%2520methods%2520from%2520PubLayNet%2520to%250ADocLayNet.%2520We%2520believe%2520this%2520work%2520will%2520inspire%2520the%2520DLA%2520community%2520to%2520further%250Ainvestigate%2520source-free%2520document%2520understanding.%2520To%2520support%2520future%2520research%2520of%250Athe%2520community%252C%2520the%2520benchmark%252C%2520models%252C%2520and%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/s3setewe/sfdla-DLAdapter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFDLA%3A%20Source-Free%20Document%20Layout%20Analysis&entry.906535625=Sebastian%20Tewes%20and%20Yufan%20Chen%20and%20Omar%20Moured%20and%20Jiaming%20Zhang%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20Document%20Layout%20Analysis%20%28DLA%29%20is%20a%20fundamental%20task%20in%20document%0Aunderstanding.%20However%2C%20existing%20DLA%20and%20adaptation%20methods%20often%20require%0Aaccess%20to%20large-scale%20source%20data%20and%20target%20labels.%20This%20requirements%20severely%0Alimiting%20their%20real-world%20applicability%2C%20particularly%20in%20privacy-sensitive%20and%0Aresource-constrained%20domains%2C%20such%20as%20financial%20statements%2C%20medical%20records%2C%0Aand%20proprietary%20business%20documents.%20According%20to%20our%20observation%2C%20directly%0Atransferring%20source-domain%20fine-tuned%20models%20on%20target%20domains%20often%20results%20in%0Aa%20significant%20performance%20drop%20%28Avg.%20-32.64%25%29.%20In%20this%20work%2C%20we%20introduce%0ASource-Free%20Document%20Layout%20Analysis%20%28SFDLA%29%2C%20aiming%20for%20adapting%20a%20pre-trained%0Asource%20DLA%20models%20to%20an%20unlabeled%20target%20domain%2C%20without%20access%20to%20any%20source%0Adata.%20To%20address%20this%20challenge%2C%20we%20establish%20the%20first%20SFDLA%20benchmark%2C%0Acovering%20three%20major%20DLA%20datasets%20for%20geometric-%20and%20content-aware%20adaptation.%0AFurthermore%2C%20we%20propose%20Document%20Layout%20Analysis%20Adapter%20%28DLAdapter%29%2C%20a%20novel%0Aframework%20that%20is%20designed%20to%20improve%20source-free%20adaptation%20across%20document%0Adomains.%20Our%20method%20achieves%20a%20%2B4.21%25%20improvement%20over%20the%20source-only%20baseline%0Aand%20a%20%2B2.26%25%20gain%20over%20existing%20source-free%20methods%20from%20PubLayNet%20to%0ADocLayNet.%20We%20believe%20this%20work%20will%20inspire%20the%20DLA%20community%20to%20further%0Ainvestigate%20source-free%20document%20understanding.%20To%20support%20future%20research%20of%0Athe%20community%2C%20the%20benchmark%2C%20models%2C%20and%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/s3setewe/sfdla-DLAdapter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18742v2&entry.124074799=Read"},
{"title": "Revisiting Compositional Generalization Capability of Large Language\n  Models Considering Instruction Following Ability", "author": "Yusuke Sakai and Hidetaka Kamigaito and Taro Watanabe", "abstract": "  In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.\n", "link": "http://arxiv.org/abs/2506.15629v1", "date": "2025-06-18", "relevancy": 2.0913, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Compositional%20Generalization%20Capability%20of%20Large%20Language%0A%20%20Models%20Considering%20Instruction%20Following%20Ability&body=Title%3A%20Revisiting%20Compositional%20Generalization%20Capability%20of%20Large%20Language%0A%20%20Models%20Considering%20Instruction%20Following%20Ability%0AAuthor%3A%20Yusuke%20Sakai%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe%0AAbstract%3A%20%20%20In%20generative%20commonsense%20reasoning%20tasks%20such%20as%20CommonGen%2C%20generative%20large%0Alanguage%20models%20%28LLMs%29%20compose%20sentences%20that%20include%20all%20given%20concepts.%0AHowever%2C%20when%20focusing%20on%20instruction-following%20capabilities%2C%20if%20a%20prompt%0Aspecifies%20a%20concept%20order%2C%20LLMs%20must%20generate%20sentences%20that%20adhere%20to%20the%0Aspecified%20order.%20To%20address%20this%2C%20we%20propose%20Ordered%20CommonGen%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20the%20compositional%20generalization%20and%20instruction-following%0Aabilities%20of%20LLMs.%20This%20benchmark%20measures%20ordered%20coverage%20to%20assess%20whether%0Aconcepts%20are%20generated%20in%20the%20specified%20order%2C%20enabling%20a%20simultaneous%0Aevaluation%20of%20both%20abilities.%20We%20conducted%20a%20comprehensive%20analysis%20using%2036%0ALLMs%20and%20found%20that%2C%20while%20LLMs%20generally%20understand%20the%20intent%20of%0Ainstructions%2C%20biases%20toward%20specific%20concept%20order%20patterns%20often%20lead%20to%0Alow-diversity%20outputs%20or%20identical%20results%20even%20when%20the%20concept%20order%20is%0Aaltered.%20Moreover%2C%20even%20the%20most%20instruction-compliant%20LLM%20achieved%20only%20about%0A75%25%20ordered%20coverage%2C%20highlighting%20the%20need%20for%20improvements%20in%20both%0Ainstruction-following%20and%20compositional%20generalization%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Compositional%2520Generalization%2520Capability%2520of%2520Large%2520Language%250A%2520%2520Models%2520Considering%2520Instruction%2520Following%2520Ability%26entry.906535625%3DYusuke%2520Sakai%2520and%2520Hidetaka%2520Kamigaito%2520and%2520Taro%2520Watanabe%26entry.1292438233%3D%2520%2520In%2520generative%2520commonsense%2520reasoning%2520tasks%2520such%2520as%2520CommonGen%252C%2520generative%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520compose%2520sentences%2520that%2520include%2520all%2520given%2520concepts.%250AHowever%252C%2520when%2520focusing%2520on%2520instruction-following%2520capabilities%252C%2520if%2520a%2520prompt%250Aspecifies%2520a%2520concept%2520order%252C%2520LLMs%2520must%2520generate%2520sentences%2520that%2520adhere%2520to%2520the%250Aspecified%2520order.%2520To%2520address%2520this%252C%2520we%2520propose%2520Ordered%2520CommonGen%252C%2520a%2520benchmark%250Adesigned%2520to%2520evaluate%2520the%2520compositional%2520generalization%2520and%2520instruction-following%250Aabilities%2520of%2520LLMs.%2520This%2520benchmark%2520measures%2520ordered%2520coverage%2520to%2520assess%2520whether%250Aconcepts%2520are%2520generated%2520in%2520the%2520specified%2520order%252C%2520enabling%2520a%2520simultaneous%250Aevaluation%2520of%2520both%2520abilities.%2520We%2520conducted%2520a%2520comprehensive%2520analysis%2520using%252036%250ALLMs%2520and%2520found%2520that%252C%2520while%2520LLMs%2520generally%2520understand%2520the%2520intent%2520of%250Ainstructions%252C%2520biases%2520toward%2520specific%2520concept%2520order%2520patterns%2520often%2520lead%2520to%250Alow-diversity%2520outputs%2520or%2520identical%2520results%2520even%2520when%2520the%2520concept%2520order%2520is%250Aaltered.%2520Moreover%252C%2520even%2520the%2520most%2520instruction-compliant%2520LLM%2520achieved%2520only%2520about%250A75%2525%2520ordered%2520coverage%252C%2520highlighting%2520the%2520need%2520for%2520improvements%2520in%2520both%250Ainstruction-following%2520and%2520compositional%2520generalization%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Compositional%20Generalization%20Capability%20of%20Large%20Language%0A%20%20Models%20Considering%20Instruction%20Following%20Ability&entry.906535625=Yusuke%20Sakai%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe&entry.1292438233=%20%20In%20generative%20commonsense%20reasoning%20tasks%20such%20as%20CommonGen%2C%20generative%20large%0Alanguage%20models%20%28LLMs%29%20compose%20sentences%20that%20include%20all%20given%20concepts.%0AHowever%2C%20when%20focusing%20on%20instruction-following%20capabilities%2C%20if%20a%20prompt%0Aspecifies%20a%20concept%20order%2C%20LLMs%20must%20generate%20sentences%20that%20adhere%20to%20the%0Aspecified%20order.%20To%20address%20this%2C%20we%20propose%20Ordered%20CommonGen%2C%20a%20benchmark%0Adesigned%20to%20evaluate%20the%20compositional%20generalization%20and%20instruction-following%0Aabilities%20of%20LLMs.%20This%20benchmark%20measures%20ordered%20coverage%20to%20assess%20whether%0Aconcepts%20are%20generated%20in%20the%20specified%20order%2C%20enabling%20a%20simultaneous%0Aevaluation%20of%20both%20abilities.%20We%20conducted%20a%20comprehensive%20analysis%20using%2036%0ALLMs%20and%20found%20that%2C%20while%20LLMs%20generally%20understand%20the%20intent%20of%0Ainstructions%2C%20biases%20toward%20specific%20concept%20order%20patterns%20often%20lead%20to%0Alow-diversity%20outputs%20or%20identical%20results%20even%20when%20the%20concept%20order%20is%0Aaltered.%20Moreover%2C%20even%20the%20most%20instruction-compliant%20LLM%20achieved%20only%20about%0A75%25%20ordered%20coverage%2C%20highlighting%20the%20need%20for%20improvements%20in%20both%0Ainstruction-following%20and%20compositional%20generalization%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15629v1&entry.124074799=Read"},
{"title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling", "author": "Md Imbesat Hassan Rizvi and Xiaodan Zhu and Iryna Gurevych", "abstract": "  Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.\n", "link": "http://arxiv.org/abs/2506.15498v1", "date": "2025-06-18", "relevancy": 2.0871, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARE%3A%20Single-Pass%20Annotation%20with%20Reference-Guided%20Evaluation%20for%0A%20%20Automatic%20Process%20Supervision%20and%20Reward%20Modelling&body=Title%3A%20SPARE%3A%20Single-Pass%20Annotation%20with%20Reference-Guided%20Evaluation%20for%0A%20%20Automatic%20Process%20Supervision%20and%20Reward%20Modelling%0AAuthor%3A%20Md%20Imbesat%20Hassan%20Rizvi%20and%20Xiaodan%20Zhu%20and%20Iryna%20Gurevych%0AAbstract%3A%20%20%20Process%20or%20step-wise%20supervision%20has%20played%20a%20crucial%20role%20in%20advancing%0Acomplex%20multi-step%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20efficient%2C%20high-quality%20automated%20process%20annotation%20remains%20a%0Asignificant%20challenge.%20To%20address%20this%2C%20we%20introduce%20Single-Pass%20Annotation%0Awith%20Reference-Guided%20Evaluation%20%28SPARE%29%2C%20a%20novel%20structured%20framework%20that%0Aenables%20single-pass%2C%20per-step%20annotation%20by%20aligning%20each%20solution%20step%20to%20one%0Aor%20multiple%20steps%20in%20a%20reference%20solution%2C%20accompanied%20by%20explicit%20reasoning%0Afor%20evaluation.%20We%20show%20that%20reference-guided%20step-level%20evaluation%20effectively%0Afacilitates%20process%20supervision%20on%20four%20datasets%20spanning%20three%20domains%3A%0Amathematical%20reasoning%2C%20multi-hop%20compositional%20question%20answering%2C%20and%20spatial%0Areasoning.%20We%20demonstrate%20that%20SPARE%2C%20when%20compared%20to%20baselines%2C%20improves%0Areasoning%20performance%20when%20used%20for%3A%20%281%29%20fine-tuning%20models%20in%20an%20offline%20RL%0Asetup%20for%20inference-time%20greedy-decoding%2C%20and%20%282%29%20training%20reward%20models%20for%0Aranking/aggregating%20multiple%20LLM-generated%20outputs.%20Additionally%2C%20SPARE%0Aachieves%20competitive%20performance%20on%20challenging%20mathematical%20datasets%20while%0Aoffering%202.6%20times%20greater%20efficiency%2C%20requiring%20only%2038%25%20of%20the%20runtime%2C%0Acompared%20to%20tree%20search-based%20automatic%20annotation.%20The%20codebase%2C%20along%20with%20a%0Atrained%20SPARE-PRM%20model%2C%20is%20publicly%20released%20to%20facilitate%20further%20research%0Aand%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARE%253A%2520Single-Pass%2520Annotation%2520with%2520Reference-Guided%2520Evaluation%2520for%250A%2520%2520Automatic%2520Process%2520Supervision%2520and%2520Reward%2520Modelling%26entry.906535625%3DMd%2520Imbesat%2520Hassan%2520Rizvi%2520and%2520Xiaodan%2520Zhu%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3D%2520%2520Process%2520or%2520step-wise%2520supervision%2520has%2520played%2520a%2520crucial%2520role%2520in%2520advancing%250Acomplex%2520multi-step%2520reasoning%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250AHowever%252C%2520efficient%252C%2520high-quality%2520automated%2520process%2520annotation%2520remains%2520a%250Asignificant%2520challenge.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Single-Pass%2520Annotation%250Awith%2520Reference-Guided%2520Evaluation%2520%2528SPARE%2529%252C%2520a%2520novel%2520structured%2520framework%2520that%250Aenables%2520single-pass%252C%2520per-step%2520annotation%2520by%2520aligning%2520each%2520solution%2520step%2520to%2520one%250Aor%2520multiple%2520steps%2520in%2520a%2520reference%2520solution%252C%2520accompanied%2520by%2520explicit%2520reasoning%250Afor%2520evaluation.%2520We%2520show%2520that%2520reference-guided%2520step-level%2520evaluation%2520effectively%250Afacilitates%2520process%2520supervision%2520on%2520four%2520datasets%2520spanning%2520three%2520domains%253A%250Amathematical%2520reasoning%252C%2520multi-hop%2520compositional%2520question%2520answering%252C%2520and%2520spatial%250Areasoning.%2520We%2520demonstrate%2520that%2520SPARE%252C%2520when%2520compared%2520to%2520baselines%252C%2520improves%250Areasoning%2520performance%2520when%2520used%2520for%253A%2520%25281%2529%2520fine-tuning%2520models%2520in%2520an%2520offline%2520RL%250Asetup%2520for%2520inference-time%2520greedy-decoding%252C%2520and%2520%25282%2529%2520training%2520reward%2520models%2520for%250Aranking/aggregating%2520multiple%2520LLM-generated%2520outputs.%2520Additionally%252C%2520SPARE%250Aachieves%2520competitive%2520performance%2520on%2520challenging%2520mathematical%2520datasets%2520while%250Aoffering%25202.6%2520times%2520greater%2520efficiency%252C%2520requiring%2520only%252038%2525%2520of%2520the%2520runtime%252C%250Acompared%2520to%2520tree%2520search-based%2520automatic%2520annotation.%2520The%2520codebase%252C%2520along%2520with%2520a%250Atrained%2520SPARE-PRM%2520model%252C%2520is%2520publicly%2520released%2520to%2520facilitate%2520further%2520research%250Aand%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARE%3A%20Single-Pass%20Annotation%20with%20Reference-Guided%20Evaluation%20for%0A%20%20Automatic%20Process%20Supervision%20and%20Reward%20Modelling&entry.906535625=Md%20Imbesat%20Hassan%20Rizvi%20and%20Xiaodan%20Zhu%20and%20Iryna%20Gurevych&entry.1292438233=%20%20Process%20or%20step-wise%20supervision%20has%20played%20a%20crucial%20role%20in%20advancing%0Acomplex%20multi-step%20reasoning%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20efficient%2C%20high-quality%20automated%20process%20annotation%20remains%20a%0Asignificant%20challenge.%20To%20address%20this%2C%20we%20introduce%20Single-Pass%20Annotation%0Awith%20Reference-Guided%20Evaluation%20%28SPARE%29%2C%20a%20novel%20structured%20framework%20that%0Aenables%20single-pass%2C%20per-step%20annotation%20by%20aligning%20each%20solution%20step%20to%20one%0Aor%20multiple%20steps%20in%20a%20reference%20solution%2C%20accompanied%20by%20explicit%20reasoning%0Afor%20evaluation.%20We%20show%20that%20reference-guided%20step-level%20evaluation%20effectively%0Afacilitates%20process%20supervision%20on%20four%20datasets%20spanning%20three%20domains%3A%0Amathematical%20reasoning%2C%20multi-hop%20compositional%20question%20answering%2C%20and%20spatial%0Areasoning.%20We%20demonstrate%20that%20SPARE%2C%20when%20compared%20to%20baselines%2C%20improves%0Areasoning%20performance%20when%20used%20for%3A%20%281%29%20fine-tuning%20models%20in%20an%20offline%20RL%0Asetup%20for%20inference-time%20greedy-decoding%2C%20and%20%282%29%20training%20reward%20models%20for%0Aranking/aggregating%20multiple%20LLM-generated%20outputs.%20Additionally%2C%20SPARE%0Aachieves%20competitive%20performance%20on%20challenging%20mathematical%20datasets%20while%0Aoffering%202.6%20times%20greater%20efficiency%2C%20requiring%20only%2038%25%20of%20the%20runtime%2C%0Acompared%20to%20tree%20search-based%20automatic%20annotation.%20The%20codebase%2C%20along%20with%20a%0Atrained%20SPARE-PRM%20model%2C%20is%20publicly%20released%20to%20facilitate%20further%20research%0Aand%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15498v1&entry.124074799=Read"},
{"title": "Memory-Efficient Differentially Private Training with Gradient Random\n  Projection", "author": "Alex Mulrooney and Devansh Gupta and James Flemings and Huanyu Zhang and Murali Annavaram and Meisam Razaviyayn and Xinwei Zhang", "abstract": "  Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.\n", "link": "http://arxiv.org/abs/2506.15588v1", "date": "2025-06-18", "relevancy": 2.0692, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5296}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5167}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Efficient%20Differentially%20Private%20Training%20with%20Gradient%20Random%0A%20%20Projection&body=Title%3A%20Memory-Efficient%20Differentially%20Private%20Training%20with%20Gradient%20Random%0A%20%20Projection%0AAuthor%3A%20Alex%20Mulrooney%20and%20Devansh%20Gupta%20and%20James%20Flemings%20and%20Huanyu%20Zhang%20and%20Murali%20Annavaram%20and%20Meisam%20Razaviyayn%20and%20Xinwei%20Zhang%0AAbstract%3A%20%20%20Differential%20privacy%20%28DP%29%20protects%20sensitive%20data%20during%20neural%20network%0Atraining%2C%20but%20standard%20methods%20like%20DP-Adam%20suffer%20from%20high%20memory%20overhead%0Adue%20to%20per-sample%20gradient%20clipping%2C%20limiting%20scalability.%20We%20introduce%0ADP-GRAPE%20%28Gradient%20RAndom%20ProjEction%29%2C%20a%20DP%20training%20method%20that%20significantly%0Areduces%20memory%20usage%20while%20maintaining%20utility%20on%20par%20with%20first-order%20DP%0Aapproaches.%20Rather%20than%20directly%20applying%20DP%20to%20GaLore%2C%20DP-GRAPE%20introduces%0Athree%20key%20modifications%3A%20%281%29%20gradients%20are%20privatized%20after%20projection%2C%20%282%29%0Arandom%20Gaussian%20matrices%20replace%20SVD-based%20subspaces%2C%20and%20%283%29%20projection%20is%0Aapplied%20during%20backpropagation.%20These%20contributions%20eliminate%20the%20need%20for%0Acostly%20SVD%20computations%2C%20enable%20substantial%20memory%20savings%2C%20and%20lead%20to%0Aimproved%20utility.%20Despite%20operating%20in%20lower-dimensional%20subspaces%2C%20our%0Atheoretical%20analysis%20shows%20that%20DP-GRAPE%20achieves%20a%20privacy-utility%20trade-off%0Acomparable%20to%20DP-SGD.%20Our%20extensive%20empirical%20experiments%20show%20that%20DP-GRAPE%0Acan%20reduce%20the%20memory%20footprint%20of%20DP%20training%20without%20sacrificing%20accuracy%20or%0Atraining%20time.%20In%20particular%2C%20DP-GRAPE%20reduces%20memory%20usage%20by%20over%2063%25%20when%0Apre-training%20Vision%20Transformers%20and%20over%2070%25%20when%20fine-tuning%20RoBERTa-Large%20as%0Acompared%20to%20DP-Adam%2C%20while%20achieving%20similar%20performance.%20We%20further%0Ademonstrate%20that%20DP-GRAPE%20scales%20to%20fine-tuning%20large%20models%20such%20as%20OPT%20with%0Aup%20to%206.7%20billion%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Efficient%2520Differentially%2520Private%2520Training%2520with%2520Gradient%2520Random%250A%2520%2520Projection%26entry.906535625%3DAlex%2520Mulrooney%2520and%2520Devansh%2520Gupta%2520and%2520James%2520Flemings%2520and%2520Huanyu%2520Zhang%2520and%2520Murali%2520Annavaram%2520and%2520Meisam%2520Razaviyayn%2520and%2520Xinwei%2520Zhang%26entry.1292438233%3D%2520%2520Differential%2520privacy%2520%2528DP%2529%2520protects%2520sensitive%2520data%2520during%2520neural%2520network%250Atraining%252C%2520but%2520standard%2520methods%2520like%2520DP-Adam%2520suffer%2520from%2520high%2520memory%2520overhead%250Adue%2520to%2520per-sample%2520gradient%2520clipping%252C%2520limiting%2520scalability.%2520We%2520introduce%250ADP-GRAPE%2520%2528Gradient%2520RAndom%2520ProjEction%2529%252C%2520a%2520DP%2520training%2520method%2520that%2520significantly%250Areduces%2520memory%2520usage%2520while%2520maintaining%2520utility%2520on%2520par%2520with%2520first-order%2520DP%250Aapproaches.%2520Rather%2520than%2520directly%2520applying%2520DP%2520to%2520GaLore%252C%2520DP-GRAPE%2520introduces%250Athree%2520key%2520modifications%253A%2520%25281%2529%2520gradients%2520are%2520privatized%2520after%2520projection%252C%2520%25282%2529%250Arandom%2520Gaussian%2520matrices%2520replace%2520SVD-based%2520subspaces%252C%2520and%2520%25283%2529%2520projection%2520is%250Aapplied%2520during%2520backpropagation.%2520These%2520contributions%2520eliminate%2520the%2520need%2520for%250Acostly%2520SVD%2520computations%252C%2520enable%2520substantial%2520memory%2520savings%252C%2520and%2520lead%2520to%250Aimproved%2520utility.%2520Despite%2520operating%2520in%2520lower-dimensional%2520subspaces%252C%2520our%250Atheoretical%2520analysis%2520shows%2520that%2520DP-GRAPE%2520achieves%2520a%2520privacy-utility%2520trade-off%250Acomparable%2520to%2520DP-SGD.%2520Our%2520extensive%2520empirical%2520experiments%2520show%2520that%2520DP-GRAPE%250Acan%2520reduce%2520the%2520memory%2520footprint%2520of%2520DP%2520training%2520without%2520sacrificing%2520accuracy%2520or%250Atraining%2520time.%2520In%2520particular%252C%2520DP-GRAPE%2520reduces%2520memory%2520usage%2520by%2520over%252063%2525%2520when%250Apre-training%2520Vision%2520Transformers%2520and%2520over%252070%2525%2520when%2520fine-tuning%2520RoBERTa-Large%2520as%250Acompared%2520to%2520DP-Adam%252C%2520while%2520achieving%2520similar%2520performance.%2520We%2520further%250Ademonstrate%2520that%2520DP-GRAPE%2520scales%2520to%2520fine-tuning%2520large%2520models%2520such%2520as%2520OPT%2520with%250Aup%2520to%25206.7%2520billion%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Efficient%20Differentially%20Private%20Training%20with%20Gradient%20Random%0A%20%20Projection&entry.906535625=Alex%20Mulrooney%20and%20Devansh%20Gupta%20and%20James%20Flemings%20and%20Huanyu%20Zhang%20and%20Murali%20Annavaram%20and%20Meisam%20Razaviyayn%20and%20Xinwei%20Zhang&entry.1292438233=%20%20Differential%20privacy%20%28DP%29%20protects%20sensitive%20data%20during%20neural%20network%0Atraining%2C%20but%20standard%20methods%20like%20DP-Adam%20suffer%20from%20high%20memory%20overhead%0Adue%20to%20per-sample%20gradient%20clipping%2C%20limiting%20scalability.%20We%20introduce%0ADP-GRAPE%20%28Gradient%20RAndom%20ProjEction%29%2C%20a%20DP%20training%20method%20that%20significantly%0Areduces%20memory%20usage%20while%20maintaining%20utility%20on%20par%20with%20first-order%20DP%0Aapproaches.%20Rather%20than%20directly%20applying%20DP%20to%20GaLore%2C%20DP-GRAPE%20introduces%0Athree%20key%20modifications%3A%20%281%29%20gradients%20are%20privatized%20after%20projection%2C%20%282%29%0Arandom%20Gaussian%20matrices%20replace%20SVD-based%20subspaces%2C%20and%20%283%29%20projection%20is%0Aapplied%20during%20backpropagation.%20These%20contributions%20eliminate%20the%20need%20for%0Acostly%20SVD%20computations%2C%20enable%20substantial%20memory%20savings%2C%20and%20lead%20to%0Aimproved%20utility.%20Despite%20operating%20in%20lower-dimensional%20subspaces%2C%20our%0Atheoretical%20analysis%20shows%20that%20DP-GRAPE%20achieves%20a%20privacy-utility%20trade-off%0Acomparable%20to%20DP-SGD.%20Our%20extensive%20empirical%20experiments%20show%20that%20DP-GRAPE%0Acan%20reduce%20the%20memory%20footprint%20of%20DP%20training%20without%20sacrificing%20accuracy%20or%0Atraining%20time.%20In%20particular%2C%20DP-GRAPE%20reduces%20memory%20usage%20by%20over%2063%25%20when%0Apre-training%20Vision%20Transformers%20and%20over%2070%25%20when%20fine-tuning%20RoBERTa-Large%20as%0Acompared%20to%20DP-Adam%2C%20while%20achieving%20similar%20performance.%20We%20further%0Ademonstrate%20that%20DP-GRAPE%20scales%20to%20fine-tuning%20large%20models%20such%20as%20OPT%20with%0Aup%20to%206.7%20billion%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15588v1&entry.124074799=Read"},
{"title": "Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and\n  Efficient Attention", "author": "Syed Haider Ali and Asrar Ahmad and Muhammad Ali and Asifullah Khan and Muhammad Shahban and Nadeem Shaukat", "abstract": "  Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment.\n", "link": "http://arxiv.org/abs/2506.15562v1", "date": "2025-06-18", "relevancy": 2.0606, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5314}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5121}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20MRI%20Tumor%20Segmentation%20using%20hybrid%20U-Net%20with%20Transformer%20and%0A%20%20Efficient%20Attention&body=Title%3A%20Automated%20MRI%20Tumor%20Segmentation%20using%20hybrid%20U-Net%20with%20Transformer%20and%0A%20%20Efficient%20Attention%0AAuthor%3A%20Syed%20Haider%20Ali%20and%20Asrar%20Ahmad%20and%20Muhammad%20Ali%20and%20Asifullah%20Khan%20and%20Muhammad%20Shahban%20and%20Nadeem%20Shaukat%0AAbstract%3A%20%20%20Cancer%20is%20an%20abnormal%20growth%20with%20potential%20to%20invade%20locally%20and%20metastasize%0Ato%20distant%20organs.%20Accurate%20auto-segmentation%20of%20the%20tumor%20and%20surrounding%0Anormal%20tissues%20is%20required%20for%20radiotherapy%20treatment%20plan%20optimization.%20Recent%0AAI-based%20segmentation%20models%20are%20generally%20trained%20on%20large%20public%20datasets%2C%0Awhich%20lack%20the%20heterogeneity%20of%20local%20patient%20populations.%20While%20these%20studies%0Aadvance%20AI-based%20medical%20image%20segmentation%2C%20research%20on%20local%20datasets%20is%0Anecessary%20to%20develop%20and%20integrate%20AI%20tumor%20segmentation%20models%20directly%20into%0Ahospital%20software%20for%20efficient%20and%20accurate%20oncology%20treatment%20planning%20and%0Aexecution.%20This%20study%20enhances%20tumor%20segmentation%20using%20computationally%0Aefficient%20hybrid%20UNet-Transformer%20models%20on%20magnetic%20resonance%20imaging%20%28MRI%29%0Adatasets%20acquired%20from%20a%20local%20hospital%20under%20strict%20privacy%20protection.%20We%0Adeveloped%20a%20robust%20data%20pipeline%20for%20seamless%20DICOM%20extraction%20and%0Apreprocessing%2C%20followed%20by%20extensive%20image%20augmentation%20to%20ensure%20model%0Ageneralization%20across%20diverse%20clinical%20settings%2C%20resulting%20in%20a%20total%20dataset%0Aof%206080%20images%20for%20training.%20Our%20novel%20architecture%20integrates%20UNet-based%0Aconvolutional%20neural%20networks%20with%20a%20transformer%20bottleneck%20and%20complementary%0Aattention%20modules%2C%20including%20efficient%20attention%2C%20Squeeze-and-Excitation%20%28SE%29%0Ablocks%2C%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20and%20ResNeXt%20blocks.%20To%0Aaccelerate%20convergence%20and%20reduce%20computational%20demands%2C%20we%20used%20a%20maximum%0Abatch%20size%20of%208%20and%20initialized%20the%20encoder%20with%20pretrained%20ImageNet%20weights%2C%0Atraining%20the%20model%20on%20dual%20NVIDIA%20T4%20GPUs%20via%20checkpointing%20to%20overcome%0AKaggle%27s%20runtime%20limits.%20Quantitative%20evaluation%20on%20the%20local%20MRI%20dataset%0Ayielded%20a%20Dice%20similarity%20coefficient%20of%200.764%20and%20an%20Intersection%20over%20Union%0A%28IoU%29%20of%200.736%2C%20demonstrating%20competitive%20performance%20despite%20limited%20data%20and%0Aunderscoring%20the%20importance%20of%20site-specific%20model%20development%20for%20clinical%0Adeployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520MRI%2520Tumor%2520Segmentation%2520using%2520hybrid%2520U-Net%2520with%2520Transformer%2520and%250A%2520%2520Efficient%2520Attention%26entry.906535625%3DSyed%2520Haider%2520Ali%2520and%2520Asrar%2520Ahmad%2520and%2520Muhammad%2520Ali%2520and%2520Asifullah%2520Khan%2520and%2520Muhammad%2520Shahban%2520and%2520Nadeem%2520Shaukat%26entry.1292438233%3D%2520%2520Cancer%2520is%2520an%2520abnormal%2520growth%2520with%2520potential%2520to%2520invade%2520locally%2520and%2520metastasize%250Ato%2520distant%2520organs.%2520Accurate%2520auto-segmentation%2520of%2520the%2520tumor%2520and%2520surrounding%250Anormal%2520tissues%2520is%2520required%2520for%2520radiotherapy%2520treatment%2520plan%2520optimization.%2520Recent%250AAI-based%2520segmentation%2520models%2520are%2520generally%2520trained%2520on%2520large%2520public%2520datasets%252C%250Awhich%2520lack%2520the%2520heterogeneity%2520of%2520local%2520patient%2520populations.%2520While%2520these%2520studies%250Aadvance%2520AI-based%2520medical%2520image%2520segmentation%252C%2520research%2520on%2520local%2520datasets%2520is%250Anecessary%2520to%2520develop%2520and%2520integrate%2520AI%2520tumor%2520segmentation%2520models%2520directly%2520into%250Ahospital%2520software%2520for%2520efficient%2520and%2520accurate%2520oncology%2520treatment%2520planning%2520and%250Aexecution.%2520This%2520study%2520enhances%2520tumor%2520segmentation%2520using%2520computationally%250Aefficient%2520hybrid%2520UNet-Transformer%2520models%2520on%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Adatasets%2520acquired%2520from%2520a%2520local%2520hospital%2520under%2520strict%2520privacy%2520protection.%2520We%250Adeveloped%2520a%2520robust%2520data%2520pipeline%2520for%2520seamless%2520DICOM%2520extraction%2520and%250Apreprocessing%252C%2520followed%2520by%2520extensive%2520image%2520augmentation%2520to%2520ensure%2520model%250Ageneralization%2520across%2520diverse%2520clinical%2520settings%252C%2520resulting%2520in%2520a%2520total%2520dataset%250Aof%25206080%2520images%2520for%2520training.%2520Our%2520novel%2520architecture%2520integrates%2520UNet-based%250Aconvolutional%2520neural%2520networks%2520with%2520a%2520transformer%2520bottleneck%2520and%2520complementary%250Aattention%2520modules%252C%2520including%2520efficient%2520attention%252C%2520Squeeze-and-Excitation%2520%2528SE%2529%250Ablocks%252C%2520Convolutional%2520Block%2520Attention%2520Module%2520%2528CBAM%2529%252C%2520and%2520ResNeXt%2520blocks.%2520To%250Aaccelerate%2520convergence%2520and%2520reduce%2520computational%2520demands%252C%2520we%2520used%2520a%2520maximum%250Abatch%2520size%2520of%25208%2520and%2520initialized%2520the%2520encoder%2520with%2520pretrained%2520ImageNet%2520weights%252C%250Atraining%2520the%2520model%2520on%2520dual%2520NVIDIA%2520T4%2520GPUs%2520via%2520checkpointing%2520to%2520overcome%250AKaggle%2527s%2520runtime%2520limits.%2520Quantitative%2520evaluation%2520on%2520the%2520local%2520MRI%2520dataset%250Ayielded%2520a%2520Dice%2520similarity%2520coefficient%2520of%25200.764%2520and%2520an%2520Intersection%2520over%2520Union%250A%2528IoU%2529%2520of%25200.736%252C%2520demonstrating%2520competitive%2520performance%2520despite%2520limited%2520data%2520and%250Aunderscoring%2520the%2520importance%2520of%2520site-specific%2520model%2520development%2520for%2520clinical%250Adeployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20MRI%20Tumor%20Segmentation%20using%20hybrid%20U-Net%20with%20Transformer%20and%0A%20%20Efficient%20Attention&entry.906535625=Syed%20Haider%20Ali%20and%20Asrar%20Ahmad%20and%20Muhammad%20Ali%20and%20Asifullah%20Khan%20and%20Muhammad%20Shahban%20and%20Nadeem%20Shaukat&entry.1292438233=%20%20Cancer%20is%20an%20abnormal%20growth%20with%20potential%20to%20invade%20locally%20and%20metastasize%0Ato%20distant%20organs.%20Accurate%20auto-segmentation%20of%20the%20tumor%20and%20surrounding%0Anormal%20tissues%20is%20required%20for%20radiotherapy%20treatment%20plan%20optimization.%20Recent%0AAI-based%20segmentation%20models%20are%20generally%20trained%20on%20large%20public%20datasets%2C%0Awhich%20lack%20the%20heterogeneity%20of%20local%20patient%20populations.%20While%20these%20studies%0Aadvance%20AI-based%20medical%20image%20segmentation%2C%20research%20on%20local%20datasets%20is%0Anecessary%20to%20develop%20and%20integrate%20AI%20tumor%20segmentation%20models%20directly%20into%0Ahospital%20software%20for%20efficient%20and%20accurate%20oncology%20treatment%20planning%20and%0Aexecution.%20This%20study%20enhances%20tumor%20segmentation%20using%20computationally%0Aefficient%20hybrid%20UNet-Transformer%20models%20on%20magnetic%20resonance%20imaging%20%28MRI%29%0Adatasets%20acquired%20from%20a%20local%20hospital%20under%20strict%20privacy%20protection.%20We%0Adeveloped%20a%20robust%20data%20pipeline%20for%20seamless%20DICOM%20extraction%20and%0Apreprocessing%2C%20followed%20by%20extensive%20image%20augmentation%20to%20ensure%20model%0Ageneralization%20across%20diverse%20clinical%20settings%2C%20resulting%20in%20a%20total%20dataset%0Aof%206080%20images%20for%20training.%20Our%20novel%20architecture%20integrates%20UNet-based%0Aconvolutional%20neural%20networks%20with%20a%20transformer%20bottleneck%20and%20complementary%0Aattention%20modules%2C%20including%20efficient%20attention%2C%20Squeeze-and-Excitation%20%28SE%29%0Ablocks%2C%20Convolutional%20Block%20Attention%20Module%20%28CBAM%29%2C%20and%20ResNeXt%20blocks.%20To%0Aaccelerate%20convergence%20and%20reduce%20computational%20demands%2C%20we%20used%20a%20maximum%0Abatch%20size%20of%208%20and%20initialized%20the%20encoder%20with%20pretrained%20ImageNet%20weights%2C%0Atraining%20the%20model%20on%20dual%20NVIDIA%20T4%20GPUs%20via%20checkpointing%20to%20overcome%0AKaggle%27s%20runtime%20limits.%20Quantitative%20evaluation%20on%20the%20local%20MRI%20dataset%0Ayielded%20a%20Dice%20similarity%20coefficient%20of%200.764%20and%20an%20Intersection%20over%20Union%0A%28IoU%29%20of%200.736%2C%20demonstrating%20competitive%20performance%20despite%20limited%20data%20and%0Aunderscoring%20the%20importance%20of%20site-specific%20model%20development%20for%20clinical%0Adeployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15562v1&entry.124074799=Read"},
{"title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and\n  Network Sparsity", "author": "Oluwadamilola Fasina and Ruben V. C. Pohle and Pei-Chun Su and Ronald R. Coifman", "abstract": "  We examine the intrinsic (within the attention head) and extrinsic (amongst\nthe attention heads) structure of the self-attention mechanism in transformers.\nTheoretical evidence for invariance of the self-attention mechanism to softmax\nactivation is obtained by appealing to paradifferential calculus, (and is\nsupported by computational examples), which relies on the intrinsic\norganization of the attention heads. Furthermore, we use an existing\nmethodology for hierarchical organization of tensors to examine network\nstructure by constructing hierarchal partition trees with respect to the query,\nkey, and head axes of network 3-tensors. Such an organization is consequential\nsince it allows one to profitably execute common signal processing tasks on a\ngeometry where the organized network 3-tensors exhibit regularity. We exemplify\nthis qualitatively, by visualizing the hierarchical organization of the tree\ncomprised of attention heads and the diffusion map embeddings, and\nquantitatively by investigating network sparsity with the expansion\ncoefficients of individual attention heads and the entire network with respect\nto the bi and tri-haar bases (respectively) on the space of queries, keys, and\nheads of the network. To showcase the utility of our theoretical and\nmethodological findings, we provide computational examples using vision and\nlanguage transformers. The ramifications of these findings are two-fold: (1) a\nsubsequent step in interpretability analysis is theoretically admitted, and can\nbe exploited empirically for downstream interpretability tasks (2) one can use\nthe network 3-tensor organization for empirical network applications such as\nmodel pruning (by virtue of network sparsity) and network architecture\ncomparison.\n", "link": "http://arxiv.org/abs/2506.15541v1", "date": "2025-06-18", "relevancy": 2.0521, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5323}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5316}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intrinsic%20and%20Extrinsic%20Organized%20Attention%3A%20Softmax%20Invariance%20and%0A%20%20Network%20Sparsity&body=Title%3A%20Intrinsic%20and%20Extrinsic%20Organized%20Attention%3A%20Softmax%20Invariance%20and%0A%20%20Network%20Sparsity%0AAuthor%3A%20Oluwadamilola%20Fasina%20and%20Ruben%20V.%20C.%20Pohle%20and%20Pei-Chun%20Su%20and%20Ronald%20R.%20Coifman%0AAbstract%3A%20%20%20We%20examine%20the%20intrinsic%20%28within%20the%20attention%20head%29%20and%20extrinsic%20%28amongst%0Athe%20attention%20heads%29%20structure%20of%20the%20self-attention%20mechanism%20in%20transformers.%0ATheoretical%20evidence%20for%20invariance%20of%20the%20self-attention%20mechanism%20to%20softmax%0Aactivation%20is%20obtained%20by%20appealing%20to%20paradifferential%20calculus%2C%20%28and%20is%0Asupported%20by%20computational%20examples%29%2C%20which%20relies%20on%20the%20intrinsic%0Aorganization%20of%20the%20attention%20heads.%20Furthermore%2C%20we%20use%20an%20existing%0Amethodology%20for%20hierarchical%20organization%20of%20tensors%20to%20examine%20network%0Astructure%20by%20constructing%20hierarchal%20partition%20trees%20with%20respect%20to%20the%20query%2C%0Akey%2C%20and%20head%20axes%20of%20network%203-tensors.%20Such%20an%20organization%20is%20consequential%0Asince%20it%20allows%20one%20to%20profitably%20execute%20common%20signal%20processing%20tasks%20on%20a%0Ageometry%20where%20the%20organized%20network%203-tensors%20exhibit%20regularity.%20We%20exemplify%0Athis%20qualitatively%2C%20by%20visualizing%20the%20hierarchical%20organization%20of%20the%20tree%0Acomprised%20of%20attention%20heads%20and%20the%20diffusion%20map%20embeddings%2C%20and%0Aquantitatively%20by%20investigating%20network%20sparsity%20with%20the%20expansion%0Acoefficients%20of%20individual%20attention%20heads%20and%20the%20entire%20network%20with%20respect%0Ato%20the%20bi%20and%20tri-haar%20bases%20%28respectively%29%20on%20the%20space%20of%20queries%2C%20keys%2C%20and%0Aheads%20of%20the%20network.%20To%20showcase%20the%20utility%20of%20our%20theoretical%20and%0Amethodological%20findings%2C%20we%20provide%20computational%20examples%20using%20vision%20and%0Alanguage%20transformers.%20The%20ramifications%20of%20these%20findings%20are%20two-fold%3A%20%281%29%20a%0Asubsequent%20step%20in%20interpretability%20analysis%20is%20theoretically%20admitted%2C%20and%20can%0Abe%20exploited%20empirically%20for%20downstream%20interpretability%20tasks%20%282%29%20one%20can%20use%0Athe%20network%203-tensor%20organization%20for%20empirical%20network%20applications%20such%20as%0Amodel%20pruning%20%28by%20virtue%20of%20network%20sparsity%29%20and%20network%20architecture%0Acomparison.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntrinsic%2520and%2520Extrinsic%2520Organized%2520Attention%253A%2520Softmax%2520Invariance%2520and%250A%2520%2520Network%2520Sparsity%26entry.906535625%3DOluwadamilola%2520Fasina%2520and%2520Ruben%2520V.%2520C.%2520Pohle%2520and%2520Pei-Chun%2520Su%2520and%2520Ronald%2520R.%2520Coifman%26entry.1292438233%3D%2520%2520We%2520examine%2520the%2520intrinsic%2520%2528within%2520the%2520attention%2520head%2529%2520and%2520extrinsic%2520%2528amongst%250Athe%2520attention%2520heads%2529%2520structure%2520of%2520the%2520self-attention%2520mechanism%2520in%2520transformers.%250ATheoretical%2520evidence%2520for%2520invariance%2520of%2520the%2520self-attention%2520mechanism%2520to%2520softmax%250Aactivation%2520is%2520obtained%2520by%2520appealing%2520to%2520paradifferential%2520calculus%252C%2520%2528and%2520is%250Asupported%2520by%2520computational%2520examples%2529%252C%2520which%2520relies%2520on%2520the%2520intrinsic%250Aorganization%2520of%2520the%2520attention%2520heads.%2520Furthermore%252C%2520we%2520use%2520an%2520existing%250Amethodology%2520for%2520hierarchical%2520organization%2520of%2520tensors%2520to%2520examine%2520network%250Astructure%2520by%2520constructing%2520hierarchal%2520partition%2520trees%2520with%2520respect%2520to%2520the%2520query%252C%250Akey%252C%2520and%2520head%2520axes%2520of%2520network%25203-tensors.%2520Such%2520an%2520organization%2520is%2520consequential%250Asince%2520it%2520allows%2520one%2520to%2520profitably%2520execute%2520common%2520signal%2520processing%2520tasks%2520on%2520a%250Ageometry%2520where%2520the%2520organized%2520network%25203-tensors%2520exhibit%2520regularity.%2520We%2520exemplify%250Athis%2520qualitatively%252C%2520by%2520visualizing%2520the%2520hierarchical%2520organization%2520of%2520the%2520tree%250Acomprised%2520of%2520attention%2520heads%2520and%2520the%2520diffusion%2520map%2520embeddings%252C%2520and%250Aquantitatively%2520by%2520investigating%2520network%2520sparsity%2520with%2520the%2520expansion%250Acoefficients%2520of%2520individual%2520attention%2520heads%2520and%2520the%2520entire%2520network%2520with%2520respect%250Ato%2520the%2520bi%2520and%2520tri-haar%2520bases%2520%2528respectively%2529%2520on%2520the%2520space%2520of%2520queries%252C%2520keys%252C%2520and%250Aheads%2520of%2520the%2520network.%2520To%2520showcase%2520the%2520utility%2520of%2520our%2520theoretical%2520and%250Amethodological%2520findings%252C%2520we%2520provide%2520computational%2520examples%2520using%2520vision%2520and%250Alanguage%2520transformers.%2520The%2520ramifications%2520of%2520these%2520findings%2520are%2520two-fold%253A%2520%25281%2529%2520a%250Asubsequent%2520step%2520in%2520interpretability%2520analysis%2520is%2520theoretically%2520admitted%252C%2520and%2520can%250Abe%2520exploited%2520empirically%2520for%2520downstream%2520interpretability%2520tasks%2520%25282%2529%2520one%2520can%2520use%250Athe%2520network%25203-tensor%2520organization%2520for%2520empirical%2520network%2520applications%2520such%2520as%250Amodel%2520pruning%2520%2528by%2520virtue%2520of%2520network%2520sparsity%2529%2520and%2520network%2520architecture%250Acomparison.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intrinsic%20and%20Extrinsic%20Organized%20Attention%3A%20Softmax%20Invariance%20and%0A%20%20Network%20Sparsity&entry.906535625=Oluwadamilola%20Fasina%20and%20Ruben%20V.%20C.%20Pohle%20and%20Pei-Chun%20Su%20and%20Ronald%20R.%20Coifman&entry.1292438233=%20%20We%20examine%20the%20intrinsic%20%28within%20the%20attention%20head%29%20and%20extrinsic%20%28amongst%0Athe%20attention%20heads%29%20structure%20of%20the%20self-attention%20mechanism%20in%20transformers.%0ATheoretical%20evidence%20for%20invariance%20of%20the%20self-attention%20mechanism%20to%20softmax%0Aactivation%20is%20obtained%20by%20appealing%20to%20paradifferential%20calculus%2C%20%28and%20is%0Asupported%20by%20computational%20examples%29%2C%20which%20relies%20on%20the%20intrinsic%0Aorganization%20of%20the%20attention%20heads.%20Furthermore%2C%20we%20use%20an%20existing%0Amethodology%20for%20hierarchical%20organization%20of%20tensors%20to%20examine%20network%0Astructure%20by%20constructing%20hierarchal%20partition%20trees%20with%20respect%20to%20the%20query%2C%0Akey%2C%20and%20head%20axes%20of%20network%203-tensors.%20Such%20an%20organization%20is%20consequential%0Asince%20it%20allows%20one%20to%20profitably%20execute%20common%20signal%20processing%20tasks%20on%20a%0Ageometry%20where%20the%20organized%20network%203-tensors%20exhibit%20regularity.%20We%20exemplify%0Athis%20qualitatively%2C%20by%20visualizing%20the%20hierarchical%20organization%20of%20the%20tree%0Acomprised%20of%20attention%20heads%20and%20the%20diffusion%20map%20embeddings%2C%20and%0Aquantitatively%20by%20investigating%20network%20sparsity%20with%20the%20expansion%0Acoefficients%20of%20individual%20attention%20heads%20and%20the%20entire%20network%20with%20respect%0Ato%20the%20bi%20and%20tri-haar%20bases%20%28respectively%29%20on%20the%20space%20of%20queries%2C%20keys%2C%20and%0Aheads%20of%20the%20network.%20To%20showcase%20the%20utility%20of%20our%20theoretical%20and%0Amethodological%20findings%2C%20we%20provide%20computational%20examples%20using%20vision%20and%0Alanguage%20transformers.%20The%20ramifications%20of%20these%20findings%20are%20two-fold%3A%20%281%29%20a%0Asubsequent%20step%20in%20interpretability%20analysis%20is%20theoretically%20admitted%2C%20and%20can%0Abe%20exploited%20empirically%20for%20downstream%20interpretability%20tasks%20%282%29%20one%20can%20use%0Athe%20network%203-tensor%20organization%20for%20empirical%20network%20applications%20such%20as%0Amodel%20pruning%20%28by%20virtue%20of%20network%20sparsity%29%20and%20network%20architecture%0Acomparison.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15541v1&entry.124074799=Read"},
{"title": "Over-squashing in Spatiotemporal Graph Neural Networks", "author": "Ivan Marisca and Jacob Bamberger and Cesare Alippi and Michael M. Bronstein", "abstract": "  Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.\n", "link": "http://arxiv.org/abs/2506.15507v1", "date": "2025-06-18", "relevancy": 2.0437, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5271}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5026}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Over-squashing%20in%20Spatiotemporal%20Graph%20Neural%20Networks&body=Title%3A%20Over-squashing%20in%20Spatiotemporal%20Graph%20Neural%20Networks%0AAuthor%3A%20Ivan%20Marisca%20and%20Jacob%20Bamberger%20and%20Cesare%20Alippi%20and%20Michael%20M.%20Bronstein%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20across%20various%0Adomains.%20However%2C%20recent%20theoretical%20advances%20have%20identified%20fundamental%0Alimitations%20in%20their%20information%20propagation%20capabilities%2C%20such%20as%0Aover-squashing%2C%20where%20distant%20nodes%20fail%20to%20effectively%20exchange%20information.%0AWhile%20extensively%20studied%20in%20static%20contexts%2C%20this%20issue%20remains%20unexplored%20in%0ASpatiotemporal%20GNNs%20%28STGNNs%29%2C%20which%20process%20sequences%20associated%20with%20graph%0Anodes.%20Nonetheless%2C%20the%20temporal%20dimension%20amplifies%20this%20challenge%20by%0Aincreasing%20the%20information%20that%20must%20be%20propagated.%20In%20this%20work%2C%20we%20formalize%0Athe%20spatiotemporal%20over-squashing%20problem%20and%20demonstrate%20its%20distinct%0Acharacteristics%20compared%20to%20the%20static%20case.%20Our%20analysis%20reveals%20that%0Acounterintuitively%2C%20convolutional%20STGNNs%20favor%20information%20propagation%20from%0Apoints%20temporally%20distant%20rather%20than%20close%20in%20time.%20Moreover%2C%20we%20prove%20that%0Aarchitectures%20that%20follow%20either%20time-and-space%20or%20time-then-space%20processing%0Aparadigms%20are%20equally%20affected%20by%20this%20phenomenon%2C%20providing%20theoretical%0Ajustification%20for%20computationally%20efficient%20implementations.%20We%20validate%20our%0Afindings%20on%20synthetic%20and%20real-world%20datasets%2C%20providing%20deeper%20insights%20into%0Atheir%20operational%20dynamics%20and%20principled%20guidance%20for%20more%20effective%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOver-squashing%2520in%2520Spatiotemporal%2520Graph%2520Neural%2520Networks%26entry.906535625%3DIvan%2520Marisca%2520and%2520Jacob%2520Bamberger%2520and%2520Cesare%2520Alippi%2520and%2520Michael%2520M.%2520Bronstein%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%2520various%250Adomains.%2520However%252C%2520recent%2520theoretical%2520advances%2520have%2520identified%2520fundamental%250Alimitations%2520in%2520their%2520information%2520propagation%2520capabilities%252C%2520such%2520as%250Aover-squashing%252C%2520where%2520distant%2520nodes%2520fail%2520to%2520effectively%2520exchange%2520information.%250AWhile%2520extensively%2520studied%2520in%2520static%2520contexts%252C%2520this%2520issue%2520remains%2520unexplored%2520in%250ASpatiotemporal%2520GNNs%2520%2528STGNNs%2529%252C%2520which%2520process%2520sequences%2520associated%2520with%2520graph%250Anodes.%2520Nonetheless%252C%2520the%2520temporal%2520dimension%2520amplifies%2520this%2520challenge%2520by%250Aincreasing%2520the%2520information%2520that%2520must%2520be%2520propagated.%2520In%2520this%2520work%252C%2520we%2520formalize%250Athe%2520spatiotemporal%2520over-squashing%2520problem%2520and%2520demonstrate%2520its%2520distinct%250Acharacteristics%2520compared%2520to%2520the%2520static%2520case.%2520Our%2520analysis%2520reveals%2520that%250Acounterintuitively%252C%2520convolutional%2520STGNNs%2520favor%2520information%2520propagation%2520from%250Apoints%2520temporally%2520distant%2520rather%2520than%2520close%2520in%2520time.%2520Moreover%252C%2520we%2520prove%2520that%250Aarchitectures%2520that%2520follow%2520either%2520time-and-space%2520or%2520time-then-space%2520processing%250Aparadigms%2520are%2520equally%2520affected%2520by%2520this%2520phenomenon%252C%2520providing%2520theoretical%250Ajustification%2520for%2520computationally%2520efficient%2520implementations.%2520We%2520validate%2520our%250Afindings%2520on%2520synthetic%2520and%2520real-world%2520datasets%252C%2520providing%2520deeper%2520insights%2520into%250Atheir%2520operational%2520dynamics%2520and%2520principled%2520guidance%2520for%2520more%2520effective%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Over-squashing%20in%20Spatiotemporal%20Graph%20Neural%20Networks&entry.906535625=Ivan%20Marisca%20and%20Jacob%20Bamberger%20and%20Cesare%20Alippi%20and%20Michael%20M.%20Bronstein&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20remarkable%20success%20across%20various%0Adomains.%20However%2C%20recent%20theoretical%20advances%20have%20identified%20fundamental%0Alimitations%20in%20their%20information%20propagation%20capabilities%2C%20such%20as%0Aover-squashing%2C%20where%20distant%20nodes%20fail%20to%20effectively%20exchange%20information.%0AWhile%20extensively%20studied%20in%20static%20contexts%2C%20this%20issue%20remains%20unexplored%20in%0ASpatiotemporal%20GNNs%20%28STGNNs%29%2C%20which%20process%20sequences%20associated%20with%20graph%0Anodes.%20Nonetheless%2C%20the%20temporal%20dimension%20amplifies%20this%20challenge%20by%0Aincreasing%20the%20information%20that%20must%20be%20propagated.%20In%20this%20work%2C%20we%20formalize%0Athe%20spatiotemporal%20over-squashing%20problem%20and%20demonstrate%20its%20distinct%0Acharacteristics%20compared%20to%20the%20static%20case.%20Our%20analysis%20reveals%20that%0Acounterintuitively%2C%20convolutional%20STGNNs%20favor%20information%20propagation%20from%0Apoints%20temporally%20distant%20rather%20than%20close%20in%20time.%20Moreover%2C%20we%20prove%20that%0Aarchitectures%20that%20follow%20either%20time-and-space%20or%20time-then-space%20processing%0Aparadigms%20are%20equally%20affected%20by%20this%20phenomenon%2C%20providing%20theoretical%0Ajustification%20for%20computationally%20efficient%20implementations.%20We%20validate%20our%0Afindings%20on%20synthetic%20and%20real-world%20datasets%2C%20providing%20deeper%20insights%20into%0Atheir%20operational%20dynamics%20and%20principled%20guidance%20for%20more%20effective%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15507v1&entry.124074799=Read"},
{"title": "Interpretable representation learning of quantum data enabled by\n  probabilistic variational autoencoders", "author": "Paulin de Schoulepnikoff and Gorka Mu\u00f1oz-Gil and Hendrik Poulsen Nautrup and Hans J. Briegel", "abstract": "  Interpretable machine learning is rapidly becoming a crucial tool for\nscientific discovery. Among existing approaches, variational autoencoders\n(VAEs) have shown promise in extracting the hidden physical features of some\ninput data, with no supervision nor prior knowledge of the system at study.\nYet, the ability of VAEs to create meaningful, interpretable representations\nrelies on their accurate approximation of the underlying probability\ndistribution of their input. When dealing with quantum data, VAEs must hence\naccount for its intrinsic randomness and complex correlations. While VAEs have\nbeen previously applied to quantum data, they have often neglected its\nprobabilistic nature, hindering the extraction of meaningful physical\ndescriptors. Here, we demonstrate that two key modifications enable VAEs to\nlearn physically meaningful latent representations: a decoder capable of\nfaithfully reproduce quantum states and a probabilistic loss tailored to this\ntask. Using benchmark quantum spin models, we identify regimes where standard\nmethods fail while the representations learned by our approach remain\nmeaningful and interpretable. Applied to experimental data from Rydberg atom\narrays, the model autonomously uncovers the phase structure without access to\nprior labels, Hamiltonian details, or knowledge of relevant order parameters,\nhighlighting its potential as an unsupervised and interpretable tool for the\nstudy of quantum systems.\n", "link": "http://arxiv.org/abs/2506.11982v2", "date": "2025-06-18", "relevancy": 2.0418, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%0A%20%20probabilistic%20variational%20autoencoders&body=Title%3A%20Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%0A%20%20probabilistic%20variational%20autoencoders%0AAuthor%3A%20Paulin%20de%20Schoulepnikoff%20and%20Gorka%20Mu%C3%B1oz-Gil%20and%20Hendrik%20Poulsen%20Nautrup%20and%20Hans%20J.%20Briegel%0AAbstract%3A%20%20%20Interpretable%20machine%20learning%20is%20rapidly%20becoming%20a%20crucial%20tool%20for%0Ascientific%20discovery.%20Among%20existing%20approaches%2C%20variational%20autoencoders%0A%28VAEs%29%20have%20shown%20promise%20in%20extracting%20the%20hidden%20physical%20features%20of%20some%0Ainput%20data%2C%20with%20no%20supervision%20nor%20prior%20knowledge%20of%20the%20system%20at%20study.%0AYet%2C%20the%20ability%20of%20VAEs%20to%20create%20meaningful%2C%20interpretable%20representations%0Arelies%20on%20their%20accurate%20approximation%20of%20the%20underlying%20probability%0Adistribution%20of%20their%20input.%20When%20dealing%20with%20quantum%20data%2C%20VAEs%20must%20hence%0Aaccount%20for%20its%20intrinsic%20randomness%20and%20complex%20correlations.%20While%20VAEs%20have%0Abeen%20previously%20applied%20to%20quantum%20data%2C%20they%20have%20often%20neglected%20its%0Aprobabilistic%20nature%2C%20hindering%20the%20extraction%20of%20meaningful%20physical%0Adescriptors.%20Here%2C%20we%20demonstrate%20that%20two%20key%20modifications%20enable%20VAEs%20to%0Alearn%20physically%20meaningful%20latent%20representations%3A%20a%20decoder%20capable%20of%0Afaithfully%20reproduce%20quantum%20states%20and%20a%20probabilistic%20loss%20tailored%20to%20this%0Atask.%20Using%20benchmark%20quantum%20spin%20models%2C%20we%20identify%20regimes%20where%20standard%0Amethods%20fail%20while%20the%20representations%20learned%20by%20our%20approach%20remain%0Ameaningful%20and%20interpretable.%20Applied%20to%20experimental%20data%20from%20Rydberg%20atom%0Aarrays%2C%20the%20model%20autonomously%20uncovers%20the%20phase%20structure%20without%20access%20to%0Aprior%20labels%2C%20Hamiltonian%20details%2C%20or%20knowledge%20of%20relevant%20order%20parameters%2C%0Ahighlighting%20its%20potential%20as%20an%20unsupervised%20and%20interpretable%20tool%20for%20the%0Astudy%20of%20quantum%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520representation%2520learning%2520of%2520quantum%2520data%2520enabled%2520by%250A%2520%2520probabilistic%2520variational%2520autoencoders%26entry.906535625%3DPaulin%2520de%2520Schoulepnikoff%2520and%2520Gorka%2520Mu%25C3%25B1oz-Gil%2520and%2520Hendrik%2520Poulsen%2520Nautrup%2520and%2520Hans%2520J.%2520Briegel%26entry.1292438233%3D%2520%2520Interpretable%2520machine%2520learning%2520is%2520rapidly%2520becoming%2520a%2520crucial%2520tool%2520for%250Ascientific%2520discovery.%2520Among%2520existing%2520approaches%252C%2520variational%2520autoencoders%250A%2528VAEs%2529%2520have%2520shown%2520promise%2520in%2520extracting%2520the%2520hidden%2520physical%2520features%2520of%2520some%250Ainput%2520data%252C%2520with%2520no%2520supervision%2520nor%2520prior%2520knowledge%2520of%2520the%2520system%2520at%2520study.%250AYet%252C%2520the%2520ability%2520of%2520VAEs%2520to%2520create%2520meaningful%252C%2520interpretable%2520representations%250Arelies%2520on%2520their%2520accurate%2520approximation%2520of%2520the%2520underlying%2520probability%250Adistribution%2520of%2520their%2520input.%2520When%2520dealing%2520with%2520quantum%2520data%252C%2520VAEs%2520must%2520hence%250Aaccount%2520for%2520its%2520intrinsic%2520randomness%2520and%2520complex%2520correlations.%2520While%2520VAEs%2520have%250Abeen%2520previously%2520applied%2520to%2520quantum%2520data%252C%2520they%2520have%2520often%2520neglected%2520its%250Aprobabilistic%2520nature%252C%2520hindering%2520the%2520extraction%2520of%2520meaningful%2520physical%250Adescriptors.%2520Here%252C%2520we%2520demonstrate%2520that%2520two%2520key%2520modifications%2520enable%2520VAEs%2520to%250Alearn%2520physically%2520meaningful%2520latent%2520representations%253A%2520a%2520decoder%2520capable%2520of%250Afaithfully%2520reproduce%2520quantum%2520states%2520and%2520a%2520probabilistic%2520loss%2520tailored%2520to%2520this%250Atask.%2520Using%2520benchmark%2520quantum%2520spin%2520models%252C%2520we%2520identify%2520regimes%2520where%2520standard%250Amethods%2520fail%2520while%2520the%2520representations%2520learned%2520by%2520our%2520approach%2520remain%250Ameaningful%2520and%2520interpretable.%2520Applied%2520to%2520experimental%2520data%2520from%2520Rydberg%2520atom%250Aarrays%252C%2520the%2520model%2520autonomously%2520uncovers%2520the%2520phase%2520structure%2520without%2520access%2520to%250Aprior%2520labels%252C%2520Hamiltonian%2520details%252C%2520or%2520knowledge%2520of%2520relevant%2520order%2520parameters%252C%250Ahighlighting%2520its%2520potential%2520as%2520an%2520unsupervised%2520and%2520interpretable%2520tool%2520for%2520the%250Astudy%2520of%2520quantum%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20representation%20learning%20of%20quantum%20data%20enabled%20by%0A%20%20probabilistic%20variational%20autoencoders&entry.906535625=Paulin%20de%20Schoulepnikoff%20and%20Gorka%20Mu%C3%B1oz-Gil%20and%20Hendrik%20Poulsen%20Nautrup%20and%20Hans%20J.%20Briegel&entry.1292438233=%20%20Interpretable%20machine%20learning%20is%20rapidly%20becoming%20a%20crucial%20tool%20for%0Ascientific%20discovery.%20Among%20existing%20approaches%2C%20variational%20autoencoders%0A%28VAEs%29%20have%20shown%20promise%20in%20extracting%20the%20hidden%20physical%20features%20of%20some%0Ainput%20data%2C%20with%20no%20supervision%20nor%20prior%20knowledge%20of%20the%20system%20at%20study.%0AYet%2C%20the%20ability%20of%20VAEs%20to%20create%20meaningful%2C%20interpretable%20representations%0Arelies%20on%20their%20accurate%20approximation%20of%20the%20underlying%20probability%0Adistribution%20of%20their%20input.%20When%20dealing%20with%20quantum%20data%2C%20VAEs%20must%20hence%0Aaccount%20for%20its%20intrinsic%20randomness%20and%20complex%20correlations.%20While%20VAEs%20have%0Abeen%20previously%20applied%20to%20quantum%20data%2C%20they%20have%20often%20neglected%20its%0Aprobabilistic%20nature%2C%20hindering%20the%20extraction%20of%20meaningful%20physical%0Adescriptors.%20Here%2C%20we%20demonstrate%20that%20two%20key%20modifications%20enable%20VAEs%20to%0Alearn%20physically%20meaningful%20latent%20representations%3A%20a%20decoder%20capable%20of%0Afaithfully%20reproduce%20quantum%20states%20and%20a%20probabilistic%20loss%20tailored%20to%20this%0Atask.%20Using%20benchmark%20quantum%20spin%20models%2C%20we%20identify%20regimes%20where%20standard%0Amethods%20fail%20while%20the%20representations%20learned%20by%20our%20approach%20remain%0Ameaningful%20and%20interpretable.%20Applied%20to%20experimental%20data%20from%20Rydberg%20atom%0Aarrays%2C%20the%20model%20autonomously%20uncovers%20the%20phase%20structure%20without%20access%20to%0Aprior%20labels%2C%20Hamiltonian%20details%2C%20or%20knowledge%20of%20relevant%20order%20parameters%2C%0Ahighlighting%20its%20potential%20as%20an%20unsupervised%20and%20interpretable%20tool%20for%20the%0Astudy%20of%20quantum%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11982v2&entry.124074799=Read"},
{"title": "Alternating Regret for Online Convex Optimization", "author": "Soumita Hait and Ping Li and Haipeng Luo and Mengxiao Zhang", "abstract": "  Motivated by alternating learning dynamics in two-player games, a recent work\nby Cevher et al.(2024) shows that $o(\\sqrt{T})$ alternating regret is possible\nfor any $T$-round adversarial Online Linear Optimization (OLO) problem, and\nleft as an open question whether the same is true for general Online Convex\nOptimization (OCO). We answer this question in the affirmative by showing that\nthe continuous Hedge algorithm achieves\n$\\tilde{\\mathcal{O}}(d^{\\frac{2}{3}}T^{\\frac{1}{3}})$ alternating regret for\nany adversarial $d$-dimensional OCO problems. We show that this implies an\nalternating learning dynamic that finds a Nash equilibrium for any\nconvex-concave zero-sum games or a coarse correlated equilibrium for any convex\ntwo-player general-sum games at a rate of\n$\\tilde{\\mathcal{O}}(d^{\\frac{2}{3}}/T^{\\frac{2}{3}})$. To further improve the\ntime complexity and/or the dimension dependence, we propose another simple\nalgorithm, Follow-the-Regularized-Leader with a regularizer whose convex\nconjugate is 3rd-order smooth, for OCO with smooth and self-concordant loss\nfunctions (such as linear or quadratic losses). We instantiate our algorithm\nwith different regularizers and show that, for example, when the decision set\nis the $\\ell_2$ ball, our algorithm achieves\n$\\tilde{\\mathcal{O}}(T^{\\frac{2}{5}})$ alternating regret with no dimension\ndependence (and a better $\\tilde{\\mathcal{O}}(T^{\\frac{1}{3}})$ bound for\nquadratic losses). We complement our results by showing some algorithm-specific\nalternating regret lower bounds, including a somewhat surprising\n$\\Omega(\\sqrt{T})$ lower bound for a Regret Matching variant that is widely\nused in alternating learning dynamics.\n", "link": "http://arxiv.org/abs/2502.12529v2", "date": "2025-06-18", "relevancy": 2.0344, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4137}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4045}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alternating%20Regret%20for%20Online%20Convex%20Optimization&body=Title%3A%20Alternating%20Regret%20for%20Online%20Convex%20Optimization%0AAuthor%3A%20Soumita%20Hait%20and%20Ping%20Li%20and%20Haipeng%20Luo%20and%20Mengxiao%20Zhang%0AAbstract%3A%20%20%20Motivated%20by%20alternating%20learning%20dynamics%20in%20two-player%20games%2C%20a%20recent%20work%0Aby%20Cevher%20et%20al.%282024%29%20shows%20that%20%24o%28%5Csqrt%7BT%7D%29%24%20alternating%20regret%20is%20possible%0Afor%20any%20%24T%24-round%20adversarial%20Online%20Linear%20Optimization%20%28OLO%29%20problem%2C%20and%0Aleft%20as%20an%20open%20question%20whether%20the%20same%20is%20true%20for%20general%20Online%20Convex%0AOptimization%20%28OCO%29.%20We%20answer%20this%20question%20in%20the%20affirmative%20by%20showing%20that%0Athe%20continuous%20Hedge%20algorithm%20achieves%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28d%5E%7B%5Cfrac%7B2%7D%7B3%7D%7DT%5E%7B%5Cfrac%7B1%7D%7B3%7D%7D%29%24%20alternating%20regret%20for%0Aany%20adversarial%20%24d%24-dimensional%20OCO%20problems.%20We%20show%20that%20this%20implies%20an%0Aalternating%20learning%20dynamic%20that%20finds%20a%20Nash%20equilibrium%20for%20any%0Aconvex-concave%20zero-sum%20games%20or%20a%20coarse%20correlated%20equilibrium%20for%20any%20convex%0Atwo-player%20general-sum%20games%20at%20a%20rate%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28d%5E%7B%5Cfrac%7B2%7D%7B3%7D%7D/T%5E%7B%5Cfrac%7B2%7D%7B3%7D%7D%29%24.%20To%20further%20improve%20the%0Atime%20complexity%20and/or%20the%20dimension%20dependence%2C%20we%20propose%20another%20simple%0Aalgorithm%2C%20Follow-the-Regularized-Leader%20with%20a%20regularizer%20whose%20convex%0Aconjugate%20is%203rd-order%20smooth%2C%20for%20OCO%20with%20smooth%20and%20self-concordant%20loss%0Afunctions%20%28such%20as%20linear%20or%20quadratic%20losses%29.%20We%20instantiate%20our%20algorithm%0Awith%20different%20regularizers%20and%20show%20that%2C%20for%20example%2C%20when%20the%20decision%20set%0Ais%20the%20%24%5Cell_2%24%20ball%2C%20our%20algorithm%20achieves%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B%5Cfrac%7B2%7D%7B5%7D%7D%29%24%20alternating%20regret%20with%20no%20dimension%0Adependence%20%28and%20a%20better%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B%5Cfrac%7B1%7D%7B3%7D%7D%29%24%20bound%20for%0Aquadratic%20losses%29.%20We%20complement%20our%20results%20by%20showing%20some%20algorithm-specific%0Aalternating%20regret%20lower%20bounds%2C%20including%20a%20somewhat%20surprising%0A%24%5COmega%28%5Csqrt%7BT%7D%29%24%20lower%20bound%20for%20a%20Regret%20Matching%20variant%20that%20is%20widely%0Aused%20in%20alternating%20learning%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlternating%2520Regret%2520for%2520Online%2520Convex%2520Optimization%26entry.906535625%3DSoumita%2520Hait%2520and%2520Ping%2520Li%2520and%2520Haipeng%2520Luo%2520and%2520Mengxiao%2520Zhang%26entry.1292438233%3D%2520%2520Motivated%2520by%2520alternating%2520learning%2520dynamics%2520in%2520two-player%2520games%252C%2520a%2520recent%2520work%250Aby%2520Cevher%2520et%2520al.%25282024%2529%2520shows%2520that%2520%2524o%2528%255Csqrt%257BT%257D%2529%2524%2520alternating%2520regret%2520is%2520possible%250Afor%2520any%2520%2524T%2524-round%2520adversarial%2520Online%2520Linear%2520Optimization%2520%2528OLO%2529%2520problem%252C%2520and%250Aleft%2520as%2520an%2520open%2520question%2520whether%2520the%2520same%2520is%2520true%2520for%2520general%2520Online%2520Convex%250AOptimization%2520%2528OCO%2529.%2520We%2520answer%2520this%2520question%2520in%2520the%2520affirmative%2520by%2520showing%2520that%250Athe%2520continuous%2520Hedge%2520algorithm%2520achieves%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528d%255E%257B%255Cfrac%257B2%257D%257B3%257D%257DT%255E%257B%255Cfrac%257B1%257D%257B3%257D%257D%2529%2524%2520alternating%2520regret%2520for%250Aany%2520adversarial%2520%2524d%2524-dimensional%2520OCO%2520problems.%2520We%2520show%2520that%2520this%2520implies%2520an%250Aalternating%2520learning%2520dynamic%2520that%2520finds%2520a%2520Nash%2520equilibrium%2520for%2520any%250Aconvex-concave%2520zero-sum%2520games%2520or%2520a%2520coarse%2520correlated%2520equilibrium%2520for%2520any%2520convex%250Atwo-player%2520general-sum%2520games%2520at%2520a%2520rate%2520of%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528d%255E%257B%255Cfrac%257B2%257D%257B3%257D%257D/T%255E%257B%255Cfrac%257B2%257D%257B3%257D%257D%2529%2524.%2520To%2520further%2520improve%2520the%250Atime%2520complexity%2520and/or%2520the%2520dimension%2520dependence%252C%2520we%2520propose%2520another%2520simple%250Aalgorithm%252C%2520Follow-the-Regularized-Leader%2520with%2520a%2520regularizer%2520whose%2520convex%250Aconjugate%2520is%25203rd-order%2520smooth%252C%2520for%2520OCO%2520with%2520smooth%2520and%2520self-concordant%2520loss%250Afunctions%2520%2528such%2520as%2520linear%2520or%2520quadratic%2520losses%2529.%2520We%2520instantiate%2520our%2520algorithm%250Awith%2520different%2520regularizers%2520and%2520show%2520that%252C%2520for%2520example%252C%2520when%2520the%2520decision%2520set%250Ais%2520the%2520%2524%255Cell_2%2524%2520ball%252C%2520our%2520algorithm%2520achieves%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528T%255E%257B%255Cfrac%257B2%257D%257B5%257D%257D%2529%2524%2520alternating%2520regret%2520with%2520no%2520dimension%250Adependence%2520%2528and%2520a%2520better%2520%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528T%255E%257B%255Cfrac%257B1%257D%257B3%257D%257D%2529%2524%2520bound%2520for%250Aquadratic%2520losses%2529.%2520We%2520complement%2520our%2520results%2520by%2520showing%2520some%2520algorithm-specific%250Aalternating%2520regret%2520lower%2520bounds%252C%2520including%2520a%2520somewhat%2520surprising%250A%2524%255COmega%2528%255Csqrt%257BT%257D%2529%2524%2520lower%2520bound%2520for%2520a%2520Regret%2520Matching%2520variant%2520that%2520is%2520widely%250Aused%2520in%2520alternating%2520learning%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alternating%20Regret%20for%20Online%20Convex%20Optimization&entry.906535625=Soumita%20Hait%20and%20Ping%20Li%20and%20Haipeng%20Luo%20and%20Mengxiao%20Zhang&entry.1292438233=%20%20Motivated%20by%20alternating%20learning%20dynamics%20in%20two-player%20games%2C%20a%20recent%20work%0Aby%20Cevher%20et%20al.%282024%29%20shows%20that%20%24o%28%5Csqrt%7BT%7D%29%24%20alternating%20regret%20is%20possible%0Afor%20any%20%24T%24-round%20adversarial%20Online%20Linear%20Optimization%20%28OLO%29%20problem%2C%20and%0Aleft%20as%20an%20open%20question%20whether%20the%20same%20is%20true%20for%20general%20Online%20Convex%0AOptimization%20%28OCO%29.%20We%20answer%20this%20question%20in%20the%20affirmative%20by%20showing%20that%0Athe%20continuous%20Hedge%20algorithm%20achieves%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28d%5E%7B%5Cfrac%7B2%7D%7B3%7D%7DT%5E%7B%5Cfrac%7B1%7D%7B3%7D%7D%29%24%20alternating%20regret%20for%0Aany%20adversarial%20%24d%24-dimensional%20OCO%20problems.%20We%20show%20that%20this%20implies%20an%0Aalternating%20learning%20dynamic%20that%20finds%20a%20Nash%20equilibrium%20for%20any%0Aconvex-concave%20zero-sum%20games%20or%20a%20coarse%20correlated%20equilibrium%20for%20any%20convex%0Atwo-player%20general-sum%20games%20at%20a%20rate%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28d%5E%7B%5Cfrac%7B2%7D%7B3%7D%7D/T%5E%7B%5Cfrac%7B2%7D%7B3%7D%7D%29%24.%20To%20further%20improve%20the%0Atime%20complexity%20and/or%20the%20dimension%20dependence%2C%20we%20propose%20another%20simple%0Aalgorithm%2C%20Follow-the-Regularized-Leader%20with%20a%20regularizer%20whose%20convex%0Aconjugate%20is%203rd-order%20smooth%2C%20for%20OCO%20with%20smooth%20and%20self-concordant%20loss%0Afunctions%20%28such%20as%20linear%20or%20quadratic%20losses%29.%20We%20instantiate%20our%20algorithm%0Awith%20different%20regularizers%20and%20show%20that%2C%20for%20example%2C%20when%20the%20decision%20set%0Ais%20the%20%24%5Cell_2%24%20ball%2C%20our%20algorithm%20achieves%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B%5Cfrac%7B2%7D%7B5%7D%7D%29%24%20alternating%20regret%20with%20no%20dimension%0Adependence%20%28and%20a%20better%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B%5Cfrac%7B1%7D%7B3%7D%7D%29%24%20bound%20for%0Aquadratic%20losses%29.%20We%20complement%20our%20results%20by%20showing%20some%20algorithm-specific%0Aalternating%20regret%20lower%20bounds%2C%20including%20a%20somewhat%20surprising%0A%24%5COmega%28%5Csqrt%7BT%7D%29%24%20lower%20bound%20for%20a%20Regret%20Matching%20variant%20that%20is%20widely%0Aused%20in%20alternating%20learning%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12529v2&entry.124074799=Read"},
{"title": "Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework", "author": "Xia Du and Xiaoyuan Liu and Jizhe Zhou and Zheng Lin and Chi-man Pun and Cong Wu and Tao Li and Zhe Chen and Wei Ni and Jun Luo", "abstract": "  With the rapid advancements in deep learning, traditional CAPTCHA schemes are\nincreasingly vulnerable to automated attacks powered by deep neural networks\n(DNNs). Existing adversarial attack methods often rely on original image\ncharacteristics, resulting in distortions that hinder human interpretation and\nlimit applicability in scenarios lacking initial input images. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel\nframework generating high-fidelity adversarial examples guided by\nattacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC\nenhances CAPTCHA diversity and supports both targeted and untargeted attacks.\nFor targeted attacks, the EDICT method optimizes dual latent variables in a\ndiffusion model for superior image quality. In untargeted attacks, especially\nfor black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-UAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show BP-UAC\nachieves high attack success rates across diverse systems, generating natural\nCAPTCHAs indistinguishable to humans and DNNs.\n", "link": "http://arxiv.org/abs/2506.10685v2", "date": "2025-06-18", "relevancy": 2.033, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5145}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5087}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsourced%20Adversarial%20CAPTCHA%3A%20A%20Bi-Phase%20Adversarial%20CAPTCHA%20Framework&body=Title%3A%20Unsourced%20Adversarial%20CAPTCHA%3A%20A%20Bi-Phase%20Adversarial%20CAPTCHA%20Framework%0AAuthor%3A%20Xia%20Du%20and%20Xiaoyuan%20Liu%20and%20Jizhe%20Zhou%20and%20Zheng%20Lin%20and%20Chi-man%20Pun%20and%20Cong%20Wu%20and%20Tao%20Li%20and%20Zhe%20Chen%20and%20Wei%20Ni%20and%20Jun%20Luo%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20in%20deep%20learning%2C%20traditional%20CAPTCHA%20schemes%20are%0Aincreasingly%20vulnerable%20to%20automated%20attacks%20powered%20by%20deep%20neural%20networks%0A%28DNNs%29.%20Existing%20adversarial%20attack%20methods%20often%20rely%20on%20original%20image%0Acharacteristics%2C%20resulting%20in%20distortions%20that%20hinder%20human%20interpretation%20and%0Alimit%20applicability%20in%20scenarios%20lacking%20initial%20input%20images.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Unsourced%20Adversarial%20CAPTCHA%20%28UAC%29%2C%20a%20novel%0Aframework%20generating%20high-fidelity%20adversarial%20examples%20guided%20by%0Aattacker-specified%20text%20prompts.%20Leveraging%20a%20Large%20Language%20Model%20%28LLM%29%2C%20UAC%0Aenhances%20CAPTCHA%20diversity%20and%20supports%20both%20targeted%20and%20untargeted%20attacks.%0AFor%20targeted%20attacks%2C%20the%20EDICT%20method%20optimizes%20dual%20latent%20variables%20in%20a%0Adiffusion%20model%20for%20superior%20image%20quality.%20In%20untargeted%20attacks%2C%20especially%0Afor%20black-box%20scenarios%2C%20we%20introduce%20bi-path%20unsourced%20adversarial%20CAPTCHA%0A%28BP-UAC%29%2C%20a%20two-step%20optimization%20strategy%20employing%20multimodal%20gradients%20and%0Abi-path%20optimization%20for%20efficient%20misclassification.%20Experiments%20show%20BP-UAC%0Aachieves%20high%20attack%20success%20rates%20across%20diverse%20systems%2C%20generating%20natural%0ACAPTCHAs%20indistinguishable%20to%20humans%20and%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10685v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsourced%2520Adversarial%2520CAPTCHA%253A%2520A%2520Bi-Phase%2520Adversarial%2520CAPTCHA%2520Framework%26entry.906535625%3DXia%2520Du%2520and%2520Xiaoyuan%2520Liu%2520and%2520Jizhe%2520Zhou%2520and%2520Zheng%2520Lin%2520and%2520Chi-man%2520Pun%2520and%2520Cong%2520Wu%2520and%2520Tao%2520Li%2520and%2520Zhe%2520Chen%2520and%2520Wei%2520Ni%2520and%2520Jun%2520Luo%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancements%2520in%2520deep%2520learning%252C%2520traditional%2520CAPTCHA%2520schemes%2520are%250Aincreasingly%2520vulnerable%2520to%2520automated%2520attacks%2520powered%2520by%2520deep%2520neural%2520networks%250A%2528DNNs%2529.%2520Existing%2520adversarial%2520attack%2520methods%2520often%2520rely%2520on%2520original%2520image%250Acharacteristics%252C%2520resulting%2520in%2520distortions%2520that%2520hinder%2520human%2520interpretation%2520and%250Alimit%2520applicability%2520in%2520scenarios%2520lacking%2520initial%2520input%2520images.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520the%2520Unsourced%2520Adversarial%2520CAPTCHA%2520%2528UAC%2529%252C%2520a%2520novel%250Aframework%2520generating%2520high-fidelity%2520adversarial%2520examples%2520guided%2520by%250Aattacker-specified%2520text%2520prompts.%2520Leveraging%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%252C%2520UAC%250Aenhances%2520CAPTCHA%2520diversity%2520and%2520supports%2520both%2520targeted%2520and%2520untargeted%2520attacks.%250AFor%2520targeted%2520attacks%252C%2520the%2520EDICT%2520method%2520optimizes%2520dual%2520latent%2520variables%2520in%2520a%250Adiffusion%2520model%2520for%2520superior%2520image%2520quality.%2520In%2520untargeted%2520attacks%252C%2520especially%250Afor%2520black-box%2520scenarios%252C%2520we%2520introduce%2520bi-path%2520unsourced%2520adversarial%2520CAPTCHA%250A%2528BP-UAC%2529%252C%2520a%2520two-step%2520optimization%2520strategy%2520employing%2520multimodal%2520gradients%2520and%250Abi-path%2520optimization%2520for%2520efficient%2520misclassification.%2520Experiments%2520show%2520BP-UAC%250Aachieves%2520high%2520attack%2520success%2520rates%2520across%2520diverse%2520systems%252C%2520generating%2520natural%250ACAPTCHAs%2520indistinguishable%2520to%2520humans%2520and%2520DNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10685v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsourced%20Adversarial%20CAPTCHA%3A%20A%20Bi-Phase%20Adversarial%20CAPTCHA%20Framework&entry.906535625=Xia%20Du%20and%20Xiaoyuan%20Liu%20and%20Jizhe%20Zhou%20and%20Zheng%20Lin%20and%20Chi-man%20Pun%20and%20Cong%20Wu%20and%20Tao%20Li%20and%20Zhe%20Chen%20and%20Wei%20Ni%20and%20Jun%20Luo&entry.1292438233=%20%20With%20the%20rapid%20advancements%20in%20deep%20learning%2C%20traditional%20CAPTCHA%20schemes%20are%0Aincreasingly%20vulnerable%20to%20automated%20attacks%20powered%20by%20deep%20neural%20networks%0A%28DNNs%29.%20Existing%20adversarial%20attack%20methods%20often%20rely%20on%20original%20image%0Acharacteristics%2C%20resulting%20in%20distortions%20that%20hinder%20human%20interpretation%20and%0Alimit%20applicability%20in%20scenarios%20lacking%20initial%20input%20images.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20Unsourced%20Adversarial%20CAPTCHA%20%28UAC%29%2C%20a%20novel%0Aframework%20generating%20high-fidelity%20adversarial%20examples%20guided%20by%0Aattacker-specified%20text%20prompts.%20Leveraging%20a%20Large%20Language%20Model%20%28LLM%29%2C%20UAC%0Aenhances%20CAPTCHA%20diversity%20and%20supports%20both%20targeted%20and%20untargeted%20attacks.%0AFor%20targeted%20attacks%2C%20the%20EDICT%20method%20optimizes%20dual%20latent%20variables%20in%20a%0Adiffusion%20model%20for%20superior%20image%20quality.%20In%20untargeted%20attacks%2C%20especially%0Afor%20black-box%20scenarios%2C%20we%20introduce%20bi-path%20unsourced%20adversarial%20CAPTCHA%0A%28BP-UAC%29%2C%20a%20two-step%20optimization%20strategy%20employing%20multimodal%20gradients%20and%0Abi-path%20optimization%20for%20efficient%20misclassification.%20Experiments%20show%20BP-UAC%0Aachieves%20high%20attack%20success%20rates%20across%20diverse%20systems%2C%20generating%20natural%0ACAPTCHAs%20indistinguishable%20to%20humans%20and%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10685v2&entry.124074799=Read"},
{"title": "An Advanced Framework for Ultra-Realistic Simulation and Digital\n  Twinning for Autonomous Vehicles", "author": "Yuankai He and Hanlin Chen and Weisong Shi", "abstract": "  Simulation is a fundamental tool in developing autonomous vehicles, enabling\nrigorous testing without the logistical and safety challenges associated with\nreal-world trials. As autonomous vehicle technologies evolve and public safety\ndemands increase, advanced, realistic simulation frameworks are critical.\nCurrent testing paradigms employ a mix of general-purpose and specialized\nsimulators, such as CARLA and IVRESS, to achieve high-fidelity results.\nHowever, these tools often struggle with compatibility due to differing\nplatform, hardware, and software requirements, severely hampering their\ncombined effectiveness. This paper introduces BlueICE, an advanced framework\nfor ultra-realistic simulation and digital twinning, to address these\nchallenges. BlueICE's innovative architecture allows for the decoupling of\ncomputing platforms, hardware, and software dependencies while offering\nresearchers customizable testing environments to meet diverse fidelity needs.\nKey features include containerization to ensure compatibility across different\nsystems, a unified communication bridge for seamless integration of various\nsimulation tools, and synchronized orchestration of input and output across\nsimulators. This framework facilitates the development of sophisticated digital\ntwins for autonomous vehicle testing and sets a new standard in simulation\naccuracy and flexibility. The paper further explores the application of BlueICE\nin two distinct case studies: the ICAT indoor testbed and the STAR campus\noutdoor testbed at the University of Delaware. These case studies demonstrate\nBlueICE's capability to create sophisticated digital twins for autonomous\nvehicle testing and underline its potential as a standardized testbed for\nfuture autonomous driving technologies.\n", "link": "http://arxiv.org/abs/2405.01328v2", "date": "2025-06-18", "relevancy": 2.0322, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5061}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Advanced%20Framework%20for%20Ultra-Realistic%20Simulation%20and%20Digital%0A%20%20Twinning%20for%20Autonomous%20Vehicles&body=Title%3A%20An%20Advanced%20Framework%20for%20Ultra-Realistic%20Simulation%20and%20Digital%0A%20%20Twinning%20for%20Autonomous%20Vehicles%0AAuthor%3A%20Yuankai%20He%20and%20Hanlin%20Chen%20and%20Weisong%20Shi%0AAbstract%3A%20%20%20Simulation%20is%20a%20fundamental%20tool%20in%20developing%20autonomous%20vehicles%2C%20enabling%0Arigorous%20testing%20without%20the%20logistical%20and%20safety%20challenges%20associated%20with%0Areal-world%20trials.%20As%20autonomous%20vehicle%20technologies%20evolve%20and%20public%20safety%0Ademands%20increase%2C%20advanced%2C%20realistic%20simulation%20frameworks%20are%20critical.%0ACurrent%20testing%20paradigms%20employ%20a%20mix%20of%20general-purpose%20and%20specialized%0Asimulators%2C%20such%20as%20CARLA%20and%20IVRESS%2C%20to%20achieve%20high-fidelity%20results.%0AHowever%2C%20these%20tools%20often%20struggle%20with%20compatibility%20due%20to%20differing%0Aplatform%2C%20hardware%2C%20and%20software%20requirements%2C%20severely%20hampering%20their%0Acombined%20effectiveness.%20This%20paper%20introduces%20BlueICE%2C%20an%20advanced%20framework%0Afor%20ultra-realistic%20simulation%20and%20digital%20twinning%2C%20to%20address%20these%0Achallenges.%20BlueICE%27s%20innovative%20architecture%20allows%20for%20the%20decoupling%20of%0Acomputing%20platforms%2C%20hardware%2C%20and%20software%20dependencies%20while%20offering%0Aresearchers%20customizable%20testing%20environments%20to%20meet%20diverse%20fidelity%20needs.%0AKey%20features%20include%20containerization%20to%20ensure%20compatibility%20across%20different%0Asystems%2C%20a%20unified%20communication%20bridge%20for%20seamless%20integration%20of%20various%0Asimulation%20tools%2C%20and%20synchronized%20orchestration%20of%20input%20and%20output%20across%0Asimulators.%20This%20framework%20facilitates%20the%20development%20of%20sophisticated%20digital%0Atwins%20for%20autonomous%20vehicle%20testing%20and%20sets%20a%20new%20standard%20in%20simulation%0Aaccuracy%20and%20flexibility.%20The%20paper%20further%20explores%20the%20application%20of%20BlueICE%0Ain%20two%20distinct%20case%20studies%3A%20the%20ICAT%20indoor%20testbed%20and%20the%20STAR%20campus%0Aoutdoor%20testbed%20at%20the%20University%20of%20Delaware.%20These%20case%20studies%20demonstrate%0ABlueICE%27s%20capability%20to%20create%20sophisticated%20digital%20twins%20for%20autonomous%0Avehicle%20testing%20and%20underline%20its%20potential%20as%20a%20standardized%20testbed%20for%0Afuture%20autonomous%20driving%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01328v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Advanced%2520Framework%2520for%2520Ultra-Realistic%2520Simulation%2520and%2520Digital%250A%2520%2520Twinning%2520for%2520Autonomous%2520Vehicles%26entry.906535625%3DYuankai%2520He%2520and%2520Hanlin%2520Chen%2520and%2520Weisong%2520Shi%26entry.1292438233%3D%2520%2520Simulation%2520is%2520a%2520fundamental%2520tool%2520in%2520developing%2520autonomous%2520vehicles%252C%2520enabling%250Arigorous%2520testing%2520without%2520the%2520logistical%2520and%2520safety%2520challenges%2520associated%2520with%250Areal-world%2520trials.%2520As%2520autonomous%2520vehicle%2520technologies%2520evolve%2520and%2520public%2520safety%250Ademands%2520increase%252C%2520advanced%252C%2520realistic%2520simulation%2520frameworks%2520are%2520critical.%250ACurrent%2520testing%2520paradigms%2520employ%2520a%2520mix%2520of%2520general-purpose%2520and%2520specialized%250Asimulators%252C%2520such%2520as%2520CARLA%2520and%2520IVRESS%252C%2520to%2520achieve%2520high-fidelity%2520results.%250AHowever%252C%2520these%2520tools%2520often%2520struggle%2520with%2520compatibility%2520due%2520to%2520differing%250Aplatform%252C%2520hardware%252C%2520and%2520software%2520requirements%252C%2520severely%2520hampering%2520their%250Acombined%2520effectiveness.%2520This%2520paper%2520introduces%2520BlueICE%252C%2520an%2520advanced%2520framework%250Afor%2520ultra-realistic%2520simulation%2520and%2520digital%2520twinning%252C%2520to%2520address%2520these%250Achallenges.%2520BlueICE%2527s%2520innovative%2520architecture%2520allows%2520for%2520the%2520decoupling%2520of%250Acomputing%2520platforms%252C%2520hardware%252C%2520and%2520software%2520dependencies%2520while%2520offering%250Aresearchers%2520customizable%2520testing%2520environments%2520to%2520meet%2520diverse%2520fidelity%2520needs.%250AKey%2520features%2520include%2520containerization%2520to%2520ensure%2520compatibility%2520across%2520different%250Asystems%252C%2520a%2520unified%2520communication%2520bridge%2520for%2520seamless%2520integration%2520of%2520various%250Asimulation%2520tools%252C%2520and%2520synchronized%2520orchestration%2520of%2520input%2520and%2520output%2520across%250Asimulators.%2520This%2520framework%2520facilitates%2520the%2520development%2520of%2520sophisticated%2520digital%250Atwins%2520for%2520autonomous%2520vehicle%2520testing%2520and%2520sets%2520a%2520new%2520standard%2520in%2520simulation%250Aaccuracy%2520and%2520flexibility.%2520The%2520paper%2520further%2520explores%2520the%2520application%2520of%2520BlueICE%250Ain%2520two%2520distinct%2520case%2520studies%253A%2520the%2520ICAT%2520indoor%2520testbed%2520and%2520the%2520STAR%2520campus%250Aoutdoor%2520testbed%2520at%2520the%2520University%2520of%2520Delaware.%2520These%2520case%2520studies%2520demonstrate%250ABlueICE%2527s%2520capability%2520to%2520create%2520sophisticated%2520digital%2520twins%2520for%2520autonomous%250Avehicle%2520testing%2520and%2520underline%2520its%2520potential%2520as%2520a%2520standardized%2520testbed%2520for%250Afuture%2520autonomous%2520driving%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01328v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Advanced%20Framework%20for%20Ultra-Realistic%20Simulation%20and%20Digital%0A%20%20Twinning%20for%20Autonomous%20Vehicles&entry.906535625=Yuankai%20He%20and%20Hanlin%20Chen%20and%20Weisong%20Shi&entry.1292438233=%20%20Simulation%20is%20a%20fundamental%20tool%20in%20developing%20autonomous%20vehicles%2C%20enabling%0Arigorous%20testing%20without%20the%20logistical%20and%20safety%20challenges%20associated%20with%0Areal-world%20trials.%20As%20autonomous%20vehicle%20technologies%20evolve%20and%20public%20safety%0Ademands%20increase%2C%20advanced%2C%20realistic%20simulation%20frameworks%20are%20critical.%0ACurrent%20testing%20paradigms%20employ%20a%20mix%20of%20general-purpose%20and%20specialized%0Asimulators%2C%20such%20as%20CARLA%20and%20IVRESS%2C%20to%20achieve%20high-fidelity%20results.%0AHowever%2C%20these%20tools%20often%20struggle%20with%20compatibility%20due%20to%20differing%0Aplatform%2C%20hardware%2C%20and%20software%20requirements%2C%20severely%20hampering%20their%0Acombined%20effectiveness.%20This%20paper%20introduces%20BlueICE%2C%20an%20advanced%20framework%0Afor%20ultra-realistic%20simulation%20and%20digital%20twinning%2C%20to%20address%20these%0Achallenges.%20BlueICE%27s%20innovative%20architecture%20allows%20for%20the%20decoupling%20of%0Acomputing%20platforms%2C%20hardware%2C%20and%20software%20dependencies%20while%20offering%0Aresearchers%20customizable%20testing%20environments%20to%20meet%20diverse%20fidelity%20needs.%0AKey%20features%20include%20containerization%20to%20ensure%20compatibility%20across%20different%0Asystems%2C%20a%20unified%20communication%20bridge%20for%20seamless%20integration%20of%20various%0Asimulation%20tools%2C%20and%20synchronized%20orchestration%20of%20input%20and%20output%20across%0Asimulators.%20This%20framework%20facilitates%20the%20development%20of%20sophisticated%20digital%0Atwins%20for%20autonomous%20vehicle%20testing%20and%20sets%20a%20new%20standard%20in%20simulation%0Aaccuracy%20and%20flexibility.%20The%20paper%20further%20explores%20the%20application%20of%20BlueICE%0Ain%20two%20distinct%20case%20studies%3A%20the%20ICAT%20indoor%20testbed%20and%20the%20STAR%20campus%0Aoutdoor%20testbed%20at%20the%20University%20of%20Delaware.%20These%20case%20studies%20demonstrate%0ABlueICE%27s%20capability%20to%20create%20sophisticated%20digital%20twins%20for%20autonomous%0Avehicle%20testing%20and%20underline%20its%20potential%20as%20a%20standardized%20testbed%20for%0Afuture%20autonomous%20driving%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01328v2&entry.124074799=Read"},
{"title": "Time-dependent density estimation using binary classifiers", "author": "Agnimitra Dasgupta and Javier Murgoitio-Esandi and Ali Fardisi and Assad A Oberai", "abstract": "  We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.\n", "link": "http://arxiv.org/abs/2506.15505v1", "date": "2025-06-18", "relevancy": 2.0299, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.533}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5032}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-dependent%20density%20estimation%20using%20binary%20classifiers&body=Title%3A%20Time-dependent%20density%20estimation%20using%20binary%20classifiers%0AAuthor%3A%20Agnimitra%20Dasgupta%20and%20Javier%20Murgoitio-Esandi%20and%20Ali%20Fardisi%20and%20Assad%20A%20Oberai%0AAbstract%3A%20%20%20We%20propose%20a%20data-driven%20method%20to%20learn%20the%20time-dependent%20probability%0Adensity%20of%20a%20multivariate%20stochastic%20process%20from%20sample%20paths%2C%20assuming%20that%0Athe%20initial%20probability%20density%20is%20known%20and%20can%20be%20evaluated.%20Our%20method%20uses%0Aa%20novel%20time-dependent%20binary%20classifier%20trained%20using%20a%20contrastive%0Aestimation-based%20objective%20that%20trains%20the%20classifier%20to%20discriminate%20between%0Arealizations%20of%20the%20stochastic%20process%20at%20two%20nearby%20time%20instants.%0ASignificantly%2C%20the%20proposed%20method%20explicitly%20models%20the%20time-dependent%0Aprobability%20distribution%2C%20which%20means%20that%20it%20is%20possible%20to%20obtain%20the%20value%0Aof%20the%20probability%20density%20within%20the%20time%20horizon%20of%20interest.%20Additionally%2C%0Athe%20input%20before%20the%20final%20activation%20in%20the%20time-dependent%20classifier%20is%20a%0Asecond-order%20approximation%20to%20the%20partial%20derivative%2C%20with%20respect%20to%20time%2C%20of%0Athe%20logarithm%20of%20the%20density.%20We%20apply%20the%20proposed%20approach%20to%20approximate%20the%0Atime-dependent%20probability%20density%20functions%20for%20systems%20driven%20by%20stochastic%0Aexcitations.%20We%20also%20use%20the%20proposed%20approach%20to%20synthesize%20new%20samples%20of%20a%0Arandom%20vector%20from%20a%20given%20set%20of%20its%20realizations.%20In%20such%20applications%2C%20we%0Agenerate%20sample%20paths%20necessary%20for%20training%20using%20stochastic%20interpolants.%0ASubsequently%2C%20new%20samples%20are%20generated%20using%20gradient-based%20Markov%20chain%20Monte%0ACarlo%20methods%20because%20automatic%20differentiation%20can%20efficiently%20provide%20the%0Anecessary%20gradient.%20Further%2C%20we%20demonstrate%20the%20utility%20of%20an%20explicit%0Aapproximation%20to%20the%20time-dependent%20probability%20density%20function%20through%0Aapplications%20in%20unsupervised%20outlier%20detection.%20Through%20several%20numerical%0Aexperiments%2C%20we%20show%20that%20the%20proposed%20method%20accurately%20reconstructs%20complex%0Atime-dependent%2C%20multi-modal%2C%20and%20near-degenerate%20densities%2C%20scales%20effectively%0Ato%20moderately%20high-dimensional%20problems%2C%20and%20reliably%20detects%20rare%20events%20among%0Areal-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-dependent%2520density%2520estimation%2520using%2520binary%2520classifiers%26entry.906535625%3DAgnimitra%2520Dasgupta%2520and%2520Javier%2520Murgoitio-Esandi%2520and%2520Ali%2520Fardisi%2520and%2520Assad%2520A%2520Oberai%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520data-driven%2520method%2520to%2520learn%2520the%2520time-dependent%2520probability%250Adensity%2520of%2520a%2520multivariate%2520stochastic%2520process%2520from%2520sample%2520paths%252C%2520assuming%2520that%250Athe%2520initial%2520probability%2520density%2520is%2520known%2520and%2520can%2520be%2520evaluated.%2520Our%2520method%2520uses%250Aa%2520novel%2520time-dependent%2520binary%2520classifier%2520trained%2520using%2520a%2520contrastive%250Aestimation-based%2520objective%2520that%2520trains%2520the%2520classifier%2520to%2520discriminate%2520between%250Arealizations%2520of%2520the%2520stochastic%2520process%2520at%2520two%2520nearby%2520time%2520instants.%250ASignificantly%252C%2520the%2520proposed%2520method%2520explicitly%2520models%2520the%2520time-dependent%250Aprobability%2520distribution%252C%2520which%2520means%2520that%2520it%2520is%2520possible%2520to%2520obtain%2520the%2520value%250Aof%2520the%2520probability%2520density%2520within%2520the%2520time%2520horizon%2520of%2520interest.%2520Additionally%252C%250Athe%2520input%2520before%2520the%2520final%2520activation%2520in%2520the%2520time-dependent%2520classifier%2520is%2520a%250Asecond-order%2520approximation%2520to%2520the%2520partial%2520derivative%252C%2520with%2520respect%2520to%2520time%252C%2520of%250Athe%2520logarithm%2520of%2520the%2520density.%2520We%2520apply%2520the%2520proposed%2520approach%2520to%2520approximate%2520the%250Atime-dependent%2520probability%2520density%2520functions%2520for%2520systems%2520driven%2520by%2520stochastic%250Aexcitations.%2520We%2520also%2520use%2520the%2520proposed%2520approach%2520to%2520synthesize%2520new%2520samples%2520of%2520a%250Arandom%2520vector%2520from%2520a%2520given%2520set%2520of%2520its%2520realizations.%2520In%2520such%2520applications%252C%2520we%250Agenerate%2520sample%2520paths%2520necessary%2520for%2520training%2520using%2520stochastic%2520interpolants.%250ASubsequently%252C%2520new%2520samples%2520are%2520generated%2520using%2520gradient-based%2520Markov%2520chain%2520Monte%250ACarlo%2520methods%2520because%2520automatic%2520differentiation%2520can%2520efficiently%2520provide%2520the%250Anecessary%2520gradient.%2520Further%252C%2520we%2520demonstrate%2520the%2520utility%2520of%2520an%2520explicit%250Aapproximation%2520to%2520the%2520time-dependent%2520probability%2520density%2520function%2520through%250Aapplications%2520in%2520unsupervised%2520outlier%2520detection.%2520Through%2520several%2520numerical%250Aexperiments%252C%2520we%2520show%2520that%2520the%2520proposed%2520method%2520accurately%2520reconstructs%2520complex%250Atime-dependent%252C%2520multi-modal%252C%2520and%2520near-degenerate%2520densities%252C%2520scales%2520effectively%250Ato%2520moderately%2520high-dimensional%2520problems%252C%2520and%2520reliably%2520detects%2520rare%2520events%2520among%250Areal-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-dependent%20density%20estimation%20using%20binary%20classifiers&entry.906535625=Agnimitra%20Dasgupta%20and%20Javier%20Murgoitio-Esandi%20and%20Ali%20Fardisi%20and%20Assad%20A%20Oberai&entry.1292438233=%20%20We%20propose%20a%20data-driven%20method%20to%20learn%20the%20time-dependent%20probability%0Adensity%20of%20a%20multivariate%20stochastic%20process%20from%20sample%20paths%2C%20assuming%20that%0Athe%20initial%20probability%20density%20is%20known%20and%20can%20be%20evaluated.%20Our%20method%20uses%0Aa%20novel%20time-dependent%20binary%20classifier%20trained%20using%20a%20contrastive%0Aestimation-based%20objective%20that%20trains%20the%20classifier%20to%20discriminate%20between%0Arealizations%20of%20the%20stochastic%20process%20at%20two%20nearby%20time%20instants.%0ASignificantly%2C%20the%20proposed%20method%20explicitly%20models%20the%20time-dependent%0Aprobability%20distribution%2C%20which%20means%20that%20it%20is%20possible%20to%20obtain%20the%20value%0Aof%20the%20probability%20density%20within%20the%20time%20horizon%20of%20interest.%20Additionally%2C%0Athe%20input%20before%20the%20final%20activation%20in%20the%20time-dependent%20classifier%20is%20a%0Asecond-order%20approximation%20to%20the%20partial%20derivative%2C%20with%20respect%20to%20time%2C%20of%0Athe%20logarithm%20of%20the%20density.%20We%20apply%20the%20proposed%20approach%20to%20approximate%20the%0Atime-dependent%20probability%20density%20functions%20for%20systems%20driven%20by%20stochastic%0Aexcitations.%20We%20also%20use%20the%20proposed%20approach%20to%20synthesize%20new%20samples%20of%20a%0Arandom%20vector%20from%20a%20given%20set%20of%20its%20realizations.%20In%20such%20applications%2C%20we%0Agenerate%20sample%20paths%20necessary%20for%20training%20using%20stochastic%20interpolants.%0ASubsequently%2C%20new%20samples%20are%20generated%20using%20gradient-based%20Markov%20chain%20Monte%0ACarlo%20methods%20because%20automatic%20differentiation%20can%20efficiently%20provide%20the%0Anecessary%20gradient.%20Further%2C%20we%20demonstrate%20the%20utility%20of%20an%20explicit%0Aapproximation%20to%20the%20time-dependent%20probability%20density%20function%20through%0Aapplications%20in%20unsupervised%20outlier%20detection.%20Through%20several%20numerical%0Aexperiments%2C%20we%20show%20that%20the%20proposed%20method%20accurately%20reconstructs%20complex%0Atime-dependent%2C%20multi-modal%2C%20and%20near-degenerate%20densities%2C%20scales%20effectively%0Ato%20moderately%20high-dimensional%20problems%2C%20and%20reliably%20detects%20rare%20events%20among%0Areal-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15505v1&entry.124074799=Read"},
{"title": "CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse\n  Myocardial Scar Synthesis and Segmentation", "author": "Farheen Ramzan and Yusuf Kiberu and Nikesh Jathanna and Shahnaz Jamil-Copley and Richard H. Clayton and  Chen and  Chen", "abstract": "  Deep learning-based myocardial scar segmentation from late gadolinium\nenhancement (LGE) cardiac MRI has shown great potential for accurate and timely\ndiagnosis and treatment planning for structural cardiac diseases. However, the\nlimited availability and variability of LGE images with high-quality scar\nlabels restrict the development of robust segmentation models. To address this,\nwe introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE\n\\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial\nScar Synthesis and Segmentation framework, a framework for anatomically\ngrounded scar generation and segmentation. At its core is the SMILE module\n(Scar Mask generation guided by cLinical knowledgE), which conditions a\ndiffusion-based generator on the clinically adopted AHA 17-segment model to\nsynthesize images with anatomically consistent and spatially diverse scar\npatterns. In addition, CLAIM employs a joint training strategy in which the\nscar segmentation network is optimized alongside the generator, aiming to\nenhance both the realism of synthesized scars and the accuracy of the scar\nsegmentation performance. Experimental results show that CLAIM produces\nanatomically coherent scar patterns and achieves higher Dice similarity with\nreal scar distributions compared to baseline models. Our approach enables\ncontrollable and realistic myocardial scar synthesis and has demonstrated\nutility for downstream medical imaging task.\n", "link": "http://arxiv.org/abs/2506.15549v1", "date": "2025-06-18", "relevancy": 2.0222, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5088}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5074}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLAIM%3A%20Clinically-Guided%20LGE%20Augmentation%20for%20Realistic%20and%20Diverse%0A%20%20Myocardial%20Scar%20Synthesis%20and%20Segmentation&body=Title%3A%20CLAIM%3A%20Clinically-Guided%20LGE%20Augmentation%20for%20Realistic%20and%20Diverse%0A%20%20Myocardial%20Scar%20Synthesis%20and%20Segmentation%0AAuthor%3A%20Farheen%20Ramzan%20and%20Yusuf%20Kiberu%20and%20Nikesh%20Jathanna%20and%20Shahnaz%20Jamil-Copley%20and%20Richard%20H.%20Clayton%20and%20%20Chen%20and%20%20Chen%0AAbstract%3A%20%20%20Deep%20learning-based%20myocardial%20scar%20segmentation%20from%20late%20gadolinium%0Aenhancement%20%28LGE%29%20cardiac%20MRI%20has%20shown%20great%20potential%20for%20accurate%20and%20timely%0Adiagnosis%20and%20treatment%20planning%20for%20structural%20cardiac%20diseases.%20However%2C%20the%0Alimited%20availability%20and%20variability%20of%20LGE%20images%20with%20high-quality%20scar%0Alabels%20restrict%20the%20development%20of%20robust%20segmentation%20models.%20To%20address%20this%2C%0Awe%20introduce%20CLAIM%3A%20%5Ctextbf%7BC%7Dlinically-Guided%20%5Ctextbf%7BL%7DGE%0A%5Ctextbf%7BA%7Dugmentation%20for%20Real%5Ctextbf%7Bi%7Dstic%20and%20Diverse%20%5Ctextbf%7BM%7Dyocardial%0AScar%20Synthesis%20and%20Segmentation%20framework%2C%20a%20framework%20for%20anatomically%0Agrounded%20scar%20generation%20and%20segmentation.%20At%20its%20core%20is%20the%20SMILE%20module%0A%28Scar%20Mask%20generation%20guided%20by%20cLinical%20knowledgE%29%2C%20which%20conditions%20a%0Adiffusion-based%20generator%20on%20the%20clinically%20adopted%20AHA%2017-segment%20model%20to%0Asynthesize%20images%20with%20anatomically%20consistent%20and%20spatially%20diverse%20scar%0Apatterns.%20In%20addition%2C%20CLAIM%20employs%20a%20joint%20training%20strategy%20in%20which%20the%0Ascar%20segmentation%20network%20is%20optimized%20alongside%20the%20generator%2C%20aiming%20to%0Aenhance%20both%20the%20realism%20of%20synthesized%20scars%20and%20the%20accuracy%20of%20the%20scar%0Asegmentation%20performance.%20Experimental%20results%20show%20that%20CLAIM%20produces%0Aanatomically%20coherent%20scar%20patterns%20and%20achieves%20higher%20Dice%20similarity%20with%0Areal%20scar%20distributions%20compared%20to%20baseline%20models.%20Our%20approach%20enables%0Acontrollable%20and%20realistic%20myocardial%20scar%20synthesis%20and%20has%20demonstrated%0Autility%20for%20downstream%20medical%20imaging%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLAIM%253A%2520Clinically-Guided%2520LGE%2520Augmentation%2520for%2520Realistic%2520and%2520Diverse%250A%2520%2520Myocardial%2520Scar%2520Synthesis%2520and%2520Segmentation%26entry.906535625%3DFarheen%2520Ramzan%2520and%2520Yusuf%2520Kiberu%2520and%2520Nikesh%2520Jathanna%2520and%2520Shahnaz%2520Jamil-Copley%2520and%2520Richard%2520H.%2520Clayton%2520and%2520%2520Chen%2520and%2520%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520myocardial%2520scar%2520segmentation%2520from%2520late%2520gadolinium%250Aenhancement%2520%2528LGE%2529%2520cardiac%2520MRI%2520has%2520shown%2520great%2520potential%2520for%2520accurate%2520and%2520timely%250Adiagnosis%2520and%2520treatment%2520planning%2520for%2520structural%2520cardiac%2520diseases.%2520However%252C%2520the%250Alimited%2520availability%2520and%2520variability%2520of%2520LGE%2520images%2520with%2520high-quality%2520scar%250Alabels%2520restrict%2520the%2520development%2520of%2520robust%2520segmentation%2520models.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520CLAIM%253A%2520%255Ctextbf%257BC%257Dlinically-Guided%2520%255Ctextbf%257BL%257DGE%250A%255Ctextbf%257BA%257Dugmentation%2520for%2520Real%255Ctextbf%257Bi%257Dstic%2520and%2520Diverse%2520%255Ctextbf%257BM%257Dyocardial%250AScar%2520Synthesis%2520and%2520Segmentation%2520framework%252C%2520a%2520framework%2520for%2520anatomically%250Agrounded%2520scar%2520generation%2520and%2520segmentation.%2520At%2520its%2520core%2520is%2520the%2520SMILE%2520module%250A%2528Scar%2520Mask%2520generation%2520guided%2520by%2520cLinical%2520knowledgE%2529%252C%2520which%2520conditions%2520a%250Adiffusion-based%2520generator%2520on%2520the%2520clinically%2520adopted%2520AHA%252017-segment%2520model%2520to%250Asynthesize%2520images%2520with%2520anatomically%2520consistent%2520and%2520spatially%2520diverse%2520scar%250Apatterns.%2520In%2520addition%252C%2520CLAIM%2520employs%2520a%2520joint%2520training%2520strategy%2520in%2520which%2520the%250Ascar%2520segmentation%2520network%2520is%2520optimized%2520alongside%2520the%2520generator%252C%2520aiming%2520to%250Aenhance%2520both%2520the%2520realism%2520of%2520synthesized%2520scars%2520and%2520the%2520accuracy%2520of%2520the%2520scar%250Asegmentation%2520performance.%2520Experimental%2520results%2520show%2520that%2520CLAIM%2520produces%250Aanatomically%2520coherent%2520scar%2520patterns%2520and%2520achieves%2520higher%2520Dice%2520similarity%2520with%250Areal%2520scar%2520distributions%2520compared%2520to%2520baseline%2520models.%2520Our%2520approach%2520enables%250Acontrollable%2520and%2520realistic%2520myocardial%2520scar%2520synthesis%2520and%2520has%2520demonstrated%250Autility%2520for%2520downstream%2520medical%2520imaging%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLAIM%3A%20Clinically-Guided%20LGE%20Augmentation%20for%20Realistic%20and%20Diverse%0A%20%20Myocardial%20Scar%20Synthesis%20and%20Segmentation&entry.906535625=Farheen%20Ramzan%20and%20Yusuf%20Kiberu%20and%20Nikesh%20Jathanna%20and%20Shahnaz%20Jamil-Copley%20and%20Richard%20H.%20Clayton%20and%20%20Chen%20and%20%20Chen&entry.1292438233=%20%20Deep%20learning-based%20myocardial%20scar%20segmentation%20from%20late%20gadolinium%0Aenhancement%20%28LGE%29%20cardiac%20MRI%20has%20shown%20great%20potential%20for%20accurate%20and%20timely%0Adiagnosis%20and%20treatment%20planning%20for%20structural%20cardiac%20diseases.%20However%2C%20the%0Alimited%20availability%20and%20variability%20of%20LGE%20images%20with%20high-quality%20scar%0Alabels%20restrict%20the%20development%20of%20robust%20segmentation%20models.%20To%20address%20this%2C%0Awe%20introduce%20CLAIM%3A%20%5Ctextbf%7BC%7Dlinically-Guided%20%5Ctextbf%7BL%7DGE%0A%5Ctextbf%7BA%7Dugmentation%20for%20Real%5Ctextbf%7Bi%7Dstic%20and%20Diverse%20%5Ctextbf%7BM%7Dyocardial%0AScar%20Synthesis%20and%20Segmentation%20framework%2C%20a%20framework%20for%20anatomically%0Agrounded%20scar%20generation%20and%20segmentation.%20At%20its%20core%20is%20the%20SMILE%20module%0A%28Scar%20Mask%20generation%20guided%20by%20cLinical%20knowledgE%29%2C%20which%20conditions%20a%0Adiffusion-based%20generator%20on%20the%20clinically%20adopted%20AHA%2017-segment%20model%20to%0Asynthesize%20images%20with%20anatomically%20consistent%20and%20spatially%20diverse%20scar%0Apatterns.%20In%20addition%2C%20CLAIM%20employs%20a%20joint%20training%20strategy%20in%20which%20the%0Ascar%20segmentation%20network%20is%20optimized%20alongside%20the%20generator%2C%20aiming%20to%0Aenhance%20both%20the%20realism%20of%20synthesized%20scars%20and%20the%20accuracy%20of%20the%20scar%0Asegmentation%20performance.%20Experimental%20results%20show%20that%20CLAIM%20produces%0Aanatomically%20coherent%20scar%20patterns%20and%20achieves%20higher%20Dice%20similarity%20with%0Areal%20scar%20distributions%20compared%20to%20baseline%20models.%20Our%20approach%20enables%0Acontrollable%20and%20realistic%20myocardial%20scar%20synthesis%20and%20has%20demonstrated%0Autility%20for%20downstream%20medical%20imaging%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15549v1&entry.124074799=Read"},
{"title": "Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time\n  Series Node Classification", "author": "Mingsen Du and Meng Chen and Yongjian Li and Xiuxin Zhang and Jiahui Gao and Cun Ji and Shoushui Wei", "abstract": "  Multivariate time series (MTS) data is generated through multiple sensors\nacross various domains such as engineering application, health monitoring, and\nthe internet of things, characterized by its temporal changes and high\ndimensional characteristics. Over the past few years, many studies have\nexplored the long-range dependencies and similarities in MTS. However,\nlong-range dependencies are difficult to model due to their temporal changes\nand high dimensionality makes it difficult to obtain similarities effectively\nand efficiently. Thus, to address these issues, we propose contrast\nsimilarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba).\nFirstly, to obtain the dynamic similarity of each sample, we initially use\ntemporal contrast learning module to acquire MTS representations. And then we\nconstruct a similarity matrix between MTS representations using Fast Dynamic\nTime Warping (FastDTW). Secondly, we apply the DPMamba to consider the\nbidirectional nature of MTS, allowing us to better capture long-range and\nshort-range dependencies within the data. Finally, we utilize the\nKolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the\ninformation interaction in the matrix and MTS node classification task. By\ncomprehensively considering the long-range dependencies and dynamic similarity\nfeatures, we achieved precise MTS node classification. We conducted experiments\non multiple University of East Anglia (UEA) MTS datasets, which encompass\ndiverse application scenarios. Our results demonstrate the superiority of our\nmethod through both supervised and semi-supervised experiments on the MTS\nclassification task.\n", "link": "http://arxiv.org/abs/2411.12222v2", "date": "2025-06-18", "relevancy": 2.0186, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5039}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrast%20Similarity-Aware%20Dual-Pathway%20Mamba%20for%20Multivariate%20Time%0A%20%20Series%20Node%20Classification&body=Title%3A%20Contrast%20Similarity-Aware%20Dual-Pathway%20Mamba%20for%20Multivariate%20Time%0A%20%20Series%20Node%20Classification%0AAuthor%3A%20Mingsen%20Du%20and%20Meng%20Chen%20and%20Yongjian%20Li%20and%20Xiuxin%20Zhang%20and%20Jiahui%20Gao%20and%20Cun%20Ji%20and%20Shoushui%20Wei%0AAbstract%3A%20%20%20Multivariate%20time%20series%20%28MTS%29%20data%20is%20generated%20through%20multiple%20sensors%0Aacross%20various%20domains%20such%20as%20engineering%20application%2C%20health%20monitoring%2C%20and%0Athe%20internet%20of%20things%2C%20characterized%20by%20its%20temporal%20changes%20and%20high%0Adimensional%20characteristics.%20Over%20the%20past%20few%20years%2C%20many%20studies%20have%0Aexplored%20the%20long-range%20dependencies%20and%20similarities%20in%20MTS.%20However%2C%0Along-range%20dependencies%20are%20difficult%20to%20model%20due%20to%20their%20temporal%20changes%0Aand%20high%20dimensionality%20makes%20it%20difficult%20to%20obtain%20similarities%20effectively%0Aand%20efficiently.%20Thus%2C%20to%20address%20these%20issues%2C%20we%20propose%20contrast%0Asimilarity-aware%20dual-pathway%20Mamba%20for%20MTS%20node%20classification%20%28CS-DPMamba%29.%0AFirstly%2C%20to%20obtain%20the%20dynamic%20similarity%20of%20each%20sample%2C%20we%20initially%20use%0Atemporal%20contrast%20learning%20module%20to%20acquire%20MTS%20representations.%20And%20then%20we%0Aconstruct%20a%20similarity%20matrix%20between%20MTS%20representations%20using%20Fast%20Dynamic%0ATime%20Warping%20%28FastDTW%29.%20Secondly%2C%20we%20apply%20the%20DPMamba%20to%20consider%20the%0Abidirectional%20nature%20of%20MTS%2C%20allowing%20us%20to%20better%20capture%20long-range%20and%0Ashort-range%20dependencies%20within%20the%20data.%20Finally%2C%20we%20utilize%20the%0AKolmogorov-Arnold%20Network%20enhanced%20Graph%20Isomorphism%20Network%20to%20complete%20the%0Ainformation%20interaction%20in%20the%20matrix%20and%20MTS%20node%20classification%20task.%20By%0Acomprehensively%20considering%20the%20long-range%20dependencies%20and%20dynamic%20similarity%0Afeatures%2C%20we%20achieved%20precise%20MTS%20node%20classification.%20We%20conducted%20experiments%0Aon%20multiple%20University%20of%20East%20Anglia%20%28UEA%29%20MTS%20datasets%2C%20which%20encompass%0Adiverse%20application%20scenarios.%20Our%20results%20demonstrate%20the%20superiority%20of%20our%0Amethod%20through%20both%20supervised%20and%20semi-supervised%20experiments%20on%20the%20MTS%0Aclassification%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrast%2520Similarity-Aware%2520Dual-Pathway%2520Mamba%2520for%2520Multivariate%2520Time%250A%2520%2520Series%2520Node%2520Classification%26entry.906535625%3DMingsen%2520Du%2520and%2520Meng%2520Chen%2520and%2520Yongjian%2520Li%2520and%2520Xiuxin%2520Zhang%2520and%2520Jiahui%2520Gao%2520and%2520Cun%2520Ji%2520and%2520Shoushui%2520Wei%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520%2528MTS%2529%2520data%2520is%2520generated%2520through%2520multiple%2520sensors%250Aacross%2520various%2520domains%2520such%2520as%2520engineering%2520application%252C%2520health%2520monitoring%252C%2520and%250Athe%2520internet%2520of%2520things%252C%2520characterized%2520by%2520its%2520temporal%2520changes%2520and%2520high%250Adimensional%2520characteristics.%2520Over%2520the%2520past%2520few%2520years%252C%2520many%2520studies%2520have%250Aexplored%2520the%2520long-range%2520dependencies%2520and%2520similarities%2520in%2520MTS.%2520However%252C%250Along-range%2520dependencies%2520are%2520difficult%2520to%2520model%2520due%2520to%2520their%2520temporal%2520changes%250Aand%2520high%2520dimensionality%2520makes%2520it%2520difficult%2520to%2520obtain%2520similarities%2520effectively%250Aand%2520efficiently.%2520Thus%252C%2520to%2520address%2520these%2520issues%252C%2520we%2520propose%2520contrast%250Asimilarity-aware%2520dual-pathway%2520Mamba%2520for%2520MTS%2520node%2520classification%2520%2528CS-DPMamba%2529.%250AFirstly%252C%2520to%2520obtain%2520the%2520dynamic%2520similarity%2520of%2520each%2520sample%252C%2520we%2520initially%2520use%250Atemporal%2520contrast%2520learning%2520module%2520to%2520acquire%2520MTS%2520representations.%2520And%2520then%2520we%250Aconstruct%2520a%2520similarity%2520matrix%2520between%2520MTS%2520representations%2520using%2520Fast%2520Dynamic%250ATime%2520Warping%2520%2528FastDTW%2529.%2520Secondly%252C%2520we%2520apply%2520the%2520DPMamba%2520to%2520consider%2520the%250Abidirectional%2520nature%2520of%2520MTS%252C%2520allowing%2520us%2520to%2520better%2520capture%2520long-range%2520and%250Ashort-range%2520dependencies%2520within%2520the%2520data.%2520Finally%252C%2520we%2520utilize%2520the%250AKolmogorov-Arnold%2520Network%2520enhanced%2520Graph%2520Isomorphism%2520Network%2520to%2520complete%2520the%250Ainformation%2520interaction%2520in%2520the%2520matrix%2520and%2520MTS%2520node%2520classification%2520task.%2520By%250Acomprehensively%2520considering%2520the%2520long-range%2520dependencies%2520and%2520dynamic%2520similarity%250Afeatures%252C%2520we%2520achieved%2520precise%2520MTS%2520node%2520classification.%2520We%2520conducted%2520experiments%250Aon%2520multiple%2520University%2520of%2520East%2520Anglia%2520%2528UEA%2529%2520MTS%2520datasets%252C%2520which%2520encompass%250Adiverse%2520application%2520scenarios.%2520Our%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%250Amethod%2520through%2520both%2520supervised%2520and%2520semi-supervised%2520experiments%2520on%2520the%2520MTS%250Aclassification%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrast%20Similarity-Aware%20Dual-Pathway%20Mamba%20for%20Multivariate%20Time%0A%20%20Series%20Node%20Classification&entry.906535625=Mingsen%20Du%20and%20Meng%20Chen%20and%20Yongjian%20Li%20and%20Xiuxin%20Zhang%20and%20Jiahui%20Gao%20and%20Cun%20Ji%20and%20Shoushui%20Wei&entry.1292438233=%20%20Multivariate%20time%20series%20%28MTS%29%20data%20is%20generated%20through%20multiple%20sensors%0Aacross%20various%20domains%20such%20as%20engineering%20application%2C%20health%20monitoring%2C%20and%0Athe%20internet%20of%20things%2C%20characterized%20by%20its%20temporal%20changes%20and%20high%0Adimensional%20characteristics.%20Over%20the%20past%20few%20years%2C%20many%20studies%20have%0Aexplored%20the%20long-range%20dependencies%20and%20similarities%20in%20MTS.%20However%2C%0Along-range%20dependencies%20are%20difficult%20to%20model%20due%20to%20their%20temporal%20changes%0Aand%20high%20dimensionality%20makes%20it%20difficult%20to%20obtain%20similarities%20effectively%0Aand%20efficiently.%20Thus%2C%20to%20address%20these%20issues%2C%20we%20propose%20contrast%0Asimilarity-aware%20dual-pathway%20Mamba%20for%20MTS%20node%20classification%20%28CS-DPMamba%29.%0AFirstly%2C%20to%20obtain%20the%20dynamic%20similarity%20of%20each%20sample%2C%20we%20initially%20use%0Atemporal%20contrast%20learning%20module%20to%20acquire%20MTS%20representations.%20And%20then%20we%0Aconstruct%20a%20similarity%20matrix%20between%20MTS%20representations%20using%20Fast%20Dynamic%0ATime%20Warping%20%28FastDTW%29.%20Secondly%2C%20we%20apply%20the%20DPMamba%20to%20consider%20the%0Abidirectional%20nature%20of%20MTS%2C%20allowing%20us%20to%20better%20capture%20long-range%20and%0Ashort-range%20dependencies%20within%20the%20data.%20Finally%2C%20we%20utilize%20the%0AKolmogorov-Arnold%20Network%20enhanced%20Graph%20Isomorphism%20Network%20to%20complete%20the%0Ainformation%20interaction%20in%20the%20matrix%20and%20MTS%20node%20classification%20task.%20By%0Acomprehensively%20considering%20the%20long-range%20dependencies%20and%20dynamic%20similarity%0Afeatures%2C%20we%20achieved%20precise%20MTS%20node%20classification.%20We%20conducted%20experiments%0Aon%20multiple%20University%20of%20East%20Anglia%20%28UEA%29%20MTS%20datasets%2C%20which%20encompass%0Adiverse%20application%20scenarios.%20Our%20results%20demonstrate%20the%20superiority%20of%20our%0Amethod%20through%20both%20supervised%20and%20semi-supervised%20experiments%20on%20the%20MTS%0Aclassification%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12222v2&entry.124074799=Read"},
{"title": "\"Generate\" the Future of Work through AI: Empirical Evidence from Online\n  Labor Markets", "author": "Jin Liu and Xingchen Xu and Xi Nan and Yongjun Li and Yong Tan", "abstract": "  Large Language Model (LLM)-based generative AI systems, such as ChatGPT,\ndemonstrate zero-shot learning capabilities across a wide range of downstream\ntasks. Owing to their general-purpose nature and potential to augment or even\nautomate job functions, these systems are poised to reshape labor market\ndynamics. However, predicting their precise impact \\textit{a priori} is\nchallenging, given AI's simultaneous effects on both demand and supply, as well\nas the strategic responses of market participants. Leveraging an extensive\ndataset from a leading online labor platform, we document a pronounced\ndisplacement effect and an overall contraction in submarkets where required\nskills closely align with core LLM functionalities. Although demand and supply\nboth decline, the reduction in supply is comparatively smaller, thereby\nintensifying competition among freelancers. Notably, further analysis shows\nthat this heightened competition is especially pronounced in\nprogramming-intensive submarkets. This pattern is attributed to\nskill-transition effects: by lowering the human-capital barrier to programming,\nChatGPT enables incumbent freelancers to enter programming tasks. Moreover,\nthese transitions are not homogeneous, with high-skilled freelancers\ncontributing disproportionately to the shift. Our findings illuminate the\nmultifaceted impacts of general-purpose AI on labor markets, highlighting not\nonly the displacement of certain occupations but also the inducement of skill\ntransitions within the labor supply. These insights offer practical\nimplications for policymakers, platform operators, and workers.\n", "link": "http://arxiv.org/abs/2308.05201v3", "date": "2025-06-18", "relevancy": 2.0152, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5162}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5055}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Generate%22%20the%20Future%20of%20Work%20through%20AI%3A%20Empirical%20Evidence%20from%20Online%0A%20%20Labor%20Markets&body=Title%3A%20%22Generate%22%20the%20Future%20of%20Work%20through%20AI%3A%20Empirical%20Evidence%20from%20Online%0A%20%20Labor%20Markets%0AAuthor%3A%20Jin%20Liu%20and%20Xingchen%20Xu%20and%20Xi%20Nan%20and%20Yongjun%20Li%20and%20Yong%20Tan%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20generative%20AI%20systems%2C%20such%20as%20ChatGPT%2C%0Ademonstrate%20zero-shot%20learning%20capabilities%20across%20a%20wide%20range%20of%20downstream%0Atasks.%20Owing%20to%20their%20general-purpose%20nature%20and%20potential%20to%20augment%20or%20even%0Aautomate%20job%20functions%2C%20these%20systems%20are%20poised%20to%20reshape%20labor%20market%0Adynamics.%20However%2C%20predicting%20their%20precise%20impact%20%5Ctextit%7Ba%20priori%7D%20is%0Achallenging%2C%20given%20AI%27s%20simultaneous%20effects%20on%20both%20demand%20and%20supply%2C%20as%20well%0Aas%20the%20strategic%20responses%20of%20market%20participants.%20Leveraging%20an%20extensive%0Adataset%20from%20a%20leading%20online%20labor%20platform%2C%20we%20document%20a%20pronounced%0Adisplacement%20effect%20and%20an%20overall%20contraction%20in%20submarkets%20where%20required%0Askills%20closely%20align%20with%20core%20LLM%20functionalities.%20Although%20demand%20and%20supply%0Aboth%20decline%2C%20the%20reduction%20in%20supply%20is%20comparatively%20smaller%2C%20thereby%0Aintensifying%20competition%20among%20freelancers.%20Notably%2C%20further%20analysis%20shows%0Athat%20this%20heightened%20competition%20is%20especially%20pronounced%20in%0Aprogramming-intensive%20submarkets.%20This%20pattern%20is%20attributed%20to%0Askill-transition%20effects%3A%20by%20lowering%20the%20human-capital%20barrier%20to%20programming%2C%0AChatGPT%20enables%20incumbent%20freelancers%20to%20enter%20programming%20tasks.%20Moreover%2C%0Athese%20transitions%20are%20not%20homogeneous%2C%20with%20high-skilled%20freelancers%0Acontributing%20disproportionately%20to%20the%20shift.%20Our%20findings%20illuminate%20the%0Amultifaceted%20impacts%20of%20general-purpose%20AI%20on%20labor%20markets%2C%20highlighting%20not%0Aonly%20the%20displacement%20of%20certain%20occupations%20but%20also%20the%20inducement%20of%20skill%0Atransitions%20within%20the%20labor%20supply.%20These%20insights%20offer%20practical%0Aimplications%20for%20policymakers%2C%20platform%20operators%2C%20and%20workers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05201v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Generate%2522%2520the%2520Future%2520of%2520Work%2520through%2520AI%253A%2520Empirical%2520Evidence%2520from%2520Online%250A%2520%2520Labor%2520Markets%26entry.906535625%3DJin%2520Liu%2520and%2520Xingchen%2520Xu%2520and%2520Xi%2520Nan%2520and%2520Yongjun%2520Li%2520and%2520Yong%2520Tan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520generative%2520AI%2520systems%252C%2520such%2520as%2520ChatGPT%252C%250Ademonstrate%2520zero-shot%2520learning%2520capabilities%2520across%2520a%2520wide%2520range%2520of%2520downstream%250Atasks.%2520Owing%2520to%2520their%2520general-purpose%2520nature%2520and%2520potential%2520to%2520augment%2520or%2520even%250Aautomate%2520job%2520functions%252C%2520these%2520systems%2520are%2520poised%2520to%2520reshape%2520labor%2520market%250Adynamics.%2520However%252C%2520predicting%2520their%2520precise%2520impact%2520%255Ctextit%257Ba%2520priori%257D%2520is%250Achallenging%252C%2520given%2520AI%2527s%2520simultaneous%2520effects%2520on%2520both%2520demand%2520and%2520supply%252C%2520as%2520well%250Aas%2520the%2520strategic%2520responses%2520of%2520market%2520participants.%2520Leveraging%2520an%2520extensive%250Adataset%2520from%2520a%2520leading%2520online%2520labor%2520platform%252C%2520we%2520document%2520a%2520pronounced%250Adisplacement%2520effect%2520and%2520an%2520overall%2520contraction%2520in%2520submarkets%2520where%2520required%250Askills%2520closely%2520align%2520with%2520core%2520LLM%2520functionalities.%2520Although%2520demand%2520and%2520supply%250Aboth%2520decline%252C%2520the%2520reduction%2520in%2520supply%2520is%2520comparatively%2520smaller%252C%2520thereby%250Aintensifying%2520competition%2520among%2520freelancers.%2520Notably%252C%2520further%2520analysis%2520shows%250Athat%2520this%2520heightened%2520competition%2520is%2520especially%2520pronounced%2520in%250Aprogramming-intensive%2520submarkets.%2520This%2520pattern%2520is%2520attributed%2520to%250Askill-transition%2520effects%253A%2520by%2520lowering%2520the%2520human-capital%2520barrier%2520to%2520programming%252C%250AChatGPT%2520enables%2520incumbent%2520freelancers%2520to%2520enter%2520programming%2520tasks.%2520Moreover%252C%250Athese%2520transitions%2520are%2520not%2520homogeneous%252C%2520with%2520high-skilled%2520freelancers%250Acontributing%2520disproportionately%2520to%2520the%2520shift.%2520Our%2520findings%2520illuminate%2520the%250Amultifaceted%2520impacts%2520of%2520general-purpose%2520AI%2520on%2520labor%2520markets%252C%2520highlighting%2520not%250Aonly%2520the%2520displacement%2520of%2520certain%2520occupations%2520but%2520also%2520the%2520inducement%2520of%2520skill%250Atransitions%2520within%2520the%2520labor%2520supply.%2520These%2520insights%2520offer%2520practical%250Aimplications%2520for%2520policymakers%252C%2520platform%2520operators%252C%2520and%2520workers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05201v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Generate%22%20the%20Future%20of%20Work%20through%20AI%3A%20Empirical%20Evidence%20from%20Online%0A%20%20Labor%20Markets&entry.906535625=Jin%20Liu%20and%20Xingchen%20Xu%20and%20Xi%20Nan%20and%20Yongjun%20Li%20and%20Yong%20Tan&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20generative%20AI%20systems%2C%20such%20as%20ChatGPT%2C%0Ademonstrate%20zero-shot%20learning%20capabilities%20across%20a%20wide%20range%20of%20downstream%0Atasks.%20Owing%20to%20their%20general-purpose%20nature%20and%20potential%20to%20augment%20or%20even%0Aautomate%20job%20functions%2C%20these%20systems%20are%20poised%20to%20reshape%20labor%20market%0Adynamics.%20However%2C%20predicting%20their%20precise%20impact%20%5Ctextit%7Ba%20priori%7D%20is%0Achallenging%2C%20given%20AI%27s%20simultaneous%20effects%20on%20both%20demand%20and%20supply%2C%20as%20well%0Aas%20the%20strategic%20responses%20of%20market%20participants.%20Leveraging%20an%20extensive%0Adataset%20from%20a%20leading%20online%20labor%20platform%2C%20we%20document%20a%20pronounced%0Adisplacement%20effect%20and%20an%20overall%20contraction%20in%20submarkets%20where%20required%0Askills%20closely%20align%20with%20core%20LLM%20functionalities.%20Although%20demand%20and%20supply%0Aboth%20decline%2C%20the%20reduction%20in%20supply%20is%20comparatively%20smaller%2C%20thereby%0Aintensifying%20competition%20among%20freelancers.%20Notably%2C%20further%20analysis%20shows%0Athat%20this%20heightened%20competition%20is%20especially%20pronounced%20in%0Aprogramming-intensive%20submarkets.%20This%20pattern%20is%20attributed%20to%0Askill-transition%20effects%3A%20by%20lowering%20the%20human-capital%20barrier%20to%20programming%2C%0AChatGPT%20enables%20incumbent%20freelancers%20to%20enter%20programming%20tasks.%20Moreover%2C%0Athese%20transitions%20are%20not%20homogeneous%2C%20with%20high-skilled%20freelancers%0Acontributing%20disproportionately%20to%20the%20shift.%20Our%20findings%20illuminate%20the%0Amultifaceted%20impacts%20of%20general-purpose%20AI%20on%20labor%20markets%2C%20highlighting%20not%0Aonly%20the%20displacement%20of%20certain%20occupations%20but%20also%20the%20inducement%20of%20skill%0Atransitions%20within%20the%20labor%20supply.%20These%20insights%20offer%20practical%0Aimplications%20for%20policymakers%2C%20platform%20operators%2C%20and%20workers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05201v3&entry.124074799=Read"},
{"title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of\n  Explainable AI", "author": "David Dembinsky and Adriano Lucieri and Stanislav Frolov and Hiba Najjar and Ko Watanabe and Andreas Dengel", "abstract": "  Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.\n", "link": "http://arxiv.org/abs/2506.15408v1", "date": "2025-06-18", "relevancy": 2.0021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unifying%20VXAI%3A%20A%20Systematic%20Review%20and%20Framework%20for%20the%20Evaluation%20of%0A%20%20Explainable%20AI&body=Title%3A%20Unifying%20VXAI%3A%20A%20Systematic%20Review%20and%20Framework%20for%20the%20Evaluation%20of%0A%20%20Explainable%20AI%0AAuthor%3A%20David%20Dembinsky%20and%20Adriano%20Lucieri%20and%20Stanislav%20Frolov%20and%20Hiba%20Najjar%20and%20Ko%20Watanabe%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Modern%20AI%20systems%20frequently%20rely%20on%20opaque%20black-box%20models%2C%20most%20notably%0ADeep%20Neural%20Networks%2C%20whose%20performance%20stems%20from%20complex%20architectures%20with%0Amillions%20of%20learned%20parameters.%20While%20powerful%2C%20their%20complexity%20poses%20a%20major%0Achallenge%20to%20trustworthiness%2C%20particularly%20due%20to%20a%20lack%20of%20transparency.%0AExplainable%20AI%20%28XAI%29%20addresses%20this%20issue%20by%20providing%20human-understandable%0Aexplanations%20of%20model%20behavior.%20However%2C%20to%20ensure%20their%20usefulness%20and%0Atrustworthiness%2C%20such%20explanations%20must%20be%20rigorously%20evaluated.%20Despite%20the%0Agrowing%20number%20of%20XAI%20methods%2C%20the%20field%20lacks%20standardized%20evaluation%0Aprotocols%20and%20consensus%20on%20appropriate%20metrics.%20To%20address%20this%20gap%2C%20we%20conduct%0Aa%20systematic%20literature%20review%20following%20the%20Preferred%20Reporting%20Items%20for%0ASystematic%20Reviews%20and%20Meta-Analyses%20%28PRISMA%29%20guidelines%20and%20introduce%20a%0Aunified%20framework%20for%20the%20eValuation%20of%20XAI%20%28VXAI%29.%20We%20identify%20362%20relevant%0Apublications%20and%20aggregate%20their%20contributions%20into%2041%20functionally%20similar%0Ametric%20groups.%20In%20addition%2C%20we%20propose%20a%20three-dimensional%20categorization%0Ascheme%20spanning%20explanation%20type%2C%20evaluation%20contextuality%2C%20and%20explanation%0Aquality%20desiderata.%20Our%20framework%20provides%20the%20most%20comprehensive%20and%0Astructured%20overview%20of%20VXAI%20to%20date.%20It%20supports%20systematic%20metric%20selection%2C%0Apromotes%20comparability%20across%20methods%2C%20and%20offers%20a%20flexible%20foundation%20for%0Afuture%20extensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnifying%2520VXAI%253A%2520A%2520Systematic%2520Review%2520and%2520Framework%2520for%2520the%2520Evaluation%2520of%250A%2520%2520Explainable%2520AI%26entry.906535625%3DDavid%2520Dembinsky%2520and%2520Adriano%2520Lucieri%2520and%2520Stanislav%2520Frolov%2520and%2520Hiba%2520Najjar%2520and%2520Ko%2520Watanabe%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Modern%2520AI%2520systems%2520frequently%2520rely%2520on%2520opaque%2520black-box%2520models%252C%2520most%2520notably%250ADeep%2520Neural%2520Networks%252C%2520whose%2520performance%2520stems%2520from%2520complex%2520architectures%2520with%250Amillions%2520of%2520learned%2520parameters.%2520While%2520powerful%252C%2520their%2520complexity%2520poses%2520a%2520major%250Achallenge%2520to%2520trustworthiness%252C%2520particularly%2520due%2520to%2520a%2520lack%2520of%2520transparency.%250AExplainable%2520AI%2520%2528XAI%2529%2520addresses%2520this%2520issue%2520by%2520providing%2520human-understandable%250Aexplanations%2520of%2520model%2520behavior.%2520However%252C%2520to%2520ensure%2520their%2520usefulness%2520and%250Atrustworthiness%252C%2520such%2520explanations%2520must%2520be%2520rigorously%2520evaluated.%2520Despite%2520the%250Agrowing%2520number%2520of%2520XAI%2520methods%252C%2520the%2520field%2520lacks%2520standardized%2520evaluation%250Aprotocols%2520and%2520consensus%2520on%2520appropriate%2520metrics.%2520To%2520address%2520this%2520gap%252C%2520we%2520conduct%250Aa%2520systematic%2520literature%2520review%2520following%2520the%2520Preferred%2520Reporting%2520Items%2520for%250ASystematic%2520Reviews%2520and%2520Meta-Analyses%2520%2528PRISMA%2529%2520guidelines%2520and%2520introduce%2520a%250Aunified%2520framework%2520for%2520the%2520eValuation%2520of%2520XAI%2520%2528VXAI%2529.%2520We%2520identify%2520362%2520relevant%250Apublications%2520and%2520aggregate%2520their%2520contributions%2520into%252041%2520functionally%2520similar%250Ametric%2520groups.%2520In%2520addition%252C%2520we%2520propose%2520a%2520three-dimensional%2520categorization%250Ascheme%2520spanning%2520explanation%2520type%252C%2520evaluation%2520contextuality%252C%2520and%2520explanation%250Aquality%2520desiderata.%2520Our%2520framework%2520provides%2520the%2520most%2520comprehensive%2520and%250Astructured%2520overview%2520of%2520VXAI%2520to%2520date.%2520It%2520supports%2520systematic%2520metric%2520selection%252C%250Apromotes%2520comparability%2520across%2520methods%252C%2520and%2520offers%2520a%2520flexible%2520foundation%2520for%250Afuture%2520extensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unifying%20VXAI%3A%20A%20Systematic%20Review%20and%20Framework%20for%20the%20Evaluation%20of%0A%20%20Explainable%20AI&entry.906535625=David%20Dembinsky%20and%20Adriano%20Lucieri%20and%20Stanislav%20Frolov%20and%20Hiba%20Najjar%20and%20Ko%20Watanabe%20and%20Andreas%20Dengel&entry.1292438233=%20%20Modern%20AI%20systems%20frequently%20rely%20on%20opaque%20black-box%20models%2C%20most%20notably%0ADeep%20Neural%20Networks%2C%20whose%20performance%20stems%20from%20complex%20architectures%20with%0Amillions%20of%20learned%20parameters.%20While%20powerful%2C%20their%20complexity%20poses%20a%20major%0Achallenge%20to%20trustworthiness%2C%20particularly%20due%20to%20a%20lack%20of%20transparency.%0AExplainable%20AI%20%28XAI%29%20addresses%20this%20issue%20by%20providing%20human-understandable%0Aexplanations%20of%20model%20behavior.%20However%2C%20to%20ensure%20their%20usefulness%20and%0Atrustworthiness%2C%20such%20explanations%20must%20be%20rigorously%20evaluated.%20Despite%20the%0Agrowing%20number%20of%20XAI%20methods%2C%20the%20field%20lacks%20standardized%20evaluation%0Aprotocols%20and%20consensus%20on%20appropriate%20metrics.%20To%20address%20this%20gap%2C%20we%20conduct%0Aa%20systematic%20literature%20review%20following%20the%20Preferred%20Reporting%20Items%20for%0ASystematic%20Reviews%20and%20Meta-Analyses%20%28PRISMA%29%20guidelines%20and%20introduce%20a%0Aunified%20framework%20for%20the%20eValuation%20of%20XAI%20%28VXAI%29.%20We%20identify%20362%20relevant%0Apublications%20and%20aggregate%20their%20contributions%20into%2041%20functionally%20similar%0Ametric%20groups.%20In%20addition%2C%20we%20propose%20a%20three-dimensional%20categorization%0Ascheme%20spanning%20explanation%20type%2C%20evaluation%20contextuality%2C%20and%20explanation%0Aquality%20desiderata.%20Our%20framework%20provides%20the%20most%20comprehensive%20and%0Astructured%20overview%20of%20VXAI%20to%20date.%20It%20supports%20systematic%20metric%20selection%2C%0Apromotes%20comparability%20across%20methods%2C%20and%20offers%20a%20flexible%20foundation%20for%0Afuture%20extensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15408v1&entry.124074799=Read"},
{"title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning", "author": "Gabrel J. Perin and Runjin Chen and Xuxi Chen and Nina S. T. Hirata and Zhangyang Wang and Junyuan Hong", "abstract": "  Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.\n", "link": "http://arxiv.org/abs/2506.15606v1", "date": "2025-06-18", "relevancy": 1.9961, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5008}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoX%3A%20Low-Rank%20Extrapolation%20Robustifies%20LLM%20Safety%20Against%20Fine-tuning&body=Title%3A%20LoX%3A%20Low-Rank%20Extrapolation%20Robustifies%20LLM%20Safety%20Against%20Fine-tuning%0AAuthor%3A%20Gabrel%20J.%20Perin%20and%20Runjin%20Chen%20and%20Xuxi%20Chen%20and%20Nina%20S.%20T.%20Hirata%20and%20Zhangyang%20Wang%20and%20Junyuan%20Hong%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20indispensable%20in%20real-world%0Aapplications.%20However%2C%20their%20widespread%20adoption%20raises%20significant%20safety%0Aconcerns%2C%20particularly%20in%20responding%20to%20socially%20harmful%20questions.%20Despite%0Asubstantial%20efforts%20to%20improve%20model%20safety%20through%20alignment%2C%20aligned%20models%0Acan%20still%20have%20their%20safety%20protections%20undermined%20by%20subsequent%20fine-tuning%20-%0Aeven%20when%20the%20additional%20training%20data%20appears%20benign.%20In%20this%20paper%2C%20we%0Aempirically%20demonstrate%20that%20this%20vulnerability%20stems%20from%20the%20sensitivity%20of%0Asafety-critical%20low-rank%20subspaces%20in%20LLM%20parameters%20to%20fine-tuning.%20Building%0Aon%20this%20insight%2C%20we%20propose%20a%20novel%20training-free%20method%2C%20termed%20Low-Rank%0AExtrapolation%20%28LoX%29%2C%20to%20enhance%20safety%20robustness%20by%20extrapolating%20the%20safety%0Asubspace%20of%20an%20aligned%20LLM.%20Our%20experimental%20results%20confirm%20the%20effectiveness%0Aof%20LoX%2C%20demonstrating%20significant%20improvements%20in%20robustness%20against%20both%0Abenign%20and%20malicious%20fine-tuning%20attacks%20while%20preserving%20the%20model%27s%0Aadaptability%20to%20new%20tasks.%20For%20instance%2C%20LoX%20leads%20to%2011%25%20to%2054%25%20absolute%0Areductions%20in%20attack%20success%20rates%20%28ASR%29%20facing%20benign%20or%20malicious%20fine-tuning%0Aattacks.%20By%20investigating%20the%20ASR%20landscape%20of%20parameters%2C%20we%20attribute%20the%0Asuccess%20of%20LoX%20to%20that%20the%20extrapolation%20moves%20LLM%20parameters%20to%20a%20flatter%0Azone%2C%20thereby%20less%20sensitive%20to%20perturbations.%20The%20code%20is%20available%20at%0Agithub.com/VITA-Group/LoX.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoX%253A%2520Low-Rank%2520Extrapolation%2520Robustifies%2520LLM%2520Safety%2520Against%2520Fine-tuning%26entry.906535625%3DGabrel%2520J.%2520Perin%2520and%2520Runjin%2520Chen%2520and%2520Xuxi%2520Chen%2520and%2520Nina%2520S.%2520T.%2520Hirata%2520and%2520Zhangyang%2520Wang%2520and%2520Junyuan%2520Hong%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520indispensable%2520in%2520real-world%250Aapplications.%2520However%252C%2520their%2520widespread%2520adoption%2520raises%2520significant%2520safety%250Aconcerns%252C%2520particularly%2520in%2520responding%2520to%2520socially%2520harmful%2520questions.%2520Despite%250Asubstantial%2520efforts%2520to%2520improve%2520model%2520safety%2520through%2520alignment%252C%2520aligned%2520models%250Acan%2520still%2520have%2520their%2520safety%2520protections%2520undermined%2520by%2520subsequent%2520fine-tuning%2520-%250Aeven%2520when%2520the%2520additional%2520training%2520data%2520appears%2520benign.%2520In%2520this%2520paper%252C%2520we%250Aempirically%2520demonstrate%2520that%2520this%2520vulnerability%2520stems%2520from%2520the%2520sensitivity%2520of%250Asafety-critical%2520low-rank%2520subspaces%2520in%2520LLM%2520parameters%2520to%2520fine-tuning.%2520Building%250Aon%2520this%2520insight%252C%2520we%2520propose%2520a%2520novel%2520training-free%2520method%252C%2520termed%2520Low-Rank%250AExtrapolation%2520%2528LoX%2529%252C%2520to%2520enhance%2520safety%2520robustness%2520by%2520extrapolating%2520the%2520safety%250Asubspace%2520of%2520an%2520aligned%2520LLM.%2520Our%2520experimental%2520results%2520confirm%2520the%2520effectiveness%250Aof%2520LoX%252C%2520demonstrating%2520significant%2520improvements%2520in%2520robustness%2520against%2520both%250Abenign%2520and%2520malicious%2520fine-tuning%2520attacks%2520while%2520preserving%2520the%2520model%2527s%250Aadaptability%2520to%2520new%2520tasks.%2520For%2520instance%252C%2520LoX%2520leads%2520to%252011%2525%2520to%252054%2525%2520absolute%250Areductions%2520in%2520attack%2520success%2520rates%2520%2528ASR%2529%2520facing%2520benign%2520or%2520malicious%2520fine-tuning%250Aattacks.%2520By%2520investigating%2520the%2520ASR%2520landscape%2520of%2520parameters%252C%2520we%2520attribute%2520the%250Asuccess%2520of%2520LoX%2520to%2520that%2520the%2520extrapolation%2520moves%2520LLM%2520parameters%2520to%2520a%2520flatter%250Azone%252C%2520thereby%2520less%2520sensitive%2520to%2520perturbations.%2520The%2520code%2520is%2520available%2520at%250Agithub.com/VITA-Group/LoX.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoX%3A%20Low-Rank%20Extrapolation%20Robustifies%20LLM%20Safety%20Against%20Fine-tuning&entry.906535625=Gabrel%20J.%20Perin%20and%20Runjin%20Chen%20and%20Xuxi%20Chen%20and%20Nina%20S.%20T.%20Hirata%20and%20Zhangyang%20Wang%20and%20Junyuan%20Hong&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20indispensable%20in%20real-world%0Aapplications.%20However%2C%20their%20widespread%20adoption%20raises%20significant%20safety%0Aconcerns%2C%20particularly%20in%20responding%20to%20socially%20harmful%20questions.%20Despite%0Asubstantial%20efforts%20to%20improve%20model%20safety%20through%20alignment%2C%20aligned%20models%0Acan%20still%20have%20their%20safety%20protections%20undermined%20by%20subsequent%20fine-tuning%20-%0Aeven%20when%20the%20additional%20training%20data%20appears%20benign.%20In%20this%20paper%2C%20we%0Aempirically%20demonstrate%20that%20this%20vulnerability%20stems%20from%20the%20sensitivity%20of%0Asafety-critical%20low-rank%20subspaces%20in%20LLM%20parameters%20to%20fine-tuning.%20Building%0Aon%20this%20insight%2C%20we%20propose%20a%20novel%20training-free%20method%2C%20termed%20Low-Rank%0AExtrapolation%20%28LoX%29%2C%20to%20enhance%20safety%20robustness%20by%20extrapolating%20the%20safety%0Asubspace%20of%20an%20aligned%20LLM.%20Our%20experimental%20results%20confirm%20the%20effectiveness%0Aof%20LoX%2C%20demonstrating%20significant%20improvements%20in%20robustness%20against%20both%0Abenign%20and%20malicious%20fine-tuning%20attacks%20while%20preserving%20the%20model%27s%0Aadaptability%20to%20new%20tasks.%20For%20instance%2C%20LoX%20leads%20to%2011%25%20to%2054%25%20absolute%0Areductions%20in%20attack%20success%20rates%20%28ASR%29%20facing%20benign%20or%20malicious%20fine-tuning%0Aattacks.%20By%20investigating%20the%20ASR%20landscape%20of%20parameters%2C%20we%20attribute%20the%0Asuccess%20of%20LoX%20to%20that%20the%20extrapolation%20moves%20LLM%20parameters%20to%20a%20flatter%0Azone%2C%20thereby%20less%20sensitive%20to%20perturbations.%20The%20code%20is%20available%20at%0Agithub.com/VITA-Group/LoX.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15606v1&entry.124074799=Read"},
{"title": "Efficient Navigation Among Movable Obstacles using a Mobile Manipulator\n  via Hierarchical Policy Learning", "author": "Taegeun Yang and Jiwoo Hwang and Jeil Jeong and Minsung Yoon and Sung-Eui Yoon", "abstract": "  We propose a hierarchical reinforcement learning (HRL) framework for\nefficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator.\nOur approach combines interaction-based obstacle property estimation with\nstructured pushing strategies, facilitating the dynamic manipulation of\nunforeseen obstacles while adhering to a pre-planned global path. The\nhigh-level policy generates pushing commands that consider environmental\nconstraints and path-tracking objectives, while the low-level policy precisely\nand stably executes these commands through coordinated whole-body movements.\nComprehensive simulation-based experiments demonstrate improvements in\nperforming NAMO tasks, including higher success rates, shortened traversed path\nlength, and reduced goal-reaching times, compared to baselines. Additionally,\nablation studies assess the efficacy of each component, while a qualitative\nanalysis further validates the accuracy and reliability of the real-time\nobstacle property estimation.\n", "link": "http://arxiv.org/abs/2506.15380v1", "date": "2025-06-18", "relevancy": 1.7269, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5841}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5825}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Navigation%20Among%20Movable%20Obstacles%20using%20a%20Mobile%20Manipulator%0A%20%20via%20Hierarchical%20Policy%20Learning&body=Title%3A%20Efficient%20Navigation%20Among%20Movable%20Obstacles%20using%20a%20Mobile%20Manipulator%0A%20%20via%20Hierarchical%20Policy%20Learning%0AAuthor%3A%20Taegeun%20Yang%20and%20Jiwoo%20Hwang%20and%20Jeil%20Jeong%20and%20Minsung%20Yoon%20and%20Sung-Eui%20Yoon%0AAbstract%3A%20%20%20We%20propose%20a%20hierarchical%20reinforcement%20learning%20%28HRL%29%20framework%20for%0Aefficient%20Navigation%20Among%20Movable%20Obstacles%20%28NAMO%29%20using%20a%20mobile%20manipulator.%0AOur%20approach%20combines%20interaction-based%20obstacle%20property%20estimation%20with%0Astructured%20pushing%20strategies%2C%20facilitating%20the%20dynamic%20manipulation%20of%0Aunforeseen%20obstacles%20while%20adhering%20to%20a%20pre-planned%20global%20path.%20The%0Ahigh-level%20policy%20generates%20pushing%20commands%20that%20consider%20environmental%0Aconstraints%20and%20path-tracking%20objectives%2C%20while%20the%20low-level%20policy%20precisely%0Aand%20stably%20executes%20these%20commands%20through%20coordinated%20whole-body%20movements.%0AComprehensive%20simulation-based%20experiments%20demonstrate%20improvements%20in%0Aperforming%20NAMO%20tasks%2C%20including%20higher%20success%20rates%2C%20shortened%20traversed%20path%0Alength%2C%20and%20reduced%20goal-reaching%20times%2C%20compared%20to%20baselines.%20Additionally%2C%0Aablation%20studies%20assess%20the%20efficacy%20of%20each%20component%2C%20while%20a%20qualitative%0Aanalysis%20further%20validates%20the%20accuracy%20and%20reliability%20of%20the%20real-time%0Aobstacle%20property%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Navigation%2520Among%2520Movable%2520Obstacles%2520using%2520a%2520Mobile%2520Manipulator%250A%2520%2520via%2520Hierarchical%2520Policy%2520Learning%26entry.906535625%3DTaegeun%2520Yang%2520and%2520Jiwoo%2520Hwang%2520and%2520Jeil%2520Jeong%2520and%2520Minsung%2520Yoon%2520and%2520Sung-Eui%2520Yoon%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520hierarchical%2520reinforcement%2520learning%2520%2528HRL%2529%2520framework%2520for%250Aefficient%2520Navigation%2520Among%2520Movable%2520Obstacles%2520%2528NAMO%2529%2520using%2520a%2520mobile%2520manipulator.%250AOur%2520approach%2520combines%2520interaction-based%2520obstacle%2520property%2520estimation%2520with%250Astructured%2520pushing%2520strategies%252C%2520facilitating%2520the%2520dynamic%2520manipulation%2520of%250Aunforeseen%2520obstacles%2520while%2520adhering%2520to%2520a%2520pre-planned%2520global%2520path.%2520The%250Ahigh-level%2520policy%2520generates%2520pushing%2520commands%2520that%2520consider%2520environmental%250Aconstraints%2520and%2520path-tracking%2520objectives%252C%2520while%2520the%2520low-level%2520policy%2520precisely%250Aand%2520stably%2520executes%2520these%2520commands%2520through%2520coordinated%2520whole-body%2520movements.%250AComprehensive%2520simulation-based%2520experiments%2520demonstrate%2520improvements%2520in%250Aperforming%2520NAMO%2520tasks%252C%2520including%2520higher%2520success%2520rates%252C%2520shortened%2520traversed%2520path%250Alength%252C%2520and%2520reduced%2520goal-reaching%2520times%252C%2520compared%2520to%2520baselines.%2520Additionally%252C%250Aablation%2520studies%2520assess%2520the%2520efficacy%2520of%2520each%2520component%252C%2520while%2520a%2520qualitative%250Aanalysis%2520further%2520validates%2520the%2520accuracy%2520and%2520reliability%2520of%2520the%2520real-time%250Aobstacle%2520property%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Navigation%20Among%20Movable%20Obstacles%20using%20a%20Mobile%20Manipulator%0A%20%20via%20Hierarchical%20Policy%20Learning&entry.906535625=Taegeun%20Yang%20and%20Jiwoo%20Hwang%20and%20Jeil%20Jeong%20and%20Minsung%20Yoon%20and%20Sung-Eui%20Yoon&entry.1292438233=%20%20We%20propose%20a%20hierarchical%20reinforcement%20learning%20%28HRL%29%20framework%20for%0Aefficient%20Navigation%20Among%20Movable%20Obstacles%20%28NAMO%29%20using%20a%20mobile%20manipulator.%0AOur%20approach%20combines%20interaction-based%20obstacle%20property%20estimation%20with%0Astructured%20pushing%20strategies%2C%20facilitating%20the%20dynamic%20manipulation%20of%0Aunforeseen%20obstacles%20while%20adhering%20to%20a%20pre-planned%20global%20path.%20The%0Ahigh-level%20policy%20generates%20pushing%20commands%20that%20consider%20environmental%0Aconstraints%20and%20path-tracking%20objectives%2C%20while%20the%20low-level%20policy%20precisely%0Aand%20stably%20executes%20these%20commands%20through%20coordinated%20whole-body%20movements.%0AComprehensive%20simulation-based%20experiments%20demonstrate%20improvements%20in%0Aperforming%20NAMO%20tasks%2C%20including%20higher%20success%20rates%2C%20shortened%20traversed%20path%0Alength%2C%20and%20reduced%20goal-reaching%20times%2C%20compared%20to%20baselines.%20Additionally%2C%0Aablation%20studies%20assess%20the%20efficacy%20of%20each%20component%2C%20while%20a%20qualitative%0Aanalysis%20further%20validates%20the%20accuracy%20and%20reliability%20of%20the%20real-time%0Aobstacle%20property%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15380v1&entry.124074799=Read"},
{"title": "Adding Chocolate to Mint: Mitigating Metric Interference in Machine\n  Translation", "author": "Jos\u00e9 Pombal and Nuno M. Guerreiro and Ricardo Rei and Andr\u00e9 F. T. Martins", "abstract": "  As automatic metrics become increasingly stronger and widely adopted, the\nrisk of unintentionally \"gaming the metric\" during model development rises.\nThis issue is caused by metric interference (MINT), i.e., the use of the same\nor related metrics for both model tuning and evaluation. MINT can misguide\npractitioners into being overoptimistic about the performance of their systems:\nas system outputs become a function of the interfering metric, their estimated\nquality loses correlation with human judgments. In this work, we analyze two\ncommon cases of MINT in machine translation-related tasks: filtering of\ntraining data, and decoding with quality signals. Importantly, we find that\nMINT strongly distorts instance-level metric scores, even when metrics are not\ndirectly optimized for-questioning the common strategy of leveraging a\ndifferent, yet related metric for evaluation that is not used for tuning. To\naddress this problem, we propose MINTADJUST, a method for more reliable\nevaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks\ntranslations and systems more accurately than state-of-the-art metrics across a\nmajority of language pairs, especially for high-quality systems. Furthermore,\nMINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.\n", "link": "http://arxiv.org/abs/2503.08327v2", "date": "2025-06-18", "relevancy": 1.8347, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4949}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4517}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adding%20Chocolate%20to%20Mint%3A%20Mitigating%20Metric%20Interference%20in%20Machine%0A%20%20Translation&body=Title%3A%20Adding%20Chocolate%20to%20Mint%3A%20Mitigating%20Metric%20Interference%20in%20Machine%0A%20%20Translation%0AAuthor%3A%20Jos%C3%A9%20Pombal%20and%20Nuno%20M.%20Guerreiro%20and%20Ricardo%20Rei%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20As%20automatic%20metrics%20become%20increasingly%20stronger%20and%20widely%20adopted%2C%20the%0Arisk%20of%20unintentionally%20%22gaming%20the%20metric%22%20during%20model%20development%20rises.%0AThis%20issue%20is%20caused%20by%20metric%20interference%20%28MINT%29%2C%20i.e.%2C%20the%20use%20of%20the%20same%0Aor%20related%20metrics%20for%20both%20model%20tuning%20and%20evaluation.%20MINT%20can%20misguide%0Apractitioners%20into%20being%20overoptimistic%20about%20the%20performance%20of%20their%20systems%3A%0Aas%20system%20outputs%20become%20a%20function%20of%20the%20interfering%20metric%2C%20their%20estimated%0Aquality%20loses%20correlation%20with%20human%20judgments.%20In%20this%20work%2C%20we%20analyze%20two%0Acommon%20cases%20of%20MINT%20in%20machine%20translation-related%20tasks%3A%20filtering%20of%0Atraining%20data%2C%20and%20decoding%20with%20quality%20signals.%20Importantly%2C%20we%20find%20that%0AMINT%20strongly%20distorts%20instance-level%20metric%20scores%2C%20even%20when%20metrics%20are%20not%0Adirectly%20optimized%20for-questioning%20the%20common%20strategy%20of%20leveraging%20a%0Adifferent%2C%20yet%20related%20metric%20for%20evaluation%20that%20is%20not%20used%20for%20tuning.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20MINTADJUST%2C%20a%20method%20for%20more%20reliable%0Aevaluation%20under%20MINT.%20On%20the%20WMT24%20MT%20shared%20task%20test%20set%2C%20MINTADJUST%20ranks%0Atranslations%20and%20systems%20more%20accurately%20than%20state-of-the-art%20metrics%20across%20a%0Amajority%20of%20language%20pairs%2C%20especially%20for%20high-quality%20systems.%20Furthermore%2C%0AMINTADJUST%20outperforms%20AUTORANK%2C%20the%20ensembling%20method%20used%20by%20the%20organizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdding%2520Chocolate%2520to%2520Mint%253A%2520Mitigating%2520Metric%2520Interference%2520in%2520Machine%250A%2520%2520Translation%26entry.906535625%3DJos%25C3%25A9%2520Pombal%2520and%2520Nuno%2520M.%2520Guerreiro%2520and%2520Ricardo%2520Rei%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520As%2520automatic%2520metrics%2520become%2520increasingly%2520stronger%2520and%2520widely%2520adopted%252C%2520the%250Arisk%2520of%2520unintentionally%2520%2522gaming%2520the%2520metric%2522%2520during%2520model%2520development%2520rises.%250AThis%2520issue%2520is%2520caused%2520by%2520metric%2520interference%2520%2528MINT%2529%252C%2520i.e.%252C%2520the%2520use%2520of%2520the%2520same%250Aor%2520related%2520metrics%2520for%2520both%2520model%2520tuning%2520and%2520evaluation.%2520MINT%2520can%2520misguide%250Apractitioners%2520into%2520being%2520overoptimistic%2520about%2520the%2520performance%2520of%2520their%2520systems%253A%250Aas%2520system%2520outputs%2520become%2520a%2520function%2520of%2520the%2520interfering%2520metric%252C%2520their%2520estimated%250Aquality%2520loses%2520correlation%2520with%2520human%2520judgments.%2520In%2520this%2520work%252C%2520we%2520analyze%2520two%250Acommon%2520cases%2520of%2520MINT%2520in%2520machine%2520translation-related%2520tasks%253A%2520filtering%2520of%250Atraining%2520data%252C%2520and%2520decoding%2520with%2520quality%2520signals.%2520Importantly%252C%2520we%2520find%2520that%250AMINT%2520strongly%2520distorts%2520instance-level%2520metric%2520scores%252C%2520even%2520when%2520metrics%2520are%2520not%250Adirectly%2520optimized%2520for-questioning%2520the%2520common%2520strategy%2520of%2520leveraging%2520a%250Adifferent%252C%2520yet%2520related%2520metric%2520for%2520evaluation%2520that%2520is%2520not%2520used%2520for%2520tuning.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520MINTADJUST%252C%2520a%2520method%2520for%2520more%2520reliable%250Aevaluation%2520under%2520MINT.%2520On%2520the%2520WMT24%2520MT%2520shared%2520task%2520test%2520set%252C%2520MINTADJUST%2520ranks%250Atranslations%2520and%2520systems%2520more%2520accurately%2520than%2520state-of-the-art%2520metrics%2520across%2520a%250Amajority%2520of%2520language%2520pairs%252C%2520especially%2520for%2520high-quality%2520systems.%2520Furthermore%252C%250AMINTADJUST%2520outperforms%2520AUTORANK%252C%2520the%2520ensembling%2520method%2520used%2520by%2520the%2520organizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adding%20Chocolate%20to%20Mint%3A%20Mitigating%20Metric%20Interference%20in%20Machine%0A%20%20Translation&entry.906535625=Jos%C3%A9%20Pombal%20and%20Nuno%20M.%20Guerreiro%20and%20Ricardo%20Rei%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20As%20automatic%20metrics%20become%20increasingly%20stronger%20and%20widely%20adopted%2C%20the%0Arisk%20of%20unintentionally%20%22gaming%20the%20metric%22%20during%20model%20development%20rises.%0AThis%20issue%20is%20caused%20by%20metric%20interference%20%28MINT%29%2C%20i.e.%2C%20the%20use%20of%20the%20same%0Aor%20related%20metrics%20for%20both%20model%20tuning%20and%20evaluation.%20MINT%20can%20misguide%0Apractitioners%20into%20being%20overoptimistic%20about%20the%20performance%20of%20their%20systems%3A%0Aas%20system%20outputs%20become%20a%20function%20of%20the%20interfering%20metric%2C%20their%20estimated%0Aquality%20loses%20correlation%20with%20human%20judgments.%20In%20this%20work%2C%20we%20analyze%20two%0Acommon%20cases%20of%20MINT%20in%20machine%20translation-related%20tasks%3A%20filtering%20of%0Atraining%20data%2C%20and%20decoding%20with%20quality%20signals.%20Importantly%2C%20we%20find%20that%0AMINT%20strongly%20distorts%20instance-level%20metric%20scores%2C%20even%20when%20metrics%20are%20not%0Adirectly%20optimized%20for-questioning%20the%20common%20strategy%20of%20leveraging%20a%0Adifferent%2C%20yet%20related%20metric%20for%20evaluation%20that%20is%20not%20used%20for%20tuning.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20MINTADJUST%2C%20a%20method%20for%20more%20reliable%0Aevaluation%20under%20MINT.%20On%20the%20WMT24%20MT%20shared%20task%20test%20set%2C%20MINTADJUST%20ranks%0Atranslations%20and%20systems%20more%20accurately%20than%20state-of-the-art%20metrics%20across%20a%0Amajority%20of%20language%20pairs%2C%20especially%20for%20high-quality%20systems.%20Furthermore%2C%0AMINTADJUST%20outperforms%20AUTORANK%2C%20the%20ensembling%20method%20used%20by%20the%20organizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08327v2&entry.124074799=Read"},
{"title": "Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint\n  Phase-shift Optimization and Multi-User Power Allocation", "author": "Kangwei Qi and Qiong Wu and Pingyi Fan and Nan Cheng and Wen Chen and Khaled B. Letaief", "abstract": "  Vehicular edge computing (VEC) is an emerging technology with significant\npotential in the field of internet of vehicles (IoV), enabling vehicles to\nperform intensive computational tasks locally or offload them to nearby edge\ndevices. However, the quality of communication links may be severely\ndeteriorated due to obstacles such as buildings, impeding the offloading\nprocess. To address this challenge, we introduce the use of Reconfigurable\nIntelligent Surfaces (RIS), which provide alternative communication pathways to\nassist vehicular communication. By dynamically adjusting the phase-shift of the\nRIS, the performance of VEC systems can be substantially improved. In this\nwork, we consider a RIS-assisted VEC system, and design an optimal scheme for\nlocal execution power, offloading power, and RIS phase-shift, where random task\narrivals and channel variations are taken into account. To address the scheme,\nwe propose an innovative deep reinforcement learning (DRL) framework that\ncombines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing\nRIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy\nGradient (MADDPG) algorithm for optimizing the power allocation of vehicle user\n(VU). Simulation results show that our proposed scheme outperforms the\ntraditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient\n(TD3) and some typical stochastic schemes.\n", "link": "http://arxiv.org/abs/2407.13123v2", "date": "2025-06-18", "relevancy": 1.8831, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4806}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconfigurable%20Intelligent%20Surface%20Aided%20Vehicular%20Edge%20Computing%3A%20Joint%0A%20%20Phase-shift%20Optimization%20and%20Multi-User%20Power%20Allocation&body=Title%3A%20Reconfigurable%20Intelligent%20Surface%20Aided%20Vehicular%20Edge%20Computing%3A%20Joint%0A%20%20Phase-shift%20Optimization%20and%20Multi-User%20Power%20Allocation%0AAuthor%3A%20Kangwei%20Qi%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Nan%20Cheng%20and%20Wen%20Chen%20and%20Khaled%20B.%20Letaief%0AAbstract%3A%20%20%20Vehicular%20edge%20computing%20%28VEC%29%20is%20an%20emerging%20technology%20with%20significant%0Apotential%20in%20the%20field%20of%20internet%20of%20vehicles%20%28IoV%29%2C%20enabling%20vehicles%20to%0Aperform%20intensive%20computational%20tasks%20locally%20or%20offload%20them%20to%20nearby%20edge%0Adevices.%20However%2C%20the%20quality%20of%20communication%20links%20may%20be%20severely%0Adeteriorated%20due%20to%20obstacles%20such%20as%20buildings%2C%20impeding%20the%20offloading%0Aprocess.%20To%20address%20this%20challenge%2C%20we%20introduce%20the%20use%20of%20Reconfigurable%0AIntelligent%20Surfaces%20%28RIS%29%2C%20which%20provide%20alternative%20communication%20pathways%20to%0Aassist%20vehicular%20communication.%20By%20dynamically%20adjusting%20the%20phase-shift%20of%20the%0ARIS%2C%20the%20performance%20of%20VEC%20systems%20can%20be%20substantially%20improved.%20In%20this%0Awork%2C%20we%20consider%20a%20RIS-assisted%20VEC%20system%2C%20and%20design%20an%20optimal%20scheme%20for%0Alocal%20execution%20power%2C%20offloading%20power%2C%20and%20RIS%20phase-shift%2C%20where%20random%20task%0Aarrivals%20and%20channel%20variations%20are%20taken%20into%20account.%20To%20address%20the%20scheme%2C%0Awe%20propose%20an%20innovative%20deep%20reinforcement%20learning%20%28DRL%29%20framework%20that%0Acombines%20the%20Deep%20Deterministic%20Policy%20Gradient%20%28DDPG%29%20algorithm%20for%20optimizing%0ARIS%20phase-shift%20coefficients%20and%20the%20Multi-Agent%20Deep%20Deterministic%20Policy%0AGradient%20%28MADDPG%29%20algorithm%20for%20optimizing%20the%20power%20allocation%20of%20vehicle%20user%0A%28VU%29.%20Simulation%20results%20show%20that%20our%20proposed%20scheme%20outperforms%20the%0Atraditional%20centralized%20DDPG%2C%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%0A%28TD3%29%20and%20some%20typical%20stochastic%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13123v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconfigurable%2520Intelligent%2520Surface%2520Aided%2520Vehicular%2520Edge%2520Computing%253A%2520Joint%250A%2520%2520Phase-shift%2520Optimization%2520and%2520Multi-User%2520Power%2520Allocation%26entry.906535625%3DKangwei%2520Qi%2520and%2520Qiong%2520Wu%2520and%2520Pingyi%2520Fan%2520and%2520Nan%2520Cheng%2520and%2520Wen%2520Chen%2520and%2520Khaled%2520B.%2520Letaief%26entry.1292438233%3D%2520%2520Vehicular%2520edge%2520computing%2520%2528VEC%2529%2520is%2520an%2520emerging%2520technology%2520with%2520significant%250Apotential%2520in%2520the%2520field%2520of%2520internet%2520of%2520vehicles%2520%2528IoV%2529%252C%2520enabling%2520vehicles%2520to%250Aperform%2520intensive%2520computational%2520tasks%2520locally%2520or%2520offload%2520them%2520to%2520nearby%2520edge%250Adevices.%2520However%252C%2520the%2520quality%2520of%2520communication%2520links%2520may%2520be%2520severely%250Adeteriorated%2520due%2520to%2520obstacles%2520such%2520as%2520buildings%252C%2520impeding%2520the%2520offloading%250Aprocess.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520the%2520use%2520of%2520Reconfigurable%250AIntelligent%2520Surfaces%2520%2528RIS%2529%252C%2520which%2520provide%2520alternative%2520communication%2520pathways%2520to%250Aassist%2520vehicular%2520communication.%2520By%2520dynamically%2520adjusting%2520the%2520phase-shift%2520of%2520the%250ARIS%252C%2520the%2520performance%2520of%2520VEC%2520systems%2520can%2520be%2520substantially%2520improved.%2520In%2520this%250Awork%252C%2520we%2520consider%2520a%2520RIS-assisted%2520VEC%2520system%252C%2520and%2520design%2520an%2520optimal%2520scheme%2520for%250Alocal%2520execution%2520power%252C%2520offloading%2520power%252C%2520and%2520RIS%2520phase-shift%252C%2520where%2520random%2520task%250Aarrivals%2520and%2520channel%2520variations%2520are%2520taken%2520into%2520account.%2520To%2520address%2520the%2520scheme%252C%250Awe%2520propose%2520an%2520innovative%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%2520framework%2520that%250Acombines%2520the%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520%2528DDPG%2529%2520algorithm%2520for%2520optimizing%250ARIS%2520phase-shift%2520coefficients%2520and%2520the%2520Multi-Agent%2520Deep%2520Deterministic%2520Policy%250AGradient%2520%2528MADDPG%2529%2520algorithm%2520for%2520optimizing%2520the%2520power%2520allocation%2520of%2520vehicle%2520user%250A%2528VU%2529.%2520Simulation%2520results%2520show%2520that%2520our%2520proposed%2520scheme%2520outperforms%2520the%250Atraditional%2520centralized%2520DDPG%252C%2520Twin%2520Delayed%2520Deep%2520Deterministic%2520Policy%2520Gradient%250A%2528TD3%2529%2520and%2520some%2520typical%2520stochastic%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13123v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconfigurable%20Intelligent%20Surface%20Aided%20Vehicular%20Edge%20Computing%3A%20Joint%0A%20%20Phase-shift%20Optimization%20and%20Multi-User%20Power%20Allocation&entry.906535625=Kangwei%20Qi%20and%20Qiong%20Wu%20and%20Pingyi%20Fan%20and%20Nan%20Cheng%20and%20Wen%20Chen%20and%20Khaled%20B.%20Letaief&entry.1292438233=%20%20Vehicular%20edge%20computing%20%28VEC%29%20is%20an%20emerging%20technology%20with%20significant%0Apotential%20in%20the%20field%20of%20internet%20of%20vehicles%20%28IoV%29%2C%20enabling%20vehicles%20to%0Aperform%20intensive%20computational%20tasks%20locally%20or%20offload%20them%20to%20nearby%20edge%0Adevices.%20However%2C%20the%20quality%20of%20communication%20links%20may%20be%20severely%0Adeteriorated%20due%20to%20obstacles%20such%20as%20buildings%2C%20impeding%20the%20offloading%0Aprocess.%20To%20address%20this%20challenge%2C%20we%20introduce%20the%20use%20of%20Reconfigurable%0AIntelligent%20Surfaces%20%28RIS%29%2C%20which%20provide%20alternative%20communication%20pathways%20to%0Aassist%20vehicular%20communication.%20By%20dynamically%20adjusting%20the%20phase-shift%20of%20the%0ARIS%2C%20the%20performance%20of%20VEC%20systems%20can%20be%20substantially%20improved.%20In%20this%0Awork%2C%20we%20consider%20a%20RIS-assisted%20VEC%20system%2C%20and%20design%20an%20optimal%20scheme%20for%0Alocal%20execution%20power%2C%20offloading%20power%2C%20and%20RIS%20phase-shift%2C%20where%20random%20task%0Aarrivals%20and%20channel%20variations%20are%20taken%20into%20account.%20To%20address%20the%20scheme%2C%0Awe%20propose%20an%20innovative%20deep%20reinforcement%20learning%20%28DRL%29%20framework%20that%0Acombines%20the%20Deep%20Deterministic%20Policy%20Gradient%20%28DDPG%29%20algorithm%20for%20optimizing%0ARIS%20phase-shift%20coefficients%20and%20the%20Multi-Agent%20Deep%20Deterministic%20Policy%0AGradient%20%28MADDPG%29%20algorithm%20for%20optimizing%20the%20power%20allocation%20of%20vehicle%20user%0A%28VU%29.%20Simulation%20results%20show%20that%20our%20proposed%20scheme%20outperforms%20the%0Atraditional%20centralized%20DDPG%2C%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%0A%28TD3%29%20and%20some%20typical%20stochastic%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13123v2&entry.124074799=Read"},
{"title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers", "author": "Tommaso Green and Martin Gubri and Haritz Puerto and Sangdoo Yun and Seong Joon Oh", "abstract": "  We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.\n", "link": "http://arxiv.org/abs/2506.15674v1", "date": "2025-06-18", "relevancy": 1.6566, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.42}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leaky%20Thoughts%3A%20Large%20Reasoning%20Models%20Are%20Not%20Private%20Thinkers&body=Title%3A%20Leaky%20Thoughts%3A%20Large%20Reasoning%20Models%20Are%20Not%20Private%20Thinkers%0AAuthor%3A%20Tommaso%20Green%20and%20Martin%20Gubri%20and%20Haritz%20Puerto%20and%20Sangdoo%20Yun%20and%20Seong%20Joon%20Oh%0AAbstract%3A%20%20%20We%20study%20privacy%20leakage%20in%20the%20reasoning%20traces%20of%20large%20reasoning%20models%0Aused%20as%20personal%20agents.%20Unlike%20final%20outputs%2C%20reasoning%20traces%20are%20often%0Aassumed%20to%20be%20internal%20and%20safe.%20We%20challenge%20this%20assumption%20by%20showing%20that%0Areasoning%20traces%20frequently%20contain%20sensitive%20user%20data%2C%20which%20can%20be%20extracted%0Avia%20prompt%20injections%20or%20accidentally%20leak%20into%20outputs.%20Through%20probing%20and%0Aagentic%20evaluations%2C%20we%20demonstrate%20that%20test-time%20compute%20approaches%2C%0Aparticularly%20increased%20reasoning%20steps%2C%20amplify%20such%20leakage.%20While%20increasing%0Athe%20budget%20of%20those%20test-time%20compute%20approaches%20makes%20models%20more%20cautious%20in%0Atheir%20final%20answers%2C%20it%20also%20leads%20them%20to%20reason%20more%20verbosely%20and%20leak%20more%0Ain%20their%20own%20thinking.%20This%20reveals%20a%20core%20tension%3A%20reasoning%20improves%20utility%0Abut%20enlarges%20the%20privacy%20attack%20surface.%20We%20argue%20that%20safety%20efforts%20must%0Aextend%20to%20the%20model%27s%20internal%20thinking%2C%20not%20just%20its%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeaky%2520Thoughts%253A%2520Large%2520Reasoning%2520Models%2520Are%2520Not%2520Private%2520Thinkers%26entry.906535625%3DTommaso%2520Green%2520and%2520Martin%2520Gubri%2520and%2520Haritz%2520Puerto%2520and%2520Sangdoo%2520Yun%2520and%2520Seong%2520Joon%2520Oh%26entry.1292438233%3D%2520%2520We%2520study%2520privacy%2520leakage%2520in%2520the%2520reasoning%2520traces%2520of%2520large%2520reasoning%2520models%250Aused%2520as%2520personal%2520agents.%2520Unlike%2520final%2520outputs%252C%2520reasoning%2520traces%2520are%2520often%250Aassumed%2520to%2520be%2520internal%2520and%2520safe.%2520We%2520challenge%2520this%2520assumption%2520by%2520showing%2520that%250Areasoning%2520traces%2520frequently%2520contain%2520sensitive%2520user%2520data%252C%2520which%2520can%2520be%2520extracted%250Avia%2520prompt%2520injections%2520or%2520accidentally%2520leak%2520into%2520outputs.%2520Through%2520probing%2520and%250Aagentic%2520evaluations%252C%2520we%2520demonstrate%2520that%2520test-time%2520compute%2520approaches%252C%250Aparticularly%2520increased%2520reasoning%2520steps%252C%2520amplify%2520such%2520leakage.%2520While%2520increasing%250Athe%2520budget%2520of%2520those%2520test-time%2520compute%2520approaches%2520makes%2520models%2520more%2520cautious%2520in%250Atheir%2520final%2520answers%252C%2520it%2520also%2520leads%2520them%2520to%2520reason%2520more%2520verbosely%2520and%2520leak%2520more%250Ain%2520their%2520own%2520thinking.%2520This%2520reveals%2520a%2520core%2520tension%253A%2520reasoning%2520improves%2520utility%250Abut%2520enlarges%2520the%2520privacy%2520attack%2520surface.%2520We%2520argue%2520that%2520safety%2520efforts%2520must%250Aextend%2520to%2520the%2520model%2527s%2520internal%2520thinking%252C%2520not%2520just%2520its%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leaky%20Thoughts%3A%20Large%20Reasoning%20Models%20Are%20Not%20Private%20Thinkers&entry.906535625=Tommaso%20Green%20and%20Martin%20Gubri%20and%20Haritz%20Puerto%20and%20Sangdoo%20Yun%20and%20Seong%20Joon%20Oh&entry.1292438233=%20%20We%20study%20privacy%20leakage%20in%20the%20reasoning%20traces%20of%20large%20reasoning%20models%0Aused%20as%20personal%20agents.%20Unlike%20final%20outputs%2C%20reasoning%20traces%20are%20often%0Aassumed%20to%20be%20internal%20and%20safe.%20We%20challenge%20this%20assumption%20by%20showing%20that%0Areasoning%20traces%20frequently%20contain%20sensitive%20user%20data%2C%20which%20can%20be%20extracted%0Avia%20prompt%20injections%20or%20accidentally%20leak%20into%20outputs.%20Through%20probing%20and%0Aagentic%20evaluations%2C%20we%20demonstrate%20that%20test-time%20compute%20approaches%2C%0Aparticularly%20increased%20reasoning%20steps%2C%20amplify%20such%20leakage.%20While%20increasing%0Athe%20budget%20of%20those%20test-time%20compute%20approaches%20makes%20models%20more%20cautious%20in%0Atheir%20final%20answers%2C%20it%20also%20leads%20them%20to%20reason%20more%20verbosely%20and%20leak%20more%0Ain%20their%20own%20thinking.%20This%20reveals%20a%20core%20tension%3A%20reasoning%20improves%20utility%0Abut%20enlarges%20the%20privacy%20attack%20surface.%20We%20argue%20that%20safety%20efforts%20must%0Aextend%20to%20the%20model%27s%20internal%20thinking%2C%20not%20just%20its%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15674v1&entry.124074799=Read"},
{"title": "A Data-Integrated Framework for Learning Fractional-Order Nonlinear\n  Dynamical Systems", "author": "Bahram Yaghooti and Chengyu Li and Bruno Sinopoli", "abstract": "  This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.\n", "link": "http://arxiv.org/abs/2506.15665v1", "date": "2025-06-18", "relevancy": 1.3694, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4643}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4559}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Data-Integrated%20Framework%20for%20Learning%20Fractional-Order%20Nonlinear%0A%20%20Dynamical%20Systems&body=Title%3A%20A%20Data-Integrated%20Framework%20for%20Learning%20Fractional-Order%20Nonlinear%0A%20%20Dynamical%20Systems%0AAuthor%3A%20Bahram%20Yaghooti%20and%20Chengyu%20Li%20and%20Bruno%20Sinopoli%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20data-integrated%20framework%20for%20learning%20the%20dynamics%20of%0Afractional-order%20nonlinear%20systems%20in%20both%20discrete-time%20and%20continuous-time%0Asettings.%20The%20proposed%20framework%20consists%20of%20two%20main%20steps.%20In%20the%20first%20step%2C%0Ainput-output%20experiments%20are%20designed%20to%20generate%20the%20necessary%20datasets%20for%0Alearning%20the%20system%20dynamics%2C%20including%20the%20fractional%20order%2C%20the%20drift%20vector%0Afield%2C%20and%20the%20control%20vector%20field.%20In%20the%20second%20step%2C%20these%20datasets%2C%20along%0Awith%20the%20memory-dependent%20property%20of%20fractional-order%20systems%2C%20are%20used%20to%0Aestimate%20the%20system%27s%20fractional%20order.%20The%20drift%20and%20control%20vector%20fields%20are%0Athen%20reconstructed%20using%20orthonormal%20basis%20functions.%20To%20validate%20the%20proposed%0Aapproach%2C%20the%20algorithm%20is%20applied%20to%20four%20benchmark%20fractional-order%20systems.%0AThe%20results%20confirm%20the%20effectiveness%20of%20the%20proposed%20framework%20in%20learning%20the%0Asystem%20dynamics%20accurately.%20Finally%2C%20the%20same%20datasets%20are%20used%20to%20learn%0Aequivalent%20integer-order%20models.%20The%20numerical%20comparisons%20demonstrate%20that%0Afractional-order%20models%20better%20capture%20long-range%20dependencies%2C%20highlighting%0Athe%20limitations%20of%20integer-order%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Data-Integrated%2520Framework%2520for%2520Learning%2520Fractional-Order%2520Nonlinear%250A%2520%2520Dynamical%2520Systems%26entry.906535625%3DBahram%2520Yaghooti%2520and%2520Chengyu%2520Li%2520and%2520Bruno%2520Sinopoli%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520data-integrated%2520framework%2520for%2520learning%2520the%2520dynamics%2520of%250Afractional-order%2520nonlinear%2520systems%2520in%2520both%2520discrete-time%2520and%2520continuous-time%250Asettings.%2520The%2520proposed%2520framework%2520consists%2520of%2520two%2520main%2520steps.%2520In%2520the%2520first%2520step%252C%250Ainput-output%2520experiments%2520are%2520designed%2520to%2520generate%2520the%2520necessary%2520datasets%2520for%250Alearning%2520the%2520system%2520dynamics%252C%2520including%2520the%2520fractional%2520order%252C%2520the%2520drift%2520vector%250Afield%252C%2520and%2520the%2520control%2520vector%2520field.%2520In%2520the%2520second%2520step%252C%2520these%2520datasets%252C%2520along%250Awith%2520the%2520memory-dependent%2520property%2520of%2520fractional-order%2520systems%252C%2520are%2520used%2520to%250Aestimate%2520the%2520system%2527s%2520fractional%2520order.%2520The%2520drift%2520and%2520control%2520vector%2520fields%2520are%250Athen%2520reconstructed%2520using%2520orthonormal%2520basis%2520functions.%2520To%2520validate%2520the%2520proposed%250Aapproach%252C%2520the%2520algorithm%2520is%2520applied%2520to%2520four%2520benchmark%2520fractional-order%2520systems.%250AThe%2520results%2520confirm%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework%2520in%2520learning%2520the%250Asystem%2520dynamics%2520accurately.%2520Finally%252C%2520the%2520same%2520datasets%2520are%2520used%2520to%2520learn%250Aequivalent%2520integer-order%2520models.%2520The%2520numerical%2520comparisons%2520demonstrate%2520that%250Afractional-order%2520models%2520better%2520capture%2520long-range%2520dependencies%252C%2520highlighting%250Athe%2520limitations%2520of%2520integer-order%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Data-Integrated%20Framework%20for%20Learning%20Fractional-Order%20Nonlinear%0A%20%20Dynamical%20Systems&entry.906535625=Bahram%20Yaghooti%20and%20Chengyu%20Li%20and%20Bruno%20Sinopoli&entry.1292438233=%20%20This%20paper%20presents%20a%20data-integrated%20framework%20for%20learning%20the%20dynamics%20of%0Afractional-order%20nonlinear%20systems%20in%20both%20discrete-time%20and%20continuous-time%0Asettings.%20The%20proposed%20framework%20consists%20of%20two%20main%20steps.%20In%20the%20first%20step%2C%0Ainput-output%20experiments%20are%20designed%20to%20generate%20the%20necessary%20datasets%20for%0Alearning%20the%20system%20dynamics%2C%20including%20the%20fractional%20order%2C%20the%20drift%20vector%0Afield%2C%20and%20the%20control%20vector%20field.%20In%20the%20second%20step%2C%20these%20datasets%2C%20along%0Awith%20the%20memory-dependent%20property%20of%20fractional-order%20systems%2C%20are%20used%20to%0Aestimate%20the%20system%27s%20fractional%20order.%20The%20drift%20and%20control%20vector%20fields%20are%0Athen%20reconstructed%20using%20orthonormal%20basis%20functions.%20To%20validate%20the%20proposed%0Aapproach%2C%20the%20algorithm%20is%20applied%20to%20four%20benchmark%20fractional-order%20systems.%0AThe%20results%20confirm%20the%20effectiveness%20of%20the%20proposed%20framework%20in%20learning%20the%0Asystem%20dynamics%20accurately.%20Finally%2C%20the%20same%20datasets%20are%20used%20to%20learn%0Aequivalent%20integer-order%20models.%20The%20numerical%20comparisons%20demonstrate%20that%0Afractional-order%20models%20better%20capture%20long-range%20dependencies%2C%20highlighting%0Athe%20limitations%20of%20integer-order%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15665v1&entry.124074799=Read"},
{"title": "FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development\n  via Automatic Evaluation", "author": "Hongda Zhu and Yiwen Zhang and Bing Zhao and Jingzhe Ding and Siyao Liu and Tong Liu and Dandan Wang and Yanan Liu and Zhaojian Li", "abstract": "  Large Language Models (LLMs) have made significant strides in front-end code\ngeneration. However, existing benchmarks exhibit several critical limitations:\nmany tasks are overly simplistic, test cases often lack rigor, and end-to-end\nvalidation is absent. These issues hinder the accurate assessment of model\nperformance. To address these challenges, we present FrontendBench, a benchmark\nco-developed by humans and LLMs. FrontendBench categorizes tasks based on code\nfunctionality and incorporates interactive test scenarios, enabling a more\ncomprehensive and practical evaluation of front-end code generation\ncapabilities. The benchmark comprises 148 meticulously crafted prompt-test case\npairs spanning five levels of web components, from basic UI elements to complex\ninteractive features. Each task reflects realistic front-end development\nchallenges. Furthermore, we introduce an automatic evaluation framework that\nexecutes generated code within a sandbox environment and assesses outcomes\nusing predefined test scripts. This framework achieves a 90.54% agreement rate\nwith expert human evaluations, demonstrating high reliability. We benchmark\nseveral state-of-the-art LLMs on FrontendBench and observe substantial\nperformance disparities in handling real-world front-end tasks. These results\nhighlight FrontendBench as a reliable and scalable benchmark, supporting\nconsistent multimodal evaluation and providing a robust foundation for future\nresearch in front-end code generation. Our data and code will be released soon.\n", "link": "http://arxiv.org/abs/2506.13832v2", "date": "2025-06-18", "relevancy": 1.871, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FrontendBench%3A%20A%20Benchmark%20for%20Evaluating%20LLMs%20on%20Front-End%20Development%0A%20%20via%20Automatic%20Evaluation&body=Title%3A%20FrontendBench%3A%20A%20Benchmark%20for%20Evaluating%20LLMs%20on%20Front-End%20Development%0A%20%20via%20Automatic%20Evaluation%0AAuthor%3A%20Hongda%20Zhu%20and%20Yiwen%20Zhang%20and%20Bing%20Zhao%20and%20Jingzhe%20Ding%20and%20Siyao%20Liu%20and%20Tong%20Liu%20and%20Dandan%20Wang%20and%20Yanan%20Liu%20and%20Zhaojian%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20front-end%20code%0Ageneration.%20However%2C%20existing%20benchmarks%20exhibit%20several%20critical%20limitations%3A%0Amany%20tasks%20are%20overly%20simplistic%2C%20test%20cases%20often%20lack%20rigor%2C%20and%20end-to-end%0Avalidation%20is%20absent.%20These%20issues%20hinder%20the%20accurate%20assessment%20of%20model%0Aperformance.%20To%20address%20these%20challenges%2C%20we%20present%20FrontendBench%2C%20a%20benchmark%0Aco-developed%20by%20humans%20and%20LLMs.%20FrontendBench%20categorizes%20tasks%20based%20on%20code%0Afunctionality%20and%20incorporates%20interactive%20test%20scenarios%2C%20enabling%20a%20more%0Acomprehensive%20and%20practical%20evaluation%20of%20front-end%20code%20generation%0Acapabilities.%20The%20benchmark%20comprises%20148%20meticulously%20crafted%20prompt-test%20case%0Apairs%20spanning%20five%20levels%20of%20web%20components%2C%20from%20basic%20UI%20elements%20to%20complex%0Ainteractive%20features.%20Each%20task%20reflects%20realistic%20front-end%20development%0Achallenges.%20Furthermore%2C%20we%20introduce%20an%20automatic%20evaluation%20framework%20that%0Aexecutes%20generated%20code%20within%20a%20sandbox%20environment%20and%20assesses%20outcomes%0Ausing%20predefined%20test%20scripts.%20This%20framework%20achieves%20a%2090.54%25%20agreement%20rate%0Awith%20expert%20human%20evaluations%2C%20demonstrating%20high%20reliability.%20We%20benchmark%0Aseveral%20state-of-the-art%20LLMs%20on%20FrontendBench%20and%20observe%20substantial%0Aperformance%20disparities%20in%20handling%20real-world%20front-end%20tasks.%20These%20results%0Ahighlight%20FrontendBench%20as%20a%20reliable%20and%20scalable%20benchmark%2C%20supporting%0Aconsistent%20multimodal%20evaluation%20and%20providing%20a%20robust%20foundation%20for%20future%0Aresearch%20in%20front-end%20code%20generation.%20Our%20data%20and%20code%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrontendBench%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520LLMs%2520on%2520Front-End%2520Development%250A%2520%2520via%2520Automatic%2520Evaluation%26entry.906535625%3DHongda%2520Zhu%2520and%2520Yiwen%2520Zhang%2520and%2520Bing%2520Zhao%2520and%2520Jingzhe%2520Ding%2520and%2520Siyao%2520Liu%2520and%2520Tong%2520Liu%2520and%2520Dandan%2520Wang%2520and%2520Yanan%2520Liu%2520and%2520Zhaojian%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520strides%2520in%2520front-end%2520code%250Ageneration.%2520However%252C%2520existing%2520benchmarks%2520exhibit%2520several%2520critical%2520limitations%253A%250Amany%2520tasks%2520are%2520overly%2520simplistic%252C%2520test%2520cases%2520often%2520lack%2520rigor%252C%2520and%2520end-to-end%250Avalidation%2520is%2520absent.%2520These%2520issues%2520hinder%2520the%2520accurate%2520assessment%2520of%2520model%250Aperformance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520FrontendBench%252C%2520a%2520benchmark%250Aco-developed%2520by%2520humans%2520and%2520LLMs.%2520FrontendBench%2520categorizes%2520tasks%2520based%2520on%2520code%250Afunctionality%2520and%2520incorporates%2520interactive%2520test%2520scenarios%252C%2520enabling%2520a%2520more%250Acomprehensive%2520and%2520practical%2520evaluation%2520of%2520front-end%2520code%2520generation%250Acapabilities.%2520The%2520benchmark%2520comprises%2520148%2520meticulously%2520crafted%2520prompt-test%2520case%250Apairs%2520spanning%2520five%2520levels%2520of%2520web%2520components%252C%2520from%2520basic%2520UI%2520elements%2520to%2520complex%250Ainteractive%2520features.%2520Each%2520task%2520reflects%2520realistic%2520front-end%2520development%250Achallenges.%2520Furthermore%252C%2520we%2520introduce%2520an%2520automatic%2520evaluation%2520framework%2520that%250Aexecutes%2520generated%2520code%2520within%2520a%2520sandbox%2520environment%2520and%2520assesses%2520outcomes%250Ausing%2520predefined%2520test%2520scripts.%2520This%2520framework%2520achieves%2520a%252090.54%2525%2520agreement%2520rate%250Awith%2520expert%2520human%2520evaluations%252C%2520demonstrating%2520high%2520reliability.%2520We%2520benchmark%250Aseveral%2520state-of-the-art%2520LLMs%2520on%2520FrontendBench%2520and%2520observe%2520substantial%250Aperformance%2520disparities%2520in%2520handling%2520real-world%2520front-end%2520tasks.%2520These%2520results%250Ahighlight%2520FrontendBench%2520as%2520a%2520reliable%2520and%2520scalable%2520benchmark%252C%2520supporting%250Aconsistent%2520multimodal%2520evaluation%2520and%2520providing%2520a%2520robust%2520foundation%2520for%2520future%250Aresearch%2520in%2520front-end%2520code%2520generation.%2520Our%2520data%2520and%2520code%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FrontendBench%3A%20A%20Benchmark%20for%20Evaluating%20LLMs%20on%20Front-End%20Development%0A%20%20via%20Automatic%20Evaluation&entry.906535625=Hongda%20Zhu%20and%20Yiwen%20Zhang%20and%20Bing%20Zhao%20and%20Jingzhe%20Ding%20and%20Siyao%20Liu%20and%20Tong%20Liu%20and%20Dandan%20Wang%20and%20Yanan%20Liu%20and%20Zhaojian%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20made%20significant%20strides%20in%20front-end%20code%0Ageneration.%20However%2C%20existing%20benchmarks%20exhibit%20several%20critical%20limitations%3A%0Amany%20tasks%20are%20overly%20simplistic%2C%20test%20cases%20often%20lack%20rigor%2C%20and%20end-to-end%0Avalidation%20is%20absent.%20These%20issues%20hinder%20the%20accurate%20assessment%20of%20model%0Aperformance.%20To%20address%20these%20challenges%2C%20we%20present%20FrontendBench%2C%20a%20benchmark%0Aco-developed%20by%20humans%20and%20LLMs.%20FrontendBench%20categorizes%20tasks%20based%20on%20code%0Afunctionality%20and%20incorporates%20interactive%20test%20scenarios%2C%20enabling%20a%20more%0Acomprehensive%20and%20practical%20evaluation%20of%20front-end%20code%20generation%0Acapabilities.%20The%20benchmark%20comprises%20148%20meticulously%20crafted%20prompt-test%20case%0Apairs%20spanning%20five%20levels%20of%20web%20components%2C%20from%20basic%20UI%20elements%20to%20complex%0Ainteractive%20features.%20Each%20task%20reflects%20realistic%20front-end%20development%0Achallenges.%20Furthermore%2C%20we%20introduce%20an%20automatic%20evaluation%20framework%20that%0Aexecutes%20generated%20code%20within%20a%20sandbox%20environment%20and%20assesses%20outcomes%0Ausing%20predefined%20test%20scripts.%20This%20framework%20achieves%20a%2090.54%25%20agreement%20rate%0Awith%20expert%20human%20evaluations%2C%20demonstrating%20high%20reliability.%20We%20benchmark%0Aseveral%20state-of-the-art%20LLMs%20on%20FrontendBench%20and%20observe%20substantial%0Aperformance%20disparities%20in%20handling%20real-world%20front-end%20tasks.%20These%20results%0Ahighlight%20FrontendBench%20as%20a%20reliable%20and%20scalable%20benchmark%2C%20supporting%0Aconsistent%20multimodal%20evaluation%20and%20providing%20a%20robust%20foundation%20for%20future%0Aresearch%20in%20front-end%20code%20generation.%20Our%20data%20and%20code%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13832v2&entry.124074799=Read"},
{"title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video\n  Super-Resolution", "author": "Yujing Sun and Lingchen Sun and Shuaizheng Liu and Rongyuan Wu and Zhengqiang Zhang and Lei Zhang", "abstract": "  It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.\n", "link": "http://arxiv.org/abs/2506.15591v1", "date": "2025-06-18", "relevancy": 1.8466, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.669}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6109}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Step%20Diffusion%20for%20Detail-Rich%20and%20Temporally%20Consistent%20Video%0A%20%20Super-Resolution&body=Title%3A%20One-Step%20Diffusion%20for%20Detail-Rich%20and%20Temporally%20Consistent%20Video%0A%20%20Super-Resolution%0AAuthor%3A%20Yujing%20Sun%20and%20Lingchen%20Sun%20and%20Shuaizheng%20Liu%20and%20Rongyuan%20Wu%20and%20Zhengqiang%20Zhang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20It%20is%20a%20challenging%20problem%20to%20reproduce%20rich%20spatial%20details%20while%0Amaintaining%20temporal%20consistency%20in%20real-world%20video%20super-resolution%0A%28Real-VSR%29%2C%20especially%20when%20we%20leverage%20pre-trained%20generative%20models%20such%20as%0Astable%20diffusion%20%28SD%29%20for%20realistic%20details%20synthesis.%20Existing%20SD-based%0AReal-VSR%20methods%20often%20compromise%20spatial%20details%20for%20temporal%20coherence%2C%0Aresulting%20in%20suboptimal%20visual%20quality.%20We%20argue%20that%20the%20key%20lies%20in%20how%20to%0Aeffectively%20extract%20the%20degradation-robust%20temporal%20consistency%20priors%20from%20the%0Alow-quality%20%28LQ%29%20input%20video%20and%20enhance%20the%20video%20details%20while%20maintaining%0Athe%20extracted%20consistency%20priors.%20To%20achieve%20this%2C%20we%20propose%20a%20Dual%20LoRA%0ALearning%20%28DLoRAL%29%20paradigm%20to%20train%20an%20effective%20SD-based%20one-step%20diffusion%0Amodel%2C%20achieving%20realistic%20frame%20details%20and%20temporal%20consistency%0Asimultaneously.%20Specifically%2C%20we%20introduce%20a%20Cross-Frame%20Retrieval%20%28CFR%29%20module%0Ato%20aggregate%20complementary%20information%20across%20frames%2C%20and%20train%20a%0AConsistency-LoRA%20%28C-LoRA%29%20to%20learn%20robust%20temporal%20representations%20from%0Adegraded%20inputs.%20After%20consistency%20learning%2C%20we%20fix%20the%20CFR%20and%20C-LoRA%20modules%0Aand%20train%20a%20Detail-LoRA%20%28D-LoRA%29%20to%20enhance%20spatial%20details%20while%20aligning%20with%0Athe%20temporal%20space%20defined%20by%20C-LoRA%20to%20keep%20temporal%20coherence.%20The%20two%20phases%0Aalternate%20iteratively%20for%20optimization%2C%20collaboratively%20delivering%20consistent%0Aand%20detail-rich%20outputs.%20During%20inference%2C%20the%20two%20LoRA%20branches%20are%20merged%0Ainto%20the%20SD%20model%2C%20allowing%20efficient%20and%20high-quality%20video%20restoration%20in%20a%0Asingle%20diffusion%20step.%20Experiments%20show%20that%20DLoRAL%20achieves%20strong%20performance%0Ain%20both%20accuracy%20and%20speed.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/yjsunnn/DLoRAL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Step%2520Diffusion%2520for%2520Detail-Rich%2520and%2520Temporally%2520Consistent%2520Video%250A%2520%2520Super-Resolution%26entry.906535625%3DYujing%2520Sun%2520and%2520Lingchen%2520Sun%2520and%2520Shuaizheng%2520Liu%2520and%2520Rongyuan%2520Wu%2520and%2520Zhengqiang%2520Zhang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520It%2520is%2520a%2520challenging%2520problem%2520to%2520reproduce%2520rich%2520spatial%2520details%2520while%250Amaintaining%2520temporal%2520consistency%2520in%2520real-world%2520video%2520super-resolution%250A%2528Real-VSR%2529%252C%2520especially%2520when%2520we%2520leverage%2520pre-trained%2520generative%2520models%2520such%2520as%250Astable%2520diffusion%2520%2528SD%2529%2520for%2520realistic%2520details%2520synthesis.%2520Existing%2520SD-based%250AReal-VSR%2520methods%2520often%2520compromise%2520spatial%2520details%2520for%2520temporal%2520coherence%252C%250Aresulting%2520in%2520suboptimal%2520visual%2520quality.%2520We%2520argue%2520that%2520the%2520key%2520lies%2520in%2520how%2520to%250Aeffectively%2520extract%2520the%2520degradation-robust%2520temporal%2520consistency%2520priors%2520from%2520the%250Alow-quality%2520%2528LQ%2529%2520input%2520video%2520and%2520enhance%2520the%2520video%2520details%2520while%2520maintaining%250Athe%2520extracted%2520consistency%2520priors.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%2520Dual%2520LoRA%250ALearning%2520%2528DLoRAL%2529%2520paradigm%2520to%2520train%2520an%2520effective%2520SD-based%2520one-step%2520diffusion%250Amodel%252C%2520achieving%2520realistic%2520frame%2520details%2520and%2520temporal%2520consistency%250Asimultaneously.%2520Specifically%252C%2520we%2520introduce%2520a%2520Cross-Frame%2520Retrieval%2520%2528CFR%2529%2520module%250Ato%2520aggregate%2520complementary%2520information%2520across%2520frames%252C%2520and%2520train%2520a%250AConsistency-LoRA%2520%2528C-LoRA%2529%2520to%2520learn%2520robust%2520temporal%2520representations%2520from%250Adegraded%2520inputs.%2520After%2520consistency%2520learning%252C%2520we%2520fix%2520the%2520CFR%2520and%2520C-LoRA%2520modules%250Aand%2520train%2520a%2520Detail-LoRA%2520%2528D-LoRA%2529%2520to%2520enhance%2520spatial%2520details%2520while%2520aligning%2520with%250Athe%2520temporal%2520space%2520defined%2520by%2520C-LoRA%2520to%2520keep%2520temporal%2520coherence.%2520The%2520two%2520phases%250Aalternate%2520iteratively%2520for%2520optimization%252C%2520collaboratively%2520delivering%2520consistent%250Aand%2520detail-rich%2520outputs.%2520During%2520inference%252C%2520the%2520two%2520LoRA%2520branches%2520are%2520merged%250Ainto%2520the%2520SD%2520model%252C%2520allowing%2520efficient%2520and%2520high-quality%2520video%2520restoration%2520in%2520a%250Asingle%2520diffusion%2520step.%2520Experiments%2520show%2520that%2520DLoRAL%2520achieves%2520strong%2520performance%250Ain%2520both%2520accuracy%2520and%2520speed.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/yjsunnn/DLoRAL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Step%20Diffusion%20for%20Detail-Rich%20and%20Temporally%20Consistent%20Video%0A%20%20Super-Resolution&entry.906535625=Yujing%20Sun%20and%20Lingchen%20Sun%20and%20Shuaizheng%20Liu%20and%20Rongyuan%20Wu%20and%20Zhengqiang%20Zhang%20and%20Lei%20Zhang&entry.1292438233=%20%20It%20is%20a%20challenging%20problem%20to%20reproduce%20rich%20spatial%20details%20while%0Amaintaining%20temporal%20consistency%20in%20real-world%20video%20super-resolution%0A%28Real-VSR%29%2C%20especially%20when%20we%20leverage%20pre-trained%20generative%20models%20such%20as%0Astable%20diffusion%20%28SD%29%20for%20realistic%20details%20synthesis.%20Existing%20SD-based%0AReal-VSR%20methods%20often%20compromise%20spatial%20details%20for%20temporal%20coherence%2C%0Aresulting%20in%20suboptimal%20visual%20quality.%20We%20argue%20that%20the%20key%20lies%20in%20how%20to%0Aeffectively%20extract%20the%20degradation-robust%20temporal%20consistency%20priors%20from%20the%0Alow-quality%20%28LQ%29%20input%20video%20and%20enhance%20the%20video%20details%20while%20maintaining%0Athe%20extracted%20consistency%20priors.%20To%20achieve%20this%2C%20we%20propose%20a%20Dual%20LoRA%0ALearning%20%28DLoRAL%29%20paradigm%20to%20train%20an%20effective%20SD-based%20one-step%20diffusion%0Amodel%2C%20achieving%20realistic%20frame%20details%20and%20temporal%20consistency%0Asimultaneously.%20Specifically%2C%20we%20introduce%20a%20Cross-Frame%20Retrieval%20%28CFR%29%20module%0Ato%20aggregate%20complementary%20information%20across%20frames%2C%20and%20train%20a%0AConsistency-LoRA%20%28C-LoRA%29%20to%20learn%20robust%20temporal%20representations%20from%0Adegraded%20inputs.%20After%20consistency%20learning%2C%20we%20fix%20the%20CFR%20and%20C-LoRA%20modules%0Aand%20train%20a%20Detail-LoRA%20%28D-LoRA%29%20to%20enhance%20spatial%20details%20while%20aligning%20with%0Athe%20temporal%20space%20defined%20by%20C-LoRA%20to%20keep%20temporal%20coherence.%20The%20two%20phases%0Aalternate%20iteratively%20for%20optimization%2C%20collaboratively%20delivering%20consistent%0Aand%20detail-rich%20outputs.%20During%20inference%2C%20the%20two%20LoRA%20branches%20are%20merged%0Ainto%20the%20SD%20model%2C%20allowing%20efficient%20and%20high-quality%20video%20restoration%20in%20a%0Asingle%20diffusion%20step.%20Experiments%20show%20that%20DLoRAL%20achieves%20strong%20performance%0Ain%20both%20accuracy%20and%20speed.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/yjsunnn/DLoRAL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15591v1&entry.124074799=Read"},
{"title": "UniRelight: Learning Joint Decomposition and Synthesis for Video\n  Relighting", "author": "Kai He and Ruofan Liang and Jacob Munkberg and Jon Hasselgren and Nandita Vijaykumar and Alexander Keller and Sanja Fidler and Igor Gilitschenski and Zan Gojcic and Zian Wang", "abstract": "  We address the challenge of relighting a single image or video, a task that\ndemands precise scene intrinsic understanding and high-quality light transport\nsynthesis. Existing end-to-end relighting models are often limited by the\nscarcity of paired multi-illumination data, restricting their ability to\ngeneralize across diverse scenes. Conversely, two-stage pipelines that combine\ninverse and forward rendering can mitigate data requirements but are\nsusceptible to error accumulation and often fail to produce realistic outputs\nunder complex lighting conditions or with sophisticated materials. In this\nwork, we introduce a general-purpose approach that jointly estimates albedo and\nsynthesizes relit outputs in a single pass, harnessing the generative\ncapabilities of video diffusion models. This joint formulation enhances\nimplicit scene comprehension and facilitates the creation of realistic lighting\neffects and intricate material interactions, such as shadows, reflections, and\ntransparency. Trained on synthetic multi-illumination data and extensive\nautomatically labeled real-world videos, our model demonstrates strong\ngeneralization across diverse domains and surpasses previous methods in both\nvisual fidelity and temporal consistency.\n", "link": "http://arxiv.org/abs/2506.15673v1", "date": "2025-06-18", "relevancy": 1.76, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5948}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5919}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniRelight%3A%20Learning%20Joint%20Decomposition%20and%20Synthesis%20for%20Video%0A%20%20Relighting&body=Title%3A%20UniRelight%3A%20Learning%20Joint%20Decomposition%20and%20Synthesis%20for%20Video%0A%20%20Relighting%0AAuthor%3A%20Kai%20He%20and%20Ruofan%20Liang%20and%20Jacob%20Munkberg%20and%20Jon%20Hasselgren%20and%20Nandita%20Vijaykumar%20and%20Alexander%20Keller%20and%20Sanja%20Fidler%20and%20Igor%20Gilitschenski%20and%20Zan%20Gojcic%20and%20Zian%20Wang%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20relighting%20a%20single%20image%20or%20video%2C%20a%20task%20that%0Ademands%20precise%20scene%20intrinsic%20understanding%20and%20high-quality%20light%20transport%0Asynthesis.%20Existing%20end-to-end%20relighting%20models%20are%20often%20limited%20by%20the%0Ascarcity%20of%20paired%20multi-illumination%20data%2C%20restricting%20their%20ability%20to%0Ageneralize%20across%20diverse%20scenes.%20Conversely%2C%20two-stage%20pipelines%20that%20combine%0Ainverse%20and%20forward%20rendering%20can%20mitigate%20data%20requirements%20but%20are%0Asusceptible%20to%20error%20accumulation%20and%20often%20fail%20to%20produce%20realistic%20outputs%0Aunder%20complex%20lighting%20conditions%20or%20with%20sophisticated%20materials.%20In%20this%0Awork%2C%20we%20introduce%20a%20general-purpose%20approach%20that%20jointly%20estimates%20albedo%20and%0Asynthesizes%20relit%20outputs%20in%20a%20single%20pass%2C%20harnessing%20the%20generative%0Acapabilities%20of%20video%20diffusion%20models.%20This%20joint%20formulation%20enhances%0Aimplicit%20scene%20comprehension%20and%20facilitates%20the%20creation%20of%20realistic%20lighting%0Aeffects%20and%20intricate%20material%20interactions%2C%20such%20as%20shadows%2C%20reflections%2C%20and%0Atransparency.%20Trained%20on%20synthetic%20multi-illumination%20data%20and%20extensive%0Aautomatically%20labeled%20real-world%20videos%2C%20our%20model%20demonstrates%20strong%0Ageneralization%20across%20diverse%20domains%20and%20surpasses%20previous%20methods%20in%20both%0Avisual%20fidelity%20and%20temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniRelight%253A%2520Learning%2520Joint%2520Decomposition%2520and%2520Synthesis%2520for%2520Video%250A%2520%2520Relighting%26entry.906535625%3DKai%2520He%2520and%2520Ruofan%2520Liang%2520and%2520Jacob%2520Munkberg%2520and%2520Jon%2520Hasselgren%2520and%2520Nandita%2520Vijaykumar%2520and%2520Alexander%2520Keller%2520and%2520Sanja%2520Fidler%2520and%2520Igor%2520Gilitschenski%2520and%2520Zan%2520Gojcic%2520and%2520Zian%2520Wang%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520relighting%2520a%2520single%2520image%2520or%2520video%252C%2520a%2520task%2520that%250Ademands%2520precise%2520scene%2520intrinsic%2520understanding%2520and%2520high-quality%2520light%2520transport%250Asynthesis.%2520Existing%2520end-to-end%2520relighting%2520models%2520are%2520often%2520limited%2520by%2520the%250Ascarcity%2520of%2520paired%2520multi-illumination%2520data%252C%2520restricting%2520their%2520ability%2520to%250Ageneralize%2520across%2520diverse%2520scenes.%2520Conversely%252C%2520two-stage%2520pipelines%2520that%2520combine%250Ainverse%2520and%2520forward%2520rendering%2520can%2520mitigate%2520data%2520requirements%2520but%2520are%250Asusceptible%2520to%2520error%2520accumulation%2520and%2520often%2520fail%2520to%2520produce%2520realistic%2520outputs%250Aunder%2520complex%2520lighting%2520conditions%2520or%2520with%2520sophisticated%2520materials.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520general-purpose%2520approach%2520that%2520jointly%2520estimates%2520albedo%2520and%250Asynthesizes%2520relit%2520outputs%2520in%2520a%2520single%2520pass%252C%2520harnessing%2520the%2520generative%250Acapabilities%2520of%2520video%2520diffusion%2520models.%2520This%2520joint%2520formulation%2520enhances%250Aimplicit%2520scene%2520comprehension%2520and%2520facilitates%2520the%2520creation%2520of%2520realistic%2520lighting%250Aeffects%2520and%2520intricate%2520material%2520interactions%252C%2520such%2520as%2520shadows%252C%2520reflections%252C%2520and%250Atransparency.%2520Trained%2520on%2520synthetic%2520multi-illumination%2520data%2520and%2520extensive%250Aautomatically%2520labeled%2520real-world%2520videos%252C%2520our%2520model%2520demonstrates%2520strong%250Ageneralization%2520across%2520diverse%2520domains%2520and%2520surpasses%2520previous%2520methods%2520in%2520both%250Avisual%2520fidelity%2520and%2520temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniRelight%3A%20Learning%20Joint%20Decomposition%20and%20Synthesis%20for%20Video%0A%20%20Relighting&entry.906535625=Kai%20He%20and%20Ruofan%20Liang%20and%20Jacob%20Munkberg%20and%20Jon%20Hasselgren%20and%20Nandita%20Vijaykumar%20and%20Alexander%20Keller%20and%20Sanja%20Fidler%20and%20Igor%20Gilitschenski%20and%20Zan%20Gojcic%20and%20Zian%20Wang&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20relighting%20a%20single%20image%20or%20video%2C%20a%20task%20that%0Ademands%20precise%20scene%20intrinsic%20understanding%20and%20high-quality%20light%20transport%0Asynthesis.%20Existing%20end-to-end%20relighting%20models%20are%20often%20limited%20by%20the%0Ascarcity%20of%20paired%20multi-illumination%20data%2C%20restricting%20their%20ability%20to%0Ageneralize%20across%20diverse%20scenes.%20Conversely%2C%20two-stage%20pipelines%20that%20combine%0Ainverse%20and%20forward%20rendering%20can%20mitigate%20data%20requirements%20but%20are%0Asusceptible%20to%20error%20accumulation%20and%20often%20fail%20to%20produce%20realistic%20outputs%0Aunder%20complex%20lighting%20conditions%20or%20with%20sophisticated%20materials.%20In%20this%0Awork%2C%20we%20introduce%20a%20general-purpose%20approach%20that%20jointly%20estimates%20albedo%20and%0Asynthesizes%20relit%20outputs%20in%20a%20single%20pass%2C%20harnessing%20the%20generative%0Acapabilities%20of%20video%20diffusion%20models.%20This%20joint%20formulation%20enhances%0Aimplicit%20scene%20comprehension%20and%20facilitates%20the%20creation%20of%20realistic%20lighting%0Aeffects%20and%20intricate%20material%20interactions%2C%20such%20as%20shadows%2C%20reflections%2C%20and%0Atransparency.%20Trained%20on%20synthetic%20multi-illumination%20data%20and%20extensive%0Aautomatically%20labeled%20real-world%20videos%2C%20our%20model%20demonstrates%20strong%0Ageneralization%20across%20diverse%20domains%20and%20surpasses%20previous%20methods%20in%20both%0Avisual%20fidelity%20and%20temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15673v1&entry.124074799=Read"},
{"title": "$k$-Submodular Interdiction Problems under Distributional\n  Risk-Receptiveness and Robustness: Application to Machine Learning", "author": "Seonghun Park and Manish Bansal", "abstract": "  We study submodular optimization in adversarial context, applicable to\nmachine learning problems such as feature selection using data susceptible to\nuncertainties and attacks. We focus on Stackelberg games between an attacker\n(or interdictor) and a defender where the attacker aims to minimize the\ndefender's objective of maximizing a $k$-submodular function. We allow\nuncertainties arising from the success of attacks and inherent data noise, and\naddress challenges due to incomplete knowledge of the probability distribution\nof random parameters. Specifically, we introduce Distributionally Robust\n$k$-Submodular Interdiction Problem (DRO $k$-SIP) and Distributionally\nRisk-Receptive $k$-Submodular Interdiction Problem (DRR $k$-SIP) along with\nfinitely convergent exact algorithms for solving them. When solving the DRO\n$k$-SIP, the attacker optimizes their expected payoff with respect to the\nworst-case probability distribution within the ambiguity set, and thereby have\nrobust attack strategies despite distributional ambiguity. In contrast, the DRR\n$k$-SIP identifies attacker strategies with the best-case probability\ndistribution, and identifies critical vulnerabilities for the defender. The\noptimal values derived from both DRO $k$-SIP and DRR $k$-SIP offer a confidence\ninterval-like range for the expected value of the defender's objective\nfunction, capturing distributional ambiguity. We conduct computational\nexperiments on instances of feature selection and sensor placement problems,\nusing Wisconsin breast cancer data and synthetic data, respectively.\n", "link": "http://arxiv.org/abs/2406.13023v4", "date": "2025-06-18", "relevancy": 1.3575, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4619}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4575}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24k%24-Submodular%20Interdiction%20Problems%20under%20Distributional%0A%20%20Risk-Receptiveness%20and%20Robustness%3A%20Application%20to%20Machine%20Learning&body=Title%3A%20%24k%24-Submodular%20Interdiction%20Problems%20under%20Distributional%0A%20%20Risk-Receptiveness%20and%20Robustness%3A%20Application%20to%20Machine%20Learning%0AAuthor%3A%20Seonghun%20Park%20and%20Manish%20Bansal%0AAbstract%3A%20%20%20We%20study%20submodular%20optimization%20in%20adversarial%20context%2C%20applicable%20to%0Amachine%20learning%20problems%20such%20as%20feature%20selection%20using%20data%20susceptible%20to%0Auncertainties%20and%20attacks.%20We%20focus%20on%20Stackelberg%20games%20between%20an%20attacker%0A%28or%20interdictor%29%20and%20a%20defender%20where%20the%20attacker%20aims%20to%20minimize%20the%0Adefender%27s%20objective%20of%20maximizing%20a%20%24k%24-submodular%20function.%20We%20allow%0Auncertainties%20arising%20from%20the%20success%20of%20attacks%20and%20inherent%20data%20noise%2C%20and%0Aaddress%20challenges%20due%20to%20incomplete%20knowledge%20of%20the%20probability%20distribution%0Aof%20random%20parameters.%20Specifically%2C%20we%20introduce%20Distributionally%20Robust%0A%24k%24-Submodular%20Interdiction%20Problem%20%28DRO%20%24k%24-SIP%29%20and%20Distributionally%0ARisk-Receptive%20%24k%24-Submodular%20Interdiction%20Problem%20%28DRR%20%24k%24-SIP%29%20along%20with%0Afinitely%20convergent%20exact%20algorithms%20for%20solving%20them.%20When%20solving%20the%20DRO%0A%24k%24-SIP%2C%20the%20attacker%20optimizes%20their%20expected%20payoff%20with%20respect%20to%20the%0Aworst-case%20probability%20distribution%20within%20the%20ambiguity%20set%2C%20and%20thereby%20have%0Arobust%20attack%20strategies%20despite%20distributional%20ambiguity.%20In%20contrast%2C%20the%20DRR%0A%24k%24-SIP%20identifies%20attacker%20strategies%20with%20the%20best-case%20probability%0Adistribution%2C%20and%20identifies%20critical%20vulnerabilities%20for%20the%20defender.%20The%0Aoptimal%20values%20derived%20from%20both%20DRO%20%24k%24-SIP%20and%20DRR%20%24k%24-SIP%20offer%20a%20confidence%0Ainterval-like%20range%20for%20the%20expected%20value%20of%20the%20defender%27s%20objective%0Afunction%2C%20capturing%20distributional%20ambiguity.%20We%20conduct%20computational%0Aexperiments%20on%20instances%20of%20feature%20selection%20and%20sensor%20placement%20problems%2C%0Ausing%20Wisconsin%20breast%20cancer%20data%20and%20synthetic%20data%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13023v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524k%2524-Submodular%2520Interdiction%2520Problems%2520under%2520Distributional%250A%2520%2520Risk-Receptiveness%2520and%2520Robustness%253A%2520Application%2520to%2520Machine%2520Learning%26entry.906535625%3DSeonghun%2520Park%2520and%2520Manish%2520Bansal%26entry.1292438233%3D%2520%2520We%2520study%2520submodular%2520optimization%2520in%2520adversarial%2520context%252C%2520applicable%2520to%250Amachine%2520learning%2520problems%2520such%2520as%2520feature%2520selection%2520using%2520data%2520susceptible%2520to%250Auncertainties%2520and%2520attacks.%2520We%2520focus%2520on%2520Stackelberg%2520games%2520between%2520an%2520attacker%250A%2528or%2520interdictor%2529%2520and%2520a%2520defender%2520where%2520the%2520attacker%2520aims%2520to%2520minimize%2520the%250Adefender%2527s%2520objective%2520of%2520maximizing%2520a%2520%2524k%2524-submodular%2520function.%2520We%2520allow%250Auncertainties%2520arising%2520from%2520the%2520success%2520of%2520attacks%2520and%2520inherent%2520data%2520noise%252C%2520and%250Aaddress%2520challenges%2520due%2520to%2520incomplete%2520knowledge%2520of%2520the%2520probability%2520distribution%250Aof%2520random%2520parameters.%2520Specifically%252C%2520we%2520introduce%2520Distributionally%2520Robust%250A%2524k%2524-Submodular%2520Interdiction%2520Problem%2520%2528DRO%2520%2524k%2524-SIP%2529%2520and%2520Distributionally%250ARisk-Receptive%2520%2524k%2524-Submodular%2520Interdiction%2520Problem%2520%2528DRR%2520%2524k%2524-SIP%2529%2520along%2520with%250Afinitely%2520convergent%2520exact%2520algorithms%2520for%2520solving%2520them.%2520When%2520solving%2520the%2520DRO%250A%2524k%2524-SIP%252C%2520the%2520attacker%2520optimizes%2520their%2520expected%2520payoff%2520with%2520respect%2520to%2520the%250Aworst-case%2520probability%2520distribution%2520within%2520the%2520ambiguity%2520set%252C%2520and%2520thereby%2520have%250Arobust%2520attack%2520strategies%2520despite%2520distributional%2520ambiguity.%2520In%2520contrast%252C%2520the%2520DRR%250A%2524k%2524-SIP%2520identifies%2520attacker%2520strategies%2520with%2520the%2520best-case%2520probability%250Adistribution%252C%2520and%2520identifies%2520critical%2520vulnerabilities%2520for%2520the%2520defender.%2520The%250Aoptimal%2520values%2520derived%2520from%2520both%2520DRO%2520%2524k%2524-SIP%2520and%2520DRR%2520%2524k%2524-SIP%2520offer%2520a%2520confidence%250Ainterval-like%2520range%2520for%2520the%2520expected%2520value%2520of%2520the%2520defender%2527s%2520objective%250Afunction%252C%2520capturing%2520distributional%2520ambiguity.%2520We%2520conduct%2520computational%250Aexperiments%2520on%2520instances%2520of%2520feature%2520selection%2520and%2520sensor%2520placement%2520problems%252C%250Ausing%2520Wisconsin%2520breast%2520cancer%2520data%2520and%2520synthetic%2520data%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13023v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24k%24-Submodular%20Interdiction%20Problems%20under%20Distributional%0A%20%20Risk-Receptiveness%20and%20Robustness%3A%20Application%20to%20Machine%20Learning&entry.906535625=Seonghun%20Park%20and%20Manish%20Bansal&entry.1292438233=%20%20We%20study%20submodular%20optimization%20in%20adversarial%20context%2C%20applicable%20to%0Amachine%20learning%20problems%20such%20as%20feature%20selection%20using%20data%20susceptible%20to%0Auncertainties%20and%20attacks.%20We%20focus%20on%20Stackelberg%20games%20between%20an%20attacker%0A%28or%20interdictor%29%20and%20a%20defender%20where%20the%20attacker%20aims%20to%20minimize%20the%0Adefender%27s%20objective%20of%20maximizing%20a%20%24k%24-submodular%20function.%20We%20allow%0Auncertainties%20arising%20from%20the%20success%20of%20attacks%20and%20inherent%20data%20noise%2C%20and%0Aaddress%20challenges%20due%20to%20incomplete%20knowledge%20of%20the%20probability%20distribution%0Aof%20random%20parameters.%20Specifically%2C%20we%20introduce%20Distributionally%20Robust%0A%24k%24-Submodular%20Interdiction%20Problem%20%28DRO%20%24k%24-SIP%29%20and%20Distributionally%0ARisk-Receptive%20%24k%24-Submodular%20Interdiction%20Problem%20%28DRR%20%24k%24-SIP%29%20along%20with%0Afinitely%20convergent%20exact%20algorithms%20for%20solving%20them.%20When%20solving%20the%20DRO%0A%24k%24-SIP%2C%20the%20attacker%20optimizes%20their%20expected%20payoff%20with%20respect%20to%20the%0Aworst-case%20probability%20distribution%20within%20the%20ambiguity%20set%2C%20and%20thereby%20have%0Arobust%20attack%20strategies%20despite%20distributional%20ambiguity.%20In%20contrast%2C%20the%20DRR%0A%24k%24-SIP%20identifies%20attacker%20strategies%20with%20the%20best-case%20probability%0Adistribution%2C%20and%20identifies%20critical%20vulnerabilities%20for%20the%20defender.%20The%0Aoptimal%20values%20derived%20from%20both%20DRO%20%24k%24-SIP%20and%20DRR%20%24k%24-SIP%20offer%20a%20confidence%0Ainterval-like%20range%20for%20the%20expected%20value%20of%20the%20defender%27s%20objective%0Afunction%2C%20capturing%20distributional%20ambiguity.%20We%20conduct%20computational%0Aexperiments%20on%20instances%20of%20feature%20selection%20and%20sensor%20placement%20problems%2C%0Ausing%20Wisconsin%20breast%20cancer%20data%20and%20synthetic%20data%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13023v4&entry.124074799=Read"},
{"title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D\n  Rewards", "author": "Qingming Liu and Zhen Liu and Dinghuai Zhang and Kui Jia", "abstract": "  Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.\n", "link": "http://arxiv.org/abs/2506.15684v1", "date": "2025-06-18", "relevancy": 1.7651, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6004}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nabla-R2D3%3A%20Effective%20and%20Efficient%203D%20Diffusion%20Alignment%20with%202D%0A%20%20Rewards&body=Title%3A%20Nabla-R2D3%3A%20Effective%20and%20Efficient%203D%20Diffusion%20Alignment%20with%202D%0A%20%20Rewards%0AAuthor%3A%20Qingming%20Liu%20and%20Zhen%20Liu%20and%20Dinghuai%20Zhang%20and%20Kui%20Jia%0AAbstract%3A%20%20%20Generating%20high-quality%20and%20photorealistic%203D%20assets%20remains%20a%20longstanding%0Achallenge%20in%203D%20vision%20and%20computer%20graphics.%20Although%20state-of-the-art%0Agenerative%20models%2C%20such%20as%20diffusion%20models%2C%20have%20made%20significant%20progress%20in%0A3D%20generation%2C%20they%20often%20fall%20short%20of%20human-designed%20content%20due%20to%20limited%0Aability%20to%20follow%20instructions%2C%20align%20with%20human%20preferences%2C%20or%20produce%0Arealistic%20textures%2C%20geometries%2C%20and%20physical%20attributes.%20In%20this%20paper%2C%20we%0Aintroduce%20Nabla-R2D3%2C%20a%20highly%20effective%20and%20sample-efficient%20reinforcement%0Alearning%20alignment%20framework%20for%203D-native%20diffusion%20models%20using%202D%20rewards.%0ABuilt%20upon%20the%20recently%20proposed%20Nabla-GFlowNet%20method%2C%20which%20matches%20the%20score%0Afunction%20to%20reward%20gradients%20in%20a%20principled%20manner%20for%20reward%20finetuning%2C%20our%0ANabla-R2D3%20enables%20effective%20adaptation%20of%203D%20diffusion%20models%20using%20only%202D%0Areward%20signals.%20Extensive%20experiments%20show%20that%2C%20unlike%20vanilla%20finetuning%0Abaselines%20which%20either%20struggle%20to%20converge%20or%20suffer%20from%20reward%20hacking%2C%0ANabla-R2D3%20consistently%20achieves%20higher%20rewards%20and%20reduced%20prior%20forgetting%0Awithin%20a%20few%20finetuning%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.15684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNabla-R2D3%253A%2520Effective%2520and%2520Efficient%25203D%2520Diffusion%2520Alignment%2520with%25202D%250A%2520%2520Rewards%26entry.906535625%3DQingming%2520Liu%2520and%2520Zhen%2520Liu%2520and%2520Dinghuai%2520Zhang%2520and%2520Kui%2520Jia%26entry.1292438233%3D%2520%2520Generating%2520high-quality%2520and%2520photorealistic%25203D%2520assets%2520remains%2520a%2520longstanding%250Achallenge%2520in%25203D%2520vision%2520and%2520computer%2520graphics.%2520Although%2520state-of-the-art%250Agenerative%2520models%252C%2520such%2520as%2520diffusion%2520models%252C%2520have%2520made%2520significant%2520progress%2520in%250A3D%2520generation%252C%2520they%2520often%2520fall%2520short%2520of%2520human-designed%2520content%2520due%2520to%2520limited%250Aability%2520to%2520follow%2520instructions%252C%2520align%2520with%2520human%2520preferences%252C%2520or%2520produce%250Arealistic%2520textures%252C%2520geometries%252C%2520and%2520physical%2520attributes.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Nabla-R2D3%252C%2520a%2520highly%2520effective%2520and%2520sample-efficient%2520reinforcement%250Alearning%2520alignment%2520framework%2520for%25203D-native%2520diffusion%2520models%2520using%25202D%2520rewards.%250ABuilt%2520upon%2520the%2520recently%2520proposed%2520Nabla-GFlowNet%2520method%252C%2520which%2520matches%2520the%2520score%250Afunction%2520to%2520reward%2520gradients%2520in%2520a%2520principled%2520manner%2520for%2520reward%2520finetuning%252C%2520our%250ANabla-R2D3%2520enables%2520effective%2520adaptation%2520of%25203D%2520diffusion%2520models%2520using%2520only%25202D%250Areward%2520signals.%2520Extensive%2520experiments%2520show%2520that%252C%2520unlike%2520vanilla%2520finetuning%250Abaselines%2520which%2520either%2520struggle%2520to%2520converge%2520or%2520suffer%2520from%2520reward%2520hacking%252C%250ANabla-R2D3%2520consistently%2520achieves%2520higher%2520rewards%2520and%2520reduced%2520prior%2520forgetting%250Awithin%2520a%2520few%2520finetuning%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nabla-R2D3%3A%20Effective%20and%20Efficient%203D%20Diffusion%20Alignment%20with%202D%0A%20%20Rewards&entry.906535625=Qingming%20Liu%20and%20Zhen%20Liu%20and%20Dinghuai%20Zhang%20and%20Kui%20Jia&entry.1292438233=%20%20Generating%20high-quality%20and%20photorealistic%203D%20assets%20remains%20a%20longstanding%0Achallenge%20in%203D%20vision%20and%20computer%20graphics.%20Although%20state-of-the-art%0Agenerative%20models%2C%20such%20as%20diffusion%20models%2C%20have%20made%20significant%20progress%20in%0A3D%20generation%2C%20they%20often%20fall%20short%20of%20human-designed%20content%20due%20to%20limited%0Aability%20to%20follow%20instructions%2C%20align%20with%20human%20preferences%2C%20or%20produce%0Arealistic%20textures%2C%20geometries%2C%20and%20physical%20attributes.%20In%20this%20paper%2C%20we%0Aintroduce%20Nabla-R2D3%2C%20a%20highly%20effective%20and%20sample-efficient%20reinforcement%0Alearning%20alignment%20framework%20for%203D-native%20diffusion%20models%20using%202D%20rewards.%0ABuilt%20upon%20the%20recently%20proposed%20Nabla-GFlowNet%20method%2C%20which%20matches%20the%20score%0Afunction%20to%20reward%20gradients%20in%20a%20principled%20manner%20for%20reward%20finetuning%2C%20our%0ANabla-R2D3%20enables%20effective%20adaptation%20of%203D%20diffusion%20models%20using%20only%202D%0Areward%20signals.%20Extensive%20experiments%20show%20that%2C%20unlike%20vanilla%20finetuning%0Abaselines%20which%20either%20struggle%20to%20converge%20or%20suffer%20from%20reward%20hacking%2C%0ANabla-R2D3%20consistently%20achieves%20higher%20rewards%20and%20reduced%20prior%20forgetting%0Awithin%20a%20few%20finetuning%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.15684v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


