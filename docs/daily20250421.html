<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250420.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from\n  Monocular Videos", "author": "Zetong Zhang and Manuel Kaufmann and Lixin Xue and Jie Song and Martin R. Oswald", "abstract": "  Creating a photorealistic scene and human reconstruction from a single\nmonocular in-the-wild video figures prominently in the perception of a\nhuman-centric 3D world. Recent neural rendering advances have enabled holistic\nhuman-scene reconstruction but require pre-calibrated camera and human poses,\nand days of training time. In this work, we introduce a novel unified framework\nthat simultaneously performs camera tracking, human pose estimation and\nhuman-scene reconstruction in an online fashion. 3D Gaussian Splatting is\nutilized to learn Gaussian primitives for humans and scenes efficiently, and\nreconstruction-based camera tracking and human pose estimation modules are\ndesigned to enable holistic understanding and effective disentanglement of pose\nand appearance. Specifically, we design a human deformation module to\nreconstruct the details and enhance generalizability to out-of-distribution\nposes faithfully. Aiming to learn the spatial correlation between human and\nscene accurately, we introduce occlusion-aware human silhouette rendering and\nmonocular geometric priors, which further improve reconstruction quality.\nExperiments on the EMDB and NeuMan datasets demonstrate superior or on-par\nperformance with existing methods in camera tracking, human pose estimation,\nnovel view synthesis and runtime. Our project page is at\nhttps://eth-ait.github.io/ODHSR.\n", "link": "http://arxiv.org/abs/2504.13167v2", "date": "2025-04-18", "relevancy": 3.3794, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6831}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.675}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos&body=Title%3A%20ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos%0AAuthor%3A%20Zetong%20Zhang%20and%20Manuel%20Kaufmann%20and%20Lixin%20Xue%20and%20Jie%20Song%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Creating%20a%20photorealistic%20scene%20and%20human%20reconstruction%20from%20a%20single%0Amonocular%20in-the-wild%20video%20figures%20prominently%20in%20the%20perception%20of%20a%0Ahuman-centric%203D%20world.%20Recent%20neural%20rendering%20advances%20have%20enabled%20holistic%0Ahuman-scene%20reconstruction%20but%20require%20pre-calibrated%20camera%20and%20human%20poses%2C%0Aand%20days%20of%20training%20time.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20unified%20framework%0Athat%20simultaneously%20performs%20camera%20tracking%2C%20human%20pose%20estimation%20and%0Ahuman-scene%20reconstruction%20in%20an%20online%20fashion.%203D%20Gaussian%20Splatting%20is%0Autilized%20to%20learn%20Gaussian%20primitives%20for%20humans%20and%20scenes%20efficiently%2C%20and%0Areconstruction-based%20camera%20tracking%20and%20human%20pose%20estimation%20modules%20are%0Adesigned%20to%20enable%20holistic%20understanding%20and%20effective%20disentanglement%20of%20pose%0Aand%20appearance.%20Specifically%2C%20we%20design%20a%20human%20deformation%20module%20to%0Areconstruct%20the%20details%20and%20enhance%20generalizability%20to%20out-of-distribution%0Aposes%20faithfully.%20Aiming%20to%20learn%20the%20spatial%20correlation%20between%20human%20and%0Ascene%20accurately%2C%20we%20introduce%20occlusion-aware%20human%20silhouette%20rendering%20and%0Amonocular%20geometric%20priors%2C%20which%20further%20improve%20reconstruction%20quality.%0AExperiments%20on%20the%20EMDB%20and%20NeuMan%20datasets%20demonstrate%20superior%20or%20on-par%0Aperformance%20with%20existing%20methods%20in%20camera%20tracking%2C%20human%20pose%20estimation%2C%0Anovel%20view%20synthesis%20and%20runtime.%20Our%20project%20page%20is%20at%0Ahttps%3A//eth-ait.github.io/ODHSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13167v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODHSR%253A%2520Online%2520Dense%25203D%2520Reconstruction%2520of%2520Humans%2520and%2520Scenes%2520from%250A%2520%2520Monocular%2520Videos%26entry.906535625%3DZetong%2520Zhang%2520and%2520Manuel%2520Kaufmann%2520and%2520Lixin%2520Xue%2520and%2520Jie%2520Song%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Creating%2520a%2520photorealistic%2520scene%2520and%2520human%2520reconstruction%2520from%2520a%2520single%250Amonocular%2520in-the-wild%2520video%2520figures%2520prominently%2520in%2520the%2520perception%2520of%2520a%250Ahuman-centric%25203D%2520world.%2520Recent%2520neural%2520rendering%2520advances%2520have%2520enabled%2520holistic%250Ahuman-scene%2520reconstruction%2520but%2520require%2520pre-calibrated%2520camera%2520and%2520human%2520poses%252C%250Aand%2520days%2520of%2520training%2520time.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520unified%2520framework%250Athat%2520simultaneously%2520performs%2520camera%2520tracking%252C%2520human%2520pose%2520estimation%2520and%250Ahuman-scene%2520reconstruction%2520in%2520an%2520online%2520fashion.%25203D%2520Gaussian%2520Splatting%2520is%250Autilized%2520to%2520learn%2520Gaussian%2520primitives%2520for%2520humans%2520and%2520scenes%2520efficiently%252C%2520and%250Areconstruction-based%2520camera%2520tracking%2520and%2520human%2520pose%2520estimation%2520modules%2520are%250Adesigned%2520to%2520enable%2520holistic%2520understanding%2520and%2520effective%2520disentanglement%2520of%2520pose%250Aand%2520appearance.%2520Specifically%252C%2520we%2520design%2520a%2520human%2520deformation%2520module%2520to%250Areconstruct%2520the%2520details%2520and%2520enhance%2520generalizability%2520to%2520out-of-distribution%250Aposes%2520faithfully.%2520Aiming%2520to%2520learn%2520the%2520spatial%2520correlation%2520between%2520human%2520and%250Ascene%2520accurately%252C%2520we%2520introduce%2520occlusion-aware%2520human%2520silhouette%2520rendering%2520and%250Amonocular%2520geometric%2520priors%252C%2520which%2520further%2520improve%2520reconstruction%2520quality.%250AExperiments%2520on%2520the%2520EMDB%2520and%2520NeuMan%2520datasets%2520demonstrate%2520superior%2520or%2520on-par%250Aperformance%2520with%2520existing%2520methods%2520in%2520camera%2520tracking%252C%2520human%2520pose%2520estimation%252C%250Anovel%2520view%2520synthesis%2520and%2520runtime.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//eth-ait.github.io/ODHSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13167v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODHSR%3A%20Online%20Dense%203D%20Reconstruction%20of%20Humans%20and%20Scenes%20from%0A%20%20Monocular%20Videos&entry.906535625=Zetong%20Zhang%20and%20Manuel%20Kaufmann%20and%20Lixin%20Xue%20and%20Jie%20Song%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Creating%20a%20photorealistic%20scene%20and%20human%20reconstruction%20from%20a%20single%0Amonocular%20in-the-wild%20video%20figures%20prominently%20in%20the%20perception%20of%20a%0Ahuman-centric%203D%20world.%20Recent%20neural%20rendering%20advances%20have%20enabled%20holistic%0Ahuman-scene%20reconstruction%20but%20require%20pre-calibrated%20camera%20and%20human%20poses%2C%0Aand%20days%20of%20training%20time.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20unified%20framework%0Athat%20simultaneously%20performs%20camera%20tracking%2C%20human%20pose%20estimation%20and%0Ahuman-scene%20reconstruction%20in%20an%20online%20fashion.%203D%20Gaussian%20Splatting%20is%0Autilized%20to%20learn%20Gaussian%20primitives%20for%20humans%20and%20scenes%20efficiently%2C%20and%0Areconstruction-based%20camera%20tracking%20and%20human%20pose%20estimation%20modules%20are%0Adesigned%20to%20enable%20holistic%20understanding%20and%20effective%20disentanglement%20of%20pose%0Aand%20appearance.%20Specifically%2C%20we%20design%20a%20human%20deformation%20module%20to%0Areconstruct%20the%20details%20and%20enhance%20generalizability%20to%20out-of-distribution%0Aposes%20faithfully.%20Aiming%20to%20learn%20the%20spatial%20correlation%20between%20human%20and%0Ascene%20accurately%2C%20we%20introduce%20occlusion-aware%20human%20silhouette%20rendering%20and%0Amonocular%20geometric%20priors%2C%20which%20further%20improve%20reconstruction%20quality.%0AExperiments%20on%20the%20EMDB%20and%20NeuMan%20datasets%20demonstrate%20superior%20or%20on-par%0Aperformance%20with%20existing%20methods%20in%20camera%20tracking%2C%20human%20pose%20estimation%2C%0Anovel%20view%20synthesis%20and%20runtime.%20Our%20project%20page%20is%20at%0Ahttps%3A//eth-ait.github.io/ODHSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13167v2&entry.124074799=Read"},
{"title": "SLAM&Render: A Benchmark for the Intersection Between Neural Rendering,\n  Gaussian Splatting and SLAM", "author": "Samuel Cerezo and Gaetano Meli and Tom\u00e1s Berriel Martins and Kirill Safronov and Javier Civera", "abstract": "  Models and methods originally developed for novel view synthesis and scene\nrendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are\nincreasingly being adopted as representations in Simultaneous Localization and\nMapping (SLAM). However, existing datasets fail to include the specific\nchallenges of both fields, such as multimodality and sequentiality in SLAM or\ngeneralization across viewpoints and illumination conditions in neural\nrendering. To bridge this gap, we introduce SLAM&Render, a novel dataset\ndesigned to benchmark methods in the intersection between SLAM and novel view\nrendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot\nkinematic data, and ground-truth pose streams. By releasing robot kinematic\ndata, the dataset also enables the assessment of novel SLAM strategies when\napplied to robot manipulators. The dataset sequences span five different setups\nfeaturing consumer and industrial objects under four different lighting\nconditions, with separate training and test trajectories per scene, as well as\nobject rearrangements. Our experimental results, obtained with several\nbaselines from the literature, validate SLAM&Render as a relevant benchmark for\nthis emerging research area.\n", "link": "http://arxiv.org/abs/2504.13713v1", "date": "2025-04-18", "relevancy": 3.1869, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7117}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6146}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM%26Render%3A%20A%20Benchmark%20for%20the%20Intersection%20Between%20Neural%20Rendering%2C%0A%20%20Gaussian%20Splatting%20and%20SLAM&body=Title%3A%20SLAM%26Render%3A%20A%20Benchmark%20for%20the%20Intersection%20Between%20Neural%20Rendering%2C%0A%20%20Gaussian%20Splatting%20and%20SLAM%0AAuthor%3A%20Samuel%20Cerezo%20and%20Gaetano%20Meli%20and%20Tom%C3%A1s%20Berriel%20Martins%20and%20Kirill%20Safronov%20and%20Javier%20Civera%0AAbstract%3A%20%20%20Models%20and%20methods%20originally%20developed%20for%20novel%20view%20synthesis%20and%20scene%0Arendering%2C%20such%20as%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%20Gaussian%20Splatting%2C%20are%0Aincreasingly%20being%20adopted%20as%20representations%20in%20Simultaneous%20Localization%20and%0AMapping%20%28SLAM%29.%20However%2C%20existing%20datasets%20fail%20to%20include%20the%20specific%0Achallenges%20of%20both%20fields%2C%20such%20as%20multimodality%20and%20sequentiality%20in%20SLAM%20or%0Ageneralization%20across%20viewpoints%20and%20illumination%20conditions%20in%20neural%0Arendering.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SLAM%26Render%2C%20a%20novel%20dataset%0Adesigned%20to%20benchmark%20methods%20in%20the%20intersection%20between%20SLAM%20and%20novel%20view%0Arendering.%20It%20consists%20of%2040%20sequences%20with%20synchronized%20RGB%2C%20depth%2C%20IMU%2C%20robot%0Akinematic%20data%2C%20and%20ground-truth%20pose%20streams.%20By%20releasing%20robot%20kinematic%0Adata%2C%20the%20dataset%20also%20enables%20the%20assessment%20of%20novel%20SLAM%20strategies%20when%0Aapplied%20to%20robot%20manipulators.%20The%20dataset%20sequences%20span%20five%20different%20setups%0Afeaturing%20consumer%20and%20industrial%20objects%20under%20four%20different%20lighting%0Aconditions%2C%20with%20separate%20training%20and%20test%20trajectories%20per%20scene%2C%20as%20well%20as%0Aobject%20rearrangements.%20Our%20experimental%20results%2C%20obtained%20with%20several%0Abaselines%20from%20the%20literature%2C%20validate%20SLAM%26Render%20as%20a%20relevant%20benchmark%20for%0Athis%20emerging%20research%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM%2526Render%253A%2520A%2520Benchmark%2520for%2520the%2520Intersection%2520Between%2520Neural%2520Rendering%252C%250A%2520%2520Gaussian%2520Splatting%2520and%2520SLAM%26entry.906535625%3DSamuel%2520Cerezo%2520and%2520Gaetano%2520Meli%2520and%2520Tom%25C3%25A1s%2520Berriel%2520Martins%2520and%2520Kirill%2520Safronov%2520and%2520Javier%2520Civera%26entry.1292438233%3D%2520%2520Models%2520and%2520methods%2520originally%2520developed%2520for%2520novel%2520view%2520synthesis%2520and%2520scene%250Arendering%252C%2520such%2520as%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520and%2520Gaussian%2520Splatting%252C%2520are%250Aincreasingly%2520being%2520adopted%2520as%2520representations%2520in%2520Simultaneous%2520Localization%2520and%250AMapping%2520%2528SLAM%2529.%2520However%252C%2520existing%2520datasets%2520fail%2520to%2520include%2520the%2520specific%250Achallenges%2520of%2520both%2520fields%252C%2520such%2520as%2520multimodality%2520and%2520sequentiality%2520in%2520SLAM%2520or%250Ageneralization%2520across%2520viewpoints%2520and%2520illumination%2520conditions%2520in%2520neural%250Arendering.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520SLAM%2526Render%252C%2520a%2520novel%2520dataset%250Adesigned%2520to%2520benchmark%2520methods%2520in%2520the%2520intersection%2520between%2520SLAM%2520and%2520novel%2520view%250Arendering.%2520It%2520consists%2520of%252040%2520sequences%2520with%2520synchronized%2520RGB%252C%2520depth%252C%2520IMU%252C%2520robot%250Akinematic%2520data%252C%2520and%2520ground-truth%2520pose%2520streams.%2520By%2520releasing%2520robot%2520kinematic%250Adata%252C%2520the%2520dataset%2520also%2520enables%2520the%2520assessment%2520of%2520novel%2520SLAM%2520strategies%2520when%250Aapplied%2520to%2520robot%2520manipulators.%2520The%2520dataset%2520sequences%2520span%2520five%2520different%2520setups%250Afeaturing%2520consumer%2520and%2520industrial%2520objects%2520under%2520four%2520different%2520lighting%250Aconditions%252C%2520with%2520separate%2520training%2520and%2520test%2520trajectories%2520per%2520scene%252C%2520as%2520well%2520as%250Aobject%2520rearrangements.%2520Our%2520experimental%2520results%252C%2520obtained%2520with%2520several%250Abaselines%2520from%2520the%2520literature%252C%2520validate%2520SLAM%2526Render%2520as%2520a%2520relevant%2520benchmark%2520for%250Athis%2520emerging%2520research%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM%26Render%3A%20A%20Benchmark%20for%20the%20Intersection%20Between%20Neural%20Rendering%2C%0A%20%20Gaussian%20Splatting%20and%20SLAM&entry.906535625=Samuel%20Cerezo%20and%20Gaetano%20Meli%20and%20Tom%C3%A1s%20Berriel%20Martins%20and%20Kirill%20Safronov%20and%20Javier%20Civera&entry.1292438233=%20%20Models%20and%20methods%20originally%20developed%20for%20novel%20view%20synthesis%20and%20scene%0Arendering%2C%20such%20as%20Neural%20Radiance%20Fields%20%28NeRF%29%20and%20Gaussian%20Splatting%2C%20are%0Aincreasingly%20being%20adopted%20as%20representations%20in%20Simultaneous%20Localization%20and%0AMapping%20%28SLAM%29.%20However%2C%20existing%20datasets%20fail%20to%20include%20the%20specific%0Achallenges%20of%20both%20fields%2C%20such%20as%20multimodality%20and%20sequentiality%20in%20SLAM%20or%0Ageneralization%20across%20viewpoints%20and%20illumination%20conditions%20in%20neural%0Arendering.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SLAM%26Render%2C%20a%20novel%20dataset%0Adesigned%20to%20benchmark%20methods%20in%20the%20intersection%20between%20SLAM%20and%20novel%20view%0Arendering.%20It%20consists%20of%2040%20sequences%20with%20synchronized%20RGB%2C%20depth%2C%20IMU%2C%20robot%0Akinematic%20data%2C%20and%20ground-truth%20pose%20streams.%20By%20releasing%20robot%20kinematic%0Adata%2C%20the%20dataset%20also%20enables%20the%20assessment%20of%20novel%20SLAM%20strategies%20when%0Aapplied%20to%20robot%20manipulators.%20The%20dataset%20sequences%20span%20five%20different%20setups%0Afeaturing%20consumer%20and%20industrial%20objects%20under%20four%20different%20lighting%0Aconditions%2C%20with%20separate%20training%20and%20test%20trajectories%20per%20scene%2C%20as%20well%20as%0Aobject%20rearrangements.%20Our%20experimental%20results%2C%20obtained%20with%20several%0Abaselines%20from%20the%20literature%2C%20validate%20SLAM%26Render%20as%20a%20relevant%20benchmark%20for%0Athis%20emerging%20research%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13713v1&entry.124074799=Read"},
{"title": "Green Robotic Mixed Reality with Gaussian Splatting", "author": "Chenxuan Liu and He Li and Zongze Li and Shuai Wang and Wei Xu and Kejiang Ye and Derrick Wing Kwan Ng and Chengzhong Xu", "abstract": "  Realizing green communication in robotic mixed reality (RoboMR) systems\npresents a challenge, due to the necessity of uploading high-resolution images\nat high frequencies through wireless channels. This paper proposes Gaussian\nsplatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and\nmakes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS\nmodel which enables the simulator to opportunistically render a photo-realistic\nview from the robot's pose, thereby reducing the need for excessive image\nuploads. Since the GS model may involve discrepancies compared to the actual\nenvironments, a GS cross-layer optimization (GSCLO) framework is further\nproposed, which jointly optimizes content switching (i.e., deciding whether to\nupload image or not) and power allocation across different frames. The GSCLO\nproblem is solved by an accelerated penalty optimization (APO) algorithm.\nExperiments demonstrate that the proposed GSRMR reduces the communication\nenergy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with\nAPO outperforms extensive baseline schemes, in terms of peak signal-to-noise\nratio (PSNR) and structural similarity index measure (SSIM).\n", "link": "http://arxiv.org/abs/2504.13697v1", "date": "2025-04-18", "relevancy": 3.1065, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6755}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5956}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Green%20Robotic%20Mixed%20Reality%20with%20Gaussian%20Splatting&body=Title%3A%20Green%20Robotic%20Mixed%20Reality%20with%20Gaussian%20Splatting%0AAuthor%3A%20Chenxuan%20Liu%20and%20He%20Li%20and%20Zongze%20Li%20and%20Shuai%20Wang%20and%20Wei%20Xu%20and%20Kejiang%20Ye%20and%20Derrick%20Wing%20Kwan%20Ng%20and%20Chengzhong%20Xu%0AAbstract%3A%20%20%20Realizing%20green%20communication%20in%20robotic%20mixed%20reality%20%28RoboMR%29%20systems%0Apresents%20a%20challenge%2C%20due%20to%20the%20necessity%20of%20uploading%20high-resolution%20images%0Aat%20high%20frequencies%20through%20wireless%20channels.%20This%20paper%20proposes%20Gaussian%0Asplatting%20%28GS%29%20RoboMR%20%28GSRMR%29%2C%20which%20achieves%20a%20lower%20energy%20consumption%20and%0Amakes%20a%20concrete%20step%20towards%20green%20RoboMR.%20The%20crux%20to%20GSRMR%20is%20to%20build%20a%20GS%0Amodel%20which%20enables%20the%20simulator%20to%20opportunistically%20render%20a%20photo-realistic%0Aview%20from%20the%20robot%27s%20pose%2C%20thereby%20reducing%20the%20need%20for%20excessive%20image%0Auploads.%20Since%20the%20GS%20model%20may%20involve%20discrepancies%20compared%20to%20the%20actual%0Aenvironments%2C%20a%20GS%20cross-layer%20optimization%20%28GSCLO%29%20framework%20is%20further%0Aproposed%2C%20which%20jointly%20optimizes%20content%20switching%20%28i.e.%2C%20deciding%20whether%20to%0Aupload%20image%20or%20not%29%20and%20power%20allocation%20across%20different%20frames.%20The%20GSCLO%0Aproblem%20is%20solved%20by%20an%20accelerated%20penalty%20optimization%20%28APO%29%20algorithm.%0AExperiments%20demonstrate%20that%20the%20proposed%20GSRMR%20reduces%20the%20communication%0Aenergy%20by%20over%2010x%20compared%20with%20RoboMR.%20Furthermore%2C%20the%20proposed%20GSRMR%20with%0AAPO%20outperforms%20extensive%20baseline%20schemes%2C%20in%20terms%20of%20peak%20signal-to-noise%0Aratio%20%28PSNR%29%20and%20structural%20similarity%20index%20measure%20%28SSIM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreen%2520Robotic%2520Mixed%2520Reality%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DChenxuan%2520Liu%2520and%2520He%2520Li%2520and%2520Zongze%2520Li%2520and%2520Shuai%2520Wang%2520and%2520Wei%2520Xu%2520and%2520Kejiang%2520Ye%2520and%2520Derrick%2520Wing%2520Kwan%2520Ng%2520and%2520Chengzhong%2520Xu%26entry.1292438233%3D%2520%2520Realizing%2520green%2520communication%2520in%2520robotic%2520mixed%2520reality%2520%2528RoboMR%2529%2520systems%250Apresents%2520a%2520challenge%252C%2520due%2520to%2520the%2520necessity%2520of%2520uploading%2520high-resolution%2520images%250Aat%2520high%2520frequencies%2520through%2520wireless%2520channels.%2520This%2520paper%2520proposes%2520Gaussian%250Asplatting%2520%2528GS%2529%2520RoboMR%2520%2528GSRMR%2529%252C%2520which%2520achieves%2520a%2520lower%2520energy%2520consumption%2520and%250Amakes%2520a%2520concrete%2520step%2520towards%2520green%2520RoboMR.%2520The%2520crux%2520to%2520GSRMR%2520is%2520to%2520build%2520a%2520GS%250Amodel%2520which%2520enables%2520the%2520simulator%2520to%2520opportunistically%2520render%2520a%2520photo-realistic%250Aview%2520from%2520the%2520robot%2527s%2520pose%252C%2520thereby%2520reducing%2520the%2520need%2520for%2520excessive%2520image%250Auploads.%2520Since%2520the%2520GS%2520model%2520may%2520involve%2520discrepancies%2520compared%2520to%2520the%2520actual%250Aenvironments%252C%2520a%2520GS%2520cross-layer%2520optimization%2520%2528GSCLO%2529%2520framework%2520is%2520further%250Aproposed%252C%2520which%2520jointly%2520optimizes%2520content%2520switching%2520%2528i.e.%252C%2520deciding%2520whether%2520to%250Aupload%2520image%2520or%2520not%2529%2520and%2520power%2520allocation%2520across%2520different%2520frames.%2520The%2520GSCLO%250Aproblem%2520is%2520solved%2520by%2520an%2520accelerated%2520penalty%2520optimization%2520%2528APO%2529%2520algorithm.%250AExperiments%2520demonstrate%2520that%2520the%2520proposed%2520GSRMR%2520reduces%2520the%2520communication%250Aenergy%2520by%2520over%252010x%2520compared%2520with%2520RoboMR.%2520Furthermore%252C%2520the%2520proposed%2520GSRMR%2520with%250AAPO%2520outperforms%2520extensive%2520baseline%2520schemes%252C%2520in%2520terms%2520of%2520peak%2520signal-to-noise%250Aratio%2520%2528PSNR%2529%2520and%2520structural%2520similarity%2520index%2520measure%2520%2528SSIM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Green%20Robotic%20Mixed%20Reality%20with%20Gaussian%20Splatting&entry.906535625=Chenxuan%20Liu%20and%20He%20Li%20and%20Zongze%20Li%20and%20Shuai%20Wang%20and%20Wei%20Xu%20and%20Kejiang%20Ye%20and%20Derrick%20Wing%20Kwan%20Ng%20and%20Chengzhong%20Xu&entry.1292438233=%20%20Realizing%20green%20communication%20in%20robotic%20mixed%20reality%20%28RoboMR%29%20systems%0Apresents%20a%20challenge%2C%20due%20to%20the%20necessity%20of%20uploading%20high-resolution%20images%0Aat%20high%20frequencies%20through%20wireless%20channels.%20This%20paper%20proposes%20Gaussian%0Asplatting%20%28GS%29%20RoboMR%20%28GSRMR%29%2C%20which%20achieves%20a%20lower%20energy%20consumption%20and%0Amakes%20a%20concrete%20step%20towards%20green%20RoboMR.%20The%20crux%20to%20GSRMR%20is%20to%20build%20a%20GS%0Amodel%20which%20enables%20the%20simulator%20to%20opportunistically%20render%20a%20photo-realistic%0Aview%20from%20the%20robot%27s%20pose%2C%20thereby%20reducing%20the%20need%20for%20excessive%20image%0Auploads.%20Since%20the%20GS%20model%20may%20involve%20discrepancies%20compared%20to%20the%20actual%0Aenvironments%2C%20a%20GS%20cross-layer%20optimization%20%28GSCLO%29%20framework%20is%20further%0Aproposed%2C%20which%20jointly%20optimizes%20content%20switching%20%28i.e.%2C%20deciding%20whether%20to%0Aupload%20image%20or%20not%29%20and%20power%20allocation%20across%20different%20frames.%20The%20GSCLO%0Aproblem%20is%20solved%20by%20an%20accelerated%20penalty%20optimization%20%28APO%29%20algorithm.%0AExperiments%20demonstrate%20that%20the%20proposed%20GSRMR%20reduces%20the%20communication%0Aenergy%20by%20over%2010x%20compared%20with%20RoboMR.%20Furthermore%2C%20the%20proposed%20GSRMR%20with%0AAPO%20outperforms%20extensive%20baseline%20schemes%2C%20in%20terms%20of%20peak%20signal-to-noise%0Aratio%20%28PSNR%29%20and%20structural%20similarity%20index%20measure%20%28SSIM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13697v1&entry.124074799=Read"},
{"title": "VCR: A Task for Pixel-Level Complex Reasoning in Vision Language Models\n  via Restoring Occluded Text", "author": "Tianyu Zhang and Suyuchen Wang and Lu Li and Ge Zhang and Perouz Taslakian and Sai Rajeswar and Jie Fu and Bang Liu and Yoshua Bengio", "abstract": "  We introduce Visual Caption Restoration (VCR), a novel vision-language task\nthat challenges models to accurately restore partially obscured texts using\npixel-level hints within images. This task stems from the observation that text\nembedded in images is intrinsically different from common visual elements and\nnatural language due to the need to align the modalities of vision, text, and\ntext embedded in images. While numerous works have integrated text embedded in\nimages into visual question-answering tasks, approaches to these tasks\ngenerally rely on optical character recognition or masked language modeling,\nthus reducing the task to mainly text-based processing. However, text-based\nprocessing becomes ineffective in VCR as accurate text restoration depends on\nthe combined information from provided images, context, and subtle cues from\nthe tiny exposed areas of masked texts. We develop a pipeline to generate\nsynthetic images for the VCR task using image-caption pairs, with adjustable\ncaption visibility to control the task difficulty. With this pipeline, we\nconstruct a dataset for VCR called VCR-Wiki using images with captions from\nWikipedia, comprising 2.11M English and 346K Chinese entities in both easy and\nhard split variants. Our results reveal that current vision language models\nsignificantly lag behind human performance in the VCR task, and merely\nfine-tuning the models on our dataset does not lead to notable improvements. We\nrelease VCR-Wiki and the data construction code to facilitate future research.\n", "link": "http://arxiv.org/abs/2406.06462v4", "date": "2025-04-18", "relevancy": 2.8522, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5805}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCR%3A%20A%20Task%20for%20Pixel-Level%20Complex%20Reasoning%20in%20Vision%20Language%20Models%0A%20%20via%20Restoring%20Occluded%20Text&body=Title%3A%20VCR%3A%20A%20Task%20for%20Pixel-Level%20Complex%20Reasoning%20in%20Vision%20Language%20Models%0A%20%20via%20Restoring%20Occluded%20Text%0AAuthor%3A%20Tianyu%20Zhang%20and%20Suyuchen%20Wang%20and%20Lu%20Li%20and%20Ge%20Zhang%20and%20Perouz%20Taslakian%20and%20Sai%20Rajeswar%20and%20Jie%20Fu%20and%20Bang%20Liu%20and%20Yoshua%20Bengio%0AAbstract%3A%20%20%20We%20introduce%20Visual%20Caption%20Restoration%20%28VCR%29%2C%20a%20novel%20vision-language%20task%0Athat%20challenges%20models%20to%20accurately%20restore%20partially%20obscured%20texts%20using%0Apixel-level%20hints%20within%20images.%20This%20task%20stems%20from%20the%20observation%20that%20text%0Aembedded%20in%20images%20is%20intrinsically%20different%20from%20common%20visual%20elements%20and%0Anatural%20language%20due%20to%20the%20need%20to%20align%20the%20modalities%20of%20vision%2C%20text%2C%20and%0Atext%20embedded%20in%20images.%20While%20numerous%20works%20have%20integrated%20text%20embedded%20in%0Aimages%20into%20visual%20question-answering%20tasks%2C%20approaches%20to%20these%20tasks%0Agenerally%20rely%20on%20optical%20character%20recognition%20or%20masked%20language%20modeling%2C%0Athus%20reducing%20the%20task%20to%20mainly%20text-based%20processing.%20However%2C%20text-based%0Aprocessing%20becomes%20ineffective%20in%20VCR%20as%20accurate%20text%20restoration%20depends%20on%0Athe%20combined%20information%20from%20provided%20images%2C%20context%2C%20and%20subtle%20cues%20from%0Athe%20tiny%20exposed%20areas%20of%20masked%20texts.%20We%20develop%20a%20pipeline%20to%20generate%0Asynthetic%20images%20for%20the%20VCR%20task%20using%20image-caption%20pairs%2C%20with%20adjustable%0Acaption%20visibility%20to%20control%20the%20task%20difficulty.%20With%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20for%20VCR%20called%20VCR-Wiki%20using%20images%20with%20captions%20from%0AWikipedia%2C%20comprising%202.11M%20English%20and%20346K%20Chinese%20entities%20in%20both%20easy%20and%0Ahard%20split%20variants.%20Our%20results%20reveal%20that%20current%20vision%20language%20models%0Asignificantly%20lag%20behind%20human%20performance%20in%20the%20VCR%20task%2C%20and%20merely%0Afine-tuning%20the%20models%20on%20our%20dataset%20does%20not%20lead%20to%20notable%20improvements.%20We%0Arelease%20VCR-Wiki%20and%20the%20data%20construction%20code%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06462v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCR%253A%2520A%2520Task%2520for%2520Pixel-Level%2520Complex%2520Reasoning%2520in%2520Vision%2520Language%2520Models%250A%2520%2520via%2520Restoring%2520Occluded%2520Text%26entry.906535625%3DTianyu%2520Zhang%2520and%2520Suyuchen%2520Wang%2520and%2520Lu%2520Li%2520and%2520Ge%2520Zhang%2520and%2520Perouz%2520Taslakian%2520and%2520Sai%2520Rajeswar%2520and%2520Jie%2520Fu%2520and%2520Bang%2520Liu%2520and%2520Yoshua%2520Bengio%26entry.1292438233%3D%2520%2520We%2520introduce%2520Visual%2520Caption%2520Restoration%2520%2528VCR%2529%252C%2520a%2520novel%2520vision-language%2520task%250Athat%2520challenges%2520models%2520to%2520accurately%2520restore%2520partially%2520obscured%2520texts%2520using%250Apixel-level%2520hints%2520within%2520images.%2520This%2520task%2520stems%2520from%2520the%2520observation%2520that%2520text%250Aembedded%2520in%2520images%2520is%2520intrinsically%2520different%2520from%2520common%2520visual%2520elements%2520and%250Anatural%2520language%2520due%2520to%2520the%2520need%2520to%2520align%2520the%2520modalities%2520of%2520vision%252C%2520text%252C%2520and%250Atext%2520embedded%2520in%2520images.%2520While%2520numerous%2520works%2520have%2520integrated%2520text%2520embedded%2520in%250Aimages%2520into%2520visual%2520question-answering%2520tasks%252C%2520approaches%2520to%2520these%2520tasks%250Agenerally%2520rely%2520on%2520optical%2520character%2520recognition%2520or%2520masked%2520language%2520modeling%252C%250Athus%2520reducing%2520the%2520task%2520to%2520mainly%2520text-based%2520processing.%2520However%252C%2520text-based%250Aprocessing%2520becomes%2520ineffective%2520in%2520VCR%2520as%2520accurate%2520text%2520restoration%2520depends%2520on%250Athe%2520combined%2520information%2520from%2520provided%2520images%252C%2520context%252C%2520and%2520subtle%2520cues%2520from%250Athe%2520tiny%2520exposed%2520areas%2520of%2520masked%2520texts.%2520We%2520develop%2520a%2520pipeline%2520to%2520generate%250Asynthetic%2520images%2520for%2520the%2520VCR%2520task%2520using%2520image-caption%2520pairs%252C%2520with%2520adjustable%250Acaption%2520visibility%2520to%2520control%2520the%2520task%2520difficulty.%2520With%2520this%2520pipeline%252C%2520we%250Aconstruct%2520a%2520dataset%2520for%2520VCR%2520called%2520VCR-Wiki%2520using%2520images%2520with%2520captions%2520from%250AWikipedia%252C%2520comprising%25202.11M%2520English%2520and%2520346K%2520Chinese%2520entities%2520in%2520both%2520easy%2520and%250Ahard%2520split%2520variants.%2520Our%2520results%2520reveal%2520that%2520current%2520vision%2520language%2520models%250Asignificantly%2520lag%2520behind%2520human%2520performance%2520in%2520the%2520VCR%2520task%252C%2520and%2520merely%250Afine-tuning%2520the%2520models%2520on%2520our%2520dataset%2520does%2520not%2520lead%2520to%2520notable%2520improvements.%2520We%250Arelease%2520VCR-Wiki%2520and%2520the%2520data%2520construction%2520code%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06462v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCR%3A%20A%20Task%20for%20Pixel-Level%20Complex%20Reasoning%20in%20Vision%20Language%20Models%0A%20%20via%20Restoring%20Occluded%20Text&entry.906535625=Tianyu%20Zhang%20and%20Suyuchen%20Wang%20and%20Lu%20Li%20and%20Ge%20Zhang%20and%20Perouz%20Taslakian%20and%20Sai%20Rajeswar%20and%20Jie%20Fu%20and%20Bang%20Liu%20and%20Yoshua%20Bengio&entry.1292438233=%20%20We%20introduce%20Visual%20Caption%20Restoration%20%28VCR%29%2C%20a%20novel%20vision-language%20task%0Athat%20challenges%20models%20to%20accurately%20restore%20partially%20obscured%20texts%20using%0Apixel-level%20hints%20within%20images.%20This%20task%20stems%20from%20the%20observation%20that%20text%0Aembedded%20in%20images%20is%20intrinsically%20different%20from%20common%20visual%20elements%20and%0Anatural%20language%20due%20to%20the%20need%20to%20align%20the%20modalities%20of%20vision%2C%20text%2C%20and%0Atext%20embedded%20in%20images.%20While%20numerous%20works%20have%20integrated%20text%20embedded%20in%0Aimages%20into%20visual%20question-answering%20tasks%2C%20approaches%20to%20these%20tasks%0Agenerally%20rely%20on%20optical%20character%20recognition%20or%20masked%20language%20modeling%2C%0Athus%20reducing%20the%20task%20to%20mainly%20text-based%20processing.%20However%2C%20text-based%0Aprocessing%20becomes%20ineffective%20in%20VCR%20as%20accurate%20text%20restoration%20depends%20on%0Athe%20combined%20information%20from%20provided%20images%2C%20context%2C%20and%20subtle%20cues%20from%0Athe%20tiny%20exposed%20areas%20of%20masked%20texts.%20We%20develop%20a%20pipeline%20to%20generate%0Asynthetic%20images%20for%20the%20VCR%20task%20using%20image-caption%20pairs%2C%20with%20adjustable%0Acaption%20visibility%20to%20control%20the%20task%20difficulty.%20With%20this%20pipeline%2C%20we%0Aconstruct%20a%20dataset%20for%20VCR%20called%20VCR-Wiki%20using%20images%20with%20captions%20from%0AWikipedia%2C%20comprising%202.11M%20English%20and%20346K%20Chinese%20entities%20in%20both%20easy%20and%0Ahard%20split%20variants.%20Our%20results%20reveal%20that%20current%20vision%20language%20models%0Asignificantly%20lag%20behind%20human%20performance%20in%20the%20VCR%20task%2C%20and%20merely%0Afine-tuning%20the%20models%20on%20our%20dataset%20does%20not%20lead%20to%20notable%20improvements.%20We%0Arelease%20VCR-Wiki%20and%20the%20data%20construction%20code%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06462v4&entry.124074799=Read"},
{"title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn\n  Text-to-Image Generation", "author": "Minbin Huang and Yanxin Long and Xinchi Deng and Ruihang Chu and Jiangfeng Xiong and Xiaodan Liang and Hong Cheng and Qinglin Lu and Wei Liu", "abstract": "  Text-to-image (T2I) generation models have significantly advanced in recent\nyears. However, effective interaction with these models is challenging for\naverage users due to the need for specialized prompt engineering knowledge and\nthe inability to perform multi-turn image generation, hindering a dynamic and\niterative creation process. Recent attempts have tried to equip Multi-modal\nLarge Language Models (MLLMs) with T2I models to bring the user's natural\nlanguage instructions into reality. Hence, the output modality of MLLMs is\nextended, and the multi-turn generation quality of T2I models is enhanced\nthanks to the strong multi-modal comprehension ability of MLLMs. However, many\nof these works face challenges in identifying correct output modalities and\ngenerating coherent images accordingly as the number of output modalities\nincreases and the conversations go deeper. Therefore, we propose DialogGen, an\neffective pipeline to align off-the-shelf MLLMs and T2I models to build a\nMulti-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image\ngeneration. It is composed of drawing prompt alignment, careful training data\ncuration, and error correction. Moreover, as the field of MIDS flourishes,\ncomprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms\nof output modality correctness and multi-modal output coherence. To address\nthis issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a\ncomprehensive bilingual benchmark designed to assess the ability of MLLMs to\ngenerate accurate and coherent multi-modal content that supports image editing.\nIt contains two evaluation metrics to measure the model's ability to switch\nmodalities and the coherence of the output images. Our extensive experiments on\nDialogBen and user study demonstrate the effectiveness of DialogGen compared\nwith other State-of-the-Art models.\n", "link": "http://arxiv.org/abs/2403.08857v3", "date": "2025-04-18", "relevancy": 2.8332, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation&body=Title%3A%20DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Minbin%20Huang%20and%20Yanxin%20Long%20and%20Xinchi%20Deng%20and%20Ruihang%20Chu%20and%20Jiangfeng%20Xiong%20and%20Xiaodan%20Liang%20and%20Hong%20Cheng%20and%20Qinglin%20Lu%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20models%20have%20significantly%20advanced%20in%20recent%0Ayears.%20However%2C%20effective%20interaction%20with%20these%20models%20is%20challenging%20for%0Aaverage%20users%20due%20to%20the%20need%20for%20specialized%20prompt%20engineering%20knowledge%20and%0Athe%20inability%20to%20perform%20multi-turn%20image%20generation%2C%20hindering%20a%20dynamic%20and%0Aiterative%20creation%20process.%20Recent%20attempts%20have%20tried%20to%20equip%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%20with%20T2I%20models%20to%20bring%20the%20user%27s%20natural%0Alanguage%20instructions%20into%20reality.%20Hence%2C%20the%20output%20modality%20of%20MLLMs%20is%0Aextended%2C%20and%20the%20multi-turn%20generation%20quality%20of%20T2I%20models%20is%20enhanced%0Athanks%20to%20the%20strong%20multi-modal%20comprehension%20ability%20of%20MLLMs.%20However%2C%20many%0Aof%20these%20works%20face%20challenges%20in%20identifying%20correct%20output%20modalities%20and%0Agenerating%20coherent%20images%20accordingly%20as%20the%20number%20of%20output%20modalities%0Aincreases%20and%20the%20conversations%20go%20deeper.%20Therefore%2C%20we%20propose%20DialogGen%2C%20an%0Aeffective%20pipeline%20to%20align%20off-the-shelf%20MLLMs%20and%20T2I%20models%20to%20build%20a%0AMulti-modal%20Interactive%20Dialogue%20System%20%28MIDS%29%20for%20multi-turn%20Text-to-Image%0Ageneration.%20It%20is%20composed%20of%20drawing%20prompt%20alignment%2C%20careful%20training%20data%0Acuration%2C%20and%20error%20correction.%20Moreover%2C%20as%20the%20field%20of%20MIDS%20flourishes%2C%0Acomprehensive%20benchmarks%20are%20urgently%20needed%20to%20evaluate%20MIDS%20fairly%20in%20terms%0Aof%20output%20modality%20correctness%20and%20multi-modal%20output%20coherence.%20To%20address%0Athis%20issue%2C%20we%20introduce%20the%20Multi-modal%20Dialogue%20Benchmark%20%28DialogBen%29%2C%20a%0Acomprehensive%20bilingual%20benchmark%20designed%20to%20assess%20the%20ability%20of%20MLLMs%20to%0Agenerate%20accurate%20and%20coherent%20multi-modal%20content%20that%20supports%20image%20editing.%0AIt%20contains%20two%20evaluation%20metrics%20to%20measure%20the%20model%27s%20ability%20to%20switch%0Amodalities%20and%20the%20coherence%20of%20the%20output%20images.%20Our%20extensive%20experiments%20on%0ADialogBen%20and%20user%20study%20demonstrate%20the%20effectiveness%20of%20DialogGen%20compared%0Awith%20other%20State-of-the-Art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08857v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDialogGen%253A%2520Multi-modal%2520Interactive%2520Dialogue%2520System%2520for%2520Multi-turn%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DMinbin%2520Huang%2520and%2520Yanxin%2520Long%2520and%2520Xinchi%2520Deng%2520and%2520Ruihang%2520Chu%2520and%2520Jiangfeng%2520Xiong%2520and%2520Xiaodan%2520Liang%2520and%2520Hong%2520Cheng%2520and%2520Qinglin%2520Lu%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520models%2520have%2520significantly%2520advanced%2520in%2520recent%250Ayears.%2520However%252C%2520effective%2520interaction%2520with%2520these%2520models%2520is%2520challenging%2520for%250Aaverage%2520users%2520due%2520to%2520the%2520need%2520for%2520specialized%2520prompt%2520engineering%2520knowledge%2520and%250Athe%2520inability%2520to%2520perform%2520multi-turn%2520image%2520generation%252C%2520hindering%2520a%2520dynamic%2520and%250Aiterative%2520creation%2520process.%2520Recent%2520attempts%2520have%2520tried%2520to%2520equip%2520Multi-modal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%2520with%2520T2I%2520models%2520to%2520bring%2520the%2520user%2527s%2520natural%250Alanguage%2520instructions%2520into%2520reality.%2520Hence%252C%2520the%2520output%2520modality%2520of%2520MLLMs%2520is%250Aextended%252C%2520and%2520the%2520multi-turn%2520generation%2520quality%2520of%2520T2I%2520models%2520is%2520enhanced%250Athanks%2520to%2520the%2520strong%2520multi-modal%2520comprehension%2520ability%2520of%2520MLLMs.%2520However%252C%2520many%250Aof%2520these%2520works%2520face%2520challenges%2520in%2520identifying%2520correct%2520output%2520modalities%2520and%250Agenerating%2520coherent%2520images%2520accordingly%2520as%2520the%2520number%2520of%2520output%2520modalities%250Aincreases%2520and%2520the%2520conversations%2520go%2520deeper.%2520Therefore%252C%2520we%2520propose%2520DialogGen%252C%2520an%250Aeffective%2520pipeline%2520to%2520align%2520off-the-shelf%2520MLLMs%2520and%2520T2I%2520models%2520to%2520build%2520a%250AMulti-modal%2520Interactive%2520Dialogue%2520System%2520%2528MIDS%2529%2520for%2520multi-turn%2520Text-to-Image%250Ageneration.%2520It%2520is%2520composed%2520of%2520drawing%2520prompt%2520alignment%252C%2520careful%2520training%2520data%250Acuration%252C%2520and%2520error%2520correction.%2520Moreover%252C%2520as%2520the%2520field%2520of%2520MIDS%2520flourishes%252C%250Acomprehensive%2520benchmarks%2520are%2520urgently%2520needed%2520to%2520evaluate%2520MIDS%2520fairly%2520in%2520terms%250Aof%2520output%2520modality%2520correctness%2520and%2520multi-modal%2520output%2520coherence.%2520To%2520address%250Athis%2520issue%252C%2520we%2520introduce%2520the%2520Multi-modal%2520Dialogue%2520Benchmark%2520%2528DialogBen%2529%252C%2520a%250Acomprehensive%2520bilingual%2520benchmark%2520designed%2520to%2520assess%2520the%2520ability%2520of%2520MLLMs%2520to%250Agenerate%2520accurate%2520and%2520coherent%2520multi-modal%2520content%2520that%2520supports%2520image%2520editing.%250AIt%2520contains%2520two%2520evaluation%2520metrics%2520to%2520measure%2520the%2520model%2527s%2520ability%2520to%2520switch%250Amodalities%2520and%2520the%2520coherence%2520of%2520the%2520output%2520images.%2520Our%2520extensive%2520experiments%2520on%250ADialogBen%2520and%2520user%2520study%2520demonstrate%2520the%2520effectiveness%2520of%2520DialogGen%2520compared%250Awith%2520other%2520State-of-the-Art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08857v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation&entry.906535625=Minbin%20Huang%20and%20Yanxin%20Long%20and%20Xinchi%20Deng%20and%20Ruihang%20Chu%20and%20Jiangfeng%20Xiong%20and%20Xiaodan%20Liang%20and%20Hong%20Cheng%20and%20Qinglin%20Lu%20and%20Wei%20Liu&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20models%20have%20significantly%20advanced%20in%20recent%0Ayears.%20However%2C%20effective%20interaction%20with%20these%20models%20is%20challenging%20for%0Aaverage%20users%20due%20to%20the%20need%20for%20specialized%20prompt%20engineering%20knowledge%20and%0Athe%20inability%20to%20perform%20multi-turn%20image%20generation%2C%20hindering%20a%20dynamic%20and%0Aiterative%20creation%20process.%20Recent%20attempts%20have%20tried%20to%20equip%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%20with%20T2I%20models%20to%20bring%20the%20user%27s%20natural%0Alanguage%20instructions%20into%20reality.%20Hence%2C%20the%20output%20modality%20of%20MLLMs%20is%0Aextended%2C%20and%20the%20multi-turn%20generation%20quality%20of%20T2I%20models%20is%20enhanced%0Athanks%20to%20the%20strong%20multi-modal%20comprehension%20ability%20of%20MLLMs.%20However%2C%20many%0Aof%20these%20works%20face%20challenges%20in%20identifying%20correct%20output%20modalities%20and%0Agenerating%20coherent%20images%20accordingly%20as%20the%20number%20of%20output%20modalities%0Aincreases%20and%20the%20conversations%20go%20deeper.%20Therefore%2C%20we%20propose%20DialogGen%2C%20an%0Aeffective%20pipeline%20to%20align%20off-the-shelf%20MLLMs%20and%20T2I%20models%20to%20build%20a%0AMulti-modal%20Interactive%20Dialogue%20System%20%28MIDS%29%20for%20multi-turn%20Text-to-Image%0Ageneration.%20It%20is%20composed%20of%20drawing%20prompt%20alignment%2C%20careful%20training%20data%0Acuration%2C%20and%20error%20correction.%20Moreover%2C%20as%20the%20field%20of%20MIDS%20flourishes%2C%0Acomprehensive%20benchmarks%20are%20urgently%20needed%20to%20evaluate%20MIDS%20fairly%20in%20terms%0Aof%20output%20modality%20correctness%20and%20multi-modal%20output%20coherence.%20To%20address%0Athis%20issue%2C%20we%20introduce%20the%20Multi-modal%20Dialogue%20Benchmark%20%28DialogBen%29%2C%20a%0Acomprehensive%20bilingual%20benchmark%20designed%20to%20assess%20the%20ability%20of%20MLLMs%20to%0Agenerate%20accurate%20and%20coherent%20multi-modal%20content%20that%20supports%20image%20editing.%0AIt%20contains%20two%20evaluation%20metrics%20to%20measure%20the%20model%27s%20ability%20to%20switch%0Amodalities%20and%20the%20coherence%20of%20the%20output%20images.%20Our%20extensive%20experiments%20on%0ADialogBen%20and%20user%20study%20demonstrate%20the%20effectiveness%20of%20DialogGen%20compared%0Awith%20other%20State-of-the-Art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08857v3&entry.124074799=Read"},
{"title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation\n  Learning", "author": "Yang Yue and Yulin Wang and Chenxin Tao and Pan Liu and Shiji Song and Gao Huang", "abstract": "  Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.\n", "link": "http://arxiv.org/abs/2504.13820v1", "date": "2025-04-18", "relevancy": 2.8295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CheXWorld%3A%20Exploring%20Image%20World%20Modeling%20for%20Radiograph%20Representation%0A%20%20Learning&body=Title%3A%20CheXWorld%3A%20Exploring%20Image%20World%20Modeling%20for%20Radiograph%20Representation%0A%20%20Learning%0AAuthor%3A%20Yang%20Yue%20and%20Yulin%20Wang%20and%20Chenxin%20Tao%20and%20Pan%20Liu%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Humans%20can%20develop%20internal%20world%20models%20that%20encode%20common%20sense%20knowledge%2C%0Atelling%20them%20how%20the%20world%20works%20and%20predicting%20the%20consequences%20of%20their%0Aactions.%20This%20concept%20has%20emerged%20as%20a%20promising%20direction%20for%20establishing%0Ageneral-purpose%20machine-learning%20models%20in%20recent%20preliminary%20works%2C%20e.g.%2C%20for%0Avisual%20representation%20learning.%20In%20this%20paper%2C%20we%20present%20CheXWorld%2C%20the%20first%0Aeffort%20towards%20a%20self-supervised%20world%20model%20for%20radiographic%20images.%0ASpecifically%2C%20our%20work%20develops%20a%20unified%20framework%20that%20simultaneously%20models%0Athree%20aspects%20of%20medical%20knowledge%20essential%20for%20qualified%20radiologists%2C%0Aincluding%201%29%20local%20anatomical%20structures%20describing%20the%20fine-grained%0Acharacteristics%20of%20local%20tissues%20%28e.g.%2C%20architectures%2C%20shapes%2C%20and%20textures%29%3B%0A2%29%20global%20anatomical%20layouts%20describing%20the%20global%20organization%20of%20the%20human%0Abody%20%28e.g.%2C%20layouts%20of%20organs%20and%20skeletons%29%3B%20and%203%29%20domain%20variations%20that%0Aencourage%20CheXWorld%20to%20model%20the%20transitions%20across%20different%20appearance%0Adomains%20of%20radiographs%20%28e.g.%2C%20varying%20clarity%2C%20contrast%2C%20and%20exposure%20caused%20by%0Acollecting%20radiographs%20from%20different%20hospitals%2C%20devices%2C%20or%20patients%29.%0AEmpirically%2C%20we%20design%20tailored%20qualitative%20and%20quantitative%20analyses%2C%0Arevealing%20that%20CheXWorld%20successfully%20captures%20these%20three%20dimensions%20of%0Amedical%20knowledge.%20Furthermore%2C%20transfer%20learning%20experiments%20across%20eight%0Amedical%20image%20classification%20and%20segmentation%20benchmarks%20showcase%20that%0ACheXWorld%20significantly%20outperforms%20existing%20SSL%20methods%20and%20large-scale%0Amedical%20foundation%20models.%20Code%20%26%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/CheXWorld.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCheXWorld%253A%2520Exploring%2520Image%2520World%2520Modeling%2520for%2520Radiograph%2520Representation%250A%2520%2520Learning%26entry.906535625%3DYang%2520Yue%2520and%2520Yulin%2520Wang%2520and%2520Chenxin%2520Tao%2520and%2520Pan%2520Liu%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Humans%2520can%2520develop%2520internal%2520world%2520models%2520that%2520encode%2520common%2520sense%2520knowledge%252C%250Atelling%2520them%2520how%2520the%2520world%2520works%2520and%2520predicting%2520the%2520consequences%2520of%2520their%250Aactions.%2520This%2520concept%2520has%2520emerged%2520as%2520a%2520promising%2520direction%2520for%2520establishing%250Ageneral-purpose%2520machine-learning%2520models%2520in%2520recent%2520preliminary%2520works%252C%2520e.g.%252C%2520for%250Avisual%2520representation%2520learning.%2520In%2520this%2520paper%252C%2520we%2520present%2520CheXWorld%252C%2520the%2520first%250Aeffort%2520towards%2520a%2520self-supervised%2520world%2520model%2520for%2520radiographic%2520images.%250ASpecifically%252C%2520our%2520work%2520develops%2520a%2520unified%2520framework%2520that%2520simultaneously%2520models%250Athree%2520aspects%2520of%2520medical%2520knowledge%2520essential%2520for%2520qualified%2520radiologists%252C%250Aincluding%25201%2529%2520local%2520anatomical%2520structures%2520describing%2520the%2520fine-grained%250Acharacteristics%2520of%2520local%2520tissues%2520%2528e.g.%252C%2520architectures%252C%2520shapes%252C%2520and%2520textures%2529%253B%250A2%2529%2520global%2520anatomical%2520layouts%2520describing%2520the%2520global%2520organization%2520of%2520the%2520human%250Abody%2520%2528e.g.%252C%2520layouts%2520of%2520organs%2520and%2520skeletons%2529%253B%2520and%25203%2529%2520domain%2520variations%2520that%250Aencourage%2520CheXWorld%2520to%2520model%2520the%2520transitions%2520across%2520different%2520appearance%250Adomains%2520of%2520radiographs%2520%2528e.g.%252C%2520varying%2520clarity%252C%2520contrast%252C%2520and%2520exposure%2520caused%2520by%250Acollecting%2520radiographs%2520from%2520different%2520hospitals%252C%2520devices%252C%2520or%2520patients%2529.%250AEmpirically%252C%2520we%2520design%2520tailored%2520qualitative%2520and%2520quantitative%2520analyses%252C%250Arevealing%2520that%2520CheXWorld%2520successfully%2520captures%2520these%2520three%2520dimensions%2520of%250Amedical%2520knowledge.%2520Furthermore%252C%2520transfer%2520learning%2520experiments%2520across%2520eight%250Amedical%2520image%2520classification%2520and%2520segmentation%2520benchmarks%2520showcase%2520that%250ACheXWorld%2520significantly%2520outperforms%2520existing%2520SSL%2520methods%2520and%2520large-scale%250Amedical%2520foundation%2520models.%2520Code%2520%2526%2520pre-trained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/LeapLabTHU/CheXWorld.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CheXWorld%3A%20Exploring%20Image%20World%20Modeling%20for%20Radiograph%20Representation%0A%20%20Learning&entry.906535625=Yang%20Yue%20and%20Yulin%20Wang%20and%20Chenxin%20Tao%20and%20Pan%20Liu%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Humans%20can%20develop%20internal%20world%20models%20that%20encode%20common%20sense%20knowledge%2C%0Atelling%20them%20how%20the%20world%20works%20and%20predicting%20the%20consequences%20of%20their%0Aactions.%20This%20concept%20has%20emerged%20as%20a%20promising%20direction%20for%20establishing%0Ageneral-purpose%20machine-learning%20models%20in%20recent%20preliminary%20works%2C%20e.g.%2C%20for%0Avisual%20representation%20learning.%20In%20this%20paper%2C%20we%20present%20CheXWorld%2C%20the%20first%0Aeffort%20towards%20a%20self-supervised%20world%20model%20for%20radiographic%20images.%0ASpecifically%2C%20our%20work%20develops%20a%20unified%20framework%20that%20simultaneously%20models%0Athree%20aspects%20of%20medical%20knowledge%20essential%20for%20qualified%20radiologists%2C%0Aincluding%201%29%20local%20anatomical%20structures%20describing%20the%20fine-grained%0Acharacteristics%20of%20local%20tissues%20%28e.g.%2C%20architectures%2C%20shapes%2C%20and%20textures%29%3B%0A2%29%20global%20anatomical%20layouts%20describing%20the%20global%20organization%20of%20the%20human%0Abody%20%28e.g.%2C%20layouts%20of%20organs%20and%20skeletons%29%3B%20and%203%29%20domain%20variations%20that%0Aencourage%20CheXWorld%20to%20model%20the%20transitions%20across%20different%20appearance%0Adomains%20of%20radiographs%20%28e.g.%2C%20varying%20clarity%2C%20contrast%2C%20and%20exposure%20caused%20by%0Acollecting%20radiographs%20from%20different%20hospitals%2C%20devices%2C%20or%20patients%29.%0AEmpirically%2C%20we%20design%20tailored%20qualitative%20and%20quantitative%20analyses%2C%0Arevealing%20that%20CheXWorld%20successfully%20captures%20these%20three%20dimensions%20of%0Amedical%20knowledge.%20Furthermore%2C%20transfer%20learning%20experiments%20across%20eight%0Amedical%20image%20classification%20and%20segmentation%20benchmarks%20showcase%20that%0ACheXWorld%20significantly%20outperforms%20existing%20SSL%20methods%20and%20large-scale%0Amedical%20foundation%20models.%20Code%20%26%20pre-trained%20models%20are%20available%20at%0Ahttps%3A//github.com/LeapLabTHU/CheXWorld.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13820v1&entry.124074799=Read"},
{"title": "Scaling sparse feature circuit finding for in-context learning", "author": "Dmitrii Kharlapenko and Stepan Shabalin and Fazl Barez and Arthur Conmy and Neel Nanda", "abstract": "  Sparse autoencoders (SAEs) are a popular tool for interpreting large language\nmodel activations, but their utility in addressing open questions in\ninterpretability remains unclear. In this work, we demonstrate their\neffectiveness by using SAEs to deepen our understanding of the mechanism behind\nin-context learning (ICL). We identify abstract SAE features that (i) encode\nthe model's knowledge of which task to execute and (ii) whose latent vectors\ncausally induce the task zero-shot. This aligns with prior work showing that\nICL is mediated by task vectors. We further demonstrate that these task vectors\nare well approximated by a sparse sum of SAE latents, including these\ntask-execution features. To explore the ICL mechanism, we adapt the sparse\nfeature circuits methodology of Marks et al. (2024) to work for the much larger\nGemma-1 2B model, with 30 times as many parameters, and to the more complex\ntask of ICL. Through circuit finding, we discover task-detecting features with\ncorresponding SAE latents that activate earlier in the prompt, that detect when\ntasks have been performed. They are causally linked with task-execution\nfeatures through the attention and MLP sublayers.\n", "link": "http://arxiv.org/abs/2504.13756v1", "date": "2025-04-18", "relevancy": 2.7529, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20sparse%20feature%20circuit%20finding%20for%20in-context%20learning&body=Title%3A%20Scaling%20sparse%20feature%20circuit%20finding%20for%20in-context%20learning%0AAuthor%3A%20Dmitrii%20Kharlapenko%20and%20Stepan%20Shabalin%20and%20Fazl%20Barez%20and%20Arthur%20Conmy%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20tool%20for%20interpreting%20large%20language%0Amodel%20activations%2C%20but%20their%20utility%20in%20addressing%20open%20questions%20in%0Ainterpretability%20remains%20unclear.%20In%20this%20work%2C%20we%20demonstrate%20their%0Aeffectiveness%20by%20using%20SAEs%20to%20deepen%20our%20understanding%20of%20the%20mechanism%20behind%0Ain-context%20learning%20%28ICL%29.%20We%20identify%20abstract%20SAE%20features%20that%20%28i%29%20encode%0Athe%20model%27s%20knowledge%20of%20which%20task%20to%20execute%20and%20%28ii%29%20whose%20latent%20vectors%0Acausally%20induce%20the%20task%20zero-shot.%20This%20aligns%20with%20prior%20work%20showing%20that%0AICL%20is%20mediated%20by%20task%20vectors.%20We%20further%20demonstrate%20that%20these%20task%20vectors%0Aare%20well%20approximated%20by%20a%20sparse%20sum%20of%20SAE%20latents%2C%20including%20these%0Atask-execution%20features.%20To%20explore%20the%20ICL%20mechanism%2C%20we%20adapt%20the%20sparse%0Afeature%20circuits%20methodology%20of%20Marks%20et%20al.%20%282024%29%20to%20work%20for%20the%20much%20larger%0AGemma-1%202B%20model%2C%20with%2030%20times%20as%20many%20parameters%2C%20and%20to%20the%20more%20complex%0Atask%20of%20ICL.%20Through%20circuit%20finding%2C%20we%20discover%20task-detecting%20features%20with%0Acorresponding%20SAE%20latents%20that%20activate%20earlier%20in%20the%20prompt%2C%20that%20detect%20when%0Atasks%20have%20been%20performed.%20They%20are%20causally%20linked%20with%20task-execution%0Afeatures%20through%20the%20attention%20and%20MLP%20sublayers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520sparse%2520feature%2520circuit%2520finding%2520for%2520in-context%2520learning%26entry.906535625%3DDmitrii%2520Kharlapenko%2520and%2520Stepan%2520Shabalin%2520and%2520Fazl%2520Barez%2520and%2520Arthur%2520Conmy%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520popular%2520tool%2520for%2520interpreting%2520large%2520language%250Amodel%2520activations%252C%2520but%2520their%2520utility%2520in%2520addressing%2520open%2520questions%2520in%250Ainterpretability%2520remains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520their%250Aeffectiveness%2520by%2520using%2520SAEs%2520to%2520deepen%2520our%2520understanding%2520of%2520the%2520mechanism%2520behind%250Ain-context%2520learning%2520%2528ICL%2529.%2520We%2520identify%2520abstract%2520SAE%2520features%2520that%2520%2528i%2529%2520encode%250Athe%2520model%2527s%2520knowledge%2520of%2520which%2520task%2520to%2520execute%2520and%2520%2528ii%2529%2520whose%2520latent%2520vectors%250Acausally%2520induce%2520the%2520task%2520zero-shot.%2520This%2520aligns%2520with%2520prior%2520work%2520showing%2520that%250AICL%2520is%2520mediated%2520by%2520task%2520vectors.%2520We%2520further%2520demonstrate%2520that%2520these%2520task%2520vectors%250Aare%2520well%2520approximated%2520by%2520a%2520sparse%2520sum%2520of%2520SAE%2520latents%252C%2520including%2520these%250Atask-execution%2520features.%2520To%2520explore%2520the%2520ICL%2520mechanism%252C%2520we%2520adapt%2520the%2520sparse%250Afeature%2520circuits%2520methodology%2520of%2520Marks%2520et%2520al.%2520%25282024%2529%2520to%2520work%2520for%2520the%2520much%2520larger%250AGemma-1%25202B%2520model%252C%2520with%252030%2520times%2520as%2520many%2520parameters%252C%2520and%2520to%2520the%2520more%2520complex%250Atask%2520of%2520ICL.%2520Through%2520circuit%2520finding%252C%2520we%2520discover%2520task-detecting%2520features%2520with%250Acorresponding%2520SAE%2520latents%2520that%2520activate%2520earlier%2520in%2520the%2520prompt%252C%2520that%2520detect%2520when%250Atasks%2520have%2520been%2520performed.%2520They%2520are%2520causally%2520linked%2520with%2520task-execution%250Afeatures%2520through%2520the%2520attention%2520and%2520MLP%2520sublayers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20sparse%20feature%20circuit%20finding%20for%20in-context%20learning&entry.906535625=Dmitrii%20Kharlapenko%20and%20Stepan%20Shabalin%20and%20Fazl%20Barez%20and%20Arthur%20Conmy%20and%20Neel%20Nanda&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20popular%20tool%20for%20interpreting%20large%20language%0Amodel%20activations%2C%20but%20their%20utility%20in%20addressing%20open%20questions%20in%0Ainterpretability%20remains%20unclear.%20In%20this%20work%2C%20we%20demonstrate%20their%0Aeffectiveness%20by%20using%20SAEs%20to%20deepen%20our%20understanding%20of%20the%20mechanism%20behind%0Ain-context%20learning%20%28ICL%29.%20We%20identify%20abstract%20SAE%20features%20that%20%28i%29%20encode%0Athe%20model%27s%20knowledge%20of%20which%20task%20to%20execute%20and%20%28ii%29%20whose%20latent%20vectors%0Acausally%20induce%20the%20task%20zero-shot.%20This%20aligns%20with%20prior%20work%20showing%20that%0AICL%20is%20mediated%20by%20task%20vectors.%20We%20further%20demonstrate%20that%20these%20task%20vectors%0Aare%20well%20approximated%20by%20a%20sparse%20sum%20of%20SAE%20latents%2C%20including%20these%0Atask-execution%20features.%20To%20explore%20the%20ICL%20mechanism%2C%20we%20adapt%20the%20sparse%0Afeature%20circuits%20methodology%20of%20Marks%20et%20al.%20%282024%29%20to%20work%20for%20the%20much%20larger%0AGemma-1%202B%20model%2C%20with%2030%20times%20as%20many%20parameters%2C%20and%20to%20the%20more%20complex%0Atask%20of%20ICL.%20Through%20circuit%20finding%2C%20we%20discover%20task-detecting%20features%20with%0Acorresponding%20SAE%20latents%20that%20activate%20earlier%20in%20the%20prompt%2C%20that%20detect%20when%0Atasks%20have%20been%20performed.%20They%20are%20causally%20linked%20with%20task-execution%0Afeatures%20through%20the%20attention%20and%20MLP%20sublayers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13756v1&entry.124074799=Read"},
{"title": "Exploring Multimodal Prompt for Visualization Authoring with Large\n  Language Models", "author": "Zhen Wen and Luoxuan Weng and Yinghao Tang and Runjin Zhang and Yuxin Liu and Bo Pan and Minfeng Zhu and Wei Chen", "abstract": "  Recent advances in large language models (LLMs) have shown great potential in\nautomating the process of visualization authoring through simple natural\nlanguage utterances. However, instructing LLMs using natural language is\nlimited in precision and expressiveness for conveying visualization intent,\nleading to misinterpretation and time-consuming iterations. To address these\nlimitations, we conduct an empirical study to understand how LLMs interpret\nambiguous or incomplete text prompts in the context of visualization authoring,\nand the conditions making LLMs misinterpret user intent. Informed by the\nfindings, we introduce visual prompts as a complementary input modality to text\nprompts, which help clarify user intent and improve LLMs' interpretation\nabilities. To explore the potential of multimodal prompting in visualization\nauthoring, we design VisPilot, which enables users to easily create\nvisualizations using multimodal prompts, including text, sketches, and direct\nmanipulations on existing visualizations. Through two case studies and a\ncontrolled user study, we demonstrate that VisPilot provides a more intuitive\nway to create visualizations without affecting the overall task efficiency\ncompared to text-only prompting approaches. Furthermore, we analyze the impact\nof text and visual prompts in different visualization tasks. Our findings\nhighlight the importance of multimodal prompting in improving the usability of\nLLMs for visualization authoring. We discuss design implications for future\nvisualization systems and provide insights into how multimodal prompts can\nenhance human-AI collaboration in creative visualization tasks. All materials\nare available at https://OSF.IO/2QRAK.\n", "link": "http://arxiv.org/abs/2504.13700v1", "date": "2025-04-18", "relevancy": 2.6854, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Multimodal%20Prompt%20for%20Visualization%20Authoring%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Exploring%20Multimodal%20Prompt%20for%20Visualization%20Authoring%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Zhen%20Wen%20and%20Luoxuan%20Weng%20and%20Yinghao%20Tang%20and%20Runjin%20Zhang%20and%20Yuxin%20Liu%20and%20Bo%20Pan%20and%20Minfeng%20Zhu%20and%20Wei%20Chen%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20great%20potential%20in%0Aautomating%20the%20process%20of%20visualization%20authoring%20through%20simple%20natural%0Alanguage%20utterances.%20However%2C%20instructing%20LLMs%20using%20natural%20language%20is%0Alimited%20in%20precision%20and%20expressiveness%20for%20conveying%20visualization%20intent%2C%0Aleading%20to%20misinterpretation%20and%20time-consuming%20iterations.%20To%20address%20these%0Alimitations%2C%20we%20conduct%20an%20empirical%20study%20to%20understand%20how%20LLMs%20interpret%0Aambiguous%20or%20incomplete%20text%20prompts%20in%20the%20context%20of%20visualization%20authoring%2C%0Aand%20the%20conditions%20making%20LLMs%20misinterpret%20user%20intent.%20Informed%20by%20the%0Afindings%2C%20we%20introduce%20visual%20prompts%20as%20a%20complementary%20input%20modality%20to%20text%0Aprompts%2C%20which%20help%20clarify%20user%20intent%20and%20improve%20LLMs%27%20interpretation%0Aabilities.%20To%20explore%20the%20potential%20of%20multimodal%20prompting%20in%20visualization%0Aauthoring%2C%20we%20design%20VisPilot%2C%20which%20enables%20users%20to%20easily%20create%0Avisualizations%20using%20multimodal%20prompts%2C%20including%20text%2C%20sketches%2C%20and%20direct%0Amanipulations%20on%20existing%20visualizations.%20Through%20two%20case%20studies%20and%20a%0Acontrolled%20user%20study%2C%20we%20demonstrate%20that%20VisPilot%20provides%20a%20more%20intuitive%0Away%20to%20create%20visualizations%20without%20affecting%20the%20overall%20task%20efficiency%0Acompared%20to%20text-only%20prompting%20approaches.%20Furthermore%2C%20we%20analyze%20the%20impact%0Aof%20text%20and%20visual%20prompts%20in%20different%20visualization%20tasks.%20Our%20findings%0Ahighlight%20the%20importance%20of%20multimodal%20prompting%20in%20improving%20the%20usability%20of%0ALLMs%20for%20visualization%20authoring.%20We%20discuss%20design%20implications%20for%20future%0Avisualization%20systems%20and%20provide%20insights%20into%20how%20multimodal%20prompts%20can%0Aenhance%20human-AI%20collaboration%20in%20creative%20visualization%20tasks.%20All%20materials%0Aare%20available%20at%20https%3A//OSF.IO/2QRAK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Multimodal%2520Prompt%2520for%2520Visualization%2520Authoring%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DZhen%2520Wen%2520and%2520Luoxuan%2520Weng%2520and%2520Yinghao%2520Tang%2520and%2520Runjin%2520Zhang%2520and%2520Yuxin%2520Liu%2520and%2520Bo%2520Pan%2520and%2520Minfeng%2520Zhu%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520great%2520potential%2520in%250Aautomating%2520the%2520process%2520of%2520visualization%2520authoring%2520through%2520simple%2520natural%250Alanguage%2520utterances.%2520However%252C%2520instructing%2520LLMs%2520using%2520natural%2520language%2520is%250Alimited%2520in%2520precision%2520and%2520expressiveness%2520for%2520conveying%2520visualization%2520intent%252C%250Aleading%2520to%2520misinterpretation%2520and%2520time-consuming%2520iterations.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520to%2520understand%2520how%2520LLMs%2520interpret%250Aambiguous%2520or%2520incomplete%2520text%2520prompts%2520in%2520the%2520context%2520of%2520visualization%2520authoring%252C%250Aand%2520the%2520conditions%2520making%2520LLMs%2520misinterpret%2520user%2520intent.%2520Informed%2520by%2520the%250Afindings%252C%2520we%2520introduce%2520visual%2520prompts%2520as%2520a%2520complementary%2520input%2520modality%2520to%2520text%250Aprompts%252C%2520which%2520help%2520clarify%2520user%2520intent%2520and%2520improve%2520LLMs%2527%2520interpretation%250Aabilities.%2520To%2520explore%2520the%2520potential%2520of%2520multimodal%2520prompting%2520in%2520visualization%250Aauthoring%252C%2520we%2520design%2520VisPilot%252C%2520which%2520enables%2520users%2520to%2520easily%2520create%250Avisualizations%2520using%2520multimodal%2520prompts%252C%2520including%2520text%252C%2520sketches%252C%2520and%2520direct%250Amanipulations%2520on%2520existing%2520visualizations.%2520Through%2520two%2520case%2520studies%2520and%2520a%250Acontrolled%2520user%2520study%252C%2520we%2520demonstrate%2520that%2520VisPilot%2520provides%2520a%2520more%2520intuitive%250Away%2520to%2520create%2520visualizations%2520without%2520affecting%2520the%2520overall%2520task%2520efficiency%250Acompared%2520to%2520text-only%2520prompting%2520approaches.%2520Furthermore%252C%2520we%2520analyze%2520the%2520impact%250Aof%2520text%2520and%2520visual%2520prompts%2520in%2520different%2520visualization%2520tasks.%2520Our%2520findings%250Ahighlight%2520the%2520importance%2520of%2520multimodal%2520prompting%2520in%2520improving%2520the%2520usability%2520of%250ALLMs%2520for%2520visualization%2520authoring.%2520We%2520discuss%2520design%2520implications%2520for%2520future%250Avisualization%2520systems%2520and%2520provide%2520insights%2520into%2520how%2520multimodal%2520prompts%2520can%250Aenhance%2520human-AI%2520collaboration%2520in%2520creative%2520visualization%2520tasks.%2520All%2520materials%250Aare%2520available%2520at%2520https%253A//OSF.IO/2QRAK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Multimodal%20Prompt%20for%20Visualization%20Authoring%20with%20Large%0A%20%20Language%20Models&entry.906535625=Zhen%20Wen%20and%20Luoxuan%20Weng%20and%20Yinghao%20Tang%20and%20Runjin%20Zhang%20and%20Yuxin%20Liu%20and%20Bo%20Pan%20and%20Minfeng%20Zhu%20and%20Wei%20Chen&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20shown%20great%20potential%20in%0Aautomating%20the%20process%20of%20visualization%20authoring%20through%20simple%20natural%0Alanguage%20utterances.%20However%2C%20instructing%20LLMs%20using%20natural%20language%20is%0Alimited%20in%20precision%20and%20expressiveness%20for%20conveying%20visualization%20intent%2C%0Aleading%20to%20misinterpretation%20and%20time-consuming%20iterations.%20To%20address%20these%0Alimitations%2C%20we%20conduct%20an%20empirical%20study%20to%20understand%20how%20LLMs%20interpret%0Aambiguous%20or%20incomplete%20text%20prompts%20in%20the%20context%20of%20visualization%20authoring%2C%0Aand%20the%20conditions%20making%20LLMs%20misinterpret%20user%20intent.%20Informed%20by%20the%0Afindings%2C%20we%20introduce%20visual%20prompts%20as%20a%20complementary%20input%20modality%20to%20text%0Aprompts%2C%20which%20help%20clarify%20user%20intent%20and%20improve%20LLMs%27%20interpretation%0Aabilities.%20To%20explore%20the%20potential%20of%20multimodal%20prompting%20in%20visualization%0Aauthoring%2C%20we%20design%20VisPilot%2C%20which%20enables%20users%20to%20easily%20create%0Avisualizations%20using%20multimodal%20prompts%2C%20including%20text%2C%20sketches%2C%20and%20direct%0Amanipulations%20on%20existing%20visualizations.%20Through%20two%20case%20studies%20and%20a%0Acontrolled%20user%20study%2C%20we%20demonstrate%20that%20VisPilot%20provides%20a%20more%20intuitive%0Away%20to%20create%20visualizations%20without%20affecting%20the%20overall%20task%20efficiency%0Acompared%20to%20text-only%20prompting%20approaches.%20Furthermore%2C%20we%20analyze%20the%20impact%0Aof%20text%20and%20visual%20prompts%20in%20different%20visualization%20tasks.%20Our%20findings%0Ahighlight%20the%20importance%20of%20multimodal%20prompting%20in%20improving%20the%20usability%20of%0ALLMs%20for%20visualization%20authoring.%20We%20discuss%20design%20implications%20for%20future%0Avisualization%20systems%20and%20provide%20insights%20into%20how%20multimodal%20prompts%20can%0Aenhance%20human-AI%20collaboration%20in%20creative%20visualization%20tasks.%20All%20materials%0Aare%20available%20at%20https%3A//OSF.IO/2QRAK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13700v1&entry.124074799=Read"},
{"title": "KAN or MLP? Point Cloud Shows the Way Forward", "author": "Yan Shi and Qingdong He and Yijun Liu and Xiaoyu Liu and Jingyong Su", "abstract": "  Multi-Layer Perceptrons (MLPs) have become one of the fundamental\narchitectural component in point cloud analysis due to its effective feature\nlearning mechanism. However, when processing complex geometric structures in\npoint clouds, MLPs' fixed activation functions struggle to efficiently capture\nlocal geometric features, while suffering from poor parameter efficiency and\nhigh model redundancy. In this paper, we propose PointKAN, which applies\nKolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate\ntheir efficacy in hierarchical feature representation. First, we introduce a\nGeometric Affine Module (GAM) to transform local features, improving the\nmodel's robustness to geometric variations. Next, in the Local Feature\nProcessing (LFP), a parallel structure extracts both group-level features and\nglobal context, providing a rich representation of both fine details and\noverall structure. Finally, these features are combined and processed in the\nGlobal Feature Processing (GFP). By repeating these operations, the receptive\nfield gradually expands, enabling the model to capture complete geometric\ninformation of the point cloud. To overcome the high parameter counts and\ncomputational inefficiency of standard KANs, we develop Efficient-KANs in the\nPointKAN-elite variant, which significantly reduces parameters while\nmaintaining accuracy. Experimental results demonstrate that PointKAN\noutperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN,\nand ShapeNetPart, with particularly strong performance in Few-shot Learning\ntask. Additionally, PointKAN achieves substantial reductions in parameter\ncounts and computational complexity (FLOPs). This work highlights the potential\nof KANs-based architectures in 3D vision and opens new avenues for research in\npoint cloud understanding.\n", "link": "http://arxiv.org/abs/2504.13593v1", "date": "2025-04-18", "relevancy": 2.6721, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAN%20or%20MLP%3F%20Point%20Cloud%20Shows%20the%20Way%20Forward&body=Title%3A%20KAN%20or%20MLP%3F%20Point%20Cloud%20Shows%20the%20Way%20Forward%0AAuthor%3A%20Yan%20Shi%20and%20Qingdong%20He%20and%20Yijun%20Liu%20and%20Xiaoyu%20Liu%20and%20Jingyong%20Su%0AAbstract%3A%20%20%20Multi-Layer%20Perceptrons%20%28MLPs%29%20have%20become%20one%20of%20the%20fundamental%0Aarchitectural%20component%20in%20point%20cloud%20analysis%20due%20to%20its%20effective%20feature%0Alearning%20mechanism.%20However%2C%20when%20processing%20complex%20geometric%20structures%20in%0Apoint%20clouds%2C%20MLPs%27%20fixed%20activation%20functions%20struggle%20to%20efficiently%20capture%0Alocal%20geometric%20features%2C%20while%20suffering%20from%20poor%20parameter%20efficiency%20and%0Ahigh%20model%20redundancy.%20In%20this%20paper%2C%20we%20propose%20PointKAN%2C%20which%20applies%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20to%20point%20cloud%20analysis%20tasks%20to%20investigate%0Atheir%20efficacy%20in%20hierarchical%20feature%20representation.%20First%2C%20we%20introduce%20a%0AGeometric%20Affine%20Module%20%28GAM%29%20to%20transform%20local%20features%2C%20improving%20the%0Amodel%27s%20robustness%20to%20geometric%20variations.%20Next%2C%20in%20the%20Local%20Feature%0AProcessing%20%28LFP%29%2C%20a%20parallel%20structure%20extracts%20both%20group-level%20features%20and%0Aglobal%20context%2C%20providing%20a%20rich%20representation%20of%20both%20fine%20details%20and%0Aoverall%20structure.%20Finally%2C%20these%20features%20are%20combined%20and%20processed%20in%20the%0AGlobal%20Feature%20Processing%20%28GFP%29.%20By%20repeating%20these%20operations%2C%20the%20receptive%0Afield%20gradually%20expands%2C%20enabling%20the%20model%20to%20capture%20complete%20geometric%0Ainformation%20of%20the%20point%20cloud.%20To%20overcome%20the%20high%20parameter%20counts%20and%0Acomputational%20inefficiency%20of%20standard%20KANs%2C%20we%20develop%20Efficient-KANs%20in%20the%0APointKAN-elite%20variant%2C%20which%20significantly%20reduces%20parameters%20while%0Amaintaining%20accuracy.%20Experimental%20results%20demonstrate%20that%20PointKAN%0Aoutperforms%20PointMLP%20on%20benchmark%20datasets%20such%20as%20ModelNet40%2C%20ScanObjectNN%2C%0Aand%20ShapeNetPart%2C%20with%20particularly%20strong%20performance%20in%20Few-shot%20Learning%0Atask.%20Additionally%2C%20PointKAN%20achieves%20substantial%20reductions%20in%20parameter%0Acounts%20and%20computational%20complexity%20%28FLOPs%29.%20This%20work%20highlights%20the%20potential%0Aof%20KANs-based%20architectures%20in%203D%20vision%20and%20opens%20new%20avenues%20for%20research%20in%0Apoint%20cloud%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAN%2520or%2520MLP%253F%2520Point%2520Cloud%2520Shows%2520the%2520Way%2520Forward%26entry.906535625%3DYan%2520Shi%2520and%2520Qingdong%2520He%2520and%2520Yijun%2520Liu%2520and%2520Xiaoyu%2520Liu%2520and%2520Jingyong%2520Su%26entry.1292438233%3D%2520%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%2520have%2520become%2520one%2520of%2520the%2520fundamental%250Aarchitectural%2520component%2520in%2520point%2520cloud%2520analysis%2520due%2520to%2520its%2520effective%2520feature%250Alearning%2520mechanism.%2520However%252C%2520when%2520processing%2520complex%2520geometric%2520structures%2520in%250Apoint%2520clouds%252C%2520MLPs%2527%2520fixed%2520activation%2520functions%2520struggle%2520to%2520efficiently%2520capture%250Alocal%2520geometric%2520features%252C%2520while%2520suffering%2520from%2520poor%2520parameter%2520efficiency%2520and%250Ahigh%2520model%2520redundancy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PointKAN%252C%2520which%2520applies%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520to%2520point%2520cloud%2520analysis%2520tasks%2520to%2520investigate%250Atheir%2520efficacy%2520in%2520hierarchical%2520feature%2520representation.%2520First%252C%2520we%2520introduce%2520a%250AGeometric%2520Affine%2520Module%2520%2528GAM%2529%2520to%2520transform%2520local%2520features%252C%2520improving%2520the%250Amodel%2527s%2520robustness%2520to%2520geometric%2520variations.%2520Next%252C%2520in%2520the%2520Local%2520Feature%250AProcessing%2520%2528LFP%2529%252C%2520a%2520parallel%2520structure%2520extracts%2520both%2520group-level%2520features%2520and%250Aglobal%2520context%252C%2520providing%2520a%2520rich%2520representation%2520of%2520both%2520fine%2520details%2520and%250Aoverall%2520structure.%2520Finally%252C%2520these%2520features%2520are%2520combined%2520and%2520processed%2520in%2520the%250AGlobal%2520Feature%2520Processing%2520%2528GFP%2529.%2520By%2520repeating%2520these%2520operations%252C%2520the%2520receptive%250Afield%2520gradually%2520expands%252C%2520enabling%2520the%2520model%2520to%2520capture%2520complete%2520geometric%250Ainformation%2520of%2520the%2520point%2520cloud.%2520To%2520overcome%2520the%2520high%2520parameter%2520counts%2520and%250Acomputational%2520inefficiency%2520of%2520standard%2520KANs%252C%2520we%2520develop%2520Efficient-KANs%2520in%2520the%250APointKAN-elite%2520variant%252C%2520which%2520significantly%2520reduces%2520parameters%2520while%250Amaintaining%2520accuracy.%2520Experimental%2520results%2520demonstrate%2520that%2520PointKAN%250Aoutperforms%2520PointMLP%2520on%2520benchmark%2520datasets%2520such%2520as%2520ModelNet40%252C%2520ScanObjectNN%252C%250Aand%2520ShapeNetPart%252C%2520with%2520particularly%2520strong%2520performance%2520in%2520Few-shot%2520Learning%250Atask.%2520Additionally%252C%2520PointKAN%2520achieves%2520substantial%2520reductions%2520in%2520parameter%250Acounts%2520and%2520computational%2520complexity%2520%2528FLOPs%2529.%2520This%2520work%2520highlights%2520the%2520potential%250Aof%2520KANs-based%2520architectures%2520in%25203D%2520vision%2520and%2520opens%2520new%2520avenues%2520for%2520research%2520in%250Apoint%2520cloud%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAN%20or%20MLP%3F%20Point%20Cloud%20Shows%20the%20Way%20Forward&entry.906535625=Yan%20Shi%20and%20Qingdong%20He%20and%20Yijun%20Liu%20and%20Xiaoyu%20Liu%20and%20Jingyong%20Su&entry.1292438233=%20%20Multi-Layer%20Perceptrons%20%28MLPs%29%20have%20become%20one%20of%20the%20fundamental%0Aarchitectural%20component%20in%20point%20cloud%20analysis%20due%20to%20its%20effective%20feature%0Alearning%20mechanism.%20However%2C%20when%20processing%20complex%20geometric%20structures%20in%0Apoint%20clouds%2C%20MLPs%27%20fixed%20activation%20functions%20struggle%20to%20efficiently%20capture%0Alocal%20geometric%20features%2C%20while%20suffering%20from%20poor%20parameter%20efficiency%20and%0Ahigh%20model%20redundancy.%20In%20this%20paper%2C%20we%20propose%20PointKAN%2C%20which%20applies%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20to%20point%20cloud%20analysis%20tasks%20to%20investigate%0Atheir%20efficacy%20in%20hierarchical%20feature%20representation.%20First%2C%20we%20introduce%20a%0AGeometric%20Affine%20Module%20%28GAM%29%20to%20transform%20local%20features%2C%20improving%20the%0Amodel%27s%20robustness%20to%20geometric%20variations.%20Next%2C%20in%20the%20Local%20Feature%0AProcessing%20%28LFP%29%2C%20a%20parallel%20structure%20extracts%20both%20group-level%20features%20and%0Aglobal%20context%2C%20providing%20a%20rich%20representation%20of%20both%20fine%20details%20and%0Aoverall%20structure.%20Finally%2C%20these%20features%20are%20combined%20and%20processed%20in%20the%0AGlobal%20Feature%20Processing%20%28GFP%29.%20By%20repeating%20these%20operations%2C%20the%20receptive%0Afield%20gradually%20expands%2C%20enabling%20the%20model%20to%20capture%20complete%20geometric%0Ainformation%20of%20the%20point%20cloud.%20To%20overcome%20the%20high%20parameter%20counts%20and%0Acomputational%20inefficiency%20of%20standard%20KANs%2C%20we%20develop%20Efficient-KANs%20in%20the%0APointKAN-elite%20variant%2C%20which%20significantly%20reduces%20parameters%20while%0Amaintaining%20accuracy.%20Experimental%20results%20demonstrate%20that%20PointKAN%0Aoutperforms%20PointMLP%20on%20benchmark%20datasets%20such%20as%20ModelNet40%2C%20ScanObjectNN%2C%0Aand%20ShapeNetPart%2C%20with%20particularly%20strong%20performance%20in%20Few-shot%20Learning%0Atask.%20Additionally%2C%20PointKAN%20achieves%20substantial%20reductions%20in%20parameter%0Acounts%20and%20computational%20complexity%20%28FLOPs%29.%20This%20work%20highlights%20the%20potential%0Aof%20KANs-based%20architectures%20in%203D%20vision%20and%20opens%20new%20avenues%20for%20research%20in%0Apoint%20cloud%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13593v1&entry.124074799=Read"},
{"title": "Babysit A Language Model From Scratch: Interactive Language Learning by\n  Trials and Demonstrations", "author": "Ziqiao Ma and Zekun Wang and Joyce Chai", "abstract": "  Humans are efficient language learners and inherently social creatures. Our\nlanguage development is largely shaped by our social interactions, for example,\nthe demonstration and feedback from caregivers. Contrary to human language\nlearning, recent advancements in large language models have primarily adopted a\nnon-interactive training paradigm, and refined pre-trained models through\nfeedback afterward. In this work, we explore how corrective feedback from\ninteractions influences neural language acquisition from scratch through\nsystematically controlled experiments, assessing whether it contributes to word\nlearning efficiency in language models. We introduce a trial-and-demonstration\n(TnD) learning framework that incorporates three distinct components: student\ntrials, teacher demonstrations, and a reward conditioned on language competence\nat various developmental stages. Our experiments reveal that the TnD approach\naccelerates word acquisition for student models of equal and smaller numbers of\nparameters, and we highlight the significance of both trials and\ndemonstrations. We further show that the teacher's choices of words influence\nstudents' word-specific learning efficiency, and a practice-makes-perfect\neffect is evident by a strong correlation between the frequency of words in\ntrials and their respective learning curves. Our findings suggest that\ninteractive language learning, with teacher demonstrations and active trials,\ncan facilitate efficient word learning in language models.\n", "link": "http://arxiv.org/abs/2405.13828v2", "date": "2025-04-18", "relevancy": 2.6103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Babysit%20A%20Language%20Model%20From%20Scratch%3A%20Interactive%20Language%20Learning%20by%0A%20%20Trials%20and%20Demonstrations&body=Title%3A%20Babysit%20A%20Language%20Model%20From%20Scratch%3A%20Interactive%20Language%20Learning%20by%0A%20%20Trials%20and%20Demonstrations%0AAuthor%3A%20Ziqiao%20Ma%20and%20Zekun%20Wang%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20Humans%20are%20efficient%20language%20learners%20and%20inherently%20social%20creatures.%20Our%0Alanguage%20development%20is%20largely%20shaped%20by%20our%20social%20interactions%2C%20for%20example%2C%0Athe%20demonstration%20and%20feedback%20from%20caregivers.%20Contrary%20to%20human%20language%0Alearning%2C%20recent%20advancements%20in%20large%20language%20models%20have%20primarily%20adopted%20a%0Anon-interactive%20training%20paradigm%2C%20and%20refined%20pre-trained%20models%20through%0Afeedback%20afterward.%20In%20this%20work%2C%20we%20explore%20how%20corrective%20feedback%20from%0Ainteractions%20influences%20neural%20language%20acquisition%20from%20scratch%20through%0Asystematically%20controlled%20experiments%2C%20assessing%20whether%20it%20contributes%20to%20word%0Alearning%20efficiency%20in%20language%20models.%20We%20introduce%20a%20trial-and-demonstration%0A%28TnD%29%20learning%20framework%20that%20incorporates%20three%20distinct%20components%3A%20student%0Atrials%2C%20teacher%20demonstrations%2C%20and%20a%20reward%20conditioned%20on%20language%20competence%0Aat%20various%20developmental%20stages.%20Our%20experiments%20reveal%20that%20the%20TnD%20approach%0Aaccelerates%20word%20acquisition%20for%20student%20models%20of%20equal%20and%20smaller%20numbers%20of%0Aparameters%2C%20and%20we%20highlight%20the%20significance%20of%20both%20trials%20and%0Ademonstrations.%20We%20further%20show%20that%20the%20teacher%27s%20choices%20of%20words%20influence%0Astudents%27%20word-specific%20learning%20efficiency%2C%20and%20a%20practice-makes-perfect%0Aeffect%20is%20evident%20by%20a%20strong%20correlation%20between%20the%20frequency%20of%20words%20in%0Atrials%20and%20their%20respective%20learning%20curves.%20Our%20findings%20suggest%20that%0Ainteractive%20language%20learning%2C%20with%20teacher%20demonstrations%20and%20active%20trials%2C%0Acan%20facilitate%20efficient%20word%20learning%20in%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBabysit%2520A%2520Language%2520Model%2520From%2520Scratch%253A%2520Interactive%2520Language%2520Learning%2520by%250A%2520%2520Trials%2520and%2520Demonstrations%26entry.906535625%3DZiqiao%2520Ma%2520and%2520Zekun%2520Wang%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520Humans%2520are%2520efficient%2520language%2520learners%2520and%2520inherently%2520social%2520creatures.%2520Our%250Alanguage%2520development%2520is%2520largely%2520shaped%2520by%2520our%2520social%2520interactions%252C%2520for%2520example%252C%250Athe%2520demonstration%2520and%2520feedback%2520from%2520caregivers.%2520Contrary%2520to%2520human%2520language%250Alearning%252C%2520recent%2520advancements%2520in%2520large%2520language%2520models%2520have%2520primarily%2520adopted%2520a%250Anon-interactive%2520training%2520paradigm%252C%2520and%2520refined%2520pre-trained%2520models%2520through%250Afeedback%2520afterward.%2520In%2520this%2520work%252C%2520we%2520explore%2520how%2520corrective%2520feedback%2520from%250Ainteractions%2520influences%2520neural%2520language%2520acquisition%2520from%2520scratch%2520through%250Asystematically%2520controlled%2520experiments%252C%2520assessing%2520whether%2520it%2520contributes%2520to%2520word%250Alearning%2520efficiency%2520in%2520language%2520models.%2520We%2520introduce%2520a%2520trial-and-demonstration%250A%2528TnD%2529%2520learning%2520framework%2520that%2520incorporates%2520three%2520distinct%2520components%253A%2520student%250Atrials%252C%2520teacher%2520demonstrations%252C%2520and%2520a%2520reward%2520conditioned%2520on%2520language%2520competence%250Aat%2520various%2520developmental%2520stages.%2520Our%2520experiments%2520reveal%2520that%2520the%2520TnD%2520approach%250Aaccelerates%2520word%2520acquisition%2520for%2520student%2520models%2520of%2520equal%2520and%2520smaller%2520numbers%2520of%250Aparameters%252C%2520and%2520we%2520highlight%2520the%2520significance%2520of%2520both%2520trials%2520and%250Ademonstrations.%2520We%2520further%2520show%2520that%2520the%2520teacher%2527s%2520choices%2520of%2520words%2520influence%250Astudents%2527%2520word-specific%2520learning%2520efficiency%252C%2520and%2520a%2520practice-makes-perfect%250Aeffect%2520is%2520evident%2520by%2520a%2520strong%2520correlation%2520between%2520the%2520frequency%2520of%2520words%2520in%250Atrials%2520and%2520their%2520respective%2520learning%2520curves.%2520Our%2520findings%2520suggest%2520that%250Ainteractive%2520language%2520learning%252C%2520with%2520teacher%2520demonstrations%2520and%2520active%2520trials%252C%250Acan%2520facilitate%2520efficient%2520word%2520learning%2520in%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Babysit%20A%20Language%20Model%20From%20Scratch%3A%20Interactive%20Language%20Learning%20by%0A%20%20Trials%20and%20Demonstrations&entry.906535625=Ziqiao%20Ma%20and%20Zekun%20Wang%20and%20Joyce%20Chai&entry.1292438233=%20%20Humans%20are%20efficient%20language%20learners%20and%20inherently%20social%20creatures.%20Our%0Alanguage%20development%20is%20largely%20shaped%20by%20our%20social%20interactions%2C%20for%20example%2C%0Athe%20demonstration%20and%20feedback%20from%20caregivers.%20Contrary%20to%20human%20language%0Alearning%2C%20recent%20advancements%20in%20large%20language%20models%20have%20primarily%20adopted%20a%0Anon-interactive%20training%20paradigm%2C%20and%20refined%20pre-trained%20models%20through%0Afeedback%20afterward.%20In%20this%20work%2C%20we%20explore%20how%20corrective%20feedback%20from%0Ainteractions%20influences%20neural%20language%20acquisition%20from%20scratch%20through%0Asystematically%20controlled%20experiments%2C%20assessing%20whether%20it%20contributes%20to%20word%0Alearning%20efficiency%20in%20language%20models.%20We%20introduce%20a%20trial-and-demonstration%0A%28TnD%29%20learning%20framework%20that%20incorporates%20three%20distinct%20components%3A%20student%0Atrials%2C%20teacher%20demonstrations%2C%20and%20a%20reward%20conditioned%20on%20language%20competence%0Aat%20various%20developmental%20stages.%20Our%20experiments%20reveal%20that%20the%20TnD%20approach%0Aaccelerates%20word%20acquisition%20for%20student%20models%20of%20equal%20and%20smaller%20numbers%20of%0Aparameters%2C%20and%20we%20highlight%20the%20significance%20of%20both%20trials%20and%0Ademonstrations.%20We%20further%20show%20that%20the%20teacher%27s%20choices%20of%20words%20influence%0Astudents%27%20word-specific%20learning%20efficiency%2C%20and%20a%20practice-makes-perfect%0Aeffect%20is%20evident%20by%20a%20strong%20correlation%20between%20the%20frequency%20of%20words%20in%0Atrials%20and%20their%20respective%20learning%20curves.%20Our%20findings%20suggest%20that%0Ainteractive%20language%20learning%2C%20with%20teacher%20demonstrations%20and%20active%20trials%2C%0Acan%20facilitate%20efficient%20word%20learning%20in%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13828v2&entry.124074799=Read"},
{"title": "FocusNet: Transformer-enhanced Polyp Segmentation with Local and Pooling\n  Attention", "author": "Jun Zeng and KC Santosh and Deepak Rajan Nayak and Thomas de Lange and Jonas Varkey and Tyler Berzin and Debesh Jha", "abstract": "  Colonoscopy is vital in the early diagnosis of colorectal polyps. Regular\nscreenings can effectively prevent benign polyps from progressing to CRC. While\ndeep learning has made impressive strides in polyp segmentation, most existing\nmodels are trained on single-modality and single-center data, making them less\neffective in real-world clinical environments. To overcome these limitations,\nwe propose FocusNet, a Transformer-enhanced focus attention network designed to\nimprove polyp segmentation. FocusNet incorporates three essential modules: the\nCross-semantic Interaction Decoder Module (CIDM) for generating coarse\nsegmentation maps, the Detail Enhancement Module (DEM) for refining shallow\nfeatures, and the Focus Attention Module (FAM), to balance local detail and\nglobal context through local and pooling attention mechanisms. We evaluate our\nmodel on PolypDB, a newly introduced dataset with multi-modality and\nmulti-center data for building more reliable segmentation methods. Extensive\nexperiments showed that FocusNet consistently outperforms existing\nstate-of-the-art approaches with a high dice coefficients of 82.47% on the BLI\nmodality, 88.46% on FICE, 92.04% on LCI, 82.09% on the NBI and 93.42% on WLI\nmodality, demonstrating its accuracy and robustness across five different\nmodalities. The source code for FocusNet is available at\nhttps://github.com/JunZengz/FocusNet.\n", "link": "http://arxiv.org/abs/2504.13597v1", "date": "2025-04-18", "relevancy": 2.5833, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocusNet%3A%20Transformer-enhanced%20Polyp%20Segmentation%20with%20Local%20and%20Pooling%0A%20%20Attention&body=Title%3A%20FocusNet%3A%20Transformer-enhanced%20Polyp%20Segmentation%20with%20Local%20and%20Pooling%0A%20%20Attention%0AAuthor%3A%20Jun%20Zeng%20and%20KC%20Santosh%20and%20Deepak%20Rajan%20Nayak%20and%20Thomas%20de%20Lange%20and%20Jonas%20Varkey%20and%20Tyler%20Berzin%20and%20Debesh%20Jha%0AAbstract%3A%20%20%20Colonoscopy%20is%20vital%20in%20the%20early%20diagnosis%20of%20colorectal%20polyps.%20Regular%0Ascreenings%20can%20effectively%20prevent%20benign%20polyps%20from%20progressing%20to%20CRC.%20While%0Adeep%20learning%20has%20made%20impressive%20strides%20in%20polyp%20segmentation%2C%20most%20existing%0Amodels%20are%20trained%20on%20single-modality%20and%20single-center%20data%2C%20making%20them%20less%0Aeffective%20in%20real-world%20clinical%20environments.%20To%20overcome%20these%20limitations%2C%0Awe%20propose%20FocusNet%2C%20a%20Transformer-enhanced%20focus%20attention%20network%20designed%20to%0Aimprove%20polyp%20segmentation.%20FocusNet%20incorporates%20three%20essential%20modules%3A%20the%0ACross-semantic%20Interaction%20Decoder%20Module%20%28CIDM%29%20for%20generating%20coarse%0Asegmentation%20maps%2C%20the%20Detail%20Enhancement%20Module%20%28DEM%29%20for%20refining%20shallow%0Afeatures%2C%20and%20the%20Focus%20Attention%20Module%20%28FAM%29%2C%20to%20balance%20local%20detail%20and%0Aglobal%20context%20through%20local%20and%20pooling%20attention%20mechanisms.%20We%20evaluate%20our%0Amodel%20on%20PolypDB%2C%20a%20newly%20introduced%20dataset%20with%20multi-modality%20and%0Amulti-center%20data%20for%20building%20more%20reliable%20segmentation%20methods.%20Extensive%0Aexperiments%20showed%20that%20FocusNet%20consistently%20outperforms%20existing%0Astate-of-the-art%20approaches%20with%20a%20high%20dice%20coefficients%20of%2082.47%25%20on%20the%20BLI%0Amodality%2C%2088.46%25%20on%20FICE%2C%2092.04%25%20on%20LCI%2C%2082.09%25%20on%20the%20NBI%20and%2093.42%25%20on%20WLI%0Amodality%2C%20demonstrating%20its%20accuracy%20and%20robustness%20across%20five%20different%0Amodalities.%20The%20source%20code%20for%20FocusNet%20is%20available%20at%0Ahttps%3A//github.com/JunZengz/FocusNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocusNet%253A%2520Transformer-enhanced%2520Polyp%2520Segmentation%2520with%2520Local%2520and%2520Pooling%250A%2520%2520Attention%26entry.906535625%3DJun%2520Zeng%2520and%2520KC%2520Santosh%2520and%2520Deepak%2520Rajan%2520Nayak%2520and%2520Thomas%2520de%2520Lange%2520and%2520Jonas%2520Varkey%2520and%2520Tyler%2520Berzin%2520and%2520Debesh%2520Jha%26entry.1292438233%3D%2520%2520Colonoscopy%2520is%2520vital%2520in%2520the%2520early%2520diagnosis%2520of%2520colorectal%2520polyps.%2520Regular%250Ascreenings%2520can%2520effectively%2520prevent%2520benign%2520polyps%2520from%2520progressing%2520to%2520CRC.%2520While%250Adeep%2520learning%2520has%2520made%2520impressive%2520strides%2520in%2520polyp%2520segmentation%252C%2520most%2520existing%250Amodels%2520are%2520trained%2520on%2520single-modality%2520and%2520single-center%2520data%252C%2520making%2520them%2520less%250Aeffective%2520in%2520real-world%2520clinical%2520environments.%2520To%2520overcome%2520these%2520limitations%252C%250Awe%2520propose%2520FocusNet%252C%2520a%2520Transformer-enhanced%2520focus%2520attention%2520network%2520designed%2520to%250Aimprove%2520polyp%2520segmentation.%2520FocusNet%2520incorporates%2520three%2520essential%2520modules%253A%2520the%250ACross-semantic%2520Interaction%2520Decoder%2520Module%2520%2528CIDM%2529%2520for%2520generating%2520coarse%250Asegmentation%2520maps%252C%2520the%2520Detail%2520Enhancement%2520Module%2520%2528DEM%2529%2520for%2520refining%2520shallow%250Afeatures%252C%2520and%2520the%2520Focus%2520Attention%2520Module%2520%2528FAM%2529%252C%2520to%2520balance%2520local%2520detail%2520and%250Aglobal%2520context%2520through%2520local%2520and%2520pooling%2520attention%2520mechanisms.%2520We%2520evaluate%2520our%250Amodel%2520on%2520PolypDB%252C%2520a%2520newly%2520introduced%2520dataset%2520with%2520multi-modality%2520and%250Amulti-center%2520data%2520for%2520building%2520more%2520reliable%2520segmentation%2520methods.%2520Extensive%250Aexperiments%2520showed%2520that%2520FocusNet%2520consistently%2520outperforms%2520existing%250Astate-of-the-art%2520approaches%2520with%2520a%2520high%2520dice%2520coefficients%2520of%252082.47%2525%2520on%2520the%2520BLI%250Amodality%252C%252088.46%2525%2520on%2520FICE%252C%252092.04%2525%2520on%2520LCI%252C%252082.09%2525%2520on%2520the%2520NBI%2520and%252093.42%2525%2520on%2520WLI%250Amodality%252C%2520demonstrating%2520its%2520accuracy%2520and%2520robustness%2520across%2520five%2520different%250Amodalities.%2520The%2520source%2520code%2520for%2520FocusNet%2520is%2520available%2520at%250Ahttps%253A//github.com/JunZengz/FocusNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusNet%3A%20Transformer-enhanced%20Polyp%20Segmentation%20with%20Local%20and%20Pooling%0A%20%20Attention&entry.906535625=Jun%20Zeng%20and%20KC%20Santosh%20and%20Deepak%20Rajan%20Nayak%20and%20Thomas%20de%20Lange%20and%20Jonas%20Varkey%20and%20Tyler%20Berzin%20and%20Debesh%20Jha&entry.1292438233=%20%20Colonoscopy%20is%20vital%20in%20the%20early%20diagnosis%20of%20colorectal%20polyps.%20Regular%0Ascreenings%20can%20effectively%20prevent%20benign%20polyps%20from%20progressing%20to%20CRC.%20While%0Adeep%20learning%20has%20made%20impressive%20strides%20in%20polyp%20segmentation%2C%20most%20existing%0Amodels%20are%20trained%20on%20single-modality%20and%20single-center%20data%2C%20making%20them%20less%0Aeffective%20in%20real-world%20clinical%20environments.%20To%20overcome%20these%20limitations%2C%0Awe%20propose%20FocusNet%2C%20a%20Transformer-enhanced%20focus%20attention%20network%20designed%20to%0Aimprove%20polyp%20segmentation.%20FocusNet%20incorporates%20three%20essential%20modules%3A%20the%0ACross-semantic%20Interaction%20Decoder%20Module%20%28CIDM%29%20for%20generating%20coarse%0Asegmentation%20maps%2C%20the%20Detail%20Enhancement%20Module%20%28DEM%29%20for%20refining%20shallow%0Afeatures%2C%20and%20the%20Focus%20Attention%20Module%20%28FAM%29%2C%20to%20balance%20local%20detail%20and%0Aglobal%20context%20through%20local%20and%20pooling%20attention%20mechanisms.%20We%20evaluate%20our%0Amodel%20on%20PolypDB%2C%20a%20newly%20introduced%20dataset%20with%20multi-modality%20and%0Amulti-center%20data%20for%20building%20more%20reliable%20segmentation%20methods.%20Extensive%0Aexperiments%20showed%20that%20FocusNet%20consistently%20outperforms%20existing%0Astate-of-the-art%20approaches%20with%20a%20high%20dice%20coefficients%20of%2082.47%25%20on%20the%20BLI%0Amodality%2C%2088.46%25%20on%20FICE%2C%2092.04%25%20on%20LCI%2C%2082.09%25%20on%20the%20NBI%20and%2093.42%25%20on%20WLI%0Amodality%2C%20demonstrating%20its%20accuracy%20and%20robustness%20across%20five%20different%0Amodalities.%20The%20source%20code%20for%20FocusNet%20is%20available%20at%0Ahttps%3A//github.com/JunZengz/FocusNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13597v1&entry.124074799=Read"},
{"title": "Learning to Attribute with Attention", "author": "Benjamin Cohen-Wang and Yung-Sung Chuang and Aleksander Madry", "abstract": "  Given a sequence of tokens generated by a language model, we may want to\nidentify the preceding tokens that influence the model to generate this\nsequence. Performing such token attribution is expensive; a common approach is\nto ablate preceding tokens and directly measure their effects. To reduce the\ncost of token attribution, we revisit attention weights as a heuristic for how\na language model uses previous tokens. Naive approaches to attribute model\nbehavior with attention (e.g., averaging attention weights across attention\nheads to estimate a token's influence) have been found to be unreliable. To\nattain faithful attributions, we propose treating the attention weights of\ndifferent attention heads as features. This way, we can learn how to\neffectively leverage attention weights for attribution (using signal from\nablations). Our resulting method, Attribution with Attention (AT2), reliably\nperforms on par with approaches that involve many ablations, while being\nsignificantly more efficient. To showcase the utility of AT2, we use it to\nprune less important parts of a provided context in a question answering\nsetting, improving answer quality. We provide code for AT2 at\nhttps://github.com/MadryLab/AT2 .\n", "link": "http://arxiv.org/abs/2504.13752v1", "date": "2025-04-18", "relevancy": 2.5427, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5539}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Attribute%20with%20Attention&body=Title%3A%20Learning%20to%20Attribute%20with%20Attention%0AAuthor%3A%20Benjamin%20Cohen-Wang%20and%20Yung-Sung%20Chuang%20and%20Aleksander%20Madry%0AAbstract%3A%20%20%20Given%20a%20sequence%20of%20tokens%20generated%20by%20a%20language%20model%2C%20we%20may%20want%20to%0Aidentify%20the%20preceding%20tokens%20that%20influence%20the%20model%20to%20generate%20this%0Asequence.%20Performing%20such%20token%20attribution%20is%20expensive%3B%20a%20common%20approach%20is%0Ato%20ablate%20preceding%20tokens%20and%20directly%20measure%20their%20effects.%20To%20reduce%20the%0Acost%20of%20token%20attribution%2C%20we%20revisit%20attention%20weights%20as%20a%20heuristic%20for%20how%0Aa%20language%20model%20uses%20previous%20tokens.%20Naive%20approaches%20to%20attribute%20model%0Abehavior%20with%20attention%20%28e.g.%2C%20averaging%20attention%20weights%20across%20attention%0Aheads%20to%20estimate%20a%20token%27s%20influence%29%20have%20been%20found%20to%20be%20unreliable.%20To%0Aattain%20faithful%20attributions%2C%20we%20propose%20treating%20the%20attention%20weights%20of%0Adifferent%20attention%20heads%20as%20features.%20This%20way%2C%20we%20can%20learn%20how%20to%0Aeffectively%20leverage%20attention%20weights%20for%20attribution%20%28using%20signal%20from%0Aablations%29.%20Our%20resulting%20method%2C%20Attribution%20with%20Attention%20%28AT2%29%2C%20reliably%0Aperforms%20on%20par%20with%20approaches%20that%20involve%20many%20ablations%2C%20while%20being%0Asignificantly%20more%20efficient.%20To%20showcase%20the%20utility%20of%20AT2%2C%20we%20use%20it%20to%0Aprune%20less%20important%20parts%20of%20a%20provided%20context%20in%20a%20question%20answering%0Asetting%2C%20improving%20answer%20quality.%20We%20provide%20code%20for%20AT2%20at%0Ahttps%3A//github.com/MadryLab/AT2%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Attribute%2520with%2520Attention%26entry.906535625%3DBenjamin%2520Cohen-Wang%2520and%2520Yung-Sung%2520Chuang%2520and%2520Aleksander%2520Madry%26entry.1292438233%3D%2520%2520Given%2520a%2520sequence%2520of%2520tokens%2520generated%2520by%2520a%2520language%2520model%252C%2520we%2520may%2520want%2520to%250Aidentify%2520the%2520preceding%2520tokens%2520that%2520influence%2520the%2520model%2520to%2520generate%2520this%250Asequence.%2520Performing%2520such%2520token%2520attribution%2520is%2520expensive%253B%2520a%2520common%2520approach%2520is%250Ato%2520ablate%2520preceding%2520tokens%2520and%2520directly%2520measure%2520their%2520effects.%2520To%2520reduce%2520the%250Acost%2520of%2520token%2520attribution%252C%2520we%2520revisit%2520attention%2520weights%2520as%2520a%2520heuristic%2520for%2520how%250Aa%2520language%2520model%2520uses%2520previous%2520tokens.%2520Naive%2520approaches%2520to%2520attribute%2520model%250Abehavior%2520with%2520attention%2520%2528e.g.%252C%2520averaging%2520attention%2520weights%2520across%2520attention%250Aheads%2520to%2520estimate%2520a%2520token%2527s%2520influence%2529%2520have%2520been%2520found%2520to%2520be%2520unreliable.%2520To%250Aattain%2520faithful%2520attributions%252C%2520we%2520propose%2520treating%2520the%2520attention%2520weights%2520of%250Adifferent%2520attention%2520heads%2520as%2520features.%2520This%2520way%252C%2520we%2520can%2520learn%2520how%2520to%250Aeffectively%2520leverage%2520attention%2520weights%2520for%2520attribution%2520%2528using%2520signal%2520from%250Aablations%2529.%2520Our%2520resulting%2520method%252C%2520Attribution%2520with%2520Attention%2520%2528AT2%2529%252C%2520reliably%250Aperforms%2520on%2520par%2520with%2520approaches%2520that%2520involve%2520many%2520ablations%252C%2520while%2520being%250Asignificantly%2520more%2520efficient.%2520To%2520showcase%2520the%2520utility%2520of%2520AT2%252C%2520we%2520use%2520it%2520to%250Aprune%2520less%2520important%2520parts%2520of%2520a%2520provided%2520context%2520in%2520a%2520question%2520answering%250Asetting%252C%2520improving%2520answer%2520quality.%2520We%2520provide%2520code%2520for%2520AT2%2520at%250Ahttps%253A//github.com/MadryLab/AT2%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Attribute%20with%20Attention&entry.906535625=Benjamin%20Cohen-Wang%20and%20Yung-Sung%20Chuang%20and%20Aleksander%20Madry&entry.1292438233=%20%20Given%20a%20sequence%20of%20tokens%20generated%20by%20a%20language%20model%2C%20we%20may%20want%20to%0Aidentify%20the%20preceding%20tokens%20that%20influence%20the%20model%20to%20generate%20this%0Asequence.%20Performing%20such%20token%20attribution%20is%20expensive%3B%20a%20common%20approach%20is%0Ato%20ablate%20preceding%20tokens%20and%20directly%20measure%20their%20effects.%20To%20reduce%20the%0Acost%20of%20token%20attribution%2C%20we%20revisit%20attention%20weights%20as%20a%20heuristic%20for%20how%0Aa%20language%20model%20uses%20previous%20tokens.%20Naive%20approaches%20to%20attribute%20model%0Abehavior%20with%20attention%20%28e.g.%2C%20averaging%20attention%20weights%20across%20attention%0Aheads%20to%20estimate%20a%20token%27s%20influence%29%20have%20been%20found%20to%20be%20unreliable.%20To%0Aattain%20faithful%20attributions%2C%20we%20propose%20treating%20the%20attention%20weights%20of%0Adifferent%20attention%20heads%20as%20features.%20This%20way%2C%20we%20can%20learn%20how%20to%0Aeffectively%20leverage%20attention%20weights%20for%20attribution%20%28using%20signal%20from%0Aablations%29.%20Our%20resulting%20method%2C%20Attribution%20with%20Attention%20%28AT2%29%2C%20reliably%0Aperforms%20on%20par%20with%20approaches%20that%20involve%20many%20ablations%2C%20while%20being%0Asignificantly%20more%20efficient.%20To%20showcase%20the%20utility%20of%20AT2%2C%20we%20use%20it%20to%0Aprune%20less%20important%20parts%20of%20a%20provided%20context%20in%20a%20question%20answering%0Asetting%2C%20improving%20answer%20quality.%20We%20provide%20code%20for%20AT2%20at%0Ahttps%3A//github.com/MadryLab/AT2%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13752v1&entry.124074799=Read"},
{"title": "Cross-Hierarchical Bidirectional Consistency Learning for Fine-Grained\n  Visual Classification", "author": "Pengxiang Gao and Yihao Liang and Yanzhi Song and Zhouwang Yang", "abstract": "  Fine-Grained Visual Classification (FGVC) aims to categorize closely related\nsubclasses, a task complicated by minimal inter-class differences and\nsignificant intra-class variance. Existing methods often rely on additional\nannotations for image classification, overlooking the valuable information\nembedded in Tree Hierarchies that depict hierarchical label relationships. To\nleverage this knowledge to improve classification accuracy and consistency, we\npropose a novel Cross-Hierarchical Bidirectional Consistency Learning (CHBC)\nframework. The CHBC framework extracts discriminative features across various\nhierarchies using a specially designed module to decompose and enhance\nattention masks and features. We employ bidirectional consistency loss to\nregulate the classification outcomes across different hierarchies, ensuring\nlabel prediction consistency and reducing misclassification. Experiments on\nthree widely used FGVC datasets validate the effectiveness of the CHBC\nframework. Ablation studies further investigate the application strategies of\nfeature enhancement and consistency constraints, underscoring the significant\ncontributions of the proposed modules.\n", "link": "http://arxiv.org/abs/2504.13608v1", "date": "2025-04-18", "relevancy": 2.5097, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5076}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5011}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Hierarchical%20Bidirectional%20Consistency%20Learning%20for%20Fine-Grained%0A%20%20Visual%20Classification&body=Title%3A%20Cross-Hierarchical%20Bidirectional%20Consistency%20Learning%20for%20Fine-Grained%0A%20%20Visual%20Classification%0AAuthor%3A%20Pengxiang%20Gao%20and%20Yihao%20Liang%20and%20Yanzhi%20Song%20and%20Zhouwang%20Yang%0AAbstract%3A%20%20%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20aims%20to%20categorize%20closely%20related%0Asubclasses%2C%20a%20task%20complicated%20by%20minimal%20inter-class%20differences%20and%0Asignificant%20intra-class%20variance.%20Existing%20methods%20often%20rely%20on%20additional%0Aannotations%20for%20image%20classification%2C%20overlooking%20the%20valuable%20information%0Aembedded%20in%20Tree%20Hierarchies%20that%20depict%20hierarchical%20label%20relationships.%20To%0Aleverage%20this%20knowledge%20to%20improve%20classification%20accuracy%20and%20consistency%2C%20we%0Apropose%20a%20novel%20Cross-Hierarchical%20Bidirectional%20Consistency%20Learning%20%28CHBC%29%0Aframework.%20The%20CHBC%20framework%20extracts%20discriminative%20features%20across%20various%0Ahierarchies%20using%20a%20specially%20designed%20module%20to%20decompose%20and%20enhance%0Aattention%20masks%20and%20features.%20We%20employ%20bidirectional%20consistency%20loss%20to%0Aregulate%20the%20classification%20outcomes%20across%20different%20hierarchies%2C%20ensuring%0Alabel%20prediction%20consistency%20and%20reducing%20misclassification.%20Experiments%20on%0Athree%20widely%20used%20FGVC%20datasets%20validate%20the%20effectiveness%20of%20the%20CHBC%0Aframework.%20Ablation%20studies%20further%20investigate%20the%20application%20strategies%20of%0Afeature%20enhancement%20and%20consistency%20constraints%2C%20underscoring%20the%20significant%0Acontributions%20of%20the%20proposed%20modules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Hierarchical%2520Bidirectional%2520Consistency%2520Learning%2520for%2520Fine-Grained%250A%2520%2520Visual%2520Classification%26entry.906535625%3DPengxiang%2520Gao%2520and%2520Yihao%2520Liang%2520and%2520Yanzhi%2520Song%2520and%2520Zhouwang%2520Yang%26entry.1292438233%3D%2520%2520Fine-Grained%2520Visual%2520Classification%2520%2528FGVC%2529%2520aims%2520to%2520categorize%2520closely%2520related%250Asubclasses%252C%2520a%2520task%2520complicated%2520by%2520minimal%2520inter-class%2520differences%2520and%250Asignificant%2520intra-class%2520variance.%2520Existing%2520methods%2520often%2520rely%2520on%2520additional%250Aannotations%2520for%2520image%2520classification%252C%2520overlooking%2520the%2520valuable%2520information%250Aembedded%2520in%2520Tree%2520Hierarchies%2520that%2520depict%2520hierarchical%2520label%2520relationships.%2520To%250Aleverage%2520this%2520knowledge%2520to%2520improve%2520classification%2520accuracy%2520and%2520consistency%252C%2520we%250Apropose%2520a%2520novel%2520Cross-Hierarchical%2520Bidirectional%2520Consistency%2520Learning%2520%2528CHBC%2529%250Aframework.%2520The%2520CHBC%2520framework%2520extracts%2520discriminative%2520features%2520across%2520various%250Ahierarchies%2520using%2520a%2520specially%2520designed%2520module%2520to%2520decompose%2520and%2520enhance%250Aattention%2520masks%2520and%2520features.%2520We%2520employ%2520bidirectional%2520consistency%2520loss%2520to%250Aregulate%2520the%2520classification%2520outcomes%2520across%2520different%2520hierarchies%252C%2520ensuring%250Alabel%2520prediction%2520consistency%2520and%2520reducing%2520misclassification.%2520Experiments%2520on%250Athree%2520widely%2520used%2520FGVC%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520the%2520CHBC%250Aframework.%2520Ablation%2520studies%2520further%2520investigate%2520the%2520application%2520strategies%2520of%250Afeature%2520enhancement%2520and%2520consistency%2520constraints%252C%2520underscoring%2520the%2520significant%250Acontributions%2520of%2520the%2520proposed%2520modules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Hierarchical%20Bidirectional%20Consistency%20Learning%20for%20Fine-Grained%0A%20%20Visual%20Classification&entry.906535625=Pengxiang%20Gao%20and%20Yihao%20Liang%20and%20Yanzhi%20Song%20and%20Zhouwang%20Yang&entry.1292438233=%20%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20aims%20to%20categorize%20closely%20related%0Asubclasses%2C%20a%20task%20complicated%20by%20minimal%20inter-class%20differences%20and%0Asignificant%20intra-class%20variance.%20Existing%20methods%20often%20rely%20on%20additional%0Aannotations%20for%20image%20classification%2C%20overlooking%20the%20valuable%20information%0Aembedded%20in%20Tree%20Hierarchies%20that%20depict%20hierarchical%20label%20relationships.%20To%0Aleverage%20this%20knowledge%20to%20improve%20classification%20accuracy%20and%20consistency%2C%20we%0Apropose%20a%20novel%20Cross-Hierarchical%20Bidirectional%20Consistency%20Learning%20%28CHBC%29%0Aframework.%20The%20CHBC%20framework%20extracts%20discriminative%20features%20across%20various%0Ahierarchies%20using%20a%20specially%20designed%20module%20to%20decompose%20and%20enhance%0Aattention%20masks%20and%20features.%20We%20employ%20bidirectional%20consistency%20loss%20to%0Aregulate%20the%20classification%20outcomes%20across%20different%20hierarchies%2C%20ensuring%0Alabel%20prediction%20consistency%20and%20reducing%20misclassification.%20Experiments%20on%0Athree%20widely%20used%20FGVC%20datasets%20validate%20the%20effectiveness%20of%20the%20CHBC%0Aframework.%20Ablation%20studies%20further%20investigate%20the%20application%20strategies%20of%0Afeature%20enhancement%20and%20consistency%20constraints%2C%20underscoring%20the%20significant%0Acontributions%20of%20the%20proposed%20modules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13608v1&entry.124074799=Read"},
{"title": "Parameter-Efficient Continual Fine-Tuning: A Survey", "author": "Eric Nuertey Coleman and Luigi Quarantiello and Ziyue Liu and Qinwen Yang and Samrat Mukherjee and Julio Hurtado and Vincenzo Lomonaco", "abstract": "  The emergence of large pre-trained networks has revolutionized the AI field,\nunlocking new possibilities and achieving unprecedented performance. However,\nthese models inherit a fundamental limitation from traditional Machine Learning\napproaches: their strong dependence on the \\textit{i.i.d.} assumption hinders\ntheir adaptability to dynamic learning scenarios. We believe the next\nbreakthrough in AI lies in enabling efficient adaptation to evolving\nenvironments -- such as the real world -- where new data and tasks arrive\nsequentially. This challenge defines the field of Continual Learning (CL), a\nMachine Learning paradigm focused on developing lifelong learning neural\nmodels. One alternative to efficiently adapt these large-scale models is known\nParameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of\nadapting the model to a particular data or scenario by performing small and\nefficient modifications, achieving similar performance to full fine-tuning.\nHowever, these techniques still lack the ability to adjust the model to\nmultiple tasks continually, as they suffer from the issue of Catastrophic\nForgetting. In this survey, we first provide an overview of CL algorithms and\nPEFT methods before reviewing the state-of-the-art on Parameter-Efficient\nContinual Fine-Tuning (PECFT). We examine various approaches, discuss\nevaluation metrics, and explore potential future research directions. Our goal\nis to highlight the synergy between CL and Parameter-Efficient Fine-Tuning,\nguide researchers in this field, and pave the way for novel future research\ndirections.\n", "link": "http://arxiv.org/abs/2504.13822v1", "date": "2025-04-18", "relevancy": 2.4984, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5022}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5016}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Efficient%20Continual%20Fine-Tuning%3A%20A%20Survey&body=Title%3A%20Parameter-Efficient%20Continual%20Fine-Tuning%3A%20A%20Survey%0AAuthor%3A%20Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Ziyue%20Liu%20and%20Qinwen%20Yang%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20pre-trained%20networks%20has%20revolutionized%20the%20AI%20field%2C%0Aunlocking%20new%20possibilities%20and%20achieving%20unprecedented%20performance.%20However%2C%0Athese%20models%20inherit%20a%20fundamental%20limitation%20from%20traditional%20Machine%20Learning%0Aapproaches%3A%20their%20strong%20dependence%20on%20the%20%5Ctextit%7Bi.i.d.%7D%20assumption%20hinders%0Atheir%20adaptability%20to%20dynamic%20learning%20scenarios.%20We%20believe%20the%20next%0Abreakthrough%20in%20AI%20lies%20in%20enabling%20efficient%20adaptation%20to%20evolving%0Aenvironments%20--%20such%20as%20the%20real%20world%20--%20where%20new%20data%20and%20tasks%20arrive%0Asequentially.%20This%20challenge%20defines%20the%20field%20of%20Continual%20Learning%20%28CL%29%2C%20a%0AMachine%20Learning%20paradigm%20focused%20on%20developing%20lifelong%20learning%20neural%0Amodels.%20One%20alternative%20to%20efficiently%20adapt%20these%20large-scale%20models%20is%20known%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29.%20These%20methods%20tackle%20the%20issue%20of%0Aadapting%20the%20model%20to%20a%20particular%20data%20or%20scenario%20by%20performing%20small%20and%0Aefficient%20modifications%2C%20achieving%20similar%20performance%20to%20full%20fine-tuning.%0AHowever%2C%20these%20techniques%20still%20lack%20the%20ability%20to%20adjust%20the%20model%20to%0Amultiple%20tasks%20continually%2C%20as%20they%20suffer%20from%20the%20issue%20of%20Catastrophic%0AForgetting.%20In%20this%20survey%2C%20we%20first%20provide%20an%20overview%20of%20CL%20algorithms%20and%0APEFT%20methods%20before%20reviewing%20the%20state-of-the-art%20on%20Parameter-Efficient%0AContinual%20Fine-Tuning%20%28PECFT%29.%20We%20examine%20various%20approaches%2C%20discuss%0Aevaluation%20metrics%2C%20and%20explore%20potential%20future%20research%20directions.%20Our%20goal%0Ais%20to%20highlight%20the%20synergy%20between%20CL%20and%20Parameter-Efficient%20Fine-Tuning%2C%0Aguide%20researchers%20in%20this%20field%2C%20and%20pave%20the%20way%20for%20novel%20future%20research%0Adirections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Efficient%2520Continual%2520Fine-Tuning%253A%2520A%2520Survey%26entry.906535625%3DEric%2520Nuertey%2520Coleman%2520and%2520Luigi%2520Quarantiello%2520and%2520Ziyue%2520Liu%2520and%2520Qinwen%2520Yang%2520and%2520Samrat%2520Mukherjee%2520and%2520Julio%2520Hurtado%2520and%2520Vincenzo%2520Lomonaco%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520pre-trained%2520networks%2520has%2520revolutionized%2520the%2520AI%2520field%252C%250Aunlocking%2520new%2520possibilities%2520and%2520achieving%2520unprecedented%2520performance.%2520However%252C%250Athese%2520models%2520inherit%2520a%2520fundamental%2520limitation%2520from%2520traditional%2520Machine%2520Learning%250Aapproaches%253A%2520their%2520strong%2520dependence%2520on%2520the%2520%255Ctextit%257Bi.i.d.%257D%2520assumption%2520hinders%250Atheir%2520adaptability%2520to%2520dynamic%2520learning%2520scenarios.%2520We%2520believe%2520the%2520next%250Abreakthrough%2520in%2520AI%2520lies%2520in%2520enabling%2520efficient%2520adaptation%2520to%2520evolving%250Aenvironments%2520--%2520such%2520as%2520the%2520real%2520world%2520--%2520where%2520new%2520data%2520and%2520tasks%2520arrive%250Asequentially.%2520This%2520challenge%2520defines%2520the%2520field%2520of%2520Continual%2520Learning%2520%2528CL%2529%252C%2520a%250AMachine%2520Learning%2520paradigm%2520focused%2520on%2520developing%2520lifelong%2520learning%2520neural%250Amodels.%2520One%2520alternative%2520to%2520efficiently%2520adapt%2520these%2520large-scale%2520models%2520is%2520known%250AParameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529.%2520These%2520methods%2520tackle%2520the%2520issue%2520of%250Aadapting%2520the%2520model%2520to%2520a%2520particular%2520data%2520or%2520scenario%2520by%2520performing%2520small%2520and%250Aefficient%2520modifications%252C%2520achieving%2520similar%2520performance%2520to%2520full%2520fine-tuning.%250AHowever%252C%2520these%2520techniques%2520still%2520lack%2520the%2520ability%2520to%2520adjust%2520the%2520model%2520to%250Amultiple%2520tasks%2520continually%252C%2520as%2520they%2520suffer%2520from%2520the%2520issue%2520of%2520Catastrophic%250AForgetting.%2520In%2520this%2520survey%252C%2520we%2520first%2520provide%2520an%2520overview%2520of%2520CL%2520algorithms%2520and%250APEFT%2520methods%2520before%2520reviewing%2520the%2520state-of-the-art%2520on%2520Parameter-Efficient%250AContinual%2520Fine-Tuning%2520%2528PECFT%2529.%2520We%2520examine%2520various%2520approaches%252C%2520discuss%250Aevaluation%2520metrics%252C%2520and%2520explore%2520potential%2520future%2520research%2520directions.%2520Our%2520goal%250Ais%2520to%2520highlight%2520the%2520synergy%2520between%2520CL%2520and%2520Parameter-Efficient%2520Fine-Tuning%252C%250Aguide%2520researchers%2520in%2520this%2520field%252C%2520and%2520pave%2520the%2520way%2520for%2520novel%2520future%2520research%250Adirections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Efficient%20Continual%20Fine-Tuning%3A%20A%20Survey&entry.906535625=Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Ziyue%20Liu%20and%20Qinwen%20Yang%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco&entry.1292438233=%20%20The%20emergence%20of%20large%20pre-trained%20networks%20has%20revolutionized%20the%20AI%20field%2C%0Aunlocking%20new%20possibilities%20and%20achieving%20unprecedented%20performance.%20However%2C%0Athese%20models%20inherit%20a%20fundamental%20limitation%20from%20traditional%20Machine%20Learning%0Aapproaches%3A%20their%20strong%20dependence%20on%20the%20%5Ctextit%7Bi.i.d.%7D%20assumption%20hinders%0Atheir%20adaptability%20to%20dynamic%20learning%20scenarios.%20We%20believe%20the%20next%0Abreakthrough%20in%20AI%20lies%20in%20enabling%20efficient%20adaptation%20to%20evolving%0Aenvironments%20--%20such%20as%20the%20real%20world%20--%20where%20new%20data%20and%20tasks%20arrive%0Asequentially.%20This%20challenge%20defines%20the%20field%20of%20Continual%20Learning%20%28CL%29%2C%20a%0AMachine%20Learning%20paradigm%20focused%20on%20developing%20lifelong%20learning%20neural%0Amodels.%20One%20alternative%20to%20efficiently%20adapt%20these%20large-scale%20models%20is%20known%0AParameter-Efficient%20Fine-Tuning%20%28PEFT%29.%20These%20methods%20tackle%20the%20issue%20of%0Aadapting%20the%20model%20to%20a%20particular%20data%20or%20scenario%20by%20performing%20small%20and%0Aefficient%20modifications%2C%20achieving%20similar%20performance%20to%20full%20fine-tuning.%0AHowever%2C%20these%20techniques%20still%20lack%20the%20ability%20to%20adjust%20the%20model%20to%0Amultiple%20tasks%20continually%2C%20as%20they%20suffer%20from%20the%20issue%20of%20Catastrophic%0AForgetting.%20In%20this%20survey%2C%20we%20first%20provide%20an%20overview%20of%20CL%20algorithms%20and%0APEFT%20methods%20before%20reviewing%20the%20state-of-the-art%20on%20Parameter-Efficient%0AContinual%20Fine-Tuning%20%28PECFT%29.%20We%20examine%20various%20approaches%2C%20discuss%0Aevaluation%20metrics%2C%20and%20explore%20potential%20future%20research%20directions.%20Our%20goal%0Ais%20to%20highlight%20the%20synergy%20between%20CL%20and%20Parameter-Efficient%20Fine-Tuning%2C%0Aguide%20researchers%20in%20this%20field%2C%20and%20pave%20the%20way%20for%20novel%20future%20research%0Adirections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13822v1&entry.124074799=Read"},
{"title": "Adaptive Long-term Embedding with Denoising and Augmentation for\n  Recommendation", "author": "Zahra Akhlaghi and Mostafa Haghir Chehreghani", "abstract": "  The rapid growth of the internet has made personalized recommendation systems\nindispensable. Graph-based sequential recommendation systems, powered by Graph\nNeural Networks (GNNs), effectively capture complex user-item interactions but\noften face challenges such as noise and static representations. In this paper,\nwe introduce the Adaptive Long-term Embedding with Denoising and Augmentation\nfor Recommendation (ALDA4Rec) method, a novel model that constructs an\nitem-item graph, filters noise through community detection, and enriches\nuser-item interactions. Graph Convolutional Networks (GCNs) are then employed\nto learn short-term representations, while averaging, GRUs, and attention\nmechanisms are utilized to model long-term embeddings. An MLP-based adaptive\nweighting strategy is further incorporated to dynamically optimize long-term\nuser preferences. Experiments conducted on four real-world datasets demonstrate\nthat ALDA4Rec outperforms state-of-the-art baselines, delivering notable\nimprovements in both accuracy and robustness. The source code is available at\nhttps://github.com/zahraakhlaghi/ALDA4Rec.\n", "link": "http://arxiv.org/abs/2504.13614v1", "date": "2025-04-18", "relevancy": 2.4974, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5226}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4924}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Long-term%20Embedding%20with%20Denoising%20and%20Augmentation%20for%0A%20%20Recommendation&body=Title%3A%20Adaptive%20Long-term%20Embedding%20with%20Denoising%20and%20Augmentation%20for%0A%20%20Recommendation%0AAuthor%3A%20Zahra%20Akhlaghi%20and%20Mostafa%20Haghir%20Chehreghani%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20the%20internet%20has%20made%20personalized%20recommendation%20systems%0Aindispensable.%20Graph-based%20sequential%20recommendation%20systems%2C%20powered%20by%20Graph%0ANeural%20Networks%20%28GNNs%29%2C%20effectively%20capture%20complex%20user-item%20interactions%20but%0Aoften%20face%20challenges%20such%20as%20noise%20and%20static%20representations.%20In%20this%20paper%2C%0Awe%20introduce%20the%20Adaptive%20Long-term%20Embedding%20with%20Denoising%20and%20Augmentation%0Afor%20Recommendation%20%28ALDA4Rec%29%20method%2C%20a%20novel%20model%20that%20constructs%20an%0Aitem-item%20graph%2C%20filters%20noise%20through%20community%20detection%2C%20and%20enriches%0Auser-item%20interactions.%20Graph%20Convolutional%20Networks%20%28GCNs%29%20are%20then%20employed%0Ato%20learn%20short-term%20representations%2C%20while%20averaging%2C%20GRUs%2C%20and%20attention%0Amechanisms%20are%20utilized%20to%20model%20long-term%20embeddings.%20An%20MLP-based%20adaptive%0Aweighting%20strategy%20is%20further%20incorporated%20to%20dynamically%20optimize%20long-term%0Auser%20preferences.%20Experiments%20conducted%20on%20four%20real-world%20datasets%20demonstrate%0Athat%20ALDA4Rec%20outperforms%20state-of-the-art%20baselines%2C%20delivering%20notable%0Aimprovements%20in%20both%20accuracy%20and%20robustness.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zahraakhlaghi/ALDA4Rec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Long-term%2520Embedding%2520with%2520Denoising%2520and%2520Augmentation%2520for%250A%2520%2520Recommendation%26entry.906535625%3DZahra%2520Akhlaghi%2520and%2520Mostafa%2520Haghir%2520Chehreghani%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520the%2520internet%2520has%2520made%2520personalized%2520recommendation%2520systems%250Aindispensable.%2520Graph-based%2520sequential%2520recommendation%2520systems%252C%2520powered%2520by%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%252C%2520effectively%2520capture%2520complex%2520user-item%2520interactions%2520but%250Aoften%2520face%2520challenges%2520such%2520as%2520noise%2520and%2520static%2520representations.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520the%2520Adaptive%2520Long-term%2520Embedding%2520with%2520Denoising%2520and%2520Augmentation%250Afor%2520Recommendation%2520%2528ALDA4Rec%2529%2520method%252C%2520a%2520novel%2520model%2520that%2520constructs%2520an%250Aitem-item%2520graph%252C%2520filters%2520noise%2520through%2520community%2520detection%252C%2520and%2520enriches%250Auser-item%2520interactions.%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520are%2520then%2520employed%250Ato%2520learn%2520short-term%2520representations%252C%2520while%2520averaging%252C%2520GRUs%252C%2520and%2520attention%250Amechanisms%2520are%2520utilized%2520to%2520model%2520long-term%2520embeddings.%2520An%2520MLP-based%2520adaptive%250Aweighting%2520strategy%2520is%2520further%2520incorporated%2520to%2520dynamically%2520optimize%2520long-term%250Auser%2520preferences.%2520Experiments%2520conducted%2520on%2520four%2520real-world%2520datasets%2520demonstrate%250Athat%2520ALDA4Rec%2520outperforms%2520state-of-the-art%2520baselines%252C%2520delivering%2520notable%250Aimprovements%2520in%2520both%2520accuracy%2520and%2520robustness.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zahraakhlaghi/ALDA4Rec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Long-term%20Embedding%20with%20Denoising%20and%20Augmentation%20for%0A%20%20Recommendation&entry.906535625=Zahra%20Akhlaghi%20and%20Mostafa%20Haghir%20Chehreghani&entry.1292438233=%20%20The%20rapid%20growth%20of%20the%20internet%20has%20made%20personalized%20recommendation%20systems%0Aindispensable.%20Graph-based%20sequential%20recommendation%20systems%2C%20powered%20by%20Graph%0ANeural%20Networks%20%28GNNs%29%2C%20effectively%20capture%20complex%20user-item%20interactions%20but%0Aoften%20face%20challenges%20such%20as%20noise%20and%20static%20representations.%20In%20this%20paper%2C%0Awe%20introduce%20the%20Adaptive%20Long-term%20Embedding%20with%20Denoising%20and%20Augmentation%0Afor%20Recommendation%20%28ALDA4Rec%29%20method%2C%20a%20novel%20model%20that%20constructs%20an%0Aitem-item%20graph%2C%20filters%20noise%20through%20community%20detection%2C%20and%20enriches%0Auser-item%20interactions.%20Graph%20Convolutional%20Networks%20%28GCNs%29%20are%20then%20employed%0Ato%20learn%20short-term%20representations%2C%20while%20averaging%2C%20GRUs%2C%20and%20attention%0Amechanisms%20are%20utilized%20to%20model%20long-term%20embeddings.%20An%20MLP-based%20adaptive%0Aweighting%20strategy%20is%20further%20incorporated%20to%20dynamically%20optimize%20long-term%0Auser%20preferences.%20Experiments%20conducted%20on%20four%20real-world%20datasets%20demonstrate%0Athat%20ALDA4Rec%20outperforms%20state-of-the-art%20baselines%2C%20delivering%20notable%0Aimprovements%20in%20both%20accuracy%20and%20robustness.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zahraakhlaghi/ALDA4Rec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13614v1&entry.124074799=Read"},
{"title": "LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory\n  Prior from Historical Traversals", "author": "Shanshuai Yuan and Julong Wei and Muer Tie and Xiangyun Ren and Zhongxue Gan and Wenchao Ding", "abstract": "  Vision-based 3D semantic occupancy prediction is critical for autonomous\ndriving, enabling unified modeling of static infrastructure and dynamic agents.\nIn practice, autonomous vehicles may repeatedly traverse identical geographic\nlocations under varying environmental conditions, such as weather fluctuations\nand illumination changes. Existing methods in 3D occupancy prediction\npredominantly integrate adjacent temporal contexts. However, these works\nneglect to leverage perceptual information, which is acquired from historical\ntraversals of identical geographic locations. In this paper, we propose\nLongterm Memory Prior Occupancy (LMPOcc), the first 3D occupancy prediction\nmethodology that exploits long-term memory priors derived from historical\ntraversal perceptual outputs. We introduce a plug-and-play architecture that\nintegrates long-term memory priors to enhance local perception while\nsimultaneously constructing global occupancy representations. To adaptively\naggregate prior features and current features, we develop an efficient\nlightweight Current-Prior Fusion module. Moreover, we propose a model-agnostic\nprior format to ensure compatibility across diverse occupancy prediction\nbaselines. LMPOcc achieves state-of-the-art performance validated on the\nOcc3D-nuScenes benchmark, especially on static semantic categories.\nAdditionally, experimental results demonstrate LMPOcc's ability to construct\nglobal occupancy through multi-vehicle crowdsourcing.\n", "link": "http://arxiv.org/abs/2504.13596v1", "date": "2025-04-18", "relevancy": 2.466, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.641}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMPOcc%3A%203D%20Semantic%20Occupancy%20Prediction%20Utilizing%20Long-Term%20Memory%0A%20%20Prior%20from%20Historical%20Traversals&body=Title%3A%20LMPOcc%3A%203D%20Semantic%20Occupancy%20Prediction%20Utilizing%20Long-Term%20Memory%0A%20%20Prior%20from%20Historical%20Traversals%0AAuthor%3A%20Shanshuai%20Yuan%20and%20Julong%20Wei%20and%20Muer%20Tie%20and%20Xiangyun%20Ren%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding%0AAbstract%3A%20%20%20Vision-based%203D%20semantic%20occupancy%20prediction%20is%20critical%20for%20autonomous%0Adriving%2C%20enabling%20unified%20modeling%20of%20static%20infrastructure%20and%20dynamic%20agents.%0AIn%20practice%2C%20autonomous%20vehicles%20may%20repeatedly%20traverse%20identical%20geographic%0Alocations%20under%20varying%20environmental%20conditions%2C%20such%20as%20weather%20fluctuations%0Aand%20illumination%20changes.%20Existing%20methods%20in%203D%20occupancy%20prediction%0Apredominantly%20integrate%20adjacent%20temporal%20contexts.%20However%2C%20these%20works%0Aneglect%20to%20leverage%20perceptual%20information%2C%20which%20is%20acquired%20from%20historical%0Atraversals%20of%20identical%20geographic%20locations.%20In%20this%20paper%2C%20we%20propose%0ALongterm%20Memory%20Prior%20Occupancy%20%28LMPOcc%29%2C%20the%20first%203D%20occupancy%20prediction%0Amethodology%20that%20exploits%20long-term%20memory%20priors%20derived%20from%20historical%0Atraversal%20perceptual%20outputs.%20We%20introduce%20a%20plug-and-play%20architecture%20that%0Aintegrates%20long-term%20memory%20priors%20to%20enhance%20local%20perception%20while%0Asimultaneously%20constructing%20global%20occupancy%20representations.%20To%20adaptively%0Aaggregate%20prior%20features%20and%20current%20features%2C%20we%20develop%20an%20efficient%0Alightweight%20Current-Prior%20Fusion%20module.%20Moreover%2C%20we%20propose%20a%20model-agnostic%0Aprior%20format%20to%20ensure%20compatibility%20across%20diverse%20occupancy%20prediction%0Abaselines.%20LMPOcc%20achieves%20state-of-the-art%20performance%20validated%20on%20the%0AOcc3D-nuScenes%20benchmark%2C%20especially%20on%20static%20semantic%20categories.%0AAdditionally%2C%20experimental%20results%20demonstrate%20LMPOcc%27s%20ability%20to%20construct%0Aglobal%20occupancy%20through%20multi-vehicle%20crowdsourcing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMPOcc%253A%25203D%2520Semantic%2520Occupancy%2520Prediction%2520Utilizing%2520Long-Term%2520Memory%250A%2520%2520Prior%2520from%2520Historical%2520Traversals%26entry.906535625%3DShanshuai%2520Yuan%2520and%2520Julong%2520Wei%2520and%2520Muer%2520Tie%2520and%2520Xiangyun%2520Ren%2520and%2520Zhongxue%2520Gan%2520and%2520Wenchao%2520Ding%26entry.1292438233%3D%2520%2520Vision-based%25203D%2520semantic%2520occupancy%2520prediction%2520is%2520critical%2520for%2520autonomous%250Adriving%252C%2520enabling%2520unified%2520modeling%2520of%2520static%2520infrastructure%2520and%2520dynamic%2520agents.%250AIn%2520practice%252C%2520autonomous%2520vehicles%2520may%2520repeatedly%2520traverse%2520identical%2520geographic%250Alocations%2520under%2520varying%2520environmental%2520conditions%252C%2520such%2520as%2520weather%2520fluctuations%250Aand%2520illumination%2520changes.%2520Existing%2520methods%2520in%25203D%2520occupancy%2520prediction%250Apredominantly%2520integrate%2520adjacent%2520temporal%2520contexts.%2520However%252C%2520these%2520works%250Aneglect%2520to%2520leverage%2520perceptual%2520information%252C%2520which%2520is%2520acquired%2520from%2520historical%250Atraversals%2520of%2520identical%2520geographic%2520locations.%2520In%2520this%2520paper%252C%2520we%2520propose%250ALongterm%2520Memory%2520Prior%2520Occupancy%2520%2528LMPOcc%2529%252C%2520the%2520first%25203D%2520occupancy%2520prediction%250Amethodology%2520that%2520exploits%2520long-term%2520memory%2520priors%2520derived%2520from%2520historical%250Atraversal%2520perceptual%2520outputs.%2520We%2520introduce%2520a%2520plug-and-play%2520architecture%2520that%250Aintegrates%2520long-term%2520memory%2520priors%2520to%2520enhance%2520local%2520perception%2520while%250Asimultaneously%2520constructing%2520global%2520occupancy%2520representations.%2520To%2520adaptively%250Aaggregate%2520prior%2520features%2520and%2520current%2520features%252C%2520we%2520develop%2520an%2520efficient%250Alightweight%2520Current-Prior%2520Fusion%2520module.%2520Moreover%252C%2520we%2520propose%2520a%2520model-agnostic%250Aprior%2520format%2520to%2520ensure%2520compatibility%2520across%2520diverse%2520occupancy%2520prediction%250Abaselines.%2520LMPOcc%2520achieves%2520state-of-the-art%2520performance%2520validated%2520on%2520the%250AOcc3D-nuScenes%2520benchmark%252C%2520especially%2520on%2520static%2520semantic%2520categories.%250AAdditionally%252C%2520experimental%2520results%2520demonstrate%2520LMPOcc%2527s%2520ability%2520to%2520construct%250Aglobal%2520occupancy%2520through%2520multi-vehicle%2520crowdsourcing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMPOcc%3A%203D%20Semantic%20Occupancy%20Prediction%20Utilizing%20Long-Term%20Memory%0A%20%20Prior%20from%20Historical%20Traversals&entry.906535625=Shanshuai%20Yuan%20and%20Julong%20Wei%20and%20Muer%20Tie%20and%20Xiangyun%20Ren%20and%20Zhongxue%20Gan%20and%20Wenchao%20Ding&entry.1292438233=%20%20Vision-based%203D%20semantic%20occupancy%20prediction%20is%20critical%20for%20autonomous%0Adriving%2C%20enabling%20unified%20modeling%20of%20static%20infrastructure%20and%20dynamic%20agents.%0AIn%20practice%2C%20autonomous%20vehicles%20may%20repeatedly%20traverse%20identical%20geographic%0Alocations%20under%20varying%20environmental%20conditions%2C%20such%20as%20weather%20fluctuations%0Aand%20illumination%20changes.%20Existing%20methods%20in%203D%20occupancy%20prediction%0Apredominantly%20integrate%20adjacent%20temporal%20contexts.%20However%2C%20these%20works%0Aneglect%20to%20leverage%20perceptual%20information%2C%20which%20is%20acquired%20from%20historical%0Atraversals%20of%20identical%20geographic%20locations.%20In%20this%20paper%2C%20we%20propose%0ALongterm%20Memory%20Prior%20Occupancy%20%28LMPOcc%29%2C%20the%20first%203D%20occupancy%20prediction%0Amethodology%20that%20exploits%20long-term%20memory%20priors%20derived%20from%20historical%0Atraversal%20perceptual%20outputs.%20We%20introduce%20a%20plug-and-play%20architecture%20that%0Aintegrates%20long-term%20memory%20priors%20to%20enhance%20local%20perception%20while%0Asimultaneously%20constructing%20global%20occupancy%20representations.%20To%20adaptively%0Aaggregate%20prior%20features%20and%20current%20features%2C%20we%20develop%20an%20efficient%0Alightweight%20Current-Prior%20Fusion%20module.%20Moreover%2C%20we%20propose%20a%20model-agnostic%0Aprior%20format%20to%20ensure%20compatibility%20across%20diverse%20occupancy%20prediction%0Abaselines.%20LMPOcc%20achieves%20state-of-the-art%20performance%20validated%20on%20the%0AOcc3D-nuScenes%20benchmark%2C%20especially%20on%20static%20semantic%20categories.%0AAdditionally%2C%20experimental%20results%20demonstrate%20LMPOcc%27s%20ability%20to%20construct%0Aglobal%20occupancy%20through%20multi-vehicle%20crowdsourcing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13596v1&entry.124074799=Read"},
{"title": "MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting\n  Mitigation in GFSCIL", "author": "Jinhui Pang and Changqing Lin and Hao Lin and Jinglin He and Zhengjun Li and Zhihui Zhang and Xiaoshuai Hao", "abstract": "  Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to\ncontinually learn from limited samples of novel tasks after initial training on\na large base dataset. Existing GFSCIL approaches typically utilize Prototypical\nNetworks (PNs) for metric-based class representations and fine-tune the model\nduring the incremental learning stage. However, these PN-based methods\noversimplify learning via novel query set fine-tuning and fail to integrate\nGraph Continual Learning (GCL) techniques due to architectural constraints. To\naddress these challenges, we propose a more rigorous and practical setting for\nGFSCIL that excludes query sets during the incremental training phase. Building\non this foundation, we introduce Model-Agnostic Meta Graph Continual Learning\n(MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL.\nSpecifically, by calculating the incremental second-order gradient during the\nmeta-training stage, we endow the model to learn high-quality priors that\nenhance incremental learning by aligning its behaviors across both the\nmeta-training and incremental learning stages. Extensive experiments on four\nmainstream graph datasets demonstrate that MEGA achieves state-of-the-art\nresults and enhances the effectiveness of various GCL methods in GFSCIL. We\nbelieve that our proposed MEGA serves as a model-agnostic GFSCIL paradigm,\npaving the way for future research.\n", "link": "http://arxiv.org/abs/2504.13691v1", "date": "2025-04-18", "relevancy": 2.4397, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4956}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4889}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL&body=Title%3A%20MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL%0AAuthor%3A%20Jinhui%20Pang%20and%20Changqing%20Lin%20and%20Hao%20Lin%20and%20Jinglin%20He%20and%20Zhengjun%20Li%20and%20Zhihui%20Zhang%20and%20Xiaoshuai%20Hao%0AAbstract%3A%20%20%20Graph%20Few-Shot%20Class-Incremental%20Learning%20%28GFSCIL%29%20enables%20models%20to%0Acontinually%20learn%20from%20limited%20samples%20of%20novel%20tasks%20after%20initial%20training%20on%0Aa%20large%20base%20dataset.%20Existing%20GFSCIL%20approaches%20typically%20utilize%20Prototypical%0ANetworks%20%28PNs%29%20for%20metric-based%20class%20representations%20and%20fine-tune%20the%20model%0Aduring%20the%20incremental%20learning%20stage.%20However%2C%20these%20PN-based%20methods%0Aoversimplify%20learning%20via%20novel%20query%20set%20fine-tuning%20and%20fail%20to%20integrate%0AGraph%20Continual%20Learning%20%28GCL%29%20techniques%20due%20to%20architectural%20constraints.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20more%20rigorous%20and%20practical%20setting%20for%0AGFSCIL%20that%20excludes%20query%20sets%20during%20the%20incremental%20training%20phase.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20Model-Agnostic%20Meta%20Graph%20Continual%20Learning%0A%28MEGA%29%2C%20aimed%20at%20effectively%20alleviating%20catastrophic%20forgetting%20for%20GFSCIL.%0ASpecifically%2C%20by%20calculating%20the%20incremental%20second-order%20gradient%20during%20the%0Ameta-training%20stage%2C%20we%20endow%20the%20model%20to%20learn%20high-quality%20priors%20that%0Aenhance%20incremental%20learning%20by%20aligning%20its%20behaviors%20across%20both%20the%0Ameta-training%20and%20incremental%20learning%20stages.%20Extensive%20experiments%20on%20four%0Amainstream%20graph%20datasets%20demonstrate%20that%20MEGA%20achieves%20state-of-the-art%0Aresults%20and%20enhances%20the%20effectiveness%20of%20various%20GCL%20methods%20in%20GFSCIL.%20We%0Abelieve%20that%20our%20proposed%20MEGA%20serves%20as%20a%20model-agnostic%20GFSCIL%20paradigm%2C%0Apaving%20the%20way%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA%253A%2520Second-Order%2520Gradient%2520Alignment%2520for%2520Catastrophic%2520Forgetting%250A%2520%2520Mitigation%2520in%2520GFSCIL%26entry.906535625%3DJinhui%2520Pang%2520and%2520Changqing%2520Lin%2520and%2520Hao%2520Lin%2520and%2520Jinglin%2520He%2520and%2520Zhengjun%2520Li%2520and%2520Zhihui%2520Zhang%2520and%2520Xiaoshuai%2520Hao%26entry.1292438233%3D%2520%2520Graph%2520Few-Shot%2520Class-Incremental%2520Learning%2520%2528GFSCIL%2529%2520enables%2520models%2520to%250Acontinually%2520learn%2520from%2520limited%2520samples%2520of%2520novel%2520tasks%2520after%2520initial%2520training%2520on%250Aa%2520large%2520base%2520dataset.%2520Existing%2520GFSCIL%2520approaches%2520typically%2520utilize%2520Prototypical%250ANetworks%2520%2528PNs%2529%2520for%2520metric-based%2520class%2520representations%2520and%2520fine-tune%2520the%2520model%250Aduring%2520the%2520incremental%2520learning%2520stage.%2520However%252C%2520these%2520PN-based%2520methods%250Aoversimplify%2520learning%2520via%2520novel%2520query%2520set%2520fine-tuning%2520and%2520fail%2520to%2520integrate%250AGraph%2520Continual%2520Learning%2520%2528GCL%2529%2520techniques%2520due%2520to%2520architectural%2520constraints.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520more%2520rigorous%2520and%2520practical%2520setting%2520for%250AGFSCIL%2520that%2520excludes%2520query%2520sets%2520during%2520the%2520incremental%2520training%2520phase.%2520Building%250Aon%2520this%2520foundation%252C%2520we%2520introduce%2520Model-Agnostic%2520Meta%2520Graph%2520Continual%2520Learning%250A%2528MEGA%2529%252C%2520aimed%2520at%2520effectively%2520alleviating%2520catastrophic%2520forgetting%2520for%2520GFSCIL.%250ASpecifically%252C%2520by%2520calculating%2520the%2520incremental%2520second-order%2520gradient%2520during%2520the%250Ameta-training%2520stage%252C%2520we%2520endow%2520the%2520model%2520to%2520learn%2520high-quality%2520priors%2520that%250Aenhance%2520incremental%2520learning%2520by%2520aligning%2520its%2520behaviors%2520across%2520both%2520the%250Ameta-training%2520and%2520incremental%2520learning%2520stages.%2520Extensive%2520experiments%2520on%2520four%250Amainstream%2520graph%2520datasets%2520demonstrate%2520that%2520MEGA%2520achieves%2520state-of-the-art%250Aresults%2520and%2520enhances%2520the%2520effectiveness%2520of%2520various%2520GCL%2520methods%2520in%2520GFSCIL.%2520We%250Abelieve%2520that%2520our%2520proposed%2520MEGA%2520serves%2520as%2520a%2520model-agnostic%2520GFSCIL%2520paradigm%252C%250Apaving%2520the%2520way%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA%3A%20Second-Order%20Gradient%20Alignment%20for%20Catastrophic%20Forgetting%0A%20%20Mitigation%20in%20GFSCIL&entry.906535625=Jinhui%20Pang%20and%20Changqing%20Lin%20and%20Hao%20Lin%20and%20Jinglin%20He%20and%20Zhengjun%20Li%20and%20Zhihui%20Zhang%20and%20Xiaoshuai%20Hao&entry.1292438233=%20%20Graph%20Few-Shot%20Class-Incremental%20Learning%20%28GFSCIL%29%20enables%20models%20to%0Acontinually%20learn%20from%20limited%20samples%20of%20novel%20tasks%20after%20initial%20training%20on%0Aa%20large%20base%20dataset.%20Existing%20GFSCIL%20approaches%20typically%20utilize%20Prototypical%0ANetworks%20%28PNs%29%20for%20metric-based%20class%20representations%20and%20fine-tune%20the%20model%0Aduring%20the%20incremental%20learning%20stage.%20However%2C%20these%20PN-based%20methods%0Aoversimplify%20learning%20via%20novel%20query%20set%20fine-tuning%20and%20fail%20to%20integrate%0AGraph%20Continual%20Learning%20%28GCL%29%20techniques%20due%20to%20architectural%20constraints.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20more%20rigorous%20and%20practical%20setting%20for%0AGFSCIL%20that%20excludes%20query%20sets%20during%20the%20incremental%20training%20phase.%20Building%0Aon%20this%20foundation%2C%20we%20introduce%20Model-Agnostic%20Meta%20Graph%20Continual%20Learning%0A%28MEGA%29%2C%20aimed%20at%20effectively%20alleviating%20catastrophic%20forgetting%20for%20GFSCIL.%0ASpecifically%2C%20by%20calculating%20the%20incremental%20second-order%20gradient%20during%20the%0Ameta-training%20stage%2C%20we%20endow%20the%20model%20to%20learn%20high-quality%20priors%20that%0Aenhance%20incremental%20learning%20by%20aligning%20its%20behaviors%20across%20both%20the%0Ameta-training%20and%20incremental%20learning%20stages.%20Extensive%20experiments%20on%20four%0Amainstream%20graph%20datasets%20demonstrate%20that%20MEGA%20achieves%20state-of-the-art%0Aresults%20and%20enhances%20the%20effectiveness%20of%20various%20GCL%20methods%20in%20GFSCIL.%20We%0Abelieve%20that%20our%20proposed%20MEGA%20serves%20as%20a%20model-agnostic%20GFSCIL%20paradigm%2C%0Apaving%20the%20way%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13691v1&entry.124074799=Read"},
{"title": "A Theory of LLM Sampling: Part Descriptive and Part Prescriptive", "author": "Sarath Sivaprasad and Pramod Kaushik and Sahar Abdelnabi and Mario Fritz", "abstract": "  Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under-explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns.\n", "link": "http://arxiv.org/abs/2402.11005v3", "date": "2025-04-18", "relevancy": 2.4094, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theory%20of%20LLM%20Sampling%3A%20Part%20Descriptive%20and%20Part%20Prescriptive&body=Title%3A%20A%20Theory%20of%20LLM%20Sampling%3A%20Part%20Descriptive%20and%20Part%20Prescriptive%0AAuthor%3A%20Sarath%20Sivaprasad%20and%20Pramod%20Kaushik%20and%20Sahar%20Abdelnabi%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20utilized%20in%20autonomous%0Adecision-making%2C%20where%20they%20sample%20options%20from%20vast%20action%20spaces.%20However%2C%0Athe%20heuristics%20that%20guide%20this%20sampling%20process%20remain%20under-explored.%20We%20study%0Athis%20sampling%20behavior%20and%20show%20that%20this%20underlying%20heuristics%20resembles%20that%0Aof%20human%20decision-making%3A%20comprising%20a%20descriptive%20component%20%28reflecting%0Astatistical%20norm%29%20and%20a%20prescriptive%20component%20%28implicit%20ideal%20encoded%20in%20the%0ALLM%29%20of%20a%20concept.%20We%20show%20that%20this%20deviation%20of%20a%20sample%20from%20the%20statistical%0Anorm%20towards%20a%20prescriptive%20component%20consistently%20appears%20in%20concepts%20across%0Adiverse%20real-world%20domains%20like%20public%20health%2C%20and%20economic%20trends.%20To%20further%0Aillustrate%20the%20theory%2C%20we%20demonstrate%20that%20concept%20prototypes%20in%20LLMs%20are%0Aaffected%20by%20prescriptive%20norms%2C%20similar%20to%20the%20concept%20of%20normality%20in%20humans.%0AThrough%20case%20studies%20and%20comparison%20with%20human%20studies%2C%20we%20illustrate%20that%20in%0Areal-world%20applications%2C%20the%20shift%20of%20samples%20toward%20an%20ideal%20value%20in%20LLMs%27%0Aoutputs%20can%20result%20in%20significantly%20biased%20decision-making%2C%20raising%20ethical%0Aconcerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11005v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theory%2520of%2520LLM%2520Sampling%253A%2520Part%2520Descriptive%2520and%2520Part%2520Prescriptive%26entry.906535625%3DSarath%2520Sivaprasad%2520and%2520Pramod%2520Kaushik%2520and%2520Sahar%2520Abdelnabi%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520utilized%2520in%2520autonomous%250Adecision-making%252C%2520where%2520they%2520sample%2520options%2520from%2520vast%2520action%2520spaces.%2520However%252C%250Athe%2520heuristics%2520that%2520guide%2520this%2520sampling%2520process%2520remain%2520under-explored.%2520We%2520study%250Athis%2520sampling%2520behavior%2520and%2520show%2520that%2520this%2520underlying%2520heuristics%2520resembles%2520that%250Aof%2520human%2520decision-making%253A%2520comprising%2520a%2520descriptive%2520component%2520%2528reflecting%250Astatistical%2520norm%2529%2520and%2520a%2520prescriptive%2520component%2520%2528implicit%2520ideal%2520encoded%2520in%2520the%250ALLM%2529%2520of%2520a%2520concept.%2520We%2520show%2520that%2520this%2520deviation%2520of%2520a%2520sample%2520from%2520the%2520statistical%250Anorm%2520towards%2520a%2520prescriptive%2520component%2520consistently%2520appears%2520in%2520concepts%2520across%250Adiverse%2520real-world%2520domains%2520like%2520public%2520health%252C%2520and%2520economic%2520trends.%2520To%2520further%250Aillustrate%2520the%2520theory%252C%2520we%2520demonstrate%2520that%2520concept%2520prototypes%2520in%2520LLMs%2520are%250Aaffected%2520by%2520prescriptive%2520norms%252C%2520similar%2520to%2520the%2520concept%2520of%2520normality%2520in%2520humans.%250AThrough%2520case%2520studies%2520and%2520comparison%2520with%2520human%2520studies%252C%2520we%2520illustrate%2520that%2520in%250Areal-world%2520applications%252C%2520the%2520shift%2520of%2520samples%2520toward%2520an%2520ideal%2520value%2520in%2520LLMs%2527%250Aoutputs%2520can%2520result%2520in%2520significantly%2520biased%2520decision-making%252C%2520raising%2520ethical%250Aconcerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11005v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theory%20of%20LLM%20Sampling%3A%20Part%20Descriptive%20and%20Part%20Prescriptive&entry.906535625=Sarath%20Sivaprasad%20and%20Pramod%20Kaushik%20and%20Sahar%20Abdelnabi%20and%20Mario%20Fritz&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20utilized%20in%20autonomous%0Adecision-making%2C%20where%20they%20sample%20options%20from%20vast%20action%20spaces.%20However%2C%0Athe%20heuristics%20that%20guide%20this%20sampling%20process%20remain%20under-explored.%20We%20study%0Athis%20sampling%20behavior%20and%20show%20that%20this%20underlying%20heuristics%20resembles%20that%0Aof%20human%20decision-making%3A%20comprising%20a%20descriptive%20component%20%28reflecting%0Astatistical%20norm%29%20and%20a%20prescriptive%20component%20%28implicit%20ideal%20encoded%20in%20the%0ALLM%29%20of%20a%20concept.%20We%20show%20that%20this%20deviation%20of%20a%20sample%20from%20the%20statistical%0Anorm%20towards%20a%20prescriptive%20component%20consistently%20appears%20in%20concepts%20across%0Adiverse%20real-world%20domains%20like%20public%20health%2C%20and%20economic%20trends.%20To%20further%0Aillustrate%20the%20theory%2C%20we%20demonstrate%20that%20concept%20prototypes%20in%20LLMs%20are%0Aaffected%20by%20prescriptive%20norms%2C%20similar%20to%20the%20concept%20of%20normality%20in%20humans.%0AThrough%20case%20studies%20and%20comparison%20with%20human%20studies%2C%20we%20illustrate%20that%20in%0Areal-world%20applications%2C%20the%20shift%20of%20samples%20toward%20an%20ideal%20value%20in%20LLMs%27%0Aoutputs%20can%20result%20in%20significantly%20biased%20decision-making%2C%20raising%20ethical%0Aconcerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11005v3&entry.124074799=Read"},
{"title": "Decoding Vision Transformers: the Diffusion Steering Lens", "author": "Ryota Takatsuki and Sonia Joseph and Ippei Fujisawa and Ryota Kanai", "abstract": "  Logit Lens is a widely adopted method for mechanistic interpretability of\ntransformer-based language models, enabling the analysis of how internal\nrepresentations evolve across layers by projecting them into the output\nvocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is\ntechnically straightforward, its direct use faces limitations in capturing the\nrichness of visual representations. Building on the work of Toker et al.\n(2024)~\\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize\nintermediate representations in the text encoders of text-to-image diffusion\nmodels, we demonstrate that while Diffusion Lens can effectively visualize\nresidual stream representations in image encoders, it fails to capture the\ndirect contributions of individual submodules. To overcome this limitation, we\npropose \\textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach\nthat steers submodule outputs and patches subsequent indirect contributions. We\nvalidate our method through interventional studies, showing that DSL provides\nan intuitive and reliable interpretation of the internal processing in ViTs.\n", "link": "http://arxiv.org/abs/2504.13763v1", "date": "2025-04-18", "relevancy": 2.3981, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.601}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Vision%20Transformers%3A%20the%20Diffusion%20Steering%20Lens&body=Title%3A%20Decoding%20Vision%20Transformers%3A%20the%20Diffusion%20Steering%20Lens%0AAuthor%3A%20Ryota%20Takatsuki%20and%20Sonia%20Joseph%20and%20Ippei%20Fujisawa%20and%20Ryota%20Kanai%0AAbstract%3A%20%20%20Logit%20Lens%20is%20a%20widely%20adopted%20method%20for%20mechanistic%20interpretability%20of%0Atransformer-based%20language%20models%2C%20enabling%20the%20analysis%20of%20how%20internal%0Arepresentations%20evolve%20across%20layers%20by%20projecting%20them%20into%20the%20output%0Avocabulary%20space.%20Although%20applying%20Logit%20Lens%20to%20Vision%20Transformers%20%28ViTs%29%20is%0Atechnically%20straightforward%2C%20its%20direct%20use%20faces%20limitations%20in%20capturing%20the%0Arichness%20of%20visual%20representations.%20Building%20on%20the%20work%20of%20Toker%20et%20al.%0A%282024%29~%5Ccite%7BToker2024-ve%7D%2C%20who%20introduced%20Diffusion%20Lens%20to%20visualize%0Aintermediate%20representations%20in%20the%20text%20encoders%20of%20text-to-image%20diffusion%0Amodels%2C%20we%20demonstrate%20that%20while%20Diffusion%20Lens%20can%20effectively%20visualize%0Aresidual%20stream%20representations%20in%20image%20encoders%2C%20it%20fails%20to%20capture%20the%0Adirect%20contributions%20of%20individual%20submodules.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20%5Ctextbf%7BDiffusion%20Steering%20Lens%7D%20%28DSL%29%2C%20a%20novel%2C%20training-free%20approach%0Athat%20steers%20submodule%20outputs%20and%20patches%20subsequent%20indirect%20contributions.%20We%0Avalidate%20our%20method%20through%20interventional%20studies%2C%20showing%20that%20DSL%20provides%0Aan%20intuitive%20and%20reliable%20interpretation%20of%20the%20internal%20processing%20in%20ViTs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Vision%2520Transformers%253A%2520the%2520Diffusion%2520Steering%2520Lens%26entry.906535625%3DRyota%2520Takatsuki%2520and%2520Sonia%2520Joseph%2520and%2520Ippei%2520Fujisawa%2520and%2520Ryota%2520Kanai%26entry.1292438233%3D%2520%2520Logit%2520Lens%2520is%2520a%2520widely%2520adopted%2520method%2520for%2520mechanistic%2520interpretability%2520of%250Atransformer-based%2520language%2520models%252C%2520enabling%2520the%2520analysis%2520of%2520how%2520internal%250Arepresentations%2520evolve%2520across%2520layers%2520by%2520projecting%2520them%2520into%2520the%2520output%250Avocabulary%2520space.%2520Although%2520applying%2520Logit%2520Lens%2520to%2520Vision%2520Transformers%2520%2528ViTs%2529%2520is%250Atechnically%2520straightforward%252C%2520its%2520direct%2520use%2520faces%2520limitations%2520in%2520capturing%2520the%250Arichness%2520of%2520visual%2520representations.%2520Building%2520on%2520the%2520work%2520of%2520Toker%2520et%2520al.%250A%25282024%2529~%255Ccite%257BToker2024-ve%257D%252C%2520who%2520introduced%2520Diffusion%2520Lens%2520to%2520visualize%250Aintermediate%2520representations%2520in%2520the%2520text%2520encoders%2520of%2520text-to-image%2520diffusion%250Amodels%252C%2520we%2520demonstrate%2520that%2520while%2520Diffusion%2520Lens%2520can%2520effectively%2520visualize%250Aresidual%2520stream%2520representations%2520in%2520image%2520encoders%252C%2520it%2520fails%2520to%2520capture%2520the%250Adirect%2520contributions%2520of%2520individual%2520submodules.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520%255Ctextbf%257BDiffusion%2520Steering%2520Lens%257D%2520%2528DSL%2529%252C%2520a%2520novel%252C%2520training-free%2520approach%250Athat%2520steers%2520submodule%2520outputs%2520and%2520patches%2520subsequent%2520indirect%2520contributions.%2520We%250Avalidate%2520our%2520method%2520through%2520interventional%2520studies%252C%2520showing%2520that%2520DSL%2520provides%250Aan%2520intuitive%2520and%2520reliable%2520interpretation%2520of%2520the%2520internal%2520processing%2520in%2520ViTs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Vision%20Transformers%3A%20the%20Diffusion%20Steering%20Lens&entry.906535625=Ryota%20Takatsuki%20and%20Sonia%20Joseph%20and%20Ippei%20Fujisawa%20and%20Ryota%20Kanai&entry.1292438233=%20%20Logit%20Lens%20is%20a%20widely%20adopted%20method%20for%20mechanistic%20interpretability%20of%0Atransformer-based%20language%20models%2C%20enabling%20the%20analysis%20of%20how%20internal%0Arepresentations%20evolve%20across%20layers%20by%20projecting%20them%20into%20the%20output%0Avocabulary%20space.%20Although%20applying%20Logit%20Lens%20to%20Vision%20Transformers%20%28ViTs%29%20is%0Atechnically%20straightforward%2C%20its%20direct%20use%20faces%20limitations%20in%20capturing%20the%0Arichness%20of%20visual%20representations.%20Building%20on%20the%20work%20of%20Toker%20et%20al.%0A%282024%29~%5Ccite%7BToker2024-ve%7D%2C%20who%20introduced%20Diffusion%20Lens%20to%20visualize%0Aintermediate%20representations%20in%20the%20text%20encoders%20of%20text-to-image%20diffusion%0Amodels%2C%20we%20demonstrate%20that%20while%20Diffusion%20Lens%20can%20effectively%20visualize%0Aresidual%20stream%20representations%20in%20image%20encoders%2C%20it%20fails%20to%20capture%20the%0Adirect%20contributions%20of%20individual%20submodules.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20%5Ctextbf%7BDiffusion%20Steering%20Lens%7D%20%28DSL%29%2C%20a%20novel%2C%20training-free%20approach%0Athat%20steers%20submodule%20outputs%20and%20patches%20subsequent%20indirect%20contributions.%20We%0Avalidate%20our%20method%20through%20interventional%20studies%2C%20showing%20that%20DSL%20provides%0Aan%20intuitive%20and%20reliable%20interpretation%20of%20the%20internal%20processing%20in%20ViTs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13763v1&entry.124074799=Read"},
{"title": "Doppler-SLAM: Doppler-Aided Radar-Inertial and LiDAR-Inertial\n  Simultaneous Localization and Mapping", "author": "Dong Wang and Hannes Haag and Daniel Casado Herraez and Stefan May and Cyrill Stachniss and Andreas Nuechter", "abstract": "  Simultaneous localization and mapping (SLAM) is a critical capability for\nautonomous systems. Traditional SLAM approaches, which often rely on visual or\nLiDAR sensors, face significant challenges in adverse conditions such as low\nlight or featureless environments. To overcome these limitations, we propose a\nnovel Doppler-aided radar-inertial and LiDAR-inertial SLAM framework that\nleverages the complementary strengths of 4D radar, FMCW LiDAR, and inertial\nmeasurement units. Our system integrates Doppler velocity measurements and\nspatial data into a tightly-coupled front-end and graph optimization back-end\nto provide enhanced ego velocity estimation, accurate odometry, and robust\nmapping. We also introduce a Doppler-based scan-matching technique to improve\nfront-end odometry in dynamic environments. In addition, our framework\nincorporates an innovative online extrinsic calibration mechanism, utilizing\nDoppler velocity and loop closure to dynamically maintain sensor alignment.\nExtensive evaluations on both public and proprietary datasets show that our\nsystem significantly outperforms state-of-the-art radar-SLAM and LiDAR-SLAM\nframeworks in terms of accuracy and robustness. To encourage further research,\nthe code of our Doppler-SLAM and our dataset are available at:\nhttps://github.com/Wayne-DWA/Doppler-SLAM.\n", "link": "http://arxiv.org/abs/2504.11634v2", "date": "2025-04-18", "relevancy": 2.3943, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.616}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doppler-SLAM%3A%20Doppler-Aided%20Radar-Inertial%20and%20LiDAR-Inertial%0A%20%20Simultaneous%20Localization%20and%20Mapping&body=Title%3A%20Doppler-SLAM%3A%20Doppler-Aided%20Radar-Inertial%20and%20LiDAR-Inertial%0A%20%20Simultaneous%20Localization%20and%20Mapping%0AAuthor%3A%20Dong%20Wang%20and%20Hannes%20Haag%20and%20Daniel%20Casado%20Herraez%20and%20Stefan%20May%20and%20Cyrill%20Stachniss%20and%20Andreas%20Nuechter%0AAbstract%3A%20%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20a%20critical%20capability%20for%0Aautonomous%20systems.%20Traditional%20SLAM%20approaches%2C%20which%20often%20rely%20on%20visual%20or%0ALiDAR%20sensors%2C%20face%20significant%20challenges%20in%20adverse%20conditions%20such%20as%20low%0Alight%20or%20featureless%20environments.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20Doppler-aided%20radar-inertial%20and%20LiDAR-inertial%20SLAM%20framework%20that%0Aleverages%20the%20complementary%20strengths%20of%204D%20radar%2C%20FMCW%20LiDAR%2C%20and%20inertial%0Ameasurement%20units.%20Our%20system%20integrates%20Doppler%20velocity%20measurements%20and%0Aspatial%20data%20into%20a%20tightly-coupled%20front-end%20and%20graph%20optimization%20back-end%0Ato%20provide%20enhanced%20ego%20velocity%20estimation%2C%20accurate%20odometry%2C%20and%20robust%0Amapping.%20We%20also%20introduce%20a%20Doppler-based%20scan-matching%20technique%20to%20improve%0Afront-end%20odometry%20in%20dynamic%20environments.%20In%20addition%2C%20our%20framework%0Aincorporates%20an%20innovative%20online%20extrinsic%20calibration%20mechanism%2C%20utilizing%0ADoppler%20velocity%20and%20loop%20closure%20to%20dynamically%20maintain%20sensor%20alignment.%0AExtensive%20evaluations%20on%20both%20public%20and%20proprietary%20datasets%20show%20that%20our%0Asystem%20significantly%20outperforms%20state-of-the-art%20radar-SLAM%20and%20LiDAR-SLAM%0Aframeworks%20in%20terms%20of%20accuracy%20and%20robustness.%20To%20encourage%20further%20research%2C%0Athe%20code%20of%20our%20Doppler-SLAM%20and%20our%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/Wayne-DWA/Doppler-SLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoppler-SLAM%253A%2520Doppler-Aided%2520Radar-Inertial%2520and%2520LiDAR-Inertial%250A%2520%2520Simultaneous%2520Localization%2520and%2520Mapping%26entry.906535625%3DDong%2520Wang%2520and%2520Hannes%2520Haag%2520and%2520Daniel%2520Casado%2520Herraez%2520and%2520Stefan%2520May%2520and%2520Cyrill%2520Stachniss%2520and%2520Andreas%2520Nuechter%26entry.1292438233%3D%2520%2520Simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520is%2520a%2520critical%2520capability%2520for%250Aautonomous%2520systems.%2520Traditional%2520SLAM%2520approaches%252C%2520which%2520often%2520rely%2520on%2520visual%2520or%250ALiDAR%2520sensors%252C%2520face%2520significant%2520challenges%2520in%2520adverse%2520conditions%2520such%2520as%2520low%250Alight%2520or%2520featureless%2520environments.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520Doppler-aided%2520radar-inertial%2520and%2520LiDAR-inertial%2520SLAM%2520framework%2520that%250Aleverages%2520the%2520complementary%2520strengths%2520of%25204D%2520radar%252C%2520FMCW%2520LiDAR%252C%2520and%2520inertial%250Ameasurement%2520units.%2520Our%2520system%2520integrates%2520Doppler%2520velocity%2520measurements%2520and%250Aspatial%2520data%2520into%2520a%2520tightly-coupled%2520front-end%2520and%2520graph%2520optimization%2520back-end%250Ato%2520provide%2520enhanced%2520ego%2520velocity%2520estimation%252C%2520accurate%2520odometry%252C%2520and%2520robust%250Amapping.%2520We%2520also%2520introduce%2520a%2520Doppler-based%2520scan-matching%2520technique%2520to%2520improve%250Afront-end%2520odometry%2520in%2520dynamic%2520environments.%2520In%2520addition%252C%2520our%2520framework%250Aincorporates%2520an%2520innovative%2520online%2520extrinsic%2520calibration%2520mechanism%252C%2520utilizing%250ADoppler%2520velocity%2520and%2520loop%2520closure%2520to%2520dynamically%2520maintain%2520sensor%2520alignment.%250AExtensive%2520evaluations%2520on%2520both%2520public%2520and%2520proprietary%2520datasets%2520show%2520that%2520our%250Asystem%2520significantly%2520outperforms%2520state-of-the-art%2520radar-SLAM%2520and%2520LiDAR-SLAM%250Aframeworks%2520in%2520terms%2520of%2520accuracy%2520and%2520robustness.%2520To%2520encourage%2520further%2520research%252C%250Athe%2520code%2520of%2520our%2520Doppler-SLAM%2520and%2520our%2520dataset%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Wayne-DWA/Doppler-SLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doppler-SLAM%3A%20Doppler-Aided%20Radar-Inertial%20and%20LiDAR-Inertial%0A%20%20Simultaneous%20Localization%20and%20Mapping&entry.906535625=Dong%20Wang%20and%20Hannes%20Haag%20and%20Daniel%20Casado%20Herraez%20and%20Stefan%20May%20and%20Cyrill%20Stachniss%20and%20Andreas%20Nuechter&entry.1292438233=%20%20Simultaneous%20localization%20and%20mapping%20%28SLAM%29%20is%20a%20critical%20capability%20for%0Aautonomous%20systems.%20Traditional%20SLAM%20approaches%2C%20which%20often%20rely%20on%20visual%20or%0ALiDAR%20sensors%2C%20face%20significant%20challenges%20in%20adverse%20conditions%20such%20as%20low%0Alight%20or%20featureless%20environments.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%0Anovel%20Doppler-aided%20radar-inertial%20and%20LiDAR-inertial%20SLAM%20framework%20that%0Aleverages%20the%20complementary%20strengths%20of%204D%20radar%2C%20FMCW%20LiDAR%2C%20and%20inertial%0Ameasurement%20units.%20Our%20system%20integrates%20Doppler%20velocity%20measurements%20and%0Aspatial%20data%20into%20a%20tightly-coupled%20front-end%20and%20graph%20optimization%20back-end%0Ato%20provide%20enhanced%20ego%20velocity%20estimation%2C%20accurate%20odometry%2C%20and%20robust%0Amapping.%20We%20also%20introduce%20a%20Doppler-based%20scan-matching%20technique%20to%20improve%0Afront-end%20odometry%20in%20dynamic%20environments.%20In%20addition%2C%20our%20framework%0Aincorporates%20an%20innovative%20online%20extrinsic%20calibration%20mechanism%2C%20utilizing%0ADoppler%20velocity%20and%20loop%20closure%20to%20dynamically%20maintain%20sensor%20alignment.%0AExtensive%20evaluations%20on%20both%20public%20and%20proprietary%20datasets%20show%20that%20our%0Asystem%20significantly%20outperforms%20state-of-the-art%20radar-SLAM%20and%20LiDAR-SLAM%0Aframeworks%20in%20terms%20of%20accuracy%20and%20robustness.%20To%20encourage%20further%20research%2C%0Athe%20code%20of%20our%20Doppler-SLAM%20and%20our%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/Wayne-DWA/Doppler-SLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11634v2&entry.124074799=Read"},
{"title": "Training-free Task-oriented Grasp Generation", "author": "Jiaming Wang and Jizhuo Chen and Diwen Liu and Linh K\u00e4stner", "abstract": "  This paper presents a training-free pipeline for task-oriented grasp\ngeneration that combines pre-trained grasp generation models with\nvision-language models (VLMs). Unlike traditional approaches that focus solely\non stable grasps, our method incorporates task-specific requirements by\nleveraging the semantic reasoning capabilities of VLMs. We evaluate five\nquerying strategies, each utilizing different visual representations of\ncandidate grasps, and demonstrate significant improvements over a baseline\nmethod in both grasp success and task compliance rates, with absolute gains of\nup to 36.9\\% in overall success rate. Our results underline the potential of\nVLMs to enhance task-oriented manipulation, providing insights for future\nresearch in robotic grasping and human-robot interaction.\n", "link": "http://arxiv.org/abs/2502.04873v2", "date": "2025-04-18", "relevancy": 2.3815, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6614}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Task-oriented%20Grasp%20Generation&body=Title%3A%20Training-free%20Task-oriented%20Grasp%20Generation%0AAuthor%3A%20Jiaming%20Wang%20and%20Jizhuo%20Chen%20and%20Diwen%20Liu%20and%20Linh%20K%C3%A4stner%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20training-free%20pipeline%20for%20task-oriented%20grasp%0Ageneration%20that%20combines%20pre-trained%20grasp%20generation%20models%20with%0Avision-language%20models%20%28VLMs%29.%20Unlike%20traditional%20approaches%20that%20focus%20solely%0Aon%20stable%20grasps%2C%20our%20method%20incorporates%20task-specific%20requirements%20by%0Aleveraging%20the%20semantic%20reasoning%20capabilities%20of%20VLMs.%20We%20evaluate%20five%0Aquerying%20strategies%2C%20each%20utilizing%20different%20visual%20representations%20of%0Acandidate%20grasps%2C%20and%20demonstrate%20significant%20improvements%20over%20a%20baseline%0Amethod%20in%20both%20grasp%20success%20and%20task%20compliance%20rates%2C%20with%20absolute%20gains%20of%0Aup%20to%2036.9%5C%25%20in%20overall%20success%20rate.%20Our%20results%20underline%20the%20potential%20of%0AVLMs%20to%20enhance%20task-oriented%20manipulation%2C%20providing%20insights%20for%20future%0Aresearch%20in%20robotic%20grasping%20and%20human-robot%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Task-oriented%2520Grasp%2520Generation%26entry.906535625%3DJiaming%2520Wang%2520and%2520Jizhuo%2520Chen%2520and%2520Diwen%2520Liu%2520and%2520Linh%2520K%25C3%25A4stner%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520training-free%2520pipeline%2520for%2520task-oriented%2520grasp%250Ageneration%2520that%2520combines%2520pre-trained%2520grasp%2520generation%2520models%2520with%250Avision-language%2520models%2520%2528VLMs%2529.%2520Unlike%2520traditional%2520approaches%2520that%2520focus%2520solely%250Aon%2520stable%2520grasps%252C%2520our%2520method%2520incorporates%2520task-specific%2520requirements%2520by%250Aleveraging%2520the%2520semantic%2520reasoning%2520capabilities%2520of%2520VLMs.%2520We%2520evaluate%2520five%250Aquerying%2520strategies%252C%2520each%2520utilizing%2520different%2520visual%2520representations%2520of%250Acandidate%2520grasps%252C%2520and%2520demonstrate%2520significant%2520improvements%2520over%2520a%2520baseline%250Amethod%2520in%2520both%2520grasp%2520success%2520and%2520task%2520compliance%2520rates%252C%2520with%2520absolute%2520gains%2520of%250Aup%2520to%252036.9%255C%2525%2520in%2520overall%2520success%2520rate.%2520Our%2520results%2520underline%2520the%2520potential%2520of%250AVLMs%2520to%2520enhance%2520task-oriented%2520manipulation%252C%2520providing%2520insights%2520for%2520future%250Aresearch%2520in%2520robotic%2520grasping%2520and%2520human-robot%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Task-oriented%20Grasp%20Generation&entry.906535625=Jiaming%20Wang%20and%20Jizhuo%20Chen%20and%20Diwen%20Liu%20and%20Linh%20K%C3%A4stner&entry.1292438233=%20%20This%20paper%20presents%20a%20training-free%20pipeline%20for%20task-oriented%20grasp%0Ageneration%20that%20combines%20pre-trained%20grasp%20generation%20models%20with%0Avision-language%20models%20%28VLMs%29.%20Unlike%20traditional%20approaches%20that%20focus%20solely%0Aon%20stable%20grasps%2C%20our%20method%20incorporates%20task-specific%20requirements%20by%0Aleveraging%20the%20semantic%20reasoning%20capabilities%20of%20VLMs.%20We%20evaluate%20five%0Aquerying%20strategies%2C%20each%20utilizing%20different%20visual%20representations%20of%0Acandidate%20grasps%2C%20and%20demonstrate%20significant%20improvements%20over%20a%20baseline%0Amethod%20in%20both%20grasp%20success%20and%20task%20compliance%20rates%2C%20with%20absolute%20gains%20of%0Aup%20to%2036.9%5C%25%20in%20overall%20success%20rate.%20Our%20results%20underline%20the%20potential%20of%0AVLMs%20to%20enhance%20task-oriented%20manipulation%2C%20providing%20insights%20for%20future%0Aresearch%20in%20robotic%20grasping%20and%20human-robot%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04873v2&entry.124074799=Read"},
{"title": "Can postgraduate translation students identify machine-generated text?", "author": "Michael Farrell", "abstract": "  Given the growing use of generative artificial intelligence as a tool for\ncreating multilingual content and bypassing both machine and traditional\ntranslation methods, this study explores the ability of linguistically trained\nindividuals to discern machine-generated output from human-written text (HT).\nAfter brief training sessions on the textual anomalies typically found in\nsynthetic text (ST), twenty-three postgraduate translation students analysed\nexcerpts of Italian prose and assigned likelihood scores to indicate whether\nthey believed they were human-written or AI-generated (ChatGPT-4o). The results\nshow that, on average, the students struggled to distinguish between HT and ST,\nwith only two participants achieving notable accuracy. Closer analysis revealed\nthat the students often identified the same textual anomalies in both HT and\nST, although features such as low burstiness and self-contradiction were more\nfrequently associated with ST. These findings suggest the need for improvements\nin the preparatory training. Moreover, the study raises questions about the\nnecessity of editing synthetic text to make it sound more human-like and\nrecommends further research to determine whether AI-generated text is already\nsufficiently natural-sounding not to require further refinement.\n", "link": "http://arxiv.org/abs/2504.09164v2", "date": "2025-04-18", "relevancy": 2.3783, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5329}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4551}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20postgraduate%20translation%20students%20identify%20machine-generated%20text%3F&body=Title%3A%20Can%20postgraduate%20translation%20students%20identify%20machine-generated%20text%3F%0AAuthor%3A%20Michael%20Farrell%0AAbstract%3A%20%20%20Given%20the%20growing%20use%20of%20generative%20artificial%20intelligence%20as%20a%20tool%20for%0Acreating%20multilingual%20content%20and%20bypassing%20both%20machine%20and%20traditional%0Atranslation%20methods%2C%20this%20study%20explores%20the%20ability%20of%20linguistically%20trained%0Aindividuals%20to%20discern%20machine-generated%20output%20from%20human-written%20text%20%28HT%29.%0AAfter%20brief%20training%20sessions%20on%20the%20textual%20anomalies%20typically%20found%20in%0Asynthetic%20text%20%28ST%29%2C%20twenty-three%20postgraduate%20translation%20students%20analysed%0Aexcerpts%20of%20Italian%20prose%20and%20assigned%20likelihood%20scores%20to%20indicate%20whether%0Athey%20believed%20they%20were%20human-written%20or%20AI-generated%20%28ChatGPT-4o%29.%20The%20results%0Ashow%20that%2C%20on%20average%2C%20the%20students%20struggled%20to%20distinguish%20between%20HT%20and%20ST%2C%0Awith%20only%20two%20participants%20achieving%20notable%20accuracy.%20Closer%20analysis%20revealed%0Athat%20the%20students%20often%20identified%20the%20same%20textual%20anomalies%20in%20both%20HT%20and%0AST%2C%20although%20features%20such%20as%20low%20burstiness%20and%20self-contradiction%20were%20more%0Afrequently%20associated%20with%20ST.%20These%20findings%20suggest%20the%20need%20for%20improvements%0Ain%20the%20preparatory%20training.%20Moreover%2C%20the%20study%20raises%20questions%20about%20the%0Anecessity%20of%20editing%20synthetic%20text%20to%20make%20it%20sound%20more%20human-like%20and%0Arecommends%20further%20research%20to%20determine%20whether%20AI-generated%20text%20is%20already%0Asufficiently%20natural-sounding%20not%20to%20require%20further%20refinement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09164v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520postgraduate%2520translation%2520students%2520identify%2520machine-generated%2520text%253F%26entry.906535625%3DMichael%2520Farrell%26entry.1292438233%3D%2520%2520Given%2520the%2520growing%2520use%2520of%2520generative%2520artificial%2520intelligence%2520as%2520a%2520tool%2520for%250Acreating%2520multilingual%2520content%2520and%2520bypassing%2520both%2520machine%2520and%2520traditional%250Atranslation%2520methods%252C%2520this%2520study%2520explores%2520the%2520ability%2520of%2520linguistically%2520trained%250Aindividuals%2520to%2520discern%2520machine-generated%2520output%2520from%2520human-written%2520text%2520%2528HT%2529.%250AAfter%2520brief%2520training%2520sessions%2520on%2520the%2520textual%2520anomalies%2520typically%2520found%2520in%250Asynthetic%2520text%2520%2528ST%2529%252C%2520twenty-three%2520postgraduate%2520translation%2520students%2520analysed%250Aexcerpts%2520of%2520Italian%2520prose%2520and%2520assigned%2520likelihood%2520scores%2520to%2520indicate%2520whether%250Athey%2520believed%2520they%2520were%2520human-written%2520or%2520AI-generated%2520%2528ChatGPT-4o%2529.%2520The%2520results%250Ashow%2520that%252C%2520on%2520average%252C%2520the%2520students%2520struggled%2520to%2520distinguish%2520between%2520HT%2520and%2520ST%252C%250Awith%2520only%2520two%2520participants%2520achieving%2520notable%2520accuracy.%2520Closer%2520analysis%2520revealed%250Athat%2520the%2520students%2520often%2520identified%2520the%2520same%2520textual%2520anomalies%2520in%2520both%2520HT%2520and%250AST%252C%2520although%2520features%2520such%2520as%2520low%2520burstiness%2520and%2520self-contradiction%2520were%2520more%250Afrequently%2520associated%2520with%2520ST.%2520These%2520findings%2520suggest%2520the%2520need%2520for%2520improvements%250Ain%2520the%2520preparatory%2520training.%2520Moreover%252C%2520the%2520study%2520raises%2520questions%2520about%2520the%250Anecessity%2520of%2520editing%2520synthetic%2520text%2520to%2520make%2520it%2520sound%2520more%2520human-like%2520and%250Arecommends%2520further%2520research%2520to%2520determine%2520whether%2520AI-generated%2520text%2520is%2520already%250Asufficiently%2520natural-sounding%2520not%2520to%2520require%2520further%2520refinement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09164v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20postgraduate%20translation%20students%20identify%20machine-generated%20text%3F&entry.906535625=Michael%20Farrell&entry.1292438233=%20%20Given%20the%20growing%20use%20of%20generative%20artificial%20intelligence%20as%20a%20tool%20for%0Acreating%20multilingual%20content%20and%20bypassing%20both%20machine%20and%20traditional%0Atranslation%20methods%2C%20this%20study%20explores%20the%20ability%20of%20linguistically%20trained%0Aindividuals%20to%20discern%20machine-generated%20output%20from%20human-written%20text%20%28HT%29.%0AAfter%20brief%20training%20sessions%20on%20the%20textual%20anomalies%20typically%20found%20in%0Asynthetic%20text%20%28ST%29%2C%20twenty-three%20postgraduate%20translation%20students%20analysed%0Aexcerpts%20of%20Italian%20prose%20and%20assigned%20likelihood%20scores%20to%20indicate%20whether%0Athey%20believed%20they%20were%20human-written%20or%20AI-generated%20%28ChatGPT-4o%29.%20The%20results%0Ashow%20that%2C%20on%20average%2C%20the%20students%20struggled%20to%20distinguish%20between%20HT%20and%20ST%2C%0Awith%20only%20two%20participants%20achieving%20notable%20accuracy.%20Closer%20analysis%20revealed%0Athat%20the%20students%20often%20identified%20the%20same%20textual%20anomalies%20in%20both%20HT%20and%0AST%2C%20although%20features%20such%20as%20low%20burstiness%20and%20self-contradiction%20were%20more%0Afrequently%20associated%20with%20ST.%20These%20findings%20suggest%20the%20need%20for%20improvements%0Ain%20the%20preparatory%20training.%20Moreover%2C%20the%20study%20raises%20questions%20about%20the%0Anecessity%20of%20editing%20synthetic%20text%20to%20make%20it%20sound%20more%20human-like%20and%0Arecommends%20further%20research%20to%20determine%20whether%20AI-generated%20text%20is%20already%0Asufficiently%20natural-sounding%20not%20to%20require%20further%20refinement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09164v2&entry.124074799=Read"},
{"title": "The Binary and Ternary Quantization Can Improve Feature Discrimination", "author": "Weizhi Lu and Mingrui Chen and Weiyu Li", "abstract": "  In machine learning, quantization is widely used to simplify data\nrepresentation and facilitate algorithm deployment on hardware. Given the\nfundamental role of classification in machine learning, it is crucial to\ninvestigate the impact of quantization on classification. Current research\nprimarily focuses on quantization errors, operating under the premise that\nhigher quantization errors generally result in lower classification\nperformance. However, this premise lacks a solid theoretical foundation and\noften contradicts empirical findings. For instance, certain extremely low\nbit-width quantization methods, such as $\\{0,1\\}$-binary quantization and $\\{0,\n\\pm1\\}$-ternary quantization, can achieve comparable or even superior\nclassification accuracy compared to the original non-quantized data, despite\nexhibiting high quantization errors. To more accurately evaluate classification\nperformance, we propose to directly investigate the feature discrimination of\nquantized data, instead of analyzing its quantization error. Interestingly, it\nis found that both binary and ternary quantization methods can improve, rather\nthan degrade, the feature discrimination of the original data. This remarkable\nperformance is validated through classification experiments across various data\ntypes, including images, speech, and texts.\n", "link": "http://arxiv.org/abs/2504.13792v1", "date": "2025-04-18", "relevancy": 2.3724, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4879}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.471}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Binary%20and%20Ternary%20Quantization%20Can%20Improve%20Feature%20Discrimination&body=Title%3A%20The%20Binary%20and%20Ternary%20Quantization%20Can%20Improve%20Feature%20Discrimination%0AAuthor%3A%20Weizhi%20Lu%20and%20Mingrui%20Chen%20and%20Weiyu%20Li%0AAbstract%3A%20%20%20In%20machine%20learning%2C%20quantization%20is%20widely%20used%20to%20simplify%20data%0Arepresentation%20and%20facilitate%20algorithm%20deployment%20on%20hardware.%20Given%20the%0Afundamental%20role%20of%20classification%20in%20machine%20learning%2C%20it%20is%20crucial%20to%0Ainvestigate%20the%20impact%20of%20quantization%20on%20classification.%20Current%20research%0Aprimarily%20focuses%20on%20quantization%20errors%2C%20operating%20under%20the%20premise%20that%0Ahigher%20quantization%20errors%20generally%20result%20in%20lower%20classification%0Aperformance.%20However%2C%20this%20premise%20lacks%20a%20solid%20theoretical%20foundation%20and%0Aoften%20contradicts%20empirical%20findings.%20For%20instance%2C%20certain%20extremely%20low%0Abit-width%20quantization%20methods%2C%20such%20as%20%24%5C%7B0%2C1%5C%7D%24-binary%20quantization%20and%20%24%5C%7B0%2C%0A%5Cpm1%5C%7D%24-ternary%20quantization%2C%20can%20achieve%20comparable%20or%20even%20superior%0Aclassification%20accuracy%20compared%20to%20the%20original%20non-quantized%20data%2C%20despite%0Aexhibiting%20high%20quantization%20errors.%20To%20more%20accurately%20evaluate%20classification%0Aperformance%2C%20we%20propose%20to%20directly%20investigate%20the%20feature%20discrimination%20of%0Aquantized%20data%2C%20instead%20of%20analyzing%20its%20quantization%20error.%20Interestingly%2C%20it%0Ais%20found%20that%20both%20binary%20and%20ternary%20quantization%20methods%20can%20improve%2C%20rather%0Athan%20degrade%2C%20the%20feature%20discrimination%20of%20the%20original%20data.%20This%20remarkable%0Aperformance%20is%20validated%20through%20classification%20experiments%20across%20various%20data%0Atypes%2C%20including%20images%2C%20speech%2C%20and%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Binary%2520and%2520Ternary%2520Quantization%2520Can%2520Improve%2520Feature%2520Discrimination%26entry.906535625%3DWeizhi%2520Lu%2520and%2520Mingrui%2520Chen%2520and%2520Weiyu%2520Li%26entry.1292438233%3D%2520%2520In%2520machine%2520learning%252C%2520quantization%2520is%2520widely%2520used%2520to%2520simplify%2520data%250Arepresentation%2520and%2520facilitate%2520algorithm%2520deployment%2520on%2520hardware.%2520Given%2520the%250Afundamental%2520role%2520of%2520classification%2520in%2520machine%2520learning%252C%2520it%2520is%2520crucial%2520to%250Ainvestigate%2520the%2520impact%2520of%2520quantization%2520on%2520classification.%2520Current%2520research%250Aprimarily%2520focuses%2520on%2520quantization%2520errors%252C%2520operating%2520under%2520the%2520premise%2520that%250Ahigher%2520quantization%2520errors%2520generally%2520result%2520in%2520lower%2520classification%250Aperformance.%2520However%252C%2520this%2520premise%2520lacks%2520a%2520solid%2520theoretical%2520foundation%2520and%250Aoften%2520contradicts%2520empirical%2520findings.%2520For%2520instance%252C%2520certain%2520extremely%2520low%250Abit-width%2520quantization%2520methods%252C%2520such%2520as%2520%2524%255C%257B0%252C1%255C%257D%2524-binary%2520quantization%2520and%2520%2524%255C%257B0%252C%250A%255Cpm1%255C%257D%2524-ternary%2520quantization%252C%2520can%2520achieve%2520comparable%2520or%2520even%2520superior%250Aclassification%2520accuracy%2520compared%2520to%2520the%2520original%2520non-quantized%2520data%252C%2520despite%250Aexhibiting%2520high%2520quantization%2520errors.%2520To%2520more%2520accurately%2520evaluate%2520classification%250Aperformance%252C%2520we%2520propose%2520to%2520directly%2520investigate%2520the%2520feature%2520discrimination%2520of%250Aquantized%2520data%252C%2520instead%2520of%2520analyzing%2520its%2520quantization%2520error.%2520Interestingly%252C%2520it%250Ais%2520found%2520that%2520both%2520binary%2520and%2520ternary%2520quantization%2520methods%2520can%2520improve%252C%2520rather%250Athan%2520degrade%252C%2520the%2520feature%2520discrimination%2520of%2520the%2520original%2520data.%2520This%2520remarkable%250Aperformance%2520is%2520validated%2520through%2520classification%2520experiments%2520across%2520various%2520data%250Atypes%252C%2520including%2520images%252C%2520speech%252C%2520and%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Binary%20and%20Ternary%20Quantization%20Can%20Improve%20Feature%20Discrimination&entry.906535625=Weizhi%20Lu%20and%20Mingrui%20Chen%20and%20Weiyu%20Li&entry.1292438233=%20%20In%20machine%20learning%2C%20quantization%20is%20widely%20used%20to%20simplify%20data%0Arepresentation%20and%20facilitate%20algorithm%20deployment%20on%20hardware.%20Given%20the%0Afundamental%20role%20of%20classification%20in%20machine%20learning%2C%20it%20is%20crucial%20to%0Ainvestigate%20the%20impact%20of%20quantization%20on%20classification.%20Current%20research%0Aprimarily%20focuses%20on%20quantization%20errors%2C%20operating%20under%20the%20premise%20that%0Ahigher%20quantization%20errors%20generally%20result%20in%20lower%20classification%0Aperformance.%20However%2C%20this%20premise%20lacks%20a%20solid%20theoretical%20foundation%20and%0Aoften%20contradicts%20empirical%20findings.%20For%20instance%2C%20certain%20extremely%20low%0Abit-width%20quantization%20methods%2C%20such%20as%20%24%5C%7B0%2C1%5C%7D%24-binary%20quantization%20and%20%24%5C%7B0%2C%0A%5Cpm1%5C%7D%24-ternary%20quantization%2C%20can%20achieve%20comparable%20or%20even%20superior%0Aclassification%20accuracy%20compared%20to%20the%20original%20non-quantized%20data%2C%20despite%0Aexhibiting%20high%20quantization%20errors.%20To%20more%20accurately%20evaluate%20classification%0Aperformance%2C%20we%20propose%20to%20directly%20investigate%20the%20feature%20discrimination%20of%0Aquantized%20data%2C%20instead%20of%20analyzing%20its%20quantization%20error.%20Interestingly%2C%20it%0Ais%20found%20that%20both%20binary%20and%20ternary%20quantization%20methods%20can%20improve%2C%20rather%0Athan%20degrade%2C%20the%20feature%20discrimination%20of%20the%20original%20data.%20This%20remarkable%0Aperformance%20is%20validated%20through%20classification%20experiments%20across%20various%20data%0Atypes%2C%20including%20images%2C%20speech%2C%20and%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13792v1&entry.124074799=Read"},
{"title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?", "author": "Hao Zhao and Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion", "abstract": "  In-context learning (ICL) allows LLMs to learn from examples without changing\ntheir weights: this is a particularly promising capability for long-context\nLLMs that can potentially learn from many examples. Recently, Lin et al. (2024)\nproposed URIAL, a method using only three in-context examples to align base\nLLMs, achieving non-trivial instruction following performance. In this work, we\nshow that, while effective, ICL alignment with URIAL still underperforms\ncompared to instruction fine-tuning on the established benchmark MT-Bench,\nespecially with more capable base LLMs. We then uncover the most relevant\nelements for successful in-context alignment, finding the crucial role of the\ndecoding parameters. Based on these insights, we show that the approach of\nURIAL can indeed be improved by adding high-quality, potentially carefully\nselected via greedy search, demonstrations in context, getting closer to the\nperformance of instruct models. Finally, we provide the first, to our\nknowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for\ninstruction following in the low data regime, where ICL can be a viable\nalternative to IFT. Overall, our work advances the understanding of ICL as an\nalignment technique and its relationship to IFT. We provide our code at\nhttps://github.com/tml-epfl/icl-alignment.\n", "link": "http://arxiv.org/abs/2405.19874v3", "date": "2025-04-18", "relevancy": 2.3673, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20In-Context%20Learning%20Sufficient%20for%20Instruction%20Following%20in%20LLMs%3F&body=Title%3A%20Is%20In-Context%20Learning%20Sufficient%20for%20Instruction%20Following%20in%20LLMs%3F%0AAuthor%3A%20Hao%20Zhao%20and%20Maksym%20Andriushchenko%20and%20Francesco%20Croce%20and%20Nicolas%20Flammarion%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20allows%20LLMs%20to%20learn%20from%20examples%20without%20changing%0Atheir%20weights%3A%20this%20is%20a%20particularly%20promising%20capability%20for%20long-context%0ALLMs%20that%20can%20potentially%20learn%20from%20many%20examples.%20Recently%2C%20Lin%20et%20al.%20%282024%29%0Aproposed%20URIAL%2C%20a%20method%20using%20only%20three%20in-context%20examples%20to%20align%20base%0ALLMs%2C%20achieving%20non-trivial%20instruction%20following%20performance.%20In%20this%20work%2C%20we%0Ashow%20that%2C%20while%20effective%2C%20ICL%20alignment%20with%20URIAL%20still%20underperforms%0Acompared%20to%20instruction%20fine-tuning%20on%20the%20established%20benchmark%20MT-Bench%2C%0Aespecially%20with%20more%20capable%20base%20LLMs.%20We%20then%20uncover%20the%20most%20relevant%0Aelements%20for%20successful%20in-context%20alignment%2C%20finding%20the%20crucial%20role%20of%20the%0Adecoding%20parameters.%20Based%20on%20these%20insights%2C%20we%20show%20that%20the%20approach%20of%0AURIAL%20can%20indeed%20be%20improved%20by%20adding%20high-quality%2C%20potentially%20carefully%0Aselected%20via%20greedy%20search%2C%20demonstrations%20in%20context%2C%20getting%20closer%20to%20the%0Aperformance%20of%20instruct%20models.%20Finally%2C%20we%20provide%20the%20first%2C%20to%20our%0Aknowledge%2C%20systematic%20comparison%20of%20ICL%20and%20instruction%20fine-tuning%20%28IFT%29%20for%0Ainstruction%20following%20in%20the%20low%20data%20regime%2C%20where%20ICL%20can%20be%20a%20viable%0Aalternative%20to%20IFT.%20Overall%2C%20our%20work%20advances%20the%20understanding%20of%20ICL%20as%20an%0Aalignment%20technique%20and%20its%20relationship%20to%20IFT.%20We%20provide%20our%20code%20at%0Ahttps%3A//github.com/tml-epfl/icl-alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19874v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520In-Context%2520Learning%2520Sufficient%2520for%2520Instruction%2520Following%2520in%2520LLMs%253F%26entry.906535625%3DHao%2520Zhao%2520and%2520Maksym%2520Andriushchenko%2520and%2520Francesco%2520Croce%2520and%2520Nicolas%2520Flammarion%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520allows%2520LLMs%2520to%2520learn%2520from%2520examples%2520without%2520changing%250Atheir%2520weights%253A%2520this%2520is%2520a%2520particularly%2520promising%2520capability%2520for%2520long-context%250ALLMs%2520that%2520can%2520potentially%2520learn%2520from%2520many%2520examples.%2520Recently%252C%2520Lin%2520et%2520al.%2520%25282024%2529%250Aproposed%2520URIAL%252C%2520a%2520method%2520using%2520only%2520three%2520in-context%2520examples%2520to%2520align%2520base%250ALLMs%252C%2520achieving%2520non-trivial%2520instruction%2520following%2520performance.%2520In%2520this%2520work%252C%2520we%250Ashow%2520that%252C%2520while%2520effective%252C%2520ICL%2520alignment%2520with%2520URIAL%2520still%2520underperforms%250Acompared%2520to%2520instruction%2520fine-tuning%2520on%2520the%2520established%2520benchmark%2520MT-Bench%252C%250Aespecially%2520with%2520more%2520capable%2520base%2520LLMs.%2520We%2520then%2520uncover%2520the%2520most%2520relevant%250Aelements%2520for%2520successful%2520in-context%2520alignment%252C%2520finding%2520the%2520crucial%2520role%2520of%2520the%250Adecoding%2520parameters.%2520Based%2520on%2520these%2520insights%252C%2520we%2520show%2520that%2520the%2520approach%2520of%250AURIAL%2520can%2520indeed%2520be%2520improved%2520by%2520adding%2520high-quality%252C%2520potentially%2520carefully%250Aselected%2520via%2520greedy%2520search%252C%2520demonstrations%2520in%2520context%252C%2520getting%2520closer%2520to%2520the%250Aperformance%2520of%2520instruct%2520models.%2520Finally%252C%2520we%2520provide%2520the%2520first%252C%2520to%2520our%250Aknowledge%252C%2520systematic%2520comparison%2520of%2520ICL%2520and%2520instruction%2520fine-tuning%2520%2528IFT%2529%2520for%250Ainstruction%2520following%2520in%2520the%2520low%2520data%2520regime%252C%2520where%2520ICL%2520can%2520be%2520a%2520viable%250Aalternative%2520to%2520IFT.%2520Overall%252C%2520our%2520work%2520advances%2520the%2520understanding%2520of%2520ICL%2520as%2520an%250Aalignment%2520technique%2520and%2520its%2520relationship%2520to%2520IFT.%2520We%2520provide%2520our%2520code%2520at%250Ahttps%253A//github.com/tml-epfl/icl-alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19874v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20In-Context%20Learning%20Sufficient%20for%20Instruction%20Following%20in%20LLMs%3F&entry.906535625=Hao%20Zhao%20and%20Maksym%20Andriushchenko%20and%20Francesco%20Croce%20and%20Nicolas%20Flammarion&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20allows%20LLMs%20to%20learn%20from%20examples%20without%20changing%0Atheir%20weights%3A%20this%20is%20a%20particularly%20promising%20capability%20for%20long-context%0ALLMs%20that%20can%20potentially%20learn%20from%20many%20examples.%20Recently%2C%20Lin%20et%20al.%20%282024%29%0Aproposed%20URIAL%2C%20a%20method%20using%20only%20three%20in-context%20examples%20to%20align%20base%0ALLMs%2C%20achieving%20non-trivial%20instruction%20following%20performance.%20In%20this%20work%2C%20we%0Ashow%20that%2C%20while%20effective%2C%20ICL%20alignment%20with%20URIAL%20still%20underperforms%0Acompared%20to%20instruction%20fine-tuning%20on%20the%20established%20benchmark%20MT-Bench%2C%0Aespecially%20with%20more%20capable%20base%20LLMs.%20We%20then%20uncover%20the%20most%20relevant%0Aelements%20for%20successful%20in-context%20alignment%2C%20finding%20the%20crucial%20role%20of%20the%0Adecoding%20parameters.%20Based%20on%20these%20insights%2C%20we%20show%20that%20the%20approach%20of%0AURIAL%20can%20indeed%20be%20improved%20by%20adding%20high-quality%2C%20potentially%20carefully%0Aselected%20via%20greedy%20search%2C%20demonstrations%20in%20context%2C%20getting%20closer%20to%20the%0Aperformance%20of%20instruct%20models.%20Finally%2C%20we%20provide%20the%20first%2C%20to%20our%0Aknowledge%2C%20systematic%20comparison%20of%20ICL%20and%20instruction%20fine-tuning%20%28IFT%29%20for%0Ainstruction%20following%20in%20the%20low%20data%20regime%2C%20where%20ICL%20can%20be%20a%20viable%0Aalternative%20to%20IFT.%20Overall%2C%20our%20work%20advances%20the%20understanding%20of%20ICL%20as%20an%0Aalignment%20technique%20and%20its%20relationship%20to%20IFT.%20We%20provide%20our%20code%20at%0Ahttps%3A//github.com/tml-epfl/icl-alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19874v3&entry.124074799=Read"},
{"title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a\n  Computer Programming Teaching Assistant", "author": "Marc Ballestero-Rib\u00f3 and Daniel Ortiz-Mart\u00ednez", "abstract": "  The dream of achieving a student-teacher ratio of 1:1 is closer than ever\nthanks to the emergence of large language models (LLMs). One potential\napplication of these models in the educational field would be to provide\nfeedback to students in university introductory programming courses, so that a\nstudent struggling to solve a basic implementation problem could seek help from\nan LLM available 24/7. This article focuses on studying three aspects related\nto such an application. First, the performance of two well-known models,\nGPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The\nempirical results showed that GPT-4T performs much better than GPT-3.5T,\nhowever, it is not yet ready for use in a real-world scenario. This is due to\nthe possibility of generating incorrect information that potential users may\nnot always be able to detect. Second, the article proposes a carefully designed\nprompt using in-context learning techniques that allows automating important\nparts of the evaluation process, as well as providing a lower bound for the\nfraction of feedbacks containing incorrect information, saving time and effort.\nThis was possible because the resulting feedback has a programmatically\nanalyzable structure that incorporates diagnostic information about the LLM's\nperformance in solving the requested task. Third, the article also suggests a\npossible strategy for implementing a practical learning tool based on LLMs,\nwhich is rooted on the proposed prompting techniques. This strategy opens up a\nwhole range of interesting possibilities from a pedagogical perspective.\n", "link": "http://arxiv.org/abs/2501.17176v2", "date": "2025-04-18", "relevancy": 2.3639, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4932}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4629}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Based%20Cost-Effective%20Evaluation%20and%20Operation%20of%20ChatGPT%20as%20a%0A%20%20Computer%20Programming%20Teaching%20Assistant&body=Title%3A%20Prompt-Based%20Cost-Effective%20Evaluation%20and%20Operation%20of%20ChatGPT%20as%20a%0A%20%20Computer%20Programming%20Teaching%20Assistant%0AAuthor%3A%20Marc%20Ballestero-Rib%C3%B3%20and%20Daniel%20Ortiz-Mart%C3%ADnez%0AAbstract%3A%20%20%20The%20dream%20of%20achieving%20a%20student-teacher%20ratio%20of%201%3A1%20is%20closer%20than%20ever%0Athanks%20to%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29.%20One%20potential%0Aapplication%20of%20these%20models%20in%20the%20educational%20field%20would%20be%20to%20provide%0Afeedback%20to%20students%20in%20university%20introductory%20programming%20courses%2C%20so%20that%20a%0Astudent%20struggling%20to%20solve%20a%20basic%20implementation%20problem%20could%20seek%20help%20from%0Aan%20LLM%20available%2024/7.%20This%20article%20focuses%20on%20studying%20three%20aspects%20related%0Ato%20such%20an%20application.%20First%2C%20the%20performance%20of%20two%20well-known%20models%2C%0AGPT-3.5T%20and%20GPT-4T%2C%20in%20providing%20feedback%20to%20students%20is%20evaluated.%20The%0Aempirical%20results%20showed%20that%20GPT-4T%20performs%20much%20better%20than%20GPT-3.5T%2C%0Ahowever%2C%20it%20is%20not%20yet%20ready%20for%20use%20in%20a%20real-world%20scenario.%20This%20is%20due%20to%0Athe%20possibility%20of%20generating%20incorrect%20information%20that%20potential%20users%20may%0Anot%20always%20be%20able%20to%20detect.%20Second%2C%20the%20article%20proposes%20a%20carefully%20designed%0Aprompt%20using%20in-context%20learning%20techniques%20that%20allows%20automating%20important%0Aparts%20of%20the%20evaluation%20process%2C%20as%20well%20as%20providing%20a%20lower%20bound%20for%20the%0Afraction%20of%20feedbacks%20containing%20incorrect%20information%2C%20saving%20time%20and%20effort.%0AThis%20was%20possible%20because%20the%20resulting%20feedback%20has%20a%20programmatically%0Aanalyzable%20structure%20that%20incorporates%20diagnostic%20information%20about%20the%20LLM%27s%0Aperformance%20in%20solving%20the%20requested%20task.%20Third%2C%20the%20article%20also%20suggests%20a%0Apossible%20strategy%20for%20implementing%20a%20practical%20learning%20tool%20based%20on%20LLMs%2C%0Awhich%20is%20rooted%20on%20the%20proposed%20prompting%20techniques.%20This%20strategy%20opens%20up%20a%0Awhole%20range%20of%20interesting%20possibilities%20from%20a%20pedagogical%20perspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Based%2520Cost-Effective%2520Evaluation%2520and%2520Operation%2520of%2520ChatGPT%2520as%2520a%250A%2520%2520Computer%2520Programming%2520Teaching%2520Assistant%26entry.906535625%3DMarc%2520Ballestero-Rib%25C3%25B3%2520and%2520Daniel%2520Ortiz-Mart%25C3%25ADnez%26entry.1292438233%3D%2520%2520The%2520dream%2520of%2520achieving%2520a%2520student-teacher%2520ratio%2520of%25201%253A1%2520is%2520closer%2520than%2520ever%250Athanks%2520to%2520the%2520emergence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520One%2520potential%250Aapplication%2520of%2520these%2520models%2520in%2520the%2520educational%2520field%2520would%2520be%2520to%2520provide%250Afeedback%2520to%2520students%2520in%2520university%2520introductory%2520programming%2520courses%252C%2520so%2520that%2520a%250Astudent%2520struggling%2520to%2520solve%2520a%2520basic%2520implementation%2520problem%2520could%2520seek%2520help%2520from%250Aan%2520LLM%2520available%252024/7.%2520This%2520article%2520focuses%2520on%2520studying%2520three%2520aspects%2520related%250Ato%2520such%2520an%2520application.%2520First%252C%2520the%2520performance%2520of%2520two%2520well-known%2520models%252C%250AGPT-3.5T%2520and%2520GPT-4T%252C%2520in%2520providing%2520feedback%2520to%2520students%2520is%2520evaluated.%2520The%250Aempirical%2520results%2520showed%2520that%2520GPT-4T%2520performs%2520much%2520better%2520than%2520GPT-3.5T%252C%250Ahowever%252C%2520it%2520is%2520not%2520yet%2520ready%2520for%2520use%2520in%2520a%2520real-world%2520scenario.%2520This%2520is%2520due%2520to%250Athe%2520possibility%2520of%2520generating%2520incorrect%2520information%2520that%2520potential%2520users%2520may%250Anot%2520always%2520be%2520able%2520to%2520detect.%2520Second%252C%2520the%2520article%2520proposes%2520a%2520carefully%2520designed%250Aprompt%2520using%2520in-context%2520learning%2520techniques%2520that%2520allows%2520automating%2520important%250Aparts%2520of%2520the%2520evaluation%2520process%252C%2520as%2520well%2520as%2520providing%2520a%2520lower%2520bound%2520for%2520the%250Afraction%2520of%2520feedbacks%2520containing%2520incorrect%2520information%252C%2520saving%2520time%2520and%2520effort.%250AThis%2520was%2520possible%2520because%2520the%2520resulting%2520feedback%2520has%2520a%2520programmatically%250Aanalyzable%2520structure%2520that%2520incorporates%2520diagnostic%2520information%2520about%2520the%2520LLM%2527s%250Aperformance%2520in%2520solving%2520the%2520requested%2520task.%2520Third%252C%2520the%2520article%2520also%2520suggests%2520a%250Apossible%2520strategy%2520for%2520implementing%2520a%2520practical%2520learning%2520tool%2520based%2520on%2520LLMs%252C%250Awhich%2520is%2520rooted%2520on%2520the%2520proposed%2520prompting%2520techniques.%2520This%2520strategy%2520opens%2520up%2520a%250Awhole%2520range%2520of%2520interesting%2520possibilities%2520from%2520a%2520pedagogical%2520perspective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Based%20Cost-Effective%20Evaluation%20and%20Operation%20of%20ChatGPT%20as%20a%0A%20%20Computer%20Programming%20Teaching%20Assistant&entry.906535625=Marc%20Ballestero-Rib%C3%B3%20and%20Daniel%20Ortiz-Mart%C3%ADnez&entry.1292438233=%20%20The%20dream%20of%20achieving%20a%20student-teacher%20ratio%20of%201%3A1%20is%20closer%20than%20ever%0Athanks%20to%20the%20emergence%20of%20large%20language%20models%20%28LLMs%29.%20One%20potential%0Aapplication%20of%20these%20models%20in%20the%20educational%20field%20would%20be%20to%20provide%0Afeedback%20to%20students%20in%20university%20introductory%20programming%20courses%2C%20so%20that%20a%0Astudent%20struggling%20to%20solve%20a%20basic%20implementation%20problem%20could%20seek%20help%20from%0Aan%20LLM%20available%2024/7.%20This%20article%20focuses%20on%20studying%20three%20aspects%20related%0Ato%20such%20an%20application.%20First%2C%20the%20performance%20of%20two%20well-known%20models%2C%0AGPT-3.5T%20and%20GPT-4T%2C%20in%20providing%20feedback%20to%20students%20is%20evaluated.%20The%0Aempirical%20results%20showed%20that%20GPT-4T%20performs%20much%20better%20than%20GPT-3.5T%2C%0Ahowever%2C%20it%20is%20not%20yet%20ready%20for%20use%20in%20a%20real-world%20scenario.%20This%20is%20due%20to%0Athe%20possibility%20of%20generating%20incorrect%20information%20that%20potential%20users%20may%0Anot%20always%20be%20able%20to%20detect.%20Second%2C%20the%20article%20proposes%20a%20carefully%20designed%0Aprompt%20using%20in-context%20learning%20techniques%20that%20allows%20automating%20important%0Aparts%20of%20the%20evaluation%20process%2C%20as%20well%20as%20providing%20a%20lower%20bound%20for%20the%0Afraction%20of%20feedbacks%20containing%20incorrect%20information%2C%20saving%20time%20and%20effort.%0AThis%20was%20possible%20because%20the%20resulting%20feedback%20has%20a%20programmatically%0Aanalyzable%20structure%20that%20incorporates%20diagnostic%20information%20about%20the%20LLM%27s%0Aperformance%20in%20solving%20the%20requested%20task.%20Third%2C%20the%20article%20also%20suggests%20a%0Apossible%20strategy%20for%20implementing%20a%20practical%20learning%20tool%20based%20on%20LLMs%2C%0Awhich%20is%20rooted%20on%20the%20proposed%20prompting%20techniques.%20This%20strategy%20opens%20up%20a%0Awhole%20range%20of%20interesting%20possibilities%20from%20a%20pedagogical%20perspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17176v2&entry.124074799=Read"},
{"title": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion", "author": "Junhwa Hur and Charles Herrmann and Saurabh Saxena and Janne Kontkanen and Wei-Sheng Lai and Yichang Shih and Michael Rubinstein and David J. Fleet and Deqing Sun", "abstract": "  Despite the recent progress, existing frame interpolation methods still\nstruggle with processing extremely high resolution input and handling\nchallenging cases such as repetitive textures, thin objects, and large motion.\nTo address these issues, we introduce a patch-based cascaded pixel diffusion\nmodel for high resolution frame interpolation, HIFI, that excels in these\nscenarios while achieving competitive performance on standard benchmarks.\nCascades, which generate a series of images from low to high resolution, can\nhelp significantly with large or complex motion that require both global\ncontext for a coarse solution and detailed context for high resolution output.\nHowever, contrary to prior work on cascaded diffusion models which perform\ndiffusion on increasingly large resolutions, we use a single model that always\nperforms diffusion at the same resolution and upsamples by processing patches\nof the inputs and the prior solution. At inference time, this drastically\nreduces memory usage and allows a single model, solving both frame\ninterpolation (base model's task) and spatial up-sampling, saving training cost\nas well. HIFI excels at high-resolution images and complex repeated textures\nthat require global context, achieving comparable or state-of-the-art\nperformance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We\nfurther introduce a new dataset, LaMoR, that focuses on particularly\nchallenging cases, and HIFI significantly outperforms other baselines. Please\nvisit our project page for video results: https://hifi-diffusion.github.io\n", "link": "http://arxiv.org/abs/2410.11838v2", "date": "2025-04-18", "relevancy": 2.3544, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6025}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6022}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Resolution%20Frame%20Interpolation%20with%20Patch-based%20Cascaded%20Diffusion&body=Title%3A%20High-Resolution%20Frame%20Interpolation%20with%20Patch-based%20Cascaded%20Diffusion%0AAuthor%3A%20Junhwa%20Hur%20and%20Charles%20Herrmann%20and%20Saurabh%20Saxena%20and%20Janne%20Kontkanen%20and%20Wei-Sheng%20Lai%20and%20Yichang%20Shih%20and%20Michael%20Rubinstein%20and%20David%20J.%20Fleet%20and%20Deqing%20Sun%0AAbstract%3A%20%20%20Despite%20the%20recent%20progress%2C%20existing%20frame%20interpolation%20methods%20still%0Astruggle%20with%20processing%20extremely%20high%20resolution%20input%20and%20handling%0Achallenging%20cases%20such%20as%20repetitive%20textures%2C%20thin%20objects%2C%20and%20large%20motion.%0ATo%20address%20these%20issues%2C%20we%20introduce%20a%20patch-based%20cascaded%20pixel%20diffusion%0Amodel%20for%20high%20resolution%20frame%20interpolation%2C%20HIFI%2C%20that%20excels%20in%20these%0Ascenarios%20while%20achieving%20competitive%20performance%20on%20standard%20benchmarks.%0ACascades%2C%20which%20generate%20a%20series%20of%20images%20from%20low%20to%20high%20resolution%2C%20can%0Ahelp%20significantly%20with%20large%20or%20complex%20motion%20that%20require%20both%20global%0Acontext%20for%20a%20coarse%20solution%20and%20detailed%20context%20for%20high%20resolution%20output.%0AHowever%2C%20contrary%20to%20prior%20work%20on%20cascaded%20diffusion%20models%20which%20perform%0Adiffusion%20on%20increasingly%20large%20resolutions%2C%20we%20use%20a%20single%20model%20that%20always%0Aperforms%20diffusion%20at%20the%20same%20resolution%20and%20upsamples%20by%20processing%20patches%0Aof%20the%20inputs%20and%20the%20prior%20solution.%20At%20inference%20time%2C%20this%20drastically%0Areduces%20memory%20usage%20and%20allows%20a%20single%20model%2C%20solving%20both%20frame%0Ainterpolation%20%28base%20model%27s%20task%29%20and%20spatial%20up-sampling%2C%20saving%20training%20cost%0Aas%20well.%20HIFI%20excels%20at%20high-resolution%20images%20and%20complex%20repeated%20textures%0Athat%20require%20global%20context%2C%20achieving%20comparable%20or%20state-of-the-art%0Aperformance%20on%20various%20benchmarks%20%28Vimeo%2C%20Xiph%2C%20X-Test%2C%20and%20SEPE-8K%29.%20We%0Afurther%20introduce%20a%20new%20dataset%2C%20LaMoR%2C%20that%20focuses%20on%20particularly%0Achallenging%20cases%2C%20and%20HIFI%20significantly%20outperforms%20other%20baselines.%20Please%0Avisit%20our%20project%20page%20for%20video%20results%3A%20https%3A//hifi-diffusion.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11838v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Resolution%2520Frame%2520Interpolation%2520with%2520Patch-based%2520Cascaded%2520Diffusion%26entry.906535625%3DJunhwa%2520Hur%2520and%2520Charles%2520Herrmann%2520and%2520Saurabh%2520Saxena%2520and%2520Janne%2520Kontkanen%2520and%2520Wei-Sheng%2520Lai%2520and%2520Yichang%2520Shih%2520and%2520Michael%2520Rubinstein%2520and%2520David%2520J.%2520Fleet%2520and%2520Deqing%2520Sun%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520progress%252C%2520existing%2520frame%2520interpolation%2520methods%2520still%250Astruggle%2520with%2520processing%2520extremely%2520high%2520resolution%2520input%2520and%2520handling%250Achallenging%2520cases%2520such%2520as%2520repetitive%2520textures%252C%2520thin%2520objects%252C%2520and%2520large%2520motion.%250ATo%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520patch-based%2520cascaded%2520pixel%2520diffusion%250Amodel%2520for%2520high%2520resolution%2520frame%2520interpolation%252C%2520HIFI%252C%2520that%2520excels%2520in%2520these%250Ascenarios%2520while%2520achieving%2520competitive%2520performance%2520on%2520standard%2520benchmarks.%250ACascades%252C%2520which%2520generate%2520a%2520series%2520of%2520images%2520from%2520low%2520to%2520high%2520resolution%252C%2520can%250Ahelp%2520significantly%2520with%2520large%2520or%2520complex%2520motion%2520that%2520require%2520both%2520global%250Acontext%2520for%2520a%2520coarse%2520solution%2520and%2520detailed%2520context%2520for%2520high%2520resolution%2520output.%250AHowever%252C%2520contrary%2520to%2520prior%2520work%2520on%2520cascaded%2520diffusion%2520models%2520which%2520perform%250Adiffusion%2520on%2520increasingly%2520large%2520resolutions%252C%2520we%2520use%2520a%2520single%2520model%2520that%2520always%250Aperforms%2520diffusion%2520at%2520the%2520same%2520resolution%2520and%2520upsamples%2520by%2520processing%2520patches%250Aof%2520the%2520inputs%2520and%2520the%2520prior%2520solution.%2520At%2520inference%2520time%252C%2520this%2520drastically%250Areduces%2520memory%2520usage%2520and%2520allows%2520a%2520single%2520model%252C%2520solving%2520both%2520frame%250Ainterpolation%2520%2528base%2520model%2527s%2520task%2529%2520and%2520spatial%2520up-sampling%252C%2520saving%2520training%2520cost%250Aas%2520well.%2520HIFI%2520excels%2520at%2520high-resolution%2520images%2520and%2520complex%2520repeated%2520textures%250Athat%2520require%2520global%2520context%252C%2520achieving%2520comparable%2520or%2520state-of-the-art%250Aperformance%2520on%2520various%2520benchmarks%2520%2528Vimeo%252C%2520Xiph%252C%2520X-Test%252C%2520and%2520SEPE-8K%2529.%2520We%250Afurther%2520introduce%2520a%2520new%2520dataset%252C%2520LaMoR%252C%2520that%2520focuses%2520on%2520particularly%250Achallenging%2520cases%252C%2520and%2520HIFI%2520significantly%2520outperforms%2520other%2520baselines.%2520Please%250Avisit%2520our%2520project%2520page%2520for%2520video%2520results%253A%2520https%253A//hifi-diffusion.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11838v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Resolution%20Frame%20Interpolation%20with%20Patch-based%20Cascaded%20Diffusion&entry.906535625=Junhwa%20Hur%20and%20Charles%20Herrmann%20and%20Saurabh%20Saxena%20and%20Janne%20Kontkanen%20and%20Wei-Sheng%20Lai%20and%20Yichang%20Shih%20and%20Michael%20Rubinstein%20and%20David%20J.%20Fleet%20and%20Deqing%20Sun&entry.1292438233=%20%20Despite%20the%20recent%20progress%2C%20existing%20frame%20interpolation%20methods%20still%0Astruggle%20with%20processing%20extremely%20high%20resolution%20input%20and%20handling%0Achallenging%20cases%20such%20as%20repetitive%20textures%2C%20thin%20objects%2C%20and%20large%20motion.%0ATo%20address%20these%20issues%2C%20we%20introduce%20a%20patch-based%20cascaded%20pixel%20diffusion%0Amodel%20for%20high%20resolution%20frame%20interpolation%2C%20HIFI%2C%20that%20excels%20in%20these%0Ascenarios%20while%20achieving%20competitive%20performance%20on%20standard%20benchmarks.%0ACascades%2C%20which%20generate%20a%20series%20of%20images%20from%20low%20to%20high%20resolution%2C%20can%0Ahelp%20significantly%20with%20large%20or%20complex%20motion%20that%20require%20both%20global%0Acontext%20for%20a%20coarse%20solution%20and%20detailed%20context%20for%20high%20resolution%20output.%0AHowever%2C%20contrary%20to%20prior%20work%20on%20cascaded%20diffusion%20models%20which%20perform%0Adiffusion%20on%20increasingly%20large%20resolutions%2C%20we%20use%20a%20single%20model%20that%20always%0Aperforms%20diffusion%20at%20the%20same%20resolution%20and%20upsamples%20by%20processing%20patches%0Aof%20the%20inputs%20and%20the%20prior%20solution.%20At%20inference%20time%2C%20this%20drastically%0Areduces%20memory%20usage%20and%20allows%20a%20single%20model%2C%20solving%20both%20frame%0Ainterpolation%20%28base%20model%27s%20task%29%20and%20spatial%20up-sampling%2C%20saving%20training%20cost%0Aas%20well.%20HIFI%20excels%20at%20high-resolution%20images%20and%20complex%20repeated%20textures%0Athat%20require%20global%20context%2C%20achieving%20comparable%20or%20state-of-the-art%0Aperformance%20on%20various%20benchmarks%20%28Vimeo%2C%20Xiph%2C%20X-Test%2C%20and%20SEPE-8K%29.%20We%0Afurther%20introduce%20a%20new%20dataset%2C%20LaMoR%2C%20that%20focuses%20on%20particularly%0Achallenging%20cases%2C%20and%20HIFI%20significantly%20outperforms%20other%20baselines.%20Please%0Avisit%20our%20project%20page%20for%20video%20results%3A%20https%3A//hifi-diffusion.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11838v2&entry.124074799=Read"},
{"title": "An OpenMind for 3D medical vision self-supervised learning", "author": "Tassilo Wald and Constantin Ulrich and Jonathan Suprijadi and Sebastian Ziegler and Michal Nohel and Robin Peretzke and Gregor K\u00f6hler and Klaus H. Maier-Hein", "abstract": "  The field of self-supervised learning (SSL) for 3D medical images lacks\nconsistency and standardization. While many methods have been developed, it is\nimpossible to identify the current state-of-the-art, due to i) varying and\nsmall pretraining datasets, ii) varying architectures, and iii) being evaluated\non differing downstream datasets. In this paper, we bring clarity to this field\nand lay the foundation for further method advancements through three key\ncontributions: We a) publish the largest publicly available pre-training\ndataset comprising 114k 3D brain MRI volumes, enabling all practitioners to\npre-train on a large-scale dataset. We b) benchmark existing 3D self-supervised\nlearning methods on this dataset for a state-of-the-art CNN and Transformer\narchitecture, clarifying the state of 3D SSL pre-training. Among many findings,\nwe show that pre-trained methods can exceed a strong from-scratch nnU-Net\nResEnc-L baseline. Lastly, we c) publish the code of our pre-training and\nfine-tuning frameworks and provide the pre-trained models created during the\nbenchmarking process to facilitate rapid adoption and reproduction.\n", "link": "http://arxiv.org/abs/2412.17041v2", "date": "2025-04-18", "relevancy": 2.3101, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20OpenMind%20for%203D%20medical%20vision%20self-supervised%20learning&body=Title%3A%20An%20OpenMind%20for%203D%20medical%20vision%20self-supervised%20learning%0AAuthor%3A%20Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Jonathan%20Suprijadi%20and%20Sebastian%20Ziegler%20and%20Michal%20Nohel%20and%20Robin%20Peretzke%20and%20Gregor%20K%C3%B6hler%20and%20Klaus%20H.%20Maier-Hein%0AAbstract%3A%20%20%20The%20field%20of%20self-supervised%20learning%20%28SSL%29%20for%203D%20medical%20images%20lacks%0Aconsistency%20and%20standardization.%20While%20many%20methods%20have%20been%20developed%2C%20it%20is%0Aimpossible%20to%20identify%20the%20current%20state-of-the-art%2C%20due%20to%20i%29%20varying%20and%0Asmall%20pretraining%20datasets%2C%20ii%29%20varying%20architectures%2C%20and%20iii%29%20being%20evaluated%0Aon%20differing%20downstream%20datasets.%20In%20this%20paper%2C%20we%20bring%20clarity%20to%20this%20field%0Aand%20lay%20the%20foundation%20for%20further%20method%20advancements%20through%20three%20key%0Acontributions%3A%20We%20a%29%20publish%20the%20largest%20publicly%20available%20pre-training%0Adataset%20comprising%20114k%203D%20brain%20MRI%20volumes%2C%20enabling%20all%20practitioners%20to%0Apre-train%20on%20a%20large-scale%20dataset.%20We%20b%29%20benchmark%20existing%203D%20self-supervised%0Alearning%20methods%20on%20this%20dataset%20for%20a%20state-of-the-art%20CNN%20and%20Transformer%0Aarchitecture%2C%20clarifying%20the%20state%20of%203D%20SSL%20pre-training.%20Among%20many%20findings%2C%0Awe%20show%20that%20pre-trained%20methods%20can%20exceed%20a%20strong%20from-scratch%20nnU-Net%0AResEnc-L%20baseline.%20Lastly%2C%20we%20c%29%20publish%20the%20code%20of%20our%20pre-training%20and%0Afine-tuning%20frameworks%20and%20provide%20the%20pre-trained%20models%20created%20during%20the%0Abenchmarking%20process%20to%20facilitate%20rapid%20adoption%20and%20reproduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17041v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520OpenMind%2520for%25203D%2520medical%2520vision%2520self-supervised%2520learning%26entry.906535625%3DTassilo%2520Wald%2520and%2520Constantin%2520Ulrich%2520and%2520Jonathan%2520Suprijadi%2520and%2520Sebastian%2520Ziegler%2520and%2520Michal%2520Nohel%2520and%2520Robin%2520Peretzke%2520and%2520Gregor%2520K%25C3%25B6hler%2520and%2520Klaus%2520H.%2520Maier-Hein%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520self-supervised%2520learning%2520%2528SSL%2529%2520for%25203D%2520medical%2520images%2520lacks%250Aconsistency%2520and%2520standardization.%2520While%2520many%2520methods%2520have%2520been%2520developed%252C%2520it%2520is%250Aimpossible%2520to%2520identify%2520the%2520current%2520state-of-the-art%252C%2520due%2520to%2520i%2529%2520varying%2520and%250Asmall%2520pretraining%2520datasets%252C%2520ii%2529%2520varying%2520architectures%252C%2520and%2520iii%2529%2520being%2520evaluated%250Aon%2520differing%2520downstream%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520bring%2520clarity%2520to%2520this%2520field%250Aand%2520lay%2520the%2520foundation%2520for%2520further%2520method%2520advancements%2520through%2520three%2520key%250Acontributions%253A%2520We%2520a%2529%2520publish%2520the%2520largest%2520publicly%2520available%2520pre-training%250Adataset%2520comprising%2520114k%25203D%2520brain%2520MRI%2520volumes%252C%2520enabling%2520all%2520practitioners%2520to%250Apre-train%2520on%2520a%2520large-scale%2520dataset.%2520We%2520b%2529%2520benchmark%2520existing%25203D%2520self-supervised%250Alearning%2520methods%2520on%2520this%2520dataset%2520for%2520a%2520state-of-the-art%2520CNN%2520and%2520Transformer%250Aarchitecture%252C%2520clarifying%2520the%2520state%2520of%25203D%2520SSL%2520pre-training.%2520Among%2520many%2520findings%252C%250Awe%2520show%2520that%2520pre-trained%2520methods%2520can%2520exceed%2520a%2520strong%2520from-scratch%2520nnU-Net%250AResEnc-L%2520baseline.%2520Lastly%252C%2520we%2520c%2529%2520publish%2520the%2520code%2520of%2520our%2520pre-training%2520and%250Afine-tuning%2520frameworks%2520and%2520provide%2520the%2520pre-trained%2520models%2520created%2520during%2520the%250Abenchmarking%2520process%2520to%2520facilitate%2520rapid%2520adoption%2520and%2520reproduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17041v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20OpenMind%20for%203D%20medical%20vision%20self-supervised%20learning&entry.906535625=Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Jonathan%20Suprijadi%20and%20Sebastian%20Ziegler%20and%20Michal%20Nohel%20and%20Robin%20Peretzke%20and%20Gregor%20K%C3%B6hler%20and%20Klaus%20H.%20Maier-Hein&entry.1292438233=%20%20The%20field%20of%20self-supervised%20learning%20%28SSL%29%20for%203D%20medical%20images%20lacks%0Aconsistency%20and%20standardization.%20While%20many%20methods%20have%20been%20developed%2C%20it%20is%0Aimpossible%20to%20identify%20the%20current%20state-of-the-art%2C%20due%20to%20i%29%20varying%20and%0Asmall%20pretraining%20datasets%2C%20ii%29%20varying%20architectures%2C%20and%20iii%29%20being%20evaluated%0Aon%20differing%20downstream%20datasets.%20In%20this%20paper%2C%20we%20bring%20clarity%20to%20this%20field%0Aand%20lay%20the%20foundation%20for%20further%20method%20advancements%20through%20three%20key%0Acontributions%3A%20We%20a%29%20publish%20the%20largest%20publicly%20available%20pre-training%0Adataset%20comprising%20114k%203D%20brain%20MRI%20volumes%2C%20enabling%20all%20practitioners%20to%0Apre-train%20on%20a%20large-scale%20dataset.%20We%20b%29%20benchmark%20existing%203D%20self-supervised%0Alearning%20methods%20on%20this%20dataset%20for%20a%20state-of-the-art%20CNN%20and%20Transformer%0Aarchitecture%2C%20clarifying%20the%20state%20of%203D%20SSL%20pre-training.%20Among%20many%20findings%2C%0Awe%20show%20that%20pre-trained%20methods%20can%20exceed%20a%20strong%20from-scratch%20nnU-Net%0AResEnc-L%20baseline.%20Lastly%2C%20we%20c%29%20publish%20the%20code%20of%20our%20pre-training%20and%0Afine-tuning%20frameworks%20and%20provide%20the%20pre-trained%20models%20created%20during%20the%0Abenchmarking%20process%20to%20facilitate%20rapid%20adoption%20and%20reproduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17041v2&entry.124074799=Read"},
{"title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions", "author": "Muhammad Usama and Syeda Aisha Asim and Syed Bilal Ali and Syed Talal Wasim and Umair Bin Mansoor", "abstract": "  Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications.\n", "link": "http://arxiv.org/abs/2504.13690v1", "date": "2025-04-18", "relevancy": 2.3045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysing%20the%20Robustness%20of%20Vision-Language-Models%20to%20Common%20Corruptions&body=Title%3A%20Analysing%20the%20Robustness%20of%20Vision-Language-Models%20to%20Common%20Corruptions%0AAuthor%3A%20Muhammad%20Usama%20and%20Syeda%20Aisha%20Asim%20and%20Syed%20Bilal%20Ali%20and%20Syed%20Talal%20Wasim%20and%20Umair%20Bin%20Mansoor%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Aunderstanding%20and%20reasoning%20about%20visual%20and%20textual%20content.%20However%2C%20their%0Arobustness%20to%20common%20image%20corruptions%20remains%20under-explored.%20In%20this%20work%2C%20we%0Apresent%20the%20first%20comprehensive%20analysis%20of%20VLM%20robustness%20across%2019%20corruption%0Atypes%20from%20the%20ImageNet-C%20benchmark%2C%20spanning%20four%20categories%3A%20noise%2C%20blur%2C%0Aweather%2C%20and%20digital%20distortions.%20We%20introduce%20two%20new%20benchmarks%2C%20TextVQA-C%0Aand%20GQA-C%2C%20to%20systematically%20evaluate%20how%20corruptions%20affect%20scene%20text%0Aunderstanding%20and%20object-based%20reasoning%2C%20respectively.%20Our%20analysis%20reveals%0Athat%20transformer-based%20VLMs%20exhibit%20distinct%20vulnerability%20patterns%20across%0Atasks%3A%20text%20recognition%20deteriorates%20most%20severely%20under%20blur%20and%20snow%0Acorruptions%2C%20while%20object%20reasoning%20shows%20higher%20sensitivity%20to%20corruptions%0Asuch%20as%20frost%20and%20impulse%20noise.%20We%20connect%20these%20observations%20to%20the%0Afrequency-domain%20characteristics%20of%20different%20corruptions%2C%20revealing%20how%0Atransformers%27%20inherent%20bias%20toward%20low-frequency%20processing%20explains%20their%0Adifferential%20robustness%20patterns.%20Our%20findings%20provide%20valuable%20insights%20for%0Adeveloping%20more%20corruption-robust%20vision-language%20models%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysing%2520the%2520Robustness%2520of%2520Vision-Language-Models%2520to%2520Common%2520Corruptions%26entry.906535625%3DMuhammad%2520Usama%2520and%2520Syeda%2520Aisha%2520Asim%2520and%2520Syed%2520Bilal%2520Ali%2520and%2520Syed%2520Talal%2520Wasim%2520and%2520Umair%2520Bin%2520Mansoor%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Aunderstanding%2520and%2520reasoning%2520about%2520visual%2520and%2520textual%2520content.%2520However%252C%2520their%250Arobustness%2520to%2520common%2520image%2520corruptions%2520remains%2520under-explored.%2520In%2520this%2520work%252C%2520we%250Apresent%2520the%2520first%2520comprehensive%2520analysis%2520of%2520VLM%2520robustness%2520across%252019%2520corruption%250Atypes%2520from%2520the%2520ImageNet-C%2520benchmark%252C%2520spanning%2520four%2520categories%253A%2520noise%252C%2520blur%252C%250Aweather%252C%2520and%2520digital%2520distortions.%2520We%2520introduce%2520two%2520new%2520benchmarks%252C%2520TextVQA-C%250Aand%2520GQA-C%252C%2520to%2520systematically%2520evaluate%2520how%2520corruptions%2520affect%2520scene%2520text%250Aunderstanding%2520and%2520object-based%2520reasoning%252C%2520respectively.%2520Our%2520analysis%2520reveals%250Athat%2520transformer-based%2520VLMs%2520exhibit%2520distinct%2520vulnerability%2520patterns%2520across%250Atasks%253A%2520text%2520recognition%2520deteriorates%2520most%2520severely%2520under%2520blur%2520and%2520snow%250Acorruptions%252C%2520while%2520object%2520reasoning%2520shows%2520higher%2520sensitivity%2520to%2520corruptions%250Asuch%2520as%2520frost%2520and%2520impulse%2520noise.%2520We%2520connect%2520these%2520observations%2520to%2520the%250Afrequency-domain%2520characteristics%2520of%2520different%2520corruptions%252C%2520revealing%2520how%250Atransformers%2527%2520inherent%2520bias%2520toward%2520low-frequency%2520processing%2520explains%2520their%250Adifferential%2520robustness%2520patterns.%2520Our%2520findings%2520provide%2520valuable%2520insights%2520for%250Adeveloping%2520more%2520corruption-robust%2520vision-language%2520models%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysing%20the%20Robustness%20of%20Vision-Language-Models%20to%20Common%20Corruptions&entry.906535625=Muhammad%20Usama%20and%20Syeda%20Aisha%20Asim%20and%20Syed%20Bilal%20Ali%20and%20Syed%20Talal%20Wasim%20and%20Umair%20Bin%20Mansoor&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Aunderstanding%20and%20reasoning%20about%20visual%20and%20textual%20content.%20However%2C%20their%0Arobustness%20to%20common%20image%20corruptions%20remains%20under-explored.%20In%20this%20work%2C%20we%0Apresent%20the%20first%20comprehensive%20analysis%20of%20VLM%20robustness%20across%2019%20corruption%0Atypes%20from%20the%20ImageNet-C%20benchmark%2C%20spanning%20four%20categories%3A%20noise%2C%20blur%2C%0Aweather%2C%20and%20digital%20distortions.%20We%20introduce%20two%20new%20benchmarks%2C%20TextVQA-C%0Aand%20GQA-C%2C%20to%20systematically%20evaluate%20how%20corruptions%20affect%20scene%20text%0Aunderstanding%20and%20object-based%20reasoning%2C%20respectively.%20Our%20analysis%20reveals%0Athat%20transformer-based%20VLMs%20exhibit%20distinct%20vulnerability%20patterns%20across%0Atasks%3A%20text%20recognition%20deteriorates%20most%20severely%20under%20blur%20and%20snow%0Acorruptions%2C%20while%20object%20reasoning%20shows%20higher%20sensitivity%20to%20corruptions%0Asuch%20as%20frost%20and%20impulse%20noise.%20We%20connect%20these%20observations%20to%20the%0Afrequency-domain%20characteristics%20of%20different%20corruptions%2C%20revealing%20how%0Atransformers%27%20inherent%20bias%20toward%20low-frequency%20processing%20explains%20their%0Adifferential%20robustness%20patterns.%20Our%20findings%20provide%20valuable%20insights%20for%0Adeveloping%20more%20corruption-robust%20vision-language%20models%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13690v1&entry.124074799=Read"},
{"title": "Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D\n  Semantic Occupancy Prediction", "author": "Dubing Chen and Huan Zheng and Jin Fang and Xingping Dong and Xianfei Li and Wenlong Liao and Tao He and Pai Peng and Jianbing Shen", "abstract": "  We present GDFusion, a temporal fusion method for vision-based 3D semantic\noccupancy prediction (VisionOcc). GDFusion opens up the underexplored aspects\nof temporal fusion within the VisionOcc framework, focusing on both temporal\ncues and fusion strategies. It systematically examines the entire VisionOcc\npipeline, identifying three fundamental yet previously overlooked temporal\ncues: scene-level consistency, motion calibration, and geometric\ncomplementation. These cues capture diverse facets of temporal evolution and\nmake distinct contributions across various modules in the VisionOcc framework.\nTo effectively fuse temporal signals across heterogeneous representations, we\npropose a novel fusion strategy by reinterpreting the formulation of vanilla\nRNNs. This reinterpretation leverages gradient descent on features to unify the\nintegration of diverse temporal information, seamlessly embedding the proposed\ntemporal cues into the network. Extensive experiments on nuScenes demonstrate\nthat GDFusion significantly outperforms established baselines. Notably, on\nOcc3D benchmark, it achieves 1.4\\%-4.8\\% mIoU improvements and reduces memory\nconsumption by 27\\%-72\\%.\n", "link": "http://arxiv.org/abs/2504.12959v2", "date": "2025-04-18", "relevancy": 2.3025, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.579}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5774}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Temporal%20Fusion%20with%20a%20Unified%20Gradient%20Descent%20View%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction&body=Title%3A%20Rethinking%20Temporal%20Fusion%20with%20a%20Unified%20Gradient%20Descent%20View%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction%0AAuthor%3A%20Dubing%20Chen%20and%20Huan%20Zheng%20and%20Jin%20Fang%20and%20Xingping%20Dong%20and%20Xianfei%20Li%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng%20and%20Jianbing%20Shen%0AAbstract%3A%20%20%20We%20present%20GDFusion%2C%20a%20temporal%20fusion%20method%20for%20vision-based%203D%20semantic%0Aoccupancy%20prediction%20%28VisionOcc%29.%20GDFusion%20opens%20up%20the%20underexplored%20aspects%0Aof%20temporal%20fusion%20within%20the%20VisionOcc%20framework%2C%20focusing%20on%20both%20temporal%0Acues%20and%20fusion%20strategies.%20It%20systematically%20examines%20the%20entire%20VisionOcc%0Apipeline%2C%20identifying%20three%20fundamental%20yet%20previously%20overlooked%20temporal%0Acues%3A%20scene-level%20consistency%2C%20motion%20calibration%2C%20and%20geometric%0Acomplementation.%20These%20cues%20capture%20diverse%20facets%20of%20temporal%20evolution%20and%0Amake%20distinct%20contributions%20across%20various%20modules%20in%20the%20VisionOcc%20framework.%0ATo%20effectively%20fuse%20temporal%20signals%20across%20heterogeneous%20representations%2C%20we%0Apropose%20a%20novel%20fusion%20strategy%20by%20reinterpreting%20the%20formulation%20of%20vanilla%0ARNNs.%20This%20reinterpretation%20leverages%20gradient%20descent%20on%20features%20to%20unify%20the%0Aintegration%20of%20diverse%20temporal%20information%2C%20seamlessly%20embedding%20the%20proposed%0Atemporal%20cues%20into%20the%20network.%20Extensive%20experiments%20on%20nuScenes%20demonstrate%0Athat%20GDFusion%20significantly%20outperforms%20established%20baselines.%20Notably%2C%20on%0AOcc3D%20benchmark%2C%20it%20achieves%201.4%5C%25-4.8%5C%25%20mIoU%20improvements%20and%20reduces%20memory%0Aconsumption%20by%2027%5C%25-72%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Temporal%2520Fusion%2520with%2520a%2520Unified%2520Gradient%2520Descent%2520View%2520for%25203D%250A%2520%2520Semantic%2520Occupancy%2520Prediction%26entry.906535625%3DDubing%2520Chen%2520and%2520Huan%2520Zheng%2520and%2520Jin%2520Fang%2520and%2520Xingping%2520Dong%2520and%2520Xianfei%2520Li%2520and%2520Wenlong%2520Liao%2520and%2520Tao%2520He%2520and%2520Pai%2520Peng%2520and%2520Jianbing%2520Shen%26entry.1292438233%3D%2520%2520We%2520present%2520GDFusion%252C%2520a%2520temporal%2520fusion%2520method%2520for%2520vision-based%25203D%2520semantic%250Aoccupancy%2520prediction%2520%2528VisionOcc%2529.%2520GDFusion%2520opens%2520up%2520the%2520underexplored%2520aspects%250Aof%2520temporal%2520fusion%2520within%2520the%2520VisionOcc%2520framework%252C%2520focusing%2520on%2520both%2520temporal%250Acues%2520and%2520fusion%2520strategies.%2520It%2520systematically%2520examines%2520the%2520entire%2520VisionOcc%250Apipeline%252C%2520identifying%2520three%2520fundamental%2520yet%2520previously%2520overlooked%2520temporal%250Acues%253A%2520scene-level%2520consistency%252C%2520motion%2520calibration%252C%2520and%2520geometric%250Acomplementation.%2520These%2520cues%2520capture%2520diverse%2520facets%2520of%2520temporal%2520evolution%2520and%250Amake%2520distinct%2520contributions%2520across%2520various%2520modules%2520in%2520the%2520VisionOcc%2520framework.%250ATo%2520effectively%2520fuse%2520temporal%2520signals%2520across%2520heterogeneous%2520representations%252C%2520we%250Apropose%2520a%2520novel%2520fusion%2520strategy%2520by%2520reinterpreting%2520the%2520formulation%2520of%2520vanilla%250ARNNs.%2520This%2520reinterpretation%2520leverages%2520gradient%2520descent%2520on%2520features%2520to%2520unify%2520the%250Aintegration%2520of%2520diverse%2520temporal%2520information%252C%2520seamlessly%2520embedding%2520the%2520proposed%250Atemporal%2520cues%2520into%2520the%2520network.%2520Extensive%2520experiments%2520on%2520nuScenes%2520demonstrate%250Athat%2520GDFusion%2520significantly%2520outperforms%2520established%2520baselines.%2520Notably%252C%2520on%250AOcc3D%2520benchmark%252C%2520it%2520achieves%25201.4%255C%2525-4.8%255C%2525%2520mIoU%2520improvements%2520and%2520reduces%2520memory%250Aconsumption%2520by%252027%255C%2525-72%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Temporal%20Fusion%20with%20a%20Unified%20Gradient%20Descent%20View%20for%203D%0A%20%20Semantic%20Occupancy%20Prediction&entry.906535625=Dubing%20Chen%20and%20Huan%20Zheng%20and%20Jin%20Fang%20and%20Xingping%20Dong%20and%20Xianfei%20Li%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng%20and%20Jianbing%20Shen&entry.1292438233=%20%20We%20present%20GDFusion%2C%20a%20temporal%20fusion%20method%20for%20vision-based%203D%20semantic%0Aoccupancy%20prediction%20%28VisionOcc%29.%20GDFusion%20opens%20up%20the%20underexplored%20aspects%0Aof%20temporal%20fusion%20within%20the%20VisionOcc%20framework%2C%20focusing%20on%20both%20temporal%0Acues%20and%20fusion%20strategies.%20It%20systematically%20examines%20the%20entire%20VisionOcc%0Apipeline%2C%20identifying%20three%20fundamental%20yet%20previously%20overlooked%20temporal%0Acues%3A%20scene-level%20consistency%2C%20motion%20calibration%2C%20and%20geometric%0Acomplementation.%20These%20cues%20capture%20diverse%20facets%20of%20temporal%20evolution%20and%0Amake%20distinct%20contributions%20across%20various%20modules%20in%20the%20VisionOcc%20framework.%0ATo%20effectively%20fuse%20temporal%20signals%20across%20heterogeneous%20representations%2C%20we%0Apropose%20a%20novel%20fusion%20strategy%20by%20reinterpreting%20the%20formulation%20of%20vanilla%0ARNNs.%20This%20reinterpretation%20leverages%20gradient%20descent%20on%20features%20to%20unify%20the%0Aintegration%20of%20diverse%20temporal%20information%2C%20seamlessly%20embedding%20the%20proposed%0Atemporal%20cues%20into%20the%20network.%20Extensive%20experiments%20on%20nuScenes%20demonstrate%0Athat%20GDFusion%20significantly%20outperforms%20established%20baselines.%20Notably%2C%20on%0AOcc3D%20benchmark%2C%20it%20achieves%201.4%5C%25-4.8%5C%25%20mIoU%20improvements%20and%20reduces%20memory%0Aconsumption%20by%2027%5C%25-72%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12959v2&entry.124074799=Read"},
{"title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI\n  Agents", "author": "Run Luo and Lu Wang and Wanwei He and Xiaobo Xia", "abstract": "  Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.\n", "link": "http://arxiv.org/abs/2504.10458v3", "date": "2025-04-18", "relevancy": 2.2711, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5938}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.582}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-R1%20%3A%20A%20Generalist%20R1-Style%20Vision-Language%20Action%20Model%20For%20GUI%0A%20%20Agents&body=Title%3A%20GUI-R1%20%3A%20A%20Generalist%20R1-Style%20Vision-Language%20Action%20Model%20For%20GUI%0A%20%20Agents%0AAuthor%3A%20Run%20Luo%20and%20Lu%20Wang%20and%20Wanwei%20He%20and%20Xiaobo%20Xia%0AAbstract%3A%20%20%20Existing%20efforts%20in%20building%20Graphical%20User%20Interface%20%28GUI%29%20agents%20largely%0Arely%20on%20the%20training%20paradigm%20of%20supervised%20fine-tuning%20on%20Large%0AVision-Language%20Models%20%28LVLMs%29.%20However%2C%20this%20approach%20not%20only%20demands%0Aextensive%20amounts%20of%20training%20data%20but%20also%20struggles%20to%20effectively%20understand%0AGUI%20screenshots%20and%20generalize%20to%20unseen%20interfaces.%20The%20issue%20significantly%0Alimits%20its%20application%20in%20real-world%20scenarios%2C%20especially%20for%20high-level%0Atasks.%20Inspired%20by%20Reinforcement%20Fine-Tuning%20%28RFT%29%20in%20large%20reasoning%20models%0A%28e.g.%2C%20DeepSeek-R1%29%2C%20which%20efficiently%20enhances%20the%20problem-solving%0Acapabilities%20of%20large%20language%20models%20in%20real-world%20settings%2C%20we%20propose%20%5Cname%2C%0Athe%20first%20reinforcement%20learning%20framework%20designed%20to%20enhance%20the%20GUI%0Acapabilities%20of%20LVLMs%20in%20high-level%20real-world%20task%20scenarios%2C%20through%20unified%0Aaction%20space%20rule%20modeling.%20By%20leveraging%20a%20small%20amount%20of%20carefully%20curated%0Ahigh-quality%20data%20across%20multiple%20platforms%20%28including%20Windows%2C%20Linux%2C%20MacOS%2C%0AAndroid%2C%20and%20Web%29%20and%20employing%20policy%20optimization%20algorithms%20such%20as%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20to%20update%20the%20model%2C%20%5Cname%20achieves%0Asuperior%20performance%20using%20only%200.02%5C%25%20of%20the%20data%20%283K%20vs.%2013M%29%20compared%20to%0Aprevious%20state-of-the-art%20methods%20like%20OS-Atlas%20across%20eight%20benchmarks%0Aspanning%20three%20different%20platforms%20%28mobile%2C%20desktop%2C%20and%20web%29.%20These%20results%0Ademonstrate%20the%20immense%20potential%20of%20reinforcement%20learning%20based%20on%20unified%0Aaction%20space%20rule%20modeling%20in%20improving%20the%20execution%20capabilities%20of%20LVLMs%20for%0Areal-world%20GUI%20agent%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10458v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-R1%2520%253A%2520A%2520Generalist%2520R1-Style%2520Vision-Language%2520Action%2520Model%2520For%2520GUI%250A%2520%2520Agents%26entry.906535625%3DRun%2520Luo%2520and%2520Lu%2520Wang%2520and%2520Wanwei%2520He%2520and%2520Xiaobo%2520Xia%26entry.1292438233%3D%2520%2520Existing%2520efforts%2520in%2520building%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520agents%2520largely%250Arely%2520on%2520the%2520training%2520paradigm%2520of%2520supervised%2520fine-tuning%2520on%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529.%2520However%252C%2520this%2520approach%2520not%2520only%2520demands%250Aextensive%2520amounts%2520of%2520training%2520data%2520but%2520also%2520struggles%2520to%2520effectively%2520understand%250AGUI%2520screenshots%2520and%2520generalize%2520to%2520unseen%2520interfaces.%2520The%2520issue%2520significantly%250Alimits%2520its%2520application%2520in%2520real-world%2520scenarios%252C%2520especially%2520for%2520high-level%250Atasks.%2520Inspired%2520by%2520Reinforcement%2520Fine-Tuning%2520%2528RFT%2529%2520in%2520large%2520reasoning%2520models%250A%2528e.g.%252C%2520DeepSeek-R1%2529%252C%2520which%2520efficiently%2520enhances%2520the%2520problem-solving%250Acapabilities%2520of%2520large%2520language%2520models%2520in%2520real-world%2520settings%252C%2520we%2520propose%2520%255Cname%252C%250Athe%2520first%2520reinforcement%2520learning%2520framework%2520designed%2520to%2520enhance%2520the%2520GUI%250Acapabilities%2520of%2520LVLMs%2520in%2520high-level%2520real-world%2520task%2520scenarios%252C%2520through%2520unified%250Aaction%2520space%2520rule%2520modeling.%2520By%2520leveraging%2520a%2520small%2520amount%2520of%2520carefully%2520curated%250Ahigh-quality%2520data%2520across%2520multiple%2520platforms%2520%2528including%2520Windows%252C%2520Linux%252C%2520MacOS%252C%250AAndroid%252C%2520and%2520Web%2529%2520and%2520employing%2520policy%2520optimization%2520algorithms%2520such%2520as%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520update%2520the%2520model%252C%2520%255Cname%2520achieves%250Asuperior%2520performance%2520using%2520only%25200.02%255C%2525%2520of%2520the%2520data%2520%25283K%2520vs.%252013M%2529%2520compared%2520to%250Aprevious%2520state-of-the-art%2520methods%2520like%2520OS-Atlas%2520across%2520eight%2520benchmarks%250Aspanning%2520three%2520different%2520platforms%2520%2528mobile%252C%2520desktop%252C%2520and%2520web%2529.%2520These%2520results%250Ademonstrate%2520the%2520immense%2520potential%2520of%2520reinforcement%2520learning%2520based%2520on%2520unified%250Aaction%2520space%2520rule%2520modeling%2520in%2520improving%2520the%2520execution%2520capabilities%2520of%2520LVLMs%2520for%250Areal-world%2520GUI%2520agent%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10458v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-R1%20%3A%20A%20Generalist%20R1-Style%20Vision-Language%20Action%20Model%20For%20GUI%0A%20%20Agents&entry.906535625=Run%20Luo%20and%20Lu%20Wang%20and%20Wanwei%20He%20and%20Xiaobo%20Xia&entry.1292438233=%20%20Existing%20efforts%20in%20building%20Graphical%20User%20Interface%20%28GUI%29%20agents%20largely%0Arely%20on%20the%20training%20paradigm%20of%20supervised%20fine-tuning%20on%20Large%0AVision-Language%20Models%20%28LVLMs%29.%20However%2C%20this%20approach%20not%20only%20demands%0Aextensive%20amounts%20of%20training%20data%20but%20also%20struggles%20to%20effectively%20understand%0AGUI%20screenshots%20and%20generalize%20to%20unseen%20interfaces.%20The%20issue%20significantly%0Alimits%20its%20application%20in%20real-world%20scenarios%2C%20especially%20for%20high-level%0Atasks.%20Inspired%20by%20Reinforcement%20Fine-Tuning%20%28RFT%29%20in%20large%20reasoning%20models%0A%28e.g.%2C%20DeepSeek-R1%29%2C%20which%20efficiently%20enhances%20the%20problem-solving%0Acapabilities%20of%20large%20language%20models%20in%20real-world%20settings%2C%20we%20propose%20%5Cname%2C%0Athe%20first%20reinforcement%20learning%20framework%20designed%20to%20enhance%20the%20GUI%0Acapabilities%20of%20LVLMs%20in%20high-level%20real-world%20task%20scenarios%2C%20through%20unified%0Aaction%20space%20rule%20modeling.%20By%20leveraging%20a%20small%20amount%20of%20carefully%20curated%0Ahigh-quality%20data%20across%20multiple%20platforms%20%28including%20Windows%2C%20Linux%2C%20MacOS%2C%0AAndroid%2C%20and%20Web%29%20and%20employing%20policy%20optimization%20algorithms%20such%20as%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20to%20update%20the%20model%2C%20%5Cname%20achieves%0Asuperior%20performance%20using%20only%200.02%5C%25%20of%20the%20data%20%283K%20vs.%2013M%29%20compared%20to%0Aprevious%20state-of-the-art%20methods%20like%20OS-Atlas%20across%20eight%20benchmarks%0Aspanning%20three%20different%20platforms%20%28mobile%2C%20desktop%2C%20and%20web%29.%20These%20results%0Ademonstrate%20the%20immense%20potential%20of%20reinforcement%20learning%20based%20on%20unified%0Aaction%20space%20rule%20modeling%20in%20improving%20the%20execution%20capabilities%20of%20LVLMs%20for%0Areal-world%20GUI%20agent%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10458v3&entry.124074799=Read"},
{"title": "EyecareGPT: Boosting Comprehensive Ophthalmology Understanding with\n  Tailored Dataset, Benchmark and Model", "author": "Sijing Li and Tianwei Lin and Lingshuai Lin and Wenqiao Zhang and Jiang Liu and Xiaoda Yang and Juncheng Li and Yucheng He and Xiaohui Song and Jun Xiao and Yueting Zhuang and Beng Chin Ooi", "abstract": "  Medical Large Vision-Language Models (Med-LVLMs) demonstrate significant\npotential in healthcare, but their reliance on general medical data and\ncoarse-grained global visual understanding limits them in intelligent\nophthalmic diagnosis. Currently, intelligent ophthalmic diagnosis faces three\nmajor challenges: (i) Data. The lack of deeply annotated, high-quality,\nmulti-modal ophthalmic visual instruction data; (ii) Benchmark. The absence of\na comprehensive and systematic benchmark for evaluating diagnostic performance;\n(iii) Model. The difficulty of adapting holistic visual architectures to\nfine-grained, region-specific ophthalmic lesion identification. In this paper,\nwe propose the Eyecare Kit, which systematically tackles the aforementioned\nthree key challenges with the tailored dataset, benchmark and model: First, we\nconstruct a multi-agent data engine with real-life ophthalmology data to\nproduce Eyecare-100K, a high-quality ophthalmic visual instruction dataset.\nSubsequently, we design Eyecare-Bench, a benchmark that comprehensively\nevaluates the overall performance of LVLMs on intelligent ophthalmic diagnosis\ntasks across multiple dimensions. Finally, we develop the EyecareGPT, optimized\nfor fine-grained ophthalmic visual understanding thoroughly, which incorporates\nan adaptive resolution mechanism and a layer-wise dense connector. Extensive\nexperimental results indicate that the EyecareGPT achieves state-of-the-art\nperformance in a range of ophthalmic tasks, underscoring its significant\npotential for the advancement of open research in intelligent ophthalmic\ndiagnosis. Our project is available at https://github.com/DCDmllm/EyecareGPT.\n", "link": "http://arxiv.org/abs/2504.13650v1", "date": "2025-04-18", "relevancy": 2.2388, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EyecareGPT%3A%20Boosting%20Comprehensive%20Ophthalmology%20Understanding%20with%0A%20%20Tailored%20Dataset%2C%20Benchmark%20and%20Model&body=Title%3A%20EyecareGPT%3A%20Boosting%20Comprehensive%20Ophthalmology%20Understanding%20with%0A%20%20Tailored%20Dataset%2C%20Benchmark%20and%20Model%0AAuthor%3A%20Sijing%20Li%20and%20Tianwei%20Lin%20and%20Lingshuai%20Lin%20and%20Wenqiao%20Zhang%20and%20Jiang%20Liu%20and%20Xiaoda%20Yang%20and%20Juncheng%20Li%20and%20Yucheng%20He%20and%20Xiaohui%20Song%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi%0AAbstract%3A%20%20%20Medical%20Large%20Vision-Language%20Models%20%28Med-LVLMs%29%20demonstrate%20significant%0Apotential%20in%20healthcare%2C%20but%20their%20reliance%20on%20general%20medical%20data%20and%0Acoarse-grained%20global%20visual%20understanding%20limits%20them%20in%20intelligent%0Aophthalmic%20diagnosis.%20Currently%2C%20intelligent%20ophthalmic%20diagnosis%20faces%20three%0Amajor%20challenges%3A%20%28i%29%20Data.%20The%20lack%20of%20deeply%20annotated%2C%20high-quality%2C%0Amulti-modal%20ophthalmic%20visual%20instruction%20data%3B%20%28ii%29%20Benchmark.%20The%20absence%20of%0Aa%20comprehensive%20and%20systematic%20benchmark%20for%20evaluating%20diagnostic%20performance%3B%0A%28iii%29%20Model.%20The%20difficulty%20of%20adapting%20holistic%20visual%20architectures%20to%0Afine-grained%2C%20region-specific%20ophthalmic%20lesion%20identification.%20In%20this%20paper%2C%0Awe%20propose%20the%20Eyecare%20Kit%2C%20which%20systematically%20tackles%20the%20aforementioned%0Athree%20key%20challenges%20with%20the%20tailored%20dataset%2C%20benchmark%20and%20model%3A%20First%2C%20we%0Aconstruct%20a%20multi-agent%20data%20engine%20with%20real-life%20ophthalmology%20data%20to%0Aproduce%20Eyecare-100K%2C%20a%20high-quality%20ophthalmic%20visual%20instruction%20dataset.%0ASubsequently%2C%20we%20design%20Eyecare-Bench%2C%20a%20benchmark%20that%20comprehensively%0Aevaluates%20the%20overall%20performance%20of%20LVLMs%20on%20intelligent%20ophthalmic%20diagnosis%0Atasks%20across%20multiple%20dimensions.%20Finally%2C%20we%20develop%20the%20EyecareGPT%2C%20optimized%0Afor%20fine-grained%20ophthalmic%20visual%20understanding%20thoroughly%2C%20which%20incorporates%0Aan%20adaptive%20resolution%20mechanism%20and%20a%20layer-wise%20dense%20connector.%20Extensive%0Aexperimental%20results%20indicate%20that%20the%20EyecareGPT%20achieves%20state-of-the-art%0Aperformance%20in%20a%20range%20of%20ophthalmic%20tasks%2C%20underscoring%20its%20significant%0Apotential%20for%20the%20advancement%20of%20open%20research%20in%20intelligent%20ophthalmic%0Adiagnosis.%20Our%20project%20is%20available%20at%20https%3A//github.com/DCDmllm/EyecareGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyecareGPT%253A%2520Boosting%2520Comprehensive%2520Ophthalmology%2520Understanding%2520with%250A%2520%2520Tailored%2520Dataset%252C%2520Benchmark%2520and%2520Model%26entry.906535625%3DSijing%2520Li%2520and%2520Tianwei%2520Lin%2520and%2520Lingshuai%2520Lin%2520and%2520Wenqiao%2520Zhang%2520and%2520Jiang%2520Liu%2520and%2520Xiaoda%2520Yang%2520and%2520Juncheng%2520Li%2520and%2520Yucheng%2520He%2520and%2520Xiaohui%2520Song%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%2520and%2520Beng%2520Chin%2520Ooi%26entry.1292438233%3D%2520%2520Medical%2520Large%2520Vision-Language%2520Models%2520%2528Med-LVLMs%2529%2520demonstrate%2520significant%250Apotential%2520in%2520healthcare%252C%2520but%2520their%2520reliance%2520on%2520general%2520medical%2520data%2520and%250Acoarse-grained%2520global%2520visual%2520understanding%2520limits%2520them%2520in%2520intelligent%250Aophthalmic%2520diagnosis.%2520Currently%252C%2520intelligent%2520ophthalmic%2520diagnosis%2520faces%2520three%250Amajor%2520challenges%253A%2520%2528i%2529%2520Data.%2520The%2520lack%2520of%2520deeply%2520annotated%252C%2520high-quality%252C%250Amulti-modal%2520ophthalmic%2520visual%2520instruction%2520data%253B%2520%2528ii%2529%2520Benchmark.%2520The%2520absence%2520of%250Aa%2520comprehensive%2520and%2520systematic%2520benchmark%2520for%2520evaluating%2520diagnostic%2520performance%253B%250A%2528iii%2529%2520Model.%2520The%2520difficulty%2520of%2520adapting%2520holistic%2520visual%2520architectures%2520to%250Afine-grained%252C%2520region-specific%2520ophthalmic%2520lesion%2520identification.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%2520Eyecare%2520Kit%252C%2520which%2520systematically%2520tackles%2520the%2520aforementioned%250Athree%2520key%2520challenges%2520with%2520the%2520tailored%2520dataset%252C%2520benchmark%2520and%2520model%253A%2520First%252C%2520we%250Aconstruct%2520a%2520multi-agent%2520data%2520engine%2520with%2520real-life%2520ophthalmology%2520data%2520to%250Aproduce%2520Eyecare-100K%252C%2520a%2520high-quality%2520ophthalmic%2520visual%2520instruction%2520dataset.%250ASubsequently%252C%2520we%2520design%2520Eyecare-Bench%252C%2520a%2520benchmark%2520that%2520comprehensively%250Aevaluates%2520the%2520overall%2520performance%2520of%2520LVLMs%2520on%2520intelligent%2520ophthalmic%2520diagnosis%250Atasks%2520across%2520multiple%2520dimensions.%2520Finally%252C%2520we%2520develop%2520the%2520EyecareGPT%252C%2520optimized%250Afor%2520fine-grained%2520ophthalmic%2520visual%2520understanding%2520thoroughly%252C%2520which%2520incorporates%250Aan%2520adaptive%2520resolution%2520mechanism%2520and%2520a%2520layer-wise%2520dense%2520connector.%2520Extensive%250Aexperimental%2520results%2520indicate%2520that%2520the%2520EyecareGPT%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520a%2520range%2520of%2520ophthalmic%2520tasks%252C%2520underscoring%2520its%2520significant%250Apotential%2520for%2520the%2520advancement%2520of%2520open%2520research%2520in%2520intelligent%2520ophthalmic%250Adiagnosis.%2520Our%2520project%2520is%2520available%2520at%2520https%253A//github.com/DCDmllm/EyecareGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EyecareGPT%3A%20Boosting%20Comprehensive%20Ophthalmology%20Understanding%20with%0A%20%20Tailored%20Dataset%2C%20Benchmark%20and%20Model&entry.906535625=Sijing%20Li%20and%20Tianwei%20Lin%20and%20Lingshuai%20Lin%20and%20Wenqiao%20Zhang%20and%20Jiang%20Liu%20and%20Xiaoda%20Yang%20and%20Juncheng%20Li%20and%20Yucheng%20He%20and%20Xiaohui%20Song%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%20and%20Beng%20Chin%20Ooi&entry.1292438233=%20%20Medical%20Large%20Vision-Language%20Models%20%28Med-LVLMs%29%20demonstrate%20significant%0Apotential%20in%20healthcare%2C%20but%20their%20reliance%20on%20general%20medical%20data%20and%0Acoarse-grained%20global%20visual%20understanding%20limits%20them%20in%20intelligent%0Aophthalmic%20diagnosis.%20Currently%2C%20intelligent%20ophthalmic%20diagnosis%20faces%20three%0Amajor%20challenges%3A%20%28i%29%20Data.%20The%20lack%20of%20deeply%20annotated%2C%20high-quality%2C%0Amulti-modal%20ophthalmic%20visual%20instruction%20data%3B%20%28ii%29%20Benchmark.%20The%20absence%20of%0Aa%20comprehensive%20and%20systematic%20benchmark%20for%20evaluating%20diagnostic%20performance%3B%0A%28iii%29%20Model.%20The%20difficulty%20of%20adapting%20holistic%20visual%20architectures%20to%0Afine-grained%2C%20region-specific%20ophthalmic%20lesion%20identification.%20In%20this%20paper%2C%0Awe%20propose%20the%20Eyecare%20Kit%2C%20which%20systematically%20tackles%20the%20aforementioned%0Athree%20key%20challenges%20with%20the%20tailored%20dataset%2C%20benchmark%20and%20model%3A%20First%2C%20we%0Aconstruct%20a%20multi-agent%20data%20engine%20with%20real-life%20ophthalmology%20data%20to%0Aproduce%20Eyecare-100K%2C%20a%20high-quality%20ophthalmic%20visual%20instruction%20dataset.%0ASubsequently%2C%20we%20design%20Eyecare-Bench%2C%20a%20benchmark%20that%20comprehensively%0Aevaluates%20the%20overall%20performance%20of%20LVLMs%20on%20intelligent%20ophthalmic%20diagnosis%0Atasks%20across%20multiple%20dimensions.%20Finally%2C%20we%20develop%20the%20EyecareGPT%2C%20optimized%0Afor%20fine-grained%20ophthalmic%20visual%20understanding%20thoroughly%2C%20which%20incorporates%0Aan%20adaptive%20resolution%20mechanism%20and%20a%20layer-wise%20dense%20connector.%20Extensive%0Aexperimental%20results%20indicate%20that%20the%20EyecareGPT%20achieves%20state-of-the-art%0Aperformance%20in%20a%20range%20of%20ophthalmic%20tasks%2C%20underscoring%20its%20significant%0Apotential%20for%20the%20advancement%20of%20open%20research%20in%20intelligent%20ophthalmic%0Adiagnosis.%20Our%20project%20is%20available%20at%20https%3A//github.com/DCDmllm/EyecareGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13650v1&entry.124074799=Read"},
{"title": "Visual Intention Grounding for Egocentric Assistants", "author": "Pengzhan Sun and Junbin Xiao and Tze Ho Elden Tse and Yicong Li and Arjun Akula and Angela Yao", "abstract": "  Visual grounding associates textual descriptions with objects in an image.\nConventional methods target third-person image inputs and named object queries.\nIn applications such as AI assistants, the perspective shifts -- inputs are\negocentric, and objects may be referred to implicitly through needs and\nintentions. To bridge this gap, we introduce EgoIntention, the first dataset\nfor egocentric visual intention grounding. EgoIntention challenges multimodal\nLLMs to 1) understand and ignore unintended contextual objects and 2) reason\nabout uncommon object functionalities. Benchmark results show that current\nmodels misidentify context objects and lack affordance understanding in\negocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it\nenables hybrid training with normal descriptions and egocentric intentions with\na chained intention reasoning and object grounding mechanism. RoG significantly\noutperforms naive finetuning and hybrid training on EgoIntention, while\nmaintaining or slightly improving naive description grounding. This advancement\nenables unified visual grounding for egocentric and exocentric visual inputs\nwhile handling explicit object queries and implicit human intentions.\n", "link": "http://arxiv.org/abs/2504.13621v1", "date": "2025-04-18", "relevancy": 2.2368, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5847}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Intention%20Grounding%20for%20Egocentric%20Assistants&body=Title%3A%20Visual%20Intention%20Grounding%20for%20Egocentric%20Assistants%0AAuthor%3A%20Pengzhan%20Sun%20and%20Junbin%20Xiao%20and%20Tze%20Ho%20Elden%20Tse%20and%20Yicong%20Li%20and%20Arjun%20Akula%20and%20Angela%20Yao%0AAbstract%3A%20%20%20Visual%20grounding%20associates%20textual%20descriptions%20with%20objects%20in%20an%20image.%0AConventional%20methods%20target%20third-person%20image%20inputs%20and%20named%20object%20queries.%0AIn%20applications%20such%20as%20AI%20assistants%2C%20the%20perspective%20shifts%20--%20inputs%20are%0Aegocentric%2C%20and%20objects%20may%20be%20referred%20to%20implicitly%20through%20needs%20and%0Aintentions.%20To%20bridge%20this%20gap%2C%20we%20introduce%20EgoIntention%2C%20the%20first%20dataset%0Afor%20egocentric%20visual%20intention%20grounding.%20EgoIntention%20challenges%20multimodal%0ALLMs%20to%201%29%20understand%20and%20ignore%20unintended%20contextual%20objects%20and%202%29%20reason%0Aabout%20uncommon%20object%20functionalities.%20Benchmark%20results%20show%20that%20current%0Amodels%20misidentify%20context%20objects%20and%20lack%20affordance%20understanding%20in%0Aegocentric%20views.%20We%20also%20propose%20Reason-to-Ground%20%28RoG%29%20instruction%20tuning%3B%20it%0Aenables%20hybrid%20training%20with%20normal%20descriptions%20and%20egocentric%20intentions%20with%0Aa%20chained%20intention%20reasoning%20and%20object%20grounding%20mechanism.%20RoG%20significantly%0Aoutperforms%20naive%20finetuning%20and%20hybrid%20training%20on%20EgoIntention%2C%20while%0Amaintaining%20or%20slightly%20improving%20naive%20description%20grounding.%20This%20advancement%0Aenables%20unified%20visual%20grounding%20for%20egocentric%20and%20exocentric%20visual%20inputs%0Awhile%20handling%20explicit%20object%20queries%20and%20implicit%20human%20intentions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Intention%2520Grounding%2520for%2520Egocentric%2520Assistants%26entry.906535625%3DPengzhan%2520Sun%2520and%2520Junbin%2520Xiao%2520and%2520Tze%2520Ho%2520Elden%2520Tse%2520and%2520Yicong%2520Li%2520and%2520Arjun%2520Akula%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520Visual%2520grounding%2520associates%2520textual%2520descriptions%2520with%2520objects%2520in%2520an%2520image.%250AConventional%2520methods%2520target%2520third-person%2520image%2520inputs%2520and%2520named%2520object%2520queries.%250AIn%2520applications%2520such%2520as%2520AI%2520assistants%252C%2520the%2520perspective%2520shifts%2520--%2520inputs%2520are%250Aegocentric%252C%2520and%2520objects%2520may%2520be%2520referred%2520to%2520implicitly%2520through%2520needs%2520and%250Aintentions.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520EgoIntention%252C%2520the%2520first%2520dataset%250Afor%2520egocentric%2520visual%2520intention%2520grounding.%2520EgoIntention%2520challenges%2520multimodal%250ALLMs%2520to%25201%2529%2520understand%2520and%2520ignore%2520unintended%2520contextual%2520objects%2520and%25202%2529%2520reason%250Aabout%2520uncommon%2520object%2520functionalities.%2520Benchmark%2520results%2520show%2520that%2520current%250Amodels%2520misidentify%2520context%2520objects%2520and%2520lack%2520affordance%2520understanding%2520in%250Aegocentric%2520views.%2520We%2520also%2520propose%2520Reason-to-Ground%2520%2528RoG%2529%2520instruction%2520tuning%253B%2520it%250Aenables%2520hybrid%2520training%2520with%2520normal%2520descriptions%2520and%2520egocentric%2520intentions%2520with%250Aa%2520chained%2520intention%2520reasoning%2520and%2520object%2520grounding%2520mechanism.%2520RoG%2520significantly%250Aoutperforms%2520naive%2520finetuning%2520and%2520hybrid%2520training%2520on%2520EgoIntention%252C%2520while%250Amaintaining%2520or%2520slightly%2520improving%2520naive%2520description%2520grounding.%2520This%2520advancement%250Aenables%2520unified%2520visual%2520grounding%2520for%2520egocentric%2520and%2520exocentric%2520visual%2520inputs%250Awhile%2520handling%2520explicit%2520object%2520queries%2520and%2520implicit%2520human%2520intentions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Intention%20Grounding%20for%20Egocentric%20Assistants&entry.906535625=Pengzhan%20Sun%20and%20Junbin%20Xiao%20and%20Tze%20Ho%20Elden%20Tse%20and%20Yicong%20Li%20and%20Arjun%20Akula%20and%20Angela%20Yao&entry.1292438233=%20%20Visual%20grounding%20associates%20textual%20descriptions%20with%20objects%20in%20an%20image.%0AConventional%20methods%20target%20third-person%20image%20inputs%20and%20named%20object%20queries.%0AIn%20applications%20such%20as%20AI%20assistants%2C%20the%20perspective%20shifts%20--%20inputs%20are%0Aegocentric%2C%20and%20objects%20may%20be%20referred%20to%20implicitly%20through%20needs%20and%0Aintentions.%20To%20bridge%20this%20gap%2C%20we%20introduce%20EgoIntention%2C%20the%20first%20dataset%0Afor%20egocentric%20visual%20intention%20grounding.%20EgoIntention%20challenges%20multimodal%0ALLMs%20to%201%29%20understand%20and%20ignore%20unintended%20contextual%20objects%20and%202%29%20reason%0Aabout%20uncommon%20object%20functionalities.%20Benchmark%20results%20show%20that%20current%0Amodels%20misidentify%20context%20objects%20and%20lack%20affordance%20understanding%20in%0Aegocentric%20views.%20We%20also%20propose%20Reason-to-Ground%20%28RoG%29%20instruction%20tuning%3B%20it%0Aenables%20hybrid%20training%20with%20normal%20descriptions%20and%20egocentric%20intentions%20with%0Aa%20chained%20intention%20reasoning%20and%20object%20grounding%20mechanism.%20RoG%20significantly%0Aoutperforms%20naive%20finetuning%20and%20hybrid%20training%20on%20EgoIntention%2C%20while%0Amaintaining%20or%20slightly%20improving%20naive%20description%20grounding.%20This%20advancement%0Aenables%20unified%20visual%20grounding%20for%20egocentric%20and%20exocentric%20visual%20inputs%0Awhile%20handling%20explicit%20object%20queries%20and%20implicit%20human%20intentions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13621v1&entry.124074799=Read"},
{"title": "Compile Scene Graphs with Reinforcement Learning", "author": "Zuyao Chen and Jinlin Wu and Zhen Lei and Marc Pollefeys and Chang Wen Chen", "abstract": "  Next token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. Given the structured\nnature of scene graphs, we design a graph-centric reward function that\nintegrates node-level rewards, edge-level rewards, and a format consistency\nreward. Our experiments demonstrate that rule-based RL substantially enhances\nmodel performance in the SGG task, achieving a zero failure rate--unlike\nsupervised fine-tuning (SFT), which struggles to generalize effectively. Our\ncode is available at https://github.com/gpt4vision/R1-SGG.\n", "link": "http://arxiv.org/abs/2504.13617v1", "date": "2025-04-18", "relevancy": 2.2213, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compile%20Scene%20Graphs%20with%20Reinforcement%20Learning&body=Title%3A%20Compile%20Scene%20Graphs%20with%20Reinforcement%20Learning%0AAuthor%3A%20Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Marc%20Pollefeys%20and%20Chang%20Wen%20Chen%0AAbstract%3A%20%20%20Next%20token%20prediction%20is%20the%20fundamental%20principle%20for%20training%20large%0Alanguage%20models%20%28LLMs%29%2C%20and%20reinforcement%20learning%20%28RL%29%20further%20enhances%20their%0Areasoning%20performance.%20As%20an%20effective%20way%20to%20model%20language%2C%20image%2C%20video%2C%20and%0Aother%20modalities%2C%20the%20use%20of%20LLMs%20for%20end-to-end%20extraction%20of%20structured%0Avisual%20representations%2C%20such%20as%20scene%20graphs%2C%20remains%20underexplored.%20It%0Arequires%20the%20model%20to%20accurately%20produce%20a%20set%20of%20objects%20and%20relationship%0Atriplets%2C%20rather%20than%20generating%20text%20token%20by%20token.%20To%20achieve%20this%2C%20we%0Aintroduce%20R1-SGG%2C%20a%20multimodal%20LLM%20%28M-LLM%29%20initially%20trained%20via%20supervised%0Afine-tuning%20%28SFT%29%20on%20the%20scene%20graph%20dataset%20and%20subsequently%20refined%20using%0Areinforcement%20learning%20to%20enhance%20its%20ability%20to%20generate%20scene%20graphs%20in%20an%0Aend-to-end%20manner.%20The%20SFT%20follows%20a%20conventional%20prompt-response%20paradigm%2C%0Awhile%20RL%20requires%20the%20design%20of%20effective%20reward%20signals.%20Given%20the%20structured%0Anature%20of%20scene%20graphs%2C%20we%20design%20a%20graph-centric%20reward%20function%20that%0Aintegrates%20node-level%20rewards%2C%20edge-level%20rewards%2C%20and%20a%20format%20consistency%0Areward.%20Our%20experiments%20demonstrate%20that%20rule-based%20RL%20substantially%20enhances%0Amodel%20performance%20in%20the%20SGG%20task%2C%20achieving%20a%20zero%20failure%20rate--unlike%0Asupervised%20fine-tuning%20%28SFT%29%2C%20which%20struggles%20to%20generalize%20effectively.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/gpt4vision/R1-SGG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompile%2520Scene%2520Graphs%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DZuyao%2520Chen%2520and%2520Jinlin%2520Wu%2520and%2520Zhen%2520Lei%2520and%2520Marc%2520Pollefeys%2520and%2520Chang%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520Next%2520token%2520prediction%2520is%2520the%2520fundamental%2520principle%2520for%2520training%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520and%2520reinforcement%2520learning%2520%2528RL%2529%2520further%2520enhances%2520their%250Areasoning%2520performance.%2520As%2520an%2520effective%2520way%2520to%2520model%2520language%252C%2520image%252C%2520video%252C%2520and%250Aother%2520modalities%252C%2520the%2520use%2520of%2520LLMs%2520for%2520end-to-end%2520extraction%2520of%2520structured%250Avisual%2520representations%252C%2520such%2520as%2520scene%2520graphs%252C%2520remains%2520underexplored.%2520It%250Arequires%2520the%2520model%2520to%2520accurately%2520produce%2520a%2520set%2520of%2520objects%2520and%2520relationship%250Atriplets%252C%2520rather%2520than%2520generating%2520text%2520token%2520by%2520token.%2520To%2520achieve%2520this%252C%2520we%250Aintroduce%2520R1-SGG%252C%2520a%2520multimodal%2520LLM%2520%2528M-LLM%2529%2520initially%2520trained%2520via%2520supervised%250Afine-tuning%2520%2528SFT%2529%2520on%2520the%2520scene%2520graph%2520dataset%2520and%2520subsequently%2520refined%2520using%250Areinforcement%2520learning%2520to%2520enhance%2520its%2520ability%2520to%2520generate%2520scene%2520graphs%2520in%2520an%250Aend-to-end%2520manner.%2520The%2520SFT%2520follows%2520a%2520conventional%2520prompt-response%2520paradigm%252C%250Awhile%2520RL%2520requires%2520the%2520design%2520of%2520effective%2520reward%2520signals.%2520Given%2520the%2520structured%250Anature%2520of%2520scene%2520graphs%252C%2520we%2520design%2520a%2520graph-centric%2520reward%2520function%2520that%250Aintegrates%2520node-level%2520rewards%252C%2520edge-level%2520rewards%252C%2520and%2520a%2520format%2520consistency%250Areward.%2520Our%2520experiments%2520demonstrate%2520that%2520rule-based%2520RL%2520substantially%2520enhances%250Amodel%2520performance%2520in%2520the%2520SGG%2520task%252C%2520achieving%2520a%2520zero%2520failure%2520rate--unlike%250Asupervised%2520fine-tuning%2520%2528SFT%2529%252C%2520which%2520struggles%2520to%2520generalize%2520effectively.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/gpt4vision/R1-SGG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compile%20Scene%20Graphs%20with%20Reinforcement%20Learning&entry.906535625=Zuyao%20Chen%20and%20Jinlin%20Wu%20and%20Zhen%20Lei%20and%20Marc%20Pollefeys%20and%20Chang%20Wen%20Chen&entry.1292438233=%20%20Next%20token%20prediction%20is%20the%20fundamental%20principle%20for%20training%20large%0Alanguage%20models%20%28LLMs%29%2C%20and%20reinforcement%20learning%20%28RL%29%20further%20enhances%20their%0Areasoning%20performance.%20As%20an%20effective%20way%20to%20model%20language%2C%20image%2C%20video%2C%20and%0Aother%20modalities%2C%20the%20use%20of%20LLMs%20for%20end-to-end%20extraction%20of%20structured%0Avisual%20representations%2C%20such%20as%20scene%20graphs%2C%20remains%20underexplored.%20It%0Arequires%20the%20model%20to%20accurately%20produce%20a%20set%20of%20objects%20and%20relationship%0Atriplets%2C%20rather%20than%20generating%20text%20token%20by%20token.%20To%20achieve%20this%2C%20we%0Aintroduce%20R1-SGG%2C%20a%20multimodal%20LLM%20%28M-LLM%29%20initially%20trained%20via%20supervised%0Afine-tuning%20%28SFT%29%20on%20the%20scene%20graph%20dataset%20and%20subsequently%20refined%20using%0Areinforcement%20learning%20to%20enhance%20its%20ability%20to%20generate%20scene%20graphs%20in%20an%0Aend-to-end%20manner.%20The%20SFT%20follows%20a%20conventional%20prompt-response%20paradigm%2C%0Awhile%20RL%20requires%20the%20design%20of%20effective%20reward%20signals.%20Given%20the%20structured%0Anature%20of%20scene%20graphs%2C%20we%20design%20a%20graph-centric%20reward%20function%20that%0Aintegrates%20node-level%20rewards%2C%20edge-level%20rewards%2C%20and%20a%20format%20consistency%0Areward.%20Our%20experiments%20demonstrate%20that%20rule-based%20RL%20substantially%20enhances%0Amodel%20performance%20in%20the%20SGG%20task%2C%20achieving%20a%20zero%20failure%20rate--unlike%0Asupervised%20fine-tuning%20%28SFT%29%2C%20which%20struggles%20to%20generalize%20effectively.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/gpt4vision/R1-SGG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13617v1&entry.124074799=Read"},
{"title": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability", "author": "Zhengtong Xu and Zichen Miao and Qiang Qiu and Zhe Zhang and Yu She", "abstract": "  Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. Visuomotor\npolicies enhanced by DiffOG generate smoother, constraint-compliant action\ntrajectories in a more interpretable way. DiffOG exhibits strong generalization\ncapabilities and high flexibility. We evaluated DiffOG across 11 simulated\ntasks and 2 real-world tasks. The results demonstrate that DiffOG significantly\nenhances the trajectory quality of visuomotor policies while having minimal\nimpact on policy performance, outperforming trajectory processing baselines\nsuch as greedy constraint clipping and penalty-based trajectory optimization.\nFurthermore, DiffOG achieves superior performance compared to existing\nconstrained visuomotor policy.\n", "link": "http://arxiv.org/abs/2504.13807v1", "date": "2025-04-18", "relevancy": 2.2174, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.57}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5432}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffOG%3A%20Differentiable%20Policy%20Trajectory%20Optimization%20with%0A%20%20Generalizability&body=Title%3A%20DiffOG%3A%20Differentiable%20Policy%20Trajectory%20Optimization%20with%0A%20%20Generalizability%0AAuthor%3A%20Zhengtong%20Xu%20and%20Zichen%20Miao%20and%20Qiang%20Qiu%20and%20Zhe%20Zhang%20and%20Yu%20She%0AAbstract%3A%20%20%20Imitation%20learning-based%20visuomotor%20policies%20excel%20at%20manipulation%20tasks%20but%0Aoften%20produce%20suboptimal%20action%20trajectories%20compared%20to%20model-based%20methods.%0ADirectly%20mapping%20camera%20data%20to%20actions%20via%20neural%20networks%20can%20result%20in%20jerky%0Amotions%20and%20difficulties%20in%20meeting%20critical%20constraints%2C%20compromising%20safety%0Aand%20robustness%20in%20real-world%20deployment.%20For%20tasks%20that%20require%20high%20robustness%0Aor%20strict%20adherence%20to%20constraints%2C%20ensuring%20trajectory%20quality%20is%20crucial.%0AHowever%2C%20the%20lack%20of%20interpretability%20in%20neural%20networks%20makes%20it%20challenging%0Ato%20generate%20constraint-compliant%20actions%20in%20a%20controlled%20manner.%20This%20paper%0Aintroduces%20differentiable%20policy%20trajectory%20optimization%20with%20generalizability%0A%28DiffOG%29%2C%20a%20learning-based%20trajectory%20optimization%20framework%20designed%20to%0Aenhance%20visuomotor%20policies.%20By%20leveraging%20the%20proposed%20differentiable%0Aformulation%20of%20trajectory%20optimization%20with%20transformer%2C%20DiffOG%20seamlessly%0Aintegrates%20policies%20with%20a%20generalizable%20optimization%20layer.%20Visuomotor%0Apolicies%20enhanced%20by%20DiffOG%20generate%20smoother%2C%20constraint-compliant%20action%0Atrajectories%20in%20a%20more%20interpretable%20way.%20DiffOG%20exhibits%20strong%20generalization%0Acapabilities%20and%20high%20flexibility.%20We%20evaluated%20DiffOG%20across%2011%20simulated%0Atasks%20and%202%20real-world%20tasks.%20The%20results%20demonstrate%20that%20DiffOG%20significantly%0Aenhances%20the%20trajectory%20quality%20of%20visuomotor%20policies%20while%20having%20minimal%0Aimpact%20on%20policy%20performance%2C%20outperforming%20trajectory%20processing%20baselines%0Asuch%20as%20greedy%20constraint%20clipping%20and%20penalty-based%20trajectory%20optimization.%0AFurthermore%2C%20DiffOG%20achieves%20superior%20performance%20compared%20to%20existing%0Aconstrained%20visuomotor%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffOG%253A%2520Differentiable%2520Policy%2520Trajectory%2520Optimization%2520with%250A%2520%2520Generalizability%26entry.906535625%3DZhengtong%2520Xu%2520and%2520Zichen%2520Miao%2520and%2520Qiang%2520Qiu%2520and%2520Zhe%2520Zhang%2520and%2520Yu%2520She%26entry.1292438233%3D%2520%2520Imitation%2520learning-based%2520visuomotor%2520policies%2520excel%2520at%2520manipulation%2520tasks%2520but%250Aoften%2520produce%2520suboptimal%2520action%2520trajectories%2520compared%2520to%2520model-based%2520methods.%250ADirectly%2520mapping%2520camera%2520data%2520to%2520actions%2520via%2520neural%2520networks%2520can%2520result%2520in%2520jerky%250Amotions%2520and%2520difficulties%2520in%2520meeting%2520critical%2520constraints%252C%2520compromising%2520safety%250Aand%2520robustness%2520in%2520real-world%2520deployment.%2520For%2520tasks%2520that%2520require%2520high%2520robustness%250Aor%2520strict%2520adherence%2520to%2520constraints%252C%2520ensuring%2520trajectory%2520quality%2520is%2520crucial.%250AHowever%252C%2520the%2520lack%2520of%2520interpretability%2520in%2520neural%2520networks%2520makes%2520it%2520challenging%250Ato%2520generate%2520constraint-compliant%2520actions%2520in%2520a%2520controlled%2520manner.%2520This%2520paper%250Aintroduces%2520differentiable%2520policy%2520trajectory%2520optimization%2520with%2520generalizability%250A%2528DiffOG%2529%252C%2520a%2520learning-based%2520trajectory%2520optimization%2520framework%2520designed%2520to%250Aenhance%2520visuomotor%2520policies.%2520By%2520leveraging%2520the%2520proposed%2520differentiable%250Aformulation%2520of%2520trajectory%2520optimization%2520with%2520transformer%252C%2520DiffOG%2520seamlessly%250Aintegrates%2520policies%2520with%2520a%2520generalizable%2520optimization%2520layer.%2520Visuomotor%250Apolicies%2520enhanced%2520by%2520DiffOG%2520generate%2520smoother%252C%2520constraint-compliant%2520action%250Atrajectories%2520in%2520a%2520more%2520interpretable%2520way.%2520DiffOG%2520exhibits%2520strong%2520generalization%250Acapabilities%2520and%2520high%2520flexibility.%2520We%2520evaluated%2520DiffOG%2520across%252011%2520simulated%250Atasks%2520and%25202%2520real-world%2520tasks.%2520The%2520results%2520demonstrate%2520that%2520DiffOG%2520significantly%250Aenhances%2520the%2520trajectory%2520quality%2520of%2520visuomotor%2520policies%2520while%2520having%2520minimal%250Aimpact%2520on%2520policy%2520performance%252C%2520outperforming%2520trajectory%2520processing%2520baselines%250Asuch%2520as%2520greedy%2520constraint%2520clipping%2520and%2520penalty-based%2520trajectory%2520optimization.%250AFurthermore%252C%2520DiffOG%2520achieves%2520superior%2520performance%2520compared%2520to%2520existing%250Aconstrained%2520visuomotor%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffOG%3A%20Differentiable%20Policy%20Trajectory%20Optimization%20with%0A%20%20Generalizability&entry.906535625=Zhengtong%20Xu%20and%20Zichen%20Miao%20and%20Qiang%20Qiu%20and%20Zhe%20Zhang%20and%20Yu%20She&entry.1292438233=%20%20Imitation%20learning-based%20visuomotor%20policies%20excel%20at%20manipulation%20tasks%20but%0Aoften%20produce%20suboptimal%20action%20trajectories%20compared%20to%20model-based%20methods.%0ADirectly%20mapping%20camera%20data%20to%20actions%20via%20neural%20networks%20can%20result%20in%20jerky%0Amotions%20and%20difficulties%20in%20meeting%20critical%20constraints%2C%20compromising%20safety%0Aand%20robustness%20in%20real-world%20deployment.%20For%20tasks%20that%20require%20high%20robustness%0Aor%20strict%20adherence%20to%20constraints%2C%20ensuring%20trajectory%20quality%20is%20crucial.%0AHowever%2C%20the%20lack%20of%20interpretability%20in%20neural%20networks%20makes%20it%20challenging%0Ato%20generate%20constraint-compliant%20actions%20in%20a%20controlled%20manner.%20This%20paper%0Aintroduces%20differentiable%20policy%20trajectory%20optimization%20with%20generalizability%0A%28DiffOG%29%2C%20a%20learning-based%20trajectory%20optimization%20framework%20designed%20to%0Aenhance%20visuomotor%20policies.%20By%20leveraging%20the%20proposed%20differentiable%0Aformulation%20of%20trajectory%20optimization%20with%20transformer%2C%20DiffOG%20seamlessly%0Aintegrates%20policies%20with%20a%20generalizable%20optimization%20layer.%20Visuomotor%0Apolicies%20enhanced%20by%20DiffOG%20generate%20smoother%2C%20constraint-compliant%20action%0Atrajectories%20in%20a%20more%20interpretable%20way.%20DiffOG%20exhibits%20strong%20generalization%0Acapabilities%20and%20high%20flexibility.%20We%20evaluated%20DiffOG%20across%2011%20simulated%0Atasks%20and%202%20real-world%20tasks.%20The%20results%20demonstrate%20that%20DiffOG%20significantly%0Aenhances%20the%20trajectory%20quality%20of%20visuomotor%20policies%20while%20having%20minimal%0Aimpact%20on%20policy%20performance%2C%20outperforming%20trajectory%20processing%20baselines%0Asuch%20as%20greedy%20constraint%20clipping%20and%20penalty-based%20trajectory%20optimization.%0AFurthermore%2C%20DiffOG%20achieves%20superior%20performance%20compared%20to%20existing%0Aconstrained%20visuomotor%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13807v1&entry.124074799=Read"},
{"title": "LeOCLR: Leveraging Original Images for Contrastive Learning of Visual\n  Representations", "author": "Mohammad Alkhalefi and Georgios Leontidis and Mingjun Zhong", "abstract": "  Contrastive instance discrimination methods outperform supervised learning in\ndownstream tasks such as image classification and object detection. However,\nthese methods rely heavily on data augmentation during representation learning,\nwhich can lead to suboptimal results if not implemented carefully. A common\naugmentation technique in contrastive learning is random cropping followed by\nresizing. This can degrade the quality of representation learning when the two\nrandom crops contain distinct semantic content. To tackle this issue, we\nintroduce LeOCLR (Leveraging Original Images for Contrastive Learning of Visual\nRepresentations), a framework that employs a novel instance discrimination\napproach and an adapted loss function. This method prevents the loss of\nimportant semantic features caused by mapping different object parts during\nrepresentation learning. Our experiments demonstrate that LeOCLR consistently\nimproves representation learning across various datasets, outperforming\nbaseline models. For instance, LeOCLR surpasses MoCo-v2 by 5.1% on ImageNet-1K\nin linear evaluation and outperforms several other methods on transfer learning\nand object detection tasks.\n", "link": "http://arxiv.org/abs/2403.06813v4", "date": "2025-04-18", "relevancy": 2.2167, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations&body=Title%3A%20LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations%0AAuthor%3A%20Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong%0AAbstract%3A%20%20%20Contrastive%20instance%20discrimination%20methods%20outperform%20supervised%20learning%20in%0Adownstream%20tasks%20such%20as%20image%20classification%20and%20object%20detection.%20However%2C%0Athese%20methods%20rely%20heavily%20on%20data%20augmentation%20during%20representation%20learning%2C%0Awhich%20can%20lead%20to%20suboptimal%20results%20if%20not%20implemented%20carefully.%20A%20common%0Aaugmentation%20technique%20in%20contrastive%20learning%20is%20random%20cropping%20followed%20by%0Aresizing.%20This%20can%20degrade%20the%20quality%20of%20representation%20learning%20when%20the%20two%0Arandom%20crops%20contain%20distinct%20semantic%20content.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20LeOCLR%20%28Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0ARepresentations%29%2C%20a%20framework%20that%20employs%20a%20novel%20instance%20discrimination%0Aapproach%20and%20an%20adapted%20loss%20function.%20This%20method%20prevents%20the%20loss%20of%0Aimportant%20semantic%20features%20caused%20by%20mapping%20different%20object%20parts%20during%0Arepresentation%20learning.%20Our%20experiments%20demonstrate%20that%20LeOCLR%20consistently%0Aimproves%20representation%20learning%20across%20various%20datasets%2C%20outperforming%0Abaseline%20models.%20For%20instance%2C%20LeOCLR%20surpasses%20MoCo-v2%20by%205.1%25%20on%20ImageNet-1K%0Ain%20linear%20evaluation%20and%20outperforms%20several%20other%20methods%20on%20transfer%20learning%0Aand%20object%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06813v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeOCLR%253A%2520Leveraging%2520Original%2520Images%2520for%2520Contrastive%2520Learning%2520of%2520Visual%250A%2520%2520Representations%26entry.906535625%3DMohammad%2520Alkhalefi%2520and%2520Georgios%2520Leontidis%2520and%2520Mingjun%2520Zhong%26entry.1292438233%3D%2520%2520Contrastive%2520instance%2520discrimination%2520methods%2520outperform%2520supervised%2520learning%2520in%250Adownstream%2520tasks%2520such%2520as%2520image%2520classification%2520and%2520object%2520detection.%2520However%252C%250Athese%2520methods%2520rely%2520heavily%2520on%2520data%2520augmentation%2520during%2520representation%2520learning%252C%250Awhich%2520can%2520lead%2520to%2520suboptimal%2520results%2520if%2520not%2520implemented%2520carefully.%2520A%2520common%250Aaugmentation%2520technique%2520in%2520contrastive%2520learning%2520is%2520random%2520cropping%2520followed%2520by%250Aresizing.%2520This%2520can%2520degrade%2520the%2520quality%2520of%2520representation%2520learning%2520when%2520the%2520two%250Arandom%2520crops%2520contain%2520distinct%2520semantic%2520content.%2520To%2520tackle%2520this%2520issue%252C%2520we%250Aintroduce%2520LeOCLR%2520%2528Leveraging%2520Original%2520Images%2520for%2520Contrastive%2520Learning%2520of%2520Visual%250ARepresentations%2529%252C%2520a%2520framework%2520that%2520employs%2520a%2520novel%2520instance%2520discrimination%250Aapproach%2520and%2520an%2520adapted%2520loss%2520function.%2520This%2520method%2520prevents%2520the%2520loss%2520of%250Aimportant%2520semantic%2520features%2520caused%2520by%2520mapping%2520different%2520object%2520parts%2520during%250Arepresentation%2520learning.%2520Our%2520experiments%2520demonstrate%2520that%2520LeOCLR%2520consistently%250Aimproves%2520representation%2520learning%2520across%2520various%2520datasets%252C%2520outperforming%250Abaseline%2520models.%2520For%2520instance%252C%2520LeOCLR%2520surpasses%2520MoCo-v2%2520by%25205.1%2525%2520on%2520ImageNet-1K%250Ain%2520linear%2520evaluation%2520and%2520outperforms%2520several%2520other%2520methods%2520on%2520transfer%2520learning%250Aand%2520object%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06813v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeOCLR%3A%20Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0A%20%20Representations&entry.906535625=Mohammad%20Alkhalefi%20and%20Georgios%20Leontidis%20and%20Mingjun%20Zhong&entry.1292438233=%20%20Contrastive%20instance%20discrimination%20methods%20outperform%20supervised%20learning%20in%0Adownstream%20tasks%20such%20as%20image%20classification%20and%20object%20detection.%20However%2C%0Athese%20methods%20rely%20heavily%20on%20data%20augmentation%20during%20representation%20learning%2C%0Awhich%20can%20lead%20to%20suboptimal%20results%20if%20not%20implemented%20carefully.%20A%20common%0Aaugmentation%20technique%20in%20contrastive%20learning%20is%20random%20cropping%20followed%20by%0Aresizing.%20This%20can%20degrade%20the%20quality%20of%20representation%20learning%20when%20the%20two%0Arandom%20crops%20contain%20distinct%20semantic%20content.%20To%20tackle%20this%20issue%2C%20we%0Aintroduce%20LeOCLR%20%28Leveraging%20Original%20Images%20for%20Contrastive%20Learning%20of%20Visual%0ARepresentations%29%2C%20a%20framework%20that%20employs%20a%20novel%20instance%20discrimination%0Aapproach%20and%20an%20adapted%20loss%20function.%20This%20method%20prevents%20the%20loss%20of%0Aimportant%20semantic%20features%20caused%20by%20mapping%20different%20object%20parts%20during%0Arepresentation%20learning.%20Our%20experiments%20demonstrate%20that%20LeOCLR%20consistently%0Aimproves%20representation%20learning%20across%20various%20datasets%2C%20outperforming%0Abaseline%20models.%20For%20instance%2C%20LeOCLR%20surpasses%20MoCo-v2%20by%205.1%25%20on%20ImageNet-1K%0Ain%20linear%20evaluation%20and%20outperforms%20several%20other%20methods%20on%20transfer%20learning%0Aand%20object%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06813v4&entry.124074799=Read"},
{"title": "DAM-Net: Domain Adaptation Network with Micro-Labeled Fine-Tuning for\n  Change Detection", "author": "Hongjia Chen and Xin Xu and Fangling Pu", "abstract": "  Change detection (CD) in remote sensing imagery plays a crucial role in\nvarious applications such as urban planning, damage assessment, and resource\nmanagement. While deep learning approaches have significantly advanced CD\nperformance, current methods suffer from poor domain adaptability, requiring\nextensive labeled data for retraining when applied to new scenarios. This\nlimitation severely restricts their practical applications across different\ndatasets. In this work, we propose DAM-Net: a Domain Adaptation Network with\nMicro-Labeled Fine-Tuning for CD. Our network introduces adversarial domain\nadaptation to CD for, utilizing a specially designed segmentation-discriminator\nand alternating training strategy to enable effective transfer between domains.\nAdditionally, we propose a novel Micro-Labeled Fine-Tuning approach that\nstrategically selects and labels a minimal amount of samples (less than 1%) to\nenhance domain adaptation. The network incorporates a Multi-Temporal\nTransformer for feature fusion and optimized backbone structure based on\nprevious research. Experiments conducted on the LEVIR-CD and WHU-CD datasets\ndemonstrate that DAM-Net significantly outperforms existing domain adaptation\nmethods, achieving comparable performance to semi-supervised approaches that\nrequire 10% labeled data while using only 0.3% labeled samples. Our approach\nsignificantly advances cross-dataset CD applications and provides a new\nparadigm for efficient domain adaptation in remote sensing. The source code of\nDAM-Net will be made publicly available upon publication.\n", "link": "http://arxiv.org/abs/2504.13748v1", "date": "2025-04-18", "relevancy": 2.2155, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5643}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5559}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAM-Net%3A%20Domain%20Adaptation%20Network%20with%20Micro-Labeled%20Fine-Tuning%20for%0A%20%20Change%20Detection&body=Title%3A%20DAM-Net%3A%20Domain%20Adaptation%20Network%20with%20Micro-Labeled%20Fine-Tuning%20for%0A%20%20Change%20Detection%0AAuthor%3A%20Hongjia%20Chen%20and%20Xin%20Xu%20and%20Fangling%20Pu%0AAbstract%3A%20%20%20Change%20detection%20%28CD%29%20in%20remote%20sensing%20imagery%20plays%20a%20crucial%20role%20in%0Avarious%20applications%20such%20as%20urban%20planning%2C%20damage%20assessment%2C%20and%20resource%0Amanagement.%20While%20deep%20learning%20approaches%20have%20significantly%20advanced%20CD%0Aperformance%2C%20current%20methods%20suffer%20from%20poor%20domain%20adaptability%2C%20requiring%0Aextensive%20labeled%20data%20for%20retraining%20when%20applied%20to%20new%20scenarios.%20This%0Alimitation%20severely%20restricts%20their%20practical%20applications%20across%20different%0Adatasets.%20In%20this%20work%2C%20we%20propose%20DAM-Net%3A%20a%20Domain%20Adaptation%20Network%20with%0AMicro-Labeled%20Fine-Tuning%20for%20CD.%20Our%20network%20introduces%20adversarial%20domain%0Aadaptation%20to%20CD%20for%2C%20utilizing%20a%20specially%20designed%20segmentation-discriminator%0Aand%20alternating%20training%20strategy%20to%20enable%20effective%20transfer%20between%20domains.%0AAdditionally%2C%20we%20propose%20a%20novel%20Micro-Labeled%20Fine-Tuning%20approach%20that%0Astrategically%20selects%20and%20labels%20a%20minimal%20amount%20of%20samples%20%28less%20than%201%25%29%20to%0Aenhance%20domain%20adaptation.%20The%20network%20incorporates%20a%20Multi-Temporal%0ATransformer%20for%20feature%20fusion%20and%20optimized%20backbone%20structure%20based%20on%0Aprevious%20research.%20Experiments%20conducted%20on%20the%20LEVIR-CD%20and%20WHU-CD%20datasets%0Ademonstrate%20that%20DAM-Net%20significantly%20outperforms%20existing%20domain%20adaptation%0Amethods%2C%20achieving%20comparable%20performance%20to%20semi-supervised%20approaches%20that%0Arequire%2010%25%20labeled%20data%20while%20using%20only%200.3%25%20labeled%20samples.%20Our%20approach%0Asignificantly%20advances%20cross-dataset%20CD%20applications%20and%20provides%20a%20new%0Aparadigm%20for%20efficient%20domain%20adaptation%20in%20remote%20sensing.%20The%20source%20code%20of%0ADAM-Net%20will%20be%20made%20publicly%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAM-Net%253A%2520Domain%2520Adaptation%2520Network%2520with%2520Micro-Labeled%2520Fine-Tuning%2520for%250A%2520%2520Change%2520Detection%26entry.906535625%3DHongjia%2520Chen%2520and%2520Xin%2520Xu%2520and%2520Fangling%2520Pu%26entry.1292438233%3D%2520%2520Change%2520detection%2520%2528CD%2529%2520in%2520remote%2520sensing%2520imagery%2520plays%2520a%2520crucial%2520role%2520in%250Avarious%2520applications%2520such%2520as%2520urban%2520planning%252C%2520damage%2520assessment%252C%2520and%2520resource%250Amanagement.%2520While%2520deep%2520learning%2520approaches%2520have%2520significantly%2520advanced%2520CD%250Aperformance%252C%2520current%2520methods%2520suffer%2520from%2520poor%2520domain%2520adaptability%252C%2520requiring%250Aextensive%2520labeled%2520data%2520for%2520retraining%2520when%2520applied%2520to%2520new%2520scenarios.%2520This%250Alimitation%2520severely%2520restricts%2520their%2520practical%2520applications%2520across%2520different%250Adatasets.%2520In%2520this%2520work%252C%2520we%2520propose%2520DAM-Net%253A%2520a%2520Domain%2520Adaptation%2520Network%2520with%250AMicro-Labeled%2520Fine-Tuning%2520for%2520CD.%2520Our%2520network%2520introduces%2520adversarial%2520domain%250Aadaptation%2520to%2520CD%2520for%252C%2520utilizing%2520a%2520specially%2520designed%2520segmentation-discriminator%250Aand%2520alternating%2520training%2520strategy%2520to%2520enable%2520effective%2520transfer%2520between%2520domains.%250AAdditionally%252C%2520we%2520propose%2520a%2520novel%2520Micro-Labeled%2520Fine-Tuning%2520approach%2520that%250Astrategically%2520selects%2520and%2520labels%2520a%2520minimal%2520amount%2520of%2520samples%2520%2528less%2520than%25201%2525%2529%2520to%250Aenhance%2520domain%2520adaptation.%2520The%2520network%2520incorporates%2520a%2520Multi-Temporal%250ATransformer%2520for%2520feature%2520fusion%2520and%2520optimized%2520backbone%2520structure%2520based%2520on%250Aprevious%2520research.%2520Experiments%2520conducted%2520on%2520the%2520LEVIR-CD%2520and%2520WHU-CD%2520datasets%250Ademonstrate%2520that%2520DAM-Net%2520significantly%2520outperforms%2520existing%2520domain%2520adaptation%250Amethods%252C%2520achieving%2520comparable%2520performance%2520to%2520semi-supervised%2520approaches%2520that%250Arequire%252010%2525%2520labeled%2520data%2520while%2520using%2520only%25200.3%2525%2520labeled%2520samples.%2520Our%2520approach%250Asignificantly%2520advances%2520cross-dataset%2520CD%2520applications%2520and%2520provides%2520a%2520new%250Aparadigm%2520for%2520efficient%2520domain%2520adaptation%2520in%2520remote%2520sensing.%2520The%2520source%2520code%2520of%250ADAM-Net%2520will%2520be%2520made%2520publicly%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAM-Net%3A%20Domain%20Adaptation%20Network%20with%20Micro-Labeled%20Fine-Tuning%20for%0A%20%20Change%20Detection&entry.906535625=Hongjia%20Chen%20and%20Xin%20Xu%20and%20Fangling%20Pu&entry.1292438233=%20%20Change%20detection%20%28CD%29%20in%20remote%20sensing%20imagery%20plays%20a%20crucial%20role%20in%0Avarious%20applications%20such%20as%20urban%20planning%2C%20damage%20assessment%2C%20and%20resource%0Amanagement.%20While%20deep%20learning%20approaches%20have%20significantly%20advanced%20CD%0Aperformance%2C%20current%20methods%20suffer%20from%20poor%20domain%20adaptability%2C%20requiring%0Aextensive%20labeled%20data%20for%20retraining%20when%20applied%20to%20new%20scenarios.%20This%0Alimitation%20severely%20restricts%20their%20practical%20applications%20across%20different%0Adatasets.%20In%20this%20work%2C%20we%20propose%20DAM-Net%3A%20a%20Domain%20Adaptation%20Network%20with%0AMicro-Labeled%20Fine-Tuning%20for%20CD.%20Our%20network%20introduces%20adversarial%20domain%0Aadaptation%20to%20CD%20for%2C%20utilizing%20a%20specially%20designed%20segmentation-discriminator%0Aand%20alternating%20training%20strategy%20to%20enable%20effective%20transfer%20between%20domains.%0AAdditionally%2C%20we%20propose%20a%20novel%20Micro-Labeled%20Fine-Tuning%20approach%20that%0Astrategically%20selects%20and%20labels%20a%20minimal%20amount%20of%20samples%20%28less%20than%201%25%29%20to%0Aenhance%20domain%20adaptation.%20The%20network%20incorporates%20a%20Multi-Temporal%0ATransformer%20for%20feature%20fusion%20and%20optimized%20backbone%20structure%20based%20on%0Aprevious%20research.%20Experiments%20conducted%20on%20the%20LEVIR-CD%20and%20WHU-CD%20datasets%0Ademonstrate%20that%20DAM-Net%20significantly%20outperforms%20existing%20domain%20adaptation%0Amethods%2C%20achieving%20comparable%20performance%20to%20semi-supervised%20approaches%20that%0Arequire%2010%25%20labeled%20data%20while%20using%20only%200.3%25%20labeled%20samples.%20Our%20approach%0Asignificantly%20advances%20cross-dataset%20CD%20applications%20and%20provides%20a%20new%0Aparadigm%20for%20efficient%20domain%20adaptation%20in%20remote%20sensing.%20The%20source%20code%20of%0ADAM-Net%20will%20be%20made%20publicly%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13748v1&entry.124074799=Read"},
{"title": "MambaMIM: Pre-training Mamba with State Space Token Interpolation and\n  its Application to Medical Image Segmentation", "author": "Fenghe Tang and Bingkun Nian and Yingtai Li and Zihang Jiang and Jie Yang and Wei Liu and S. Kevin Zhou", "abstract": "  Recently, the state space model Mamba has demonstrated efficient\nlong-sequence modeling capabilities, particularly for addressing long-sequence\nvisual tasks in 3D medical imaging. However, existing generative\nself-supervised learning methods have not yet fully unleashed Mamba's potential\nfor handling long-range dependencies because they overlook the inherent causal\nproperties of state space sequences in masked modeling. To address this\nchallenge, we propose a general-purpose pre-training framework called MambaMIM,\na masked image modeling method based on a novel TOKen-Interpolation strategy\n(TOKI) for the selective structure state space sequence, which learns causal\nrelationships of state space within the masked sequence. Further, MambaMIM\nintroduces a bottom-up 3D hybrid masking strategy to maintain a masking\nconsistency across different architectures and can be used on any single or\nhybrid Mamba architecture to enhance its multi-scale and long-range\nrepresentation capability. We pre-train MambaMIM on a large-scale dataset of\n6.8K CT scans and evaluate its performance across eight public medical\nsegmentation benchmarks. Extensive downstream experiments reveal the\nfeasibility and advancement of using Mamba for medical image pre-training. In\nparticular, when we apply the MambaMIM to a customized architecture that\nhybridizes MedNeXt and Vision Mamba, we consistently obtain the\nstate-of-the-art segmentation performance. The code is available at:\nhttps://github.com/FengheTan9/MambaMIM.\n", "link": "http://arxiv.org/abs/2408.08070v2", "date": "2025-04-18", "relevancy": 2.2132, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5762}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.538}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaMIM%3A%20Pre-training%20Mamba%20with%20State%20Space%20Token%20Interpolation%20and%0A%20%20its%20Application%20to%20Medical%20Image%20Segmentation&body=Title%3A%20MambaMIM%3A%20Pre-training%20Mamba%20with%20State%20Space%20Token%20Interpolation%20and%0A%20%20its%20Application%20to%20Medical%20Image%20Segmentation%0AAuthor%3A%20Fenghe%20Tang%20and%20Bingkun%20Nian%20and%20Yingtai%20Li%20and%20Zihang%20Jiang%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20the%20state%20space%20model%20Mamba%20has%20demonstrated%20efficient%0Along-sequence%20modeling%20capabilities%2C%20particularly%20for%20addressing%20long-sequence%0Avisual%20tasks%20in%203D%20medical%20imaging.%20However%2C%20existing%20generative%0Aself-supervised%20learning%20methods%20have%20not%20yet%20fully%20unleashed%20Mamba%27s%20potential%0Afor%20handling%20long-range%20dependencies%20because%20they%20overlook%20the%20inherent%20causal%0Aproperties%20of%20state%20space%20sequences%20in%20masked%20modeling.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20general-purpose%20pre-training%20framework%20called%20MambaMIM%2C%0Aa%20masked%20image%20modeling%20method%20based%20on%20a%20novel%20TOKen-Interpolation%20strategy%0A%28TOKI%29%20for%20the%20selective%20structure%20state%20space%20sequence%2C%20which%20learns%20causal%0Arelationships%20of%20state%20space%20within%20the%20masked%20sequence.%20Further%2C%20MambaMIM%0Aintroduces%20a%20bottom-up%203D%20hybrid%20masking%20strategy%20to%20maintain%20a%20masking%0Aconsistency%20across%20different%20architectures%20and%20can%20be%20used%20on%20any%20single%20or%0Ahybrid%20Mamba%20architecture%20to%20enhance%20its%20multi-scale%20and%20long-range%0Arepresentation%20capability.%20We%20pre-train%20MambaMIM%20on%20a%20large-scale%20dataset%20of%0A6.8K%20CT%20scans%20and%20evaluate%20its%20performance%20across%20eight%20public%20medical%0Asegmentation%20benchmarks.%20Extensive%20downstream%20experiments%20reveal%20the%0Afeasibility%20and%20advancement%20of%20using%20Mamba%20for%20medical%20image%20pre-training.%20In%0Aparticular%2C%20when%20we%20apply%20the%20MambaMIM%20to%20a%20customized%20architecture%20that%0Ahybridizes%20MedNeXt%20and%20Vision%20Mamba%2C%20we%20consistently%20obtain%20the%0Astate-of-the-art%20segmentation%20performance.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FengheTan9/MambaMIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08070v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaMIM%253A%2520Pre-training%2520Mamba%2520with%2520State%2520Space%2520Token%2520Interpolation%2520and%250A%2520%2520its%2520Application%2520to%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DFenghe%2520Tang%2520and%2520Bingkun%2520Nian%2520and%2520Yingtai%2520Li%2520and%2520Zihang%2520Jiang%2520and%2520Jie%2520Yang%2520and%2520Wei%2520Liu%2520and%2520S.%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520state%2520space%2520model%2520Mamba%2520has%2520demonstrated%2520efficient%250Along-sequence%2520modeling%2520capabilities%252C%2520particularly%2520for%2520addressing%2520long-sequence%250Avisual%2520tasks%2520in%25203D%2520medical%2520imaging.%2520However%252C%2520existing%2520generative%250Aself-supervised%2520learning%2520methods%2520have%2520not%2520yet%2520fully%2520unleashed%2520Mamba%2527s%2520potential%250Afor%2520handling%2520long-range%2520dependencies%2520because%2520they%2520overlook%2520the%2520inherent%2520causal%250Aproperties%2520of%2520state%2520space%2520sequences%2520in%2520masked%2520modeling.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520a%2520general-purpose%2520pre-training%2520framework%2520called%2520MambaMIM%252C%250Aa%2520masked%2520image%2520modeling%2520method%2520based%2520on%2520a%2520novel%2520TOKen-Interpolation%2520strategy%250A%2528TOKI%2529%2520for%2520the%2520selective%2520structure%2520state%2520space%2520sequence%252C%2520which%2520learns%2520causal%250Arelationships%2520of%2520state%2520space%2520within%2520the%2520masked%2520sequence.%2520Further%252C%2520MambaMIM%250Aintroduces%2520a%2520bottom-up%25203D%2520hybrid%2520masking%2520strategy%2520to%2520maintain%2520a%2520masking%250Aconsistency%2520across%2520different%2520architectures%2520and%2520can%2520be%2520used%2520on%2520any%2520single%2520or%250Ahybrid%2520Mamba%2520architecture%2520to%2520enhance%2520its%2520multi-scale%2520and%2520long-range%250Arepresentation%2520capability.%2520We%2520pre-train%2520MambaMIM%2520on%2520a%2520large-scale%2520dataset%2520of%250A6.8K%2520CT%2520scans%2520and%2520evaluate%2520its%2520performance%2520across%2520eight%2520public%2520medical%250Asegmentation%2520benchmarks.%2520Extensive%2520downstream%2520experiments%2520reveal%2520the%250Afeasibility%2520and%2520advancement%2520of%2520using%2520Mamba%2520for%2520medical%2520image%2520pre-training.%2520In%250Aparticular%252C%2520when%2520we%2520apply%2520the%2520MambaMIM%2520to%2520a%2520customized%2520architecture%2520that%250Ahybridizes%2520MedNeXt%2520and%2520Vision%2520Mamba%252C%2520we%2520consistently%2520obtain%2520the%250Astate-of-the-art%2520segmentation%2520performance.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/FengheTan9/MambaMIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08070v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaMIM%3A%20Pre-training%20Mamba%20with%20State%20Space%20Token%20Interpolation%20and%0A%20%20its%20Application%20to%20Medical%20Image%20Segmentation&entry.906535625=Fenghe%20Tang%20and%20Bingkun%20Nian%20and%20Yingtai%20Li%20and%20Zihang%20Jiang%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Recently%2C%20the%20state%20space%20model%20Mamba%20has%20demonstrated%20efficient%0Along-sequence%20modeling%20capabilities%2C%20particularly%20for%20addressing%20long-sequence%0Avisual%20tasks%20in%203D%20medical%20imaging.%20However%2C%20existing%20generative%0Aself-supervised%20learning%20methods%20have%20not%20yet%20fully%20unleashed%20Mamba%27s%20potential%0Afor%20handling%20long-range%20dependencies%20because%20they%20overlook%20the%20inherent%20causal%0Aproperties%20of%20state%20space%20sequences%20in%20masked%20modeling.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20general-purpose%20pre-training%20framework%20called%20MambaMIM%2C%0Aa%20masked%20image%20modeling%20method%20based%20on%20a%20novel%20TOKen-Interpolation%20strategy%0A%28TOKI%29%20for%20the%20selective%20structure%20state%20space%20sequence%2C%20which%20learns%20causal%0Arelationships%20of%20state%20space%20within%20the%20masked%20sequence.%20Further%2C%20MambaMIM%0Aintroduces%20a%20bottom-up%203D%20hybrid%20masking%20strategy%20to%20maintain%20a%20masking%0Aconsistency%20across%20different%20architectures%20and%20can%20be%20used%20on%20any%20single%20or%0Ahybrid%20Mamba%20architecture%20to%20enhance%20its%20multi-scale%20and%20long-range%0Arepresentation%20capability.%20We%20pre-train%20MambaMIM%20on%20a%20large-scale%20dataset%20of%0A6.8K%20CT%20scans%20and%20evaluate%20its%20performance%20across%20eight%20public%20medical%0Asegmentation%20benchmarks.%20Extensive%20downstream%20experiments%20reveal%20the%0Afeasibility%20and%20advancement%20of%20using%20Mamba%20for%20medical%20image%20pre-training.%20In%0Aparticular%2C%20when%20we%20apply%20the%20MambaMIM%20to%20a%20customized%20architecture%20that%0Ahybridizes%20MedNeXt%20and%20Vision%20Mamba%2C%20we%20consistently%20obtain%20the%0Astate-of-the-art%20segmentation%20performance.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/FengheTan9/MambaMIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08070v2&entry.124074799=Read"},
{"title": "FocusTrack: A Self-Adaptive Local Sampling Algorithm for Efficient\n  Anti-UAV Tracking", "author": "Ying Wang and Tingfa Xu and Jianan Li", "abstract": "  Anti-UAV tracking poses significant challenges, including small target sizes,\nabrupt camera motion, and cluttered infrared backgrounds. Existing tracking\nparadigms can be broadly categorized into global- and local-based methods.\nGlobal-based trackers, such as SiamDT, achieve high accuracy by scanning the\nentire field of view but suffer from excessive computational overhead, limiting\nreal-world deployment. In contrast, local-based methods, including OSTrack and\nROMTrack, efficiently restrict the search region but struggle when targets\nundergo significant displacements due to abrupt camera motion. Through\npreliminary experiments, it is evident that a local tracker, when paired with\nadaptive search region adjustment, can significantly enhance tracking accuracy,\nnarrowing the gap between local and global trackers. To address this challenge,\nwe propose FocusTrack, a novel framework that dynamically refines the search\nregion and strengthens feature representations, achieving an optimal balance\nbetween computational efficiency and tracking accuracy. Specifically, our\nSearch Region Adjustment (SRA) strategy estimates the target presence\nprobability and adaptively adjusts the field of view, ensuring the target\nremains within focus. Furthermore, to counteract feature degradation caused by\nvarying search regions, the Attention-to-Mask (ATM) module is proposed. This\nmodule integrates hierarchical information, enriching the target\nrepresentations with fine-grained details. Experimental results demonstrate\nthat FocusTrack achieves state-of-the-art performance, obtaining 67.7% AUC on\nAntiUAV and 62.8% AUC on AntiUAV410, outperforming the baseline tracker by 8.5%\nand 9.1% AUC, respectively. In terms of efficiency, FocusTrack surpasses\nglobal-based trackers, requiring only 30G MACs and achieving 143 fps with\nFocusTrack (SRA) and 44 fps with the full version, both enabling real-time\ntracking.\n", "link": "http://arxiv.org/abs/2504.13604v1", "date": "2025-04-18", "relevancy": 2.2127, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5627}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.553}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocusTrack%3A%20A%20Self-Adaptive%20Local%20Sampling%20Algorithm%20for%20Efficient%0A%20%20Anti-UAV%20Tracking&body=Title%3A%20FocusTrack%3A%20A%20Self-Adaptive%20Local%20Sampling%20Algorithm%20for%20Efficient%0A%20%20Anti-UAV%20Tracking%0AAuthor%3A%20Ying%20Wang%20and%20Tingfa%20Xu%20and%20Jianan%20Li%0AAbstract%3A%20%20%20Anti-UAV%20tracking%20poses%20significant%20challenges%2C%20including%20small%20target%20sizes%2C%0Aabrupt%20camera%20motion%2C%20and%20cluttered%20infrared%20backgrounds.%20Existing%20tracking%0Aparadigms%20can%20be%20broadly%20categorized%20into%20global-%20and%20local-based%20methods.%0AGlobal-based%20trackers%2C%20such%20as%20SiamDT%2C%20achieve%20high%20accuracy%20by%20scanning%20the%0Aentire%20field%20of%20view%20but%20suffer%20from%20excessive%20computational%20overhead%2C%20limiting%0Areal-world%20deployment.%20In%20contrast%2C%20local-based%20methods%2C%20including%20OSTrack%20and%0AROMTrack%2C%20efficiently%20restrict%20the%20search%20region%20but%20struggle%20when%20targets%0Aundergo%20significant%20displacements%20due%20to%20abrupt%20camera%20motion.%20Through%0Apreliminary%20experiments%2C%20it%20is%20evident%20that%20a%20local%20tracker%2C%20when%20paired%20with%0Aadaptive%20search%20region%20adjustment%2C%20can%20significantly%20enhance%20tracking%20accuracy%2C%0Anarrowing%20the%20gap%20between%20local%20and%20global%20trackers.%20To%20address%20this%20challenge%2C%0Awe%20propose%20FocusTrack%2C%20a%20novel%20framework%20that%20dynamically%20refines%20the%20search%0Aregion%20and%20strengthens%20feature%20representations%2C%20achieving%20an%20optimal%20balance%0Abetween%20computational%20efficiency%20and%20tracking%20accuracy.%20Specifically%2C%20our%0ASearch%20Region%20Adjustment%20%28SRA%29%20strategy%20estimates%20the%20target%20presence%0Aprobability%20and%20adaptively%20adjusts%20the%20field%20of%20view%2C%20ensuring%20the%20target%0Aremains%20within%20focus.%20Furthermore%2C%20to%20counteract%20feature%20degradation%20caused%20by%0Avarying%20search%20regions%2C%20the%20Attention-to-Mask%20%28ATM%29%20module%20is%20proposed.%20This%0Amodule%20integrates%20hierarchical%20information%2C%20enriching%20the%20target%0Arepresentations%20with%20fine-grained%20details.%20Experimental%20results%20demonstrate%0Athat%20FocusTrack%20achieves%20state-of-the-art%20performance%2C%20obtaining%2067.7%25%20AUC%20on%0AAntiUAV%20and%2062.8%25%20AUC%20on%20AntiUAV410%2C%20outperforming%20the%20baseline%20tracker%20by%208.5%25%0Aand%209.1%25%20AUC%2C%20respectively.%20In%20terms%20of%20efficiency%2C%20FocusTrack%20surpasses%0Aglobal-based%20trackers%2C%20requiring%20only%2030G%20MACs%20and%20achieving%20143%20fps%20with%0AFocusTrack%20%28SRA%29%20and%2044%20fps%20with%20the%20full%20version%2C%20both%20enabling%20real-time%0Atracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocusTrack%253A%2520A%2520Self-Adaptive%2520Local%2520Sampling%2520Algorithm%2520for%2520Efficient%250A%2520%2520Anti-UAV%2520Tracking%26entry.906535625%3DYing%2520Wang%2520and%2520Tingfa%2520Xu%2520and%2520Jianan%2520Li%26entry.1292438233%3D%2520%2520Anti-UAV%2520tracking%2520poses%2520significant%2520challenges%252C%2520including%2520small%2520target%2520sizes%252C%250Aabrupt%2520camera%2520motion%252C%2520and%2520cluttered%2520infrared%2520backgrounds.%2520Existing%2520tracking%250Aparadigms%2520can%2520be%2520broadly%2520categorized%2520into%2520global-%2520and%2520local-based%2520methods.%250AGlobal-based%2520trackers%252C%2520such%2520as%2520SiamDT%252C%2520achieve%2520high%2520accuracy%2520by%2520scanning%2520the%250Aentire%2520field%2520of%2520view%2520but%2520suffer%2520from%2520excessive%2520computational%2520overhead%252C%2520limiting%250Areal-world%2520deployment.%2520In%2520contrast%252C%2520local-based%2520methods%252C%2520including%2520OSTrack%2520and%250AROMTrack%252C%2520efficiently%2520restrict%2520the%2520search%2520region%2520but%2520struggle%2520when%2520targets%250Aundergo%2520significant%2520displacements%2520due%2520to%2520abrupt%2520camera%2520motion.%2520Through%250Apreliminary%2520experiments%252C%2520it%2520is%2520evident%2520that%2520a%2520local%2520tracker%252C%2520when%2520paired%2520with%250Aadaptive%2520search%2520region%2520adjustment%252C%2520can%2520significantly%2520enhance%2520tracking%2520accuracy%252C%250Anarrowing%2520the%2520gap%2520between%2520local%2520and%2520global%2520trackers.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520FocusTrack%252C%2520a%2520novel%2520framework%2520that%2520dynamically%2520refines%2520the%2520search%250Aregion%2520and%2520strengthens%2520feature%2520representations%252C%2520achieving%2520an%2520optimal%2520balance%250Abetween%2520computational%2520efficiency%2520and%2520tracking%2520accuracy.%2520Specifically%252C%2520our%250ASearch%2520Region%2520Adjustment%2520%2528SRA%2529%2520strategy%2520estimates%2520the%2520target%2520presence%250Aprobability%2520and%2520adaptively%2520adjusts%2520the%2520field%2520of%2520view%252C%2520ensuring%2520the%2520target%250Aremains%2520within%2520focus.%2520Furthermore%252C%2520to%2520counteract%2520feature%2520degradation%2520caused%2520by%250Avarying%2520search%2520regions%252C%2520the%2520Attention-to-Mask%2520%2528ATM%2529%2520module%2520is%2520proposed.%2520This%250Amodule%2520integrates%2520hierarchical%2520information%252C%2520enriching%2520the%2520target%250Arepresentations%2520with%2520fine-grained%2520details.%2520Experimental%2520results%2520demonstrate%250Athat%2520FocusTrack%2520achieves%2520state-of-the-art%2520performance%252C%2520obtaining%252067.7%2525%2520AUC%2520on%250AAntiUAV%2520and%252062.8%2525%2520AUC%2520on%2520AntiUAV410%252C%2520outperforming%2520the%2520baseline%2520tracker%2520by%25208.5%2525%250Aand%25209.1%2525%2520AUC%252C%2520respectively.%2520In%2520terms%2520of%2520efficiency%252C%2520FocusTrack%2520surpasses%250Aglobal-based%2520trackers%252C%2520requiring%2520only%252030G%2520MACs%2520and%2520achieving%2520143%2520fps%2520with%250AFocusTrack%2520%2528SRA%2529%2520and%252044%2520fps%2520with%2520the%2520full%2520version%252C%2520both%2520enabling%2520real-time%250Atracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocusTrack%3A%20A%20Self-Adaptive%20Local%20Sampling%20Algorithm%20for%20Efficient%0A%20%20Anti-UAV%20Tracking&entry.906535625=Ying%20Wang%20and%20Tingfa%20Xu%20and%20Jianan%20Li&entry.1292438233=%20%20Anti-UAV%20tracking%20poses%20significant%20challenges%2C%20including%20small%20target%20sizes%2C%0Aabrupt%20camera%20motion%2C%20and%20cluttered%20infrared%20backgrounds.%20Existing%20tracking%0Aparadigms%20can%20be%20broadly%20categorized%20into%20global-%20and%20local-based%20methods.%0AGlobal-based%20trackers%2C%20such%20as%20SiamDT%2C%20achieve%20high%20accuracy%20by%20scanning%20the%0Aentire%20field%20of%20view%20but%20suffer%20from%20excessive%20computational%20overhead%2C%20limiting%0Areal-world%20deployment.%20In%20contrast%2C%20local-based%20methods%2C%20including%20OSTrack%20and%0AROMTrack%2C%20efficiently%20restrict%20the%20search%20region%20but%20struggle%20when%20targets%0Aundergo%20significant%20displacements%20due%20to%20abrupt%20camera%20motion.%20Through%0Apreliminary%20experiments%2C%20it%20is%20evident%20that%20a%20local%20tracker%2C%20when%20paired%20with%0Aadaptive%20search%20region%20adjustment%2C%20can%20significantly%20enhance%20tracking%20accuracy%2C%0Anarrowing%20the%20gap%20between%20local%20and%20global%20trackers.%20To%20address%20this%20challenge%2C%0Awe%20propose%20FocusTrack%2C%20a%20novel%20framework%20that%20dynamically%20refines%20the%20search%0Aregion%20and%20strengthens%20feature%20representations%2C%20achieving%20an%20optimal%20balance%0Abetween%20computational%20efficiency%20and%20tracking%20accuracy.%20Specifically%2C%20our%0ASearch%20Region%20Adjustment%20%28SRA%29%20strategy%20estimates%20the%20target%20presence%0Aprobability%20and%20adaptively%20adjusts%20the%20field%20of%20view%2C%20ensuring%20the%20target%0Aremains%20within%20focus.%20Furthermore%2C%20to%20counteract%20feature%20degradation%20caused%20by%0Avarying%20search%20regions%2C%20the%20Attention-to-Mask%20%28ATM%29%20module%20is%20proposed.%20This%0Amodule%20integrates%20hierarchical%20information%2C%20enriching%20the%20target%0Arepresentations%20with%20fine-grained%20details.%20Experimental%20results%20demonstrate%0Athat%20FocusTrack%20achieves%20state-of-the-art%20performance%2C%20obtaining%2067.7%25%20AUC%20on%0AAntiUAV%20and%2062.8%25%20AUC%20on%20AntiUAV410%2C%20outperforming%20the%20baseline%20tracker%20by%208.5%25%0Aand%209.1%25%20AUC%2C%20respectively.%20In%20terms%20of%20efficiency%2C%20FocusTrack%20surpasses%0Aglobal-based%20trackers%2C%20requiring%20only%2030G%20MACs%20and%20achieving%20143%20fps%20with%0AFocusTrack%20%28SRA%29%20and%2044%20fps%20with%20the%20full%20version%2C%20both%20enabling%20real-time%0Atracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13604v1&entry.124074799=Read"},
{"title": "Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph\n  Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems", "author": "Vinay Sharma and R\u00e9mi Tanguy Oddon and Pietro Tesini and Jens Ravesloot and Cees Taal and Olga Fink", "abstract": "  Accurate real-time modeling of multi-body dynamical systems is essential for\nenabling digital twin applications across industries. While many data-driven\napproaches aim to learn system dynamics, jointly predicting internal loads and\nsystem trajectories remains a key challenge. This dual prediction is especially\nimportant for fault detection and predictive maintenance, where internal\nloads-such as contact forces-act as early indicators of faults, reflecting wear\nor misalignment before affecting motion. These forces also serve as inputs to\ndegradation models (e.g., crack growth), enabling damage prediction and\nremaining useful life estimation. We propose Equi-Euler GraphNet, a\nphysics-informed graph neural network (GNN) that simultaneously predicts\ninternal forces and global trajectories in multi-body systems. In this\nmesh-free framework, nodes represent system components and edges encode\ninteractions. Equi-Euler GraphNet introduces two inductive biases: (1) an\nequivariant message-passing scheme, interpreting edge messages as interaction\nforces consistent under Euclidean transformations; and (2) a temporal-aware\niterative node update mechanism, based on Euler integration, to capture\ninfluence of distant interactions over time. Tailored for cylindrical roller\nbearings, it decouples ring dynamics from constrained motion of rolling\nelements. Trained on high-fidelity multiphysics simulations, Equi-Euler\nGraphNet generalizes beyond the training distribution, accurately predicting\nloads and trajectories under unseen speeds, loads, and configurations. It\noutperforms state-of-the-art GNNs focused on trajectory prediction, delivering\nstable rollouts over thousands of time steps with minimal error accumulation.\nAchieving up to a 200x speedup over conventional solvers while maintaining\ncomparable accuracy, it serves as an efficient reduced-order model for digital\ntwins, design, and maintenance.\n", "link": "http://arxiv.org/abs/2504.13768v1", "date": "2025-04-18", "relevancy": 2.209, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5556}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equi-Euler%20GraphNet%3A%20An%20Equivariant%2C%20Temporal-Dynamics%20Informed%20Graph%0A%20%20Neural%20Network%20for%20Dual%20Force%20and%20Trajectory%20Prediction%20in%20Multi-Body%20Systems&body=Title%3A%20Equi-Euler%20GraphNet%3A%20An%20Equivariant%2C%20Temporal-Dynamics%20Informed%20Graph%0A%20%20Neural%20Network%20for%20Dual%20Force%20and%20Trajectory%20Prediction%20in%20Multi-Body%20Systems%0AAuthor%3A%20Vinay%20Sharma%20and%20R%C3%A9mi%20Tanguy%20Oddon%20and%20Pietro%20Tesini%20and%20Jens%20Ravesloot%20and%20Cees%20Taal%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Accurate%20real-time%20modeling%20of%20multi-body%20dynamical%20systems%20is%20essential%20for%0Aenabling%20digital%20twin%20applications%20across%20industries.%20While%20many%20data-driven%0Aapproaches%20aim%20to%20learn%20system%20dynamics%2C%20jointly%20predicting%20internal%20loads%20and%0Asystem%20trajectories%20remains%20a%20key%20challenge.%20This%20dual%20prediction%20is%20especially%0Aimportant%20for%20fault%20detection%20and%20predictive%20maintenance%2C%20where%20internal%0Aloads-such%20as%20contact%20forces-act%20as%20early%20indicators%20of%20faults%2C%20reflecting%20wear%0Aor%20misalignment%20before%20affecting%20motion.%20These%20forces%20also%20serve%20as%20inputs%20to%0Adegradation%20models%20%28e.g.%2C%20crack%20growth%29%2C%20enabling%20damage%20prediction%20and%0Aremaining%20useful%20life%20estimation.%20We%20propose%20Equi-Euler%20GraphNet%2C%20a%0Aphysics-informed%20graph%20neural%20network%20%28GNN%29%20that%20simultaneously%20predicts%0Ainternal%20forces%20and%20global%20trajectories%20in%20multi-body%20systems.%20In%20this%0Amesh-free%20framework%2C%20nodes%20represent%20system%20components%20and%20edges%20encode%0Ainteractions.%20Equi-Euler%20GraphNet%20introduces%20two%20inductive%20biases%3A%20%281%29%20an%0Aequivariant%20message-passing%20scheme%2C%20interpreting%20edge%20messages%20as%20interaction%0Aforces%20consistent%20under%20Euclidean%20transformations%3B%20and%20%282%29%20a%20temporal-aware%0Aiterative%20node%20update%20mechanism%2C%20based%20on%20Euler%20integration%2C%20to%20capture%0Ainfluence%20of%20distant%20interactions%20over%20time.%20Tailored%20for%20cylindrical%20roller%0Abearings%2C%20it%20decouples%20ring%20dynamics%20from%20constrained%20motion%20of%20rolling%0Aelements.%20Trained%20on%20high-fidelity%20multiphysics%20simulations%2C%20Equi-Euler%0AGraphNet%20generalizes%20beyond%20the%20training%20distribution%2C%20accurately%20predicting%0Aloads%20and%20trajectories%20under%20unseen%20speeds%2C%20loads%2C%20and%20configurations.%20It%0Aoutperforms%20state-of-the-art%20GNNs%20focused%20on%20trajectory%20prediction%2C%20delivering%0Astable%20rollouts%20over%20thousands%20of%20time%20steps%20with%20minimal%20error%20accumulation.%0AAchieving%20up%20to%20a%20200x%20speedup%20over%20conventional%20solvers%20while%20maintaining%0Acomparable%20accuracy%2C%20it%20serves%20as%20an%20efficient%20reduced-order%20model%20for%20digital%0Atwins%2C%20design%2C%20and%20maintenance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEqui-Euler%2520GraphNet%253A%2520An%2520Equivariant%252C%2520Temporal-Dynamics%2520Informed%2520Graph%250A%2520%2520Neural%2520Network%2520for%2520Dual%2520Force%2520and%2520Trajectory%2520Prediction%2520in%2520Multi-Body%2520Systems%26entry.906535625%3DVinay%2520Sharma%2520and%2520R%25C3%25A9mi%2520Tanguy%2520Oddon%2520and%2520Pietro%2520Tesini%2520and%2520Jens%2520Ravesloot%2520and%2520Cees%2520Taal%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Accurate%2520real-time%2520modeling%2520of%2520multi-body%2520dynamical%2520systems%2520is%2520essential%2520for%250Aenabling%2520digital%2520twin%2520applications%2520across%2520industries.%2520While%2520many%2520data-driven%250Aapproaches%2520aim%2520to%2520learn%2520system%2520dynamics%252C%2520jointly%2520predicting%2520internal%2520loads%2520and%250Asystem%2520trajectories%2520remains%2520a%2520key%2520challenge.%2520This%2520dual%2520prediction%2520is%2520especially%250Aimportant%2520for%2520fault%2520detection%2520and%2520predictive%2520maintenance%252C%2520where%2520internal%250Aloads-such%2520as%2520contact%2520forces-act%2520as%2520early%2520indicators%2520of%2520faults%252C%2520reflecting%2520wear%250Aor%2520misalignment%2520before%2520affecting%2520motion.%2520These%2520forces%2520also%2520serve%2520as%2520inputs%2520to%250Adegradation%2520models%2520%2528e.g.%252C%2520crack%2520growth%2529%252C%2520enabling%2520damage%2520prediction%2520and%250Aremaining%2520useful%2520life%2520estimation.%2520We%2520propose%2520Equi-Euler%2520GraphNet%252C%2520a%250Aphysics-informed%2520graph%2520neural%2520network%2520%2528GNN%2529%2520that%2520simultaneously%2520predicts%250Ainternal%2520forces%2520and%2520global%2520trajectories%2520in%2520multi-body%2520systems.%2520In%2520this%250Amesh-free%2520framework%252C%2520nodes%2520represent%2520system%2520components%2520and%2520edges%2520encode%250Ainteractions.%2520Equi-Euler%2520GraphNet%2520introduces%2520two%2520inductive%2520biases%253A%2520%25281%2529%2520an%250Aequivariant%2520message-passing%2520scheme%252C%2520interpreting%2520edge%2520messages%2520as%2520interaction%250Aforces%2520consistent%2520under%2520Euclidean%2520transformations%253B%2520and%2520%25282%2529%2520a%2520temporal-aware%250Aiterative%2520node%2520update%2520mechanism%252C%2520based%2520on%2520Euler%2520integration%252C%2520to%2520capture%250Ainfluence%2520of%2520distant%2520interactions%2520over%2520time.%2520Tailored%2520for%2520cylindrical%2520roller%250Abearings%252C%2520it%2520decouples%2520ring%2520dynamics%2520from%2520constrained%2520motion%2520of%2520rolling%250Aelements.%2520Trained%2520on%2520high-fidelity%2520multiphysics%2520simulations%252C%2520Equi-Euler%250AGraphNet%2520generalizes%2520beyond%2520the%2520training%2520distribution%252C%2520accurately%2520predicting%250Aloads%2520and%2520trajectories%2520under%2520unseen%2520speeds%252C%2520loads%252C%2520and%2520configurations.%2520It%250Aoutperforms%2520state-of-the-art%2520GNNs%2520focused%2520on%2520trajectory%2520prediction%252C%2520delivering%250Astable%2520rollouts%2520over%2520thousands%2520of%2520time%2520steps%2520with%2520minimal%2520error%2520accumulation.%250AAchieving%2520up%2520to%2520a%2520200x%2520speedup%2520over%2520conventional%2520solvers%2520while%2520maintaining%250Acomparable%2520accuracy%252C%2520it%2520serves%2520as%2520an%2520efficient%2520reduced-order%2520model%2520for%2520digital%250Atwins%252C%2520design%252C%2520and%2520maintenance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equi-Euler%20GraphNet%3A%20An%20Equivariant%2C%20Temporal-Dynamics%20Informed%20Graph%0A%20%20Neural%20Network%20for%20Dual%20Force%20and%20Trajectory%20Prediction%20in%20Multi-Body%20Systems&entry.906535625=Vinay%20Sharma%20and%20R%C3%A9mi%20Tanguy%20Oddon%20and%20Pietro%20Tesini%20and%20Jens%20Ravesloot%20and%20Cees%20Taal%20and%20Olga%20Fink&entry.1292438233=%20%20Accurate%20real-time%20modeling%20of%20multi-body%20dynamical%20systems%20is%20essential%20for%0Aenabling%20digital%20twin%20applications%20across%20industries.%20While%20many%20data-driven%0Aapproaches%20aim%20to%20learn%20system%20dynamics%2C%20jointly%20predicting%20internal%20loads%20and%0Asystem%20trajectories%20remains%20a%20key%20challenge.%20This%20dual%20prediction%20is%20especially%0Aimportant%20for%20fault%20detection%20and%20predictive%20maintenance%2C%20where%20internal%0Aloads-such%20as%20contact%20forces-act%20as%20early%20indicators%20of%20faults%2C%20reflecting%20wear%0Aor%20misalignment%20before%20affecting%20motion.%20These%20forces%20also%20serve%20as%20inputs%20to%0Adegradation%20models%20%28e.g.%2C%20crack%20growth%29%2C%20enabling%20damage%20prediction%20and%0Aremaining%20useful%20life%20estimation.%20We%20propose%20Equi-Euler%20GraphNet%2C%20a%0Aphysics-informed%20graph%20neural%20network%20%28GNN%29%20that%20simultaneously%20predicts%0Ainternal%20forces%20and%20global%20trajectories%20in%20multi-body%20systems.%20In%20this%0Amesh-free%20framework%2C%20nodes%20represent%20system%20components%20and%20edges%20encode%0Ainteractions.%20Equi-Euler%20GraphNet%20introduces%20two%20inductive%20biases%3A%20%281%29%20an%0Aequivariant%20message-passing%20scheme%2C%20interpreting%20edge%20messages%20as%20interaction%0Aforces%20consistent%20under%20Euclidean%20transformations%3B%20and%20%282%29%20a%20temporal-aware%0Aiterative%20node%20update%20mechanism%2C%20based%20on%20Euler%20integration%2C%20to%20capture%0Ainfluence%20of%20distant%20interactions%20over%20time.%20Tailored%20for%20cylindrical%20roller%0Abearings%2C%20it%20decouples%20ring%20dynamics%20from%20constrained%20motion%20of%20rolling%0Aelements.%20Trained%20on%20high-fidelity%20multiphysics%20simulations%2C%20Equi-Euler%0AGraphNet%20generalizes%20beyond%20the%20training%20distribution%2C%20accurately%20predicting%0Aloads%20and%20trajectories%20under%20unseen%20speeds%2C%20loads%2C%20and%20configurations.%20It%0Aoutperforms%20state-of-the-art%20GNNs%20focused%20on%20trajectory%20prediction%2C%20delivering%0Astable%20rollouts%20over%20thousands%20of%20time%20steps%20with%20minimal%20error%20accumulation.%0AAchieving%20up%20to%20a%20200x%20speedup%20over%20conventional%20solvers%20while%20maintaining%0Acomparable%20accuracy%2C%20it%20serves%20as%20an%20efficient%20reduced-order%20model%20for%20digital%0Atwins%2C%20design%2C%20and%20maintenance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13768v1&entry.124074799=Read"},
{"title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation\n  with Multimodal Generative Pretraining", "author": "Dongyang Liu and Shitian Zhao and Le Zhuo and Weifeng Lin and Yu Qiao and Hongsheng Li and Peng Gao", "abstract": "  We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. By initializing from\nmultimodal Generative PreTraining (mGPT), we demonstrate that decoder-only\nAutoregressive (AR) model can achieve image generation performance comparable\nto modern diffusion models with high efficiency through Flexible Progressive\nSupervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image\nRepresentation (UniRep), Lumina-mGPT can flexibly generate high-quality images\nof varying aspect ratios. Building on the strong image generation capabilities,\nwe further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial\nattempt to elevate Lumina-mGPT into a unified multi-modal generalist. The\nresulting model demonstrates versatile multimodal capabilities, including\nvisual generation tasks like text-to-image/multiview generation and\ncontrollable generation, visual recognition tasks like segmentation and depth\nestimation, and vision-language tasks like multi-turn visual question\nanswering, showing the rosy potential of the technical direction. Codes and\ncheckpoints are available at https://github.com/Alpha-VLLM/Lumina-mGPT.\n", "link": "http://arxiv.org/abs/2408.02657v2", "date": "2025-04-18", "relevancy": 2.2071, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5535}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5514}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining&body=Title%3A%20Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining%0AAuthor%3A%20Dongyang%20Liu%20and%20Shitian%20Zhao%20and%20Le%20Zhuo%20and%20Weifeng%20Lin%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%20and%20Peng%20Gao%0AAbstract%3A%20%20%20We%20present%20Lumina-mGPT%2C%20a%20family%20of%20multimodal%20autoregressive%20models%20capable%0Aof%20various%20vision%20and%20language%20tasks%2C%20particularly%20excelling%20in%20generating%0Aflexible%20photorealistic%20images%20from%20text%20descriptions.%20By%20initializing%20from%0Amultimodal%20Generative%20PreTraining%20%28mGPT%29%2C%20we%20demonstrate%20that%20decoder-only%0AAutoregressive%20%28AR%29%20model%20can%20achieve%20image%20generation%20performance%20comparable%0Ato%20modern%20diffusion%20models%20with%20high%20efficiency%20through%20Flexible%20Progressive%0ASupervised%20Fine-tuning%20%28FP-SFT%29.%20Equipped%20with%20our%20proposed%20Unambiguous%20image%0ARepresentation%20%28UniRep%29%2C%20Lumina-mGPT%20can%20flexibly%20generate%20high-quality%20images%0Aof%20varying%20aspect%20ratios.%20Building%20on%20the%20strong%20image%20generation%20capabilities%2C%0Awe%20further%20explore%20Ominiponent%20Supervised%20Fine-tuning%20%28Omni-SFT%29%2C%20an%20initial%0Aattempt%20to%20elevate%20Lumina-mGPT%20into%20a%20unified%20multi-modal%20generalist.%20The%0Aresulting%20model%20demonstrates%20versatile%20multimodal%20capabilities%2C%20including%0Avisual%20generation%20tasks%20like%20text-to-image/multiview%20generation%20and%0Acontrollable%20generation%2C%20visual%20recognition%20tasks%20like%20segmentation%20and%20depth%0Aestimation%2C%20and%20vision-language%20tasks%20like%20multi-turn%20visual%20question%0Aanswering%2C%20showing%20the%20rosy%20potential%20of%20the%20technical%20direction.%20Codes%20and%0Acheckpoints%20are%20available%20at%20https%3A//github.com/Alpha-VLLM/Lumina-mGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumina-mGPT%253A%2520Illuminate%2520Flexible%2520Photorealistic%2520Text-to-Image%2520Generation%250A%2520%2520with%2520Multimodal%2520Generative%2520Pretraining%26entry.906535625%3DDongyang%2520Liu%2520and%2520Shitian%2520Zhao%2520and%2520Le%2520Zhuo%2520and%2520Weifeng%2520Lin%2520and%2520Yu%2520Qiao%2520and%2520Hongsheng%2520Li%2520and%2520Peng%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520Lumina-mGPT%252C%2520a%2520family%2520of%2520multimodal%2520autoregressive%2520models%2520capable%250Aof%2520various%2520vision%2520and%2520language%2520tasks%252C%2520particularly%2520excelling%2520in%2520generating%250Aflexible%2520photorealistic%2520images%2520from%2520text%2520descriptions.%2520By%2520initializing%2520from%250Amultimodal%2520Generative%2520PreTraining%2520%2528mGPT%2529%252C%2520we%2520demonstrate%2520that%2520decoder-only%250AAutoregressive%2520%2528AR%2529%2520model%2520can%2520achieve%2520image%2520generation%2520performance%2520comparable%250Ato%2520modern%2520diffusion%2520models%2520with%2520high%2520efficiency%2520through%2520Flexible%2520Progressive%250ASupervised%2520Fine-tuning%2520%2528FP-SFT%2529.%2520Equipped%2520with%2520our%2520proposed%2520Unambiguous%2520image%250ARepresentation%2520%2528UniRep%2529%252C%2520Lumina-mGPT%2520can%2520flexibly%2520generate%2520high-quality%2520images%250Aof%2520varying%2520aspect%2520ratios.%2520Building%2520on%2520the%2520strong%2520image%2520generation%2520capabilities%252C%250Awe%2520further%2520explore%2520Ominiponent%2520Supervised%2520Fine-tuning%2520%2528Omni-SFT%2529%252C%2520an%2520initial%250Aattempt%2520to%2520elevate%2520Lumina-mGPT%2520into%2520a%2520unified%2520multi-modal%2520generalist.%2520The%250Aresulting%2520model%2520demonstrates%2520versatile%2520multimodal%2520capabilities%252C%2520including%250Avisual%2520generation%2520tasks%2520like%2520text-to-image/multiview%2520generation%2520and%250Acontrollable%2520generation%252C%2520visual%2520recognition%2520tasks%2520like%2520segmentation%2520and%2520depth%250Aestimation%252C%2520and%2520vision-language%2520tasks%2520like%2520multi-turn%2520visual%2520question%250Aanswering%252C%2520showing%2520the%2520rosy%2520potential%2520of%2520the%2520technical%2520direction.%2520Codes%2520and%250Acheckpoints%2520are%2520available%2520at%2520https%253A//github.com/Alpha-VLLM/Lumina-mGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumina-mGPT%3A%20Illuminate%20Flexible%20Photorealistic%20Text-to-Image%20Generation%0A%20%20with%20Multimodal%20Generative%20Pretraining&entry.906535625=Dongyang%20Liu%20and%20Shitian%20Zhao%20and%20Le%20Zhuo%20and%20Weifeng%20Lin%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%20and%20Peng%20Gao&entry.1292438233=%20%20We%20present%20Lumina-mGPT%2C%20a%20family%20of%20multimodal%20autoregressive%20models%20capable%0Aof%20various%20vision%20and%20language%20tasks%2C%20particularly%20excelling%20in%20generating%0Aflexible%20photorealistic%20images%20from%20text%20descriptions.%20By%20initializing%20from%0Amultimodal%20Generative%20PreTraining%20%28mGPT%29%2C%20we%20demonstrate%20that%20decoder-only%0AAutoregressive%20%28AR%29%20model%20can%20achieve%20image%20generation%20performance%20comparable%0Ato%20modern%20diffusion%20models%20with%20high%20efficiency%20through%20Flexible%20Progressive%0ASupervised%20Fine-tuning%20%28FP-SFT%29.%20Equipped%20with%20our%20proposed%20Unambiguous%20image%0ARepresentation%20%28UniRep%29%2C%20Lumina-mGPT%20can%20flexibly%20generate%20high-quality%20images%0Aof%20varying%20aspect%20ratios.%20Building%20on%20the%20strong%20image%20generation%20capabilities%2C%0Awe%20further%20explore%20Ominiponent%20Supervised%20Fine-tuning%20%28Omni-SFT%29%2C%20an%20initial%0Aattempt%20to%20elevate%20Lumina-mGPT%20into%20a%20unified%20multi-modal%20generalist.%20The%0Aresulting%20model%20demonstrates%20versatile%20multimodal%20capabilities%2C%20including%0Avisual%20generation%20tasks%20like%20text-to-image/multiview%20generation%20and%0Acontrollable%20generation%2C%20visual%20recognition%20tasks%20like%20segmentation%20and%20depth%0Aestimation%2C%20and%20vision-language%20tasks%20like%20multi-turn%20visual%20question%0Aanswering%2C%20showing%20the%20rosy%20potential%20of%20the%20technical%20direction.%20Codes%20and%0Acheckpoints%20are%20available%20at%20https%3A//github.com/Alpha-VLLM/Lumina-mGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02657v2&entry.124074799=Read"},
{"title": "MLEP: Multi-granularity Local Entropy Patterns for Universal\n  AI-generated Image Detection", "author": "Lin Yuan and Xiaowan Li and Yan Zhang and Jiawei Zhang and Hongbo Li and Xinbo Gao", "abstract": "  Advancements in image generation technologies have raised significant\nconcerns about their potential misuse, such as producing misinformation and\ndeepfakes. Therefore, there is an urgent need for effective methods to detect\nAI-generated images (AIGI). Despite progress in AIGI detection, achieving\nreliable performance across diverse generation models and scenes remains\nchallenging due to the lack of source-invariant features and limited\ngeneralization capabilities in existing methods. In this work, we explore the\npotential of using image entropy as a cue for AIGI detection and propose\nMulti-granularity Local Entropy Patterns (MLEP), a set of entropy feature maps\ncomputed across shuffled small patches over multiple image scaled. MLEP\ncomprehensively captures pixel relationships across dimensions and scales while\nsignificantly disrupting image semantics, reducing potential content bias.\nLeveraging MLEP, a robust CNN-based classifier for AIGI detection can be\ntrained. Extensive experiments conducted in an open-world scenario, evaluating\nimages synthesized by 32 distinct generative models, demonstrate significant\nimprovements over state-of-the-art methods in both accuracy and generalization.\n", "link": "http://arxiv.org/abs/2504.13726v1", "date": "2025-04-18", "relevancy": 2.2008, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5897}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5459}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLEP%3A%20Multi-granularity%20Local%20Entropy%20Patterns%20for%20Universal%0A%20%20AI-generated%20Image%20Detection&body=Title%3A%20MLEP%3A%20Multi-granularity%20Local%20Entropy%20Patterns%20for%20Universal%0A%20%20AI-generated%20Image%20Detection%0AAuthor%3A%20Lin%20Yuan%20and%20Xiaowan%20Li%20and%20Yan%20Zhang%20and%20Jiawei%20Zhang%20and%20Hongbo%20Li%20and%20Xinbo%20Gao%0AAbstract%3A%20%20%20Advancements%20in%20image%20generation%20technologies%20have%20raised%20significant%0Aconcerns%20about%20their%20potential%20misuse%2C%20such%20as%20producing%20misinformation%20and%0Adeepfakes.%20Therefore%2C%20there%20is%20an%20urgent%20need%20for%20effective%20methods%20to%20detect%0AAI-generated%20images%20%28AIGI%29.%20Despite%20progress%20in%20AIGI%20detection%2C%20achieving%0Areliable%20performance%20across%20diverse%20generation%20models%20and%20scenes%20remains%0Achallenging%20due%20to%20the%20lack%20of%20source-invariant%20features%20and%20limited%0Ageneralization%20capabilities%20in%20existing%20methods.%20In%20this%20work%2C%20we%20explore%20the%0Apotential%20of%20using%20image%20entropy%20as%20a%20cue%20for%20AIGI%20detection%20and%20propose%0AMulti-granularity%20Local%20Entropy%20Patterns%20%28MLEP%29%2C%20a%20set%20of%20entropy%20feature%20maps%0Acomputed%20across%20shuffled%20small%20patches%20over%20multiple%20image%20scaled.%20MLEP%0Acomprehensively%20captures%20pixel%20relationships%20across%20dimensions%20and%20scales%20while%0Asignificantly%20disrupting%20image%20semantics%2C%20reducing%20potential%20content%20bias.%0ALeveraging%20MLEP%2C%20a%20robust%20CNN-based%20classifier%20for%20AIGI%20detection%20can%20be%0Atrained.%20Extensive%20experiments%20conducted%20in%20an%20open-world%20scenario%2C%20evaluating%0Aimages%20synthesized%20by%2032%20distinct%20generative%20models%2C%20demonstrate%20significant%0Aimprovements%20over%20state-of-the-art%20methods%20in%20both%20accuracy%20and%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLEP%253A%2520Multi-granularity%2520Local%2520Entropy%2520Patterns%2520for%2520Universal%250A%2520%2520AI-generated%2520Image%2520Detection%26entry.906535625%3DLin%2520Yuan%2520and%2520Xiaowan%2520Li%2520and%2520Yan%2520Zhang%2520and%2520Jiawei%2520Zhang%2520and%2520Hongbo%2520Li%2520and%2520Xinbo%2520Gao%26entry.1292438233%3D%2520%2520Advancements%2520in%2520image%2520generation%2520technologies%2520have%2520raised%2520significant%250Aconcerns%2520about%2520their%2520potential%2520misuse%252C%2520such%2520as%2520producing%2520misinformation%2520and%250Adeepfakes.%2520Therefore%252C%2520there%2520is%2520an%2520urgent%2520need%2520for%2520effective%2520methods%2520to%2520detect%250AAI-generated%2520images%2520%2528AIGI%2529.%2520Despite%2520progress%2520in%2520AIGI%2520detection%252C%2520achieving%250Areliable%2520performance%2520across%2520diverse%2520generation%2520models%2520and%2520scenes%2520remains%250Achallenging%2520due%2520to%2520the%2520lack%2520of%2520source-invariant%2520features%2520and%2520limited%250Ageneralization%2520capabilities%2520in%2520existing%2520methods.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Apotential%2520of%2520using%2520image%2520entropy%2520as%2520a%2520cue%2520for%2520AIGI%2520detection%2520and%2520propose%250AMulti-granularity%2520Local%2520Entropy%2520Patterns%2520%2528MLEP%2529%252C%2520a%2520set%2520of%2520entropy%2520feature%2520maps%250Acomputed%2520across%2520shuffled%2520small%2520patches%2520over%2520multiple%2520image%2520scaled.%2520MLEP%250Acomprehensively%2520captures%2520pixel%2520relationships%2520across%2520dimensions%2520and%2520scales%2520while%250Asignificantly%2520disrupting%2520image%2520semantics%252C%2520reducing%2520potential%2520content%2520bias.%250ALeveraging%2520MLEP%252C%2520a%2520robust%2520CNN-based%2520classifier%2520for%2520AIGI%2520detection%2520can%2520be%250Atrained.%2520Extensive%2520experiments%2520conducted%2520in%2520an%2520open-world%2520scenario%252C%2520evaluating%250Aimages%2520synthesized%2520by%252032%2520distinct%2520generative%2520models%252C%2520demonstrate%2520significant%250Aimprovements%2520over%2520state-of-the-art%2520methods%2520in%2520both%2520accuracy%2520and%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLEP%3A%20Multi-granularity%20Local%20Entropy%20Patterns%20for%20Universal%0A%20%20AI-generated%20Image%20Detection&entry.906535625=Lin%20Yuan%20and%20Xiaowan%20Li%20and%20Yan%20Zhang%20and%20Jiawei%20Zhang%20and%20Hongbo%20Li%20and%20Xinbo%20Gao&entry.1292438233=%20%20Advancements%20in%20image%20generation%20technologies%20have%20raised%20significant%0Aconcerns%20about%20their%20potential%20misuse%2C%20such%20as%20producing%20misinformation%20and%0Adeepfakes.%20Therefore%2C%20there%20is%20an%20urgent%20need%20for%20effective%20methods%20to%20detect%0AAI-generated%20images%20%28AIGI%29.%20Despite%20progress%20in%20AIGI%20detection%2C%20achieving%0Areliable%20performance%20across%20diverse%20generation%20models%20and%20scenes%20remains%0Achallenging%20due%20to%20the%20lack%20of%20source-invariant%20features%20and%20limited%0Ageneralization%20capabilities%20in%20existing%20methods.%20In%20this%20work%2C%20we%20explore%20the%0Apotential%20of%20using%20image%20entropy%20as%20a%20cue%20for%20AIGI%20detection%20and%20propose%0AMulti-granularity%20Local%20Entropy%20Patterns%20%28MLEP%29%2C%20a%20set%20of%20entropy%20feature%20maps%0Acomputed%20across%20shuffled%20small%20patches%20over%20multiple%20image%20scaled.%20MLEP%0Acomprehensively%20captures%20pixel%20relationships%20across%20dimensions%20and%20scales%20while%0Asignificantly%20disrupting%20image%20semantics%2C%20reducing%20potential%20content%20bias.%0ALeveraging%20MLEP%2C%20a%20robust%20CNN-based%20classifier%20for%20AIGI%20detection%20can%20be%0Atrained.%20Extensive%20experiments%20conducted%20in%20an%20open-world%20scenario%2C%20evaluating%0Aimages%20synthesized%20by%2032%20distinct%20generative%20models%2C%20demonstrate%20significant%0Aimprovements%20over%20state-of-the-art%20methods%20in%20both%20accuracy%20and%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13726v1&entry.124074799=Read"},
{"title": "Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of\n  ChatGPT-Generated Code", "author": "Antonio Della Porta and Stefano Lambiase and Fabio Palomba", "abstract": "  Large Language Models (LLMs) have rapidly transformed software development,\nespecially in code generation. However, their inconsistent performance, prone\nto hallucinations and quality issues, complicates program comprehension and\nhinders maintainability. Research indicates that prompt engineering-the\npractice of designing inputs to direct LLMs toward generating relevant\noutputs-may help address these challenges. In this regard, researchers have\nintroduced prompt patterns, structured templates intended to guide users in\nformulating their requests. However, the influence of prompt patterns on code\nquality has yet to be thoroughly investigated. An improved understanding of\nthis relationship would be essential to advancing our collective knowledge on\nhow to effectively use LLMs for code generation, thereby enhancing their\nunderstandability in contemporary software development. This paper empirically\ninvestigates the impact of prompt patterns on code quality, specifically\nmaintainability, security, and reliability, using the Dev-GPT dataset. Results\nshow that Zero-Shot prompting is most common, followed by Zero-Shot with\nChain-of-Thought and Few-Shot. Analysis of 7583 code files across quality\nmetrics revealed minimal issues, with Kruskal-Wallis tests indicating no\nsignificant differences among patterns, suggesting that prompt structure may\nnot substantially impact these quality metrics in ChatGPT-assisted code\ngeneration.\n", "link": "http://arxiv.org/abs/2504.13656v1", "date": "2025-04-18", "relevancy": 2.2004, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5276}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Prompt%20Patterns%20Affect%20Code%20Quality%3F%20A%20First%20Empirical%20Assessment%20of%0A%20%20ChatGPT-Generated%20Code&body=Title%3A%20Do%20Prompt%20Patterns%20Affect%20Code%20Quality%3F%20A%20First%20Empirical%20Assessment%20of%0A%20%20ChatGPT-Generated%20Code%0AAuthor%3A%20Antonio%20Della%20Porta%20and%20Stefano%20Lambiase%20and%20Fabio%20Palomba%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20rapidly%20transformed%20software%20development%2C%0Aespecially%20in%20code%20generation.%20However%2C%20their%20inconsistent%20performance%2C%20prone%0Ato%20hallucinations%20and%20quality%20issues%2C%20complicates%20program%20comprehension%20and%0Ahinders%20maintainability.%20Research%20indicates%20that%20prompt%20engineering-the%0Apractice%20of%20designing%20inputs%20to%20direct%20LLMs%20toward%20generating%20relevant%0Aoutputs-may%20help%20address%20these%20challenges.%20In%20this%20regard%2C%20researchers%20have%0Aintroduced%20prompt%20patterns%2C%20structured%20templates%20intended%20to%20guide%20users%20in%0Aformulating%20their%20requests.%20However%2C%20the%20influence%20of%20prompt%20patterns%20on%20code%0Aquality%20has%20yet%20to%20be%20thoroughly%20investigated.%20An%20improved%20understanding%20of%0Athis%20relationship%20would%20be%20essential%20to%20advancing%20our%20collective%20knowledge%20on%0Ahow%20to%20effectively%20use%20LLMs%20for%20code%20generation%2C%20thereby%20enhancing%20their%0Aunderstandability%20in%20contemporary%20software%20development.%20This%20paper%20empirically%0Ainvestigates%20the%20impact%20of%20prompt%20patterns%20on%20code%20quality%2C%20specifically%0Amaintainability%2C%20security%2C%20and%20reliability%2C%20using%20the%20Dev-GPT%20dataset.%20Results%0Ashow%20that%20Zero-Shot%20prompting%20is%20most%20common%2C%20followed%20by%20Zero-Shot%20with%0AChain-of-Thought%20and%20Few-Shot.%20Analysis%20of%207583%20code%20files%20across%20quality%0Ametrics%20revealed%20minimal%20issues%2C%20with%20Kruskal-Wallis%20tests%20indicating%20no%0Asignificant%20differences%20among%20patterns%2C%20suggesting%20that%20prompt%20structure%20may%0Anot%20substantially%20impact%20these%20quality%20metrics%20in%20ChatGPT-assisted%20code%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Prompt%2520Patterns%2520Affect%2520Code%2520Quality%253F%2520A%2520First%2520Empirical%2520Assessment%2520of%250A%2520%2520ChatGPT-Generated%2520Code%26entry.906535625%3DAntonio%2520Della%2520Porta%2520and%2520Stefano%2520Lambiase%2520and%2520Fabio%2520Palomba%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520rapidly%2520transformed%2520software%2520development%252C%250Aespecially%2520in%2520code%2520generation.%2520However%252C%2520their%2520inconsistent%2520performance%252C%2520prone%250Ato%2520hallucinations%2520and%2520quality%2520issues%252C%2520complicates%2520program%2520comprehension%2520and%250Ahinders%2520maintainability.%2520Research%2520indicates%2520that%2520prompt%2520engineering-the%250Apractice%2520of%2520designing%2520inputs%2520to%2520direct%2520LLMs%2520toward%2520generating%2520relevant%250Aoutputs-may%2520help%2520address%2520these%2520challenges.%2520In%2520this%2520regard%252C%2520researchers%2520have%250Aintroduced%2520prompt%2520patterns%252C%2520structured%2520templates%2520intended%2520to%2520guide%2520users%2520in%250Aformulating%2520their%2520requests.%2520However%252C%2520the%2520influence%2520of%2520prompt%2520patterns%2520on%2520code%250Aquality%2520has%2520yet%2520to%2520be%2520thoroughly%2520investigated.%2520An%2520improved%2520understanding%2520of%250Athis%2520relationship%2520would%2520be%2520essential%2520to%2520advancing%2520our%2520collective%2520knowledge%2520on%250Ahow%2520to%2520effectively%2520use%2520LLMs%2520for%2520code%2520generation%252C%2520thereby%2520enhancing%2520their%250Aunderstandability%2520in%2520contemporary%2520software%2520development.%2520This%2520paper%2520empirically%250Ainvestigates%2520the%2520impact%2520of%2520prompt%2520patterns%2520on%2520code%2520quality%252C%2520specifically%250Amaintainability%252C%2520security%252C%2520and%2520reliability%252C%2520using%2520the%2520Dev-GPT%2520dataset.%2520Results%250Ashow%2520that%2520Zero-Shot%2520prompting%2520is%2520most%2520common%252C%2520followed%2520by%2520Zero-Shot%2520with%250AChain-of-Thought%2520and%2520Few-Shot.%2520Analysis%2520of%25207583%2520code%2520files%2520across%2520quality%250Ametrics%2520revealed%2520minimal%2520issues%252C%2520with%2520Kruskal-Wallis%2520tests%2520indicating%2520no%250Asignificant%2520differences%2520among%2520patterns%252C%2520suggesting%2520that%2520prompt%2520structure%2520may%250Anot%2520substantially%2520impact%2520these%2520quality%2520metrics%2520in%2520ChatGPT-assisted%2520code%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Prompt%20Patterns%20Affect%20Code%20Quality%3F%20A%20First%20Empirical%20Assessment%20of%0A%20%20ChatGPT-Generated%20Code&entry.906535625=Antonio%20Della%20Porta%20and%20Stefano%20Lambiase%20and%20Fabio%20Palomba&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20rapidly%20transformed%20software%20development%2C%0Aespecially%20in%20code%20generation.%20However%2C%20their%20inconsistent%20performance%2C%20prone%0Ato%20hallucinations%20and%20quality%20issues%2C%20complicates%20program%20comprehension%20and%0Ahinders%20maintainability.%20Research%20indicates%20that%20prompt%20engineering-the%0Apractice%20of%20designing%20inputs%20to%20direct%20LLMs%20toward%20generating%20relevant%0Aoutputs-may%20help%20address%20these%20challenges.%20In%20this%20regard%2C%20researchers%20have%0Aintroduced%20prompt%20patterns%2C%20structured%20templates%20intended%20to%20guide%20users%20in%0Aformulating%20their%20requests.%20However%2C%20the%20influence%20of%20prompt%20patterns%20on%20code%0Aquality%20has%20yet%20to%20be%20thoroughly%20investigated.%20An%20improved%20understanding%20of%0Athis%20relationship%20would%20be%20essential%20to%20advancing%20our%20collective%20knowledge%20on%0Ahow%20to%20effectively%20use%20LLMs%20for%20code%20generation%2C%20thereby%20enhancing%20their%0Aunderstandability%20in%20contemporary%20software%20development.%20This%20paper%20empirically%0Ainvestigates%20the%20impact%20of%20prompt%20patterns%20on%20code%20quality%2C%20specifically%0Amaintainability%2C%20security%2C%20and%20reliability%2C%20using%20the%20Dev-GPT%20dataset.%20Results%0Ashow%20that%20Zero-Shot%20prompting%20is%20most%20common%2C%20followed%20by%20Zero-Shot%20with%0AChain-of-Thought%20and%20Few-Shot.%20Analysis%20of%207583%20code%20files%20across%20quality%0Ametrics%20revealed%20minimal%20issues%2C%20with%20Kruskal-Wallis%20tests%20indicating%20no%0Asignificant%20differences%20among%20patterns%2C%20suggesting%20that%20prompt%20structure%20may%0Anot%20substantially%20impact%20these%20quality%20metrics%20in%20ChatGPT-assisted%20code%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13656v1&entry.124074799=Read"},
{"title": "IReNe: Instant Recoloring of Neural Radiance Fields", "author": "Alessio Mazzucchelli and Adrian Garcia-Garcia and Elena Garces and Fernando Rivas-Manzaneque and Francesc Moreno-Noguer and Adrian Penate-Sanchez", "abstract": "  Advances in NERFs have allowed for 3D scene reconstructions and novel view\nsynthesis. Yet, efficiently editing these representations while retaining\nphotorealism is an emerging challenge. Recent methods face three primary\nlimitations: they're slow for interactive use, lack precision at object\nboundaries, and struggle to ensure multi-view consistency. We introduce IReNe\nto address these limitations, enabling swift, near real-time color editing in\nNeRF. Leveraging a pre-trained NeRF model and a single training image with\nuser-applied color edits, IReNe swiftly adjusts network parameters in seconds.\nThis adjustment allows the model to generate new scene views, accurately\nrepresenting the color changes from the training image while also controlling\nobject boundaries and view-specific effects. Object boundary control is\nachieved by integrating a trainable segmentation module into the model. The\nprocess gains efficiency by retraining only the weights of the last network\nlayer. We observed that neurons in this layer can be classified into those\nresponsible for view-dependent appearance and those contributing to diffuse\nappearance. We introduce an automated classification approach to identify these\nneuron types and exclusively fine-tune the weights of the diffuse neurons. This\nfurther accelerates training and ensures consistent color edits across\ndifferent views. A thorough validation on a new dataset, with edited object\ncolors, shows significant quantitative and qualitative advancements over\ncompetitors, accelerating speeds by 5x to 500x.\n", "link": "http://arxiv.org/abs/2405.19876v3", "date": "2025-04-18", "relevancy": 2.1824, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5674}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5388}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IReNe%3A%20Instant%20Recoloring%20of%20Neural%20Radiance%20Fields&body=Title%3A%20IReNe%3A%20Instant%20Recoloring%20of%20Neural%20Radiance%20Fields%0AAuthor%3A%20Alessio%20Mazzucchelli%20and%20Adrian%20Garcia-Garcia%20and%20Elena%20Garces%20and%20Fernando%20Rivas-Manzaneque%20and%20Francesc%20Moreno-Noguer%20and%20Adrian%20Penate-Sanchez%0AAbstract%3A%20%20%20Advances%20in%20NERFs%20have%20allowed%20for%203D%20scene%20reconstructions%20and%20novel%20view%0Asynthesis.%20Yet%2C%20efficiently%20editing%20these%20representations%20while%20retaining%0Aphotorealism%20is%20an%20emerging%20challenge.%20Recent%20methods%20face%20three%20primary%0Alimitations%3A%20they%27re%20slow%20for%20interactive%20use%2C%20lack%20precision%20at%20object%0Aboundaries%2C%20and%20struggle%20to%20ensure%20multi-view%20consistency.%20We%20introduce%20IReNe%0Ato%20address%20these%20limitations%2C%20enabling%20swift%2C%20near%20real-time%20color%20editing%20in%0ANeRF.%20Leveraging%20a%20pre-trained%20NeRF%20model%20and%20a%20single%20training%20image%20with%0Auser-applied%20color%20edits%2C%20IReNe%20swiftly%20adjusts%20network%20parameters%20in%20seconds.%0AThis%20adjustment%20allows%20the%20model%20to%20generate%20new%20scene%20views%2C%20accurately%0Arepresenting%20the%20color%20changes%20from%20the%20training%20image%20while%20also%20controlling%0Aobject%20boundaries%20and%20view-specific%20effects.%20Object%20boundary%20control%20is%0Aachieved%20by%20integrating%20a%20trainable%20segmentation%20module%20into%20the%20model.%20The%0Aprocess%20gains%20efficiency%20by%20retraining%20only%20the%20weights%20of%20the%20last%20network%0Alayer.%20We%20observed%20that%20neurons%20in%20this%20layer%20can%20be%20classified%20into%20those%0Aresponsible%20for%20view-dependent%20appearance%20and%20those%20contributing%20to%20diffuse%0Aappearance.%20We%20introduce%20an%20automated%20classification%20approach%20to%20identify%20these%0Aneuron%20types%20and%20exclusively%20fine-tune%20the%20weights%20of%20the%20diffuse%20neurons.%20This%0Afurther%20accelerates%20training%20and%20ensures%20consistent%20color%20edits%20across%0Adifferent%20views.%20A%20thorough%20validation%20on%20a%20new%20dataset%2C%20with%20edited%20object%0Acolors%2C%20shows%20significant%20quantitative%20and%20qualitative%20advancements%20over%0Acompetitors%2C%20accelerating%20speeds%20by%205x%20to%20500x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19876v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIReNe%253A%2520Instant%2520Recoloring%2520of%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DAlessio%2520Mazzucchelli%2520and%2520Adrian%2520Garcia-Garcia%2520and%2520Elena%2520Garces%2520and%2520Fernando%2520Rivas-Manzaneque%2520and%2520Francesc%2520Moreno-Noguer%2520and%2520Adrian%2520Penate-Sanchez%26entry.1292438233%3D%2520%2520Advances%2520in%2520NERFs%2520have%2520allowed%2520for%25203D%2520scene%2520reconstructions%2520and%2520novel%2520view%250Asynthesis.%2520Yet%252C%2520efficiently%2520editing%2520these%2520representations%2520while%2520retaining%250Aphotorealism%2520is%2520an%2520emerging%2520challenge.%2520Recent%2520methods%2520face%2520three%2520primary%250Alimitations%253A%2520they%2527re%2520slow%2520for%2520interactive%2520use%252C%2520lack%2520precision%2520at%2520object%250Aboundaries%252C%2520and%2520struggle%2520to%2520ensure%2520multi-view%2520consistency.%2520We%2520introduce%2520IReNe%250Ato%2520address%2520these%2520limitations%252C%2520enabling%2520swift%252C%2520near%2520real-time%2520color%2520editing%2520in%250ANeRF.%2520Leveraging%2520a%2520pre-trained%2520NeRF%2520model%2520and%2520a%2520single%2520training%2520image%2520with%250Auser-applied%2520color%2520edits%252C%2520IReNe%2520swiftly%2520adjusts%2520network%2520parameters%2520in%2520seconds.%250AThis%2520adjustment%2520allows%2520the%2520model%2520to%2520generate%2520new%2520scene%2520views%252C%2520accurately%250Arepresenting%2520the%2520color%2520changes%2520from%2520the%2520training%2520image%2520while%2520also%2520controlling%250Aobject%2520boundaries%2520and%2520view-specific%2520effects.%2520Object%2520boundary%2520control%2520is%250Aachieved%2520by%2520integrating%2520a%2520trainable%2520segmentation%2520module%2520into%2520the%2520model.%2520The%250Aprocess%2520gains%2520efficiency%2520by%2520retraining%2520only%2520the%2520weights%2520of%2520the%2520last%2520network%250Alayer.%2520We%2520observed%2520that%2520neurons%2520in%2520this%2520layer%2520can%2520be%2520classified%2520into%2520those%250Aresponsible%2520for%2520view-dependent%2520appearance%2520and%2520those%2520contributing%2520to%2520diffuse%250Aappearance.%2520We%2520introduce%2520an%2520automated%2520classification%2520approach%2520to%2520identify%2520these%250Aneuron%2520types%2520and%2520exclusively%2520fine-tune%2520the%2520weights%2520of%2520the%2520diffuse%2520neurons.%2520This%250Afurther%2520accelerates%2520training%2520and%2520ensures%2520consistent%2520color%2520edits%2520across%250Adifferent%2520views.%2520A%2520thorough%2520validation%2520on%2520a%2520new%2520dataset%252C%2520with%2520edited%2520object%250Acolors%252C%2520shows%2520significant%2520quantitative%2520and%2520qualitative%2520advancements%2520over%250Acompetitors%252C%2520accelerating%2520speeds%2520by%25205x%2520to%2520500x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19876v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IReNe%3A%20Instant%20Recoloring%20of%20Neural%20Radiance%20Fields&entry.906535625=Alessio%20Mazzucchelli%20and%20Adrian%20Garcia-Garcia%20and%20Elena%20Garces%20and%20Fernando%20Rivas-Manzaneque%20and%20Francesc%20Moreno-Noguer%20and%20Adrian%20Penate-Sanchez&entry.1292438233=%20%20Advances%20in%20NERFs%20have%20allowed%20for%203D%20scene%20reconstructions%20and%20novel%20view%0Asynthesis.%20Yet%2C%20efficiently%20editing%20these%20representations%20while%20retaining%0Aphotorealism%20is%20an%20emerging%20challenge.%20Recent%20methods%20face%20three%20primary%0Alimitations%3A%20they%27re%20slow%20for%20interactive%20use%2C%20lack%20precision%20at%20object%0Aboundaries%2C%20and%20struggle%20to%20ensure%20multi-view%20consistency.%20We%20introduce%20IReNe%0Ato%20address%20these%20limitations%2C%20enabling%20swift%2C%20near%20real-time%20color%20editing%20in%0ANeRF.%20Leveraging%20a%20pre-trained%20NeRF%20model%20and%20a%20single%20training%20image%20with%0Auser-applied%20color%20edits%2C%20IReNe%20swiftly%20adjusts%20network%20parameters%20in%20seconds.%0AThis%20adjustment%20allows%20the%20model%20to%20generate%20new%20scene%20views%2C%20accurately%0Arepresenting%20the%20color%20changes%20from%20the%20training%20image%20while%20also%20controlling%0Aobject%20boundaries%20and%20view-specific%20effects.%20Object%20boundary%20control%20is%0Aachieved%20by%20integrating%20a%20trainable%20segmentation%20module%20into%20the%20model.%20The%0Aprocess%20gains%20efficiency%20by%20retraining%20only%20the%20weights%20of%20the%20last%20network%0Alayer.%20We%20observed%20that%20neurons%20in%20this%20layer%20can%20be%20classified%20into%20those%0Aresponsible%20for%20view-dependent%20appearance%20and%20those%20contributing%20to%20diffuse%0Aappearance.%20We%20introduce%20an%20automated%20classification%20approach%20to%20identify%20these%0Aneuron%20types%20and%20exclusively%20fine-tune%20the%20weights%20of%20the%20diffuse%20neurons.%20This%0Afurther%20accelerates%20training%20and%20ensures%20consistent%20color%20edits%20across%0Adifferent%20views.%20A%20thorough%20validation%20on%20a%20new%20dataset%2C%20with%20edited%20object%0Acolors%2C%20shows%20significant%20quantitative%20and%20qualitative%20advancements%20over%0Acompetitors%2C%20accelerating%20speeds%20by%205x%20to%20500x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19876v3&entry.124074799=Read"},
{"title": "Human-aligned Deep Learning: Explainability, Causality, and Biological\n  Inspiration", "author": "Gianluca Carloni", "abstract": "  This work aligns deep learning (DL) with human reasoning capabilities and\nneeds to enable more efficient, interpretable, and robust image classification.\nWe approach this from three perspectives: explainability, causality, and\nbiological vision. Introduction and background open this work before diving\ninto operative chapters. First, we assess neural networks' visualization\ntechniques for medical images and validate an explainable-by-design method for\nbreast mass classification. A comprehensive review at the intersection of XAI\nand causality follows, where we introduce a general scaffold to organize past\nand future research, laying the groundwork for our second perspective. In the\ncausality direction, we propose novel modules that exploit feature\nco-occurrence in medical images, leading to more effective and explainable\npredictions. We further introduce CROCODILE, a general framework that\nintegrates causal concepts, contrastive learning, feature disentanglement, and\nprior knowledge to enhance generalization. Lastly, we explore biological\nvision, examining how humans recognize objects, and propose CoCoReco, a\nconnectivity-inspired network with context-aware attention mechanisms. Overall,\nour key findings include: (i) simple activation maximization lacks insight for\nmedical imaging DL models; (ii) prototypical-part learning is effective and\nradiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak\ncausal signals can be leveraged without a priori information to improve\nperformance and interpretability; (v) our framework generalizes across medical\ndomains and out-of-distribution data; (vi) incorporating biological circuit\nmotifs improves human-aligned recognition. This work contributes toward\nhuman-aligned DL and highlights pathways to bridge the gap between research and\nclinical adoption, with implications for improved trust, diagnostic accuracy,\nand safe deployment.\n", "link": "http://arxiv.org/abs/2504.13717v1", "date": "2025-04-18", "relevancy": 2.1677, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-aligned%20Deep%20Learning%3A%20Explainability%2C%20Causality%2C%20and%20Biological%0A%20%20Inspiration&body=Title%3A%20Human-aligned%20Deep%20Learning%3A%20Explainability%2C%20Causality%2C%20and%20Biological%0A%20%20Inspiration%0AAuthor%3A%20Gianluca%20Carloni%0AAbstract%3A%20%20%20This%20work%20aligns%20deep%20learning%20%28DL%29%20with%20human%20reasoning%20capabilities%20and%0Aneeds%20to%20enable%20more%20efficient%2C%20interpretable%2C%20and%20robust%20image%20classification.%0AWe%20approach%20this%20from%20three%20perspectives%3A%20explainability%2C%20causality%2C%20and%0Abiological%20vision.%20Introduction%20and%20background%20open%20this%20work%20before%20diving%0Ainto%20operative%20chapters.%20First%2C%20we%20assess%20neural%20networks%27%20visualization%0Atechniques%20for%20medical%20images%20and%20validate%20an%20explainable-by-design%20method%20for%0Abreast%20mass%20classification.%20A%20comprehensive%20review%20at%20the%20intersection%20of%20XAI%0Aand%20causality%20follows%2C%20where%20we%20introduce%20a%20general%20scaffold%20to%20organize%20past%0Aand%20future%20research%2C%20laying%20the%20groundwork%20for%20our%20second%20perspective.%20In%20the%0Acausality%20direction%2C%20we%20propose%20novel%20modules%20that%20exploit%20feature%0Aco-occurrence%20in%20medical%20images%2C%20leading%20to%20more%20effective%20and%20explainable%0Apredictions.%20We%20further%20introduce%20CROCODILE%2C%20a%20general%20framework%20that%0Aintegrates%20causal%20concepts%2C%20contrastive%20learning%2C%20feature%20disentanglement%2C%20and%0Aprior%20knowledge%20to%20enhance%20generalization.%20Lastly%2C%20we%20explore%20biological%0Avision%2C%20examining%20how%20humans%20recognize%20objects%2C%20and%20propose%20CoCoReco%2C%20a%0Aconnectivity-inspired%20network%20with%20context-aware%20attention%20mechanisms.%20Overall%2C%0Aour%20key%20findings%20include%3A%20%28i%29%20simple%20activation%20maximization%20lacks%20insight%20for%0Amedical%20imaging%20DL%20models%3B%20%28ii%29%20prototypical-part%20learning%20is%20effective%20and%0Aradiologically%20aligned%3B%20%28iii%29%20XAI%20and%20causal%20ML%20are%20deeply%20connected%3B%20%28iv%29%20weak%0Acausal%20signals%20can%20be%20leveraged%20without%20a%20priori%20information%20to%20improve%0Aperformance%20and%20interpretability%3B%20%28v%29%20our%20framework%20generalizes%20across%20medical%0Adomains%20and%20out-of-distribution%20data%3B%20%28vi%29%20incorporating%20biological%20circuit%0Amotifs%20improves%20human-aligned%20recognition.%20This%20work%20contributes%20toward%0Ahuman-aligned%20DL%20and%20highlights%20pathways%20to%20bridge%20the%20gap%20between%20research%20and%0Aclinical%20adoption%2C%20with%20implications%20for%20improved%20trust%2C%20diagnostic%20accuracy%2C%0Aand%20safe%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-aligned%2520Deep%2520Learning%253A%2520Explainability%252C%2520Causality%252C%2520and%2520Biological%250A%2520%2520Inspiration%26entry.906535625%3DGianluca%2520Carloni%26entry.1292438233%3D%2520%2520This%2520work%2520aligns%2520deep%2520learning%2520%2528DL%2529%2520with%2520human%2520reasoning%2520capabilities%2520and%250Aneeds%2520to%2520enable%2520more%2520efficient%252C%2520interpretable%252C%2520and%2520robust%2520image%2520classification.%250AWe%2520approach%2520this%2520from%2520three%2520perspectives%253A%2520explainability%252C%2520causality%252C%2520and%250Abiological%2520vision.%2520Introduction%2520and%2520background%2520open%2520this%2520work%2520before%2520diving%250Ainto%2520operative%2520chapters.%2520First%252C%2520we%2520assess%2520neural%2520networks%2527%2520visualization%250Atechniques%2520for%2520medical%2520images%2520and%2520validate%2520an%2520explainable-by-design%2520method%2520for%250Abreast%2520mass%2520classification.%2520A%2520comprehensive%2520review%2520at%2520the%2520intersection%2520of%2520XAI%250Aand%2520causality%2520follows%252C%2520where%2520we%2520introduce%2520a%2520general%2520scaffold%2520to%2520organize%2520past%250Aand%2520future%2520research%252C%2520laying%2520the%2520groundwork%2520for%2520our%2520second%2520perspective.%2520In%2520the%250Acausality%2520direction%252C%2520we%2520propose%2520novel%2520modules%2520that%2520exploit%2520feature%250Aco-occurrence%2520in%2520medical%2520images%252C%2520leading%2520to%2520more%2520effective%2520and%2520explainable%250Apredictions.%2520We%2520further%2520introduce%2520CROCODILE%252C%2520a%2520general%2520framework%2520that%250Aintegrates%2520causal%2520concepts%252C%2520contrastive%2520learning%252C%2520feature%2520disentanglement%252C%2520and%250Aprior%2520knowledge%2520to%2520enhance%2520generalization.%2520Lastly%252C%2520we%2520explore%2520biological%250Avision%252C%2520examining%2520how%2520humans%2520recognize%2520objects%252C%2520and%2520propose%2520CoCoReco%252C%2520a%250Aconnectivity-inspired%2520network%2520with%2520context-aware%2520attention%2520mechanisms.%2520Overall%252C%250Aour%2520key%2520findings%2520include%253A%2520%2528i%2529%2520simple%2520activation%2520maximization%2520lacks%2520insight%2520for%250Amedical%2520imaging%2520DL%2520models%253B%2520%2528ii%2529%2520prototypical-part%2520learning%2520is%2520effective%2520and%250Aradiologically%2520aligned%253B%2520%2528iii%2529%2520XAI%2520and%2520causal%2520ML%2520are%2520deeply%2520connected%253B%2520%2528iv%2529%2520weak%250Acausal%2520signals%2520can%2520be%2520leveraged%2520without%2520a%2520priori%2520information%2520to%2520improve%250Aperformance%2520and%2520interpretability%253B%2520%2528v%2529%2520our%2520framework%2520generalizes%2520across%2520medical%250Adomains%2520and%2520out-of-distribution%2520data%253B%2520%2528vi%2529%2520incorporating%2520biological%2520circuit%250Amotifs%2520improves%2520human-aligned%2520recognition.%2520This%2520work%2520contributes%2520toward%250Ahuman-aligned%2520DL%2520and%2520highlights%2520pathways%2520to%2520bridge%2520the%2520gap%2520between%2520research%2520and%250Aclinical%2520adoption%252C%2520with%2520implications%2520for%2520improved%2520trust%252C%2520diagnostic%2520accuracy%252C%250Aand%2520safe%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-aligned%20Deep%20Learning%3A%20Explainability%2C%20Causality%2C%20and%20Biological%0A%20%20Inspiration&entry.906535625=Gianluca%20Carloni&entry.1292438233=%20%20This%20work%20aligns%20deep%20learning%20%28DL%29%20with%20human%20reasoning%20capabilities%20and%0Aneeds%20to%20enable%20more%20efficient%2C%20interpretable%2C%20and%20robust%20image%20classification.%0AWe%20approach%20this%20from%20three%20perspectives%3A%20explainability%2C%20causality%2C%20and%0Abiological%20vision.%20Introduction%20and%20background%20open%20this%20work%20before%20diving%0Ainto%20operative%20chapters.%20First%2C%20we%20assess%20neural%20networks%27%20visualization%0Atechniques%20for%20medical%20images%20and%20validate%20an%20explainable-by-design%20method%20for%0Abreast%20mass%20classification.%20A%20comprehensive%20review%20at%20the%20intersection%20of%20XAI%0Aand%20causality%20follows%2C%20where%20we%20introduce%20a%20general%20scaffold%20to%20organize%20past%0Aand%20future%20research%2C%20laying%20the%20groundwork%20for%20our%20second%20perspective.%20In%20the%0Acausality%20direction%2C%20we%20propose%20novel%20modules%20that%20exploit%20feature%0Aco-occurrence%20in%20medical%20images%2C%20leading%20to%20more%20effective%20and%20explainable%0Apredictions.%20We%20further%20introduce%20CROCODILE%2C%20a%20general%20framework%20that%0Aintegrates%20causal%20concepts%2C%20contrastive%20learning%2C%20feature%20disentanglement%2C%20and%0Aprior%20knowledge%20to%20enhance%20generalization.%20Lastly%2C%20we%20explore%20biological%0Avision%2C%20examining%20how%20humans%20recognize%20objects%2C%20and%20propose%20CoCoReco%2C%20a%0Aconnectivity-inspired%20network%20with%20context-aware%20attention%20mechanisms.%20Overall%2C%0Aour%20key%20findings%20include%3A%20%28i%29%20simple%20activation%20maximization%20lacks%20insight%20for%0Amedical%20imaging%20DL%20models%3B%20%28ii%29%20prototypical-part%20learning%20is%20effective%20and%0Aradiologically%20aligned%3B%20%28iii%29%20XAI%20and%20causal%20ML%20are%20deeply%20connected%3B%20%28iv%29%20weak%0Acausal%20signals%20can%20be%20leveraged%20without%20a%20priori%20information%20to%20improve%0Aperformance%20and%20interpretability%3B%20%28v%29%20our%20framework%20generalizes%20across%20medical%0Adomains%20and%20out-of-distribution%20data%3B%20%28vi%29%20incorporating%20biological%20circuit%0Amotifs%20improves%20human-aligned%20recognition.%20This%20work%20contributes%20toward%0Ahuman-aligned%20DL%20and%20highlights%20pathways%20to%20bridge%20the%20gap%20between%20research%20and%0Aclinical%20adoption%2C%20with%20implications%20for%20improved%20trust%2C%20diagnostic%20accuracy%2C%0Aand%20safe%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13717v1&entry.124074799=Read"},
{"title": "NRGBoost: Energy-Based Generative Boosted Trees", "author": "Jo\u00e3o Bravo", "abstract": "  Despite the rise to dominance of deep learning in unstructured data domains,\ntree-based methods such as Random Forests (RF) and Gradient Boosted Decision\nTrees (GBDT) are still the workhorses for handling discriminative tasks on\ntabular data. We explore generative extensions of these popular algorithms with\na focus on explicitly modeling the data density (up to a normalization\nconstant), thus enabling other applications besides sampling. As our main\ncontribution we propose an energy-based generative boosting algorithm that is\nanalogous to the second-order boosting implemented in popular libraries like\nXGBoost. We show that, despite producing a generative model capable of handling\ninference tasks over any input variable, our proposed algorithm can achieve\nsimilar discriminative performance to GBDT on a number of real world tabular\ndatasets, outperforming alternative generative approaches. At the same time, we\nshow that it is also competitive with neural-network-based models for sampling.\nCode is available at https://github.com/ajoo/nrgboost.\n", "link": "http://arxiv.org/abs/2410.03535v2", "date": "2025-04-18", "relevancy": 2.1641, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5762}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5212}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NRGBoost%3A%20Energy-Based%20Generative%20Boosted%20Trees&body=Title%3A%20NRGBoost%3A%20Energy-Based%20Generative%20Boosted%20Trees%0AAuthor%3A%20Jo%C3%A3o%20Bravo%0AAbstract%3A%20%20%20Despite%20the%20rise%20to%20dominance%20of%20deep%20learning%20in%20unstructured%20data%20domains%2C%0Atree-based%20methods%20such%20as%20Random%20Forests%20%28RF%29%20and%20Gradient%20Boosted%20Decision%0ATrees%20%28GBDT%29%20are%20still%20the%20workhorses%20for%20handling%20discriminative%20tasks%20on%0Atabular%20data.%20We%20explore%20generative%20extensions%20of%20these%20popular%20algorithms%20with%0Aa%20focus%20on%20explicitly%20modeling%20the%20data%20density%20%28up%20to%20a%20normalization%0Aconstant%29%2C%20thus%20enabling%20other%20applications%20besides%20sampling.%20As%20our%20main%0Acontribution%20we%20propose%20an%20energy-based%20generative%20boosting%20algorithm%20that%20is%0Aanalogous%20to%20the%20second-order%20boosting%20implemented%20in%20popular%20libraries%20like%0AXGBoost.%20We%20show%20that%2C%20despite%20producing%20a%20generative%20model%20capable%20of%20handling%0Ainference%20tasks%20over%20any%20input%20variable%2C%20our%20proposed%20algorithm%20can%20achieve%0Asimilar%20discriminative%20performance%20to%20GBDT%20on%20a%20number%20of%20real%20world%20tabular%0Adatasets%2C%20outperforming%20alternative%20generative%20approaches.%20At%20the%20same%20time%2C%20we%0Ashow%20that%20it%20is%20also%20competitive%20with%20neural-network-based%20models%20for%20sampling.%0ACode%20is%20available%20at%20https%3A//github.com/ajoo/nrgboost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03535v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNRGBoost%253A%2520Energy-Based%2520Generative%2520Boosted%2520Trees%26entry.906535625%3DJo%25C3%25A3o%2520Bravo%26entry.1292438233%3D%2520%2520Despite%2520the%2520rise%2520to%2520dominance%2520of%2520deep%2520learning%2520in%2520unstructured%2520data%2520domains%252C%250Atree-based%2520methods%2520such%2520as%2520Random%2520Forests%2520%2528RF%2529%2520and%2520Gradient%2520Boosted%2520Decision%250ATrees%2520%2528GBDT%2529%2520are%2520still%2520the%2520workhorses%2520for%2520handling%2520discriminative%2520tasks%2520on%250Atabular%2520data.%2520We%2520explore%2520generative%2520extensions%2520of%2520these%2520popular%2520algorithms%2520with%250Aa%2520focus%2520on%2520explicitly%2520modeling%2520the%2520data%2520density%2520%2528up%2520to%2520a%2520normalization%250Aconstant%2529%252C%2520thus%2520enabling%2520other%2520applications%2520besides%2520sampling.%2520As%2520our%2520main%250Acontribution%2520we%2520propose%2520an%2520energy-based%2520generative%2520boosting%2520algorithm%2520that%2520is%250Aanalogous%2520to%2520the%2520second-order%2520boosting%2520implemented%2520in%2520popular%2520libraries%2520like%250AXGBoost.%2520We%2520show%2520that%252C%2520despite%2520producing%2520a%2520generative%2520model%2520capable%2520of%2520handling%250Ainference%2520tasks%2520over%2520any%2520input%2520variable%252C%2520our%2520proposed%2520algorithm%2520can%2520achieve%250Asimilar%2520discriminative%2520performance%2520to%2520GBDT%2520on%2520a%2520number%2520of%2520real%2520world%2520tabular%250Adatasets%252C%2520outperforming%2520alternative%2520generative%2520approaches.%2520At%2520the%2520same%2520time%252C%2520we%250Ashow%2520that%2520it%2520is%2520also%2520competitive%2520with%2520neural-network-based%2520models%2520for%2520sampling.%250ACode%2520is%2520available%2520at%2520https%253A//github.com/ajoo/nrgboost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03535v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NRGBoost%3A%20Energy-Based%20Generative%20Boosted%20Trees&entry.906535625=Jo%C3%A3o%20Bravo&entry.1292438233=%20%20Despite%20the%20rise%20to%20dominance%20of%20deep%20learning%20in%20unstructured%20data%20domains%2C%0Atree-based%20methods%20such%20as%20Random%20Forests%20%28RF%29%20and%20Gradient%20Boosted%20Decision%0ATrees%20%28GBDT%29%20are%20still%20the%20workhorses%20for%20handling%20discriminative%20tasks%20on%0Atabular%20data.%20We%20explore%20generative%20extensions%20of%20these%20popular%20algorithms%20with%0Aa%20focus%20on%20explicitly%20modeling%20the%20data%20density%20%28up%20to%20a%20normalization%0Aconstant%29%2C%20thus%20enabling%20other%20applications%20besides%20sampling.%20As%20our%20main%0Acontribution%20we%20propose%20an%20energy-based%20generative%20boosting%20algorithm%20that%20is%0Aanalogous%20to%20the%20second-order%20boosting%20implemented%20in%20popular%20libraries%20like%0AXGBoost.%20We%20show%20that%2C%20despite%20producing%20a%20generative%20model%20capable%20of%20handling%0Ainference%20tasks%20over%20any%20input%20variable%2C%20our%20proposed%20algorithm%20can%20achieve%0Asimilar%20discriminative%20performance%20to%20GBDT%20on%20a%20number%20of%20real%20world%20tabular%0Adatasets%2C%20outperforming%20alternative%20generative%20approaches.%20At%20the%20same%20time%2C%20we%0Ashow%20that%20it%20is%20also%20competitive%20with%20neural-network-based%20models%20for%20sampling.%0ACode%20is%20available%20at%20https%3A//github.com/ajoo/nrgboost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03535v2&entry.124074799=Read"},
{"title": "LimitNet: Progressive, Content-Aware Image Offloading for Extremely Weak\n  Devices & Networks", "author": "Ali Hojjat and Janek Haberer and Tayyaba Zainab and Olaf Landsiedel", "abstract": "  IoT devices have limited hardware capabilities and are often deployed in\nremote areas. Consequently, advanced vision models surpass such devices'\nprocessing and storage capabilities, requiring offloading of such tasks to the\ncloud. However, remote areas often rely on LPWANs technology with limited\nbandwidth, high packet loss rates, and extremely low duty cycles, which makes\nfast offloading for time-sensitive inference challenging. Today's approaches,\nwhich are deployable on weak devices, generate a non-progressive bit stream,\nand therefore, their decoding quality suffers strongly when data is only\npartially available on the cloud at a deadline due to limited bandwidth or\npacket losses.\n  In this paper, we introduce LimitNet, a progressive, content-aware image\ncompression model designed for extremely weak devices and networks. LimitNet's\nlightweight progressive encoder prioritizes critical data during transmission\nbased on the content of the image, which gives the cloud the opportunity to run\ninference even with partial data availability.\n  Experimental results demonstrate that LimitNet, on average, compared to SOTA,\nachieves 14.01 p.p. (percentage point) higher accuracy on ImageNet1000, 18.01\npp on CIFAR100, and 0.1 higher mAP@0.5 on COCO. Also, on average, LimitNet\nsaves 61.24% bandwidth on ImageNet1000, 83.68% on CIFAR100, and 42.25% on the\nCOCO dataset compared to SOTA, while it only has 4% more encoding time compared\nto JPEG (with a fixed quality) on STM32F7 (Cortex-M7).\n", "link": "http://arxiv.org/abs/2504.13736v1", "date": "2025-04-18", "relevancy": 2.1579, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5991}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5513}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LimitNet%3A%20Progressive%2C%20Content-Aware%20Image%20Offloading%20for%20Extremely%20Weak%0A%20%20Devices%20%26%20Networks&body=Title%3A%20LimitNet%3A%20Progressive%2C%20Content-Aware%20Image%20Offloading%20for%20Extremely%20Weak%0A%20%20Devices%20%26%20Networks%0AAuthor%3A%20Ali%20Hojjat%20and%20Janek%20Haberer%20and%20Tayyaba%20Zainab%20and%20Olaf%20Landsiedel%0AAbstract%3A%20%20%20IoT%20devices%20have%20limited%20hardware%20capabilities%20and%20are%20often%20deployed%20in%0Aremote%20areas.%20Consequently%2C%20advanced%20vision%20models%20surpass%20such%20devices%27%0Aprocessing%20and%20storage%20capabilities%2C%20requiring%20offloading%20of%20such%20tasks%20to%20the%0Acloud.%20However%2C%20remote%20areas%20often%20rely%20on%20LPWANs%20technology%20with%20limited%0Abandwidth%2C%20high%20packet%20loss%20rates%2C%20and%20extremely%20low%20duty%20cycles%2C%20which%20makes%0Afast%20offloading%20for%20time-sensitive%20inference%20challenging.%20Today%27s%20approaches%2C%0Awhich%20are%20deployable%20on%20weak%20devices%2C%20generate%20a%20non-progressive%20bit%20stream%2C%0Aand%20therefore%2C%20their%20decoding%20quality%20suffers%20strongly%20when%20data%20is%20only%0Apartially%20available%20on%20the%20cloud%20at%20a%20deadline%20due%20to%20limited%20bandwidth%20or%0Apacket%20losses.%0A%20%20In%20this%20paper%2C%20we%20introduce%20LimitNet%2C%20a%20progressive%2C%20content-aware%20image%0Acompression%20model%20designed%20for%20extremely%20weak%20devices%20and%20networks.%20LimitNet%27s%0Alightweight%20progressive%20encoder%20prioritizes%20critical%20data%20during%20transmission%0Abased%20on%20the%20content%20of%20the%20image%2C%20which%20gives%20the%20cloud%20the%20opportunity%20to%20run%0Ainference%20even%20with%20partial%20data%20availability.%0A%20%20Experimental%20results%20demonstrate%20that%20LimitNet%2C%20on%20average%2C%20compared%20to%20SOTA%2C%0Aachieves%2014.01%20p.p.%20%28percentage%20point%29%20higher%20accuracy%20on%20ImageNet1000%2C%2018.01%0App%20on%20CIFAR100%2C%20and%200.1%20higher%20mAP%400.5%20on%20COCO.%20Also%2C%20on%20average%2C%20LimitNet%0Asaves%2061.24%25%20bandwidth%20on%20ImageNet1000%2C%2083.68%25%20on%20CIFAR100%2C%20and%2042.25%25%20on%20the%0ACOCO%20dataset%20compared%20to%20SOTA%2C%20while%20it%20only%20has%204%25%20more%20encoding%20time%20compared%0Ato%20JPEG%20%28with%20a%20fixed%20quality%29%20on%20STM32F7%20%28Cortex-M7%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimitNet%253A%2520Progressive%252C%2520Content-Aware%2520Image%2520Offloading%2520for%2520Extremely%2520Weak%250A%2520%2520Devices%2520%2526%2520Networks%26entry.906535625%3DAli%2520Hojjat%2520and%2520Janek%2520Haberer%2520and%2520Tayyaba%2520Zainab%2520and%2520Olaf%2520Landsiedel%26entry.1292438233%3D%2520%2520IoT%2520devices%2520have%2520limited%2520hardware%2520capabilities%2520and%2520are%2520often%2520deployed%2520in%250Aremote%2520areas.%2520Consequently%252C%2520advanced%2520vision%2520models%2520surpass%2520such%2520devices%2527%250Aprocessing%2520and%2520storage%2520capabilities%252C%2520requiring%2520offloading%2520of%2520such%2520tasks%2520to%2520the%250Acloud.%2520However%252C%2520remote%2520areas%2520often%2520rely%2520on%2520LPWANs%2520technology%2520with%2520limited%250Abandwidth%252C%2520high%2520packet%2520loss%2520rates%252C%2520and%2520extremely%2520low%2520duty%2520cycles%252C%2520which%2520makes%250Afast%2520offloading%2520for%2520time-sensitive%2520inference%2520challenging.%2520Today%2527s%2520approaches%252C%250Awhich%2520are%2520deployable%2520on%2520weak%2520devices%252C%2520generate%2520a%2520non-progressive%2520bit%2520stream%252C%250Aand%2520therefore%252C%2520their%2520decoding%2520quality%2520suffers%2520strongly%2520when%2520data%2520is%2520only%250Apartially%2520available%2520on%2520the%2520cloud%2520at%2520a%2520deadline%2520due%2520to%2520limited%2520bandwidth%2520or%250Apacket%2520losses.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520LimitNet%252C%2520a%2520progressive%252C%2520content-aware%2520image%250Acompression%2520model%2520designed%2520for%2520extremely%2520weak%2520devices%2520and%2520networks.%2520LimitNet%2527s%250Alightweight%2520progressive%2520encoder%2520prioritizes%2520critical%2520data%2520during%2520transmission%250Abased%2520on%2520the%2520content%2520of%2520the%2520image%252C%2520which%2520gives%2520the%2520cloud%2520the%2520opportunity%2520to%2520run%250Ainference%2520even%2520with%2520partial%2520data%2520availability.%250A%2520%2520Experimental%2520results%2520demonstrate%2520that%2520LimitNet%252C%2520on%2520average%252C%2520compared%2520to%2520SOTA%252C%250Aachieves%252014.01%2520p.p.%2520%2528percentage%2520point%2529%2520higher%2520accuracy%2520on%2520ImageNet1000%252C%252018.01%250App%2520on%2520CIFAR100%252C%2520and%25200.1%2520higher%2520mAP%25400.5%2520on%2520COCO.%2520Also%252C%2520on%2520average%252C%2520LimitNet%250Asaves%252061.24%2525%2520bandwidth%2520on%2520ImageNet1000%252C%252083.68%2525%2520on%2520CIFAR100%252C%2520and%252042.25%2525%2520on%2520the%250ACOCO%2520dataset%2520compared%2520to%2520SOTA%252C%2520while%2520it%2520only%2520has%25204%2525%2520more%2520encoding%2520time%2520compared%250Ato%2520JPEG%2520%2528with%2520a%2520fixed%2520quality%2529%2520on%2520STM32F7%2520%2528Cortex-M7%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LimitNet%3A%20Progressive%2C%20Content-Aware%20Image%20Offloading%20for%20Extremely%20Weak%0A%20%20Devices%20%26%20Networks&entry.906535625=Ali%20Hojjat%20and%20Janek%20Haberer%20and%20Tayyaba%20Zainab%20and%20Olaf%20Landsiedel&entry.1292438233=%20%20IoT%20devices%20have%20limited%20hardware%20capabilities%20and%20are%20often%20deployed%20in%0Aremote%20areas.%20Consequently%2C%20advanced%20vision%20models%20surpass%20such%20devices%27%0Aprocessing%20and%20storage%20capabilities%2C%20requiring%20offloading%20of%20such%20tasks%20to%20the%0Acloud.%20However%2C%20remote%20areas%20often%20rely%20on%20LPWANs%20technology%20with%20limited%0Abandwidth%2C%20high%20packet%20loss%20rates%2C%20and%20extremely%20low%20duty%20cycles%2C%20which%20makes%0Afast%20offloading%20for%20time-sensitive%20inference%20challenging.%20Today%27s%20approaches%2C%0Awhich%20are%20deployable%20on%20weak%20devices%2C%20generate%20a%20non-progressive%20bit%20stream%2C%0Aand%20therefore%2C%20their%20decoding%20quality%20suffers%20strongly%20when%20data%20is%20only%0Apartially%20available%20on%20the%20cloud%20at%20a%20deadline%20due%20to%20limited%20bandwidth%20or%0Apacket%20losses.%0A%20%20In%20this%20paper%2C%20we%20introduce%20LimitNet%2C%20a%20progressive%2C%20content-aware%20image%0Acompression%20model%20designed%20for%20extremely%20weak%20devices%20and%20networks.%20LimitNet%27s%0Alightweight%20progressive%20encoder%20prioritizes%20critical%20data%20during%20transmission%0Abased%20on%20the%20content%20of%20the%20image%2C%20which%20gives%20the%20cloud%20the%20opportunity%20to%20run%0Ainference%20even%20with%20partial%20data%20availability.%0A%20%20Experimental%20results%20demonstrate%20that%20LimitNet%2C%20on%20average%2C%20compared%20to%20SOTA%2C%0Aachieves%2014.01%20p.p.%20%28percentage%20point%29%20higher%20accuracy%20on%20ImageNet1000%2C%2018.01%0App%20on%20CIFAR100%2C%20and%200.1%20higher%20mAP%400.5%20on%20COCO.%20Also%2C%20on%20average%2C%20LimitNet%0Asaves%2061.24%25%20bandwidth%20on%20ImageNet1000%2C%2083.68%25%20on%20CIFAR100%2C%20and%2042.25%25%20on%20the%0ACOCO%20dataset%20compared%20to%20SOTA%2C%20while%20it%20only%20has%204%25%20more%20encoding%20time%20compared%0Ato%20JPEG%20%28with%20a%20fixed%20quality%29%20on%20STM32F7%20%28Cortex-M7%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13736v1&entry.124074799=Read"},
{"title": "ViG3D-UNet: Volumetric Vascular Connectivity-Aware Segmentation via 3D\n  Vision Graph Representation", "author": "Bowen Liu and Chunlei Meng and Wei Lin and Hongda Zhang and Ziqing Zhou and Zhongxue Gan and Chun Ouyang", "abstract": "  Accurate vascular segmentation is essential for coronary visualization and\nthe diagnosis of coronary heart disease. This task involves the extraction of\nsparse tree-like vascular branches from the volumetric space. However, existing\nmethods have faced significant challenges due to discontinuous vascular\nsegmentation and missing endpoints. To address this issue, a 3D vision graph\nneural network framework, named ViG3D-UNet, was introduced. This method\nintegrates 3D graph representation and aggregation within a U-shaped\narchitecture to facilitate continuous vascular segmentation. The ViG3D module\ncaptures volumetric vascular connectivity and topology, while the convolutional\nmodule extracts fine vascular details. These two branches are combined through\nchannel attention to form the encoder feature. Subsequently, a paperclip-shaped\noffset decoder minimizes redundant computations in the sparse feature space and\nrestores the feature map size to match the original input dimensions. To\nevaluate the effectiveness of the proposed approach for continuous vascular\nsegmentation, evaluations were performed on two public datasets, ASOCA and\nImageCAS. The segmentation results show that the ViG3D-UNet surpassed competing\nmethods in maintaining vascular segmentation connectivity while achieving high\nsegmentation accuracy. Our code will be available soon.\n", "link": "http://arxiv.org/abs/2504.13599v1", "date": "2025-04-18", "relevancy": 2.1566, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5517}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5366}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViG3D-UNet%3A%20Volumetric%20Vascular%20Connectivity-Aware%20Segmentation%20via%203D%0A%20%20Vision%20Graph%20Representation&body=Title%3A%20ViG3D-UNet%3A%20Volumetric%20Vascular%20Connectivity-Aware%20Segmentation%20via%203D%0A%20%20Vision%20Graph%20Representation%0AAuthor%3A%20Bowen%20Liu%20and%20Chunlei%20Meng%20and%20Wei%20Lin%20and%20Hongda%20Zhang%20and%20Ziqing%20Zhou%20and%20Zhongxue%20Gan%20and%20Chun%20Ouyang%0AAbstract%3A%20%20%20Accurate%20vascular%20segmentation%20is%20essential%20for%20coronary%20visualization%20and%0Athe%20diagnosis%20of%20coronary%20heart%20disease.%20This%20task%20involves%20the%20extraction%20of%0Asparse%20tree-like%20vascular%20branches%20from%20the%20volumetric%20space.%20However%2C%20existing%0Amethods%20have%20faced%20significant%20challenges%20due%20to%20discontinuous%20vascular%0Asegmentation%20and%20missing%20endpoints.%20To%20address%20this%20issue%2C%20a%203D%20vision%20graph%0Aneural%20network%20framework%2C%20named%20ViG3D-UNet%2C%20was%20introduced.%20This%20method%0Aintegrates%203D%20graph%20representation%20and%20aggregation%20within%20a%20U-shaped%0Aarchitecture%20to%20facilitate%20continuous%20vascular%20segmentation.%20The%20ViG3D%20module%0Acaptures%20volumetric%20vascular%20connectivity%20and%20topology%2C%20while%20the%20convolutional%0Amodule%20extracts%20fine%20vascular%20details.%20These%20two%20branches%20are%20combined%20through%0Achannel%20attention%20to%20form%20the%20encoder%20feature.%20Subsequently%2C%20a%20paperclip-shaped%0Aoffset%20decoder%20minimizes%20redundant%20computations%20in%20the%20sparse%20feature%20space%20and%0Arestores%20the%20feature%20map%20size%20to%20match%20the%20original%20input%20dimensions.%20To%0Aevaluate%20the%20effectiveness%20of%20the%20proposed%20approach%20for%20continuous%20vascular%0Asegmentation%2C%20evaluations%20were%20performed%20on%20two%20public%20datasets%2C%20ASOCA%20and%0AImageCAS.%20The%20segmentation%20results%20show%20that%20the%20ViG3D-UNet%20surpassed%20competing%0Amethods%20in%20maintaining%20vascular%20segmentation%20connectivity%20while%20achieving%20high%0Asegmentation%20accuracy.%20Our%20code%20will%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViG3D-UNet%253A%2520Volumetric%2520Vascular%2520Connectivity-Aware%2520Segmentation%2520via%25203D%250A%2520%2520Vision%2520Graph%2520Representation%26entry.906535625%3DBowen%2520Liu%2520and%2520Chunlei%2520Meng%2520and%2520Wei%2520Lin%2520and%2520Hongda%2520Zhang%2520and%2520Ziqing%2520Zhou%2520and%2520Zhongxue%2520Gan%2520and%2520Chun%2520Ouyang%26entry.1292438233%3D%2520%2520Accurate%2520vascular%2520segmentation%2520is%2520essential%2520for%2520coronary%2520visualization%2520and%250Athe%2520diagnosis%2520of%2520coronary%2520heart%2520disease.%2520This%2520task%2520involves%2520the%2520extraction%2520of%250Asparse%2520tree-like%2520vascular%2520branches%2520from%2520the%2520volumetric%2520space.%2520However%252C%2520existing%250Amethods%2520have%2520faced%2520significant%2520challenges%2520due%2520to%2520discontinuous%2520vascular%250Asegmentation%2520and%2520missing%2520endpoints.%2520To%2520address%2520this%2520issue%252C%2520a%25203D%2520vision%2520graph%250Aneural%2520network%2520framework%252C%2520named%2520ViG3D-UNet%252C%2520was%2520introduced.%2520This%2520method%250Aintegrates%25203D%2520graph%2520representation%2520and%2520aggregation%2520within%2520a%2520U-shaped%250Aarchitecture%2520to%2520facilitate%2520continuous%2520vascular%2520segmentation.%2520The%2520ViG3D%2520module%250Acaptures%2520volumetric%2520vascular%2520connectivity%2520and%2520topology%252C%2520while%2520the%2520convolutional%250Amodule%2520extracts%2520fine%2520vascular%2520details.%2520These%2520two%2520branches%2520are%2520combined%2520through%250Achannel%2520attention%2520to%2520form%2520the%2520encoder%2520feature.%2520Subsequently%252C%2520a%2520paperclip-shaped%250Aoffset%2520decoder%2520minimizes%2520redundant%2520computations%2520in%2520the%2520sparse%2520feature%2520space%2520and%250Arestores%2520the%2520feature%2520map%2520size%2520to%2520match%2520the%2520original%2520input%2520dimensions.%2520To%250Aevaluate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520for%2520continuous%2520vascular%250Asegmentation%252C%2520evaluations%2520were%2520performed%2520on%2520two%2520public%2520datasets%252C%2520ASOCA%2520and%250AImageCAS.%2520The%2520segmentation%2520results%2520show%2520that%2520the%2520ViG3D-UNet%2520surpassed%2520competing%250Amethods%2520in%2520maintaining%2520vascular%2520segmentation%2520connectivity%2520while%2520achieving%2520high%250Asegmentation%2520accuracy.%2520Our%2520code%2520will%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViG3D-UNet%3A%20Volumetric%20Vascular%20Connectivity-Aware%20Segmentation%20via%203D%0A%20%20Vision%20Graph%20Representation&entry.906535625=Bowen%20Liu%20and%20Chunlei%20Meng%20and%20Wei%20Lin%20and%20Hongda%20Zhang%20and%20Ziqing%20Zhou%20and%20Zhongxue%20Gan%20and%20Chun%20Ouyang&entry.1292438233=%20%20Accurate%20vascular%20segmentation%20is%20essential%20for%20coronary%20visualization%20and%0Athe%20diagnosis%20of%20coronary%20heart%20disease.%20This%20task%20involves%20the%20extraction%20of%0Asparse%20tree-like%20vascular%20branches%20from%20the%20volumetric%20space.%20However%2C%20existing%0Amethods%20have%20faced%20significant%20challenges%20due%20to%20discontinuous%20vascular%0Asegmentation%20and%20missing%20endpoints.%20To%20address%20this%20issue%2C%20a%203D%20vision%20graph%0Aneural%20network%20framework%2C%20named%20ViG3D-UNet%2C%20was%20introduced.%20This%20method%0Aintegrates%203D%20graph%20representation%20and%20aggregation%20within%20a%20U-shaped%0Aarchitecture%20to%20facilitate%20continuous%20vascular%20segmentation.%20The%20ViG3D%20module%0Acaptures%20volumetric%20vascular%20connectivity%20and%20topology%2C%20while%20the%20convolutional%0Amodule%20extracts%20fine%20vascular%20details.%20These%20two%20branches%20are%20combined%20through%0Achannel%20attention%20to%20form%20the%20encoder%20feature.%20Subsequently%2C%20a%20paperclip-shaped%0Aoffset%20decoder%20minimizes%20redundant%20computations%20in%20the%20sparse%20feature%20space%20and%0Arestores%20the%20feature%20map%20size%20to%20match%20the%20original%20input%20dimensions.%20To%0Aevaluate%20the%20effectiveness%20of%20the%20proposed%20approach%20for%20continuous%20vascular%0Asegmentation%2C%20evaluations%20were%20performed%20on%20two%20public%20datasets%2C%20ASOCA%20and%0AImageCAS.%20The%20segmentation%20results%20show%20that%20the%20ViG3D-UNet%20surpassed%20competing%0Amethods%20in%20maintaining%20vascular%20segmentation%20connectivity%20while%20achieving%20high%0Asegmentation%20accuracy.%20Our%20code%20will%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13599v1&entry.124074799=Read"},
{"title": "RefComp: A Reference-guided Unified Framework for Unpaired Point Cloud\n  Completion", "author": "Yixuan Yang and Jinyu Yang and Zixiang Zhao and Victor Sanchez and Feng Zheng", "abstract": "  The unpaired point cloud completion task aims to complete a partial point\ncloud by using models trained with no ground truth. Existing unpaired point\ncloud completion methods are class-aware, i.e., a separate model is needed for\neach object class. Since they have limited generalization capabilities, these\nmethods perform poorly in real-world scenarios when confronted with a wide\nrange of point clouds of generic 3D objects. In this paper, we propose a novel\nunpaired point cloud completion framework, namely the Reference-guided\nCompletion (RefComp) framework, which attains strong performance in both the\nclass-aware and class-agnostic training settings. The RefComp framework\ntransforms the unpaired completion problem into a shape translation problem,\nwhich is solved in the latent feature space of the partial point clouds. To\nthis end, we introduce the use of partial-complete point cloud pairs, which are\nretrieved by using the partial point cloud to be completed as a template. These\npoint cloud pairs are used as reference data to guide the completion process.\nOur RefComp framework uses a reference branch and a target branch with shared\nparameters for shape fusion and shape translation via a Latent Shape Fusion\nModule (LSFM) to enhance the structural features along the completion pipeline.\nExtensive experiments demonstrate that the RefComp framework achieves not only\nstate-of-the-art performance in the class-aware training setting but also\ncompetitive results in the class-agnostic training setting on both virtual\nscans and real-world datasets.\n", "link": "http://arxiv.org/abs/2504.13788v1", "date": "2025-04-18", "relevancy": 2.1488, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5454}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5343}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefComp%3A%20A%20Reference-guided%20Unified%20Framework%20for%20Unpaired%20Point%20Cloud%0A%20%20Completion&body=Title%3A%20RefComp%3A%20A%20Reference-guided%20Unified%20Framework%20for%20Unpaired%20Point%20Cloud%0A%20%20Completion%0AAuthor%3A%20Yixuan%20Yang%20and%20Jinyu%20Yang%20and%20Zixiang%20Zhao%20and%20Victor%20Sanchez%20and%20Feng%20Zheng%0AAbstract%3A%20%20%20The%20unpaired%20point%20cloud%20completion%20task%20aims%20to%20complete%20a%20partial%20point%0Acloud%20by%20using%20models%20trained%20with%20no%20ground%20truth.%20Existing%20unpaired%20point%0Acloud%20completion%20methods%20are%20class-aware%2C%20i.e.%2C%20a%20separate%20model%20is%20needed%20for%0Aeach%20object%20class.%20Since%20they%20have%20limited%20generalization%20capabilities%2C%20these%0Amethods%20perform%20poorly%20in%20real-world%20scenarios%20when%20confronted%20with%20a%20wide%0Arange%20of%20point%20clouds%20of%20generic%203D%20objects.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aunpaired%20point%20cloud%20completion%20framework%2C%20namely%20the%20Reference-guided%0ACompletion%20%28RefComp%29%20framework%2C%20which%20attains%20strong%20performance%20in%20both%20the%0Aclass-aware%20and%20class-agnostic%20training%20settings.%20The%20RefComp%20framework%0Atransforms%20the%20unpaired%20completion%20problem%20into%20a%20shape%20translation%20problem%2C%0Awhich%20is%20solved%20in%20the%20latent%20feature%20space%20of%20the%20partial%20point%20clouds.%20To%0Athis%20end%2C%20we%20introduce%20the%20use%20of%20partial-complete%20point%20cloud%20pairs%2C%20which%20are%0Aretrieved%20by%20using%20the%20partial%20point%20cloud%20to%20be%20completed%20as%20a%20template.%20These%0Apoint%20cloud%20pairs%20are%20used%20as%20reference%20data%20to%20guide%20the%20completion%20process.%0AOur%20RefComp%20framework%20uses%20a%20reference%20branch%20and%20a%20target%20branch%20with%20shared%0Aparameters%20for%20shape%20fusion%20and%20shape%20translation%20via%20a%20Latent%20Shape%20Fusion%0AModule%20%28LSFM%29%20to%20enhance%20the%20structural%20features%20along%20the%20completion%20pipeline.%0AExtensive%20experiments%20demonstrate%20that%20the%20RefComp%20framework%20achieves%20not%20only%0Astate-of-the-art%20performance%20in%20the%20class-aware%20training%20setting%20but%20also%0Acompetitive%20results%20in%20the%20class-agnostic%20training%20setting%20on%20both%20virtual%0Ascans%20and%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefComp%253A%2520A%2520Reference-guided%2520Unified%2520Framework%2520for%2520Unpaired%2520Point%2520Cloud%250A%2520%2520Completion%26entry.906535625%3DYixuan%2520Yang%2520and%2520Jinyu%2520Yang%2520and%2520Zixiang%2520Zhao%2520and%2520Victor%2520Sanchez%2520and%2520Feng%2520Zheng%26entry.1292438233%3D%2520%2520The%2520unpaired%2520point%2520cloud%2520completion%2520task%2520aims%2520to%2520complete%2520a%2520partial%2520point%250Acloud%2520by%2520using%2520models%2520trained%2520with%2520no%2520ground%2520truth.%2520Existing%2520unpaired%2520point%250Acloud%2520completion%2520methods%2520are%2520class-aware%252C%2520i.e.%252C%2520a%2520separate%2520model%2520is%2520needed%2520for%250Aeach%2520object%2520class.%2520Since%2520they%2520have%2520limited%2520generalization%2520capabilities%252C%2520these%250Amethods%2520perform%2520poorly%2520in%2520real-world%2520scenarios%2520when%2520confronted%2520with%2520a%2520wide%250Arange%2520of%2520point%2520clouds%2520of%2520generic%25203D%2520objects.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aunpaired%2520point%2520cloud%2520completion%2520framework%252C%2520namely%2520the%2520Reference-guided%250ACompletion%2520%2528RefComp%2529%2520framework%252C%2520which%2520attains%2520strong%2520performance%2520in%2520both%2520the%250Aclass-aware%2520and%2520class-agnostic%2520training%2520settings.%2520The%2520RefComp%2520framework%250Atransforms%2520the%2520unpaired%2520completion%2520problem%2520into%2520a%2520shape%2520translation%2520problem%252C%250Awhich%2520is%2520solved%2520in%2520the%2520latent%2520feature%2520space%2520of%2520the%2520partial%2520point%2520clouds.%2520To%250Athis%2520end%252C%2520we%2520introduce%2520the%2520use%2520of%2520partial-complete%2520point%2520cloud%2520pairs%252C%2520which%2520are%250Aretrieved%2520by%2520using%2520the%2520partial%2520point%2520cloud%2520to%2520be%2520completed%2520as%2520a%2520template.%2520These%250Apoint%2520cloud%2520pairs%2520are%2520used%2520as%2520reference%2520data%2520to%2520guide%2520the%2520completion%2520process.%250AOur%2520RefComp%2520framework%2520uses%2520a%2520reference%2520branch%2520and%2520a%2520target%2520branch%2520with%2520shared%250Aparameters%2520for%2520shape%2520fusion%2520and%2520shape%2520translation%2520via%2520a%2520Latent%2520Shape%2520Fusion%250AModule%2520%2528LSFM%2529%2520to%2520enhance%2520the%2520structural%2520features%2520along%2520the%2520completion%2520pipeline.%250AExtensive%2520experiments%2520demonstrate%2520that%2520the%2520RefComp%2520framework%2520achieves%2520not%2520only%250Astate-of-the-art%2520performance%2520in%2520the%2520class-aware%2520training%2520setting%2520but%2520also%250Acompetitive%2520results%2520in%2520the%2520class-agnostic%2520training%2520setting%2520on%2520both%2520virtual%250Ascans%2520and%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefComp%3A%20A%20Reference-guided%20Unified%20Framework%20for%20Unpaired%20Point%20Cloud%0A%20%20Completion&entry.906535625=Yixuan%20Yang%20and%20Jinyu%20Yang%20and%20Zixiang%20Zhao%20and%20Victor%20Sanchez%20and%20Feng%20Zheng&entry.1292438233=%20%20The%20unpaired%20point%20cloud%20completion%20task%20aims%20to%20complete%20a%20partial%20point%0Acloud%20by%20using%20models%20trained%20with%20no%20ground%20truth.%20Existing%20unpaired%20point%0Acloud%20completion%20methods%20are%20class-aware%2C%20i.e.%2C%20a%20separate%20model%20is%20needed%20for%0Aeach%20object%20class.%20Since%20they%20have%20limited%20generalization%20capabilities%2C%20these%0Amethods%20perform%20poorly%20in%20real-world%20scenarios%20when%20confronted%20with%20a%20wide%0Arange%20of%20point%20clouds%20of%20generic%203D%20objects.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aunpaired%20point%20cloud%20completion%20framework%2C%20namely%20the%20Reference-guided%0ACompletion%20%28RefComp%29%20framework%2C%20which%20attains%20strong%20performance%20in%20both%20the%0Aclass-aware%20and%20class-agnostic%20training%20settings.%20The%20RefComp%20framework%0Atransforms%20the%20unpaired%20completion%20problem%20into%20a%20shape%20translation%20problem%2C%0Awhich%20is%20solved%20in%20the%20latent%20feature%20space%20of%20the%20partial%20point%20clouds.%20To%0Athis%20end%2C%20we%20introduce%20the%20use%20of%20partial-complete%20point%20cloud%20pairs%2C%20which%20are%0Aretrieved%20by%20using%20the%20partial%20point%20cloud%20to%20be%20completed%20as%20a%20template.%20These%0Apoint%20cloud%20pairs%20are%20used%20as%20reference%20data%20to%20guide%20the%20completion%20process.%0AOur%20RefComp%20framework%20uses%20a%20reference%20branch%20and%20a%20target%20branch%20with%20shared%0Aparameters%20for%20shape%20fusion%20and%20shape%20translation%20via%20a%20Latent%20Shape%20Fusion%0AModule%20%28LSFM%29%20to%20enhance%20the%20structural%20features%20along%20the%20completion%20pipeline.%0AExtensive%20experiments%20demonstrate%20that%20the%20RefComp%20framework%20achieves%20not%20only%0Astate-of-the-art%20performance%20in%20the%20class-aware%20training%20setting%20but%20also%0Acompetitive%20results%20in%20the%20class-agnostic%20training%20setting%20on%20both%20virtual%0Ascans%20and%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13788v1&entry.124074799=Read"},
{"title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs", "author": "Christoph Schuhmann and Gollam Rabby and Ameya Prabhu and Tawsif Ahmed and Andreas Hochlehnert and Huu Nguyen and Nick Akinci and Ludwig Schmidt and Robert Kaczmarczyk and S\u00f6ren Auer and Jenia Jitsev and Matthias Bethge", "abstract": "  Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We propose a new\nidea for the community to adopt: convert scholarly documents into knowledge\npreserving, but style agnostic representations we term Knowledge Units using\nLLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95\\%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.\n", "link": "http://arxiv.org/abs/2502.19413v2", "date": "2025-04-18", "relevancy": 2.1441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Project%20Alexandria%3A%20Towards%20Freeing%20Scientific%20Knowledge%20from%20Copyright%0A%20%20Burdens%20via%20LLMs&body=Title%3A%20Project%20Alexandria%3A%20Towards%20Freeing%20Scientific%20Knowledge%20from%20Copyright%0A%20%20Burdens%20via%20LLMs%0AAuthor%3A%20Christoph%20Schuhmann%20and%20Gollam%20Rabby%20and%20Ameya%20Prabhu%20and%20Tawsif%20Ahmed%20and%20Andreas%20Hochlehnert%20and%20Huu%20Nguyen%20and%20Nick%20Akinci%20and%20Ludwig%20Schmidt%20and%20Robert%20Kaczmarczyk%20and%20S%C3%B6ren%20Auer%20and%20Jenia%20Jitsev%20and%20Matthias%20Bethge%0AAbstract%3A%20%20%20Paywalls%2C%20licenses%20and%20copyright%20rules%20often%20restrict%20the%20broad%20dissemination%0Aand%20reuse%20of%20scientific%20knowledge.%20We%20take%20the%20position%20that%20it%20is%20both%20legally%0Aand%20technically%20feasible%20to%20extract%20the%20scientific%20knowledge%20in%20scholarly%0Atexts.%20Current%20methods%2C%20like%20text%20embeddings%2C%20fail%20to%20reliably%20preserve%20factual%0Acontent%2C%20and%20simple%20paraphrasing%20may%20not%20be%20legally%20sound.%20We%20propose%20a%20new%0Aidea%20for%20the%20community%20to%20adopt%3A%20convert%20scholarly%20documents%20into%20knowledge%0Apreserving%2C%20but%20style%20agnostic%20representations%20we%20term%20Knowledge%20Units%20using%0ALLMs.%20These%20units%20use%20structured%20data%20capturing%20entities%2C%20attributes%20and%0Arelationships%20without%20stylistic%20content.%20We%20provide%20evidence%20that%20Knowledge%0AUnits%20%281%29%20form%20a%20legally%20defensible%20framework%20for%20sharing%20knowledge%20from%0Acopyrighted%20research%20texts%2C%20based%20on%20legal%20analyses%20of%20German%20copyright%20law%20and%0AU.S.%20Fair%20Use%20doctrine%2C%20and%20%282%29%20preserve%20most%20%28~95%5C%25%29%20factual%20knowledge%20from%0Aoriginal%20text%2C%20measured%20by%20MCQ%20performance%20on%20facts%20from%20the%20original%0Acopyrighted%20text%20across%20four%20research%20domains.%20Freeing%20scientific%20knowledge%0Afrom%20copyright%20promises%20transformative%20benefits%20for%20scientific%20research%20and%0Aeducation%20by%20allowing%20language%20models%20to%20reuse%20important%20facts%20from%20copyrighted%0Atext.%20To%20support%20this%2C%20we%20share%20open-source%20tools%20for%20converting%20research%0Adocuments%20into%20Knowledge%20Units.%20Overall%2C%20our%20work%20posits%20the%20feasibility%20of%0Ademocratizing%20access%20to%20scientific%20knowledge%20while%20respecting%20copyright.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProject%2520Alexandria%253A%2520Towards%2520Freeing%2520Scientific%2520Knowledge%2520from%2520Copyright%250A%2520%2520Burdens%2520via%2520LLMs%26entry.906535625%3DChristoph%2520Schuhmann%2520and%2520Gollam%2520Rabby%2520and%2520Ameya%2520Prabhu%2520and%2520Tawsif%2520Ahmed%2520and%2520Andreas%2520Hochlehnert%2520and%2520Huu%2520Nguyen%2520and%2520Nick%2520Akinci%2520and%2520Ludwig%2520Schmidt%2520and%2520Robert%2520Kaczmarczyk%2520and%2520S%25C3%25B6ren%2520Auer%2520and%2520Jenia%2520Jitsev%2520and%2520Matthias%2520Bethge%26entry.1292438233%3D%2520%2520Paywalls%252C%2520licenses%2520and%2520copyright%2520rules%2520often%2520restrict%2520the%2520broad%2520dissemination%250Aand%2520reuse%2520of%2520scientific%2520knowledge.%2520We%2520take%2520the%2520position%2520that%2520it%2520is%2520both%2520legally%250Aand%2520technically%2520feasible%2520to%2520extract%2520the%2520scientific%2520knowledge%2520in%2520scholarly%250Atexts.%2520Current%2520methods%252C%2520like%2520text%2520embeddings%252C%2520fail%2520to%2520reliably%2520preserve%2520factual%250Acontent%252C%2520and%2520simple%2520paraphrasing%2520may%2520not%2520be%2520legally%2520sound.%2520We%2520propose%2520a%2520new%250Aidea%2520for%2520the%2520community%2520to%2520adopt%253A%2520convert%2520scholarly%2520documents%2520into%2520knowledge%250Apreserving%252C%2520but%2520style%2520agnostic%2520representations%2520we%2520term%2520Knowledge%2520Units%2520using%250ALLMs.%2520These%2520units%2520use%2520structured%2520data%2520capturing%2520entities%252C%2520attributes%2520and%250Arelationships%2520without%2520stylistic%2520content.%2520We%2520provide%2520evidence%2520that%2520Knowledge%250AUnits%2520%25281%2529%2520form%2520a%2520legally%2520defensible%2520framework%2520for%2520sharing%2520knowledge%2520from%250Acopyrighted%2520research%2520texts%252C%2520based%2520on%2520legal%2520analyses%2520of%2520German%2520copyright%2520law%2520and%250AU.S.%2520Fair%2520Use%2520doctrine%252C%2520and%2520%25282%2529%2520preserve%2520most%2520%2528~95%255C%2525%2529%2520factual%2520knowledge%2520from%250Aoriginal%2520text%252C%2520measured%2520by%2520MCQ%2520performance%2520on%2520facts%2520from%2520the%2520original%250Acopyrighted%2520text%2520across%2520four%2520research%2520domains.%2520Freeing%2520scientific%2520knowledge%250Afrom%2520copyright%2520promises%2520transformative%2520benefits%2520for%2520scientific%2520research%2520and%250Aeducation%2520by%2520allowing%2520language%2520models%2520to%2520reuse%2520important%2520facts%2520from%2520copyrighted%250Atext.%2520To%2520support%2520this%252C%2520we%2520share%2520open-source%2520tools%2520for%2520converting%2520research%250Adocuments%2520into%2520Knowledge%2520Units.%2520Overall%252C%2520our%2520work%2520posits%2520the%2520feasibility%2520of%250Ademocratizing%2520access%2520to%2520scientific%2520knowledge%2520while%2520respecting%2520copyright.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Project%20Alexandria%3A%20Towards%20Freeing%20Scientific%20Knowledge%20from%20Copyright%0A%20%20Burdens%20via%20LLMs&entry.906535625=Christoph%20Schuhmann%20and%20Gollam%20Rabby%20and%20Ameya%20Prabhu%20and%20Tawsif%20Ahmed%20and%20Andreas%20Hochlehnert%20and%20Huu%20Nguyen%20and%20Nick%20Akinci%20and%20Ludwig%20Schmidt%20and%20Robert%20Kaczmarczyk%20and%20S%C3%B6ren%20Auer%20and%20Jenia%20Jitsev%20and%20Matthias%20Bethge&entry.1292438233=%20%20Paywalls%2C%20licenses%20and%20copyright%20rules%20often%20restrict%20the%20broad%20dissemination%0Aand%20reuse%20of%20scientific%20knowledge.%20We%20take%20the%20position%20that%20it%20is%20both%20legally%0Aand%20technically%20feasible%20to%20extract%20the%20scientific%20knowledge%20in%20scholarly%0Atexts.%20Current%20methods%2C%20like%20text%20embeddings%2C%20fail%20to%20reliably%20preserve%20factual%0Acontent%2C%20and%20simple%20paraphrasing%20may%20not%20be%20legally%20sound.%20We%20propose%20a%20new%0Aidea%20for%20the%20community%20to%20adopt%3A%20convert%20scholarly%20documents%20into%20knowledge%0Apreserving%2C%20but%20style%20agnostic%20representations%20we%20term%20Knowledge%20Units%20using%0ALLMs.%20These%20units%20use%20structured%20data%20capturing%20entities%2C%20attributes%20and%0Arelationships%20without%20stylistic%20content.%20We%20provide%20evidence%20that%20Knowledge%0AUnits%20%281%29%20form%20a%20legally%20defensible%20framework%20for%20sharing%20knowledge%20from%0Acopyrighted%20research%20texts%2C%20based%20on%20legal%20analyses%20of%20German%20copyright%20law%20and%0AU.S.%20Fair%20Use%20doctrine%2C%20and%20%282%29%20preserve%20most%20%28~95%5C%25%29%20factual%20knowledge%20from%0Aoriginal%20text%2C%20measured%20by%20MCQ%20performance%20on%20facts%20from%20the%20original%0Acopyrighted%20text%20across%20four%20research%20domains.%20Freeing%20scientific%20knowledge%0Afrom%20copyright%20promises%20transformative%20benefits%20for%20scientific%20research%20and%0Aeducation%20by%20allowing%20language%20models%20to%20reuse%20important%20facts%20from%20copyrighted%0Atext.%20To%20support%20this%2C%20we%20share%20open-source%20tools%20for%20converting%20research%0Adocuments%20into%20Knowledge%20Units.%20Overall%2C%20our%20work%20posits%20the%20feasibility%20of%0Ademocratizing%20access%20to%20scientific%20knowledge%20while%20respecting%20copyright.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19413v2&entry.124074799=Read"},
{"title": "AnyTSR: Any-Scale Thermal Super-Resolution for UAV", "author": "Mengyuan Li and Changhong Fu and Ziyu Lu and Zijie Zhang and Haobo Zuo and Liangliang Yao", "abstract": "  Thermal imaging can greatly enhance the application of intelligent unmanned\naerial vehicles (UAV) in challenging environments. However, the inherent low\nresolution of thermal sensors leads to insufficient details and blurred\nboundaries. Super-resolution (SR) offers a promising solution to address this\nissue, while most existing SR methods are designed for fixed-scale SR. They are\ncomputationally expensive and inflexible in practical applications. To address\nabove issues, this work proposes a novel any-scale thermal SR method (AnyTSR)\nfor UAV within a single model. Specifically, a new image encoder is proposed to\nexplicitly assign specific feature code to enable more accurate and flexible\nrepresentation. Additionally, by effectively embedding coordinate offset\ninformation into the local feature ensemble, an innovative any-scale upsampler\nis proposed to better understand spatial relationships and reduce artifacts.\nMoreover, a novel dataset (UAV-TSR), covering both land and water scenes, is\nconstructed for thermal SR tasks. Experimental results demonstrate that the\nproposed method consistently outperforms state-of-the-art methods across all\nscaling factors as well as generates more accurate and detailed high-resolution\nimages. The code is located at https://github.com/vision4robotics/AnyTSR.\n", "link": "http://arxiv.org/abs/2504.13682v1", "date": "2025-04-18", "relevancy": 2.1093, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5448}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5416}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyTSR%3A%20Any-Scale%20Thermal%20Super-Resolution%20for%20UAV&body=Title%3A%20AnyTSR%3A%20Any-Scale%20Thermal%20Super-Resolution%20for%20UAV%0AAuthor%3A%20Mengyuan%20Li%20and%20Changhong%20Fu%20and%20Ziyu%20Lu%20and%20Zijie%20Zhang%20and%20Haobo%20Zuo%20and%20Liangliang%20Yao%0AAbstract%3A%20%20%20Thermal%20imaging%20can%20greatly%20enhance%20the%20application%20of%20intelligent%20unmanned%0Aaerial%20vehicles%20%28UAV%29%20in%20challenging%20environments.%20However%2C%20the%20inherent%20low%0Aresolution%20of%20thermal%20sensors%20leads%20to%20insufficient%20details%20and%20blurred%0Aboundaries.%20Super-resolution%20%28SR%29%20offers%20a%20promising%20solution%20to%20address%20this%0Aissue%2C%20while%20most%20existing%20SR%20methods%20are%20designed%20for%20fixed-scale%20SR.%20They%20are%0Acomputationally%20expensive%20and%20inflexible%20in%20practical%20applications.%20To%20address%0Aabove%20issues%2C%20this%20work%20proposes%20a%20novel%20any-scale%20thermal%20SR%20method%20%28AnyTSR%29%0Afor%20UAV%20within%20a%20single%20model.%20Specifically%2C%20a%20new%20image%20encoder%20is%20proposed%20to%0Aexplicitly%20assign%20specific%20feature%20code%20to%20enable%20more%20accurate%20and%20flexible%0Arepresentation.%20Additionally%2C%20by%20effectively%20embedding%20coordinate%20offset%0Ainformation%20into%20the%20local%20feature%20ensemble%2C%20an%20innovative%20any-scale%20upsampler%0Ais%20proposed%20to%20better%20understand%20spatial%20relationships%20and%20reduce%20artifacts.%0AMoreover%2C%20a%20novel%20dataset%20%28UAV-TSR%29%2C%20covering%20both%20land%20and%20water%20scenes%2C%20is%0Aconstructed%20for%20thermal%20SR%20tasks.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20consistently%20outperforms%20state-of-the-art%20methods%20across%20all%0Ascaling%20factors%20as%20well%20as%20generates%20more%20accurate%20and%20detailed%20high-resolution%0Aimages.%20The%20code%20is%20located%20at%20https%3A//github.com/vision4robotics/AnyTSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyTSR%253A%2520Any-Scale%2520Thermal%2520Super-Resolution%2520for%2520UAV%26entry.906535625%3DMengyuan%2520Li%2520and%2520Changhong%2520Fu%2520and%2520Ziyu%2520Lu%2520and%2520Zijie%2520Zhang%2520and%2520Haobo%2520Zuo%2520and%2520Liangliang%2520Yao%26entry.1292438233%3D%2520%2520Thermal%2520imaging%2520can%2520greatly%2520enhance%2520the%2520application%2520of%2520intelligent%2520unmanned%250Aaerial%2520vehicles%2520%2528UAV%2529%2520in%2520challenging%2520environments.%2520However%252C%2520the%2520inherent%2520low%250Aresolution%2520of%2520thermal%2520sensors%2520leads%2520to%2520insufficient%2520details%2520and%2520blurred%250Aboundaries.%2520Super-resolution%2520%2528SR%2529%2520offers%2520a%2520promising%2520solution%2520to%2520address%2520this%250Aissue%252C%2520while%2520most%2520existing%2520SR%2520methods%2520are%2520designed%2520for%2520fixed-scale%2520SR.%2520They%2520are%250Acomputationally%2520expensive%2520and%2520inflexible%2520in%2520practical%2520applications.%2520To%2520address%250Aabove%2520issues%252C%2520this%2520work%2520proposes%2520a%2520novel%2520any-scale%2520thermal%2520SR%2520method%2520%2528AnyTSR%2529%250Afor%2520UAV%2520within%2520a%2520single%2520model.%2520Specifically%252C%2520a%2520new%2520image%2520encoder%2520is%2520proposed%2520to%250Aexplicitly%2520assign%2520specific%2520feature%2520code%2520to%2520enable%2520more%2520accurate%2520and%2520flexible%250Arepresentation.%2520Additionally%252C%2520by%2520effectively%2520embedding%2520coordinate%2520offset%250Ainformation%2520into%2520the%2520local%2520feature%2520ensemble%252C%2520an%2520innovative%2520any-scale%2520upsampler%250Ais%2520proposed%2520to%2520better%2520understand%2520spatial%2520relationships%2520and%2520reduce%2520artifacts.%250AMoreover%252C%2520a%2520novel%2520dataset%2520%2528UAV-TSR%2529%252C%2520covering%2520both%2520land%2520and%2520water%2520scenes%252C%2520is%250Aconstructed%2520for%2520thermal%2520SR%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520consistently%2520outperforms%2520state-of-the-art%2520methods%2520across%2520all%250Ascaling%2520factors%2520as%2520well%2520as%2520generates%2520more%2520accurate%2520and%2520detailed%2520high-resolution%250Aimages.%2520The%2520code%2520is%2520located%2520at%2520https%253A//github.com/vision4robotics/AnyTSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyTSR%3A%20Any-Scale%20Thermal%20Super-Resolution%20for%20UAV&entry.906535625=Mengyuan%20Li%20and%20Changhong%20Fu%20and%20Ziyu%20Lu%20and%20Zijie%20Zhang%20and%20Haobo%20Zuo%20and%20Liangliang%20Yao&entry.1292438233=%20%20Thermal%20imaging%20can%20greatly%20enhance%20the%20application%20of%20intelligent%20unmanned%0Aaerial%20vehicles%20%28UAV%29%20in%20challenging%20environments.%20However%2C%20the%20inherent%20low%0Aresolution%20of%20thermal%20sensors%20leads%20to%20insufficient%20details%20and%20blurred%0Aboundaries.%20Super-resolution%20%28SR%29%20offers%20a%20promising%20solution%20to%20address%20this%0Aissue%2C%20while%20most%20existing%20SR%20methods%20are%20designed%20for%20fixed-scale%20SR.%20They%20are%0Acomputationally%20expensive%20and%20inflexible%20in%20practical%20applications.%20To%20address%0Aabove%20issues%2C%20this%20work%20proposes%20a%20novel%20any-scale%20thermal%20SR%20method%20%28AnyTSR%29%0Afor%20UAV%20within%20a%20single%20model.%20Specifically%2C%20a%20new%20image%20encoder%20is%20proposed%20to%0Aexplicitly%20assign%20specific%20feature%20code%20to%20enable%20more%20accurate%20and%20flexible%0Arepresentation.%20Additionally%2C%20by%20effectively%20embedding%20coordinate%20offset%0Ainformation%20into%20the%20local%20feature%20ensemble%2C%20an%20innovative%20any-scale%20upsampler%0Ais%20proposed%20to%20better%20understand%20spatial%20relationships%20and%20reduce%20artifacts.%0AMoreover%2C%20a%20novel%20dataset%20%28UAV-TSR%29%2C%20covering%20both%20land%20and%20water%20scenes%2C%20is%0Aconstructed%20for%20thermal%20SR%20tasks.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20method%20consistently%20outperforms%20state-of-the-art%20methods%20across%20all%0Ascaling%20factors%20as%20well%20as%20generates%20more%20accurate%20and%20detailed%20high-resolution%0Aimages.%20The%20code%20is%20located%20at%20https%3A//github.com/vision4robotics/AnyTSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13682v1&entry.124074799=Read"},
{"title": "Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint\n  Matching", "author": "Aaron Havens and Benjamin Kurt Miller and Bing Yan and Carles Domingo-Enrich and Anuroop Sriram and Brandon Wood and Daniel Levine and Bin Hu and Brandon Amos and Brian Karrer and Xiang Fu and Guan-Horng Liu and Ricky T. Q. Chen", "abstract": "  We introduce Adjoint Sampling, a highly scalable and efficient algorithm for\nlearning diffusion processes that sample from unnormalized densities, or energy\nfunctions. It is the first on-policy approach that allows significantly more\ngradient updates than the number of energy evaluations and model samples,\nallowing us to scale to much larger problem settings than previously explored\nby similar methods. Our framework is theoretically grounded in stochastic\noptimal control and shares the same theoretical guarantees as Adjoint Matching,\nbeing able to train without the need for corrective measures that push samples\ntowards the target distribution. We show how to incorporate key symmetries, as\nwell as periodic boundary conditions, for modeling molecules in both cartesian\nand torsional coordinates. We demonstrate the effectiveness of our approach\nthrough extensive experiments on classical energy functions, and further scale\nup to neural network-based energy models where we perform amortized conformer\ngeneration across many molecular systems. To encourage further research in\ndeveloping highly scalable sampling methods, we plan to open source these\nchallenging benchmarks, where successful methods can directly impact progress\nin computational chemistry.\n", "link": "http://arxiv.org/abs/2504.11713v2", "date": "2025-04-18", "relevancy": 2.1013, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5308}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5222}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adjoint%20Sampling%3A%20Highly%20Scalable%20Diffusion%20Samplers%20via%20Adjoint%0A%20%20Matching&body=Title%3A%20Adjoint%20Sampling%3A%20Highly%20Scalable%20Diffusion%20Samplers%20via%20Adjoint%0A%20%20Matching%0AAuthor%3A%20Aaron%20Havens%20and%20Benjamin%20Kurt%20Miller%20and%20Bing%20Yan%20and%20Carles%20Domingo-Enrich%20and%20Anuroop%20Sriram%20and%20Brandon%20Wood%20and%20Daniel%20Levine%20and%20Bin%20Hu%20and%20Brandon%20Amos%20and%20Brian%20Karrer%20and%20Xiang%20Fu%20and%20Guan-Horng%20Liu%20and%20Ricky%20T.%20Q.%20Chen%0AAbstract%3A%20%20%20We%20introduce%20Adjoint%20Sampling%2C%20a%20highly%20scalable%20and%20efficient%20algorithm%20for%0Alearning%20diffusion%20processes%20that%20sample%20from%20unnormalized%20densities%2C%20or%20energy%0Afunctions.%20It%20is%20the%20first%20on-policy%20approach%20that%20allows%20significantly%20more%0Agradient%20updates%20than%20the%20number%20of%20energy%20evaluations%20and%20model%20samples%2C%0Aallowing%20us%20to%20scale%20to%20much%20larger%20problem%20settings%20than%20previously%20explored%0Aby%20similar%20methods.%20Our%20framework%20is%20theoretically%20grounded%20in%20stochastic%0Aoptimal%20control%20and%20shares%20the%20same%20theoretical%20guarantees%20as%20Adjoint%20Matching%2C%0Abeing%20able%20to%20train%20without%20the%20need%20for%20corrective%20measures%20that%20push%20samples%0Atowards%20the%20target%20distribution.%20We%20show%20how%20to%20incorporate%20key%20symmetries%2C%20as%0Awell%20as%20periodic%20boundary%20conditions%2C%20for%20modeling%20molecules%20in%20both%20cartesian%0Aand%20torsional%20coordinates.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Athrough%20extensive%20experiments%20on%20classical%20energy%20functions%2C%20and%20further%20scale%0Aup%20to%20neural%20network-based%20energy%20models%20where%20we%20perform%20amortized%20conformer%0Ageneration%20across%20many%20molecular%20systems.%20To%20encourage%20further%20research%20in%0Adeveloping%20highly%20scalable%20sampling%20methods%2C%20we%20plan%20to%20open%20source%20these%0Achallenging%20benchmarks%2C%20where%20successful%20methods%20can%20directly%20impact%20progress%0Ain%20computational%20chemistry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.11713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdjoint%2520Sampling%253A%2520Highly%2520Scalable%2520Diffusion%2520Samplers%2520via%2520Adjoint%250A%2520%2520Matching%26entry.906535625%3DAaron%2520Havens%2520and%2520Benjamin%2520Kurt%2520Miller%2520and%2520Bing%2520Yan%2520and%2520Carles%2520Domingo-Enrich%2520and%2520Anuroop%2520Sriram%2520and%2520Brandon%2520Wood%2520and%2520Daniel%2520Levine%2520and%2520Bin%2520Hu%2520and%2520Brandon%2520Amos%2520and%2520Brian%2520Karrer%2520and%2520Xiang%2520Fu%2520and%2520Guan-Horng%2520Liu%2520and%2520Ricky%2520T.%2520Q.%2520Chen%26entry.1292438233%3D%2520%2520We%2520introduce%2520Adjoint%2520Sampling%252C%2520a%2520highly%2520scalable%2520and%2520efficient%2520algorithm%2520for%250Alearning%2520diffusion%2520processes%2520that%2520sample%2520from%2520unnormalized%2520densities%252C%2520or%2520energy%250Afunctions.%2520It%2520is%2520the%2520first%2520on-policy%2520approach%2520that%2520allows%2520significantly%2520more%250Agradient%2520updates%2520than%2520the%2520number%2520of%2520energy%2520evaluations%2520and%2520model%2520samples%252C%250Aallowing%2520us%2520to%2520scale%2520to%2520much%2520larger%2520problem%2520settings%2520than%2520previously%2520explored%250Aby%2520similar%2520methods.%2520Our%2520framework%2520is%2520theoretically%2520grounded%2520in%2520stochastic%250Aoptimal%2520control%2520and%2520shares%2520the%2520same%2520theoretical%2520guarantees%2520as%2520Adjoint%2520Matching%252C%250Abeing%2520able%2520to%2520train%2520without%2520the%2520need%2520for%2520corrective%2520measures%2520that%2520push%2520samples%250Atowards%2520the%2520target%2520distribution.%2520We%2520show%2520how%2520to%2520incorporate%2520key%2520symmetries%252C%2520as%250Awell%2520as%2520periodic%2520boundary%2520conditions%252C%2520for%2520modeling%2520molecules%2520in%2520both%2520cartesian%250Aand%2520torsional%2520coordinates.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%250Athrough%2520extensive%2520experiments%2520on%2520classical%2520energy%2520functions%252C%2520and%2520further%2520scale%250Aup%2520to%2520neural%2520network-based%2520energy%2520models%2520where%2520we%2520perform%2520amortized%2520conformer%250Ageneration%2520across%2520many%2520molecular%2520systems.%2520To%2520encourage%2520further%2520research%2520in%250Adeveloping%2520highly%2520scalable%2520sampling%2520methods%252C%2520we%2520plan%2520to%2520open%2520source%2520these%250Achallenging%2520benchmarks%252C%2520where%2520successful%2520methods%2520can%2520directly%2520impact%2520progress%250Ain%2520computational%2520chemistry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.11713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adjoint%20Sampling%3A%20Highly%20Scalable%20Diffusion%20Samplers%20via%20Adjoint%0A%20%20Matching&entry.906535625=Aaron%20Havens%20and%20Benjamin%20Kurt%20Miller%20and%20Bing%20Yan%20and%20Carles%20Domingo-Enrich%20and%20Anuroop%20Sriram%20and%20Brandon%20Wood%20and%20Daniel%20Levine%20and%20Bin%20Hu%20and%20Brandon%20Amos%20and%20Brian%20Karrer%20and%20Xiang%20Fu%20and%20Guan-Horng%20Liu%20and%20Ricky%20T.%20Q.%20Chen&entry.1292438233=%20%20We%20introduce%20Adjoint%20Sampling%2C%20a%20highly%20scalable%20and%20efficient%20algorithm%20for%0Alearning%20diffusion%20processes%20that%20sample%20from%20unnormalized%20densities%2C%20or%20energy%0Afunctions.%20It%20is%20the%20first%20on-policy%20approach%20that%20allows%20significantly%20more%0Agradient%20updates%20than%20the%20number%20of%20energy%20evaluations%20and%20model%20samples%2C%0Aallowing%20us%20to%20scale%20to%20much%20larger%20problem%20settings%20than%20previously%20explored%0Aby%20similar%20methods.%20Our%20framework%20is%20theoretically%20grounded%20in%20stochastic%0Aoptimal%20control%20and%20shares%20the%20same%20theoretical%20guarantees%20as%20Adjoint%20Matching%2C%0Abeing%20able%20to%20train%20without%20the%20need%20for%20corrective%20measures%20that%20push%20samples%0Atowards%20the%20target%20distribution.%20We%20show%20how%20to%20incorporate%20key%20symmetries%2C%20as%0Awell%20as%20periodic%20boundary%20conditions%2C%20for%20modeling%20molecules%20in%20both%20cartesian%0Aand%20torsional%20coordinates.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%0Athrough%20extensive%20experiments%20on%20classical%20energy%20functions%2C%20and%20further%20scale%0Aup%20to%20neural%20network-based%20energy%20models%20where%20we%20perform%20amortized%20conformer%0Ageneration%20across%20many%20molecular%20systems.%20To%20encourage%20further%20research%20in%0Adeveloping%20highly%20scalable%20sampling%20methods%2C%20we%20plan%20to%20open%20source%20these%0Achallenging%20benchmarks%2C%20where%20successful%20methods%20can%20directly%20impact%20progress%0Ain%20computational%20chemistry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.11713v2&entry.124074799=Read"},
{"title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?", "author": "Yang Yue and Zhiqi Chen and Rui Lu and Andrew Zhao and Zhaokai Wang and Yang Yue and Shiji Song and Gao Huang", "abstract": "  Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io\n", "link": "http://arxiv.org/abs/2504.13837v1", "date": "2025-04-18", "relevancy": 2.098, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Reinforcement%20Learning%20Really%20Incentivize%20Reasoning%20Capacity%20in%0A%20%20LLMs%20Beyond%20the%20Base%20Model%3F&body=Title%3A%20Does%20Reinforcement%20Learning%20Really%20Incentivize%20Reasoning%20Capacity%20in%0A%20%20LLMs%20Beyond%20the%20Base%20Model%3F%0AAuthor%3A%20Yang%20Yue%20and%20Zhiqi%20Chen%20and%20Rui%20Lu%20and%20Andrew%20Zhao%20and%20Zhaokai%20Wang%20and%20Yang%20Yue%20and%20Shiji%20Song%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20recently%0Ademonstrated%20notable%20success%20in%20enhancing%20the%20reasoning%20capabilities%20of%20LLMs%2C%0Aparticularly%20in%20mathematics%20and%20programming%20tasks.%20It%20is%20widely%20believed%20that%0ARLVR%20enables%20LLMs%20to%20continuously%20self-improve%2C%20thus%20acquiring%20novel%20reasoning%0Aabilities%20that%20exceed%20corresponding%20base%20models%27%20capacity.%20In%20this%20study%2C%0Ahowever%2C%20we%20critically%20re-examines%20this%20assumption%20by%20measuring%20the%0Apass%40%5Ctextit%7Bk%7D%20metric%20with%20large%20values%20of%20%5Ctextit%7Bk%7D%20to%20explore%20the%20reasoning%0Acapability%20boundary%20of%20the%20models%20across%20a%20wide%20range%20of%20model%20families%20and%0Abenchmarks.%20Surprisingly%2C%20the%20RL%20does%20%5Cemph%7Bnot%7D%2C%20in%20fact%2C%20elicit%20fundamentally%0Anew%20reasoning%20patterns.%20While%20RL-trained%20models%20outperform%20their%20base%20models%20at%0Asmaller%20values%20of%20%24k%24%20%28%5Ceg%2C%20%24k%24%3D1%29%2C%20base%20models%20can%20achieve%20a%20comparable%20or%0Aeven%20higher%20pass%40%24k%24%20score%20compared%20to%20their%20RL%20counterparts%20at%20large%20%24k%24%0Avalues.%20The%20reasoning%20paths%20generated%20by%20RL-trained%20models%20are%20already%20included%0Ain%20the%20base%20models%27%20sampling%20distribution%2C%20suggesting%20that%20most%20reasoning%0Aabilities%20manifested%20in%20RL-trained%20models%20are%20already%20obtained%20by%20base%20models.%0AFurther%20analysis%20shows%20that%20RL%20training%20boosts%20the%20performance%20by%20biasing%20the%0Amodel%27s%20output%20distribution%20toward%20paths%20that%20are%20more%20likely%20to%20yield%20rewards%2C%0Atherefore%20sampling%20correct%20responses%20more%20efficiently.%20But%20this%20also%20results%20in%0Aa%20narrower%20reasoning%20capability%20boundary%20compared%20to%20base%20models.%20Similar%0Aresults%20are%20observed%20in%20visual%20reasoning%20tasks%20trained%20with%20RLVR.%20Moreover%2C%20we%0Afind%20that%20distillation%20can%20genuinely%20introduce%20new%20knowledge%20into%20the%20model%2C%0Adifferent%20from%20RLVR.%20These%20findings%20underscore%20a%20critical%20limitation%20of%20RLVR%20in%0Aadvancing%20LLM%20reasoning%20abilities%20which%20requires%20us%20to%20fundamentally%20rethink%0Athe%20impact%20of%20RL%20training%20in%20reasoning%20LLMs%20and%20the%20need%20of%20a%20better%20paradigm.%0AProject%20Page%3A%20https%3A//limit-of-RLVR.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Reinforcement%2520Learning%2520Really%2520Incentivize%2520Reasoning%2520Capacity%2520in%250A%2520%2520LLMs%2520Beyond%2520the%2520Base%2520Model%253F%26entry.906535625%3DYang%2520Yue%2520and%2520Zhiqi%2520Chen%2520and%2520Rui%2520Lu%2520and%2520Andrew%2520Zhao%2520and%2520Zhaokai%2520Wang%2520and%2520Yang%2520Yue%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520recently%250Ademonstrated%2520notable%2520success%2520in%2520enhancing%2520the%2520reasoning%2520capabilities%2520of%2520LLMs%252C%250Aparticularly%2520in%2520mathematics%2520and%2520programming%2520tasks.%2520It%2520is%2520widely%2520believed%2520that%250ARLVR%2520enables%2520LLMs%2520to%2520continuously%2520self-improve%252C%2520thus%2520acquiring%2520novel%2520reasoning%250Aabilities%2520that%2520exceed%2520corresponding%2520base%2520models%2527%2520capacity.%2520In%2520this%2520study%252C%250Ahowever%252C%2520we%2520critically%2520re-examines%2520this%2520assumption%2520by%2520measuring%2520the%250Apass%2540%255Ctextit%257Bk%257D%2520metric%2520with%2520large%2520values%2520of%2520%255Ctextit%257Bk%257D%2520to%2520explore%2520the%2520reasoning%250Acapability%2520boundary%2520of%2520the%2520models%2520across%2520a%2520wide%2520range%2520of%2520model%2520families%2520and%250Abenchmarks.%2520Surprisingly%252C%2520the%2520RL%2520does%2520%255Cemph%257Bnot%257D%252C%2520in%2520fact%252C%2520elicit%2520fundamentally%250Anew%2520reasoning%2520patterns.%2520While%2520RL-trained%2520models%2520outperform%2520their%2520base%2520models%2520at%250Asmaller%2520values%2520of%2520%2524k%2524%2520%2528%255Ceg%252C%2520%2524k%2524%253D1%2529%252C%2520base%2520models%2520can%2520achieve%2520a%2520comparable%2520or%250Aeven%2520higher%2520pass%2540%2524k%2524%2520score%2520compared%2520to%2520their%2520RL%2520counterparts%2520at%2520large%2520%2524k%2524%250Avalues.%2520The%2520reasoning%2520paths%2520generated%2520by%2520RL-trained%2520models%2520are%2520already%2520included%250Ain%2520the%2520base%2520models%2527%2520sampling%2520distribution%252C%2520suggesting%2520that%2520most%2520reasoning%250Aabilities%2520manifested%2520in%2520RL-trained%2520models%2520are%2520already%2520obtained%2520by%2520base%2520models.%250AFurther%2520analysis%2520shows%2520that%2520RL%2520training%2520boosts%2520the%2520performance%2520by%2520biasing%2520the%250Amodel%2527s%2520output%2520distribution%2520toward%2520paths%2520that%2520are%2520more%2520likely%2520to%2520yield%2520rewards%252C%250Atherefore%2520sampling%2520correct%2520responses%2520more%2520efficiently.%2520But%2520this%2520also%2520results%2520in%250Aa%2520narrower%2520reasoning%2520capability%2520boundary%2520compared%2520to%2520base%2520models.%2520Similar%250Aresults%2520are%2520observed%2520in%2520visual%2520reasoning%2520tasks%2520trained%2520with%2520RLVR.%2520Moreover%252C%2520we%250Afind%2520that%2520distillation%2520can%2520genuinely%2520introduce%2520new%2520knowledge%2520into%2520the%2520model%252C%250Adifferent%2520from%2520RLVR.%2520These%2520findings%2520underscore%2520a%2520critical%2520limitation%2520of%2520RLVR%2520in%250Aadvancing%2520LLM%2520reasoning%2520abilities%2520which%2520requires%2520us%2520to%2520fundamentally%2520rethink%250Athe%2520impact%2520of%2520RL%2520training%2520in%2520reasoning%2520LLMs%2520and%2520the%2520need%2520of%2520a%2520better%2520paradigm.%250AProject%2520Page%253A%2520https%253A//limit-of-RLVR.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Reinforcement%20Learning%20Really%20Incentivize%20Reasoning%20Capacity%20in%0A%20%20LLMs%20Beyond%20the%20Base%20Model%3F&entry.906535625=Yang%20Yue%20and%20Zhiqi%20Chen%20and%20Rui%20Lu%20and%20Andrew%20Zhao%20and%20Zhaokai%20Wang%20and%20Yang%20Yue%20and%20Shiji%20Song%20and%20Gao%20Huang&entry.1292438233=%20%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20recently%0Ademonstrated%20notable%20success%20in%20enhancing%20the%20reasoning%20capabilities%20of%20LLMs%2C%0Aparticularly%20in%20mathematics%20and%20programming%20tasks.%20It%20is%20widely%20believed%20that%0ARLVR%20enables%20LLMs%20to%20continuously%20self-improve%2C%20thus%20acquiring%20novel%20reasoning%0Aabilities%20that%20exceed%20corresponding%20base%20models%27%20capacity.%20In%20this%20study%2C%0Ahowever%2C%20we%20critically%20re-examines%20this%20assumption%20by%20measuring%20the%0Apass%40%5Ctextit%7Bk%7D%20metric%20with%20large%20values%20of%20%5Ctextit%7Bk%7D%20to%20explore%20the%20reasoning%0Acapability%20boundary%20of%20the%20models%20across%20a%20wide%20range%20of%20model%20families%20and%0Abenchmarks.%20Surprisingly%2C%20the%20RL%20does%20%5Cemph%7Bnot%7D%2C%20in%20fact%2C%20elicit%20fundamentally%0Anew%20reasoning%20patterns.%20While%20RL-trained%20models%20outperform%20their%20base%20models%20at%0Asmaller%20values%20of%20%24k%24%20%28%5Ceg%2C%20%24k%24%3D1%29%2C%20base%20models%20can%20achieve%20a%20comparable%20or%0Aeven%20higher%20pass%40%24k%24%20score%20compared%20to%20their%20RL%20counterparts%20at%20large%20%24k%24%0Avalues.%20The%20reasoning%20paths%20generated%20by%20RL-trained%20models%20are%20already%20included%0Ain%20the%20base%20models%27%20sampling%20distribution%2C%20suggesting%20that%20most%20reasoning%0Aabilities%20manifested%20in%20RL-trained%20models%20are%20already%20obtained%20by%20base%20models.%0AFurther%20analysis%20shows%20that%20RL%20training%20boosts%20the%20performance%20by%20biasing%20the%0Amodel%27s%20output%20distribution%20toward%20paths%20that%20are%20more%20likely%20to%20yield%20rewards%2C%0Atherefore%20sampling%20correct%20responses%20more%20efficiently.%20But%20this%20also%20results%20in%0Aa%20narrower%20reasoning%20capability%20boundary%20compared%20to%20base%20models.%20Similar%0Aresults%20are%20observed%20in%20visual%20reasoning%20tasks%20trained%20with%20RLVR.%20Moreover%2C%20we%0Afind%20that%20distillation%20can%20genuinely%20introduce%20new%20knowledge%20into%20the%20model%2C%0Adifferent%20from%20RLVR.%20These%20findings%20underscore%20a%20critical%20limitation%20of%20RLVR%20in%0Aadvancing%20LLM%20reasoning%20abilities%20which%20requires%20us%20to%20fundamentally%20rethink%0Athe%20impact%20of%20RL%20training%20in%20reasoning%20LLMs%20and%20the%20need%20of%20a%20better%20paradigm.%0AProject%20Page%3A%20https%3A//limit-of-RLVR.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13837v1&entry.124074799=Read"},
{"title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "author": "Shijie Xia and Yiwei Qin and Xuefeng Li and Yan Ma and Run-Ze Fan and Steffi Chern and Haoyang Zou and Fan Zhou and Xiangkun Hu and Jiahe Jin and Yanheng He and Yixin Ye and Yixiu Liu and Pengfei Liu", "abstract": "  The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations in knowledge\nlatency, shallow reasoning, and constrained cognitive processes. During this\nera, prompt engineering emerged as our primary interface with AI, enabling\ndialogue-level communication through natural language. We now witness the\nemergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering\n", "link": "http://arxiv.org/abs/2504.13828v1", "date": "2025-04-18", "relevancy": 2.0807, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5584}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4993}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering&body=Title%3A%20Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering%0AAuthor%3A%20Shijie%20Xia%20and%20Yiwei%20Qin%20and%20Xuefeng%20Li%20and%20Yan%20Ma%20and%20Run-Ze%20Fan%20and%20Steffi%20Chern%20and%20Haoyang%20Zou%20and%20Fan%20Zhou%20and%20Xiangkun%20Hu%20and%20Jiahe%20Jin%20and%20Yanheng%20He%20and%20Yixin%20Ye%20and%20Yixiu%20Liu%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20The%20first%20generation%20of%20Large%20Language%20Models%20-%20what%20might%20be%20called%20%22Act%20I%22%0Aof%20generative%20AI%20%282020-2023%29%20-%20achieved%20remarkable%20success%20through%20massive%0Aparameter%20and%20data%20scaling%2C%20yet%20exhibited%20fundamental%20limitations%20in%20knowledge%0Alatency%2C%20shallow%20reasoning%2C%20and%20constrained%20cognitive%20processes.%20During%20this%0Aera%2C%20prompt%20engineering%20emerged%20as%20our%20primary%20interface%20with%20AI%2C%20enabling%0Adialogue-level%20communication%20through%20natural%20language.%20We%20now%20witness%20the%0Aemergence%20of%20%22Act%20II%22%20%282024-present%29%2C%20where%20models%20are%20transitioning%20from%0Aknowledge-retrieval%20systems%20%28in%20latent%20space%29%20to%20thought-construction%20engines%0Athrough%20test-time%20scaling%20techniques.%20This%20new%20paradigm%20establishes%20a%0Amind-level%20connection%20with%20AI%20through%20language-based%20thoughts.%20In%20this%20paper%2C%0Awe%20clarify%20the%20conceptual%20foundations%20of%20cognition%20engineering%20and%20explain%20why%0Athis%20moment%20is%20critical%20for%20its%20development.%20We%20systematically%20break%20down%20these%0Aadvanced%20approaches%20through%20comprehensive%20tutorials%20and%20optimized%0Aimplementations%2C%20democratizing%20access%20to%20cognition%20engineering%20and%20enabling%0Aevery%20practitioner%20to%20participate%20in%20AI%27s%20second%20act.%20We%20provide%20a%20regularly%0Aupdated%20collection%20of%20papers%20on%20test-time%20scaling%20in%20the%20GitHub%20Repository%3A%0Ahttps%3A//github.com/GAIR-NLP/cognition-engineering%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Act%2520II%253A%2520Test%2520Time%2520Scaling%2520Drives%2520Cognition%2520Engineering%26entry.906535625%3DShijie%2520Xia%2520and%2520Yiwei%2520Qin%2520and%2520Xuefeng%2520Li%2520and%2520Yan%2520Ma%2520and%2520Run-Ze%2520Fan%2520and%2520Steffi%2520Chern%2520and%2520Haoyang%2520Zou%2520and%2520Fan%2520Zhou%2520and%2520Xiangkun%2520Hu%2520and%2520Jiahe%2520Jin%2520and%2520Yanheng%2520He%2520and%2520Yixin%2520Ye%2520and%2520Yixiu%2520Liu%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520The%2520first%2520generation%2520of%2520Large%2520Language%2520Models%2520-%2520what%2520might%2520be%2520called%2520%2522Act%2520I%2522%250Aof%2520generative%2520AI%2520%25282020-2023%2529%2520-%2520achieved%2520remarkable%2520success%2520through%2520massive%250Aparameter%2520and%2520data%2520scaling%252C%2520yet%2520exhibited%2520fundamental%2520limitations%2520in%2520knowledge%250Alatency%252C%2520shallow%2520reasoning%252C%2520and%2520constrained%2520cognitive%2520processes.%2520During%2520this%250Aera%252C%2520prompt%2520engineering%2520emerged%2520as%2520our%2520primary%2520interface%2520with%2520AI%252C%2520enabling%250Adialogue-level%2520communication%2520through%2520natural%2520language.%2520We%2520now%2520witness%2520the%250Aemergence%2520of%2520%2522Act%2520II%2522%2520%25282024-present%2529%252C%2520where%2520models%2520are%2520transitioning%2520from%250Aknowledge-retrieval%2520systems%2520%2528in%2520latent%2520space%2529%2520to%2520thought-construction%2520engines%250Athrough%2520test-time%2520scaling%2520techniques.%2520This%2520new%2520paradigm%2520establishes%2520a%250Amind-level%2520connection%2520with%2520AI%2520through%2520language-based%2520thoughts.%2520In%2520this%2520paper%252C%250Awe%2520clarify%2520the%2520conceptual%2520foundations%2520of%2520cognition%2520engineering%2520and%2520explain%2520why%250Athis%2520moment%2520is%2520critical%2520for%2520its%2520development.%2520We%2520systematically%2520break%2520down%2520these%250Aadvanced%2520approaches%2520through%2520comprehensive%2520tutorials%2520and%2520optimized%250Aimplementations%252C%2520democratizing%2520access%2520to%2520cognition%2520engineering%2520and%2520enabling%250Aevery%2520practitioner%2520to%2520participate%2520in%2520AI%2527s%2520second%2520act.%2520We%2520provide%2520a%2520regularly%250Aupdated%2520collection%2520of%2520papers%2520on%2520test-time%2520scaling%2520in%2520the%2520GitHub%2520Repository%253A%250Ahttps%253A//github.com/GAIR-NLP/cognition-engineering%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering&entry.906535625=Shijie%20Xia%20and%20Yiwei%20Qin%20and%20Xuefeng%20Li%20and%20Yan%20Ma%20and%20Run-Ze%20Fan%20and%20Steffi%20Chern%20and%20Haoyang%20Zou%20and%20Fan%20Zhou%20and%20Xiangkun%20Hu%20and%20Jiahe%20Jin%20and%20Yanheng%20He%20and%20Yixin%20Ye%20and%20Yixiu%20Liu%20and%20Pengfei%20Liu&entry.1292438233=%20%20The%20first%20generation%20of%20Large%20Language%20Models%20-%20what%20might%20be%20called%20%22Act%20I%22%0Aof%20generative%20AI%20%282020-2023%29%20-%20achieved%20remarkable%20success%20through%20massive%0Aparameter%20and%20data%20scaling%2C%20yet%20exhibited%20fundamental%20limitations%20in%20knowledge%0Alatency%2C%20shallow%20reasoning%2C%20and%20constrained%20cognitive%20processes.%20During%20this%0Aera%2C%20prompt%20engineering%20emerged%20as%20our%20primary%20interface%20with%20AI%2C%20enabling%0Adialogue-level%20communication%20through%20natural%20language.%20We%20now%20witness%20the%0Aemergence%20of%20%22Act%20II%22%20%282024-present%29%2C%20where%20models%20are%20transitioning%20from%0Aknowledge-retrieval%20systems%20%28in%20latent%20space%29%20to%20thought-construction%20engines%0Athrough%20test-time%20scaling%20techniques.%20This%20new%20paradigm%20establishes%20a%0Amind-level%20connection%20with%20AI%20through%20language-based%20thoughts.%20In%20this%20paper%2C%0Awe%20clarify%20the%20conceptual%20foundations%20of%20cognition%20engineering%20and%20explain%20why%0Athis%20moment%20is%20critical%20for%20its%20development.%20We%20systematically%20break%20down%20these%0Aadvanced%20approaches%20through%20comprehensive%20tutorials%20and%20optimized%0Aimplementations%2C%20democratizing%20access%20to%20cognition%20engineering%20and%20enabling%0Aevery%20practitioner%20to%20participate%20in%20AI%27s%20second%20act.%20We%20provide%20a%20regularly%0Aupdated%20collection%20of%20papers%20on%20test-time%20scaling%20in%20the%20GitHub%20Repository%3A%0Ahttps%3A//github.com/GAIR-NLP/cognition-engineering%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13828v1&entry.124074799=Read"},
{"title": "Order is All You Need for Categorical Data Clustering", "author": "Yiqun Zhang and Mingjie Zhao and Hong Jia and Yang Lu and Mengke Li and Yiu-ming Cheung", "abstract": "  Categorical data composed of qualitative valued attributes are ubiquitous in\nmachine learning tasks. Due to the lack of well-defined metric space,\ncategorical data distributions are difficult to be intuitively understood.\nClustering is a popular data analysis technique suitable for data distribution\nunderstanding. However, the success of clustering often relies on reasonable\ndistance metrics, which happens to be what categorical data naturally lack.\nThis paper therefore introduces a new finding that the order relation among\nattribute values is the decisive factor in clustering accuracy, and is also the\nkey to understanding categorical data clusters, because the essence of\nclustering is to order the clusters in terms of their admission to samples. To\nobtain the orders, we propose a new learning paradigm that allows joint\nlearning of clusters and the orders. It alternatively partitions the data into\nclusters based on the distance metric built upon the orders and estimates the\nmost likely orders according to the clusters. The algorithm achieves superior\nclustering accuracy with a convergence guarantee, and the learned orders\nfacilitate the understanding of the non-intuitive cluster distribution of\ncategorical data. Extensive experiments with ablation studies, statistical\nevidence, and case studies have validated the new insight into the importance\nof value order and the method proposition. The source code is temporarily\nopened in https://anonymous.4open.science/r/OCL-demo.\n", "link": "http://arxiv.org/abs/2411.15189v3", "date": "2025-04-18", "relevancy": 2.0513, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4245}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4046}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Order%20is%20All%20You%20Need%20for%20Categorical%20Data%20Clustering&body=Title%3A%20Order%20is%20All%20You%20Need%20for%20Categorical%20Data%20Clustering%0AAuthor%3A%20Yiqun%20Zhang%20and%20Mingjie%20Zhao%20and%20Hong%20Jia%20and%20Yang%20Lu%20and%20Mengke%20Li%20and%20Yiu-ming%20Cheung%0AAbstract%3A%20%20%20Categorical%20data%20composed%20of%20qualitative%20valued%20attributes%20are%20ubiquitous%20in%0Amachine%20learning%20tasks.%20Due%20to%20the%20lack%20of%20well-defined%20metric%20space%2C%0Acategorical%20data%20distributions%20are%20difficult%20to%20be%20intuitively%20understood.%0AClustering%20is%20a%20popular%20data%20analysis%20technique%20suitable%20for%20data%20distribution%0Aunderstanding.%20However%2C%20the%20success%20of%20clustering%20often%20relies%20on%20reasonable%0Adistance%20metrics%2C%20which%20happens%20to%20be%20what%20categorical%20data%20naturally%20lack.%0AThis%20paper%20therefore%20introduces%20a%20new%20finding%20that%20the%20order%20relation%20among%0Aattribute%20values%20is%20the%20decisive%20factor%20in%20clustering%20accuracy%2C%20and%20is%20also%20the%0Akey%20to%20understanding%20categorical%20data%20clusters%2C%20because%20the%20essence%20of%0Aclustering%20is%20to%20order%20the%20clusters%20in%20terms%20of%20their%20admission%20to%20samples.%20To%0Aobtain%20the%20orders%2C%20we%20propose%20a%20new%20learning%20paradigm%20that%20allows%20joint%0Alearning%20of%20clusters%20and%20the%20orders.%20It%20alternatively%20partitions%20the%20data%20into%0Aclusters%20based%20on%20the%20distance%20metric%20built%20upon%20the%20orders%20and%20estimates%20the%0Amost%20likely%20orders%20according%20to%20the%20clusters.%20The%20algorithm%20achieves%20superior%0Aclustering%20accuracy%20with%20a%20convergence%20guarantee%2C%20and%20the%20learned%20orders%0Afacilitate%20the%20understanding%20of%20the%20non-intuitive%20cluster%20distribution%20of%0Acategorical%20data.%20Extensive%20experiments%20with%20ablation%20studies%2C%20statistical%0Aevidence%2C%20and%20case%20studies%20have%20validated%20the%20new%20insight%20into%20the%20importance%0Aof%20value%20order%20and%20the%20method%20proposition.%20The%20source%20code%20is%20temporarily%0Aopened%20in%20https%3A//anonymous.4open.science/r/OCL-demo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15189v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrder%2520is%2520All%2520You%2520Need%2520for%2520Categorical%2520Data%2520Clustering%26entry.906535625%3DYiqun%2520Zhang%2520and%2520Mingjie%2520Zhao%2520and%2520Hong%2520Jia%2520and%2520Yang%2520Lu%2520and%2520Mengke%2520Li%2520and%2520Yiu-ming%2520Cheung%26entry.1292438233%3D%2520%2520Categorical%2520data%2520composed%2520of%2520qualitative%2520valued%2520attributes%2520are%2520ubiquitous%2520in%250Amachine%2520learning%2520tasks.%2520Due%2520to%2520the%2520lack%2520of%2520well-defined%2520metric%2520space%252C%250Acategorical%2520data%2520distributions%2520are%2520difficult%2520to%2520be%2520intuitively%2520understood.%250AClustering%2520is%2520a%2520popular%2520data%2520analysis%2520technique%2520suitable%2520for%2520data%2520distribution%250Aunderstanding.%2520However%252C%2520the%2520success%2520of%2520clustering%2520often%2520relies%2520on%2520reasonable%250Adistance%2520metrics%252C%2520which%2520happens%2520to%2520be%2520what%2520categorical%2520data%2520naturally%2520lack.%250AThis%2520paper%2520therefore%2520introduces%2520a%2520new%2520finding%2520that%2520the%2520order%2520relation%2520among%250Aattribute%2520values%2520is%2520the%2520decisive%2520factor%2520in%2520clustering%2520accuracy%252C%2520and%2520is%2520also%2520the%250Akey%2520to%2520understanding%2520categorical%2520data%2520clusters%252C%2520because%2520the%2520essence%2520of%250Aclustering%2520is%2520to%2520order%2520the%2520clusters%2520in%2520terms%2520of%2520their%2520admission%2520to%2520samples.%2520To%250Aobtain%2520the%2520orders%252C%2520we%2520propose%2520a%2520new%2520learning%2520paradigm%2520that%2520allows%2520joint%250Alearning%2520of%2520clusters%2520and%2520the%2520orders.%2520It%2520alternatively%2520partitions%2520the%2520data%2520into%250Aclusters%2520based%2520on%2520the%2520distance%2520metric%2520built%2520upon%2520the%2520orders%2520and%2520estimates%2520the%250Amost%2520likely%2520orders%2520according%2520to%2520the%2520clusters.%2520The%2520algorithm%2520achieves%2520superior%250Aclustering%2520accuracy%2520with%2520a%2520convergence%2520guarantee%252C%2520and%2520the%2520learned%2520orders%250Afacilitate%2520the%2520understanding%2520of%2520the%2520non-intuitive%2520cluster%2520distribution%2520of%250Acategorical%2520data.%2520Extensive%2520experiments%2520with%2520ablation%2520studies%252C%2520statistical%250Aevidence%252C%2520and%2520case%2520studies%2520have%2520validated%2520the%2520new%2520insight%2520into%2520the%2520importance%250Aof%2520value%2520order%2520and%2520the%2520method%2520proposition.%2520The%2520source%2520code%2520is%2520temporarily%250Aopened%2520in%2520https%253A//anonymous.4open.science/r/OCL-demo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15189v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Order%20is%20All%20You%20Need%20for%20Categorical%20Data%20Clustering&entry.906535625=Yiqun%20Zhang%20and%20Mingjie%20Zhao%20and%20Hong%20Jia%20and%20Yang%20Lu%20and%20Mengke%20Li%20and%20Yiu-ming%20Cheung&entry.1292438233=%20%20Categorical%20data%20composed%20of%20qualitative%20valued%20attributes%20are%20ubiquitous%20in%0Amachine%20learning%20tasks.%20Due%20to%20the%20lack%20of%20well-defined%20metric%20space%2C%0Acategorical%20data%20distributions%20are%20difficult%20to%20be%20intuitively%20understood.%0AClustering%20is%20a%20popular%20data%20analysis%20technique%20suitable%20for%20data%20distribution%0Aunderstanding.%20However%2C%20the%20success%20of%20clustering%20often%20relies%20on%20reasonable%0Adistance%20metrics%2C%20which%20happens%20to%20be%20what%20categorical%20data%20naturally%20lack.%0AThis%20paper%20therefore%20introduces%20a%20new%20finding%20that%20the%20order%20relation%20among%0Aattribute%20values%20is%20the%20decisive%20factor%20in%20clustering%20accuracy%2C%20and%20is%20also%20the%0Akey%20to%20understanding%20categorical%20data%20clusters%2C%20because%20the%20essence%20of%0Aclustering%20is%20to%20order%20the%20clusters%20in%20terms%20of%20their%20admission%20to%20samples.%20To%0Aobtain%20the%20orders%2C%20we%20propose%20a%20new%20learning%20paradigm%20that%20allows%20joint%0Alearning%20of%20clusters%20and%20the%20orders.%20It%20alternatively%20partitions%20the%20data%20into%0Aclusters%20based%20on%20the%20distance%20metric%20built%20upon%20the%20orders%20and%20estimates%20the%0Amost%20likely%20orders%20according%20to%20the%20clusters.%20The%20algorithm%20achieves%20superior%0Aclustering%20accuracy%20with%20a%20convergence%20guarantee%2C%20and%20the%20learned%20orders%0Afacilitate%20the%20understanding%20of%20the%20non-intuitive%20cluster%20distribution%20of%0Acategorical%20data.%20Extensive%20experiments%20with%20ablation%20studies%2C%20statistical%0Aevidence%2C%20and%20case%20studies%20have%20validated%20the%20new%20insight%20into%20the%20importance%0Aof%20value%20order%20and%20the%20method%20proposition.%20The%20source%20code%20is%20temporarily%0Aopened%20in%20https%3A//anonymous.4open.science/r/OCL-demo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15189v3&entry.124074799=Read"},
{"title": "Understanding Epistemic Language with a Language-augmented Bayesian\n  Theory of Mind", "author": "Lance Ying and Tan Zhi-Xuan and Lionel Wong and Vikash Mansinghka and Joshua B. Tenenbaum", "abstract": "  How do people understand and evaluate claims about others' beliefs, even\nthough these beliefs cannot be directly observed? In this paper, we introduce a\ncognitive model of epistemic language interpretation, grounded in Bayesian\ninferences about other agents' goals, beliefs, and intentions: a\nlanguage-augmented Bayesian theory-of-mind (LaBToM). By translating natural\nlanguage into an epistemic ``language-of-thought'' with grammar-constrained LLM\ndecoding, then evaluating these translations against the inferences produced by\ninverting a generative model of rational action and perception, LaBToM captures\ngraded plausibility judgments of epistemic claims. We validate our model in an\nexperiment where participants watch an agent navigate a maze to find keys\nhidden in boxes needed to reach their goal, then rate sentences about the\nagent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and\nablated models, our model correlates highly with human judgments for a wide\nrange of expressions, including modal language, uncertainty expressions,\nknowledge claims, likelihood comparisons, and attributions of false belief.\n", "link": "http://arxiv.org/abs/2408.12022v2", "date": "2025-04-18", "relevancy": 2.044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Epistemic%20Language%20with%20a%20Language-augmented%20Bayesian%0A%20%20Theory%20of%20Mind&body=Title%3A%20Understanding%20Epistemic%20Language%20with%20a%20Language-augmented%20Bayesian%0A%20%20Theory%20of%20Mind%0AAuthor%3A%20Lance%20Ying%20and%20Tan%20Zhi-Xuan%20and%20Lionel%20Wong%20and%20Vikash%20Mansinghka%20and%20Joshua%20B.%20Tenenbaum%0AAbstract%3A%20%20%20How%20do%20people%20understand%20and%20evaluate%20claims%20about%20others%27%20beliefs%2C%20even%0Athough%20these%20beliefs%20cannot%20be%20directly%20observed%3F%20In%20this%20paper%2C%20we%20introduce%20a%0Acognitive%20model%20of%20epistemic%20language%20interpretation%2C%20grounded%20in%20Bayesian%0Ainferences%20about%20other%20agents%27%20goals%2C%20beliefs%2C%20and%20intentions%3A%20a%0Alanguage-augmented%20Bayesian%20theory-of-mind%20%28LaBToM%29.%20By%20translating%20natural%0Alanguage%20into%20an%20epistemic%20%60%60language-of-thought%27%27%20with%20grammar-constrained%20LLM%0Adecoding%2C%20then%20evaluating%20these%20translations%20against%20the%20inferences%20produced%20by%0Ainverting%20a%20generative%20model%20of%20rational%20action%20and%20perception%2C%20LaBToM%20captures%0Agraded%20plausibility%20judgments%20of%20epistemic%20claims.%20We%20validate%20our%20model%20in%20an%0Aexperiment%20where%20participants%20watch%20an%20agent%20navigate%20a%20maze%20to%20find%20keys%0Ahidden%20in%20boxes%20needed%20to%20reach%20their%20goal%2C%20then%20rate%20sentences%20about%20the%0Aagent%27s%20beliefs.%20In%20contrast%20with%20multimodal%20LLMs%20%28GPT-4o%2C%20Gemini%20Pro%29%20and%0Aablated%20models%2C%20our%20model%20correlates%20highly%20with%20human%20judgments%20for%20a%20wide%0Arange%20of%20expressions%2C%20including%20modal%20language%2C%20uncertainty%20expressions%2C%0Aknowledge%20claims%2C%20likelihood%20comparisons%2C%20and%20attributions%20of%20false%20belief.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Epistemic%2520Language%2520with%2520a%2520Language-augmented%2520Bayesian%250A%2520%2520Theory%2520of%2520Mind%26entry.906535625%3DLance%2520Ying%2520and%2520Tan%2520Zhi-Xuan%2520and%2520Lionel%2520Wong%2520and%2520Vikash%2520Mansinghka%2520and%2520Joshua%2520B.%2520Tenenbaum%26entry.1292438233%3D%2520%2520How%2520do%2520people%2520understand%2520and%2520evaluate%2520claims%2520about%2520others%2527%2520beliefs%252C%2520even%250Athough%2520these%2520beliefs%2520cannot%2520be%2520directly%2520observed%253F%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Acognitive%2520model%2520of%2520epistemic%2520language%2520interpretation%252C%2520grounded%2520in%2520Bayesian%250Ainferences%2520about%2520other%2520agents%2527%2520goals%252C%2520beliefs%252C%2520and%2520intentions%253A%2520a%250Alanguage-augmented%2520Bayesian%2520theory-of-mind%2520%2528LaBToM%2529.%2520By%2520translating%2520natural%250Alanguage%2520into%2520an%2520epistemic%2520%2560%2560language-of-thought%2527%2527%2520with%2520grammar-constrained%2520LLM%250Adecoding%252C%2520then%2520evaluating%2520these%2520translations%2520against%2520the%2520inferences%2520produced%2520by%250Ainverting%2520a%2520generative%2520model%2520of%2520rational%2520action%2520and%2520perception%252C%2520LaBToM%2520captures%250Agraded%2520plausibility%2520judgments%2520of%2520epistemic%2520claims.%2520We%2520validate%2520our%2520model%2520in%2520an%250Aexperiment%2520where%2520participants%2520watch%2520an%2520agent%2520navigate%2520a%2520maze%2520to%2520find%2520keys%250Ahidden%2520in%2520boxes%2520needed%2520to%2520reach%2520their%2520goal%252C%2520then%2520rate%2520sentences%2520about%2520the%250Aagent%2527s%2520beliefs.%2520In%2520contrast%2520with%2520multimodal%2520LLMs%2520%2528GPT-4o%252C%2520Gemini%2520Pro%2529%2520and%250Aablated%2520models%252C%2520our%2520model%2520correlates%2520highly%2520with%2520human%2520judgments%2520for%2520a%2520wide%250Arange%2520of%2520expressions%252C%2520including%2520modal%2520language%252C%2520uncertainty%2520expressions%252C%250Aknowledge%2520claims%252C%2520likelihood%2520comparisons%252C%2520and%2520attributions%2520of%2520false%2520belief.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Epistemic%20Language%20with%20a%20Language-augmented%20Bayesian%0A%20%20Theory%20of%20Mind&entry.906535625=Lance%20Ying%20and%20Tan%20Zhi-Xuan%20and%20Lionel%20Wong%20and%20Vikash%20Mansinghka%20and%20Joshua%20B.%20Tenenbaum&entry.1292438233=%20%20How%20do%20people%20understand%20and%20evaluate%20claims%20about%20others%27%20beliefs%2C%20even%0Athough%20these%20beliefs%20cannot%20be%20directly%20observed%3F%20In%20this%20paper%2C%20we%20introduce%20a%0Acognitive%20model%20of%20epistemic%20language%20interpretation%2C%20grounded%20in%20Bayesian%0Ainferences%20about%20other%20agents%27%20goals%2C%20beliefs%2C%20and%20intentions%3A%20a%0Alanguage-augmented%20Bayesian%20theory-of-mind%20%28LaBToM%29.%20By%20translating%20natural%0Alanguage%20into%20an%20epistemic%20%60%60language-of-thought%27%27%20with%20grammar-constrained%20LLM%0Adecoding%2C%20then%20evaluating%20these%20translations%20against%20the%20inferences%20produced%20by%0Ainverting%20a%20generative%20model%20of%20rational%20action%20and%20perception%2C%20LaBToM%20captures%0Agraded%20plausibility%20judgments%20of%20epistemic%20claims.%20We%20validate%20our%20model%20in%20an%0Aexperiment%20where%20participants%20watch%20an%20agent%20navigate%20a%20maze%20to%20find%20keys%0Ahidden%20in%20boxes%20needed%20to%20reach%20their%20goal%2C%20then%20rate%20sentences%20about%20the%0Aagent%27s%20beliefs.%20In%20contrast%20with%20multimodal%20LLMs%20%28GPT-4o%2C%20Gemini%20Pro%29%20and%0Aablated%20models%2C%20our%20model%20correlates%20highly%20with%20human%20judgments%20for%20a%20wide%0Arange%20of%20expressions%2C%20including%20modal%20language%2C%20uncertainty%20expressions%2C%0Aknowledge%20claims%2C%20likelihood%20comparisons%2C%20and%20attributions%20of%20false%20belief.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12022v2&entry.124074799=Read"},
{"title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning", "author": "Yixuan Even Xu and Yash Savani and Fei Fang and Zico Kolter", "abstract": "  Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark.\n", "link": "http://arxiv.org/abs/2504.13818v1", "date": "2025-04-18", "relevancy": 2.0439, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5434}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4904}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20All%20Rollouts%20are%20Useful%3A%20Down-Sampling%20Rollouts%20in%20LLM%20Reinforcement%0A%20%20Learning&body=Title%3A%20Not%20All%20Rollouts%20are%20Useful%3A%20Down-Sampling%20Rollouts%20in%20LLM%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Yixuan%20Even%20Xu%20and%20Yash%20Savani%20and%20Fei%20Fang%20and%20Zico%20Kolter%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20enhancing%0Areasoning%20capabilities%20in%20large%20language%20models%2C%20but%20faces%20a%20fundamental%0Aasymmetry%20in%20computation%20and%20memory%20requirements%3A%20inference%20is%20embarrassingly%0Aparallel%20with%20a%20minimal%20memory%20footprint%2C%20while%20policy%20updates%20require%0Aextensive%20synchronization%20and%20are%20memory-intensive.%20To%20address%20this%20asymmetry%2C%0Awe%20introduce%20PODS%20%28Policy%20Optimization%20with%20Down-Sampling%29%2C%20a%20framework%20that%0Astrategically%20decouples%20these%20phases%20by%20generating%20numerous%20rollouts%20in%0Aparallel%20but%20updating%20only%20on%20an%20informative%20subset.%20Within%20this%20framework%2C%20we%0Adevelop%20max-variance%20down-sampling%2C%20a%20theoretically%20motivated%20method%20that%0Aselects%20rollouts%20with%20maximally%20diverse%20reward%20signals.%20We%20prove%20that%20this%0Aapproach%20has%20an%20efficient%20algorithmic%20solution%2C%20and%20empirically%20demonstrate%0Athat%20GRPO%20with%20PODS%20using%20max-variance%20down-sampling%20achieves%20superior%0Aperformance%20over%20standard%20GRPO%20on%20the%20GSM8K%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520All%2520Rollouts%2520are%2520Useful%253A%2520Down-Sampling%2520Rollouts%2520in%2520LLM%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DYixuan%2520Even%2520Xu%2520and%2520Yash%2520Savani%2520and%2520Fei%2520Fang%2520and%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520enhancing%250Areasoning%2520capabilities%2520in%2520large%2520language%2520models%252C%2520but%2520faces%2520a%2520fundamental%250Aasymmetry%2520in%2520computation%2520and%2520memory%2520requirements%253A%2520inference%2520is%2520embarrassingly%250Aparallel%2520with%2520a%2520minimal%2520memory%2520footprint%252C%2520while%2520policy%2520updates%2520require%250Aextensive%2520synchronization%2520and%2520are%2520memory-intensive.%2520To%2520address%2520this%2520asymmetry%252C%250Awe%2520introduce%2520PODS%2520%2528Policy%2520Optimization%2520with%2520Down-Sampling%2529%252C%2520a%2520framework%2520that%250Astrategically%2520decouples%2520these%2520phases%2520by%2520generating%2520numerous%2520rollouts%2520in%250Aparallel%2520but%2520updating%2520only%2520on%2520an%2520informative%2520subset.%2520Within%2520this%2520framework%252C%2520we%250Adevelop%2520max-variance%2520down-sampling%252C%2520a%2520theoretically%2520motivated%2520method%2520that%250Aselects%2520rollouts%2520with%2520maximally%2520diverse%2520reward%2520signals.%2520We%2520prove%2520that%2520this%250Aapproach%2520has%2520an%2520efficient%2520algorithmic%2520solution%252C%2520and%2520empirically%2520demonstrate%250Athat%2520GRPO%2520with%2520PODS%2520using%2520max-variance%2520down-sampling%2520achieves%2520superior%250Aperformance%2520over%2520standard%2520GRPO%2520on%2520the%2520GSM8K%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20All%20Rollouts%20are%20Useful%3A%20Down-Sampling%20Rollouts%20in%20LLM%20Reinforcement%0A%20%20Learning&entry.906535625=Yixuan%20Even%20Xu%20and%20Yash%20Savani%20and%20Fei%20Fang%20and%20Zico%20Kolter&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20powerful%20paradigm%20for%20enhancing%0Areasoning%20capabilities%20in%20large%20language%20models%2C%20but%20faces%20a%20fundamental%0Aasymmetry%20in%20computation%20and%20memory%20requirements%3A%20inference%20is%20embarrassingly%0Aparallel%20with%20a%20minimal%20memory%20footprint%2C%20while%20policy%20updates%20require%0Aextensive%20synchronization%20and%20are%20memory-intensive.%20To%20address%20this%20asymmetry%2C%0Awe%20introduce%20PODS%20%28Policy%20Optimization%20with%20Down-Sampling%29%2C%20a%20framework%20that%0Astrategically%20decouples%20these%20phases%20by%20generating%20numerous%20rollouts%20in%0Aparallel%20but%20updating%20only%20on%20an%20informative%20subset.%20Within%20this%20framework%2C%20we%0Adevelop%20max-variance%20down-sampling%2C%20a%20theoretically%20motivated%20method%20that%0Aselects%20rollouts%20with%20maximally%20diverse%20reward%20signals.%20We%20prove%20that%20this%0Aapproach%20has%20an%20efficient%20algorithmic%20solution%2C%20and%20empirically%20demonstrate%0Athat%20GRPO%20with%20PODS%20using%20max-variance%20down-sampling%20achieves%20superior%0Aperformance%20over%20standard%20GRPO%20on%20the%20GSM8K%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13818v1&entry.124074799=Read"},
{"title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via\n  Open-ended Interaction Simulation", "author": "Yichen Wu and Xudong Pan and Geng Hong and Min Yang", "abstract": "  As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors.\n", "link": "http://arxiv.org/abs/2504.13707v1", "date": "2025-04-18", "relevancy": 2.0401, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenDeception%3A%20Benchmarking%20and%20Investigating%20AI%20Deceptive%20Behaviors%20via%0A%20%20Open-ended%20Interaction%20Simulation&body=Title%3A%20OpenDeception%3A%20Benchmarking%20and%20Investigating%20AI%20Deceptive%20Behaviors%20via%0A%20%20Open-ended%20Interaction%20Simulation%0AAuthor%3A%20Yichen%20Wu%20and%20Xudong%20Pan%20and%20Geng%20Hong%20and%20Min%20Yang%0AAbstract%3A%20%20%20As%20the%20general%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20improve%20and%20agent%0Aapplications%20become%20more%20widespread%2C%20the%20underlying%20deception%20risks%20urgently%0Arequire%20systematic%20evaluation%20and%20effective%20oversight.%20Unlike%20existing%0Aevaluation%20which%20uses%20simulated%20games%20or%20presents%20limited%20choices%2C%20we%20introduce%0AOpenDeception%2C%20a%20novel%20deception%20evaluation%20framework%20with%20an%20open-ended%0Ascenario%20dataset.%20OpenDeception%20jointly%20evaluates%20both%20the%20deception%20intention%0Aand%20capabilities%20of%20LLM-based%20agents%20by%20inspecting%20their%20internal%20reasoning%0Aprocess.%20Specifically%2C%20we%20construct%20five%20types%20of%20common%20use%20cases%20where%20LLMs%0Aintensively%20interact%20with%20the%20user%2C%20each%20consisting%20of%20ten%20diverse%2C%20concrete%0Ascenarios%20from%20the%20real%20world.%20To%20avoid%20ethical%20concerns%20and%20costs%20of%20high-risk%0Adeceptive%20interactions%20with%20human%20testers%2C%20we%20propose%20to%20simulate%20the%0Amulti-turn%20dialogue%20via%20agent%20simulation.%20Extensive%20evaluation%20of%20eleven%0Amainstream%20LLMs%20on%20OpenDeception%20highlights%20the%20urgent%20need%20to%20address%0Adeception%20risks%20and%20security%20concerns%20in%20LLM-based%20agents%3A%20the%20deception%0Aintention%20ratio%20across%20the%20models%20exceeds%2080%25%2C%20while%20the%20deception%20success%20rate%0Asurpasses%2050%25.%20Furthermore%2C%20we%20observe%20that%20LLMs%20with%20stronger%20capabilities%20do%0Aexhibit%20a%20higher%20risk%20of%20deception%2C%20which%20calls%20for%20more%20alignment%20efforts%20on%0Ainhibiting%20deceptive%20behaviors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenDeception%253A%2520Benchmarking%2520and%2520Investigating%2520AI%2520Deceptive%2520Behaviors%2520via%250A%2520%2520Open-ended%2520Interaction%2520Simulation%26entry.906535625%3DYichen%2520Wu%2520and%2520Xudong%2520Pan%2520and%2520Geng%2520Hong%2520and%2520Min%2520Yang%26entry.1292438233%3D%2520%2520As%2520the%2520general%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520improve%2520and%2520agent%250Aapplications%2520become%2520more%2520widespread%252C%2520the%2520underlying%2520deception%2520risks%2520urgently%250Arequire%2520systematic%2520evaluation%2520and%2520effective%2520oversight.%2520Unlike%2520existing%250Aevaluation%2520which%2520uses%2520simulated%2520games%2520or%2520presents%2520limited%2520choices%252C%2520we%2520introduce%250AOpenDeception%252C%2520a%2520novel%2520deception%2520evaluation%2520framework%2520with%2520an%2520open-ended%250Ascenario%2520dataset.%2520OpenDeception%2520jointly%2520evaluates%2520both%2520the%2520deception%2520intention%250Aand%2520capabilities%2520of%2520LLM-based%2520agents%2520by%2520inspecting%2520their%2520internal%2520reasoning%250Aprocess.%2520Specifically%252C%2520we%2520construct%2520five%2520types%2520of%2520common%2520use%2520cases%2520where%2520LLMs%250Aintensively%2520interact%2520with%2520the%2520user%252C%2520each%2520consisting%2520of%2520ten%2520diverse%252C%2520concrete%250Ascenarios%2520from%2520the%2520real%2520world.%2520To%2520avoid%2520ethical%2520concerns%2520and%2520costs%2520of%2520high-risk%250Adeceptive%2520interactions%2520with%2520human%2520testers%252C%2520we%2520propose%2520to%2520simulate%2520the%250Amulti-turn%2520dialogue%2520via%2520agent%2520simulation.%2520Extensive%2520evaluation%2520of%2520eleven%250Amainstream%2520LLMs%2520on%2520OpenDeception%2520highlights%2520the%2520urgent%2520need%2520to%2520address%250Adeception%2520risks%2520and%2520security%2520concerns%2520in%2520LLM-based%2520agents%253A%2520the%2520deception%250Aintention%2520ratio%2520across%2520the%2520models%2520exceeds%252080%2525%252C%2520while%2520the%2520deception%2520success%2520rate%250Asurpasses%252050%2525.%2520Furthermore%252C%2520we%2520observe%2520that%2520LLMs%2520with%2520stronger%2520capabilities%2520do%250Aexhibit%2520a%2520higher%2520risk%2520of%2520deception%252C%2520which%2520calls%2520for%2520more%2520alignment%2520efforts%2520on%250Ainhibiting%2520deceptive%2520behaviors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDeception%3A%20Benchmarking%20and%20Investigating%20AI%20Deceptive%20Behaviors%20via%0A%20%20Open-ended%20Interaction%20Simulation&entry.906535625=Yichen%20Wu%20and%20Xudong%20Pan%20and%20Geng%20Hong%20and%20Min%20Yang&entry.1292438233=%20%20As%20the%20general%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20improve%20and%20agent%0Aapplications%20become%20more%20widespread%2C%20the%20underlying%20deception%20risks%20urgently%0Arequire%20systematic%20evaluation%20and%20effective%20oversight.%20Unlike%20existing%0Aevaluation%20which%20uses%20simulated%20games%20or%20presents%20limited%20choices%2C%20we%20introduce%0AOpenDeception%2C%20a%20novel%20deception%20evaluation%20framework%20with%20an%20open-ended%0Ascenario%20dataset.%20OpenDeception%20jointly%20evaluates%20both%20the%20deception%20intention%0Aand%20capabilities%20of%20LLM-based%20agents%20by%20inspecting%20their%20internal%20reasoning%0Aprocess.%20Specifically%2C%20we%20construct%20five%20types%20of%20common%20use%20cases%20where%20LLMs%0Aintensively%20interact%20with%20the%20user%2C%20each%20consisting%20of%20ten%20diverse%2C%20concrete%0Ascenarios%20from%20the%20real%20world.%20To%20avoid%20ethical%20concerns%20and%20costs%20of%20high-risk%0Adeceptive%20interactions%20with%20human%20testers%2C%20we%20propose%20to%20simulate%20the%0Amulti-turn%20dialogue%20via%20agent%20simulation.%20Extensive%20evaluation%20of%20eleven%0Amainstream%20LLMs%20on%20OpenDeception%20highlights%20the%20urgent%20need%20to%20address%0Adeception%20risks%20and%20security%20concerns%20in%20LLM-based%20agents%3A%20the%20deception%0Aintention%20ratio%20across%20the%20models%20exceeds%2080%25%2C%20while%20the%20deception%20success%20rate%0Asurpasses%2050%25.%20Furthermore%2C%20we%20observe%20that%20LLMs%20with%20stronger%20capabilities%20do%0Aexhibit%20a%20higher%20risk%20of%20deception%2C%20which%20calls%20for%20more%20alignment%20efforts%20on%0Ainhibiting%20deceptive%20behaviors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13707v1&entry.124074799=Read"},
{"title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space", "author": "Yicheng Chen and Yining Li and Kai Hu and Zerun Ma and Haochen Ye and Kai Chen", "abstract": "  Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\n\\textbf{M}aximize the \\textbf{I}nformation \\textbf{G}ain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.\n", "link": "http://arxiv.org/abs/2504.13835v1", "date": "2025-04-18", "relevancy": 2.0221, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5166}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIG%3A%20Automatic%20Data%20Selection%20for%20Instruction%20Tuning%20by%20Maximizing%0A%20%20Information%20Gain%20in%20Semantic%20Space&body=Title%3A%20MIG%3A%20Automatic%20Data%20Selection%20for%20Instruction%20Tuning%20by%20Maximizing%0A%20%20Information%20Gain%20in%20Semantic%20Space%0AAuthor%3A%20Yicheng%20Chen%20and%20Yining%20Li%20and%20Kai%20Hu%20and%20Zerun%20Ma%20and%20Haochen%20Ye%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Data%20quality%20and%20diversity%20are%20key%20to%20the%20construction%20of%20effective%0Ainstruction-tuning%20datasets.%20%25%20With%20the%20increasing%20availability%20of%20open-source%0Ainstruction-tuning%20datasets%2C%20it%20is%20advantageous%20to%20automatically%20select%0Ahigh-quality%20and%20diverse%20subsets%20from%20a%20vast%20amount%20of%20data.%20%25%20Existing%20methods%0Atypically%20prioritize%20instance%20quality%20and%20use%20heuristic%20rules%20to%20maintain%0Adiversity.%20%25%20However%2C%20this%20absence%20of%20a%20comprehensive%20view%20of%20the%20entire%0Acollection%20often%20leads%20to%20suboptimal%20results.%20%25%20Moreover%2C%20heuristic%20rules%0Agenerally%20focus%20on%20distance%20or%20clustering%20within%20the%20embedding%20space%2C%20which%0Afails%20to%20accurately%20capture%20the%20intent%20of%20complex%20instructions%20in%20the%20semantic%0Aspace.%20%25%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20unified%20method%20for%20quantifying%20the%0Ainformation%20content%20of%20datasets.%20This%20method%20models%20the%20semantic%20space%20by%0Aconstructing%20a%20label%20graph%20and%20quantifies%20diversity%20based%20on%20the%20distribution%0Aof%20information%20within%20the%20graph.%20%25%20Based%20on%20such%20a%20measurement%2C%20we%20further%0Aintroduce%20an%20efficient%20sampling%20method%20that%20selects%20data%20samples%20iteratively%20to%0A%5Ctextbf%7BM%7Daximize%20the%20%5Ctextbf%7BI%7Dnformation%20%5Ctextbf%7BG%7Dain%20%28MIG%29%20in%20semantic%0Aspace.%20%25%20Experiments%20on%20various%20datasets%20and%20base%20models%20demonstrate%20that%20MIG%0Aconsistently%20outperforms%20state-of-the-art%20methods.%20%25%20Notably%2C%20the%20model%0Afine-tuned%20with%205%5C%25%20Tulu3%20data%20sampled%20by%20MIG%20achieves%20comparable%20performance%0Ato%20the%20official%20SFT%20model%20trained%20on%20the%20full%20dataset%2C%20with%20improvements%20of%0A%2B5.73%5C%25%20on%20AlpacaEval%20and%20%2B6.89%5C%25%20on%20Wildbench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIG%253A%2520Automatic%2520Data%2520Selection%2520for%2520Instruction%2520Tuning%2520by%2520Maximizing%250A%2520%2520Information%2520Gain%2520in%2520Semantic%2520Space%26entry.906535625%3DYicheng%2520Chen%2520and%2520Yining%2520Li%2520and%2520Kai%2520Hu%2520and%2520Zerun%2520Ma%2520and%2520Haochen%2520Ye%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Data%2520quality%2520and%2520diversity%2520are%2520key%2520to%2520the%2520construction%2520of%2520effective%250Ainstruction-tuning%2520datasets.%2520%2525%2520With%2520the%2520increasing%2520availability%2520of%2520open-source%250Ainstruction-tuning%2520datasets%252C%2520it%2520is%2520advantageous%2520to%2520automatically%2520select%250Ahigh-quality%2520and%2520diverse%2520subsets%2520from%2520a%2520vast%2520amount%2520of%2520data.%2520%2525%2520Existing%2520methods%250Atypically%2520prioritize%2520instance%2520quality%2520and%2520use%2520heuristic%2520rules%2520to%2520maintain%250Adiversity.%2520%2525%2520However%252C%2520this%2520absence%2520of%2520a%2520comprehensive%2520view%2520of%2520the%2520entire%250Acollection%2520often%2520leads%2520to%2520suboptimal%2520results.%2520%2525%2520Moreover%252C%2520heuristic%2520rules%250Agenerally%2520focus%2520on%2520distance%2520or%2520clustering%2520within%2520the%2520embedding%2520space%252C%2520which%250Afails%2520to%2520accurately%2520capture%2520the%2520intent%2520of%2520complex%2520instructions%2520in%2520the%2520semantic%250Aspace.%2520%2525%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520unified%2520method%2520for%2520quantifying%2520the%250Ainformation%2520content%2520of%2520datasets.%2520This%2520method%2520models%2520the%2520semantic%2520space%2520by%250Aconstructing%2520a%2520label%2520graph%2520and%2520quantifies%2520diversity%2520based%2520on%2520the%2520distribution%250Aof%2520information%2520within%2520the%2520graph.%2520%2525%2520Based%2520on%2520such%2520a%2520measurement%252C%2520we%2520further%250Aintroduce%2520an%2520efficient%2520sampling%2520method%2520that%2520selects%2520data%2520samples%2520iteratively%2520to%250A%255Ctextbf%257BM%257Daximize%2520the%2520%255Ctextbf%257BI%257Dnformation%2520%255Ctextbf%257BG%257Dain%2520%2528MIG%2529%2520in%2520semantic%250Aspace.%2520%2525%2520Experiments%2520on%2520various%2520datasets%2520and%2520base%2520models%2520demonstrate%2520that%2520MIG%250Aconsistently%2520outperforms%2520state-of-the-art%2520methods.%2520%2525%2520Notably%252C%2520the%2520model%250Afine-tuned%2520with%25205%255C%2525%2520Tulu3%2520data%2520sampled%2520by%2520MIG%2520achieves%2520comparable%2520performance%250Ato%2520the%2520official%2520SFT%2520model%2520trained%2520on%2520the%2520full%2520dataset%252C%2520with%2520improvements%2520of%250A%252B5.73%255C%2525%2520on%2520AlpacaEval%2520and%2520%252B6.89%255C%2525%2520on%2520Wildbench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIG%3A%20Automatic%20Data%20Selection%20for%20Instruction%20Tuning%20by%20Maximizing%0A%20%20Information%20Gain%20in%20Semantic%20Space&entry.906535625=Yicheng%20Chen%20and%20Yining%20Li%20and%20Kai%20Hu%20and%20Zerun%20Ma%20and%20Haochen%20Ye%20and%20Kai%20Chen&entry.1292438233=%20%20Data%20quality%20and%20diversity%20are%20key%20to%20the%20construction%20of%20effective%0Ainstruction-tuning%20datasets.%20%25%20With%20the%20increasing%20availability%20of%20open-source%0Ainstruction-tuning%20datasets%2C%20it%20is%20advantageous%20to%20automatically%20select%0Ahigh-quality%20and%20diverse%20subsets%20from%20a%20vast%20amount%20of%20data.%20%25%20Existing%20methods%0Atypically%20prioritize%20instance%20quality%20and%20use%20heuristic%20rules%20to%20maintain%0Adiversity.%20%25%20However%2C%20this%20absence%20of%20a%20comprehensive%20view%20of%20the%20entire%0Acollection%20often%20leads%20to%20suboptimal%20results.%20%25%20Moreover%2C%20heuristic%20rules%0Agenerally%20focus%20on%20distance%20or%20clustering%20within%20the%20embedding%20space%2C%20which%0Afails%20to%20accurately%20capture%20the%20intent%20of%20complex%20instructions%20in%20the%20semantic%0Aspace.%20%25%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20unified%20method%20for%20quantifying%20the%0Ainformation%20content%20of%20datasets.%20This%20method%20models%20the%20semantic%20space%20by%0Aconstructing%20a%20label%20graph%20and%20quantifies%20diversity%20based%20on%20the%20distribution%0Aof%20information%20within%20the%20graph.%20%25%20Based%20on%20such%20a%20measurement%2C%20we%20further%0Aintroduce%20an%20efficient%20sampling%20method%20that%20selects%20data%20samples%20iteratively%20to%0A%5Ctextbf%7BM%7Daximize%20the%20%5Ctextbf%7BI%7Dnformation%20%5Ctextbf%7BG%7Dain%20%28MIG%29%20in%20semantic%0Aspace.%20%25%20Experiments%20on%20various%20datasets%20and%20base%20models%20demonstrate%20that%20MIG%0Aconsistently%20outperforms%20state-of-the-art%20methods.%20%25%20Notably%2C%20the%20model%0Afine-tuned%20with%205%5C%25%20Tulu3%20data%20sampled%20by%20MIG%20achieves%20comparable%20performance%0Ato%20the%20official%20SFT%20model%20trained%20on%20the%20full%20dataset%2C%20with%20improvements%20of%0A%2B5.73%5C%25%20on%20AlpacaEval%20and%20%2B6.89%5C%25%20on%20Wildbench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13835v1&entry.124074799=Read"},
{"title": "Argumentative Large Language Models for Explainable and Contestable\n  Claim Verification", "author": "Gabriel Freedman and Adam Dejl and Deniz Gorur and Xiang Yin and Antonio Rago and Francesca Toni", "abstract": "  The profusion of knowledge encoded in large language models (LLMs) and their\nability to apply this knowledge zero-shot in a range of settings makes them\npromising candidates for use in decision-making. However, they are currently\nlimited by their inability to provide outputs which can be faithfully explained\nand effectively contested to correct mistakes. In this paper, we attempt to\nreconcile these strengths and weaknesses by introducing \\emph{argumentative\nLLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning.\nConcretely, ArgLLMs construct argumentation frameworks, which then serve as the\nbasis for formal reasoning in support of decision-making. The interpretable\nnature of these argumentation frameworks and formal reasoning means that any\ndecision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs'\nperformance experimentally in comparison with state-of-the-art techniques, in\nthe context of the decision-making task of claim verification. We also define\nnovel properties to characterise contestability and assess ArgLLMs formally in\nterms of these properties.\n", "link": "http://arxiv.org/abs/2405.02079v3", "date": "2025-04-18", "relevancy": 2.0002, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Argumentative%20Large%20Language%20Models%20for%20Explainable%20and%20Contestable%0A%20%20Claim%20Verification&body=Title%3A%20Argumentative%20Large%20Language%20Models%20for%20Explainable%20and%20Contestable%0A%20%20Claim%20Verification%0AAuthor%3A%20Gabriel%20Freedman%20and%20Adam%20Dejl%20and%20Deniz%20Gorur%20and%20Xiang%20Yin%20and%20Antonio%20Rago%20and%20Francesca%20Toni%0AAbstract%3A%20%20%20The%20profusion%20of%20knowledge%20encoded%20in%20large%20language%20models%20%28LLMs%29%20and%20their%0Aability%20to%20apply%20this%20knowledge%20zero-shot%20in%20a%20range%20of%20settings%20makes%20them%0Apromising%20candidates%20for%20use%20in%20decision-making.%20However%2C%20they%20are%20currently%0Alimited%20by%20their%20inability%20to%20provide%20outputs%20which%20can%20be%20faithfully%20explained%0Aand%20effectively%20contested%20to%20correct%20mistakes.%20In%20this%20paper%2C%20we%20attempt%20to%0Areconcile%20these%20strengths%20and%20weaknesses%20by%20introducing%20%5Cemph%7Bargumentative%0ALLMs%20%28ArgLLMs%29%7D%2C%20a%20method%20for%20augmenting%20LLMs%20with%20argumentative%20reasoning.%0AConcretely%2C%20ArgLLMs%20construct%20argumentation%20frameworks%2C%20which%20then%20serve%20as%20the%0Abasis%20for%20formal%20reasoning%20in%20support%20of%20decision-making.%20The%20interpretable%0Anature%20of%20these%20argumentation%20frameworks%20and%20formal%20reasoning%20means%20that%20any%0Adecision%20made%20by%20ArgLLMs%20may%20be%20explained%20and%20contested.%20We%20evaluate%20ArgLLMs%27%0Aperformance%20experimentally%20in%20comparison%20with%20state-of-the-art%20techniques%2C%20in%0Athe%20context%20of%20the%20decision-making%20task%20of%20claim%20verification.%20We%20also%20define%0Anovel%20properties%20to%20characterise%20contestability%20and%20assess%20ArgLLMs%20formally%20in%0Aterms%20of%20these%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02079v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArgumentative%2520Large%2520Language%2520Models%2520for%2520Explainable%2520and%2520Contestable%250A%2520%2520Claim%2520Verification%26entry.906535625%3DGabriel%2520Freedman%2520and%2520Adam%2520Dejl%2520and%2520Deniz%2520Gorur%2520and%2520Xiang%2520Yin%2520and%2520Antonio%2520Rago%2520and%2520Francesca%2520Toni%26entry.1292438233%3D%2520%2520The%2520profusion%2520of%2520knowledge%2520encoded%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520their%250Aability%2520to%2520apply%2520this%2520knowledge%2520zero-shot%2520in%2520a%2520range%2520of%2520settings%2520makes%2520them%250Apromising%2520candidates%2520for%2520use%2520in%2520decision-making.%2520However%252C%2520they%2520are%2520currently%250Alimited%2520by%2520their%2520inability%2520to%2520provide%2520outputs%2520which%2520can%2520be%2520faithfully%2520explained%250Aand%2520effectively%2520contested%2520to%2520correct%2520mistakes.%2520In%2520this%2520paper%252C%2520we%2520attempt%2520to%250Areconcile%2520these%2520strengths%2520and%2520weaknesses%2520by%2520introducing%2520%255Cemph%257Bargumentative%250ALLMs%2520%2528ArgLLMs%2529%257D%252C%2520a%2520method%2520for%2520augmenting%2520LLMs%2520with%2520argumentative%2520reasoning.%250AConcretely%252C%2520ArgLLMs%2520construct%2520argumentation%2520frameworks%252C%2520which%2520then%2520serve%2520as%2520the%250Abasis%2520for%2520formal%2520reasoning%2520in%2520support%2520of%2520decision-making.%2520The%2520interpretable%250Anature%2520of%2520these%2520argumentation%2520frameworks%2520and%2520formal%2520reasoning%2520means%2520that%2520any%250Adecision%2520made%2520by%2520ArgLLMs%2520may%2520be%2520explained%2520and%2520contested.%2520We%2520evaluate%2520ArgLLMs%2527%250Aperformance%2520experimentally%2520in%2520comparison%2520with%2520state-of-the-art%2520techniques%252C%2520in%250Athe%2520context%2520of%2520the%2520decision-making%2520task%2520of%2520claim%2520verification.%2520We%2520also%2520define%250Anovel%2520properties%2520to%2520characterise%2520contestability%2520and%2520assess%2520ArgLLMs%2520formally%2520in%250Aterms%2520of%2520these%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02079v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Argumentative%20Large%20Language%20Models%20for%20Explainable%20and%20Contestable%0A%20%20Claim%20Verification&entry.906535625=Gabriel%20Freedman%20and%20Adam%20Dejl%20and%20Deniz%20Gorur%20and%20Xiang%20Yin%20and%20Antonio%20Rago%20and%20Francesca%20Toni&entry.1292438233=%20%20The%20profusion%20of%20knowledge%20encoded%20in%20large%20language%20models%20%28LLMs%29%20and%20their%0Aability%20to%20apply%20this%20knowledge%20zero-shot%20in%20a%20range%20of%20settings%20makes%20them%0Apromising%20candidates%20for%20use%20in%20decision-making.%20However%2C%20they%20are%20currently%0Alimited%20by%20their%20inability%20to%20provide%20outputs%20which%20can%20be%20faithfully%20explained%0Aand%20effectively%20contested%20to%20correct%20mistakes.%20In%20this%20paper%2C%20we%20attempt%20to%0Areconcile%20these%20strengths%20and%20weaknesses%20by%20introducing%20%5Cemph%7Bargumentative%0ALLMs%20%28ArgLLMs%29%7D%2C%20a%20method%20for%20augmenting%20LLMs%20with%20argumentative%20reasoning.%0AConcretely%2C%20ArgLLMs%20construct%20argumentation%20frameworks%2C%20which%20then%20serve%20as%20the%0Abasis%20for%20formal%20reasoning%20in%20support%20of%20decision-making.%20The%20interpretable%0Anature%20of%20these%20argumentation%20frameworks%20and%20formal%20reasoning%20means%20that%20any%0Adecision%20made%20by%20ArgLLMs%20may%20be%20explained%20and%20contested.%20We%20evaluate%20ArgLLMs%27%0Aperformance%20experimentally%20in%20comparison%20with%20state-of-the-art%20techniques%2C%20in%0Athe%20context%20of%20the%20decision-making%20task%20of%20claim%20verification.%20We%20also%20define%0Anovel%20properties%20to%20characterise%20contestability%20and%20assess%20ArgLLMs%20formally%20in%0Aterms%20of%20these%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02079v3&entry.124074799=Read"},
{"title": "Variational Stochastic Gradient Descent for Deep Neural Networks", "author": "Haotian Chen and Anna Kuzina and Babak Esmaeili and Jakub M Tomczak", "abstract": "  Current state-of-the-art optimizers are adaptive gradient-based optimization\nmethods such as Adam. Recently, there has been an increasing interest in\nformulating gradient-based optimizers in a probabilistic framework for better\nmodeling the uncertainty of the gradients. Here, we propose to combine both\napproaches, resulting in the Variational Stochastic Gradient Descent (VSGD)\noptimizer. We model gradient updates as a probabilistic model and utilize\nstochastic variational inference (SVI) to derive an efficient and effective\nupdate rule. Further, we show how our VSGD method relates to other adaptive\ngradient-based optimizers like Adam. Lastly, we carry out experiments on two\nimage classification datasets and four deep neural network architectures, where\nwe show that VSGD outperforms Adam and SGD.\n", "link": "http://arxiv.org/abs/2404.06549v2", "date": "2025-04-18", "relevancy": 1.9989, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4875}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variational%20Stochastic%20Gradient%20Descent%20for%20Deep%20Neural%20Networks&body=Title%3A%20Variational%20Stochastic%20Gradient%20Descent%20for%20Deep%20Neural%20Networks%0AAuthor%3A%20Haotian%20Chen%20and%20Anna%20Kuzina%20and%20Babak%20Esmaeili%20and%20Jakub%20M%20Tomczak%0AAbstract%3A%20%20%20Current%20state-of-the-art%20optimizers%20are%20adaptive%20gradient-based%20optimization%0Amethods%20such%20as%20Adam.%20Recently%2C%20there%20has%20been%20an%20increasing%20interest%20in%0Aformulating%20gradient-based%20optimizers%20in%20a%20probabilistic%20framework%20for%20better%0Amodeling%20the%20uncertainty%20of%20the%20gradients.%20Here%2C%20we%20propose%20to%20combine%20both%0Aapproaches%2C%20resulting%20in%20the%20Variational%20Stochastic%20Gradient%20Descent%20%28VSGD%29%0Aoptimizer.%20We%20model%20gradient%20updates%20as%20a%20probabilistic%20model%20and%20utilize%0Astochastic%20variational%20inference%20%28SVI%29%20to%20derive%20an%20efficient%20and%20effective%0Aupdate%20rule.%20Further%2C%20we%20show%20how%20our%20VSGD%20method%20relates%20to%20other%20adaptive%0Agradient-based%20optimizers%20like%20Adam.%20Lastly%2C%20we%20carry%20out%20experiments%20on%20two%0Aimage%20classification%20datasets%20and%20four%20deep%20neural%20network%20architectures%2C%20where%0Awe%20show%20that%20VSGD%20outperforms%20Adam%20and%20SGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariational%2520Stochastic%2520Gradient%2520Descent%2520for%2520Deep%2520Neural%2520Networks%26entry.906535625%3DHaotian%2520Chen%2520and%2520Anna%2520Kuzina%2520and%2520Babak%2520Esmaeili%2520and%2520Jakub%2520M%2520Tomczak%26entry.1292438233%3D%2520%2520Current%2520state-of-the-art%2520optimizers%2520are%2520adaptive%2520gradient-based%2520optimization%250Amethods%2520such%2520as%2520Adam.%2520Recently%252C%2520there%2520has%2520been%2520an%2520increasing%2520interest%2520in%250Aformulating%2520gradient-based%2520optimizers%2520in%2520a%2520probabilistic%2520framework%2520for%2520better%250Amodeling%2520the%2520uncertainty%2520of%2520the%2520gradients.%2520Here%252C%2520we%2520propose%2520to%2520combine%2520both%250Aapproaches%252C%2520resulting%2520in%2520the%2520Variational%2520Stochastic%2520Gradient%2520Descent%2520%2528VSGD%2529%250Aoptimizer.%2520We%2520model%2520gradient%2520updates%2520as%2520a%2520probabilistic%2520model%2520and%2520utilize%250Astochastic%2520variational%2520inference%2520%2528SVI%2529%2520to%2520derive%2520an%2520efficient%2520and%2520effective%250Aupdate%2520rule.%2520Further%252C%2520we%2520show%2520how%2520our%2520VSGD%2520method%2520relates%2520to%2520other%2520adaptive%250Agradient-based%2520optimizers%2520like%2520Adam.%2520Lastly%252C%2520we%2520carry%2520out%2520experiments%2520on%2520two%250Aimage%2520classification%2520datasets%2520and%2520four%2520deep%2520neural%2520network%2520architectures%252C%2520where%250Awe%2520show%2520that%2520VSGD%2520outperforms%2520Adam%2520and%2520SGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variational%20Stochastic%20Gradient%20Descent%20for%20Deep%20Neural%20Networks&entry.906535625=Haotian%20Chen%20and%20Anna%20Kuzina%20and%20Babak%20Esmaeili%20and%20Jakub%20M%20Tomczak&entry.1292438233=%20%20Current%20state-of-the-art%20optimizers%20are%20adaptive%20gradient-based%20optimization%0Amethods%20such%20as%20Adam.%20Recently%2C%20there%20has%20been%20an%20increasing%20interest%20in%0Aformulating%20gradient-based%20optimizers%20in%20a%20probabilistic%20framework%20for%20better%0Amodeling%20the%20uncertainty%20of%20the%20gradients.%20Here%2C%20we%20propose%20to%20combine%20both%0Aapproaches%2C%20resulting%20in%20the%20Variational%20Stochastic%20Gradient%20Descent%20%28VSGD%29%0Aoptimizer.%20We%20model%20gradient%20updates%20as%20a%20probabilistic%20model%20and%20utilize%0Astochastic%20variational%20inference%20%28SVI%29%20to%20derive%20an%20efficient%20and%20effective%0Aupdate%20rule.%20Further%2C%20we%20show%20how%20our%20VSGD%20method%20relates%20to%20other%20adaptive%0Agradient-based%20optimizers%20like%20Adam.%20Lastly%2C%20we%20carry%20out%20experiments%20on%20two%0Aimage%20classification%20datasets%20and%20four%20deep%20neural%20network%20architectures%2C%20where%0Awe%20show%20that%20VSGD%20outperforms%20Adam%20and%20SGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06549v2&entry.124074799=Read"},
{"title": "Granular Ball Twin Support Vector Machine", "author": "A. Quadir and M. Sajid and M. Tanveer", "abstract": "  On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood\nEstimator in Mixture ModelsTwin support vector machine (TSVM) is an emerging\nmachine learning model with versatile applicability in classification and\nregression endeavors. Nevertheless, TSVM confronts noteworthy challenges: $(i)$\nthe imperative demand for matrix inversions presents formidable obstacles to\nits efficiency and applicability on large-scale datasets; $(ii)$ the omission\nof the structural risk minimization (SRM) principle in its primal formulation\nheightens the vulnerability to overfitting risks; and $(iii)$ the TSVM exhibits\na high susceptibility to noise and outliers, and also demonstrates instability\nwhen subjected to resampling. In view of the aforementioned challenges, we\npropose the granular ball twin support vector machine (GBTSVM). GBTSVM takes\ngranular balls, rather than individual data points, as inputs to construct a\nclassifier. These granular balls, characterized by their coarser granularity,\nexhibit robustness to resampling and reduced susceptibility to the impact of\nnoise and outliers. We further propose a novel large-scale granular ball twin\nsupport vector machine (LS-GBTSVM). LS-GBTSVM's optimization formulation\nensures two critical facets: $(i)$ it eliminates the need for matrix\ninversions, streamlining the LS-GBTSVM's computational efficiency, and $(ii)$\nit incorporates the SRM principle through the incorporation of regularization\nterms, effectively addressing the issue of overfitting. The proposed LS-GBTSVM\nexemplifies efficiency, scalability for large datasets, and robustness against\nnoise and outliers. We conduct a comprehensive evaluation of the GBTSVM and\nLS-GBTSVM models on benchmark datasets from UCI, KEEL, and NDC datasets. Our\nexperimental findings and statistical analyses affirm the superior\ngeneralization prowess of the proposed GBTSVM and LS-GBTSVM models.\n", "link": "http://arxiv.org/abs/2410.04774v2", "date": "2025-04-18", "relevancy": 1.9963, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5112}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4929}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Granular%20Ball%20Twin%20Support%20Vector%20Machine&body=Title%3A%20Granular%20Ball%20Twin%20Support%20Vector%20Machine%0AAuthor%3A%20A.%20Quadir%20and%20M.%20Sajid%20and%20M.%20Tanveer%0AAbstract%3A%20%20%20On%20Efficient%20and%20Scalable%20Computation%20of%20the%20Nonparametric%20Maximum%20Likelihood%0AEstimator%20in%20Mixture%20ModelsTwin%20support%20vector%20machine%20%28TSVM%29%20is%20an%20emerging%0Amachine%20learning%20model%20with%20versatile%20applicability%20in%20classification%20and%0Aregression%20endeavors.%20Nevertheless%2C%20TSVM%20confronts%20noteworthy%20challenges%3A%20%24%28i%29%24%0Athe%20imperative%20demand%20for%20matrix%20inversions%20presents%20formidable%20obstacles%20to%0Aits%20efficiency%20and%20applicability%20on%20large-scale%20datasets%3B%20%24%28ii%29%24%20the%20omission%0Aof%20the%20structural%20risk%20minimization%20%28SRM%29%20principle%20in%20its%20primal%20formulation%0Aheightens%20the%20vulnerability%20to%20overfitting%20risks%3B%20and%20%24%28iii%29%24%20the%20TSVM%20exhibits%0Aa%20high%20susceptibility%20to%20noise%20and%20outliers%2C%20and%20also%20demonstrates%20instability%0Awhen%20subjected%20to%20resampling.%20In%20view%20of%20the%20aforementioned%20challenges%2C%20we%0Apropose%20the%20granular%20ball%20twin%20support%20vector%20machine%20%28GBTSVM%29.%20GBTSVM%20takes%0Agranular%20balls%2C%20rather%20than%20individual%20data%20points%2C%20as%20inputs%20to%20construct%20a%0Aclassifier.%20These%20granular%20balls%2C%20characterized%20by%20their%20coarser%20granularity%2C%0Aexhibit%20robustness%20to%20resampling%20and%20reduced%20susceptibility%20to%20the%20impact%20of%0Anoise%20and%20outliers.%20We%20further%20propose%20a%20novel%20large-scale%20granular%20ball%20twin%0Asupport%20vector%20machine%20%28LS-GBTSVM%29.%20LS-GBTSVM%27s%20optimization%20formulation%0Aensures%20two%20critical%20facets%3A%20%24%28i%29%24%20it%20eliminates%20the%20need%20for%20matrix%0Ainversions%2C%20streamlining%20the%20LS-GBTSVM%27s%20computational%20efficiency%2C%20and%20%24%28ii%29%24%0Ait%20incorporates%20the%20SRM%20principle%20through%20the%20incorporation%20of%20regularization%0Aterms%2C%20effectively%20addressing%20the%20issue%20of%20overfitting.%20The%20proposed%20LS-GBTSVM%0Aexemplifies%20efficiency%2C%20scalability%20for%20large%20datasets%2C%20and%20robustness%20against%0Anoise%20and%20outliers.%20We%20conduct%20a%20comprehensive%20evaluation%20of%20the%20GBTSVM%20and%0ALS-GBTSVM%20models%20on%20benchmark%20datasets%20from%20UCI%2C%20KEEL%2C%20and%20NDC%20datasets.%20Our%0Aexperimental%20findings%20and%20statistical%20analyses%20affirm%20the%20superior%0Ageneralization%20prowess%20of%20the%20proposed%20GBTSVM%20and%20LS-GBTSVM%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGranular%2520Ball%2520Twin%2520Support%2520Vector%2520Machine%26entry.906535625%3DA.%2520Quadir%2520and%2520M.%2520Sajid%2520and%2520M.%2520Tanveer%26entry.1292438233%3D%2520%2520On%2520Efficient%2520and%2520Scalable%2520Computation%2520of%2520the%2520Nonparametric%2520Maximum%2520Likelihood%250AEstimator%2520in%2520Mixture%2520ModelsTwin%2520support%2520vector%2520machine%2520%2528TSVM%2529%2520is%2520an%2520emerging%250Amachine%2520learning%2520model%2520with%2520versatile%2520applicability%2520in%2520classification%2520and%250Aregression%2520endeavors.%2520Nevertheless%252C%2520TSVM%2520confronts%2520noteworthy%2520challenges%253A%2520%2524%2528i%2529%2524%250Athe%2520imperative%2520demand%2520for%2520matrix%2520inversions%2520presents%2520formidable%2520obstacles%2520to%250Aits%2520efficiency%2520and%2520applicability%2520on%2520large-scale%2520datasets%253B%2520%2524%2528ii%2529%2524%2520the%2520omission%250Aof%2520the%2520structural%2520risk%2520minimization%2520%2528SRM%2529%2520principle%2520in%2520its%2520primal%2520formulation%250Aheightens%2520the%2520vulnerability%2520to%2520overfitting%2520risks%253B%2520and%2520%2524%2528iii%2529%2524%2520the%2520TSVM%2520exhibits%250Aa%2520high%2520susceptibility%2520to%2520noise%2520and%2520outliers%252C%2520and%2520also%2520demonstrates%2520instability%250Awhen%2520subjected%2520to%2520resampling.%2520In%2520view%2520of%2520the%2520aforementioned%2520challenges%252C%2520we%250Apropose%2520the%2520granular%2520ball%2520twin%2520support%2520vector%2520machine%2520%2528GBTSVM%2529.%2520GBTSVM%2520takes%250Agranular%2520balls%252C%2520rather%2520than%2520individual%2520data%2520points%252C%2520as%2520inputs%2520to%2520construct%2520a%250Aclassifier.%2520These%2520granular%2520balls%252C%2520characterized%2520by%2520their%2520coarser%2520granularity%252C%250Aexhibit%2520robustness%2520to%2520resampling%2520and%2520reduced%2520susceptibility%2520to%2520the%2520impact%2520of%250Anoise%2520and%2520outliers.%2520We%2520further%2520propose%2520a%2520novel%2520large-scale%2520granular%2520ball%2520twin%250Asupport%2520vector%2520machine%2520%2528LS-GBTSVM%2529.%2520LS-GBTSVM%2527s%2520optimization%2520formulation%250Aensures%2520two%2520critical%2520facets%253A%2520%2524%2528i%2529%2524%2520it%2520eliminates%2520the%2520need%2520for%2520matrix%250Ainversions%252C%2520streamlining%2520the%2520LS-GBTSVM%2527s%2520computational%2520efficiency%252C%2520and%2520%2524%2528ii%2529%2524%250Ait%2520incorporates%2520the%2520SRM%2520principle%2520through%2520the%2520incorporation%2520of%2520regularization%250Aterms%252C%2520effectively%2520addressing%2520the%2520issue%2520of%2520overfitting.%2520The%2520proposed%2520LS-GBTSVM%250Aexemplifies%2520efficiency%252C%2520scalability%2520for%2520large%2520datasets%252C%2520and%2520robustness%2520against%250Anoise%2520and%2520outliers.%2520We%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520the%2520GBTSVM%2520and%250ALS-GBTSVM%2520models%2520on%2520benchmark%2520datasets%2520from%2520UCI%252C%2520KEEL%252C%2520and%2520NDC%2520datasets.%2520Our%250Aexperimental%2520findings%2520and%2520statistical%2520analyses%2520affirm%2520the%2520superior%250Ageneralization%2520prowess%2520of%2520the%2520proposed%2520GBTSVM%2520and%2520LS-GBTSVM%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Granular%20Ball%20Twin%20Support%20Vector%20Machine&entry.906535625=A.%20Quadir%20and%20M.%20Sajid%20and%20M.%20Tanveer&entry.1292438233=%20%20On%20Efficient%20and%20Scalable%20Computation%20of%20the%20Nonparametric%20Maximum%20Likelihood%0AEstimator%20in%20Mixture%20ModelsTwin%20support%20vector%20machine%20%28TSVM%29%20is%20an%20emerging%0Amachine%20learning%20model%20with%20versatile%20applicability%20in%20classification%20and%0Aregression%20endeavors.%20Nevertheless%2C%20TSVM%20confronts%20noteworthy%20challenges%3A%20%24%28i%29%24%0Athe%20imperative%20demand%20for%20matrix%20inversions%20presents%20formidable%20obstacles%20to%0Aits%20efficiency%20and%20applicability%20on%20large-scale%20datasets%3B%20%24%28ii%29%24%20the%20omission%0Aof%20the%20structural%20risk%20minimization%20%28SRM%29%20principle%20in%20its%20primal%20formulation%0Aheightens%20the%20vulnerability%20to%20overfitting%20risks%3B%20and%20%24%28iii%29%24%20the%20TSVM%20exhibits%0Aa%20high%20susceptibility%20to%20noise%20and%20outliers%2C%20and%20also%20demonstrates%20instability%0Awhen%20subjected%20to%20resampling.%20In%20view%20of%20the%20aforementioned%20challenges%2C%20we%0Apropose%20the%20granular%20ball%20twin%20support%20vector%20machine%20%28GBTSVM%29.%20GBTSVM%20takes%0Agranular%20balls%2C%20rather%20than%20individual%20data%20points%2C%20as%20inputs%20to%20construct%20a%0Aclassifier.%20These%20granular%20balls%2C%20characterized%20by%20their%20coarser%20granularity%2C%0Aexhibit%20robustness%20to%20resampling%20and%20reduced%20susceptibility%20to%20the%20impact%20of%0Anoise%20and%20outliers.%20We%20further%20propose%20a%20novel%20large-scale%20granular%20ball%20twin%0Asupport%20vector%20machine%20%28LS-GBTSVM%29.%20LS-GBTSVM%27s%20optimization%20formulation%0Aensures%20two%20critical%20facets%3A%20%24%28i%29%24%20it%20eliminates%20the%20need%20for%20matrix%0Ainversions%2C%20streamlining%20the%20LS-GBTSVM%27s%20computational%20efficiency%2C%20and%20%24%28ii%29%24%0Ait%20incorporates%20the%20SRM%20principle%20through%20the%20incorporation%20of%20regularization%0Aterms%2C%20effectively%20addressing%20the%20issue%20of%20overfitting.%20The%20proposed%20LS-GBTSVM%0Aexemplifies%20efficiency%2C%20scalability%20for%20large%20datasets%2C%20and%20robustness%20against%0Anoise%20and%20outliers.%20We%20conduct%20a%20comprehensive%20evaluation%20of%20the%20GBTSVM%20and%0ALS-GBTSVM%20models%20on%20benchmark%20datasets%20from%20UCI%2C%20KEEL%2C%20and%20NDC%20datasets.%20Our%0Aexperimental%20findings%20and%20statistical%20analyses%20affirm%20the%20superior%0Ageneralization%20prowess%20of%20the%20proposed%20GBTSVM%20and%20LS-GBTSVM%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04774v2&entry.124074799=Read"},
{"title": "Fairness and Robustness in Machine Unlearning", "author": "Khoa Tran and Simon S. Woo", "abstract": "  Machine unlearning poses the challenge of ``how to eliminate the influence of\nspecific data from a pretrained model'' in regard to privacy concerns. While\nprior research on approximated unlearning has demonstrated accuracy and\nefficiency in time complexity, we claim that it falls short of achieving exact\nunlearning, and we are the first to focus on fairness and robustness in machine\nunlearning algorithms. Our study presents fairness Conjectures for a\nwell-trained model, based on the variance-bias trade-off characteristic, and\nconsiders their relevance to robustness. Our Conjectures are supported by\nexperiments conducted on the two most widely used model architectures, ResNet\nand ViT, demonstrating the correlation between fairness and robustness:\n\\textit{the higher fairness-gap is, the more the model is sensitive and\nvulnerable}. In addition, our experiments demonstrate the vulnerability of\ncurrent state-of-the-art approximated unlearning algorithms to adversarial\nattacks, where their unlearned models suffer a significant drop in accuracy\ncompared to the exact-unlearned models. We claim that our fairness-gap\nmeasurement and robustness metric should be used to evaluate the unlearning\nalgorithm. Furthermore, we demonstrate that unlearning in the intermediate and\nlast layers is sufficient and cost-effective for time and memory complexity.\n", "link": "http://arxiv.org/abs/2504.13610v1", "date": "2025-04-18", "relevancy": 1.984, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5144}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4977}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness%20and%20Robustness%20in%20Machine%20Unlearning&body=Title%3A%20Fairness%20and%20Robustness%20in%20Machine%20Unlearning%0AAuthor%3A%20Khoa%20Tran%20and%20Simon%20S.%20Woo%0AAbstract%3A%20%20%20Machine%20unlearning%20poses%20the%20challenge%20of%20%60%60how%20to%20eliminate%20the%20influence%20of%0Aspecific%20data%20from%20a%20pretrained%20model%27%27%20in%20regard%20to%20privacy%20concerns.%20While%0Aprior%20research%20on%20approximated%20unlearning%20has%20demonstrated%20accuracy%20and%0Aefficiency%20in%20time%20complexity%2C%20we%20claim%20that%20it%20falls%20short%20of%20achieving%20exact%0Aunlearning%2C%20and%20we%20are%20the%20first%20to%20focus%20on%20fairness%20and%20robustness%20in%20machine%0Aunlearning%20algorithms.%20Our%20study%20presents%20fairness%20Conjectures%20for%20a%0Awell-trained%20model%2C%20based%20on%20the%20variance-bias%20trade-off%20characteristic%2C%20and%0Aconsiders%20their%20relevance%20to%20robustness.%20Our%20Conjectures%20are%20supported%20by%0Aexperiments%20conducted%20on%20the%20two%20most%20widely%20used%20model%20architectures%2C%20ResNet%0Aand%20ViT%2C%20demonstrating%20the%20correlation%20between%20fairness%20and%20robustness%3A%0A%5Ctextit%7Bthe%20higher%20fairness-gap%20is%2C%20the%20more%20the%20model%20is%20sensitive%20and%0Avulnerable%7D.%20In%20addition%2C%20our%20experiments%20demonstrate%20the%20vulnerability%20of%0Acurrent%20state-of-the-art%20approximated%20unlearning%20algorithms%20to%20adversarial%0Aattacks%2C%20where%20their%20unlearned%20models%20suffer%20a%20significant%20drop%20in%20accuracy%0Acompared%20to%20the%20exact-unlearned%20models.%20We%20claim%20that%20our%20fairness-gap%0Ameasurement%20and%20robustness%20metric%20should%20be%20used%20to%20evaluate%20the%20unlearning%0Aalgorithm.%20Furthermore%2C%20we%20demonstrate%20that%20unlearning%20in%20the%20intermediate%20and%0Alast%20layers%20is%20sufficient%20and%20cost-effective%20for%20time%20and%20memory%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness%2520and%2520Robustness%2520in%2520Machine%2520Unlearning%26entry.906535625%3DKhoa%2520Tran%2520and%2520Simon%2520S.%2520Woo%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520poses%2520the%2520challenge%2520of%2520%2560%2560how%2520to%2520eliminate%2520the%2520influence%2520of%250Aspecific%2520data%2520from%2520a%2520pretrained%2520model%2527%2527%2520in%2520regard%2520to%2520privacy%2520concerns.%2520While%250Aprior%2520research%2520on%2520approximated%2520unlearning%2520has%2520demonstrated%2520accuracy%2520and%250Aefficiency%2520in%2520time%2520complexity%252C%2520we%2520claim%2520that%2520it%2520falls%2520short%2520of%2520achieving%2520exact%250Aunlearning%252C%2520and%2520we%2520are%2520the%2520first%2520to%2520focus%2520on%2520fairness%2520and%2520robustness%2520in%2520machine%250Aunlearning%2520algorithms.%2520Our%2520study%2520presents%2520fairness%2520Conjectures%2520for%2520a%250Awell-trained%2520model%252C%2520based%2520on%2520the%2520variance-bias%2520trade-off%2520characteristic%252C%2520and%250Aconsiders%2520their%2520relevance%2520to%2520robustness.%2520Our%2520Conjectures%2520are%2520supported%2520by%250Aexperiments%2520conducted%2520on%2520the%2520two%2520most%2520widely%2520used%2520model%2520architectures%252C%2520ResNet%250Aand%2520ViT%252C%2520demonstrating%2520the%2520correlation%2520between%2520fairness%2520and%2520robustness%253A%250A%255Ctextit%257Bthe%2520higher%2520fairness-gap%2520is%252C%2520the%2520more%2520the%2520model%2520is%2520sensitive%2520and%250Avulnerable%257D.%2520In%2520addition%252C%2520our%2520experiments%2520demonstrate%2520the%2520vulnerability%2520of%250Acurrent%2520state-of-the-art%2520approximated%2520unlearning%2520algorithms%2520to%2520adversarial%250Aattacks%252C%2520where%2520their%2520unlearned%2520models%2520suffer%2520a%2520significant%2520drop%2520in%2520accuracy%250Acompared%2520to%2520the%2520exact-unlearned%2520models.%2520We%2520claim%2520that%2520our%2520fairness-gap%250Ameasurement%2520and%2520robustness%2520metric%2520should%2520be%2520used%2520to%2520evaluate%2520the%2520unlearning%250Aalgorithm.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520unlearning%2520in%2520the%2520intermediate%2520and%250Alast%2520layers%2520is%2520sufficient%2520and%2520cost-effective%2520for%2520time%2520and%2520memory%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness%20and%20Robustness%20in%20Machine%20Unlearning&entry.906535625=Khoa%20Tran%20and%20Simon%20S.%20Woo&entry.1292438233=%20%20Machine%20unlearning%20poses%20the%20challenge%20of%20%60%60how%20to%20eliminate%20the%20influence%20of%0Aspecific%20data%20from%20a%20pretrained%20model%27%27%20in%20regard%20to%20privacy%20concerns.%20While%0Aprior%20research%20on%20approximated%20unlearning%20has%20demonstrated%20accuracy%20and%0Aefficiency%20in%20time%20complexity%2C%20we%20claim%20that%20it%20falls%20short%20of%20achieving%20exact%0Aunlearning%2C%20and%20we%20are%20the%20first%20to%20focus%20on%20fairness%20and%20robustness%20in%20machine%0Aunlearning%20algorithms.%20Our%20study%20presents%20fairness%20Conjectures%20for%20a%0Awell-trained%20model%2C%20based%20on%20the%20variance-bias%20trade-off%20characteristic%2C%20and%0Aconsiders%20their%20relevance%20to%20robustness.%20Our%20Conjectures%20are%20supported%20by%0Aexperiments%20conducted%20on%20the%20two%20most%20widely%20used%20model%20architectures%2C%20ResNet%0Aand%20ViT%2C%20demonstrating%20the%20correlation%20between%20fairness%20and%20robustness%3A%0A%5Ctextit%7Bthe%20higher%20fairness-gap%20is%2C%20the%20more%20the%20model%20is%20sensitive%20and%0Avulnerable%7D.%20In%20addition%2C%20our%20experiments%20demonstrate%20the%20vulnerability%20of%0Acurrent%20state-of-the-art%20approximated%20unlearning%20algorithms%20to%20adversarial%0Aattacks%2C%20where%20their%20unlearned%20models%20suffer%20a%20significant%20drop%20in%20accuracy%0Acompared%20to%20the%20exact-unlearned%20models.%20We%20claim%20that%20our%20fairness-gap%0Ameasurement%20and%20robustness%20metric%20should%20be%20used%20to%20evaluate%20the%20unlearning%0Aalgorithm.%20Furthermore%2C%20we%20demonstrate%20that%20unlearning%20in%20the%20intermediate%20and%0Alast%20layers%20is%20sufficient%20and%20cost-effective%20for%20time%20and%20memory%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13610v1&entry.124074799=Read"},
{"title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs", "author": "Tamim Al Mahmud and Najeeb Jebreel and Josep Domingo-Ferrer and David Sanchez", "abstract": "  Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information.\n", "link": "http://arxiv.org/abs/2504.13774v1", "date": "2025-04-18", "relevancy": 1.9739, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5516}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4891}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP2Unlearning%3A%20An%20Efficient%20and%20Guaranteed%20Unlearning%20Framework%20for%20LLMs&body=Title%3A%20DP2Unlearning%3A%20An%20Efficient%20and%20Guaranteed%20Unlearning%20Framework%20for%20LLMs%0AAuthor%3A%20Tamim%20Al%20Mahmud%20and%20Najeeb%20Jebreel%20and%20Josep%20Domingo-Ferrer%20and%20David%20Sanchez%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20revolutionized%20language%20processing%0Atasks%20but%20have%20also%20brought%20ethical%20and%20legal%20issues.%20LLMs%20have%20a%20tendency%20to%0Amemorize%20potentially%20private%20or%20copyrighted%20information%20present%20in%20the%20training%0Adata%2C%20which%20might%20then%20be%20delivered%20to%20end%20users%20at%20inference%20time.%20When%20this%0Ahappens%2C%20a%20naive%20solution%20is%20to%20retrain%20the%20model%20from%20scratch%20after%20excluding%0Athe%20undesired%20data.%20Although%20this%20guarantees%20that%20the%20target%20data%20have%20been%0Aforgotten%2C%20it%20is%20also%20prohibitively%20expensive%20for%20LLMs.%20Approximate%20unlearning%0Aoffers%20a%20more%20efficient%20alternative%2C%20as%20it%20consists%20of%20ex%20post%20modifications%20of%0Athe%20trained%20model%20itself%20to%20prevent%20undesirable%20results%2C%20but%20it%20lacks%0Aforgetting%20guarantees%20because%20it%20relies%20solely%20on%20empirical%20evidence.%20In%20this%0Awork%2C%20we%20present%20DP2Unlearning%2C%20a%20novel%20LLM%20unlearning%20framework%20that%20offers%0Aformal%20forgetting%20guarantees%20at%20a%20significantly%20lower%20cost%20than%20retraining%20from%0Ascratch%20on%20the%20data%20to%20be%20retained.%20DP2Unlearning%20involves%20training%20LLMs%20on%0Atextual%20data%20protected%20using%20%7B%5Cepsilon%7D-differential%20privacy%20%28DP%29%2C%20which%20later%0Aenables%20efficient%20unlearning%20with%20the%20guarantees%20against%20disclosure%20associated%0Awith%20the%20chosen%20%7B%5Cepsilon%7D.%20Our%20experiments%20demonstrate%20that%20DP2Unlearning%0Aachieves%20similar%20model%20performance%20post-unlearning%2C%20compared%20to%20an%20LLM%0Aretraining%20from%20scratch%20on%20retained%20data%20--%20the%20gold%20standard%20exact%20unlearning%0A--%20but%20at%20approximately%20half%20the%20unlearning%20cost.%20In%20addition%2C%20with%20a%0Areasonable%20computational%20cost%2C%20it%20outperforms%20approximate%20unlearning%20methods%20at%0Aboth%20preserving%20the%20utility%20of%20the%20model%20post-unlearning%20and%20effectively%0Aforgetting%20the%20targeted%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP2Unlearning%253A%2520An%2520Efficient%2520and%2520Guaranteed%2520Unlearning%2520Framework%2520for%2520LLMs%26entry.906535625%3DTamim%2520Al%2520Mahmud%2520and%2520Najeeb%2520Jebreel%2520and%2520Josep%2520Domingo-Ferrer%2520and%2520David%2520Sanchez%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520revolutionized%2520language%2520processing%250Atasks%2520but%2520have%2520also%2520brought%2520ethical%2520and%2520legal%2520issues.%2520LLMs%2520have%2520a%2520tendency%2520to%250Amemorize%2520potentially%2520private%2520or%2520copyrighted%2520information%2520present%2520in%2520the%2520training%250Adata%252C%2520which%2520might%2520then%2520be%2520delivered%2520to%2520end%2520users%2520at%2520inference%2520time.%2520When%2520this%250Ahappens%252C%2520a%2520naive%2520solution%2520is%2520to%2520retrain%2520the%2520model%2520from%2520scratch%2520after%2520excluding%250Athe%2520undesired%2520data.%2520Although%2520this%2520guarantees%2520that%2520the%2520target%2520data%2520have%2520been%250Aforgotten%252C%2520it%2520is%2520also%2520prohibitively%2520expensive%2520for%2520LLMs.%2520Approximate%2520unlearning%250Aoffers%2520a%2520more%2520efficient%2520alternative%252C%2520as%2520it%2520consists%2520of%2520ex%2520post%2520modifications%2520of%250Athe%2520trained%2520model%2520itself%2520to%2520prevent%2520undesirable%2520results%252C%2520but%2520it%2520lacks%250Aforgetting%2520guarantees%2520because%2520it%2520relies%2520solely%2520on%2520empirical%2520evidence.%2520In%2520this%250Awork%252C%2520we%2520present%2520DP2Unlearning%252C%2520a%2520novel%2520LLM%2520unlearning%2520framework%2520that%2520offers%250Aformal%2520forgetting%2520guarantees%2520at%2520a%2520significantly%2520lower%2520cost%2520than%2520retraining%2520from%250Ascratch%2520on%2520the%2520data%2520to%2520be%2520retained.%2520DP2Unlearning%2520involves%2520training%2520LLMs%2520on%250Atextual%2520data%2520protected%2520using%2520%257B%255Cepsilon%257D-differential%2520privacy%2520%2528DP%2529%252C%2520which%2520later%250Aenables%2520efficient%2520unlearning%2520with%2520the%2520guarantees%2520against%2520disclosure%2520associated%250Awith%2520the%2520chosen%2520%257B%255Cepsilon%257D.%2520Our%2520experiments%2520demonstrate%2520that%2520DP2Unlearning%250Aachieves%2520similar%2520model%2520performance%2520post-unlearning%252C%2520compared%2520to%2520an%2520LLM%250Aretraining%2520from%2520scratch%2520on%2520retained%2520data%2520--%2520the%2520gold%2520standard%2520exact%2520unlearning%250A--%2520but%2520at%2520approximately%2520half%2520the%2520unlearning%2520cost.%2520In%2520addition%252C%2520with%2520a%250Areasonable%2520computational%2520cost%252C%2520it%2520outperforms%2520approximate%2520unlearning%2520methods%2520at%250Aboth%2520preserving%2520the%2520utility%2520of%2520the%2520model%2520post-unlearning%2520and%2520effectively%250Aforgetting%2520the%2520targeted%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP2Unlearning%3A%20An%20Efficient%20and%20Guaranteed%20Unlearning%20Framework%20for%20LLMs&entry.906535625=Tamim%20Al%20Mahmud%20and%20Najeeb%20Jebreel%20and%20Josep%20Domingo-Ferrer%20and%20David%20Sanchez&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20revolutionized%20language%20processing%0Atasks%20but%20have%20also%20brought%20ethical%20and%20legal%20issues.%20LLMs%20have%20a%20tendency%20to%0Amemorize%20potentially%20private%20or%20copyrighted%20information%20present%20in%20the%20training%0Adata%2C%20which%20might%20then%20be%20delivered%20to%20end%20users%20at%20inference%20time.%20When%20this%0Ahappens%2C%20a%20naive%20solution%20is%20to%20retrain%20the%20model%20from%20scratch%20after%20excluding%0Athe%20undesired%20data.%20Although%20this%20guarantees%20that%20the%20target%20data%20have%20been%0Aforgotten%2C%20it%20is%20also%20prohibitively%20expensive%20for%20LLMs.%20Approximate%20unlearning%0Aoffers%20a%20more%20efficient%20alternative%2C%20as%20it%20consists%20of%20ex%20post%20modifications%20of%0Athe%20trained%20model%20itself%20to%20prevent%20undesirable%20results%2C%20but%20it%20lacks%0Aforgetting%20guarantees%20because%20it%20relies%20solely%20on%20empirical%20evidence.%20In%20this%0Awork%2C%20we%20present%20DP2Unlearning%2C%20a%20novel%20LLM%20unlearning%20framework%20that%20offers%0Aformal%20forgetting%20guarantees%20at%20a%20significantly%20lower%20cost%20than%20retraining%20from%0Ascratch%20on%20the%20data%20to%20be%20retained.%20DP2Unlearning%20involves%20training%20LLMs%20on%0Atextual%20data%20protected%20using%20%7B%5Cepsilon%7D-differential%20privacy%20%28DP%29%2C%20which%20later%0Aenables%20efficient%20unlearning%20with%20the%20guarantees%20against%20disclosure%20associated%0Awith%20the%20chosen%20%7B%5Cepsilon%7D.%20Our%20experiments%20demonstrate%20that%20DP2Unlearning%0Aachieves%20similar%20model%20performance%20post-unlearning%2C%20compared%20to%20an%20LLM%0Aretraining%20from%20scratch%20on%20retained%20data%20--%20the%20gold%20standard%20exact%20unlearning%0A--%20but%20at%20approximately%20half%20the%20unlearning%20cost.%20In%20addition%2C%20with%20a%0Areasonable%20computational%20cost%2C%20it%20outperforms%20approximate%20unlearning%20methods%20at%0Aboth%20preserving%20the%20utility%20of%20the%20model%20post-unlearning%20and%20effectively%0Aforgetting%20the%20targeted%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13774v1&entry.124074799=Read"},
{"title": "RiboGen: RNA Sequence and Structure Co-Generation with Equivariant\n  MultiFlow", "author": "Dana Rubin and Allan dos Santos Costa and Manvitha Ponnapati and Joseph Jacobson", "abstract": "  Ribonucleic acid (RNA) plays fundamental roles in biological systems, from\ncarrying genetic information to performing enzymatic function. Understanding\nand designing RNA can enable novel therapeutic application and biotechnological\ninnovation. To enhance RNA design, in this paper we introduce RiboGen, the\nfirst deep learning model to simultaneously generate RNA sequence and all-atom\n3D structure. RiboGen leverages the standard Flow Matching with Discrete Flow\nMatching in a multimodal data representation. RiboGen is based on Euclidean\nEquivariant neural networks for efficiently processing and learning\nthree-dimensional geometry. Our experiments show that RiboGen can efficiently\ngenerate chemically plausible and self-consistent RNA samples, suggesting that\nco-generation of sequence and structure is a competitive approach for modeling\nRNA.\n", "link": "http://arxiv.org/abs/2503.02058v4", "date": "2025-04-18", "relevancy": 1.9722, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5121}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4925}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RiboGen%3A%20RNA%20Sequence%20and%20Structure%20Co-Generation%20with%20Equivariant%0A%20%20MultiFlow&body=Title%3A%20RiboGen%3A%20RNA%20Sequence%20and%20Structure%20Co-Generation%20with%20Equivariant%0A%20%20MultiFlow%0AAuthor%3A%20Dana%20Rubin%20and%20Allan%20dos%20Santos%20Costa%20and%20Manvitha%20Ponnapati%20and%20Joseph%20Jacobson%0AAbstract%3A%20%20%20Ribonucleic%20acid%20%28RNA%29%20plays%20fundamental%20roles%20in%20biological%20systems%2C%20from%0Acarrying%20genetic%20information%20to%20performing%20enzymatic%20function.%20Understanding%0Aand%20designing%20RNA%20can%20enable%20novel%20therapeutic%20application%20and%20biotechnological%0Ainnovation.%20To%20enhance%20RNA%20design%2C%20in%20this%20paper%20we%20introduce%20RiboGen%2C%20the%0Afirst%20deep%20learning%20model%20to%20simultaneously%20generate%20RNA%20sequence%20and%20all-atom%0A3D%20structure.%20RiboGen%20leverages%20the%20standard%20Flow%20Matching%20with%20Discrete%20Flow%0AMatching%20in%20a%20multimodal%20data%20representation.%20RiboGen%20is%20based%20on%20Euclidean%0AEquivariant%20neural%20networks%20for%20efficiently%20processing%20and%20learning%0Athree-dimensional%20geometry.%20Our%20experiments%20show%20that%20RiboGen%20can%20efficiently%0Agenerate%20chemically%20plausible%20and%20self-consistent%20RNA%20samples%2C%20suggesting%20that%0Aco-generation%20of%20sequence%20and%20structure%20is%20a%20competitive%20approach%20for%20modeling%0ARNA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02058v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiboGen%253A%2520RNA%2520Sequence%2520and%2520Structure%2520Co-Generation%2520with%2520Equivariant%250A%2520%2520MultiFlow%26entry.906535625%3DDana%2520Rubin%2520and%2520Allan%2520dos%2520Santos%2520Costa%2520and%2520Manvitha%2520Ponnapati%2520and%2520Joseph%2520Jacobson%26entry.1292438233%3D%2520%2520Ribonucleic%2520acid%2520%2528RNA%2529%2520plays%2520fundamental%2520roles%2520in%2520biological%2520systems%252C%2520from%250Acarrying%2520genetic%2520information%2520to%2520performing%2520enzymatic%2520function.%2520Understanding%250Aand%2520designing%2520RNA%2520can%2520enable%2520novel%2520therapeutic%2520application%2520and%2520biotechnological%250Ainnovation.%2520To%2520enhance%2520RNA%2520design%252C%2520in%2520this%2520paper%2520we%2520introduce%2520RiboGen%252C%2520the%250Afirst%2520deep%2520learning%2520model%2520to%2520simultaneously%2520generate%2520RNA%2520sequence%2520and%2520all-atom%250A3D%2520structure.%2520RiboGen%2520leverages%2520the%2520standard%2520Flow%2520Matching%2520with%2520Discrete%2520Flow%250AMatching%2520in%2520a%2520multimodal%2520data%2520representation.%2520RiboGen%2520is%2520based%2520on%2520Euclidean%250AEquivariant%2520neural%2520networks%2520for%2520efficiently%2520processing%2520and%2520learning%250Athree-dimensional%2520geometry.%2520Our%2520experiments%2520show%2520that%2520RiboGen%2520can%2520efficiently%250Agenerate%2520chemically%2520plausible%2520and%2520self-consistent%2520RNA%2520samples%252C%2520suggesting%2520that%250Aco-generation%2520of%2520sequence%2520and%2520structure%2520is%2520a%2520competitive%2520approach%2520for%2520modeling%250ARNA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02058v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RiboGen%3A%20RNA%20Sequence%20and%20Structure%20Co-Generation%20with%20Equivariant%0A%20%20MultiFlow&entry.906535625=Dana%20Rubin%20and%20Allan%20dos%20Santos%20Costa%20and%20Manvitha%20Ponnapati%20and%20Joseph%20Jacobson&entry.1292438233=%20%20Ribonucleic%20acid%20%28RNA%29%20plays%20fundamental%20roles%20in%20biological%20systems%2C%20from%0Acarrying%20genetic%20information%20to%20performing%20enzymatic%20function.%20Understanding%0Aand%20designing%20RNA%20can%20enable%20novel%20therapeutic%20application%20and%20biotechnological%0Ainnovation.%20To%20enhance%20RNA%20design%2C%20in%20this%20paper%20we%20introduce%20RiboGen%2C%20the%0Afirst%20deep%20learning%20model%20to%20simultaneously%20generate%20RNA%20sequence%20and%20all-atom%0A3D%20structure.%20RiboGen%20leverages%20the%20standard%20Flow%20Matching%20with%20Discrete%20Flow%0AMatching%20in%20a%20multimodal%20data%20representation.%20RiboGen%20is%20based%20on%20Euclidean%0AEquivariant%20neural%20networks%20for%20efficiently%20processing%20and%20learning%0Athree-dimensional%20geometry.%20Our%20experiments%20show%20that%20RiboGen%20can%20efficiently%0Agenerate%20chemically%20plausible%20and%20self-consistent%20RNA%20samples%2C%20suggesting%20that%0Aco-generation%20of%20sequence%20and%20structure%20is%20a%20competitive%20approach%20for%20modeling%0ARNA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02058v4&entry.124074799=Read"},
{"title": "When is Task Vector Provably Effective for Model Editing? A\n  Generalization Analysis of Nonlinear Transformers", "author": "Hongkang Li and Yihua Zhang and Shuai Zhang and Meng Wang and Sijia Liu and Pin-Yu Chen", "abstract": "  Task arithmetic refers to editing the pre-trained model by adding a weighted\nsum of task vectors, each of which is the weight update from the pre-trained\nmodel to fine-tuned models for certain tasks. This approach recently gained\nattention as a computationally efficient inference method for model editing,\ne.g., multi-task learning, forgetting, and out-of-domain generalization\ncapabilities. However, the theoretical understanding of why task vectors can\nexecute various conceptual operations remains limited, due to the highly\nnon-convexity of training Transformer-based models. To the best of our\nknowledge, this paper provides the first theoretical characterization of the\ngeneralization guarantees of task vector methods on nonlinear Transformers. We\nconsider a conceptual learning setting, where each task is a binary\nclassification problem based on a discriminative pattern. We theoretically\nprove the effectiveness of task addition in simultaneously learning a set of\nirrelevant or aligned tasks, as well as the success of task negation in\nunlearning one task from irrelevant or contradictory tasks. Moreover, we prove\nthe proper selection of linear coefficients for task arithmetic to achieve\nguaranteed generalization to out-of-domain tasks. All of our theoretical\nresults hold for both dense-weight parameters and their low-rank\napproximations. Although established in a conceptual setting, our theoretical\nfindings were validated on a practical machine unlearning task using the large\nlanguage model Phi-1.5 (1.3B).\n", "link": "http://arxiv.org/abs/2504.10957v2", "date": "2025-04-18", "relevancy": 1.9707, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20is%20Task%20Vector%20Provably%20Effective%20for%20Model%20Editing%3F%20A%0A%20%20Generalization%20Analysis%20of%20Nonlinear%20Transformers&body=Title%3A%20When%20is%20Task%20Vector%20Provably%20Effective%20for%20Model%20Editing%3F%20A%0A%20%20Generalization%20Analysis%20of%20Nonlinear%20Transformers%0AAuthor%3A%20Hongkang%20Li%20and%20Yihua%20Zhang%20and%20Shuai%20Zhang%20and%20Meng%20Wang%20and%20Sijia%20Liu%20and%20Pin-Yu%20Chen%0AAbstract%3A%20%20%20Task%20arithmetic%20refers%20to%20editing%20the%20pre-trained%20model%20by%20adding%20a%20weighted%0Asum%20of%20task%20vectors%2C%20each%20of%20which%20is%20the%20weight%20update%20from%20the%20pre-trained%0Amodel%20to%20fine-tuned%20models%20for%20certain%20tasks.%20This%20approach%20recently%20gained%0Aattention%20as%20a%20computationally%20efficient%20inference%20method%20for%20model%20editing%2C%0Ae.g.%2C%20multi-task%20learning%2C%20forgetting%2C%20and%20out-of-domain%20generalization%0Acapabilities.%20However%2C%20the%20theoretical%20understanding%20of%20why%20task%20vectors%20can%0Aexecute%20various%20conceptual%20operations%20remains%20limited%2C%20due%20to%20the%20highly%0Anon-convexity%20of%20training%20Transformer-based%20models.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20paper%20provides%20the%20first%20theoretical%20characterization%20of%20the%0Ageneralization%20guarantees%20of%20task%20vector%20methods%20on%20nonlinear%20Transformers.%20We%0Aconsider%20a%20conceptual%20learning%20setting%2C%20where%20each%20task%20is%20a%20binary%0Aclassification%20problem%20based%20on%20a%20discriminative%20pattern.%20We%20theoretically%0Aprove%20the%20effectiveness%20of%20task%20addition%20in%20simultaneously%20learning%20a%20set%20of%0Airrelevant%20or%20aligned%20tasks%2C%20as%20well%20as%20the%20success%20of%20task%20negation%20in%0Aunlearning%20one%20task%20from%20irrelevant%20or%20contradictory%20tasks.%20Moreover%2C%20we%20prove%0Athe%20proper%20selection%20of%20linear%20coefficients%20for%20task%20arithmetic%20to%20achieve%0Aguaranteed%20generalization%20to%20out-of-domain%20tasks.%20All%20of%20our%20theoretical%0Aresults%20hold%20for%20both%20dense-weight%20parameters%20and%20their%20low-rank%0Aapproximations.%20Although%20established%20in%20a%20conceptual%20setting%2C%20our%20theoretical%0Afindings%20were%20validated%20on%20a%20practical%20machine%20unlearning%20task%20using%20the%20large%0Alanguage%20model%20Phi-1.5%20%281.3B%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10957v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520is%2520Task%2520Vector%2520Provably%2520Effective%2520for%2520Model%2520Editing%253F%2520A%250A%2520%2520Generalization%2520Analysis%2520of%2520Nonlinear%2520Transformers%26entry.906535625%3DHongkang%2520Li%2520and%2520Yihua%2520Zhang%2520and%2520Shuai%2520Zhang%2520and%2520Meng%2520Wang%2520and%2520Sijia%2520Liu%2520and%2520Pin-Yu%2520Chen%26entry.1292438233%3D%2520%2520Task%2520arithmetic%2520refers%2520to%2520editing%2520the%2520pre-trained%2520model%2520by%2520adding%2520a%2520weighted%250Asum%2520of%2520task%2520vectors%252C%2520each%2520of%2520which%2520is%2520the%2520weight%2520update%2520from%2520the%2520pre-trained%250Amodel%2520to%2520fine-tuned%2520models%2520for%2520certain%2520tasks.%2520This%2520approach%2520recently%2520gained%250Aattention%2520as%2520a%2520computationally%2520efficient%2520inference%2520method%2520for%2520model%2520editing%252C%250Ae.g.%252C%2520multi-task%2520learning%252C%2520forgetting%252C%2520and%2520out-of-domain%2520generalization%250Acapabilities.%2520However%252C%2520the%2520theoretical%2520understanding%2520of%2520why%2520task%2520vectors%2520can%250Aexecute%2520various%2520conceptual%2520operations%2520remains%2520limited%252C%2520due%2520to%2520the%2520highly%250Anon-convexity%2520of%2520training%2520Transformer-based%2520models.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520paper%2520provides%2520the%2520first%2520theoretical%2520characterization%2520of%2520the%250Ageneralization%2520guarantees%2520of%2520task%2520vector%2520methods%2520on%2520nonlinear%2520Transformers.%2520We%250Aconsider%2520a%2520conceptual%2520learning%2520setting%252C%2520where%2520each%2520task%2520is%2520a%2520binary%250Aclassification%2520problem%2520based%2520on%2520a%2520discriminative%2520pattern.%2520We%2520theoretically%250Aprove%2520the%2520effectiveness%2520of%2520task%2520addition%2520in%2520simultaneously%2520learning%2520a%2520set%2520of%250Airrelevant%2520or%2520aligned%2520tasks%252C%2520as%2520well%2520as%2520the%2520success%2520of%2520task%2520negation%2520in%250Aunlearning%2520one%2520task%2520from%2520irrelevant%2520or%2520contradictory%2520tasks.%2520Moreover%252C%2520we%2520prove%250Athe%2520proper%2520selection%2520of%2520linear%2520coefficients%2520for%2520task%2520arithmetic%2520to%2520achieve%250Aguaranteed%2520generalization%2520to%2520out-of-domain%2520tasks.%2520All%2520of%2520our%2520theoretical%250Aresults%2520hold%2520for%2520both%2520dense-weight%2520parameters%2520and%2520their%2520low-rank%250Aapproximations.%2520Although%2520established%2520in%2520a%2520conceptual%2520setting%252C%2520our%2520theoretical%250Afindings%2520were%2520validated%2520on%2520a%2520practical%2520machine%2520unlearning%2520task%2520using%2520the%2520large%250Alanguage%2520model%2520Phi-1.5%2520%25281.3B%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10957v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20is%20Task%20Vector%20Provably%20Effective%20for%20Model%20Editing%3F%20A%0A%20%20Generalization%20Analysis%20of%20Nonlinear%20Transformers&entry.906535625=Hongkang%20Li%20and%20Yihua%20Zhang%20and%20Shuai%20Zhang%20and%20Meng%20Wang%20and%20Sijia%20Liu%20and%20Pin-Yu%20Chen&entry.1292438233=%20%20Task%20arithmetic%20refers%20to%20editing%20the%20pre-trained%20model%20by%20adding%20a%20weighted%0Asum%20of%20task%20vectors%2C%20each%20of%20which%20is%20the%20weight%20update%20from%20the%20pre-trained%0Amodel%20to%20fine-tuned%20models%20for%20certain%20tasks.%20This%20approach%20recently%20gained%0Aattention%20as%20a%20computationally%20efficient%20inference%20method%20for%20model%20editing%2C%0Ae.g.%2C%20multi-task%20learning%2C%20forgetting%2C%20and%20out-of-domain%20generalization%0Acapabilities.%20However%2C%20the%20theoretical%20understanding%20of%20why%20task%20vectors%20can%0Aexecute%20various%20conceptual%20operations%20remains%20limited%2C%20due%20to%20the%20highly%0Anon-convexity%20of%20training%20Transformer-based%20models.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20paper%20provides%20the%20first%20theoretical%20characterization%20of%20the%0Ageneralization%20guarantees%20of%20task%20vector%20methods%20on%20nonlinear%20Transformers.%20We%0Aconsider%20a%20conceptual%20learning%20setting%2C%20where%20each%20task%20is%20a%20binary%0Aclassification%20problem%20based%20on%20a%20discriminative%20pattern.%20We%20theoretically%0Aprove%20the%20effectiveness%20of%20task%20addition%20in%20simultaneously%20learning%20a%20set%20of%0Airrelevant%20or%20aligned%20tasks%2C%20as%20well%20as%20the%20success%20of%20task%20negation%20in%0Aunlearning%20one%20task%20from%20irrelevant%20or%20contradictory%20tasks.%20Moreover%2C%20we%20prove%0Athe%20proper%20selection%20of%20linear%20coefficients%20for%20task%20arithmetic%20to%20achieve%0Aguaranteed%20generalization%20to%20out-of-domain%20tasks.%20All%20of%20our%20theoretical%0Aresults%20hold%20for%20both%20dense-weight%20parameters%20and%20their%20low-rank%0Aapproximations.%20Although%20established%20in%20a%20conceptual%20setting%2C%20our%20theoretical%0Afindings%20were%20validated%20on%20a%20practical%20machine%20unlearning%20task%20using%20the%20large%0Alanguage%20model%20Phi-1.5%20%281.3B%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10957v2&entry.124074799=Read"},
{"title": "Force and Speed in a Soft Stewart Platform", "author": "Jake Ketchum and James Avtges and Millicent Schlafly and Helena Young and Taekyoung Kim and Ryan L. Truby and Todd D. Murphey", "abstract": "  Many soft robots struggle to produce dynamic motions with fast, large\ndisplacements. We develop a parallel 6 degree-of-freedom (DoF) Stewart-Gough\nmechanism using Handed Shearing Auxetic (HSA) actuators. By using soft\nactuators, we are able to use one third as many mechatronic components as a\nrigid Stewart platform, while retaining a working payload of 2kg and an\nopen-loop bandwidth greater than 16Hz. We show that the platform is capable of\nboth precise tracing and dynamic disturbance rejection when controlling a ball\nand sliding puck using a Proportional Integral Derivative (PID) controller. We\ndevelop a machine-learning-based kinematics model and demonstrate a functional\nworkspace of roughly 10cm in each translation direction and 28 degrees in each\norientation. This 6DoF device has many of the characteristics associated with\nrigid components - power, speed, and total workspace - while capturing the\nadvantages of soft mechanisms.\n", "link": "http://arxiv.org/abs/2504.13127v2", "date": "2025-04-18", "relevancy": 1.9655, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5111}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4939}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Force%20and%20Speed%20in%20a%20Soft%20Stewart%20Platform&body=Title%3A%20Force%20and%20Speed%20in%20a%20Soft%20Stewart%20Platform%0AAuthor%3A%20Jake%20Ketchum%20and%20James%20Avtges%20and%20Millicent%20Schlafly%20and%20Helena%20Young%20and%20Taekyoung%20Kim%20and%20Ryan%20L.%20Truby%20and%20Todd%20D.%20Murphey%0AAbstract%3A%20%20%20Many%20soft%20robots%20struggle%20to%20produce%20dynamic%20motions%20with%20fast%2C%20large%0Adisplacements.%20We%20develop%20a%20parallel%206%20degree-of-freedom%20%28DoF%29%20Stewart-Gough%0Amechanism%20using%20Handed%20Shearing%20Auxetic%20%28HSA%29%20actuators.%20By%20using%20soft%0Aactuators%2C%20we%20are%20able%20to%20use%20one%20third%20as%20many%20mechatronic%20components%20as%20a%0Arigid%20Stewart%20platform%2C%20while%20retaining%20a%20working%20payload%20of%202kg%20and%20an%0Aopen-loop%20bandwidth%20greater%20than%2016Hz.%20We%20show%20that%20the%20platform%20is%20capable%20of%0Aboth%20precise%20tracing%20and%20dynamic%20disturbance%20rejection%20when%20controlling%20a%20ball%0Aand%20sliding%20puck%20using%20a%20Proportional%20Integral%20Derivative%20%28PID%29%20controller.%20We%0Adevelop%20a%20machine-learning-based%20kinematics%20model%20and%20demonstrate%20a%20functional%0Aworkspace%20of%20roughly%2010cm%20in%20each%20translation%20direction%20and%2028%20degrees%20in%20each%0Aorientation.%20This%206DoF%20device%20has%20many%20of%20the%20characteristics%20associated%20with%0Arigid%20components%20-%20power%2C%20speed%2C%20and%20total%20workspace%20-%20while%20capturing%20the%0Aadvantages%20of%20soft%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13127v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForce%2520and%2520Speed%2520in%2520a%2520Soft%2520Stewart%2520Platform%26entry.906535625%3DJake%2520Ketchum%2520and%2520James%2520Avtges%2520and%2520Millicent%2520Schlafly%2520and%2520Helena%2520Young%2520and%2520Taekyoung%2520Kim%2520and%2520Ryan%2520L.%2520Truby%2520and%2520Todd%2520D.%2520Murphey%26entry.1292438233%3D%2520%2520Many%2520soft%2520robots%2520struggle%2520to%2520produce%2520dynamic%2520motions%2520with%2520fast%252C%2520large%250Adisplacements.%2520We%2520develop%2520a%2520parallel%25206%2520degree-of-freedom%2520%2528DoF%2529%2520Stewart-Gough%250Amechanism%2520using%2520Handed%2520Shearing%2520Auxetic%2520%2528HSA%2529%2520actuators.%2520By%2520using%2520soft%250Aactuators%252C%2520we%2520are%2520able%2520to%2520use%2520one%2520third%2520as%2520many%2520mechatronic%2520components%2520as%2520a%250Arigid%2520Stewart%2520platform%252C%2520while%2520retaining%2520a%2520working%2520payload%2520of%25202kg%2520and%2520an%250Aopen-loop%2520bandwidth%2520greater%2520than%252016Hz.%2520We%2520show%2520that%2520the%2520platform%2520is%2520capable%2520of%250Aboth%2520precise%2520tracing%2520and%2520dynamic%2520disturbance%2520rejection%2520when%2520controlling%2520a%2520ball%250Aand%2520sliding%2520puck%2520using%2520a%2520Proportional%2520Integral%2520Derivative%2520%2528PID%2529%2520controller.%2520We%250Adevelop%2520a%2520machine-learning-based%2520kinematics%2520model%2520and%2520demonstrate%2520a%2520functional%250Aworkspace%2520of%2520roughly%252010cm%2520in%2520each%2520translation%2520direction%2520and%252028%2520degrees%2520in%2520each%250Aorientation.%2520This%25206DoF%2520device%2520has%2520many%2520of%2520the%2520characteristics%2520associated%2520with%250Arigid%2520components%2520-%2520power%252C%2520speed%252C%2520and%2520total%2520workspace%2520-%2520while%2520capturing%2520the%250Aadvantages%2520of%2520soft%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13127v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Force%20and%20Speed%20in%20a%20Soft%20Stewart%20Platform&entry.906535625=Jake%20Ketchum%20and%20James%20Avtges%20and%20Millicent%20Schlafly%20and%20Helena%20Young%20and%20Taekyoung%20Kim%20and%20Ryan%20L.%20Truby%20and%20Todd%20D.%20Murphey&entry.1292438233=%20%20Many%20soft%20robots%20struggle%20to%20produce%20dynamic%20motions%20with%20fast%2C%20large%0Adisplacements.%20We%20develop%20a%20parallel%206%20degree-of-freedom%20%28DoF%29%20Stewart-Gough%0Amechanism%20using%20Handed%20Shearing%20Auxetic%20%28HSA%29%20actuators.%20By%20using%20soft%0Aactuators%2C%20we%20are%20able%20to%20use%20one%20third%20as%20many%20mechatronic%20components%20as%20a%0Arigid%20Stewart%20platform%2C%20while%20retaining%20a%20working%20payload%20of%202kg%20and%20an%0Aopen-loop%20bandwidth%20greater%20than%2016Hz.%20We%20show%20that%20the%20platform%20is%20capable%20of%0Aboth%20precise%20tracing%20and%20dynamic%20disturbance%20rejection%20when%20controlling%20a%20ball%0Aand%20sliding%20puck%20using%20a%20Proportional%20Integral%20Derivative%20%28PID%29%20controller.%20We%0Adevelop%20a%20machine-learning-based%20kinematics%20model%20and%20demonstrate%20a%20functional%0Aworkspace%20of%20roughly%2010cm%20in%20each%20translation%20direction%20and%2028%20degrees%20in%20each%0Aorientation.%20This%206DoF%20device%20has%20many%20of%20the%20characteristics%20associated%20with%0Arigid%20components%20-%20power%2C%20speed%2C%20and%20total%20workspace%20-%20while%20capturing%20the%0Aadvantages%20of%20soft%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13127v2&entry.124074799=Read"},
{"title": "Zebrafish Counting Using Event Stream Data", "author": "Qianghua Chen and Huiyu Wang and Li Ming and Ying Zhao", "abstract": "  Zebrafish share a high degree of homology with human genes and are commonly\nused as model organism in biomedical research. For medical laboratories,\ncounting zebrafish is a daily task. Due to the tiny size of zebrafish, manual\nvisual counting is challenging. Existing counting methods are either not\napplicable to small fishes or have too many limitations. The paper proposed a\nzebrafish counting algorithm based on the event stream data. Firstly, an event\ncamera is applied for data acquisition. Secondly, camera calibration and image\nfusion were preformed successively. Then, the trajectory information was used\nto improve the counting accuracy. Finally, the counting results were averaged\nover an empirical of period and rounded up to get the final results. To\nevaluate the accuracy of the algorithm, 20 zebrafish were put in a four-liter\nbreeding tank. Among 100 counting trials, the average accuracy reached 97.95%.\nAs compared with traditional algorithms, the proposed one offers a simpler\nimplementation and achieves higher accuracy.\n", "link": "http://arxiv.org/abs/2504.13692v1", "date": "2025-04-18", "relevancy": 1.9641, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3999}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3964}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zebrafish%20Counting%20Using%20Event%20Stream%20Data&body=Title%3A%20Zebrafish%20Counting%20Using%20Event%20Stream%20Data%0AAuthor%3A%20Qianghua%20Chen%20and%20Huiyu%20Wang%20and%20Li%20Ming%20and%20Ying%20Zhao%0AAbstract%3A%20%20%20Zebrafish%20share%20a%20high%20degree%20of%20homology%20with%20human%20genes%20and%20are%20commonly%0Aused%20as%20model%20organism%20in%20biomedical%20research.%20For%20medical%20laboratories%2C%0Acounting%20zebrafish%20is%20a%20daily%20task.%20Due%20to%20the%20tiny%20size%20of%20zebrafish%2C%20manual%0Avisual%20counting%20is%20challenging.%20Existing%20counting%20methods%20are%20either%20not%0Aapplicable%20to%20small%20fishes%20or%20have%20too%20many%20limitations.%20The%20paper%20proposed%20a%0Azebrafish%20counting%20algorithm%20based%20on%20the%20event%20stream%20data.%20Firstly%2C%20an%20event%0Acamera%20is%20applied%20for%20data%20acquisition.%20Secondly%2C%20camera%20calibration%20and%20image%0Afusion%20were%20preformed%20successively.%20Then%2C%20the%20trajectory%20information%20was%20used%0Ato%20improve%20the%20counting%20accuracy.%20Finally%2C%20the%20counting%20results%20were%20averaged%0Aover%20an%20empirical%20of%20period%20and%20rounded%20up%20to%20get%20the%20final%20results.%20To%0Aevaluate%20the%20accuracy%20of%20the%20algorithm%2C%2020%20zebrafish%20were%20put%20in%20a%20four-liter%0Abreeding%20tank.%20Among%20100%20counting%20trials%2C%20the%20average%20accuracy%20reached%2097.95%25.%0AAs%20compared%20with%20traditional%20algorithms%2C%20the%20proposed%20one%20offers%20a%20simpler%0Aimplementation%20and%20achieves%20higher%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZebrafish%2520Counting%2520Using%2520Event%2520Stream%2520Data%26entry.906535625%3DQianghua%2520Chen%2520and%2520Huiyu%2520Wang%2520and%2520Li%2520Ming%2520and%2520Ying%2520Zhao%26entry.1292438233%3D%2520%2520Zebrafish%2520share%2520a%2520high%2520degree%2520of%2520homology%2520with%2520human%2520genes%2520and%2520are%2520commonly%250Aused%2520as%2520model%2520organism%2520in%2520biomedical%2520research.%2520For%2520medical%2520laboratories%252C%250Acounting%2520zebrafish%2520is%2520a%2520daily%2520task.%2520Due%2520to%2520the%2520tiny%2520size%2520of%2520zebrafish%252C%2520manual%250Avisual%2520counting%2520is%2520challenging.%2520Existing%2520counting%2520methods%2520are%2520either%2520not%250Aapplicable%2520to%2520small%2520fishes%2520or%2520have%2520too%2520many%2520limitations.%2520The%2520paper%2520proposed%2520a%250Azebrafish%2520counting%2520algorithm%2520based%2520on%2520the%2520event%2520stream%2520data.%2520Firstly%252C%2520an%2520event%250Acamera%2520is%2520applied%2520for%2520data%2520acquisition.%2520Secondly%252C%2520camera%2520calibration%2520and%2520image%250Afusion%2520were%2520preformed%2520successively.%2520Then%252C%2520the%2520trajectory%2520information%2520was%2520used%250Ato%2520improve%2520the%2520counting%2520accuracy.%2520Finally%252C%2520the%2520counting%2520results%2520were%2520averaged%250Aover%2520an%2520empirical%2520of%2520period%2520and%2520rounded%2520up%2520to%2520get%2520the%2520final%2520results.%2520To%250Aevaluate%2520the%2520accuracy%2520of%2520the%2520algorithm%252C%252020%2520zebrafish%2520were%2520put%2520in%2520a%2520four-liter%250Abreeding%2520tank.%2520Among%2520100%2520counting%2520trials%252C%2520the%2520average%2520accuracy%2520reached%252097.95%2525.%250AAs%2520compared%2520with%2520traditional%2520algorithms%252C%2520the%2520proposed%2520one%2520offers%2520a%2520simpler%250Aimplementation%2520and%2520achieves%2520higher%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zebrafish%20Counting%20Using%20Event%20Stream%20Data&entry.906535625=Qianghua%20Chen%20and%20Huiyu%20Wang%20and%20Li%20Ming%20and%20Ying%20Zhao&entry.1292438233=%20%20Zebrafish%20share%20a%20high%20degree%20of%20homology%20with%20human%20genes%20and%20are%20commonly%0Aused%20as%20model%20organism%20in%20biomedical%20research.%20For%20medical%20laboratories%2C%0Acounting%20zebrafish%20is%20a%20daily%20task.%20Due%20to%20the%20tiny%20size%20of%20zebrafish%2C%20manual%0Avisual%20counting%20is%20challenging.%20Existing%20counting%20methods%20are%20either%20not%0Aapplicable%20to%20small%20fishes%20or%20have%20too%20many%20limitations.%20The%20paper%20proposed%20a%0Azebrafish%20counting%20algorithm%20based%20on%20the%20event%20stream%20data.%20Firstly%2C%20an%20event%0Acamera%20is%20applied%20for%20data%20acquisition.%20Secondly%2C%20camera%20calibration%20and%20image%0Afusion%20were%20preformed%20successively.%20Then%2C%20the%20trajectory%20information%20was%20used%0Ato%20improve%20the%20counting%20accuracy.%20Finally%2C%20the%20counting%20results%20were%20averaged%0Aover%20an%20empirical%20of%20period%20and%20rounded%20up%20to%20get%20the%20final%20results.%20To%0Aevaluate%20the%20accuracy%20of%20the%20algorithm%2C%2020%20zebrafish%20were%20put%20in%20a%20four-liter%0Abreeding%20tank.%20Among%20100%20counting%20trials%2C%20the%20average%20accuracy%20reached%2097.95%25.%0AAs%20compared%20with%20traditional%20algorithms%2C%20the%20proposed%20one%20offers%20a%20simpler%0Aimplementation%20and%20achieves%20higher%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13692v1&entry.124074799=Read"},
{"title": "Unsupervised Machine Learning Hybrid Approach Integrating Linear\n  Programming in Loss Function: A Robust Optimization Technique", "author": "Andrew Kiruluta and Andreas Lemos", "abstract": "  This paper presents a novel hybrid approach that integrates linear\nprogramming (LP) within the loss function of an unsupervised machine learning\nmodel. By leveraging the strengths of both optimization techniques and machine\nlearning, this method introduces a robust framework for solving complex\noptimization problems where traditional methods may fall short. The proposed\napproach encapsulates the constraints and objectives of a linear programming\nproblem directly into the loss function, guiding the learning process to adhere\nto these constraints while optimizing the desired outcomes. This technique not\nonly preserves the interpretability of linear programming but also benefits\nfrom the flexibility and adaptability of machine learning, making it\nparticularly well-suited for unsupervised or semi-supervised learning\nscenarios.\n", "link": "http://arxiv.org/abs/2408.09967v2", "date": "2025-04-18", "relevancy": 1.9615, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5229}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.468}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Machine%20Learning%20Hybrid%20Approach%20Integrating%20Linear%0A%20%20Programming%20in%20Loss%20Function%3A%20A%20Robust%20Optimization%20Technique&body=Title%3A%20Unsupervised%20Machine%20Learning%20Hybrid%20Approach%20Integrating%20Linear%0A%20%20Programming%20in%20Loss%20Function%3A%20A%20Robust%20Optimization%20Technique%0AAuthor%3A%20Andrew%20Kiruluta%20and%20Andreas%20Lemos%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20hybrid%20approach%20that%20integrates%20linear%0Aprogramming%20%28LP%29%20within%20the%20loss%20function%20of%20an%20unsupervised%20machine%20learning%0Amodel.%20By%20leveraging%20the%20strengths%20of%20both%20optimization%20techniques%20and%20machine%0Alearning%2C%20this%20method%20introduces%20a%20robust%20framework%20for%20solving%20complex%0Aoptimization%20problems%20where%20traditional%20methods%20may%20fall%20short.%20The%20proposed%0Aapproach%20encapsulates%20the%20constraints%20and%20objectives%20of%20a%20linear%20programming%0Aproblem%20directly%20into%20the%20loss%20function%2C%20guiding%20the%20learning%20process%20to%20adhere%0Ato%20these%20constraints%20while%20optimizing%20the%20desired%20outcomes.%20This%20technique%20not%0Aonly%20preserves%20the%20interpretability%20of%20linear%20programming%20but%20also%20benefits%0Afrom%20the%20flexibility%20and%20adaptability%20of%20machine%20learning%2C%20making%20it%0Aparticularly%20well-suited%20for%20unsupervised%20or%20semi-supervised%20learning%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09967v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Machine%2520Learning%2520Hybrid%2520Approach%2520Integrating%2520Linear%250A%2520%2520Programming%2520in%2520Loss%2520Function%253A%2520A%2520Robust%2520Optimization%2520Technique%26entry.906535625%3DAndrew%2520Kiruluta%2520and%2520Andreas%2520Lemos%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520hybrid%2520approach%2520that%2520integrates%2520linear%250Aprogramming%2520%2528LP%2529%2520within%2520the%2520loss%2520function%2520of%2520an%2520unsupervised%2520machine%2520learning%250Amodel.%2520By%2520leveraging%2520the%2520strengths%2520of%2520both%2520optimization%2520techniques%2520and%2520machine%250Alearning%252C%2520this%2520method%2520introduces%2520a%2520robust%2520framework%2520for%2520solving%2520complex%250Aoptimization%2520problems%2520where%2520traditional%2520methods%2520may%2520fall%2520short.%2520The%2520proposed%250Aapproach%2520encapsulates%2520the%2520constraints%2520and%2520objectives%2520of%2520a%2520linear%2520programming%250Aproblem%2520directly%2520into%2520the%2520loss%2520function%252C%2520guiding%2520the%2520learning%2520process%2520to%2520adhere%250Ato%2520these%2520constraints%2520while%2520optimizing%2520the%2520desired%2520outcomes.%2520This%2520technique%2520not%250Aonly%2520preserves%2520the%2520interpretability%2520of%2520linear%2520programming%2520but%2520also%2520benefits%250Afrom%2520the%2520flexibility%2520and%2520adaptability%2520of%2520machine%2520learning%252C%2520making%2520it%250Aparticularly%2520well-suited%2520for%2520unsupervised%2520or%2520semi-supervised%2520learning%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09967v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Machine%20Learning%20Hybrid%20Approach%20Integrating%20Linear%0A%20%20Programming%20in%20Loss%20Function%3A%20A%20Robust%20Optimization%20Technique&entry.906535625=Andrew%20Kiruluta%20and%20Andreas%20Lemos&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20hybrid%20approach%20that%20integrates%20linear%0Aprogramming%20%28LP%29%20within%20the%20loss%20function%20of%20an%20unsupervised%20machine%20learning%0Amodel.%20By%20leveraging%20the%20strengths%20of%20both%20optimization%20techniques%20and%20machine%0Alearning%2C%20this%20method%20introduces%20a%20robust%20framework%20for%20solving%20complex%0Aoptimization%20problems%20where%20traditional%20methods%20may%20fall%20short.%20The%20proposed%0Aapproach%20encapsulates%20the%20constraints%20and%20objectives%20of%20a%20linear%20programming%0Aproblem%20directly%20into%20the%20loss%20function%2C%20guiding%20the%20learning%20process%20to%20adhere%0Ato%20these%20constraints%20while%20optimizing%20the%20desired%20outcomes.%20This%20technique%20not%0Aonly%20preserves%20the%20interpretability%20of%20linear%20programming%20but%20also%20benefits%0Afrom%20the%20flexibility%20and%20adaptability%20of%20machine%20learning%2C%20making%20it%0Aparticularly%20well-suited%20for%20unsupervised%20or%20semi-supervised%20learning%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09967v2&entry.124074799=Read"},
{"title": "Fighting Fires from Space: Leveraging Vision Transformers for Enhanced\n  Wildfire Detection and Characterization", "author": "Aman Agarwal and James Gearon and Raksha Rank and Etienne Chenevert", "abstract": "  Wildfires are increasing in intensity, frequency, and duration across large\nparts of the world as a result of anthropogenic climate change. Modern hazard\ndetection and response systems that deal with wildfires are under-equipped for\nsustained wildfire seasons. Recent work has proved automated wildfire detection\nusing Convolutional Neural Networks (CNNs) trained on satellite imagery are\ncapable of high-accuracy results. However, CNNs are computationally expensive\nto train and only incorporate local image context. Recently, Vision\nTransformers (ViTs) have gained popularity for their efficient training and\ntheir ability to include both local and global contextual information. In this\nwork, we show that ViT can outperform well-trained and specialized CNNs to\ndetect wildfires on a previously published dataset of LandSat-8 imagery. One of\nour ViTs outperforms the baseline CNN comparison by 0.92%. However, we find our\nown implementation of CNN-based UNet to perform best in every category, showing\ntheir sustained utility in image tasks. Overall, ViTs are comparably capable in\ndetecting wildfires as CNNs, though well-tuned CNNs are still the best\ntechnique for detecting wildfire with our UNet providing an IoU of 93.58%,\nbetter than the baseline UNet by some 4.58%.\n", "link": "http://arxiv.org/abs/2504.13776v1", "date": "2025-04-18", "relevancy": 1.9576, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5042}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4902}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fighting%20Fires%20from%20Space%3A%20Leveraging%20Vision%20Transformers%20for%20Enhanced%0A%20%20Wildfire%20Detection%20and%20Characterization&body=Title%3A%20Fighting%20Fires%20from%20Space%3A%20Leveraging%20Vision%20Transformers%20for%20Enhanced%0A%20%20Wildfire%20Detection%20and%20Characterization%0AAuthor%3A%20Aman%20Agarwal%20and%20James%20Gearon%20and%20Raksha%20Rank%20and%20Etienne%20Chenevert%0AAbstract%3A%20%20%20Wildfires%20are%20increasing%20in%20intensity%2C%20frequency%2C%20and%20duration%20across%20large%0Aparts%20of%20the%20world%20as%20a%20result%20of%20anthropogenic%20climate%20change.%20Modern%20hazard%0Adetection%20and%20response%20systems%20that%20deal%20with%20wildfires%20are%20under-equipped%20for%0Asustained%20wildfire%20seasons.%20Recent%20work%20has%20proved%20automated%20wildfire%20detection%0Ausing%20Convolutional%20Neural%20Networks%20%28CNNs%29%20trained%20on%20satellite%20imagery%20are%0Acapable%20of%20high-accuracy%20results.%20However%2C%20CNNs%20are%20computationally%20expensive%0Ato%20train%20and%20only%20incorporate%20local%20image%20context.%20Recently%2C%20Vision%0ATransformers%20%28ViTs%29%20have%20gained%20popularity%20for%20their%20efficient%20training%20and%0Atheir%20ability%20to%20include%20both%20local%20and%20global%20contextual%20information.%20In%20this%0Awork%2C%20we%20show%20that%20ViT%20can%20outperform%20well-trained%20and%20specialized%20CNNs%20to%0Adetect%20wildfires%20on%20a%20previously%20published%20dataset%20of%20LandSat-8%20imagery.%20One%20of%0Aour%20ViTs%20outperforms%20the%20baseline%20CNN%20comparison%20by%200.92%25.%20However%2C%20we%20find%20our%0Aown%20implementation%20of%20CNN-based%20UNet%20to%20perform%20best%20in%20every%20category%2C%20showing%0Atheir%20sustained%20utility%20in%20image%20tasks.%20Overall%2C%20ViTs%20are%20comparably%20capable%20in%0Adetecting%20wildfires%20as%20CNNs%2C%20though%20well-tuned%20CNNs%20are%20still%20the%20best%0Atechnique%20for%20detecting%20wildfire%20with%20our%20UNet%20providing%20an%20IoU%20of%2093.58%25%2C%0Abetter%20than%20the%20baseline%20UNet%20by%20some%204.58%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFighting%2520Fires%2520from%2520Space%253A%2520Leveraging%2520Vision%2520Transformers%2520for%2520Enhanced%250A%2520%2520Wildfire%2520Detection%2520and%2520Characterization%26entry.906535625%3DAman%2520Agarwal%2520and%2520James%2520Gearon%2520and%2520Raksha%2520Rank%2520and%2520Etienne%2520Chenevert%26entry.1292438233%3D%2520%2520Wildfires%2520are%2520increasing%2520in%2520intensity%252C%2520frequency%252C%2520and%2520duration%2520across%2520large%250Aparts%2520of%2520the%2520world%2520as%2520a%2520result%2520of%2520anthropogenic%2520climate%2520change.%2520Modern%2520hazard%250Adetection%2520and%2520response%2520systems%2520that%2520deal%2520with%2520wildfires%2520are%2520under-equipped%2520for%250Asustained%2520wildfire%2520seasons.%2520Recent%2520work%2520has%2520proved%2520automated%2520wildfire%2520detection%250Ausing%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520trained%2520on%2520satellite%2520imagery%2520are%250Acapable%2520of%2520high-accuracy%2520results.%2520However%252C%2520CNNs%2520are%2520computationally%2520expensive%250Ato%2520train%2520and%2520only%2520incorporate%2520local%2520image%2520context.%2520Recently%252C%2520Vision%250ATransformers%2520%2528ViTs%2529%2520have%2520gained%2520popularity%2520for%2520their%2520efficient%2520training%2520and%250Atheir%2520ability%2520to%2520include%2520both%2520local%2520and%2520global%2520contextual%2520information.%2520In%2520this%250Awork%252C%2520we%2520show%2520that%2520ViT%2520can%2520outperform%2520well-trained%2520and%2520specialized%2520CNNs%2520to%250Adetect%2520wildfires%2520on%2520a%2520previously%2520published%2520dataset%2520of%2520LandSat-8%2520imagery.%2520One%2520of%250Aour%2520ViTs%2520outperforms%2520the%2520baseline%2520CNN%2520comparison%2520by%25200.92%2525.%2520However%252C%2520we%2520find%2520our%250Aown%2520implementation%2520of%2520CNN-based%2520UNet%2520to%2520perform%2520best%2520in%2520every%2520category%252C%2520showing%250Atheir%2520sustained%2520utility%2520in%2520image%2520tasks.%2520Overall%252C%2520ViTs%2520are%2520comparably%2520capable%2520in%250Adetecting%2520wildfires%2520as%2520CNNs%252C%2520though%2520well-tuned%2520CNNs%2520are%2520still%2520the%2520best%250Atechnique%2520for%2520detecting%2520wildfire%2520with%2520our%2520UNet%2520providing%2520an%2520IoU%2520of%252093.58%2525%252C%250Abetter%2520than%2520the%2520baseline%2520UNet%2520by%2520some%25204.58%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fighting%20Fires%20from%20Space%3A%20Leveraging%20Vision%20Transformers%20for%20Enhanced%0A%20%20Wildfire%20Detection%20and%20Characterization&entry.906535625=Aman%20Agarwal%20and%20James%20Gearon%20and%20Raksha%20Rank%20and%20Etienne%20Chenevert&entry.1292438233=%20%20Wildfires%20are%20increasing%20in%20intensity%2C%20frequency%2C%20and%20duration%20across%20large%0Aparts%20of%20the%20world%20as%20a%20result%20of%20anthropogenic%20climate%20change.%20Modern%20hazard%0Adetection%20and%20response%20systems%20that%20deal%20with%20wildfires%20are%20under-equipped%20for%0Asustained%20wildfire%20seasons.%20Recent%20work%20has%20proved%20automated%20wildfire%20detection%0Ausing%20Convolutional%20Neural%20Networks%20%28CNNs%29%20trained%20on%20satellite%20imagery%20are%0Acapable%20of%20high-accuracy%20results.%20However%2C%20CNNs%20are%20computationally%20expensive%0Ato%20train%20and%20only%20incorporate%20local%20image%20context.%20Recently%2C%20Vision%0ATransformers%20%28ViTs%29%20have%20gained%20popularity%20for%20their%20efficient%20training%20and%0Atheir%20ability%20to%20include%20both%20local%20and%20global%20contextual%20information.%20In%20this%0Awork%2C%20we%20show%20that%20ViT%20can%20outperform%20well-trained%20and%20specialized%20CNNs%20to%0Adetect%20wildfires%20on%20a%20previously%20published%20dataset%20of%20LandSat-8%20imagery.%20One%20of%0Aour%20ViTs%20outperforms%20the%20baseline%20CNN%20comparison%20by%200.92%25.%20However%2C%20we%20find%20our%0Aown%20implementation%20of%20CNN-based%20UNet%20to%20perform%20best%20in%20every%20category%2C%20showing%0Atheir%20sustained%20utility%20in%20image%20tasks.%20Overall%2C%20ViTs%20are%20comparably%20capable%20in%0Adetecting%20wildfires%20as%20CNNs%2C%20though%20well-tuned%20CNNs%20are%20still%20the%20best%0Atechnique%20for%20detecting%20wildfire%20with%20our%20UNet%20providing%20an%20IoU%20of%2093.58%25%2C%0Abetter%20than%20the%20baseline%20UNet%20by%20some%204.58%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13776v1&entry.124074799=Read"},
{"title": "Fragile Watermarking for Image Certification Using Deep Steganographic\n  Embedding", "author": "Davide Ghiani and Jefferson David Rodriguez Chivata and Stefano Lilliu and Simone Maurizio La Cava and Marco Micheletto and Giulia Orr\u00f9 and Federico Lama and Gian Luca Marcialis", "abstract": "  Modern identity verification systems increasingly rely on facial images\nembedded in biometric documents such as electronic passports. To ensure global\ninteroperability and security, these images must comply with strict standards\ndefined by the International Civil Aviation Organization (ICAO), which specify\nacquisition, quality, and format requirements. However, once issued, these\nimages may undergo unintentional degradations (e.g., compression, resizing) or\nmalicious manipulations (e.g., morphing) and deceive facial recognition\nsystems. In this study, we explore fragile watermarking, based on deep\nsteganographic embedding as a proactive mechanism to certify the authenticity\nof ICAO-compliant facial images. By embedding a hidden image within the\nofficial photo at the time of issuance, we establish an integrity marker that\nbecomes sensitive to any post-issuance modification. We assess how a range of\nimage manipulations affects the recovered hidden image and show that\ndegradation artifacts can serve as robust forensic cues. Furthermore, we\npropose a classification framework that analyzes the revealed content to detect\nand categorize the type of manipulation applied. Our experiments demonstrate\nhigh detection accuracy, including cross-method scenarios with multiple deep\nsteganography-based models. These findings support the viability of fragile\nwatermarking via steganographic embedding as a valuable tool for biometric\ndocument integrity verification.\n", "link": "http://arxiv.org/abs/2504.13759v1", "date": "2025-04-18", "relevancy": 1.9569, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5242}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4699}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fragile%20Watermarking%20for%20Image%20Certification%20Using%20Deep%20Steganographic%0A%20%20Embedding&body=Title%3A%20Fragile%20Watermarking%20for%20Image%20Certification%20Using%20Deep%20Steganographic%0A%20%20Embedding%0AAuthor%3A%20Davide%20Ghiani%20and%20Jefferson%20David%20Rodriguez%20Chivata%20and%20Stefano%20Lilliu%20and%20Simone%20Maurizio%20La%20Cava%20and%20Marco%20Micheletto%20and%20Giulia%20Orr%C3%B9%20and%20Federico%20Lama%20and%20Gian%20Luca%20Marcialis%0AAbstract%3A%20%20%20Modern%20identity%20verification%20systems%20increasingly%20rely%20on%20facial%20images%0Aembedded%20in%20biometric%20documents%20such%20as%20electronic%20passports.%20To%20ensure%20global%0Ainteroperability%20and%20security%2C%20these%20images%20must%20comply%20with%20strict%20standards%0Adefined%20by%20the%20International%20Civil%20Aviation%20Organization%20%28ICAO%29%2C%20which%20specify%0Aacquisition%2C%20quality%2C%20and%20format%20requirements.%20However%2C%20once%20issued%2C%20these%0Aimages%20may%20undergo%20unintentional%20degradations%20%28e.g.%2C%20compression%2C%20resizing%29%20or%0Amalicious%20manipulations%20%28e.g.%2C%20morphing%29%20and%20deceive%20facial%20recognition%0Asystems.%20In%20this%20study%2C%20we%20explore%20fragile%20watermarking%2C%20based%20on%20deep%0Asteganographic%20embedding%20as%20a%20proactive%20mechanism%20to%20certify%20the%20authenticity%0Aof%20ICAO-compliant%20facial%20images.%20By%20embedding%20a%20hidden%20image%20within%20the%0Aofficial%20photo%20at%20the%20time%20of%20issuance%2C%20we%20establish%20an%20integrity%20marker%20that%0Abecomes%20sensitive%20to%20any%20post-issuance%20modification.%20We%20assess%20how%20a%20range%20of%0Aimage%20manipulations%20affects%20the%20recovered%20hidden%20image%20and%20show%20that%0Adegradation%20artifacts%20can%20serve%20as%20robust%20forensic%20cues.%20Furthermore%2C%20we%0Apropose%20a%20classification%20framework%20that%20analyzes%20the%20revealed%20content%20to%20detect%0Aand%20categorize%20the%20type%20of%20manipulation%20applied.%20Our%20experiments%20demonstrate%0Ahigh%20detection%20accuracy%2C%20including%20cross-method%20scenarios%20with%20multiple%20deep%0Asteganography-based%20models.%20These%20findings%20support%20the%20viability%20of%20fragile%0Awatermarking%20via%20steganographic%20embedding%20as%20a%20valuable%20tool%20for%20biometric%0Adocument%20integrity%20verification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFragile%2520Watermarking%2520for%2520Image%2520Certification%2520Using%2520Deep%2520Steganographic%250A%2520%2520Embedding%26entry.906535625%3DDavide%2520Ghiani%2520and%2520Jefferson%2520David%2520Rodriguez%2520Chivata%2520and%2520Stefano%2520Lilliu%2520and%2520Simone%2520Maurizio%2520La%2520Cava%2520and%2520Marco%2520Micheletto%2520and%2520Giulia%2520Orr%25C3%25B9%2520and%2520Federico%2520Lama%2520and%2520Gian%2520Luca%2520Marcialis%26entry.1292438233%3D%2520%2520Modern%2520identity%2520verification%2520systems%2520increasingly%2520rely%2520on%2520facial%2520images%250Aembedded%2520in%2520biometric%2520documents%2520such%2520as%2520electronic%2520passports.%2520To%2520ensure%2520global%250Ainteroperability%2520and%2520security%252C%2520these%2520images%2520must%2520comply%2520with%2520strict%2520standards%250Adefined%2520by%2520the%2520International%2520Civil%2520Aviation%2520Organization%2520%2528ICAO%2529%252C%2520which%2520specify%250Aacquisition%252C%2520quality%252C%2520and%2520format%2520requirements.%2520However%252C%2520once%2520issued%252C%2520these%250Aimages%2520may%2520undergo%2520unintentional%2520degradations%2520%2528e.g.%252C%2520compression%252C%2520resizing%2529%2520or%250Amalicious%2520manipulations%2520%2528e.g.%252C%2520morphing%2529%2520and%2520deceive%2520facial%2520recognition%250Asystems.%2520In%2520this%2520study%252C%2520we%2520explore%2520fragile%2520watermarking%252C%2520based%2520on%2520deep%250Asteganographic%2520embedding%2520as%2520a%2520proactive%2520mechanism%2520to%2520certify%2520the%2520authenticity%250Aof%2520ICAO-compliant%2520facial%2520images.%2520By%2520embedding%2520a%2520hidden%2520image%2520within%2520the%250Aofficial%2520photo%2520at%2520the%2520time%2520of%2520issuance%252C%2520we%2520establish%2520an%2520integrity%2520marker%2520that%250Abecomes%2520sensitive%2520to%2520any%2520post-issuance%2520modification.%2520We%2520assess%2520how%2520a%2520range%2520of%250Aimage%2520manipulations%2520affects%2520the%2520recovered%2520hidden%2520image%2520and%2520show%2520that%250Adegradation%2520artifacts%2520can%2520serve%2520as%2520robust%2520forensic%2520cues.%2520Furthermore%252C%2520we%250Apropose%2520a%2520classification%2520framework%2520that%2520analyzes%2520the%2520revealed%2520content%2520to%2520detect%250Aand%2520categorize%2520the%2520type%2520of%2520manipulation%2520applied.%2520Our%2520experiments%2520demonstrate%250Ahigh%2520detection%2520accuracy%252C%2520including%2520cross-method%2520scenarios%2520with%2520multiple%2520deep%250Asteganography-based%2520models.%2520These%2520findings%2520support%2520the%2520viability%2520of%2520fragile%250Awatermarking%2520via%2520steganographic%2520embedding%2520as%2520a%2520valuable%2520tool%2520for%2520biometric%250Adocument%2520integrity%2520verification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fragile%20Watermarking%20for%20Image%20Certification%20Using%20Deep%20Steganographic%0A%20%20Embedding&entry.906535625=Davide%20Ghiani%20and%20Jefferson%20David%20Rodriguez%20Chivata%20and%20Stefano%20Lilliu%20and%20Simone%20Maurizio%20La%20Cava%20and%20Marco%20Micheletto%20and%20Giulia%20Orr%C3%B9%20and%20Federico%20Lama%20and%20Gian%20Luca%20Marcialis&entry.1292438233=%20%20Modern%20identity%20verification%20systems%20increasingly%20rely%20on%20facial%20images%0Aembedded%20in%20biometric%20documents%20such%20as%20electronic%20passports.%20To%20ensure%20global%0Ainteroperability%20and%20security%2C%20these%20images%20must%20comply%20with%20strict%20standards%0Adefined%20by%20the%20International%20Civil%20Aviation%20Organization%20%28ICAO%29%2C%20which%20specify%0Aacquisition%2C%20quality%2C%20and%20format%20requirements.%20However%2C%20once%20issued%2C%20these%0Aimages%20may%20undergo%20unintentional%20degradations%20%28e.g.%2C%20compression%2C%20resizing%29%20or%0Amalicious%20manipulations%20%28e.g.%2C%20morphing%29%20and%20deceive%20facial%20recognition%0Asystems.%20In%20this%20study%2C%20we%20explore%20fragile%20watermarking%2C%20based%20on%20deep%0Asteganographic%20embedding%20as%20a%20proactive%20mechanism%20to%20certify%20the%20authenticity%0Aof%20ICAO-compliant%20facial%20images.%20By%20embedding%20a%20hidden%20image%20within%20the%0Aofficial%20photo%20at%20the%20time%20of%20issuance%2C%20we%20establish%20an%20integrity%20marker%20that%0Abecomes%20sensitive%20to%20any%20post-issuance%20modification.%20We%20assess%20how%20a%20range%20of%0Aimage%20manipulations%20affects%20the%20recovered%20hidden%20image%20and%20show%20that%0Adegradation%20artifacts%20can%20serve%20as%20robust%20forensic%20cues.%20Furthermore%2C%20we%0Apropose%20a%20classification%20framework%20that%20analyzes%20the%20revealed%20content%20to%20detect%0Aand%20categorize%20the%20type%20of%20manipulation%20applied.%20Our%20experiments%20demonstrate%0Ahigh%20detection%20accuracy%2C%20including%20cross-method%20scenarios%20with%20multiple%20deep%0Asteganography-based%20models.%20These%20findings%20support%20the%20viability%20of%20fragile%0Awatermarking%20via%20steganographic%20embedding%20as%20a%20valuable%20tool%20for%20biometric%0Adocument%20integrity%20verification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13759v1&entry.124074799=Read"},
{"title": "Revisiting Uncertainty Quantification Evaluation in Language Models:\n  Spurious Interactions with Response Length Bias Results", "author": "Andrea Santilli and Adam Golinski and Michael Kirchhof and Federico Danieli and Arno Blaas and Miao Xiong and Luca Zappella and Sinead Williamson", "abstract": "  Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.\n", "link": "http://arxiv.org/abs/2504.13677v1", "date": "2025-04-18", "relevancy": 1.9554, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5265}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.496}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Uncertainty%20Quantification%20Evaluation%20in%20Language%20Models%3A%0A%20%20Spurious%20Interactions%20with%20Response%20Length%20Bias%20Results&body=Title%3A%20Revisiting%20Uncertainty%20Quantification%20Evaluation%20in%20Language%20Models%3A%0A%20%20Spurious%20Interactions%20with%20Response%20Length%20Bias%20Results%0AAuthor%3A%20Andrea%20Santilli%20and%20Adam%20Golinski%20and%20Michael%20Kirchhof%20and%20Federico%20Danieli%20and%20Arno%20Blaas%20and%20Miao%20Xiong%20and%20Luca%20Zappella%20and%20Sinead%20Williamson%0AAbstract%3A%20%20%20Uncertainty%20Quantification%20%28UQ%29%20in%20Language%20Models%20%28LMs%29%20is%20crucial%20for%0Aimproving%20their%20safety%20and%20reliability.%20Evaluations%20often%20use%20performance%0Ametrics%20like%20AUROC%20to%20assess%20how%20well%20UQ%20methods%20%28e.g.%2C%20negative%20sequence%0Aprobabilities%29%20correlate%20with%20task%20correctness%20functions%20%28e.g.%2C%20ROUGE-L%29.%20In%0Athis%20paper%2C%20we%20show%20that%20commonly%20used%20correctness%20functions%20bias%20UQ%0Aevaluations%20by%20inflating%20the%20performance%20of%20certain%20UQ%20methods.%20We%20evaluate%207%0Acorrectness%20functions%20--%20from%20lexical-based%20and%20embedding-based%20metrics%20to%0ALLM-as-a-judge%20approaches%20--%20across%204%20datasets%20x%204%20models%20x%206%20UQ%20methods.%20Our%0Aanalysis%20reveals%20that%20length%20biases%20in%20the%20errors%20of%20these%20correctness%0Afunctions%20distort%20UQ%20assessments%20by%20interacting%20with%20length%20biases%20in%20UQ%0Amethods.%20We%20identify%20LLM-as-a-judge%20approaches%20as%20among%20the%20least%20length-biased%0Achoices%20and%20hence%20a%20potential%20solution%20to%20mitigate%20these%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Uncertainty%2520Quantification%2520Evaluation%2520in%2520Language%2520Models%253A%250A%2520%2520Spurious%2520Interactions%2520with%2520Response%2520Length%2520Bias%2520Results%26entry.906535625%3DAndrea%2520Santilli%2520and%2520Adam%2520Golinski%2520and%2520Michael%2520Kirchhof%2520and%2520Federico%2520Danieli%2520and%2520Arno%2520Blaas%2520and%2520Miao%2520Xiong%2520and%2520Luca%2520Zappella%2520and%2520Sinead%2520Williamson%26entry.1292438233%3D%2520%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520in%2520Language%2520Models%2520%2528LMs%2529%2520is%2520crucial%2520for%250Aimproving%2520their%2520safety%2520and%2520reliability.%2520Evaluations%2520often%2520use%2520performance%250Ametrics%2520like%2520AUROC%2520to%2520assess%2520how%2520well%2520UQ%2520methods%2520%2528e.g.%252C%2520negative%2520sequence%250Aprobabilities%2529%2520correlate%2520with%2520task%2520correctness%2520functions%2520%2528e.g.%252C%2520ROUGE-L%2529.%2520In%250Athis%2520paper%252C%2520we%2520show%2520that%2520commonly%2520used%2520correctness%2520functions%2520bias%2520UQ%250Aevaluations%2520by%2520inflating%2520the%2520performance%2520of%2520certain%2520UQ%2520methods.%2520We%2520evaluate%25207%250Acorrectness%2520functions%2520--%2520from%2520lexical-based%2520and%2520embedding-based%2520metrics%2520to%250ALLM-as-a-judge%2520approaches%2520--%2520across%25204%2520datasets%2520x%25204%2520models%2520x%25206%2520UQ%2520methods.%2520Our%250Aanalysis%2520reveals%2520that%2520length%2520biases%2520in%2520the%2520errors%2520of%2520these%2520correctness%250Afunctions%2520distort%2520UQ%2520assessments%2520by%2520interacting%2520with%2520length%2520biases%2520in%2520UQ%250Amethods.%2520We%2520identify%2520LLM-as-a-judge%2520approaches%2520as%2520among%2520the%2520least%2520length-biased%250Achoices%2520and%2520hence%2520a%2520potential%2520solution%2520to%2520mitigate%2520these%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Uncertainty%20Quantification%20Evaluation%20in%20Language%20Models%3A%0A%20%20Spurious%20Interactions%20with%20Response%20Length%20Bias%20Results&entry.906535625=Andrea%20Santilli%20and%20Adam%20Golinski%20and%20Michael%20Kirchhof%20and%20Federico%20Danieli%20and%20Arno%20Blaas%20and%20Miao%20Xiong%20and%20Luca%20Zappella%20and%20Sinead%20Williamson&entry.1292438233=%20%20Uncertainty%20Quantification%20%28UQ%29%20in%20Language%20Models%20%28LMs%29%20is%20crucial%20for%0Aimproving%20their%20safety%20and%20reliability.%20Evaluations%20often%20use%20performance%0Ametrics%20like%20AUROC%20to%20assess%20how%20well%20UQ%20methods%20%28e.g.%2C%20negative%20sequence%0Aprobabilities%29%20correlate%20with%20task%20correctness%20functions%20%28e.g.%2C%20ROUGE-L%29.%20In%0Athis%20paper%2C%20we%20show%20that%20commonly%20used%20correctness%20functions%20bias%20UQ%0Aevaluations%20by%20inflating%20the%20performance%20of%20certain%20UQ%20methods.%20We%20evaluate%207%0Acorrectness%20functions%20--%20from%20lexical-based%20and%20embedding-based%20metrics%20to%0ALLM-as-a-judge%20approaches%20--%20across%204%20datasets%20x%204%20models%20x%206%20UQ%20methods.%20Our%0Aanalysis%20reveals%20that%20length%20biases%20in%20the%20errors%20of%20these%20correctness%0Afunctions%20distort%20UQ%20assessments%20by%20interacting%20with%20length%20biases%20in%20UQ%0Amethods.%20We%20identify%20LLM-as-a-judge%20approaches%20as%20among%20the%20least%20length-biased%0Achoices%20and%20hence%20a%20potential%20solution%20to%20mitigate%20these%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13677v1&entry.124074799=Read"},
{"title": "BRIGHT: A globally distributed multimodal building damage assessment\n  dataset with very-high-resolution for all-weather disaster response", "author": "Hongruixuan Chen and Jian Song and Olivier Dietrich and Clifford Broni-Bediako and Weihao Xuan and Junjue Wang and Xinlei Shao and Yimin Wei and Junshi Xia and Cuiling Lan and Konrad Schindler and Naoto Yokoya", "abstract": "  Disaster events occur around the world and cause significant damage to human\nlife and property. Earth observation (EO) data enables rapid and comprehensive\nbuilding damage assessment (BDA), an essential capability in the aftermath of a\ndisaster to reduce human casualties and to inform disaster relief efforts.\nRecent research focuses on the development of AI models to achieve accurate\nmapping of unseen disaster events, mostly using optical EO data. However,\nsolutions based on optical data are limited to clear skies and daylight hours,\npreventing a prompt response to disasters. Integrating multimodal (MM) EO data,\nparticularly the combination of optical and SAR imagery, makes it possible to\nprovide all-weather, day-and-night disaster responses. Despite this potential,\nthe development of robust multimodal AI models has been constrained by the lack\nof suitable benchmark datasets. In this paper, we present a BDA dataset using\nveRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based\nall-weather disaster response. To the best of our knowledge, BRIGHT is the\nfirst open-access, globally distributed, event-diverse MM dataset specifically\ncurated to support AI-based disaster response. It covers five types of natural\ndisasters and two types of man-made disasters across 14 regions worldwide, with\na particular focus on developing countries where external assistance is most\nneeded. The optical and SAR imagery in BRIGHT, with a spatial resolution\nbetween 0.3-1 meters, provides detailed representations of individual\nbuildings, making it ideal for precise BDA. In our experiments, we have tested\nseven advanced AI models trained with our BRIGHT to validate the\ntransferability and robustness. The dataset and code are available at\nhttps://github.com/ChenHongruixuan/BRIGHT. BRIGHT also serves as the official\ndataset for the 2025 IEEE GRSS Data Fusion Contest.\n", "link": "http://arxiv.org/abs/2501.06019v3", "date": "2025-04-18", "relevancy": 1.9409, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4933}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4921}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRIGHT%3A%20A%20globally%20distributed%20multimodal%20building%20damage%20assessment%0A%20%20dataset%20with%20very-high-resolution%20for%20all-weather%20disaster%20response&body=Title%3A%20BRIGHT%3A%20A%20globally%20distributed%20multimodal%20building%20damage%20assessment%0A%20%20dataset%20with%20very-high-resolution%20for%20all-weather%20disaster%20response%0AAuthor%3A%20Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Olivier%20Dietrich%20and%20Clifford%20Broni-Bediako%20and%20Weihao%20Xuan%20and%20Junjue%20Wang%20and%20Xinlei%20Shao%20and%20Yimin%20Wei%20and%20Junshi%20Xia%20and%20Cuiling%20Lan%20and%20Konrad%20Schindler%20and%20Naoto%20Yokoya%0AAbstract%3A%20%20%20Disaster%20events%20occur%20around%20the%20world%20and%20cause%20significant%20damage%20to%20human%0Alife%20and%20property.%20Earth%20observation%20%28EO%29%20data%20enables%20rapid%20and%20comprehensive%0Abuilding%20damage%20assessment%20%28BDA%29%2C%20an%20essential%20capability%20in%20the%20aftermath%20of%20a%0Adisaster%20to%20reduce%20human%20casualties%20and%20to%20inform%20disaster%20relief%20efforts.%0ARecent%20research%20focuses%20on%20the%20development%20of%20AI%20models%20to%20achieve%20accurate%0Amapping%20of%20unseen%20disaster%20events%2C%20mostly%20using%20optical%20EO%20data.%20However%2C%0Asolutions%20based%20on%20optical%20data%20are%20limited%20to%20clear%20skies%20and%20daylight%20hours%2C%0Apreventing%20a%20prompt%20response%20to%20disasters.%20Integrating%20multimodal%20%28MM%29%20EO%20data%2C%0Aparticularly%20the%20combination%20of%20optical%20and%20SAR%20imagery%2C%20makes%20it%20possible%20to%0Aprovide%20all-weather%2C%20day-and-night%20disaster%20responses.%20Despite%20this%20potential%2C%0Athe%20development%20of%20robust%20multimodal%20AI%20models%20has%20been%20constrained%20by%20the%20lack%0Aof%20suitable%20benchmark%20datasets.%20In%20this%20paper%2C%20we%20present%20a%20BDA%20dataset%20using%0AveRy-hIGH-resoluTion%20optical%20and%20SAR%20imagery%20%28BRIGHT%29%20to%20support%20AI-based%0Aall-weather%20disaster%20response.%20To%20the%20best%20of%20our%20knowledge%2C%20BRIGHT%20is%20the%0Afirst%20open-access%2C%20globally%20distributed%2C%20event-diverse%20MM%20dataset%20specifically%0Acurated%20to%20support%20AI-based%20disaster%20response.%20It%20covers%20five%20types%20of%20natural%0Adisasters%20and%20two%20types%20of%20man-made%20disasters%20across%2014%20regions%20worldwide%2C%20with%0Aa%20particular%20focus%20on%20developing%20countries%20where%20external%20assistance%20is%20most%0Aneeded.%20The%20optical%20and%20SAR%20imagery%20in%20BRIGHT%2C%20with%20a%20spatial%20resolution%0Abetween%200.3-1%20meters%2C%20provides%20detailed%20representations%20of%20individual%0Abuildings%2C%20making%20it%20ideal%20for%20precise%20BDA.%20In%20our%20experiments%2C%20we%20have%20tested%0Aseven%20advanced%20AI%20models%20trained%20with%20our%20BRIGHT%20to%20validate%20the%0Atransferability%20and%20robustness.%20The%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChenHongruixuan/BRIGHT.%20BRIGHT%20also%20serves%20as%20the%20official%0Adataset%20for%20the%202025%20IEEE%20GRSS%20Data%20Fusion%20Contest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06019v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRIGHT%253A%2520A%2520globally%2520distributed%2520multimodal%2520building%2520damage%2520assessment%250A%2520%2520dataset%2520with%2520very-high-resolution%2520for%2520all-weather%2520disaster%2520response%26entry.906535625%3DHongruixuan%2520Chen%2520and%2520Jian%2520Song%2520and%2520Olivier%2520Dietrich%2520and%2520Clifford%2520Broni-Bediako%2520and%2520Weihao%2520Xuan%2520and%2520Junjue%2520Wang%2520and%2520Xinlei%2520Shao%2520and%2520Yimin%2520Wei%2520and%2520Junshi%2520Xia%2520and%2520Cuiling%2520Lan%2520and%2520Konrad%2520Schindler%2520and%2520Naoto%2520Yokoya%26entry.1292438233%3D%2520%2520Disaster%2520events%2520occur%2520around%2520the%2520world%2520and%2520cause%2520significant%2520damage%2520to%2520human%250Alife%2520and%2520property.%2520Earth%2520observation%2520%2528EO%2529%2520data%2520enables%2520rapid%2520and%2520comprehensive%250Abuilding%2520damage%2520assessment%2520%2528BDA%2529%252C%2520an%2520essential%2520capability%2520in%2520the%2520aftermath%2520of%2520a%250Adisaster%2520to%2520reduce%2520human%2520casualties%2520and%2520to%2520inform%2520disaster%2520relief%2520efforts.%250ARecent%2520research%2520focuses%2520on%2520the%2520development%2520of%2520AI%2520models%2520to%2520achieve%2520accurate%250Amapping%2520of%2520unseen%2520disaster%2520events%252C%2520mostly%2520using%2520optical%2520EO%2520data.%2520However%252C%250Asolutions%2520based%2520on%2520optical%2520data%2520are%2520limited%2520to%2520clear%2520skies%2520and%2520daylight%2520hours%252C%250Apreventing%2520a%2520prompt%2520response%2520to%2520disasters.%2520Integrating%2520multimodal%2520%2528MM%2529%2520EO%2520data%252C%250Aparticularly%2520the%2520combination%2520of%2520optical%2520and%2520SAR%2520imagery%252C%2520makes%2520it%2520possible%2520to%250Aprovide%2520all-weather%252C%2520day-and-night%2520disaster%2520responses.%2520Despite%2520this%2520potential%252C%250Athe%2520development%2520of%2520robust%2520multimodal%2520AI%2520models%2520has%2520been%2520constrained%2520by%2520the%2520lack%250Aof%2520suitable%2520benchmark%2520datasets.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520BDA%2520dataset%2520using%250AveRy-hIGH-resoluTion%2520optical%2520and%2520SAR%2520imagery%2520%2528BRIGHT%2529%2520to%2520support%2520AI-based%250Aall-weather%2520disaster%2520response.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520BRIGHT%2520is%2520the%250Afirst%2520open-access%252C%2520globally%2520distributed%252C%2520event-diverse%2520MM%2520dataset%2520specifically%250Acurated%2520to%2520support%2520AI-based%2520disaster%2520response.%2520It%2520covers%2520five%2520types%2520of%2520natural%250Adisasters%2520and%2520two%2520types%2520of%2520man-made%2520disasters%2520across%252014%2520regions%2520worldwide%252C%2520with%250Aa%2520particular%2520focus%2520on%2520developing%2520countries%2520where%2520external%2520assistance%2520is%2520most%250Aneeded.%2520The%2520optical%2520and%2520SAR%2520imagery%2520in%2520BRIGHT%252C%2520with%2520a%2520spatial%2520resolution%250Abetween%25200.3-1%2520meters%252C%2520provides%2520detailed%2520representations%2520of%2520individual%250Abuildings%252C%2520making%2520it%2520ideal%2520for%2520precise%2520BDA.%2520In%2520our%2520experiments%252C%2520we%2520have%2520tested%250Aseven%2520advanced%2520AI%2520models%2520trained%2520with%2520our%2520BRIGHT%2520to%2520validate%2520the%250Atransferability%2520and%2520robustness.%2520The%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/ChenHongruixuan/BRIGHT.%2520BRIGHT%2520also%2520serves%2520as%2520the%2520official%250Adataset%2520for%2520the%25202025%2520IEEE%2520GRSS%2520Data%2520Fusion%2520Contest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06019v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRIGHT%3A%20A%20globally%20distributed%20multimodal%20building%20damage%20assessment%0A%20%20dataset%20with%20very-high-resolution%20for%20all-weather%20disaster%20response&entry.906535625=Hongruixuan%20Chen%20and%20Jian%20Song%20and%20Olivier%20Dietrich%20and%20Clifford%20Broni-Bediako%20and%20Weihao%20Xuan%20and%20Junjue%20Wang%20and%20Xinlei%20Shao%20and%20Yimin%20Wei%20and%20Junshi%20Xia%20and%20Cuiling%20Lan%20and%20Konrad%20Schindler%20and%20Naoto%20Yokoya&entry.1292438233=%20%20Disaster%20events%20occur%20around%20the%20world%20and%20cause%20significant%20damage%20to%20human%0Alife%20and%20property.%20Earth%20observation%20%28EO%29%20data%20enables%20rapid%20and%20comprehensive%0Abuilding%20damage%20assessment%20%28BDA%29%2C%20an%20essential%20capability%20in%20the%20aftermath%20of%20a%0Adisaster%20to%20reduce%20human%20casualties%20and%20to%20inform%20disaster%20relief%20efforts.%0ARecent%20research%20focuses%20on%20the%20development%20of%20AI%20models%20to%20achieve%20accurate%0Amapping%20of%20unseen%20disaster%20events%2C%20mostly%20using%20optical%20EO%20data.%20However%2C%0Asolutions%20based%20on%20optical%20data%20are%20limited%20to%20clear%20skies%20and%20daylight%20hours%2C%0Apreventing%20a%20prompt%20response%20to%20disasters.%20Integrating%20multimodal%20%28MM%29%20EO%20data%2C%0Aparticularly%20the%20combination%20of%20optical%20and%20SAR%20imagery%2C%20makes%20it%20possible%20to%0Aprovide%20all-weather%2C%20day-and-night%20disaster%20responses.%20Despite%20this%20potential%2C%0Athe%20development%20of%20robust%20multimodal%20AI%20models%20has%20been%20constrained%20by%20the%20lack%0Aof%20suitable%20benchmark%20datasets.%20In%20this%20paper%2C%20we%20present%20a%20BDA%20dataset%20using%0AveRy-hIGH-resoluTion%20optical%20and%20SAR%20imagery%20%28BRIGHT%29%20to%20support%20AI-based%0Aall-weather%20disaster%20response.%20To%20the%20best%20of%20our%20knowledge%2C%20BRIGHT%20is%20the%0Afirst%20open-access%2C%20globally%20distributed%2C%20event-diverse%20MM%20dataset%20specifically%0Acurated%20to%20support%20AI-based%20disaster%20response.%20It%20covers%20five%20types%20of%20natural%0Adisasters%20and%20two%20types%20of%20man-made%20disasters%20across%2014%20regions%20worldwide%2C%20with%0Aa%20particular%20focus%20on%20developing%20countries%20where%20external%20assistance%20is%20most%0Aneeded.%20The%20optical%20and%20SAR%20imagery%20in%20BRIGHT%2C%20with%20a%20spatial%20resolution%0Abetween%200.3-1%20meters%2C%20provides%20detailed%20representations%20of%20individual%0Abuildings%2C%20making%20it%20ideal%20for%20precise%20BDA.%20In%20our%20experiments%2C%20we%20have%20tested%0Aseven%20advanced%20AI%20models%20trained%20with%20our%20BRIGHT%20to%20validate%20the%0Atransferability%20and%20robustness.%20The%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/ChenHongruixuan/BRIGHT.%20BRIGHT%20also%20serves%20as%20the%20official%0Adataset%20for%20the%202025%20IEEE%20GRSS%20Data%20Fusion%20Contest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06019v3&entry.124074799=Read"},
{"title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to\n  Address Multimodal Hallucination", "author": "Hao Yin and Guangzong Si and Zilei Wang", "abstract": "  Contrastive decoding strategies are widely used to reduce hallucinations in\nmultimodal large language models (MLLMs). These methods work by constructing\ncontrastive samples to induce hallucinations and then suppressing them in the\noutput distribution. However, this paper demonstrates that such approaches fail\nto effectively mitigate the hallucination problem. The performance improvements\nobserved on POPE Benchmark are largely driven by two misleading factors: (1)\ncrude, unidirectional adjustments to the model's output distribution and (2)\nthe adaptive plausibility constraint, which reduces the sampling strategy to\ngreedy search. To further illustrate these issues, we introduce a series of\nspurious improvement methods and evaluate their performance against contrastive\ndecoding techniques. Experimental results reveal that the observed performance\ngains in contrastive decoding are entirely unrelated to its intended goal of\nmitigating hallucinations. Our findings challenge common assumptions about the\neffectiveness of contrastive decoding strategies and pave the way for\ndeveloping genuinely effective solutions to hallucinations in MLLMs.\n", "link": "http://arxiv.org/abs/2504.10020v2", "date": "2025-04-18", "relevancy": 1.9394, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4861}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Mirage%20of%20Performance%20Gains%3A%20Why%20Contrastive%20Decoding%20Fails%20to%0A%20%20Address%20Multimodal%20Hallucination&body=Title%3A%20The%20Mirage%20of%20Performance%20Gains%3A%20Why%20Contrastive%20Decoding%20Fails%20to%0A%20%20Address%20Multimodal%20Hallucination%0AAuthor%3A%20Hao%20Yin%20and%20Guangzong%20Si%20and%20Zilei%20Wang%0AAbstract%3A%20%20%20Contrastive%20decoding%20strategies%20are%20widely%20used%20to%20reduce%20hallucinations%20in%0Amultimodal%20large%20language%20models%20%28MLLMs%29.%20These%20methods%20work%20by%20constructing%0Acontrastive%20samples%20to%20induce%20hallucinations%20and%20then%20suppressing%20them%20in%20the%0Aoutput%20distribution.%20However%2C%20this%20paper%20demonstrates%20that%20such%20approaches%20fail%0Ato%20effectively%20mitigate%20the%20hallucination%20problem.%20The%20performance%20improvements%0Aobserved%20on%20POPE%20Benchmark%20are%20largely%20driven%20by%20two%20misleading%20factors%3A%20%281%29%0Acrude%2C%20unidirectional%20adjustments%20to%20the%20model%27s%20output%20distribution%20and%20%282%29%0Athe%20adaptive%20plausibility%20constraint%2C%20which%20reduces%20the%20sampling%20strategy%20to%0Agreedy%20search.%20To%20further%20illustrate%20these%20issues%2C%20we%20introduce%20a%20series%20of%0Aspurious%20improvement%20methods%20and%20evaluate%20their%20performance%20against%20contrastive%0Adecoding%20techniques.%20Experimental%20results%20reveal%20that%20the%20observed%20performance%0Agains%20in%20contrastive%20decoding%20are%20entirely%20unrelated%20to%20its%20intended%20goal%20of%0Amitigating%20hallucinations.%20Our%20findings%20challenge%20common%20assumptions%20about%20the%0Aeffectiveness%20of%20contrastive%20decoding%20strategies%20and%20pave%20the%20way%20for%0Adeveloping%20genuinely%20effective%20solutions%20to%20hallucinations%20in%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10020v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Mirage%2520of%2520Performance%2520Gains%253A%2520Why%2520Contrastive%2520Decoding%2520Fails%2520to%250A%2520%2520Address%2520Multimodal%2520Hallucination%26entry.906535625%3DHao%2520Yin%2520and%2520Guangzong%2520Si%2520and%2520Zilei%2520Wang%26entry.1292438233%3D%2520%2520Contrastive%2520decoding%2520strategies%2520are%2520widely%2520used%2520to%2520reduce%2520hallucinations%2520in%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520These%2520methods%2520work%2520by%2520constructing%250Acontrastive%2520samples%2520to%2520induce%2520hallucinations%2520and%2520then%2520suppressing%2520them%2520in%2520the%250Aoutput%2520distribution.%2520However%252C%2520this%2520paper%2520demonstrates%2520that%2520such%2520approaches%2520fail%250Ato%2520effectively%2520mitigate%2520the%2520hallucination%2520problem.%2520The%2520performance%2520improvements%250Aobserved%2520on%2520POPE%2520Benchmark%2520are%2520largely%2520driven%2520by%2520two%2520misleading%2520factors%253A%2520%25281%2529%250Acrude%252C%2520unidirectional%2520adjustments%2520to%2520the%2520model%2527s%2520output%2520distribution%2520and%2520%25282%2529%250Athe%2520adaptive%2520plausibility%2520constraint%252C%2520which%2520reduces%2520the%2520sampling%2520strategy%2520to%250Agreedy%2520search.%2520To%2520further%2520illustrate%2520these%2520issues%252C%2520we%2520introduce%2520a%2520series%2520of%250Aspurious%2520improvement%2520methods%2520and%2520evaluate%2520their%2520performance%2520against%2520contrastive%250Adecoding%2520techniques.%2520Experimental%2520results%2520reveal%2520that%2520the%2520observed%2520performance%250Agains%2520in%2520contrastive%2520decoding%2520are%2520entirely%2520unrelated%2520to%2520its%2520intended%2520goal%2520of%250Amitigating%2520hallucinations.%2520Our%2520findings%2520challenge%2520common%2520assumptions%2520about%2520the%250Aeffectiveness%2520of%2520contrastive%2520decoding%2520strategies%2520and%2520pave%2520the%2520way%2520for%250Adeveloping%2520genuinely%2520effective%2520solutions%2520to%2520hallucinations%2520in%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10020v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Mirage%20of%20Performance%20Gains%3A%20Why%20Contrastive%20Decoding%20Fails%20to%0A%20%20Address%20Multimodal%20Hallucination&entry.906535625=Hao%20Yin%20and%20Guangzong%20Si%20and%20Zilei%20Wang&entry.1292438233=%20%20Contrastive%20decoding%20strategies%20are%20widely%20used%20to%20reduce%20hallucinations%20in%0Amultimodal%20large%20language%20models%20%28MLLMs%29.%20These%20methods%20work%20by%20constructing%0Acontrastive%20samples%20to%20induce%20hallucinations%20and%20then%20suppressing%20them%20in%20the%0Aoutput%20distribution.%20However%2C%20this%20paper%20demonstrates%20that%20such%20approaches%20fail%0Ato%20effectively%20mitigate%20the%20hallucination%20problem.%20The%20performance%20improvements%0Aobserved%20on%20POPE%20Benchmark%20are%20largely%20driven%20by%20two%20misleading%20factors%3A%20%281%29%0Acrude%2C%20unidirectional%20adjustments%20to%20the%20model%27s%20output%20distribution%20and%20%282%29%0Athe%20adaptive%20plausibility%20constraint%2C%20which%20reduces%20the%20sampling%20strategy%20to%0Agreedy%20search.%20To%20further%20illustrate%20these%20issues%2C%20we%20introduce%20a%20series%20of%0Aspurious%20improvement%20methods%20and%20evaluate%20their%20performance%20against%20contrastive%0Adecoding%20techniques.%20Experimental%20results%20reveal%20that%20the%20observed%20performance%0Agains%20in%20contrastive%20decoding%20are%20entirely%20unrelated%20to%20its%20intended%20goal%20of%0Amitigating%20hallucinations.%20Our%20findings%20challenge%20common%20assumptions%20about%20the%0Aeffectiveness%20of%20contrastive%20decoding%20strategies%20and%20pave%20the%20way%20for%0Adeveloping%20genuinely%20effective%20solutions%20to%20hallucinations%20in%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10020v2&entry.124074799=Read"},
{"title": "Probabilistic Stability Guarantees for Feature Attributions", "author": "Helen Jin and Anton Xue and Weiqiu You and Surbhi Goel and Eric Wong", "abstract": "  Stability guarantees are an emerging tool for evaluating feature\nattributions, but existing certification methods rely on smoothed classifiers\nand often yield conservative guarantees. To address these limitations, we\nintroduce soft stability and propose a simple, model-agnostic, and\nsample-efficient stability certification algorithm (SCA) that provides\nnon-trivial and interpretable guarantees for any attribution. Moreover, we show\nthat mild smoothing enables a graceful tradeoff between accuracy and stability,\nin contrast to prior certification methods that require a more aggressive\ncompromise. Using Boolean function analysis, we give a novel characterization\nof stability under smoothing. We evaluate SCA on vision and language tasks, and\ndemonstrate the effectiveness of soft stability in measuring the robustness of\nexplanation methods.\n", "link": "http://arxiv.org/abs/2504.13787v1", "date": "2025-04-18", "relevancy": 1.9051, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5133}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4854}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Stability%20Guarantees%20for%20Feature%20Attributions&body=Title%3A%20Probabilistic%20Stability%20Guarantees%20for%20Feature%20Attributions%0AAuthor%3A%20Helen%20Jin%20and%20Anton%20Xue%20and%20Weiqiu%20You%20and%20Surbhi%20Goel%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Stability%20guarantees%20are%20an%20emerging%20tool%20for%20evaluating%20feature%0Aattributions%2C%20but%20existing%20certification%20methods%20rely%20on%20smoothed%20classifiers%0Aand%20often%20yield%20conservative%20guarantees.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20soft%20stability%20and%20propose%20a%20simple%2C%20model-agnostic%2C%20and%0Asample-efficient%20stability%20certification%20algorithm%20%28SCA%29%20that%20provides%0Anon-trivial%20and%20interpretable%20guarantees%20for%20any%20attribution.%20Moreover%2C%20we%20show%0Athat%20mild%20smoothing%20enables%20a%20graceful%20tradeoff%20between%20accuracy%20and%20stability%2C%0Ain%20contrast%20to%20prior%20certification%20methods%20that%20require%20a%20more%20aggressive%0Acompromise.%20Using%20Boolean%20function%20analysis%2C%20we%20give%20a%20novel%20characterization%0Aof%20stability%20under%20smoothing.%20We%20evaluate%20SCA%20on%20vision%20and%20language%20tasks%2C%20and%0Ademonstrate%20the%20effectiveness%20of%20soft%20stability%20in%20measuring%20the%20robustness%20of%0Aexplanation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Stability%2520Guarantees%2520for%2520Feature%2520Attributions%26entry.906535625%3DHelen%2520Jin%2520and%2520Anton%2520Xue%2520and%2520Weiqiu%2520You%2520and%2520Surbhi%2520Goel%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Stability%2520guarantees%2520are%2520an%2520emerging%2520tool%2520for%2520evaluating%2520feature%250Aattributions%252C%2520but%2520existing%2520certification%2520methods%2520rely%2520on%2520smoothed%2520classifiers%250Aand%2520often%2520yield%2520conservative%2520guarantees.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aintroduce%2520soft%2520stability%2520and%2520propose%2520a%2520simple%252C%2520model-agnostic%252C%2520and%250Asample-efficient%2520stability%2520certification%2520algorithm%2520%2528SCA%2529%2520that%2520provides%250Anon-trivial%2520and%2520interpretable%2520guarantees%2520for%2520any%2520attribution.%2520Moreover%252C%2520we%2520show%250Athat%2520mild%2520smoothing%2520enables%2520a%2520graceful%2520tradeoff%2520between%2520accuracy%2520and%2520stability%252C%250Ain%2520contrast%2520to%2520prior%2520certification%2520methods%2520that%2520require%2520a%2520more%2520aggressive%250Acompromise.%2520Using%2520Boolean%2520function%2520analysis%252C%2520we%2520give%2520a%2520novel%2520characterization%250Aof%2520stability%2520under%2520smoothing.%2520We%2520evaluate%2520SCA%2520on%2520vision%2520and%2520language%2520tasks%252C%2520and%250Ademonstrate%2520the%2520effectiveness%2520of%2520soft%2520stability%2520in%2520measuring%2520the%2520robustness%2520of%250Aexplanation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Stability%20Guarantees%20for%20Feature%20Attributions&entry.906535625=Helen%20Jin%20and%20Anton%20Xue%20and%20Weiqiu%20You%20and%20Surbhi%20Goel%20and%20Eric%20Wong&entry.1292438233=%20%20Stability%20guarantees%20are%20an%20emerging%20tool%20for%20evaluating%20feature%0Aattributions%2C%20but%20existing%20certification%20methods%20rely%20on%20smoothed%20classifiers%0Aand%20often%20yield%20conservative%20guarantees.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20soft%20stability%20and%20propose%20a%20simple%2C%20model-agnostic%2C%20and%0Asample-efficient%20stability%20certification%20algorithm%20%28SCA%29%20that%20provides%0Anon-trivial%20and%20interpretable%20guarantees%20for%20any%20attribution.%20Moreover%2C%20we%20show%0Athat%20mild%20smoothing%20enables%20a%20graceful%20tradeoff%20between%20accuracy%20and%20stability%2C%0Ain%20contrast%20to%20prior%20certification%20methods%20that%20require%20a%20more%20aggressive%0Acompromise.%20Using%20Boolean%20function%20analysis%2C%20we%20give%20a%20novel%20characterization%0Aof%20stability%20under%20smoothing.%20We%20evaluate%20SCA%20on%20vision%20and%20language%20tasks%2C%20and%0Ademonstrate%20the%20effectiveness%20of%20soft%20stability%20in%20measuring%20the%20robustness%20of%0Aexplanation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13787v1&entry.124074799=Read"},
{"title": "Large Language Models Will Change The Way Children Think About\n  Technology And Impact Every Interaction Paradigm", "author": "Russell Beale", "abstract": "  This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future.\n", "link": "http://arxiv.org/abs/2504.13667v1", "date": "2025-04-18", "relevancy": 1.8876, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Will%20Change%20The%20Way%20Children%20Think%20About%0A%20%20Technology%20And%20Impact%20Every%20Interaction%20Paradigm&body=Title%3A%20Large%20Language%20Models%20Will%20Change%20The%20Way%20Children%20Think%20About%0A%20%20Technology%20And%20Impact%20Every%20Interaction%20Paradigm%0AAuthor%3A%20Russell%20Beale%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20hopeful%20perspective%20on%20the%20potentially%20dramatic%20impacts%0Aof%20Large%20Language%20Models%20on%20how%20we%20children%20learn%20and%20how%20they%20will%20expect%20to%0Ainteract%20with%20technology.%20We%20review%20the%20effects%20of%20LLMs%20on%20education%20so%20far%2C%0Aand%20make%20the%20case%20that%20these%20effects%20are%20minor%20compared%20to%20the%20upcoming%20changes%0Athat%20are%20occurring.%20We%20present%20a%20small%20scenario%20and%20self-ethnographic%20study%0Ademonstrating%20the%20effects%20of%20these%20changes%2C%20and%20define%20five%20significant%0Aconsiderations%20that%20interactive%20systems%20designers%20will%20have%20to%20accommodate%20in%0Athe%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Will%2520Change%2520The%2520Way%2520Children%2520Think%2520About%250A%2520%2520Technology%2520And%2520Impact%2520Every%2520Interaction%2520Paradigm%26entry.906535625%3DRussell%2520Beale%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520hopeful%2520perspective%2520on%2520the%2520potentially%2520dramatic%2520impacts%250Aof%2520Large%2520Language%2520Models%2520on%2520how%2520we%2520children%2520learn%2520and%2520how%2520they%2520will%2520expect%2520to%250Ainteract%2520with%2520technology.%2520We%2520review%2520the%2520effects%2520of%2520LLMs%2520on%2520education%2520so%2520far%252C%250Aand%2520make%2520the%2520case%2520that%2520these%2520effects%2520are%2520minor%2520compared%2520to%2520the%2520upcoming%2520changes%250Athat%2520are%2520occurring.%2520We%2520present%2520a%2520small%2520scenario%2520and%2520self-ethnographic%2520study%250Ademonstrating%2520the%2520effects%2520of%2520these%2520changes%252C%2520and%2520define%2520five%2520significant%250Aconsiderations%2520that%2520interactive%2520systems%2520designers%2520will%2520have%2520to%2520accommodate%2520in%250Athe%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Will%20Change%20The%20Way%20Children%20Think%20About%0A%20%20Technology%20And%20Impact%20Every%20Interaction%20Paradigm&entry.906535625=Russell%20Beale&entry.1292438233=%20%20This%20paper%20presents%20a%20hopeful%20perspective%20on%20the%20potentially%20dramatic%20impacts%0Aof%20Large%20Language%20Models%20on%20how%20we%20children%20learn%20and%20how%20they%20will%20expect%20to%0Ainteract%20with%20technology.%20We%20review%20the%20effects%20of%20LLMs%20on%20education%20so%20far%2C%0Aand%20make%20the%20case%20that%20these%20effects%20are%20minor%20compared%20to%20the%20upcoming%20changes%0Athat%20are%20occurring.%20We%20present%20a%20small%20scenario%20and%20self-ethnographic%20study%0Ademonstrating%20the%20effects%20of%20these%20changes%2C%20and%20define%20five%20significant%0Aconsiderations%20that%20interactive%20systems%20designers%20will%20have%20to%20accommodate%20in%0Athe%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13667v1&entry.124074799=Read"},
{"title": "Feature Alignment and Representation Transfer in Knowledge Distillation\n  for Large Language Models", "author": "Junjie Yang and Junhao Song and Xudong Han and Ziqian Bi and Tianyang Wang and Chia Xin Liang and Xinyuan Song and Yichao Zhang and Qian Niu and Benji Peng and Keyu Chen and Ming Liu", "abstract": "  Knowledge distillation (KD) is a technique for transferring knowledge from\ncomplex teacher models to simpler student models, significantly enhancing model\nefficiency and accuracy. It has demonstrated substantial advancements in\nvarious applications including image classification, object detection, language\nmodeling, text classification, and sentiment analysis. Recent innovations in KD\nmethods, such as attention-based approaches, block-wise logit distillation, and\ndecoupling distillation, have notably improved student model performance. These\ntechniques focus on stimulus complexity, attention mechanisms, and global\ninformation capture to optimize knowledge transfer. In addition, KD has proven\neffective in compressing large language models while preserving accuracy,\nreducing computational overhead, and improving inference speed. This survey\nsynthesizes the latest literature, highlighting key findings, contributions,\nand future directions in knowledge distillation to provide insights for\nresearchers and practitioners on its evolving role in artificial intelligence\nand machine learning.\n", "link": "http://arxiv.org/abs/2504.13825v1", "date": "2025-04-18", "relevancy": 1.8778, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4792}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4703}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Alignment%20and%20Representation%20Transfer%20in%20Knowledge%20Distillation%0A%20%20for%20Large%20Language%20Models&body=Title%3A%20Feature%20Alignment%20and%20Representation%20Transfer%20in%20Knowledge%20Distillation%0A%20%20for%20Large%20Language%20Models%0AAuthor%3A%20Junjie%20Yang%20and%20Junhao%20Song%20and%20Xudong%20Han%20and%20Ziqian%20Bi%20and%20Tianyang%20Wang%20and%20Chia%20Xin%20Liang%20and%20Xinyuan%20Song%20and%20Yichao%20Zhang%20and%20Qian%20Niu%20and%20Benji%20Peng%20and%20Keyu%20Chen%20and%20Ming%20Liu%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20is%20a%20technique%20for%20transferring%20knowledge%20from%0Acomplex%20teacher%20models%20to%20simpler%20student%20models%2C%20significantly%20enhancing%20model%0Aefficiency%20and%20accuracy.%20It%20has%20demonstrated%20substantial%20advancements%20in%0Avarious%20applications%20including%20image%20classification%2C%20object%20detection%2C%20language%0Amodeling%2C%20text%20classification%2C%20and%20sentiment%20analysis.%20Recent%20innovations%20in%20KD%0Amethods%2C%20such%20as%20attention-based%20approaches%2C%20block-wise%20logit%20distillation%2C%20and%0Adecoupling%20distillation%2C%20have%20notably%20improved%20student%20model%20performance.%20These%0Atechniques%20focus%20on%20stimulus%20complexity%2C%20attention%20mechanisms%2C%20and%20global%0Ainformation%20capture%20to%20optimize%20knowledge%20transfer.%20In%20addition%2C%20KD%20has%20proven%0Aeffective%20in%20compressing%20large%20language%20models%20while%20preserving%20accuracy%2C%0Areducing%20computational%20overhead%2C%20and%20improving%20inference%20speed.%20This%20survey%0Asynthesizes%20the%20latest%20literature%2C%20highlighting%20key%20findings%2C%20contributions%2C%0Aand%20future%20directions%20in%20knowledge%20distillation%20to%20provide%20insights%20for%0Aresearchers%20and%20practitioners%20on%20its%20evolving%20role%20in%20artificial%20intelligence%0Aand%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Alignment%2520and%2520Representation%2520Transfer%2520in%2520Knowledge%2520Distillation%250A%2520%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DJunjie%2520Yang%2520and%2520Junhao%2520Song%2520and%2520Xudong%2520Han%2520and%2520Ziqian%2520Bi%2520and%2520Tianyang%2520Wang%2520and%2520Chia%2520Xin%2520Liang%2520and%2520Xinyuan%2520Song%2520and%2520Yichao%2520Zhang%2520and%2520Qian%2520Niu%2520and%2520Benji%2520Peng%2520and%2520Keyu%2520Chen%2520and%2520Ming%2520Liu%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520is%2520a%2520technique%2520for%2520transferring%2520knowledge%2520from%250Acomplex%2520teacher%2520models%2520to%2520simpler%2520student%2520models%252C%2520significantly%2520enhancing%2520model%250Aefficiency%2520and%2520accuracy.%2520It%2520has%2520demonstrated%2520substantial%2520advancements%2520in%250Avarious%2520applications%2520including%2520image%2520classification%252C%2520object%2520detection%252C%2520language%250Amodeling%252C%2520text%2520classification%252C%2520and%2520sentiment%2520analysis.%2520Recent%2520innovations%2520in%2520KD%250Amethods%252C%2520such%2520as%2520attention-based%2520approaches%252C%2520block-wise%2520logit%2520distillation%252C%2520and%250Adecoupling%2520distillation%252C%2520have%2520notably%2520improved%2520student%2520model%2520performance.%2520These%250Atechniques%2520focus%2520on%2520stimulus%2520complexity%252C%2520attention%2520mechanisms%252C%2520and%2520global%250Ainformation%2520capture%2520to%2520optimize%2520knowledge%2520transfer.%2520In%2520addition%252C%2520KD%2520has%2520proven%250Aeffective%2520in%2520compressing%2520large%2520language%2520models%2520while%2520preserving%2520accuracy%252C%250Areducing%2520computational%2520overhead%252C%2520and%2520improving%2520inference%2520speed.%2520This%2520survey%250Asynthesizes%2520the%2520latest%2520literature%252C%2520highlighting%2520key%2520findings%252C%2520contributions%252C%250Aand%2520future%2520directions%2520in%2520knowledge%2520distillation%2520to%2520provide%2520insights%2520for%250Aresearchers%2520and%2520practitioners%2520on%2520its%2520evolving%2520role%2520in%2520artificial%2520intelligence%250Aand%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Alignment%20and%20Representation%20Transfer%20in%20Knowledge%20Distillation%0A%20%20for%20Large%20Language%20Models&entry.906535625=Junjie%20Yang%20and%20Junhao%20Song%20and%20Xudong%20Han%20and%20Ziqian%20Bi%20and%20Tianyang%20Wang%20and%20Chia%20Xin%20Liang%20and%20Xinyuan%20Song%20and%20Yichao%20Zhang%20and%20Qian%20Niu%20and%20Benji%20Peng%20and%20Keyu%20Chen%20and%20Ming%20Liu&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20is%20a%20technique%20for%20transferring%20knowledge%20from%0Acomplex%20teacher%20models%20to%20simpler%20student%20models%2C%20significantly%20enhancing%20model%0Aefficiency%20and%20accuracy.%20It%20has%20demonstrated%20substantial%20advancements%20in%0Avarious%20applications%20including%20image%20classification%2C%20object%20detection%2C%20language%0Amodeling%2C%20text%20classification%2C%20and%20sentiment%20analysis.%20Recent%20innovations%20in%20KD%0Amethods%2C%20such%20as%20attention-based%20approaches%2C%20block-wise%20logit%20distillation%2C%20and%0Adecoupling%20distillation%2C%20have%20notably%20improved%20student%20model%20performance.%20These%0Atechniques%20focus%20on%20stimulus%20complexity%2C%20attention%20mechanisms%2C%20and%20global%0Ainformation%20capture%20to%20optimize%20knowledge%20transfer.%20In%20addition%2C%20KD%20has%20proven%0Aeffective%20in%20compressing%20large%20language%20models%20while%20preserving%20accuracy%2C%0Areducing%20computational%20overhead%2C%20and%20improving%20inference%20speed.%20This%20survey%0Asynthesizes%20the%20latest%20literature%2C%20highlighting%20key%20findings%2C%20contributions%2C%0Aand%20future%20directions%20in%20knowledge%20distillation%20to%20provide%20insights%20for%0Aresearchers%20and%20practitioners%20on%20its%20evolving%20role%20in%20artificial%20intelligence%0Aand%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13825v1&entry.124074799=Read"},
{"title": "Efficient algorithms for the Hadamard decomposition", "author": "Samuel Wertz and Arnaud Vandaele and Nicolas Gillis", "abstract": "  The Hadamard decomposition is a powerful technique for data analysis and\nmatrix compression, which decomposes a given matrix into the element-wise\nproduct of two or more low-rank matrices. In this paper, we develop an\nefficient algorithm to solve this problem, leveraging an alternating\noptimization approach that decomposes the global non-convex problem into a\nseries of convex sub-problems. To improve performance, we explore advanced\ninitialization strategies inspired by the singular value decomposition (SVD)\nand incorporate acceleration techniques by introducing momentum-based updates.\nBeyond optimizing the two-matrix case, we also extend the Hadamard\ndecomposition framework to support more than two low-rank matrices, enabling\napproximations with higher effective ranks while preserving computational\nefficiency. Finally, we conduct extensive experiments to compare our method\nwith the existing gradient descent-based approaches for the Hadamard\ndecomposition and with traditional low-rank approximation techniques. The\nresults highlight the effectiveness of our proposed method across diverse\ndatasets.\n", "link": "http://arxiv.org/abs/2504.13633v1", "date": "2025-04-18", "relevancy": 1.838, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4639}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4628}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20algorithms%20for%20the%20Hadamard%20decomposition&body=Title%3A%20Efficient%20algorithms%20for%20the%20Hadamard%20decomposition%0AAuthor%3A%20Samuel%20Wertz%20and%20Arnaud%20Vandaele%20and%20Nicolas%20Gillis%0AAbstract%3A%20%20%20The%20Hadamard%20decomposition%20is%20a%20powerful%20technique%20for%20data%20analysis%20and%0Amatrix%20compression%2C%20which%20decomposes%20a%20given%20matrix%20into%20the%20element-wise%0Aproduct%20of%20two%20or%20more%20low-rank%20matrices.%20In%20this%20paper%2C%20we%20develop%20an%0Aefficient%20algorithm%20to%20solve%20this%20problem%2C%20leveraging%20an%20alternating%0Aoptimization%20approach%20that%20decomposes%20the%20global%20non-convex%20problem%20into%20a%0Aseries%20of%20convex%20sub-problems.%20To%20improve%20performance%2C%20we%20explore%20advanced%0Ainitialization%20strategies%20inspired%20by%20the%20singular%20value%20decomposition%20%28SVD%29%0Aand%20incorporate%20acceleration%20techniques%20by%20introducing%20momentum-based%20updates.%0ABeyond%20optimizing%20the%20two-matrix%20case%2C%20we%20also%20extend%20the%20Hadamard%0Adecomposition%20framework%20to%20support%20more%20than%20two%20low-rank%20matrices%2C%20enabling%0Aapproximations%20with%20higher%20effective%20ranks%20while%20preserving%20computational%0Aefficiency.%20Finally%2C%20we%20conduct%20extensive%20experiments%20to%20compare%20our%20method%0Awith%20the%20existing%20gradient%20descent-based%20approaches%20for%20the%20Hadamard%0Adecomposition%20and%20with%20traditional%20low-rank%20approximation%20techniques.%20The%0Aresults%20highlight%20the%20effectiveness%20of%20our%20proposed%20method%20across%20diverse%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520algorithms%2520for%2520the%2520Hadamard%2520decomposition%26entry.906535625%3DSamuel%2520Wertz%2520and%2520Arnaud%2520Vandaele%2520and%2520Nicolas%2520Gillis%26entry.1292438233%3D%2520%2520The%2520Hadamard%2520decomposition%2520is%2520a%2520powerful%2520technique%2520for%2520data%2520analysis%2520and%250Amatrix%2520compression%252C%2520which%2520decomposes%2520a%2520given%2520matrix%2520into%2520the%2520element-wise%250Aproduct%2520of%2520two%2520or%2520more%2520low-rank%2520matrices.%2520In%2520this%2520paper%252C%2520we%2520develop%2520an%250Aefficient%2520algorithm%2520to%2520solve%2520this%2520problem%252C%2520leveraging%2520an%2520alternating%250Aoptimization%2520approach%2520that%2520decomposes%2520the%2520global%2520non-convex%2520problem%2520into%2520a%250Aseries%2520of%2520convex%2520sub-problems.%2520To%2520improve%2520performance%252C%2520we%2520explore%2520advanced%250Ainitialization%2520strategies%2520inspired%2520by%2520the%2520singular%2520value%2520decomposition%2520%2528SVD%2529%250Aand%2520incorporate%2520acceleration%2520techniques%2520by%2520introducing%2520momentum-based%2520updates.%250ABeyond%2520optimizing%2520the%2520two-matrix%2520case%252C%2520we%2520also%2520extend%2520the%2520Hadamard%250Adecomposition%2520framework%2520to%2520support%2520more%2520than%2520two%2520low-rank%2520matrices%252C%2520enabling%250Aapproximations%2520with%2520higher%2520effective%2520ranks%2520while%2520preserving%2520computational%250Aefficiency.%2520Finally%252C%2520we%2520conduct%2520extensive%2520experiments%2520to%2520compare%2520our%2520method%250Awith%2520the%2520existing%2520gradient%2520descent-based%2520approaches%2520for%2520the%2520Hadamard%250Adecomposition%2520and%2520with%2520traditional%2520low-rank%2520approximation%2520techniques.%2520The%250Aresults%2520highlight%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%2520across%2520diverse%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20algorithms%20for%20the%20Hadamard%20decomposition&entry.906535625=Samuel%20Wertz%20and%20Arnaud%20Vandaele%20and%20Nicolas%20Gillis&entry.1292438233=%20%20The%20Hadamard%20decomposition%20is%20a%20powerful%20technique%20for%20data%20analysis%20and%0Amatrix%20compression%2C%20which%20decomposes%20a%20given%20matrix%20into%20the%20element-wise%0Aproduct%20of%20two%20or%20more%20low-rank%20matrices.%20In%20this%20paper%2C%20we%20develop%20an%0Aefficient%20algorithm%20to%20solve%20this%20problem%2C%20leveraging%20an%20alternating%0Aoptimization%20approach%20that%20decomposes%20the%20global%20non-convex%20problem%20into%20a%0Aseries%20of%20convex%20sub-problems.%20To%20improve%20performance%2C%20we%20explore%20advanced%0Ainitialization%20strategies%20inspired%20by%20the%20singular%20value%20decomposition%20%28SVD%29%0Aand%20incorporate%20acceleration%20techniques%20by%20introducing%20momentum-based%20updates.%0ABeyond%20optimizing%20the%20two-matrix%20case%2C%20we%20also%20extend%20the%20Hadamard%0Adecomposition%20framework%20to%20support%20more%20than%20two%20low-rank%20matrices%2C%20enabling%0Aapproximations%20with%20higher%20effective%20ranks%20while%20preserving%20computational%0Aefficiency.%20Finally%2C%20we%20conduct%20extensive%20experiments%20to%20compare%20our%20method%0Awith%20the%20existing%20gradient%20descent-based%20approaches%20for%20the%20Hadamard%0Adecomposition%20and%20with%20traditional%20low-rank%20approximation%20techniques.%20The%0Aresults%20highlight%20the%20effectiveness%20of%20our%20proposed%20method%20across%20diverse%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13633v1&entry.124074799=Read"},
{"title": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class\n  Trajectory Prediction", "author": "Yushen He and Lei Zhao and Tianchen Deng and Zipeng Fang and Weidong Chen", "abstract": "  Service mobile robots are often required to avoid dynamic objects while\nperforming their tasks, but they usually have only limited computational\nresources. So we present a lightweight multi-modal framework for 3D object\ndetection and trajectory prediction. Our system synergistically integrates\nLiDAR and camera inputs to achieve real-time perception of pedestrians,\nvehicles, and riders in 3D space. The framework proposes two novel modules: 1)\na Cross-Modal Deformable Transformer (CMDT) for object detection with high\naccuracy and acceptable amount of computation, and 2) a Reference\nTrajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse\ntrajectory prediction of mult-class objects with flexible trajectory lengths.\nEvaluations on the CODa benchmark demonstrate superior performance over\nexisting methods across detection (+2.03% in mAP) and trajectory prediction\n(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits\nexceptional deployability - when implemented on a wheelchair robot with an\nentry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To\nfacilitate reproducibility and practical deployment, we release the related\ncode of the method at https://github.com/TossherO/3D_Perception and its ROS\ninference version at https://github.com/TossherO/ros_packages.\n", "link": "http://arxiv.org/abs/2504.13647v1", "date": "2025-04-18", "relevancy": 1.8243, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6696}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20LiDAR-Camera%203D%20Dynamic%20Object%20Detection%20and%20Multi-Class%0A%20%20Trajectory%20Prediction&body=Title%3A%20Lightweight%20LiDAR-Camera%203D%20Dynamic%20Object%20Detection%20and%20Multi-Class%0A%20%20Trajectory%20Prediction%0AAuthor%3A%20Yushen%20He%20and%20Lei%20Zhao%20and%20Tianchen%20Deng%20and%20Zipeng%20Fang%20and%20Weidong%20Chen%0AAbstract%3A%20%20%20Service%20mobile%20robots%20are%20often%20required%20to%20avoid%20dynamic%20objects%20while%0Aperforming%20their%20tasks%2C%20but%20they%20usually%20have%20only%20limited%20computational%0Aresources.%20So%20we%20present%20a%20lightweight%20multi-modal%20framework%20for%203D%20object%0Adetection%20and%20trajectory%20prediction.%20Our%20system%20synergistically%20integrates%0ALiDAR%20and%20camera%20inputs%20to%20achieve%20real-time%20perception%20of%20pedestrians%2C%0Avehicles%2C%20and%20riders%20in%203D%20space.%20The%20framework%20proposes%20two%20novel%20modules%3A%201%29%0Aa%20Cross-Modal%20Deformable%20Transformer%20%28CMDT%29%20for%20object%20detection%20with%20high%0Aaccuracy%20and%20acceptable%20amount%20of%20computation%2C%20and%202%29%20a%20Reference%0ATrajectory-based%20Multi-Class%20Transformer%20%28RTMCT%29%20for%20efficient%20and%20diverse%0Atrajectory%20prediction%20of%20mult-class%20objects%20with%20flexible%20trajectory%20lengths.%0AEvaluations%20on%20the%20CODa%20benchmark%20demonstrate%20superior%20performance%20over%0Aexisting%20methods%20across%20detection%20%28%2B2.03%25%20in%20mAP%29%20and%20trajectory%20prediction%0A%28-0.408m%20in%20minADE5%20of%20pedestrians%29%20metrics.%20Remarkably%2C%20the%20system%20exhibits%0Aexceptional%20deployability%20-%20when%20implemented%20on%20a%20wheelchair%20robot%20with%20an%0Aentry-level%20NVIDIA%203060%20GPU%2C%20it%20achieves%20real-time%20inference%20at%2013.2%20fps.%20To%0Afacilitate%20reproducibility%20and%20practical%20deployment%2C%20we%20release%20the%20related%0Acode%20of%20the%20method%20at%20https%3A//github.com/TossherO/3D_Perception%20and%20its%20ROS%0Ainference%20version%20at%20https%3A//github.com/TossherO/ros_packages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520LiDAR-Camera%25203D%2520Dynamic%2520Object%2520Detection%2520and%2520Multi-Class%250A%2520%2520Trajectory%2520Prediction%26entry.906535625%3DYushen%2520He%2520and%2520Lei%2520Zhao%2520and%2520Tianchen%2520Deng%2520and%2520Zipeng%2520Fang%2520and%2520Weidong%2520Chen%26entry.1292438233%3D%2520%2520Service%2520mobile%2520robots%2520are%2520often%2520required%2520to%2520avoid%2520dynamic%2520objects%2520while%250Aperforming%2520their%2520tasks%252C%2520but%2520they%2520usually%2520have%2520only%2520limited%2520computational%250Aresources.%2520So%2520we%2520present%2520a%2520lightweight%2520multi-modal%2520framework%2520for%25203D%2520object%250Adetection%2520and%2520trajectory%2520prediction.%2520Our%2520system%2520synergistically%2520integrates%250ALiDAR%2520and%2520camera%2520inputs%2520to%2520achieve%2520real-time%2520perception%2520of%2520pedestrians%252C%250Avehicles%252C%2520and%2520riders%2520in%25203D%2520space.%2520The%2520framework%2520proposes%2520two%2520novel%2520modules%253A%25201%2529%250Aa%2520Cross-Modal%2520Deformable%2520Transformer%2520%2528CMDT%2529%2520for%2520object%2520detection%2520with%2520high%250Aaccuracy%2520and%2520acceptable%2520amount%2520of%2520computation%252C%2520and%25202%2529%2520a%2520Reference%250ATrajectory-based%2520Multi-Class%2520Transformer%2520%2528RTMCT%2529%2520for%2520efficient%2520and%2520diverse%250Atrajectory%2520prediction%2520of%2520mult-class%2520objects%2520with%2520flexible%2520trajectory%2520lengths.%250AEvaluations%2520on%2520the%2520CODa%2520benchmark%2520demonstrate%2520superior%2520performance%2520over%250Aexisting%2520methods%2520across%2520detection%2520%2528%252B2.03%2525%2520in%2520mAP%2529%2520and%2520trajectory%2520prediction%250A%2528-0.408m%2520in%2520minADE5%2520of%2520pedestrians%2529%2520metrics.%2520Remarkably%252C%2520the%2520system%2520exhibits%250Aexceptional%2520deployability%2520-%2520when%2520implemented%2520on%2520a%2520wheelchair%2520robot%2520with%2520an%250Aentry-level%2520NVIDIA%25203060%2520GPU%252C%2520it%2520achieves%2520real-time%2520inference%2520at%252013.2%2520fps.%2520To%250Afacilitate%2520reproducibility%2520and%2520practical%2520deployment%252C%2520we%2520release%2520the%2520related%250Acode%2520of%2520the%2520method%2520at%2520https%253A//github.com/TossherO/3D_Perception%2520and%2520its%2520ROS%250Ainference%2520version%2520at%2520https%253A//github.com/TossherO/ros_packages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20LiDAR-Camera%203D%20Dynamic%20Object%20Detection%20and%20Multi-Class%0A%20%20Trajectory%20Prediction&entry.906535625=Yushen%20He%20and%20Lei%20Zhao%20and%20Tianchen%20Deng%20and%20Zipeng%20Fang%20and%20Weidong%20Chen&entry.1292438233=%20%20Service%20mobile%20robots%20are%20often%20required%20to%20avoid%20dynamic%20objects%20while%0Aperforming%20their%20tasks%2C%20but%20they%20usually%20have%20only%20limited%20computational%0Aresources.%20So%20we%20present%20a%20lightweight%20multi-modal%20framework%20for%203D%20object%0Adetection%20and%20trajectory%20prediction.%20Our%20system%20synergistically%20integrates%0ALiDAR%20and%20camera%20inputs%20to%20achieve%20real-time%20perception%20of%20pedestrians%2C%0Avehicles%2C%20and%20riders%20in%203D%20space.%20The%20framework%20proposes%20two%20novel%20modules%3A%201%29%0Aa%20Cross-Modal%20Deformable%20Transformer%20%28CMDT%29%20for%20object%20detection%20with%20high%0Aaccuracy%20and%20acceptable%20amount%20of%20computation%2C%20and%202%29%20a%20Reference%0ATrajectory-based%20Multi-Class%20Transformer%20%28RTMCT%29%20for%20efficient%20and%20diverse%0Atrajectory%20prediction%20of%20mult-class%20objects%20with%20flexible%20trajectory%20lengths.%0AEvaluations%20on%20the%20CODa%20benchmark%20demonstrate%20superior%20performance%20over%0Aexisting%20methods%20across%20detection%20%28%2B2.03%25%20in%20mAP%29%20and%20trajectory%20prediction%0A%28-0.408m%20in%20minADE5%20of%20pedestrians%29%20metrics.%20Remarkably%2C%20the%20system%20exhibits%0Aexceptional%20deployability%20-%20when%20implemented%20on%20a%20wheelchair%20robot%20with%20an%0Aentry-level%20NVIDIA%203060%20GPU%2C%20it%20achieves%20real-time%20inference%20at%2013.2%20fps.%20To%0Afacilitate%20reproducibility%20and%20practical%20deployment%2C%20we%20release%20the%20related%0Acode%20of%20the%20method%20at%20https%3A//github.com/TossherO/3D_Perception%20and%20its%20ROS%0Ainference%20version%20at%20https%3A//github.com/TossherO/ros_packages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13647v1&entry.124074799=Read"},
{"title": "Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel\n  Fields", "author": "Yuhang Huang and SHilong Zou and Xinwang Liu and Kai Xu", "abstract": "  This paper presents a novel latent 3D diffusion model for the generation of\nneural voxel fields, aiming to achieve accurate part-aware structures. Compared\nto existing methods, there are two key designs to ensure high-quality and\naccurate part-aware generation. On one hand, we introduce a latent 3D diffusion\nprocess for neural voxel fields, enabling generation at significantly higher\nresolutions that can accurately capture rich textural and geometric details. On\nthe other hand, a part-aware shape decoder is introduced to integrate the part\ncodes into the neural voxel fields, guiding the accurate part decomposition and\nproducing high-quality rendering results. Through extensive experimentation and\ncomparisons with state-of-the-art methods, we evaluate our approach across four\ndifferent classes of data. The results demonstrate the superior generative\ncapabilities of our proposed method in part-aware shape generation,\noutperforming existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.00998v4", "date": "2025-04-18", "relevancy": 1.8212, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6365}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6023}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Part-aware%20Shape%20Generation%20with%20Latent%203D%20Diffusion%20of%20Neural%20Voxel%0A%20%20Fields&body=Title%3A%20Part-aware%20Shape%20Generation%20with%20Latent%203D%20Diffusion%20of%20Neural%20Voxel%0A%20%20Fields%0AAuthor%3A%20Yuhang%20Huang%20and%20SHilong%20Zou%20and%20Xinwang%20Liu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20latent%203D%20diffusion%20model%20for%20the%20generation%20of%0Aneural%20voxel%20fields%2C%20aiming%20to%20achieve%20accurate%20part-aware%20structures.%20Compared%0Ato%20existing%20methods%2C%20there%20are%20two%20key%20designs%20to%20ensure%20high-quality%20and%0Aaccurate%20part-aware%20generation.%20On%20one%20hand%2C%20we%20introduce%20a%20latent%203D%20diffusion%0Aprocess%20for%20neural%20voxel%20fields%2C%20enabling%20generation%20at%20significantly%20higher%0Aresolutions%20that%20can%20accurately%20capture%20rich%20textural%20and%20geometric%20details.%20On%0Athe%20other%20hand%2C%20a%20part-aware%20shape%20decoder%20is%20introduced%20to%20integrate%20the%20part%0Acodes%20into%20the%20neural%20voxel%20fields%2C%20guiding%20the%20accurate%20part%20decomposition%20and%0Aproducing%20high-quality%20rendering%20results.%20Through%20extensive%20experimentation%20and%0Acomparisons%20with%20state-of-the-art%20methods%2C%20we%20evaluate%20our%20approach%20across%20four%0Adifferent%20classes%20of%20data.%20The%20results%20demonstrate%20the%20superior%20generative%0Acapabilities%20of%20our%20proposed%20method%20in%20part-aware%20shape%20generation%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00998v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPart-aware%2520Shape%2520Generation%2520with%2520Latent%25203D%2520Diffusion%2520of%2520Neural%2520Voxel%250A%2520%2520Fields%26entry.906535625%3DYuhang%2520Huang%2520and%2520SHilong%2520Zou%2520and%2520Xinwang%2520Liu%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520latent%25203D%2520diffusion%2520model%2520for%2520the%2520generation%2520of%250Aneural%2520voxel%2520fields%252C%2520aiming%2520to%2520achieve%2520accurate%2520part-aware%2520structures.%2520Compared%250Ato%2520existing%2520methods%252C%2520there%2520are%2520two%2520key%2520designs%2520to%2520ensure%2520high-quality%2520and%250Aaccurate%2520part-aware%2520generation.%2520On%2520one%2520hand%252C%2520we%2520introduce%2520a%2520latent%25203D%2520diffusion%250Aprocess%2520for%2520neural%2520voxel%2520fields%252C%2520enabling%2520generation%2520at%2520significantly%2520higher%250Aresolutions%2520that%2520can%2520accurately%2520capture%2520rich%2520textural%2520and%2520geometric%2520details.%2520On%250Athe%2520other%2520hand%252C%2520a%2520part-aware%2520shape%2520decoder%2520is%2520introduced%2520to%2520integrate%2520the%2520part%250Acodes%2520into%2520the%2520neural%2520voxel%2520fields%252C%2520guiding%2520the%2520accurate%2520part%2520decomposition%2520and%250Aproducing%2520high-quality%2520rendering%2520results.%2520Through%2520extensive%2520experimentation%2520and%250Acomparisons%2520with%2520state-of-the-art%2520methods%252C%2520we%2520evaluate%2520our%2520approach%2520across%2520four%250Adifferent%2520classes%2520of%2520data.%2520The%2520results%2520demonstrate%2520the%2520superior%2520generative%250Acapabilities%2520of%2520our%2520proposed%2520method%2520in%2520part-aware%2520shape%2520generation%252C%250Aoutperforming%2520existing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00998v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-aware%20Shape%20Generation%20with%20Latent%203D%20Diffusion%20of%20Neural%20Voxel%0A%20%20Fields&entry.906535625=Yuhang%20Huang%20and%20SHilong%20Zou%20and%20Xinwang%20Liu%20and%20Kai%20Xu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20latent%203D%20diffusion%20model%20for%20the%20generation%20of%0Aneural%20voxel%20fields%2C%20aiming%20to%20achieve%20accurate%20part-aware%20structures.%20Compared%0Ato%20existing%20methods%2C%20there%20are%20two%20key%20designs%20to%20ensure%20high-quality%20and%0Aaccurate%20part-aware%20generation.%20On%20one%20hand%2C%20we%20introduce%20a%20latent%203D%20diffusion%0Aprocess%20for%20neural%20voxel%20fields%2C%20enabling%20generation%20at%20significantly%20higher%0Aresolutions%20that%20can%20accurately%20capture%20rich%20textural%20and%20geometric%20details.%20On%0Athe%20other%20hand%2C%20a%20part-aware%20shape%20decoder%20is%20introduced%20to%20integrate%20the%20part%0Acodes%20into%20the%20neural%20voxel%20fields%2C%20guiding%20the%20accurate%20part%20decomposition%20and%0Aproducing%20high-quality%20rendering%20results.%20Through%20extensive%20experimentation%20and%0Acomparisons%20with%20state-of-the-art%20methods%2C%20we%20evaluate%20our%20approach%20across%20four%0Adifferent%20classes%20of%20data.%20The%20results%20demonstrate%20the%20superior%20generative%0Acapabilities%20of%20our%20proposed%20method%20in%20part-aware%20shape%20generation%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00998v4&entry.124074799=Read"},
{"title": "Reinforcement Learning with Graph Attention for Routing and Wavelength\n  Assignment with Lightpath Reuse", "author": "Michael Doherty and Alejandra Beghelli", "abstract": "  Many works have investigated reinforcement learning (RL) for routing and\nspectrum assignment on flex-grid networks but only one work to date has\nexamined RL for fixed-grid with flex-rate transponders, despite production\nsystems using this paradigm. Flex-rate transponders allow existing lightpaths\nto accommodate new services, a task we term routing and wavelength assignment\nwith lightpath reuse (RWA-LR). We re-examine this problem and present a\nthorough benchmarking of heuristic algorithms for RWA-LR, which are shown to\nhave 6% increased throughput when candidate paths are ordered by number of\nhops, rather than total length. We train an RL agent for RWA-LR with graph\nattention networks for the policy and value functions to exploit the\ngraph-structured data. We provide details of our methodology and open source\nall of our code for reproduction. We outperform the previous state-of-the-art\nRL approach by 2.5% (17.4 Tbps mean additional throughput) and the best\nheuristic by 1.2% (8.5 Tbps mean additional throughput). This marginal gain\nhighlights the difficulty in learning effective RL policies on long horizon\nresource allocation tasks.\n", "link": "http://arxiv.org/abs/2502.14741v2", "date": "2025-04-18", "relevancy": 1.8, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4652}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.441}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20with%20Graph%20Attention%20for%20Routing%20and%20Wavelength%0A%20%20Assignment%20with%20Lightpath%20Reuse&body=Title%3A%20Reinforcement%20Learning%20with%20Graph%20Attention%20for%20Routing%20and%20Wavelength%0A%20%20Assignment%20with%20Lightpath%20Reuse%0AAuthor%3A%20Michael%20Doherty%20and%20Alejandra%20Beghelli%0AAbstract%3A%20%20%20Many%20works%20have%20investigated%20reinforcement%20learning%20%28RL%29%20for%20routing%20and%0Aspectrum%20assignment%20on%20flex-grid%20networks%20but%20only%20one%20work%20to%20date%20has%0Aexamined%20RL%20for%20fixed-grid%20with%20flex-rate%20transponders%2C%20despite%20production%0Asystems%20using%20this%20paradigm.%20Flex-rate%20transponders%20allow%20existing%20lightpaths%0Ato%20accommodate%20new%20services%2C%20a%20task%20we%20term%20routing%20and%20wavelength%20assignment%0Awith%20lightpath%20reuse%20%28RWA-LR%29.%20We%20re-examine%20this%20problem%20and%20present%20a%0Athorough%20benchmarking%20of%20heuristic%20algorithms%20for%20RWA-LR%2C%20which%20are%20shown%20to%0Ahave%206%25%20increased%20throughput%20when%20candidate%20paths%20are%20ordered%20by%20number%20of%0Ahops%2C%20rather%20than%20total%20length.%20We%20train%20an%20RL%20agent%20for%20RWA-LR%20with%20graph%0Aattention%20networks%20for%20the%20policy%20and%20value%20functions%20to%20exploit%20the%0Agraph-structured%20data.%20We%20provide%20details%20of%20our%20methodology%20and%20open%20source%0Aall%20of%20our%20code%20for%20reproduction.%20We%20outperform%20the%20previous%20state-of-the-art%0ARL%20approach%20by%202.5%25%20%2817.4%20Tbps%20mean%20additional%20throughput%29%20and%20the%20best%0Aheuristic%20by%201.2%25%20%288.5%20Tbps%20mean%20additional%20throughput%29.%20This%20marginal%20gain%0Ahighlights%20the%20difficulty%20in%20learning%20effective%20RL%20policies%20on%20long%20horizon%0Aresource%20allocation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14741v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520with%2520Graph%2520Attention%2520for%2520Routing%2520and%2520Wavelength%250A%2520%2520Assignment%2520with%2520Lightpath%2520Reuse%26entry.906535625%3DMichael%2520Doherty%2520and%2520Alejandra%2520Beghelli%26entry.1292438233%3D%2520%2520Many%2520works%2520have%2520investigated%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520routing%2520and%250Aspectrum%2520assignment%2520on%2520flex-grid%2520networks%2520but%2520only%2520one%2520work%2520to%2520date%2520has%250Aexamined%2520RL%2520for%2520fixed-grid%2520with%2520flex-rate%2520transponders%252C%2520despite%2520production%250Asystems%2520using%2520this%2520paradigm.%2520Flex-rate%2520transponders%2520allow%2520existing%2520lightpaths%250Ato%2520accommodate%2520new%2520services%252C%2520a%2520task%2520we%2520term%2520routing%2520and%2520wavelength%2520assignment%250Awith%2520lightpath%2520reuse%2520%2528RWA-LR%2529.%2520We%2520re-examine%2520this%2520problem%2520and%2520present%2520a%250Athorough%2520benchmarking%2520of%2520heuristic%2520algorithms%2520for%2520RWA-LR%252C%2520which%2520are%2520shown%2520to%250Ahave%25206%2525%2520increased%2520throughput%2520when%2520candidate%2520paths%2520are%2520ordered%2520by%2520number%2520of%250Ahops%252C%2520rather%2520than%2520total%2520length.%2520We%2520train%2520an%2520RL%2520agent%2520for%2520RWA-LR%2520with%2520graph%250Aattention%2520networks%2520for%2520the%2520policy%2520and%2520value%2520functions%2520to%2520exploit%2520the%250Agraph-structured%2520data.%2520We%2520provide%2520details%2520of%2520our%2520methodology%2520and%2520open%2520source%250Aall%2520of%2520our%2520code%2520for%2520reproduction.%2520We%2520outperform%2520the%2520previous%2520state-of-the-art%250ARL%2520approach%2520by%25202.5%2525%2520%252817.4%2520Tbps%2520mean%2520additional%2520throughput%2529%2520and%2520the%2520best%250Aheuristic%2520by%25201.2%2525%2520%25288.5%2520Tbps%2520mean%2520additional%2520throughput%2529.%2520This%2520marginal%2520gain%250Ahighlights%2520the%2520difficulty%2520in%2520learning%2520effective%2520RL%2520policies%2520on%2520long%2520horizon%250Aresource%2520allocation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14741v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20with%20Graph%20Attention%20for%20Routing%20and%20Wavelength%0A%20%20Assignment%20with%20Lightpath%20Reuse&entry.906535625=Michael%20Doherty%20and%20Alejandra%20Beghelli&entry.1292438233=%20%20Many%20works%20have%20investigated%20reinforcement%20learning%20%28RL%29%20for%20routing%20and%0Aspectrum%20assignment%20on%20flex-grid%20networks%20but%20only%20one%20work%20to%20date%20has%0Aexamined%20RL%20for%20fixed-grid%20with%20flex-rate%20transponders%2C%20despite%20production%0Asystems%20using%20this%20paradigm.%20Flex-rate%20transponders%20allow%20existing%20lightpaths%0Ato%20accommodate%20new%20services%2C%20a%20task%20we%20term%20routing%20and%20wavelength%20assignment%0Awith%20lightpath%20reuse%20%28RWA-LR%29.%20We%20re-examine%20this%20problem%20and%20present%20a%0Athorough%20benchmarking%20of%20heuristic%20algorithms%20for%20RWA-LR%2C%20which%20are%20shown%20to%0Ahave%206%25%20increased%20throughput%20when%20candidate%20paths%20are%20ordered%20by%20number%20of%0Ahops%2C%20rather%20than%20total%20length.%20We%20train%20an%20RL%20agent%20for%20RWA-LR%20with%20graph%0Aattention%20networks%20for%20the%20policy%20and%20value%20functions%20to%20exploit%20the%0Agraph-structured%20data.%20We%20provide%20details%20of%20our%20methodology%20and%20open%20source%0Aall%20of%20our%20code%20for%20reproduction.%20We%20outperform%20the%20previous%20state-of-the-art%0ARL%20approach%20by%202.5%25%20%2817.4%20Tbps%20mean%20additional%20throughput%29%20and%20the%20best%0Aheuristic%20by%201.2%25%20%288.5%20Tbps%20mean%20additional%20throughput%29.%20This%20marginal%20gain%0Ahighlights%20the%20difficulty%20in%20learning%20effective%20RL%20policies%20on%20long%20horizon%0Aresource%20allocation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14741v2&entry.124074799=Read"},
{"title": "Algorithms for mean-field variational inference via polyhedral\n  optimization in the Wasserstein space", "author": "Yiheng Jiang and Sinho Chewi and Aram-Alexandre Pooladian", "abstract": "  We develop a theory of finite-dimensional polyhedral subsets over the\nWasserstein space and optimization of functionals over them via first-order\nmethods. Our main application is to the problem of mean-field variational\ninference, which seeks to approximate a distribution $\\pi$ over $\\mathbb{R}^d$\nby a product measure $\\pi^\\star$. When $\\pi$ is strongly log-concave and\nlog-smooth, we provide (1) approximation rates certifying that $\\pi^\\star$ is\nclose to the minimizer $\\pi^\\star_\\diamond$ of the KL divergence over a\n\\emph{polyhedral} set $\\mathcal{P}_\\diamond$, and (2) an algorithm for\nminimizing $\\text{KL}(\\cdot\\|\\pi)$ over $\\mathcal{P}_\\diamond$ based on\naccelerated gradient descent over $\\R^d$. As a byproduct of our analysis, we\nobtain the first end-to-end analysis for gradient-based algorithms for MFVI.\n", "link": "http://arxiv.org/abs/2312.02849v3", "date": "2025-04-18", "relevancy": 1.7864, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4851}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4416}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithms%20for%20mean-field%20variational%20inference%20via%20polyhedral%0A%20%20optimization%20in%20the%20Wasserstein%20space&body=Title%3A%20Algorithms%20for%20mean-field%20variational%20inference%20via%20polyhedral%0A%20%20optimization%20in%20the%20Wasserstein%20space%0AAuthor%3A%20Yiheng%20Jiang%20and%20Sinho%20Chewi%20and%20Aram-Alexandre%20Pooladian%0AAbstract%3A%20%20%20We%20develop%20a%20theory%20of%20finite-dimensional%20polyhedral%20subsets%20over%20the%0AWasserstein%20space%20and%20optimization%20of%20functionals%20over%20them%20via%20first-order%0Amethods.%20Our%20main%20application%20is%20to%20the%20problem%20of%20mean-field%20variational%0Ainference%2C%20which%20seeks%20to%20approximate%20a%20distribution%20%24%5Cpi%24%20over%20%24%5Cmathbb%7BR%7D%5Ed%24%0Aby%20a%20product%20measure%20%24%5Cpi%5E%5Cstar%24.%20When%20%24%5Cpi%24%20is%20strongly%20log-concave%20and%0Alog-smooth%2C%20we%20provide%20%281%29%20approximation%20rates%20certifying%20that%20%24%5Cpi%5E%5Cstar%24%20is%0Aclose%20to%20the%20minimizer%20%24%5Cpi%5E%5Cstar_%5Cdiamond%24%20of%20the%20KL%20divergence%20over%20a%0A%5Cemph%7Bpolyhedral%7D%20set%20%24%5Cmathcal%7BP%7D_%5Cdiamond%24%2C%20and%20%282%29%20an%20algorithm%20for%0Aminimizing%20%24%5Ctext%7BKL%7D%28%5Ccdot%5C%7C%5Cpi%29%24%20over%20%24%5Cmathcal%7BP%7D_%5Cdiamond%24%20based%20on%0Aaccelerated%20gradient%20descent%20over%20%24%5CR%5Ed%24.%20As%20a%20byproduct%20of%20our%20analysis%2C%20we%0Aobtain%20the%20first%20end-to-end%20analysis%20for%20gradient-based%20algorithms%20for%20MFVI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02849v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithms%2520for%2520mean-field%2520variational%2520inference%2520via%2520polyhedral%250A%2520%2520optimization%2520in%2520the%2520Wasserstein%2520space%26entry.906535625%3DYiheng%2520Jiang%2520and%2520Sinho%2520Chewi%2520and%2520Aram-Alexandre%2520Pooladian%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520theory%2520of%2520finite-dimensional%2520polyhedral%2520subsets%2520over%2520the%250AWasserstein%2520space%2520and%2520optimization%2520of%2520functionals%2520over%2520them%2520via%2520first-order%250Amethods.%2520Our%2520main%2520application%2520is%2520to%2520the%2520problem%2520of%2520mean-field%2520variational%250Ainference%252C%2520which%2520seeks%2520to%2520approximate%2520a%2520distribution%2520%2524%255Cpi%2524%2520over%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%250Aby%2520a%2520product%2520measure%2520%2524%255Cpi%255E%255Cstar%2524.%2520When%2520%2524%255Cpi%2524%2520is%2520strongly%2520log-concave%2520and%250Alog-smooth%252C%2520we%2520provide%2520%25281%2529%2520approximation%2520rates%2520certifying%2520that%2520%2524%255Cpi%255E%255Cstar%2524%2520is%250Aclose%2520to%2520the%2520minimizer%2520%2524%255Cpi%255E%255Cstar_%255Cdiamond%2524%2520of%2520the%2520KL%2520divergence%2520over%2520a%250A%255Cemph%257Bpolyhedral%257D%2520set%2520%2524%255Cmathcal%257BP%257D_%255Cdiamond%2524%252C%2520and%2520%25282%2529%2520an%2520algorithm%2520for%250Aminimizing%2520%2524%255Ctext%257BKL%257D%2528%255Ccdot%255C%257C%255Cpi%2529%2524%2520over%2520%2524%255Cmathcal%257BP%257D_%255Cdiamond%2524%2520based%2520on%250Aaccelerated%2520gradient%2520descent%2520over%2520%2524%255CR%255Ed%2524.%2520As%2520a%2520byproduct%2520of%2520our%2520analysis%252C%2520we%250Aobtain%2520the%2520first%2520end-to-end%2520analysis%2520for%2520gradient-based%2520algorithms%2520for%2520MFVI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02849v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithms%20for%20mean-field%20variational%20inference%20via%20polyhedral%0A%20%20optimization%20in%20the%20Wasserstein%20space&entry.906535625=Yiheng%20Jiang%20and%20Sinho%20Chewi%20and%20Aram-Alexandre%20Pooladian&entry.1292438233=%20%20We%20develop%20a%20theory%20of%20finite-dimensional%20polyhedral%20subsets%20over%20the%0AWasserstein%20space%20and%20optimization%20of%20functionals%20over%20them%20via%20first-order%0Amethods.%20Our%20main%20application%20is%20to%20the%20problem%20of%20mean-field%20variational%0Ainference%2C%20which%20seeks%20to%20approximate%20a%20distribution%20%24%5Cpi%24%20over%20%24%5Cmathbb%7BR%7D%5Ed%24%0Aby%20a%20product%20measure%20%24%5Cpi%5E%5Cstar%24.%20When%20%24%5Cpi%24%20is%20strongly%20log-concave%20and%0Alog-smooth%2C%20we%20provide%20%281%29%20approximation%20rates%20certifying%20that%20%24%5Cpi%5E%5Cstar%24%20is%0Aclose%20to%20the%20minimizer%20%24%5Cpi%5E%5Cstar_%5Cdiamond%24%20of%20the%20KL%20divergence%20over%20a%0A%5Cemph%7Bpolyhedral%7D%20set%20%24%5Cmathcal%7BP%7D_%5Cdiamond%24%2C%20and%20%282%29%20an%20algorithm%20for%0Aminimizing%20%24%5Ctext%7BKL%7D%28%5Ccdot%5C%7C%5Cpi%29%24%20over%20%24%5Cmathcal%7BP%7D_%5Cdiamond%24%20based%20on%0Aaccelerated%20gradient%20descent%20over%20%24%5CR%5Ed%24.%20As%20a%20byproduct%20of%20our%20analysis%2C%20we%0Aobtain%20the%20first%20end-to-end%20analysis%20for%20gradient-based%20algorithms%20for%20MFVI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02849v3&entry.124074799=Read"},
{"title": "Energy-Latency Attacks via Sponge Poisoning", "author": "Antonio Emanuele Cin\u00e0 and Ambra Demontis and Battista Biggio and Fabio Roli and Marcello Pelillo", "abstract": "  Sponge examples are test-time inputs optimized to increase energy consumption\nand prediction latency of deep networks deployed on hardware accelerators. By\nincreasing the fraction of neurons activated during classification, these\nattacks reduce sparsity in network activation patterns, worsening the\nperformance of hardware accelerators. In this work, we present a novel\ntraining-time attack, named sponge poisoning, which aims to worsen energy\nconsumption and prediction latency of neural networks on any test input without\naffecting classification accuracy. To stage this attack, we assume that the\nattacker can control only a few model updates during training -- a likely\nscenario, e.g., when model training is outsourced to an untrusted third party\nor distributed via federated learning. Our extensive experiments on image\nclassification tasks show that sponge poisoning is effective, and that\nfine-tuning poisoned models to repair them poses prohibitive costs for most\nusers, highlighting that tackling sponge poisoning remains an open issue.\n", "link": "http://arxiv.org/abs/2203.08147v5", "date": "2025-04-18", "relevancy": 1.781, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4821}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4407}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Latency%20Attacks%20via%20Sponge%20Poisoning&body=Title%3A%20Energy-Latency%20Attacks%20via%20Sponge%20Poisoning%0AAuthor%3A%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Fabio%20Roli%20and%20Marcello%20Pelillo%0AAbstract%3A%20%20%20Sponge%20examples%20are%20test-time%20inputs%20optimized%20to%20increase%20energy%20consumption%0Aand%20prediction%20latency%20of%20deep%20networks%20deployed%20on%20hardware%20accelerators.%20By%0Aincreasing%20the%20fraction%20of%20neurons%20activated%20during%20classification%2C%20these%0Aattacks%20reduce%20sparsity%20in%20network%20activation%20patterns%2C%20worsening%20the%0Aperformance%20of%20hardware%20accelerators.%20In%20this%20work%2C%20we%20present%20a%20novel%0Atraining-time%20attack%2C%20named%20sponge%20poisoning%2C%20which%20aims%20to%20worsen%20energy%0Aconsumption%20and%20prediction%20latency%20of%20neural%20networks%20on%20any%20test%20input%20without%0Aaffecting%20classification%20accuracy.%20To%20stage%20this%20attack%2C%20we%20assume%20that%20the%0Aattacker%20can%20control%20only%20a%20few%20model%20updates%20during%20training%20--%20a%20likely%0Ascenario%2C%20e.g.%2C%20when%20model%20training%20is%20outsourced%20to%20an%20untrusted%20third%20party%0Aor%20distributed%20via%20federated%20learning.%20Our%20extensive%20experiments%20on%20image%0Aclassification%20tasks%20show%20that%20sponge%20poisoning%20is%20effective%2C%20and%20that%0Afine-tuning%20poisoned%20models%20to%20repair%20them%20poses%20prohibitive%20costs%20for%20most%0Ausers%2C%20highlighting%20that%20tackling%20sponge%20poisoning%20remains%20an%20open%20issue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.08147v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Latency%2520Attacks%2520via%2520Sponge%2520Poisoning%26entry.906535625%3DAntonio%2520Emanuele%2520Cin%25C3%25A0%2520and%2520Ambra%2520Demontis%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%2520and%2520Marcello%2520Pelillo%26entry.1292438233%3D%2520%2520Sponge%2520examples%2520are%2520test-time%2520inputs%2520optimized%2520to%2520increase%2520energy%2520consumption%250Aand%2520prediction%2520latency%2520of%2520deep%2520networks%2520deployed%2520on%2520hardware%2520accelerators.%2520By%250Aincreasing%2520the%2520fraction%2520of%2520neurons%2520activated%2520during%2520classification%252C%2520these%250Aattacks%2520reduce%2520sparsity%2520in%2520network%2520activation%2520patterns%252C%2520worsening%2520the%250Aperformance%2520of%2520hardware%2520accelerators.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%250Atraining-time%2520attack%252C%2520named%2520sponge%2520poisoning%252C%2520which%2520aims%2520to%2520worsen%2520energy%250Aconsumption%2520and%2520prediction%2520latency%2520of%2520neural%2520networks%2520on%2520any%2520test%2520input%2520without%250Aaffecting%2520classification%2520accuracy.%2520To%2520stage%2520this%2520attack%252C%2520we%2520assume%2520that%2520the%250Aattacker%2520can%2520control%2520only%2520a%2520few%2520model%2520updates%2520during%2520training%2520--%2520a%2520likely%250Ascenario%252C%2520e.g.%252C%2520when%2520model%2520training%2520is%2520outsourced%2520to%2520an%2520untrusted%2520third%2520party%250Aor%2520distributed%2520via%2520federated%2520learning.%2520Our%2520extensive%2520experiments%2520on%2520image%250Aclassification%2520tasks%2520show%2520that%2520sponge%2520poisoning%2520is%2520effective%252C%2520and%2520that%250Afine-tuning%2520poisoned%2520models%2520to%2520repair%2520them%2520poses%2520prohibitive%2520costs%2520for%2520most%250Ausers%252C%2520highlighting%2520that%2520tackling%2520sponge%2520poisoning%2520remains%2520an%2520open%2520issue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.08147v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Latency%20Attacks%20via%20Sponge%20Poisoning&entry.906535625=Antonio%20Emanuele%20Cin%C3%A0%20and%20Ambra%20Demontis%20and%20Battista%20Biggio%20and%20Fabio%20Roli%20and%20Marcello%20Pelillo&entry.1292438233=%20%20Sponge%20examples%20are%20test-time%20inputs%20optimized%20to%20increase%20energy%20consumption%0Aand%20prediction%20latency%20of%20deep%20networks%20deployed%20on%20hardware%20accelerators.%20By%0Aincreasing%20the%20fraction%20of%20neurons%20activated%20during%20classification%2C%20these%0Aattacks%20reduce%20sparsity%20in%20network%20activation%20patterns%2C%20worsening%20the%0Aperformance%20of%20hardware%20accelerators.%20In%20this%20work%2C%20we%20present%20a%20novel%0Atraining-time%20attack%2C%20named%20sponge%20poisoning%2C%20which%20aims%20to%20worsen%20energy%0Aconsumption%20and%20prediction%20latency%20of%20neural%20networks%20on%20any%20test%20input%20without%0Aaffecting%20classification%20accuracy.%20To%20stage%20this%20attack%2C%20we%20assume%20that%20the%0Aattacker%20can%20control%20only%20a%20few%20model%20updates%20during%20training%20--%20a%20likely%0Ascenario%2C%20e.g.%2C%20when%20model%20training%20is%20outsourced%20to%20an%20untrusted%20third%20party%0Aor%20distributed%20via%20federated%20learning.%20Our%20extensive%20experiments%20on%20image%0Aclassification%20tasks%20show%20that%20sponge%20poisoning%20is%20effective%2C%20and%20that%0Afine-tuning%20poisoned%20models%20to%20repair%20them%20poses%20prohibitive%20costs%20for%20most%0Ausers%2C%20highlighting%20that%20tackling%20sponge%20poisoning%20remains%20an%20open%20issue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.08147v5&entry.124074799=Read"},
{"title": "SupResDiffGAN a new approach for the Super-Resolution task", "author": "Dawid Kope\u0107 and Wojciech Koz\u0142owski and Maciej Wizerkaniuk and Dawid Krutul and Jan Koco\u0144 and Maciej Zi\u0119ba", "abstract": "  In this work, we present SupResDiffGAN, a novel hybrid architecture that\ncombines the strengths of Generative Adversarial Networks (GANs) and diffusion\nmodels for super-resolution tasks. By leveraging latent space representations\nand reducing the number of diffusion steps, SupResDiffGAN achieves\nsignificantly faster inference times than other diffusion-based\nsuper-resolution models while maintaining competitive perceptual quality. To\nprevent discriminator overfitting, we propose adaptive noise corruption,\nensuring a stable balance between the generator and the discriminator during\ntraining. Extensive experiments on benchmark datasets show that our approach\noutperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency\nand image quality. This work bridges the performance gap between diffusion- and\nGAN-based methods, laying the foundation for real-time applications of\ndiffusion models in high-resolution image generation.\n", "link": "http://arxiv.org/abs/2504.13622v1", "date": "2025-04-18", "relevancy": 1.7808, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6064}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.59}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SupResDiffGAN%20a%20new%20approach%20for%20the%20Super-Resolution%20task&body=Title%3A%20SupResDiffGAN%20a%20new%20approach%20for%20the%20Super-Resolution%20task%0AAuthor%3A%20Dawid%20Kope%C4%87%20and%20Wojciech%20Koz%C5%82owski%20and%20Maciej%20Wizerkaniuk%20and%20Dawid%20Krutul%20and%20Jan%20Koco%C5%84%20and%20Maciej%20Zi%C4%99ba%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20SupResDiffGAN%2C%20a%20novel%20hybrid%20architecture%20that%0Acombines%20the%20strengths%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20diffusion%0Amodels%20for%20super-resolution%20tasks.%20By%20leveraging%20latent%20space%20representations%0Aand%20reducing%20the%20number%20of%20diffusion%20steps%2C%20SupResDiffGAN%20achieves%0Asignificantly%20faster%20inference%20times%20than%20other%20diffusion-based%0Asuper-resolution%20models%20while%20maintaining%20competitive%20perceptual%20quality.%20To%0Aprevent%20discriminator%20overfitting%2C%20we%20propose%20adaptive%20noise%20corruption%2C%0Aensuring%20a%20stable%20balance%20between%20the%20generator%20and%20the%20discriminator%20during%0Atraining.%20Extensive%20experiments%20on%20benchmark%20datasets%20show%20that%20our%20approach%0Aoutperforms%20traditional%20diffusion%20models%20such%20as%20SR3%20and%20I%24%5E2%24SB%20in%20efficiency%0Aand%20image%20quality.%20This%20work%20bridges%20the%20performance%20gap%20between%20diffusion-%20and%0AGAN-based%20methods%2C%20laying%20the%20foundation%20for%20real-time%20applications%20of%0Adiffusion%20models%20in%20high-resolution%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSupResDiffGAN%2520a%2520new%2520approach%2520for%2520the%2520Super-Resolution%2520task%26entry.906535625%3DDawid%2520Kope%25C4%2587%2520and%2520Wojciech%2520Koz%25C5%2582owski%2520and%2520Maciej%2520Wizerkaniuk%2520and%2520Dawid%2520Krutul%2520and%2520Jan%2520Koco%25C5%2584%2520and%2520Maciej%2520Zi%25C4%2599ba%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520SupResDiffGAN%252C%2520a%2520novel%2520hybrid%2520architecture%2520that%250Acombines%2520the%2520strengths%2520of%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520and%2520diffusion%250Amodels%2520for%2520super-resolution%2520tasks.%2520By%2520leveraging%2520latent%2520space%2520representations%250Aand%2520reducing%2520the%2520number%2520of%2520diffusion%2520steps%252C%2520SupResDiffGAN%2520achieves%250Asignificantly%2520faster%2520inference%2520times%2520than%2520other%2520diffusion-based%250Asuper-resolution%2520models%2520while%2520maintaining%2520competitive%2520perceptual%2520quality.%2520To%250Aprevent%2520discriminator%2520overfitting%252C%2520we%2520propose%2520adaptive%2520noise%2520corruption%252C%250Aensuring%2520a%2520stable%2520balance%2520between%2520the%2520generator%2520and%2520the%2520discriminator%2520during%250Atraining.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520show%2520that%2520our%2520approach%250Aoutperforms%2520traditional%2520diffusion%2520models%2520such%2520as%2520SR3%2520and%2520I%2524%255E2%2524SB%2520in%2520efficiency%250Aand%2520image%2520quality.%2520This%2520work%2520bridges%2520the%2520performance%2520gap%2520between%2520diffusion-%2520and%250AGAN-based%2520methods%252C%2520laying%2520the%2520foundation%2520for%2520real-time%2520applications%2520of%250Adiffusion%2520models%2520in%2520high-resolution%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SupResDiffGAN%20a%20new%20approach%20for%20the%20Super-Resolution%20task&entry.906535625=Dawid%20Kope%C4%87%20and%20Wojciech%20Koz%C5%82owski%20and%20Maciej%20Wizerkaniuk%20and%20Dawid%20Krutul%20and%20Jan%20Koco%C5%84%20and%20Maciej%20Zi%C4%99ba&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20SupResDiffGAN%2C%20a%20novel%20hybrid%20architecture%20that%0Acombines%20the%20strengths%20of%20Generative%20Adversarial%20Networks%20%28GANs%29%20and%20diffusion%0Amodels%20for%20super-resolution%20tasks.%20By%20leveraging%20latent%20space%20representations%0Aand%20reducing%20the%20number%20of%20diffusion%20steps%2C%20SupResDiffGAN%20achieves%0Asignificantly%20faster%20inference%20times%20than%20other%20diffusion-based%0Asuper-resolution%20models%20while%20maintaining%20competitive%20perceptual%20quality.%20To%0Aprevent%20discriminator%20overfitting%2C%20we%20propose%20adaptive%20noise%20corruption%2C%0Aensuring%20a%20stable%20balance%20between%20the%20generator%20and%20the%20discriminator%20during%0Atraining.%20Extensive%20experiments%20on%20benchmark%20datasets%20show%20that%20our%20approach%0Aoutperforms%20traditional%20diffusion%20models%20such%20as%20SR3%20and%20I%24%5E2%24SB%20in%20efficiency%0Aand%20image%20quality.%20This%20work%20bridges%20the%20performance%20gap%20between%20diffusion-%20and%0AGAN-based%20methods%2C%20laying%20the%20foundation%20for%20real-time%20applications%20of%0Adiffusion%20models%20in%20high-resolution%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13622v1&entry.124074799=Read"},
{"title": "Near-Polynomially Competitive Active Logistic Regression", "author": "Yihan Zhou and Eric Price and Trung Nguyen", "abstract": "  We address the problem of active logistic regression in the realizable\nsetting. It is well known that active learning can require exponentially fewer\nlabel queries compared to passive learning, in some cases using $\\log\n\\frac{1}{\\eps}$ rather than $\\poly(1/\\eps)$ labels to get error $\\eps$ larger\nthan the optimum.\n  We present the first algorithm that is polynomially competitive with the\noptimal algorithm on every input instance, up to factors polylogarithmic in the\nerror and domain size. In particular, if any algorithm achieves label\ncomplexity polylogarithmic in $\\eps$, so does ours. Our algorithm is based on\nefficient sampling and can be extended to learn more general class of\nfunctions. We further support our theoretical results with experiments\ndemonstrating performance gains for logistic regression compared to existing\nactive learning algorithms.\n", "link": "http://arxiv.org/abs/2503.05981v4", "date": "2025-04-18", "relevancy": 1.7629, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4583}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4483}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-Polynomially%20Competitive%20Active%20Logistic%20Regression&body=Title%3A%20Near-Polynomially%20Competitive%20Active%20Logistic%20Regression%0AAuthor%3A%20Yihan%20Zhou%20and%20Eric%20Price%20and%20Trung%20Nguyen%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20active%20logistic%20regression%20in%20the%20realizable%0Asetting.%20It%20is%20well%20known%20that%20active%20learning%20can%20require%20exponentially%20fewer%0Alabel%20queries%20compared%20to%20passive%20learning%2C%20in%20some%20cases%20using%20%24%5Clog%0A%5Cfrac%7B1%7D%7B%5Ceps%7D%24%20rather%20than%20%24%5Cpoly%281/%5Ceps%29%24%20labels%20to%20get%20error%20%24%5Ceps%24%20larger%0Athan%20the%20optimum.%0A%20%20We%20present%20the%20first%20algorithm%20that%20is%20polynomially%20competitive%20with%20the%0Aoptimal%20algorithm%20on%20every%20input%20instance%2C%20up%20to%20factors%20polylogarithmic%20in%20the%0Aerror%20and%20domain%20size.%20In%20particular%2C%20if%20any%20algorithm%20achieves%20label%0Acomplexity%20polylogarithmic%20in%20%24%5Ceps%24%2C%20so%20does%20ours.%20Our%20algorithm%20is%20based%20on%0Aefficient%20sampling%20and%20can%20be%20extended%20to%20learn%20more%20general%20class%20of%0Afunctions.%20We%20further%20support%20our%20theoretical%20results%20with%20experiments%0Ademonstrating%20performance%20gains%20for%20logistic%20regression%20compared%20to%20existing%0Aactive%20learning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05981v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-Polynomially%2520Competitive%2520Active%2520Logistic%2520Regression%26entry.906535625%3DYihan%2520Zhou%2520and%2520Eric%2520Price%2520and%2520Trung%2520Nguyen%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520active%2520logistic%2520regression%2520in%2520the%2520realizable%250Asetting.%2520It%2520is%2520well%2520known%2520that%2520active%2520learning%2520can%2520require%2520exponentially%2520fewer%250Alabel%2520queries%2520compared%2520to%2520passive%2520learning%252C%2520in%2520some%2520cases%2520using%2520%2524%255Clog%250A%255Cfrac%257B1%257D%257B%255Ceps%257D%2524%2520rather%2520than%2520%2524%255Cpoly%25281/%255Ceps%2529%2524%2520labels%2520to%2520get%2520error%2520%2524%255Ceps%2524%2520larger%250Athan%2520the%2520optimum.%250A%2520%2520We%2520present%2520the%2520first%2520algorithm%2520that%2520is%2520polynomially%2520competitive%2520with%2520the%250Aoptimal%2520algorithm%2520on%2520every%2520input%2520instance%252C%2520up%2520to%2520factors%2520polylogarithmic%2520in%2520the%250Aerror%2520and%2520domain%2520size.%2520In%2520particular%252C%2520if%2520any%2520algorithm%2520achieves%2520label%250Acomplexity%2520polylogarithmic%2520in%2520%2524%255Ceps%2524%252C%2520so%2520does%2520ours.%2520Our%2520algorithm%2520is%2520based%2520on%250Aefficient%2520sampling%2520and%2520can%2520be%2520extended%2520to%2520learn%2520more%2520general%2520class%2520of%250Afunctions.%2520We%2520further%2520support%2520our%2520theoretical%2520results%2520with%2520experiments%250Ademonstrating%2520performance%2520gains%2520for%2520logistic%2520regression%2520compared%2520to%2520existing%250Aactive%2520learning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05981v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-Polynomially%20Competitive%20Active%20Logistic%20Regression&entry.906535625=Yihan%20Zhou%20and%20Eric%20Price%20and%20Trung%20Nguyen&entry.1292438233=%20%20We%20address%20the%20problem%20of%20active%20logistic%20regression%20in%20the%20realizable%0Asetting.%20It%20is%20well%20known%20that%20active%20learning%20can%20require%20exponentially%20fewer%0Alabel%20queries%20compared%20to%20passive%20learning%2C%20in%20some%20cases%20using%20%24%5Clog%0A%5Cfrac%7B1%7D%7B%5Ceps%7D%24%20rather%20than%20%24%5Cpoly%281/%5Ceps%29%24%20labels%20to%20get%20error%20%24%5Ceps%24%20larger%0Athan%20the%20optimum.%0A%20%20We%20present%20the%20first%20algorithm%20that%20is%20polynomially%20competitive%20with%20the%0Aoptimal%20algorithm%20on%20every%20input%20instance%2C%20up%20to%20factors%20polylogarithmic%20in%20the%0Aerror%20and%20domain%20size.%20In%20particular%2C%20if%20any%20algorithm%20achieves%20label%0Acomplexity%20polylogarithmic%20in%20%24%5Ceps%24%2C%20so%20does%20ours.%20Our%20algorithm%20is%20based%20on%0Aefficient%20sampling%20and%20can%20be%20extended%20to%20learn%20more%20general%20class%20of%0Afunctions.%20We%20further%20support%20our%20theoretical%20results%20with%20experiments%0Ademonstrating%20performance%20gains%20for%20logistic%20regression%20compared%20to%20existing%0Aactive%20learning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05981v4&entry.124074799=Read"},
{"title": "Conformal Prediction Regions are Imprecise Highest Density Regions", "author": "Michele Caprio and Yusuf Sale and Eyke H\u00fcllermeier", "abstract": "  Recently, Cella and Martin proved how, under an assumption called consonance,\na credal set (i.e. a closed and convex set of probabilities) can be derived\nfrom the conformal transducer associated with transductive conformal\nprediction. We show that the Imprecise Highest Density Region (IHDR) associated\nwith such a credal set corresponds to the classical Conformal Prediction\nRegion. In proving this result, we establish a new relationship between\nConformal Prediction and Imprecise Probability (IP) theories, via the IP\nconcept of a cloud. A byproduct of our presentation is the discovery that\nconsonant plausibility functions are monoid homomorphisms, a new algebraic\nproperty of an IP tool.\n", "link": "http://arxiv.org/abs/2502.06331v2", "date": "2025-04-18", "relevancy": 1.7565, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4467}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4428}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Prediction%20Regions%20are%20Imprecise%20Highest%20Density%20Regions&body=Title%3A%20Conformal%20Prediction%20Regions%20are%20Imprecise%20Highest%20Density%20Regions%0AAuthor%3A%20Michele%20Caprio%20and%20Yusuf%20Sale%20and%20Eyke%20H%C3%BCllermeier%0AAbstract%3A%20%20%20Recently%2C%20Cella%20and%20Martin%20proved%20how%2C%20under%20an%20assumption%20called%20consonance%2C%0Aa%20credal%20set%20%28i.e.%20a%20closed%20and%20convex%20set%20of%20probabilities%29%20can%20be%20derived%0Afrom%20the%20conformal%20transducer%20associated%20with%20transductive%20conformal%0Aprediction.%20We%20show%20that%20the%20Imprecise%20Highest%20Density%20Region%20%28IHDR%29%20associated%0Awith%20such%20a%20credal%20set%20corresponds%20to%20the%20classical%20Conformal%20Prediction%0ARegion.%20In%20proving%20this%20result%2C%20we%20establish%20a%20new%20relationship%20between%0AConformal%20Prediction%20and%20Imprecise%20Probability%20%28IP%29%20theories%2C%20via%20the%20IP%0Aconcept%20of%20a%20cloud.%20A%20byproduct%20of%20our%20presentation%20is%20the%20discovery%20that%0Aconsonant%20plausibility%20functions%20are%20monoid%20homomorphisms%2C%20a%20new%20algebraic%0Aproperty%20of%20an%20IP%20tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06331v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Prediction%2520Regions%2520are%2520Imprecise%2520Highest%2520Density%2520Regions%26entry.906535625%3DMichele%2520Caprio%2520and%2520Yusuf%2520Sale%2520and%2520Eyke%2520H%25C3%25BCllermeier%26entry.1292438233%3D%2520%2520Recently%252C%2520Cella%2520and%2520Martin%2520proved%2520how%252C%2520under%2520an%2520assumption%2520called%2520consonance%252C%250Aa%2520credal%2520set%2520%2528i.e.%2520a%2520closed%2520and%2520convex%2520set%2520of%2520probabilities%2529%2520can%2520be%2520derived%250Afrom%2520the%2520conformal%2520transducer%2520associated%2520with%2520transductive%2520conformal%250Aprediction.%2520We%2520show%2520that%2520the%2520Imprecise%2520Highest%2520Density%2520Region%2520%2528IHDR%2529%2520associated%250Awith%2520such%2520a%2520credal%2520set%2520corresponds%2520to%2520the%2520classical%2520Conformal%2520Prediction%250ARegion.%2520In%2520proving%2520this%2520result%252C%2520we%2520establish%2520a%2520new%2520relationship%2520between%250AConformal%2520Prediction%2520and%2520Imprecise%2520Probability%2520%2528IP%2529%2520theories%252C%2520via%2520the%2520IP%250Aconcept%2520of%2520a%2520cloud.%2520A%2520byproduct%2520of%2520our%2520presentation%2520is%2520the%2520discovery%2520that%250Aconsonant%2520plausibility%2520functions%2520are%2520monoid%2520homomorphisms%252C%2520a%2520new%2520algebraic%250Aproperty%2520of%2520an%2520IP%2520tool.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06331v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Prediction%20Regions%20are%20Imprecise%20Highest%20Density%20Regions&entry.906535625=Michele%20Caprio%20and%20Yusuf%20Sale%20and%20Eyke%20H%C3%BCllermeier&entry.1292438233=%20%20Recently%2C%20Cella%20and%20Martin%20proved%20how%2C%20under%20an%20assumption%20called%20consonance%2C%0Aa%20credal%20set%20%28i.e.%20a%20closed%20and%20convex%20set%20of%20probabilities%29%20can%20be%20derived%0Afrom%20the%20conformal%20transducer%20associated%20with%20transductive%20conformal%0Aprediction.%20We%20show%20that%20the%20Imprecise%20Highest%20Density%20Region%20%28IHDR%29%20associated%0Awith%20such%20a%20credal%20set%20corresponds%20to%20the%20classical%20Conformal%20Prediction%0ARegion.%20In%20proving%20this%20result%2C%20we%20establish%20a%20new%20relationship%20between%0AConformal%20Prediction%20and%20Imprecise%20Probability%20%28IP%29%20theories%2C%20via%20the%20IP%0Aconcept%20of%20a%20cloud.%20A%20byproduct%20of%20our%20presentation%20is%20the%20discovery%20that%0Aconsonant%20plausibility%20functions%20are%20monoid%20homomorphisms%2C%20a%20new%20algebraic%0Aproperty%20of%20an%20IP%20tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06331v2&entry.124074799=Read"},
{"title": "Robust Humanoid Walking on Compliant and Uneven Terrain with Deep\n  Reinforcement Learning", "author": "Rohan P. Singh and Mitsuharu Morisawa and Mehdi Benallegue and Zhaoming Xie and Fumio Kanehiro", "abstract": "  For the deployment of legged robots in real-world environments, it is\nessential to develop robust locomotion control methods for challenging terrains\nthat may exhibit unexpected deformability and irregularity. In this paper, we\nexplore the application of sim-to-real deep reinforcement learning (RL) for the\ndesign of bipedal locomotion controllers for humanoid robots on compliant and\nuneven terrains. Our key contribution is to show that a simple training\ncurriculum for exposing the RL agent to randomized terrains in simulation can\nachieve robust walking on a real humanoid robot using only proprioceptive\nfeedback. We train an end-to-end bipedal locomotion policy using the proposed\napproach, and show extensive real-robot demonstration on the HRP-5P humanoid\nover several difficult terrains inside and outside the lab environment.\nFurther, we argue that the robustness of a bipedal walking policy can be\nimproved if the robot is allowed to exhibit aperiodic motion with variable\nstepping frequency. We propose a new control policy to enable modification of\nthe observed clock signal, leading to adaptive gait frequencies depending on\nthe terrain and command velocity. Through simulation experiments, we show the\neffectiveness of this policy specifically for walking over challenging terrains\nby controlling swing and stance durations. The code for training and evaluation\nis available online at https://github.com/rohanpsingh/LearningHumanoidWalking.\nDemo video is available at https://www.youtube.com/watch?v=ZgfNzGAkk2Q.\n", "link": "http://arxiv.org/abs/2504.13619v1", "date": "2025-04-18", "relevancy": 1.7509, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6379}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5693}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Humanoid%20Walking%20on%20Compliant%20and%20Uneven%20Terrain%20with%20Deep%0A%20%20Reinforcement%20Learning&body=Title%3A%20Robust%20Humanoid%20Walking%20on%20Compliant%20and%20Uneven%20Terrain%20with%20Deep%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Rohan%20P.%20Singh%20and%20Mitsuharu%20Morisawa%20and%20Mehdi%20Benallegue%20and%20Zhaoming%20Xie%20and%20Fumio%20Kanehiro%0AAbstract%3A%20%20%20For%20the%20deployment%20of%20legged%20robots%20in%20real-world%20environments%2C%20it%20is%0Aessential%20to%20develop%20robust%20locomotion%20control%20methods%20for%20challenging%20terrains%0Athat%20may%20exhibit%20unexpected%20deformability%20and%20irregularity.%20In%20this%20paper%2C%20we%0Aexplore%20the%20application%20of%20sim-to-real%20deep%20reinforcement%20learning%20%28RL%29%20for%20the%0Adesign%20of%20bipedal%20locomotion%20controllers%20for%20humanoid%20robots%20on%20compliant%20and%0Auneven%20terrains.%20Our%20key%20contribution%20is%20to%20show%20that%20a%20simple%20training%0Acurriculum%20for%20exposing%20the%20RL%20agent%20to%20randomized%20terrains%20in%20simulation%20can%0Aachieve%20robust%20walking%20on%20a%20real%20humanoid%20robot%20using%20only%20proprioceptive%0Afeedback.%20We%20train%20an%20end-to-end%20bipedal%20locomotion%20policy%20using%20the%20proposed%0Aapproach%2C%20and%20show%20extensive%20real-robot%20demonstration%20on%20the%20HRP-5P%20humanoid%0Aover%20several%20difficult%20terrains%20inside%20and%20outside%20the%20lab%20environment.%0AFurther%2C%20we%20argue%20that%20the%20robustness%20of%20a%20bipedal%20walking%20policy%20can%20be%0Aimproved%20if%20the%20robot%20is%20allowed%20to%20exhibit%20aperiodic%20motion%20with%20variable%0Astepping%20frequency.%20We%20propose%20a%20new%20control%20policy%20to%20enable%20modification%20of%0Athe%20observed%20clock%20signal%2C%20leading%20to%20adaptive%20gait%20frequencies%20depending%20on%0Athe%20terrain%20and%20command%20velocity.%20Through%20simulation%20experiments%2C%20we%20show%20the%0Aeffectiveness%20of%20this%20policy%20specifically%20for%20walking%20over%20challenging%20terrains%0Aby%20controlling%20swing%20and%20stance%20durations.%20The%20code%20for%20training%20and%20evaluation%0Ais%20available%20online%20at%20https%3A//github.com/rohanpsingh/LearningHumanoidWalking.%0ADemo%20video%20is%20available%20at%20https%3A//www.youtube.com/watch%3Fv%3DZgfNzGAkk2Q.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Humanoid%2520Walking%2520on%2520Compliant%2520and%2520Uneven%2520Terrain%2520with%2520Deep%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DRohan%2520P.%2520Singh%2520and%2520Mitsuharu%2520Morisawa%2520and%2520Mehdi%2520Benallegue%2520and%2520Zhaoming%2520Xie%2520and%2520Fumio%2520Kanehiro%26entry.1292438233%3D%2520%2520For%2520the%2520deployment%2520of%2520legged%2520robots%2520in%2520real-world%2520environments%252C%2520it%2520is%250Aessential%2520to%2520develop%2520robust%2520locomotion%2520control%2520methods%2520for%2520challenging%2520terrains%250Athat%2520may%2520exhibit%2520unexpected%2520deformability%2520and%2520irregularity.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520the%2520application%2520of%2520sim-to-real%2520deep%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520the%250Adesign%2520of%2520bipedal%2520locomotion%2520controllers%2520for%2520humanoid%2520robots%2520on%2520compliant%2520and%250Auneven%2520terrains.%2520Our%2520key%2520contribution%2520is%2520to%2520show%2520that%2520a%2520simple%2520training%250Acurriculum%2520for%2520exposing%2520the%2520RL%2520agent%2520to%2520randomized%2520terrains%2520in%2520simulation%2520can%250Aachieve%2520robust%2520walking%2520on%2520a%2520real%2520humanoid%2520robot%2520using%2520only%2520proprioceptive%250Afeedback.%2520We%2520train%2520an%2520end-to-end%2520bipedal%2520locomotion%2520policy%2520using%2520the%2520proposed%250Aapproach%252C%2520and%2520show%2520extensive%2520real-robot%2520demonstration%2520on%2520the%2520HRP-5P%2520humanoid%250Aover%2520several%2520difficult%2520terrains%2520inside%2520and%2520outside%2520the%2520lab%2520environment.%250AFurther%252C%2520we%2520argue%2520that%2520the%2520robustness%2520of%2520a%2520bipedal%2520walking%2520policy%2520can%2520be%250Aimproved%2520if%2520the%2520robot%2520is%2520allowed%2520to%2520exhibit%2520aperiodic%2520motion%2520with%2520variable%250Astepping%2520frequency.%2520We%2520propose%2520a%2520new%2520control%2520policy%2520to%2520enable%2520modification%2520of%250Athe%2520observed%2520clock%2520signal%252C%2520leading%2520to%2520adaptive%2520gait%2520frequencies%2520depending%2520on%250Athe%2520terrain%2520and%2520command%2520velocity.%2520Through%2520simulation%2520experiments%252C%2520we%2520show%2520the%250Aeffectiveness%2520of%2520this%2520policy%2520specifically%2520for%2520walking%2520over%2520challenging%2520terrains%250Aby%2520controlling%2520swing%2520and%2520stance%2520durations.%2520The%2520code%2520for%2520training%2520and%2520evaluation%250Ais%2520available%2520online%2520at%2520https%253A//github.com/rohanpsingh/LearningHumanoidWalking.%250ADemo%2520video%2520is%2520available%2520at%2520https%253A//www.youtube.com/watch%253Fv%253DZgfNzGAkk2Q.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Humanoid%20Walking%20on%20Compliant%20and%20Uneven%20Terrain%20with%20Deep%0A%20%20Reinforcement%20Learning&entry.906535625=Rohan%20P.%20Singh%20and%20Mitsuharu%20Morisawa%20and%20Mehdi%20Benallegue%20and%20Zhaoming%20Xie%20and%20Fumio%20Kanehiro&entry.1292438233=%20%20For%20the%20deployment%20of%20legged%20robots%20in%20real-world%20environments%2C%20it%20is%0Aessential%20to%20develop%20robust%20locomotion%20control%20methods%20for%20challenging%20terrains%0Athat%20may%20exhibit%20unexpected%20deformability%20and%20irregularity.%20In%20this%20paper%2C%20we%0Aexplore%20the%20application%20of%20sim-to-real%20deep%20reinforcement%20learning%20%28RL%29%20for%20the%0Adesign%20of%20bipedal%20locomotion%20controllers%20for%20humanoid%20robots%20on%20compliant%20and%0Auneven%20terrains.%20Our%20key%20contribution%20is%20to%20show%20that%20a%20simple%20training%0Acurriculum%20for%20exposing%20the%20RL%20agent%20to%20randomized%20terrains%20in%20simulation%20can%0Aachieve%20robust%20walking%20on%20a%20real%20humanoid%20robot%20using%20only%20proprioceptive%0Afeedback.%20We%20train%20an%20end-to-end%20bipedal%20locomotion%20policy%20using%20the%20proposed%0Aapproach%2C%20and%20show%20extensive%20real-robot%20demonstration%20on%20the%20HRP-5P%20humanoid%0Aover%20several%20difficult%20terrains%20inside%20and%20outside%20the%20lab%20environment.%0AFurther%2C%20we%20argue%20that%20the%20robustness%20of%20a%20bipedal%20walking%20policy%20can%20be%0Aimproved%20if%20the%20robot%20is%20allowed%20to%20exhibit%20aperiodic%20motion%20with%20variable%0Astepping%20frequency.%20We%20propose%20a%20new%20control%20policy%20to%20enable%20modification%20of%0Athe%20observed%20clock%20signal%2C%20leading%20to%20adaptive%20gait%20frequencies%20depending%20on%0Athe%20terrain%20and%20command%20velocity.%20Through%20simulation%20experiments%2C%20we%20show%20the%0Aeffectiveness%20of%20this%20policy%20specifically%20for%20walking%20over%20challenging%20terrains%0Aby%20controlling%20swing%20and%20stance%20durations.%20The%20code%20for%20training%20and%20evaluation%0Ais%20available%20online%20at%20https%3A//github.com/rohanpsingh/LearningHumanoidWalking.%0ADemo%20video%20is%20available%20at%20https%3A//www.youtube.com/watch%3Fv%3DZgfNzGAkk2Q.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13619v1&entry.124074799=Read"},
{"title": "A Dynamic Safety Shield for Safe and Efficient Reinforcement Learning of\n  Navigation Tasks", "author": "Murad Dawood and Ahmed Shokry and Maren Bennewitz", "abstract": "  Reinforcement learning (RL) has been successfully applied to a variety of\nrobotics applications, where it outperforms classical methods. However, the\nsafety aspect of RL and the transfer to the real world remain an open\nchallenge. A prominent field for tackling this challenge and ensuring the\nsafety of the agents during training and execution is safe reinforcement\nlearning. Safe RL can be achieved through constrained RL and safe exploration\napproaches. The former learns the safety constraints over the course of\ntraining to achieve a safe behavior by the end of training, at the cost of high\nnumber of collisions at earlier stages of the training. The latter offers\nrobust safety by enforcing the safety constraints as hard constraints, which\nprevents collisions but hinders the exploration of the RL agent, resulting in\nlower rewards and poor performance. To overcome those drawbacks, we propose a\nnovel safety shield, that combines the robustness of the optimization-based\ncontrollers with the long prediction capabilities of the RL agents, allowing\nthe RL agent to adaptively tune the parameters of the controller. Our approach\nis able to improve the exploration of the RL agents for navigation tasks, while\nminimizing the number of collisions. Experiments in simulation show that our\napproach outperforms state-of-the-art baselines in the reached\ngoals-to-collisions ratio in different challenging environments. The\ngoals-to-collisions ratio metrics emphasizes the importance of minimizing the\nnumber of collisions, while learning to accomplish the task. Our approach\nachieves a higher number of reached goals compared to the classic safety\nshields and fewer collisions compared to constrained RL approaches. Finally, we\ndemonstrate the performance of the proposed method in a real-world experiment.\n", "link": "http://arxiv.org/abs/2412.04153v2", "date": "2025-04-18", "relevancy": 1.6992, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5771}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5564}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dynamic%20Safety%20Shield%20for%20Safe%20and%20Efficient%20Reinforcement%20Learning%20of%0A%20%20Navigation%20Tasks&body=Title%3A%20A%20Dynamic%20Safety%20Shield%20for%20Safe%20and%20Efficient%20Reinforcement%20Learning%20of%0A%20%20Navigation%20Tasks%0AAuthor%3A%20Murad%20Dawood%20and%20Ahmed%20Shokry%20and%20Maren%20Bennewitz%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20been%20successfully%20applied%20to%20a%20variety%20of%0Arobotics%20applications%2C%20where%20it%20outperforms%20classical%20methods.%20However%2C%20the%0Asafety%20aspect%20of%20RL%20and%20the%20transfer%20to%20the%20real%20world%20remain%20an%20open%0Achallenge.%20A%20prominent%20field%20for%20tackling%20this%20challenge%20and%20ensuring%20the%0Asafety%20of%20the%20agents%20during%20training%20and%20execution%20is%20safe%20reinforcement%0Alearning.%20Safe%20RL%20can%20be%20achieved%20through%20constrained%20RL%20and%20safe%20exploration%0Aapproaches.%20The%20former%20learns%20the%20safety%20constraints%20over%20the%20course%20of%0Atraining%20to%20achieve%20a%20safe%20behavior%20by%20the%20end%20of%20training%2C%20at%20the%20cost%20of%20high%0Anumber%20of%20collisions%20at%20earlier%20stages%20of%20the%20training.%20The%20latter%20offers%0Arobust%20safety%20by%20enforcing%20the%20safety%20constraints%20as%20hard%20constraints%2C%20which%0Aprevents%20collisions%20but%20hinders%20the%20exploration%20of%20the%20RL%20agent%2C%20resulting%20in%0Alower%20rewards%20and%20poor%20performance.%20To%20overcome%20those%20drawbacks%2C%20we%20propose%20a%0Anovel%20safety%20shield%2C%20that%20combines%20the%20robustness%20of%20the%20optimization-based%0Acontrollers%20with%20the%20long%20prediction%20capabilities%20of%20the%20RL%20agents%2C%20allowing%0Athe%20RL%20agent%20to%20adaptively%20tune%20the%20parameters%20of%20the%20controller.%20Our%20approach%0Ais%20able%20to%20improve%20the%20exploration%20of%20the%20RL%20agents%20for%20navigation%20tasks%2C%20while%0Aminimizing%20the%20number%20of%20collisions.%20Experiments%20in%20simulation%20show%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20baselines%20in%20the%20reached%0Agoals-to-collisions%20ratio%20in%20different%20challenging%20environments.%20The%0Agoals-to-collisions%20ratio%20metrics%20emphasizes%20the%20importance%20of%20minimizing%20the%0Anumber%20of%20collisions%2C%20while%20learning%20to%20accomplish%20the%20task.%20Our%20approach%0Aachieves%20a%20higher%20number%20of%20reached%20goals%20compared%20to%20the%20classic%20safety%0Ashields%20and%20fewer%20collisions%20compared%20to%20constrained%20RL%20approaches.%20Finally%2C%20we%0Ademonstrate%20the%20performance%20of%20the%20proposed%20method%20in%20a%20real-world%20experiment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04153v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dynamic%2520Safety%2520Shield%2520for%2520Safe%2520and%2520Efficient%2520Reinforcement%2520Learning%2520of%250A%2520%2520Navigation%2520Tasks%26entry.906535625%3DMurad%2520Dawood%2520and%2520Ahmed%2520Shokry%2520and%2520Maren%2520Bennewitz%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520been%2520successfully%2520applied%2520to%2520a%2520variety%2520of%250Arobotics%2520applications%252C%2520where%2520it%2520outperforms%2520classical%2520methods.%2520However%252C%2520the%250Asafety%2520aspect%2520of%2520RL%2520and%2520the%2520transfer%2520to%2520the%2520real%2520world%2520remain%2520an%2520open%250Achallenge.%2520A%2520prominent%2520field%2520for%2520tackling%2520this%2520challenge%2520and%2520ensuring%2520the%250Asafety%2520of%2520the%2520agents%2520during%2520training%2520and%2520execution%2520is%2520safe%2520reinforcement%250Alearning.%2520Safe%2520RL%2520can%2520be%2520achieved%2520through%2520constrained%2520RL%2520and%2520safe%2520exploration%250Aapproaches.%2520The%2520former%2520learns%2520the%2520safety%2520constraints%2520over%2520the%2520course%2520of%250Atraining%2520to%2520achieve%2520a%2520safe%2520behavior%2520by%2520the%2520end%2520of%2520training%252C%2520at%2520the%2520cost%2520of%2520high%250Anumber%2520of%2520collisions%2520at%2520earlier%2520stages%2520of%2520the%2520training.%2520The%2520latter%2520offers%250Arobust%2520safety%2520by%2520enforcing%2520the%2520safety%2520constraints%2520as%2520hard%2520constraints%252C%2520which%250Aprevents%2520collisions%2520but%2520hinders%2520the%2520exploration%2520of%2520the%2520RL%2520agent%252C%2520resulting%2520in%250Alower%2520rewards%2520and%2520poor%2520performance.%2520To%2520overcome%2520those%2520drawbacks%252C%2520we%2520propose%2520a%250Anovel%2520safety%2520shield%252C%2520that%2520combines%2520the%2520robustness%2520of%2520the%2520optimization-based%250Acontrollers%2520with%2520the%2520long%2520prediction%2520capabilities%2520of%2520the%2520RL%2520agents%252C%2520allowing%250Athe%2520RL%2520agent%2520to%2520adaptively%2520tune%2520the%2520parameters%2520of%2520the%2520controller.%2520Our%2520approach%250Ais%2520able%2520to%2520improve%2520the%2520exploration%2520of%2520the%2520RL%2520agents%2520for%2520navigation%2520tasks%252C%2520while%250Aminimizing%2520the%2520number%2520of%2520collisions.%2520Experiments%2520in%2520simulation%2520show%2520that%2520our%250Aapproach%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520the%2520reached%250Agoals-to-collisions%2520ratio%2520in%2520different%2520challenging%2520environments.%2520The%250Agoals-to-collisions%2520ratio%2520metrics%2520emphasizes%2520the%2520importance%2520of%2520minimizing%2520the%250Anumber%2520of%2520collisions%252C%2520while%2520learning%2520to%2520accomplish%2520the%2520task.%2520Our%2520approach%250Aachieves%2520a%2520higher%2520number%2520of%2520reached%2520goals%2520compared%2520to%2520the%2520classic%2520safety%250Ashields%2520and%2520fewer%2520collisions%2520compared%2520to%2520constrained%2520RL%2520approaches.%2520Finally%252C%2520we%250Ademonstrate%2520the%2520performance%2520of%2520the%2520proposed%2520method%2520in%2520a%2520real-world%2520experiment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04153v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dynamic%20Safety%20Shield%20for%20Safe%20and%20Efficient%20Reinforcement%20Learning%20of%0A%20%20Navigation%20Tasks&entry.906535625=Murad%20Dawood%20and%20Ahmed%20Shokry%20and%20Maren%20Bennewitz&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20been%20successfully%20applied%20to%20a%20variety%20of%0Arobotics%20applications%2C%20where%20it%20outperforms%20classical%20methods.%20However%2C%20the%0Asafety%20aspect%20of%20RL%20and%20the%20transfer%20to%20the%20real%20world%20remain%20an%20open%0Achallenge.%20A%20prominent%20field%20for%20tackling%20this%20challenge%20and%20ensuring%20the%0Asafety%20of%20the%20agents%20during%20training%20and%20execution%20is%20safe%20reinforcement%0Alearning.%20Safe%20RL%20can%20be%20achieved%20through%20constrained%20RL%20and%20safe%20exploration%0Aapproaches.%20The%20former%20learns%20the%20safety%20constraints%20over%20the%20course%20of%0Atraining%20to%20achieve%20a%20safe%20behavior%20by%20the%20end%20of%20training%2C%20at%20the%20cost%20of%20high%0Anumber%20of%20collisions%20at%20earlier%20stages%20of%20the%20training.%20The%20latter%20offers%0Arobust%20safety%20by%20enforcing%20the%20safety%20constraints%20as%20hard%20constraints%2C%20which%0Aprevents%20collisions%20but%20hinders%20the%20exploration%20of%20the%20RL%20agent%2C%20resulting%20in%0Alower%20rewards%20and%20poor%20performance.%20To%20overcome%20those%20drawbacks%2C%20we%20propose%20a%0Anovel%20safety%20shield%2C%20that%20combines%20the%20robustness%20of%20the%20optimization-based%0Acontrollers%20with%20the%20long%20prediction%20capabilities%20of%20the%20RL%20agents%2C%20allowing%0Athe%20RL%20agent%20to%20adaptively%20tune%20the%20parameters%20of%20the%20controller.%20Our%20approach%0Ais%20able%20to%20improve%20the%20exploration%20of%20the%20RL%20agents%20for%20navigation%20tasks%2C%20while%0Aminimizing%20the%20number%20of%20collisions.%20Experiments%20in%20simulation%20show%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20baselines%20in%20the%20reached%0Agoals-to-collisions%20ratio%20in%20different%20challenging%20environments.%20The%0Agoals-to-collisions%20ratio%20metrics%20emphasizes%20the%20importance%20of%20minimizing%20the%0Anumber%20of%20collisions%2C%20while%20learning%20to%20accomplish%20the%20task.%20Our%20approach%0Aachieves%20a%20higher%20number%20of%20reached%20goals%20compared%20to%20the%20classic%20safety%0Ashields%20and%20fewer%20collisions%20compared%20to%20constrained%20RL%20approaches.%20Finally%2C%20we%0Ademonstrate%20the%20performance%20of%20the%20proposed%20method%20in%20a%20real-world%20experiment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04153v2&entry.124074799=Read"},
{"title": "Efficient Parameter Adaptation for Multi-Modal Medical Image\n  Segmentation and Prognosis", "author": "Numan Saeed and Shahad Hardan and Muhammad Ridzuan and Nada Saadi and Karthik Nandakumar and Mohammad Yaqub", "abstract": "  Cancer detection and prognosis relies heavily on medical imaging,\nparticularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise\nin tumor segmentation by fusing information from these modalities. However, a\ncritical bottleneck exists: the dependency on CT-PET data concurrently for\ntraining and inference, posing a challenge due to the limited availability of\nPET scans. Hence, there is a clear need for a flexible and efficient framework\nthat can be trained with the widely available CT scans and can be still adapted\nfor PET scans when they become available. In this work, we propose a\nparameter-efficient multi-modal adaptation (PEMMA) framework for lightweight\nupgrading of a transformer-based segmentation model trained only on CT scans\nsuch that it can be efficiently adapted for use with PET scans when they become\navailable. This framework is further extended to perform prognosis task\nmaintaining the same efficient cross-modal fine-tuning approach. The proposed\napproach is tested with two well-known segementation backbones, namely UNETR\nand Swin UNETR. Our approach offers two main advantages. Firstly, we leverage\nthe inherent modularity of the transformer architecture and perform low-rank\nadaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the\nattention weights to achieve parameter-efficient adaptation. Secondly, by\nminimizing cross-modal entanglement, PEMMA allows updates using only one\nmodality without causing catastrophic forgetting in the other. Our method\nachieves comparable performance to early fusion, but with only 8% of the\ntrainable parameters, and demonstrates a significant +28% Dice score\nimprovement on PET scans when trained with a single modality. Furthermore, in\nprognosis, our method improves the concordance index by +10% when adapting a\nCT-pretrained model to include PET scans, and by +23% when adapting for both\nPET and EHR data.\n", "link": "http://arxiv.org/abs/2504.13645v1", "date": "2025-04-18", "relevancy": 1.6905, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5829}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Parameter%20Adaptation%20for%20Multi-Modal%20Medical%20Image%0A%20%20Segmentation%20and%20Prognosis&body=Title%3A%20Efficient%20Parameter%20Adaptation%20for%20Multi-Modal%20Medical%20Image%0A%20%20Segmentation%20and%20Prognosis%0AAuthor%3A%20Numan%20Saeed%20and%20Shahad%20Hardan%20and%20Muhammad%20Ridzuan%20and%20Nada%20Saadi%20and%20Karthik%20Nandakumar%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Cancer%20detection%20and%20prognosis%20relies%20heavily%20on%20medical%20imaging%2C%0Aparticularly%20CT%20and%20PET%20scans.%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20shown%20promise%0Ain%20tumor%20segmentation%20by%20fusing%20information%20from%20these%20modalities.%20However%2C%20a%0Acritical%20bottleneck%20exists%3A%20the%20dependency%20on%20CT-PET%20data%20concurrently%20for%0Atraining%20and%20inference%2C%20posing%20a%20challenge%20due%20to%20the%20limited%20availability%20of%0APET%20scans.%20Hence%2C%20there%20is%20a%20clear%20need%20for%20a%20flexible%20and%20efficient%20framework%0Athat%20can%20be%20trained%20with%20the%20widely%20available%20CT%20scans%20and%20can%20be%20still%20adapted%0Afor%20PET%20scans%20when%20they%20become%20available.%20In%20this%20work%2C%20we%20propose%20a%0Aparameter-efficient%20multi-modal%20adaptation%20%28PEMMA%29%20framework%20for%20lightweight%0Aupgrading%20of%20a%20transformer-based%20segmentation%20model%20trained%20only%20on%20CT%20scans%0Asuch%20that%20it%20can%20be%20efficiently%20adapted%20for%20use%20with%20PET%20scans%20when%20they%20become%0Aavailable.%20This%20framework%20is%20further%20extended%20to%20perform%20prognosis%20task%0Amaintaining%20the%20same%20efficient%20cross-modal%20fine-tuning%20approach.%20The%20proposed%0Aapproach%20is%20tested%20with%20two%20well-known%20segementation%20backbones%2C%20namely%20UNETR%0Aand%20Swin%20UNETR.%20Our%20approach%20offers%20two%20main%20advantages.%20Firstly%2C%20we%20leverage%0Athe%20inherent%20modularity%20of%20the%20transformer%20architecture%20and%20perform%20low-rank%0Aadaptation%20%28LoRA%29%20as%20well%20as%20decomposed%20low-rank%20adaptation%20%28DoRA%29%20of%20the%0Aattention%20weights%20to%20achieve%20parameter-efficient%20adaptation.%20Secondly%2C%20by%0Aminimizing%20cross-modal%20entanglement%2C%20PEMMA%20allows%20updates%20using%20only%20one%0Amodality%20without%20causing%20catastrophic%20forgetting%20in%20the%20other.%20Our%20method%0Aachieves%20comparable%20performance%20to%20early%20fusion%2C%20but%20with%20only%208%25%20of%20the%0Atrainable%20parameters%2C%20and%20demonstrates%20a%20significant%20%2B28%25%20Dice%20score%0Aimprovement%20on%20PET%20scans%20when%20trained%20with%20a%20single%20modality.%20Furthermore%2C%20in%0Aprognosis%2C%20our%20method%20improves%20the%20concordance%20index%20by%20%2B10%25%20when%20adapting%20a%0ACT-pretrained%20model%20to%20include%20PET%20scans%2C%20and%20by%20%2B23%25%20when%20adapting%20for%20both%0APET%20and%20EHR%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Parameter%2520Adaptation%2520for%2520Multi-Modal%2520Medical%2520Image%250A%2520%2520Segmentation%2520and%2520Prognosis%26entry.906535625%3DNuman%2520Saeed%2520and%2520Shahad%2520Hardan%2520and%2520Muhammad%2520Ridzuan%2520and%2520Nada%2520Saadi%2520and%2520Karthik%2520Nandakumar%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Cancer%2520detection%2520and%2520prognosis%2520relies%2520heavily%2520on%2520medical%2520imaging%252C%250Aparticularly%2520CT%2520and%2520PET%2520scans.%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520have%2520shown%2520promise%250Ain%2520tumor%2520segmentation%2520by%2520fusing%2520information%2520from%2520these%2520modalities.%2520However%252C%2520a%250Acritical%2520bottleneck%2520exists%253A%2520the%2520dependency%2520on%2520CT-PET%2520data%2520concurrently%2520for%250Atraining%2520and%2520inference%252C%2520posing%2520a%2520challenge%2520due%2520to%2520the%2520limited%2520availability%2520of%250APET%2520scans.%2520Hence%252C%2520there%2520is%2520a%2520clear%2520need%2520for%2520a%2520flexible%2520and%2520efficient%2520framework%250Athat%2520can%2520be%2520trained%2520with%2520the%2520widely%2520available%2520CT%2520scans%2520and%2520can%2520be%2520still%2520adapted%250Afor%2520PET%2520scans%2520when%2520they%2520become%2520available.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Aparameter-efficient%2520multi-modal%2520adaptation%2520%2528PEMMA%2529%2520framework%2520for%2520lightweight%250Aupgrading%2520of%2520a%2520transformer-based%2520segmentation%2520model%2520trained%2520only%2520on%2520CT%2520scans%250Asuch%2520that%2520it%2520can%2520be%2520efficiently%2520adapted%2520for%2520use%2520with%2520PET%2520scans%2520when%2520they%2520become%250Aavailable.%2520This%2520framework%2520is%2520further%2520extended%2520to%2520perform%2520prognosis%2520task%250Amaintaining%2520the%2520same%2520efficient%2520cross-modal%2520fine-tuning%2520approach.%2520The%2520proposed%250Aapproach%2520is%2520tested%2520with%2520two%2520well-known%2520segementation%2520backbones%252C%2520namely%2520UNETR%250Aand%2520Swin%2520UNETR.%2520Our%2520approach%2520offers%2520two%2520main%2520advantages.%2520Firstly%252C%2520we%2520leverage%250Athe%2520inherent%2520modularity%2520of%2520the%2520transformer%2520architecture%2520and%2520perform%2520low-rank%250Aadaptation%2520%2528LoRA%2529%2520as%2520well%2520as%2520decomposed%2520low-rank%2520adaptation%2520%2528DoRA%2529%2520of%2520the%250Aattention%2520weights%2520to%2520achieve%2520parameter-efficient%2520adaptation.%2520Secondly%252C%2520by%250Aminimizing%2520cross-modal%2520entanglement%252C%2520PEMMA%2520allows%2520updates%2520using%2520only%2520one%250Amodality%2520without%2520causing%2520catastrophic%2520forgetting%2520in%2520the%2520other.%2520Our%2520method%250Aachieves%2520comparable%2520performance%2520to%2520early%2520fusion%252C%2520but%2520with%2520only%25208%2525%2520of%2520the%250Atrainable%2520parameters%252C%2520and%2520demonstrates%2520a%2520significant%2520%252B28%2525%2520Dice%2520score%250Aimprovement%2520on%2520PET%2520scans%2520when%2520trained%2520with%2520a%2520single%2520modality.%2520Furthermore%252C%2520in%250Aprognosis%252C%2520our%2520method%2520improves%2520the%2520concordance%2520index%2520by%2520%252B10%2525%2520when%2520adapting%2520a%250ACT-pretrained%2520model%2520to%2520include%2520PET%2520scans%252C%2520and%2520by%2520%252B23%2525%2520when%2520adapting%2520for%2520both%250APET%2520and%2520EHR%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Parameter%20Adaptation%20for%20Multi-Modal%20Medical%20Image%0A%20%20Segmentation%20and%20Prognosis&entry.906535625=Numan%20Saeed%20and%20Shahad%20Hardan%20and%20Muhammad%20Ridzuan%20and%20Nada%20Saadi%20and%20Karthik%20Nandakumar%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Cancer%20detection%20and%20prognosis%20relies%20heavily%20on%20medical%20imaging%2C%0Aparticularly%20CT%20and%20PET%20scans.%20Deep%20Neural%20Networks%20%28DNNs%29%20have%20shown%20promise%0Ain%20tumor%20segmentation%20by%20fusing%20information%20from%20these%20modalities.%20However%2C%20a%0Acritical%20bottleneck%20exists%3A%20the%20dependency%20on%20CT-PET%20data%20concurrently%20for%0Atraining%20and%20inference%2C%20posing%20a%20challenge%20due%20to%20the%20limited%20availability%20of%0APET%20scans.%20Hence%2C%20there%20is%20a%20clear%20need%20for%20a%20flexible%20and%20efficient%20framework%0Athat%20can%20be%20trained%20with%20the%20widely%20available%20CT%20scans%20and%20can%20be%20still%20adapted%0Afor%20PET%20scans%20when%20they%20become%20available.%20In%20this%20work%2C%20we%20propose%20a%0Aparameter-efficient%20multi-modal%20adaptation%20%28PEMMA%29%20framework%20for%20lightweight%0Aupgrading%20of%20a%20transformer-based%20segmentation%20model%20trained%20only%20on%20CT%20scans%0Asuch%20that%20it%20can%20be%20efficiently%20adapted%20for%20use%20with%20PET%20scans%20when%20they%20become%0Aavailable.%20This%20framework%20is%20further%20extended%20to%20perform%20prognosis%20task%0Amaintaining%20the%20same%20efficient%20cross-modal%20fine-tuning%20approach.%20The%20proposed%0Aapproach%20is%20tested%20with%20two%20well-known%20segementation%20backbones%2C%20namely%20UNETR%0Aand%20Swin%20UNETR.%20Our%20approach%20offers%20two%20main%20advantages.%20Firstly%2C%20we%20leverage%0Athe%20inherent%20modularity%20of%20the%20transformer%20architecture%20and%20perform%20low-rank%0Aadaptation%20%28LoRA%29%20as%20well%20as%20decomposed%20low-rank%20adaptation%20%28DoRA%29%20of%20the%0Aattention%20weights%20to%20achieve%20parameter-efficient%20adaptation.%20Secondly%2C%20by%0Aminimizing%20cross-modal%20entanglement%2C%20PEMMA%20allows%20updates%20using%20only%20one%0Amodality%20without%20causing%20catastrophic%20forgetting%20in%20the%20other.%20Our%20method%0Aachieves%20comparable%20performance%20to%20early%20fusion%2C%20but%20with%20only%208%25%20of%20the%0Atrainable%20parameters%2C%20and%20demonstrates%20a%20significant%20%2B28%25%20Dice%20score%0Aimprovement%20on%20PET%20scans%20when%20trained%20with%20a%20single%20modality.%20Furthermore%2C%20in%0Aprognosis%2C%20our%20method%20improves%20the%20concordance%20index%20by%20%2B10%25%20when%20adapting%20a%0ACT-pretrained%20model%20to%20include%20PET%20scans%2C%20and%20by%20%2B23%25%20when%20adapting%20for%20both%0APET%20and%20EHR%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13645v1&entry.124074799=Read"},
{"title": "Robust image classification with multi-modal large language models", "author": "Francesco Villani and Igor Maljkovic and Dario Lazzaro and Angelo Sotgiu and Antonio Emanuele Cin\u00e0 and Fabio Roli", "abstract": "  Deep Neural Networks are vulnerable to adversarial examples, i.e., carefully\ncrafted input samples that can cause models to make incorrect predictions with\nhigh confidence. To mitigate these vulnerabilities, adversarial training and\ndetection-based defenses have been proposed to strengthen models in advance.\nHowever, most of these approaches focus on a single data modality, overlooking\nthe relationships between visual patterns and textual descriptions of the\ninput. In this paper, we propose a novel defense, MultiShield, designed to\ncombine and complement these defenses with multi-modal information to further\nenhance their robustness. MultiShield leverages multi-modal large language\nmodels to detect adversarial examples and abstain from uncertain\nclassifications when there is no alignment between textual and visual\nrepresentations of the input. Extensive evaluations on CIFAR-10 and ImageNet\ndatasets, using robust and non-robust image classification models, demonstrate\nthat MultiShield can be easily integrated to detect and reject adversarial\nexamples, outperforming the original defenses.\n", "link": "http://arxiv.org/abs/2412.10353v2", "date": "2025-04-18", "relevancy": 1.6876, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5879}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5366}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20image%20classification%20with%20multi-modal%20large%20language%20models&body=Title%3A%20Robust%20image%20classification%20with%20multi-modal%20large%20language%20models%0AAuthor%3A%20Francesco%20Villani%20and%20Igor%20Maljkovic%20and%20Dario%20Lazzaro%20and%20Angelo%20Sotgiu%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20are%20vulnerable%20to%20adversarial%20examples%2C%20i.e.%2C%20carefully%0Acrafted%20input%20samples%20that%20can%20cause%20models%20to%20make%20incorrect%20predictions%20with%0Ahigh%20confidence.%20To%20mitigate%20these%20vulnerabilities%2C%20adversarial%20training%20and%0Adetection-based%20defenses%20have%20been%20proposed%20to%20strengthen%20models%20in%20advance.%0AHowever%2C%20most%20of%20these%20approaches%20focus%20on%20a%20single%20data%20modality%2C%20overlooking%0Athe%20relationships%20between%20visual%20patterns%20and%20textual%20descriptions%20of%20the%0Ainput.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20defense%2C%20MultiShield%2C%20designed%20to%0Acombine%20and%20complement%20these%20defenses%20with%20multi-modal%20information%20to%20further%0Aenhance%20their%20robustness.%20MultiShield%20leverages%20multi-modal%20large%20language%0Amodels%20to%20detect%20adversarial%20examples%20and%20abstain%20from%20uncertain%0Aclassifications%20when%20there%20is%20no%20alignment%20between%20textual%20and%20visual%0Arepresentations%20of%20the%20input.%20Extensive%20evaluations%20on%20CIFAR-10%20and%20ImageNet%0Adatasets%2C%20using%20robust%20and%20non-robust%20image%20classification%20models%2C%20demonstrate%0Athat%20MultiShield%20can%20be%20easily%20integrated%20to%20detect%20and%20reject%20adversarial%0Aexamples%2C%20outperforming%20the%20original%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10353v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520image%2520classification%2520with%2520multi-modal%2520large%2520language%2520models%26entry.906535625%3DFrancesco%2520Villani%2520and%2520Igor%2520Maljkovic%2520and%2520Dario%2520Lazzaro%2520and%2520Angelo%2520Sotgiu%2520and%2520Antonio%2520Emanuele%2520Cin%25C3%25A0%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520are%2520vulnerable%2520to%2520adversarial%2520examples%252C%2520i.e.%252C%2520carefully%250Acrafted%2520input%2520samples%2520that%2520can%2520cause%2520models%2520to%2520make%2520incorrect%2520predictions%2520with%250Ahigh%2520confidence.%2520To%2520mitigate%2520these%2520vulnerabilities%252C%2520adversarial%2520training%2520and%250Adetection-based%2520defenses%2520have%2520been%2520proposed%2520to%2520strengthen%2520models%2520in%2520advance.%250AHowever%252C%2520most%2520of%2520these%2520approaches%2520focus%2520on%2520a%2520single%2520data%2520modality%252C%2520overlooking%250Athe%2520relationships%2520between%2520visual%2520patterns%2520and%2520textual%2520descriptions%2520of%2520the%250Ainput.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520defense%252C%2520MultiShield%252C%2520designed%2520to%250Acombine%2520and%2520complement%2520these%2520defenses%2520with%2520multi-modal%2520information%2520to%2520further%250Aenhance%2520their%2520robustness.%2520MultiShield%2520leverages%2520multi-modal%2520large%2520language%250Amodels%2520to%2520detect%2520adversarial%2520examples%2520and%2520abstain%2520from%2520uncertain%250Aclassifications%2520when%2520there%2520is%2520no%2520alignment%2520between%2520textual%2520and%2520visual%250Arepresentations%2520of%2520the%2520input.%2520Extensive%2520evaluations%2520on%2520CIFAR-10%2520and%2520ImageNet%250Adatasets%252C%2520using%2520robust%2520and%2520non-robust%2520image%2520classification%2520models%252C%2520demonstrate%250Athat%2520MultiShield%2520can%2520be%2520easily%2520integrated%2520to%2520detect%2520and%2520reject%2520adversarial%250Aexamples%252C%2520outperforming%2520the%2520original%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10353v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20image%20classification%20with%20multi-modal%20large%20language%20models&entry.906535625=Francesco%20Villani%20and%20Igor%20Maljkovic%20and%20Dario%20Lazzaro%20and%20Angelo%20Sotgiu%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Fabio%20Roli&entry.1292438233=%20%20Deep%20Neural%20Networks%20are%20vulnerable%20to%20adversarial%20examples%2C%20i.e.%2C%20carefully%0Acrafted%20input%20samples%20that%20can%20cause%20models%20to%20make%20incorrect%20predictions%20with%0Ahigh%20confidence.%20To%20mitigate%20these%20vulnerabilities%2C%20adversarial%20training%20and%0Adetection-based%20defenses%20have%20been%20proposed%20to%20strengthen%20models%20in%20advance.%0AHowever%2C%20most%20of%20these%20approaches%20focus%20on%20a%20single%20data%20modality%2C%20overlooking%0Athe%20relationships%20between%20visual%20patterns%20and%20textual%20descriptions%20of%20the%0Ainput.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20defense%2C%20MultiShield%2C%20designed%20to%0Acombine%20and%20complement%20these%20defenses%20with%20multi-modal%20information%20to%20further%0Aenhance%20their%20robustness.%20MultiShield%20leverages%20multi-modal%20large%20language%0Amodels%20to%20detect%20adversarial%20examples%20and%20abstain%20from%20uncertain%0Aclassifications%20when%20there%20is%20no%20alignment%20between%20textual%20and%20visual%0Arepresentations%20of%20the%20input.%20Extensive%20evaluations%20on%20CIFAR-10%20and%20ImageNet%0Adatasets%2C%20using%20robust%20and%20non-robust%20image%20classification%20models%2C%20demonstrate%0Athat%20MultiShield%20can%20be%20easily%20integrated%20to%20detect%20and%20reject%20adversarial%0Aexamples%2C%20outperforming%20the%20original%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10353v2&entry.124074799=Read"},
{"title": "Entropic Time Schedulers for Generative Diffusion Models", "author": "Dejan Stancevic and Luca Ambrogioni", "abstract": "  The practical performance of generative diffusion models depends on the\nappropriate choice of the noise scheduling function, which can also be\nequivalently expressed as a time reparameterization. In this paper, we present\na time scheduler that selects sampling points based on entropy rather than\nuniform time spacing, ensuring that each point contributes an equal amount of\ninformation to the final generation. We prove that this time reparameterization\ndoes not depend on the initial choice of time. Furthermore, we provide a\ntractable exact formula to estimate this \\emph{entropic time} for a trained\nmodel using the training loss without substantial overhead. Alongside the\nentropic time, inspired by the optimality results, we introduce a rescaled\nentropic time. In our experiments with mixtures of Gaussian distributions and\nImageNet, we show that using the (rescaled) entropic times greatly improves the\ninference performance of trained models. In particular, we found that the image\nquality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can\nbe substantially increased by the rescaled entropic time reparameterization\nwithout increasing the number of function evaluations, with greater\nimprovements in the few NFEs regime.\n", "link": "http://arxiv.org/abs/2504.13612v1", "date": "2025-04-18", "relevancy": 1.6834, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5712}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropic%20Time%20Schedulers%20for%20Generative%20Diffusion%20Models&body=Title%3A%20Entropic%20Time%20Schedulers%20for%20Generative%20Diffusion%20Models%0AAuthor%3A%20Dejan%20Stancevic%20and%20Luca%20Ambrogioni%0AAbstract%3A%20%20%20The%20practical%20performance%20of%20generative%20diffusion%20models%20depends%20on%20the%0Aappropriate%20choice%20of%20the%20noise%20scheduling%20function%2C%20which%20can%20also%20be%0Aequivalently%20expressed%20as%20a%20time%20reparameterization.%20In%20this%20paper%2C%20we%20present%0Aa%20time%20scheduler%20that%20selects%20sampling%20points%20based%20on%20entropy%20rather%20than%0Auniform%20time%20spacing%2C%20ensuring%20that%20each%20point%20contributes%20an%20equal%20amount%20of%0Ainformation%20to%20the%20final%20generation.%20We%20prove%20that%20this%20time%20reparameterization%0Adoes%20not%20depend%20on%20the%20initial%20choice%20of%20time.%20Furthermore%2C%20we%20provide%20a%0Atractable%20exact%20formula%20to%20estimate%20this%20%5Cemph%7Bentropic%20time%7D%20for%20a%20trained%0Amodel%20using%20the%20training%20loss%20without%20substantial%20overhead.%20Alongside%20the%0Aentropic%20time%2C%20inspired%20by%20the%20optimality%20results%2C%20we%20introduce%20a%20rescaled%0Aentropic%20time.%20In%20our%20experiments%20with%20mixtures%20of%20Gaussian%20distributions%20and%0AImageNet%2C%20we%20show%20that%20using%20the%20%28rescaled%29%20entropic%20times%20greatly%20improves%20the%0Ainference%20performance%20of%20trained%20models.%20In%20particular%2C%20we%20found%20that%20the%20image%0Aquality%20in%20pretrained%20EDM2%20models%2C%20as%20evaluated%20by%20FID%20and%20FD-DINO%20scores%2C%20can%0Abe%20substantially%20increased%20by%20the%20rescaled%20entropic%20time%20reparameterization%0Awithout%20increasing%20the%20number%20of%20function%20evaluations%2C%20with%20greater%0Aimprovements%20in%20the%20few%20NFEs%20regime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropic%2520Time%2520Schedulers%2520for%2520Generative%2520Diffusion%2520Models%26entry.906535625%3DDejan%2520Stancevic%2520and%2520Luca%2520Ambrogioni%26entry.1292438233%3D%2520%2520The%2520practical%2520performance%2520of%2520generative%2520diffusion%2520models%2520depends%2520on%2520the%250Aappropriate%2520choice%2520of%2520the%2520noise%2520scheduling%2520function%252C%2520which%2520can%2520also%2520be%250Aequivalently%2520expressed%2520as%2520a%2520time%2520reparameterization.%2520In%2520this%2520paper%252C%2520we%2520present%250Aa%2520time%2520scheduler%2520that%2520selects%2520sampling%2520points%2520based%2520on%2520entropy%2520rather%2520than%250Auniform%2520time%2520spacing%252C%2520ensuring%2520that%2520each%2520point%2520contributes%2520an%2520equal%2520amount%2520of%250Ainformation%2520to%2520the%2520final%2520generation.%2520We%2520prove%2520that%2520this%2520time%2520reparameterization%250Adoes%2520not%2520depend%2520on%2520the%2520initial%2520choice%2520of%2520time.%2520Furthermore%252C%2520we%2520provide%2520a%250Atractable%2520exact%2520formula%2520to%2520estimate%2520this%2520%255Cemph%257Bentropic%2520time%257D%2520for%2520a%2520trained%250Amodel%2520using%2520the%2520training%2520loss%2520without%2520substantial%2520overhead.%2520Alongside%2520the%250Aentropic%2520time%252C%2520inspired%2520by%2520the%2520optimality%2520results%252C%2520we%2520introduce%2520a%2520rescaled%250Aentropic%2520time.%2520In%2520our%2520experiments%2520with%2520mixtures%2520of%2520Gaussian%2520distributions%2520and%250AImageNet%252C%2520we%2520show%2520that%2520using%2520the%2520%2528rescaled%2529%2520entropic%2520times%2520greatly%2520improves%2520the%250Ainference%2520performance%2520of%2520trained%2520models.%2520In%2520particular%252C%2520we%2520found%2520that%2520the%2520image%250Aquality%2520in%2520pretrained%2520EDM2%2520models%252C%2520as%2520evaluated%2520by%2520FID%2520and%2520FD-DINO%2520scores%252C%2520can%250Abe%2520substantially%2520increased%2520by%2520the%2520rescaled%2520entropic%2520time%2520reparameterization%250Awithout%2520increasing%2520the%2520number%2520of%2520function%2520evaluations%252C%2520with%2520greater%250Aimprovements%2520in%2520the%2520few%2520NFEs%2520regime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropic%20Time%20Schedulers%20for%20Generative%20Diffusion%20Models&entry.906535625=Dejan%20Stancevic%20and%20Luca%20Ambrogioni&entry.1292438233=%20%20The%20practical%20performance%20of%20generative%20diffusion%20models%20depends%20on%20the%0Aappropriate%20choice%20of%20the%20noise%20scheduling%20function%2C%20which%20can%20also%20be%0Aequivalently%20expressed%20as%20a%20time%20reparameterization.%20In%20this%20paper%2C%20we%20present%0Aa%20time%20scheduler%20that%20selects%20sampling%20points%20based%20on%20entropy%20rather%20than%0Auniform%20time%20spacing%2C%20ensuring%20that%20each%20point%20contributes%20an%20equal%20amount%20of%0Ainformation%20to%20the%20final%20generation.%20We%20prove%20that%20this%20time%20reparameterization%0Adoes%20not%20depend%20on%20the%20initial%20choice%20of%20time.%20Furthermore%2C%20we%20provide%20a%0Atractable%20exact%20formula%20to%20estimate%20this%20%5Cemph%7Bentropic%20time%7D%20for%20a%20trained%0Amodel%20using%20the%20training%20loss%20without%20substantial%20overhead.%20Alongside%20the%0Aentropic%20time%2C%20inspired%20by%20the%20optimality%20results%2C%20we%20introduce%20a%20rescaled%0Aentropic%20time.%20In%20our%20experiments%20with%20mixtures%20of%20Gaussian%20distributions%20and%0AImageNet%2C%20we%20show%20that%20using%20the%20%28rescaled%29%20entropic%20times%20greatly%20improves%20the%0Ainference%20performance%20of%20trained%20models.%20In%20particular%2C%20we%20found%20that%20the%20image%0Aquality%20in%20pretrained%20EDM2%20models%2C%20as%20evaluated%20by%20FID%20and%20FD-DINO%20scores%2C%20can%0Abe%20substantially%20increased%20by%20the%20rescaled%20entropic%20time%20reparameterization%0Awithout%20increasing%20the%20number%20of%20function%20evaluations%2C%20with%20greater%0Aimprovements%20in%20the%20few%20NFEs%20regime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13612v1&entry.124074799=Read"},
{"title": "Imitation Learning with Precisely Labeled Human Demonstrations", "author": "Yilong Song", "abstract": "  Within the imitation learning paradigm, training generalist robots requires\nlarge-scale datasets obtainable only through diverse curation. Due to the\nrelative ease to collect, human demonstrations constitute a valuable addition\nwhen incorporated appropriately. However, existing methods utilizing human\ndemonstrations face challenges in inferring precise actions, ameliorating\nembodiment gaps, and fusing with frontier generalist robot training pipelines.\nIn this work, building on prior studies that demonstrate the viability of using\nhand-held grippers for efficient data collection, we leverage the user's\ncontrol over the gripper's appearance--specifically by assigning it a unique,\neasily segmentable color--to enable simple and reliable application of the\nRANSAC and ICP registration method for precise end-effector pose estimation. We\nshow in simulation that precisely labeled human demonstrations on their own\nallow policies to reach on average 88.1% of the performance of using robot\ndemonstrations, and boost policy performance when combined with robot\ndemonstrations, despite the inherent embodiment gap.\n", "link": "http://arxiv.org/abs/2504.13803v1", "date": "2025-04-18", "relevancy": 1.1241, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5746}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5609}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitation%20Learning%20with%20Precisely%20Labeled%20Human%20Demonstrations&body=Title%3A%20Imitation%20Learning%20with%20Precisely%20Labeled%20Human%20Demonstrations%0AAuthor%3A%20Yilong%20Song%0AAbstract%3A%20%20%20Within%20the%20imitation%20learning%20paradigm%2C%20training%20generalist%20robots%20requires%0Alarge-scale%20datasets%20obtainable%20only%20through%20diverse%20curation.%20Due%20to%20the%0Arelative%20ease%20to%20collect%2C%20human%20demonstrations%20constitute%20a%20valuable%20addition%0Awhen%20incorporated%20appropriately.%20However%2C%20existing%20methods%20utilizing%20human%0Ademonstrations%20face%20challenges%20in%20inferring%20precise%20actions%2C%20ameliorating%0Aembodiment%20gaps%2C%20and%20fusing%20with%20frontier%20generalist%20robot%20training%20pipelines.%0AIn%20this%20work%2C%20building%20on%20prior%20studies%20that%20demonstrate%20the%20viability%20of%20using%0Ahand-held%20grippers%20for%20efficient%20data%20collection%2C%20we%20leverage%20the%20user%27s%0Acontrol%20over%20the%20gripper%27s%20appearance--specifically%20by%20assigning%20it%20a%20unique%2C%0Aeasily%20segmentable%20color--to%20enable%20simple%20and%20reliable%20application%20of%20the%0ARANSAC%20and%20ICP%20registration%20method%20for%20precise%20end-effector%20pose%20estimation.%20We%0Ashow%20in%20simulation%20that%20precisely%20labeled%20human%20demonstrations%20on%20their%20own%0Aallow%20policies%20to%20reach%20on%20average%2088.1%25%20of%20the%20performance%20of%20using%20robot%0Ademonstrations%2C%20and%20boost%20policy%20performance%20when%20combined%20with%20robot%0Ademonstrations%2C%20despite%20the%20inherent%20embodiment%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitation%2520Learning%2520with%2520Precisely%2520Labeled%2520Human%2520Demonstrations%26entry.906535625%3DYilong%2520Song%26entry.1292438233%3D%2520%2520Within%2520the%2520imitation%2520learning%2520paradigm%252C%2520training%2520generalist%2520robots%2520requires%250Alarge-scale%2520datasets%2520obtainable%2520only%2520through%2520diverse%2520curation.%2520Due%2520to%2520the%250Arelative%2520ease%2520to%2520collect%252C%2520human%2520demonstrations%2520constitute%2520a%2520valuable%2520addition%250Awhen%2520incorporated%2520appropriately.%2520However%252C%2520existing%2520methods%2520utilizing%2520human%250Ademonstrations%2520face%2520challenges%2520in%2520inferring%2520precise%2520actions%252C%2520ameliorating%250Aembodiment%2520gaps%252C%2520and%2520fusing%2520with%2520frontier%2520generalist%2520robot%2520training%2520pipelines.%250AIn%2520this%2520work%252C%2520building%2520on%2520prior%2520studies%2520that%2520demonstrate%2520the%2520viability%2520of%2520using%250Ahand-held%2520grippers%2520for%2520efficient%2520data%2520collection%252C%2520we%2520leverage%2520the%2520user%2527s%250Acontrol%2520over%2520the%2520gripper%2527s%2520appearance--specifically%2520by%2520assigning%2520it%2520a%2520unique%252C%250Aeasily%2520segmentable%2520color--to%2520enable%2520simple%2520and%2520reliable%2520application%2520of%2520the%250ARANSAC%2520and%2520ICP%2520registration%2520method%2520for%2520precise%2520end-effector%2520pose%2520estimation.%2520We%250Ashow%2520in%2520simulation%2520that%2520precisely%2520labeled%2520human%2520demonstrations%2520on%2520their%2520own%250Aallow%2520policies%2520to%2520reach%2520on%2520average%252088.1%2525%2520of%2520the%2520performance%2520of%2520using%2520robot%250Ademonstrations%252C%2520and%2520boost%2520policy%2520performance%2520when%2520combined%2520with%2520robot%250Ademonstrations%252C%2520despite%2520the%2520inherent%2520embodiment%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitation%20Learning%20with%20Precisely%20Labeled%20Human%20Demonstrations&entry.906535625=Yilong%20Song&entry.1292438233=%20%20Within%20the%20imitation%20learning%20paradigm%2C%20training%20generalist%20robots%20requires%0Alarge-scale%20datasets%20obtainable%20only%20through%20diverse%20curation.%20Due%20to%20the%0Arelative%20ease%20to%20collect%2C%20human%20demonstrations%20constitute%20a%20valuable%20addition%0Awhen%20incorporated%20appropriately.%20However%2C%20existing%20methods%20utilizing%20human%0Ademonstrations%20face%20challenges%20in%20inferring%20precise%20actions%2C%20ameliorating%0Aembodiment%20gaps%2C%20and%20fusing%20with%20frontier%20generalist%20robot%20training%20pipelines.%0AIn%20this%20work%2C%20building%20on%20prior%20studies%20that%20demonstrate%20the%20viability%20of%20using%0Ahand-held%20grippers%20for%20efficient%20data%20collection%2C%20we%20leverage%20the%20user%27s%0Acontrol%20over%20the%20gripper%27s%20appearance--specifically%20by%20assigning%20it%20a%20unique%2C%0Aeasily%20segmentable%20color--to%20enable%20simple%20and%20reliable%20application%20of%20the%0ARANSAC%20and%20ICP%20registration%20method%20for%20precise%20end-effector%20pose%20estimation.%20We%0Ashow%20in%20simulation%20that%20precisely%20labeled%20human%20demonstrations%20on%20their%20own%0Aallow%20policies%20to%20reach%20on%20average%2088.1%25%20of%20the%20performance%20of%20using%20robot%0Ademonstrations%2C%20and%20boost%20policy%20performance%20when%20combined%20with%20robot%0Ademonstrations%2C%20despite%20the%20inherent%20embodiment%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13803v1&entry.124074799=Read"},
{"title": "Near-optimal algorithms for private estimation and sequential testing of\n  collision probability", "author": "Robert Busa-Fekete and Umar Syed", "abstract": "  We present new algorithms for estimating and testing \\emph{collision\nprobability}, a fundamental measure of the spread of a discrete distribution\nthat is widely used in many scientific fields. We describe an algorithm that\nsatisfies $(\\alpha, \\beta)$-local differential privacy and estimates collision\nprobability with error at most $\\epsilon$ using\n$\\tilde{O}\\left(\\frac{\\log(1/\\beta)}{\\alpha^2 \\epsilon^2}\\right)$ samples for\n$\\alpha \\le 1$, which improves over previous work by a factor of\n$\\frac{1}{\\alpha^2}$. We also present a sequential testing algorithm for\ncollision probability, which can distinguish between collision probability\nvalues that are separated by $\\epsilon$ using $\\tilde{O}(\\frac{1}{\\epsilon^2})$\nsamples, even when $\\epsilon$ is unknown. Our algorithms have nearly the\noptimal sample complexity, and in experiments we show that they require\nsignificantly fewer samples than previous methods.\n", "link": "http://arxiv.org/abs/2504.13804v1", "date": "2025-04-18", "relevancy": 1.6704, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4183}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Near-optimal%20algorithms%20for%20private%20estimation%20and%20sequential%20testing%20of%0A%20%20collision%20probability&body=Title%3A%20Near-optimal%20algorithms%20for%20private%20estimation%20and%20sequential%20testing%20of%0A%20%20collision%20probability%0AAuthor%3A%20Robert%20Busa-Fekete%20and%20Umar%20Syed%0AAbstract%3A%20%20%20We%20present%20new%20algorithms%20for%20estimating%20and%20testing%20%5Cemph%7Bcollision%0Aprobability%7D%2C%20a%20fundamental%20measure%20of%20the%20spread%20of%20a%20discrete%20distribution%0Athat%20is%20widely%20used%20in%20many%20scientific%20fields.%20We%20describe%20an%20algorithm%20that%0Asatisfies%20%24%28%5Calpha%2C%20%5Cbeta%29%24-local%20differential%20privacy%20and%20estimates%20collision%0Aprobability%20with%20error%20at%20most%20%24%5Cepsilon%24%20using%0A%24%5Ctilde%7BO%7D%5Cleft%28%5Cfrac%7B%5Clog%281/%5Cbeta%29%7D%7B%5Calpha%5E2%20%5Cepsilon%5E2%7D%5Cright%29%24%20samples%20for%0A%24%5Calpha%20%5Cle%201%24%2C%20which%20improves%20over%20previous%20work%20by%20a%20factor%20of%0A%24%5Cfrac%7B1%7D%7B%5Calpha%5E2%7D%24.%20We%20also%20present%20a%20sequential%20testing%20algorithm%20for%0Acollision%20probability%2C%20which%20can%20distinguish%20between%20collision%20probability%0Avalues%20that%20are%20separated%20by%20%24%5Cepsilon%24%20using%20%24%5Ctilde%7BO%7D%28%5Cfrac%7B1%7D%7B%5Cepsilon%5E2%7D%29%24%0Asamples%2C%20even%20when%20%24%5Cepsilon%24%20is%20unknown.%20Our%20algorithms%20have%20nearly%20the%0Aoptimal%20sample%20complexity%2C%20and%20in%20experiments%20we%20show%20that%20they%20require%0Asignificantly%20fewer%20samples%20than%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNear-optimal%2520algorithms%2520for%2520private%2520estimation%2520and%2520sequential%2520testing%2520of%250A%2520%2520collision%2520probability%26entry.906535625%3DRobert%2520Busa-Fekete%2520and%2520Umar%2520Syed%26entry.1292438233%3D%2520%2520We%2520present%2520new%2520algorithms%2520for%2520estimating%2520and%2520testing%2520%255Cemph%257Bcollision%250Aprobability%257D%252C%2520a%2520fundamental%2520measure%2520of%2520the%2520spread%2520of%2520a%2520discrete%2520distribution%250Athat%2520is%2520widely%2520used%2520in%2520many%2520scientific%2520fields.%2520We%2520describe%2520an%2520algorithm%2520that%250Asatisfies%2520%2524%2528%255Calpha%252C%2520%255Cbeta%2529%2524-local%2520differential%2520privacy%2520and%2520estimates%2520collision%250Aprobability%2520with%2520error%2520at%2520most%2520%2524%255Cepsilon%2524%2520using%250A%2524%255Ctilde%257BO%257D%255Cleft%2528%255Cfrac%257B%255Clog%25281/%255Cbeta%2529%257D%257B%255Calpha%255E2%2520%255Cepsilon%255E2%257D%255Cright%2529%2524%2520samples%2520for%250A%2524%255Calpha%2520%255Cle%25201%2524%252C%2520which%2520improves%2520over%2520previous%2520work%2520by%2520a%2520factor%2520of%250A%2524%255Cfrac%257B1%257D%257B%255Calpha%255E2%257D%2524.%2520We%2520also%2520present%2520a%2520sequential%2520testing%2520algorithm%2520for%250Acollision%2520probability%252C%2520which%2520can%2520distinguish%2520between%2520collision%2520probability%250Avalues%2520that%2520are%2520separated%2520by%2520%2524%255Cepsilon%2524%2520using%2520%2524%255Ctilde%257BO%257D%2528%255Cfrac%257B1%257D%257B%255Cepsilon%255E2%257D%2529%2524%250Asamples%252C%2520even%2520when%2520%2524%255Cepsilon%2524%2520is%2520unknown.%2520Our%2520algorithms%2520have%2520nearly%2520the%250Aoptimal%2520sample%2520complexity%252C%2520and%2520in%2520experiments%2520we%2520show%2520that%2520they%2520require%250Asignificantly%2520fewer%2520samples%2520than%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Near-optimal%20algorithms%20for%20private%20estimation%20and%20sequential%20testing%20of%0A%20%20collision%20probability&entry.906535625=Robert%20Busa-Fekete%20and%20Umar%20Syed&entry.1292438233=%20%20We%20present%20new%20algorithms%20for%20estimating%20and%20testing%20%5Cemph%7Bcollision%0Aprobability%7D%2C%20a%20fundamental%20measure%20of%20the%20spread%20of%20a%20discrete%20distribution%0Athat%20is%20widely%20used%20in%20many%20scientific%20fields.%20We%20describe%20an%20algorithm%20that%0Asatisfies%20%24%28%5Calpha%2C%20%5Cbeta%29%24-local%20differential%20privacy%20and%20estimates%20collision%0Aprobability%20with%20error%20at%20most%20%24%5Cepsilon%24%20using%0A%24%5Ctilde%7BO%7D%5Cleft%28%5Cfrac%7B%5Clog%281/%5Cbeta%29%7D%7B%5Calpha%5E2%20%5Cepsilon%5E2%7D%5Cright%29%24%20samples%20for%0A%24%5Calpha%20%5Cle%201%24%2C%20which%20improves%20over%20previous%20work%20by%20a%20factor%20of%0A%24%5Cfrac%7B1%7D%7B%5Calpha%5E2%7D%24.%20We%20also%20present%20a%20sequential%20testing%20algorithm%20for%0Acollision%20probability%2C%20which%20can%20distinguish%20between%20collision%20probability%0Avalues%20that%20are%20separated%20by%20%24%5Cepsilon%24%20using%20%24%5Ctilde%7BO%7D%28%5Cfrac%7B1%7D%7B%5Cepsilon%5E2%7D%29%24%0Asamples%2C%20even%20when%20%24%5Cepsilon%24%20is%20unknown.%20Our%20algorithms%20have%20nearly%20the%0Aoptimal%20sample%20complexity%2C%20and%20in%20experiments%20we%20show%20that%20they%20require%0Asignificantly%20fewer%20samples%20than%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13804v1&entry.124074799=Read"},
{"title": "Multi-Type Context-Aware Conversational Recommender Systems via\n  Mixture-of-Experts", "author": "Jie Zou and Cheng Lin and Weikang Guo and Zheng Wang and Jiwei Wei and Yang Yang and Hengtao Shen", "abstract": "  Conversational recommender systems enable natural language conversations and\nthus lead to a more engaging and effective recommendation scenario. As the\nconversations for recommender systems usually contain limited contextual\ninformation, many existing conversational recommender systems incorporate\nexternal sources to enrich the contextual information. However, how to combine\ndifferent types of contextual information is still a challenge. In this paper,\nwe propose a multi-type context-aware conversational recommender system, called\nMCCRS, effectively fusing multi-type contextual information via\nmixture-of-experts to improve conversational recommender systems. MCCRS\nincorporates both structured information and unstructured information,\nincluding the structured knowledge graph, unstructured conversation history,\nand unstructured item reviews. It consists of several experts, with each expert\nspecialized in a particular domain (i.e., one specific contextual information).\nMultiple experts are then coordinated by a ChairBot to generate the final\nresults. Our proposed MCCRS model takes advantage of different contextual\ninformation and the specialization of different experts followed by a ChairBot\nbreaks the model bottleneck on a single contextual information. Experimental\nresults demonstrate that our proposed MCCRS method achieves significantly\nhigher performance compared to existing baselines.\n", "link": "http://arxiv.org/abs/2504.13655v1", "date": "2025-04-18", "relevancy": 1.4425, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5172}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4991}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Type%20Context-Aware%20Conversational%20Recommender%20Systems%20via%0A%20%20Mixture-of-Experts&body=Title%3A%20Multi-Type%20Context-Aware%20Conversational%20Recommender%20Systems%20via%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Jie%20Zou%20and%20Cheng%20Lin%20and%20Weikang%20Guo%20and%20Zheng%20Wang%20and%20Jiwei%20Wei%20and%20Yang%20Yang%20and%20Hengtao%20Shen%0AAbstract%3A%20%20%20Conversational%20recommender%20systems%20enable%20natural%20language%20conversations%20and%0Athus%20lead%20to%20a%20more%20engaging%20and%20effective%20recommendation%20scenario.%20As%20the%0Aconversations%20for%20recommender%20systems%20usually%20contain%20limited%20contextual%0Ainformation%2C%20many%20existing%20conversational%20recommender%20systems%20incorporate%0Aexternal%20sources%20to%20enrich%20the%20contextual%20information.%20However%2C%20how%20to%20combine%0Adifferent%20types%20of%20contextual%20information%20is%20still%20a%20challenge.%20In%20this%20paper%2C%0Awe%20propose%20a%20multi-type%20context-aware%20conversational%20recommender%20system%2C%20called%0AMCCRS%2C%20effectively%20fusing%20multi-type%20contextual%20information%20via%0Amixture-of-experts%20to%20improve%20conversational%20recommender%20systems.%20MCCRS%0Aincorporates%20both%20structured%20information%20and%20unstructured%20information%2C%0Aincluding%20the%20structured%20knowledge%20graph%2C%20unstructured%20conversation%20history%2C%0Aand%20unstructured%20item%20reviews.%20It%20consists%20of%20several%20experts%2C%20with%20each%20expert%0Aspecialized%20in%20a%20particular%20domain%20%28i.e.%2C%20one%20specific%20contextual%20information%29.%0AMultiple%20experts%20are%20then%20coordinated%20by%20a%20ChairBot%20to%20generate%20the%20final%0Aresults.%20Our%20proposed%20MCCRS%20model%20takes%20advantage%20of%20different%20contextual%0Ainformation%20and%20the%20specialization%20of%20different%20experts%20followed%20by%20a%20ChairBot%0Abreaks%20the%20model%20bottleneck%20on%20a%20single%20contextual%20information.%20Experimental%0Aresults%20demonstrate%20that%20our%20proposed%20MCCRS%20method%20achieves%20significantly%0Ahigher%20performance%20compared%20to%20existing%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Type%2520Context-Aware%2520Conversational%2520Recommender%2520Systems%2520via%250A%2520%2520Mixture-of-Experts%26entry.906535625%3DJie%2520Zou%2520and%2520Cheng%2520Lin%2520and%2520Weikang%2520Guo%2520and%2520Zheng%2520Wang%2520and%2520Jiwei%2520Wei%2520and%2520Yang%2520Yang%2520and%2520Hengtao%2520Shen%26entry.1292438233%3D%2520%2520Conversational%2520recommender%2520systems%2520enable%2520natural%2520language%2520conversations%2520and%250Athus%2520lead%2520to%2520a%2520more%2520engaging%2520and%2520effective%2520recommendation%2520scenario.%2520As%2520the%250Aconversations%2520for%2520recommender%2520systems%2520usually%2520contain%2520limited%2520contextual%250Ainformation%252C%2520many%2520existing%2520conversational%2520recommender%2520systems%2520incorporate%250Aexternal%2520sources%2520to%2520enrich%2520the%2520contextual%2520information.%2520However%252C%2520how%2520to%2520combine%250Adifferent%2520types%2520of%2520contextual%2520information%2520is%2520still%2520a%2520challenge.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520multi-type%2520context-aware%2520conversational%2520recommender%2520system%252C%2520called%250AMCCRS%252C%2520effectively%2520fusing%2520multi-type%2520contextual%2520information%2520via%250Amixture-of-experts%2520to%2520improve%2520conversational%2520recommender%2520systems.%2520MCCRS%250Aincorporates%2520both%2520structured%2520information%2520and%2520unstructured%2520information%252C%250Aincluding%2520the%2520structured%2520knowledge%2520graph%252C%2520unstructured%2520conversation%2520history%252C%250Aand%2520unstructured%2520item%2520reviews.%2520It%2520consists%2520of%2520several%2520experts%252C%2520with%2520each%2520expert%250Aspecialized%2520in%2520a%2520particular%2520domain%2520%2528i.e.%252C%2520one%2520specific%2520contextual%2520information%2529.%250AMultiple%2520experts%2520are%2520then%2520coordinated%2520by%2520a%2520ChairBot%2520to%2520generate%2520the%2520final%250Aresults.%2520Our%2520proposed%2520MCCRS%2520model%2520takes%2520advantage%2520of%2520different%2520contextual%250Ainformation%2520and%2520the%2520specialization%2520of%2520different%2520experts%2520followed%2520by%2520a%2520ChairBot%250Abreaks%2520the%2520model%2520bottleneck%2520on%2520a%2520single%2520contextual%2520information.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520proposed%2520MCCRS%2520method%2520achieves%2520significantly%250Ahigher%2520performance%2520compared%2520to%2520existing%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Type%20Context-Aware%20Conversational%20Recommender%20Systems%20via%0A%20%20Mixture-of-Experts&entry.906535625=Jie%20Zou%20and%20Cheng%20Lin%20and%20Weikang%20Guo%20and%20Zheng%20Wang%20and%20Jiwei%20Wei%20and%20Yang%20Yang%20and%20Hengtao%20Shen&entry.1292438233=%20%20Conversational%20recommender%20systems%20enable%20natural%20language%20conversations%20and%0Athus%20lead%20to%20a%20more%20engaging%20and%20effective%20recommendation%20scenario.%20As%20the%0Aconversations%20for%20recommender%20systems%20usually%20contain%20limited%20contextual%0Ainformation%2C%20many%20existing%20conversational%20recommender%20systems%20incorporate%0Aexternal%20sources%20to%20enrich%20the%20contextual%20information.%20However%2C%20how%20to%20combine%0Adifferent%20types%20of%20contextual%20information%20is%20still%20a%20challenge.%20In%20this%20paper%2C%0Awe%20propose%20a%20multi-type%20context-aware%20conversational%20recommender%20system%2C%20called%0AMCCRS%2C%20effectively%20fusing%20multi-type%20contextual%20information%20via%0Amixture-of-experts%20to%20improve%20conversational%20recommender%20systems.%20MCCRS%0Aincorporates%20both%20structured%20information%20and%20unstructured%20information%2C%0Aincluding%20the%20structured%20knowledge%20graph%2C%20unstructured%20conversation%20history%2C%0Aand%20unstructured%20item%20reviews.%20It%20consists%20of%20several%20experts%2C%20with%20each%20expert%0Aspecialized%20in%20a%20particular%20domain%20%28i.e.%2C%20one%20specific%20contextual%20information%29.%0AMultiple%20experts%20are%20then%20coordinated%20by%20a%20ChairBot%20to%20generate%20the%20final%0Aresults.%20Our%20proposed%20MCCRS%20model%20takes%20advantage%20of%20different%20contextual%0Ainformation%20and%20the%20specialization%20of%20different%20experts%20followed%20by%20a%20ChairBot%0Abreaks%20the%20model%20bottleneck%20on%20a%20single%20contextual%20information.%20Experimental%0Aresults%20demonstrate%20that%20our%20proposed%20MCCRS%20method%20achieves%20significantly%0Ahigher%20performance%20compared%20to%20existing%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13655v1&entry.124074799=Read"},
{"title": "Variable transformations in consistent loss functions", "author": "Hristos Tyralis and Georgia Papacharalampous", "abstract": "  Loss functions constructed by applying transformations to the realization and\nprediction variables of (strictly) consistent loss functions have been\nextensively studied empirically, yet their theoretical foundations remain\nunexplored. To address this gap, we establish formal characterizations of\n(strict) consistency for such transformed loss functions and their\ncorresponding elicitable functionals. Our analysis focuses on two interrelated\ncases: (a) transformations applied solely to the realization variable and (b)\nbijective transformations applied jointly to both the realization and\nprediction variables. These cases extend the well-established framework of\ntransformations applied exclusively to the prediction variable, as formalized\nby Osband's revelation principle. We further develop analogous\ncharacterizations for (strict) identification functions. The resulting\ntheoretical framework is broadly applicable to statistical and machine learning\nmethodologies. When applied to Bregman and expectile loss functions, our\nframework enables two key advancements: (a) the interpretation of empirical\nfindings from models trained with transformed loss functions and (b) the\nsystematic construction of novel identifiable and elicitable functionals,\nincluding the g-transformed expectation and g-transformed expectile. By\nunifying theoretical insights with practical applications, this work advances\nprincipled methodologies for designing loss functions in complex predictive\ntasks. Applications of the framework to simulated and real-world data\nillustrate its practical utility in diverse settings.\n", "link": "http://arxiv.org/abs/2502.16542v2", "date": "2025-04-18", "relevancy": 1.3658, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4653}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variable%20transformations%20in%20consistent%20loss%20functions&body=Title%3A%20Variable%20transformations%20in%20consistent%20loss%20functions%0AAuthor%3A%20Hristos%20Tyralis%20and%20Georgia%20Papacharalampous%0AAbstract%3A%20%20%20Loss%20functions%20constructed%20by%20applying%20transformations%20to%20the%20realization%20and%0Aprediction%20variables%20of%20%28strictly%29%20consistent%20loss%20functions%20have%20been%0Aextensively%20studied%20empirically%2C%20yet%20their%20theoretical%20foundations%20remain%0Aunexplored.%20To%20address%20this%20gap%2C%20we%20establish%20formal%20characterizations%20of%0A%28strict%29%20consistency%20for%20such%20transformed%20loss%20functions%20and%20their%0Acorresponding%20elicitable%20functionals.%20Our%20analysis%20focuses%20on%20two%20interrelated%0Acases%3A%20%28a%29%20transformations%20applied%20solely%20to%20the%20realization%20variable%20and%20%28b%29%0Abijective%20transformations%20applied%20jointly%20to%20both%20the%20realization%20and%0Aprediction%20variables.%20These%20cases%20extend%20the%20well-established%20framework%20of%0Atransformations%20applied%20exclusively%20to%20the%20prediction%20variable%2C%20as%20formalized%0Aby%20Osband%27s%20revelation%20principle.%20We%20further%20develop%20analogous%0Acharacterizations%20for%20%28strict%29%20identification%20functions.%20The%20resulting%0Atheoretical%20framework%20is%20broadly%20applicable%20to%20statistical%20and%20machine%20learning%0Amethodologies.%20When%20applied%20to%20Bregman%20and%20expectile%20loss%20functions%2C%20our%0Aframework%20enables%20two%20key%20advancements%3A%20%28a%29%20the%20interpretation%20of%20empirical%0Afindings%20from%20models%20trained%20with%20transformed%20loss%20functions%20and%20%28b%29%20the%0Asystematic%20construction%20of%20novel%20identifiable%20and%20elicitable%20functionals%2C%0Aincluding%20the%20g-transformed%20expectation%20and%20g-transformed%20expectile.%20By%0Aunifying%20theoretical%20insights%20with%20practical%20applications%2C%20this%20work%20advances%0Aprincipled%20methodologies%20for%20designing%20loss%20functions%20in%20complex%20predictive%0Atasks.%20Applications%20of%20the%20framework%20to%20simulated%20and%20real-world%20data%0Aillustrate%20its%20practical%20utility%20in%20diverse%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariable%2520transformations%2520in%2520consistent%2520loss%2520functions%26entry.906535625%3DHristos%2520Tyralis%2520and%2520Georgia%2520Papacharalampous%26entry.1292438233%3D%2520%2520Loss%2520functions%2520constructed%2520by%2520applying%2520transformations%2520to%2520the%2520realization%2520and%250Aprediction%2520variables%2520of%2520%2528strictly%2529%2520consistent%2520loss%2520functions%2520have%2520been%250Aextensively%2520studied%2520empirically%252C%2520yet%2520their%2520theoretical%2520foundations%2520remain%250Aunexplored.%2520To%2520address%2520this%2520gap%252C%2520we%2520establish%2520formal%2520characterizations%2520of%250A%2528strict%2529%2520consistency%2520for%2520such%2520transformed%2520loss%2520functions%2520and%2520their%250Acorresponding%2520elicitable%2520functionals.%2520Our%2520analysis%2520focuses%2520on%2520two%2520interrelated%250Acases%253A%2520%2528a%2529%2520transformations%2520applied%2520solely%2520to%2520the%2520realization%2520variable%2520and%2520%2528b%2529%250Abijective%2520transformations%2520applied%2520jointly%2520to%2520both%2520the%2520realization%2520and%250Aprediction%2520variables.%2520These%2520cases%2520extend%2520the%2520well-established%2520framework%2520of%250Atransformations%2520applied%2520exclusively%2520to%2520the%2520prediction%2520variable%252C%2520as%2520formalized%250Aby%2520Osband%2527s%2520revelation%2520principle.%2520We%2520further%2520develop%2520analogous%250Acharacterizations%2520for%2520%2528strict%2529%2520identification%2520functions.%2520The%2520resulting%250Atheoretical%2520framework%2520is%2520broadly%2520applicable%2520to%2520statistical%2520and%2520machine%2520learning%250Amethodologies.%2520When%2520applied%2520to%2520Bregman%2520and%2520expectile%2520loss%2520functions%252C%2520our%250Aframework%2520enables%2520two%2520key%2520advancements%253A%2520%2528a%2529%2520the%2520interpretation%2520of%2520empirical%250Afindings%2520from%2520models%2520trained%2520with%2520transformed%2520loss%2520functions%2520and%2520%2528b%2529%2520the%250Asystematic%2520construction%2520of%2520novel%2520identifiable%2520and%2520elicitable%2520functionals%252C%250Aincluding%2520the%2520g-transformed%2520expectation%2520and%2520g-transformed%2520expectile.%2520By%250Aunifying%2520theoretical%2520insights%2520with%2520practical%2520applications%252C%2520this%2520work%2520advances%250Aprincipled%2520methodologies%2520for%2520designing%2520loss%2520functions%2520in%2520complex%2520predictive%250Atasks.%2520Applications%2520of%2520the%2520framework%2520to%2520simulated%2520and%2520real-world%2520data%250Aillustrate%2520its%2520practical%2520utility%2520in%2520diverse%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variable%20transformations%20in%20consistent%20loss%20functions&entry.906535625=Hristos%20Tyralis%20and%20Georgia%20Papacharalampous&entry.1292438233=%20%20Loss%20functions%20constructed%20by%20applying%20transformations%20to%20the%20realization%20and%0Aprediction%20variables%20of%20%28strictly%29%20consistent%20loss%20functions%20have%20been%0Aextensively%20studied%20empirically%2C%20yet%20their%20theoretical%20foundations%20remain%0Aunexplored.%20To%20address%20this%20gap%2C%20we%20establish%20formal%20characterizations%20of%0A%28strict%29%20consistency%20for%20such%20transformed%20loss%20functions%20and%20their%0Acorresponding%20elicitable%20functionals.%20Our%20analysis%20focuses%20on%20two%20interrelated%0Acases%3A%20%28a%29%20transformations%20applied%20solely%20to%20the%20realization%20variable%20and%20%28b%29%0Abijective%20transformations%20applied%20jointly%20to%20both%20the%20realization%20and%0Aprediction%20variables.%20These%20cases%20extend%20the%20well-established%20framework%20of%0Atransformations%20applied%20exclusively%20to%20the%20prediction%20variable%2C%20as%20formalized%0Aby%20Osband%27s%20revelation%20principle.%20We%20further%20develop%20analogous%0Acharacterizations%20for%20%28strict%29%20identification%20functions.%20The%20resulting%0Atheoretical%20framework%20is%20broadly%20applicable%20to%20statistical%20and%20machine%20learning%0Amethodologies.%20When%20applied%20to%20Bregman%20and%20expectile%20loss%20functions%2C%20our%0Aframework%20enables%20two%20key%20advancements%3A%20%28a%29%20the%20interpretation%20of%20empirical%0Afindings%20from%20models%20trained%20with%20transformed%20loss%20functions%20and%20%28b%29%20the%0Asystematic%20construction%20of%20novel%20identifiable%20and%20elicitable%20functionals%2C%0Aincluding%20the%20g-transformed%20expectation%20and%20g-transformed%20expectile.%20By%0Aunifying%20theoretical%20insights%20with%20practical%20applications%2C%20this%20work%20advances%0Aprincipled%20methodologies%20for%20designing%20loss%20functions%20in%20complex%20predictive%0Atasks.%20Applications%20of%20the%20framework%20to%20simulated%20and%20real-world%20data%0Aillustrate%20its%20practical%20utility%20in%20diverse%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16542v2&entry.124074799=Read"},
{"title": "Learning Through Retrospection: Improving Trajectory Prediction for\n  Automated Driving with Error Feedback", "author": "Steffen Hagedorn and Aron Distelzweig and Marcel Hallgarten and Alexandru P. Condurache", "abstract": "  In automated driving, predicting trajectories of surrounding vehicles\nsupports reasoning about scene dynamics and enables safe planning for the ego\nvehicle. However, existing models handle predictions as an instantaneous task\nof forecasting future trajectories based on observed information. As time\nproceeds, the next prediction is made independently of the previous one, which\nmeans that the model cannot correct its errors during inference and will repeat\nthem. To alleviate this problem and better leverage temporal data, we propose a\nnovel retrospection technique. Through training on closed-loop rollouts the\nmodel learns to use aggregated feedback. Given new observations it reflects on\nprevious predictions and analyzes its errors to improve the quality of\nsubsequent predictions. Thus, the model can learn to correct systematic errors\nduring inference. Comprehensive experiments on nuScenes and Argoverse\ndemonstrate a considerable decrease in minimum Average Displacement Error of up\nto 31.9% compared to the state-of-the-art baseline without retrospection. We\nfurther showcase the robustness of our technique by demonstrating a better\nhandling of out-of-distribution scenarios with undetected road-users.\n", "link": "http://arxiv.org/abs/2504.13785v1", "date": "2025-04-18", "relevancy": 1.6238, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5456}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Through%20Retrospection%3A%20Improving%20Trajectory%20Prediction%20for%0A%20%20Automated%20Driving%20with%20Error%20Feedback&body=Title%3A%20Learning%20Through%20Retrospection%3A%20Improving%20Trajectory%20Prediction%20for%0A%20%20Automated%20Driving%20with%20Error%20Feedback%0AAuthor%3A%20Steffen%20Hagedorn%20and%20Aron%20Distelzweig%20and%20Marcel%20Hallgarten%20and%20Alexandru%20P.%20Condurache%0AAbstract%3A%20%20%20In%20automated%20driving%2C%20predicting%20trajectories%20of%20surrounding%20vehicles%0Asupports%20reasoning%20about%20scene%20dynamics%20and%20enables%20safe%20planning%20for%20the%20ego%0Avehicle.%20However%2C%20existing%20models%20handle%20predictions%20as%20an%20instantaneous%20task%0Aof%20forecasting%20future%20trajectories%20based%20on%20observed%20information.%20As%20time%0Aproceeds%2C%20the%20next%20prediction%20is%20made%20independently%20of%20the%20previous%20one%2C%20which%0Ameans%20that%20the%20model%20cannot%20correct%20its%20errors%20during%20inference%20and%20will%20repeat%0Athem.%20To%20alleviate%20this%20problem%20and%20better%20leverage%20temporal%20data%2C%20we%20propose%20a%0Anovel%20retrospection%20technique.%20Through%20training%20on%20closed-loop%20rollouts%20the%0Amodel%20learns%20to%20use%20aggregated%20feedback.%20Given%20new%20observations%20it%20reflects%20on%0Aprevious%20predictions%20and%20analyzes%20its%20errors%20to%20improve%20the%20quality%20of%0Asubsequent%20predictions.%20Thus%2C%20the%20model%20can%20learn%20to%20correct%20systematic%20errors%0Aduring%20inference.%20Comprehensive%20experiments%20on%20nuScenes%20and%20Argoverse%0Ademonstrate%20a%20considerable%20decrease%20in%20minimum%20Average%20Displacement%20Error%20of%20up%0Ato%2031.9%25%20compared%20to%20the%20state-of-the-art%20baseline%20without%20retrospection.%20We%0Afurther%20showcase%20the%20robustness%20of%20our%20technique%20by%20demonstrating%20a%20better%0Ahandling%20of%20out-of-distribution%20scenarios%20with%20undetected%20road-users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Through%2520Retrospection%253A%2520Improving%2520Trajectory%2520Prediction%2520for%250A%2520%2520Automated%2520Driving%2520with%2520Error%2520Feedback%26entry.906535625%3DSteffen%2520Hagedorn%2520and%2520Aron%2520Distelzweig%2520and%2520Marcel%2520Hallgarten%2520and%2520Alexandru%2520P.%2520Condurache%26entry.1292438233%3D%2520%2520In%2520automated%2520driving%252C%2520predicting%2520trajectories%2520of%2520surrounding%2520vehicles%250Asupports%2520reasoning%2520about%2520scene%2520dynamics%2520and%2520enables%2520safe%2520planning%2520for%2520the%2520ego%250Avehicle.%2520However%252C%2520existing%2520models%2520handle%2520predictions%2520as%2520an%2520instantaneous%2520task%250Aof%2520forecasting%2520future%2520trajectories%2520based%2520on%2520observed%2520information.%2520As%2520time%250Aproceeds%252C%2520the%2520next%2520prediction%2520is%2520made%2520independently%2520of%2520the%2520previous%2520one%252C%2520which%250Ameans%2520that%2520the%2520model%2520cannot%2520correct%2520its%2520errors%2520during%2520inference%2520and%2520will%2520repeat%250Athem.%2520To%2520alleviate%2520this%2520problem%2520and%2520better%2520leverage%2520temporal%2520data%252C%2520we%2520propose%2520a%250Anovel%2520retrospection%2520technique.%2520Through%2520training%2520on%2520closed-loop%2520rollouts%2520the%250Amodel%2520learns%2520to%2520use%2520aggregated%2520feedback.%2520Given%2520new%2520observations%2520it%2520reflects%2520on%250Aprevious%2520predictions%2520and%2520analyzes%2520its%2520errors%2520to%2520improve%2520the%2520quality%2520of%250Asubsequent%2520predictions.%2520Thus%252C%2520the%2520model%2520can%2520learn%2520to%2520correct%2520systematic%2520errors%250Aduring%2520inference.%2520Comprehensive%2520experiments%2520on%2520nuScenes%2520and%2520Argoverse%250Ademonstrate%2520a%2520considerable%2520decrease%2520in%2520minimum%2520Average%2520Displacement%2520Error%2520of%2520up%250Ato%252031.9%2525%2520compared%2520to%2520the%2520state-of-the-art%2520baseline%2520without%2520retrospection.%2520We%250Afurther%2520showcase%2520the%2520robustness%2520of%2520our%2520technique%2520by%2520demonstrating%2520a%2520better%250Ahandling%2520of%2520out-of-distribution%2520scenarios%2520with%2520undetected%2520road-users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Through%20Retrospection%3A%20Improving%20Trajectory%20Prediction%20for%0A%20%20Automated%20Driving%20with%20Error%20Feedback&entry.906535625=Steffen%20Hagedorn%20and%20Aron%20Distelzweig%20and%20Marcel%20Hallgarten%20and%20Alexandru%20P.%20Condurache&entry.1292438233=%20%20In%20automated%20driving%2C%20predicting%20trajectories%20of%20surrounding%20vehicles%0Asupports%20reasoning%20about%20scene%20dynamics%20and%20enables%20safe%20planning%20for%20the%20ego%0Avehicle.%20However%2C%20existing%20models%20handle%20predictions%20as%20an%20instantaneous%20task%0Aof%20forecasting%20future%20trajectories%20based%20on%20observed%20information.%20As%20time%0Aproceeds%2C%20the%20next%20prediction%20is%20made%20independently%20of%20the%20previous%20one%2C%20which%0Ameans%20that%20the%20model%20cannot%20correct%20its%20errors%20during%20inference%20and%20will%20repeat%0Athem.%20To%20alleviate%20this%20problem%20and%20better%20leverage%20temporal%20data%2C%20we%20propose%20a%0Anovel%20retrospection%20technique.%20Through%20training%20on%20closed-loop%20rollouts%20the%0Amodel%20learns%20to%20use%20aggregated%20feedback.%20Given%20new%20observations%20it%20reflects%20on%0Aprevious%20predictions%20and%20analyzes%20its%20errors%20to%20improve%20the%20quality%20of%0Asubsequent%20predictions.%20Thus%2C%20the%20model%20can%20learn%20to%20correct%20systematic%20errors%0Aduring%20inference.%20Comprehensive%20experiments%20on%20nuScenes%20and%20Argoverse%0Ademonstrate%20a%20considerable%20decrease%20in%20minimum%20Average%20Displacement%20Error%20of%20up%0Ato%2031.9%25%20compared%20to%20the%20state-of-the-art%20baseline%20without%20retrospection.%20We%0Afurther%20showcase%20the%20robustness%20of%20our%20technique%20by%20demonstrating%20a%20better%0Ahandling%20of%20out-of-distribution%20scenarios%20with%20undetected%20road-users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13785v1&entry.124074799=Read"},
{"title": "Transferrable Surrogates in Expressive Neural Architecture Search Spaces", "author": "Shiwen Qin and Gabriela Kadlecov\u00e1 and Martin Pil\u00e1t and Shay B. Cohen and Roman Neruda and Elliot J. Crowley and Jovita Lukasik and Linus Ericsson", "abstract": "  Neural architecture search (NAS) faces a challenge in balancing the\nexploration of expressive, broad search spaces that enable architectural\ninnovation with the need for efficient evaluation of architectures to\neffectively search such spaces. We investigate surrogate model training for\nimproving search in highly expressive NAS search spaces based on context-free\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\nhave high predictive power for the performance of architectures both within and\nacross datasets, ii) these surrogates can be used to filter out bad\narchitectures when searching on novel datasets, thereby significantly speeding\nup search and achieving better final performances, and iii) the surrogates can\nbe further used directly as the search objective for huge speed-ups.\n", "link": "http://arxiv.org/abs/2504.12971v2", "date": "2025-04-18", "relevancy": 1.0374, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5349}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5147}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferrable%20Surrogates%20in%20Expressive%20Neural%20Architecture%20Search%20Spaces&body=Title%3A%20Transferrable%20Surrogates%20in%20Expressive%20Neural%20Architecture%20Search%20Spaces%0AAuthor%3A%20Shiwen%20Qin%20and%20Gabriela%20Kadlecov%C3%A1%20and%20Martin%20Pil%C3%A1t%20and%20Shay%20B.%20Cohen%20and%20Roman%20Neruda%20and%20Elliot%20J.%20Crowley%20and%20Jovita%20Lukasik%20and%20Linus%20Ericsson%0AAbstract%3A%20%20%20Neural%20architecture%20search%20%28NAS%29%20faces%20a%20challenge%20in%20balancing%20the%0Aexploration%20of%20expressive%2C%20broad%20search%20spaces%20that%20enable%20architectural%0Ainnovation%20with%20the%20need%20for%20efficient%20evaluation%20of%20architectures%20to%0Aeffectively%20search%20such%20spaces.%20We%20investigate%20surrogate%20model%20training%20for%0Aimproving%20search%20in%20highly%20expressive%20NAS%20search%20spaces%20based%20on%20context-free%0Agrammars.%20We%20show%20that%20i%29%20surrogate%20models%20trained%20either%20using%20zero-cost-proxy%0Ametrics%20and%20neural%20graph%20features%20%28GRAF%29%20or%20by%20fine-tuning%20an%20off-the-shelf%20LM%0Ahave%20high%20predictive%20power%20for%20the%20performance%20of%20architectures%20both%20within%20and%0Aacross%20datasets%2C%20ii%29%20these%20surrogates%20can%20be%20used%20to%20filter%20out%20bad%0Aarchitectures%20when%20searching%20on%20novel%20datasets%2C%20thereby%20significantly%20speeding%0Aup%20search%20and%20achieving%20better%20final%20performances%2C%20and%20iii%29%20the%20surrogates%20can%0Abe%20further%20used%20directly%20as%20the%20search%20objective%20for%20huge%20speed-ups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12971v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferrable%2520Surrogates%2520in%2520Expressive%2520Neural%2520Architecture%2520Search%2520Spaces%26entry.906535625%3DShiwen%2520Qin%2520and%2520Gabriela%2520Kadlecov%25C3%25A1%2520and%2520Martin%2520Pil%25C3%25A1t%2520and%2520Shay%2520B.%2520Cohen%2520and%2520Roman%2520Neruda%2520and%2520Elliot%2520J.%2520Crowley%2520and%2520Jovita%2520Lukasik%2520and%2520Linus%2520Ericsson%26entry.1292438233%3D%2520%2520Neural%2520architecture%2520search%2520%2528NAS%2529%2520faces%2520a%2520challenge%2520in%2520balancing%2520the%250Aexploration%2520of%2520expressive%252C%2520broad%2520search%2520spaces%2520that%2520enable%2520architectural%250Ainnovation%2520with%2520the%2520need%2520for%2520efficient%2520evaluation%2520of%2520architectures%2520to%250Aeffectively%2520search%2520such%2520spaces.%2520We%2520investigate%2520surrogate%2520model%2520training%2520for%250Aimproving%2520search%2520in%2520highly%2520expressive%2520NAS%2520search%2520spaces%2520based%2520on%2520context-free%250Agrammars.%2520We%2520show%2520that%2520i%2529%2520surrogate%2520models%2520trained%2520either%2520using%2520zero-cost-proxy%250Ametrics%2520and%2520neural%2520graph%2520features%2520%2528GRAF%2529%2520or%2520by%2520fine-tuning%2520an%2520off-the-shelf%2520LM%250Ahave%2520high%2520predictive%2520power%2520for%2520the%2520performance%2520of%2520architectures%2520both%2520within%2520and%250Aacross%2520datasets%252C%2520ii%2529%2520these%2520surrogates%2520can%2520be%2520used%2520to%2520filter%2520out%2520bad%250Aarchitectures%2520when%2520searching%2520on%2520novel%2520datasets%252C%2520thereby%2520significantly%2520speeding%250Aup%2520search%2520and%2520achieving%2520better%2520final%2520performances%252C%2520and%2520iii%2529%2520the%2520surrogates%2520can%250Abe%2520further%2520used%2520directly%2520as%2520the%2520search%2520objective%2520for%2520huge%2520speed-ups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12971v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferrable%20Surrogates%20in%20Expressive%20Neural%20Architecture%20Search%20Spaces&entry.906535625=Shiwen%20Qin%20and%20Gabriela%20Kadlecov%C3%A1%20and%20Martin%20Pil%C3%A1t%20and%20Shay%20B.%20Cohen%20and%20Roman%20Neruda%20and%20Elliot%20J.%20Crowley%20and%20Jovita%20Lukasik%20and%20Linus%20Ericsson&entry.1292438233=%20%20Neural%20architecture%20search%20%28NAS%29%20faces%20a%20challenge%20in%20balancing%20the%0Aexploration%20of%20expressive%2C%20broad%20search%20spaces%20that%20enable%20architectural%0Ainnovation%20with%20the%20need%20for%20efficient%20evaluation%20of%20architectures%20to%0Aeffectively%20search%20such%20spaces.%20We%20investigate%20surrogate%20model%20training%20for%0Aimproving%20search%20in%20highly%20expressive%20NAS%20search%20spaces%20based%20on%20context-free%0Agrammars.%20We%20show%20that%20i%29%20surrogate%20models%20trained%20either%20using%20zero-cost-proxy%0Ametrics%20and%20neural%20graph%20features%20%28GRAF%29%20or%20by%20fine-tuning%20an%20off-the-shelf%20LM%0Ahave%20high%20predictive%20power%20for%20the%20performance%20of%20architectures%20both%20within%20and%0Aacross%20datasets%2C%20ii%29%20these%20surrogates%20can%20be%20used%20to%20filter%20out%20bad%0Aarchitectures%20when%20searching%20on%20novel%20datasets%2C%20thereby%20significantly%20speeding%0Aup%20search%20and%20achieving%20better%20final%20performances%2C%20and%20iii%29%20the%20surrogates%20can%0Abe%20further%20used%20directly%20as%20the%20search%20objective%20for%20huge%20speed-ups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12971v2&entry.124074799=Read"},
{"title": "Transformer Encoder and Multi-features Time2Vec for Financial Prediction", "author": "Nguyen Kim Hai Bui and Nguyen Duy Chien and P\u00e9ter Kov\u00e1cs and Gerg\u0151 Bogn\u00e1r", "abstract": "  Financial prediction is a complex and challenging task of time series\nanalysis and signal processing, expected to model both short-term fluctuations\nand long-term temporal dependencies. Transformers have remarkable success\nmostly in natural language processing using attention mechanism, which also\ninfluenced the time series community. The ability to capture both short and\nlong-range dependencies helps to understand the financial market and to\nrecognize price patterns, leading to successful applications of Transformers in\nstock prediction. Although, the previous research predominantly focuses on\nindividual features and singular predictions, that limits the model's ability\nto understand broader market trends. In reality, within sectors such as finance\nand technology, companies belonging to the same industry often exhibit\ncorrelated stock price movements.\n  In this paper, we develop a novel neural network architecture by integrating\nTime2Vec with the Encoder of the Transformer model. Based on the study of\ndifferent markets, we propose a novel correlation feature selection method.\nThrough a comprehensive fine-tuning of multiple hyperparameters, we conduct a\ncomparative analysis of our results against benchmark models. We conclude that\nour method outperforms other state-of-the-art encoding methods such as\npositional encoding, and we also conclude that selecting correlation features\nenhance the accuracy of predicting multiple stock prices.\n", "link": "http://arxiv.org/abs/2504.13801v1", "date": "2025-04-18", "relevancy": 1.3835, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4697}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4684}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20Encoder%20and%20Multi-features%20Time2Vec%20for%20Financial%20Prediction&body=Title%3A%20Transformer%20Encoder%20and%20Multi-features%20Time2Vec%20for%20Financial%20Prediction%0AAuthor%3A%20Nguyen%20Kim%20Hai%20Bui%20and%20Nguyen%20Duy%20Chien%20and%20P%C3%A9ter%20Kov%C3%A1cs%20and%20Gerg%C5%91%20Bogn%C3%A1r%0AAbstract%3A%20%20%20Financial%20prediction%20is%20a%20complex%20and%20challenging%20task%20of%20time%20series%0Aanalysis%20and%20signal%20processing%2C%20expected%20to%20model%20both%20short-term%20fluctuations%0Aand%20long-term%20temporal%20dependencies.%20Transformers%20have%20remarkable%20success%0Amostly%20in%20natural%20language%20processing%20using%20attention%20mechanism%2C%20which%20also%0Ainfluenced%20the%20time%20series%20community.%20The%20ability%20to%20capture%20both%20short%20and%0Along-range%20dependencies%20helps%20to%20understand%20the%20financial%20market%20and%20to%0Arecognize%20price%20patterns%2C%20leading%20to%20successful%20applications%20of%20Transformers%20in%0Astock%20prediction.%20Although%2C%20the%20previous%20research%20predominantly%20focuses%20on%0Aindividual%20features%20and%20singular%20predictions%2C%20that%20limits%20the%20model%27s%20ability%0Ato%20understand%20broader%20market%20trends.%20In%20reality%2C%20within%20sectors%20such%20as%20finance%0Aand%20technology%2C%20companies%20belonging%20to%20the%20same%20industry%20often%20exhibit%0Acorrelated%20stock%20price%20movements.%0A%20%20In%20this%20paper%2C%20we%20develop%20a%20novel%20neural%20network%20architecture%20by%20integrating%0ATime2Vec%20with%20the%20Encoder%20of%20the%20Transformer%20model.%20Based%20on%20the%20study%20of%0Adifferent%20markets%2C%20we%20propose%20a%20novel%20correlation%20feature%20selection%20method.%0AThrough%20a%20comprehensive%20fine-tuning%20of%20multiple%20hyperparameters%2C%20we%20conduct%20a%0Acomparative%20analysis%20of%20our%20results%20against%20benchmark%20models.%20We%20conclude%20that%0Aour%20method%20outperforms%20other%20state-of-the-art%20encoding%20methods%20such%20as%0Apositional%20encoding%2C%20and%20we%20also%20conclude%20that%20selecting%20correlation%20features%0Aenhance%20the%20accuracy%20of%20predicting%20multiple%20stock%20prices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520Encoder%2520and%2520Multi-features%2520Time2Vec%2520for%2520Financial%2520Prediction%26entry.906535625%3DNguyen%2520Kim%2520Hai%2520Bui%2520and%2520Nguyen%2520Duy%2520Chien%2520and%2520P%25C3%25A9ter%2520Kov%25C3%25A1cs%2520and%2520Gerg%25C5%2591%2520Bogn%25C3%25A1r%26entry.1292438233%3D%2520%2520Financial%2520prediction%2520is%2520a%2520complex%2520and%2520challenging%2520task%2520of%2520time%2520series%250Aanalysis%2520and%2520signal%2520processing%252C%2520expected%2520to%2520model%2520both%2520short-term%2520fluctuations%250Aand%2520long-term%2520temporal%2520dependencies.%2520Transformers%2520have%2520remarkable%2520success%250Amostly%2520in%2520natural%2520language%2520processing%2520using%2520attention%2520mechanism%252C%2520which%2520also%250Ainfluenced%2520the%2520time%2520series%2520community.%2520The%2520ability%2520to%2520capture%2520both%2520short%2520and%250Along-range%2520dependencies%2520helps%2520to%2520understand%2520the%2520financial%2520market%2520and%2520to%250Arecognize%2520price%2520patterns%252C%2520leading%2520to%2520successful%2520applications%2520of%2520Transformers%2520in%250Astock%2520prediction.%2520Although%252C%2520the%2520previous%2520research%2520predominantly%2520focuses%2520on%250Aindividual%2520features%2520and%2520singular%2520predictions%252C%2520that%2520limits%2520the%2520model%2527s%2520ability%250Ato%2520understand%2520broader%2520market%2520trends.%2520In%2520reality%252C%2520within%2520sectors%2520such%2520as%2520finance%250Aand%2520technology%252C%2520companies%2520belonging%2520to%2520the%2520same%2520industry%2520often%2520exhibit%250Acorrelated%2520stock%2520price%2520movements.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520novel%2520neural%2520network%2520architecture%2520by%2520integrating%250ATime2Vec%2520with%2520the%2520Encoder%2520of%2520the%2520Transformer%2520model.%2520Based%2520on%2520the%2520study%2520of%250Adifferent%2520markets%252C%2520we%2520propose%2520a%2520novel%2520correlation%2520feature%2520selection%2520method.%250AThrough%2520a%2520comprehensive%2520fine-tuning%2520of%2520multiple%2520hyperparameters%252C%2520we%2520conduct%2520a%250Acomparative%2520analysis%2520of%2520our%2520results%2520against%2520benchmark%2520models.%2520We%2520conclude%2520that%250Aour%2520method%2520outperforms%2520other%2520state-of-the-art%2520encoding%2520methods%2520such%2520as%250Apositional%2520encoding%252C%2520and%2520we%2520also%2520conclude%2520that%2520selecting%2520correlation%2520features%250Aenhance%2520the%2520accuracy%2520of%2520predicting%2520multiple%2520stock%2520prices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20Encoder%20and%20Multi-features%20Time2Vec%20for%20Financial%20Prediction&entry.906535625=Nguyen%20Kim%20Hai%20Bui%20and%20Nguyen%20Duy%20Chien%20and%20P%C3%A9ter%20Kov%C3%A1cs%20and%20Gerg%C5%91%20Bogn%C3%A1r&entry.1292438233=%20%20Financial%20prediction%20is%20a%20complex%20and%20challenging%20task%20of%20time%20series%0Aanalysis%20and%20signal%20processing%2C%20expected%20to%20model%20both%20short-term%20fluctuations%0Aand%20long-term%20temporal%20dependencies.%20Transformers%20have%20remarkable%20success%0Amostly%20in%20natural%20language%20processing%20using%20attention%20mechanism%2C%20which%20also%0Ainfluenced%20the%20time%20series%20community.%20The%20ability%20to%20capture%20both%20short%20and%0Along-range%20dependencies%20helps%20to%20understand%20the%20financial%20market%20and%20to%0Arecognize%20price%20patterns%2C%20leading%20to%20successful%20applications%20of%20Transformers%20in%0Astock%20prediction.%20Although%2C%20the%20previous%20research%20predominantly%20focuses%20on%0Aindividual%20features%20and%20singular%20predictions%2C%20that%20limits%20the%20model%27s%20ability%0Ato%20understand%20broader%20market%20trends.%20In%20reality%2C%20within%20sectors%20such%20as%20finance%0Aand%20technology%2C%20companies%20belonging%20to%20the%20same%20industry%20often%20exhibit%0Acorrelated%20stock%20price%20movements.%0A%20%20In%20this%20paper%2C%20we%20develop%20a%20novel%20neural%20network%20architecture%20by%20integrating%0ATime2Vec%20with%20the%20Encoder%20of%20the%20Transformer%20model.%20Based%20on%20the%20study%20of%0Adifferent%20markets%2C%20we%20propose%20a%20novel%20correlation%20feature%20selection%20method.%0AThrough%20a%20comprehensive%20fine-tuning%20of%20multiple%20hyperparameters%2C%20we%20conduct%20a%0Acomparative%20analysis%20of%20our%20results%20against%20benchmark%20models.%20We%20conclude%20that%0Aour%20method%20outperforms%20other%20state-of-the-art%20encoding%20methods%20such%20as%0Apositional%20encoding%2C%20and%20we%20also%20conclude%20that%20selecting%20correlation%20features%0Aenhance%20the%20accuracy%20of%20predicting%20multiple%20stock%20prices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13801v1&entry.124074799=Read"},
{"title": "Performance Analysis of a Mass-Spring-Damper Deformable Linear Object\n  Model in Robotic Simulation Frameworks", "author": "Andrea Govoni and Nadia Zubair and Simone Soprani and Gianluca Palli", "abstract": "  The modelling of Deformable Linear Objects (DLOs) such as cables, wires, and\nstrings presents significant challenges due to their flexible and deformable\nnature. In robotics, accurately simulating the dynamic behavior of DLOs is\nessential for automating tasks like wire handling and assembly. The presented\nstudy is a preliminary analysis aimed at force data collection through domain\nrandomization (DR) for training a robot in simulation, using a\nMass-Spring-Damper (MSD) system as the reference model. The study aims to\nassess the impact of model parameter variations on DLO dynamics, using Isaac\nSim and Gazebo to validate the applicability of DR technique in these\nscenarios.\n", "link": "http://arxiv.org/abs/2504.13659v1", "date": "2025-04-18", "relevancy": 1.4536, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4869}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4847}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20Analysis%20of%20a%20Mass-Spring-Damper%20Deformable%20Linear%20Object%0A%20%20Model%20in%20Robotic%20Simulation%20Frameworks&body=Title%3A%20Performance%20Analysis%20of%20a%20Mass-Spring-Damper%20Deformable%20Linear%20Object%0A%20%20Model%20in%20Robotic%20Simulation%20Frameworks%0AAuthor%3A%20Andrea%20Govoni%20and%20Nadia%20Zubair%20and%20Simone%20Soprani%20and%20Gianluca%20Palli%0AAbstract%3A%20%20%20The%20modelling%20of%20Deformable%20Linear%20Objects%20%28DLOs%29%20such%20as%20cables%2C%20wires%2C%20and%0Astrings%20presents%20significant%20challenges%20due%20to%20their%20flexible%20and%20deformable%0Anature.%20In%20robotics%2C%20accurately%20simulating%20the%20dynamic%20behavior%20of%20DLOs%20is%0Aessential%20for%20automating%20tasks%20like%20wire%20handling%20and%20assembly.%20The%20presented%0Astudy%20is%20a%20preliminary%20analysis%20aimed%20at%20force%20data%20collection%20through%20domain%0Arandomization%20%28DR%29%20for%20training%20a%20robot%20in%20simulation%2C%20using%20a%0AMass-Spring-Damper%20%28MSD%29%20system%20as%20the%20reference%20model.%20The%20study%20aims%20to%0Aassess%20the%20impact%20of%20model%20parameter%20variations%20on%20DLO%20dynamics%2C%20using%20Isaac%0ASim%20and%20Gazebo%20to%20validate%20the%20applicability%20of%20DR%20technique%20in%20these%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520Analysis%2520of%2520a%2520Mass-Spring-Damper%2520Deformable%2520Linear%2520Object%250A%2520%2520Model%2520in%2520Robotic%2520Simulation%2520Frameworks%26entry.906535625%3DAndrea%2520Govoni%2520and%2520Nadia%2520Zubair%2520and%2520Simone%2520Soprani%2520and%2520Gianluca%2520Palli%26entry.1292438233%3D%2520%2520The%2520modelling%2520of%2520Deformable%2520Linear%2520Objects%2520%2528DLOs%2529%2520such%2520as%2520cables%252C%2520wires%252C%2520and%250Astrings%2520presents%2520significant%2520challenges%2520due%2520to%2520their%2520flexible%2520and%2520deformable%250Anature.%2520In%2520robotics%252C%2520accurately%2520simulating%2520the%2520dynamic%2520behavior%2520of%2520DLOs%2520is%250Aessential%2520for%2520automating%2520tasks%2520like%2520wire%2520handling%2520and%2520assembly.%2520The%2520presented%250Astudy%2520is%2520a%2520preliminary%2520analysis%2520aimed%2520at%2520force%2520data%2520collection%2520through%2520domain%250Arandomization%2520%2528DR%2529%2520for%2520training%2520a%2520robot%2520in%2520simulation%252C%2520using%2520a%250AMass-Spring-Damper%2520%2528MSD%2529%2520system%2520as%2520the%2520reference%2520model.%2520The%2520study%2520aims%2520to%250Aassess%2520the%2520impact%2520of%2520model%2520parameter%2520variations%2520on%2520DLO%2520dynamics%252C%2520using%2520Isaac%250ASim%2520and%2520Gazebo%2520to%2520validate%2520the%2520applicability%2520of%2520DR%2520technique%2520in%2520these%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Analysis%20of%20a%20Mass-Spring-Damper%20Deformable%20Linear%20Object%0A%20%20Model%20in%20Robotic%20Simulation%20Frameworks&entry.906535625=Andrea%20Govoni%20and%20Nadia%20Zubair%20and%20Simone%20Soprani%20and%20Gianluca%20Palli&entry.1292438233=%20%20The%20modelling%20of%20Deformable%20Linear%20Objects%20%28DLOs%29%20such%20as%20cables%2C%20wires%2C%20and%0Astrings%20presents%20significant%20challenges%20due%20to%20their%20flexible%20and%20deformable%0Anature.%20In%20robotics%2C%20accurately%20simulating%20the%20dynamic%20behavior%20of%20DLOs%20is%0Aessential%20for%20automating%20tasks%20like%20wire%20handling%20and%20assembly.%20The%20presented%0Astudy%20is%20a%20preliminary%20analysis%20aimed%20at%20force%20data%20collection%20through%20domain%0Arandomization%20%28DR%29%20for%20training%20a%20robot%20in%20simulation%2C%20using%20a%0AMass-Spring-Damper%20%28MSD%29%20system%20as%20the%20reference%20model.%20The%20study%20aims%20to%0Aassess%20the%20impact%20of%20model%20parameter%20variations%20on%20DLO%20dynamics%2C%20using%20Isaac%0ASim%20and%20Gazebo%20to%20validate%20the%20applicability%20of%20DR%20technique%20in%20these%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13659v1&entry.124074799=Read"},
{"title": "Unified Manipulability and Compliance Analysis of Modular Soft-Rigid\n  Hybrid Fingers", "author": "Jianshu Zhou and Boyuan Liang and Junda Huang and Masayoshi Tomizuka", "abstract": "  This paper presents a unified framework to analyze the manipulability and\ncompliance of modular soft-rigid hybrid robotic fingers. The approach applies\nto both hydraulic and pneumatic actuation systems. A Jacobian-based formulation\nmaps actuator inputs to joint and task-space responses. Hydraulic actuators are\nmodeled under incompressible assumptions, while pneumatic actuators are\ndescribed using nonlinear pressure-volume relations. The framework enables\nconsistent evaluation of manipulability ellipsoids and compliance matrices\nacross actuation modes. We validate the analysis using two representative\nhands: DexCo (hydraulic) and Edgy-2 (pneumatic). Results highlight\nactuation-dependent trade-offs in dexterity and passive stiffness. These\nfindings provide insights for structure-aware design and actuator selection in\nsoft-rigid robotic fingers.\n", "link": "http://arxiv.org/abs/2504.13800v1", "date": "2025-04-18", "relevancy": 1.574, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5414}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5216}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Manipulability%20and%20Compliance%20Analysis%20of%20Modular%20Soft-Rigid%0A%20%20Hybrid%20Fingers&body=Title%3A%20Unified%20Manipulability%20and%20Compliance%20Analysis%20of%20Modular%20Soft-Rigid%0A%20%20Hybrid%20Fingers%0AAuthor%3A%20Jianshu%20Zhou%20and%20Boyuan%20Liang%20and%20Junda%20Huang%20and%20Masayoshi%20Tomizuka%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20unified%20framework%20to%20analyze%20the%20manipulability%20and%0Acompliance%20of%20modular%20soft-rigid%20hybrid%20robotic%20fingers.%20The%20approach%20applies%0Ato%20both%20hydraulic%20and%20pneumatic%20actuation%20systems.%20A%20Jacobian-based%20formulation%0Amaps%20actuator%20inputs%20to%20joint%20and%20task-space%20responses.%20Hydraulic%20actuators%20are%0Amodeled%20under%20incompressible%20assumptions%2C%20while%20pneumatic%20actuators%20are%0Adescribed%20using%20nonlinear%20pressure-volume%20relations.%20The%20framework%20enables%0Aconsistent%20evaluation%20of%20manipulability%20ellipsoids%20and%20compliance%20matrices%0Aacross%20actuation%20modes.%20We%20validate%20the%20analysis%20using%20two%20representative%0Ahands%3A%20DexCo%20%28hydraulic%29%20and%20Edgy-2%20%28pneumatic%29.%20Results%20highlight%0Aactuation-dependent%20trade-offs%20in%20dexterity%20and%20passive%20stiffness.%20These%0Afindings%20provide%20insights%20for%20structure-aware%20design%20and%20actuator%20selection%20in%0Asoft-rigid%20robotic%20fingers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Manipulability%2520and%2520Compliance%2520Analysis%2520of%2520Modular%2520Soft-Rigid%250A%2520%2520Hybrid%2520Fingers%26entry.906535625%3DJianshu%2520Zhou%2520and%2520Boyuan%2520Liang%2520and%2520Junda%2520Huang%2520and%2520Masayoshi%2520Tomizuka%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520unified%2520framework%2520to%2520analyze%2520the%2520manipulability%2520and%250Acompliance%2520of%2520modular%2520soft-rigid%2520hybrid%2520robotic%2520fingers.%2520The%2520approach%2520applies%250Ato%2520both%2520hydraulic%2520and%2520pneumatic%2520actuation%2520systems.%2520A%2520Jacobian-based%2520formulation%250Amaps%2520actuator%2520inputs%2520to%2520joint%2520and%2520task-space%2520responses.%2520Hydraulic%2520actuators%2520are%250Amodeled%2520under%2520incompressible%2520assumptions%252C%2520while%2520pneumatic%2520actuators%2520are%250Adescribed%2520using%2520nonlinear%2520pressure-volume%2520relations.%2520The%2520framework%2520enables%250Aconsistent%2520evaluation%2520of%2520manipulability%2520ellipsoids%2520and%2520compliance%2520matrices%250Aacross%2520actuation%2520modes.%2520We%2520validate%2520the%2520analysis%2520using%2520two%2520representative%250Ahands%253A%2520DexCo%2520%2528hydraulic%2529%2520and%2520Edgy-2%2520%2528pneumatic%2529.%2520Results%2520highlight%250Aactuation-dependent%2520trade-offs%2520in%2520dexterity%2520and%2520passive%2520stiffness.%2520These%250Afindings%2520provide%2520insights%2520for%2520structure-aware%2520design%2520and%2520actuator%2520selection%2520in%250Asoft-rigid%2520robotic%2520fingers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Manipulability%20and%20Compliance%20Analysis%20of%20Modular%20Soft-Rigid%0A%20%20Hybrid%20Fingers&entry.906535625=Jianshu%20Zhou%20and%20Boyuan%20Liang%20and%20Junda%20Huang%20and%20Masayoshi%20Tomizuka&entry.1292438233=%20%20This%20paper%20presents%20a%20unified%20framework%20to%20analyze%20the%20manipulability%20and%0Acompliance%20of%20modular%20soft-rigid%20hybrid%20robotic%20fingers.%20The%20approach%20applies%0Ato%20both%20hydraulic%20and%20pneumatic%20actuation%20systems.%20A%20Jacobian-based%20formulation%0Amaps%20actuator%20inputs%20to%20joint%20and%20task-space%20responses.%20Hydraulic%20actuators%20are%0Amodeled%20under%20incompressible%20assumptions%2C%20while%20pneumatic%20actuators%20are%0Adescribed%20using%20nonlinear%20pressure-volume%20relations.%20The%20framework%20enables%0Aconsistent%20evaluation%20of%20manipulability%20ellipsoids%20and%20compliance%20matrices%0Aacross%20actuation%20modes.%20We%20validate%20the%20analysis%20using%20two%20representative%0Ahands%3A%20DexCo%20%28hydraulic%29%20and%20Edgy-2%20%28pneumatic%29.%20Results%20highlight%0Aactuation-dependent%20trade-offs%20in%20dexterity%20and%20passive%20stiffness.%20These%0Afindings%20provide%20insights%20for%20structure-aware%20design%20and%20actuator%20selection%20in%0Asoft-rigid%20robotic%20fingers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13800v1&entry.124074799=Read"},
{"title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents", "author": "Maksym Andriushchenko and Alexandra Souly and Mateusz Dziemian and Derek Duenas and Maxwell Lin and Justin Wang and Dan Hendrycks and Andy Zou and Zico Kolter and Matt Fredrikson and Eric Winsor and Jerome Wynne and Yarin Gal and Xander Davies", "abstract": "  The robustness of LLMs to jailbreak attacks, where users design prompts to\ncircumvent safety measures and misuse model capabilities, has been studied\nprimarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which\nuse external tools and can execute multi-stage tasks -- may pose a greater risk\nif misused, but their robustness remains underexplored. To facilitate research\non LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark\nincludes a diverse set of 110 explicitly malicious agent tasks (440 with\naugmentations), covering 11 harm categories including fraud, cybercrime, and\nharassment. In addition to measuring whether models refuse harmful agentic\nrequests, scoring well on AgentHarm requires jailbroken agents to maintain\ntheir capabilities following an attack to complete a multi-step task. We\nevaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly\ncompliant with malicious agent requests without jailbreaking, (2) simple\nuniversal jailbreak templates can be adapted to effectively jailbreak agents,\nand (3) these jailbreaks enable coherent and malicious multi-step agent\nbehavior and retain model capabilities. To enable simple and reliable\nevaluation of attacks and defenses for LLM-based agents, we publicly release\nAgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.\n", "link": "http://arxiv.org/abs/2410.09024v3", "date": "2025-04-18", "relevancy": 1.6504, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4198}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4076}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentHarm%3A%20A%20Benchmark%20for%20Measuring%20Harmfulness%20of%20LLM%20Agents&body=Title%3A%20AgentHarm%3A%20A%20Benchmark%20for%20Measuring%20Harmfulness%20of%20LLM%20Agents%0AAuthor%3A%20Maksym%20Andriushchenko%20and%20Alexandra%20Souly%20and%20Mateusz%20Dziemian%20and%20Derek%20Duenas%20and%20Maxwell%20Lin%20and%20Justin%20Wang%20and%20Dan%20Hendrycks%20and%20Andy%20Zou%20and%20Zico%20Kolter%20and%20Matt%20Fredrikson%20and%20Eric%20Winsor%20and%20Jerome%20Wynne%20and%20Yarin%20Gal%20and%20Xander%20Davies%0AAbstract%3A%20%20%20The%20robustness%20of%20LLMs%20to%20jailbreak%20attacks%2C%20where%20users%20design%20prompts%20to%0Acircumvent%20safety%20measures%20and%20misuse%20model%20capabilities%2C%20has%20been%20studied%0Aprimarily%20for%20LLMs%20acting%20as%20simple%20chatbots.%20Meanwhile%2C%20LLM%20agents%20--%20which%0Ause%20external%20tools%20and%20can%20execute%20multi-stage%20tasks%20--%20may%20pose%20a%20greater%20risk%0Aif%20misused%2C%20but%20their%20robustness%20remains%20underexplored.%20To%20facilitate%20research%0Aon%20LLM%20agent%20misuse%2C%20we%20propose%20a%20new%20benchmark%20called%20AgentHarm.%20The%20benchmark%0Aincludes%20a%20diverse%20set%20of%20110%20explicitly%20malicious%20agent%20tasks%20%28440%20with%0Aaugmentations%29%2C%20covering%2011%20harm%20categories%20including%20fraud%2C%20cybercrime%2C%20and%0Aharassment.%20In%20addition%20to%20measuring%20whether%20models%20refuse%20harmful%20agentic%0Arequests%2C%20scoring%20well%20on%20AgentHarm%20requires%20jailbroken%20agents%20to%20maintain%0Atheir%20capabilities%20following%20an%20attack%20to%20complete%20a%20multi-step%20task.%20We%0Aevaluate%20a%20range%20of%20leading%20LLMs%2C%20and%20find%20%281%29%20leading%20LLMs%20are%20surprisingly%0Acompliant%20with%20malicious%20agent%20requests%20without%20jailbreaking%2C%20%282%29%20simple%0Auniversal%20jailbreak%20templates%20can%20be%20adapted%20to%20effectively%20jailbreak%20agents%2C%0Aand%20%283%29%20these%20jailbreaks%20enable%20coherent%20and%20malicious%20multi-step%20agent%0Abehavior%20and%20retain%20model%20capabilities.%20To%20enable%20simple%20and%20reliable%0Aevaluation%20of%20attacks%20and%20defenses%20for%20LLM-based%20agents%2C%20we%20publicly%20release%0AAgentHarm%20at%20https%3A//huggingface.co/datasets/ai-safety-institute/AgentHarm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09024v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentHarm%253A%2520A%2520Benchmark%2520for%2520Measuring%2520Harmfulness%2520of%2520LLM%2520Agents%26entry.906535625%3DMaksym%2520Andriushchenko%2520and%2520Alexandra%2520Souly%2520and%2520Mateusz%2520Dziemian%2520and%2520Derek%2520Duenas%2520and%2520Maxwell%2520Lin%2520and%2520Justin%2520Wang%2520and%2520Dan%2520Hendrycks%2520and%2520Andy%2520Zou%2520and%2520Zico%2520Kolter%2520and%2520Matt%2520Fredrikson%2520and%2520Eric%2520Winsor%2520and%2520Jerome%2520Wynne%2520and%2520Yarin%2520Gal%2520and%2520Xander%2520Davies%26entry.1292438233%3D%2520%2520The%2520robustness%2520of%2520LLMs%2520to%2520jailbreak%2520attacks%252C%2520where%2520users%2520design%2520prompts%2520to%250Acircumvent%2520safety%2520measures%2520and%2520misuse%2520model%2520capabilities%252C%2520has%2520been%2520studied%250Aprimarily%2520for%2520LLMs%2520acting%2520as%2520simple%2520chatbots.%2520Meanwhile%252C%2520LLM%2520agents%2520--%2520which%250Ause%2520external%2520tools%2520and%2520can%2520execute%2520multi-stage%2520tasks%2520--%2520may%2520pose%2520a%2520greater%2520risk%250Aif%2520misused%252C%2520but%2520their%2520robustness%2520remains%2520underexplored.%2520To%2520facilitate%2520research%250Aon%2520LLM%2520agent%2520misuse%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520called%2520AgentHarm.%2520The%2520benchmark%250Aincludes%2520a%2520diverse%2520set%2520of%2520110%2520explicitly%2520malicious%2520agent%2520tasks%2520%2528440%2520with%250Aaugmentations%2529%252C%2520covering%252011%2520harm%2520categories%2520including%2520fraud%252C%2520cybercrime%252C%2520and%250Aharassment.%2520In%2520addition%2520to%2520measuring%2520whether%2520models%2520refuse%2520harmful%2520agentic%250Arequests%252C%2520scoring%2520well%2520on%2520AgentHarm%2520requires%2520jailbroken%2520agents%2520to%2520maintain%250Atheir%2520capabilities%2520following%2520an%2520attack%2520to%2520complete%2520a%2520multi-step%2520task.%2520We%250Aevaluate%2520a%2520range%2520of%2520leading%2520LLMs%252C%2520and%2520find%2520%25281%2529%2520leading%2520LLMs%2520are%2520surprisingly%250Acompliant%2520with%2520malicious%2520agent%2520requests%2520without%2520jailbreaking%252C%2520%25282%2529%2520simple%250Auniversal%2520jailbreak%2520templates%2520can%2520be%2520adapted%2520to%2520effectively%2520jailbreak%2520agents%252C%250Aand%2520%25283%2529%2520these%2520jailbreaks%2520enable%2520coherent%2520and%2520malicious%2520multi-step%2520agent%250Abehavior%2520and%2520retain%2520model%2520capabilities.%2520To%2520enable%2520simple%2520and%2520reliable%250Aevaluation%2520of%2520attacks%2520and%2520defenses%2520for%2520LLM-based%2520agents%252C%2520we%2520publicly%2520release%250AAgentHarm%2520at%2520https%253A//huggingface.co/datasets/ai-safety-institute/AgentHarm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09024v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentHarm%3A%20A%20Benchmark%20for%20Measuring%20Harmfulness%20of%20LLM%20Agents&entry.906535625=Maksym%20Andriushchenko%20and%20Alexandra%20Souly%20and%20Mateusz%20Dziemian%20and%20Derek%20Duenas%20and%20Maxwell%20Lin%20and%20Justin%20Wang%20and%20Dan%20Hendrycks%20and%20Andy%20Zou%20and%20Zico%20Kolter%20and%20Matt%20Fredrikson%20and%20Eric%20Winsor%20and%20Jerome%20Wynne%20and%20Yarin%20Gal%20and%20Xander%20Davies&entry.1292438233=%20%20The%20robustness%20of%20LLMs%20to%20jailbreak%20attacks%2C%20where%20users%20design%20prompts%20to%0Acircumvent%20safety%20measures%20and%20misuse%20model%20capabilities%2C%20has%20been%20studied%0Aprimarily%20for%20LLMs%20acting%20as%20simple%20chatbots.%20Meanwhile%2C%20LLM%20agents%20--%20which%0Ause%20external%20tools%20and%20can%20execute%20multi-stage%20tasks%20--%20may%20pose%20a%20greater%20risk%0Aif%20misused%2C%20but%20their%20robustness%20remains%20underexplored.%20To%20facilitate%20research%0Aon%20LLM%20agent%20misuse%2C%20we%20propose%20a%20new%20benchmark%20called%20AgentHarm.%20The%20benchmark%0Aincludes%20a%20diverse%20set%20of%20110%20explicitly%20malicious%20agent%20tasks%20%28440%20with%0Aaugmentations%29%2C%20covering%2011%20harm%20categories%20including%20fraud%2C%20cybercrime%2C%20and%0Aharassment.%20In%20addition%20to%20measuring%20whether%20models%20refuse%20harmful%20agentic%0Arequests%2C%20scoring%20well%20on%20AgentHarm%20requires%20jailbroken%20agents%20to%20maintain%0Atheir%20capabilities%20following%20an%20attack%20to%20complete%20a%20multi-step%20task.%20We%0Aevaluate%20a%20range%20of%20leading%20LLMs%2C%20and%20find%20%281%29%20leading%20LLMs%20are%20surprisingly%0Acompliant%20with%20malicious%20agent%20requests%20without%20jailbreaking%2C%20%282%29%20simple%0Auniversal%20jailbreak%20templates%20can%20be%20adapted%20to%20effectively%20jailbreak%20agents%2C%0Aand%20%283%29%20these%20jailbreaks%20enable%20coherent%20and%20malicious%20multi-step%20agent%0Abehavior%20and%20retain%20model%20capabilities.%20To%20enable%20simple%20and%20reliable%0Aevaluation%20of%20attacks%20and%20defenses%20for%20LLM-based%20agents%2C%20we%20publicly%20release%0AAgentHarm%20at%20https%3A//huggingface.co/datasets/ai-safety-institute/AgentHarm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09024v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


