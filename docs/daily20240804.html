<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240801.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and\n  Illumination Disentanglement", "author": "Mark Boss and Zixuan Huang and Aaryaman Vasishta and Varun Jampani", "abstract": "  We present SF3D, a novel method for rapid and high-quality textured object\nmesh reconstruction from a single image in just 0.5 seconds. Unlike most\nexisting approaches, SF3D is explicitly trained for mesh generation,\nincorporating a fast UV unwrapping technique that enables swift texture\ngeneration rather than relying on vertex colors. The method also learns to\npredict material parameters and normal maps to enhance the visual quality of\nthe reconstructed 3D meshes. Furthermore, SF3D integrates a delighting step to\neffectively remove low-frequency illumination effects, ensuring that the\nreconstructed meshes can be easily used in novel illumination conditions.\nExperiments demonstrate the superior performance of SF3D over the existing\ntechniques. Project page: https://stable-fast-3d.github.io\n", "link": "http://arxiv.org/abs/2408.00653v1", "date": "2024-08-01", "relevancy": 3.1284, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.634}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SF3D%3A%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-unwrapping%20and%0A%20%20Illumination%20Disentanglement&body=Title%3A%20SF3D%3A%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-unwrapping%20and%0A%20%20Illumination%20Disentanglement%0AAuthor%3A%20Mark%20Boss%20and%20Zixuan%20Huang%20and%20Aaryaman%20Vasishta%20and%20Varun%20Jampani%0AAbstract%3A%20%20%20We%20present%20SF3D%2C%20a%20novel%20method%20for%20rapid%20and%20high-quality%20textured%20object%0Amesh%20reconstruction%20from%20a%20single%20image%20in%20just%200.5%20seconds.%20Unlike%20most%0Aexisting%20approaches%2C%20SF3D%20is%20explicitly%20trained%20for%20mesh%20generation%2C%0Aincorporating%20a%20fast%20UV%20unwrapping%20technique%20that%20enables%20swift%20texture%0Ageneration%20rather%20than%20relying%20on%20vertex%20colors.%20The%20method%20also%20learns%20to%0Apredict%20material%20parameters%20and%20normal%20maps%20to%20enhance%20the%20visual%20quality%20of%0Athe%20reconstructed%203D%20meshes.%20Furthermore%2C%20SF3D%20integrates%20a%20delighting%20step%20to%0Aeffectively%20remove%20low-frequency%20illumination%20effects%2C%20ensuring%20that%20the%0Areconstructed%20meshes%20can%20be%20easily%20used%20in%20novel%20illumination%20conditions.%0AExperiments%20demonstrate%20the%20superior%20performance%20of%20SF3D%20over%20the%20existing%0Atechniques.%20Project%20page%3A%20https%3A//stable-fast-3d.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSF3D%253A%2520Stable%2520Fast%25203D%2520Mesh%2520Reconstruction%2520with%2520UV-unwrapping%2520and%250A%2520%2520Illumination%2520Disentanglement%26entry.906535625%3DMark%2520Boss%2520and%2520Zixuan%2520Huang%2520and%2520Aaryaman%2520Vasishta%2520and%2520Varun%2520Jampani%26entry.1292438233%3D%2520%2520We%2520present%2520SF3D%252C%2520a%2520novel%2520method%2520for%2520rapid%2520and%2520high-quality%2520textured%2520object%250Amesh%2520reconstruction%2520from%2520a%2520single%2520image%2520in%2520just%25200.5%2520seconds.%2520Unlike%2520most%250Aexisting%2520approaches%252C%2520SF3D%2520is%2520explicitly%2520trained%2520for%2520mesh%2520generation%252C%250Aincorporating%2520a%2520fast%2520UV%2520unwrapping%2520technique%2520that%2520enables%2520swift%2520texture%250Ageneration%2520rather%2520than%2520relying%2520on%2520vertex%2520colors.%2520The%2520method%2520also%2520learns%2520to%250Apredict%2520material%2520parameters%2520and%2520normal%2520maps%2520to%2520enhance%2520the%2520visual%2520quality%2520of%250Athe%2520reconstructed%25203D%2520meshes.%2520Furthermore%252C%2520SF3D%2520integrates%2520a%2520delighting%2520step%2520to%250Aeffectively%2520remove%2520low-frequency%2520illumination%2520effects%252C%2520ensuring%2520that%2520the%250Areconstructed%2520meshes%2520can%2520be%2520easily%2520used%2520in%2520novel%2520illumination%2520conditions.%250AExperiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520SF3D%2520over%2520the%2520existing%250Atechniques.%2520Project%2520page%253A%2520https%253A//stable-fast-3d.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SF3D%3A%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-unwrapping%20and%0A%20%20Illumination%20Disentanglement&entry.906535625=Mark%20Boss%20and%20Zixuan%20Huang%20and%20Aaryaman%20Vasishta%20and%20Varun%20Jampani&entry.1292438233=%20%20We%20present%20SF3D%2C%20a%20novel%20method%20for%20rapid%20and%20high-quality%20textured%20object%0Amesh%20reconstruction%20from%20a%20single%20image%20in%20just%200.5%20seconds.%20Unlike%20most%0Aexisting%20approaches%2C%20SF3D%20is%20explicitly%20trained%20for%20mesh%20generation%2C%0Aincorporating%20a%20fast%20UV%20unwrapping%20technique%20that%20enables%20swift%20texture%0Ageneration%20rather%20than%20relying%20on%20vertex%20colors.%20The%20method%20also%20learns%20to%0Apredict%20material%20parameters%20and%20normal%20maps%20to%20enhance%20the%20visual%20quality%20of%0Athe%20reconstructed%203D%20meshes.%20Furthermore%2C%20SF3D%20integrates%20a%20delighting%20step%20to%0Aeffectively%20remove%20low-frequency%20illumination%20effects%2C%20ensuring%20that%20the%0Areconstructed%20meshes%20can%20be%20easily%20used%20in%20novel%20illumination%20conditions.%0AExperiments%20demonstrate%20the%20superior%20performance%20of%20SF3D%20over%20the%20existing%0Atechniques.%20Project%20page%3A%20https%3A//stable-fast-3d.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00653v1&entry.124074799=Read"},
{"title": "Segment anything model 2: an application to 2D and 3D medical images", "author": "Haoyu Dong and Hanxue Gu and Yaqian Chen and Jichen Yang and Maciej A. Mazurowski", "abstract": "  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment a variety of objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we provide an extensive evaluation of SAM\n2's ability to segment both 2D and 3D medical images. We collect 18 medical\nimaging datasets, including common 3D modalities such as computed tomography\n(CT), magnetic resonance imaging (MRI), and positron emission tomography (PET)\nas well as 2D modalities such as X-ray and ultrasound. We consider two\nevaluation pipelines of SAM 2: (1) multi-frame 3D segmentation, where prompts\nare provided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. We learn that SAM 2 exhibits similar performance as SAM\nunder single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n", "link": "http://arxiv.org/abs/2408.00756v1", "date": "2024-08-01", "relevancy": 2.9818, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5994}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5948}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images&body=Title%3A%20Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images%0AAuthor%3A%20Haoyu%20Dong%20and%20Hanxue%20Gu%20and%20Yaqian%20Chen%20and%20Jichen%20Yang%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20gained%20significant%20attention%20because%20of%20its%0Aability%20to%20segment%20a%20variety%20of%20objects%20in%20images%20given%20a%20prompt.%20The%20recently%0Adeveloped%20SAM%202%20has%20extended%20this%20ability%20to%20video%20inputs.%20This%20opens%20an%0Aopportunity%20to%20apply%20SAM%20to%203D%20images%2C%20one%20of%20the%20fundamental%20tasks%20in%20the%0Amedical%20imaging%20field.%20In%20this%20paper%2C%20we%20provide%20an%20extensive%20evaluation%20of%20SAM%0A2%27s%20ability%20to%20segment%20both%202D%20and%203D%20medical%20images.%20We%20collect%2018%20medical%0Aimaging%20datasets%2C%20including%20common%203D%20modalities%20such%20as%20computed%20tomography%0A%28CT%29%2C%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20positron%20emission%20tomography%20%28PET%29%0Aas%20well%20as%202D%20modalities%20such%20as%20X-ray%20and%20ultrasound.%20We%20consider%20two%0Aevaluation%20pipelines%20of%20SAM%202%3A%20%281%29%20multi-frame%203D%20segmentation%2C%20where%20prompts%0Aare%20provided%20to%20one%20or%20multiple%20slice%28s%29%20selected%20from%20the%20volume%2C%20and%20%282%29%0Asingle-frame%202D%20segmentation%2C%20where%20prompts%20are%20provided%20to%20each%20slice.%20The%0Aformer%20is%20only%20applicable%20to%203D%20modalities%2C%20while%20the%20latter%20applies%20to%20both%202D%0Aand%203D%20modalities.%20We%20learn%20that%20SAM%202%20exhibits%20similar%20performance%20as%20SAM%0Aunder%20single-frame%202D%20segmentation%2C%20and%20has%20variable%20performance%20under%0Amulti-frame%203D%20segmentation%20depending%20on%20the%20choices%20of%20slices%20to%20annotate%2C%20the%0Adirection%20of%20the%20propagation%2C%20the%20predictions%20utilized%20during%20the%20propagation%2C%0Aetc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520anything%2520model%25202%253A%2520an%2520application%2520to%25202D%2520and%25203D%2520medical%2520images%26entry.906535625%3DHaoyu%2520Dong%2520and%2520Hanxue%2520Gu%2520and%2520Yaqian%2520Chen%2520and%2520Jichen%2520Yang%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520gained%2520significant%2520attention%2520because%2520of%2520its%250Aability%2520to%2520segment%2520a%2520variety%2520of%2520objects%2520in%2520images%2520given%2520a%2520prompt.%2520The%2520recently%250Adeveloped%2520SAM%25202%2520has%2520extended%2520this%2520ability%2520to%2520video%2520inputs.%2520This%2520opens%2520an%250Aopportunity%2520to%2520apply%2520SAM%2520to%25203D%2520images%252C%2520one%2520of%2520the%2520fundamental%2520tasks%2520in%2520the%250Amedical%2520imaging%2520field.%2520In%2520this%2520paper%252C%2520we%2520provide%2520an%2520extensive%2520evaluation%2520of%2520SAM%250A2%2527s%2520ability%2520to%2520segment%2520both%25202D%2520and%25203D%2520medical%2520images.%2520We%2520collect%252018%2520medical%250Aimaging%2520datasets%252C%2520including%2520common%25203D%2520modalities%2520such%2520as%2520computed%2520tomography%250A%2528CT%2529%252C%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%252C%2520and%2520positron%2520emission%2520tomography%2520%2528PET%2529%250Aas%2520well%2520as%25202D%2520modalities%2520such%2520as%2520X-ray%2520and%2520ultrasound.%2520We%2520consider%2520two%250Aevaluation%2520pipelines%2520of%2520SAM%25202%253A%2520%25281%2529%2520multi-frame%25203D%2520segmentation%252C%2520where%2520prompts%250Aare%2520provided%2520to%2520one%2520or%2520multiple%2520slice%2528s%2529%2520selected%2520from%2520the%2520volume%252C%2520and%2520%25282%2529%250Asingle-frame%25202D%2520segmentation%252C%2520where%2520prompts%2520are%2520provided%2520to%2520each%2520slice.%2520The%250Aformer%2520is%2520only%2520applicable%2520to%25203D%2520modalities%252C%2520while%2520the%2520latter%2520applies%2520to%2520both%25202D%250Aand%25203D%2520modalities.%2520We%2520learn%2520that%2520SAM%25202%2520exhibits%2520similar%2520performance%2520as%2520SAM%250Aunder%2520single-frame%25202D%2520segmentation%252C%2520and%2520has%2520variable%2520performance%2520under%250Amulti-frame%25203D%2520segmentation%2520depending%2520on%2520the%2520choices%2520of%2520slices%2520to%2520annotate%252C%2520the%250Adirection%2520of%2520the%2520propagation%252C%2520the%2520predictions%2520utilized%2520during%2520the%2520propagation%252C%250Aetc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images&entry.906535625=Haoyu%20Dong%20and%20Hanxue%20Gu%20and%20Yaqian%20Chen%20and%20Jichen%20Yang%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20gained%20significant%20attention%20because%20of%20its%0Aability%20to%20segment%20a%20variety%20of%20objects%20in%20images%20given%20a%20prompt.%20The%20recently%0Adeveloped%20SAM%202%20has%20extended%20this%20ability%20to%20video%20inputs.%20This%20opens%20an%0Aopportunity%20to%20apply%20SAM%20to%203D%20images%2C%20one%20of%20the%20fundamental%20tasks%20in%20the%0Amedical%20imaging%20field.%20In%20this%20paper%2C%20we%20provide%20an%20extensive%20evaluation%20of%20SAM%0A2%27s%20ability%20to%20segment%20both%202D%20and%203D%20medical%20images.%20We%20collect%2018%20medical%0Aimaging%20datasets%2C%20including%20common%203D%20modalities%20such%20as%20computed%20tomography%0A%28CT%29%2C%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20positron%20emission%20tomography%20%28PET%29%0Aas%20well%20as%202D%20modalities%20such%20as%20X-ray%20and%20ultrasound.%20We%20consider%20two%0Aevaluation%20pipelines%20of%20SAM%202%3A%20%281%29%20multi-frame%203D%20segmentation%2C%20where%20prompts%0Aare%20provided%20to%20one%20or%20multiple%20slice%28s%29%20selected%20from%20the%20volume%2C%20and%20%282%29%0Asingle-frame%202D%20segmentation%2C%20where%20prompts%20are%20provided%20to%20each%20slice.%20The%0Aformer%20is%20only%20applicable%20to%203D%20modalities%2C%20while%20the%20latter%20applies%20to%20both%202D%0Aand%203D%20modalities.%20We%20learn%20that%20SAM%202%20exhibits%20similar%20performance%20as%20SAM%0Aunder%20single-frame%202D%20segmentation%2C%20and%20has%20variable%20performance%20under%0Amulti-frame%203D%20segmentation%20depending%20on%20the%20choices%20of%20slices%20to%20annotate%2C%20the%0Adirection%20of%20the%20propagation%2C%20the%20predictions%20utilized%20during%20the%20propagation%2C%0Aetc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00756v1&entry.124074799=Read"},
{"title": "UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified\n  Model", "author": "Xiangyu Fan and Jiaqi Li and Zhiqian Lin and Weiye Xiao and Lei Yang", "abstract": "  Audio-driven 3D facial animation aims to map input audio to realistic facial\nmotion. Despite significant progress, limitations arise from inconsistent 3D\nannotations, restricting previous models to training on specific annotations\nand thereby constraining the training scale. In this work, we present\nUniTalker, a unified model featuring a multi-head architecture designed to\neffectively leverage datasets with varied annotations. To enhance training\nstability and ensure consistency among multi-head outputs, we employ three\ntraining strategies, namely, PCA, model warm-up, and pivot identity embedding.\nTo expand the training scale and diversity, we assemble A2F-Bench, comprising\nfive publicly available datasets and three newly curated datasets. These\ndatasets contain a wide range of audio domains, covering multilingual speech\nvoices and songs, thereby scaling the training data from commonly employed\ndatasets, typically less than 1 hour, to 18.5 hours. With a single trained\nUniTalker model, we achieve substantial lip vertex error reductions of 9.2% for\nBIWI dataset and 13.7% for Vocaset. Additionally, the pre-trained UniTalker\nexhibits promise as the foundation model for audio-driven facial animation\ntasks. Fine-tuning the pre-trained UniTalker on seen datasets further enhances\nperformance on each dataset, with an average error reduction of 6.3% on\nA2F-Bench. Moreover, fine-tuning UniTalker on an unseen dataset with only half\nthe data surpasses prior state-of-the-art models trained on the full dataset.\nThe code and dataset are available at the project page\nhttps://github.com/X-niper/UniTalker.\n", "link": "http://arxiv.org/abs/2408.00762v1", "date": "2024-08-01", "relevancy": 2.8463, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5866}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5606}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTalker%3A%20Scaling%20up%20Audio-Driven%203D%20Facial%20Animation%20through%20A%20Unified%0A%20%20Model&body=Title%3A%20UniTalker%3A%20Scaling%20up%20Audio-Driven%203D%20Facial%20Animation%20through%20A%20Unified%0A%20%20Model%0AAuthor%3A%20Xiangyu%20Fan%20and%20Jiaqi%20Li%20and%20Zhiqian%20Lin%20and%20Weiye%20Xiao%20and%20Lei%20Yang%0AAbstract%3A%20%20%20Audio-driven%203D%20facial%20animation%20aims%20to%20map%20input%20audio%20to%20realistic%20facial%0Amotion.%20Despite%20significant%20progress%2C%20limitations%20arise%20from%20inconsistent%203D%0Aannotations%2C%20restricting%20previous%20models%20to%20training%20on%20specific%20annotations%0Aand%20thereby%20constraining%20the%20training%20scale.%20In%20this%20work%2C%20we%20present%0AUniTalker%2C%20a%20unified%20model%20featuring%20a%20multi-head%20architecture%20designed%20to%0Aeffectively%20leverage%20datasets%20with%20varied%20annotations.%20To%20enhance%20training%0Astability%20and%20ensure%20consistency%20among%20multi-head%20outputs%2C%20we%20employ%20three%0Atraining%20strategies%2C%20namely%2C%20PCA%2C%20model%20warm-up%2C%20and%20pivot%20identity%20embedding.%0ATo%20expand%20the%20training%20scale%20and%20diversity%2C%20we%20assemble%20A2F-Bench%2C%20comprising%0Afive%20publicly%20available%20datasets%20and%20three%20newly%20curated%20datasets.%20These%0Adatasets%20contain%20a%20wide%20range%20of%20audio%20domains%2C%20covering%20multilingual%20speech%0Avoices%20and%20songs%2C%20thereby%20scaling%20the%20training%20data%20from%20commonly%20employed%0Adatasets%2C%20typically%20less%20than%201%20hour%2C%20to%2018.5%20hours.%20With%20a%20single%20trained%0AUniTalker%20model%2C%20we%20achieve%20substantial%20lip%20vertex%20error%20reductions%20of%209.2%25%20for%0ABIWI%20dataset%20and%2013.7%25%20for%20Vocaset.%20Additionally%2C%20the%20pre-trained%20UniTalker%0Aexhibits%20promise%20as%20the%20foundation%20model%20for%20audio-driven%20facial%20animation%0Atasks.%20Fine-tuning%20the%20pre-trained%20UniTalker%20on%20seen%20datasets%20further%20enhances%0Aperformance%20on%20each%20dataset%2C%20with%20an%20average%20error%20reduction%20of%206.3%25%20on%0AA2F-Bench.%20Moreover%2C%20fine-tuning%20UniTalker%20on%20an%20unseen%20dataset%20with%20only%20half%0Athe%20data%20surpasses%20prior%20state-of-the-art%20models%20trained%20on%20the%20full%20dataset.%0AThe%20code%20and%20dataset%20are%20available%20at%20the%20project%20page%0Ahttps%3A//github.com/X-niper/UniTalker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTalker%253A%2520Scaling%2520up%2520Audio-Driven%25203D%2520Facial%2520Animation%2520through%2520A%2520Unified%250A%2520%2520Model%26entry.906535625%3DXiangyu%2520Fan%2520and%2520Jiaqi%2520Li%2520and%2520Zhiqian%2520Lin%2520and%2520Weiye%2520Xiao%2520and%2520Lei%2520Yang%26entry.1292438233%3D%2520%2520Audio-driven%25203D%2520facial%2520animation%2520aims%2520to%2520map%2520input%2520audio%2520to%2520realistic%2520facial%250Amotion.%2520Despite%2520significant%2520progress%252C%2520limitations%2520arise%2520from%2520inconsistent%25203D%250Aannotations%252C%2520restricting%2520previous%2520models%2520to%2520training%2520on%2520specific%2520annotations%250Aand%2520thereby%2520constraining%2520the%2520training%2520scale.%2520In%2520this%2520work%252C%2520we%2520present%250AUniTalker%252C%2520a%2520unified%2520model%2520featuring%2520a%2520multi-head%2520architecture%2520designed%2520to%250Aeffectively%2520leverage%2520datasets%2520with%2520varied%2520annotations.%2520To%2520enhance%2520training%250Astability%2520and%2520ensure%2520consistency%2520among%2520multi-head%2520outputs%252C%2520we%2520employ%2520three%250Atraining%2520strategies%252C%2520namely%252C%2520PCA%252C%2520model%2520warm-up%252C%2520and%2520pivot%2520identity%2520embedding.%250ATo%2520expand%2520the%2520training%2520scale%2520and%2520diversity%252C%2520we%2520assemble%2520A2F-Bench%252C%2520comprising%250Afive%2520publicly%2520available%2520datasets%2520and%2520three%2520newly%2520curated%2520datasets.%2520These%250Adatasets%2520contain%2520a%2520wide%2520range%2520of%2520audio%2520domains%252C%2520covering%2520multilingual%2520speech%250Avoices%2520and%2520songs%252C%2520thereby%2520scaling%2520the%2520training%2520data%2520from%2520commonly%2520employed%250Adatasets%252C%2520typically%2520less%2520than%25201%2520hour%252C%2520to%252018.5%2520hours.%2520With%2520a%2520single%2520trained%250AUniTalker%2520model%252C%2520we%2520achieve%2520substantial%2520lip%2520vertex%2520error%2520reductions%2520of%25209.2%2525%2520for%250ABIWI%2520dataset%2520and%252013.7%2525%2520for%2520Vocaset.%2520Additionally%252C%2520the%2520pre-trained%2520UniTalker%250Aexhibits%2520promise%2520as%2520the%2520foundation%2520model%2520for%2520audio-driven%2520facial%2520animation%250Atasks.%2520Fine-tuning%2520the%2520pre-trained%2520UniTalker%2520on%2520seen%2520datasets%2520further%2520enhances%250Aperformance%2520on%2520each%2520dataset%252C%2520with%2520an%2520average%2520error%2520reduction%2520of%25206.3%2525%2520on%250AA2F-Bench.%2520Moreover%252C%2520fine-tuning%2520UniTalker%2520on%2520an%2520unseen%2520dataset%2520with%2520only%2520half%250Athe%2520data%2520surpasses%2520prior%2520state-of-the-art%2520models%2520trained%2520on%2520the%2520full%2520dataset.%250AThe%2520code%2520and%2520dataset%2520are%2520available%2520at%2520the%2520project%2520page%250Ahttps%253A//github.com/X-niper/UniTalker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTalker%3A%20Scaling%20up%20Audio-Driven%203D%20Facial%20Animation%20through%20A%20Unified%0A%20%20Model&entry.906535625=Xiangyu%20Fan%20and%20Jiaqi%20Li%20and%20Zhiqian%20Lin%20and%20Weiye%20Xiao%20and%20Lei%20Yang&entry.1292438233=%20%20Audio-driven%203D%20facial%20animation%20aims%20to%20map%20input%20audio%20to%20realistic%20facial%0Amotion.%20Despite%20significant%20progress%2C%20limitations%20arise%20from%20inconsistent%203D%0Aannotations%2C%20restricting%20previous%20models%20to%20training%20on%20specific%20annotations%0Aand%20thereby%20constraining%20the%20training%20scale.%20In%20this%20work%2C%20we%20present%0AUniTalker%2C%20a%20unified%20model%20featuring%20a%20multi-head%20architecture%20designed%20to%0Aeffectively%20leverage%20datasets%20with%20varied%20annotations.%20To%20enhance%20training%0Astability%20and%20ensure%20consistency%20among%20multi-head%20outputs%2C%20we%20employ%20three%0Atraining%20strategies%2C%20namely%2C%20PCA%2C%20model%20warm-up%2C%20and%20pivot%20identity%20embedding.%0ATo%20expand%20the%20training%20scale%20and%20diversity%2C%20we%20assemble%20A2F-Bench%2C%20comprising%0Afive%20publicly%20available%20datasets%20and%20three%20newly%20curated%20datasets.%20These%0Adatasets%20contain%20a%20wide%20range%20of%20audio%20domains%2C%20covering%20multilingual%20speech%0Avoices%20and%20songs%2C%20thereby%20scaling%20the%20training%20data%20from%20commonly%20employed%0Adatasets%2C%20typically%20less%20than%201%20hour%2C%20to%2018.5%20hours.%20With%20a%20single%20trained%0AUniTalker%20model%2C%20we%20achieve%20substantial%20lip%20vertex%20error%20reductions%20of%209.2%25%20for%0ABIWI%20dataset%20and%2013.7%25%20for%20Vocaset.%20Additionally%2C%20the%20pre-trained%20UniTalker%0Aexhibits%20promise%20as%20the%20foundation%20model%20for%20audio-driven%20facial%20animation%0Atasks.%20Fine-tuning%20the%20pre-trained%20UniTalker%20on%20seen%20datasets%20further%20enhances%0Aperformance%20on%20each%20dataset%2C%20with%20an%20average%20error%20reduction%20of%206.3%25%20on%0AA2F-Bench.%20Moreover%2C%20fine-tuning%20UniTalker%20on%20an%20unseen%20dataset%20with%20only%20half%0Athe%20data%20surpasses%20prior%20state-of-the-art%20models%20trained%20on%20the%20full%20dataset.%0AThe%20code%20and%20dataset%20are%20available%20at%20the%20project%20page%0Ahttps%3A//github.com/X-niper/UniTalker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00762v1&entry.124074799=Read"},
{"title": "Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment\n  and Multi-Scale Token Recycling", "author": "Jianan Jiang and Hao Tang and Zhilin Jiang and Weiren Yu and Di Wu", "abstract": "  Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims to minimize the\ndistance between sketches and corresponding images in the embedding space.\nHowever, scalability is hindered by the growing complexity of solutions, mainly\ndue to the abstract nature of fine-grained sketches. In this paper, we propose\nan effective approach to narrow the gap between the two domains. It mainly\nfacilitates unified mutual information sharing both intra- and inter-samples,\nrather than treating them as a single feature alignment problem between\nmodalities. Specifically, our approach includes: (i) Employing dual\nweight-sharing networks to optimize alignment within the sketch and image\ndomain, which also effectively mitigates model learning saturation issues. (ii)\nIntroducing an objective optimization function based on contrastive loss to\nenhance the model's ability to align features in both intra- and inter-samples.\n(iii) Presenting a self-supervised Multi-Scale Token Recycling (MSTR) Module\nfeatured by recycling discarded patch tokens in multi-scale features, further\nenhancing representation capability and retrieval performance. Our framework\nachieves excellent results on CNN- and ViT-based backbones. Extensive\nexperiments demonstrate its superiority over existing methods. We also\nintroduce Cloths-V1, the first professional fashion sketch-image dataset,\nutilized to validate our method and will be beneficial for other applications\n", "link": "http://arxiv.org/abs/2406.11551v3", "date": "2024-08-01", "relevancy": 2.7858, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5744}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Self-Supervised%20FG-SBIR%20with%20Unified%20Sample%20Feature%20Alignment%0A%20%20and%20Multi-Scale%20Token%20Recycling&body=Title%3A%20Towards%20Self-Supervised%20FG-SBIR%20with%20Unified%20Sample%20Feature%20Alignment%0A%20%20and%20Multi-Scale%20Token%20Recycling%0AAuthor%3A%20Jianan%20Jiang%20and%20Hao%20Tang%20and%20Zhilin%20Jiang%20and%20Weiren%20Yu%20and%20Di%20Wu%0AAbstract%3A%20%20%20Fine-Grained%20Sketch-Based%20Image%20Retrieval%20%28FG-SBIR%29%20aims%20to%20minimize%20the%0Adistance%20between%20sketches%20and%20corresponding%20images%20in%20the%20embedding%20space.%0AHowever%2C%20scalability%20is%20hindered%20by%20the%20growing%20complexity%20of%20solutions%2C%20mainly%0Adue%20to%20the%20abstract%20nature%20of%20fine-grained%20sketches.%20In%20this%20paper%2C%20we%20propose%0Aan%20effective%20approach%20to%20narrow%20the%20gap%20between%20the%20two%20domains.%20It%20mainly%0Afacilitates%20unified%20mutual%20information%20sharing%20both%20intra-%20and%20inter-samples%2C%0Arather%20than%20treating%20them%20as%20a%20single%20feature%20alignment%20problem%20between%0Amodalities.%20Specifically%2C%20our%20approach%20includes%3A%20%28i%29%20Employing%20dual%0Aweight-sharing%20networks%20to%20optimize%20alignment%20within%20the%20sketch%20and%20image%0Adomain%2C%20which%20also%20effectively%20mitigates%20model%20learning%20saturation%20issues.%20%28ii%29%0AIntroducing%20an%20objective%20optimization%20function%20based%20on%20contrastive%20loss%20to%0Aenhance%20the%20model%27s%20ability%20to%20align%20features%20in%20both%20intra-%20and%20inter-samples.%0A%28iii%29%20Presenting%20a%20self-supervised%20Multi-Scale%20Token%20Recycling%20%28MSTR%29%20Module%0Afeatured%20by%20recycling%20discarded%20patch%20tokens%20in%20multi-scale%20features%2C%20further%0Aenhancing%20representation%20capability%20and%20retrieval%20performance.%20Our%20framework%0Aachieves%20excellent%20results%20on%20CNN-%20and%20ViT-based%20backbones.%20Extensive%0Aexperiments%20demonstrate%20its%20superiority%20over%20existing%20methods.%20We%20also%0Aintroduce%20Cloths-V1%2C%20the%20first%20professional%20fashion%20sketch-image%20dataset%2C%0Autilized%20to%20validate%20our%20method%20and%20will%20be%20beneficial%20for%20other%20applications%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11551v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Self-Supervised%2520FG-SBIR%2520with%2520Unified%2520Sample%2520Feature%2520Alignment%250A%2520%2520and%2520Multi-Scale%2520Token%2520Recycling%26entry.906535625%3DJianan%2520Jiang%2520and%2520Hao%2520Tang%2520and%2520Zhilin%2520Jiang%2520and%2520Weiren%2520Yu%2520and%2520Di%2520Wu%26entry.1292438233%3D%2520%2520Fine-Grained%2520Sketch-Based%2520Image%2520Retrieval%2520%2528FG-SBIR%2529%2520aims%2520to%2520minimize%2520the%250Adistance%2520between%2520sketches%2520and%2520corresponding%2520images%2520in%2520the%2520embedding%2520space.%250AHowever%252C%2520scalability%2520is%2520hindered%2520by%2520the%2520growing%2520complexity%2520of%2520solutions%252C%2520mainly%250Adue%2520to%2520the%2520abstract%2520nature%2520of%2520fine-grained%2520sketches.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aan%2520effective%2520approach%2520to%2520narrow%2520the%2520gap%2520between%2520the%2520two%2520domains.%2520It%2520mainly%250Afacilitates%2520unified%2520mutual%2520information%2520sharing%2520both%2520intra-%2520and%2520inter-samples%252C%250Arather%2520than%2520treating%2520them%2520as%2520a%2520single%2520feature%2520alignment%2520problem%2520between%250Amodalities.%2520Specifically%252C%2520our%2520approach%2520includes%253A%2520%2528i%2529%2520Employing%2520dual%250Aweight-sharing%2520networks%2520to%2520optimize%2520alignment%2520within%2520the%2520sketch%2520and%2520image%250Adomain%252C%2520which%2520also%2520effectively%2520mitigates%2520model%2520learning%2520saturation%2520issues.%2520%2528ii%2529%250AIntroducing%2520an%2520objective%2520optimization%2520function%2520based%2520on%2520contrastive%2520loss%2520to%250Aenhance%2520the%2520model%2527s%2520ability%2520to%2520align%2520features%2520in%2520both%2520intra-%2520and%2520inter-samples.%250A%2528iii%2529%2520Presenting%2520a%2520self-supervised%2520Multi-Scale%2520Token%2520Recycling%2520%2528MSTR%2529%2520Module%250Afeatured%2520by%2520recycling%2520discarded%2520patch%2520tokens%2520in%2520multi-scale%2520features%252C%2520further%250Aenhancing%2520representation%2520capability%2520and%2520retrieval%2520performance.%2520Our%2520framework%250Aachieves%2520excellent%2520results%2520on%2520CNN-%2520and%2520ViT-based%2520backbones.%2520Extensive%250Aexperiments%2520demonstrate%2520its%2520superiority%2520over%2520existing%2520methods.%2520We%2520also%250Aintroduce%2520Cloths-V1%252C%2520the%2520first%2520professional%2520fashion%2520sketch-image%2520dataset%252C%250Autilized%2520to%2520validate%2520our%2520method%2520and%2520will%2520be%2520beneficial%2520for%2520other%2520applications%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11551v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Self-Supervised%20FG-SBIR%20with%20Unified%20Sample%20Feature%20Alignment%0A%20%20and%20Multi-Scale%20Token%20Recycling&entry.906535625=Jianan%20Jiang%20and%20Hao%20Tang%20and%20Zhilin%20Jiang%20and%20Weiren%20Yu%20and%20Di%20Wu&entry.1292438233=%20%20Fine-Grained%20Sketch-Based%20Image%20Retrieval%20%28FG-SBIR%29%20aims%20to%20minimize%20the%0Adistance%20between%20sketches%20and%20corresponding%20images%20in%20the%20embedding%20space.%0AHowever%2C%20scalability%20is%20hindered%20by%20the%20growing%20complexity%20of%20solutions%2C%20mainly%0Adue%20to%20the%20abstract%20nature%20of%20fine-grained%20sketches.%20In%20this%20paper%2C%20we%20propose%0Aan%20effective%20approach%20to%20narrow%20the%20gap%20between%20the%20two%20domains.%20It%20mainly%0Afacilitates%20unified%20mutual%20information%20sharing%20both%20intra-%20and%20inter-samples%2C%0Arather%20than%20treating%20them%20as%20a%20single%20feature%20alignment%20problem%20between%0Amodalities.%20Specifically%2C%20our%20approach%20includes%3A%20%28i%29%20Employing%20dual%0Aweight-sharing%20networks%20to%20optimize%20alignment%20within%20the%20sketch%20and%20image%0Adomain%2C%20which%20also%20effectively%20mitigates%20model%20learning%20saturation%20issues.%20%28ii%29%0AIntroducing%20an%20objective%20optimization%20function%20based%20on%20contrastive%20loss%20to%0Aenhance%20the%20model%27s%20ability%20to%20align%20features%20in%20both%20intra-%20and%20inter-samples.%0A%28iii%29%20Presenting%20a%20self-supervised%20Multi-Scale%20Token%20Recycling%20%28MSTR%29%20Module%0Afeatured%20by%20recycling%20discarded%20patch%20tokens%20in%20multi-scale%20features%2C%20further%0Aenhancing%20representation%20capability%20and%20retrieval%20performance.%20Our%20framework%0Aachieves%20excellent%20results%20on%20CNN-%20and%20ViT-based%20backbones.%20Extensive%0Aexperiments%20demonstrate%20its%20superiority%20over%20existing%20methods.%20We%20also%0Aintroduce%20Cloths-V1%2C%20the%20first%20professional%20fashion%20sketch-image%20dataset%2C%0Autilized%20to%20validate%20our%20method%20and%20will%20be%20beneficial%20for%20other%20applications%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11551v3&entry.124074799=Read"},
{"title": "A Systematic Review on Long-Tailed Learning", "author": "Chongsheng Zhang and George Almpanidis and Gaojuan Fan and Binquan Deng and Yanbo Zhang and Ji Liu and Aouaidjia Kamel and Paolo Soda and Jo\u00e3o Gama", "abstract": "  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n", "link": "http://arxiv.org/abs/2408.00483v1", "date": "2024-08-01", "relevancy": 2.7395, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6118}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5199}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Review%20on%20Long-Tailed%20Learning&body=Title%3A%20A%20Systematic%20Review%20on%20Long-Tailed%20Learning%0AAuthor%3A%20Chongsheng%20Zhang%20and%20George%20Almpanidis%20and%20Gaojuan%20Fan%20and%20Binquan%20Deng%20and%20Yanbo%20Zhang%20and%20Ji%20Liu%20and%20Aouaidjia%20Kamel%20and%20Paolo%20Soda%20and%20Jo%C3%A3o%20Gama%0AAbstract%3A%20%20%20Long-tailed%20data%20is%20a%20special%20type%20of%20multi-class%20imbalanced%20data%20with%20a%20very%0Alarge%20amount%20of%20minority/tail%20classes%20that%20have%20a%20very%20significant%20combined%0Ainfluence.%20Long-tailed%20learning%20aims%20to%20build%20high-performance%20models%20on%0Adatasets%20with%20long-tailed%20distributions%2C%20which%20can%20identify%20all%20the%20classes%0Awith%20high%20accuracy%2C%20in%20particular%20the%20minority/tail%20classes.%20It%20is%20a%0Acutting-edge%20research%20direction%20that%20has%20attracted%20a%20remarkable%20amount%20of%0Aresearch%20effort%20in%20the%20past%20few%20years.%20In%20this%20paper%2C%20we%20present%20a%0Acomprehensive%20survey%20of%20latest%20advances%20in%20long-tailed%20visual%20learning.%20We%0Afirst%20propose%20a%20new%20taxonomy%20for%20long-tailed%20learning%2C%20which%20consists%20of%20eight%0Adifferent%20dimensions%2C%20including%20data%20balancing%2C%20neural%20architecture%2C%20feature%0Aenrichment%2C%20logits%20adjustment%2C%20loss%20function%2C%20bells%20and%20whistles%2C%20network%0Aoptimization%2C%20and%20post%20hoc%20processing%20techniques.%20Based%20on%20our%20proposed%0Ataxonomy%2C%20we%20present%20a%20systematic%20review%20of%20long-tailed%20learning%20methods%2C%0Adiscussing%20their%20commonalities%20and%20alignable%20differences.%20We%20also%20analyze%20the%0Adifferences%20between%20imbalance%20learning%20and%20long-tailed%20learning%20approaches.%0AFinally%2C%20we%20discuss%20prospects%20and%20future%20directions%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Review%2520on%2520Long-Tailed%2520Learning%26entry.906535625%3DChongsheng%2520Zhang%2520and%2520George%2520Almpanidis%2520and%2520Gaojuan%2520Fan%2520and%2520Binquan%2520Deng%2520and%2520Yanbo%2520Zhang%2520and%2520Ji%2520Liu%2520and%2520Aouaidjia%2520Kamel%2520and%2520Paolo%2520Soda%2520and%2520Jo%25C3%25A3o%2520Gama%26entry.1292438233%3D%2520%2520Long-tailed%2520data%2520is%2520a%2520special%2520type%2520of%2520multi-class%2520imbalanced%2520data%2520with%2520a%2520very%250Alarge%2520amount%2520of%2520minority/tail%2520classes%2520that%2520have%2520a%2520very%2520significant%2520combined%250Ainfluence.%2520Long-tailed%2520learning%2520aims%2520to%2520build%2520high-performance%2520models%2520on%250Adatasets%2520with%2520long-tailed%2520distributions%252C%2520which%2520can%2520identify%2520all%2520the%2520classes%250Awith%2520high%2520accuracy%252C%2520in%2520particular%2520the%2520minority/tail%2520classes.%2520It%2520is%2520a%250Acutting-edge%2520research%2520direction%2520that%2520has%2520attracted%2520a%2520remarkable%2520amount%2520of%250Aresearch%2520effort%2520in%2520the%2520past%2520few%2520years.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Acomprehensive%2520survey%2520of%2520latest%2520advances%2520in%2520long-tailed%2520visual%2520learning.%2520We%250Afirst%2520propose%2520a%2520new%2520taxonomy%2520for%2520long-tailed%2520learning%252C%2520which%2520consists%2520of%2520eight%250Adifferent%2520dimensions%252C%2520including%2520data%2520balancing%252C%2520neural%2520architecture%252C%2520feature%250Aenrichment%252C%2520logits%2520adjustment%252C%2520loss%2520function%252C%2520bells%2520and%2520whistles%252C%2520network%250Aoptimization%252C%2520and%2520post%2520hoc%2520processing%2520techniques.%2520Based%2520on%2520our%2520proposed%250Ataxonomy%252C%2520we%2520present%2520a%2520systematic%2520review%2520of%2520long-tailed%2520learning%2520methods%252C%250Adiscussing%2520their%2520commonalities%2520and%2520alignable%2520differences.%2520We%2520also%2520analyze%2520the%250Adifferences%2520between%2520imbalance%2520learning%2520and%2520long-tailed%2520learning%2520approaches.%250AFinally%252C%2520we%2520discuss%2520prospects%2520and%2520future%2520directions%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Review%20on%20Long-Tailed%20Learning&entry.906535625=Chongsheng%20Zhang%20and%20George%20Almpanidis%20and%20Gaojuan%20Fan%20and%20Binquan%20Deng%20and%20Yanbo%20Zhang%20and%20Ji%20Liu%20and%20Aouaidjia%20Kamel%20and%20Paolo%20Soda%20and%20Jo%C3%A3o%20Gama&entry.1292438233=%20%20Long-tailed%20data%20is%20a%20special%20type%20of%20multi-class%20imbalanced%20data%20with%20a%20very%0Alarge%20amount%20of%20minority/tail%20classes%20that%20have%20a%20very%20significant%20combined%0Ainfluence.%20Long-tailed%20learning%20aims%20to%20build%20high-performance%20models%20on%0Adatasets%20with%20long-tailed%20distributions%2C%20which%20can%20identify%20all%20the%20classes%0Awith%20high%20accuracy%2C%20in%20particular%20the%20minority/tail%20classes.%20It%20is%20a%0Acutting-edge%20research%20direction%20that%20has%20attracted%20a%20remarkable%20amount%20of%0Aresearch%20effort%20in%20the%20past%20few%20years.%20In%20this%20paper%2C%20we%20present%20a%0Acomprehensive%20survey%20of%20latest%20advances%20in%20long-tailed%20visual%20learning.%20We%0Afirst%20propose%20a%20new%20taxonomy%20for%20long-tailed%20learning%2C%20which%20consists%20of%20eight%0Adifferent%20dimensions%2C%20including%20data%20balancing%2C%20neural%20architecture%2C%20feature%0Aenrichment%2C%20logits%20adjustment%2C%20loss%20function%2C%20bells%20and%20whistles%2C%20network%0Aoptimization%2C%20and%20post%20hoc%20processing%20techniques.%20Based%20on%20our%20proposed%0Ataxonomy%2C%20we%20present%20a%20systematic%20review%20of%20long-tailed%20learning%20methods%2C%0Adiscussing%20their%20commonalities%20and%20alignable%20differences.%20We%20also%20analyze%20the%0Adifferences%20between%20imbalance%20learning%20and%20long-tailed%20learning%20approaches.%0AFinally%2C%20we%20discuss%20prospects%20and%20future%20directions%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00483v1&entry.124074799=Read"},
{"title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse\n  Autoencoders", "author": "Senthooran Rajamanoharan and Tom Lieberum and Nicolas Sonnerat and Arthur Conmy and Vikrant Varma and J\u00e1nos Kram\u00e1r and Neel Nanda", "abstract": "  Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.\n", "link": "http://arxiv.org/abs/2407.14435v3", "date": "2024-08-01", "relevancy": 2.6151, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5606}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5082}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders&body=Title%3A%20Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20Senthooran%20Rajamanoharan%20and%20Tom%20Lieberum%20and%20Nicolas%20Sonnerat%20and%20Arthur%20Conmy%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20unsupervised%20approach%20for%0Aidentifying%20causally%20relevant%20and%20interpretable%20linear%20features%20in%20a%20language%0Amodel%27s%20%28LM%29%20activations.%20To%20be%20useful%20for%20downstream%20tasks%2C%20SAEs%20need%20to%0Adecompose%20LM%20activations%20faithfully%3B%20yet%20to%20be%20interpretable%20the%20decomposition%0Amust%20be%20sparse%20--%20two%20objectives%20that%20are%20in%20tension.%20In%20this%20paper%2C%20we%0Aintroduce%20JumpReLU%20SAEs%2C%20which%20achieve%20state-of-the-art%20reconstruction%20fidelity%0Aat%20a%20given%20sparsity%20level%20on%20Gemma%202%209B%20activations%2C%20compared%20to%20other%20recent%0Aadvances%20such%20as%20Gated%20and%20TopK%20SAEs.%20We%20also%20show%20that%20this%20improvement%20does%0Anot%20come%20at%20the%20cost%20of%20interpretability%20through%20manual%20and%20automated%0Ainterpretability%20studies.%20JumpReLU%20SAEs%20are%20a%20simple%20modification%20of%20vanilla%0A%28ReLU%29%20SAEs%20--%20where%20we%20replace%20the%20ReLU%20with%20a%20discontinuous%20JumpReLU%0Aactivation%20function%20--%20and%20are%20similarly%20efficient%20to%20train%20and%20run.%20By%0Autilising%20straight-through-estimators%20%28STEs%29%20in%20a%20principled%20manner%2C%20we%20show%0Ahow%20it%20is%20possible%20to%20train%20JumpReLU%20SAEs%20effectively%20despite%20the%20discontinuous%0AJumpReLU%20function%20introduced%20in%20the%20SAE%27s%20forward%20pass.%20Similarly%2C%20we%20use%20STEs%0Ato%20directly%20train%20L0%20to%20be%20sparse%2C%20instead%20of%20training%20on%20proxies%20such%20as%20L1%2C%0Aavoiding%20problems%20like%20shrinkage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14435v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJumping%2520Ahead%253A%2520Improving%2520Reconstruction%2520Fidelity%2520with%2520JumpReLU%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DSenthooran%2520Rajamanoharan%2520and%2520Tom%2520Lieberum%2520and%2520Nicolas%2520Sonnerat%2520and%2520Arthur%2520Conmy%2520and%2520Vikrant%2520Varma%2520and%2520J%25C3%25A1nos%2520Kram%25C3%25A1r%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520%2528SAEs%2529%2520are%2520a%2520promising%2520unsupervised%2520approach%2520for%250Aidentifying%2520causally%2520relevant%2520and%2520interpretable%2520linear%2520features%2520in%2520a%2520language%250Amodel%2527s%2520%2528LM%2529%2520activations.%2520To%2520be%2520useful%2520for%2520downstream%2520tasks%252C%2520SAEs%2520need%2520to%250Adecompose%2520LM%2520activations%2520faithfully%253B%2520yet%2520to%2520be%2520interpretable%2520the%2520decomposition%250Amust%2520be%2520sparse%2520--%2520two%2520objectives%2520that%2520are%2520in%2520tension.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520JumpReLU%2520SAEs%252C%2520which%2520achieve%2520state-of-the-art%2520reconstruction%2520fidelity%250Aat%2520a%2520given%2520sparsity%2520level%2520on%2520Gemma%25202%25209B%2520activations%252C%2520compared%2520to%2520other%2520recent%250Aadvances%2520such%2520as%2520Gated%2520and%2520TopK%2520SAEs.%2520We%2520also%2520show%2520that%2520this%2520improvement%2520does%250Anot%2520come%2520at%2520the%2520cost%2520of%2520interpretability%2520through%2520manual%2520and%2520automated%250Ainterpretability%2520studies.%2520JumpReLU%2520SAEs%2520are%2520a%2520simple%2520modification%2520of%2520vanilla%250A%2528ReLU%2529%2520SAEs%2520--%2520where%2520we%2520replace%2520the%2520ReLU%2520with%2520a%2520discontinuous%2520JumpReLU%250Aactivation%2520function%2520--%2520and%2520are%2520similarly%2520efficient%2520to%2520train%2520and%2520run.%2520By%250Autilising%2520straight-through-estimators%2520%2528STEs%2529%2520in%2520a%2520principled%2520manner%252C%2520we%2520show%250Ahow%2520it%2520is%2520possible%2520to%2520train%2520JumpReLU%2520SAEs%2520effectively%2520despite%2520the%2520discontinuous%250AJumpReLU%2520function%2520introduced%2520in%2520the%2520SAE%2527s%2520forward%2520pass.%2520Similarly%252C%2520we%2520use%2520STEs%250Ato%2520directly%2520train%2520L0%2520to%2520be%2520sparse%252C%2520instead%2520of%2520training%2520on%2520proxies%2520such%2520as%2520L1%252C%250Aavoiding%2520problems%2520like%2520shrinkage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14435v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jumping%20Ahead%3A%20Improving%20Reconstruction%20Fidelity%20with%20JumpReLU%20Sparse%0A%20%20Autoencoders&entry.906535625=Senthooran%20Rajamanoharan%20and%20Tom%20Lieberum%20and%20Nicolas%20Sonnerat%20and%20Arthur%20Conmy%20and%20Vikrant%20Varma%20and%20J%C3%A1nos%20Kram%C3%A1r%20and%20Neel%20Nanda&entry.1292438233=%20%20Sparse%20autoencoders%20%28SAEs%29%20are%20a%20promising%20unsupervised%20approach%20for%0Aidentifying%20causally%20relevant%20and%20interpretable%20linear%20features%20in%20a%20language%0Amodel%27s%20%28LM%29%20activations.%20To%20be%20useful%20for%20downstream%20tasks%2C%20SAEs%20need%20to%0Adecompose%20LM%20activations%20faithfully%3B%20yet%20to%20be%20interpretable%20the%20decomposition%0Amust%20be%20sparse%20--%20two%20objectives%20that%20are%20in%20tension.%20In%20this%20paper%2C%20we%0Aintroduce%20JumpReLU%20SAEs%2C%20which%20achieve%20state-of-the-art%20reconstruction%20fidelity%0Aat%20a%20given%20sparsity%20level%20on%20Gemma%202%209B%20activations%2C%20compared%20to%20other%20recent%0Aadvances%20such%20as%20Gated%20and%20TopK%20SAEs.%20We%20also%20show%20that%20this%20improvement%20does%0Anot%20come%20at%20the%20cost%20of%20interpretability%20through%20manual%20and%20automated%0Ainterpretability%20studies.%20JumpReLU%20SAEs%20are%20a%20simple%20modification%20of%20vanilla%0A%28ReLU%29%20SAEs%20--%20where%20we%20replace%20the%20ReLU%20with%20a%20discontinuous%20JumpReLU%0Aactivation%20function%20--%20and%20are%20similarly%20efficient%20to%20train%20and%20run.%20By%0Autilising%20straight-through-estimators%20%28STEs%29%20in%20a%20principled%20manner%2C%20we%20show%0Ahow%20it%20is%20possible%20to%20train%20JumpReLU%20SAEs%20effectively%20despite%20the%20discontinuous%0AJumpReLU%20function%20introduced%20in%20the%20SAE%27s%20forward%20pass.%20Similarly%2C%20we%20use%20STEs%0Ato%20directly%20train%20L0%20to%20be%20sparse%2C%20instead%20of%20training%20on%20proxies%20such%20as%20L1%2C%0Aavoiding%20problems%20like%20shrinkage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14435v3&entry.124074799=Read"},
{"title": "Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and\n  Flexible Scene Text Retrieval", "author": "Gangyan Zeng and Yuan Zhang and Jin Wei and Dongbao Yang and Peng Zhang and Yiwen Gao and Xugong Qin and Yu Zhou", "abstract": "  Scene text retrieval aims to find all images containing the query text from\nan image gallery. Current efforts tend to adopt an Optical Character\nRecognition (OCR) pipeline, which requires complicated text detection and/or\nrecognition processes, resulting in inefficient and inflexible retrieval.\nDifferent from them, in this work we propose to explore the intrinsic potential\nof Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text\nretrieval. Through empirical analysis, we observe that the main challenges of\nCLIP as a text retriever are: 1) limited text perceptual scale, and 2)\nentangled visual-semantic concepts. To this end, a novel model termed FDP\n(Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text\nvia shifting the attention to the text area and probing the hidden text\nknowledge, and then divides the query text into content word and function word\nfor processing, in which a semantic-aware prompting scheme and a distracted\nqueries assistance module are utilized. Extensive experiments show that FDP\nsignificantly enhances the inference speed while achieving better or\ncompetitive retrieval accuracy compared to existing methods. Notably, on the\nIIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4\ntimes faster speed. Furthermore, additional experiments under phrase-level and\nattribute-aware scene text retrieval settings validate FDP's particular\nadvantages in handling diverse forms of query text. The source code will be\npublicly available at https://github.com/Gyann-z/FDP.\n", "link": "http://arxiv.org/abs/2408.00441v1", "date": "2024-08-01", "relevancy": 2.6068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5566}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5107}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focus%2C%20Distinguish%2C%20and%20Prompt%3A%20Unleashing%20CLIP%20for%20Efficient%20and%0A%20%20Flexible%20Scene%20Text%20Retrieval&body=Title%3A%20Focus%2C%20Distinguish%2C%20and%20Prompt%3A%20Unleashing%20CLIP%20for%20Efficient%20and%0A%20%20Flexible%20Scene%20Text%20Retrieval%0AAuthor%3A%20Gangyan%20Zeng%20and%20Yuan%20Zhang%20and%20Jin%20Wei%20and%20Dongbao%20Yang%20and%20Peng%20Zhang%20and%20Yiwen%20Gao%20and%20Xugong%20Qin%20and%20Yu%20Zhou%0AAbstract%3A%20%20%20Scene%20text%20retrieval%20aims%20to%20find%20all%20images%20containing%20the%20query%20text%20from%0Aan%20image%20gallery.%20Current%20efforts%20tend%20to%20adopt%20an%20Optical%20Character%0ARecognition%20%28OCR%29%20pipeline%2C%20which%20requires%20complicated%20text%20detection%20and/or%0Arecognition%20processes%2C%20resulting%20in%20inefficient%20and%20inflexible%20retrieval.%0ADifferent%20from%20them%2C%20in%20this%20work%20we%20propose%20to%20explore%20the%20intrinsic%20potential%0Aof%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20for%20OCR-free%20scene%20text%0Aretrieval.%20Through%20empirical%20analysis%2C%20we%20observe%20that%20the%20main%20challenges%20of%0ACLIP%20as%20a%20text%20retriever%20are%3A%201%29%20limited%20text%20perceptual%20scale%2C%20and%202%29%0Aentangled%20visual-semantic%20concepts.%20To%20this%20end%2C%20a%20novel%20model%20termed%20FDP%0A%28Focus%2C%20Distinguish%2C%20and%20Prompt%29%20is%20developed.%20FDP%20first%20focuses%20on%20scene%20text%0Avia%20shifting%20the%20attention%20to%20the%20text%20area%20and%20probing%20the%20hidden%20text%0Aknowledge%2C%20and%20then%20divides%20the%20query%20text%20into%20content%20word%20and%20function%20word%0Afor%20processing%2C%20in%20which%20a%20semantic-aware%20prompting%20scheme%20and%20a%20distracted%0Aqueries%20assistance%20module%20are%20utilized.%20Extensive%20experiments%20show%20that%20FDP%0Asignificantly%20enhances%20the%20inference%20speed%20while%20achieving%20better%20or%0Acompetitive%20retrieval%20accuracy%20compared%20to%20existing%20methods.%20Notably%2C%20on%20the%0AIIIT-STR%20benchmark%2C%20FDP%20surpasses%20the%20state-of-the-art%20model%20by%204.37%25%20with%20a%204%0Atimes%20faster%20speed.%20Furthermore%2C%20additional%20experiments%20under%20phrase-level%20and%0Aattribute-aware%20scene%20text%20retrieval%20settings%20validate%20FDP%27s%20particular%0Aadvantages%20in%20handling%20diverse%20forms%20of%20query%20text.%20The%20source%20code%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/Gyann-z/FDP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocus%252C%2520Distinguish%252C%2520and%2520Prompt%253A%2520Unleashing%2520CLIP%2520for%2520Efficient%2520and%250A%2520%2520Flexible%2520Scene%2520Text%2520Retrieval%26entry.906535625%3DGangyan%2520Zeng%2520and%2520Yuan%2520Zhang%2520and%2520Jin%2520Wei%2520and%2520Dongbao%2520Yang%2520and%2520Peng%2520Zhang%2520and%2520Yiwen%2520Gao%2520and%2520Xugong%2520Qin%2520and%2520Yu%2520Zhou%26entry.1292438233%3D%2520%2520Scene%2520text%2520retrieval%2520aims%2520to%2520find%2520all%2520images%2520containing%2520the%2520query%2520text%2520from%250Aan%2520image%2520gallery.%2520Current%2520efforts%2520tend%2520to%2520adopt%2520an%2520Optical%2520Character%250ARecognition%2520%2528OCR%2529%2520pipeline%252C%2520which%2520requires%2520complicated%2520text%2520detection%2520and/or%250Arecognition%2520processes%252C%2520resulting%2520in%2520inefficient%2520and%2520inflexible%2520retrieval.%250ADifferent%2520from%2520them%252C%2520in%2520this%2520work%2520we%2520propose%2520to%2520explore%2520the%2520intrinsic%2520potential%250Aof%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%2520for%2520OCR-free%2520scene%2520text%250Aretrieval.%2520Through%2520empirical%2520analysis%252C%2520we%2520observe%2520that%2520the%2520main%2520challenges%2520of%250ACLIP%2520as%2520a%2520text%2520retriever%2520are%253A%25201%2529%2520limited%2520text%2520perceptual%2520scale%252C%2520and%25202%2529%250Aentangled%2520visual-semantic%2520concepts.%2520To%2520this%2520end%252C%2520a%2520novel%2520model%2520termed%2520FDP%250A%2528Focus%252C%2520Distinguish%252C%2520and%2520Prompt%2529%2520is%2520developed.%2520FDP%2520first%2520focuses%2520on%2520scene%2520text%250Avia%2520shifting%2520the%2520attention%2520to%2520the%2520text%2520area%2520and%2520probing%2520the%2520hidden%2520text%250Aknowledge%252C%2520and%2520then%2520divides%2520the%2520query%2520text%2520into%2520content%2520word%2520and%2520function%2520word%250Afor%2520processing%252C%2520in%2520which%2520a%2520semantic-aware%2520prompting%2520scheme%2520and%2520a%2520distracted%250Aqueries%2520assistance%2520module%2520are%2520utilized.%2520Extensive%2520experiments%2520show%2520that%2520FDP%250Asignificantly%2520enhances%2520the%2520inference%2520speed%2520while%2520achieving%2520better%2520or%250Acompetitive%2520retrieval%2520accuracy%2520compared%2520to%2520existing%2520methods.%2520Notably%252C%2520on%2520the%250AIIIT-STR%2520benchmark%252C%2520FDP%2520surpasses%2520the%2520state-of-the-art%2520model%2520by%25204.37%2525%2520with%2520a%25204%250Atimes%2520faster%2520speed.%2520Furthermore%252C%2520additional%2520experiments%2520under%2520phrase-level%2520and%250Aattribute-aware%2520scene%2520text%2520retrieval%2520settings%2520validate%2520FDP%2527s%2520particular%250Aadvantages%2520in%2520handling%2520diverse%2520forms%2520of%2520query%2520text.%2520The%2520source%2520code%2520will%2520be%250Apublicly%2520available%2520at%2520https%253A//github.com/Gyann-z/FDP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focus%2C%20Distinguish%2C%20and%20Prompt%3A%20Unleashing%20CLIP%20for%20Efficient%20and%0A%20%20Flexible%20Scene%20Text%20Retrieval&entry.906535625=Gangyan%20Zeng%20and%20Yuan%20Zhang%20and%20Jin%20Wei%20and%20Dongbao%20Yang%20and%20Peng%20Zhang%20and%20Yiwen%20Gao%20and%20Xugong%20Qin%20and%20Yu%20Zhou&entry.1292438233=%20%20Scene%20text%20retrieval%20aims%20to%20find%20all%20images%20containing%20the%20query%20text%20from%0Aan%20image%20gallery.%20Current%20efforts%20tend%20to%20adopt%20an%20Optical%20Character%0ARecognition%20%28OCR%29%20pipeline%2C%20which%20requires%20complicated%20text%20detection%20and/or%0Arecognition%20processes%2C%20resulting%20in%20inefficient%20and%20inflexible%20retrieval.%0ADifferent%20from%20them%2C%20in%20this%20work%20we%20propose%20to%20explore%20the%20intrinsic%20potential%0Aof%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%20for%20OCR-free%20scene%20text%0Aretrieval.%20Through%20empirical%20analysis%2C%20we%20observe%20that%20the%20main%20challenges%20of%0ACLIP%20as%20a%20text%20retriever%20are%3A%201%29%20limited%20text%20perceptual%20scale%2C%20and%202%29%0Aentangled%20visual-semantic%20concepts.%20To%20this%20end%2C%20a%20novel%20model%20termed%20FDP%0A%28Focus%2C%20Distinguish%2C%20and%20Prompt%29%20is%20developed.%20FDP%20first%20focuses%20on%20scene%20text%0Avia%20shifting%20the%20attention%20to%20the%20text%20area%20and%20probing%20the%20hidden%20text%0Aknowledge%2C%20and%20then%20divides%20the%20query%20text%20into%20content%20word%20and%20function%20word%0Afor%20processing%2C%20in%20which%20a%20semantic-aware%20prompting%20scheme%20and%20a%20distracted%0Aqueries%20assistance%20module%20are%20utilized.%20Extensive%20experiments%20show%20that%20FDP%0Asignificantly%20enhances%20the%20inference%20speed%20while%20achieving%20better%20or%0Acompetitive%20retrieval%20accuracy%20compared%20to%20existing%20methods.%20Notably%2C%20on%20the%0AIIIT-STR%20benchmark%2C%20FDP%20surpasses%20the%20state-of-the-art%20model%20by%204.37%25%20with%20a%204%0Atimes%20faster%20speed.%20Furthermore%2C%20additional%20experiments%20under%20phrase-level%20and%0Aattribute-aware%20scene%20text%20retrieval%20settings%20validate%20FDP%27s%20particular%0Aadvantages%20in%20handling%20diverse%20forms%20of%20query%20text.%20The%20source%20code%20will%20be%0Apublicly%20available%20at%20https%3A//github.com/Gyann-z/FDP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00441v1&entry.124074799=Read"},
{"title": "Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry\n  Reconstruction in Field Conditions", "author": "Muhammad Arbab Arshad and Talukder Jubery and James Afful and Anushrut Jignasu and Aditya Balu and Baskar Ganapathysubramanian and Soumik Sarkar and Adarsh Krishnamurthy", "abstract": "  We evaluate different Neural Radiance Fields (NeRFs) techniques for\nreconstructing (3D) plants in varied environments, from indoor settings to\noutdoor fields. Traditional techniques often struggle to capture the complex\ndetails of plants, which is crucial for botanical and agricultural\nunderstanding. We evaluate three scenarios with increasing complexity and\ncompare the results with the point cloud obtained using LiDAR as ground truth\ndata. In the most realistic field scenario, the NeRF models achieve a 74.65% F1\nscore with 30 minutes of training on the GPU, highlighting the efficiency and\naccuracy of NeRFs in challenging environments. These findings not only\ndemonstrate the potential of NeRF in detailed and realistic 3D plant modeling\nbut also suggest practical approaches for enhancing the speed and efficiency of\nthe 3D reconstruction process.\n", "link": "http://arxiv.org/abs/2402.10344v2", "date": "2024-08-01", "relevancy": 2.5597, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5428}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20for%203D%20Plant%20Geometry%0A%20%20Reconstruction%20in%20Field%20Conditions&body=Title%3A%20Evaluating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20for%203D%20Plant%20Geometry%0A%20%20Reconstruction%20in%20Field%20Conditions%0AAuthor%3A%20Muhammad%20Arbab%20Arshad%20and%20Talukder%20Jubery%20and%20James%20Afful%20and%20Anushrut%20Jignasu%20and%20Aditya%20Balu%20and%20Baskar%20Ganapathysubramanian%20and%20Soumik%20Sarkar%20and%20Adarsh%20Krishnamurthy%0AAbstract%3A%20%20%20We%20evaluate%20different%20Neural%20Radiance%20Fields%20%28NeRFs%29%20techniques%20for%0Areconstructing%20%283D%29%20plants%20in%20varied%20environments%2C%20from%20indoor%20settings%20to%0Aoutdoor%20fields.%20Traditional%20techniques%20often%20struggle%20to%20capture%20the%20complex%0Adetails%20of%20plants%2C%20which%20is%20crucial%20for%20botanical%20and%20agricultural%0Aunderstanding.%20We%20evaluate%20three%20scenarios%20with%20increasing%20complexity%20and%0Acompare%20the%20results%20with%20the%20point%20cloud%20obtained%20using%20LiDAR%20as%20ground%20truth%0Adata.%20In%20the%20most%20realistic%20field%20scenario%2C%20the%20NeRF%20models%20achieve%20a%2074.65%25%20F1%0Ascore%20with%2030%20minutes%20of%20training%20on%20the%20GPU%2C%20highlighting%20the%20efficiency%20and%0Aaccuracy%20of%20NeRFs%20in%20challenging%20environments.%20These%20findings%20not%20only%0Ademonstrate%20the%20potential%20of%20NeRF%20in%20detailed%20and%20realistic%203D%20plant%20modeling%0Abut%20also%20suggest%20practical%20approaches%20for%20enhancing%20the%20speed%20and%20efficiency%20of%0Athe%203D%20reconstruction%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520for%25203D%2520Plant%2520Geometry%250A%2520%2520Reconstruction%2520in%2520Field%2520Conditions%26entry.906535625%3DMuhammad%2520Arbab%2520Arshad%2520and%2520Talukder%2520Jubery%2520and%2520James%2520Afful%2520and%2520Anushrut%2520Jignasu%2520and%2520Aditya%2520Balu%2520and%2520Baskar%2520Ganapathysubramanian%2520and%2520Soumik%2520Sarkar%2520and%2520Adarsh%2520Krishnamurthy%26entry.1292438233%3D%2520%2520We%2520evaluate%2520different%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520techniques%2520for%250Areconstructing%2520%25283D%2529%2520plants%2520in%2520varied%2520environments%252C%2520from%2520indoor%2520settings%2520to%250Aoutdoor%2520fields.%2520Traditional%2520techniques%2520often%2520struggle%2520to%2520capture%2520the%2520complex%250Adetails%2520of%2520plants%252C%2520which%2520is%2520crucial%2520for%2520botanical%2520and%2520agricultural%250Aunderstanding.%2520We%2520evaluate%2520three%2520scenarios%2520with%2520increasing%2520complexity%2520and%250Acompare%2520the%2520results%2520with%2520the%2520point%2520cloud%2520obtained%2520using%2520LiDAR%2520as%2520ground%2520truth%250Adata.%2520In%2520the%2520most%2520realistic%2520field%2520scenario%252C%2520the%2520NeRF%2520models%2520achieve%2520a%252074.65%2525%2520F1%250Ascore%2520with%252030%2520minutes%2520of%2520training%2520on%2520the%2520GPU%252C%2520highlighting%2520the%2520efficiency%2520and%250Aaccuracy%2520of%2520NeRFs%2520in%2520challenging%2520environments.%2520These%2520findings%2520not%2520only%250Ademonstrate%2520the%2520potential%2520of%2520NeRF%2520in%2520detailed%2520and%2520realistic%25203D%2520plant%2520modeling%250Abut%2520also%2520suggest%2520practical%2520approaches%2520for%2520enhancing%2520the%2520speed%2520and%2520efficiency%2520of%250Athe%25203D%2520reconstruction%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Neural%20Radiance%20Fields%20%28NeRFs%29%20for%203D%20Plant%20Geometry%0A%20%20Reconstruction%20in%20Field%20Conditions&entry.906535625=Muhammad%20Arbab%20Arshad%20and%20Talukder%20Jubery%20and%20James%20Afful%20and%20Anushrut%20Jignasu%20and%20Aditya%20Balu%20and%20Baskar%20Ganapathysubramanian%20and%20Soumik%20Sarkar%20and%20Adarsh%20Krishnamurthy&entry.1292438233=%20%20We%20evaluate%20different%20Neural%20Radiance%20Fields%20%28NeRFs%29%20techniques%20for%0Areconstructing%20%283D%29%20plants%20in%20varied%20environments%2C%20from%20indoor%20settings%20to%0Aoutdoor%20fields.%20Traditional%20techniques%20often%20struggle%20to%20capture%20the%20complex%0Adetails%20of%20plants%2C%20which%20is%20crucial%20for%20botanical%20and%20agricultural%0Aunderstanding.%20We%20evaluate%20three%20scenarios%20with%20increasing%20complexity%20and%0Acompare%20the%20results%20with%20the%20point%20cloud%20obtained%20using%20LiDAR%20as%20ground%20truth%0Adata.%20In%20the%20most%20realistic%20field%20scenario%2C%20the%20NeRF%20models%20achieve%20a%2074.65%25%20F1%0Ascore%20with%2030%20minutes%20of%20training%20on%20the%20GPU%2C%20highlighting%20the%20efficiency%20and%0Aaccuracy%20of%20NeRFs%20in%20challenging%20environments.%20These%20findings%20not%20only%0Ademonstrate%20the%20potential%20of%20NeRF%20in%20detailed%20and%20realistic%203D%20plant%20modeling%0Abut%20also%20suggest%20practical%20approaches%20for%20enhancing%20the%20speed%20and%20efficiency%20of%0Athe%203D%20reconstruction%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10344v2&entry.124074799=Read"},
{"title": "You Can't Ignore Either: Unifying Structure and Feature Denoising for\n  Robust Graph Learning", "author": "Tianmeng Yang and Jiahao Meng and Min Zhou and Yaming Yang and Yujing Wang and Xiangtai Li and Yunhai Tong", "abstract": "  Recent research on the robustness of Graph Neural Networks (GNNs) under\nnoises or attacks has attracted great attention due to its importance in\nreal-world applications. Most previous methods explore a single noise source,\nrecovering corrupt node embedding by reliable structures bias or developing\nstructure learning with reliable node features. However, the noises and attacks\nmay come from both structures and features in graphs, making the graph\ndenoising a dilemma and challenging problem. In this paper, we develop a\nunified graph denoising (UGD) framework to unravel the deadlock between\nstructure and feature denoising. Specifically, a high-order neighborhood\nproximity evaluation method is proposed to recognize noisy edges, considering\nfeatures may be perturbed simultaneously. Moreover, we propose to refine noisy\nfeatures with reconstruction based on a graph auto-encoder. An iterative\nupdating algorithm is further designed to optimize the framework and acquire a\nclean graph, thus enabling robust graph learning for downstream tasks. Our UGD\nframework is self-supervised and can be easily implemented as a plug-and-play\nmodule. We carry out extensive experiments, which proves the effectiveness and\nadvantages of our method. Code is avalaible at\nhttps://github.com/YoungTimmy/UGD.\n", "link": "http://arxiv.org/abs/2408.00700v1", "date": "2024-08-01", "relevancy": 2.559, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.515}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5142}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20Can%27t%20Ignore%20Either%3A%20Unifying%20Structure%20and%20Feature%20Denoising%20for%0A%20%20Robust%20Graph%20Learning&body=Title%3A%20You%20Can%27t%20Ignore%20Either%3A%20Unifying%20Structure%20and%20Feature%20Denoising%20for%0A%20%20Robust%20Graph%20Learning%0AAuthor%3A%20Tianmeng%20Yang%20and%20Jiahao%20Meng%20and%20Min%20Zhou%20and%20Yaming%20Yang%20and%20Yujing%20Wang%20and%20Xiangtai%20Li%20and%20Yunhai%20Tong%0AAbstract%3A%20%20%20Recent%20research%20on%20the%20robustness%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20under%0Anoises%20or%20attacks%20has%20attracted%20great%20attention%20due%20to%20its%20importance%20in%0Areal-world%20applications.%20Most%20previous%20methods%20explore%20a%20single%20noise%20source%2C%0Arecovering%20corrupt%20node%20embedding%20by%20reliable%20structures%20bias%20or%20developing%0Astructure%20learning%20with%20reliable%20node%20features.%20However%2C%20the%20noises%20and%20attacks%0Amay%20come%20from%20both%20structures%20and%20features%20in%20graphs%2C%20making%20the%20graph%0Adenoising%20a%20dilemma%20and%20challenging%20problem.%20In%20this%20paper%2C%20we%20develop%20a%0Aunified%20graph%20denoising%20%28UGD%29%20framework%20to%20unravel%20the%20deadlock%20between%0Astructure%20and%20feature%20denoising.%20Specifically%2C%20a%20high-order%20neighborhood%0Aproximity%20evaluation%20method%20is%20proposed%20to%20recognize%20noisy%20edges%2C%20considering%0Afeatures%20may%20be%20perturbed%20simultaneously.%20Moreover%2C%20we%20propose%20to%20refine%20noisy%0Afeatures%20with%20reconstruction%20based%20on%20a%20graph%20auto-encoder.%20An%20iterative%0Aupdating%20algorithm%20is%20further%20designed%20to%20optimize%20the%20framework%20and%20acquire%20a%0Aclean%20graph%2C%20thus%20enabling%20robust%20graph%20learning%20for%20downstream%20tasks.%20Our%20UGD%0Aframework%20is%20self-supervised%20and%20can%20be%20easily%20implemented%20as%20a%20plug-and-play%0Amodule.%20We%20carry%20out%20extensive%20experiments%2C%20which%20proves%20the%20effectiveness%20and%0Aadvantages%20of%20our%20method.%20Code%20is%20avalaible%20at%0Ahttps%3A//github.com/YoungTimmy/UGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520Can%2527t%2520Ignore%2520Either%253A%2520Unifying%2520Structure%2520and%2520Feature%2520Denoising%2520for%250A%2520%2520Robust%2520Graph%2520Learning%26entry.906535625%3DTianmeng%2520Yang%2520and%2520Jiahao%2520Meng%2520and%2520Min%2520Zhou%2520and%2520Yaming%2520Yang%2520and%2520Yujing%2520Wang%2520and%2520Xiangtai%2520Li%2520and%2520Yunhai%2520Tong%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520the%2520robustness%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520under%250Anoises%2520or%2520attacks%2520has%2520attracted%2520great%2520attention%2520due%2520to%2520its%2520importance%2520in%250Areal-world%2520applications.%2520Most%2520previous%2520methods%2520explore%2520a%2520single%2520noise%2520source%252C%250Arecovering%2520corrupt%2520node%2520embedding%2520by%2520reliable%2520structures%2520bias%2520or%2520developing%250Astructure%2520learning%2520with%2520reliable%2520node%2520features.%2520However%252C%2520the%2520noises%2520and%2520attacks%250Amay%2520come%2520from%2520both%2520structures%2520and%2520features%2520in%2520graphs%252C%2520making%2520the%2520graph%250Adenoising%2520a%2520dilemma%2520and%2520challenging%2520problem.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%250Aunified%2520graph%2520denoising%2520%2528UGD%2529%2520framework%2520to%2520unravel%2520the%2520deadlock%2520between%250Astructure%2520and%2520feature%2520denoising.%2520Specifically%252C%2520a%2520high-order%2520neighborhood%250Aproximity%2520evaluation%2520method%2520is%2520proposed%2520to%2520recognize%2520noisy%2520edges%252C%2520considering%250Afeatures%2520may%2520be%2520perturbed%2520simultaneously.%2520Moreover%252C%2520we%2520propose%2520to%2520refine%2520noisy%250Afeatures%2520with%2520reconstruction%2520based%2520on%2520a%2520graph%2520auto-encoder.%2520An%2520iterative%250Aupdating%2520algorithm%2520is%2520further%2520designed%2520to%2520optimize%2520the%2520framework%2520and%2520acquire%2520a%250Aclean%2520graph%252C%2520thus%2520enabling%2520robust%2520graph%2520learning%2520for%2520downstream%2520tasks.%2520Our%2520UGD%250Aframework%2520is%2520self-supervised%2520and%2520can%2520be%2520easily%2520implemented%2520as%2520a%2520plug-and-play%250Amodule.%2520We%2520carry%2520out%2520extensive%2520experiments%252C%2520which%2520proves%2520the%2520effectiveness%2520and%250Aadvantages%2520of%2520our%2520method.%2520Code%2520is%2520avalaible%2520at%250Ahttps%253A//github.com/YoungTimmy/UGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20Can%27t%20Ignore%20Either%3A%20Unifying%20Structure%20and%20Feature%20Denoising%20for%0A%20%20Robust%20Graph%20Learning&entry.906535625=Tianmeng%20Yang%20and%20Jiahao%20Meng%20and%20Min%20Zhou%20and%20Yaming%20Yang%20and%20Yujing%20Wang%20and%20Xiangtai%20Li%20and%20Yunhai%20Tong&entry.1292438233=%20%20Recent%20research%20on%20the%20robustness%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20under%0Anoises%20or%20attacks%20has%20attracted%20great%20attention%20due%20to%20its%20importance%20in%0Areal-world%20applications.%20Most%20previous%20methods%20explore%20a%20single%20noise%20source%2C%0Arecovering%20corrupt%20node%20embedding%20by%20reliable%20structures%20bias%20or%20developing%0Astructure%20learning%20with%20reliable%20node%20features.%20However%2C%20the%20noises%20and%20attacks%0Amay%20come%20from%20both%20structures%20and%20features%20in%20graphs%2C%20making%20the%20graph%0Adenoising%20a%20dilemma%20and%20challenging%20problem.%20In%20this%20paper%2C%20we%20develop%20a%0Aunified%20graph%20denoising%20%28UGD%29%20framework%20to%20unravel%20the%20deadlock%20between%0Astructure%20and%20feature%20denoising.%20Specifically%2C%20a%20high-order%20neighborhood%0Aproximity%20evaluation%20method%20is%20proposed%20to%20recognize%20noisy%20edges%2C%20considering%0Afeatures%20may%20be%20perturbed%20simultaneously.%20Moreover%2C%20we%20propose%20to%20refine%20noisy%0Afeatures%20with%20reconstruction%20based%20on%20a%20graph%20auto-encoder.%20An%20iterative%0Aupdating%20algorithm%20is%20further%20designed%20to%20optimize%20the%20framework%20and%20acquire%20a%0Aclean%20graph%2C%20thus%20enabling%20robust%20graph%20learning%20for%20downstream%20tasks.%20Our%20UGD%0Aframework%20is%20self-supervised%20and%20can%20be%20easily%20implemented%20as%20a%20plug-and-play%0Amodule.%20We%20carry%20out%20extensive%20experiments%2C%20which%20proves%20the%20effectiveness%20and%0Aadvantages%20of%20our%20method.%20Code%20is%20avalaible%20at%0Ahttps%3A//github.com/YoungTimmy/UGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00700v1&entry.124074799=Read"},
{"title": "GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks", "author": "Ihor Stepanov and Mykhailo Shtopko", "abstract": "  Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.\n", "link": "http://arxiv.org/abs/2406.12925v2", "date": "2024-08-01", "relevancy": 2.5563, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5471}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4953}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLiNER%20multi-task%3A%20Generalist%20Lightweight%20Model%20for%20Various%20Information%0A%20%20Extraction%20Tasks&body=Title%3A%20GLiNER%20multi-task%3A%20Generalist%20Lightweight%20Model%20for%20Various%20Information%0A%20%20Extraction%20Tasks%0AAuthor%3A%20Ihor%20Stepanov%20and%20Mykhailo%20Shtopko%0AAbstract%3A%20%20%20Information%20extraction%20tasks%20require%20both%20accurate%2C%20efficient%2C%20and%0Ageneralisable%20models.%20Classical%20supervised%20deep%20learning%20approaches%20can%20achieve%0Athe%20required%20performance%2C%20but%20they%20need%20large%20datasets%20and%20are%20limited%20in%20their%0Aability%20to%20adapt%20to%20different%20tasks.%20On%20the%20other%20hand%2C%20large%20language%20models%0A%28LLMs%29%20demonstrate%20good%20generalization%2C%20meaning%20that%20they%20can%20adapt%20to%20many%0Adifferent%20tasks%20based%20on%20user%20requests.%20However%2C%20LLMs%20are%20computationally%0Aexpensive%20and%20tend%20to%20fail%20to%20generate%20structured%20outputs.%20In%20this%20article%2C%20we%0Awill%20introduce%20a%20new%20kind%20of%20GLiNER%20model%20that%20can%20be%20used%20for%20various%0Ainformation%20extraction%20tasks%20while%20being%20a%20small%20encoder%20model.%20Our%20model%0Aachieved%20SoTA%20performance%20on%20zero-shot%20NER%20benchmarks%20and%20leading%20performance%0Aon%20question-answering%2C%20summarization%20and%20relation%20extraction%20tasks.%0AAdditionally%2C%20in%20this%20article%2C%20we%20will%20cover%20experimental%20results%20on%0Aself-learning%20approaches%20for%20named%20entity%20recognition%20using%20GLiNER%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLiNER%2520multi-task%253A%2520Generalist%2520Lightweight%2520Model%2520for%2520Various%2520Information%250A%2520%2520Extraction%2520Tasks%26entry.906535625%3DIhor%2520Stepanov%2520and%2520Mykhailo%2520Shtopko%26entry.1292438233%3D%2520%2520Information%2520extraction%2520tasks%2520require%2520both%2520accurate%252C%2520efficient%252C%2520and%250Ageneralisable%2520models.%2520Classical%2520supervised%2520deep%2520learning%2520approaches%2520can%2520achieve%250Athe%2520required%2520performance%252C%2520but%2520they%2520need%2520large%2520datasets%2520and%2520are%2520limited%2520in%2520their%250Aability%2520to%2520adapt%2520to%2520different%2520tasks.%2520On%2520the%2520other%2520hand%252C%2520large%2520language%2520models%250A%2528LLMs%2529%2520demonstrate%2520good%2520generalization%252C%2520meaning%2520that%2520they%2520can%2520adapt%2520to%2520many%250Adifferent%2520tasks%2520based%2520on%2520user%2520requests.%2520However%252C%2520LLMs%2520are%2520computationally%250Aexpensive%2520and%2520tend%2520to%2520fail%2520to%2520generate%2520structured%2520outputs.%2520In%2520this%2520article%252C%2520we%250Awill%2520introduce%2520a%2520new%2520kind%2520of%2520GLiNER%2520model%2520that%2520can%2520be%2520used%2520for%2520various%250Ainformation%2520extraction%2520tasks%2520while%2520being%2520a%2520small%2520encoder%2520model.%2520Our%2520model%250Aachieved%2520SoTA%2520performance%2520on%2520zero-shot%2520NER%2520benchmarks%2520and%2520leading%2520performance%250Aon%2520question-answering%252C%2520summarization%2520and%2520relation%2520extraction%2520tasks.%250AAdditionally%252C%2520in%2520this%2520article%252C%2520we%2520will%2520cover%2520experimental%2520results%2520on%250Aself-learning%2520approaches%2520for%2520named%2520entity%2520recognition%2520using%2520GLiNER%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLiNER%20multi-task%3A%20Generalist%20Lightweight%20Model%20for%20Various%20Information%0A%20%20Extraction%20Tasks&entry.906535625=Ihor%20Stepanov%20and%20Mykhailo%20Shtopko&entry.1292438233=%20%20Information%20extraction%20tasks%20require%20both%20accurate%2C%20efficient%2C%20and%0Ageneralisable%20models.%20Classical%20supervised%20deep%20learning%20approaches%20can%20achieve%0Athe%20required%20performance%2C%20but%20they%20need%20large%20datasets%20and%20are%20limited%20in%20their%0Aability%20to%20adapt%20to%20different%20tasks.%20On%20the%20other%20hand%2C%20large%20language%20models%0A%28LLMs%29%20demonstrate%20good%20generalization%2C%20meaning%20that%20they%20can%20adapt%20to%20many%0Adifferent%20tasks%20based%20on%20user%20requests.%20However%2C%20LLMs%20are%20computationally%0Aexpensive%20and%20tend%20to%20fail%20to%20generate%20structured%20outputs.%20In%20this%20article%2C%20we%0Awill%20introduce%20a%20new%20kind%20of%20GLiNER%20model%20that%20can%20be%20used%20for%20various%0Ainformation%20extraction%20tasks%20while%20being%20a%20small%20encoder%20model.%20Our%20model%0Aachieved%20SoTA%20performance%20on%20zero-shot%20NER%20benchmarks%20and%20leading%20performance%0Aon%20question-answering%2C%20summarization%20and%20relation%20extraction%20tasks.%0AAdditionally%2C%20in%20this%20article%2C%20we%20will%20cover%20experimental%20results%20on%0Aself-learning%20approaches%20for%20named%20entity%20recognition%20using%20GLiNER%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12925v2&entry.124074799=Read"},
{"title": "Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image\n  Segmentation with U-Net", "author": "Fangyijie Wang and Gu\u00e9nol\u00e9 Silvestre and Kathleen M. Curran", "abstract": "  Fetal head segmentation is a crucial step in measuring the fetal head\ncircumference (HC) during gestation, an important biometric in obstetrics for\nmonitoring fetal growth. However, manual biometry generation is time-consuming\nand results in inconsistent accuracy. To address this issue, convolutional\nneural network (CNN) models have been utilized to improve the efficiency of\nmedical biometry. But training a CNN network from scratch is a challenging\ntask, we proposed a Transfer Learning (TL) method. Our approach involves\nfine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to\nperform segmentation on a set of fetal head ultrasound (US) images with limited\neffort. This method addresses the challenges associated with training a CNN\nnetwork from scratch. It suggests that our proposed FT strategy yields\nsegmentation performance that is comparable when trained with a reduced number\nof parameters by 85.8%. And our proposed FT strategy outperforms other\nstrategies with smaller trainable parameter sizes below 4.4 million. Thus, we\ncontend that it can serve as a dependable FT approach for reducing the size of\nmodels in medical image analysis. Our key findings highlight the importance of\nthe balance between model performance and size in developing Artificial\nIntelligence (AI) applications by TL methods. Code is available at\nhttps://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.\n", "link": "http://arxiv.org/abs/2307.09067v2", "date": "2024-08-01", "relevancy": 2.5413, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5226}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5024}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluate%20Fine-tuning%20Strategies%20for%20Fetal%20Head%20Ultrasound%20Image%0A%20%20Segmentation%20with%20U-Net&body=Title%3A%20Evaluate%20Fine-tuning%20Strategies%20for%20Fetal%20Head%20Ultrasound%20Image%0A%20%20Segmentation%20with%20U-Net%0AAuthor%3A%20Fangyijie%20Wang%20and%20Gu%C3%A9nol%C3%A9%20Silvestre%20and%20Kathleen%20M.%20Curran%0AAbstract%3A%20%20%20Fetal%20head%20segmentation%20is%20a%20crucial%20step%20in%20measuring%20the%20fetal%20head%0Acircumference%20%28HC%29%20during%20gestation%2C%20an%20important%20biometric%20in%20obstetrics%20for%0Amonitoring%20fetal%20growth.%20However%2C%20manual%20biometry%20generation%20is%20time-consuming%0Aand%20results%20in%20inconsistent%20accuracy.%20To%20address%20this%20issue%2C%20convolutional%0Aneural%20network%20%28CNN%29%20models%20have%20been%20utilized%20to%20improve%20the%20efficiency%20of%0Amedical%20biometry.%20But%20training%20a%20CNN%20network%20from%20scratch%20is%20a%20challenging%0Atask%2C%20we%20proposed%20a%20Transfer%20Learning%20%28TL%29%20method.%20Our%20approach%20involves%0Afine-tuning%20%28FT%29%20a%20U-Net%20network%20with%20a%20lightweight%20MobileNet%20as%20the%20encoder%20to%0Aperform%20segmentation%20on%20a%20set%20of%20fetal%20head%20ultrasound%20%28US%29%20images%20with%20limited%0Aeffort.%20This%20method%20addresses%20the%20challenges%20associated%20with%20training%20a%20CNN%0Anetwork%20from%20scratch.%20It%20suggests%20that%20our%20proposed%20FT%20strategy%20yields%0Asegmentation%20performance%20that%20is%20comparable%20when%20trained%20with%20a%20reduced%20number%0Aof%20parameters%20by%2085.8%25.%20And%20our%20proposed%20FT%20strategy%20outperforms%20other%0Astrategies%20with%20smaller%20trainable%20parameter%20sizes%20below%204.4%20million.%20Thus%2C%20we%0Acontend%20that%20it%20can%20serve%20as%20a%20dependable%20FT%20approach%20for%20reducing%20the%20size%20of%0Amodels%20in%20medical%20image%20analysis.%20Our%20key%20findings%20highlight%20the%20importance%20of%0Athe%20balance%20between%20model%20performance%20and%20size%20in%20developing%20Artificial%0AIntelligence%20%28AI%29%20applications%20by%20TL%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluate%2520Fine-tuning%2520Strategies%2520for%2520Fetal%2520Head%2520Ultrasound%2520Image%250A%2520%2520Segmentation%2520with%2520U-Net%26entry.906535625%3DFangyijie%2520Wang%2520and%2520Gu%25C3%25A9nol%25C3%25A9%2520Silvestre%2520and%2520Kathleen%2520M.%2520Curran%26entry.1292438233%3D%2520%2520Fetal%2520head%2520segmentation%2520is%2520a%2520crucial%2520step%2520in%2520measuring%2520the%2520fetal%2520head%250Acircumference%2520%2528HC%2529%2520during%2520gestation%252C%2520an%2520important%2520biometric%2520in%2520obstetrics%2520for%250Amonitoring%2520fetal%2520growth.%2520However%252C%2520manual%2520biometry%2520generation%2520is%2520time-consuming%250Aand%2520results%2520in%2520inconsistent%2520accuracy.%2520To%2520address%2520this%2520issue%252C%2520convolutional%250Aneural%2520network%2520%2528CNN%2529%2520models%2520have%2520been%2520utilized%2520to%2520improve%2520the%2520efficiency%2520of%250Amedical%2520biometry.%2520But%2520training%2520a%2520CNN%2520network%2520from%2520scratch%2520is%2520a%2520challenging%250Atask%252C%2520we%2520proposed%2520a%2520Transfer%2520Learning%2520%2528TL%2529%2520method.%2520Our%2520approach%2520involves%250Afine-tuning%2520%2528FT%2529%2520a%2520U-Net%2520network%2520with%2520a%2520lightweight%2520MobileNet%2520as%2520the%2520encoder%2520to%250Aperform%2520segmentation%2520on%2520a%2520set%2520of%2520fetal%2520head%2520ultrasound%2520%2528US%2529%2520images%2520with%2520limited%250Aeffort.%2520This%2520method%2520addresses%2520the%2520challenges%2520associated%2520with%2520training%2520a%2520CNN%250Anetwork%2520from%2520scratch.%2520It%2520suggests%2520that%2520our%2520proposed%2520FT%2520strategy%2520yields%250Asegmentation%2520performance%2520that%2520is%2520comparable%2520when%2520trained%2520with%2520a%2520reduced%2520number%250Aof%2520parameters%2520by%252085.8%2525.%2520And%2520our%2520proposed%2520FT%2520strategy%2520outperforms%2520other%250Astrategies%2520with%2520smaller%2520trainable%2520parameter%2520sizes%2520below%25204.4%2520million.%2520Thus%252C%2520we%250Acontend%2520that%2520it%2520can%2520serve%2520as%2520a%2520dependable%2520FT%2520approach%2520for%2520reducing%2520the%2520size%2520of%250Amodels%2520in%2520medical%2520image%2520analysis.%2520Our%2520key%2520findings%2520highlight%2520the%2520importance%2520of%250Athe%2520balance%2520between%2520model%2520performance%2520and%2520size%2520in%2520developing%2520Artificial%250AIntelligence%2520%2528AI%2529%2520applications%2520by%2520TL%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluate%20Fine-tuning%20Strategies%20for%20Fetal%20Head%20Ultrasound%20Image%0A%20%20Segmentation%20with%20U-Net&entry.906535625=Fangyijie%20Wang%20and%20Gu%C3%A9nol%C3%A9%20Silvestre%20and%20Kathleen%20M.%20Curran&entry.1292438233=%20%20Fetal%20head%20segmentation%20is%20a%20crucial%20step%20in%20measuring%20the%20fetal%20head%0Acircumference%20%28HC%29%20during%20gestation%2C%20an%20important%20biometric%20in%20obstetrics%20for%0Amonitoring%20fetal%20growth.%20However%2C%20manual%20biometry%20generation%20is%20time-consuming%0Aand%20results%20in%20inconsistent%20accuracy.%20To%20address%20this%20issue%2C%20convolutional%0Aneural%20network%20%28CNN%29%20models%20have%20been%20utilized%20to%20improve%20the%20efficiency%20of%0Amedical%20biometry.%20But%20training%20a%20CNN%20network%20from%20scratch%20is%20a%20challenging%0Atask%2C%20we%20proposed%20a%20Transfer%20Learning%20%28TL%29%20method.%20Our%20approach%20involves%0Afine-tuning%20%28FT%29%20a%20U-Net%20network%20with%20a%20lightweight%20MobileNet%20as%20the%20encoder%20to%0Aperform%20segmentation%20on%20a%20set%20of%20fetal%20head%20ultrasound%20%28US%29%20images%20with%20limited%0Aeffort.%20This%20method%20addresses%20the%20challenges%20associated%20with%20training%20a%20CNN%0Anetwork%20from%20scratch.%20It%20suggests%20that%20our%20proposed%20FT%20strategy%20yields%0Asegmentation%20performance%20that%20is%20comparable%20when%20trained%20with%20a%20reduced%20number%0Aof%20parameters%20by%2085.8%25.%20And%20our%20proposed%20FT%20strategy%20outperforms%20other%0Astrategies%20with%20smaller%20trainable%20parameter%20sizes%20below%204.4%20million.%20Thus%2C%20we%0Acontend%20that%20it%20can%20serve%20as%20a%20dependable%20FT%20approach%20for%20reducing%20the%20size%20of%0Amodels%20in%20medical%20image%20analysis.%20Our%20key%20findings%20highlight%20the%20importance%20of%0Athe%20balance%20between%20model%20performance%20and%20size%20in%20developing%20Artificial%0AIntelligence%20%28AI%29%20applications%20by%20TL%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09067v2&entry.124074799=Read"},
{"title": "Label merge-and-split: A graph-colouring approach for memory-efficient\n  brain parcellation", "author": "Aaron Kujawa and Reuben Dorent and Sebastien Ourselin and Tom Vercauteren", "abstract": "  Whole brain parcellation requires inferring hundreds of segmentation labels\nin large image volumes and thus presents significant practical challenges for\ndeep learning approaches. We introduce label merge-and-split, a method that\nfirst greatly reduces the effective number of labels required for\nlearning-based whole brain parcellation and then recovers original labels.\nUsing a greedy graph colouring algorithm, our method automatically groups and\nmerges multiple spatially separate labels prior to model training and\ninference. The merged labels may be semantically unrelated. A deep learning\nmodel is trained to predict merged labels. At inference time, original labels\nare restored using atlas-based influence regions. In our experiments, the\nproposed approach reduces the number of labels by up to 68% while achieving\nsegmentation accuracy comparable to the baseline method without label merging\nand splitting. Moreover, model training and inference times as well as GPU\nmemory requirements were reduced significantly. The proposed method can be\napplied to all semantic segmentation tasks with a large number of spatially\nseparate classes within an atlas-based prior.\n", "link": "http://arxiv.org/abs/2404.10572v2", "date": "2024-08-01", "relevancy": 2.5021, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5132}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label%20merge-and-split%3A%20A%20graph-colouring%20approach%20for%20memory-efficient%0A%20%20brain%20parcellation&body=Title%3A%20Label%20merge-and-split%3A%20A%20graph-colouring%20approach%20for%20memory-efficient%0A%20%20brain%20parcellation%0AAuthor%3A%20Aaron%20Kujawa%20and%20Reuben%20Dorent%20and%20Sebastien%20Ourselin%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Whole%20brain%20parcellation%20requires%20inferring%20hundreds%20of%20segmentation%20labels%0Ain%20large%20image%20volumes%20and%20thus%20presents%20significant%20practical%20challenges%20for%0Adeep%20learning%20approaches.%20We%20introduce%20label%20merge-and-split%2C%20a%20method%20that%0Afirst%20greatly%20reduces%20the%20effective%20number%20of%20labels%20required%20for%0Alearning-based%20whole%20brain%20parcellation%20and%20then%20recovers%20original%20labels.%0AUsing%20a%20greedy%20graph%20colouring%20algorithm%2C%20our%20method%20automatically%20groups%20and%0Amerges%20multiple%20spatially%20separate%20labels%20prior%20to%20model%20training%20and%0Ainference.%20The%20merged%20labels%20may%20be%20semantically%20unrelated.%20A%20deep%20learning%0Amodel%20is%20trained%20to%20predict%20merged%20labels.%20At%20inference%20time%2C%20original%20labels%0Aare%20restored%20using%20atlas-based%20influence%20regions.%20In%20our%20experiments%2C%20the%0Aproposed%20approach%20reduces%20the%20number%20of%20labels%20by%20up%20to%2068%25%20while%20achieving%0Asegmentation%20accuracy%20comparable%20to%20the%20baseline%20method%20without%20label%20merging%0Aand%20splitting.%20Moreover%2C%20model%20training%20and%20inference%20times%20as%20well%20as%20GPU%0Amemory%20requirements%20were%20reduced%20significantly.%20The%20proposed%20method%20can%20be%0Aapplied%20to%20all%20semantic%20segmentation%20tasks%20with%20a%20large%20number%20of%20spatially%0Aseparate%20classes%20within%20an%20atlas-based%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10572v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel%2520merge-and-split%253A%2520A%2520graph-colouring%2520approach%2520for%2520memory-efficient%250A%2520%2520brain%2520parcellation%26entry.906535625%3DAaron%2520Kujawa%2520and%2520Reuben%2520Dorent%2520and%2520Sebastien%2520Ourselin%2520and%2520Tom%2520Vercauteren%26entry.1292438233%3D%2520%2520Whole%2520brain%2520parcellation%2520requires%2520inferring%2520hundreds%2520of%2520segmentation%2520labels%250Ain%2520large%2520image%2520volumes%2520and%2520thus%2520presents%2520significant%2520practical%2520challenges%2520for%250Adeep%2520learning%2520approaches.%2520We%2520introduce%2520label%2520merge-and-split%252C%2520a%2520method%2520that%250Afirst%2520greatly%2520reduces%2520the%2520effective%2520number%2520of%2520labels%2520required%2520for%250Alearning-based%2520whole%2520brain%2520parcellation%2520and%2520then%2520recovers%2520original%2520labels.%250AUsing%2520a%2520greedy%2520graph%2520colouring%2520algorithm%252C%2520our%2520method%2520automatically%2520groups%2520and%250Amerges%2520multiple%2520spatially%2520separate%2520labels%2520prior%2520to%2520model%2520training%2520and%250Ainference.%2520The%2520merged%2520labels%2520may%2520be%2520semantically%2520unrelated.%2520A%2520deep%2520learning%250Amodel%2520is%2520trained%2520to%2520predict%2520merged%2520labels.%2520At%2520inference%2520time%252C%2520original%2520labels%250Aare%2520restored%2520using%2520atlas-based%2520influence%2520regions.%2520In%2520our%2520experiments%252C%2520the%250Aproposed%2520approach%2520reduces%2520the%2520number%2520of%2520labels%2520by%2520up%2520to%252068%2525%2520while%2520achieving%250Asegmentation%2520accuracy%2520comparable%2520to%2520the%2520baseline%2520method%2520without%2520label%2520merging%250Aand%2520splitting.%2520Moreover%252C%2520model%2520training%2520and%2520inference%2520times%2520as%2520well%2520as%2520GPU%250Amemory%2520requirements%2520were%2520reduced%2520significantly.%2520The%2520proposed%2520method%2520can%2520be%250Aapplied%2520to%2520all%2520semantic%2520segmentation%2520tasks%2520with%2520a%2520large%2520number%2520of%2520spatially%250Aseparate%2520classes%2520within%2520an%2520atlas-based%2520prior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10572v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label%20merge-and-split%3A%20A%20graph-colouring%20approach%20for%20memory-efficient%0A%20%20brain%20parcellation&entry.906535625=Aaron%20Kujawa%20and%20Reuben%20Dorent%20and%20Sebastien%20Ourselin%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Whole%20brain%20parcellation%20requires%20inferring%20hundreds%20of%20segmentation%20labels%0Ain%20large%20image%20volumes%20and%20thus%20presents%20significant%20practical%20challenges%20for%0Adeep%20learning%20approaches.%20We%20introduce%20label%20merge-and-split%2C%20a%20method%20that%0Afirst%20greatly%20reduces%20the%20effective%20number%20of%20labels%20required%20for%0Alearning-based%20whole%20brain%20parcellation%20and%20then%20recovers%20original%20labels.%0AUsing%20a%20greedy%20graph%20colouring%20algorithm%2C%20our%20method%20automatically%20groups%20and%0Amerges%20multiple%20spatially%20separate%20labels%20prior%20to%20model%20training%20and%0Ainference.%20The%20merged%20labels%20may%20be%20semantically%20unrelated.%20A%20deep%20learning%0Amodel%20is%20trained%20to%20predict%20merged%20labels.%20At%20inference%20time%2C%20original%20labels%0Aare%20restored%20using%20atlas-based%20influence%20regions.%20In%20our%20experiments%2C%20the%0Aproposed%20approach%20reduces%20the%20number%20of%20labels%20by%20up%20to%2068%25%20while%20achieving%0Asegmentation%20accuracy%20comparable%20to%20the%20baseline%20method%20without%20label%20merging%0Aand%20splitting.%20Moreover%2C%20model%20training%20and%20inference%20times%20as%20well%20as%20GPU%0Amemory%20requirements%20were%20reduced%20significantly.%20The%20proposed%20method%20can%20be%0Aapplied%20to%20all%20semantic%20segmentation%20tasks%20with%20a%20large%20number%20of%20spatially%0Aseparate%20classes%20within%20an%20atlas-based%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10572v2&entry.124074799=Read"},
{"title": "Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal\n  Language Model", "author": "Benlin Liu and Yuhao Dong and Yiqin Wang and Yongming Rao and Yansong Tang and Wei-Chiu Ma and Ranjay Krishna", "abstract": "  Multimodal language models (MLLMs) are increasingly being implemented in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Despite their potential, current top models\nwithin our community still fall short in adequately understanding spatial and\ntemporal dimensions. We introduce Coarse Correspondence, a simple,\ntraining-free, effective, and general-purpose visual prompting method to elicit\n3D and temporal understanding in multimodal LLMs. Our method uses a lightweight\ntracking model to find object correspondences between frames in a video or\nbetween sets of image viewpoints. It selects the most frequent object instances\nand visualizes them with markers with unique IDs in the image. With this simple\napproach, we achieve state-of-the-art results on 3D understanding benchmarks\nincluding ScanQA (+20.5\\%) and a subset of OpenEQA (+9.7\\%), and on long-form\nvideo benchmarks such as EgoSchema (+6.0\\%). We also curate a small diagnostic\ndataset to evaluate whether MLLMs can reason about space from a described\nviewpoint other than the camera viewpoint. Again, Coarse Correspondence\nimproves spatial perspective-taking abilities but we highlight that MLLMs\nstruggle with this task. Together, we demonstrate that our simple prompting\nmethod can significantly aid downstream tasks that require 3D or temporal\nreasoning.\n", "link": "http://arxiv.org/abs/2408.00754v1", "date": "2024-08-01", "relevancy": 2.4934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6636}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6022}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse%20Correspondence%20Elicit%203D%20Spacetime%20Understanding%20in%20Multimodal%0A%20%20Language%20Model&body=Title%3A%20Coarse%20Correspondence%20Elicit%203D%20Spacetime%20Understanding%20in%20Multimodal%0A%20%20Language%20Model%0AAuthor%3A%20Benlin%20Liu%20and%20Yuhao%20Dong%20and%20Yiqin%20Wang%20and%20Yongming%20Rao%20and%20Yansong%20Tang%20and%20Wei-Chiu%20Ma%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Multimodal%20language%20models%20%28MLLMs%29%20are%20increasingly%20being%20implemented%20in%0Areal-world%20environments%2C%20necessitating%20their%20ability%20to%20interpret%203D%20spaces%20and%0Acomprehend%20temporal%20dynamics.%20Despite%20their%20potential%2C%20current%20top%20models%0Awithin%20our%20community%20still%20fall%20short%20in%20adequately%20understanding%20spatial%20and%0Atemporal%20dimensions.%20We%20introduce%20Coarse%20Correspondence%2C%20a%20simple%2C%0Atraining-free%2C%20effective%2C%20and%20general-purpose%20visual%20prompting%20method%20to%20elicit%0A3D%20and%20temporal%20understanding%20in%20multimodal%20LLMs.%20Our%20method%20uses%20a%20lightweight%0Atracking%20model%20to%20find%20object%20correspondences%20between%20frames%20in%20a%20video%20or%0Abetween%20sets%20of%20image%20viewpoints.%20It%20selects%20the%20most%20frequent%20object%20instances%0Aand%20visualizes%20them%20with%20markers%20with%20unique%20IDs%20in%20the%20image.%20With%20this%20simple%0Aapproach%2C%20we%20achieve%20state-of-the-art%20results%20on%203D%20understanding%20benchmarks%0Aincluding%20ScanQA%20%28%2B20.5%5C%25%29%20and%20a%20subset%20of%20OpenEQA%20%28%2B9.7%5C%25%29%2C%20and%20on%20long-form%0Avideo%20benchmarks%20such%20as%20EgoSchema%20%28%2B6.0%5C%25%29.%20We%20also%20curate%20a%20small%20diagnostic%0Adataset%20to%20evaluate%20whether%20MLLMs%20can%20reason%20about%20space%20from%20a%20described%0Aviewpoint%20other%20than%20the%20camera%20viewpoint.%20Again%2C%20Coarse%20Correspondence%0Aimproves%20spatial%20perspective-taking%20abilities%20but%20we%20highlight%20that%20MLLMs%0Astruggle%20with%20this%20task.%20Together%2C%20we%20demonstrate%20that%20our%20simple%20prompting%0Amethod%20can%20significantly%20aid%20downstream%20tasks%20that%20require%203D%20or%20temporal%0Areasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse%2520Correspondence%2520Elicit%25203D%2520Spacetime%2520Understanding%2520in%2520Multimodal%250A%2520%2520Language%2520Model%26entry.906535625%3DBenlin%2520Liu%2520and%2520Yuhao%2520Dong%2520and%2520Yiqin%2520Wang%2520and%2520Yongming%2520Rao%2520and%2520Yansong%2520Tang%2520and%2520Wei-Chiu%2520Ma%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Multimodal%2520language%2520models%2520%2528MLLMs%2529%2520are%2520increasingly%2520being%2520implemented%2520in%250Areal-world%2520environments%252C%2520necessitating%2520their%2520ability%2520to%2520interpret%25203D%2520spaces%2520and%250Acomprehend%2520temporal%2520dynamics.%2520Despite%2520their%2520potential%252C%2520current%2520top%2520models%250Awithin%2520our%2520community%2520still%2520fall%2520short%2520in%2520adequately%2520understanding%2520spatial%2520and%250Atemporal%2520dimensions.%2520We%2520introduce%2520Coarse%2520Correspondence%252C%2520a%2520simple%252C%250Atraining-free%252C%2520effective%252C%2520and%2520general-purpose%2520visual%2520prompting%2520method%2520to%2520elicit%250A3D%2520and%2520temporal%2520understanding%2520in%2520multimodal%2520LLMs.%2520Our%2520method%2520uses%2520a%2520lightweight%250Atracking%2520model%2520to%2520find%2520object%2520correspondences%2520between%2520frames%2520in%2520a%2520video%2520or%250Abetween%2520sets%2520of%2520image%2520viewpoints.%2520It%2520selects%2520the%2520most%2520frequent%2520object%2520instances%250Aand%2520visualizes%2520them%2520with%2520markers%2520with%2520unique%2520IDs%2520in%2520the%2520image.%2520With%2520this%2520simple%250Aapproach%252C%2520we%2520achieve%2520state-of-the-art%2520results%2520on%25203D%2520understanding%2520benchmarks%250Aincluding%2520ScanQA%2520%2528%252B20.5%255C%2525%2529%2520and%2520a%2520subset%2520of%2520OpenEQA%2520%2528%252B9.7%255C%2525%2529%252C%2520and%2520on%2520long-form%250Avideo%2520benchmarks%2520such%2520as%2520EgoSchema%2520%2528%252B6.0%255C%2525%2529.%2520We%2520also%2520curate%2520a%2520small%2520diagnostic%250Adataset%2520to%2520evaluate%2520whether%2520MLLMs%2520can%2520reason%2520about%2520space%2520from%2520a%2520described%250Aviewpoint%2520other%2520than%2520the%2520camera%2520viewpoint.%2520Again%252C%2520Coarse%2520Correspondence%250Aimproves%2520spatial%2520perspective-taking%2520abilities%2520but%2520we%2520highlight%2520that%2520MLLMs%250Astruggle%2520with%2520this%2520task.%2520Together%252C%2520we%2520demonstrate%2520that%2520our%2520simple%2520prompting%250Amethod%2520can%2520significantly%2520aid%2520downstream%2520tasks%2520that%2520require%25203D%2520or%2520temporal%250Areasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse%20Correspondence%20Elicit%203D%20Spacetime%20Understanding%20in%20Multimodal%0A%20%20Language%20Model&entry.906535625=Benlin%20Liu%20and%20Yuhao%20Dong%20and%20Yiqin%20Wang%20and%20Yongming%20Rao%20and%20Yansong%20Tang%20and%20Wei-Chiu%20Ma%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Multimodal%20language%20models%20%28MLLMs%29%20are%20increasingly%20being%20implemented%20in%0Areal-world%20environments%2C%20necessitating%20their%20ability%20to%20interpret%203D%20spaces%20and%0Acomprehend%20temporal%20dynamics.%20Despite%20their%20potential%2C%20current%20top%20models%0Awithin%20our%20community%20still%20fall%20short%20in%20adequately%20understanding%20spatial%20and%0Atemporal%20dimensions.%20We%20introduce%20Coarse%20Correspondence%2C%20a%20simple%2C%0Atraining-free%2C%20effective%2C%20and%20general-purpose%20visual%20prompting%20method%20to%20elicit%0A3D%20and%20temporal%20understanding%20in%20multimodal%20LLMs.%20Our%20method%20uses%20a%20lightweight%0Atracking%20model%20to%20find%20object%20correspondences%20between%20frames%20in%20a%20video%20or%0Abetween%20sets%20of%20image%20viewpoints.%20It%20selects%20the%20most%20frequent%20object%20instances%0Aand%20visualizes%20them%20with%20markers%20with%20unique%20IDs%20in%20the%20image.%20With%20this%20simple%0Aapproach%2C%20we%20achieve%20state-of-the-art%20results%20on%203D%20understanding%20benchmarks%0Aincluding%20ScanQA%20%28%2B20.5%5C%25%29%20and%20a%20subset%20of%20OpenEQA%20%28%2B9.7%5C%25%29%2C%20and%20on%20long-form%0Avideo%20benchmarks%20such%20as%20EgoSchema%20%28%2B6.0%5C%25%29.%20We%20also%20curate%20a%20small%20diagnostic%0Adataset%20to%20evaluate%20whether%20MLLMs%20can%20reason%20about%20space%20from%20a%20described%0Aviewpoint%20other%20than%20the%20camera%20viewpoint.%20Again%2C%20Coarse%20Correspondence%0Aimproves%20spatial%20perspective-taking%20abilities%20but%20we%20highlight%20that%20MLLMs%0Astruggle%20with%20this%20task.%20Together%2C%20we%20demonstrate%20that%20our%20simple%20prompting%0Amethod%20can%20significantly%20aid%20downstream%20tasks%20that%20require%203D%20or%20temporal%0Areasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00754v1&entry.124074799=Read"},
{"title": "Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual\n  Inversion", "author": "Manuel Kansy and Jacek Naruniec and Christopher Schroers and Markus Gross and Romann M. Weber", "abstract": "  Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.\n", "link": "http://arxiv.org/abs/2408.00458v1", "date": "2024-08-01", "relevancy": 2.4791, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6953}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6138}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reenact%20Anything%3A%20Semantic%20Video%20Motion%20Transfer%20Using%20Motion-Textual%0A%20%20Inversion&body=Title%3A%20Reenact%20Anything%3A%20Semantic%20Video%20Motion%20Transfer%20Using%20Motion-Textual%0A%20%20Inversion%0AAuthor%3A%20Manuel%20Kansy%20and%20Jacek%20Naruniec%20and%20Christopher%20Schroers%20and%20Markus%20Gross%20and%20Romann%20M.%20Weber%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20a%20tremendous%20improvement%20in%20the%20quality%20of%20video%0Ageneration%20and%20editing%20approaches.%20While%20several%20techniques%20focus%20on%20editing%0Aappearance%2C%20few%20address%20motion.%20Current%20approaches%20using%20text%2C%20trajectories%2C%20or%0Abounding%20boxes%20are%20limited%20to%20simple%20motions%2C%20so%20we%20specify%20motions%20with%20a%0Asingle%20motion%20reference%20video%20instead.%20We%20further%20propose%20to%20use%20a%20pre-trained%0Aimage-to-video%20model%20rather%20than%20a%20text-to-video%20model.%20This%20approach%20allows%20us%0Ato%20preserve%20the%20exact%20appearance%20and%20position%20of%20a%20target%20object%20or%20scene%20and%0Ahelps%20disentangle%20appearance%20from%20motion.%20Our%20method%2C%20called%20motion-textual%0Ainversion%2C%20leverages%20our%20observation%20that%20image-to-video%20models%20extract%0Aappearance%20mainly%20from%20the%20%28latent%29%20image%20input%2C%20while%20the%20text/image%20embedding%0Ainjected%20via%20cross-attention%20predominantly%20controls%20motion.%20We%20thus%20represent%0Amotion%20using%20text/image%20embedding%20tokens.%20By%20operating%20on%20an%20inflated%0Amotion-text%20embedding%20containing%20multiple%20text/image%20embedding%20tokens%20per%0Aframe%2C%20we%20achieve%20a%20high%20temporal%20motion%20granularity.%20Once%20optimized%20on%20the%0Amotion%20reference%20video%2C%20this%20embedding%20can%20be%20applied%20to%20various%20target%20images%0Ato%20generate%20videos%20with%20semantically%20similar%20motions.%20Our%20approach%20does%20not%0Arequire%20spatial%20alignment%20between%20the%20motion%20reference%20video%20and%20target%20image%2C%0Ageneralizes%20across%20various%20domains%2C%20and%20can%20be%20applied%20to%20various%20tasks%20such%20as%0Afull-body%20and%20face%20reenactment%2C%20as%20well%20as%20controlling%20the%20motion%20of%20inanimate%0Aobjects%20and%20the%20camera.%20We%20empirically%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20in%20the%20semantic%20video%20motion%20transfer%20task%2C%20significantly%20outperforming%0Aexisting%20methods%20in%20this%20context.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReenact%2520Anything%253A%2520Semantic%2520Video%2520Motion%2520Transfer%2520Using%2520Motion-Textual%250A%2520%2520Inversion%26entry.906535625%3DManuel%2520Kansy%2520and%2520Jacek%2520Naruniec%2520and%2520Christopher%2520Schroers%2520and%2520Markus%2520Gross%2520and%2520Romann%2520M.%2520Weber%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520a%2520tremendous%2520improvement%2520in%2520the%2520quality%2520of%2520video%250Ageneration%2520and%2520editing%2520approaches.%2520While%2520several%2520techniques%2520focus%2520on%2520editing%250Aappearance%252C%2520few%2520address%2520motion.%2520Current%2520approaches%2520using%2520text%252C%2520trajectories%252C%2520or%250Abounding%2520boxes%2520are%2520limited%2520to%2520simple%2520motions%252C%2520so%2520we%2520specify%2520motions%2520with%2520a%250Asingle%2520motion%2520reference%2520video%2520instead.%2520We%2520further%2520propose%2520to%2520use%2520a%2520pre-trained%250Aimage-to-video%2520model%2520rather%2520than%2520a%2520text-to-video%2520model.%2520This%2520approach%2520allows%2520us%250Ato%2520preserve%2520the%2520exact%2520appearance%2520and%2520position%2520of%2520a%2520target%2520object%2520or%2520scene%2520and%250Ahelps%2520disentangle%2520appearance%2520from%2520motion.%2520Our%2520method%252C%2520called%2520motion-textual%250Ainversion%252C%2520leverages%2520our%2520observation%2520that%2520image-to-video%2520models%2520extract%250Aappearance%2520mainly%2520from%2520the%2520%2528latent%2529%2520image%2520input%252C%2520while%2520the%2520text/image%2520embedding%250Ainjected%2520via%2520cross-attention%2520predominantly%2520controls%2520motion.%2520We%2520thus%2520represent%250Amotion%2520using%2520text/image%2520embedding%2520tokens.%2520By%2520operating%2520on%2520an%2520inflated%250Amotion-text%2520embedding%2520containing%2520multiple%2520text/image%2520embedding%2520tokens%2520per%250Aframe%252C%2520we%2520achieve%2520a%2520high%2520temporal%2520motion%2520granularity.%2520Once%2520optimized%2520on%2520the%250Amotion%2520reference%2520video%252C%2520this%2520embedding%2520can%2520be%2520applied%2520to%2520various%2520target%2520images%250Ato%2520generate%2520videos%2520with%2520semantically%2520similar%2520motions.%2520Our%2520approach%2520does%2520not%250Arequire%2520spatial%2520alignment%2520between%2520the%2520motion%2520reference%2520video%2520and%2520target%2520image%252C%250Ageneralizes%2520across%2520various%2520domains%252C%2520and%2520can%2520be%2520applied%2520to%2520various%2520tasks%2520such%2520as%250Afull-body%2520and%2520face%2520reenactment%252C%2520as%2520well%2520as%2520controlling%2520the%2520motion%2520of%2520inanimate%250Aobjects%2520and%2520the%2520camera.%2520We%2520empirically%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Amethod%2520in%2520the%2520semantic%2520video%2520motion%2520transfer%2520task%252C%2520significantly%2520outperforming%250Aexisting%2520methods%2520in%2520this%2520context.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reenact%20Anything%3A%20Semantic%20Video%20Motion%20Transfer%20Using%20Motion-Textual%0A%20%20Inversion&entry.906535625=Manuel%20Kansy%20and%20Jacek%20Naruniec%20and%20Christopher%20Schroers%20and%20Markus%20Gross%20and%20Romann%20M.%20Weber&entry.1292438233=%20%20Recent%20years%20have%20seen%20a%20tremendous%20improvement%20in%20the%20quality%20of%20video%0Ageneration%20and%20editing%20approaches.%20While%20several%20techniques%20focus%20on%20editing%0Aappearance%2C%20few%20address%20motion.%20Current%20approaches%20using%20text%2C%20trajectories%2C%20or%0Abounding%20boxes%20are%20limited%20to%20simple%20motions%2C%20so%20we%20specify%20motions%20with%20a%0Asingle%20motion%20reference%20video%20instead.%20We%20further%20propose%20to%20use%20a%20pre-trained%0Aimage-to-video%20model%20rather%20than%20a%20text-to-video%20model.%20This%20approach%20allows%20us%0Ato%20preserve%20the%20exact%20appearance%20and%20position%20of%20a%20target%20object%20or%20scene%20and%0Ahelps%20disentangle%20appearance%20from%20motion.%20Our%20method%2C%20called%20motion-textual%0Ainversion%2C%20leverages%20our%20observation%20that%20image-to-video%20models%20extract%0Aappearance%20mainly%20from%20the%20%28latent%29%20image%20input%2C%20while%20the%20text/image%20embedding%0Ainjected%20via%20cross-attention%20predominantly%20controls%20motion.%20We%20thus%20represent%0Amotion%20using%20text/image%20embedding%20tokens.%20By%20operating%20on%20an%20inflated%0Amotion-text%20embedding%20containing%20multiple%20text/image%20embedding%20tokens%20per%0Aframe%2C%20we%20achieve%20a%20high%20temporal%20motion%20granularity.%20Once%20optimized%20on%20the%0Amotion%20reference%20video%2C%20this%20embedding%20can%20be%20applied%20to%20various%20target%20images%0Ato%20generate%20videos%20with%20semantically%20similar%20motions.%20Our%20approach%20does%20not%0Arequire%20spatial%20alignment%20between%20the%20motion%20reference%20video%20and%20target%20image%2C%0Ageneralizes%20across%20various%20domains%2C%20and%20can%20be%20applied%20to%20various%20tasks%20such%20as%0Afull-body%20and%20face%20reenactment%2C%20as%20well%20as%20controlling%20the%20motion%20of%20inanimate%0Aobjects%20and%20the%20camera.%20We%20empirically%20demonstrate%20the%20effectiveness%20of%20our%0Amethod%20in%20the%20semantic%20video%20motion%20transfer%20task%2C%20significantly%20outperforming%0Aexisting%20methods%20in%20this%20context.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00458v1&entry.124074799=Read"},
{"title": "Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object\n  Detection", "author": "Ruiyang Zhang and Hu Zhang and Hang Yu and Zhedong Zheng", "abstract": "  Unsupervised 3D object detection aims to identify objects of interest from\nunlabeled raw data, such as LiDAR points. Recent approaches usually adopt\npseudo 3D bounding boxes (3D bboxes) from clustering algorithm to initialize\nthe model training, and then iteratively updating both pseudo labels and the\ntrained model. However, pseudo bboxes inevitably contain noises, and such\ninaccurate annotation accumulates to the final model, compromising the\nperformance. Therefore, in an attempt to mitigate the negative impact of pseudo\nbboxes, we introduce a new uncertainty-aware framework. In particular, Our\nmethod consists of two primary components: uncertainty estimation and\nuncertainty regularization. (1) In the uncertainty estimation phase, we\nincorporate an extra auxiliary detection branch alongside the primary detector.\nThe prediction disparity between the primary and auxiliary detectors is\nleveraged to estimate uncertainty at the box coordinate level, including\nposition, shape, orientation. (2) Based on the assessed uncertainty, we\nregularize the model training via adaptively adjusting every 3D bboxes\ncoordinates. For pseudo bbox coordinates with high uncertainty, we assign a\nrelatively low loss weight. Experiment verifies that the proposed method is\nrobust against the noisy pseudo bboxes, yielding substantial improvements on\nnuScenes and Lyft compared to existing techniques, with increases of 6.9% in\nAP$_{BEV}$ and 2.5% in AP$_{3D}$ on nuScenes, and 2.2% in AP$_{BEV}$ and 1.0%\nin AP$_{3D}$ on Lyft.\n", "link": "http://arxiv.org/abs/2408.00619v1", "date": "2024-08-01", "relevancy": 2.4546, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.7054}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6229}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Uncertainty-aware%20Bounding%20Boxes%20for%20Unsupervised%203D%20Object%0A%20%20Detection&body=Title%3A%20Harnessing%20Uncertainty-aware%20Bounding%20Boxes%20for%20Unsupervised%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Ruiyang%20Zhang%20and%20Hu%20Zhang%20and%20Hang%20Yu%20and%20Zhedong%20Zheng%0AAbstract%3A%20%20%20Unsupervised%203D%20object%20detection%20aims%20to%20identify%20objects%20of%20interest%20from%0Aunlabeled%20raw%20data%2C%20such%20as%20LiDAR%20points.%20Recent%20approaches%20usually%20adopt%0Apseudo%203D%20bounding%20boxes%20%283D%20bboxes%29%20from%20clustering%20algorithm%20to%20initialize%0Athe%20model%20training%2C%20and%20then%20iteratively%20updating%20both%20pseudo%20labels%20and%20the%0Atrained%20model.%20However%2C%20pseudo%20bboxes%20inevitably%20contain%20noises%2C%20and%20such%0Ainaccurate%20annotation%20accumulates%20to%20the%20final%20model%2C%20compromising%20the%0Aperformance.%20Therefore%2C%20in%20an%20attempt%20to%20mitigate%20the%20negative%20impact%20of%20pseudo%0Abboxes%2C%20we%20introduce%20a%20new%20uncertainty-aware%20framework.%20In%20particular%2C%20Our%0Amethod%20consists%20of%20two%20primary%20components%3A%20uncertainty%20estimation%20and%0Auncertainty%20regularization.%20%281%29%20In%20the%20uncertainty%20estimation%20phase%2C%20we%0Aincorporate%20an%20extra%20auxiliary%20detection%20branch%20alongside%20the%20primary%20detector.%0AThe%20prediction%20disparity%20between%20the%20primary%20and%20auxiliary%20detectors%20is%0Aleveraged%20to%20estimate%20uncertainty%20at%20the%20box%20coordinate%20level%2C%20including%0Aposition%2C%20shape%2C%20orientation.%20%282%29%20Based%20on%20the%20assessed%20uncertainty%2C%20we%0Aregularize%20the%20model%20training%20via%20adaptively%20adjusting%20every%203D%20bboxes%0Acoordinates.%20For%20pseudo%20bbox%20coordinates%20with%20high%20uncertainty%2C%20we%20assign%20a%0Arelatively%20low%20loss%20weight.%20Experiment%20verifies%20that%20the%20proposed%20method%20is%0Arobust%20against%20the%20noisy%20pseudo%20bboxes%2C%20yielding%20substantial%20improvements%20on%0AnuScenes%20and%20Lyft%20compared%20to%20existing%20techniques%2C%20with%20increases%20of%206.9%25%20in%0AAP%24_%7BBEV%7D%24%20and%202.5%25%20in%20AP%24_%7B3D%7D%24%20on%20nuScenes%2C%20and%202.2%25%20in%20AP%24_%7BBEV%7D%24%20and%201.0%25%0Ain%20AP%24_%7B3D%7D%24%20on%20Lyft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Uncertainty-aware%2520Bounding%2520Boxes%2520for%2520Unsupervised%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DRuiyang%2520Zhang%2520and%2520Hu%2520Zhang%2520and%2520Hang%2520Yu%2520and%2520Zhedong%2520Zheng%26entry.1292438233%3D%2520%2520Unsupervised%25203D%2520object%2520detection%2520aims%2520to%2520identify%2520objects%2520of%2520interest%2520from%250Aunlabeled%2520raw%2520data%252C%2520such%2520as%2520LiDAR%2520points.%2520Recent%2520approaches%2520usually%2520adopt%250Apseudo%25203D%2520bounding%2520boxes%2520%25283D%2520bboxes%2529%2520from%2520clustering%2520algorithm%2520to%2520initialize%250Athe%2520model%2520training%252C%2520and%2520then%2520iteratively%2520updating%2520both%2520pseudo%2520labels%2520and%2520the%250Atrained%2520model.%2520However%252C%2520pseudo%2520bboxes%2520inevitably%2520contain%2520noises%252C%2520and%2520such%250Ainaccurate%2520annotation%2520accumulates%2520to%2520the%2520final%2520model%252C%2520compromising%2520the%250Aperformance.%2520Therefore%252C%2520in%2520an%2520attempt%2520to%2520mitigate%2520the%2520negative%2520impact%2520of%2520pseudo%250Abboxes%252C%2520we%2520introduce%2520a%2520new%2520uncertainty-aware%2520framework.%2520In%2520particular%252C%2520Our%250Amethod%2520consists%2520of%2520two%2520primary%2520components%253A%2520uncertainty%2520estimation%2520and%250Auncertainty%2520regularization.%2520%25281%2529%2520In%2520the%2520uncertainty%2520estimation%2520phase%252C%2520we%250Aincorporate%2520an%2520extra%2520auxiliary%2520detection%2520branch%2520alongside%2520the%2520primary%2520detector.%250AThe%2520prediction%2520disparity%2520between%2520the%2520primary%2520and%2520auxiliary%2520detectors%2520is%250Aleveraged%2520to%2520estimate%2520uncertainty%2520at%2520the%2520box%2520coordinate%2520level%252C%2520including%250Aposition%252C%2520shape%252C%2520orientation.%2520%25282%2529%2520Based%2520on%2520the%2520assessed%2520uncertainty%252C%2520we%250Aregularize%2520the%2520model%2520training%2520via%2520adaptively%2520adjusting%2520every%25203D%2520bboxes%250Acoordinates.%2520For%2520pseudo%2520bbox%2520coordinates%2520with%2520high%2520uncertainty%252C%2520we%2520assign%2520a%250Arelatively%2520low%2520loss%2520weight.%2520Experiment%2520verifies%2520that%2520the%2520proposed%2520method%2520is%250Arobust%2520against%2520the%2520noisy%2520pseudo%2520bboxes%252C%2520yielding%2520substantial%2520improvements%2520on%250AnuScenes%2520and%2520Lyft%2520compared%2520to%2520existing%2520techniques%252C%2520with%2520increases%2520of%25206.9%2525%2520in%250AAP%2524_%257BBEV%257D%2524%2520and%25202.5%2525%2520in%2520AP%2524_%257B3D%257D%2524%2520on%2520nuScenes%252C%2520and%25202.2%2525%2520in%2520AP%2524_%257BBEV%257D%2524%2520and%25201.0%2525%250Ain%2520AP%2524_%257B3D%257D%2524%2520on%2520Lyft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Uncertainty-aware%20Bounding%20Boxes%20for%20Unsupervised%203D%20Object%0A%20%20Detection&entry.906535625=Ruiyang%20Zhang%20and%20Hu%20Zhang%20and%20Hang%20Yu%20and%20Zhedong%20Zheng&entry.1292438233=%20%20Unsupervised%203D%20object%20detection%20aims%20to%20identify%20objects%20of%20interest%20from%0Aunlabeled%20raw%20data%2C%20such%20as%20LiDAR%20points.%20Recent%20approaches%20usually%20adopt%0Apseudo%203D%20bounding%20boxes%20%283D%20bboxes%29%20from%20clustering%20algorithm%20to%20initialize%0Athe%20model%20training%2C%20and%20then%20iteratively%20updating%20both%20pseudo%20labels%20and%20the%0Atrained%20model.%20However%2C%20pseudo%20bboxes%20inevitably%20contain%20noises%2C%20and%20such%0Ainaccurate%20annotation%20accumulates%20to%20the%20final%20model%2C%20compromising%20the%0Aperformance.%20Therefore%2C%20in%20an%20attempt%20to%20mitigate%20the%20negative%20impact%20of%20pseudo%0Abboxes%2C%20we%20introduce%20a%20new%20uncertainty-aware%20framework.%20In%20particular%2C%20Our%0Amethod%20consists%20of%20two%20primary%20components%3A%20uncertainty%20estimation%20and%0Auncertainty%20regularization.%20%281%29%20In%20the%20uncertainty%20estimation%20phase%2C%20we%0Aincorporate%20an%20extra%20auxiliary%20detection%20branch%20alongside%20the%20primary%20detector.%0AThe%20prediction%20disparity%20between%20the%20primary%20and%20auxiliary%20detectors%20is%0Aleveraged%20to%20estimate%20uncertainty%20at%20the%20box%20coordinate%20level%2C%20including%0Aposition%2C%20shape%2C%20orientation.%20%282%29%20Based%20on%20the%20assessed%20uncertainty%2C%20we%0Aregularize%20the%20model%20training%20via%20adaptively%20adjusting%20every%203D%20bboxes%0Acoordinates.%20For%20pseudo%20bbox%20coordinates%20with%20high%20uncertainty%2C%20we%20assign%20a%0Arelatively%20low%20loss%20weight.%20Experiment%20verifies%20that%20the%20proposed%20method%20is%0Arobust%20against%20the%20noisy%20pseudo%20bboxes%2C%20yielding%20substantial%20improvements%20on%0AnuScenes%20and%20Lyft%20compared%20to%20existing%20techniques%2C%20with%20increases%20of%206.9%25%20in%0AAP%24_%7BBEV%7D%24%20and%202.5%25%20in%20AP%24_%7B3D%7D%24%20on%20nuScenes%2C%20and%202.2%25%20in%20AP%24_%7BBEV%7D%24%20and%201.0%25%0Ain%20AP%24_%7B3D%7D%24%20on%20Lyft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00619v1&entry.124074799=Read"},
{"title": "CrystalTac: 3D-Printed Vision-Based Tactile Sensor Family through Rapid\n  Monolithic Manufacturing Technique", "author": "Wen Fan and Haoran Li and Dandan Zhang", "abstract": "  Recently, vision-based tactile sensors (VBTSs) have gained popularity in\nrobotics systems. The sensing mechanisms of most VBTSs can be categorised based\non the type of tactile features they capture. Each category requires specific\nstructural designs to convert physical contact into optical information. The\ncomplex architectures of VBTSs pose challenges for traditional manufacturing\ntechniques in terms of design flexibility, cost-effectiveness, and quality\nstability. Previous research has shown that monolithic manufacturing using\nmulti-material 3D printing technology can partially address these challenges.\nThis study introduces the CrystalTac family, a series of VBTSs designed with a\nunique sensing mechanism and fabricated through rapid monolithic manufacturing.\nCase studies on CrystalTac-type sensors demonstrate their effective performance\nin tasks involving tactile perception, along with impressive cost-effectiveness\nand design flexibility. The CrystalTac family aims to highlight the potential\nof monolithic manufacturing in VBTS development and inspire further research in\ntactile sensing and manipulation.\n", "link": "http://arxiv.org/abs/2408.00638v1", "date": "2024-08-01", "relevancy": 2.4172, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5047}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5047}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrystalTac%3A%203D-Printed%20Vision-Based%20Tactile%20Sensor%20Family%20through%20Rapid%0A%20%20Monolithic%20Manufacturing%20Technique&body=Title%3A%20CrystalTac%3A%203D-Printed%20Vision-Based%20Tactile%20Sensor%20Family%20through%20Rapid%0A%20%20Monolithic%20Manufacturing%20Technique%0AAuthor%3A%20Wen%20Fan%20and%20Haoran%20Li%20and%20Dandan%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20vision-based%20tactile%20sensors%20%28VBTSs%29%20have%20gained%20popularity%20in%0Arobotics%20systems.%20The%20sensing%20mechanisms%20of%20most%20VBTSs%20can%20be%20categorised%20based%0Aon%20the%20type%20of%20tactile%20features%20they%20capture.%20Each%20category%20requires%20specific%0Astructural%20designs%20to%20convert%20physical%20contact%20into%20optical%20information.%20The%0Acomplex%20architectures%20of%20VBTSs%20pose%20challenges%20for%20traditional%20manufacturing%0Atechniques%20in%20terms%20of%20design%20flexibility%2C%20cost-effectiveness%2C%20and%20quality%0Astability.%20Previous%20research%20has%20shown%20that%20monolithic%20manufacturing%20using%0Amulti-material%203D%20printing%20technology%20can%20partially%20address%20these%20challenges.%0AThis%20study%20introduces%20the%20CrystalTac%20family%2C%20a%20series%20of%20VBTSs%20designed%20with%20a%0Aunique%20sensing%20mechanism%20and%20fabricated%20through%20rapid%20monolithic%20manufacturing.%0ACase%20studies%20on%20CrystalTac-type%20sensors%20demonstrate%20their%20effective%20performance%0Ain%20tasks%20involving%20tactile%20perception%2C%20along%20with%20impressive%20cost-effectiveness%0Aand%20design%20flexibility.%20The%20CrystalTac%20family%20aims%20to%20highlight%20the%20potential%0Aof%20monolithic%20manufacturing%20in%20VBTS%20development%20and%20inspire%20further%20research%20in%0Atactile%20sensing%20and%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrystalTac%253A%25203D-Printed%2520Vision-Based%2520Tactile%2520Sensor%2520Family%2520through%2520Rapid%250A%2520%2520Monolithic%2520Manufacturing%2520Technique%26entry.906535625%3DWen%2520Fan%2520and%2520Haoran%2520Li%2520and%2520Dandan%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520vision-based%2520tactile%2520sensors%2520%2528VBTSs%2529%2520have%2520gained%2520popularity%2520in%250Arobotics%2520systems.%2520The%2520sensing%2520mechanisms%2520of%2520most%2520VBTSs%2520can%2520be%2520categorised%2520based%250Aon%2520the%2520type%2520of%2520tactile%2520features%2520they%2520capture.%2520Each%2520category%2520requires%2520specific%250Astructural%2520designs%2520to%2520convert%2520physical%2520contact%2520into%2520optical%2520information.%2520The%250Acomplex%2520architectures%2520of%2520VBTSs%2520pose%2520challenges%2520for%2520traditional%2520manufacturing%250Atechniques%2520in%2520terms%2520of%2520design%2520flexibility%252C%2520cost-effectiveness%252C%2520and%2520quality%250Astability.%2520Previous%2520research%2520has%2520shown%2520that%2520monolithic%2520manufacturing%2520using%250Amulti-material%25203D%2520printing%2520technology%2520can%2520partially%2520address%2520these%2520challenges.%250AThis%2520study%2520introduces%2520the%2520CrystalTac%2520family%252C%2520a%2520series%2520of%2520VBTSs%2520designed%2520with%2520a%250Aunique%2520sensing%2520mechanism%2520and%2520fabricated%2520through%2520rapid%2520monolithic%2520manufacturing.%250ACase%2520studies%2520on%2520CrystalTac-type%2520sensors%2520demonstrate%2520their%2520effective%2520performance%250Ain%2520tasks%2520involving%2520tactile%2520perception%252C%2520along%2520with%2520impressive%2520cost-effectiveness%250Aand%2520design%2520flexibility.%2520The%2520CrystalTac%2520family%2520aims%2520to%2520highlight%2520the%2520potential%250Aof%2520monolithic%2520manufacturing%2520in%2520VBTS%2520development%2520and%2520inspire%2520further%2520research%2520in%250Atactile%2520sensing%2520and%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrystalTac%3A%203D-Printed%20Vision-Based%20Tactile%20Sensor%20Family%20through%20Rapid%0A%20%20Monolithic%20Manufacturing%20Technique&entry.906535625=Wen%20Fan%20and%20Haoran%20Li%20and%20Dandan%20Zhang&entry.1292438233=%20%20Recently%2C%20vision-based%20tactile%20sensors%20%28VBTSs%29%20have%20gained%20popularity%20in%0Arobotics%20systems.%20The%20sensing%20mechanisms%20of%20most%20VBTSs%20can%20be%20categorised%20based%0Aon%20the%20type%20of%20tactile%20features%20they%20capture.%20Each%20category%20requires%20specific%0Astructural%20designs%20to%20convert%20physical%20contact%20into%20optical%20information.%20The%0Acomplex%20architectures%20of%20VBTSs%20pose%20challenges%20for%20traditional%20manufacturing%0Atechniques%20in%20terms%20of%20design%20flexibility%2C%20cost-effectiveness%2C%20and%20quality%0Astability.%20Previous%20research%20has%20shown%20that%20monolithic%20manufacturing%20using%0Amulti-material%203D%20printing%20technology%20can%20partially%20address%20these%20challenges.%0AThis%20study%20introduces%20the%20CrystalTac%20family%2C%20a%20series%20of%20VBTSs%20designed%20with%20a%0Aunique%20sensing%20mechanism%20and%20fabricated%20through%20rapid%20monolithic%20manufacturing.%0ACase%20studies%20on%20CrystalTac-type%20sensors%20demonstrate%20their%20effective%20performance%0Ain%20tasks%20involving%20tactile%20perception%2C%20along%20with%20impressive%20cost-effectiveness%0Aand%20design%20flexibility.%20The%20CrystalTac%20family%20aims%20to%20highlight%20the%20potential%0Aof%20monolithic%20manufacturing%20in%20VBTS%20development%20and%20inspire%20further%20research%20in%0Atactile%20sensing%20and%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00638v1&entry.124074799=Read"},
{"title": "Enhancing Ethereum Fraud Detection via Generative and Contrastive\n  Self-supervision", "author": "Chenxiang Jin and Jiajun Zhou and Chenxuan Xie and Shanqing Yu and Qi Xuan and Xiaoniu Yang", "abstract": "  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be released on GitHub soon.\n", "link": "http://arxiv.org/abs/2408.00641v1", "date": "2024-08-01", "relevancy": 2.3914, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4962}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4703}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Ethereum%20Fraud%20Detection%20via%20Generative%20and%20Contrastive%0A%20%20Self-supervision&body=Title%3A%20Enhancing%20Ethereum%20Fraud%20Detection%20via%20Generative%20and%20Contrastive%0A%20%20Self-supervision%0AAuthor%3A%20Chenxiang%20Jin%20and%20Jiajun%20Zhou%20and%20Chenxuan%20Xie%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang%0AAbstract%3A%20%20%20The%20rampant%20fraudulent%20activities%20on%20Ethereum%20hinder%20the%20healthy%20development%0Aof%20the%20blockchain%20ecosystem%2C%20necessitating%20the%20reinforcement%20of%20regulations.%0AHowever%2C%20multiple%20imbalances%20involving%20account%20interaction%20frequencies%20and%0Ainteraction%20types%20in%20the%20Ethereum%20transaction%20environment%20pose%20significant%0Achallenges%20to%20data%20mining-based%20fraud%20detection%20research.%20To%20address%20this%2C%20we%0Afirst%20propose%20the%20concept%20of%20meta-interactions%20to%20refine%20interaction%20behaviors%0Ain%20Ethereum%2C%20and%20based%20on%20this%2C%20we%20present%20a%20dual%20self-supervision%20enhanced%0AEthereum%20fraud%20detection%20framework%2C%20named%20Meta-IFD.%20This%20framework%20initially%0Aintroduces%20a%20generative%20self-supervision%20mechanism%20to%20augment%20the%20interaction%0Afeatures%20of%20accounts%2C%20followed%20by%20a%20contrastive%20self-supervision%20mechanism%20to%0Adifferentiate%20various%20behavior%20patterns%2C%20and%20ultimately%20characterizes%20the%0Abehavioral%20representations%20of%20accounts%20and%20mines%20potential%20fraud%20risks%20through%0Amulti-view%20interaction%20feature%20learning.%20Extensive%20experiments%20on%20real%20Ethereum%0Adatasets%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20framework%20in%0Adetecting%20common%20Ethereum%20fraud%20behaviors%20such%20as%20Ponzi%20schemes%20and%20phishing%0Ascams.%20Additionally%2C%20the%20generative%20module%20can%20effectively%20alleviate%20the%0Ainteraction%20distribution%20imbalance%20in%20Ethereum%20data%2C%20while%20the%20contrastive%0Amodule%20significantly%20enhances%20the%20framework%27s%20ability%20to%20distinguish%20different%0Abehavior%20patterns.%20The%20source%20code%20will%20be%20released%20on%20GitHub%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Ethereum%2520Fraud%2520Detection%2520via%2520Generative%2520and%2520Contrastive%250A%2520%2520Self-supervision%26entry.906535625%3DChenxiang%2520Jin%2520and%2520Jiajun%2520Zhou%2520and%2520Chenxuan%2520Xie%2520and%2520Shanqing%2520Yu%2520and%2520Qi%2520Xuan%2520and%2520Xiaoniu%2520Yang%26entry.1292438233%3D%2520%2520The%2520rampant%2520fraudulent%2520activities%2520on%2520Ethereum%2520hinder%2520the%2520healthy%2520development%250Aof%2520the%2520blockchain%2520ecosystem%252C%2520necessitating%2520the%2520reinforcement%2520of%2520regulations.%250AHowever%252C%2520multiple%2520imbalances%2520involving%2520account%2520interaction%2520frequencies%2520and%250Ainteraction%2520types%2520in%2520the%2520Ethereum%2520transaction%2520environment%2520pose%2520significant%250Achallenges%2520to%2520data%2520mining-based%2520fraud%2520detection%2520research.%2520To%2520address%2520this%252C%2520we%250Afirst%2520propose%2520the%2520concept%2520of%2520meta-interactions%2520to%2520refine%2520interaction%2520behaviors%250Ain%2520Ethereum%252C%2520and%2520based%2520on%2520this%252C%2520we%2520present%2520a%2520dual%2520self-supervision%2520enhanced%250AEthereum%2520fraud%2520detection%2520framework%252C%2520named%2520Meta-IFD.%2520This%2520framework%2520initially%250Aintroduces%2520a%2520generative%2520self-supervision%2520mechanism%2520to%2520augment%2520the%2520interaction%250Afeatures%2520of%2520accounts%252C%2520followed%2520by%2520a%2520contrastive%2520self-supervision%2520mechanism%2520to%250Adifferentiate%2520various%2520behavior%2520patterns%252C%2520and%2520ultimately%2520characterizes%2520the%250Abehavioral%2520representations%2520of%2520accounts%2520and%2520mines%2520potential%2520fraud%2520risks%2520through%250Amulti-view%2520interaction%2520feature%2520learning.%2520Extensive%2520experiments%2520on%2520real%2520Ethereum%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%2520framework%2520in%250Adetecting%2520common%2520Ethereum%2520fraud%2520behaviors%2520such%2520as%2520Ponzi%2520schemes%2520and%2520phishing%250Ascams.%2520Additionally%252C%2520the%2520generative%2520module%2520can%2520effectively%2520alleviate%2520the%250Ainteraction%2520distribution%2520imbalance%2520in%2520Ethereum%2520data%252C%2520while%2520the%2520contrastive%250Amodule%2520significantly%2520enhances%2520the%2520framework%2527s%2520ability%2520to%2520distinguish%2520different%250Abehavior%2520patterns.%2520The%2520source%2520code%2520will%2520be%2520released%2520on%2520GitHub%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Ethereum%20Fraud%20Detection%20via%20Generative%20and%20Contrastive%0A%20%20Self-supervision&entry.906535625=Chenxiang%20Jin%20and%20Jiajun%20Zhou%20and%20Chenxuan%20Xie%20and%20Shanqing%20Yu%20and%20Qi%20Xuan%20and%20Xiaoniu%20Yang&entry.1292438233=%20%20The%20rampant%20fraudulent%20activities%20on%20Ethereum%20hinder%20the%20healthy%20development%0Aof%20the%20blockchain%20ecosystem%2C%20necessitating%20the%20reinforcement%20of%20regulations.%0AHowever%2C%20multiple%20imbalances%20involving%20account%20interaction%20frequencies%20and%0Ainteraction%20types%20in%20the%20Ethereum%20transaction%20environment%20pose%20significant%0Achallenges%20to%20data%20mining-based%20fraud%20detection%20research.%20To%20address%20this%2C%20we%0Afirst%20propose%20the%20concept%20of%20meta-interactions%20to%20refine%20interaction%20behaviors%0Ain%20Ethereum%2C%20and%20based%20on%20this%2C%20we%20present%20a%20dual%20self-supervision%20enhanced%0AEthereum%20fraud%20detection%20framework%2C%20named%20Meta-IFD.%20This%20framework%20initially%0Aintroduces%20a%20generative%20self-supervision%20mechanism%20to%20augment%20the%20interaction%0Afeatures%20of%20accounts%2C%20followed%20by%20a%20contrastive%20self-supervision%20mechanism%20to%0Adifferentiate%20various%20behavior%20patterns%2C%20and%20ultimately%20characterizes%20the%0Abehavioral%20representations%20of%20accounts%20and%20mines%20potential%20fraud%20risks%20through%0Amulti-view%20interaction%20feature%20learning.%20Extensive%20experiments%20on%20real%20Ethereum%0Adatasets%20demonstrate%20the%20effectiveness%20and%20superiority%20of%20our%20framework%20in%0Adetecting%20common%20Ethereum%20fraud%20behaviors%20such%20as%20Ponzi%20schemes%20and%20phishing%0Ascams.%20Additionally%2C%20the%20generative%20module%20can%20effectively%20alleviate%20the%0Ainteraction%20distribution%20imbalance%20in%20Ethereum%20data%2C%20while%20the%20contrastive%0Amodule%20significantly%20enhances%20the%20framework%27s%20ability%20to%20distinguish%20different%0Abehavior%20patterns.%20The%20source%20code%20will%20be%20released%20on%20GitHub%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00641v1&entry.124074799=Read"},
{"title": "MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D\n  Object Detection", "author": "Youjia Fu and Zihao Xu and Junsong Fu and Huixia Xue and Shuqiu Tan and Lei Li", "abstract": "  Recent advancements in transformer-based monocular 3D object detection\ntechniques have exhibited exceptional performance in inferring 3D attributes\nfrom single 2D images. However, most existing methods rely on\nresource-intensive transformer architectures, which often lead to significant\ndrops in computational efficiency and performance when handling long sequence\ndata. To address these challenges and advance monocular 3D object detection\ntechnology, we propose an innovative network architecture, MonoMM, a\nMulti-scale \\textbf{M}amba-Enhanced network for real-time Monocular 3D object\ndetection. This well-designed architecture primarily includes the following two\ncore modules: Focused Multi-Scale Fusion (FMF) Module, which focuses on\neffectively preserving and fusing image information from different scales with\nlower computational resource consumption. By precisely regulating the\ninformation flow, the FMF module enhances the model adaptability and robustness\nto scale variations while maintaining image details. Depth-Aware Feature\nEnhancement Mamba (DMB) Module: It utilizes the fused features from image\ncharacteristics as input and employs a novel adaptive strategy to globally\nintegrate depth information and visual information. This depth fusion strategy\nnot only improves the accuracy of depth estimation but also enhances the model\nperformance under different viewing angles and environmental conditions.\nMoreover, the modular design of MonoMM provides high flexibility and\nscalability, facilitating adjustments and optimizations according to specific\napplication needs. Extensive experiments conducted on the KITTI dataset show\nthat our method outperforms previous monocular methods and achieves real-time\ndetection.\n", "link": "http://arxiv.org/abs/2408.00438v1", "date": "2024-08-01", "relevancy": 2.3901, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5938}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoMM%3A%20A%20Multi-scale%20Mamba-Enhanced%20Network%20for%20Real-time%20Monocular%203D%0A%20%20Object%20Detection&body=Title%3A%20MonoMM%3A%20A%20Multi-scale%20Mamba-Enhanced%20Network%20for%20Real-time%20Monocular%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Youjia%20Fu%20and%20Zihao%20Xu%20and%20Junsong%20Fu%20and%20Huixia%20Xue%20and%20Shuqiu%20Tan%20and%20Lei%20Li%0AAbstract%3A%20%20%20Recent%20advancements%20in%20transformer-based%20monocular%203D%20object%20detection%0Atechniques%20have%20exhibited%20exceptional%20performance%20in%20inferring%203D%20attributes%0Afrom%20single%202D%20images.%20However%2C%20most%20existing%20methods%20rely%20on%0Aresource-intensive%20transformer%20architectures%2C%20which%20often%20lead%20to%20significant%0Adrops%20in%20computational%20efficiency%20and%20performance%20when%20handling%20long%20sequence%0Adata.%20To%20address%20these%20challenges%20and%20advance%20monocular%203D%20object%20detection%0Atechnology%2C%20we%20propose%20an%20innovative%20network%20architecture%2C%20MonoMM%2C%20a%0AMulti-scale%20%5Ctextbf%7BM%7Damba-Enhanced%20network%20for%20real-time%20Monocular%203D%20object%0Adetection.%20This%20well-designed%20architecture%20primarily%20includes%20the%20following%20two%0Acore%20modules%3A%20Focused%20Multi-Scale%20Fusion%20%28FMF%29%20Module%2C%20which%20focuses%20on%0Aeffectively%20preserving%20and%20fusing%20image%20information%20from%20different%20scales%20with%0Alower%20computational%20resource%20consumption.%20By%20precisely%20regulating%20the%0Ainformation%20flow%2C%20the%20FMF%20module%20enhances%20the%20model%20adaptability%20and%20robustness%0Ato%20scale%20variations%20while%20maintaining%20image%20details.%20Depth-Aware%20Feature%0AEnhancement%20Mamba%20%28DMB%29%20Module%3A%20It%20utilizes%20the%20fused%20features%20from%20image%0Acharacteristics%20as%20input%20and%20employs%20a%20novel%20adaptive%20strategy%20to%20globally%0Aintegrate%20depth%20information%20and%20visual%20information.%20This%20depth%20fusion%20strategy%0Anot%20only%20improves%20the%20accuracy%20of%20depth%20estimation%20but%20also%20enhances%20the%20model%0Aperformance%20under%20different%20viewing%20angles%20and%20environmental%20conditions.%0AMoreover%2C%20the%20modular%20design%20of%20MonoMM%20provides%20high%20flexibility%20and%0Ascalability%2C%20facilitating%20adjustments%20and%20optimizations%20according%20to%20specific%0Aapplication%20needs.%20Extensive%20experiments%20conducted%20on%20the%20KITTI%20dataset%20show%0Athat%20our%20method%20outperforms%20previous%20monocular%20methods%20and%20achieves%20real-time%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoMM%253A%2520A%2520Multi-scale%2520Mamba-Enhanced%2520Network%2520for%2520Real-time%2520Monocular%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DYoujia%2520Fu%2520and%2520Zihao%2520Xu%2520and%2520Junsong%2520Fu%2520and%2520Huixia%2520Xue%2520and%2520Shuqiu%2520Tan%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520transformer-based%2520monocular%25203D%2520object%2520detection%250Atechniques%2520have%2520exhibited%2520exceptional%2520performance%2520in%2520inferring%25203D%2520attributes%250Afrom%2520single%25202D%2520images.%2520However%252C%2520most%2520existing%2520methods%2520rely%2520on%250Aresource-intensive%2520transformer%2520architectures%252C%2520which%2520often%2520lead%2520to%2520significant%250Adrops%2520in%2520computational%2520efficiency%2520and%2520performance%2520when%2520handling%2520long%2520sequence%250Adata.%2520To%2520address%2520these%2520challenges%2520and%2520advance%2520monocular%25203D%2520object%2520detection%250Atechnology%252C%2520we%2520propose%2520an%2520innovative%2520network%2520architecture%252C%2520MonoMM%252C%2520a%250AMulti-scale%2520%255Ctextbf%257BM%257Damba-Enhanced%2520network%2520for%2520real-time%2520Monocular%25203D%2520object%250Adetection.%2520This%2520well-designed%2520architecture%2520primarily%2520includes%2520the%2520following%2520two%250Acore%2520modules%253A%2520Focused%2520Multi-Scale%2520Fusion%2520%2528FMF%2529%2520Module%252C%2520which%2520focuses%2520on%250Aeffectively%2520preserving%2520and%2520fusing%2520image%2520information%2520from%2520different%2520scales%2520with%250Alower%2520computational%2520resource%2520consumption.%2520By%2520precisely%2520regulating%2520the%250Ainformation%2520flow%252C%2520the%2520FMF%2520module%2520enhances%2520the%2520model%2520adaptability%2520and%2520robustness%250Ato%2520scale%2520variations%2520while%2520maintaining%2520image%2520details.%2520Depth-Aware%2520Feature%250AEnhancement%2520Mamba%2520%2528DMB%2529%2520Module%253A%2520It%2520utilizes%2520the%2520fused%2520features%2520from%2520image%250Acharacteristics%2520as%2520input%2520and%2520employs%2520a%2520novel%2520adaptive%2520strategy%2520to%2520globally%250Aintegrate%2520depth%2520information%2520and%2520visual%2520information.%2520This%2520depth%2520fusion%2520strategy%250Anot%2520only%2520improves%2520the%2520accuracy%2520of%2520depth%2520estimation%2520but%2520also%2520enhances%2520the%2520model%250Aperformance%2520under%2520different%2520viewing%2520angles%2520and%2520environmental%2520conditions.%250AMoreover%252C%2520the%2520modular%2520design%2520of%2520MonoMM%2520provides%2520high%2520flexibility%2520and%250Ascalability%252C%2520facilitating%2520adjustments%2520and%2520optimizations%2520according%2520to%2520specific%250Aapplication%2520needs.%2520Extensive%2520experiments%2520conducted%2520on%2520the%2520KITTI%2520dataset%2520show%250Athat%2520our%2520method%2520outperforms%2520previous%2520monocular%2520methods%2520and%2520achieves%2520real-time%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoMM%3A%20A%20Multi-scale%20Mamba-Enhanced%20Network%20for%20Real-time%20Monocular%203D%0A%20%20Object%20Detection&entry.906535625=Youjia%20Fu%20and%20Zihao%20Xu%20and%20Junsong%20Fu%20and%20Huixia%20Xue%20and%20Shuqiu%20Tan%20and%20Lei%20Li&entry.1292438233=%20%20Recent%20advancements%20in%20transformer-based%20monocular%203D%20object%20detection%0Atechniques%20have%20exhibited%20exceptional%20performance%20in%20inferring%203D%20attributes%0Afrom%20single%202D%20images.%20However%2C%20most%20existing%20methods%20rely%20on%0Aresource-intensive%20transformer%20architectures%2C%20which%20often%20lead%20to%20significant%0Adrops%20in%20computational%20efficiency%20and%20performance%20when%20handling%20long%20sequence%0Adata.%20To%20address%20these%20challenges%20and%20advance%20monocular%203D%20object%20detection%0Atechnology%2C%20we%20propose%20an%20innovative%20network%20architecture%2C%20MonoMM%2C%20a%0AMulti-scale%20%5Ctextbf%7BM%7Damba-Enhanced%20network%20for%20real-time%20Monocular%203D%20object%0Adetection.%20This%20well-designed%20architecture%20primarily%20includes%20the%20following%20two%0Acore%20modules%3A%20Focused%20Multi-Scale%20Fusion%20%28FMF%29%20Module%2C%20which%20focuses%20on%0Aeffectively%20preserving%20and%20fusing%20image%20information%20from%20different%20scales%20with%0Alower%20computational%20resource%20consumption.%20By%20precisely%20regulating%20the%0Ainformation%20flow%2C%20the%20FMF%20module%20enhances%20the%20model%20adaptability%20and%20robustness%0Ato%20scale%20variations%20while%20maintaining%20image%20details.%20Depth-Aware%20Feature%0AEnhancement%20Mamba%20%28DMB%29%20Module%3A%20It%20utilizes%20the%20fused%20features%20from%20image%0Acharacteristics%20as%20input%20and%20employs%20a%20novel%20adaptive%20strategy%20to%20globally%0Aintegrate%20depth%20information%20and%20visual%20information.%20This%20depth%20fusion%20strategy%0Anot%20only%20improves%20the%20accuracy%20of%20depth%20estimation%20but%20also%20enhances%20the%20model%0Aperformance%20under%20different%20viewing%20angles%20and%20environmental%20conditions.%0AMoreover%2C%20the%20modular%20design%20of%20MonoMM%20provides%20high%20flexibility%20and%0Ascalability%2C%20facilitating%20adjustments%20and%20optimizations%20according%20to%20specific%0Aapplication%20needs.%20Extensive%20experiments%20conducted%20on%20the%20KITTI%20dataset%20show%0Athat%20our%20method%20outperforms%20previous%20monocular%20methods%20and%20achieves%20real-time%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00438v1&entry.124074799=Read"},
{"title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity", "author": "Kanghyun Choi and Hye Yoon Lee and Dain Kwon and SunJong Park and Kyuyeun Kim and Noseong Park and Jinho Lee", "abstract": "  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise MimiQ, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n", "link": "http://arxiv.org/abs/2407.20021v3", "date": "2024-08-01", "relevancy": 2.3719, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6174}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5777}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MimiQ%3A%20Low-Bit%20Data-Free%20Quantization%20of%20Vision%20Transformers%20with%0A%20%20Encouraging%20Inter-Head%20Attention%20Similarity&body=Title%3A%20MimiQ%3A%20Low-Bit%20Data-Free%20Quantization%20of%20Vision%20Transformers%20with%0A%20%20Encouraging%20Inter-Head%20Attention%20Similarity%0AAuthor%3A%20Kanghyun%20Choi%20and%20Hye%20Yoon%20Lee%20and%20Dain%20Kwon%20and%20SunJong%20Park%20and%20Kyuyeun%20Kim%20and%20Noseong%20Park%20and%20Jinho%20Lee%0AAbstract%3A%20%20%20Data-free%20quantization%20%28DFQ%29%20is%20a%20technique%20that%20creates%20a%20lightweight%0Anetwork%20from%20its%20full-precision%20counterpart%20without%20the%20original%20training%20data%2C%0Aoften%20through%20a%20synthetic%20dataset.%20Although%20several%20DFQ%20methods%20have%20been%0Aproposed%20for%20vision%20transformer%20%28ViT%29%20architectures%2C%20they%20fail%20to%20achieve%0Aefficacy%20in%20low-bit%20settings.%20Examining%20the%20existing%20methods%2C%20we%20identify%20that%0Atheir%20synthetic%20data%20produce%20misaligned%20attention%20maps%2C%20while%20those%20of%20the%20real%0Asamples%20are%20highly%20aligned.%20From%20the%20observation%20of%20aligned%20attention%2C%20we%20find%0Athat%20aligning%20attention%20maps%20of%20synthetic%20data%20helps%20to%20improve%20the%20overall%0Aperformance%20of%20quantized%20ViTs.%20Motivated%20by%20this%20finding%2C%20we%20devise%20MimiQ%2C%20a%0Anovel%20DFQ%20method%20designed%20for%20ViTs%20that%20focuses%20on%20inter-head%20attention%0Asimilarity.%20First%2C%20we%20generate%20synthetic%20data%20by%20aligning%20head-wise%20attention%0Aresponses%20in%20relation%20to%20spatial%20query%20patches.%20Then%2C%20we%20apply%20head-wise%0Astructural%20attention%20distillation%20to%20align%20the%20attention%20maps%20of%20the%20quantized%0Anetwork%20to%20those%20of%20the%20full-precision%20teacher.%20The%20experimental%20results%20show%0Athat%20the%20proposed%20method%20significantly%20outperforms%20baselines%2C%20setting%20a%20new%0Astate-of-the-art%20performance%20for%20data-free%20ViT%20quantization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20021v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMimiQ%253A%2520Low-Bit%2520Data-Free%2520Quantization%2520of%2520Vision%2520Transformers%2520with%250A%2520%2520Encouraging%2520Inter-Head%2520Attention%2520Similarity%26entry.906535625%3DKanghyun%2520Choi%2520and%2520Hye%2520Yoon%2520Lee%2520and%2520Dain%2520Kwon%2520and%2520SunJong%2520Park%2520and%2520Kyuyeun%2520Kim%2520and%2520Noseong%2520Park%2520and%2520Jinho%2520Lee%26entry.1292438233%3D%2520%2520Data-free%2520quantization%2520%2528DFQ%2529%2520is%2520a%2520technique%2520that%2520creates%2520a%2520lightweight%250Anetwork%2520from%2520its%2520full-precision%2520counterpart%2520without%2520the%2520original%2520training%2520data%252C%250Aoften%2520through%2520a%2520synthetic%2520dataset.%2520Although%2520several%2520DFQ%2520methods%2520have%2520been%250Aproposed%2520for%2520vision%2520transformer%2520%2528ViT%2529%2520architectures%252C%2520they%2520fail%2520to%2520achieve%250Aefficacy%2520in%2520low-bit%2520settings.%2520Examining%2520the%2520existing%2520methods%252C%2520we%2520identify%2520that%250Atheir%2520synthetic%2520data%2520produce%2520misaligned%2520attention%2520maps%252C%2520while%2520those%2520of%2520the%2520real%250Asamples%2520are%2520highly%2520aligned.%2520From%2520the%2520observation%2520of%2520aligned%2520attention%252C%2520we%2520find%250Athat%2520aligning%2520attention%2520maps%2520of%2520synthetic%2520data%2520helps%2520to%2520improve%2520the%2520overall%250Aperformance%2520of%2520quantized%2520ViTs.%2520Motivated%2520by%2520this%2520finding%252C%2520we%2520devise%2520MimiQ%252C%2520a%250Anovel%2520DFQ%2520method%2520designed%2520for%2520ViTs%2520that%2520focuses%2520on%2520inter-head%2520attention%250Asimilarity.%2520First%252C%2520we%2520generate%2520synthetic%2520data%2520by%2520aligning%2520head-wise%2520attention%250Aresponses%2520in%2520relation%2520to%2520spatial%2520query%2520patches.%2520Then%252C%2520we%2520apply%2520head-wise%250Astructural%2520attention%2520distillation%2520to%2520align%2520the%2520attention%2520maps%2520of%2520the%2520quantized%250Anetwork%2520to%2520those%2520of%2520the%2520full-precision%2520teacher.%2520The%2520experimental%2520results%2520show%250Athat%2520the%2520proposed%2520method%2520significantly%2520outperforms%2520baselines%252C%2520setting%2520a%2520new%250Astate-of-the-art%2520performance%2520for%2520data-free%2520ViT%2520quantization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20021v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MimiQ%3A%20Low-Bit%20Data-Free%20Quantization%20of%20Vision%20Transformers%20with%0A%20%20Encouraging%20Inter-Head%20Attention%20Similarity&entry.906535625=Kanghyun%20Choi%20and%20Hye%20Yoon%20Lee%20and%20Dain%20Kwon%20and%20SunJong%20Park%20and%20Kyuyeun%20Kim%20and%20Noseong%20Park%20and%20Jinho%20Lee&entry.1292438233=%20%20Data-free%20quantization%20%28DFQ%29%20is%20a%20technique%20that%20creates%20a%20lightweight%0Anetwork%20from%20its%20full-precision%20counterpart%20without%20the%20original%20training%20data%2C%0Aoften%20through%20a%20synthetic%20dataset.%20Although%20several%20DFQ%20methods%20have%20been%0Aproposed%20for%20vision%20transformer%20%28ViT%29%20architectures%2C%20they%20fail%20to%20achieve%0Aefficacy%20in%20low-bit%20settings.%20Examining%20the%20existing%20methods%2C%20we%20identify%20that%0Atheir%20synthetic%20data%20produce%20misaligned%20attention%20maps%2C%20while%20those%20of%20the%20real%0Asamples%20are%20highly%20aligned.%20From%20the%20observation%20of%20aligned%20attention%2C%20we%20find%0Athat%20aligning%20attention%20maps%20of%20synthetic%20data%20helps%20to%20improve%20the%20overall%0Aperformance%20of%20quantized%20ViTs.%20Motivated%20by%20this%20finding%2C%20we%20devise%20MimiQ%2C%20a%0Anovel%20DFQ%20method%20designed%20for%20ViTs%20that%20focuses%20on%20inter-head%20attention%0Asimilarity.%20First%2C%20we%20generate%20synthetic%20data%20by%20aligning%20head-wise%20attention%0Aresponses%20in%20relation%20to%20spatial%20query%20patches.%20Then%2C%20we%20apply%20head-wise%0Astructural%20attention%20distillation%20to%20align%20the%20attention%20maps%20of%20the%20quantized%0Anetwork%20to%20those%20of%20the%20full-precision%20teacher.%20The%20experimental%20results%20show%0Athat%20the%20proposed%20method%20significantly%20outperforms%20baselines%2C%20setting%20a%20new%0Astate-of-the-art%20performance%20for%20data-free%20ViT%20quantization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20021v3&entry.124074799=Read"},
{"title": "What makes for good morphology representations for spatial omics?", "author": "Eduard Chelebian and Christophe Avenel and Carolina W\u00e4hlby", "abstract": "  Spatial omics has transformed our understanding of tissue architecture by\npreserving spatial context of gene expression patterns. Simultaneously,\nadvances in imaging AI have enabled extraction of morphological features\ndescribing the tissue. The intersection of spatial omics and imaging AI\npresents opportunities for a more holistic understanding. In this review we\nintroduce a framework for categorizing spatial omics-morphology combination\nmethods, focusing on how morphological features can be translated or integrated\ninto spatial omics analyses. By translation we mean finding morphological\nfeatures that spatially correlate with gene expression patterns with the\npurpose of predicting gene expression. Such features can be used to generate\nsuper-resolution gene expression maps or infer genetic information from\nclinical H&E-stained samples. By integration we mean finding morphological\nfeatures that spatially complement gene expression patterns with the purpose of\nenriching information. Such features can be used to define spatial domains,\nespecially where gene expression has preceded morphological changes and where\nmorphology remains after gene expression. We discuss learning strategies and\ndirections for further development of the field.\n", "link": "http://arxiv.org/abs/2407.20660v2", "date": "2024-08-01", "relevancy": 2.36, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5032}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4642}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20makes%20for%20good%20morphology%20representations%20for%20spatial%20omics%3F&body=Title%3A%20What%20makes%20for%20good%20morphology%20representations%20for%20spatial%20omics%3F%0AAuthor%3A%20Eduard%20Chelebian%20and%20Christophe%20Avenel%20and%20Carolina%20W%C3%A4hlby%0AAbstract%3A%20%20%20Spatial%20omics%20has%20transformed%20our%20understanding%20of%20tissue%20architecture%20by%0Apreserving%20spatial%20context%20of%20gene%20expression%20patterns.%20Simultaneously%2C%0Aadvances%20in%20imaging%20AI%20have%20enabled%20extraction%20of%20morphological%20features%0Adescribing%20the%20tissue.%20The%20intersection%20of%20spatial%20omics%20and%20imaging%20AI%0Apresents%20opportunities%20for%20a%20more%20holistic%20understanding.%20In%20this%20review%20we%0Aintroduce%20a%20framework%20for%20categorizing%20spatial%20omics-morphology%20combination%0Amethods%2C%20focusing%20on%20how%20morphological%20features%20can%20be%20translated%20or%20integrated%0Ainto%20spatial%20omics%20analyses.%20By%20translation%20we%20mean%20finding%20morphological%0Afeatures%20that%20spatially%20correlate%20with%20gene%20expression%20patterns%20with%20the%0Apurpose%20of%20predicting%20gene%20expression.%20Such%20features%20can%20be%20used%20to%20generate%0Asuper-resolution%20gene%20expression%20maps%20or%20infer%20genetic%20information%20from%0Aclinical%20H%26E-stained%20samples.%20By%20integration%20we%20mean%20finding%20morphological%0Afeatures%20that%20spatially%20complement%20gene%20expression%20patterns%20with%20the%20purpose%20of%0Aenriching%20information.%20Such%20features%20can%20be%20used%20to%20define%20spatial%20domains%2C%0Aespecially%20where%20gene%20expression%20has%20preceded%20morphological%20changes%20and%20where%0Amorphology%20remains%20after%20gene%20expression.%20We%20discuss%20learning%20strategies%20and%0Adirections%20for%20further%20development%20of%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520makes%2520for%2520good%2520morphology%2520representations%2520for%2520spatial%2520omics%253F%26entry.906535625%3DEduard%2520Chelebian%2520and%2520Christophe%2520Avenel%2520and%2520Carolina%2520W%25C3%25A4hlby%26entry.1292438233%3D%2520%2520Spatial%2520omics%2520has%2520transformed%2520our%2520understanding%2520of%2520tissue%2520architecture%2520by%250Apreserving%2520spatial%2520context%2520of%2520gene%2520expression%2520patterns.%2520Simultaneously%252C%250Aadvances%2520in%2520imaging%2520AI%2520have%2520enabled%2520extraction%2520of%2520morphological%2520features%250Adescribing%2520the%2520tissue.%2520The%2520intersection%2520of%2520spatial%2520omics%2520and%2520imaging%2520AI%250Apresents%2520opportunities%2520for%2520a%2520more%2520holistic%2520understanding.%2520In%2520this%2520review%2520we%250Aintroduce%2520a%2520framework%2520for%2520categorizing%2520spatial%2520omics-morphology%2520combination%250Amethods%252C%2520focusing%2520on%2520how%2520morphological%2520features%2520can%2520be%2520translated%2520or%2520integrated%250Ainto%2520spatial%2520omics%2520analyses.%2520By%2520translation%2520we%2520mean%2520finding%2520morphological%250Afeatures%2520that%2520spatially%2520correlate%2520with%2520gene%2520expression%2520patterns%2520with%2520the%250Apurpose%2520of%2520predicting%2520gene%2520expression.%2520Such%2520features%2520can%2520be%2520used%2520to%2520generate%250Asuper-resolution%2520gene%2520expression%2520maps%2520or%2520infer%2520genetic%2520information%2520from%250Aclinical%2520H%2526E-stained%2520samples.%2520By%2520integration%2520we%2520mean%2520finding%2520morphological%250Afeatures%2520that%2520spatially%2520complement%2520gene%2520expression%2520patterns%2520with%2520the%2520purpose%2520of%250Aenriching%2520information.%2520Such%2520features%2520can%2520be%2520used%2520to%2520define%2520spatial%2520domains%252C%250Aespecially%2520where%2520gene%2520expression%2520has%2520preceded%2520morphological%2520changes%2520and%2520where%250Amorphology%2520remains%2520after%2520gene%2520expression.%2520We%2520discuss%2520learning%2520strategies%2520and%250Adirections%2520for%2520further%2520development%2520of%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20makes%20for%20good%20morphology%20representations%20for%20spatial%20omics%3F&entry.906535625=Eduard%20Chelebian%20and%20Christophe%20Avenel%20and%20Carolina%20W%C3%A4hlby&entry.1292438233=%20%20Spatial%20omics%20has%20transformed%20our%20understanding%20of%20tissue%20architecture%20by%0Apreserving%20spatial%20context%20of%20gene%20expression%20patterns.%20Simultaneously%2C%0Aadvances%20in%20imaging%20AI%20have%20enabled%20extraction%20of%20morphological%20features%0Adescribing%20the%20tissue.%20The%20intersection%20of%20spatial%20omics%20and%20imaging%20AI%0Apresents%20opportunities%20for%20a%20more%20holistic%20understanding.%20In%20this%20review%20we%0Aintroduce%20a%20framework%20for%20categorizing%20spatial%20omics-morphology%20combination%0Amethods%2C%20focusing%20on%20how%20morphological%20features%20can%20be%20translated%20or%20integrated%0Ainto%20spatial%20omics%20analyses.%20By%20translation%20we%20mean%20finding%20morphological%0Afeatures%20that%20spatially%20correlate%20with%20gene%20expression%20patterns%20with%20the%0Apurpose%20of%20predicting%20gene%20expression.%20Such%20features%20can%20be%20used%20to%20generate%0Asuper-resolution%20gene%20expression%20maps%20or%20infer%20genetic%20information%20from%0Aclinical%20H%26E-stained%20samples.%20By%20integration%20we%20mean%20finding%20morphological%0Afeatures%20that%20spatially%20complement%20gene%20expression%20patterns%20with%20the%20purpose%20of%0Aenriching%20information.%20Such%20features%20can%20be%20used%20to%20define%20spatial%20domains%2C%0Aespecially%20where%20gene%20expression%20has%20preceded%20morphological%20changes%20and%20where%0Amorphology%20remains%20after%20gene%20expression.%20We%20discuss%20learning%20strategies%20and%0Adirections%20for%20further%20development%20of%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20660v2&entry.124074799=Read"},
{"title": "Collaborative Vision-Text Representation Optimizing for Open-Vocabulary\n  Segmentation", "author": "Siyu Jiao and Hongguang Zhu and Jiannan Huang and Yao Zhao and Yunchao Wei and Humphrey Shi", "abstract": "  Pre-trained vision-language models, e.g. CLIP, have been increasingly used to\naddress the challenging Open-Vocabulary Segmentation (OVS) task, benefiting\nfrom their well-aligned vision-text embedding space. Typical solutions involve\neither freezing CLIP during training to unilaterally maintain its zero-shot\ncapability, or fine-tuning CLIP vision encoder to achieve perceptual\nsensitivity to local regions. However, few of them incorporate vision-text\ncollaborative optimization. Based on this, we propose the Content-Dependent\nTransfer to adaptively enhance each text embedding by interacting with the\ninput image, which presents a parameter-efficient way to optimize the text\nrepresentation. Besides, we additionally introduce a Representation\nCompensation strategy, reviewing the original CLIP-V representation as\ncompensation to maintain the zero-shot capability of CLIP. In this way, the\nvision and text representation of CLIP are optimized collaboratively, enhancing\nthe alignment of the vision-text feature space. To the best of our knowledge,\nwe are the first to establish the collaborative vision-text optimizing\nmechanism within the OVS field. Extensive experiments demonstrate our method\nachieves superior performance on popular OVS benchmarks. In open-vocabulary\nsemantic segmentation, our method outperforms the previous state-of-the-art\napproaches by +0.5, +2.3, +3.4, +0.4 and +1.1 mIoU, respectively on A-847,\nA-150, PC-459, PC-59 and PAS-20. Furthermore, in a panoptic setting on ADE20K,\nwe achieve the performance of 27.1 PQ, 73.5 SQ, and 32.9 RQ. Code will be\navailable at https://github.com/jiaosiyu1999/MAFT-Plus.git .\n", "link": "http://arxiv.org/abs/2408.00744v1", "date": "2024-08-01", "relevancy": 2.335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6231}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5559}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Vision-Text%20Representation%20Optimizing%20for%20Open-Vocabulary%0A%20%20Segmentation&body=Title%3A%20Collaborative%20Vision-Text%20Representation%20Optimizing%20for%20Open-Vocabulary%0A%20%20Segmentation%0AAuthor%3A%20Siyu%20Jiao%20and%20Hongguang%20Zhu%20and%20Jiannan%20Huang%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%20and%20Humphrey%20Shi%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%2C%20e.g.%20CLIP%2C%20have%20been%20increasingly%20used%20to%0Aaddress%20the%20challenging%20Open-Vocabulary%20Segmentation%20%28OVS%29%20task%2C%20benefiting%0Afrom%20their%20well-aligned%20vision-text%20embedding%20space.%20Typical%20solutions%20involve%0Aeither%20freezing%20CLIP%20during%20training%20to%20unilaterally%20maintain%20its%20zero-shot%0Acapability%2C%20or%20fine-tuning%20CLIP%20vision%20encoder%20to%20achieve%20perceptual%0Asensitivity%20to%20local%20regions.%20However%2C%20few%20of%20them%20incorporate%20vision-text%0Acollaborative%20optimization.%20Based%20on%20this%2C%20we%20propose%20the%20Content-Dependent%0ATransfer%20to%20adaptively%20enhance%20each%20text%20embedding%20by%20interacting%20with%20the%0Ainput%20image%2C%20which%20presents%20a%20parameter-efficient%20way%20to%20optimize%20the%20text%0Arepresentation.%20Besides%2C%20we%20additionally%20introduce%20a%20Representation%0ACompensation%20strategy%2C%20reviewing%20the%20original%20CLIP-V%20representation%20as%0Acompensation%20to%20maintain%20the%20zero-shot%20capability%20of%20CLIP.%20In%20this%20way%2C%20the%0Avision%20and%20text%20representation%20of%20CLIP%20are%20optimized%20collaboratively%2C%20enhancing%0Athe%20alignment%20of%20the%20vision-text%20feature%20space.%20To%20the%20best%20of%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20establish%20the%20collaborative%20vision-text%20optimizing%0Amechanism%20within%20the%20OVS%20field.%20Extensive%20experiments%20demonstrate%20our%20method%0Aachieves%20superior%20performance%20on%20popular%20OVS%20benchmarks.%20In%20open-vocabulary%0Asemantic%20segmentation%2C%20our%20method%20outperforms%20the%20previous%20state-of-the-art%0Aapproaches%20by%20%2B0.5%2C%20%2B2.3%2C%20%2B3.4%2C%20%2B0.4%20and%20%2B1.1%20mIoU%2C%20respectively%20on%20A-847%2C%0AA-150%2C%20PC-459%2C%20PC-59%20and%20PAS-20.%20Furthermore%2C%20in%20a%20panoptic%20setting%20on%20ADE20K%2C%0Awe%20achieve%20the%20performance%20of%2027.1%20PQ%2C%2073.5%20SQ%2C%20and%2032.9%20RQ.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/jiaosiyu1999/MAFT-Plus.git%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Vision-Text%2520Representation%2520Optimizing%2520for%2520Open-Vocabulary%250A%2520%2520Segmentation%26entry.906535625%3DSiyu%2520Jiao%2520and%2520Hongguang%2520Zhu%2520and%2520Jiannan%2520Huang%2520and%2520Yao%2520Zhao%2520and%2520Yunchao%2520Wei%2520and%2520Humphrey%2520Shi%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%252C%2520e.g.%2520CLIP%252C%2520have%2520been%2520increasingly%2520used%2520to%250Aaddress%2520the%2520challenging%2520Open-Vocabulary%2520Segmentation%2520%2528OVS%2529%2520task%252C%2520benefiting%250Afrom%2520their%2520well-aligned%2520vision-text%2520embedding%2520space.%2520Typical%2520solutions%2520involve%250Aeither%2520freezing%2520CLIP%2520during%2520training%2520to%2520unilaterally%2520maintain%2520its%2520zero-shot%250Acapability%252C%2520or%2520fine-tuning%2520CLIP%2520vision%2520encoder%2520to%2520achieve%2520perceptual%250Asensitivity%2520to%2520local%2520regions.%2520However%252C%2520few%2520of%2520them%2520incorporate%2520vision-text%250Acollaborative%2520optimization.%2520Based%2520on%2520this%252C%2520we%2520propose%2520the%2520Content-Dependent%250ATransfer%2520to%2520adaptively%2520enhance%2520each%2520text%2520embedding%2520by%2520interacting%2520with%2520the%250Ainput%2520image%252C%2520which%2520presents%2520a%2520parameter-efficient%2520way%2520to%2520optimize%2520the%2520text%250Arepresentation.%2520Besides%252C%2520we%2520additionally%2520introduce%2520a%2520Representation%250ACompensation%2520strategy%252C%2520reviewing%2520the%2520original%2520CLIP-V%2520representation%2520as%250Acompensation%2520to%2520maintain%2520the%2520zero-shot%2520capability%2520of%2520CLIP.%2520In%2520this%2520way%252C%2520the%250Avision%2520and%2520text%2520representation%2520of%2520CLIP%2520are%2520optimized%2520collaboratively%252C%2520enhancing%250Athe%2520alignment%2520of%2520the%2520vision-text%2520feature%2520space.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Awe%2520are%2520the%2520first%2520to%2520establish%2520the%2520collaborative%2520vision-text%2520optimizing%250Amechanism%2520within%2520the%2520OVS%2520field.%2520Extensive%2520experiments%2520demonstrate%2520our%2520method%250Aachieves%2520superior%2520performance%2520on%2520popular%2520OVS%2520benchmarks.%2520In%2520open-vocabulary%250Asemantic%2520segmentation%252C%2520our%2520method%2520outperforms%2520the%2520previous%2520state-of-the-art%250Aapproaches%2520by%2520%252B0.5%252C%2520%252B2.3%252C%2520%252B3.4%252C%2520%252B0.4%2520and%2520%252B1.1%2520mIoU%252C%2520respectively%2520on%2520A-847%252C%250AA-150%252C%2520PC-459%252C%2520PC-59%2520and%2520PAS-20.%2520Furthermore%252C%2520in%2520a%2520panoptic%2520setting%2520on%2520ADE20K%252C%250Awe%2520achieve%2520the%2520performance%2520of%252027.1%2520PQ%252C%252073.5%2520SQ%252C%2520and%252032.9%2520RQ.%2520Code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/jiaosiyu1999/MAFT-Plus.git%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Vision-Text%20Representation%20Optimizing%20for%20Open-Vocabulary%0A%20%20Segmentation&entry.906535625=Siyu%20Jiao%20and%20Hongguang%20Zhu%20and%20Jiannan%20Huang%20and%20Yao%20Zhao%20and%20Yunchao%20Wei%20and%20Humphrey%20Shi&entry.1292438233=%20%20Pre-trained%20vision-language%20models%2C%20e.g.%20CLIP%2C%20have%20been%20increasingly%20used%20to%0Aaddress%20the%20challenging%20Open-Vocabulary%20Segmentation%20%28OVS%29%20task%2C%20benefiting%0Afrom%20their%20well-aligned%20vision-text%20embedding%20space.%20Typical%20solutions%20involve%0Aeither%20freezing%20CLIP%20during%20training%20to%20unilaterally%20maintain%20its%20zero-shot%0Acapability%2C%20or%20fine-tuning%20CLIP%20vision%20encoder%20to%20achieve%20perceptual%0Asensitivity%20to%20local%20regions.%20However%2C%20few%20of%20them%20incorporate%20vision-text%0Acollaborative%20optimization.%20Based%20on%20this%2C%20we%20propose%20the%20Content-Dependent%0ATransfer%20to%20adaptively%20enhance%20each%20text%20embedding%20by%20interacting%20with%20the%0Ainput%20image%2C%20which%20presents%20a%20parameter-efficient%20way%20to%20optimize%20the%20text%0Arepresentation.%20Besides%2C%20we%20additionally%20introduce%20a%20Representation%0ACompensation%20strategy%2C%20reviewing%20the%20original%20CLIP-V%20representation%20as%0Acompensation%20to%20maintain%20the%20zero-shot%20capability%20of%20CLIP.%20In%20this%20way%2C%20the%0Avision%20and%20text%20representation%20of%20CLIP%20are%20optimized%20collaboratively%2C%20enhancing%0Athe%20alignment%20of%20the%20vision-text%20feature%20space.%20To%20the%20best%20of%20our%20knowledge%2C%0Awe%20are%20the%20first%20to%20establish%20the%20collaborative%20vision-text%20optimizing%0Amechanism%20within%20the%20OVS%20field.%20Extensive%20experiments%20demonstrate%20our%20method%0Aachieves%20superior%20performance%20on%20popular%20OVS%20benchmarks.%20In%20open-vocabulary%0Asemantic%20segmentation%2C%20our%20method%20outperforms%20the%20previous%20state-of-the-art%0Aapproaches%20by%20%2B0.5%2C%20%2B2.3%2C%20%2B3.4%2C%20%2B0.4%20and%20%2B1.1%20mIoU%2C%20respectively%20on%20A-847%2C%0AA-150%2C%20PC-459%2C%20PC-59%20and%20PAS-20.%20Furthermore%2C%20in%20a%20panoptic%20setting%20on%20ADE20K%2C%0Awe%20achieve%20the%20performance%20of%2027.1%20PQ%2C%2073.5%20SQ%2C%20and%2032.9%20RQ.%20Code%20will%20be%0Aavailable%20at%20https%3A//github.com/jiaosiyu1999/MAFT-Plus.git%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00744v1&entry.124074799=Read"},
{"title": "Enhancing convolutional neural network generalizability via low-rank\n  weight approximation", "author": "Chenyin Gao and Shu Yang and Anru R. Zhang", "abstract": "  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n", "link": "http://arxiv.org/abs/2209.12715v2", "date": "2024-08-01", "relevancy": 2.3283, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6004}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5746}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20convolutional%20neural%20network%20generalizability%20via%20low-rank%0A%20%20weight%20approximation&body=Title%3A%20Enhancing%20convolutional%20neural%20network%20generalizability%20via%20low-rank%0A%20%20weight%20approximation%0AAuthor%3A%20Chenyin%20Gao%20and%20Shu%20Yang%20and%20Anru%20R.%20Zhang%0AAbstract%3A%20%20%20Noise%20is%20ubiquitous%20during%20image%20acquisition.%20Sufficient%20denoising%20is%20often%0Aan%20important%20first%20step%20for%20image%20processing.%20In%20recent%20decades%2C%20deep%20neural%0Anetworks%20%28DNNs%29%20have%20been%20widely%20used%20for%20image%20denoising.%20Most%20DNN-based%20image%0Adenoising%20methods%20require%20a%20large-scale%20dataset%20or%20focus%20on%20supervised%0Asettings%2C%20in%20which%20single/pairs%20of%20clean%20images%20or%20a%20set%20of%20noisy%20images%20are%0Arequired.%20This%20poses%20a%20significant%20burden%20on%20the%20image%20acquisition%20process.%0AMoreover%2C%20denoisers%20trained%20on%20datasets%20of%20limited%20scale%20may%20incur%0Aover-fitting.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20a%20new%20self-supervised%0Aframework%20for%20image%20denoising%20based%20on%20the%20Tucker%20low-rank%20tensor%0Aapproximation.%20With%20the%20proposed%20design%2C%20we%20are%20able%20to%20characterize%20our%0Adenoiser%20with%20fewer%20parameters%20and%20train%20it%20based%20on%20a%20single%20image%2C%20which%0Aconsiderably%20improves%20the%20model%27s%20generalizability%20and%20reduces%20the%20cost%20of%20data%0Aacquisition.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20noisy%0Aimages%20have%20been%20conducted.%20Empirical%20results%20show%20that%20our%20proposed%20method%0Aoutperforms%20existing%20non-learning-based%20methods%20%28e.g.%2C%20low-pass%20filter%2C%0Anon-local%20mean%29%2C%20single-image%20unsupervised%20denoisers%20%28e.g.%2C%20DIP%2C%20NN%2BBM3D%29%0Aevaluated%20on%20both%20in-sample%20and%20out-sample%20datasets.%20The%20proposed%20method%20even%0Aachieves%20comparable%20performances%20with%20some%20supervised%20methods%20%28e.g.%2C%20DnCNN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.12715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520convolutional%2520neural%2520network%2520generalizability%2520via%2520low-rank%250A%2520%2520weight%2520approximation%26entry.906535625%3DChenyin%2520Gao%2520and%2520Shu%2520Yang%2520and%2520Anru%2520R.%2520Zhang%26entry.1292438233%3D%2520%2520Noise%2520is%2520ubiquitous%2520during%2520image%2520acquisition.%2520Sufficient%2520denoising%2520is%2520often%250Aan%2520important%2520first%2520step%2520for%2520image%2520processing.%2520In%2520recent%2520decades%252C%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529%2520have%2520been%2520widely%2520used%2520for%2520image%2520denoising.%2520Most%2520DNN-based%2520image%250Adenoising%2520methods%2520require%2520a%2520large-scale%2520dataset%2520or%2520focus%2520on%2520supervised%250Asettings%252C%2520in%2520which%2520single/pairs%2520of%2520clean%2520images%2520or%2520a%2520set%2520of%2520noisy%2520images%2520are%250Arequired.%2520This%2520poses%2520a%2520significant%2520burden%2520on%2520the%2520image%2520acquisition%2520process.%250AMoreover%252C%2520denoisers%2520trained%2520on%2520datasets%2520of%2520limited%2520scale%2520may%2520incur%250Aover-fitting.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520introduce%2520a%2520new%2520self-supervised%250Aframework%2520for%2520image%2520denoising%2520based%2520on%2520the%2520Tucker%2520low-rank%2520tensor%250Aapproximation.%2520With%2520the%2520proposed%2520design%252C%2520we%2520are%2520able%2520to%2520characterize%2520our%250Adenoiser%2520with%2520fewer%2520parameters%2520and%2520train%2520it%2520based%2520on%2520a%2520single%2520image%252C%2520which%250Aconsiderably%2520improves%2520the%2520model%2527s%2520generalizability%2520and%2520reduces%2520the%2520cost%2520of%2520data%250Aacquisition.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520noisy%250Aimages%2520have%2520been%2520conducted.%2520Empirical%2520results%2520show%2520that%2520our%2520proposed%2520method%250Aoutperforms%2520existing%2520non-learning-based%2520methods%2520%2528e.g.%252C%2520low-pass%2520filter%252C%250Anon-local%2520mean%2529%252C%2520single-image%2520unsupervised%2520denoisers%2520%2528e.g.%252C%2520DIP%252C%2520NN%252BBM3D%2529%250Aevaluated%2520on%2520both%2520in-sample%2520and%2520out-sample%2520datasets.%2520The%2520proposed%2520method%2520even%250Aachieves%2520comparable%2520performances%2520with%2520some%2520supervised%2520methods%2520%2528e.g.%252C%2520DnCNN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.12715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20convolutional%20neural%20network%20generalizability%20via%20low-rank%0A%20%20weight%20approximation&entry.906535625=Chenyin%20Gao%20and%20Shu%20Yang%20and%20Anru%20R.%20Zhang&entry.1292438233=%20%20Noise%20is%20ubiquitous%20during%20image%20acquisition.%20Sufficient%20denoising%20is%20often%0Aan%20important%20first%20step%20for%20image%20processing.%20In%20recent%20decades%2C%20deep%20neural%0Anetworks%20%28DNNs%29%20have%20been%20widely%20used%20for%20image%20denoising.%20Most%20DNN-based%20image%0Adenoising%20methods%20require%20a%20large-scale%20dataset%20or%20focus%20on%20supervised%0Asettings%2C%20in%20which%20single/pairs%20of%20clean%20images%20or%20a%20set%20of%20noisy%20images%20are%0Arequired.%20This%20poses%20a%20significant%20burden%20on%20the%20image%20acquisition%20process.%0AMoreover%2C%20denoisers%20trained%20on%20datasets%20of%20limited%20scale%20may%20incur%0Aover-fitting.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20a%20new%20self-supervised%0Aframework%20for%20image%20denoising%20based%20on%20the%20Tucker%20low-rank%20tensor%0Aapproximation.%20With%20the%20proposed%20design%2C%20we%20are%20able%20to%20characterize%20our%0Adenoiser%20with%20fewer%20parameters%20and%20train%20it%20based%20on%20a%20single%20image%2C%20which%0Aconsiderably%20improves%20the%20model%27s%20generalizability%20and%20reduces%20the%20cost%20of%20data%0Aacquisition.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20noisy%0Aimages%20have%20been%20conducted.%20Empirical%20results%20show%20that%20our%20proposed%20method%0Aoutperforms%20existing%20non-learning-based%20methods%20%28e.g.%2C%20low-pass%20filter%2C%0Anon-local%20mean%29%2C%20single-image%20unsupervised%20denoisers%20%28e.g.%2C%20DIP%2C%20NN%2BBM3D%29%0Aevaluated%20on%20both%20in-sample%20and%20out-sample%20datasets.%20The%20proposed%20method%20even%0Aachieves%20comparable%20performances%20with%20some%20supervised%20methods%20%28e.g.%2C%20DnCNN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.12715v2&entry.124074799=Read"},
{"title": "AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation", "author": "Mengkang Hu and Pu Zhao and Can Xu and Qingfeng Sun and Jianguang Lou and Qingwei Lin and Ping Luo and Saravan Rajmohan and Dongmei Zhang", "abstract": "  Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.\n", "link": "http://arxiv.org/abs/2408.00764v1", "date": "2024-08-01", "relevancy": 2.3188, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6024}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5701}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentGen%3A%20Enhancing%20Planning%20Abilities%20for%20Large%20Language%20Model%20based%0A%20%20Agent%20via%20Environment%20and%20Task%20Generation&body=Title%3A%20AgentGen%3A%20Enhancing%20Planning%20Abilities%20for%20Large%20Language%20Model%20based%0A%20%20Agent%20via%20Environment%20and%20Task%20Generation%0AAuthor%3A%20Mengkang%20Hu%20and%20Pu%20Zhao%20and%20Can%20Xu%20and%20Qingfeng%20Sun%20and%20Jianguang%20Lou%20and%20Qingwei%20Lin%20and%20Ping%20Luo%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20based%20agents%20have%20garnered%20significant%20attention%0Aand%20are%20becoming%20increasingly%20popular.%20Furthermore%2C%20planning%20ability%20is%20a%0Acrucial%20component%20of%20an%20LLM-based%20agent%2C%20involving%20interaction%20with%20the%0Aenvironment%20and%20executing%20actions%20to%20complete%20a%20planning%20task%2C%20which%20generally%0Aentails%20achieving%20a%20desired%20goal%20from%20an%20initial%20state.%20This%20paper%20investigates%0Aenhancing%20the%20planning%20abilities%20of%20LLMs%20through%20instruction%20tuning%2C%20referred%0Ato%20as%20agent%20training.%20Recent%20studies%20have%20demonstrated%20that%20utilizing%0Aexpert-level%20trajectory%20for%20instruction-tuning%20LLMs%20effectively%20enhances%20their%0Aplanning%20capabilities.%20However%2C%20existing%20work%20primarily%20focuses%20on%20synthesizing%0Atrajectories%20from%20manually%20designed%20planning%20tasks%20and%20environments.%20The%0Alabor-intensive%20nature%20of%20creating%20these%20environments%20and%20tasks%20impedes%20the%0Ageneration%20of%20sufficiently%20varied%20and%20extensive%20trajectories.%20To%20address%20this%0Alimitation%2C%20this%20paper%20explores%20the%20automated%20synthesis%20of%20diverse%20environments%0Aand%20a%20gradual%20range%20of%20planning%20tasks%2C%20from%20easy%20to%20difficult.%20We%20introduce%20a%0Aframework%2C%20AgentGen%2C%20that%20leverages%20LLMs%20first%20to%20generate%20environments%20and%0Asubsequently%20generate%20planning%20tasks%20conditioned%20on%20these%20environments.%0ASpecifically%2C%20to%20improve%20environmental%20diversity%2C%20we%20propose%20using%20an%0Ainspiration%20corpus%20composed%20of%20various%20domain-specific%20text%20segments%20as%20the%0Acontext%20for%20synthesizing%20environments.%20Moreover%2C%20to%20increase%20the%20difficulty%0Adiversity%20of%20generated%20planning%20tasks%2C%20we%20propose%20a%20bidirectional%20evolution%0Amethod%2C%20Bi-Evol%2C%20that%20evolves%20planning%20tasks%20from%20easier%20and%20harder%20directions%0Ato%20synthesize%20a%20task%20set%20with%20a%20smoother%20difficulty%20curve.%20The%20evaluation%0Aresults%20derived%20from%20AgentBoard%20show%20that%20AgentGen%20greatly%20improves%20LLMs%27%0Aplanning%20ability%2C%20e.g.%2C%20the%20AgentGen%20instruction-tuned%20Llama-3%208B%20surpasses%0AGPT-3.5%20in%20overall%20performance.%20Moreover%2C%20in%20certain%20tasks%2C%20it%20even%20outperforms%0AGPT-4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentGen%253A%2520Enhancing%2520Planning%2520Abilities%2520for%2520Large%2520Language%2520Model%2520based%250A%2520%2520Agent%2520via%2520Environment%2520and%2520Task%2520Generation%26entry.906535625%3DMengkang%2520Hu%2520and%2520Pu%2520Zhao%2520and%2520Can%2520Xu%2520and%2520Qingfeng%2520Sun%2520and%2520Jianguang%2520Lou%2520and%2520Qingwei%2520Lin%2520and%2520Ping%2520Luo%2520and%2520Saravan%2520Rajmohan%2520and%2520Dongmei%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520agents%2520have%2520garnered%2520significant%2520attention%250Aand%2520are%2520becoming%2520increasingly%2520popular.%2520Furthermore%252C%2520planning%2520ability%2520is%2520a%250Acrucial%2520component%2520of%2520an%2520LLM-based%2520agent%252C%2520involving%2520interaction%2520with%2520the%250Aenvironment%2520and%2520executing%2520actions%2520to%2520complete%2520a%2520planning%2520task%252C%2520which%2520generally%250Aentails%2520achieving%2520a%2520desired%2520goal%2520from%2520an%2520initial%2520state.%2520This%2520paper%2520investigates%250Aenhancing%2520the%2520planning%2520abilities%2520of%2520LLMs%2520through%2520instruction%2520tuning%252C%2520referred%250Ato%2520as%2520agent%2520training.%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520utilizing%250Aexpert-level%2520trajectory%2520for%2520instruction-tuning%2520LLMs%2520effectively%2520enhances%2520their%250Aplanning%2520capabilities.%2520However%252C%2520existing%2520work%2520primarily%2520focuses%2520on%2520synthesizing%250Atrajectories%2520from%2520manually%2520designed%2520planning%2520tasks%2520and%2520environments.%2520The%250Alabor-intensive%2520nature%2520of%2520creating%2520these%2520environments%2520and%2520tasks%2520impedes%2520the%250Ageneration%2520of%2520sufficiently%2520varied%2520and%2520extensive%2520trajectories.%2520To%2520address%2520this%250Alimitation%252C%2520this%2520paper%2520explores%2520the%2520automated%2520synthesis%2520of%2520diverse%2520environments%250Aand%2520a%2520gradual%2520range%2520of%2520planning%2520tasks%252C%2520from%2520easy%2520to%2520difficult.%2520We%2520introduce%2520a%250Aframework%252C%2520AgentGen%252C%2520that%2520leverages%2520LLMs%2520first%2520to%2520generate%2520environments%2520and%250Asubsequently%2520generate%2520planning%2520tasks%2520conditioned%2520on%2520these%2520environments.%250ASpecifically%252C%2520to%2520improve%2520environmental%2520diversity%252C%2520we%2520propose%2520using%2520an%250Ainspiration%2520corpus%2520composed%2520of%2520various%2520domain-specific%2520text%2520segments%2520as%2520the%250Acontext%2520for%2520synthesizing%2520environments.%2520Moreover%252C%2520to%2520increase%2520the%2520difficulty%250Adiversity%2520of%2520generated%2520planning%2520tasks%252C%2520we%2520propose%2520a%2520bidirectional%2520evolution%250Amethod%252C%2520Bi-Evol%252C%2520that%2520evolves%2520planning%2520tasks%2520from%2520easier%2520and%2520harder%2520directions%250Ato%2520synthesize%2520a%2520task%2520set%2520with%2520a%2520smoother%2520difficulty%2520curve.%2520The%2520evaluation%250Aresults%2520derived%2520from%2520AgentBoard%2520show%2520that%2520AgentGen%2520greatly%2520improves%2520LLMs%2527%250Aplanning%2520ability%252C%2520e.g.%252C%2520the%2520AgentGen%2520instruction-tuned%2520Llama-3%25208B%2520surpasses%250AGPT-3.5%2520in%2520overall%2520performance.%2520Moreover%252C%2520in%2520certain%2520tasks%252C%2520it%2520even%2520outperforms%250AGPT-4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentGen%3A%20Enhancing%20Planning%20Abilities%20for%20Large%20Language%20Model%20based%0A%20%20Agent%20via%20Environment%20and%20Task%20Generation&entry.906535625=Mengkang%20Hu%20and%20Pu%20Zhao%20and%20Can%20Xu%20and%20Qingfeng%20Sun%20and%20Jianguang%20Lou%20and%20Qingwei%20Lin%20and%20Ping%20Luo%20and%20Saravan%20Rajmohan%20and%20Dongmei%20Zhang&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20based%20agents%20have%20garnered%20significant%20attention%0Aand%20are%20becoming%20increasingly%20popular.%20Furthermore%2C%20planning%20ability%20is%20a%0Acrucial%20component%20of%20an%20LLM-based%20agent%2C%20involving%20interaction%20with%20the%0Aenvironment%20and%20executing%20actions%20to%20complete%20a%20planning%20task%2C%20which%20generally%0Aentails%20achieving%20a%20desired%20goal%20from%20an%20initial%20state.%20This%20paper%20investigates%0Aenhancing%20the%20planning%20abilities%20of%20LLMs%20through%20instruction%20tuning%2C%20referred%0Ato%20as%20agent%20training.%20Recent%20studies%20have%20demonstrated%20that%20utilizing%0Aexpert-level%20trajectory%20for%20instruction-tuning%20LLMs%20effectively%20enhances%20their%0Aplanning%20capabilities.%20However%2C%20existing%20work%20primarily%20focuses%20on%20synthesizing%0Atrajectories%20from%20manually%20designed%20planning%20tasks%20and%20environments.%20The%0Alabor-intensive%20nature%20of%20creating%20these%20environments%20and%20tasks%20impedes%20the%0Ageneration%20of%20sufficiently%20varied%20and%20extensive%20trajectories.%20To%20address%20this%0Alimitation%2C%20this%20paper%20explores%20the%20automated%20synthesis%20of%20diverse%20environments%0Aand%20a%20gradual%20range%20of%20planning%20tasks%2C%20from%20easy%20to%20difficult.%20We%20introduce%20a%0Aframework%2C%20AgentGen%2C%20that%20leverages%20LLMs%20first%20to%20generate%20environments%20and%0Asubsequently%20generate%20planning%20tasks%20conditioned%20on%20these%20environments.%0ASpecifically%2C%20to%20improve%20environmental%20diversity%2C%20we%20propose%20using%20an%0Ainspiration%20corpus%20composed%20of%20various%20domain-specific%20text%20segments%20as%20the%0Acontext%20for%20synthesizing%20environments.%20Moreover%2C%20to%20increase%20the%20difficulty%0Adiversity%20of%20generated%20planning%20tasks%2C%20we%20propose%20a%20bidirectional%20evolution%0Amethod%2C%20Bi-Evol%2C%20that%20evolves%20planning%20tasks%20from%20easier%20and%20harder%20directions%0Ato%20synthesize%20a%20task%20set%20with%20a%20smoother%20difficulty%20curve.%20The%20evaluation%0Aresults%20derived%20from%20AgentBoard%20show%20that%20AgentGen%20greatly%20improves%20LLMs%27%0Aplanning%20ability%2C%20e.g.%2C%20the%20AgentGen%20instruction-tuned%20Llama-3%208B%20surpasses%0AGPT-3.5%20in%20overall%20performance.%20Moreover%2C%20in%20certain%20tasks%2C%20it%20even%20outperforms%0AGPT-4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00764v1&entry.124074799=Read"},
{"title": "Image Super-Resolution with Taylor Expansion Approximation and Large\n  Field Reception", "author": "Jiancong Feng and Yuan-Gen Wang and Mingjie Li and Fengchuang Xing", "abstract": "  Self-similarity techniques are booming in blind super-resolution (SR) due to\naccurate estimation of the degradation types involved in low-resolution images.\nHowever, high-dimensional matrix multiplication within self-similarity\ncomputation prohibitively consumes massive computational costs. We find that\nthe high-dimensional attention map is derived from the matrix multiplication\nbetween Query and Key, followed by a softmax function. This softmax makes the\nmatrix multiplication between Query and Key inseparable, posing a great\nchallenge in simplifying computational complexity. To address this issue, we\nfirst propose a second-order Taylor expansion approximation (STEA) to separate\nthe matrix multiplication of Query and Key, resulting in the complexity\nreduction from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Then, we design a\nmulti-scale large field reception (MLFR) to compensate for the performance\ndegradation caused by STEA. Finally, we apply these two core designs to\nlaboratory and real-world scenarios by constructing LabNet and RealNet,\nrespectively. Extensive experimental results tested on five synthetic datasets\ndemonstrate that our LabNet sets a new benchmark in qualitative and\nquantitative evaluations. Tested on the RealWorld38 dataset, our RealNet\nachieves superior visual quality over existing methods. Ablation studies\nfurther verify the contributions of STEA and MLFR towards both LabNet and\nRealNet frameworks.\n", "link": "http://arxiv.org/abs/2408.00470v1", "date": "2024-08-01", "relevancy": 2.3123, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.585}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5827}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Super-Resolution%20with%20Taylor%20Expansion%20Approximation%20and%20Large%0A%20%20Field%20Reception&body=Title%3A%20Image%20Super-Resolution%20with%20Taylor%20Expansion%20Approximation%20and%20Large%0A%20%20Field%20Reception%0AAuthor%3A%20Jiancong%20Feng%20and%20Yuan-Gen%20Wang%20and%20Mingjie%20Li%20and%20Fengchuang%20Xing%0AAbstract%3A%20%20%20Self-similarity%20techniques%20are%20booming%20in%20blind%20super-resolution%20%28SR%29%20due%20to%0Aaccurate%20estimation%20of%20the%20degradation%20types%20involved%20in%20low-resolution%20images.%0AHowever%2C%20high-dimensional%20matrix%20multiplication%20within%20self-similarity%0Acomputation%20prohibitively%20consumes%20massive%20computational%20costs.%20We%20find%20that%0Athe%20high-dimensional%20attention%20map%20is%20derived%20from%20the%20matrix%20multiplication%0Abetween%20Query%20and%20Key%2C%20followed%20by%20a%20softmax%20function.%20This%20softmax%20makes%20the%0Amatrix%20multiplication%20between%20Query%20and%20Key%20inseparable%2C%20posing%20a%20great%0Achallenge%20in%20simplifying%20computational%20complexity.%20To%20address%20this%20issue%2C%20we%0Afirst%20propose%20a%20second-order%20Taylor%20expansion%20approximation%20%28STEA%29%20to%20separate%0Athe%20matrix%20multiplication%20of%20Query%20and%20Key%2C%20resulting%20in%20the%20complexity%0Areduction%20from%20%24%5Cmathcal%7BO%7D%28N%5E2%29%24%20to%20%24%5Cmathcal%7BO%7D%28N%29%24.%20Then%2C%20we%20design%20a%0Amulti-scale%20large%20field%20reception%20%28MLFR%29%20to%20compensate%20for%20the%20performance%0Adegradation%20caused%20by%20STEA.%20Finally%2C%20we%20apply%20these%20two%20core%20designs%20to%0Alaboratory%20and%20real-world%20scenarios%20by%20constructing%20LabNet%20and%20RealNet%2C%0Arespectively.%20Extensive%20experimental%20results%20tested%20on%20five%20synthetic%20datasets%0Ademonstrate%20that%20our%20LabNet%20sets%20a%20new%20benchmark%20in%20qualitative%20and%0Aquantitative%20evaluations.%20Tested%20on%20the%20RealWorld38%20dataset%2C%20our%20RealNet%0Aachieves%20superior%20visual%20quality%20over%20existing%20methods.%20Ablation%20studies%0Afurther%20verify%20the%20contributions%20of%20STEA%20and%20MLFR%20towards%20both%20LabNet%20and%0ARealNet%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00470v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Super-Resolution%2520with%2520Taylor%2520Expansion%2520Approximation%2520and%2520Large%250A%2520%2520Field%2520Reception%26entry.906535625%3DJiancong%2520Feng%2520and%2520Yuan-Gen%2520Wang%2520and%2520Mingjie%2520Li%2520and%2520Fengchuang%2520Xing%26entry.1292438233%3D%2520%2520Self-similarity%2520techniques%2520are%2520booming%2520in%2520blind%2520super-resolution%2520%2528SR%2529%2520due%2520to%250Aaccurate%2520estimation%2520of%2520the%2520degradation%2520types%2520involved%2520in%2520low-resolution%2520images.%250AHowever%252C%2520high-dimensional%2520matrix%2520multiplication%2520within%2520self-similarity%250Acomputation%2520prohibitively%2520consumes%2520massive%2520computational%2520costs.%2520We%2520find%2520that%250Athe%2520high-dimensional%2520attention%2520map%2520is%2520derived%2520from%2520the%2520matrix%2520multiplication%250Abetween%2520Query%2520and%2520Key%252C%2520followed%2520by%2520a%2520softmax%2520function.%2520This%2520softmax%2520makes%2520the%250Amatrix%2520multiplication%2520between%2520Query%2520and%2520Key%2520inseparable%252C%2520posing%2520a%2520great%250Achallenge%2520in%2520simplifying%2520computational%2520complexity.%2520To%2520address%2520this%2520issue%252C%2520we%250Afirst%2520propose%2520a%2520second-order%2520Taylor%2520expansion%2520approximation%2520%2528STEA%2529%2520to%2520separate%250Athe%2520matrix%2520multiplication%2520of%2520Query%2520and%2520Key%252C%2520resulting%2520in%2520the%2520complexity%250Areduction%2520from%2520%2524%255Cmathcal%257BO%257D%2528N%255E2%2529%2524%2520to%2520%2524%255Cmathcal%257BO%257D%2528N%2529%2524.%2520Then%252C%2520we%2520design%2520a%250Amulti-scale%2520large%2520field%2520reception%2520%2528MLFR%2529%2520to%2520compensate%2520for%2520the%2520performance%250Adegradation%2520caused%2520by%2520STEA.%2520Finally%252C%2520we%2520apply%2520these%2520two%2520core%2520designs%2520to%250Alaboratory%2520and%2520real-world%2520scenarios%2520by%2520constructing%2520LabNet%2520and%2520RealNet%252C%250Arespectively.%2520Extensive%2520experimental%2520results%2520tested%2520on%2520five%2520synthetic%2520datasets%250Ademonstrate%2520that%2520our%2520LabNet%2520sets%2520a%2520new%2520benchmark%2520in%2520qualitative%2520and%250Aquantitative%2520evaluations.%2520Tested%2520on%2520the%2520RealWorld38%2520dataset%252C%2520our%2520RealNet%250Aachieves%2520superior%2520visual%2520quality%2520over%2520existing%2520methods.%2520Ablation%2520studies%250Afurther%2520verify%2520the%2520contributions%2520of%2520STEA%2520and%2520MLFR%2520towards%2520both%2520LabNet%2520and%250ARealNet%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00470v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Super-Resolution%20with%20Taylor%20Expansion%20Approximation%20and%20Large%0A%20%20Field%20Reception&entry.906535625=Jiancong%20Feng%20and%20Yuan-Gen%20Wang%20and%20Mingjie%20Li%20and%20Fengchuang%20Xing&entry.1292438233=%20%20Self-similarity%20techniques%20are%20booming%20in%20blind%20super-resolution%20%28SR%29%20due%20to%0Aaccurate%20estimation%20of%20the%20degradation%20types%20involved%20in%20low-resolution%20images.%0AHowever%2C%20high-dimensional%20matrix%20multiplication%20within%20self-similarity%0Acomputation%20prohibitively%20consumes%20massive%20computational%20costs.%20We%20find%20that%0Athe%20high-dimensional%20attention%20map%20is%20derived%20from%20the%20matrix%20multiplication%0Abetween%20Query%20and%20Key%2C%20followed%20by%20a%20softmax%20function.%20This%20softmax%20makes%20the%0Amatrix%20multiplication%20between%20Query%20and%20Key%20inseparable%2C%20posing%20a%20great%0Achallenge%20in%20simplifying%20computational%20complexity.%20To%20address%20this%20issue%2C%20we%0Afirst%20propose%20a%20second-order%20Taylor%20expansion%20approximation%20%28STEA%29%20to%20separate%0Athe%20matrix%20multiplication%20of%20Query%20and%20Key%2C%20resulting%20in%20the%20complexity%0Areduction%20from%20%24%5Cmathcal%7BO%7D%28N%5E2%29%24%20to%20%24%5Cmathcal%7BO%7D%28N%29%24.%20Then%2C%20we%20design%20a%0Amulti-scale%20large%20field%20reception%20%28MLFR%29%20to%20compensate%20for%20the%20performance%0Adegradation%20caused%20by%20STEA.%20Finally%2C%20we%20apply%20these%20two%20core%20designs%20to%0Alaboratory%20and%20real-world%20scenarios%20by%20constructing%20LabNet%20and%20RealNet%2C%0Arespectively.%20Extensive%20experimental%20results%20tested%20on%20five%20synthetic%20datasets%0Ademonstrate%20that%20our%20LabNet%20sets%20a%20new%20benchmark%20in%20qualitative%20and%0Aquantitative%20evaluations.%20Tested%20on%20the%20RealWorld38%20dataset%2C%20our%20RealNet%0Aachieves%20superior%20visual%20quality%20over%20existing%20methods.%20Ablation%20studies%0Afurther%20verify%20the%20contributions%20of%20STEA%20and%20MLFR%20towards%20both%20LabNet%20and%0ARealNet%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00470v1&entry.124074799=Read"},
{"title": "Learned Compression of Point Cloud Geometry and Attributes in a Single\n  Model through Multimodal Rate-Control", "author": "Michael Rudolph and Aron Riemenschneider and Amr Rizk", "abstract": "  Point cloud compression is essential to experience volumetric multimedia as\nit drastically reduces the required streaming data rates. Point attributes,\nspecifically colors, extend the challenge of lossy compression beyond geometric\nrepresentation to achieving joint reconstruction of texture and geometry.\nState-of-the-art methods separate geometry and attributes to compress them\nindividually. This comes at a computational cost, requiring an encoder and a\ndecoder for each modality. Additionally, as attribute compression methods\nrequire the same geometry for encoding and decoding, the encoder emulates the\ndecoder-side geometry reconstruction as an input step to project and compress\nthe attributes. In this work, we propose to learn joint compression of geometry\nand attributes using a single, adaptive autoencoder model, embedding both\nmodalities into a unified latent space which is then entropy encoded. Key to\nthe technique is to replace the search for trade-offs between rate, attribute\nquality and geometry quality, through conditioning the model on the desired\nqualities of both modalities, bypassing the need for training model ensembles.\nTo differentiate important point cloud regions during encoding or to allow\nview-dependent compression for user-centered streaming, conditioning is\npointwise, which allows for local quality and rate variation. Our evaluation\nshows comparable performance to state-of-the-art compression methods for\ngeometry and attributes, while reducing complexity compared to related\ncompression methods.\n", "link": "http://arxiv.org/abs/2408.00599v1", "date": "2024-08-01", "relevancy": 2.3039, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6162}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5478}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Compression%20of%20Point%20Cloud%20Geometry%20and%20Attributes%20in%20a%20Single%0A%20%20Model%20through%20Multimodal%20Rate-Control&body=Title%3A%20Learned%20Compression%20of%20Point%20Cloud%20Geometry%20and%20Attributes%20in%20a%20Single%0A%20%20Model%20through%20Multimodal%20Rate-Control%0AAuthor%3A%20Michael%20Rudolph%20and%20Aron%20Riemenschneider%20and%20Amr%20Rizk%0AAbstract%3A%20%20%20Point%20cloud%20compression%20is%20essential%20to%20experience%20volumetric%20multimedia%20as%0Ait%20drastically%20reduces%20the%20required%20streaming%20data%20rates.%20Point%20attributes%2C%0Aspecifically%20colors%2C%20extend%20the%20challenge%20of%20lossy%20compression%20beyond%20geometric%0Arepresentation%20to%20achieving%20joint%20reconstruction%20of%20texture%20and%20geometry.%0AState-of-the-art%20methods%20separate%20geometry%20and%20attributes%20to%20compress%20them%0Aindividually.%20This%20comes%20at%20a%20computational%20cost%2C%20requiring%20an%20encoder%20and%20a%0Adecoder%20for%20each%20modality.%20Additionally%2C%20as%20attribute%20compression%20methods%0Arequire%20the%20same%20geometry%20for%20encoding%20and%20decoding%2C%20the%20encoder%20emulates%20the%0Adecoder-side%20geometry%20reconstruction%20as%20an%20input%20step%20to%20project%20and%20compress%0Athe%20attributes.%20In%20this%20work%2C%20we%20propose%20to%20learn%20joint%20compression%20of%20geometry%0Aand%20attributes%20using%20a%20single%2C%20adaptive%20autoencoder%20model%2C%20embedding%20both%0Amodalities%20into%20a%20unified%20latent%20space%20which%20is%20then%20entropy%20encoded.%20Key%20to%0Athe%20technique%20is%20to%20replace%20the%20search%20for%20trade-offs%20between%20rate%2C%20attribute%0Aquality%20and%20geometry%20quality%2C%20through%20conditioning%20the%20model%20on%20the%20desired%0Aqualities%20of%20both%20modalities%2C%20bypassing%20the%20need%20for%20training%20model%20ensembles.%0ATo%20differentiate%20important%20point%20cloud%20regions%20during%20encoding%20or%20to%20allow%0Aview-dependent%20compression%20for%20user-centered%20streaming%2C%20conditioning%20is%0Apointwise%2C%20which%20allows%20for%20local%20quality%20and%20rate%20variation.%20Our%20evaluation%0Ashows%20comparable%20performance%20to%20state-of-the-art%20compression%20methods%20for%0Ageometry%20and%20attributes%2C%20while%20reducing%20complexity%20compared%20to%20related%0Acompression%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Compression%2520of%2520Point%2520Cloud%2520Geometry%2520and%2520Attributes%2520in%2520a%2520Single%250A%2520%2520Model%2520through%2520Multimodal%2520Rate-Control%26entry.906535625%3DMichael%2520Rudolph%2520and%2520Aron%2520Riemenschneider%2520and%2520Amr%2520Rizk%26entry.1292438233%3D%2520%2520Point%2520cloud%2520compression%2520is%2520essential%2520to%2520experience%2520volumetric%2520multimedia%2520as%250Ait%2520drastically%2520reduces%2520the%2520required%2520streaming%2520data%2520rates.%2520Point%2520attributes%252C%250Aspecifically%2520colors%252C%2520extend%2520the%2520challenge%2520of%2520lossy%2520compression%2520beyond%2520geometric%250Arepresentation%2520to%2520achieving%2520joint%2520reconstruction%2520of%2520texture%2520and%2520geometry.%250AState-of-the-art%2520methods%2520separate%2520geometry%2520and%2520attributes%2520to%2520compress%2520them%250Aindividually.%2520This%2520comes%2520at%2520a%2520computational%2520cost%252C%2520requiring%2520an%2520encoder%2520and%2520a%250Adecoder%2520for%2520each%2520modality.%2520Additionally%252C%2520as%2520attribute%2520compression%2520methods%250Arequire%2520the%2520same%2520geometry%2520for%2520encoding%2520and%2520decoding%252C%2520the%2520encoder%2520emulates%2520the%250Adecoder-side%2520geometry%2520reconstruction%2520as%2520an%2520input%2520step%2520to%2520project%2520and%2520compress%250Athe%2520attributes.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520learn%2520joint%2520compression%2520of%2520geometry%250Aand%2520attributes%2520using%2520a%2520single%252C%2520adaptive%2520autoencoder%2520model%252C%2520embedding%2520both%250Amodalities%2520into%2520a%2520unified%2520latent%2520space%2520which%2520is%2520then%2520entropy%2520encoded.%2520Key%2520to%250Athe%2520technique%2520is%2520to%2520replace%2520the%2520search%2520for%2520trade-offs%2520between%2520rate%252C%2520attribute%250Aquality%2520and%2520geometry%2520quality%252C%2520through%2520conditioning%2520the%2520model%2520on%2520the%2520desired%250Aqualities%2520of%2520both%2520modalities%252C%2520bypassing%2520the%2520need%2520for%2520training%2520model%2520ensembles.%250ATo%2520differentiate%2520important%2520point%2520cloud%2520regions%2520during%2520encoding%2520or%2520to%2520allow%250Aview-dependent%2520compression%2520for%2520user-centered%2520streaming%252C%2520conditioning%2520is%250Apointwise%252C%2520which%2520allows%2520for%2520local%2520quality%2520and%2520rate%2520variation.%2520Our%2520evaluation%250Ashows%2520comparable%2520performance%2520to%2520state-of-the-art%2520compression%2520methods%2520for%250Ageometry%2520and%2520attributes%252C%2520while%2520reducing%2520complexity%2520compared%2520to%2520related%250Acompression%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Compression%20of%20Point%20Cloud%20Geometry%20and%20Attributes%20in%20a%20Single%0A%20%20Model%20through%20Multimodal%20Rate-Control&entry.906535625=Michael%20Rudolph%20and%20Aron%20Riemenschneider%20and%20Amr%20Rizk&entry.1292438233=%20%20Point%20cloud%20compression%20is%20essential%20to%20experience%20volumetric%20multimedia%20as%0Ait%20drastically%20reduces%20the%20required%20streaming%20data%20rates.%20Point%20attributes%2C%0Aspecifically%20colors%2C%20extend%20the%20challenge%20of%20lossy%20compression%20beyond%20geometric%0Arepresentation%20to%20achieving%20joint%20reconstruction%20of%20texture%20and%20geometry.%0AState-of-the-art%20methods%20separate%20geometry%20and%20attributes%20to%20compress%20them%0Aindividually.%20This%20comes%20at%20a%20computational%20cost%2C%20requiring%20an%20encoder%20and%20a%0Adecoder%20for%20each%20modality.%20Additionally%2C%20as%20attribute%20compression%20methods%0Arequire%20the%20same%20geometry%20for%20encoding%20and%20decoding%2C%20the%20encoder%20emulates%20the%0Adecoder-side%20geometry%20reconstruction%20as%20an%20input%20step%20to%20project%20and%20compress%0Athe%20attributes.%20In%20this%20work%2C%20we%20propose%20to%20learn%20joint%20compression%20of%20geometry%0Aand%20attributes%20using%20a%20single%2C%20adaptive%20autoencoder%20model%2C%20embedding%20both%0Amodalities%20into%20a%20unified%20latent%20space%20which%20is%20then%20entropy%20encoded.%20Key%20to%0Athe%20technique%20is%20to%20replace%20the%20search%20for%20trade-offs%20between%20rate%2C%20attribute%0Aquality%20and%20geometry%20quality%2C%20through%20conditioning%20the%20model%20on%20the%20desired%0Aqualities%20of%20both%20modalities%2C%20bypassing%20the%20need%20for%20training%20model%20ensembles.%0ATo%20differentiate%20important%20point%20cloud%20regions%20during%20encoding%20or%20to%20allow%0Aview-dependent%20compression%20for%20user-centered%20streaming%2C%20conditioning%20is%0Apointwise%2C%20which%20allows%20for%20local%20quality%20and%20rate%20variation.%20Our%20evaluation%0Ashows%20comparable%20performance%20to%20state-of-the-art%20compression%20methods%20for%0Ageometry%20and%20attributes%2C%20while%20reducing%20complexity%20compared%20to%20related%0Acompression%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00599v1&entry.124074799=Read"},
{"title": "Text-Guided Video Masked Autoencoder", "author": "David Fan and Jue Wang and Shuai Liao and Zhikang Zhang and Vimal Bhat and Xinyu Li", "abstract": "  Recent video masked autoencoder (MAE) works have designed improved masking\nalgorithms focused on saliency. These works leverage visual cues such as motion\nto mask the most salient regions. However, the robustness of such visual cues\ndepends on how often input videos match underlying assumptions. On the other\nhand, natural language description is an information dense representation of\nvideo that implicitly captures saliency without requiring modality-specific\nassumptions, and has not been explored yet for video MAE. To this end, we\nintroduce a novel text-guided masking algorithm (TGM) that masks the video\nregions with highest correspondence to paired captions. Without leveraging any\nexplicit visual cues for saliency, our TGM is competitive with state-of-the-art\nmasking algorithms such as motion-guided masking. To further benefit from the\nsemantics of natural language for masked reconstruction, we next introduce a\nunified framework for joint MAE and masked video-text contrastive learning. We\nshow that across existing masking algorithms, unifying MAE and masked\nvideo-text contrastive learning improves downstream performance compared to\npure MAE on a variety of video recognition tasks, especially for linear probe.\nWithin this unified framework, our TGM achieves the best relative performance\non five action recognition and one egocentric datasets, highlighting the\ncomplementary nature of natural language for masked video modeling.\n", "link": "http://arxiv.org/abs/2408.00759v1", "date": "2024-08-01", "relevancy": 2.2915, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6012}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5789}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Guided%20Video%20Masked%20Autoencoder&body=Title%3A%20Text-Guided%20Video%20Masked%20Autoencoder%0AAuthor%3A%20David%20Fan%20and%20Jue%20Wang%20and%20Shuai%20Liao%20and%20Zhikang%20Zhang%20and%20Vimal%20Bhat%20and%20Xinyu%20Li%0AAbstract%3A%20%20%20Recent%20video%20masked%20autoencoder%20%28MAE%29%20works%20have%20designed%20improved%20masking%0Aalgorithms%20focused%20on%20saliency.%20These%20works%20leverage%20visual%20cues%20such%20as%20motion%0Ato%20mask%20the%20most%20salient%20regions.%20However%2C%20the%20robustness%20of%20such%20visual%20cues%0Adepends%20on%20how%20often%20input%20videos%20match%20underlying%20assumptions.%20On%20the%20other%0Ahand%2C%20natural%20language%20description%20is%20an%20information%20dense%20representation%20of%0Avideo%20that%20implicitly%20captures%20saliency%20without%20requiring%20modality-specific%0Aassumptions%2C%20and%20has%20not%20been%20explored%20yet%20for%20video%20MAE.%20To%20this%20end%2C%20we%0Aintroduce%20a%20novel%20text-guided%20masking%20algorithm%20%28TGM%29%20that%20masks%20the%20video%0Aregions%20with%20highest%20correspondence%20to%20paired%20captions.%20Without%20leveraging%20any%0Aexplicit%20visual%20cues%20for%20saliency%2C%20our%20TGM%20is%20competitive%20with%20state-of-the-art%0Amasking%20algorithms%20such%20as%20motion-guided%20masking.%20To%20further%20benefit%20from%20the%0Asemantics%20of%20natural%20language%20for%20masked%20reconstruction%2C%20we%20next%20introduce%20a%0Aunified%20framework%20for%20joint%20MAE%20and%20masked%20video-text%20contrastive%20learning.%20We%0Ashow%20that%20across%20existing%20masking%20algorithms%2C%20unifying%20MAE%20and%20masked%0Avideo-text%20contrastive%20learning%20improves%20downstream%20performance%20compared%20to%0Apure%20MAE%20on%20a%20variety%20of%20video%20recognition%20tasks%2C%20especially%20for%20linear%20probe.%0AWithin%20this%20unified%20framework%2C%20our%20TGM%20achieves%20the%20best%20relative%20performance%0Aon%20five%20action%20recognition%20and%20one%20egocentric%20datasets%2C%20highlighting%20the%0Acomplementary%20nature%20of%20natural%20language%20for%20masked%20video%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Guided%2520Video%2520Masked%2520Autoencoder%26entry.906535625%3DDavid%2520Fan%2520and%2520Jue%2520Wang%2520and%2520Shuai%2520Liao%2520and%2520Zhikang%2520Zhang%2520and%2520Vimal%2520Bhat%2520and%2520Xinyu%2520Li%26entry.1292438233%3D%2520%2520Recent%2520video%2520masked%2520autoencoder%2520%2528MAE%2529%2520works%2520have%2520designed%2520improved%2520masking%250Aalgorithms%2520focused%2520on%2520saliency.%2520These%2520works%2520leverage%2520visual%2520cues%2520such%2520as%2520motion%250Ato%2520mask%2520the%2520most%2520salient%2520regions.%2520However%252C%2520the%2520robustness%2520of%2520such%2520visual%2520cues%250Adepends%2520on%2520how%2520often%2520input%2520videos%2520match%2520underlying%2520assumptions.%2520On%2520the%2520other%250Ahand%252C%2520natural%2520language%2520description%2520is%2520an%2520information%2520dense%2520representation%2520of%250Avideo%2520that%2520implicitly%2520captures%2520saliency%2520without%2520requiring%2520modality-specific%250Aassumptions%252C%2520and%2520has%2520not%2520been%2520explored%2520yet%2520for%2520video%2520MAE.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520novel%2520text-guided%2520masking%2520algorithm%2520%2528TGM%2529%2520that%2520masks%2520the%2520video%250Aregions%2520with%2520highest%2520correspondence%2520to%2520paired%2520captions.%2520Without%2520leveraging%2520any%250Aexplicit%2520visual%2520cues%2520for%2520saliency%252C%2520our%2520TGM%2520is%2520competitive%2520with%2520state-of-the-art%250Amasking%2520algorithms%2520such%2520as%2520motion-guided%2520masking.%2520To%2520further%2520benefit%2520from%2520the%250Asemantics%2520of%2520natural%2520language%2520for%2520masked%2520reconstruction%252C%2520we%2520next%2520introduce%2520a%250Aunified%2520framework%2520for%2520joint%2520MAE%2520and%2520masked%2520video-text%2520contrastive%2520learning.%2520We%250Ashow%2520that%2520across%2520existing%2520masking%2520algorithms%252C%2520unifying%2520MAE%2520and%2520masked%250Avideo-text%2520contrastive%2520learning%2520improves%2520downstream%2520performance%2520compared%2520to%250Apure%2520MAE%2520on%2520a%2520variety%2520of%2520video%2520recognition%2520tasks%252C%2520especially%2520for%2520linear%2520probe.%250AWithin%2520this%2520unified%2520framework%252C%2520our%2520TGM%2520achieves%2520the%2520best%2520relative%2520performance%250Aon%2520five%2520action%2520recognition%2520and%2520one%2520egocentric%2520datasets%252C%2520highlighting%2520the%250Acomplementary%2520nature%2520of%2520natural%2520language%2520for%2520masked%2520video%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Guided%20Video%20Masked%20Autoencoder&entry.906535625=David%20Fan%20and%20Jue%20Wang%20and%20Shuai%20Liao%20and%20Zhikang%20Zhang%20and%20Vimal%20Bhat%20and%20Xinyu%20Li&entry.1292438233=%20%20Recent%20video%20masked%20autoencoder%20%28MAE%29%20works%20have%20designed%20improved%20masking%0Aalgorithms%20focused%20on%20saliency.%20These%20works%20leverage%20visual%20cues%20such%20as%20motion%0Ato%20mask%20the%20most%20salient%20regions.%20However%2C%20the%20robustness%20of%20such%20visual%20cues%0Adepends%20on%20how%20often%20input%20videos%20match%20underlying%20assumptions.%20On%20the%20other%0Ahand%2C%20natural%20language%20description%20is%20an%20information%20dense%20representation%20of%0Avideo%20that%20implicitly%20captures%20saliency%20without%20requiring%20modality-specific%0Aassumptions%2C%20and%20has%20not%20been%20explored%20yet%20for%20video%20MAE.%20To%20this%20end%2C%20we%0Aintroduce%20a%20novel%20text-guided%20masking%20algorithm%20%28TGM%29%20that%20masks%20the%20video%0Aregions%20with%20highest%20correspondence%20to%20paired%20captions.%20Without%20leveraging%20any%0Aexplicit%20visual%20cues%20for%20saliency%2C%20our%20TGM%20is%20competitive%20with%20state-of-the-art%0Amasking%20algorithms%20such%20as%20motion-guided%20masking.%20To%20further%20benefit%20from%20the%0Asemantics%20of%20natural%20language%20for%20masked%20reconstruction%2C%20we%20next%20introduce%20a%0Aunified%20framework%20for%20joint%20MAE%20and%20masked%20video-text%20contrastive%20learning.%20We%0Ashow%20that%20across%20existing%20masking%20algorithms%2C%20unifying%20MAE%20and%20masked%0Avideo-text%20contrastive%20learning%20improves%20downstream%20performance%20compared%20to%0Apure%20MAE%20on%20a%20variety%20of%20video%20recognition%20tasks%2C%20especially%20for%20linear%20probe.%0AWithin%20this%20unified%20framework%2C%20our%20TGM%20achieves%20the%20best%20relative%20performance%0Aon%20five%20action%20recognition%20and%20one%20egocentric%20datasets%2C%20highlighting%20the%0Acomplementary%20nature%20of%20natural%20language%20for%20masked%20video%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00759v1&entry.124074799=Read"},
{"title": "Grappa -- A Machine Learned Molecular Mechanics Force Field", "author": "Leif Seute and Eric Hartmann and Jan St\u00fchmer and Frauke Gr\u00e4ter", "abstract": "  Simulating large molecular systems over long timescales requires force fields\nthat are both accurate and efficient. In recent years, E(3) equivariant neural\nnetworks have lifted the tension between computational efficiency and accuracy\nof force fields, but they are still several orders of magnitude more expensive\nthan established molecular mechanics (MM) force fields. Here, we propose\nGrappa, a machine learning framework to predict MM parameters from the\nmolecular graph, employing a graph attentional neural network and a transformer\nwith symmetry-preserving positional encoding. The resulting Grappa force field\noutperformstabulated and machine-learned MM force fields in terms of accuracy\nat the same computational efficiency and can be used in existing Molecular\nDynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces\nof small molecules, peptides, RNA and - showcasing its extensibility to\nuncharted regions of chemical space - radicals at state-of-the-art MM accuracy.\nWe demonstrate Grappa's transferability to macromolecules in MD simulations\nfrom a small fast folding protein up to a whole virus particle. Our force field\nsets the stage for biomolecular simulations closer to chemical accuracy, but\nwith the same computational cost as established protein force fields.\n", "link": "http://arxiv.org/abs/2404.00050v2", "date": "2024-08-01", "relevancy": 2.2915, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4729}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4585}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grappa%20--%20A%20Machine%20Learned%20Molecular%20Mechanics%20Force%20Field&body=Title%3A%20Grappa%20--%20A%20Machine%20Learned%20Molecular%20Mechanics%20Force%20Field%0AAuthor%3A%20Leif%20Seute%20and%20Eric%20Hartmann%20and%20Jan%20St%C3%BChmer%20and%20Frauke%20Gr%C3%A4ter%0AAbstract%3A%20%20%20Simulating%20large%20molecular%20systems%20over%20long%20timescales%20requires%20force%20fields%0Athat%20are%20both%20accurate%20and%20efficient.%20In%20recent%20years%2C%20E%283%29%20equivariant%20neural%0Anetworks%20have%20lifted%20the%20tension%20between%20computational%20efficiency%20and%20accuracy%0Aof%20force%20fields%2C%20but%20they%20are%20still%20several%20orders%20of%20magnitude%20more%20expensive%0Athan%20established%20molecular%20mechanics%20%28MM%29%20force%20fields.%20Here%2C%20we%20propose%0AGrappa%2C%20a%20machine%20learning%20framework%20to%20predict%20MM%20parameters%20from%20the%0Amolecular%20graph%2C%20employing%20a%20graph%20attentional%20neural%20network%20and%20a%20transformer%0Awith%20symmetry-preserving%20positional%20encoding.%20The%20resulting%20Grappa%20force%20field%0Aoutperformstabulated%20and%20machine-learned%20MM%20force%20fields%20in%20terms%20of%20accuracy%0Aat%20the%20same%20computational%20efficiency%20and%20can%20be%20used%20in%20existing%20Molecular%0ADynamics%20%28MD%29%20engines%20like%20GROMACS%20and%20OpenMM.%20It%20predicts%20energies%20and%20forces%0Aof%20small%20molecules%2C%20peptides%2C%20RNA%20and%20-%20showcasing%20its%20extensibility%20to%0Auncharted%20regions%20of%20chemical%20space%20-%20radicals%20at%20state-of-the-art%20MM%20accuracy.%0AWe%20demonstrate%20Grappa%27s%20transferability%20to%20macromolecules%20in%20MD%20simulations%0Afrom%20a%20small%20fast%20folding%20protein%20up%20to%20a%20whole%20virus%20particle.%20Our%20force%20field%0Asets%20the%20stage%20for%20biomolecular%20simulations%20closer%20to%20chemical%20accuracy%2C%20but%0Awith%20the%20same%20computational%20cost%20as%20established%20protein%20force%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrappa%2520--%2520A%2520Machine%2520Learned%2520Molecular%2520Mechanics%2520Force%2520Field%26entry.906535625%3DLeif%2520Seute%2520and%2520Eric%2520Hartmann%2520and%2520Jan%2520St%25C3%25BChmer%2520and%2520Frauke%2520Gr%25C3%25A4ter%26entry.1292438233%3D%2520%2520Simulating%2520large%2520molecular%2520systems%2520over%2520long%2520timescales%2520requires%2520force%2520fields%250Athat%2520are%2520both%2520accurate%2520and%2520efficient.%2520In%2520recent%2520years%252C%2520E%25283%2529%2520equivariant%2520neural%250Anetworks%2520have%2520lifted%2520the%2520tension%2520between%2520computational%2520efficiency%2520and%2520accuracy%250Aof%2520force%2520fields%252C%2520but%2520they%2520are%2520still%2520several%2520orders%2520of%2520magnitude%2520more%2520expensive%250Athan%2520established%2520molecular%2520mechanics%2520%2528MM%2529%2520force%2520fields.%2520Here%252C%2520we%2520propose%250AGrappa%252C%2520a%2520machine%2520learning%2520framework%2520to%2520predict%2520MM%2520parameters%2520from%2520the%250Amolecular%2520graph%252C%2520employing%2520a%2520graph%2520attentional%2520neural%2520network%2520and%2520a%2520transformer%250Awith%2520symmetry-preserving%2520positional%2520encoding.%2520The%2520resulting%2520Grappa%2520force%2520field%250Aoutperformstabulated%2520and%2520machine-learned%2520MM%2520force%2520fields%2520in%2520terms%2520of%2520accuracy%250Aat%2520the%2520same%2520computational%2520efficiency%2520and%2520can%2520be%2520used%2520in%2520existing%2520Molecular%250ADynamics%2520%2528MD%2529%2520engines%2520like%2520GROMACS%2520and%2520OpenMM.%2520It%2520predicts%2520energies%2520and%2520forces%250Aof%2520small%2520molecules%252C%2520peptides%252C%2520RNA%2520and%2520-%2520showcasing%2520its%2520extensibility%2520to%250Auncharted%2520regions%2520of%2520chemical%2520space%2520-%2520radicals%2520at%2520state-of-the-art%2520MM%2520accuracy.%250AWe%2520demonstrate%2520Grappa%2527s%2520transferability%2520to%2520macromolecules%2520in%2520MD%2520simulations%250Afrom%2520a%2520small%2520fast%2520folding%2520protein%2520up%2520to%2520a%2520whole%2520virus%2520particle.%2520Our%2520force%2520field%250Asets%2520the%2520stage%2520for%2520biomolecular%2520simulations%2520closer%2520to%2520chemical%2520accuracy%252C%2520but%250Awith%2520the%2520same%2520computational%2520cost%2520as%2520established%2520protein%2520force%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grappa%20--%20A%20Machine%20Learned%20Molecular%20Mechanics%20Force%20Field&entry.906535625=Leif%20Seute%20and%20Eric%20Hartmann%20and%20Jan%20St%C3%BChmer%20and%20Frauke%20Gr%C3%A4ter&entry.1292438233=%20%20Simulating%20large%20molecular%20systems%20over%20long%20timescales%20requires%20force%20fields%0Athat%20are%20both%20accurate%20and%20efficient.%20In%20recent%20years%2C%20E%283%29%20equivariant%20neural%0Anetworks%20have%20lifted%20the%20tension%20between%20computational%20efficiency%20and%20accuracy%0Aof%20force%20fields%2C%20but%20they%20are%20still%20several%20orders%20of%20magnitude%20more%20expensive%0Athan%20established%20molecular%20mechanics%20%28MM%29%20force%20fields.%20Here%2C%20we%20propose%0AGrappa%2C%20a%20machine%20learning%20framework%20to%20predict%20MM%20parameters%20from%20the%0Amolecular%20graph%2C%20employing%20a%20graph%20attentional%20neural%20network%20and%20a%20transformer%0Awith%20symmetry-preserving%20positional%20encoding.%20The%20resulting%20Grappa%20force%20field%0Aoutperformstabulated%20and%20machine-learned%20MM%20force%20fields%20in%20terms%20of%20accuracy%0Aat%20the%20same%20computational%20efficiency%20and%20can%20be%20used%20in%20existing%20Molecular%0ADynamics%20%28MD%29%20engines%20like%20GROMACS%20and%20OpenMM.%20It%20predicts%20energies%20and%20forces%0Aof%20small%20molecules%2C%20peptides%2C%20RNA%20and%20-%20showcasing%20its%20extensibility%20to%0Auncharted%20regions%20of%20chemical%20space%20-%20radicals%20at%20state-of-the-art%20MM%20accuracy.%0AWe%20demonstrate%20Grappa%27s%20transferability%20to%20macromolecules%20in%20MD%20simulations%0Afrom%20a%20small%20fast%20folding%20protein%20up%20to%20a%20whole%20virus%20particle.%20Our%20force%20field%0Asets%20the%20stage%20for%20biomolecular%20simulations%20closer%20to%20chemical%20accuracy%2C%20but%0Awith%20the%20same%20computational%20cost%20as%20established%20protein%20force%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00050v2&entry.124074799=Read"},
{"title": "A new approach for encoding code and assisting code understanding", "author": "Mengdan Fan and Wei Zhang and Haiyan Zhao and Zhi Jin", "abstract": "  Some companies(e.g., Microsoft Research and Google DeepMind) have discovered\nsome of the limitations of GPTs autoregressive paradigm next-word prediction,\nmanifested in the model lack of planning, working memory, backtracking, and\nreasoning skills. GPTs rely on a local and greedy process of generating the\nnext word, without a global understanding of the task or the output.We have\nconfirmed the above limitations through specialized empirical studies of code\ncomprehension. Although GPT4 is good at producing fluent and coherent text, it\ncannot handle complex logic and generate new code that haven not been seen, and\nit relies too much on the formatting of the prompt to generate the correct\ncode.We propose a new paradigm for code understanding that goes beyond the\nnext-word prediction paradigm, inspired by the successful application of\ndiffusion techniques to image generation(Dalle2, Sora) and protein structure\ngeneration(AlphaFold3), which have no autoregressive constraints.Instead of\nencoding the code in a form that mimics natural language, we encode the code as\na heterogeneous image paradigm with a memory of global information that mimics\nboth images and protein structures.We then refer to Sora's CLIP upstream\ntext-to-image encoder model to design a text-to-code encoder model that can be\napplied to various downstream code understanding tasks.The model learns the\nglobal understanding of code under the new paradigm heterogeneous image,\nconnects the encoding space of text and code, and encodes the input of text\ninto the vector of code most similar to it.Using self-supervised comparative\nlearning on 456,360 text-code pairs, the model achieved a zero-shot prediction\nof new data. This work is the basis for future work on code generation using\ndiffusion techniques under a new paradigm to avoid autoregressive limitations.\n", "link": "http://arxiv.org/abs/2408.00521v1", "date": "2024-08-01", "relevancy": 2.2889, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6049}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5503}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20new%20approach%20for%20encoding%20code%20and%20assisting%20code%20understanding&body=Title%3A%20A%20new%20approach%20for%20encoding%20code%20and%20assisting%20code%20understanding%0AAuthor%3A%20Mengdan%20Fan%20and%20Wei%20Zhang%20and%20Haiyan%20Zhao%20and%20Zhi%20Jin%0AAbstract%3A%20%20%20Some%20companies%28e.g.%2C%20Microsoft%20Research%20and%20Google%20DeepMind%29%20have%20discovered%0Asome%20of%20the%20limitations%20of%20GPTs%20autoregressive%20paradigm%20next-word%20prediction%2C%0Amanifested%20in%20the%20model%20lack%20of%20planning%2C%20working%20memory%2C%20backtracking%2C%20and%0Areasoning%20skills.%20GPTs%20rely%20on%20a%20local%20and%20greedy%20process%20of%20generating%20the%0Anext%20word%2C%20without%20a%20global%20understanding%20of%20the%20task%20or%20the%20output.We%20have%0Aconfirmed%20the%20above%20limitations%20through%20specialized%20empirical%20studies%20of%20code%0Acomprehension.%20Although%20GPT4%20is%20good%20at%20producing%20fluent%20and%20coherent%20text%2C%20it%0Acannot%20handle%20complex%20logic%20and%20generate%20new%20code%20that%20haven%20not%20been%20seen%2C%20and%0Ait%20relies%20too%20much%20on%20the%20formatting%20of%20the%20prompt%20to%20generate%20the%20correct%0Acode.We%20propose%20a%20new%20paradigm%20for%20code%20understanding%20that%20goes%20beyond%20the%0Anext-word%20prediction%20paradigm%2C%20inspired%20by%20the%20successful%20application%20of%0Adiffusion%20techniques%20to%20image%20generation%28Dalle2%2C%20Sora%29%20and%20protein%20structure%0Ageneration%28AlphaFold3%29%2C%20which%20have%20no%20autoregressive%20constraints.Instead%20of%0Aencoding%20the%20code%20in%20a%20form%20that%20mimics%20natural%20language%2C%20we%20encode%20the%20code%20as%0Aa%20heterogeneous%20image%20paradigm%20with%20a%20memory%20of%20global%20information%20that%20mimics%0Aboth%20images%20and%20protein%20structures.We%20then%20refer%20to%20Sora%27s%20CLIP%20upstream%0Atext-to-image%20encoder%20model%20to%20design%20a%20text-to-code%20encoder%20model%20that%20can%20be%0Aapplied%20to%20various%20downstream%20code%20understanding%20tasks.The%20model%20learns%20the%0Aglobal%20understanding%20of%20code%20under%20the%20new%20paradigm%20heterogeneous%20image%2C%0Aconnects%20the%20encoding%20space%20of%20text%20and%20code%2C%20and%20encodes%20the%20input%20of%20text%0Ainto%20the%20vector%20of%20code%20most%20similar%20to%20it.Using%20self-supervised%20comparative%0Alearning%20on%20456%2C360%20text-code%20pairs%2C%20the%20model%20achieved%20a%20zero-shot%20prediction%0Aof%20new%20data.%20This%20work%20is%20the%20basis%20for%20future%20work%20on%20code%20generation%20using%0Adiffusion%20techniques%20under%20a%20new%20paradigm%20to%20avoid%20autoregressive%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520new%2520approach%2520for%2520encoding%2520code%2520and%2520assisting%2520code%2520understanding%26entry.906535625%3DMengdan%2520Fan%2520and%2520Wei%2520Zhang%2520and%2520Haiyan%2520Zhao%2520and%2520Zhi%2520Jin%26entry.1292438233%3D%2520%2520Some%2520companies%2528e.g.%252C%2520Microsoft%2520Research%2520and%2520Google%2520DeepMind%2529%2520have%2520discovered%250Asome%2520of%2520the%2520limitations%2520of%2520GPTs%2520autoregressive%2520paradigm%2520next-word%2520prediction%252C%250Amanifested%2520in%2520the%2520model%2520lack%2520of%2520planning%252C%2520working%2520memory%252C%2520backtracking%252C%2520and%250Areasoning%2520skills.%2520GPTs%2520rely%2520on%2520a%2520local%2520and%2520greedy%2520process%2520of%2520generating%2520the%250Anext%2520word%252C%2520without%2520a%2520global%2520understanding%2520of%2520the%2520task%2520or%2520the%2520output.We%2520have%250Aconfirmed%2520the%2520above%2520limitations%2520through%2520specialized%2520empirical%2520studies%2520of%2520code%250Acomprehension.%2520Although%2520GPT4%2520is%2520good%2520at%2520producing%2520fluent%2520and%2520coherent%2520text%252C%2520it%250Acannot%2520handle%2520complex%2520logic%2520and%2520generate%2520new%2520code%2520that%2520haven%2520not%2520been%2520seen%252C%2520and%250Ait%2520relies%2520too%2520much%2520on%2520the%2520formatting%2520of%2520the%2520prompt%2520to%2520generate%2520the%2520correct%250Acode.We%2520propose%2520a%2520new%2520paradigm%2520for%2520code%2520understanding%2520that%2520goes%2520beyond%2520the%250Anext-word%2520prediction%2520paradigm%252C%2520inspired%2520by%2520the%2520successful%2520application%2520of%250Adiffusion%2520techniques%2520to%2520image%2520generation%2528Dalle2%252C%2520Sora%2529%2520and%2520protein%2520structure%250Ageneration%2528AlphaFold3%2529%252C%2520which%2520have%2520no%2520autoregressive%2520constraints.Instead%2520of%250Aencoding%2520the%2520code%2520in%2520a%2520form%2520that%2520mimics%2520natural%2520language%252C%2520we%2520encode%2520the%2520code%2520as%250Aa%2520heterogeneous%2520image%2520paradigm%2520with%2520a%2520memory%2520of%2520global%2520information%2520that%2520mimics%250Aboth%2520images%2520and%2520protein%2520structures.We%2520then%2520refer%2520to%2520Sora%2527s%2520CLIP%2520upstream%250Atext-to-image%2520encoder%2520model%2520to%2520design%2520a%2520text-to-code%2520encoder%2520model%2520that%2520can%2520be%250Aapplied%2520to%2520various%2520downstream%2520code%2520understanding%2520tasks.The%2520model%2520learns%2520the%250Aglobal%2520understanding%2520of%2520code%2520under%2520the%2520new%2520paradigm%2520heterogeneous%2520image%252C%250Aconnects%2520the%2520encoding%2520space%2520of%2520text%2520and%2520code%252C%2520and%2520encodes%2520the%2520input%2520of%2520text%250Ainto%2520the%2520vector%2520of%2520code%2520most%2520similar%2520to%2520it.Using%2520self-supervised%2520comparative%250Alearning%2520on%2520456%252C360%2520text-code%2520pairs%252C%2520the%2520model%2520achieved%2520a%2520zero-shot%2520prediction%250Aof%2520new%2520data.%2520This%2520work%2520is%2520the%2520basis%2520for%2520future%2520work%2520on%2520code%2520generation%2520using%250Adiffusion%2520techniques%2520under%2520a%2520new%2520paradigm%2520to%2520avoid%2520autoregressive%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20approach%20for%20encoding%20code%20and%20assisting%20code%20understanding&entry.906535625=Mengdan%20Fan%20and%20Wei%20Zhang%20and%20Haiyan%20Zhao%20and%20Zhi%20Jin&entry.1292438233=%20%20Some%20companies%28e.g.%2C%20Microsoft%20Research%20and%20Google%20DeepMind%29%20have%20discovered%0Asome%20of%20the%20limitations%20of%20GPTs%20autoregressive%20paradigm%20next-word%20prediction%2C%0Amanifested%20in%20the%20model%20lack%20of%20planning%2C%20working%20memory%2C%20backtracking%2C%20and%0Areasoning%20skills.%20GPTs%20rely%20on%20a%20local%20and%20greedy%20process%20of%20generating%20the%0Anext%20word%2C%20without%20a%20global%20understanding%20of%20the%20task%20or%20the%20output.We%20have%0Aconfirmed%20the%20above%20limitations%20through%20specialized%20empirical%20studies%20of%20code%0Acomprehension.%20Although%20GPT4%20is%20good%20at%20producing%20fluent%20and%20coherent%20text%2C%20it%0Acannot%20handle%20complex%20logic%20and%20generate%20new%20code%20that%20haven%20not%20been%20seen%2C%20and%0Ait%20relies%20too%20much%20on%20the%20formatting%20of%20the%20prompt%20to%20generate%20the%20correct%0Acode.We%20propose%20a%20new%20paradigm%20for%20code%20understanding%20that%20goes%20beyond%20the%0Anext-word%20prediction%20paradigm%2C%20inspired%20by%20the%20successful%20application%20of%0Adiffusion%20techniques%20to%20image%20generation%28Dalle2%2C%20Sora%29%20and%20protein%20structure%0Ageneration%28AlphaFold3%29%2C%20which%20have%20no%20autoregressive%20constraints.Instead%20of%0Aencoding%20the%20code%20in%20a%20form%20that%20mimics%20natural%20language%2C%20we%20encode%20the%20code%20as%0Aa%20heterogeneous%20image%20paradigm%20with%20a%20memory%20of%20global%20information%20that%20mimics%0Aboth%20images%20and%20protein%20structures.We%20then%20refer%20to%20Sora%27s%20CLIP%20upstream%0Atext-to-image%20encoder%20model%20to%20design%20a%20text-to-code%20encoder%20model%20that%20can%20be%0Aapplied%20to%20various%20downstream%20code%20understanding%20tasks.The%20model%20learns%20the%0Aglobal%20understanding%20of%20code%20under%20the%20new%20paradigm%20heterogeneous%20image%2C%0Aconnects%20the%20encoding%20space%20of%20text%20and%20code%2C%20and%20encodes%20the%20input%20of%20text%0Ainto%20the%20vector%20of%20code%20most%20similar%20to%20it.Using%20self-supervised%20comparative%0Alearning%20on%20456%2C360%20text-code%20pairs%2C%20the%20model%20achieved%20a%20zero-shot%20prediction%0Aof%20new%20data.%20This%20work%20is%20the%20basis%20for%20future%20work%20on%20code%20generation%20using%0Adiffusion%20techniques%20under%20a%20new%20paradigm%20to%20avoid%20autoregressive%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00521v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning Based Handwriting Verification", "author": "Mihir Chauhan and Mohammad Abuzar Hashemi and Abhishek Satbhai and Mir Basheer Ali and Bina Ramamurthy and Mingchen Gao and Siwei Lyu and Sargur Srihari", "abstract": "  We present SSL-HV: Self-Supervised Learning approaches applied to the task of\nHandwriting Verification. This task involves determining whether a given pair\nof handwritten images originate from the same or different writer distribution.\nWe have compared the performance of multiple generative, contrastive SSL\napproaches against handcrafted feature extractors and supervised learning on\nCEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)\noutperforms other generative approaches achieving 76.3% accuracy, while\nResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization\n(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using\na pre-trained VAE and VICReg for the downstream task of writer verification we\nobserved a relative improvement in accuracy of 6.7% and 9% over ResNet-18\nsupervised baseline with 10% writer labels.\n", "link": "http://arxiv.org/abs/2405.18320v2", "date": "2024-08-01", "relevancy": 2.2706, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4351}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20Based%20Handwriting%20Verification&body=Title%3A%20Self-Supervised%20Learning%20Based%20Handwriting%20Verification%0AAuthor%3A%20Mihir%20Chauhan%20and%20Mohammad%20Abuzar%20Hashemi%20and%20Abhishek%20Satbhai%20and%20Mir%20Basheer%20Ali%20and%20Bina%20Ramamurthy%20and%20Mingchen%20Gao%20and%20Siwei%20Lyu%20and%20Sargur%20Srihari%0AAbstract%3A%20%20%20We%20present%20SSL-HV%3A%20Self-Supervised%20Learning%20approaches%20applied%20to%20the%20task%20of%0AHandwriting%20Verification.%20This%20task%20involves%20determining%20whether%20a%20given%20pair%0Aof%20handwritten%20images%20originate%20from%20the%20same%20or%20different%20writer%20distribution.%0AWe%20have%20compared%20the%20performance%20of%20multiple%20generative%2C%20contrastive%20SSL%0Aapproaches%20against%20handcrafted%20feature%20extractors%20and%20supervised%20learning%20on%0ACEDAR%20AND%20dataset.%20We%20show%20that%20ResNet%20based%20Variational%20Auto-Encoder%20%28VAE%29%0Aoutperforms%20other%20generative%20approaches%20achieving%2076.3%25%20accuracy%2C%20while%0AResNet-18%20fine-tuned%20using%20Variance-Invariance-Covariance%20Regularization%0A%28VICReg%29%20outperforms%20other%20contrastive%20approaches%20achieving%2078%25%20accuracy.%20Using%0Aa%20pre-trained%20VAE%20and%20VICReg%20for%20the%20downstream%20task%20of%20writer%20verification%20we%0Aobserved%20a%20relative%20improvement%20in%20accuracy%20of%206.7%25%20and%209%25%20over%20ResNet-18%0Asupervised%20baseline%20with%2010%25%20writer%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520Based%2520Handwriting%2520Verification%26entry.906535625%3DMihir%2520Chauhan%2520and%2520Mohammad%2520Abuzar%2520Hashemi%2520and%2520Abhishek%2520Satbhai%2520and%2520Mir%2520Basheer%2520Ali%2520and%2520Bina%2520Ramamurthy%2520and%2520Mingchen%2520Gao%2520and%2520Siwei%2520Lyu%2520and%2520Sargur%2520Srihari%26entry.1292438233%3D%2520%2520We%2520present%2520SSL-HV%253A%2520Self-Supervised%2520Learning%2520approaches%2520applied%2520to%2520the%2520task%2520of%250AHandwriting%2520Verification.%2520This%2520task%2520involves%2520determining%2520whether%2520a%2520given%2520pair%250Aof%2520handwritten%2520images%2520originate%2520from%2520the%2520same%2520or%2520different%2520writer%2520distribution.%250AWe%2520have%2520compared%2520the%2520performance%2520of%2520multiple%2520generative%252C%2520contrastive%2520SSL%250Aapproaches%2520against%2520handcrafted%2520feature%2520extractors%2520and%2520supervised%2520learning%2520on%250ACEDAR%2520AND%2520dataset.%2520We%2520show%2520that%2520ResNet%2520based%2520Variational%2520Auto-Encoder%2520%2528VAE%2529%250Aoutperforms%2520other%2520generative%2520approaches%2520achieving%252076.3%2525%2520accuracy%252C%2520while%250AResNet-18%2520fine-tuned%2520using%2520Variance-Invariance-Covariance%2520Regularization%250A%2528VICReg%2529%2520outperforms%2520other%2520contrastive%2520approaches%2520achieving%252078%2525%2520accuracy.%2520Using%250Aa%2520pre-trained%2520VAE%2520and%2520VICReg%2520for%2520the%2520downstream%2520task%2520of%2520writer%2520verification%2520we%250Aobserved%2520a%2520relative%2520improvement%2520in%2520accuracy%2520of%25206.7%2525%2520and%25209%2525%2520over%2520ResNet-18%250Asupervised%2520baseline%2520with%252010%2525%2520writer%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20Based%20Handwriting%20Verification&entry.906535625=Mihir%20Chauhan%20and%20Mohammad%20Abuzar%20Hashemi%20and%20Abhishek%20Satbhai%20and%20Mir%20Basheer%20Ali%20and%20Bina%20Ramamurthy%20and%20Mingchen%20Gao%20and%20Siwei%20Lyu%20and%20Sargur%20Srihari&entry.1292438233=%20%20We%20present%20SSL-HV%3A%20Self-Supervised%20Learning%20approaches%20applied%20to%20the%20task%20of%0AHandwriting%20Verification.%20This%20task%20involves%20determining%20whether%20a%20given%20pair%0Aof%20handwritten%20images%20originate%20from%20the%20same%20or%20different%20writer%20distribution.%0AWe%20have%20compared%20the%20performance%20of%20multiple%20generative%2C%20contrastive%20SSL%0Aapproaches%20against%20handcrafted%20feature%20extractors%20and%20supervised%20learning%20on%0ACEDAR%20AND%20dataset.%20We%20show%20that%20ResNet%20based%20Variational%20Auto-Encoder%20%28VAE%29%0Aoutperforms%20other%20generative%20approaches%20achieving%2076.3%25%20accuracy%2C%20while%0AResNet-18%20fine-tuned%20using%20Variance-Invariance-Covariance%20Regularization%0A%28VICReg%29%20outperforms%20other%20contrastive%20approaches%20achieving%2078%25%20accuracy.%20Using%0Aa%20pre-trained%20VAE%20and%20VICReg%20for%20the%20downstream%20task%20of%20writer%20verification%20we%0Aobserved%20a%20relative%20improvement%20in%20accuracy%20of%206.7%25%20and%209%25%20over%20ResNet-18%0Asupervised%20baseline%20with%2010%25%20writer%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18320v2&entry.124074799=Read"},
{"title": "Digital Twin-Empowered Task Assignment in Aerial MEC Network: A Resource\n  Coalition Cooperation Approach with Generative Model", "author": "Xin Tang and Qian Chen and Rong Yu and Xiaohuan Li", "abstract": "  To meet the demands for ubiquitous communication and temporary edge computing\nin 6G networks, aerial mobile edge computing (MEC) networks have been\nenvisioned as a new paradigm. However, dynamic user requests pose challenges\nfor task assignment strategies. Most of the existing research assumes that the\nstrategy is deployed on ground-based stations or UAVs, which will be\nineffective in an environment lacking infrastructure and continuous energy\nsupply. Moreover, the resource mutual exclusion problem of dynamic task\nassignment has not been effectively solved. Toward this end, we introduce the\ndigital twin (DT) into the aerial MEC network to study the resource coalition\ncooperation approach with the generative model (GM), which provides a\npreliminary coalition structure for the coalition game. Specifically, we\npropose a novel network framework that is composed of an application plane, a\nphysical plane, and a virtual plane. After that, the task assignment problem is\nsimplified to convex optimization programming with linear constraints. And\nthen, we also propose a resource coalition cooperation approach that is based\non a transferable utility (TU) coalition game to obtain an approximate optimal\nsolution. Numerical results confirm the effectiveness of our proposed approach\nin terms of energy consumption and utilization of resources.\n", "link": "http://arxiv.org/abs/2405.01555v3", "date": "2024-08-01", "relevancy": 2.2398, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4502}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.448}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Digital%20Twin-Empowered%20Task%20Assignment%20in%20Aerial%20MEC%20Network%3A%20A%20Resource%0A%20%20Coalition%20Cooperation%20Approach%20with%20Generative%20Model&body=Title%3A%20Digital%20Twin-Empowered%20Task%20Assignment%20in%20Aerial%20MEC%20Network%3A%20A%20Resource%0A%20%20Coalition%20Cooperation%20Approach%20with%20Generative%20Model%0AAuthor%3A%20Xin%20Tang%20and%20Qian%20Chen%20and%20Rong%20Yu%20and%20Xiaohuan%20Li%0AAbstract%3A%20%20%20To%20meet%20the%20demands%20for%20ubiquitous%20communication%20and%20temporary%20edge%20computing%0Ain%206G%20networks%2C%20aerial%20mobile%20edge%20computing%20%28MEC%29%20networks%20have%20been%0Aenvisioned%20as%20a%20new%20paradigm.%20However%2C%20dynamic%20user%20requests%20pose%20challenges%0Afor%20task%20assignment%20strategies.%20Most%20of%20the%20existing%20research%20assumes%20that%20the%0Astrategy%20is%20deployed%20on%20ground-based%20stations%20or%20UAVs%2C%20which%20will%20be%0Aineffective%20in%20an%20environment%20lacking%20infrastructure%20and%20continuous%20energy%0Asupply.%20Moreover%2C%20the%20resource%20mutual%20exclusion%20problem%20of%20dynamic%20task%0Aassignment%20has%20not%20been%20effectively%20solved.%20Toward%20this%20end%2C%20we%20introduce%20the%0Adigital%20twin%20%28DT%29%20into%20the%20aerial%20MEC%20network%20to%20study%20the%20resource%20coalition%0Acooperation%20approach%20with%20the%20generative%20model%20%28GM%29%2C%20which%20provides%20a%0Apreliminary%20coalition%20structure%20for%20the%20coalition%20game.%20Specifically%2C%20we%0Apropose%20a%20novel%20network%20framework%20that%20is%20composed%20of%20an%20application%20plane%2C%20a%0Aphysical%20plane%2C%20and%20a%20virtual%20plane.%20After%20that%2C%20the%20task%20assignment%20problem%20is%0Asimplified%20to%20convex%20optimization%20programming%20with%20linear%20constraints.%20And%0Athen%2C%20we%20also%20propose%20a%20resource%20coalition%20cooperation%20approach%20that%20is%20based%0Aon%20a%20transferable%20utility%20%28TU%29%20coalition%20game%20to%20obtain%20an%20approximate%20optimal%0Asolution.%20Numerical%20results%20confirm%20the%20effectiveness%20of%20our%20proposed%20approach%0Ain%20terms%20of%20energy%20consumption%20and%20utilization%20of%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01555v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDigital%2520Twin-Empowered%2520Task%2520Assignment%2520in%2520Aerial%2520MEC%2520Network%253A%2520A%2520Resource%250A%2520%2520Coalition%2520Cooperation%2520Approach%2520with%2520Generative%2520Model%26entry.906535625%3DXin%2520Tang%2520and%2520Qian%2520Chen%2520and%2520Rong%2520Yu%2520and%2520Xiaohuan%2520Li%26entry.1292438233%3D%2520%2520To%2520meet%2520the%2520demands%2520for%2520ubiquitous%2520communication%2520and%2520temporary%2520edge%2520computing%250Ain%25206G%2520networks%252C%2520aerial%2520mobile%2520edge%2520computing%2520%2528MEC%2529%2520networks%2520have%2520been%250Aenvisioned%2520as%2520a%2520new%2520paradigm.%2520However%252C%2520dynamic%2520user%2520requests%2520pose%2520challenges%250Afor%2520task%2520assignment%2520strategies.%2520Most%2520of%2520the%2520existing%2520research%2520assumes%2520that%2520the%250Astrategy%2520is%2520deployed%2520on%2520ground-based%2520stations%2520or%2520UAVs%252C%2520which%2520will%2520be%250Aineffective%2520in%2520an%2520environment%2520lacking%2520infrastructure%2520and%2520continuous%2520energy%250Asupply.%2520Moreover%252C%2520the%2520resource%2520mutual%2520exclusion%2520problem%2520of%2520dynamic%2520task%250Aassignment%2520has%2520not%2520been%2520effectively%2520solved.%2520Toward%2520this%2520end%252C%2520we%2520introduce%2520the%250Adigital%2520twin%2520%2528DT%2529%2520into%2520the%2520aerial%2520MEC%2520network%2520to%2520study%2520the%2520resource%2520coalition%250Acooperation%2520approach%2520with%2520the%2520generative%2520model%2520%2528GM%2529%252C%2520which%2520provides%2520a%250Apreliminary%2520coalition%2520structure%2520for%2520the%2520coalition%2520game.%2520Specifically%252C%2520we%250Apropose%2520a%2520novel%2520network%2520framework%2520that%2520is%2520composed%2520of%2520an%2520application%2520plane%252C%2520a%250Aphysical%2520plane%252C%2520and%2520a%2520virtual%2520plane.%2520After%2520that%252C%2520the%2520task%2520assignment%2520problem%2520is%250Asimplified%2520to%2520convex%2520optimization%2520programming%2520with%2520linear%2520constraints.%2520And%250Athen%252C%2520we%2520also%2520propose%2520a%2520resource%2520coalition%2520cooperation%2520approach%2520that%2520is%2520based%250Aon%2520a%2520transferable%2520utility%2520%2528TU%2529%2520coalition%2520game%2520to%2520obtain%2520an%2520approximate%2520optimal%250Asolution.%2520Numerical%2520results%2520confirm%2520the%2520effectiveness%2520of%2520our%2520proposed%2520approach%250Ain%2520terms%2520of%2520energy%2520consumption%2520and%2520utilization%2520of%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01555v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Digital%20Twin-Empowered%20Task%20Assignment%20in%20Aerial%20MEC%20Network%3A%20A%20Resource%0A%20%20Coalition%20Cooperation%20Approach%20with%20Generative%20Model&entry.906535625=Xin%20Tang%20and%20Qian%20Chen%20and%20Rong%20Yu%20and%20Xiaohuan%20Li&entry.1292438233=%20%20To%20meet%20the%20demands%20for%20ubiquitous%20communication%20and%20temporary%20edge%20computing%0Ain%206G%20networks%2C%20aerial%20mobile%20edge%20computing%20%28MEC%29%20networks%20have%20been%0Aenvisioned%20as%20a%20new%20paradigm.%20However%2C%20dynamic%20user%20requests%20pose%20challenges%0Afor%20task%20assignment%20strategies.%20Most%20of%20the%20existing%20research%20assumes%20that%20the%0Astrategy%20is%20deployed%20on%20ground-based%20stations%20or%20UAVs%2C%20which%20will%20be%0Aineffective%20in%20an%20environment%20lacking%20infrastructure%20and%20continuous%20energy%0Asupply.%20Moreover%2C%20the%20resource%20mutual%20exclusion%20problem%20of%20dynamic%20task%0Aassignment%20has%20not%20been%20effectively%20solved.%20Toward%20this%20end%2C%20we%20introduce%20the%0Adigital%20twin%20%28DT%29%20into%20the%20aerial%20MEC%20network%20to%20study%20the%20resource%20coalition%0Acooperation%20approach%20with%20the%20generative%20model%20%28GM%29%2C%20which%20provides%20a%0Apreliminary%20coalition%20structure%20for%20the%20coalition%20game.%20Specifically%2C%20we%0Apropose%20a%20novel%20network%20framework%20that%20is%20composed%20of%20an%20application%20plane%2C%20a%0Aphysical%20plane%2C%20and%20a%20virtual%20plane.%20After%20that%2C%20the%20task%20assignment%20problem%20is%0Asimplified%20to%20convex%20optimization%20programming%20with%20linear%20constraints.%20And%0Athen%2C%20we%20also%20propose%20a%20resource%20coalition%20cooperation%20approach%20that%20is%20based%0Aon%20a%20transferable%20utility%20%28TU%29%20coalition%20game%20to%20obtain%20an%20approximate%20optimal%0Asolution.%20Numerical%20results%20confirm%20the%20effectiveness%20of%20our%20proposed%20approach%0Ain%20terms%20of%20energy%20consumption%20and%20utilization%20of%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01555v3&entry.124074799=Read"},
{"title": "AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data\n  for 3D-Native Segmentation", "author": "Asbj\u00f8rn Munk and Jakob Ambsdorf and Sebastian Llambias and Mads Nielsen", "abstract": "  This study investigates the impact of self-supervised pretraining of 3D\nsemantic segmentation models on a large-scale, domain-specific dataset. We\nintroduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public\nsources, the largest public dataset available, and revisit a number of design\nchoices for pretraining modern segmentation architectures by simplifying and\noptimizing state-of-the-art methods, and combining them with a novel\naugmentation strategy. The resulting AMAES framework is based on\nmasked-image-modeling and intensity-based augmentation reversal and balances\nmemory usage, runtime, and finetuning performance. Using the popular U-Net and\nthe recent MedNeXt architecture as backbones, we evaluate the effect of\npretraining on three challenging downstream tasks, covering single-sequence,\nlow-resource settings, and out-of-domain generalization. The results highlight\nthat pretraining on the proposed dataset with AMAES significantly improves\nsegmentation performance in the majority of evaluated cases, and that it is\nbeneficial to pretrain the model with augmentations, despite pretraing on a\nlarge-scale dataset. Code and model checkpoints for reproducing results, as\nwell as the BRAINS-45K dataset are available at\n\\url{https://github.com/asbjrnmunk/amaes}.\n", "link": "http://arxiv.org/abs/2408.00640v1", "date": "2024-08-01", "relevancy": 2.2391, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5759}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5662}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMAES%3A%20Augmented%20Masked%20Autoencoder%20Pretraining%20on%20Public%20Brain%20MRI%20Data%0A%20%20for%203D-Native%20Segmentation&body=Title%3A%20AMAES%3A%20Augmented%20Masked%20Autoencoder%20Pretraining%20on%20Public%20Brain%20MRI%20Data%0A%20%20for%203D-Native%20Segmentation%0AAuthor%3A%20Asbj%C3%B8rn%20Munk%20and%20Jakob%20Ambsdorf%20and%20Sebastian%20Llambias%20and%20Mads%20Nielsen%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20impact%20of%20self-supervised%20pretraining%20of%203D%0Asemantic%20segmentation%20models%20on%20a%20large-scale%2C%20domain-specific%20dataset.%20We%0Aintroduce%20BRAINS-45K%2C%20a%20dataset%20of%2044%2C756%20brain%20MRI%20volumes%20from%20public%0Asources%2C%20the%20largest%20public%20dataset%20available%2C%20and%20revisit%20a%20number%20of%20design%0Achoices%20for%20pretraining%20modern%20segmentation%20architectures%20by%20simplifying%20and%0Aoptimizing%20state-of-the-art%20methods%2C%20and%20combining%20them%20with%20a%20novel%0Aaugmentation%20strategy.%20The%20resulting%20AMAES%20framework%20is%20based%20on%0Amasked-image-modeling%20and%20intensity-based%20augmentation%20reversal%20and%20balances%0Amemory%20usage%2C%20runtime%2C%20and%20finetuning%20performance.%20Using%20the%20popular%20U-Net%20and%0Athe%20recent%20MedNeXt%20architecture%20as%20backbones%2C%20we%20evaluate%20the%20effect%20of%0Apretraining%20on%20three%20challenging%20downstream%20tasks%2C%20covering%20single-sequence%2C%0Alow-resource%20settings%2C%20and%20out-of-domain%20generalization.%20The%20results%20highlight%0Athat%20pretraining%20on%20the%20proposed%20dataset%20with%20AMAES%20significantly%20improves%0Asegmentation%20performance%20in%20the%20majority%20of%20evaluated%20cases%2C%20and%20that%20it%20is%0Abeneficial%20to%20pretrain%20the%20model%20with%20augmentations%2C%20despite%20pretraing%20on%20a%0Alarge-scale%20dataset.%20Code%20and%20model%20checkpoints%20for%20reproducing%20results%2C%20as%0Awell%20as%20the%20BRAINS-45K%20dataset%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/asbjrnmunk/amaes%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMAES%253A%2520Augmented%2520Masked%2520Autoencoder%2520Pretraining%2520on%2520Public%2520Brain%2520MRI%2520Data%250A%2520%2520for%25203D-Native%2520Segmentation%26entry.906535625%3DAsbj%25C3%25B8rn%2520Munk%2520and%2520Jakob%2520Ambsdorf%2520and%2520Sebastian%2520Llambias%2520and%2520Mads%2520Nielsen%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520impact%2520of%2520self-supervised%2520pretraining%2520of%25203D%250Asemantic%2520segmentation%2520models%2520on%2520a%2520large-scale%252C%2520domain-specific%2520dataset.%2520We%250Aintroduce%2520BRAINS-45K%252C%2520a%2520dataset%2520of%252044%252C756%2520brain%2520MRI%2520volumes%2520from%2520public%250Asources%252C%2520the%2520largest%2520public%2520dataset%2520available%252C%2520and%2520revisit%2520a%2520number%2520of%2520design%250Achoices%2520for%2520pretraining%2520modern%2520segmentation%2520architectures%2520by%2520simplifying%2520and%250Aoptimizing%2520state-of-the-art%2520methods%252C%2520and%2520combining%2520them%2520with%2520a%2520novel%250Aaugmentation%2520strategy.%2520The%2520resulting%2520AMAES%2520framework%2520is%2520based%2520on%250Amasked-image-modeling%2520and%2520intensity-based%2520augmentation%2520reversal%2520and%2520balances%250Amemory%2520usage%252C%2520runtime%252C%2520and%2520finetuning%2520performance.%2520Using%2520the%2520popular%2520U-Net%2520and%250Athe%2520recent%2520MedNeXt%2520architecture%2520as%2520backbones%252C%2520we%2520evaluate%2520the%2520effect%2520of%250Apretraining%2520on%2520three%2520challenging%2520downstream%2520tasks%252C%2520covering%2520single-sequence%252C%250Alow-resource%2520settings%252C%2520and%2520out-of-domain%2520generalization.%2520The%2520results%2520highlight%250Athat%2520pretraining%2520on%2520the%2520proposed%2520dataset%2520with%2520AMAES%2520significantly%2520improves%250Asegmentation%2520performance%2520in%2520the%2520majority%2520of%2520evaluated%2520cases%252C%2520and%2520that%2520it%2520is%250Abeneficial%2520to%2520pretrain%2520the%2520model%2520with%2520augmentations%252C%2520despite%2520pretraing%2520on%2520a%250Alarge-scale%2520dataset.%2520Code%2520and%2520model%2520checkpoints%2520for%2520reproducing%2520results%252C%2520as%250Awell%2520as%2520the%2520BRAINS-45K%2520dataset%2520are%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/asbjrnmunk/amaes%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMAES%3A%20Augmented%20Masked%20Autoencoder%20Pretraining%20on%20Public%20Brain%20MRI%20Data%0A%20%20for%203D-Native%20Segmentation&entry.906535625=Asbj%C3%B8rn%20Munk%20and%20Jakob%20Ambsdorf%20and%20Sebastian%20Llambias%20and%20Mads%20Nielsen&entry.1292438233=%20%20This%20study%20investigates%20the%20impact%20of%20self-supervised%20pretraining%20of%203D%0Asemantic%20segmentation%20models%20on%20a%20large-scale%2C%20domain-specific%20dataset.%20We%0Aintroduce%20BRAINS-45K%2C%20a%20dataset%20of%2044%2C756%20brain%20MRI%20volumes%20from%20public%0Asources%2C%20the%20largest%20public%20dataset%20available%2C%20and%20revisit%20a%20number%20of%20design%0Achoices%20for%20pretraining%20modern%20segmentation%20architectures%20by%20simplifying%20and%0Aoptimizing%20state-of-the-art%20methods%2C%20and%20combining%20them%20with%20a%20novel%0Aaugmentation%20strategy.%20The%20resulting%20AMAES%20framework%20is%20based%20on%0Amasked-image-modeling%20and%20intensity-based%20augmentation%20reversal%20and%20balances%0Amemory%20usage%2C%20runtime%2C%20and%20finetuning%20performance.%20Using%20the%20popular%20U-Net%20and%0Athe%20recent%20MedNeXt%20architecture%20as%20backbones%2C%20we%20evaluate%20the%20effect%20of%0Apretraining%20on%20three%20challenging%20downstream%20tasks%2C%20covering%20single-sequence%2C%0Alow-resource%20settings%2C%20and%20out-of-domain%20generalization.%20The%20results%20highlight%0Athat%20pretraining%20on%20the%20proposed%20dataset%20with%20AMAES%20significantly%20improves%0Asegmentation%20performance%20in%20the%20majority%20of%20evaluated%20cases%2C%20and%20that%20it%20is%0Abeneficial%20to%20pretrain%20the%20model%20with%20augmentations%2C%20despite%20pretraing%20on%20a%0Alarge-scale%20dataset.%20Code%20and%20model%20checkpoints%20for%20reproducing%20results%2C%20as%0Awell%20as%20the%20BRAINS-45K%20dataset%20are%20available%20at%0A%5Curl%7Bhttps%3A//github.com/asbjrnmunk/amaes%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00640v1&entry.124074799=Read"},
{"title": "MUFASA: Multi-View Fusion and Adaptation Network with Spatial Awareness\n  for Radar Object Detection", "author": "Xiangyuan Peng and Miao Tang and Huawei Sun and Kay Bierzynski and Lorenzo Servadei and Robert Wille", "abstract": "  In recent years, approaches based on radar object detection have made\nsignificant progress in autonomous driving systems due to their robustness\nunder adverse weather compared to LiDAR. However, the sparsity of radar point\nclouds poses challenges in achieving precise object detection, highlighting the\nimportance of effective and comprehensive feature extraction technologies. To\naddress this challenge, this paper introduces a comprehensive feature\nextraction method for radar point clouds. This study first enhances the\ncapability of detection networks by using a plug-and-play module, GeoSPA. It\nleverages the Lalonde features to explore local geometric patterns.\nAdditionally, a distributed multi-view attention mechanism, DEMVA, is designed\nto integrate the shared information across the entire dataset with the global\ninformation of each individual frame. By employing the two modules, we present\nour method, MUFASA, which enhances object detection performance through\nimproved feature extraction. The approach is evaluated on the VoD and\nTJ4DRaDSet datasets to demonstrate its effectiveness. In particular, we achieve\nstate-of-the-art results among radar-based methods on the VoD dataset with the\nmAP of 50.24%.\n", "link": "http://arxiv.org/abs/2408.00565v1", "date": "2024-08-01", "relevancy": 2.2344, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5974}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUFASA%3A%20Multi-View%20Fusion%20and%20Adaptation%20Network%20with%20Spatial%20Awareness%0A%20%20for%20Radar%20Object%20Detection&body=Title%3A%20MUFASA%3A%20Multi-View%20Fusion%20and%20Adaptation%20Network%20with%20Spatial%20Awareness%0A%20%20for%20Radar%20Object%20Detection%0AAuthor%3A%20Xiangyuan%20Peng%20and%20Miao%20Tang%20and%20Huawei%20Sun%20and%20Kay%20Bierzynski%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille%0AAbstract%3A%20%20%20In%20recent%20years%2C%20approaches%20based%20on%20radar%20object%20detection%20have%20made%0Asignificant%20progress%20in%20autonomous%20driving%20systems%20due%20to%20their%20robustness%0Aunder%20adverse%20weather%20compared%20to%20LiDAR.%20However%2C%20the%20sparsity%20of%20radar%20point%0Aclouds%20poses%20challenges%20in%20achieving%20precise%20object%20detection%2C%20highlighting%20the%0Aimportance%20of%20effective%20and%20comprehensive%20feature%20extraction%20technologies.%20To%0Aaddress%20this%20challenge%2C%20this%20paper%20introduces%20a%20comprehensive%20feature%0Aextraction%20method%20for%20radar%20point%20clouds.%20This%20study%20first%20enhances%20the%0Acapability%20of%20detection%20networks%20by%20using%20a%20plug-and-play%20module%2C%20GeoSPA.%20It%0Aleverages%20the%20Lalonde%20features%20to%20explore%20local%20geometric%20patterns.%0AAdditionally%2C%20a%20distributed%20multi-view%20attention%20mechanism%2C%20DEMVA%2C%20is%20designed%0Ato%20integrate%20the%20shared%20information%20across%20the%20entire%20dataset%20with%20the%20global%0Ainformation%20of%20each%20individual%20frame.%20By%20employing%20the%20two%20modules%2C%20we%20present%0Aour%20method%2C%20MUFASA%2C%20which%20enhances%20object%20detection%20performance%20through%0Aimproved%20feature%20extraction.%20The%20approach%20is%20evaluated%20on%20the%20VoD%20and%0ATJ4DRaDSet%20datasets%20to%20demonstrate%20its%20effectiveness.%20In%20particular%2C%20we%20achieve%0Astate-of-the-art%20results%20among%20radar-based%20methods%20on%20the%20VoD%20dataset%20with%20the%0AmAP%20of%2050.24%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUFASA%253A%2520Multi-View%2520Fusion%2520and%2520Adaptation%2520Network%2520with%2520Spatial%2520Awareness%250A%2520%2520for%2520Radar%2520Object%2520Detection%26entry.906535625%3DXiangyuan%2520Peng%2520and%2520Miao%2520Tang%2520and%2520Huawei%2520Sun%2520and%2520Kay%2520Bierzynski%2520and%2520Lorenzo%2520Servadei%2520and%2520Robert%2520Wille%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520approaches%2520based%2520on%2520radar%2520object%2520detection%2520have%2520made%250Asignificant%2520progress%2520in%2520autonomous%2520driving%2520systems%2520due%2520to%2520their%2520robustness%250Aunder%2520adverse%2520weather%2520compared%2520to%2520LiDAR.%2520However%252C%2520the%2520sparsity%2520of%2520radar%2520point%250Aclouds%2520poses%2520challenges%2520in%2520achieving%2520precise%2520object%2520detection%252C%2520highlighting%2520the%250Aimportance%2520of%2520effective%2520and%2520comprehensive%2520feature%2520extraction%2520technologies.%2520To%250Aaddress%2520this%2520challenge%252C%2520this%2520paper%2520introduces%2520a%2520comprehensive%2520feature%250Aextraction%2520method%2520for%2520radar%2520point%2520clouds.%2520This%2520study%2520first%2520enhances%2520the%250Acapability%2520of%2520detection%2520networks%2520by%2520using%2520a%2520plug-and-play%2520module%252C%2520GeoSPA.%2520It%250Aleverages%2520the%2520Lalonde%2520features%2520to%2520explore%2520local%2520geometric%2520patterns.%250AAdditionally%252C%2520a%2520distributed%2520multi-view%2520attention%2520mechanism%252C%2520DEMVA%252C%2520is%2520designed%250Ato%2520integrate%2520the%2520shared%2520information%2520across%2520the%2520entire%2520dataset%2520with%2520the%2520global%250Ainformation%2520of%2520each%2520individual%2520frame.%2520By%2520employing%2520the%2520two%2520modules%252C%2520we%2520present%250Aour%2520method%252C%2520MUFASA%252C%2520which%2520enhances%2520object%2520detection%2520performance%2520through%250Aimproved%2520feature%2520extraction.%2520The%2520approach%2520is%2520evaluated%2520on%2520the%2520VoD%2520and%250ATJ4DRaDSet%2520datasets%2520to%2520demonstrate%2520its%2520effectiveness.%2520In%2520particular%252C%2520we%2520achieve%250Astate-of-the-art%2520results%2520among%2520radar-based%2520methods%2520on%2520the%2520VoD%2520dataset%2520with%2520the%250AmAP%2520of%252050.24%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUFASA%3A%20Multi-View%20Fusion%20and%20Adaptation%20Network%20with%20Spatial%20Awareness%0A%20%20for%20Radar%20Object%20Detection&entry.906535625=Xiangyuan%20Peng%20and%20Miao%20Tang%20and%20Huawei%20Sun%20and%20Kay%20Bierzynski%20and%20Lorenzo%20Servadei%20and%20Robert%20Wille&entry.1292438233=%20%20In%20recent%20years%2C%20approaches%20based%20on%20radar%20object%20detection%20have%20made%0Asignificant%20progress%20in%20autonomous%20driving%20systems%20due%20to%20their%20robustness%0Aunder%20adverse%20weather%20compared%20to%20LiDAR.%20However%2C%20the%20sparsity%20of%20radar%20point%0Aclouds%20poses%20challenges%20in%20achieving%20precise%20object%20detection%2C%20highlighting%20the%0Aimportance%20of%20effective%20and%20comprehensive%20feature%20extraction%20technologies.%20To%0Aaddress%20this%20challenge%2C%20this%20paper%20introduces%20a%20comprehensive%20feature%0Aextraction%20method%20for%20radar%20point%20clouds.%20This%20study%20first%20enhances%20the%0Acapability%20of%20detection%20networks%20by%20using%20a%20plug-and-play%20module%2C%20GeoSPA.%20It%0Aleverages%20the%20Lalonde%20features%20to%20explore%20local%20geometric%20patterns.%0AAdditionally%2C%20a%20distributed%20multi-view%20attention%20mechanism%2C%20DEMVA%2C%20is%20designed%0Ato%20integrate%20the%20shared%20information%20across%20the%20entire%20dataset%20with%20the%20global%0Ainformation%20of%20each%20individual%20frame.%20By%20employing%20the%20two%20modules%2C%20we%20present%0Aour%20method%2C%20MUFASA%2C%20which%20enhances%20object%20detection%20performance%20through%0Aimproved%20feature%20extraction.%20The%20approach%20is%20evaluated%20on%20the%20VoD%20and%0ATJ4DRaDSet%20datasets%20to%20demonstrate%20its%20effectiveness.%20In%20particular%2C%20we%20achieve%0Astate-of-the-art%20results%20among%20radar-based%20methods%20on%20the%20VoD%20dataset%20with%20the%0AmAP%20of%2050.24%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00565v1&entry.124074799=Read"},
{"title": "SegStitch: Multidimensional Transformer for Robust and Efficient Medical\n  Imaging Segmentation", "author": "Shengbo Tan and Zeyu Zhang and Ying Cai and Daji Ergu and Lin Wu and Binbin Hu and Pengzhang Yu and Yang Zhao", "abstract": "  Medical imaging segmentation plays a significant role in the automatic\nrecognition and analysis of lesions. State-of-the-art methods, particularly\nthose utilizing transformers, have been prominently adopted in 3D semantic\nsegmentation due to their superior performance in scalability and\ngeneralizability. However, plain vision transformers encounter challenges due\nto their neglect of local features and their high computational complexity. To\naddress these challenges, we introduce three key contributions: Firstly, we\nproposed SegStitch, an innovative architecture that integrates transformers\nwith denoising ODE blocks. Instead of taking whole 3D volumes as inputs, we\nadapt axial patches and customize patch-wise queries to ensure semantic\nconsistency. Additionally, we conducted extensive experiments on the BTCV and\nACDC datasets, achieving improvements up to 11.48% and 6.71% respectively in\nmDSC, compared to state-of-the-art methods. Lastly, our proposed method\ndemonstrates outstanding efficiency, reducing the number of parameters by 36.7%\nand the number of FLOPS by 10.7% compared to UNETR. This advancement holds\npromising potential for adapting our method to real-world clinical practice.\nThe code will be available at https://github.com/goblin327/SegStitch\n", "link": "http://arxiv.org/abs/2408.00496v1", "date": "2024-08-01", "relevancy": 2.2295, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5666}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5515}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegStitch%3A%20Multidimensional%20Transformer%20for%20Robust%20and%20Efficient%20Medical%0A%20%20Imaging%20Segmentation&body=Title%3A%20SegStitch%3A%20Multidimensional%20Transformer%20for%20Robust%20and%20Efficient%20Medical%0A%20%20Imaging%20Segmentation%0AAuthor%3A%20Shengbo%20Tan%20and%20Zeyu%20Zhang%20and%20Ying%20Cai%20and%20Daji%20Ergu%20and%20Lin%20Wu%20and%20Binbin%20Hu%20and%20Pengzhang%20Yu%20and%20Yang%20Zhao%0AAbstract%3A%20%20%20Medical%20imaging%20segmentation%20plays%20a%20significant%20role%20in%20the%20automatic%0Arecognition%20and%20analysis%20of%20lesions.%20State-of-the-art%20methods%2C%20particularly%0Athose%20utilizing%20transformers%2C%20have%20been%20prominently%20adopted%20in%203D%20semantic%0Asegmentation%20due%20to%20their%20superior%20performance%20in%20scalability%20and%0Ageneralizability.%20However%2C%20plain%20vision%20transformers%20encounter%20challenges%20due%0Ato%20their%20neglect%20of%20local%20features%20and%20their%20high%20computational%20complexity.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20three%20key%20contributions%3A%20Firstly%2C%20we%0Aproposed%20SegStitch%2C%20an%20innovative%20architecture%20that%20integrates%20transformers%0Awith%20denoising%20ODE%20blocks.%20Instead%20of%20taking%20whole%203D%20volumes%20as%20inputs%2C%20we%0Aadapt%20axial%20patches%20and%20customize%20patch-wise%20queries%20to%20ensure%20semantic%0Aconsistency.%20Additionally%2C%20we%20conducted%20extensive%20experiments%20on%20the%20BTCV%20and%0AACDC%20datasets%2C%20achieving%20improvements%20up%20to%2011.48%25%20and%206.71%25%20respectively%20in%0AmDSC%2C%20compared%20to%20state-of-the-art%20methods.%20Lastly%2C%20our%20proposed%20method%0Ademonstrates%20outstanding%20efficiency%2C%20reducing%20the%20number%20of%20parameters%20by%2036.7%25%0Aand%20the%20number%20of%20FLOPS%20by%2010.7%25%20compared%20to%20UNETR.%20This%20advancement%20holds%0Apromising%20potential%20for%20adapting%20our%20method%20to%20real-world%20clinical%20practice.%0AThe%20code%20will%20be%20available%20at%20https%3A//github.com/goblin327/SegStitch%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegStitch%253A%2520Multidimensional%2520Transformer%2520for%2520Robust%2520and%2520Efficient%2520Medical%250A%2520%2520Imaging%2520Segmentation%26entry.906535625%3DShengbo%2520Tan%2520and%2520Zeyu%2520Zhang%2520and%2520Ying%2520Cai%2520and%2520Daji%2520Ergu%2520and%2520Lin%2520Wu%2520and%2520Binbin%2520Hu%2520and%2520Pengzhang%2520Yu%2520and%2520Yang%2520Zhao%26entry.1292438233%3D%2520%2520Medical%2520imaging%2520segmentation%2520plays%2520a%2520significant%2520role%2520in%2520the%2520automatic%250Arecognition%2520and%2520analysis%2520of%2520lesions.%2520State-of-the-art%2520methods%252C%2520particularly%250Athose%2520utilizing%2520transformers%252C%2520have%2520been%2520prominently%2520adopted%2520in%25203D%2520semantic%250Asegmentation%2520due%2520to%2520their%2520superior%2520performance%2520in%2520scalability%2520and%250Ageneralizability.%2520However%252C%2520plain%2520vision%2520transformers%2520encounter%2520challenges%2520due%250Ato%2520their%2520neglect%2520of%2520local%2520features%2520and%2520their%2520high%2520computational%2520complexity.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520three%2520key%2520contributions%253A%2520Firstly%252C%2520we%250Aproposed%2520SegStitch%252C%2520an%2520innovative%2520architecture%2520that%2520integrates%2520transformers%250Awith%2520denoising%2520ODE%2520blocks.%2520Instead%2520of%2520taking%2520whole%25203D%2520volumes%2520as%2520inputs%252C%2520we%250Aadapt%2520axial%2520patches%2520and%2520customize%2520patch-wise%2520queries%2520to%2520ensure%2520semantic%250Aconsistency.%2520Additionally%252C%2520we%2520conducted%2520extensive%2520experiments%2520on%2520the%2520BTCV%2520and%250AACDC%2520datasets%252C%2520achieving%2520improvements%2520up%2520to%252011.48%2525%2520and%25206.71%2525%2520respectively%2520in%250AmDSC%252C%2520compared%2520to%2520state-of-the-art%2520methods.%2520Lastly%252C%2520our%2520proposed%2520method%250Ademonstrates%2520outstanding%2520efficiency%252C%2520reducing%2520the%2520number%2520of%2520parameters%2520by%252036.7%2525%250Aand%2520the%2520number%2520of%2520FLOPS%2520by%252010.7%2525%2520compared%2520to%2520UNETR.%2520This%2520advancement%2520holds%250Apromising%2520potential%2520for%2520adapting%2520our%2520method%2520to%2520real-world%2520clinical%2520practice.%250AThe%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/goblin327/SegStitch%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegStitch%3A%20Multidimensional%20Transformer%20for%20Robust%20and%20Efficient%20Medical%0A%20%20Imaging%20Segmentation&entry.906535625=Shengbo%20Tan%20and%20Zeyu%20Zhang%20and%20Ying%20Cai%20and%20Daji%20Ergu%20and%20Lin%20Wu%20and%20Binbin%20Hu%20and%20Pengzhang%20Yu%20and%20Yang%20Zhao&entry.1292438233=%20%20Medical%20imaging%20segmentation%20plays%20a%20significant%20role%20in%20the%20automatic%0Arecognition%20and%20analysis%20of%20lesions.%20State-of-the-art%20methods%2C%20particularly%0Athose%20utilizing%20transformers%2C%20have%20been%20prominently%20adopted%20in%203D%20semantic%0Asegmentation%20due%20to%20their%20superior%20performance%20in%20scalability%20and%0Ageneralizability.%20However%2C%20plain%20vision%20transformers%20encounter%20challenges%20due%0Ato%20their%20neglect%20of%20local%20features%20and%20their%20high%20computational%20complexity.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20three%20key%20contributions%3A%20Firstly%2C%20we%0Aproposed%20SegStitch%2C%20an%20innovative%20architecture%20that%20integrates%20transformers%0Awith%20denoising%20ODE%20blocks.%20Instead%20of%20taking%20whole%203D%20volumes%20as%20inputs%2C%20we%0Aadapt%20axial%20patches%20and%20customize%20patch-wise%20queries%20to%20ensure%20semantic%0Aconsistency.%20Additionally%2C%20we%20conducted%20extensive%20experiments%20on%20the%20BTCV%20and%0AACDC%20datasets%2C%20achieving%20improvements%20up%20to%2011.48%25%20and%206.71%25%20respectively%20in%0AmDSC%2C%20compared%20to%20state-of-the-art%20methods.%20Lastly%2C%20our%20proposed%20method%0Ademonstrates%20outstanding%20efficiency%2C%20reducing%20the%20number%20of%20parameters%20by%2036.7%25%0Aand%20the%20number%20of%20FLOPS%20by%2010.7%25%20compared%20to%20UNETR.%20This%20advancement%20holds%0Apromising%20potential%20for%20adapting%20our%20method%20to%20real-world%20clinical%20practice.%0AThe%20code%20will%20be%20available%20at%20https%3A//github.com/goblin327/SegStitch%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00496v1&entry.124074799=Read"},
{"title": "AutoM3L: An Automated Multimodal Machine Learning Framework with Large\n  Language Models", "author": "Daqin Luo and Chengjian Feng and Yuxuan Nong and Yiqing Shen", "abstract": "  Automated Machine Learning (AutoML) offers a promising approach to streamline\nthe training of machine learning models. However, existing AutoML frameworks\nare often limited to unimodal scenarios and require extensive manual\nconfiguration. Recent advancements in Large Language Models (LLMs) have\nshowcased their exceptional abilities in reasoning, interaction, and code\ngeneration, presenting an opportunity to develop a more automated and\nuser-friendly framework. To this end, we introduce AutoM3L, an innovative\nAutomated Multimodal Machine Learning framework that leverages LLMs as\ncontrollers to automatically construct multimodal training pipelines. AutoM3L\ncomprehends data modalities and selects appropriate models based on user\nrequirements, providing automation and interactivity. By eliminating the need\nfor manual feature engineering and hyperparameter optimization, our framework\nsimplifies user engagement and enables customization through directives,\naddressing the limitations of previous rule-based AutoML approaches. We\nevaluate the performance of AutoM3L on six diverse multimodal datasets spanning\nclassification, regression, and retrieval tasks, as well as a comprehensive set\nof unimodal datasets. The results demonstrate that AutoM3L achieves competitive\nor superior performance compared to traditional rule-based AutoML methods.\nFurthermore, a user study highlights the user-friendliness and usability of our\nframework, compared to the rule-based AutoML methods.\n", "link": "http://arxiv.org/abs/2408.00665v1", "date": "2024-08-01", "relevancy": 2.2278, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.582}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoM3L%3A%20An%20Automated%20Multimodal%20Machine%20Learning%20Framework%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20AutoM3L%3A%20An%20Automated%20Multimodal%20Machine%20Learning%20Framework%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Daqin%20Luo%20and%20Chengjian%20Feng%20and%20Yuxuan%20Nong%20and%20Yiqing%20Shen%0AAbstract%3A%20%20%20Automated%20Machine%20Learning%20%28AutoML%29%20offers%20a%20promising%20approach%20to%20streamline%0Athe%20training%20of%20machine%20learning%20models.%20However%2C%20existing%20AutoML%20frameworks%0Aare%20often%20limited%20to%20unimodal%20scenarios%20and%20require%20extensive%20manual%0Aconfiguration.%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Ashowcased%20their%20exceptional%20abilities%20in%20reasoning%2C%20interaction%2C%20and%20code%0Ageneration%2C%20presenting%20an%20opportunity%20to%20develop%20a%20more%20automated%20and%0Auser-friendly%20framework.%20To%20this%20end%2C%20we%20introduce%20AutoM3L%2C%20an%20innovative%0AAutomated%20Multimodal%20Machine%20Learning%20framework%20that%20leverages%20LLMs%20as%0Acontrollers%20to%20automatically%20construct%20multimodal%20training%20pipelines.%20AutoM3L%0Acomprehends%20data%20modalities%20and%20selects%20appropriate%20models%20based%20on%20user%0Arequirements%2C%20providing%20automation%20and%20interactivity.%20By%20eliminating%20the%20need%0Afor%20manual%20feature%20engineering%20and%20hyperparameter%20optimization%2C%20our%20framework%0Asimplifies%20user%20engagement%20and%20enables%20customization%20through%20directives%2C%0Aaddressing%20the%20limitations%20of%20previous%20rule-based%20AutoML%20approaches.%20We%0Aevaluate%20the%20performance%20of%20AutoM3L%20on%20six%20diverse%20multimodal%20datasets%20spanning%0Aclassification%2C%20regression%2C%20and%20retrieval%20tasks%2C%20as%20well%20as%20a%20comprehensive%20set%0Aof%20unimodal%20datasets.%20The%20results%20demonstrate%20that%20AutoM3L%20achieves%20competitive%0Aor%20superior%20performance%20compared%20to%20traditional%20rule-based%20AutoML%20methods.%0AFurthermore%2C%20a%20user%20study%20highlights%20the%20user-friendliness%20and%20usability%20of%20our%0Aframework%2C%20compared%20to%20the%20rule-based%20AutoML%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoM3L%253A%2520An%2520Automated%2520Multimodal%2520Machine%2520Learning%2520Framework%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DDaqin%2520Luo%2520and%2520Chengjian%2520Feng%2520and%2520Yuxuan%2520Nong%2520and%2520Yiqing%2520Shen%26entry.1292438233%3D%2520%2520Automated%2520Machine%2520Learning%2520%2528AutoML%2529%2520offers%2520a%2520promising%2520approach%2520to%2520streamline%250Athe%2520training%2520of%2520machine%2520learning%2520models.%2520However%252C%2520existing%2520AutoML%2520frameworks%250Aare%2520often%2520limited%2520to%2520unimodal%2520scenarios%2520and%2520require%2520extensive%2520manual%250Aconfiguration.%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%250Ashowcased%2520their%2520exceptional%2520abilities%2520in%2520reasoning%252C%2520interaction%252C%2520and%2520code%250Ageneration%252C%2520presenting%2520an%2520opportunity%2520to%2520develop%2520a%2520more%2520automated%2520and%250Auser-friendly%2520framework.%2520To%2520this%2520end%252C%2520we%2520introduce%2520AutoM3L%252C%2520an%2520innovative%250AAutomated%2520Multimodal%2520Machine%2520Learning%2520framework%2520that%2520leverages%2520LLMs%2520as%250Acontrollers%2520to%2520automatically%2520construct%2520multimodal%2520training%2520pipelines.%2520AutoM3L%250Acomprehends%2520data%2520modalities%2520and%2520selects%2520appropriate%2520models%2520based%2520on%2520user%250Arequirements%252C%2520providing%2520automation%2520and%2520interactivity.%2520By%2520eliminating%2520the%2520need%250Afor%2520manual%2520feature%2520engineering%2520and%2520hyperparameter%2520optimization%252C%2520our%2520framework%250Asimplifies%2520user%2520engagement%2520and%2520enables%2520customization%2520through%2520directives%252C%250Aaddressing%2520the%2520limitations%2520of%2520previous%2520rule-based%2520AutoML%2520approaches.%2520We%250Aevaluate%2520the%2520performance%2520of%2520AutoM3L%2520on%2520six%2520diverse%2520multimodal%2520datasets%2520spanning%250Aclassification%252C%2520regression%252C%2520and%2520retrieval%2520tasks%252C%2520as%2520well%2520as%2520a%2520comprehensive%2520set%250Aof%2520unimodal%2520datasets.%2520The%2520results%2520demonstrate%2520that%2520AutoM3L%2520achieves%2520competitive%250Aor%2520superior%2520performance%2520compared%2520to%2520traditional%2520rule-based%2520AutoML%2520methods.%250AFurthermore%252C%2520a%2520user%2520study%2520highlights%2520the%2520user-friendliness%2520and%2520usability%2520of%2520our%250Aframework%252C%2520compared%2520to%2520the%2520rule-based%2520AutoML%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoM3L%3A%20An%20Automated%20Multimodal%20Machine%20Learning%20Framework%20with%20Large%0A%20%20Language%20Models&entry.906535625=Daqin%20Luo%20and%20Chengjian%20Feng%20and%20Yuxuan%20Nong%20and%20Yiqing%20Shen&entry.1292438233=%20%20Automated%20Machine%20Learning%20%28AutoML%29%20offers%20a%20promising%20approach%20to%20streamline%0Athe%20training%20of%20machine%20learning%20models.%20However%2C%20existing%20AutoML%20frameworks%0Aare%20often%20limited%20to%20unimodal%20scenarios%20and%20require%20extensive%20manual%0Aconfiguration.%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%0Ashowcased%20their%20exceptional%20abilities%20in%20reasoning%2C%20interaction%2C%20and%20code%0Ageneration%2C%20presenting%20an%20opportunity%20to%20develop%20a%20more%20automated%20and%0Auser-friendly%20framework.%20To%20this%20end%2C%20we%20introduce%20AutoM3L%2C%20an%20innovative%0AAutomated%20Multimodal%20Machine%20Learning%20framework%20that%20leverages%20LLMs%20as%0Acontrollers%20to%20automatically%20construct%20multimodal%20training%20pipelines.%20AutoM3L%0Acomprehends%20data%20modalities%20and%20selects%20appropriate%20models%20based%20on%20user%0Arequirements%2C%20providing%20automation%20and%20interactivity.%20By%20eliminating%20the%20need%0Afor%20manual%20feature%20engineering%20and%20hyperparameter%20optimization%2C%20our%20framework%0Asimplifies%20user%20engagement%20and%20enables%20customization%20through%20directives%2C%0Aaddressing%20the%20limitations%20of%20previous%20rule-based%20AutoML%20approaches.%20We%0Aevaluate%20the%20performance%20of%20AutoM3L%20on%20six%20diverse%20multimodal%20datasets%20spanning%0Aclassification%2C%20regression%2C%20and%20retrieval%20tasks%2C%20as%20well%20as%20a%20comprehensive%20set%0Aof%20unimodal%20datasets.%20The%20results%20demonstrate%20that%20AutoM3L%20achieves%20competitive%0Aor%20superior%20performance%20compared%20to%20traditional%20rule-based%20AutoML%20methods.%0AFurthermore%2C%20a%20user%20study%20highlights%20the%20user-friendliness%20and%20usability%20of%20our%0Aframework%2C%20compared%20to%20the%20rule-based%20AutoML%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00665v1&entry.124074799=Read"},
{"title": "GNSS/Multi-Sensor Fusion Using Continuous-Time Factor Graph Optimization\n  for Robust Localization", "author": "Haoming Zhang and Chih-Chun Chen and Heike Vallery and Timothy D. Barfoot", "abstract": "  Accurate and robust vehicle localization in highly urbanized areas is\nchallenging. Sensors are often corrupted in those complicated and large-scale\nenvironments. This paper introduces GNSS-FGO, an online and global trajectory\nestimator that fuses GNSS observations alongside multiple sensor measurements\nfor robust vehicle localization. In GNSS-FGO, we fuse asynchronous sensor\nmeasurements into the graph with a continuous-time trajectory representation\nusing Gaussian process regression. This enables querying states at arbitrary\ntimestamps so that sensor observations are fused without requiring strict state\nand measurement synchronization. Thus, the proposed method presents a\ngeneralized factor graph for multi-sensor fusion. To evaluate and study\ndifferent GNSS fusion strategies, we fuse GNSS measurements in loose and tight\ncoupling with a speed sensor, IMU, and lidar-odometry. We employed datasets\nfrom measurement campaigns in Aachen, Duesseldorf, and Cologne in experimental\nstudies and presented comprehensive discussions on sensor observations,\nsmoother types, and hyperparameter tuning. Our results show that the proposed\napproach enables robust trajectory estimation in dense urban areas, where the\nclassic multi-sensor fusion method fails due to sensor degradation. In a test\nsequence containing a 17km route through Aachen, the proposed method results in\na mean 2D positioning error of 0.48m while fusing raw GNSS observations with\nlidar odometry in a tight coupling.\n", "link": "http://arxiv.org/abs/2309.11134v3", "date": "2024-08-01", "relevancy": 2.1756, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5538}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNSS/Multi-Sensor%20Fusion%20Using%20Continuous-Time%20Factor%20Graph%20Optimization%0A%20%20for%20Robust%20Localization&body=Title%3A%20GNSS/Multi-Sensor%20Fusion%20Using%20Continuous-Time%20Factor%20Graph%20Optimization%0A%20%20for%20Robust%20Localization%0AAuthor%3A%20Haoming%20Zhang%20and%20Chih-Chun%20Chen%20and%20Heike%20Vallery%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20Accurate%20and%20robust%20vehicle%20localization%20in%20highly%20urbanized%20areas%20is%0Achallenging.%20Sensors%20are%20often%20corrupted%20in%20those%20complicated%20and%20large-scale%0Aenvironments.%20This%20paper%20introduces%20GNSS-FGO%2C%20an%20online%20and%20global%20trajectory%0Aestimator%20that%20fuses%20GNSS%20observations%20alongside%20multiple%20sensor%20measurements%0Afor%20robust%20vehicle%20localization.%20In%20GNSS-FGO%2C%20we%20fuse%20asynchronous%20sensor%0Ameasurements%20into%20the%20graph%20with%20a%20continuous-time%20trajectory%20representation%0Ausing%20Gaussian%20process%20regression.%20This%20enables%20querying%20states%20at%20arbitrary%0Atimestamps%20so%20that%20sensor%20observations%20are%20fused%20without%20requiring%20strict%20state%0Aand%20measurement%20synchronization.%20Thus%2C%20the%20proposed%20method%20presents%20a%0Ageneralized%20factor%20graph%20for%20multi-sensor%20fusion.%20To%20evaluate%20and%20study%0Adifferent%20GNSS%20fusion%20strategies%2C%20we%20fuse%20GNSS%20measurements%20in%20loose%20and%20tight%0Acoupling%20with%20a%20speed%20sensor%2C%20IMU%2C%20and%20lidar-odometry.%20We%20employed%20datasets%0Afrom%20measurement%20campaigns%20in%20Aachen%2C%20Duesseldorf%2C%20and%20Cologne%20in%20experimental%0Astudies%20and%20presented%20comprehensive%20discussions%20on%20sensor%20observations%2C%0Asmoother%20types%2C%20and%20hyperparameter%20tuning.%20Our%20results%20show%20that%20the%20proposed%0Aapproach%20enables%20robust%20trajectory%20estimation%20in%20dense%20urban%20areas%2C%20where%20the%0Aclassic%20multi-sensor%20fusion%20method%20fails%20due%20to%20sensor%20degradation.%20In%20a%20test%0Asequence%20containing%20a%2017km%20route%20through%20Aachen%2C%20the%20proposed%20method%20results%20in%0Aa%20mean%202D%20positioning%20error%20of%200.48m%20while%20fusing%20raw%20GNSS%20observations%20with%0Alidar%20odometry%20in%20a%20tight%20coupling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11134v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNSS/Multi-Sensor%2520Fusion%2520Using%2520Continuous-Time%2520Factor%2520Graph%2520Optimization%250A%2520%2520for%2520Robust%2520Localization%26entry.906535625%3DHaoming%2520Zhang%2520and%2520Chih-Chun%2520Chen%2520and%2520Heike%2520Vallery%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520Accurate%2520and%2520robust%2520vehicle%2520localization%2520in%2520highly%2520urbanized%2520areas%2520is%250Achallenging.%2520Sensors%2520are%2520often%2520corrupted%2520in%2520those%2520complicated%2520and%2520large-scale%250Aenvironments.%2520This%2520paper%2520introduces%2520GNSS-FGO%252C%2520an%2520online%2520and%2520global%2520trajectory%250Aestimator%2520that%2520fuses%2520GNSS%2520observations%2520alongside%2520multiple%2520sensor%2520measurements%250Afor%2520robust%2520vehicle%2520localization.%2520In%2520GNSS-FGO%252C%2520we%2520fuse%2520asynchronous%2520sensor%250Ameasurements%2520into%2520the%2520graph%2520with%2520a%2520continuous-time%2520trajectory%2520representation%250Ausing%2520Gaussian%2520process%2520regression.%2520This%2520enables%2520querying%2520states%2520at%2520arbitrary%250Atimestamps%2520so%2520that%2520sensor%2520observations%2520are%2520fused%2520without%2520requiring%2520strict%2520state%250Aand%2520measurement%2520synchronization.%2520Thus%252C%2520the%2520proposed%2520method%2520presents%2520a%250Ageneralized%2520factor%2520graph%2520for%2520multi-sensor%2520fusion.%2520To%2520evaluate%2520and%2520study%250Adifferent%2520GNSS%2520fusion%2520strategies%252C%2520we%2520fuse%2520GNSS%2520measurements%2520in%2520loose%2520and%2520tight%250Acoupling%2520with%2520a%2520speed%2520sensor%252C%2520IMU%252C%2520and%2520lidar-odometry.%2520We%2520employed%2520datasets%250Afrom%2520measurement%2520campaigns%2520in%2520Aachen%252C%2520Duesseldorf%252C%2520and%2520Cologne%2520in%2520experimental%250Astudies%2520and%2520presented%2520comprehensive%2520discussions%2520on%2520sensor%2520observations%252C%250Asmoother%2520types%252C%2520and%2520hyperparameter%2520tuning.%2520Our%2520results%2520show%2520that%2520the%2520proposed%250Aapproach%2520enables%2520robust%2520trajectory%2520estimation%2520in%2520dense%2520urban%2520areas%252C%2520where%2520the%250Aclassic%2520multi-sensor%2520fusion%2520method%2520fails%2520due%2520to%2520sensor%2520degradation.%2520In%2520a%2520test%250Asequence%2520containing%2520a%252017km%2520route%2520through%2520Aachen%252C%2520the%2520proposed%2520method%2520results%2520in%250Aa%2520mean%25202D%2520positioning%2520error%2520of%25200.48m%2520while%2520fusing%2520raw%2520GNSS%2520observations%2520with%250Alidar%2520odometry%2520in%2520a%2520tight%2520coupling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11134v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNSS/Multi-Sensor%20Fusion%20Using%20Continuous-Time%20Factor%20Graph%20Optimization%0A%20%20for%20Robust%20Localization&entry.906535625=Haoming%20Zhang%20and%20Chih-Chun%20Chen%20and%20Heike%20Vallery%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20Accurate%20and%20robust%20vehicle%20localization%20in%20highly%20urbanized%20areas%20is%0Achallenging.%20Sensors%20are%20often%20corrupted%20in%20those%20complicated%20and%20large-scale%0Aenvironments.%20This%20paper%20introduces%20GNSS-FGO%2C%20an%20online%20and%20global%20trajectory%0Aestimator%20that%20fuses%20GNSS%20observations%20alongside%20multiple%20sensor%20measurements%0Afor%20robust%20vehicle%20localization.%20In%20GNSS-FGO%2C%20we%20fuse%20asynchronous%20sensor%0Ameasurements%20into%20the%20graph%20with%20a%20continuous-time%20trajectory%20representation%0Ausing%20Gaussian%20process%20regression.%20This%20enables%20querying%20states%20at%20arbitrary%0Atimestamps%20so%20that%20sensor%20observations%20are%20fused%20without%20requiring%20strict%20state%0Aand%20measurement%20synchronization.%20Thus%2C%20the%20proposed%20method%20presents%20a%0Ageneralized%20factor%20graph%20for%20multi-sensor%20fusion.%20To%20evaluate%20and%20study%0Adifferent%20GNSS%20fusion%20strategies%2C%20we%20fuse%20GNSS%20measurements%20in%20loose%20and%20tight%0Acoupling%20with%20a%20speed%20sensor%2C%20IMU%2C%20and%20lidar-odometry.%20We%20employed%20datasets%0Afrom%20measurement%20campaigns%20in%20Aachen%2C%20Duesseldorf%2C%20and%20Cologne%20in%20experimental%0Astudies%20and%20presented%20comprehensive%20discussions%20on%20sensor%20observations%2C%0Asmoother%20types%2C%20and%20hyperparameter%20tuning.%20Our%20results%20show%20that%20the%20proposed%0Aapproach%20enables%20robust%20trajectory%20estimation%20in%20dense%20urban%20areas%2C%20where%20the%0Aclassic%20multi-sensor%20fusion%20method%20fails%20due%20to%20sensor%20degradation.%20In%20a%20test%0Asequence%20containing%20a%2017km%20route%20through%20Aachen%2C%20the%20proposed%20method%20results%20in%0Aa%20mean%202D%20positioning%20error%20of%200.48m%20while%20fusing%20raw%20GNSS%20observations%20with%0Alidar%20odometry%20in%20a%20tight%20coupling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11134v3&entry.124074799=Read"},
{"title": "Contrastive Learning with Dynamic Localized Repulsion for Brain Age\n  Prediction on 3D Stiffness Maps", "author": "Jakob Tr\u00e4uble and Lucy Hiscox and Curtis Johnson and Carola-Bibiane Sch\u00f6nlieb and Gabriele Kaminski Schierle and Angelica Aviles-Rivero", "abstract": "  In the field of neuroimaging, accurate brain age prediction is pivotal for\nuncovering the complexities of brain aging and pinpointing early indicators of\nneurodegenerative conditions. Recent advancements in self-supervised learning,\nparticularly in contrastive learning, have demonstrated greater robustness when\ndealing with complex datasets. However, current approaches often fall short in\ngeneralizing across non-uniformly distributed data, prevalent in medical\nimaging scenarios. To bridge this gap, we introduce a novel contrastive loss\nthat adapts dynamically during the training process, focusing on the localized\nneighborhoods of samples. Moreover, we expand beyond traditional structural\nfeatures by incorporating brain stiffness, a mechanical property previously\nunderexplored yet promising due to its sensitivity to age-related changes. This\nwork presents the first application of self-supervised learning to brain\nmechanical properties, using compiled stiffness maps from various clinical\nstudies to predict brain age. Our approach, featuring dynamic localized loss,\nconsistently outperforms existing state-of-the-art methods, demonstrating\nsuperior performance and laying the way for new directions in brain aging\nresearch.\n", "link": "http://arxiv.org/abs/2408.00527v1", "date": "2024-08-01", "relevancy": 2.1754, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5673}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Learning%20with%20Dynamic%20Localized%20Repulsion%20for%20Brain%20Age%0A%20%20Prediction%20on%203D%20Stiffness%20Maps&body=Title%3A%20Contrastive%20Learning%20with%20Dynamic%20Localized%20Repulsion%20for%20Brain%20Age%0A%20%20Prediction%20on%203D%20Stiffness%20Maps%0AAuthor%3A%20Jakob%20Tr%C3%A4uble%20and%20Lucy%20Hiscox%20and%20Curtis%20Johnson%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Gabriele%20Kaminski%20Schierle%20and%20Angelica%20Aviles-Rivero%0AAbstract%3A%20%20%20In%20the%20field%20of%20neuroimaging%2C%20accurate%20brain%20age%20prediction%20is%20pivotal%20for%0Auncovering%20the%20complexities%20of%20brain%20aging%20and%20pinpointing%20early%20indicators%20of%0Aneurodegenerative%20conditions.%20Recent%20advancements%20in%20self-supervised%20learning%2C%0Aparticularly%20in%20contrastive%20learning%2C%20have%20demonstrated%20greater%20robustness%20when%0Adealing%20with%20complex%20datasets.%20However%2C%20current%20approaches%20often%20fall%20short%20in%0Ageneralizing%20across%20non-uniformly%20distributed%20data%2C%20prevalent%20in%20medical%0Aimaging%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20contrastive%20loss%0Athat%20adapts%20dynamically%20during%20the%20training%20process%2C%20focusing%20on%20the%20localized%0Aneighborhoods%20of%20samples.%20Moreover%2C%20we%20expand%20beyond%20traditional%20structural%0Afeatures%20by%20incorporating%20brain%20stiffness%2C%20a%20mechanical%20property%20previously%0Aunderexplored%20yet%20promising%20due%20to%20its%20sensitivity%20to%20age-related%20changes.%20This%0Awork%20presents%20the%20first%20application%20of%20self-supervised%20learning%20to%20brain%0Amechanical%20properties%2C%20using%20compiled%20stiffness%20maps%20from%20various%20clinical%0Astudies%20to%20predict%20brain%20age.%20Our%20approach%2C%20featuring%20dynamic%20localized%20loss%2C%0Aconsistently%20outperforms%20existing%20state-of-the-art%20methods%2C%20demonstrating%0Asuperior%20performance%20and%20laying%20the%20way%20for%20new%20directions%20in%20brain%20aging%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Learning%2520with%2520Dynamic%2520Localized%2520Repulsion%2520for%2520Brain%2520Age%250A%2520%2520Prediction%2520on%25203D%2520Stiffness%2520Maps%26entry.906535625%3DJakob%2520Tr%25C3%25A4uble%2520and%2520Lucy%2520Hiscox%2520and%2520Curtis%2520Johnson%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Gabriele%2520Kaminski%2520Schierle%2520and%2520Angelica%2520Aviles-Rivero%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520neuroimaging%252C%2520accurate%2520brain%2520age%2520prediction%2520is%2520pivotal%2520for%250Auncovering%2520the%2520complexities%2520of%2520brain%2520aging%2520and%2520pinpointing%2520early%2520indicators%2520of%250Aneurodegenerative%2520conditions.%2520Recent%2520advancements%2520in%2520self-supervised%2520learning%252C%250Aparticularly%2520in%2520contrastive%2520learning%252C%2520have%2520demonstrated%2520greater%2520robustness%2520when%250Adealing%2520with%2520complex%2520datasets.%2520However%252C%2520current%2520approaches%2520often%2520fall%2520short%2520in%250Ageneralizing%2520across%2520non-uniformly%2520distributed%2520data%252C%2520prevalent%2520in%2520medical%250Aimaging%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520contrastive%2520loss%250Athat%2520adapts%2520dynamically%2520during%2520the%2520training%2520process%252C%2520focusing%2520on%2520the%2520localized%250Aneighborhoods%2520of%2520samples.%2520Moreover%252C%2520we%2520expand%2520beyond%2520traditional%2520structural%250Afeatures%2520by%2520incorporating%2520brain%2520stiffness%252C%2520a%2520mechanical%2520property%2520previously%250Aunderexplored%2520yet%2520promising%2520due%2520to%2520its%2520sensitivity%2520to%2520age-related%2520changes.%2520This%250Awork%2520presents%2520the%2520first%2520application%2520of%2520self-supervised%2520learning%2520to%2520brain%250Amechanical%2520properties%252C%2520using%2520compiled%2520stiffness%2520maps%2520from%2520various%2520clinical%250Astudies%2520to%2520predict%2520brain%2520age.%2520Our%2520approach%252C%2520featuring%2520dynamic%2520localized%2520loss%252C%250Aconsistently%2520outperforms%2520existing%2520state-of-the-art%2520methods%252C%2520demonstrating%250Asuperior%2520performance%2520and%2520laying%2520the%2520way%2520for%2520new%2520directions%2520in%2520brain%2520aging%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Learning%20with%20Dynamic%20Localized%20Repulsion%20for%20Brain%20Age%0A%20%20Prediction%20on%203D%20Stiffness%20Maps&entry.906535625=Jakob%20Tr%C3%A4uble%20and%20Lucy%20Hiscox%20and%20Curtis%20Johnson%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Gabriele%20Kaminski%20Schierle%20and%20Angelica%20Aviles-Rivero&entry.1292438233=%20%20In%20the%20field%20of%20neuroimaging%2C%20accurate%20brain%20age%20prediction%20is%20pivotal%20for%0Auncovering%20the%20complexities%20of%20brain%20aging%20and%20pinpointing%20early%20indicators%20of%0Aneurodegenerative%20conditions.%20Recent%20advancements%20in%20self-supervised%20learning%2C%0Aparticularly%20in%20contrastive%20learning%2C%20have%20demonstrated%20greater%20robustness%20when%0Adealing%20with%20complex%20datasets.%20However%2C%20current%20approaches%20often%20fall%20short%20in%0Ageneralizing%20across%20non-uniformly%20distributed%20data%2C%20prevalent%20in%20medical%0Aimaging%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20novel%20contrastive%20loss%0Athat%20adapts%20dynamically%20during%20the%20training%20process%2C%20focusing%20on%20the%20localized%0Aneighborhoods%20of%20samples.%20Moreover%2C%20we%20expand%20beyond%20traditional%20structural%0Afeatures%20by%20incorporating%20brain%20stiffness%2C%20a%20mechanical%20property%20previously%0Aunderexplored%20yet%20promising%20due%20to%20its%20sensitivity%20to%20age-related%20changes.%20This%0Awork%20presents%20the%20first%20application%20of%20self-supervised%20learning%20to%20brain%0Amechanical%20properties%2C%20using%20compiled%20stiffness%20maps%20from%20various%20clinical%0Astudies%20to%20predict%20brain%20age.%20Our%20approach%2C%20featuring%20dynamic%20localized%20loss%2C%0Aconsistently%20outperforms%20existing%20state-of-the-art%20methods%2C%20demonstrating%0Asuperior%20performance%20and%20laying%20the%20way%20for%20new%20directions%20in%20brain%20aging%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00527v1&entry.124074799=Read"},
{"title": "Hilbert curves for efficient exploratory landscape analysis\n  neighbourhood sampling", "author": "Johannes J. Pienaar and Anna S. Bosman and Katherine M. Malan", "abstract": "  Landscape analysis aims to characterise optimisation problems based on their\nobjective (or fitness) function landscape properties. The problem search space\nis typically sampled, and various landscape features are estimated based on the\nsamples. One particularly salient set of features is information content, which\nrequires the samples to be sequences of neighbouring solutions, such that the\nlocal relationships between consecutive sample points are preserved. Generating\nsuch spatially correlated samples that also provide good search space coverage\nis challenging. It is therefore common to first obtain an unordered sample with\ngood search space coverage, and then apply an ordering algorithm such as the\nnearest neighbour to minimise the distance between consecutive points in the\nsample. However, the nearest neighbour algorithm becomes computationally\nprohibitive in higher dimensions, thus there is a need for more efficient\nalternatives. In this study, Hilbert space-filling curves are proposed as a\nmethod to efficiently obtain high-quality ordered samples. Hilbert curves are a\nspecial case of fractal curves, and guarantee uniform coverage of a bounded\nsearch space while providing a spatially correlated sample. We study the\neffectiveness of Hilbert curves as samplers, and discover that they are capable\nof extracting salient features at a fraction of the computational cost compared\nto Latin hypercube sampling with post-factum ordering. Further, we investigate\nthe use of Hilbert curves as an ordering strategy, and find that they order the\nsample significantly faster than the nearest neighbour ordering, without\nsacrificing the saliency of the extracted features.\n", "link": "http://arxiv.org/abs/2408.00526v1", "date": "2024-08-01", "relevancy": 2.1554, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4458}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.424}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hilbert%20curves%20for%20efficient%20exploratory%20landscape%20analysis%0A%20%20neighbourhood%20sampling&body=Title%3A%20Hilbert%20curves%20for%20efficient%20exploratory%20landscape%20analysis%0A%20%20neighbourhood%20sampling%0AAuthor%3A%20Johannes%20J.%20Pienaar%20and%20Anna%20S.%20Bosman%20and%20Katherine%20M.%20Malan%0AAbstract%3A%20%20%20Landscape%20analysis%20aims%20to%20characterise%20optimisation%20problems%20based%20on%20their%0Aobjective%20%28or%20fitness%29%20function%20landscape%20properties.%20The%20problem%20search%20space%0Ais%20typically%20sampled%2C%20and%20various%20landscape%20features%20are%20estimated%20based%20on%20the%0Asamples.%20One%20particularly%20salient%20set%20of%20features%20is%20information%20content%2C%20which%0Arequires%20the%20samples%20to%20be%20sequences%20of%20neighbouring%20solutions%2C%20such%20that%20the%0Alocal%20relationships%20between%20consecutive%20sample%20points%20are%20preserved.%20Generating%0Asuch%20spatially%20correlated%20samples%20that%20also%20provide%20good%20search%20space%20coverage%0Ais%20challenging.%20It%20is%20therefore%20common%20to%20first%20obtain%20an%20unordered%20sample%20with%0Agood%20search%20space%20coverage%2C%20and%20then%20apply%20an%20ordering%20algorithm%20such%20as%20the%0Anearest%20neighbour%20to%20minimise%20the%20distance%20between%20consecutive%20points%20in%20the%0Asample.%20However%2C%20the%20nearest%20neighbour%20algorithm%20becomes%20computationally%0Aprohibitive%20in%20higher%20dimensions%2C%20thus%20there%20is%20a%20need%20for%20more%20efficient%0Aalternatives.%20In%20this%20study%2C%20Hilbert%20space-filling%20curves%20are%20proposed%20as%20a%0Amethod%20to%20efficiently%20obtain%20high-quality%20ordered%20samples.%20Hilbert%20curves%20are%20a%0Aspecial%20case%20of%20fractal%20curves%2C%20and%20guarantee%20uniform%20coverage%20of%20a%20bounded%0Asearch%20space%20while%20providing%20a%20spatially%20correlated%20sample.%20We%20study%20the%0Aeffectiveness%20of%20Hilbert%20curves%20as%20samplers%2C%20and%20discover%20that%20they%20are%20capable%0Aof%20extracting%20salient%20features%20at%20a%20fraction%20of%20the%20computational%20cost%20compared%0Ato%20Latin%20hypercube%20sampling%20with%20post-factum%20ordering.%20Further%2C%20we%20investigate%0Athe%20use%20of%20Hilbert%20curves%20as%20an%20ordering%20strategy%2C%20and%20find%20that%20they%20order%20the%0Asample%20significantly%20faster%20than%20the%20nearest%20neighbour%20ordering%2C%20without%0Asacrificing%20the%20saliency%20of%20the%20extracted%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHilbert%2520curves%2520for%2520efficient%2520exploratory%2520landscape%2520analysis%250A%2520%2520neighbourhood%2520sampling%26entry.906535625%3DJohannes%2520J.%2520Pienaar%2520and%2520Anna%2520S.%2520Bosman%2520and%2520Katherine%2520M.%2520Malan%26entry.1292438233%3D%2520%2520Landscape%2520analysis%2520aims%2520to%2520characterise%2520optimisation%2520problems%2520based%2520on%2520their%250Aobjective%2520%2528or%2520fitness%2529%2520function%2520landscape%2520properties.%2520The%2520problem%2520search%2520space%250Ais%2520typically%2520sampled%252C%2520and%2520various%2520landscape%2520features%2520are%2520estimated%2520based%2520on%2520the%250Asamples.%2520One%2520particularly%2520salient%2520set%2520of%2520features%2520is%2520information%2520content%252C%2520which%250Arequires%2520the%2520samples%2520to%2520be%2520sequences%2520of%2520neighbouring%2520solutions%252C%2520such%2520that%2520the%250Alocal%2520relationships%2520between%2520consecutive%2520sample%2520points%2520are%2520preserved.%2520Generating%250Asuch%2520spatially%2520correlated%2520samples%2520that%2520also%2520provide%2520good%2520search%2520space%2520coverage%250Ais%2520challenging.%2520It%2520is%2520therefore%2520common%2520to%2520first%2520obtain%2520an%2520unordered%2520sample%2520with%250Agood%2520search%2520space%2520coverage%252C%2520and%2520then%2520apply%2520an%2520ordering%2520algorithm%2520such%2520as%2520the%250Anearest%2520neighbour%2520to%2520minimise%2520the%2520distance%2520between%2520consecutive%2520points%2520in%2520the%250Asample.%2520However%252C%2520the%2520nearest%2520neighbour%2520algorithm%2520becomes%2520computationally%250Aprohibitive%2520in%2520higher%2520dimensions%252C%2520thus%2520there%2520is%2520a%2520need%2520for%2520more%2520efficient%250Aalternatives.%2520In%2520this%2520study%252C%2520Hilbert%2520space-filling%2520curves%2520are%2520proposed%2520as%2520a%250Amethod%2520to%2520efficiently%2520obtain%2520high-quality%2520ordered%2520samples.%2520Hilbert%2520curves%2520are%2520a%250Aspecial%2520case%2520of%2520fractal%2520curves%252C%2520and%2520guarantee%2520uniform%2520coverage%2520of%2520a%2520bounded%250Asearch%2520space%2520while%2520providing%2520a%2520spatially%2520correlated%2520sample.%2520We%2520study%2520the%250Aeffectiveness%2520of%2520Hilbert%2520curves%2520as%2520samplers%252C%2520and%2520discover%2520that%2520they%2520are%2520capable%250Aof%2520extracting%2520salient%2520features%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520cost%2520compared%250Ato%2520Latin%2520hypercube%2520sampling%2520with%2520post-factum%2520ordering.%2520Further%252C%2520we%2520investigate%250Athe%2520use%2520of%2520Hilbert%2520curves%2520as%2520an%2520ordering%2520strategy%252C%2520and%2520find%2520that%2520they%2520order%2520the%250Asample%2520significantly%2520faster%2520than%2520the%2520nearest%2520neighbour%2520ordering%252C%2520without%250Asacrificing%2520the%2520saliency%2520of%2520the%2520extracted%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hilbert%20curves%20for%20efficient%20exploratory%20landscape%20analysis%0A%20%20neighbourhood%20sampling&entry.906535625=Johannes%20J.%20Pienaar%20and%20Anna%20S.%20Bosman%20and%20Katherine%20M.%20Malan&entry.1292438233=%20%20Landscape%20analysis%20aims%20to%20characterise%20optimisation%20problems%20based%20on%20their%0Aobjective%20%28or%20fitness%29%20function%20landscape%20properties.%20The%20problem%20search%20space%0Ais%20typically%20sampled%2C%20and%20various%20landscape%20features%20are%20estimated%20based%20on%20the%0Asamples.%20One%20particularly%20salient%20set%20of%20features%20is%20information%20content%2C%20which%0Arequires%20the%20samples%20to%20be%20sequences%20of%20neighbouring%20solutions%2C%20such%20that%20the%0Alocal%20relationships%20between%20consecutive%20sample%20points%20are%20preserved.%20Generating%0Asuch%20spatially%20correlated%20samples%20that%20also%20provide%20good%20search%20space%20coverage%0Ais%20challenging.%20It%20is%20therefore%20common%20to%20first%20obtain%20an%20unordered%20sample%20with%0Agood%20search%20space%20coverage%2C%20and%20then%20apply%20an%20ordering%20algorithm%20such%20as%20the%0Anearest%20neighbour%20to%20minimise%20the%20distance%20between%20consecutive%20points%20in%20the%0Asample.%20However%2C%20the%20nearest%20neighbour%20algorithm%20becomes%20computationally%0Aprohibitive%20in%20higher%20dimensions%2C%20thus%20there%20is%20a%20need%20for%20more%20efficient%0Aalternatives.%20In%20this%20study%2C%20Hilbert%20space-filling%20curves%20are%20proposed%20as%20a%0Amethod%20to%20efficiently%20obtain%20high-quality%20ordered%20samples.%20Hilbert%20curves%20are%20a%0Aspecial%20case%20of%20fractal%20curves%2C%20and%20guarantee%20uniform%20coverage%20of%20a%20bounded%0Asearch%20space%20while%20providing%20a%20spatially%20correlated%20sample.%20We%20study%20the%0Aeffectiveness%20of%20Hilbert%20curves%20as%20samplers%2C%20and%20discover%20that%20they%20are%20capable%0Aof%20extracting%20salient%20features%20at%20a%20fraction%20of%20the%20computational%20cost%20compared%0Ato%20Latin%20hypercube%20sampling%20with%20post-factum%20ordering.%20Further%2C%20we%20investigate%0Athe%20use%20of%20Hilbert%20curves%20as%20an%20ordering%20strategy%2C%20and%20find%20that%20they%20order%20the%0Asample%20significantly%20faster%20than%20the%20nearest%20neighbour%20ordering%2C%20without%0Asacrificing%20the%20saliency%20of%20the%20extracted%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00526v1&entry.124074799=Read"},
{"title": "How Effective are Self-Supervised Models for Contact Identification in\n  Videos", "author": "Malitha Gunawardhana and Limalka Sadith and Liel David and Daniel Harari and Muhammad Haris Khan", "abstract": "  The exploration of video content via Self-Supervised Learning (SSL) models\nhas unveiled a dynamic field of study, emphasizing both the complex challenges\nand unique opportunities inherent in this area. Despite the growing body of\nresearch, the ability of SSL models to detect physical contacts in videos\nremains largely unexplored, particularly the effectiveness of methods such as\ndownstream supervision with linear probing or full fine-tuning. This work aims\nto bridge this gap by employing eight different convolutional neural networks\n(CNNs) based video SSL models to identify instances of physical contact within\nvideo sequences specifically. The Something-Something v2 (SSv2) and\nEpic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due\nto the promising results on UCF101 and HMDB51, coupled with their limited prior\nassessment on SSv2 and EK-100. Additionally, these datasets feature diverse\nenvironments and scenarios, essential for testing the robustness and accuracy\nof video-based models. This approach not only examines the effectiveness of\neach model in recognizing physical contacts but also explores the performance\nin the action recognition downstream task. By doing so, valuable insights into\nthe adaptability of SSL models in interpreting complex, dynamic visual\ninformation are contributed.\n", "link": "http://arxiv.org/abs/2408.00498v1", "date": "2024-08-01", "relevancy": 2.1481, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.554}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5312}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Effective%20are%20Self-Supervised%20Models%20for%20Contact%20Identification%20in%0A%20%20Videos&body=Title%3A%20How%20Effective%20are%20Self-Supervised%20Models%20for%20Contact%20Identification%20in%0A%20%20Videos%0AAuthor%3A%20Malitha%20Gunawardhana%20and%20Limalka%20Sadith%20and%20Liel%20David%20and%20Daniel%20Harari%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20The%20exploration%20of%20video%20content%20via%20Self-Supervised%20Learning%20%28SSL%29%20models%0Ahas%20unveiled%20a%20dynamic%20field%20of%20study%2C%20emphasizing%20both%20the%20complex%20challenges%0Aand%20unique%20opportunities%20inherent%20in%20this%20area.%20Despite%20the%20growing%20body%20of%0Aresearch%2C%20the%20ability%20of%20SSL%20models%20to%20detect%20physical%20contacts%20in%20videos%0Aremains%20largely%20unexplored%2C%20particularly%20the%20effectiveness%20of%20methods%20such%20as%0Adownstream%20supervision%20with%20linear%20probing%20or%20full%20fine-tuning.%20This%20work%20aims%0Ato%20bridge%20this%20gap%20by%20employing%20eight%20different%20convolutional%20neural%20networks%0A%28CNNs%29%20based%20video%20SSL%20models%20to%20identify%20instances%20of%20physical%20contact%20within%0Avideo%20sequences%20specifically.%20The%20Something-Something%20v2%20%28SSv2%29%20and%0AEpic-Kitchen%20%28EK-100%29%20datasets%20were%20chosen%20for%20evaluating%20these%20approaches%20due%0Ato%20the%20promising%20results%20on%20UCF101%20and%20HMDB51%2C%20coupled%20with%20their%20limited%20prior%0Aassessment%20on%20SSv2%20and%20EK-100.%20Additionally%2C%20these%20datasets%20feature%20diverse%0Aenvironments%20and%20scenarios%2C%20essential%20for%20testing%20the%20robustness%20and%20accuracy%0Aof%20video-based%20models.%20This%20approach%20not%20only%20examines%20the%20effectiveness%20of%0Aeach%20model%20in%20recognizing%20physical%20contacts%20but%20also%20explores%20the%20performance%0Ain%20the%20action%20recognition%20downstream%20task.%20By%20doing%20so%2C%20valuable%20insights%20into%0Athe%20adaptability%20of%20SSL%20models%20in%20interpreting%20complex%2C%20dynamic%20visual%0Ainformation%20are%20contributed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Effective%2520are%2520Self-Supervised%2520Models%2520for%2520Contact%2520Identification%2520in%250A%2520%2520Videos%26entry.906535625%3DMalitha%2520Gunawardhana%2520and%2520Limalka%2520Sadith%2520and%2520Liel%2520David%2520and%2520Daniel%2520Harari%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520The%2520exploration%2520of%2520video%2520content%2520via%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520models%250Ahas%2520unveiled%2520a%2520dynamic%2520field%2520of%2520study%252C%2520emphasizing%2520both%2520the%2520complex%2520challenges%250Aand%2520unique%2520opportunities%2520inherent%2520in%2520this%2520area.%2520Despite%2520the%2520growing%2520body%2520of%250Aresearch%252C%2520the%2520ability%2520of%2520SSL%2520models%2520to%2520detect%2520physical%2520contacts%2520in%2520videos%250Aremains%2520largely%2520unexplored%252C%2520particularly%2520the%2520effectiveness%2520of%2520methods%2520such%2520as%250Adownstream%2520supervision%2520with%2520linear%2520probing%2520or%2520full%2520fine-tuning.%2520This%2520work%2520aims%250Ato%2520bridge%2520this%2520gap%2520by%2520employing%2520eight%2520different%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520based%2520video%2520SSL%2520models%2520to%2520identify%2520instances%2520of%2520physical%2520contact%2520within%250Avideo%2520sequences%2520specifically.%2520The%2520Something-Something%2520v2%2520%2528SSv2%2529%2520and%250AEpic-Kitchen%2520%2528EK-100%2529%2520datasets%2520were%2520chosen%2520for%2520evaluating%2520these%2520approaches%2520due%250Ato%2520the%2520promising%2520results%2520on%2520UCF101%2520and%2520HMDB51%252C%2520coupled%2520with%2520their%2520limited%2520prior%250Aassessment%2520on%2520SSv2%2520and%2520EK-100.%2520Additionally%252C%2520these%2520datasets%2520feature%2520diverse%250Aenvironments%2520and%2520scenarios%252C%2520essential%2520for%2520testing%2520the%2520robustness%2520and%2520accuracy%250Aof%2520video-based%2520models.%2520This%2520approach%2520not%2520only%2520examines%2520the%2520effectiveness%2520of%250Aeach%2520model%2520in%2520recognizing%2520physical%2520contacts%2520but%2520also%2520explores%2520the%2520performance%250Ain%2520the%2520action%2520recognition%2520downstream%2520task.%2520By%2520doing%2520so%252C%2520valuable%2520insights%2520into%250Athe%2520adaptability%2520of%2520SSL%2520models%2520in%2520interpreting%2520complex%252C%2520dynamic%2520visual%250Ainformation%2520are%2520contributed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Effective%20are%20Self-Supervised%20Models%20for%20Contact%20Identification%20in%0A%20%20Videos&entry.906535625=Malitha%20Gunawardhana%20and%20Limalka%20Sadith%20and%20Liel%20David%20and%20Daniel%20Harari%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20The%20exploration%20of%20video%20content%20via%20Self-Supervised%20Learning%20%28SSL%29%20models%0Ahas%20unveiled%20a%20dynamic%20field%20of%20study%2C%20emphasizing%20both%20the%20complex%20challenges%0Aand%20unique%20opportunities%20inherent%20in%20this%20area.%20Despite%20the%20growing%20body%20of%0Aresearch%2C%20the%20ability%20of%20SSL%20models%20to%20detect%20physical%20contacts%20in%20videos%0Aremains%20largely%20unexplored%2C%20particularly%20the%20effectiveness%20of%20methods%20such%20as%0Adownstream%20supervision%20with%20linear%20probing%20or%20full%20fine-tuning.%20This%20work%20aims%0Ato%20bridge%20this%20gap%20by%20employing%20eight%20different%20convolutional%20neural%20networks%0A%28CNNs%29%20based%20video%20SSL%20models%20to%20identify%20instances%20of%20physical%20contact%20within%0Avideo%20sequences%20specifically.%20The%20Something-Something%20v2%20%28SSv2%29%20and%0AEpic-Kitchen%20%28EK-100%29%20datasets%20were%20chosen%20for%20evaluating%20these%20approaches%20due%0Ato%20the%20promising%20results%20on%20UCF101%20and%20HMDB51%2C%20coupled%20with%20their%20limited%20prior%0Aassessment%20on%20SSv2%20and%20EK-100.%20Additionally%2C%20these%20datasets%20feature%20diverse%0Aenvironments%20and%20scenarios%2C%20essential%20for%20testing%20the%20robustness%20and%20accuracy%0Aof%20video-based%20models.%20This%20approach%20not%20only%20examines%20the%20effectiveness%20of%0Aeach%20model%20in%20recognizing%20physical%20contacts%20but%20also%20explores%20the%20performance%0Ain%20the%20action%20recognition%20downstream%20task.%20By%20doing%20so%2C%20valuable%20insights%20into%0Athe%20adaptability%20of%20SSL%20models%20in%20interpreting%20complex%2C%20dynamic%20visual%0Ainformation%20are%20contributed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00498v1&entry.124074799=Read"},
{"title": "Collecting Larg-Scale Robotic Datasets on a High-Speed Mobile Platform", "author": "Yuxin Lin and Jiaxuan Ma and Sizhe Gu and Jipeng Kong and Bowen Xu and Xiting Zhao and Dengji Zhao and Wenhan Cao and S\u00f6ren Schwertfeger", "abstract": "  Mobile robotics datasets are essential for research on robotics, for example\nfor research on Simultaneous Localization and Mapping (SLAM). Therefore the\nShanghaiTech Mapping Robot was constructed, that features a multitude\nhigh-performance sensors and a 16-node cluster to collect all this data. That\nrobot is based on a Clearpath Husky mobile base with a maximum speed of 1 meter\nper second. This is fine for indoor datasets, but to collect large-scale\noutdoor datasets a faster platform is needed. This system paper introduces our\nhigh-speed mobile platform for data collection. The mapping robot is secured on\nthe rear-steered flatbed car with maximum field of view. Additionally two\nencoders collect odometry data from two of the car wheels and an external\nsensor plate houses a downlooking RGB and event camera. With this setup a\ndataset of more than 10km in the underground parking garage and the outside of\nour campus was collected and is published with this paper.\n", "link": "http://arxiv.org/abs/2408.00545v1", "date": "2024-08-01", "relevancy": 2.1466, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5398}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5368}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collecting%20Larg-Scale%20Robotic%20Datasets%20on%20a%20High-Speed%20Mobile%20Platform&body=Title%3A%20Collecting%20Larg-Scale%20Robotic%20Datasets%20on%20a%20High-Speed%20Mobile%20Platform%0AAuthor%3A%20Yuxin%20Lin%20and%20Jiaxuan%20Ma%20and%20Sizhe%20Gu%20and%20Jipeng%20Kong%20and%20Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Dengji%20Zhao%20and%20Wenhan%20Cao%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20Mobile%20robotics%20datasets%20are%20essential%20for%20research%20on%20robotics%2C%20for%20example%0Afor%20research%20on%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29.%20Therefore%20the%0AShanghaiTech%20Mapping%20Robot%20was%20constructed%2C%20that%20features%20a%20multitude%0Ahigh-performance%20sensors%20and%20a%2016-node%20cluster%20to%20collect%20all%20this%20data.%20That%0Arobot%20is%20based%20on%20a%20Clearpath%20Husky%20mobile%20base%20with%20a%20maximum%20speed%20of%201%20meter%0Aper%20second.%20This%20is%20fine%20for%20indoor%20datasets%2C%20but%20to%20collect%20large-scale%0Aoutdoor%20datasets%20a%20faster%20platform%20is%20needed.%20This%20system%20paper%20introduces%20our%0Ahigh-speed%20mobile%20platform%20for%20data%20collection.%20The%20mapping%20robot%20is%20secured%20on%0Athe%20rear-steered%20flatbed%20car%20with%20maximum%20field%20of%20view.%20Additionally%20two%0Aencoders%20collect%20odometry%20data%20from%20two%20of%20the%20car%20wheels%20and%20an%20external%0Asensor%20plate%20houses%20a%20downlooking%20RGB%20and%20event%20camera.%20With%20this%20setup%20a%0Adataset%20of%20more%20than%2010km%20in%20the%20underground%20parking%20garage%20and%20the%20outside%20of%0Aour%20campus%20was%20collected%20and%20is%20published%20with%20this%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollecting%2520Larg-Scale%2520Robotic%2520Datasets%2520on%2520a%2520High-Speed%2520Mobile%2520Platform%26entry.906535625%3DYuxin%2520Lin%2520and%2520Jiaxuan%2520Ma%2520and%2520Sizhe%2520Gu%2520and%2520Jipeng%2520Kong%2520and%2520Bowen%2520Xu%2520and%2520Xiting%2520Zhao%2520and%2520Dengji%2520Zhao%2520and%2520Wenhan%2520Cao%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520Mobile%2520robotics%2520datasets%2520are%2520essential%2520for%2520research%2520on%2520robotics%252C%2520for%2520example%250Afor%2520research%2520on%2520Simultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529.%2520Therefore%2520the%250AShanghaiTech%2520Mapping%2520Robot%2520was%2520constructed%252C%2520that%2520features%2520a%2520multitude%250Ahigh-performance%2520sensors%2520and%2520a%252016-node%2520cluster%2520to%2520collect%2520all%2520this%2520data.%2520That%250Arobot%2520is%2520based%2520on%2520a%2520Clearpath%2520Husky%2520mobile%2520base%2520with%2520a%2520maximum%2520speed%2520of%25201%2520meter%250Aper%2520second.%2520This%2520is%2520fine%2520for%2520indoor%2520datasets%252C%2520but%2520to%2520collect%2520large-scale%250Aoutdoor%2520datasets%2520a%2520faster%2520platform%2520is%2520needed.%2520This%2520system%2520paper%2520introduces%2520our%250Ahigh-speed%2520mobile%2520platform%2520for%2520data%2520collection.%2520The%2520mapping%2520robot%2520is%2520secured%2520on%250Athe%2520rear-steered%2520flatbed%2520car%2520with%2520maximum%2520field%2520of%2520view.%2520Additionally%2520two%250Aencoders%2520collect%2520odometry%2520data%2520from%2520two%2520of%2520the%2520car%2520wheels%2520and%2520an%2520external%250Asensor%2520plate%2520houses%2520a%2520downlooking%2520RGB%2520and%2520event%2520camera.%2520With%2520this%2520setup%2520a%250Adataset%2520of%2520more%2520than%252010km%2520in%2520the%2520underground%2520parking%2520garage%2520and%2520the%2520outside%2520of%250Aour%2520campus%2520was%2520collected%2520and%2520is%2520published%2520with%2520this%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collecting%20Larg-Scale%20Robotic%20Datasets%20on%20a%20High-Speed%20Mobile%20Platform&entry.906535625=Yuxin%20Lin%20and%20Jiaxuan%20Ma%20and%20Sizhe%20Gu%20and%20Jipeng%20Kong%20and%20Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Dengji%20Zhao%20and%20Wenhan%20Cao%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20Mobile%20robotics%20datasets%20are%20essential%20for%20research%20on%20robotics%2C%20for%20example%0Afor%20research%20on%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29.%20Therefore%20the%0AShanghaiTech%20Mapping%20Robot%20was%20constructed%2C%20that%20features%20a%20multitude%0Ahigh-performance%20sensors%20and%20a%2016-node%20cluster%20to%20collect%20all%20this%20data.%20That%0Arobot%20is%20based%20on%20a%20Clearpath%20Husky%20mobile%20base%20with%20a%20maximum%20speed%20of%201%20meter%0Aper%20second.%20This%20is%20fine%20for%20indoor%20datasets%2C%20but%20to%20collect%20large-scale%0Aoutdoor%20datasets%20a%20faster%20platform%20is%20needed.%20This%20system%20paper%20introduces%20our%0Ahigh-speed%20mobile%20platform%20for%20data%20collection.%20The%20mapping%20robot%20is%20secured%20on%0Athe%20rear-steered%20flatbed%20car%20with%20maximum%20field%20of%20view.%20Additionally%20two%0Aencoders%20collect%20odometry%20data%20from%20two%20of%20the%20car%20wheels%20and%20an%20external%0Asensor%20plate%20houses%20a%20downlooking%20RGB%20and%20event%20camera.%20With%20this%20setup%20a%0Adataset%20of%20more%20than%2010km%20in%20the%20underground%20parking%20garage%20and%20the%20outside%20of%0Aour%20campus%20was%20collected%20and%20is%20published%20with%20this%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00545v1&entry.124074799=Read"},
{"title": "Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs", "author": "Francesco Di Salvo and David Tafler and Sebastian Doerrich and Christian Ledig", "abstract": "  Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .\n", "link": "http://arxiv.org/abs/2408.00639v1", "date": "2024-08-01", "relevancy": 2.1394, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5535}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5242}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Privacy-preserving%20datasets%20by%20capturing%20feature%20distributions%20with%0A%20%20Conditional%20VAEs&body=Title%3A%20Privacy-preserving%20datasets%20by%20capturing%20feature%20distributions%20with%0A%20%20Conditional%20VAEs%0AAuthor%3A%20Francesco%20Di%20Salvo%20and%20David%20Tafler%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig%0AAbstract%3A%20%20%20Large%20and%20well-annotated%20datasets%20are%20essential%20for%20advancing%20deep%20learning%0Aapplications%2C%20however%20often%20costly%20or%20impossible%20to%20obtain%20by%20a%20single%20entity.%0AIn%20many%20areas%2C%20including%20the%20medical%20domain%2C%20approaches%20relying%20on%20data%20sharing%0Ahave%20become%20critical%20to%20address%20those%20challenges.%20While%20effective%20in%20increasing%0Adataset%20size%20and%20diversity%2C%20data%20sharing%20raises%20significant%20privacy%20concerns.%0ACommonly%20employed%20anonymization%20methods%20based%20on%20the%20k-anonymity%20paradigm%20often%0Afail%20to%20preserve%20data%20diversity%2C%20affecting%20model%20robustness.%20This%20work%0Aintroduces%20a%20novel%20approach%20using%20Conditional%20Variational%20Autoencoders%20%28CVAEs%29%0Atrained%20on%20feature%20vectors%20extracted%20from%20large%20pre-trained%20vision%20foundation%0Amodels.%20Foundation%20models%20effectively%20detect%20and%20represent%20complex%20patterns%0Aacross%20diverse%20domains%2C%20allowing%20the%20CVAE%20to%20faithfully%20capture%20the%20embedding%0Aspace%20of%20a%20given%20data%20distribution%20to%20generate%20%28sample%29%20a%20diverse%2C%0Aprivacy-respecting%2C%20and%20potentially%20unbounded%20set%20of%20synthetic%20feature%20vectors.%0AOur%20method%20notably%20outperforms%20traditional%20approaches%20in%20both%20medical%20and%0Anatural%20image%20domains%2C%20exhibiting%20greater%20dataset%20diversity%20and%20higher%0Arobustness%20against%20perturbations%20while%20preserving%20sample%20privacy.%20These%20results%0Aunderscore%20the%20potential%20of%20generative%20models%20to%20significantly%20impact%20deep%0Alearning%20applications%20in%20data-scarce%20and%20privacy-sensitive%20environments.%20The%0Asource%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/cvae-anonymization%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivacy-preserving%2520datasets%2520by%2520capturing%2520feature%2520distributions%2520with%250A%2520%2520Conditional%2520VAEs%26entry.906535625%3DFrancesco%2520Di%2520Salvo%2520and%2520David%2520Tafler%2520and%2520Sebastian%2520Doerrich%2520and%2520Christian%2520Ledig%26entry.1292438233%3D%2520%2520Large%2520and%2520well-annotated%2520datasets%2520are%2520essential%2520for%2520advancing%2520deep%2520learning%250Aapplications%252C%2520however%2520often%2520costly%2520or%2520impossible%2520to%2520obtain%2520by%2520a%2520single%2520entity.%250AIn%2520many%2520areas%252C%2520including%2520the%2520medical%2520domain%252C%2520approaches%2520relying%2520on%2520data%2520sharing%250Ahave%2520become%2520critical%2520to%2520address%2520those%2520challenges.%2520While%2520effective%2520in%2520increasing%250Adataset%2520size%2520and%2520diversity%252C%2520data%2520sharing%2520raises%2520significant%2520privacy%2520concerns.%250ACommonly%2520employed%2520anonymization%2520methods%2520based%2520on%2520the%2520k-anonymity%2520paradigm%2520often%250Afail%2520to%2520preserve%2520data%2520diversity%252C%2520affecting%2520model%2520robustness.%2520This%2520work%250Aintroduces%2520a%2520novel%2520approach%2520using%2520Conditional%2520Variational%2520Autoencoders%2520%2528CVAEs%2529%250Atrained%2520on%2520feature%2520vectors%2520extracted%2520from%2520large%2520pre-trained%2520vision%2520foundation%250Amodels.%2520Foundation%2520models%2520effectively%2520detect%2520and%2520represent%2520complex%2520patterns%250Aacross%2520diverse%2520domains%252C%2520allowing%2520the%2520CVAE%2520to%2520faithfully%2520capture%2520the%2520embedding%250Aspace%2520of%2520a%2520given%2520data%2520distribution%2520to%2520generate%2520%2528sample%2529%2520a%2520diverse%252C%250Aprivacy-respecting%252C%2520and%2520potentially%2520unbounded%2520set%2520of%2520synthetic%2520feature%2520vectors.%250AOur%2520method%2520notably%2520outperforms%2520traditional%2520approaches%2520in%2520both%2520medical%2520and%250Anatural%2520image%2520domains%252C%2520exhibiting%2520greater%2520dataset%2520diversity%2520and%2520higher%250Arobustness%2520against%2520perturbations%2520while%2520preserving%2520sample%2520privacy.%2520These%2520results%250Aunderscore%2520the%2520potential%2520of%2520generative%2520models%2520to%2520significantly%2520impact%2520deep%250Alearning%2520applications%2520in%2520data-scarce%2520and%2520privacy-sensitive%2520environments.%2520The%250Asource%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/francescodisalvo05/cvae-anonymization%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Privacy-preserving%20datasets%20by%20capturing%20feature%20distributions%20with%0A%20%20Conditional%20VAEs&entry.906535625=Francesco%20Di%20Salvo%20and%20David%20Tafler%20and%20Sebastian%20Doerrich%20and%20Christian%20Ledig&entry.1292438233=%20%20Large%20and%20well-annotated%20datasets%20are%20essential%20for%20advancing%20deep%20learning%0Aapplications%2C%20however%20often%20costly%20or%20impossible%20to%20obtain%20by%20a%20single%20entity.%0AIn%20many%20areas%2C%20including%20the%20medical%20domain%2C%20approaches%20relying%20on%20data%20sharing%0Ahave%20become%20critical%20to%20address%20those%20challenges.%20While%20effective%20in%20increasing%0Adataset%20size%20and%20diversity%2C%20data%20sharing%20raises%20significant%20privacy%20concerns.%0ACommonly%20employed%20anonymization%20methods%20based%20on%20the%20k-anonymity%20paradigm%20often%0Afail%20to%20preserve%20data%20diversity%2C%20affecting%20model%20robustness.%20This%20work%0Aintroduces%20a%20novel%20approach%20using%20Conditional%20Variational%20Autoencoders%20%28CVAEs%29%0Atrained%20on%20feature%20vectors%20extracted%20from%20large%20pre-trained%20vision%20foundation%0Amodels.%20Foundation%20models%20effectively%20detect%20and%20represent%20complex%20patterns%0Aacross%20diverse%20domains%2C%20allowing%20the%20CVAE%20to%20faithfully%20capture%20the%20embedding%0Aspace%20of%20a%20given%20data%20distribution%20to%20generate%20%28sample%29%20a%20diverse%2C%0Aprivacy-respecting%2C%20and%20potentially%20unbounded%20set%20of%20synthetic%20feature%20vectors.%0AOur%20method%20notably%20outperforms%20traditional%20approaches%20in%20both%20medical%20and%0Anatural%20image%20domains%2C%20exhibiting%20greater%20dataset%20diversity%20and%20higher%0Arobustness%20against%20perturbations%20while%20preserving%20sample%20privacy.%20These%20results%0Aunderscore%20the%20potential%20of%20generative%20models%20to%20significantly%20impact%20deep%0Alearning%20applications%20in%20data-scarce%20and%20privacy-sensitive%20environments.%20The%0Asource%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/cvae-anonymization%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00639v1&entry.124074799=Read"},
{"title": "Vivim: a Video Vision Mamba for Medical Video Segmentation", "author": "Yijun Yang and Zhaohu Xing and Lequan Yu and Chunwang Huang and Huazhu Fu and Lei Zhu", "abstract": "  Medical video segmentation gains increasing attention in clinical practice\ndue to the redundant dynamic references in video frames. However, traditional\nconvolutional neural networks have a limited receptive field and\ntransformer-based networks are mediocre in constructing long-term dependency\nfrom the perspective of computational complexity. This bottleneck poses a\nsignificant challenge when processing longer sequences in medical video\nanalysis tasks using available devices with limited memory. Recently, state\nspace models (SSMs), famous by Mamba, have exhibited impressive achievements in\nefficient long sequence modeling, which develops deep neural networks by\nexpanding the receptive field on many vision tasks significantly.\nUnfortunately, vanilla SSMs failed to simultaneously capture causal temporal\ncues and preserve non-casual spatial information. To this end, this paper\npresents a Video Vision Mamba-based framework, dubbed as Vivim, for medical\nvideo segmentation tasks. Our Vivim can effectively compress the long-term\nspatiotemporal representation into sequences at varying scales with our\ndesigned Temporal Mamba Block. We also introduce an improved boundary-aware\naffine constraint across frames to enhance the discriminative ability of Vivim\non ambiguous lesions. Extensive experiments on thyroid segmentation, breast\nlesion segmentation in ultrasound videos, and polyp segmentation in colonoscopy\nvideos demonstrate the effectiveness and efficiency of our Vivim, superior to\nexisting methods. The code is available at:\nhttps://github.com/scott-yjyang/Vivim. The dataset will be released once\naccepted.\n", "link": "http://arxiv.org/abs/2401.14168v4", "date": "2024-08-01", "relevancy": 2.1379, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5392}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vivim%3A%20a%20Video%20Vision%20Mamba%20for%20Medical%20Video%20Segmentation&body=Title%3A%20Vivim%3A%20a%20Video%20Vision%20Mamba%20for%20Medical%20Video%20Segmentation%0AAuthor%3A%20Yijun%20Yang%20and%20Zhaohu%20Xing%20and%20Lequan%20Yu%20and%20Chunwang%20Huang%20and%20Huazhu%20Fu%20and%20Lei%20Zhu%0AAbstract%3A%20%20%20Medical%20video%20segmentation%20gains%20increasing%20attention%20in%20clinical%20practice%0Adue%20to%20the%20redundant%20dynamic%20references%20in%20video%20frames.%20However%2C%20traditional%0Aconvolutional%20neural%20networks%20have%20a%20limited%20receptive%20field%20and%0Atransformer-based%20networks%20are%20mediocre%20in%20constructing%20long-term%20dependency%0Afrom%20the%20perspective%20of%20computational%20complexity.%20This%20bottleneck%20poses%20a%0Asignificant%20challenge%20when%20processing%20longer%20sequences%20in%20medical%20video%0Aanalysis%20tasks%20using%20available%20devices%20with%20limited%20memory.%20Recently%2C%20state%0Aspace%20models%20%28SSMs%29%2C%20famous%20by%20Mamba%2C%20have%20exhibited%20impressive%20achievements%20in%0Aefficient%20long%20sequence%20modeling%2C%20which%20develops%20deep%20neural%20networks%20by%0Aexpanding%20the%20receptive%20field%20on%20many%20vision%20tasks%20significantly.%0AUnfortunately%2C%20vanilla%20SSMs%20failed%20to%20simultaneously%20capture%20causal%20temporal%0Acues%20and%20preserve%20non-casual%20spatial%20information.%20To%20this%20end%2C%20this%20paper%0Apresents%20a%20Video%20Vision%20Mamba-based%20framework%2C%20dubbed%20as%20Vivim%2C%20for%20medical%0Avideo%20segmentation%20tasks.%20Our%20Vivim%20can%20effectively%20compress%20the%20long-term%0Aspatiotemporal%20representation%20into%20sequences%20at%20varying%20scales%20with%20our%0Adesigned%20Temporal%20Mamba%20Block.%20We%20also%20introduce%20an%20improved%20boundary-aware%0Aaffine%20constraint%20across%20frames%20to%20enhance%20the%20discriminative%20ability%20of%20Vivim%0Aon%20ambiguous%20lesions.%20Extensive%20experiments%20on%20thyroid%20segmentation%2C%20breast%0Alesion%20segmentation%20in%20ultrasound%20videos%2C%20and%20polyp%20segmentation%20in%20colonoscopy%0Avideos%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20Vivim%2C%20superior%20to%0Aexisting%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/scott-yjyang/Vivim.%20The%20dataset%20will%20be%20released%20once%0Aaccepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14168v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVivim%253A%2520a%2520Video%2520Vision%2520Mamba%2520for%2520Medical%2520Video%2520Segmentation%26entry.906535625%3DYijun%2520Yang%2520and%2520Zhaohu%2520Xing%2520and%2520Lequan%2520Yu%2520and%2520Chunwang%2520Huang%2520and%2520Huazhu%2520Fu%2520and%2520Lei%2520Zhu%26entry.1292438233%3D%2520%2520Medical%2520video%2520segmentation%2520gains%2520increasing%2520attention%2520in%2520clinical%2520practice%250Adue%2520to%2520the%2520redundant%2520dynamic%2520references%2520in%2520video%2520frames.%2520However%252C%2520traditional%250Aconvolutional%2520neural%2520networks%2520have%2520a%2520limited%2520receptive%2520field%2520and%250Atransformer-based%2520networks%2520are%2520mediocre%2520in%2520constructing%2520long-term%2520dependency%250Afrom%2520the%2520perspective%2520of%2520computational%2520complexity.%2520This%2520bottleneck%2520poses%2520a%250Asignificant%2520challenge%2520when%2520processing%2520longer%2520sequences%2520in%2520medical%2520video%250Aanalysis%2520tasks%2520using%2520available%2520devices%2520with%2520limited%2520memory.%2520Recently%252C%2520state%250Aspace%2520models%2520%2528SSMs%2529%252C%2520famous%2520by%2520Mamba%252C%2520have%2520exhibited%2520impressive%2520achievements%2520in%250Aefficient%2520long%2520sequence%2520modeling%252C%2520which%2520develops%2520deep%2520neural%2520networks%2520by%250Aexpanding%2520the%2520receptive%2520field%2520on%2520many%2520vision%2520tasks%2520significantly.%250AUnfortunately%252C%2520vanilla%2520SSMs%2520failed%2520to%2520simultaneously%2520capture%2520causal%2520temporal%250Acues%2520and%2520preserve%2520non-casual%2520spatial%2520information.%2520To%2520this%2520end%252C%2520this%2520paper%250Apresents%2520a%2520Video%2520Vision%2520Mamba-based%2520framework%252C%2520dubbed%2520as%2520Vivim%252C%2520for%2520medical%250Avideo%2520segmentation%2520tasks.%2520Our%2520Vivim%2520can%2520effectively%2520compress%2520the%2520long-term%250Aspatiotemporal%2520representation%2520into%2520sequences%2520at%2520varying%2520scales%2520with%2520our%250Adesigned%2520Temporal%2520Mamba%2520Block.%2520We%2520also%2520introduce%2520an%2520improved%2520boundary-aware%250Aaffine%2520constraint%2520across%2520frames%2520to%2520enhance%2520the%2520discriminative%2520ability%2520of%2520Vivim%250Aon%2520ambiguous%2520lesions.%2520Extensive%2520experiments%2520on%2520thyroid%2520segmentation%252C%2520breast%250Alesion%2520segmentation%2520in%2520ultrasound%2520videos%252C%2520and%2520polyp%2520segmentation%2520in%2520colonoscopy%250Avideos%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%2520Vivim%252C%2520superior%2520to%250Aexisting%2520methods.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/scott-yjyang/Vivim.%2520The%2520dataset%2520will%2520be%2520released%2520once%250Aaccepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14168v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vivim%3A%20a%20Video%20Vision%20Mamba%20for%20Medical%20Video%20Segmentation&entry.906535625=Yijun%20Yang%20and%20Zhaohu%20Xing%20and%20Lequan%20Yu%20and%20Chunwang%20Huang%20and%20Huazhu%20Fu%20and%20Lei%20Zhu&entry.1292438233=%20%20Medical%20video%20segmentation%20gains%20increasing%20attention%20in%20clinical%20practice%0Adue%20to%20the%20redundant%20dynamic%20references%20in%20video%20frames.%20However%2C%20traditional%0Aconvolutional%20neural%20networks%20have%20a%20limited%20receptive%20field%20and%0Atransformer-based%20networks%20are%20mediocre%20in%20constructing%20long-term%20dependency%0Afrom%20the%20perspective%20of%20computational%20complexity.%20This%20bottleneck%20poses%20a%0Asignificant%20challenge%20when%20processing%20longer%20sequences%20in%20medical%20video%0Aanalysis%20tasks%20using%20available%20devices%20with%20limited%20memory.%20Recently%2C%20state%0Aspace%20models%20%28SSMs%29%2C%20famous%20by%20Mamba%2C%20have%20exhibited%20impressive%20achievements%20in%0Aefficient%20long%20sequence%20modeling%2C%20which%20develops%20deep%20neural%20networks%20by%0Aexpanding%20the%20receptive%20field%20on%20many%20vision%20tasks%20significantly.%0AUnfortunately%2C%20vanilla%20SSMs%20failed%20to%20simultaneously%20capture%20causal%20temporal%0Acues%20and%20preserve%20non-casual%20spatial%20information.%20To%20this%20end%2C%20this%20paper%0Apresents%20a%20Video%20Vision%20Mamba-based%20framework%2C%20dubbed%20as%20Vivim%2C%20for%20medical%0Avideo%20segmentation%20tasks.%20Our%20Vivim%20can%20effectively%20compress%20the%20long-term%0Aspatiotemporal%20representation%20into%20sequences%20at%20varying%20scales%20with%20our%0Adesigned%20Temporal%20Mamba%20Block.%20We%20also%20introduce%20an%20improved%20boundary-aware%0Aaffine%20constraint%20across%20frames%20to%20enhance%20the%20discriminative%20ability%20of%20Vivim%0Aon%20ambiguous%20lesions.%20Extensive%20experiments%20on%20thyroid%20segmentation%2C%20breast%0Alesion%20segmentation%20in%20ultrasound%20videos%2C%20and%20polyp%20segmentation%20in%20colonoscopy%0Avideos%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%20Vivim%2C%20superior%20to%0Aexisting%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/scott-yjyang/Vivim.%20The%20dataset%20will%20be%20released%20once%0Aaccepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14168v4&entry.124074799=Read"},
{"title": "MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities", "author": "Weihao Yu and Zhengyuan Yang and Linfeng Ren and Linjie Li and Jianfeng Wang and Kevin Lin and Chung-Ching Lin and Zicheng Liu and Lijuan Wang and Xinchao Wang", "abstract": "  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4.\n", "link": "http://arxiv.org/abs/2408.00765v1", "date": "2024-08-01", "relevancy": 2.1318, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5583}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5154}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Vet%20v2%3A%20A%20Challenging%20Benchmark%20to%20Evaluate%20Large%20Multimodal%20Models%0A%20%20for%20Integrated%20Capabilities&body=Title%3A%20MM-Vet%20v2%3A%20A%20Challenging%20Benchmark%20to%20Evaluate%20Large%20Multimodal%20Models%0A%20%20for%20Integrated%20Capabilities%0AAuthor%3A%20Weihao%20Yu%20and%20Zhengyuan%20Yang%20and%20Linfeng%20Ren%20and%20Linjie%20Li%20and%20Jianfeng%20Wang%20and%20Kevin%20Lin%20and%20Chung-Ching%20Lin%20and%20Zicheng%20Liu%20and%20Lijuan%20Wang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20MM-Vet%2C%20with%20open-ended%20vision-language%20questions%20targeting%20at%20evaluating%0Aintegrated%20capabilities%2C%20has%20become%20one%20of%20the%20most%20popular%20benchmarks%20for%0Alarge%20multimodal%20model%20evaluation.%20MM-Vet%20assesses%20six%20core%20vision-language%0A%28VL%29%20capabilities%3A%20recognition%2C%20knowledge%2C%20spatial%20awareness%2C%20language%0Ageneration%2C%20OCR%2C%20and%20math.%20However%2C%20its%20question%20format%20is%20restricted%20to%20single%0Aimage-text%20pairs%2C%20lacking%20the%20interleaved%20image%20and%20text%20sequences%20prevalent%20in%0Areal-world%20scenarios.%20To%20address%20this%20limitation%2C%20we%20introduce%20MM-Vet%20v2%2C%20which%0Aincludes%20a%20new%20VL%20capability%20called%20%22image-text%20sequence%20understanding%22%2C%0Aevaluating%20models%27%20ability%20to%20process%20VL%20sequences.%20Furthermore%2C%20we%20maintain%0Athe%20high%20quality%20of%20evaluation%20samples%20while%20further%20expanding%20the%20evaluation%0Aset%20size.%20Using%20MM-Vet%20v2%20to%20benchmark%20large%20multimodal%20models%2C%20we%20found%20that%0AClaude%203.5%20Sonnet%20is%20the%20best%20model%20with%20a%20score%20of%2071.8%2C%20slightly%0Aoutperforming%20GPT-4o%20which%20scored%2071.0.%20Among%20open-weight%20models%2C%0AInternVL2-Llama3-76B%20leads%20with%20a%20score%20of%2068.4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Vet%2520v2%253A%2520A%2520Challenging%2520Benchmark%2520to%2520Evaluate%2520Large%2520Multimodal%2520Models%250A%2520%2520for%2520Integrated%2520Capabilities%26entry.906535625%3DWeihao%2520Yu%2520and%2520Zhengyuan%2520Yang%2520and%2520Linfeng%2520Ren%2520and%2520Linjie%2520Li%2520and%2520Jianfeng%2520Wang%2520and%2520Kevin%2520Lin%2520and%2520Chung-Ching%2520Lin%2520and%2520Zicheng%2520Liu%2520and%2520Lijuan%2520Wang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520MM-Vet%252C%2520with%2520open-ended%2520vision-language%2520questions%2520targeting%2520at%2520evaluating%250Aintegrated%2520capabilities%252C%2520has%2520become%2520one%2520of%2520the%2520most%2520popular%2520benchmarks%2520for%250Alarge%2520multimodal%2520model%2520evaluation.%2520MM-Vet%2520assesses%2520six%2520core%2520vision-language%250A%2528VL%2529%2520capabilities%253A%2520recognition%252C%2520knowledge%252C%2520spatial%2520awareness%252C%2520language%250Ageneration%252C%2520OCR%252C%2520and%2520math.%2520However%252C%2520its%2520question%2520format%2520is%2520restricted%2520to%2520single%250Aimage-text%2520pairs%252C%2520lacking%2520the%2520interleaved%2520image%2520and%2520text%2520sequences%2520prevalent%2520in%250Areal-world%2520scenarios.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520MM-Vet%2520v2%252C%2520which%250Aincludes%2520a%2520new%2520VL%2520capability%2520called%2520%2522image-text%2520sequence%2520understanding%2522%252C%250Aevaluating%2520models%2527%2520ability%2520to%2520process%2520VL%2520sequences.%2520Furthermore%252C%2520we%2520maintain%250Athe%2520high%2520quality%2520of%2520evaluation%2520samples%2520while%2520further%2520expanding%2520the%2520evaluation%250Aset%2520size.%2520Using%2520MM-Vet%2520v2%2520to%2520benchmark%2520large%2520multimodal%2520models%252C%2520we%2520found%2520that%250AClaude%25203.5%2520Sonnet%2520is%2520the%2520best%2520model%2520with%2520a%2520score%2520of%252071.8%252C%2520slightly%250Aoutperforming%2520GPT-4o%2520which%2520scored%252071.0.%2520Among%2520open-weight%2520models%252C%250AInternVL2-Llama3-76B%2520leads%2520with%2520a%2520score%2520of%252068.4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Vet%20v2%3A%20A%20Challenging%20Benchmark%20to%20Evaluate%20Large%20Multimodal%20Models%0A%20%20for%20Integrated%20Capabilities&entry.906535625=Weihao%20Yu%20and%20Zhengyuan%20Yang%20and%20Linfeng%20Ren%20and%20Linjie%20Li%20and%20Jianfeng%20Wang%20and%20Kevin%20Lin%20and%20Chung-Ching%20Lin%20and%20Zicheng%20Liu%20and%20Lijuan%20Wang%20and%20Xinchao%20Wang&entry.1292438233=%20%20MM-Vet%2C%20with%20open-ended%20vision-language%20questions%20targeting%20at%20evaluating%0Aintegrated%20capabilities%2C%20has%20become%20one%20of%20the%20most%20popular%20benchmarks%20for%0Alarge%20multimodal%20model%20evaluation.%20MM-Vet%20assesses%20six%20core%20vision-language%0A%28VL%29%20capabilities%3A%20recognition%2C%20knowledge%2C%20spatial%20awareness%2C%20language%0Ageneration%2C%20OCR%2C%20and%20math.%20However%2C%20its%20question%20format%20is%20restricted%20to%20single%0Aimage-text%20pairs%2C%20lacking%20the%20interleaved%20image%20and%20text%20sequences%20prevalent%20in%0Areal-world%20scenarios.%20To%20address%20this%20limitation%2C%20we%20introduce%20MM-Vet%20v2%2C%20which%0Aincludes%20a%20new%20VL%20capability%20called%20%22image-text%20sequence%20understanding%22%2C%0Aevaluating%20models%27%20ability%20to%20process%20VL%20sequences.%20Furthermore%2C%20we%20maintain%0Athe%20high%20quality%20of%20evaluation%20samples%20while%20further%20expanding%20the%20evaluation%0Aset%20size.%20Using%20MM-Vet%20v2%20to%20benchmark%20large%20multimodal%20models%2C%20we%20found%20that%0AClaude%203.5%20Sonnet%20is%20the%20best%20model%20with%20a%20score%20of%2071.8%2C%20slightly%0Aoutperforming%20GPT-4o%20which%20scored%2071.0.%20Among%20open-weight%20models%2C%0AInternVL2-Llama3-76B%20leads%20with%20a%20score%20of%2068.4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00765v1&entry.124074799=Read"},
{"title": "In-Hand Singulation and Scooping Manipulation with a 5 DOF Tactile\n  Gripper", "author": "Yuhao Zhou and Pokuang Zhou and Shaoxiong Wang and Yu She", "abstract": "  Manipulation tasks often require a high degree of dexterity, typically\nnecessitating grippers with multiple degrees of freedom (DoF). While a robotic\nhand equipped with multiple fingers can execute precise and intricate\nmanipulation tasks, the inherent redundancy stemming from its extensive DoF\noften adds unnecessary complexity. In this paper, we introduce the design of a\ntactile sensor-equipped gripper with two fingers and five DoF. We present a\nnovel design integrating a GelSight tactile sensor, enhancing sensing\ncapabilities and enabling finer control during specific manipulation tasks. To\nevaluate the gripper's performance, we conduct experiments involving two\nchallenging tasks: 1) retrieving, singularizing, and classification of various\nobjects embedded in granular media, and 2) executing scooping manipulations of\ncredit cards in confined environments to achieve precise insertion. Our results\ndemonstrate the efficiency of the proposed approach, with a high success rate\nfor singulation and classification tasks, particularly for spherical objects at\nhigh as 94.3%, and a 100% success rate for scooping and inserting credit cards.\n", "link": "http://arxiv.org/abs/2408.00610v1", "date": "2024-08-01", "relevancy": 2.1314, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5462}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5245}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Hand%20Singulation%20and%20Scooping%20Manipulation%20with%20a%205%20DOF%20Tactile%0A%20%20Gripper&body=Title%3A%20In-Hand%20Singulation%20and%20Scooping%20Manipulation%20with%20a%205%20DOF%20Tactile%0A%20%20Gripper%0AAuthor%3A%20Yuhao%20Zhou%20and%20Pokuang%20Zhou%20and%20Shaoxiong%20Wang%20and%20Yu%20She%0AAbstract%3A%20%20%20Manipulation%20tasks%20often%20require%20a%20high%20degree%20of%20dexterity%2C%20typically%0Anecessitating%20grippers%20with%20multiple%20degrees%20of%20freedom%20%28DoF%29.%20While%20a%20robotic%0Ahand%20equipped%20with%20multiple%20fingers%20can%20execute%20precise%20and%20intricate%0Amanipulation%20tasks%2C%20the%20inherent%20redundancy%20stemming%20from%20its%20extensive%20DoF%0Aoften%20adds%20unnecessary%20complexity.%20In%20this%20paper%2C%20we%20introduce%20the%20design%20of%20a%0Atactile%20sensor-equipped%20gripper%20with%20two%20fingers%20and%20five%20DoF.%20We%20present%20a%0Anovel%20design%20integrating%20a%20GelSight%20tactile%20sensor%2C%20enhancing%20sensing%0Acapabilities%20and%20enabling%20finer%20control%20during%20specific%20manipulation%20tasks.%20To%0Aevaluate%20the%20gripper%27s%20performance%2C%20we%20conduct%20experiments%20involving%20two%0Achallenging%20tasks%3A%201%29%20retrieving%2C%20singularizing%2C%20and%20classification%20of%20various%0Aobjects%20embedded%20in%20granular%20media%2C%20and%202%29%20executing%20scooping%20manipulations%20of%0Acredit%20cards%20in%20confined%20environments%20to%20achieve%20precise%20insertion.%20Our%20results%0Ademonstrate%20the%20efficiency%20of%20the%20proposed%20approach%2C%20with%20a%20high%20success%20rate%0Afor%20singulation%20and%20classification%20tasks%2C%20particularly%20for%20spherical%20objects%20at%0Ahigh%20as%2094.3%25%2C%20and%20a%20100%25%20success%20rate%20for%20scooping%20and%20inserting%20credit%20cards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Hand%2520Singulation%2520and%2520Scooping%2520Manipulation%2520with%2520a%25205%2520DOF%2520Tactile%250A%2520%2520Gripper%26entry.906535625%3DYuhao%2520Zhou%2520and%2520Pokuang%2520Zhou%2520and%2520Shaoxiong%2520Wang%2520and%2520Yu%2520She%26entry.1292438233%3D%2520%2520Manipulation%2520tasks%2520often%2520require%2520a%2520high%2520degree%2520of%2520dexterity%252C%2520typically%250Anecessitating%2520grippers%2520with%2520multiple%2520degrees%2520of%2520freedom%2520%2528DoF%2529.%2520While%2520a%2520robotic%250Ahand%2520equipped%2520with%2520multiple%2520fingers%2520can%2520execute%2520precise%2520and%2520intricate%250Amanipulation%2520tasks%252C%2520the%2520inherent%2520redundancy%2520stemming%2520from%2520its%2520extensive%2520DoF%250Aoften%2520adds%2520unnecessary%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520design%2520of%2520a%250Atactile%2520sensor-equipped%2520gripper%2520with%2520two%2520fingers%2520and%2520five%2520DoF.%2520We%2520present%2520a%250Anovel%2520design%2520integrating%2520a%2520GelSight%2520tactile%2520sensor%252C%2520enhancing%2520sensing%250Acapabilities%2520and%2520enabling%2520finer%2520control%2520during%2520specific%2520manipulation%2520tasks.%2520To%250Aevaluate%2520the%2520gripper%2527s%2520performance%252C%2520we%2520conduct%2520experiments%2520involving%2520two%250Achallenging%2520tasks%253A%25201%2529%2520retrieving%252C%2520singularizing%252C%2520and%2520classification%2520of%2520various%250Aobjects%2520embedded%2520in%2520granular%2520media%252C%2520and%25202%2529%2520executing%2520scooping%2520manipulations%2520of%250Acredit%2520cards%2520in%2520confined%2520environments%2520to%2520achieve%2520precise%2520insertion.%2520Our%2520results%250Ademonstrate%2520the%2520efficiency%2520of%2520the%2520proposed%2520approach%252C%2520with%2520a%2520high%2520success%2520rate%250Afor%2520singulation%2520and%2520classification%2520tasks%252C%2520particularly%2520for%2520spherical%2520objects%2520at%250Ahigh%2520as%252094.3%2525%252C%2520and%2520a%2520100%2525%2520success%2520rate%2520for%2520scooping%2520and%2520inserting%2520credit%2520cards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Hand%20Singulation%20and%20Scooping%20Manipulation%20with%20a%205%20DOF%20Tactile%0A%20%20Gripper&entry.906535625=Yuhao%20Zhou%20and%20Pokuang%20Zhou%20and%20Shaoxiong%20Wang%20and%20Yu%20She&entry.1292438233=%20%20Manipulation%20tasks%20often%20require%20a%20high%20degree%20of%20dexterity%2C%20typically%0Anecessitating%20grippers%20with%20multiple%20degrees%20of%20freedom%20%28DoF%29.%20While%20a%20robotic%0Ahand%20equipped%20with%20multiple%20fingers%20can%20execute%20precise%20and%20intricate%0Amanipulation%20tasks%2C%20the%20inherent%20redundancy%20stemming%20from%20its%20extensive%20DoF%0Aoften%20adds%20unnecessary%20complexity.%20In%20this%20paper%2C%20we%20introduce%20the%20design%20of%20a%0Atactile%20sensor-equipped%20gripper%20with%20two%20fingers%20and%20five%20DoF.%20We%20present%20a%0Anovel%20design%20integrating%20a%20GelSight%20tactile%20sensor%2C%20enhancing%20sensing%0Acapabilities%20and%20enabling%20finer%20control%20during%20specific%20manipulation%20tasks.%20To%0Aevaluate%20the%20gripper%27s%20performance%2C%20we%20conduct%20experiments%20involving%20two%0Achallenging%20tasks%3A%201%29%20retrieving%2C%20singularizing%2C%20and%20classification%20of%20various%0Aobjects%20embedded%20in%20granular%20media%2C%20and%202%29%20executing%20scooping%20manipulations%20of%0Acredit%20cards%20in%20confined%20environments%20to%20achieve%20precise%20insertion.%20Our%20results%0Ademonstrate%20the%20efficiency%20of%20the%20proposed%20approach%2C%20with%20a%20high%20success%20rate%0Afor%20singulation%20and%20classification%20tasks%2C%20particularly%20for%20spherical%20objects%20at%0Ahigh%20as%2094.3%25%2C%20and%20a%20100%25%20success%20rate%20for%20scooping%20and%20inserting%20credit%20cards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00610v1&entry.124074799=Read"},
{"title": "Deblurring Neural Radiance Fields with Event-driven Bundle Adjustment", "author": "Yunshan Qi and Lin Zhu and Yifan Zhao and Nan Bao and Jia Li", "abstract": "  Neural Radiance Fields (NeRF) achieves impressive 3D representation learning\nand novel view synthesis results with high-quality multi-view images as input.\nHowever, motion blur in images often occurs in low-light and high-speed motion\nscenes, which significantly degrades the reconstruction quality of NeRF.\nPrevious deblurring NeRF methods struggle to estimate pose and lighting changes\nduring the exposure time, making them unable to accurately model the motion\nblur. The bio-inspired event camera measuring intensity changes with high\ntemporal resolution makes up this information deficiency. In this paper, we\npropose Event-driven Bundle Adjustment for Deblurring Neural Radiance Fields\n(EBAD-NeRF) to jointly optimize the learnable poses and NeRF parameters by\nleveraging the hybrid event-RGB data. An intensity-change-metric event loss and\na photo-metric blur loss are introduced to strengthen the explicit modeling of\ncamera motion blur. Experiments on both synthetic and real-captured data\ndemonstrate that EBAD-NeRF can obtain accurate camera trajectory during the\nexposure time and learn a sharper 3D representations compared to prior works.\n", "link": "http://arxiv.org/abs/2406.14360v2", "date": "2024-08-01", "relevancy": 2.1265, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5357}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.531}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deblurring%20Neural%20Radiance%20Fields%20with%20Event-driven%20Bundle%20Adjustment&body=Title%3A%20Deblurring%20Neural%20Radiance%20Fields%20with%20Event-driven%20Bundle%20Adjustment%0AAuthor%3A%20Yunshan%20Qi%20and%20Lin%20Zhu%20and%20Yifan%20Zhao%20and%20Nan%20Bao%20and%20Jia%20Li%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20achieves%20impressive%203D%20representation%20learning%0Aand%20novel%20view%20synthesis%20results%20with%20high-quality%20multi-view%20images%20as%20input.%0AHowever%2C%20motion%20blur%20in%20images%20often%20occurs%20in%20low-light%20and%20high-speed%20motion%0Ascenes%2C%20which%20significantly%20degrades%20the%20reconstruction%20quality%20of%20NeRF.%0APrevious%20deblurring%20NeRF%20methods%20struggle%20to%20estimate%20pose%20and%20lighting%20changes%0Aduring%20the%20exposure%20time%2C%20making%20them%20unable%20to%20accurately%20model%20the%20motion%0Ablur.%20The%20bio-inspired%20event%20camera%20measuring%20intensity%20changes%20with%20high%0Atemporal%20resolution%20makes%20up%20this%20information%20deficiency.%20In%20this%20paper%2C%20we%0Apropose%20Event-driven%20Bundle%20Adjustment%20for%20Deblurring%20Neural%20Radiance%20Fields%0A%28EBAD-NeRF%29%20to%20jointly%20optimize%20the%20learnable%20poses%20and%20NeRF%20parameters%20by%0Aleveraging%20the%20hybrid%20event-RGB%20data.%20An%20intensity-change-metric%20event%20loss%20and%0Aa%20photo-metric%20blur%20loss%20are%20introduced%20to%20strengthen%20the%20explicit%20modeling%20of%0Acamera%20motion%20blur.%20Experiments%20on%20both%20synthetic%20and%20real-captured%20data%0Ademonstrate%20that%20EBAD-NeRF%20can%20obtain%20accurate%20camera%20trajectory%20during%20the%0Aexposure%20time%20and%20learn%20a%20sharper%203D%20representations%20compared%20to%20prior%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeblurring%2520Neural%2520Radiance%2520Fields%2520with%2520Event-driven%2520Bundle%2520Adjustment%26entry.906535625%3DYunshan%2520Qi%2520and%2520Lin%2520Zhu%2520and%2520Yifan%2520Zhao%2520and%2520Nan%2520Bao%2520and%2520Jia%2520Li%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520achieves%2520impressive%25203D%2520representation%2520learning%250Aand%2520novel%2520view%2520synthesis%2520results%2520with%2520high-quality%2520multi-view%2520images%2520as%2520input.%250AHowever%252C%2520motion%2520blur%2520in%2520images%2520often%2520occurs%2520in%2520low-light%2520and%2520high-speed%2520motion%250Ascenes%252C%2520which%2520significantly%2520degrades%2520the%2520reconstruction%2520quality%2520of%2520NeRF.%250APrevious%2520deblurring%2520NeRF%2520methods%2520struggle%2520to%2520estimate%2520pose%2520and%2520lighting%2520changes%250Aduring%2520the%2520exposure%2520time%252C%2520making%2520them%2520unable%2520to%2520accurately%2520model%2520the%2520motion%250Ablur.%2520The%2520bio-inspired%2520event%2520camera%2520measuring%2520intensity%2520changes%2520with%2520high%250Atemporal%2520resolution%2520makes%2520up%2520this%2520information%2520deficiency.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Event-driven%2520Bundle%2520Adjustment%2520for%2520Deblurring%2520Neural%2520Radiance%2520Fields%250A%2528EBAD-NeRF%2529%2520to%2520jointly%2520optimize%2520the%2520learnable%2520poses%2520and%2520NeRF%2520parameters%2520by%250Aleveraging%2520the%2520hybrid%2520event-RGB%2520data.%2520An%2520intensity-change-metric%2520event%2520loss%2520and%250Aa%2520photo-metric%2520blur%2520loss%2520are%2520introduced%2520to%2520strengthen%2520the%2520explicit%2520modeling%2520of%250Acamera%2520motion%2520blur.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520real-captured%2520data%250Ademonstrate%2520that%2520EBAD-NeRF%2520can%2520obtain%2520accurate%2520camera%2520trajectory%2520during%2520the%250Aexposure%2520time%2520and%2520learn%2520a%2520sharper%25203D%2520representations%2520compared%2520to%2520prior%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deblurring%20Neural%20Radiance%20Fields%20with%20Event-driven%20Bundle%20Adjustment&entry.906535625=Yunshan%20Qi%20and%20Lin%20Zhu%20and%20Yifan%20Zhao%20and%20Nan%20Bao%20and%20Jia%20Li&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20achieves%20impressive%203D%20representation%20learning%0Aand%20novel%20view%20synthesis%20results%20with%20high-quality%20multi-view%20images%20as%20input.%0AHowever%2C%20motion%20blur%20in%20images%20often%20occurs%20in%20low-light%20and%20high-speed%20motion%0Ascenes%2C%20which%20significantly%20degrades%20the%20reconstruction%20quality%20of%20NeRF.%0APrevious%20deblurring%20NeRF%20methods%20struggle%20to%20estimate%20pose%20and%20lighting%20changes%0Aduring%20the%20exposure%20time%2C%20making%20them%20unable%20to%20accurately%20model%20the%20motion%0Ablur.%20The%20bio-inspired%20event%20camera%20measuring%20intensity%20changes%20with%20high%0Atemporal%20resolution%20makes%20up%20this%20information%20deficiency.%20In%20this%20paper%2C%20we%0Apropose%20Event-driven%20Bundle%20Adjustment%20for%20Deblurring%20Neural%20Radiance%20Fields%0A%28EBAD-NeRF%29%20to%20jointly%20optimize%20the%20learnable%20poses%20and%20NeRF%20parameters%20by%0Aleveraging%20the%20hybrid%20event-RGB%20data.%20An%20intensity-change-metric%20event%20loss%20and%0Aa%20photo-metric%20blur%20loss%20are%20introduced%20to%20strengthen%20the%20explicit%20modeling%20of%0Acamera%20motion%20blur.%20Experiments%20on%20both%20synthetic%20and%20real-captured%20data%0Ademonstrate%20that%20EBAD-NeRF%20can%20obtain%20accurate%20camera%20trajectory%20during%20the%0Aexposure%20time%20and%20learn%20a%20sharper%203D%20representations%20compared%20to%20prior%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14360v2&entry.124074799=Read"},
{"title": "Empowering Snapshot Compressive Imaging: Spatial-Spectral State Space\n  Model with Across-Scanning and Local Enhancement", "author": "Wenzhe Tian and Haijin Zeng and Yin-Ping Zhao and Yongyong Chen and Zhen Wang and Xuelong Li", "abstract": "  Snapshot Compressive Imaging (SCI) relies on decoding algorithms such as CNN\nor Transformer to reconstruct the hyperspectral image (HSI) from its compressed\nmeasurement. Although existing CNN and Transformer-based methods have proven\neffective, CNNs are limited by their inadequate modeling of long-range\ndependencies, while Transformer ones face high computational costs due to\nquadratic complexity. Recent Mamba models have demonstrated superior\nperformance over CNN and Transformer-based architectures in some visual tasks,\nbut these models have not fully utilized the local similarities in both spatial\nand spectral dimensions. Moreover, the long-sequence modeling capability of SSM\nmay offer an advantage in processing the numerous spectral bands for HSI\nreconstruction, which has not yet been explored. In this paper, we introduce a\nState Space Model with Across-Scanning and Local Enhancement, named ASLE-SSM,\nthat employs a Spatial-Spectral SSM for global-local balanced context encoding\nand cross-channel interaction promoting. Specifically, we introduce local\nscanning in the spatial dimension to balance the global and local receptive\nfields, and then propose our across-scanning method based on spatial-spectral\nlocal cubes to leverage local similarities between adjacent spectral bands and\npixels to guide the reconstruction process. These two scanning mechanisms\nextract the HSI's local features while balancing the global perspective without\nany additional costs. Experimental results illustrate ASLE-SSM's superiority\nover existing state-of-the-art methods, with an inference speed 2.4 times\nfaster than Transformer-based MST and saving 0.12 (M) of parameters, achieving\nthe lowest computational cost and parameter count.\n", "link": "http://arxiv.org/abs/2408.00629v1", "date": "2024-08-01", "relevancy": 2.0918, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5635}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5219}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Snapshot%20Compressive%20Imaging%3A%20Spatial-Spectral%20State%20Space%0A%20%20Model%20with%20Across-Scanning%20and%20Local%20Enhancement&body=Title%3A%20Empowering%20Snapshot%20Compressive%20Imaging%3A%20Spatial-Spectral%20State%20Space%0A%20%20Model%20with%20Across-Scanning%20and%20Local%20Enhancement%0AAuthor%3A%20Wenzhe%20Tian%20and%20Haijin%20Zeng%20and%20Yin-Ping%20Zhao%20and%20Yongyong%20Chen%20and%20Zhen%20Wang%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20relies%20on%20decoding%20algorithms%20such%20as%20CNN%0Aor%20Transformer%20to%20reconstruct%20the%20hyperspectral%20image%20%28HSI%29%20from%20its%20compressed%0Ameasurement.%20Although%20existing%20CNN%20and%20Transformer-based%20methods%20have%20proven%0Aeffective%2C%20CNNs%20are%20limited%20by%20their%20inadequate%20modeling%20of%20long-range%0Adependencies%2C%20while%20Transformer%20ones%20face%20high%20computational%20costs%20due%20to%0Aquadratic%20complexity.%20Recent%20Mamba%20models%20have%20demonstrated%20superior%0Aperformance%20over%20CNN%20and%20Transformer-based%20architectures%20in%20some%20visual%20tasks%2C%0Abut%20these%20models%20have%20not%20fully%20utilized%20the%20local%20similarities%20in%20both%20spatial%0Aand%20spectral%20dimensions.%20Moreover%2C%20the%20long-sequence%20modeling%20capability%20of%20SSM%0Amay%20offer%20an%20advantage%20in%20processing%20the%20numerous%20spectral%20bands%20for%20HSI%0Areconstruction%2C%20which%20has%20not%20yet%20been%20explored.%20In%20this%20paper%2C%20we%20introduce%20a%0AState%20Space%20Model%20with%20Across-Scanning%20and%20Local%20Enhancement%2C%20named%20ASLE-SSM%2C%0Athat%20employs%20a%20Spatial-Spectral%20SSM%20for%20global-local%20balanced%20context%20encoding%0Aand%20cross-channel%20interaction%20promoting.%20Specifically%2C%20we%20introduce%20local%0Ascanning%20in%20the%20spatial%20dimension%20to%20balance%20the%20global%20and%20local%20receptive%0Afields%2C%20and%20then%20propose%20our%20across-scanning%20method%20based%20on%20spatial-spectral%0Alocal%20cubes%20to%20leverage%20local%20similarities%20between%20adjacent%20spectral%20bands%20and%0Apixels%20to%20guide%20the%20reconstruction%20process.%20These%20two%20scanning%20mechanisms%0Aextract%20the%20HSI%27s%20local%20features%20while%20balancing%20the%20global%20perspective%20without%0Aany%20additional%20costs.%20Experimental%20results%20illustrate%20ASLE-SSM%27s%20superiority%0Aover%20existing%20state-of-the-art%20methods%2C%20with%20an%20inference%20speed%202.4%20times%0Afaster%20than%20Transformer-based%20MST%20and%20saving%200.12%20%28M%29%20of%20parameters%2C%20achieving%0Athe%20lowest%20computational%20cost%20and%20parameter%20count.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Snapshot%2520Compressive%2520Imaging%253A%2520Spatial-Spectral%2520State%2520Space%250A%2520%2520Model%2520with%2520Across-Scanning%2520and%2520Local%2520Enhancement%26entry.906535625%3DWenzhe%2520Tian%2520and%2520Haijin%2520Zeng%2520and%2520Yin-Ping%2520Zhao%2520and%2520Yongyong%2520Chen%2520and%2520Zhen%2520Wang%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Snapshot%2520Compressive%2520Imaging%2520%2528SCI%2529%2520relies%2520on%2520decoding%2520algorithms%2520such%2520as%2520CNN%250Aor%2520Transformer%2520to%2520reconstruct%2520the%2520hyperspectral%2520image%2520%2528HSI%2529%2520from%2520its%2520compressed%250Ameasurement.%2520Although%2520existing%2520CNN%2520and%2520Transformer-based%2520methods%2520have%2520proven%250Aeffective%252C%2520CNNs%2520are%2520limited%2520by%2520their%2520inadequate%2520modeling%2520of%2520long-range%250Adependencies%252C%2520while%2520Transformer%2520ones%2520face%2520high%2520computational%2520costs%2520due%2520to%250Aquadratic%2520complexity.%2520Recent%2520Mamba%2520models%2520have%2520demonstrated%2520superior%250Aperformance%2520over%2520CNN%2520and%2520Transformer-based%2520architectures%2520in%2520some%2520visual%2520tasks%252C%250Abut%2520these%2520models%2520have%2520not%2520fully%2520utilized%2520the%2520local%2520similarities%2520in%2520both%2520spatial%250Aand%2520spectral%2520dimensions.%2520Moreover%252C%2520the%2520long-sequence%2520modeling%2520capability%2520of%2520SSM%250Amay%2520offer%2520an%2520advantage%2520in%2520processing%2520the%2520numerous%2520spectral%2520bands%2520for%2520HSI%250Areconstruction%252C%2520which%2520has%2520not%2520yet%2520been%2520explored.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250AState%2520Space%2520Model%2520with%2520Across-Scanning%2520and%2520Local%2520Enhancement%252C%2520named%2520ASLE-SSM%252C%250Athat%2520employs%2520a%2520Spatial-Spectral%2520SSM%2520for%2520global-local%2520balanced%2520context%2520encoding%250Aand%2520cross-channel%2520interaction%2520promoting.%2520Specifically%252C%2520we%2520introduce%2520local%250Ascanning%2520in%2520the%2520spatial%2520dimension%2520to%2520balance%2520the%2520global%2520and%2520local%2520receptive%250Afields%252C%2520and%2520then%2520propose%2520our%2520across-scanning%2520method%2520based%2520on%2520spatial-spectral%250Alocal%2520cubes%2520to%2520leverage%2520local%2520similarities%2520between%2520adjacent%2520spectral%2520bands%2520and%250Apixels%2520to%2520guide%2520the%2520reconstruction%2520process.%2520These%2520two%2520scanning%2520mechanisms%250Aextract%2520the%2520HSI%2527s%2520local%2520features%2520while%2520balancing%2520the%2520global%2520perspective%2520without%250Aany%2520additional%2520costs.%2520Experimental%2520results%2520illustrate%2520ASLE-SSM%2527s%2520superiority%250Aover%2520existing%2520state-of-the-art%2520methods%252C%2520with%2520an%2520inference%2520speed%25202.4%2520times%250Afaster%2520than%2520Transformer-based%2520MST%2520and%2520saving%25200.12%2520%2528M%2529%2520of%2520parameters%252C%2520achieving%250Athe%2520lowest%2520computational%2520cost%2520and%2520parameter%2520count.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Snapshot%20Compressive%20Imaging%3A%20Spatial-Spectral%20State%20Space%0A%20%20Model%20with%20Across-Scanning%20and%20Local%20Enhancement&entry.906535625=Wenzhe%20Tian%20and%20Haijin%20Zeng%20and%20Yin-Ping%20Zhao%20and%20Yongyong%20Chen%20and%20Zhen%20Wang%20and%20Xuelong%20Li&entry.1292438233=%20%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20relies%20on%20decoding%20algorithms%20such%20as%20CNN%0Aor%20Transformer%20to%20reconstruct%20the%20hyperspectral%20image%20%28HSI%29%20from%20its%20compressed%0Ameasurement.%20Although%20existing%20CNN%20and%20Transformer-based%20methods%20have%20proven%0Aeffective%2C%20CNNs%20are%20limited%20by%20their%20inadequate%20modeling%20of%20long-range%0Adependencies%2C%20while%20Transformer%20ones%20face%20high%20computational%20costs%20due%20to%0Aquadratic%20complexity.%20Recent%20Mamba%20models%20have%20demonstrated%20superior%0Aperformance%20over%20CNN%20and%20Transformer-based%20architectures%20in%20some%20visual%20tasks%2C%0Abut%20these%20models%20have%20not%20fully%20utilized%20the%20local%20similarities%20in%20both%20spatial%0Aand%20spectral%20dimensions.%20Moreover%2C%20the%20long-sequence%20modeling%20capability%20of%20SSM%0Amay%20offer%20an%20advantage%20in%20processing%20the%20numerous%20spectral%20bands%20for%20HSI%0Areconstruction%2C%20which%20has%20not%20yet%20been%20explored.%20In%20this%20paper%2C%20we%20introduce%20a%0AState%20Space%20Model%20with%20Across-Scanning%20and%20Local%20Enhancement%2C%20named%20ASLE-SSM%2C%0Athat%20employs%20a%20Spatial-Spectral%20SSM%20for%20global-local%20balanced%20context%20encoding%0Aand%20cross-channel%20interaction%20promoting.%20Specifically%2C%20we%20introduce%20local%0Ascanning%20in%20the%20spatial%20dimension%20to%20balance%20the%20global%20and%20local%20receptive%0Afields%2C%20and%20then%20propose%20our%20across-scanning%20method%20based%20on%20spatial-spectral%0Alocal%20cubes%20to%20leverage%20local%20similarities%20between%20adjacent%20spectral%20bands%20and%0Apixels%20to%20guide%20the%20reconstruction%20process.%20These%20two%20scanning%20mechanisms%0Aextract%20the%20HSI%27s%20local%20features%20while%20balancing%20the%20global%20perspective%20without%0Aany%20additional%20costs.%20Experimental%20results%20illustrate%20ASLE-SSM%27s%20superiority%0Aover%20existing%20state-of-the-art%20methods%2C%20with%20an%20inference%20speed%202.4%20times%0Afaster%20than%20Transformer-based%20MST%20and%20saving%200.12%20%28M%29%20of%20parameters%2C%20achieving%0Athe%20lowest%20computational%20cost%20and%20parameter%20count.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00629v1&entry.124074799=Read"},
{"title": "Towards End-to-End Explainable Facial Action Unit Recognition via\n  Vision-Language Joint Learning", "author": "Xuri Ge and Junchen Fu and Fuhai Chen and Shan An and Nicu Sebe and Joemon M. Jose", "abstract": "  Facial action units (AUs), as defined in the Facial Action Coding System\n(FACS), have received significant research interest owing to their diverse\nrange of applications in facial state analysis. Current mainstream FAU\nrecognition models have a notable limitation, i.e., focusing only on the\naccuracy of AU recognition and overlooking explanations of corresponding AU\nstates. In this paper, we propose an end-to-end Vision-Language joint learning\nnetwork for explainable FAU recognition (termed VL-FAU), which aims to\nreinforce AU representation capability and language interpretability through\nthe integration of joint multimodal tasks. Specifically, VL-FAU brings together\nlanguage models to generate fine-grained local muscle descriptions and\ndistinguishable global face description when optimising FAU recognition.\nThrough this, the global facial representation and its local AU representations\nwill achieve higher distinguishability among different AUs and different\nsubjects. In addition, multi-level AU representation learning is utilised to\nimprove AU individual attention-aware representation capabilities based on\nmulti-scale combined facial stem feature. Extensive experiments on DISFA and\nBP4D AU datasets show that the proposed approach achieves superior performance\nover the state-of-the-art methods on most of the metrics. In addition, compared\nwith mainstream FAU recognition methods, VL-FAU can provide local- and\nglobal-level interpretability language descriptions with the AUs' predictions.\n", "link": "http://arxiv.org/abs/2408.00644v1", "date": "2024-08-01", "relevancy": 2.0873, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5247}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20End-to-End%20Explainable%20Facial%20Action%20Unit%20Recognition%20via%0A%20%20Vision-Language%20Joint%20Learning&body=Title%3A%20Towards%20End-to-End%20Explainable%20Facial%20Action%20Unit%20Recognition%20via%0A%20%20Vision-Language%20Joint%20Learning%0AAuthor%3A%20Xuri%20Ge%20and%20Junchen%20Fu%20and%20Fuhai%20Chen%20and%20Shan%20An%20and%20Nicu%20Sebe%20and%20Joemon%20M.%20Jose%0AAbstract%3A%20%20%20Facial%20action%20units%20%28AUs%29%2C%20as%20defined%20in%20the%20Facial%20Action%20Coding%20System%0A%28FACS%29%2C%20have%20received%20significant%20research%20interest%20owing%20to%20their%20diverse%0Arange%20of%20applications%20in%20facial%20state%20analysis.%20Current%20mainstream%20FAU%0Arecognition%20models%20have%20a%20notable%20limitation%2C%20i.e.%2C%20focusing%20only%20on%20the%0Aaccuracy%20of%20AU%20recognition%20and%20overlooking%20explanations%20of%20corresponding%20AU%0Astates.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20Vision-Language%20joint%20learning%0Anetwork%20for%20explainable%20FAU%20recognition%20%28termed%20VL-FAU%29%2C%20which%20aims%20to%0Areinforce%20AU%20representation%20capability%20and%20language%20interpretability%20through%0Athe%20integration%20of%20joint%20multimodal%20tasks.%20Specifically%2C%20VL-FAU%20brings%20together%0Alanguage%20models%20to%20generate%20fine-grained%20local%20muscle%20descriptions%20and%0Adistinguishable%20global%20face%20description%20when%20optimising%20FAU%20recognition.%0AThrough%20this%2C%20the%20global%20facial%20representation%20and%20its%20local%20AU%20representations%0Awill%20achieve%20higher%20distinguishability%20among%20different%20AUs%20and%20different%0Asubjects.%20In%20addition%2C%20multi-level%20AU%20representation%20learning%20is%20utilised%20to%0Aimprove%20AU%20individual%20attention-aware%20representation%20capabilities%20based%20on%0Amulti-scale%20combined%20facial%20stem%20feature.%20Extensive%20experiments%20on%20DISFA%20and%0ABP4D%20AU%20datasets%20show%20that%20the%20proposed%20approach%20achieves%20superior%20performance%0Aover%20the%20state-of-the-art%20methods%20on%20most%20of%20the%20metrics.%20In%20addition%2C%20compared%0Awith%20mainstream%20FAU%20recognition%20methods%2C%20VL-FAU%20can%20provide%20local-%20and%0Aglobal-level%20interpretability%20language%20descriptions%20with%20the%20AUs%27%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520End-to-End%2520Explainable%2520Facial%2520Action%2520Unit%2520Recognition%2520via%250A%2520%2520Vision-Language%2520Joint%2520Learning%26entry.906535625%3DXuri%2520Ge%2520and%2520Junchen%2520Fu%2520and%2520Fuhai%2520Chen%2520and%2520Shan%2520An%2520and%2520Nicu%2520Sebe%2520and%2520Joemon%2520M.%2520Jose%26entry.1292438233%3D%2520%2520Facial%2520action%2520units%2520%2528AUs%2529%252C%2520as%2520defined%2520in%2520the%2520Facial%2520Action%2520Coding%2520System%250A%2528FACS%2529%252C%2520have%2520received%2520significant%2520research%2520interest%2520owing%2520to%2520their%2520diverse%250Arange%2520of%2520applications%2520in%2520facial%2520state%2520analysis.%2520Current%2520mainstream%2520FAU%250Arecognition%2520models%2520have%2520a%2520notable%2520limitation%252C%2520i.e.%252C%2520focusing%2520only%2520on%2520the%250Aaccuracy%2520of%2520AU%2520recognition%2520and%2520overlooking%2520explanations%2520of%2520corresponding%2520AU%250Astates.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520end-to-end%2520Vision-Language%2520joint%2520learning%250Anetwork%2520for%2520explainable%2520FAU%2520recognition%2520%2528termed%2520VL-FAU%2529%252C%2520which%2520aims%2520to%250Areinforce%2520AU%2520representation%2520capability%2520and%2520language%2520interpretability%2520through%250Athe%2520integration%2520of%2520joint%2520multimodal%2520tasks.%2520Specifically%252C%2520VL-FAU%2520brings%2520together%250Alanguage%2520models%2520to%2520generate%2520fine-grained%2520local%2520muscle%2520descriptions%2520and%250Adistinguishable%2520global%2520face%2520description%2520when%2520optimising%2520FAU%2520recognition.%250AThrough%2520this%252C%2520the%2520global%2520facial%2520representation%2520and%2520its%2520local%2520AU%2520representations%250Awill%2520achieve%2520higher%2520distinguishability%2520among%2520different%2520AUs%2520and%2520different%250Asubjects.%2520In%2520addition%252C%2520multi-level%2520AU%2520representation%2520learning%2520is%2520utilised%2520to%250Aimprove%2520AU%2520individual%2520attention-aware%2520representation%2520capabilities%2520based%2520on%250Amulti-scale%2520combined%2520facial%2520stem%2520feature.%2520Extensive%2520experiments%2520on%2520DISFA%2520and%250ABP4D%2520AU%2520datasets%2520show%2520that%2520the%2520proposed%2520approach%2520achieves%2520superior%2520performance%250Aover%2520the%2520state-of-the-art%2520methods%2520on%2520most%2520of%2520the%2520metrics.%2520In%2520addition%252C%2520compared%250Awith%2520mainstream%2520FAU%2520recognition%2520methods%252C%2520VL-FAU%2520can%2520provide%2520local-%2520and%250Aglobal-level%2520interpretability%2520language%2520descriptions%2520with%2520the%2520AUs%2527%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20End-to-End%20Explainable%20Facial%20Action%20Unit%20Recognition%20via%0A%20%20Vision-Language%20Joint%20Learning&entry.906535625=Xuri%20Ge%20and%20Junchen%20Fu%20and%20Fuhai%20Chen%20and%20Shan%20An%20and%20Nicu%20Sebe%20and%20Joemon%20M.%20Jose&entry.1292438233=%20%20Facial%20action%20units%20%28AUs%29%2C%20as%20defined%20in%20the%20Facial%20Action%20Coding%20System%0A%28FACS%29%2C%20have%20received%20significant%20research%20interest%20owing%20to%20their%20diverse%0Arange%20of%20applications%20in%20facial%20state%20analysis.%20Current%20mainstream%20FAU%0Arecognition%20models%20have%20a%20notable%20limitation%2C%20i.e.%2C%20focusing%20only%20on%20the%0Aaccuracy%20of%20AU%20recognition%20and%20overlooking%20explanations%20of%20corresponding%20AU%0Astates.%20In%20this%20paper%2C%20we%20propose%20an%20end-to-end%20Vision-Language%20joint%20learning%0Anetwork%20for%20explainable%20FAU%20recognition%20%28termed%20VL-FAU%29%2C%20which%20aims%20to%0Areinforce%20AU%20representation%20capability%20and%20language%20interpretability%20through%0Athe%20integration%20of%20joint%20multimodal%20tasks.%20Specifically%2C%20VL-FAU%20brings%20together%0Alanguage%20models%20to%20generate%20fine-grained%20local%20muscle%20descriptions%20and%0Adistinguishable%20global%20face%20description%20when%20optimising%20FAU%20recognition.%0AThrough%20this%2C%20the%20global%20facial%20representation%20and%20its%20local%20AU%20representations%0Awill%20achieve%20higher%20distinguishability%20among%20different%20AUs%20and%20different%0Asubjects.%20In%20addition%2C%20multi-level%20AU%20representation%20learning%20is%20utilised%20to%0Aimprove%20AU%20individual%20attention-aware%20representation%20capabilities%20based%20on%0Amulti-scale%20combined%20facial%20stem%20feature.%20Extensive%20experiments%20on%20DISFA%20and%0ABP4D%20AU%20datasets%20show%20that%20the%20proposed%20approach%20achieves%20superior%20performance%0Aover%20the%20state-of-the-art%20methods%20on%20most%20of%20the%20metrics.%20In%20addition%2C%20compared%0Awith%20mainstream%20FAU%20recognition%20methods%2C%20VL-FAU%20can%20provide%20local-%20and%0Aglobal-level%20interpretability%20language%20descriptions%20with%20the%20AUs%27%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00644v1&entry.124074799=Read"},
{"title": "CARMIL: Context-Aware Regularization on Multiple Instance Learning\n  models for Whole Slide Images", "author": "Thiziri Nait Saada and Valentina Di-Proietto and Benoit Schmauch and Katharina Von Loga and Lucas Fidon", "abstract": "  Multiple Instance Learning (MIL) models have proven effective for cancer\nprognosis from Whole Slide Images. However, the original MIL formulation\nincorrectly assumes the patches of the same image to be independent, leading to\na loss of spatial context as information flows through the network.\nIncorporating contextual knowledge into predictions is particularly important\ngiven the inclination for cancerous cells to form clusters and the presence of\nspatial indicators for tumors. State-of-the-art methods often use attention\nmechanisms eventually combined with graphs to capture spatial knowledge. In\nthis paper, we take a novel and transversal approach, addressing this issue\nthrough the lens of regularization. We propose Context-Aware Regularization for\nMultiple Instance Learning (CARMIL), a versatile regularization scheme designed\nto seamlessly integrate spatial knowledge into any MIL model. Additionally, we\npresent a new and generic metric to quantify the Context-Awareness of any MIL\nmodel when applied to Whole Slide Images, resolving a previously unexplored gap\nin the field. The efficacy of our framework is evaluated for two survival\nanalysis tasks on glioblastoma (TCGA GBM) and colon cancer data (TCGA COAD).\n", "link": "http://arxiv.org/abs/2408.00427v1", "date": "2024-08-01", "relevancy": 2.0846, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.514}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARMIL%3A%20Context-Aware%20Regularization%20on%20Multiple%20Instance%20Learning%0A%20%20models%20for%20Whole%20Slide%20Images&body=Title%3A%20CARMIL%3A%20Context-Aware%20Regularization%20on%20Multiple%20Instance%20Learning%0A%20%20models%20for%20Whole%20Slide%20Images%0AAuthor%3A%20Thiziri%20Nait%20Saada%20and%20Valentina%20Di-Proietto%20and%20Benoit%20Schmauch%20and%20Katharina%20Von%20Loga%20and%20Lucas%20Fidon%0AAbstract%3A%20%20%20Multiple%20Instance%20Learning%20%28MIL%29%20models%20have%20proven%20effective%20for%20cancer%0Aprognosis%20from%20Whole%20Slide%20Images.%20However%2C%20the%20original%20MIL%20formulation%0Aincorrectly%20assumes%20the%20patches%20of%20the%20same%20image%20to%20be%20independent%2C%20leading%20to%0Aa%20loss%20of%20spatial%20context%20as%20information%20flows%20through%20the%20network.%0AIncorporating%20contextual%20knowledge%20into%20predictions%20is%20particularly%20important%0Agiven%20the%20inclination%20for%20cancerous%20cells%20to%20form%20clusters%20and%20the%20presence%20of%0Aspatial%20indicators%20for%20tumors.%20State-of-the-art%20methods%20often%20use%20attention%0Amechanisms%20eventually%20combined%20with%20graphs%20to%20capture%20spatial%20knowledge.%20In%0Athis%20paper%2C%20we%20take%20a%20novel%20and%20transversal%20approach%2C%20addressing%20this%20issue%0Athrough%20the%20lens%20of%20regularization.%20We%20propose%20Context-Aware%20Regularization%20for%0AMultiple%20Instance%20Learning%20%28CARMIL%29%2C%20a%20versatile%20regularization%20scheme%20designed%0Ato%20seamlessly%20integrate%20spatial%20knowledge%20into%20any%20MIL%20model.%20Additionally%2C%20we%0Apresent%20a%20new%20and%20generic%20metric%20to%20quantify%20the%20Context-Awareness%20of%20any%20MIL%0Amodel%20when%20applied%20to%20Whole%20Slide%20Images%2C%20resolving%20a%20previously%20unexplored%20gap%0Ain%20the%20field.%20The%20efficacy%20of%20our%20framework%20is%20evaluated%20for%20two%20survival%0Aanalysis%20tasks%20on%20glioblastoma%20%28TCGA%20GBM%29%20and%20colon%20cancer%20data%20%28TCGA%20COAD%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARMIL%253A%2520Context-Aware%2520Regularization%2520on%2520Multiple%2520Instance%2520Learning%250A%2520%2520models%2520for%2520Whole%2520Slide%2520Images%26entry.906535625%3DThiziri%2520Nait%2520Saada%2520and%2520Valentina%2520Di-Proietto%2520and%2520Benoit%2520Schmauch%2520and%2520Katharina%2520Von%2520Loga%2520and%2520Lucas%2520Fidon%26entry.1292438233%3D%2520%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520models%2520have%2520proven%2520effective%2520for%2520cancer%250Aprognosis%2520from%2520Whole%2520Slide%2520Images.%2520However%252C%2520the%2520original%2520MIL%2520formulation%250Aincorrectly%2520assumes%2520the%2520patches%2520of%2520the%2520same%2520image%2520to%2520be%2520independent%252C%2520leading%2520to%250Aa%2520loss%2520of%2520spatial%2520context%2520as%2520information%2520flows%2520through%2520the%2520network.%250AIncorporating%2520contextual%2520knowledge%2520into%2520predictions%2520is%2520particularly%2520important%250Agiven%2520the%2520inclination%2520for%2520cancerous%2520cells%2520to%2520form%2520clusters%2520and%2520the%2520presence%2520of%250Aspatial%2520indicators%2520for%2520tumors.%2520State-of-the-art%2520methods%2520often%2520use%2520attention%250Amechanisms%2520eventually%2520combined%2520with%2520graphs%2520to%2520capture%2520spatial%2520knowledge.%2520In%250Athis%2520paper%252C%2520we%2520take%2520a%2520novel%2520and%2520transversal%2520approach%252C%2520addressing%2520this%2520issue%250Athrough%2520the%2520lens%2520of%2520regularization.%2520We%2520propose%2520Context-Aware%2520Regularization%2520for%250AMultiple%2520Instance%2520Learning%2520%2528CARMIL%2529%252C%2520a%2520versatile%2520regularization%2520scheme%2520designed%250Ato%2520seamlessly%2520integrate%2520spatial%2520knowledge%2520into%2520any%2520MIL%2520model.%2520Additionally%252C%2520we%250Apresent%2520a%2520new%2520and%2520generic%2520metric%2520to%2520quantify%2520the%2520Context-Awareness%2520of%2520any%2520MIL%250Amodel%2520when%2520applied%2520to%2520Whole%2520Slide%2520Images%252C%2520resolving%2520a%2520previously%2520unexplored%2520gap%250Ain%2520the%2520field.%2520The%2520efficacy%2520of%2520our%2520framework%2520is%2520evaluated%2520for%2520two%2520survival%250Aanalysis%2520tasks%2520on%2520glioblastoma%2520%2528TCGA%2520GBM%2529%2520and%2520colon%2520cancer%2520data%2520%2528TCGA%2520COAD%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARMIL%3A%20Context-Aware%20Regularization%20on%20Multiple%20Instance%20Learning%0A%20%20models%20for%20Whole%20Slide%20Images&entry.906535625=Thiziri%20Nait%20Saada%20and%20Valentina%20Di-Proietto%20and%20Benoit%20Schmauch%20and%20Katharina%20Von%20Loga%20and%20Lucas%20Fidon&entry.1292438233=%20%20Multiple%20Instance%20Learning%20%28MIL%29%20models%20have%20proven%20effective%20for%20cancer%0Aprognosis%20from%20Whole%20Slide%20Images.%20However%2C%20the%20original%20MIL%20formulation%0Aincorrectly%20assumes%20the%20patches%20of%20the%20same%20image%20to%20be%20independent%2C%20leading%20to%0Aa%20loss%20of%20spatial%20context%20as%20information%20flows%20through%20the%20network.%0AIncorporating%20contextual%20knowledge%20into%20predictions%20is%20particularly%20important%0Agiven%20the%20inclination%20for%20cancerous%20cells%20to%20form%20clusters%20and%20the%20presence%20of%0Aspatial%20indicators%20for%20tumors.%20State-of-the-art%20methods%20often%20use%20attention%0Amechanisms%20eventually%20combined%20with%20graphs%20to%20capture%20spatial%20knowledge.%20In%0Athis%20paper%2C%20we%20take%20a%20novel%20and%20transversal%20approach%2C%20addressing%20this%20issue%0Athrough%20the%20lens%20of%20regularization.%20We%20propose%20Context-Aware%20Regularization%20for%0AMultiple%20Instance%20Learning%20%28CARMIL%29%2C%20a%20versatile%20regularization%20scheme%20designed%0Ato%20seamlessly%20integrate%20spatial%20knowledge%20into%20any%20MIL%20model.%20Additionally%2C%20we%0Apresent%20a%20new%20and%20generic%20metric%20to%20quantify%20the%20Context-Awareness%20of%20any%20MIL%0Amodel%20when%20applied%20to%20Whole%20Slide%20Images%2C%20resolving%20a%20previously%20unexplored%20gap%0Ain%20the%20field.%20The%20efficacy%20of%20our%20framework%20is%20evaluated%20for%20two%20survival%0Aanalysis%20tasks%20on%20glioblastoma%20%28TCGA%20GBM%29%20and%20colon%20cancer%20data%20%28TCGA%20COAD%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00427v1&entry.124074799=Read"},
{"title": "GalleryGPT: Analyzing Paintings with Large Multimodal Models", "author": "Yi Bin and Wenhao Shi and Yujuan Ding and Zhiqiang Hu and Zheng Wang and Yang Yang and See-Kiong Ng and Heng Tao Shen", "abstract": "  Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.\n", "link": "http://arxiv.org/abs/2408.00491v1", "date": "2024-08-01", "relevancy": 2.0749, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5297}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GalleryGPT%3A%20Analyzing%20Paintings%20with%20Large%20Multimodal%20Models&body=Title%3A%20GalleryGPT%3A%20Analyzing%20Paintings%20with%20Large%20Multimodal%20Models%0AAuthor%3A%20Yi%20Bin%20and%20Wenhao%20Shi%20and%20Yujuan%20Ding%20and%20Zhiqiang%20Hu%20and%20Zheng%20Wang%20and%20Yang%20Yang%20and%20See-Kiong%20Ng%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20%20%20Artwork%20analysis%20is%20important%20and%20fundamental%20skill%20for%20art%20appreciation%2C%0Awhich%20could%20enrich%20personal%20aesthetic%20sensibility%20and%20facilitate%20the%20critical%0Athinking%20ability.%20Understanding%20artworks%20is%20challenging%20due%20to%20its%20subjective%0Anature%2C%20diverse%20interpretations%2C%20and%20complex%20visual%20elements%2C%20requiring%0Aexpertise%20in%20art%20history%2C%20cultural%20background%2C%20and%20aesthetic%20theory.%20However%2C%0Alimited%20by%20the%20data%20collection%20and%20model%20ability%2C%20previous%20works%20for%0Aautomatically%20analyzing%20artworks%20mainly%20focus%20on%20classification%2C%20retrieval%2C%20and%0Aother%20simple%20tasks%2C%20which%20is%20far%20from%20the%20goal%20of%20AI.%20To%20facilitate%20the%0Aresearch%20progress%2C%20in%20this%20paper%2C%20we%20step%20further%20to%20compose%20comprehensive%0Aanalysis%20inspired%20by%20the%20remarkable%20perception%20and%20generation%20ability%20of%20large%0Amultimodal%20models.%20Specifically%2C%20we%20first%20propose%20a%20task%20of%20composing%20paragraph%0Aanalysis%20for%20artworks%2C%20i.e.%2C%20painting%20in%20this%20paper%2C%20only%20focusing%20on%20visual%0Acharacteristics%20to%20formulate%20more%20comprehensive%20understanding%20of%20artworks.%20To%0Asupport%20the%20research%20on%20formal%20analysis%2C%20we%20collect%20a%20large%20dataset%0APaintingForm%2C%20with%20about%2019k%20painting%20images%20and%2050k%20analysis%20paragraphs.%20We%0Afurther%20introduce%20a%20superior%20large%20multimodal%20model%20for%20painting%20analysis%0Acomposing%2C%20dubbed%20GalleryGPT%2C%20which%20is%20slightly%20modified%20and%20fine-tuned%20based%0Aon%20LLaVA%20architecture%20leveraging%20our%20collected%20data.%20We%20conduct%20formal%20analysis%0Ageneration%20and%20zero-shot%20experiments%20across%20several%20datasets%20to%20assess%20the%0Acapacity%20of%20our%20model.%20The%20results%20show%20remarkable%20performance%20improvements%0Acomparing%20with%20powerful%20baseline%20LMMs%2C%20demonstrating%20its%20superb%20ability%20of%20art%0Aanalysis%20and%20generalization.%20%5Ctextcolor%7Bblue%7D%7BThe%20codes%20and%20model%20are%20available%0Aat%3A%20https%3A//github.com/steven640pixel/GalleryGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGalleryGPT%253A%2520Analyzing%2520Paintings%2520with%2520Large%2520Multimodal%2520Models%26entry.906535625%3DYi%2520Bin%2520and%2520Wenhao%2520Shi%2520and%2520Yujuan%2520Ding%2520and%2520Zhiqiang%2520Hu%2520and%2520Zheng%2520Wang%2520and%2520Yang%2520Yang%2520and%2520See-Kiong%2520Ng%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3D%2520%2520Artwork%2520analysis%2520is%2520important%2520and%2520fundamental%2520skill%2520for%2520art%2520appreciation%252C%250Awhich%2520could%2520enrich%2520personal%2520aesthetic%2520sensibility%2520and%2520facilitate%2520the%2520critical%250Athinking%2520ability.%2520Understanding%2520artworks%2520is%2520challenging%2520due%2520to%2520its%2520subjective%250Anature%252C%2520diverse%2520interpretations%252C%2520and%2520complex%2520visual%2520elements%252C%2520requiring%250Aexpertise%2520in%2520art%2520history%252C%2520cultural%2520background%252C%2520and%2520aesthetic%2520theory.%2520However%252C%250Alimited%2520by%2520the%2520data%2520collection%2520and%2520model%2520ability%252C%2520previous%2520works%2520for%250Aautomatically%2520analyzing%2520artworks%2520mainly%2520focus%2520on%2520classification%252C%2520retrieval%252C%2520and%250Aother%2520simple%2520tasks%252C%2520which%2520is%2520far%2520from%2520the%2520goal%2520of%2520AI.%2520To%2520facilitate%2520the%250Aresearch%2520progress%252C%2520in%2520this%2520paper%252C%2520we%2520step%2520further%2520to%2520compose%2520comprehensive%250Aanalysis%2520inspired%2520by%2520the%2520remarkable%2520perception%2520and%2520generation%2520ability%2520of%2520large%250Amultimodal%2520models.%2520Specifically%252C%2520we%2520first%2520propose%2520a%2520task%2520of%2520composing%2520paragraph%250Aanalysis%2520for%2520artworks%252C%2520i.e.%252C%2520painting%2520in%2520this%2520paper%252C%2520only%2520focusing%2520on%2520visual%250Acharacteristics%2520to%2520formulate%2520more%2520comprehensive%2520understanding%2520of%2520artworks.%2520To%250Asupport%2520the%2520research%2520on%2520formal%2520analysis%252C%2520we%2520collect%2520a%2520large%2520dataset%250APaintingForm%252C%2520with%2520about%252019k%2520painting%2520images%2520and%252050k%2520analysis%2520paragraphs.%2520We%250Afurther%2520introduce%2520a%2520superior%2520large%2520multimodal%2520model%2520for%2520painting%2520analysis%250Acomposing%252C%2520dubbed%2520GalleryGPT%252C%2520which%2520is%2520slightly%2520modified%2520and%2520fine-tuned%2520based%250Aon%2520LLaVA%2520architecture%2520leveraging%2520our%2520collected%2520data.%2520We%2520conduct%2520formal%2520analysis%250Ageneration%2520and%2520zero-shot%2520experiments%2520across%2520several%2520datasets%2520to%2520assess%2520the%250Acapacity%2520of%2520our%2520model.%2520The%2520results%2520show%2520remarkable%2520performance%2520improvements%250Acomparing%2520with%2520powerful%2520baseline%2520LMMs%252C%2520demonstrating%2520its%2520superb%2520ability%2520of%2520art%250Aanalysis%2520and%2520generalization.%2520%255Ctextcolor%257Bblue%257D%257BThe%2520codes%2520and%2520model%2520are%2520available%250Aat%253A%2520https%253A//github.com/steven640pixel/GalleryGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GalleryGPT%3A%20Analyzing%20Paintings%20with%20Large%20Multimodal%20Models&entry.906535625=Yi%20Bin%20and%20Wenhao%20Shi%20and%20Yujuan%20Ding%20and%20Zhiqiang%20Hu%20and%20Zheng%20Wang%20and%20Yang%20Yang%20and%20See-Kiong%20Ng%20and%20Heng%20Tao%20Shen&entry.1292438233=%20%20Artwork%20analysis%20is%20important%20and%20fundamental%20skill%20for%20art%20appreciation%2C%0Awhich%20could%20enrich%20personal%20aesthetic%20sensibility%20and%20facilitate%20the%20critical%0Athinking%20ability.%20Understanding%20artworks%20is%20challenging%20due%20to%20its%20subjective%0Anature%2C%20diverse%20interpretations%2C%20and%20complex%20visual%20elements%2C%20requiring%0Aexpertise%20in%20art%20history%2C%20cultural%20background%2C%20and%20aesthetic%20theory.%20However%2C%0Alimited%20by%20the%20data%20collection%20and%20model%20ability%2C%20previous%20works%20for%0Aautomatically%20analyzing%20artworks%20mainly%20focus%20on%20classification%2C%20retrieval%2C%20and%0Aother%20simple%20tasks%2C%20which%20is%20far%20from%20the%20goal%20of%20AI.%20To%20facilitate%20the%0Aresearch%20progress%2C%20in%20this%20paper%2C%20we%20step%20further%20to%20compose%20comprehensive%0Aanalysis%20inspired%20by%20the%20remarkable%20perception%20and%20generation%20ability%20of%20large%0Amultimodal%20models.%20Specifically%2C%20we%20first%20propose%20a%20task%20of%20composing%20paragraph%0Aanalysis%20for%20artworks%2C%20i.e.%2C%20painting%20in%20this%20paper%2C%20only%20focusing%20on%20visual%0Acharacteristics%20to%20formulate%20more%20comprehensive%20understanding%20of%20artworks.%20To%0Asupport%20the%20research%20on%20formal%20analysis%2C%20we%20collect%20a%20large%20dataset%0APaintingForm%2C%20with%20about%2019k%20painting%20images%20and%2050k%20analysis%20paragraphs.%20We%0Afurther%20introduce%20a%20superior%20large%20multimodal%20model%20for%20painting%20analysis%0Acomposing%2C%20dubbed%20GalleryGPT%2C%20which%20is%20slightly%20modified%20and%20fine-tuned%20based%0Aon%20LLaVA%20architecture%20leveraging%20our%20collected%20data.%20We%20conduct%20formal%20analysis%0Ageneration%20and%20zero-shot%20experiments%20across%20several%20datasets%20to%20assess%20the%0Acapacity%20of%20our%20model.%20The%20results%20show%20remarkable%20performance%20improvements%0Acomparing%20with%20powerful%20baseline%20LMMs%2C%20demonstrating%20its%20superb%20ability%20of%20art%0Aanalysis%20and%20generalization.%20%5Ctextcolor%7Bblue%7D%7BThe%20codes%20and%20model%20are%20available%0Aat%3A%20https%3A//github.com/steven640pixel/GalleryGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00491v1&entry.124074799=Read"},
{"title": "Regional quality estimation for echocardiography using deep learning", "author": "Gilles Van De Vyver and Svein-Erik M\u00e5s\u00f8y and H\u00e5vard Dalen and Bj\u00f8rnar Leangen Grenne and Espen Holte and Sindre Hellum Olaisen and John Nyberg and Andreas \u00d8stvik and Lasse L\u00f8vstakken and Erik Smistad", "abstract": "  Automatic estimation of cardiac ultrasound image quality can be beneficial\nfor guiding operators and ensuring the accuracy of clinical measurements.\nPrevious work often fails to distinguish the view correctness of the\nechocardiogram from the image quality. Additionally, previous studies only\nprovide a global image quality value, which limits their practical utility. In\nthis work, we developed and compared three methods to estimate image quality:\n1) classic pixel-based metrics like the generalized contrast-to-noise ratio\n(gCNR) on myocardial segments as region of interest and left ventricle lumen as\nbackground, obtained using a U-Net segmentation 2) local image coherence\nderived from a U-Net model that predicts coherence from B-Mode images 3) a deep\nconvolutional network that predicts the quality of each region directly in an\nend-to-end fashion. We evaluate each method against manual regional image\nquality annotations by three experienced cardiologists. The results indicate\npoor performance of the gCNR metric, with Spearman correlation to the\nannotations of \\r{ho} = 0.24. The end-to-end learning model obtains the best\nresult, \\r{ho} = 0.69, comparable to the inter-observer correlation, \\r{ho} =\n0.63. Finally, the coherence-based method, with \\r{ho} = 0.58, outperformed the\nclassical metrics and is more generic than the end-to-end approach.\n", "link": "http://arxiv.org/abs/2408.00591v1", "date": "2024-08-01", "relevancy": 2.0741, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5352}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5336}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regional%20quality%20estimation%20for%20echocardiography%20using%20deep%20learning&body=Title%3A%20Regional%20quality%20estimation%20for%20echocardiography%20using%20deep%20learning%0AAuthor%3A%20Gilles%20Van%20De%20Vyver%20and%20Svein-Erik%20M%C3%A5s%C3%B8y%20and%20H%C3%A5vard%20Dalen%20and%20Bj%C3%B8rnar%20Leangen%20Grenne%20and%20Espen%20Holte%20and%20Sindre%20Hellum%20Olaisen%20and%20John%20Nyberg%20and%20Andreas%20%C3%98stvik%20and%20Lasse%20L%C3%B8vstakken%20and%20Erik%20Smistad%0AAbstract%3A%20%20%20Automatic%20estimation%20of%20cardiac%20ultrasound%20image%20quality%20can%20be%20beneficial%0Afor%20guiding%20operators%20and%20ensuring%20the%20accuracy%20of%20clinical%20measurements.%0APrevious%20work%20often%20fails%20to%20distinguish%20the%20view%20correctness%20of%20the%0Aechocardiogram%20from%20the%20image%20quality.%20Additionally%2C%20previous%20studies%20only%0Aprovide%20a%20global%20image%20quality%20value%2C%20which%20limits%20their%20practical%20utility.%20In%0Athis%20work%2C%20we%20developed%20and%20compared%20three%20methods%20to%20estimate%20image%20quality%3A%0A1%29%20classic%20pixel-based%20metrics%20like%20the%20generalized%20contrast-to-noise%20ratio%0A%28gCNR%29%20on%20myocardial%20segments%20as%20region%20of%20interest%20and%20left%20ventricle%20lumen%20as%0Abackground%2C%20obtained%20using%20a%20U-Net%20segmentation%202%29%20local%20image%20coherence%0Aderived%20from%20a%20U-Net%20model%20that%20predicts%20coherence%20from%20B-Mode%20images%203%29%20a%20deep%0Aconvolutional%20network%20that%20predicts%20the%20quality%20of%20each%20region%20directly%20in%20an%0Aend-to-end%20fashion.%20We%20evaluate%20each%20method%20against%20manual%20regional%20image%0Aquality%20annotations%20by%20three%20experienced%20cardiologists.%20The%20results%20indicate%0Apoor%20performance%20of%20the%20gCNR%20metric%2C%20with%20Spearman%20correlation%20to%20the%0Aannotations%20of%20%5Cr%7Bho%7D%20%3D%200.24.%20The%20end-to-end%20learning%20model%20obtains%20the%20best%0Aresult%2C%20%5Cr%7Bho%7D%20%3D%200.69%2C%20comparable%20to%20the%20inter-observer%20correlation%2C%20%5Cr%7Bho%7D%20%3D%0A0.63.%20Finally%2C%20the%20coherence-based%20method%2C%20with%20%5Cr%7Bho%7D%20%3D%200.58%2C%20outperformed%20the%0Aclassical%20metrics%20and%20is%20more%20generic%20than%20the%20end-to-end%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegional%2520quality%2520estimation%2520for%2520echocardiography%2520using%2520deep%2520learning%26entry.906535625%3DGilles%2520Van%2520De%2520Vyver%2520and%2520Svein-Erik%2520M%25C3%25A5s%25C3%25B8y%2520and%2520H%25C3%25A5vard%2520Dalen%2520and%2520Bj%25C3%25B8rnar%2520Leangen%2520Grenne%2520and%2520Espen%2520Holte%2520and%2520Sindre%2520Hellum%2520Olaisen%2520and%2520John%2520Nyberg%2520and%2520Andreas%2520%25C3%2598stvik%2520and%2520Lasse%2520L%25C3%25B8vstakken%2520and%2520Erik%2520Smistad%26entry.1292438233%3D%2520%2520Automatic%2520estimation%2520of%2520cardiac%2520ultrasound%2520image%2520quality%2520can%2520be%2520beneficial%250Afor%2520guiding%2520operators%2520and%2520ensuring%2520the%2520accuracy%2520of%2520clinical%2520measurements.%250APrevious%2520work%2520often%2520fails%2520to%2520distinguish%2520the%2520view%2520correctness%2520of%2520the%250Aechocardiogram%2520from%2520the%2520image%2520quality.%2520Additionally%252C%2520previous%2520studies%2520only%250Aprovide%2520a%2520global%2520image%2520quality%2520value%252C%2520which%2520limits%2520their%2520practical%2520utility.%2520In%250Athis%2520work%252C%2520we%2520developed%2520and%2520compared%2520three%2520methods%2520to%2520estimate%2520image%2520quality%253A%250A1%2529%2520classic%2520pixel-based%2520metrics%2520like%2520the%2520generalized%2520contrast-to-noise%2520ratio%250A%2528gCNR%2529%2520on%2520myocardial%2520segments%2520as%2520region%2520of%2520interest%2520and%2520left%2520ventricle%2520lumen%2520as%250Abackground%252C%2520obtained%2520using%2520a%2520U-Net%2520segmentation%25202%2529%2520local%2520image%2520coherence%250Aderived%2520from%2520a%2520U-Net%2520model%2520that%2520predicts%2520coherence%2520from%2520B-Mode%2520images%25203%2529%2520a%2520deep%250Aconvolutional%2520network%2520that%2520predicts%2520the%2520quality%2520of%2520each%2520region%2520directly%2520in%2520an%250Aend-to-end%2520fashion.%2520We%2520evaluate%2520each%2520method%2520against%2520manual%2520regional%2520image%250Aquality%2520annotations%2520by%2520three%2520experienced%2520cardiologists.%2520The%2520results%2520indicate%250Apoor%2520performance%2520of%2520the%2520gCNR%2520metric%252C%2520with%2520Spearman%2520correlation%2520to%2520the%250Aannotations%2520of%2520%255Cr%257Bho%257D%2520%253D%25200.24.%2520The%2520end-to-end%2520learning%2520model%2520obtains%2520the%2520best%250Aresult%252C%2520%255Cr%257Bho%257D%2520%253D%25200.69%252C%2520comparable%2520to%2520the%2520inter-observer%2520correlation%252C%2520%255Cr%257Bho%257D%2520%253D%250A0.63.%2520Finally%252C%2520the%2520coherence-based%2520method%252C%2520with%2520%255Cr%257Bho%257D%2520%253D%25200.58%252C%2520outperformed%2520the%250Aclassical%2520metrics%2520and%2520is%2520more%2520generic%2520than%2520the%2520end-to-end%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regional%20quality%20estimation%20for%20echocardiography%20using%20deep%20learning&entry.906535625=Gilles%20Van%20De%20Vyver%20and%20Svein-Erik%20M%C3%A5s%C3%B8y%20and%20H%C3%A5vard%20Dalen%20and%20Bj%C3%B8rnar%20Leangen%20Grenne%20and%20Espen%20Holte%20and%20Sindre%20Hellum%20Olaisen%20and%20John%20Nyberg%20and%20Andreas%20%C3%98stvik%20and%20Lasse%20L%C3%B8vstakken%20and%20Erik%20Smistad&entry.1292438233=%20%20Automatic%20estimation%20of%20cardiac%20ultrasound%20image%20quality%20can%20be%20beneficial%0Afor%20guiding%20operators%20and%20ensuring%20the%20accuracy%20of%20clinical%20measurements.%0APrevious%20work%20often%20fails%20to%20distinguish%20the%20view%20correctness%20of%20the%0Aechocardiogram%20from%20the%20image%20quality.%20Additionally%2C%20previous%20studies%20only%0Aprovide%20a%20global%20image%20quality%20value%2C%20which%20limits%20their%20practical%20utility.%20In%0Athis%20work%2C%20we%20developed%20and%20compared%20three%20methods%20to%20estimate%20image%20quality%3A%0A1%29%20classic%20pixel-based%20metrics%20like%20the%20generalized%20contrast-to-noise%20ratio%0A%28gCNR%29%20on%20myocardial%20segments%20as%20region%20of%20interest%20and%20left%20ventricle%20lumen%20as%0Abackground%2C%20obtained%20using%20a%20U-Net%20segmentation%202%29%20local%20image%20coherence%0Aderived%20from%20a%20U-Net%20model%20that%20predicts%20coherence%20from%20B-Mode%20images%203%29%20a%20deep%0Aconvolutional%20network%20that%20predicts%20the%20quality%20of%20each%20region%20directly%20in%20an%0Aend-to-end%20fashion.%20We%20evaluate%20each%20method%20against%20manual%20regional%20image%0Aquality%20annotations%20by%20three%20experienced%20cardiologists.%20The%20results%20indicate%0Apoor%20performance%20of%20the%20gCNR%20metric%2C%20with%20Spearman%20correlation%20to%20the%0Aannotations%20of%20%5Cr%7Bho%7D%20%3D%200.24.%20The%20end-to-end%20learning%20model%20obtains%20the%20best%0Aresult%2C%20%5Cr%7Bho%7D%20%3D%200.69%2C%20comparable%20to%20the%20inter-observer%20correlation%2C%20%5Cr%7Bho%7D%20%3D%0A0.63.%20Finally%2C%20the%20coherence-based%20method%2C%20with%20%5Cr%7Bho%7D%20%3D%200.58%2C%20outperformed%20the%0Aclassical%20metrics%20and%20is%20more%20generic%20than%20the%20end-to-end%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00591v1&entry.124074799=Read"},
{"title": "SynesLM: A Unified Approach for Audio-visual Speech Recognition and\n  Translation via Language Model and Synthetic Data", "author": "Yichen Lu and Jiaqi Song and Xuankai Chang and Hengwei Bian and Soumi Maiti and Shinji Watanabe", "abstract": "  In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.\n", "link": "http://arxiv.org/abs/2408.00624v1", "date": "2024-08-01", "relevancy": 2.074, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4998}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynesLM%3A%20A%20Unified%20Approach%20for%20Audio-visual%20Speech%20Recognition%20and%0A%20%20Translation%20via%20Language%20Model%20and%20Synthetic%20Data&body=Title%3A%20SynesLM%3A%20A%20Unified%20Approach%20for%20Audio-visual%20Speech%20Recognition%20and%0A%20%20Translation%20via%20Language%20Model%20and%20Synthetic%20Data%0AAuthor%3A%20Yichen%20Lu%20and%20Jiaqi%20Song%20and%20Xuankai%20Chang%20and%20Hengwei%20Bian%20and%20Soumi%20Maiti%20and%20Shinji%20Watanabe%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20SynesLM%2C%20an%20unified%20model%20which%20can%20perform%20three%0Amultimodal%20language%20understanding%20tasks%3A%20audio-visual%20automatic%20speech%0Arecognition%28AV-ASR%29%20and%20visual-aided%20speech/machine%20translation%28VST/VMT%29.%0AUnlike%20previous%20research%20that%20focused%20on%20lip%20motion%20as%20visual%20cues%20for%20speech%0Asignals%2C%20our%20work%20explores%20more%20general%20visual%20information%20within%20entire%0Aframes%2C%20such%20as%20objects%20and%20actions.%20Additionally%2C%20we%20use%20synthetic%20image%20data%0Ato%20enhance%20the%20correlation%20between%20image%20and%20speech%20data.%20We%20benchmark%20SynesLM%0Aagainst%20the%20How2%20dataset%2C%20demonstrating%20performance%20on%20par%20with%0Astate-of-the-art%20%28SOTA%29%20models%20dedicated%20to%20AV-ASR%20while%20maintaining%20our%0Amultitasking%20framework.%20Remarkably%2C%20for%20zero-shot%20AV-ASR%2C%20SynesLM%20achieved%20SOTA%0Aperformance%20by%20lowering%20the%20Word%20Error%20Rate%20%28WER%29%20from%2043.4%25%20to%2039.4%25%20on%20the%0AVisSpeech%20Dataset.%20Furthermore%2C%20our%20results%20in%20VST%20and%20VMT%20outperform%20the%0Aprevious%20results%2C%20improving%20the%20BLEU%20score%20to%2043.5%20from%2037.2%20for%20VST%2C%20and%20to%0A54.8%20from%2054.4%20for%20VMT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynesLM%253A%2520A%2520Unified%2520Approach%2520for%2520Audio-visual%2520Speech%2520Recognition%2520and%250A%2520%2520Translation%2520via%2520Language%2520Model%2520and%2520Synthetic%2520Data%26entry.906535625%3DYichen%2520Lu%2520and%2520Jiaqi%2520Song%2520and%2520Xuankai%2520Chang%2520and%2520Hengwei%2520Bian%2520and%2520Soumi%2520Maiti%2520and%2520Shinji%2520Watanabe%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520SynesLM%252C%2520an%2520unified%2520model%2520which%2520can%2520perform%2520three%250Amultimodal%2520language%2520understanding%2520tasks%253A%2520audio-visual%2520automatic%2520speech%250Arecognition%2528AV-ASR%2529%2520and%2520visual-aided%2520speech/machine%2520translation%2528VST/VMT%2529.%250AUnlike%2520previous%2520research%2520that%2520focused%2520on%2520lip%2520motion%2520as%2520visual%2520cues%2520for%2520speech%250Asignals%252C%2520our%2520work%2520explores%2520more%2520general%2520visual%2520information%2520within%2520entire%250Aframes%252C%2520such%2520as%2520objects%2520and%2520actions.%2520Additionally%252C%2520we%2520use%2520synthetic%2520image%2520data%250Ato%2520enhance%2520the%2520correlation%2520between%2520image%2520and%2520speech%2520data.%2520We%2520benchmark%2520SynesLM%250Aagainst%2520the%2520How2%2520dataset%252C%2520demonstrating%2520performance%2520on%2520par%2520with%250Astate-of-the-art%2520%2528SOTA%2529%2520models%2520dedicated%2520to%2520AV-ASR%2520while%2520maintaining%2520our%250Amultitasking%2520framework.%2520Remarkably%252C%2520for%2520zero-shot%2520AV-ASR%252C%2520SynesLM%2520achieved%2520SOTA%250Aperformance%2520by%2520lowering%2520the%2520Word%2520Error%2520Rate%2520%2528WER%2529%2520from%252043.4%2525%2520to%252039.4%2525%2520on%2520the%250AVisSpeech%2520Dataset.%2520Furthermore%252C%2520our%2520results%2520in%2520VST%2520and%2520VMT%2520outperform%2520the%250Aprevious%2520results%252C%2520improving%2520the%2520BLEU%2520score%2520to%252043.5%2520from%252037.2%2520for%2520VST%252C%2520and%2520to%250A54.8%2520from%252054.4%2520for%2520VMT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynesLM%3A%20A%20Unified%20Approach%20for%20Audio-visual%20Speech%20Recognition%20and%0A%20%20Translation%20via%20Language%20Model%20and%20Synthetic%20Data&entry.906535625=Yichen%20Lu%20and%20Jiaqi%20Song%20and%20Xuankai%20Chang%20and%20Hengwei%20Bian%20and%20Soumi%20Maiti%20and%20Shinji%20Watanabe&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20SynesLM%2C%20an%20unified%20model%20which%20can%20perform%20three%0Amultimodal%20language%20understanding%20tasks%3A%20audio-visual%20automatic%20speech%0Arecognition%28AV-ASR%29%20and%20visual-aided%20speech/machine%20translation%28VST/VMT%29.%0AUnlike%20previous%20research%20that%20focused%20on%20lip%20motion%20as%20visual%20cues%20for%20speech%0Asignals%2C%20our%20work%20explores%20more%20general%20visual%20information%20within%20entire%0Aframes%2C%20such%20as%20objects%20and%20actions.%20Additionally%2C%20we%20use%20synthetic%20image%20data%0Ato%20enhance%20the%20correlation%20between%20image%20and%20speech%20data.%20We%20benchmark%20SynesLM%0Aagainst%20the%20How2%20dataset%2C%20demonstrating%20performance%20on%20par%20with%0Astate-of-the-art%20%28SOTA%29%20models%20dedicated%20to%20AV-ASR%20while%20maintaining%20our%0Amultitasking%20framework.%20Remarkably%2C%20for%20zero-shot%20AV-ASR%2C%20SynesLM%20achieved%20SOTA%0Aperformance%20by%20lowering%20the%20Word%20Error%20Rate%20%28WER%29%20from%2043.4%25%20to%2039.4%25%20on%20the%0AVisSpeech%20Dataset.%20Furthermore%2C%20our%20results%20in%20VST%20and%20VMT%20outperform%20the%0Aprevious%20results%2C%20improving%20the%20BLEU%20score%20to%2043.5%20from%2037.2%20for%20VST%2C%20and%20to%0A54.8%20from%2054.4%20for%20VMT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00624v1&entry.124074799=Read"},
{"title": "Jailbreaking Text-to-Image Models with LLM-Based Agents", "author": "Yingkai Dong and Zheng Li and Xiangtao Meng and Ning Yu and Shanqing Guo", "abstract": "  Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving gaps in addressing generative AI safety tasks. These gaps are\nprimarily due to the challenges posed by LLM hallucinations and the lack of\nclear guidelines. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework that integrates an efficient fuzzing workflow to target\ngenerative AI models, specifically focusing on jailbreak attacks against\ntext-to-image (T2I) models with safety filters. Atlas utilizes a\nvision-language model (VLM) to assess whether a prompt triggers the T2I model's\nsafety filter. It then iteratively collaborates with both LLM and VLM to\ngenerate an alternative prompt that bypasses the filter. Atlas also enhances\nthe reasoning abilities of LLMs in attack scenarios by leveraging multi-agent\ncommunication, in-context learning (ICL) memory mechanisms, and the\nchain-of-thought (COT) approach. Our evaluation demonstrates that Atlas\nsuccessfully jailbreaks several state-of-the-art T2I models in a black-box\nsetting, which are equipped with multi-modal safety filters. In addition, Atlas\noutperforms existing methods in both query efficiency and the quality of the\ngenerated images.\n", "link": "http://arxiv.org/abs/2408.00523v1", "date": "2024-08-01", "relevancy": 2.053, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5278}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5173}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jailbreaking%20Text-to-Image%20Models%20with%20LLM-Based%20Agents&body=Title%3A%20Jailbreaking%20Text-to-Image%20Models%20with%20LLM-Based%20Agents%0AAuthor%3A%20Yingkai%20Dong%20and%20Zheng%20Li%20and%20Xiangtao%20Meng%20and%20Ning%20Yu%20and%20Shanqing%20Guo%0AAbstract%3A%20%20%20Recent%20advancements%20have%20significantly%20improved%20automated%20task-solving%0Acapabilities%20using%20autonomous%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20most%20LLM-based%20agents%20focus%20on%20dialogue%2C%20programming%2C%20or%20specialized%0Adomains%2C%20leaving%20gaps%20in%20addressing%20generative%20AI%20safety%20tasks.%20These%20gaps%20are%0Aprimarily%20due%20to%20the%20challenges%20posed%20by%20LLM%20hallucinations%20and%20the%20lack%20of%0Aclear%20guidelines.%20In%20this%20paper%2C%20we%20propose%20Atlas%2C%20an%20advanced%20LLM-based%0Amulti-agent%20framework%20that%20integrates%20an%20efficient%20fuzzing%20workflow%20to%20target%0Agenerative%20AI%20models%2C%20specifically%20focusing%20on%20jailbreak%20attacks%20against%0Atext-to-image%20%28T2I%29%20models%20with%20safety%20filters.%20Atlas%20utilizes%20a%0Avision-language%20model%20%28VLM%29%20to%20assess%20whether%20a%20prompt%20triggers%20the%20T2I%20model%27s%0Asafety%20filter.%20It%20then%20iteratively%20collaborates%20with%20both%20LLM%20and%20VLM%20to%0Agenerate%20an%20alternative%20prompt%20that%20bypasses%20the%20filter.%20Atlas%20also%20enhances%0Athe%20reasoning%20abilities%20of%20LLMs%20in%20attack%20scenarios%20by%20leveraging%20multi-agent%0Acommunication%2C%20in-context%20learning%20%28ICL%29%20memory%20mechanisms%2C%20and%20the%0Achain-of-thought%20%28COT%29%20approach.%20Our%20evaluation%20demonstrates%20that%20Atlas%0Asuccessfully%20jailbreaks%20several%20state-of-the-art%20T2I%20models%20in%20a%20black-box%0Asetting%2C%20which%20are%20equipped%20with%20multi-modal%20safety%20filters.%20In%20addition%2C%20Atlas%0Aoutperforms%20existing%20methods%20in%20both%20query%20efficiency%20and%20the%20quality%20of%20the%0Agenerated%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJailbreaking%2520Text-to-Image%2520Models%2520with%2520LLM-Based%2520Agents%26entry.906535625%3DYingkai%2520Dong%2520and%2520Zheng%2520Li%2520and%2520Xiangtao%2520Meng%2520and%2520Ning%2520Yu%2520and%2520Shanqing%2520Guo%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520have%2520significantly%2520improved%2520automated%2520task-solving%250Acapabilities%2520using%2520autonomous%2520agents%2520powered%2520by%2520large%2520language%2520models%2520%2528LLMs%2529.%250AHowever%252C%2520most%2520LLM-based%2520agents%2520focus%2520on%2520dialogue%252C%2520programming%252C%2520or%2520specialized%250Adomains%252C%2520leaving%2520gaps%2520in%2520addressing%2520generative%2520AI%2520safety%2520tasks.%2520These%2520gaps%2520are%250Aprimarily%2520due%2520to%2520the%2520challenges%2520posed%2520by%2520LLM%2520hallucinations%2520and%2520the%2520lack%2520of%250Aclear%2520guidelines.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Atlas%252C%2520an%2520advanced%2520LLM-based%250Amulti-agent%2520framework%2520that%2520integrates%2520an%2520efficient%2520fuzzing%2520workflow%2520to%2520target%250Agenerative%2520AI%2520models%252C%2520specifically%2520focusing%2520on%2520jailbreak%2520attacks%2520against%250Atext-to-image%2520%2528T2I%2529%2520models%2520with%2520safety%2520filters.%2520Atlas%2520utilizes%2520a%250Avision-language%2520model%2520%2528VLM%2529%2520to%2520assess%2520whether%2520a%2520prompt%2520triggers%2520the%2520T2I%2520model%2527s%250Asafety%2520filter.%2520It%2520then%2520iteratively%2520collaborates%2520with%2520both%2520LLM%2520and%2520VLM%2520to%250Agenerate%2520an%2520alternative%2520prompt%2520that%2520bypasses%2520the%2520filter.%2520Atlas%2520also%2520enhances%250Athe%2520reasoning%2520abilities%2520of%2520LLMs%2520in%2520attack%2520scenarios%2520by%2520leveraging%2520multi-agent%250Acommunication%252C%2520in-context%2520learning%2520%2528ICL%2529%2520memory%2520mechanisms%252C%2520and%2520the%250Achain-of-thought%2520%2528COT%2529%2520approach.%2520Our%2520evaluation%2520demonstrates%2520that%2520Atlas%250Asuccessfully%2520jailbreaks%2520several%2520state-of-the-art%2520T2I%2520models%2520in%2520a%2520black-box%250Asetting%252C%2520which%2520are%2520equipped%2520with%2520multi-modal%2520safety%2520filters.%2520In%2520addition%252C%2520Atlas%250Aoutperforms%2520existing%2520methods%2520in%2520both%2520query%2520efficiency%2520and%2520the%2520quality%2520of%2520the%250Agenerated%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jailbreaking%20Text-to-Image%20Models%20with%20LLM-Based%20Agents&entry.906535625=Yingkai%20Dong%20and%20Zheng%20Li%20and%20Xiangtao%20Meng%20and%20Ning%20Yu%20and%20Shanqing%20Guo&entry.1292438233=%20%20Recent%20advancements%20have%20significantly%20improved%20automated%20task-solving%0Acapabilities%20using%20autonomous%20agents%20powered%20by%20large%20language%20models%20%28LLMs%29.%0AHowever%2C%20most%20LLM-based%20agents%20focus%20on%20dialogue%2C%20programming%2C%20or%20specialized%0Adomains%2C%20leaving%20gaps%20in%20addressing%20generative%20AI%20safety%20tasks.%20These%20gaps%20are%0Aprimarily%20due%20to%20the%20challenges%20posed%20by%20LLM%20hallucinations%20and%20the%20lack%20of%0Aclear%20guidelines.%20In%20this%20paper%2C%20we%20propose%20Atlas%2C%20an%20advanced%20LLM-based%0Amulti-agent%20framework%20that%20integrates%20an%20efficient%20fuzzing%20workflow%20to%20target%0Agenerative%20AI%20models%2C%20specifically%20focusing%20on%20jailbreak%20attacks%20against%0Atext-to-image%20%28T2I%29%20models%20with%20safety%20filters.%20Atlas%20utilizes%20a%0Avision-language%20model%20%28VLM%29%20to%20assess%20whether%20a%20prompt%20triggers%20the%20T2I%20model%27s%0Asafety%20filter.%20It%20then%20iteratively%20collaborates%20with%20both%20LLM%20and%20VLM%20to%0Agenerate%20an%20alternative%20prompt%20that%20bypasses%20the%20filter.%20Atlas%20also%20enhances%0Athe%20reasoning%20abilities%20of%20LLMs%20in%20attack%20scenarios%20by%20leveraging%20multi-agent%0Acommunication%2C%20in-context%20learning%20%28ICL%29%20memory%20mechanisms%2C%20and%20the%0Achain-of-thought%20%28COT%29%20approach.%20Our%20evaluation%20demonstrates%20that%20Atlas%0Asuccessfully%20jailbreaks%20several%20state-of-the-art%20T2I%20models%20in%20a%20black-box%0Asetting%2C%20which%20are%20equipped%20with%20multi-modal%20safety%20filters.%20In%20addition%2C%20Atlas%0Aoutperforms%20existing%20methods%20in%20both%20query%20efficiency%20and%20the%20quality%20of%20the%0Agenerated%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00523v1&entry.124074799=Read"},
{"title": "SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image\n  Classification", "author": "Peijie Qiu and Pan Xiao and Wenhui Zhu and Yalin Wang and Aristeidis Sotiras", "abstract": "  Multiple Instance Learning (MIL) has been widely used in weakly supervised\nwhole slide image (WSI) classification. Typical MIL methods include a feature\nembedding part, which embeds the instances into features via a pre-trained\nfeature extractor, and an MIL aggregator that combines instance embeddings into\npredictions. Most efforts have typically focused on improving these parts. This\ninvolves refining the feature embeddings through self-supervised pre-training\nas well as modeling the correlations between instances separately.\n  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that\naddresses those two aspects at the same time by leveraging sparse dictionary\nlearning. The sparse dictionary learning captures the similarities of instances\nby expressing them as sparse linear combinations of atoms in an over-complete\ndictionary. In addition, imposing sparsity improves instance feature embeddings\nby suppressing irrelevant instances while retaining the most relevant ones. To\nmake the conventional sparse coding algorithm compatible with deep learning, we\nunrolled it into a sparsely coded module leveraging deep unrolling. The\nproposed SC module can be incorporated into any existing MIL framework in a\nplug-and-play manner with an acceptable computational cost. The experimental\nresults on multiple datasets demonstrated that the proposed SC module could\nsubstantially boost the performance of state-of-the-art MIL methods. The codes\nare available at\n\\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.\n", "link": "http://arxiv.org/abs/2311.00048v2", "date": "2024-08-01", "relevancy": 2.0459, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.527}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5054}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SC-MIL%3A%20Sparsely%20Coded%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Image%0A%20%20Classification&body=Title%3A%20SC-MIL%3A%20Sparsely%20Coded%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Image%0A%20%20Classification%0AAuthor%3A%20Peijie%20Qiu%20and%20Pan%20Xiao%20and%20Wenhui%20Zhu%20and%20Yalin%20Wang%20and%20Aristeidis%20Sotiras%0AAbstract%3A%20%20%20Multiple%20Instance%20Learning%20%28MIL%29%20has%20been%20widely%20used%20in%20weakly%20supervised%0Awhole%20slide%20image%20%28WSI%29%20classification.%20Typical%20MIL%20methods%20include%20a%20feature%0Aembedding%20part%2C%20which%20embeds%20the%20instances%20into%20features%20via%20a%20pre-trained%0Afeature%20extractor%2C%20and%20an%20MIL%20aggregator%20that%20combines%20instance%20embeddings%20into%0Apredictions.%20Most%20efforts%20have%20typically%20focused%20on%20improving%20these%20parts.%20This%0Ainvolves%20refining%20the%20feature%20embeddings%20through%20self-supervised%20pre-training%0Aas%20well%20as%20modeling%20the%20correlations%20between%20instances%20separately.%0A%20%20In%20this%20paper%2C%20we%20proposed%20a%20sparsely%20coding%20MIL%20%28SC-MIL%29%20method%20that%0Aaddresses%20those%20two%20aspects%20at%20the%20same%20time%20by%20leveraging%20sparse%20dictionary%0Alearning.%20The%20sparse%20dictionary%20learning%20captures%20the%20similarities%20of%20instances%0Aby%20expressing%20them%20as%20sparse%20linear%20combinations%20of%20atoms%20in%20an%20over-complete%0Adictionary.%20In%20addition%2C%20imposing%20sparsity%20improves%20instance%20feature%20embeddings%0Aby%20suppressing%20irrelevant%20instances%20while%20retaining%20the%20most%20relevant%20ones.%20To%0Amake%20the%20conventional%20sparse%20coding%20algorithm%20compatible%20with%20deep%20learning%2C%20we%0Aunrolled%20it%20into%20a%20sparsely%20coded%20module%20leveraging%20deep%20unrolling.%20The%0Aproposed%20SC%20module%20can%20be%20incorporated%20into%20any%20existing%20MIL%20framework%20in%20a%0Aplug-and-play%20manner%20with%20an%20acceptable%20computational%20cost.%20The%20experimental%0Aresults%20on%20multiple%20datasets%20demonstrated%20that%20the%20proposed%20SC%20module%20could%0Asubstantially%20boost%20the%20performance%20of%20state-of-the-art%20MIL%20methods.%20The%20codes%0Aare%20available%20at%0A%5Chref%7Bhttps%3A//github.com/sotiraslab/SCMIL.git%7D%7Bhttps%3A//github.com/sotiraslab/SCMIL.git%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSC-MIL%253A%2520Sparsely%2520Coded%2520Multiple%2520Instance%2520Learning%2520for%2520Whole%2520Slide%2520Image%250A%2520%2520Classification%26entry.906535625%3DPeijie%2520Qiu%2520and%2520Pan%2520Xiao%2520and%2520Wenhui%2520Zhu%2520and%2520Yalin%2520Wang%2520and%2520Aristeidis%2520Sotiras%26entry.1292438233%3D%2520%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%2520has%2520been%2520widely%2520used%2520in%2520weakly%2520supervised%250Awhole%2520slide%2520image%2520%2528WSI%2529%2520classification.%2520Typical%2520MIL%2520methods%2520include%2520a%2520feature%250Aembedding%2520part%252C%2520which%2520embeds%2520the%2520instances%2520into%2520features%2520via%2520a%2520pre-trained%250Afeature%2520extractor%252C%2520and%2520an%2520MIL%2520aggregator%2520that%2520combines%2520instance%2520embeddings%2520into%250Apredictions.%2520Most%2520efforts%2520have%2520typically%2520focused%2520on%2520improving%2520these%2520parts.%2520This%250Ainvolves%2520refining%2520the%2520feature%2520embeddings%2520through%2520self-supervised%2520pre-training%250Aas%2520well%2520as%2520modeling%2520the%2520correlations%2520between%2520instances%2520separately.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520proposed%2520a%2520sparsely%2520coding%2520MIL%2520%2528SC-MIL%2529%2520method%2520that%250Aaddresses%2520those%2520two%2520aspects%2520at%2520the%2520same%2520time%2520by%2520leveraging%2520sparse%2520dictionary%250Alearning.%2520The%2520sparse%2520dictionary%2520learning%2520captures%2520the%2520similarities%2520of%2520instances%250Aby%2520expressing%2520them%2520as%2520sparse%2520linear%2520combinations%2520of%2520atoms%2520in%2520an%2520over-complete%250Adictionary.%2520In%2520addition%252C%2520imposing%2520sparsity%2520improves%2520instance%2520feature%2520embeddings%250Aby%2520suppressing%2520irrelevant%2520instances%2520while%2520retaining%2520the%2520most%2520relevant%2520ones.%2520To%250Amake%2520the%2520conventional%2520sparse%2520coding%2520algorithm%2520compatible%2520with%2520deep%2520learning%252C%2520we%250Aunrolled%2520it%2520into%2520a%2520sparsely%2520coded%2520module%2520leveraging%2520deep%2520unrolling.%2520The%250Aproposed%2520SC%2520module%2520can%2520be%2520incorporated%2520into%2520any%2520existing%2520MIL%2520framework%2520in%2520a%250Aplug-and-play%2520manner%2520with%2520an%2520acceptable%2520computational%2520cost.%2520The%2520experimental%250Aresults%2520on%2520multiple%2520datasets%2520demonstrated%2520that%2520the%2520proposed%2520SC%2520module%2520could%250Asubstantially%2520boost%2520the%2520performance%2520of%2520state-of-the-art%2520MIL%2520methods.%2520The%2520codes%250Aare%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/sotiraslab/SCMIL.git%257D%257Bhttps%253A//github.com/sotiraslab/SCMIL.git%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SC-MIL%3A%20Sparsely%20Coded%20Multiple%20Instance%20Learning%20for%20Whole%20Slide%20Image%0A%20%20Classification&entry.906535625=Peijie%20Qiu%20and%20Pan%20Xiao%20and%20Wenhui%20Zhu%20and%20Yalin%20Wang%20and%20Aristeidis%20Sotiras&entry.1292438233=%20%20Multiple%20Instance%20Learning%20%28MIL%29%20has%20been%20widely%20used%20in%20weakly%20supervised%0Awhole%20slide%20image%20%28WSI%29%20classification.%20Typical%20MIL%20methods%20include%20a%20feature%0Aembedding%20part%2C%20which%20embeds%20the%20instances%20into%20features%20via%20a%20pre-trained%0Afeature%20extractor%2C%20and%20an%20MIL%20aggregator%20that%20combines%20instance%20embeddings%20into%0Apredictions.%20Most%20efforts%20have%20typically%20focused%20on%20improving%20these%20parts.%20This%0Ainvolves%20refining%20the%20feature%20embeddings%20through%20self-supervised%20pre-training%0Aas%20well%20as%20modeling%20the%20correlations%20between%20instances%20separately.%0A%20%20In%20this%20paper%2C%20we%20proposed%20a%20sparsely%20coding%20MIL%20%28SC-MIL%29%20method%20that%0Aaddresses%20those%20two%20aspects%20at%20the%20same%20time%20by%20leveraging%20sparse%20dictionary%0Alearning.%20The%20sparse%20dictionary%20learning%20captures%20the%20similarities%20of%20instances%0Aby%20expressing%20them%20as%20sparse%20linear%20combinations%20of%20atoms%20in%20an%20over-complete%0Adictionary.%20In%20addition%2C%20imposing%20sparsity%20improves%20instance%20feature%20embeddings%0Aby%20suppressing%20irrelevant%20instances%20while%20retaining%20the%20most%20relevant%20ones.%20To%0Amake%20the%20conventional%20sparse%20coding%20algorithm%20compatible%20with%20deep%20learning%2C%20we%0Aunrolled%20it%20into%20a%20sparsely%20coded%20module%20leveraging%20deep%20unrolling.%20The%0Aproposed%20SC%20module%20can%20be%20incorporated%20into%20any%20existing%20MIL%20framework%20in%20a%0Aplug-and-play%20manner%20with%20an%20acceptable%20computational%20cost.%20The%20experimental%0Aresults%20on%20multiple%20datasets%20demonstrated%20that%20the%20proposed%20SC%20module%20could%0Asubstantially%20boost%20the%20performance%20of%20state-of-the-art%20MIL%20methods.%20The%20codes%0Aare%20available%20at%0A%5Chref%7Bhttps%3A//github.com/sotiraslab/SCMIL.git%7D%7Bhttps%3A//github.com/sotiraslab/SCMIL.git%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00048v2&entry.124074799=Read"},
{"title": "Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer", "author": "Venkat Margapuri and Prapti Thapaliya and Trevor Rife", "abstract": "  Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.\n", "link": "http://arxiv.org/abs/2408.00749v1", "date": "2024-08-01", "relevancy": 2.0075, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5106}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5018}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leaf%20Angle%20Estimation%20using%20Mask%20R-CNN%20and%20LETR%20Vision%20Transformer&body=Title%3A%20Leaf%20Angle%20Estimation%20using%20Mask%20R-CNN%20and%20LETR%20Vision%20Transformer%0AAuthor%3A%20Venkat%20Margapuri%20and%20Prapti%20Thapaliya%20and%20Trevor%20Rife%0AAbstract%3A%20%20%20Modern%20day%20studies%20show%20a%20high%20degree%20of%20correlation%20between%20high%20yielding%0Acrop%20varieties%20and%20plants%20with%20upright%20leaf%20angles.%20It%20is%20observed%20that%20plants%0Awith%20upright%20leaf%20angles%20intercept%20more%20light%20than%20those%20without%20upright%20leaf%0Aangles%2C%20leading%20to%20a%20higher%20rate%20of%20photosynthesis.%20Plant%20scientists%20and%0Abreeders%20benefit%20from%20tools%20that%20can%20directly%20measure%20plant%20parameters%20in%20the%0Afield%20i.e.%20on-site%20phenotyping.%20The%20estimation%20of%20leaf%20angles%20by%20manual%20means%0Ain%20a%20field%20setting%20is%20tedious%20and%20cumbersome.%20We%20mitigate%20the%20tedium%20using%20a%0Acombination%20of%20the%20Mask%20R-CNN%20instance%20segmentation%20neural%20network%2C%20and%20Line%0ASegment%20Transformer%20%28LETR%29%2C%20a%20vision%20transformer.%20The%20proposed%20Computer%20Vision%0A%28CV%29%20pipeline%20is%20applied%20on%20two%20image%20datasets%2C%20Summer%202015-Ames%20ULA%20and%20Summer%0A2015-%20Ames%20MLA%2C%20with%20a%20combined%20total%20of%201%2C827%20plant%20images%20collected%20in%20the%0Afield%20using%20FieldBook%2C%20an%20Android%20application%20aimed%20at%20on-site%20phenotyping.%20The%0Aleaf%20angles%20estimated%20by%20the%20proposed%20pipeline%20on%20the%20image%20datasets%20are%0Acompared%20to%20two%20independent%20manual%20measurements%20using%20ImageJ%2C%20a%20Java-based%0Aimage%20processing%20program%20developed%20at%20the%20National%20Institutes%20of%20Health%20and%20the%0ALaboratory%20for%20Optical%20and%20Computational%20Instrumentation.%20The%20results%2C%20when%0Acompared%20for%20similarity%20using%20the%20Cosine%20Similarity%20measure%2C%20exhibit%200.98%0Asimilarity%20scores%20on%20both%20independent%20measurements%20of%20Summer%202015-Ames%20ULA%20and%0ASummer%202015-Ames%20MLA%20image%20datasets%2C%20demonstrating%20the%20feasibility%20of%20the%0Aproposed%20pipeline%20for%20on-site%20measurement%20of%20leaf%20angles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeaf%2520Angle%2520Estimation%2520using%2520Mask%2520R-CNN%2520and%2520LETR%2520Vision%2520Transformer%26entry.906535625%3DVenkat%2520Margapuri%2520and%2520Prapti%2520Thapaliya%2520and%2520Trevor%2520Rife%26entry.1292438233%3D%2520%2520Modern%2520day%2520studies%2520show%2520a%2520high%2520degree%2520of%2520correlation%2520between%2520high%2520yielding%250Acrop%2520varieties%2520and%2520plants%2520with%2520upright%2520leaf%2520angles.%2520It%2520is%2520observed%2520that%2520plants%250Awith%2520upright%2520leaf%2520angles%2520intercept%2520more%2520light%2520than%2520those%2520without%2520upright%2520leaf%250Aangles%252C%2520leading%2520to%2520a%2520higher%2520rate%2520of%2520photosynthesis.%2520Plant%2520scientists%2520and%250Abreeders%2520benefit%2520from%2520tools%2520that%2520can%2520directly%2520measure%2520plant%2520parameters%2520in%2520the%250Afield%2520i.e.%2520on-site%2520phenotyping.%2520The%2520estimation%2520of%2520leaf%2520angles%2520by%2520manual%2520means%250Ain%2520a%2520field%2520setting%2520is%2520tedious%2520and%2520cumbersome.%2520We%2520mitigate%2520the%2520tedium%2520using%2520a%250Acombination%2520of%2520the%2520Mask%2520R-CNN%2520instance%2520segmentation%2520neural%2520network%252C%2520and%2520Line%250ASegment%2520Transformer%2520%2528LETR%2529%252C%2520a%2520vision%2520transformer.%2520The%2520proposed%2520Computer%2520Vision%250A%2528CV%2529%2520pipeline%2520is%2520applied%2520on%2520two%2520image%2520datasets%252C%2520Summer%25202015-Ames%2520ULA%2520and%2520Summer%250A2015-%2520Ames%2520MLA%252C%2520with%2520a%2520combined%2520total%2520of%25201%252C827%2520plant%2520images%2520collected%2520in%2520the%250Afield%2520using%2520FieldBook%252C%2520an%2520Android%2520application%2520aimed%2520at%2520on-site%2520phenotyping.%2520The%250Aleaf%2520angles%2520estimated%2520by%2520the%2520proposed%2520pipeline%2520on%2520the%2520image%2520datasets%2520are%250Acompared%2520to%2520two%2520independent%2520manual%2520measurements%2520using%2520ImageJ%252C%2520a%2520Java-based%250Aimage%2520processing%2520program%2520developed%2520at%2520the%2520National%2520Institutes%2520of%2520Health%2520and%2520the%250ALaboratory%2520for%2520Optical%2520and%2520Computational%2520Instrumentation.%2520The%2520results%252C%2520when%250Acompared%2520for%2520similarity%2520using%2520the%2520Cosine%2520Similarity%2520measure%252C%2520exhibit%25200.98%250Asimilarity%2520scores%2520on%2520both%2520independent%2520measurements%2520of%2520Summer%25202015-Ames%2520ULA%2520and%250ASummer%25202015-Ames%2520MLA%2520image%2520datasets%252C%2520demonstrating%2520the%2520feasibility%2520of%2520the%250Aproposed%2520pipeline%2520for%2520on-site%2520measurement%2520of%2520leaf%2520angles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leaf%20Angle%20Estimation%20using%20Mask%20R-CNN%20and%20LETR%20Vision%20Transformer&entry.906535625=Venkat%20Margapuri%20and%20Prapti%20Thapaliya%20and%20Trevor%20Rife&entry.1292438233=%20%20Modern%20day%20studies%20show%20a%20high%20degree%20of%20correlation%20between%20high%20yielding%0Acrop%20varieties%20and%20plants%20with%20upright%20leaf%20angles.%20It%20is%20observed%20that%20plants%0Awith%20upright%20leaf%20angles%20intercept%20more%20light%20than%20those%20without%20upright%20leaf%0Aangles%2C%20leading%20to%20a%20higher%20rate%20of%20photosynthesis.%20Plant%20scientists%20and%0Abreeders%20benefit%20from%20tools%20that%20can%20directly%20measure%20plant%20parameters%20in%20the%0Afield%20i.e.%20on-site%20phenotyping.%20The%20estimation%20of%20leaf%20angles%20by%20manual%20means%0Ain%20a%20field%20setting%20is%20tedious%20and%20cumbersome.%20We%20mitigate%20the%20tedium%20using%20a%0Acombination%20of%20the%20Mask%20R-CNN%20instance%20segmentation%20neural%20network%2C%20and%20Line%0ASegment%20Transformer%20%28LETR%29%2C%20a%20vision%20transformer.%20The%20proposed%20Computer%20Vision%0A%28CV%29%20pipeline%20is%20applied%20on%20two%20image%20datasets%2C%20Summer%202015-Ames%20ULA%20and%20Summer%0A2015-%20Ames%20MLA%2C%20with%20a%20combined%20total%20of%201%2C827%20plant%20images%20collected%20in%20the%0Afield%20using%20FieldBook%2C%20an%20Android%20application%20aimed%20at%20on-site%20phenotyping.%20The%0Aleaf%20angles%20estimated%20by%20the%20proposed%20pipeline%20on%20the%20image%20datasets%20are%0Acompared%20to%20two%20independent%20manual%20measurements%20using%20ImageJ%2C%20a%20Java-based%0Aimage%20processing%20program%20developed%20at%20the%20National%20Institutes%20of%20Health%20and%20the%0ALaboratory%20for%20Optical%20and%20Computational%20Instrumentation.%20The%20results%2C%20when%0Acompared%20for%20similarity%20using%20the%20Cosine%20Similarity%20measure%2C%20exhibit%200.98%0Asimilarity%20scores%20on%20both%20independent%20measurements%20of%20Summer%202015-Ames%20ULA%20and%0ASummer%202015-Ames%20MLA%20image%20datasets%2C%20demonstrating%20the%20feasibility%20of%20the%0Aproposed%20pipeline%20for%20on-site%20measurement%20of%20leaf%20angles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00749v1&entry.124074799=Read"},
{"title": "Augmenting Channel Simulator and Semi- Supervised Learning for Efficient\n  Indoor Positioning", "author": "Yupeng Li and Xinyu Ning and Shijian Gao and Yitong Liu and Zhi Sun and Qixing Wang and Jiangzhou Wang", "abstract": "  This work aims to tackle the labor-intensive and resource-consuming task of\nindoor positioning by proposing an efficient approach. The proposed approach\ninvolves the introduction of a semi-supervised learning (SSL) with a biased\nteacher (SSLB) algorithm, which effectively utilizes both labeled and unlabeled\nchannel data. To reduce measurement expenses, unlabeled data is generated using\nan updated channel simulator (UCHS), and then weighted by adaptive confidence\nvalues to simplify the tuning of hyperparameters. Simulation results\ndemonstrate that the proposed strategy achieves superior performance while\nminimizing measurement overhead and training expense compared to existing\nbenchmarks, offering a valuable and practical solution for indoor positioning.\n", "link": "http://arxiv.org/abs/2408.00429v1", "date": "2024-08-01", "relevancy": 2.0051, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5156}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5052}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmenting%20Channel%20Simulator%20and%20Semi-%20Supervised%20Learning%20for%20Efficient%0A%20%20Indoor%20Positioning&body=Title%3A%20Augmenting%20Channel%20Simulator%20and%20Semi-%20Supervised%20Learning%20for%20Efficient%0A%20%20Indoor%20Positioning%0AAuthor%3A%20Yupeng%20Li%20and%20Xinyu%20Ning%20and%20Shijian%20Gao%20and%20Yitong%20Liu%20and%20Zhi%20Sun%20and%20Qixing%20Wang%20and%20Jiangzhou%20Wang%0AAbstract%3A%20%20%20This%20work%20aims%20to%20tackle%20the%20labor-intensive%20and%20resource-consuming%20task%20of%0Aindoor%20positioning%20by%20proposing%20an%20efficient%20approach.%20The%20proposed%20approach%0Ainvolves%20the%20introduction%20of%20a%20semi-supervised%20learning%20%28SSL%29%20with%20a%20biased%0Ateacher%20%28SSLB%29%20algorithm%2C%20which%20effectively%20utilizes%20both%20labeled%20and%20unlabeled%0Achannel%20data.%20To%20reduce%20measurement%20expenses%2C%20unlabeled%20data%20is%20generated%20using%0Aan%20updated%20channel%20simulator%20%28UCHS%29%2C%20and%20then%20weighted%20by%20adaptive%20confidence%0Avalues%20to%20simplify%20the%20tuning%20of%20hyperparameters.%20Simulation%20results%0Ademonstrate%20that%20the%20proposed%20strategy%20achieves%20superior%20performance%20while%0Aminimizing%20measurement%20overhead%20and%20training%20expense%20compared%20to%20existing%0Abenchmarks%2C%20offering%20a%20valuable%20and%20practical%20solution%20for%20indoor%20positioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmenting%2520Channel%2520Simulator%2520and%2520Semi-%2520Supervised%2520Learning%2520for%2520Efficient%250A%2520%2520Indoor%2520Positioning%26entry.906535625%3DYupeng%2520Li%2520and%2520Xinyu%2520Ning%2520and%2520Shijian%2520Gao%2520and%2520Yitong%2520Liu%2520and%2520Zhi%2520Sun%2520and%2520Qixing%2520Wang%2520and%2520Jiangzhou%2520Wang%26entry.1292438233%3D%2520%2520This%2520work%2520aims%2520to%2520tackle%2520the%2520labor-intensive%2520and%2520resource-consuming%2520task%2520of%250Aindoor%2520positioning%2520by%2520proposing%2520an%2520efficient%2520approach.%2520The%2520proposed%2520approach%250Ainvolves%2520the%2520introduction%2520of%2520a%2520semi-supervised%2520learning%2520%2528SSL%2529%2520with%2520a%2520biased%250Ateacher%2520%2528SSLB%2529%2520algorithm%252C%2520which%2520effectively%2520utilizes%2520both%2520labeled%2520and%2520unlabeled%250Achannel%2520data.%2520To%2520reduce%2520measurement%2520expenses%252C%2520unlabeled%2520data%2520is%2520generated%2520using%250Aan%2520updated%2520channel%2520simulator%2520%2528UCHS%2529%252C%2520and%2520then%2520weighted%2520by%2520adaptive%2520confidence%250Avalues%2520to%2520simplify%2520the%2520tuning%2520of%2520hyperparameters.%2520Simulation%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520strategy%2520achieves%2520superior%2520performance%2520while%250Aminimizing%2520measurement%2520overhead%2520and%2520training%2520expense%2520compared%2520to%2520existing%250Abenchmarks%252C%2520offering%2520a%2520valuable%2520and%2520practical%2520solution%2520for%2520indoor%2520positioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmenting%20Channel%20Simulator%20and%20Semi-%20Supervised%20Learning%20for%20Efficient%0A%20%20Indoor%20Positioning&entry.906535625=Yupeng%20Li%20and%20Xinyu%20Ning%20and%20Shijian%20Gao%20and%20Yitong%20Liu%20and%20Zhi%20Sun%20and%20Qixing%20Wang%20and%20Jiangzhou%20Wang&entry.1292438233=%20%20This%20work%20aims%20to%20tackle%20the%20labor-intensive%20and%20resource-consuming%20task%20of%0Aindoor%20positioning%20by%20proposing%20an%20efficient%20approach.%20The%20proposed%20approach%0Ainvolves%20the%20introduction%20of%20a%20semi-supervised%20learning%20%28SSL%29%20with%20a%20biased%0Ateacher%20%28SSLB%29%20algorithm%2C%20which%20effectively%20utilizes%20both%20labeled%20and%20unlabeled%0Achannel%20data.%20To%20reduce%20measurement%20expenses%2C%20unlabeled%20data%20is%20generated%20using%0Aan%20updated%20channel%20simulator%20%28UCHS%29%2C%20and%20then%20weighted%20by%20adaptive%20confidence%0Avalues%20to%20simplify%20the%20tuning%20of%20hyperparameters.%20Simulation%20results%0Ademonstrate%20that%20the%20proposed%20strategy%20achieves%20superior%20performance%20while%0Aminimizing%20measurement%20overhead%20and%20training%20expense%20compared%20to%20existing%0Abenchmarks%2C%20offering%20a%20valuable%20and%20practical%20solution%20for%20indoor%20positioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00429v1&entry.124074799=Read"},
{"title": "Joint Neural Networks for One-shot Object Recognition and Detection", "author": "Camilo J. Vargas and Qianni Zhang and Ebroul Izquierdo", "abstract": "  This paper presents a novel joint neural networks approach to address the\nchallenging one-shot object recognition and detection tasks. Inspired by\nSiamese neural networks and state-of-art multi-box detection approaches, the\njoint neural networks are able to perform object recognition and detection for\ncategories that remain unseen during the training process. Following the\none-shot object recognition/detection constraints, the training and testing\ndatasets do not contain overlapped classes, in other words, all the test\nclasses remain unseen during training. The joint networks architecture is able\nto effectively compare pairs of images via stacked convolutional layers of the\nquery and target inputs, recognising patterns of the same input query category\nwithout relying on previous training around this category. The proposed\napproach achieves 61.41% accuracy for one-shot object recognition on the\nMiniImageNet dataset and 47.1% mAP for one-shot object detection when trained\non the COCO dataset and tested using the Pascal VOC dataset. Code available at\nhttps://github.com/cjvargasc/JNN recog and https://github.com/cjvargasc/JNN\ndetection/\n", "link": "http://arxiv.org/abs/2408.00701v1", "date": "2024-08-01", "relevancy": 2.0005, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5104}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4986}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Neural%20Networks%20for%20One-shot%20Object%20Recognition%20and%20Detection&body=Title%3A%20Joint%20Neural%20Networks%20for%20One-shot%20Object%20Recognition%20and%20Detection%0AAuthor%3A%20Camilo%20J.%20Vargas%20and%20Qianni%20Zhang%20and%20Ebroul%20Izquierdo%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20joint%20neural%20networks%20approach%20to%20address%20the%0Achallenging%20one-shot%20object%20recognition%20and%20detection%20tasks.%20Inspired%20by%0ASiamese%20neural%20networks%20and%20state-of-art%20multi-box%20detection%20approaches%2C%20the%0Ajoint%20neural%20networks%20are%20able%20to%20perform%20object%20recognition%20and%20detection%20for%0Acategories%20that%20remain%20unseen%20during%20the%20training%20process.%20Following%20the%0Aone-shot%20object%20recognition/detection%20constraints%2C%20the%20training%20and%20testing%0Adatasets%20do%20not%20contain%20overlapped%20classes%2C%20in%20other%20words%2C%20all%20the%20test%0Aclasses%20remain%20unseen%20during%20training.%20The%20joint%20networks%20architecture%20is%20able%0Ato%20effectively%20compare%20pairs%20of%20images%20via%20stacked%20convolutional%20layers%20of%20the%0Aquery%20and%20target%20inputs%2C%20recognising%20patterns%20of%20the%20same%20input%20query%20category%0Awithout%20relying%20on%20previous%20training%20around%20this%20category.%20The%20proposed%0Aapproach%20achieves%2061.41%25%20accuracy%20for%20one-shot%20object%20recognition%20on%20the%0AMiniImageNet%20dataset%20and%2047.1%25%20mAP%20for%20one-shot%20object%20detection%20when%20trained%0Aon%20the%20COCO%20dataset%20and%20tested%20using%20the%20Pascal%20VOC%20dataset.%20Code%20available%20at%0Ahttps%3A//github.com/cjvargasc/JNN%20recog%20and%20https%3A//github.com/cjvargasc/JNN%0Adetection/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Neural%2520Networks%2520for%2520One-shot%2520Object%2520Recognition%2520and%2520Detection%26entry.906535625%3DCamilo%2520J.%2520Vargas%2520and%2520Qianni%2520Zhang%2520and%2520Ebroul%2520Izquierdo%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520joint%2520neural%2520networks%2520approach%2520to%2520address%2520the%250Achallenging%2520one-shot%2520object%2520recognition%2520and%2520detection%2520tasks.%2520Inspired%2520by%250ASiamese%2520neural%2520networks%2520and%2520state-of-art%2520multi-box%2520detection%2520approaches%252C%2520the%250Ajoint%2520neural%2520networks%2520are%2520able%2520to%2520perform%2520object%2520recognition%2520and%2520detection%2520for%250Acategories%2520that%2520remain%2520unseen%2520during%2520the%2520training%2520process.%2520Following%2520the%250Aone-shot%2520object%2520recognition/detection%2520constraints%252C%2520the%2520training%2520and%2520testing%250Adatasets%2520do%2520not%2520contain%2520overlapped%2520classes%252C%2520in%2520other%2520words%252C%2520all%2520the%2520test%250Aclasses%2520remain%2520unseen%2520during%2520training.%2520The%2520joint%2520networks%2520architecture%2520is%2520able%250Ato%2520effectively%2520compare%2520pairs%2520of%2520images%2520via%2520stacked%2520convolutional%2520layers%2520of%2520the%250Aquery%2520and%2520target%2520inputs%252C%2520recognising%2520patterns%2520of%2520the%2520same%2520input%2520query%2520category%250Awithout%2520relying%2520on%2520previous%2520training%2520around%2520this%2520category.%2520The%2520proposed%250Aapproach%2520achieves%252061.41%2525%2520accuracy%2520for%2520one-shot%2520object%2520recognition%2520on%2520the%250AMiniImageNet%2520dataset%2520and%252047.1%2525%2520mAP%2520for%2520one-shot%2520object%2520detection%2520when%2520trained%250Aon%2520the%2520COCO%2520dataset%2520and%2520tested%2520using%2520the%2520Pascal%2520VOC%2520dataset.%2520Code%2520available%2520at%250Ahttps%253A//github.com/cjvargasc/JNN%2520recog%2520and%2520https%253A//github.com/cjvargasc/JNN%250Adetection/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Neural%20Networks%20for%20One-shot%20Object%20Recognition%20and%20Detection&entry.906535625=Camilo%20J.%20Vargas%20and%20Qianni%20Zhang%20and%20Ebroul%20Izquierdo&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20joint%20neural%20networks%20approach%20to%20address%20the%0Achallenging%20one-shot%20object%20recognition%20and%20detection%20tasks.%20Inspired%20by%0ASiamese%20neural%20networks%20and%20state-of-art%20multi-box%20detection%20approaches%2C%20the%0Ajoint%20neural%20networks%20are%20able%20to%20perform%20object%20recognition%20and%20detection%20for%0Acategories%20that%20remain%20unseen%20during%20the%20training%20process.%20Following%20the%0Aone-shot%20object%20recognition/detection%20constraints%2C%20the%20training%20and%20testing%0Adatasets%20do%20not%20contain%20overlapped%20classes%2C%20in%20other%20words%2C%20all%20the%20test%0Aclasses%20remain%20unseen%20during%20training.%20The%20joint%20networks%20architecture%20is%20able%0Ato%20effectively%20compare%20pairs%20of%20images%20via%20stacked%20convolutional%20layers%20of%20the%0Aquery%20and%20target%20inputs%2C%20recognising%20patterns%20of%20the%20same%20input%20query%20category%0Awithout%20relying%20on%20previous%20training%20around%20this%20category.%20The%20proposed%0Aapproach%20achieves%2061.41%25%20accuracy%20for%20one-shot%20object%20recognition%20on%20the%0AMiniImageNet%20dataset%20and%2047.1%25%20mAP%20for%20one-shot%20object%20detection%20when%20trained%0Aon%20the%20COCO%20dataset%20and%20tested%20using%20the%20Pascal%20VOC%20dataset.%20Code%20available%20at%0Ahttps%3A//github.com/cjvargasc/JNN%20recog%20and%20https%3A//github.com/cjvargasc/JNN%0Adetection/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00701v1&entry.124074799=Read"},
{"title": "Alleviating Hallucination in Large Vision-Language Models with Active\n  Retrieval Augmentation", "author": "Xiaoye Qu and Qiyuan Chen and Wei Wei and Jishuo Sun and Jianfeng Dong", "abstract": "  Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.\n", "link": "http://arxiv.org/abs/2408.00555v1", "date": "2024-08-01", "relevancy": 1.9958, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5169}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alleviating%20Hallucination%20in%20Large%20Vision-Language%20Models%20with%20Active%0A%20%20Retrieval%20Augmentation&body=Title%3A%20Alleviating%20Hallucination%20in%20Large%20Vision-Language%20Models%20with%20Active%0A%20%20Retrieval%20Augmentation%0AAuthor%3A%20Xiaoye%20Qu%20and%20Qiyuan%20Chen%20and%20Wei%20Wei%20and%20Jishuo%20Sun%20and%20Jianfeng%20Dong%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20ability%20of%20large%20vision-language%20models%20%28LVLMs%29%20in%0Aimage%20comprehension%2C%20these%20models%20frequently%20generate%20plausible%20yet%20factually%0Aincorrect%20responses%2C%20a%20phenomenon%20known%20as%20hallucination.Recently%2C%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20augmenting%20LLMs%20by%20retrieving%20information%20from%20external%0Aknowledge%20resources%20has%20been%20proven%20as%20a%20promising%20solution%20to%20mitigate%0Ahallucinations.However%2C%20the%20retrieval%20augmentation%20in%20LVLM%20significantly%20lags%0Abehind%20the%20widespread%20applications%20of%20LVLM.%20Moreover%2C%20when%20transferred%20to%0Aaugmenting%20LVLMs%2C%20sometimes%20the%20hallucination%20degree%20of%20the%20model%20is%20even%0Aexacerbated.Motivated%20by%20the%20research%20gap%20and%20counter-intuitive%20phenomenon%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20the%20Active%20Retrieval-Augmented%20large%0Avision-language%20model%20%28ARA%29%2C%20specifically%20designed%20to%20address%20hallucinations%20by%0Aincorporating%20three%20critical%20dimensions%3A%20%28i%29%20dissecting%20the%20retrieval%20targets%0Abased%20on%20the%20inherent%20hierarchical%20structures%20of%20images.%20%28ii%29%20pinpointing%20the%0Amost%20effective%20retrieval%20methods%20and%20filtering%20out%20the%20reliable%20retrieval%0Aresults.%20%28iii%29%20timing%20the%20retrieval%20process%20to%20coincide%20with%20episodes%20of%20low%0Acertainty%2C%20while%20circumventing%20unnecessary%20retrieval%20during%20periods%20of%20high%0Acertainty.%20To%20assess%20the%20capability%20of%20our%20proposed%20ARA%20model%20in%20reducing%0Ahallucination%2C%20we%20employ%20three%20widely%20used%20LVLM%20models%20%28LLaVA-1.5%2C%20Qwen-VL%2C%20and%0AmPLUG-Owl2%29%20across%20four%20benchmarks.%20Our%20empirical%20observations%20suggest%20that%20by%0Autilizing%20fitting%20retrieval%20mechanisms%20and%20timing%20the%20retrieval%20judiciously%2C%20we%0Acan%20effectively%20mitigate%20the%20hallucination%20problem.%20We%20hope%20that%20this%20study%20can%0Aprovide%20deeper%20insights%20into%20how%20to%20adapt%20the%20retrieval%20augmentation%20to%20LVLMs%0Afor%20reducing%20hallucinations%20with%20more%20effective%20retrieval%20and%20minimal%20retrieval%0Aoccurrences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlleviating%2520Hallucination%2520in%2520Large%2520Vision-Language%2520Models%2520with%2520Active%250A%2520%2520Retrieval%2520Augmentation%26entry.906535625%3DXiaoye%2520Qu%2520and%2520Qiyuan%2520Chen%2520and%2520Wei%2520Wei%2520and%2520Jishuo%2520Sun%2520and%2520Jianfeng%2520Dong%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520ability%2520of%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520in%250Aimage%2520comprehension%252C%2520these%2520models%2520frequently%2520generate%2520plausible%2520yet%2520factually%250Aincorrect%2520responses%252C%2520a%2520phenomenon%2520known%2520as%2520hallucination.Recently%252C%2520in%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520augmenting%2520LLMs%2520by%2520retrieving%2520information%2520from%2520external%250Aknowledge%2520resources%2520has%2520been%2520proven%2520as%2520a%2520promising%2520solution%2520to%2520mitigate%250Ahallucinations.However%252C%2520the%2520retrieval%2520augmentation%2520in%2520LVLM%2520significantly%2520lags%250Abehind%2520the%2520widespread%2520applications%2520of%2520LVLM.%2520Moreover%252C%2520when%2520transferred%2520to%250Aaugmenting%2520LVLMs%252C%2520sometimes%2520the%2520hallucination%2520degree%2520of%2520the%2520model%2520is%2520even%250Aexacerbated.Motivated%2520by%2520the%2520research%2520gap%2520and%2520counter-intuitive%2520phenomenon%252C%2520we%250Aintroduce%2520a%2520novel%2520framework%252C%2520the%2520Active%2520Retrieval-Augmented%2520large%250Avision-language%2520model%2520%2528ARA%2529%252C%2520specifically%2520designed%2520to%2520address%2520hallucinations%2520by%250Aincorporating%2520three%2520critical%2520dimensions%253A%2520%2528i%2529%2520dissecting%2520the%2520retrieval%2520targets%250Abased%2520on%2520the%2520inherent%2520hierarchical%2520structures%2520of%2520images.%2520%2528ii%2529%2520pinpointing%2520the%250Amost%2520effective%2520retrieval%2520methods%2520and%2520filtering%2520out%2520the%2520reliable%2520retrieval%250Aresults.%2520%2528iii%2529%2520timing%2520the%2520retrieval%2520process%2520to%2520coincide%2520with%2520episodes%2520of%2520low%250Acertainty%252C%2520while%2520circumventing%2520unnecessary%2520retrieval%2520during%2520periods%2520of%2520high%250Acertainty.%2520To%2520assess%2520the%2520capability%2520of%2520our%2520proposed%2520ARA%2520model%2520in%2520reducing%250Ahallucination%252C%2520we%2520employ%2520three%2520widely%2520used%2520LVLM%2520models%2520%2528LLaVA-1.5%252C%2520Qwen-VL%252C%2520and%250AmPLUG-Owl2%2529%2520across%2520four%2520benchmarks.%2520Our%2520empirical%2520observations%2520suggest%2520that%2520by%250Autilizing%2520fitting%2520retrieval%2520mechanisms%2520and%2520timing%2520the%2520retrieval%2520judiciously%252C%2520we%250Acan%2520effectively%2520mitigate%2520the%2520hallucination%2520problem.%2520We%2520hope%2520that%2520this%2520study%2520can%250Aprovide%2520deeper%2520insights%2520into%2520how%2520to%2520adapt%2520the%2520retrieval%2520augmentation%2520to%2520LVLMs%250Afor%2520reducing%2520hallucinations%2520with%2520more%2520effective%2520retrieval%2520and%2520minimal%2520retrieval%250Aoccurrences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alleviating%20Hallucination%20in%20Large%20Vision-Language%20Models%20with%20Active%0A%20%20Retrieval%20Augmentation&entry.906535625=Xiaoye%20Qu%20and%20Qiyuan%20Chen%20and%20Wei%20Wei%20and%20Jishuo%20Sun%20and%20Jianfeng%20Dong&entry.1292438233=%20%20Despite%20the%20remarkable%20ability%20of%20large%20vision-language%20models%20%28LVLMs%29%20in%0Aimage%20comprehension%2C%20these%20models%20frequently%20generate%20plausible%20yet%20factually%0Aincorrect%20responses%2C%20a%20phenomenon%20known%20as%20hallucination.Recently%2C%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20augmenting%20LLMs%20by%20retrieving%20information%20from%20external%0Aknowledge%20resources%20has%20been%20proven%20as%20a%20promising%20solution%20to%20mitigate%0Ahallucinations.However%2C%20the%20retrieval%20augmentation%20in%20LVLM%20significantly%20lags%0Abehind%20the%20widespread%20applications%20of%20LVLM.%20Moreover%2C%20when%20transferred%20to%0Aaugmenting%20LVLMs%2C%20sometimes%20the%20hallucination%20degree%20of%20the%20model%20is%20even%0Aexacerbated.Motivated%20by%20the%20research%20gap%20and%20counter-intuitive%20phenomenon%2C%20we%0Aintroduce%20a%20novel%20framework%2C%20the%20Active%20Retrieval-Augmented%20large%0Avision-language%20model%20%28ARA%29%2C%20specifically%20designed%20to%20address%20hallucinations%20by%0Aincorporating%20three%20critical%20dimensions%3A%20%28i%29%20dissecting%20the%20retrieval%20targets%0Abased%20on%20the%20inherent%20hierarchical%20structures%20of%20images.%20%28ii%29%20pinpointing%20the%0Amost%20effective%20retrieval%20methods%20and%20filtering%20out%20the%20reliable%20retrieval%0Aresults.%20%28iii%29%20timing%20the%20retrieval%20process%20to%20coincide%20with%20episodes%20of%20low%0Acertainty%2C%20while%20circumventing%20unnecessary%20retrieval%20during%20periods%20of%20high%0Acertainty.%20To%20assess%20the%20capability%20of%20our%20proposed%20ARA%20model%20in%20reducing%0Ahallucination%2C%20we%20employ%20three%20widely%20used%20LVLM%20models%20%28LLaVA-1.5%2C%20Qwen-VL%2C%20and%0AmPLUG-Owl2%29%20across%20four%20benchmarks.%20Our%20empirical%20observations%20suggest%20that%20by%0Autilizing%20fitting%20retrieval%20mechanisms%20and%20timing%20the%20retrieval%20judiciously%2C%20we%0Acan%20effectively%20mitigate%20the%20hallucination%20problem.%20We%20hope%20that%20this%20study%20can%0Aprovide%20deeper%20insights%20into%20how%20to%20adapt%20the%20retrieval%20augmentation%20to%20LVLMs%0Afor%20reducing%20hallucinations%20with%20more%20effective%20retrieval%20and%20minimal%20retrieval%0Aoccurrences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00555v1&entry.124074799=Read"},
{"title": "Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation", "author": "Chu Zhao and Enneng Yang and Yuliang Liang and Pengxiang Lan and Yuting Liu and Jianzhe Zhao and Guibing Guo and Xingwei Wang", "abstract": "  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.\n", "link": "http://arxiv.org/abs/2408.00490v1", "date": "2024-08-01", "relevancy": 1.9885, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5091}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5008}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Representation%20Learning%20via%20Causal%20Diffusion%20for%0A%20%20Out-of-Distribution%20Recommendation&body=Title%3A%20Graph%20Representation%20Learning%20via%20Causal%20Diffusion%20for%0A%20%20Out-of-Distribution%20Recommendation%0AAuthor%3A%20Chu%20Zhao%20and%20Enneng%20Yang%20and%20Yuliang%20Liang%20and%20Pengxiang%20Lan%20and%20Yuting%20Liu%20and%20Jianzhe%20Zhao%20and%20Guibing%20Guo%20and%20Xingwei%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29-based%20recommendation%20algorithms%20typically%20assume%0Athat%20training%20and%20testing%20data%20are%20drawn%20from%20independent%20and%20identically%0Adistributed%20%28IID%29%20spaces.%20However%2C%20this%20assumption%20often%20fails%20in%20the%20presence%0Aof%20out-of-distribution%20%28OOD%29%20data%2C%20resulting%20in%20significant%20performance%0Adegradation.%20In%20this%20study%2C%20we%20construct%20a%20Structural%20Causal%20Model%20%28SCM%29%20to%0Aanalyze%20interaction%20data%2C%20revealing%20that%20environmental%20confounders%20%28e.g.%2C%20the%0ACOVID-19%20pandemic%29%20lead%20to%20unstable%20correlations%20in%20GNN-based%20models%2C%20thus%0Aimpairing%20their%20generalization%20to%20OOD%20data.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20approach%2C%20graph%20representation%20learning%20via%20causal%20diffusion%0A%28CausalDiffRec%29%20for%20OOD%20recommendation.%20This%20method%20enhances%20the%20model%27s%0Ageneralization%20on%20OOD%20data%20by%20eliminating%20environmental%20confounding%20factors%20and%0Alearning%20invariant%20graph%20representations.%20Specifically%2C%20we%20use%20backdoor%0Aadjustment%20and%20variational%20inference%20to%20infer%20the%20real%20environmental%0Adistribution%2C%20thereby%20eliminating%20the%20impact%20of%20environmental%20confounders.%20This%0Ainferred%20distribution%20is%20then%20used%20as%20prior%20knowledge%20to%20guide%20the%0Arepresentation%20learning%20in%20the%20reverse%20phase%20of%20the%20diffusion%20process%20to%20learn%0Athe%20invariant%20representation.%20In%20addition%2C%20we%20provide%20a%20theoretical%20derivation%0Athat%20proves%20optimizing%20the%20objective%20function%20of%20CausalDiffRec%20can%20encourage%0Athe%20model%20to%20learn%20environment-invariant%20graph%20representations%2C%20thereby%0Aachieving%20excellent%20generalization%20performance%20in%20recommendations%20under%0Adistribution%20shifts.%20Our%20extensive%20experiments%20validate%20the%20effectiveness%20of%0ACausalDiffRec%20in%20improving%20the%20generalization%20of%20OOD%20data%2C%20and%20the%20average%0Aimprovement%20is%20up%20to%2010.69%25%20on%20Food%2C%2018.83%25%20on%20KuaiRec%2C%2022.41%25%20on%20Yelp2018%2C%20and%0A11.65%25%20on%20Douban%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Representation%2520Learning%2520via%2520Causal%2520Diffusion%2520for%250A%2520%2520Out-of-Distribution%2520Recommendation%26entry.906535625%3DChu%2520Zhao%2520and%2520Enneng%2520Yang%2520and%2520Yuliang%2520Liang%2520and%2520Pengxiang%2520Lan%2520and%2520Yuting%2520Liu%2520and%2520Jianzhe%2520Zhao%2520and%2520Guibing%2520Guo%2520and%2520Xingwei%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529-based%2520recommendation%2520algorithms%2520typically%2520assume%250Athat%2520training%2520and%2520testing%2520data%2520are%2520drawn%2520from%2520independent%2520and%2520identically%250Adistributed%2520%2528IID%2529%2520spaces.%2520However%252C%2520this%2520assumption%2520often%2520fails%2520in%2520the%2520presence%250Aof%2520out-of-distribution%2520%2528OOD%2529%2520data%252C%2520resulting%2520in%2520significant%2520performance%250Adegradation.%2520In%2520this%2520study%252C%2520we%2520construct%2520a%2520Structural%2520Causal%2520Model%2520%2528SCM%2529%2520to%250Aanalyze%2520interaction%2520data%252C%2520revealing%2520that%2520environmental%2520confounders%2520%2528e.g.%252C%2520the%250ACOVID-19%2520pandemic%2529%2520lead%2520to%2520unstable%2520correlations%2520in%2520GNN-based%2520models%252C%2520thus%250Aimpairing%2520their%2520generalization%2520to%2520OOD%2520data.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Anovel%2520approach%252C%2520graph%2520representation%2520learning%2520via%2520causal%2520diffusion%250A%2528CausalDiffRec%2529%2520for%2520OOD%2520recommendation.%2520This%2520method%2520enhances%2520the%2520model%2527s%250Ageneralization%2520on%2520OOD%2520data%2520by%2520eliminating%2520environmental%2520confounding%2520factors%2520and%250Alearning%2520invariant%2520graph%2520representations.%2520Specifically%252C%2520we%2520use%2520backdoor%250Aadjustment%2520and%2520variational%2520inference%2520to%2520infer%2520the%2520real%2520environmental%250Adistribution%252C%2520thereby%2520eliminating%2520the%2520impact%2520of%2520environmental%2520confounders.%2520This%250Ainferred%2520distribution%2520is%2520then%2520used%2520as%2520prior%2520knowledge%2520to%2520guide%2520the%250Arepresentation%2520learning%2520in%2520the%2520reverse%2520phase%2520of%2520the%2520diffusion%2520process%2520to%2520learn%250Athe%2520invariant%2520representation.%2520In%2520addition%252C%2520we%2520provide%2520a%2520theoretical%2520derivation%250Athat%2520proves%2520optimizing%2520the%2520objective%2520function%2520of%2520CausalDiffRec%2520can%2520encourage%250Athe%2520model%2520to%2520learn%2520environment-invariant%2520graph%2520representations%252C%2520thereby%250Aachieving%2520excellent%2520generalization%2520performance%2520in%2520recommendations%2520under%250Adistribution%2520shifts.%2520Our%2520extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%250ACausalDiffRec%2520in%2520improving%2520the%2520generalization%2520of%2520OOD%2520data%252C%2520and%2520the%2520average%250Aimprovement%2520is%2520up%2520to%252010.69%2525%2520on%2520Food%252C%252018.83%2525%2520on%2520KuaiRec%252C%252022.41%2525%2520on%2520Yelp2018%252C%2520and%250A11.65%2525%2520on%2520Douban%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Representation%20Learning%20via%20Causal%20Diffusion%20for%0A%20%20Out-of-Distribution%20Recommendation&entry.906535625=Chu%20Zhao%20and%20Enneng%20Yang%20and%20Yuliang%20Liang%20and%20Pengxiang%20Lan%20and%20Yuting%20Liu%20and%20Jianzhe%20Zhao%20and%20Guibing%20Guo%20and%20Xingwei%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29-based%20recommendation%20algorithms%20typically%20assume%0Athat%20training%20and%20testing%20data%20are%20drawn%20from%20independent%20and%20identically%0Adistributed%20%28IID%29%20spaces.%20However%2C%20this%20assumption%20often%20fails%20in%20the%20presence%0Aof%20out-of-distribution%20%28OOD%29%20data%2C%20resulting%20in%20significant%20performance%0Adegradation.%20In%20this%20study%2C%20we%20construct%20a%20Structural%20Causal%20Model%20%28SCM%29%20to%0Aanalyze%20interaction%20data%2C%20revealing%20that%20environmental%20confounders%20%28e.g.%2C%20the%0ACOVID-19%20pandemic%29%20lead%20to%20unstable%20correlations%20in%20GNN-based%20models%2C%20thus%0Aimpairing%20their%20generalization%20to%20OOD%20data.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20approach%2C%20graph%20representation%20learning%20via%20causal%20diffusion%0A%28CausalDiffRec%29%20for%20OOD%20recommendation.%20This%20method%20enhances%20the%20model%27s%0Ageneralization%20on%20OOD%20data%20by%20eliminating%20environmental%20confounding%20factors%20and%0Alearning%20invariant%20graph%20representations.%20Specifically%2C%20we%20use%20backdoor%0Aadjustment%20and%20variational%20inference%20to%20infer%20the%20real%20environmental%0Adistribution%2C%20thereby%20eliminating%20the%20impact%20of%20environmental%20confounders.%20This%0Ainferred%20distribution%20is%20then%20used%20as%20prior%20knowledge%20to%20guide%20the%0Arepresentation%20learning%20in%20the%20reverse%20phase%20of%20the%20diffusion%20process%20to%20learn%0Athe%20invariant%20representation.%20In%20addition%2C%20we%20provide%20a%20theoretical%20derivation%0Athat%20proves%20optimizing%20the%20objective%20function%20of%20CausalDiffRec%20can%20encourage%0Athe%20model%20to%20learn%20environment-invariant%20graph%20representations%2C%20thereby%0Aachieving%20excellent%20generalization%20performance%20in%20recommendations%20under%0Adistribution%20shifts.%20Our%20extensive%20experiments%20validate%20the%20effectiveness%20of%0ACausalDiffRec%20in%20improving%20the%20generalization%20of%20OOD%20data%2C%20and%20the%20average%0Aimprovement%20is%20up%20to%2010.69%25%20on%20Food%2C%2018.83%25%20on%20KuaiRec%2C%2022.41%25%20on%20Yelp2018%2C%20and%0A11.65%25%20on%20Douban%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00490v1&entry.124074799=Read"},
{"title": "Actor-Critic Physics-informed Neural Lyapunov Control", "author": "Jiarui Wang and Mahyar Fazlyab", "abstract": "  Designing control policies for stabilization tasks with provable guarantees\nis a long-standing problem in nonlinear control. A crucial performance metric\nis the size of the resulting region of attraction, which essentially serves as\na robustness \"margin\" of the closed-loop system against uncertainties. In this\npaper, we propose a new method to train a stabilizing neural network controller\nalong with its corresponding Lyapunov certificate, aiming to maximize the\nresulting region of attraction while respecting the actuation constraints.\nCrucial to our approach is the use of Zubov's Partial Differential Equation\n(PDE), which precisely characterizes the true region of attraction of a given\ncontrol policy. Our framework follows an actor-critic pattern where we\nalternate between improving the control policy (actor) and learning a Zubov\nfunction (critic). Finally, we compute the largest certifiable region of\nattraction by invoking an SMT solver after the training procedure. Our\nnumerical experiments on several design problems show consistent and\nsignificant improvements in the size of the resulting region of attraction.\n", "link": "http://arxiv.org/abs/2403.08448v2", "date": "2024-08-01", "relevancy": 1.9743, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5102}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Actor-Critic%20Physics-informed%20Neural%20Lyapunov%20Control&body=Title%3A%20Actor-Critic%20Physics-informed%20Neural%20Lyapunov%20Control%0AAuthor%3A%20Jiarui%20Wang%20and%20Mahyar%20Fazlyab%0AAbstract%3A%20%20%20Designing%20control%20policies%20for%20stabilization%20tasks%20with%20provable%20guarantees%0Ais%20a%20long-standing%20problem%20in%20nonlinear%20control.%20A%20crucial%20performance%20metric%0Ais%20the%20size%20of%20the%20resulting%20region%20of%20attraction%2C%20which%20essentially%20serves%20as%0Aa%20robustness%20%22margin%22%20of%20the%20closed-loop%20system%20against%20uncertainties.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20method%20to%20train%20a%20stabilizing%20neural%20network%20controller%0Aalong%20with%20its%20corresponding%20Lyapunov%20certificate%2C%20aiming%20to%20maximize%20the%0Aresulting%20region%20of%20attraction%20while%20respecting%20the%20actuation%20constraints.%0ACrucial%20to%20our%20approach%20is%20the%20use%20of%20Zubov%27s%20Partial%20Differential%20Equation%0A%28PDE%29%2C%20which%20precisely%20characterizes%20the%20true%20region%20of%20attraction%20of%20a%20given%0Acontrol%20policy.%20Our%20framework%20follows%20an%20actor-critic%20pattern%20where%20we%0Aalternate%20between%20improving%20the%20control%20policy%20%28actor%29%20and%20learning%20a%20Zubov%0Afunction%20%28critic%29.%20Finally%2C%20we%20compute%20the%20largest%20certifiable%20region%20of%0Aattraction%20by%20invoking%20an%20SMT%20solver%20after%20the%20training%20procedure.%20Our%0Anumerical%20experiments%20on%20several%20design%20problems%20show%20consistent%20and%0Asignificant%20improvements%20in%20the%20size%20of%20the%20resulting%20region%20of%20attraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08448v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActor-Critic%2520Physics-informed%2520Neural%2520Lyapunov%2520Control%26entry.906535625%3DJiarui%2520Wang%2520and%2520Mahyar%2520Fazlyab%26entry.1292438233%3D%2520%2520Designing%2520control%2520policies%2520for%2520stabilization%2520tasks%2520with%2520provable%2520guarantees%250Ais%2520a%2520long-standing%2520problem%2520in%2520nonlinear%2520control.%2520A%2520crucial%2520performance%2520metric%250Ais%2520the%2520size%2520of%2520the%2520resulting%2520region%2520of%2520attraction%252C%2520which%2520essentially%2520serves%2520as%250Aa%2520robustness%2520%2522margin%2522%2520of%2520the%2520closed-loop%2520system%2520against%2520uncertainties.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520new%2520method%2520to%2520train%2520a%2520stabilizing%2520neural%2520network%2520controller%250Aalong%2520with%2520its%2520corresponding%2520Lyapunov%2520certificate%252C%2520aiming%2520to%2520maximize%2520the%250Aresulting%2520region%2520of%2520attraction%2520while%2520respecting%2520the%2520actuation%2520constraints.%250ACrucial%2520to%2520our%2520approach%2520is%2520the%2520use%2520of%2520Zubov%2527s%2520Partial%2520Differential%2520Equation%250A%2528PDE%2529%252C%2520which%2520precisely%2520characterizes%2520the%2520true%2520region%2520of%2520attraction%2520of%2520a%2520given%250Acontrol%2520policy.%2520Our%2520framework%2520follows%2520an%2520actor-critic%2520pattern%2520where%2520we%250Aalternate%2520between%2520improving%2520the%2520control%2520policy%2520%2528actor%2529%2520and%2520learning%2520a%2520Zubov%250Afunction%2520%2528critic%2529.%2520Finally%252C%2520we%2520compute%2520the%2520largest%2520certifiable%2520region%2520of%250Aattraction%2520by%2520invoking%2520an%2520SMT%2520solver%2520after%2520the%2520training%2520procedure.%2520Our%250Anumerical%2520experiments%2520on%2520several%2520design%2520problems%2520show%2520consistent%2520and%250Asignificant%2520improvements%2520in%2520the%2520size%2520of%2520the%2520resulting%2520region%2520of%2520attraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08448v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Actor-Critic%20Physics-informed%20Neural%20Lyapunov%20Control&entry.906535625=Jiarui%20Wang%20and%20Mahyar%20Fazlyab&entry.1292438233=%20%20Designing%20control%20policies%20for%20stabilization%20tasks%20with%20provable%20guarantees%0Ais%20a%20long-standing%20problem%20in%20nonlinear%20control.%20A%20crucial%20performance%20metric%0Ais%20the%20size%20of%20the%20resulting%20region%20of%20attraction%2C%20which%20essentially%20serves%20as%0Aa%20robustness%20%22margin%22%20of%20the%20closed-loop%20system%20against%20uncertainties.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20method%20to%20train%20a%20stabilizing%20neural%20network%20controller%0Aalong%20with%20its%20corresponding%20Lyapunov%20certificate%2C%20aiming%20to%20maximize%20the%0Aresulting%20region%20of%20attraction%20while%20respecting%20the%20actuation%20constraints.%0ACrucial%20to%20our%20approach%20is%20the%20use%20of%20Zubov%27s%20Partial%20Differential%20Equation%0A%28PDE%29%2C%20which%20precisely%20characterizes%20the%20true%20region%20of%20attraction%20of%20a%20given%0Acontrol%20policy.%20Our%20framework%20follows%20an%20actor-critic%20pattern%20where%20we%0Aalternate%20between%20improving%20the%20control%20policy%20%28actor%29%20and%20learning%20a%20Zubov%0Afunction%20%28critic%29.%20Finally%2C%20we%20compute%20the%20largest%20certifiable%20region%20of%0Aattraction%20by%20invoking%20an%20SMT%20solver%20after%20the%20training%20procedure.%20Our%0Anumerical%20experiments%20on%20several%20design%20problems%20show%20consistent%20and%0Asignificant%20improvements%20in%20the%20size%20of%20the%20resulting%20region%20of%20attraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08448v2&entry.124074799=Read"},
{"title": "End-to-End Reinforcement Learning of Koopman Models for Economic\n  Nonlinear Model Predictive Control", "author": "Daniel Mayfrank and Alexander Mitsos and Manuel Dahmen", "abstract": "  (Economic) nonlinear model predictive control ((e)NMPC) requires dynamic\nmodels that are sufficiently accurate and computationally tractable.\nData-driven surrogate models for mechanistic models can reduce the\ncomputational burden of (e)NMPC; however, such models are typically trained by\nsystem identification for maximum prediction accuracy on simulation samples and\nperform suboptimally in (e)NMPC. We present a method for end-to-end\nreinforcement learning of Koopman surrogate models for optimal performance as\npart of (e)NMPC. We apply our method to two applications derived from an\nestablished nonlinear continuous stirred-tank reactor model. The controller\nperformance is compared to that of (e)NMPCs utilizing models trained using\nsystem identification, and model-free neural network controllers trained using\nreinforcement learning. We show that the end-to-end trained models outperform\nthose trained using system identification in (e)NMPC, and that, in contrast to\nthe neural network controllers, the (e)NMPC controllers can react to changes in\nthe control setting without retraining.\n", "link": "http://arxiv.org/abs/2308.01674v4", "date": "2024-08-01", "relevancy": 1.9428, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5126}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5125}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Reinforcement%20Learning%20of%20Koopman%20Models%20for%20Economic%0A%20%20Nonlinear%20Model%20Predictive%20Control&body=Title%3A%20End-to-End%20Reinforcement%20Learning%20of%20Koopman%20Models%20for%20Economic%0A%20%20Nonlinear%20Model%20Predictive%20Control%0AAuthor%3A%20Daniel%20Mayfrank%20and%20Alexander%20Mitsos%20and%20Manuel%20Dahmen%0AAbstract%3A%20%20%20%28Economic%29%20nonlinear%20model%20predictive%20control%20%28%28e%29NMPC%29%20requires%20dynamic%0Amodels%20that%20are%20sufficiently%20accurate%20and%20computationally%20tractable.%0AData-driven%20surrogate%20models%20for%20mechanistic%20models%20can%20reduce%20the%0Acomputational%20burden%20of%20%28e%29NMPC%3B%20however%2C%20such%20models%20are%20typically%20trained%20by%0Asystem%20identification%20for%20maximum%20prediction%20accuracy%20on%20simulation%20samples%20and%0Aperform%20suboptimally%20in%20%28e%29NMPC.%20We%20present%20a%20method%20for%20end-to-end%0Areinforcement%20learning%20of%20Koopman%20surrogate%20models%20for%20optimal%20performance%20as%0Apart%20of%20%28e%29NMPC.%20We%20apply%20our%20method%20to%20two%20applications%20derived%20from%20an%0Aestablished%20nonlinear%20continuous%20stirred-tank%20reactor%20model.%20The%20controller%0Aperformance%20is%20compared%20to%20that%20of%20%28e%29NMPCs%20utilizing%20models%20trained%20using%0Asystem%20identification%2C%20and%20model-free%20neural%20network%20controllers%20trained%20using%0Areinforcement%20learning.%20We%20show%20that%20the%20end-to-end%20trained%20models%20outperform%0Athose%20trained%20using%20system%20identification%20in%20%28e%29NMPC%2C%20and%20that%2C%20in%20contrast%20to%0Athe%20neural%20network%20controllers%2C%20the%20%28e%29NMPC%20controllers%20can%20react%20to%20changes%20in%0Athe%20control%20setting%20without%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.01674v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Reinforcement%2520Learning%2520of%2520Koopman%2520Models%2520for%2520Economic%250A%2520%2520Nonlinear%2520Model%2520Predictive%2520Control%26entry.906535625%3DDaniel%2520Mayfrank%2520and%2520Alexander%2520Mitsos%2520and%2520Manuel%2520Dahmen%26entry.1292438233%3D%2520%2520%2528Economic%2529%2520nonlinear%2520model%2520predictive%2520control%2520%2528%2528e%2529NMPC%2529%2520requires%2520dynamic%250Amodels%2520that%2520are%2520sufficiently%2520accurate%2520and%2520computationally%2520tractable.%250AData-driven%2520surrogate%2520models%2520for%2520mechanistic%2520models%2520can%2520reduce%2520the%250Acomputational%2520burden%2520of%2520%2528e%2529NMPC%253B%2520however%252C%2520such%2520models%2520are%2520typically%2520trained%2520by%250Asystem%2520identification%2520for%2520maximum%2520prediction%2520accuracy%2520on%2520simulation%2520samples%2520and%250Aperform%2520suboptimally%2520in%2520%2528e%2529NMPC.%2520We%2520present%2520a%2520method%2520for%2520end-to-end%250Areinforcement%2520learning%2520of%2520Koopman%2520surrogate%2520models%2520for%2520optimal%2520performance%2520as%250Apart%2520of%2520%2528e%2529NMPC.%2520We%2520apply%2520our%2520method%2520to%2520two%2520applications%2520derived%2520from%2520an%250Aestablished%2520nonlinear%2520continuous%2520stirred-tank%2520reactor%2520model.%2520The%2520controller%250Aperformance%2520is%2520compared%2520to%2520that%2520of%2520%2528e%2529NMPCs%2520utilizing%2520models%2520trained%2520using%250Asystem%2520identification%252C%2520and%2520model-free%2520neural%2520network%2520controllers%2520trained%2520using%250Areinforcement%2520learning.%2520We%2520show%2520that%2520the%2520end-to-end%2520trained%2520models%2520outperform%250Athose%2520trained%2520using%2520system%2520identification%2520in%2520%2528e%2529NMPC%252C%2520and%2520that%252C%2520in%2520contrast%2520to%250Athe%2520neural%2520network%2520controllers%252C%2520the%2520%2528e%2529NMPC%2520controllers%2520can%2520react%2520to%2520changes%2520in%250Athe%2520control%2520setting%2520without%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.01674v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Reinforcement%20Learning%20of%20Koopman%20Models%20for%20Economic%0A%20%20Nonlinear%20Model%20Predictive%20Control&entry.906535625=Daniel%20Mayfrank%20and%20Alexander%20Mitsos%20and%20Manuel%20Dahmen&entry.1292438233=%20%20%28Economic%29%20nonlinear%20model%20predictive%20control%20%28%28e%29NMPC%29%20requires%20dynamic%0Amodels%20that%20are%20sufficiently%20accurate%20and%20computationally%20tractable.%0AData-driven%20surrogate%20models%20for%20mechanistic%20models%20can%20reduce%20the%0Acomputational%20burden%20of%20%28e%29NMPC%3B%20however%2C%20such%20models%20are%20typically%20trained%20by%0Asystem%20identification%20for%20maximum%20prediction%20accuracy%20on%20simulation%20samples%20and%0Aperform%20suboptimally%20in%20%28e%29NMPC.%20We%20present%20a%20method%20for%20end-to-end%0Areinforcement%20learning%20of%20Koopman%20surrogate%20models%20for%20optimal%20performance%20as%0Apart%20of%20%28e%29NMPC.%20We%20apply%20our%20method%20to%20two%20applications%20derived%20from%20an%0Aestablished%20nonlinear%20continuous%20stirred-tank%20reactor%20model.%20The%20controller%0Aperformance%20is%20compared%20to%20that%20of%20%28e%29NMPCs%20utilizing%20models%20trained%20using%0Asystem%20identification%2C%20and%20model-free%20neural%20network%20controllers%20trained%20using%0Areinforcement%20learning.%20We%20show%20that%20the%20end-to-end%20trained%20models%20outperform%0Athose%20trained%20using%20system%20identification%20in%20%28e%29NMPC%2C%20and%20that%2C%20in%20contrast%20to%0Athe%20neural%20network%20controllers%2C%20the%20%28e%29NMPC%20controllers%20can%20react%20to%20changes%20in%0Athe%20control%20setting%20without%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.01674v4&entry.124074799=Read"},
{"title": "Accelerating Full Waveform Inversion By Transfer Learning", "author": "Divya Shyam Singh and Leon Herrmann and Qing Sun and Tim B\u00fcrchner and Felix Dietrich and Stefan Kollmannsberger", "abstract": "  Full waveform inversion (FWI) is a powerful tool for reconstructing material\nfields based on sparsely measured data obtained by wave propagation. For\nspecific problems, discretizing the material field with a neural network (NN)\nimproves the robustness and reconstruction quality of the corresponding\noptimization problem. We call this method NN-based FWI. Starting from an\ninitial guess, the weights of the NN are iteratively updated to fit the\nsimulated wave signals to the sparsely measured data set. For gradient-based\noptimization, a suitable choice of the initial guess, i.e., a suitable NN\nweight initialization, is crucial for fast and robust convergence.\n  In this paper, we introduce a novel transfer learning approach to further\nimprove NN-based FWI. This approach leverages supervised pretraining to provide\na better NN weight initialization, leading to faster convergence of the\nsubsequent optimization problem. Moreover, the inversions yield physically more\nmeaningful local minima. The network is pretrained to predict the unknown\nmaterial field using the gradient information from the first iteration of\nconventional FWI. In our computational experiments on two-dimensional domains,\nthe training data set consists of reference simulations with arbitrarily\npositioned elliptical voids of different shapes and orientations. We compare\nthe performance of the proposed transfer learning NN-based FWI with three other\nmethods: conventional FWI, NN-based FWI without pretraining and conventional\nFWI with an initial guess predicted from the pretrained NN. Our results show\nthat transfer learning NN-based FWI outperforms the other methods in terms of\nconvergence speed and reconstruction quality.\n", "link": "http://arxiv.org/abs/2408.00695v1", "date": "2024-08-01", "relevancy": 1.9414, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4955}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4854}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Full%20Waveform%20Inversion%20By%20Transfer%20Learning&body=Title%3A%20Accelerating%20Full%20Waveform%20Inversion%20By%20Transfer%20Learning%0AAuthor%3A%20Divya%20Shyam%20Singh%20and%20Leon%20Herrmann%20and%20Qing%20Sun%20and%20Tim%20B%C3%BCrchner%20and%20Felix%20Dietrich%20and%20Stefan%20Kollmannsberger%0AAbstract%3A%20%20%20Full%20waveform%20inversion%20%28FWI%29%20is%20a%20powerful%20tool%20for%20reconstructing%20material%0Afields%20based%20on%20sparsely%20measured%20data%20obtained%20by%20wave%20propagation.%20For%0Aspecific%20problems%2C%20discretizing%20the%20material%20field%20with%20a%20neural%20network%20%28NN%29%0Aimproves%20the%20robustness%20and%20reconstruction%20quality%20of%20the%20corresponding%0Aoptimization%20problem.%20We%20call%20this%20method%20NN-based%20FWI.%20Starting%20from%20an%0Ainitial%20guess%2C%20the%20weights%20of%20the%20NN%20are%20iteratively%20updated%20to%20fit%20the%0Asimulated%20wave%20signals%20to%20the%20sparsely%20measured%20data%20set.%20For%20gradient-based%0Aoptimization%2C%20a%20suitable%20choice%20of%20the%20initial%20guess%2C%20i.e.%2C%20a%20suitable%20NN%0Aweight%20initialization%2C%20is%20crucial%20for%20fast%20and%20robust%20convergence.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20transfer%20learning%20approach%20to%20further%0Aimprove%20NN-based%20FWI.%20This%20approach%20leverages%20supervised%20pretraining%20to%20provide%0Aa%20better%20NN%20weight%20initialization%2C%20leading%20to%20faster%20convergence%20of%20the%0Asubsequent%20optimization%20problem.%20Moreover%2C%20the%20inversions%20yield%20physically%20more%0Ameaningful%20local%20minima.%20The%20network%20is%20pretrained%20to%20predict%20the%20unknown%0Amaterial%20field%20using%20the%20gradient%20information%20from%20the%20first%20iteration%20of%0Aconventional%20FWI.%20In%20our%20computational%20experiments%20on%20two-dimensional%20domains%2C%0Athe%20training%20data%20set%20consists%20of%20reference%20simulations%20with%20arbitrarily%0Apositioned%20elliptical%20voids%20of%20different%20shapes%20and%20orientations.%20We%20compare%0Athe%20performance%20of%20the%20proposed%20transfer%20learning%20NN-based%20FWI%20with%20three%20other%0Amethods%3A%20conventional%20FWI%2C%20NN-based%20FWI%20without%20pretraining%20and%20conventional%0AFWI%20with%20an%20initial%20guess%20predicted%20from%20the%20pretrained%20NN.%20Our%20results%20show%0Athat%20transfer%20learning%20NN-based%20FWI%20outperforms%20the%20other%20methods%20in%20terms%20of%0Aconvergence%20speed%20and%20reconstruction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Full%2520Waveform%2520Inversion%2520By%2520Transfer%2520Learning%26entry.906535625%3DDivya%2520Shyam%2520Singh%2520and%2520Leon%2520Herrmann%2520and%2520Qing%2520Sun%2520and%2520Tim%2520B%25C3%25BCrchner%2520and%2520Felix%2520Dietrich%2520and%2520Stefan%2520Kollmannsberger%26entry.1292438233%3D%2520%2520Full%2520waveform%2520inversion%2520%2528FWI%2529%2520is%2520a%2520powerful%2520tool%2520for%2520reconstructing%2520material%250Afields%2520based%2520on%2520sparsely%2520measured%2520data%2520obtained%2520by%2520wave%2520propagation.%2520For%250Aspecific%2520problems%252C%2520discretizing%2520the%2520material%2520field%2520with%2520a%2520neural%2520network%2520%2528NN%2529%250Aimproves%2520the%2520robustness%2520and%2520reconstruction%2520quality%2520of%2520the%2520corresponding%250Aoptimization%2520problem.%2520We%2520call%2520this%2520method%2520NN-based%2520FWI.%2520Starting%2520from%2520an%250Ainitial%2520guess%252C%2520the%2520weights%2520of%2520the%2520NN%2520are%2520iteratively%2520updated%2520to%2520fit%2520the%250Asimulated%2520wave%2520signals%2520to%2520the%2520sparsely%2520measured%2520data%2520set.%2520For%2520gradient-based%250Aoptimization%252C%2520a%2520suitable%2520choice%2520of%2520the%2520initial%2520guess%252C%2520i.e.%252C%2520a%2520suitable%2520NN%250Aweight%2520initialization%252C%2520is%2520crucial%2520for%2520fast%2520and%2520robust%2520convergence.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520transfer%2520learning%2520approach%2520to%2520further%250Aimprove%2520NN-based%2520FWI.%2520This%2520approach%2520leverages%2520supervised%2520pretraining%2520to%2520provide%250Aa%2520better%2520NN%2520weight%2520initialization%252C%2520leading%2520to%2520faster%2520convergence%2520of%2520the%250Asubsequent%2520optimization%2520problem.%2520Moreover%252C%2520the%2520inversions%2520yield%2520physically%2520more%250Ameaningful%2520local%2520minima.%2520The%2520network%2520is%2520pretrained%2520to%2520predict%2520the%2520unknown%250Amaterial%2520field%2520using%2520the%2520gradient%2520information%2520from%2520the%2520first%2520iteration%2520of%250Aconventional%2520FWI.%2520In%2520our%2520computational%2520experiments%2520on%2520two-dimensional%2520domains%252C%250Athe%2520training%2520data%2520set%2520consists%2520of%2520reference%2520simulations%2520with%2520arbitrarily%250Apositioned%2520elliptical%2520voids%2520of%2520different%2520shapes%2520and%2520orientations.%2520We%2520compare%250Athe%2520performance%2520of%2520the%2520proposed%2520transfer%2520learning%2520NN-based%2520FWI%2520with%2520three%2520other%250Amethods%253A%2520conventional%2520FWI%252C%2520NN-based%2520FWI%2520without%2520pretraining%2520and%2520conventional%250AFWI%2520with%2520an%2520initial%2520guess%2520predicted%2520from%2520the%2520pretrained%2520NN.%2520Our%2520results%2520show%250Athat%2520transfer%2520learning%2520NN-based%2520FWI%2520outperforms%2520the%2520other%2520methods%2520in%2520terms%2520of%250Aconvergence%2520speed%2520and%2520reconstruction%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Full%20Waveform%20Inversion%20By%20Transfer%20Learning&entry.906535625=Divya%20Shyam%20Singh%20and%20Leon%20Herrmann%20and%20Qing%20Sun%20and%20Tim%20B%C3%BCrchner%20and%20Felix%20Dietrich%20and%20Stefan%20Kollmannsberger&entry.1292438233=%20%20Full%20waveform%20inversion%20%28FWI%29%20is%20a%20powerful%20tool%20for%20reconstructing%20material%0Afields%20based%20on%20sparsely%20measured%20data%20obtained%20by%20wave%20propagation.%20For%0Aspecific%20problems%2C%20discretizing%20the%20material%20field%20with%20a%20neural%20network%20%28NN%29%0Aimproves%20the%20robustness%20and%20reconstruction%20quality%20of%20the%20corresponding%0Aoptimization%20problem.%20We%20call%20this%20method%20NN-based%20FWI.%20Starting%20from%20an%0Ainitial%20guess%2C%20the%20weights%20of%20the%20NN%20are%20iteratively%20updated%20to%20fit%20the%0Asimulated%20wave%20signals%20to%20the%20sparsely%20measured%20data%20set.%20For%20gradient-based%0Aoptimization%2C%20a%20suitable%20choice%20of%20the%20initial%20guess%2C%20i.e.%2C%20a%20suitable%20NN%0Aweight%20initialization%2C%20is%20crucial%20for%20fast%20and%20robust%20convergence.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20transfer%20learning%20approach%20to%20further%0Aimprove%20NN-based%20FWI.%20This%20approach%20leverages%20supervised%20pretraining%20to%20provide%0Aa%20better%20NN%20weight%20initialization%2C%20leading%20to%20faster%20convergence%20of%20the%0Asubsequent%20optimization%20problem.%20Moreover%2C%20the%20inversions%20yield%20physically%20more%0Ameaningful%20local%20minima.%20The%20network%20is%20pretrained%20to%20predict%20the%20unknown%0Amaterial%20field%20using%20the%20gradient%20information%20from%20the%20first%20iteration%20of%0Aconventional%20FWI.%20In%20our%20computational%20experiments%20on%20two-dimensional%20domains%2C%0Athe%20training%20data%20set%20consists%20of%20reference%20simulations%20with%20arbitrarily%0Apositioned%20elliptical%20voids%20of%20different%20shapes%20and%20orientations.%20We%20compare%0Athe%20performance%20of%20the%20proposed%20transfer%20learning%20NN-based%20FWI%20with%20three%20other%0Amethods%3A%20conventional%20FWI%2C%20NN-based%20FWI%20without%20pretraining%20and%20conventional%0AFWI%20with%20an%20initial%20guess%20predicted%20from%20the%20pretrained%20NN.%20Our%20results%20show%0Athat%20transfer%20learning%20NN-based%20FWI%20outperforms%20the%20other%20methods%20in%20terms%20of%0Aconvergence%20speed%20and%20reconstruction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00695v1&entry.124074799=Read"},
{"title": "Efficient Patient Fine-Tuned Seizure Detection with a Tensor Kernel\n  Machine", "author": "Seline J. S. de Rooij and Frederiek Wesel and Borb\u00e1la Hunyadi", "abstract": "  Recent developments in wearable devices have made accurate and efficient\nseizure detection more important than ever. A challenge in seizure detection is\nthat patient-specific models typically outperform patient-independent models.\nHowever, in a wearable device one typically starts with a patient-independent\nmodel, until such patient-specific data is available. To avoid having to\nconstruct a new classifier with this data, as required in conventional kernel\nmachines, we propose a transfer learning approach with a tensor kernel machine.\nThis method learns the primal weights in a compressed form using the canonical\npolyadic decomposition, making it possible to efficiently update the weights of\nthe patient-independent model with patient-specific data. The results show that\nthis patient fine-tuned model reaches as high a performance as a\npatient-specific SVM model with a model size that is twice as small as the\npatient-specific model and ten times as small as the patient-independent model.\n", "link": "http://arxiv.org/abs/2408.00437v1", "date": "2024-08-01", "relevancy": 1.9387, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4836}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Patient%20Fine-Tuned%20Seizure%20Detection%20with%20a%20Tensor%20Kernel%0A%20%20Machine&body=Title%3A%20Efficient%20Patient%20Fine-Tuned%20Seizure%20Detection%20with%20a%20Tensor%20Kernel%0A%20%20Machine%0AAuthor%3A%20Seline%20J.%20S.%20de%20Rooij%20and%20Frederiek%20Wesel%20and%20Borb%C3%A1la%20Hunyadi%0AAbstract%3A%20%20%20Recent%20developments%20in%20wearable%20devices%20have%20made%20accurate%20and%20efficient%0Aseizure%20detection%20more%20important%20than%20ever.%20A%20challenge%20in%20seizure%20detection%20is%0Athat%20patient-specific%20models%20typically%20outperform%20patient-independent%20models.%0AHowever%2C%20in%20a%20wearable%20device%20one%20typically%20starts%20with%20a%20patient-independent%0Amodel%2C%20until%20such%20patient-specific%20data%20is%20available.%20To%20avoid%20having%20to%0Aconstruct%20a%20new%20classifier%20with%20this%20data%2C%20as%20required%20in%20conventional%20kernel%0Amachines%2C%20we%20propose%20a%20transfer%20learning%20approach%20with%20a%20tensor%20kernel%20machine.%0AThis%20method%20learns%20the%20primal%20weights%20in%20a%20compressed%20form%20using%20the%20canonical%0Apolyadic%20decomposition%2C%20making%20it%20possible%20to%20efficiently%20update%20the%20weights%20of%0Athe%20patient-independent%20model%20with%20patient-specific%20data.%20The%20results%20show%20that%0Athis%20patient%20fine-tuned%20model%20reaches%20as%20high%20a%20performance%20as%20a%0Apatient-specific%20SVM%20model%20with%20a%20model%20size%20that%20is%20twice%20as%20small%20as%20the%0Apatient-specific%20model%20and%20ten%20times%20as%20small%20as%20the%20patient-independent%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Patient%2520Fine-Tuned%2520Seizure%2520Detection%2520with%2520a%2520Tensor%2520Kernel%250A%2520%2520Machine%26entry.906535625%3DSeline%2520J.%2520S.%2520de%2520Rooij%2520and%2520Frederiek%2520Wesel%2520and%2520Borb%25C3%25A1la%2520Hunyadi%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520wearable%2520devices%2520have%2520made%2520accurate%2520and%2520efficient%250Aseizure%2520detection%2520more%2520important%2520than%2520ever.%2520A%2520challenge%2520in%2520seizure%2520detection%2520is%250Athat%2520patient-specific%2520models%2520typically%2520outperform%2520patient-independent%2520models.%250AHowever%252C%2520in%2520a%2520wearable%2520device%2520one%2520typically%2520starts%2520with%2520a%2520patient-independent%250Amodel%252C%2520until%2520such%2520patient-specific%2520data%2520is%2520available.%2520To%2520avoid%2520having%2520to%250Aconstruct%2520a%2520new%2520classifier%2520with%2520this%2520data%252C%2520as%2520required%2520in%2520conventional%2520kernel%250Amachines%252C%2520we%2520propose%2520a%2520transfer%2520learning%2520approach%2520with%2520a%2520tensor%2520kernel%2520machine.%250AThis%2520method%2520learns%2520the%2520primal%2520weights%2520in%2520a%2520compressed%2520form%2520using%2520the%2520canonical%250Apolyadic%2520decomposition%252C%2520making%2520it%2520possible%2520to%2520efficiently%2520update%2520the%2520weights%2520of%250Athe%2520patient-independent%2520model%2520with%2520patient-specific%2520data.%2520The%2520results%2520show%2520that%250Athis%2520patient%2520fine-tuned%2520model%2520reaches%2520as%2520high%2520a%2520performance%2520as%2520a%250Apatient-specific%2520SVM%2520model%2520with%2520a%2520model%2520size%2520that%2520is%2520twice%2520as%2520small%2520as%2520the%250Apatient-specific%2520model%2520and%2520ten%2520times%2520as%2520small%2520as%2520the%2520patient-independent%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Patient%20Fine-Tuned%20Seizure%20Detection%20with%20a%20Tensor%20Kernel%0A%20%20Machine&entry.906535625=Seline%20J.%20S.%20de%20Rooij%20and%20Frederiek%20Wesel%20and%20Borb%C3%A1la%20Hunyadi&entry.1292438233=%20%20Recent%20developments%20in%20wearable%20devices%20have%20made%20accurate%20and%20efficient%0Aseizure%20detection%20more%20important%20than%20ever.%20A%20challenge%20in%20seizure%20detection%20is%0Athat%20patient-specific%20models%20typically%20outperform%20patient-independent%20models.%0AHowever%2C%20in%20a%20wearable%20device%20one%20typically%20starts%20with%20a%20patient-independent%0Amodel%2C%20until%20such%20patient-specific%20data%20is%20available.%20To%20avoid%20having%20to%0Aconstruct%20a%20new%20classifier%20with%20this%20data%2C%20as%20required%20in%20conventional%20kernel%0Amachines%2C%20we%20propose%20a%20transfer%20learning%20approach%20with%20a%20tensor%20kernel%20machine.%0AThis%20method%20learns%20the%20primal%20weights%20in%20a%20compressed%20form%20using%20the%20canonical%0Apolyadic%20decomposition%2C%20making%20it%20possible%20to%20efficiently%20update%20the%20weights%20of%0Athe%20patient-independent%20model%20with%20patient-specific%20data.%20The%20results%20show%20that%0Athis%20patient%20fine-tuned%20model%20reaches%20as%20high%20a%20performance%20as%20a%0Apatient-specific%20SVM%20model%20with%20a%20model%20size%20that%20is%20twice%20as%20small%20as%20the%0Apatient-specific%20model%20and%20ten%20times%20as%20small%20as%20the%20patient-independent%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00437v1&entry.124074799=Read"},
{"title": "Seed Kernel Counting using Domain Randomization and Object Tracking\n  Neural Networks", "author": "Venkat Margapuri and Prapti Thapaliya and Mitchell Neilsen", "abstract": "  High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping,\nis the comprehensive assessment of complex seed traits such as growth,\ndevelopment, tolerance, resistance, ecology, yield, and the measurement of\nparameters that form more complex traits. One of the key aspects of seed\nphenotyping is cereal yield estimation that the seed production industry relies\nupon to conduct their business. While mechanized seed kernel counters are\navailable in the market currently, they are often priced high and sometimes\noutside the range of small scale seed production firms' affordability. The\ndevelopment of object tracking neural network models such as You Only Look Once\n(YOLO) enables computer scientists to design algorithms that can estimate\ncereal yield inexpensively. The key bottleneck with neural network models is\nthat they require a plethora of labelled training data before they can be put\nto task. We demonstrate that the use of synthetic imagery serves as a feasible\nsubstitute to train neural networks for object tracking that includes the tasks\nof object classification and detection. Furthermore, we propose a seed kernel\ncounter that uses a low-cost mechanical hopper, trained YOLOv8 neural network\nmodel, and object tracking algorithms on StrongSORT and ByteTrack to estimate\ncereal yield from videos. The experiment yields a seed kernel count with an\naccuracy of 95.2\\% and 93.2\\% for Soy and Wheat respectively using the\nStrongSORT algorithm, and an accuray of 96.8\\% and 92.4\\% for Soy and Wheat\nrespectively using the ByteTrack algorithm.\n", "link": "http://arxiv.org/abs/2308.05846v2", "date": "2024-08-01", "relevancy": 1.9301, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4924}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seed%20Kernel%20Counting%20using%20Domain%20Randomization%20and%20Object%20Tracking%0A%20%20Neural%20Networks&body=Title%3A%20Seed%20Kernel%20Counting%20using%20Domain%20Randomization%20and%20Object%20Tracking%0A%20%20Neural%20Networks%0AAuthor%3A%20Venkat%20Margapuri%20and%20Prapti%20Thapaliya%20and%20Mitchell%20Neilsen%0AAbstract%3A%20%20%20High-throughput%20phenotyping%20%28HTP%29%20of%20seeds%2C%20also%20known%20as%20seed%20phenotyping%2C%0Ais%20the%20comprehensive%20assessment%20of%20complex%20seed%20traits%20such%20as%20growth%2C%0Adevelopment%2C%20tolerance%2C%20resistance%2C%20ecology%2C%20yield%2C%20and%20the%20measurement%20of%0Aparameters%20that%20form%20more%20complex%20traits.%20One%20of%20the%20key%20aspects%20of%20seed%0Aphenotyping%20is%20cereal%20yield%20estimation%20that%20the%20seed%20production%20industry%20relies%0Aupon%20to%20conduct%20their%20business.%20While%20mechanized%20seed%20kernel%20counters%20are%0Aavailable%20in%20the%20market%20currently%2C%20they%20are%20often%20priced%20high%20and%20sometimes%0Aoutside%20the%20range%20of%20small%20scale%20seed%20production%20firms%27%20affordability.%20The%0Adevelopment%20of%20object%20tracking%20neural%20network%20models%20such%20as%20You%20Only%20Look%20Once%0A%28YOLO%29%20enables%20computer%20scientists%20to%20design%20algorithms%20that%20can%20estimate%0Acereal%20yield%20inexpensively.%20The%20key%20bottleneck%20with%20neural%20network%20models%20is%0Athat%20they%20require%20a%20plethora%20of%20labelled%20training%20data%20before%20they%20can%20be%20put%0Ato%20task.%20We%20demonstrate%20that%20the%20use%20of%20synthetic%20imagery%20serves%20as%20a%20feasible%0Asubstitute%20to%20train%20neural%20networks%20for%20object%20tracking%20that%20includes%20the%20tasks%0Aof%20object%20classification%20and%20detection.%20Furthermore%2C%20we%20propose%20a%20seed%20kernel%0Acounter%20that%20uses%20a%20low-cost%20mechanical%20hopper%2C%20trained%20YOLOv8%20neural%20network%0Amodel%2C%20and%20object%20tracking%20algorithms%20on%20StrongSORT%20and%20ByteTrack%20to%20estimate%0Acereal%20yield%20from%20videos.%20The%20experiment%20yields%20a%20seed%20kernel%20count%20with%20an%0Aaccuracy%20of%2095.2%5C%25%20and%2093.2%5C%25%20for%20Soy%20and%20Wheat%20respectively%20using%20the%0AStrongSORT%20algorithm%2C%20and%20an%20accuray%20of%2096.8%5C%25%20and%2092.4%5C%25%20for%20Soy%20and%20Wheat%0Arespectively%20using%20the%20ByteTrack%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeed%2520Kernel%2520Counting%2520using%2520Domain%2520Randomization%2520and%2520Object%2520Tracking%250A%2520%2520Neural%2520Networks%26entry.906535625%3DVenkat%2520Margapuri%2520and%2520Prapti%2520Thapaliya%2520and%2520Mitchell%2520Neilsen%26entry.1292438233%3D%2520%2520High-throughput%2520phenotyping%2520%2528HTP%2529%2520of%2520seeds%252C%2520also%2520known%2520as%2520seed%2520phenotyping%252C%250Ais%2520the%2520comprehensive%2520assessment%2520of%2520complex%2520seed%2520traits%2520such%2520as%2520growth%252C%250Adevelopment%252C%2520tolerance%252C%2520resistance%252C%2520ecology%252C%2520yield%252C%2520and%2520the%2520measurement%2520of%250Aparameters%2520that%2520form%2520more%2520complex%2520traits.%2520One%2520of%2520the%2520key%2520aspects%2520of%2520seed%250Aphenotyping%2520is%2520cereal%2520yield%2520estimation%2520that%2520the%2520seed%2520production%2520industry%2520relies%250Aupon%2520to%2520conduct%2520their%2520business.%2520While%2520mechanized%2520seed%2520kernel%2520counters%2520are%250Aavailable%2520in%2520the%2520market%2520currently%252C%2520they%2520are%2520often%2520priced%2520high%2520and%2520sometimes%250Aoutside%2520the%2520range%2520of%2520small%2520scale%2520seed%2520production%2520firms%2527%2520affordability.%2520The%250Adevelopment%2520of%2520object%2520tracking%2520neural%2520network%2520models%2520such%2520as%2520You%2520Only%2520Look%2520Once%250A%2528YOLO%2529%2520enables%2520computer%2520scientists%2520to%2520design%2520algorithms%2520that%2520can%2520estimate%250Acereal%2520yield%2520inexpensively.%2520The%2520key%2520bottleneck%2520with%2520neural%2520network%2520models%2520is%250Athat%2520they%2520require%2520a%2520plethora%2520of%2520labelled%2520training%2520data%2520before%2520they%2520can%2520be%2520put%250Ato%2520task.%2520We%2520demonstrate%2520that%2520the%2520use%2520of%2520synthetic%2520imagery%2520serves%2520as%2520a%2520feasible%250Asubstitute%2520to%2520train%2520neural%2520networks%2520for%2520object%2520tracking%2520that%2520includes%2520the%2520tasks%250Aof%2520object%2520classification%2520and%2520detection.%2520Furthermore%252C%2520we%2520propose%2520a%2520seed%2520kernel%250Acounter%2520that%2520uses%2520a%2520low-cost%2520mechanical%2520hopper%252C%2520trained%2520YOLOv8%2520neural%2520network%250Amodel%252C%2520and%2520object%2520tracking%2520algorithms%2520on%2520StrongSORT%2520and%2520ByteTrack%2520to%2520estimate%250Acereal%2520yield%2520from%2520videos.%2520The%2520experiment%2520yields%2520a%2520seed%2520kernel%2520count%2520with%2520an%250Aaccuracy%2520of%252095.2%255C%2525%2520and%252093.2%255C%2525%2520for%2520Soy%2520and%2520Wheat%2520respectively%2520using%2520the%250AStrongSORT%2520algorithm%252C%2520and%2520an%2520accuray%2520of%252096.8%255C%2525%2520and%252092.4%255C%2525%2520for%2520Soy%2520and%2520Wheat%250Arespectively%2520using%2520the%2520ByteTrack%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seed%20Kernel%20Counting%20using%20Domain%20Randomization%20and%20Object%20Tracking%0A%20%20Neural%20Networks&entry.906535625=Venkat%20Margapuri%20and%20Prapti%20Thapaliya%20and%20Mitchell%20Neilsen&entry.1292438233=%20%20High-throughput%20phenotyping%20%28HTP%29%20of%20seeds%2C%20also%20known%20as%20seed%20phenotyping%2C%0Ais%20the%20comprehensive%20assessment%20of%20complex%20seed%20traits%20such%20as%20growth%2C%0Adevelopment%2C%20tolerance%2C%20resistance%2C%20ecology%2C%20yield%2C%20and%20the%20measurement%20of%0Aparameters%20that%20form%20more%20complex%20traits.%20One%20of%20the%20key%20aspects%20of%20seed%0Aphenotyping%20is%20cereal%20yield%20estimation%20that%20the%20seed%20production%20industry%20relies%0Aupon%20to%20conduct%20their%20business.%20While%20mechanized%20seed%20kernel%20counters%20are%0Aavailable%20in%20the%20market%20currently%2C%20they%20are%20often%20priced%20high%20and%20sometimes%0Aoutside%20the%20range%20of%20small%20scale%20seed%20production%20firms%27%20affordability.%20The%0Adevelopment%20of%20object%20tracking%20neural%20network%20models%20such%20as%20You%20Only%20Look%20Once%0A%28YOLO%29%20enables%20computer%20scientists%20to%20design%20algorithms%20that%20can%20estimate%0Acereal%20yield%20inexpensively.%20The%20key%20bottleneck%20with%20neural%20network%20models%20is%0Athat%20they%20require%20a%20plethora%20of%20labelled%20training%20data%20before%20they%20can%20be%20put%0Ato%20task.%20We%20demonstrate%20that%20the%20use%20of%20synthetic%20imagery%20serves%20as%20a%20feasible%0Asubstitute%20to%20train%20neural%20networks%20for%20object%20tracking%20that%20includes%20the%20tasks%0Aof%20object%20classification%20and%20detection.%20Furthermore%2C%20we%20propose%20a%20seed%20kernel%0Acounter%20that%20uses%20a%20low-cost%20mechanical%20hopper%2C%20trained%20YOLOv8%20neural%20network%0Amodel%2C%20and%20object%20tracking%20algorithms%20on%20StrongSORT%20and%20ByteTrack%20to%20estimate%0Acereal%20yield%20from%20videos.%20The%20experiment%20yields%20a%20seed%20kernel%20count%20with%20an%0Aaccuracy%20of%2095.2%5C%25%20and%2093.2%5C%25%20for%20Soy%20and%20Wheat%20respectively%20using%20the%0AStrongSORT%20algorithm%2C%20and%20an%20accuray%20of%2096.8%5C%25%20and%2092.4%5C%25%20for%20Soy%20and%20Wheat%0Arespectively%20using%20the%20ByteTrack%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05846v2&entry.124074799=Read"},
{"title": "AutoPV: Automatically Design Your Photovoltaic Power Forecasting Model", "author": "Dayin Chen and Xiaodan Shi and Mingkun Jiang and Haoran Zhang and Dongxiao Zhang and Yuntian Chen and Jinyue Yan", "abstract": "  Photovoltaic power forecasting (PVPF) is a critical area in time series\nforecasting (TSF), enabling the efficient utilization of solar energy. With\nadvancements in machine learning and deep learning, various models have been\napplied to PVPF tasks. However, constructing an optimal predictive architecture\nfor specific PVPF tasks remains challenging, as it requires cross-domain\nknowledge and significant labor costs. To address this challenge, we introduce\nAutoPV, a novel framework for the automated search and construction of PVPF\nmodels based on neural architecture search (NAS) technology. We develop a brand\nnew NAS search space that incorporates various data processing techniques from\nstate-of-the-art (SOTA) TSF models and typical PVPF deep learning models. The\neffectiveness of AutoPV is evaluated on diverse PVPF tasks using a dataset from\nthe Daqing Photovoltaic Station in China. Experimental results demonstrate that\nAutoPV can complete the predictive architecture construction process in a\nrelatively short time, and the newly constructed architecture is superior to\nSOTA predefined models. This work bridges the gap in applying NAS to TSF\nproblems, assisting non-experts and industries in automatically designing\neffective PVPF models.\n", "link": "http://arxiv.org/abs/2408.00601v1", "date": "2024-08-01", "relevancy": 1.9301, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.476}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoPV%3A%20Automatically%20Design%20Your%20Photovoltaic%20Power%20Forecasting%20Model&body=Title%3A%20AutoPV%3A%20Automatically%20Design%20Your%20Photovoltaic%20Power%20Forecasting%20Model%0AAuthor%3A%20Dayin%20Chen%20and%20Xiaodan%20Shi%20and%20Mingkun%20Jiang%20and%20Haoran%20Zhang%20and%20Dongxiao%20Zhang%20and%20Yuntian%20Chen%20and%20Jinyue%20Yan%0AAbstract%3A%20%20%20Photovoltaic%20power%20forecasting%20%28PVPF%29%20is%20a%20critical%20area%20in%20time%20series%0Aforecasting%20%28TSF%29%2C%20enabling%20the%20efficient%20utilization%20of%20solar%20energy.%20With%0Aadvancements%20in%20machine%20learning%20and%20deep%20learning%2C%20various%20models%20have%20been%0Aapplied%20to%20PVPF%20tasks.%20However%2C%20constructing%20an%20optimal%20predictive%20architecture%0Afor%20specific%20PVPF%20tasks%20remains%20challenging%2C%20as%20it%20requires%20cross-domain%0Aknowledge%20and%20significant%20labor%20costs.%20To%20address%20this%20challenge%2C%20we%20introduce%0AAutoPV%2C%20a%20novel%20framework%20for%20the%20automated%20search%20and%20construction%20of%20PVPF%0Amodels%20based%20on%20neural%20architecture%20search%20%28NAS%29%20technology.%20We%20develop%20a%20brand%0Anew%20NAS%20search%20space%20that%20incorporates%20various%20data%20processing%20techniques%20from%0Astate-of-the-art%20%28SOTA%29%20TSF%20models%20and%20typical%20PVPF%20deep%20learning%20models.%20The%0Aeffectiveness%20of%20AutoPV%20is%20evaluated%20on%20diverse%20PVPF%20tasks%20using%20a%20dataset%20from%0Athe%20Daqing%20Photovoltaic%20Station%20in%20China.%20Experimental%20results%20demonstrate%20that%0AAutoPV%20can%20complete%20the%20predictive%20architecture%20construction%20process%20in%20a%0Arelatively%20short%20time%2C%20and%20the%20newly%20constructed%20architecture%20is%20superior%20to%0ASOTA%20predefined%20models.%20This%20work%20bridges%20the%20gap%20in%20applying%20NAS%20to%20TSF%0Aproblems%2C%20assisting%20non-experts%20and%20industries%20in%20automatically%20designing%0Aeffective%20PVPF%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoPV%253A%2520Automatically%2520Design%2520Your%2520Photovoltaic%2520Power%2520Forecasting%2520Model%26entry.906535625%3DDayin%2520Chen%2520and%2520Xiaodan%2520Shi%2520and%2520Mingkun%2520Jiang%2520and%2520Haoran%2520Zhang%2520and%2520Dongxiao%2520Zhang%2520and%2520Yuntian%2520Chen%2520and%2520Jinyue%2520Yan%26entry.1292438233%3D%2520%2520Photovoltaic%2520power%2520forecasting%2520%2528PVPF%2529%2520is%2520a%2520critical%2520area%2520in%2520time%2520series%250Aforecasting%2520%2528TSF%2529%252C%2520enabling%2520the%2520efficient%2520utilization%2520of%2520solar%2520energy.%2520With%250Aadvancements%2520in%2520machine%2520learning%2520and%2520deep%2520learning%252C%2520various%2520models%2520have%2520been%250Aapplied%2520to%2520PVPF%2520tasks.%2520However%252C%2520constructing%2520an%2520optimal%2520predictive%2520architecture%250Afor%2520specific%2520PVPF%2520tasks%2520remains%2520challenging%252C%2520as%2520it%2520requires%2520cross-domain%250Aknowledge%2520and%2520significant%2520labor%2520costs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%250AAutoPV%252C%2520a%2520novel%2520framework%2520for%2520the%2520automated%2520search%2520and%2520construction%2520of%2520PVPF%250Amodels%2520based%2520on%2520neural%2520architecture%2520search%2520%2528NAS%2529%2520technology.%2520We%2520develop%2520a%2520brand%250Anew%2520NAS%2520search%2520space%2520that%2520incorporates%2520various%2520data%2520processing%2520techniques%2520from%250Astate-of-the-art%2520%2528SOTA%2529%2520TSF%2520models%2520and%2520typical%2520PVPF%2520deep%2520learning%2520models.%2520The%250Aeffectiveness%2520of%2520AutoPV%2520is%2520evaluated%2520on%2520diverse%2520PVPF%2520tasks%2520using%2520a%2520dataset%2520from%250Athe%2520Daqing%2520Photovoltaic%2520Station%2520in%2520China.%2520Experimental%2520results%2520demonstrate%2520that%250AAutoPV%2520can%2520complete%2520the%2520predictive%2520architecture%2520construction%2520process%2520in%2520a%250Arelatively%2520short%2520time%252C%2520and%2520the%2520newly%2520constructed%2520architecture%2520is%2520superior%2520to%250ASOTA%2520predefined%2520models.%2520This%2520work%2520bridges%2520the%2520gap%2520in%2520applying%2520NAS%2520to%2520TSF%250Aproblems%252C%2520assisting%2520non-experts%2520and%2520industries%2520in%2520automatically%2520designing%250Aeffective%2520PVPF%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoPV%3A%20Automatically%20Design%20Your%20Photovoltaic%20Power%20Forecasting%20Model&entry.906535625=Dayin%20Chen%20and%20Xiaodan%20Shi%20and%20Mingkun%20Jiang%20and%20Haoran%20Zhang%20and%20Dongxiao%20Zhang%20and%20Yuntian%20Chen%20and%20Jinyue%20Yan&entry.1292438233=%20%20Photovoltaic%20power%20forecasting%20%28PVPF%29%20is%20a%20critical%20area%20in%20time%20series%0Aforecasting%20%28TSF%29%2C%20enabling%20the%20efficient%20utilization%20of%20solar%20energy.%20With%0Aadvancements%20in%20machine%20learning%20and%20deep%20learning%2C%20various%20models%20have%20been%0Aapplied%20to%20PVPF%20tasks.%20However%2C%20constructing%20an%20optimal%20predictive%20architecture%0Afor%20specific%20PVPF%20tasks%20remains%20challenging%2C%20as%20it%20requires%20cross-domain%0Aknowledge%20and%20significant%20labor%20costs.%20To%20address%20this%20challenge%2C%20we%20introduce%0AAutoPV%2C%20a%20novel%20framework%20for%20the%20automated%20search%20and%20construction%20of%20PVPF%0Amodels%20based%20on%20neural%20architecture%20search%20%28NAS%29%20technology.%20We%20develop%20a%20brand%0Anew%20NAS%20search%20space%20that%20incorporates%20various%20data%20processing%20techniques%20from%0Astate-of-the-art%20%28SOTA%29%20TSF%20models%20and%20typical%20PVPF%20deep%20learning%20models.%20The%0Aeffectiveness%20of%20AutoPV%20is%20evaluated%20on%20diverse%20PVPF%20tasks%20using%20a%20dataset%20from%0Athe%20Daqing%20Photovoltaic%20Station%20in%20China.%20Experimental%20results%20demonstrate%20that%0AAutoPV%20can%20complete%20the%20predictive%20architecture%20construction%20process%20in%20a%0Arelatively%20short%20time%2C%20and%20the%20newly%20constructed%20architecture%20is%20superior%20to%0ASOTA%20predefined%20models.%20This%20work%20bridges%20the%20gap%20in%20applying%20NAS%20to%20TSF%0Aproblems%2C%20assisting%20non-experts%20and%20industries%20in%20automatically%20designing%0Aeffective%20PVPF%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00601v1&entry.124074799=Read"},
{"title": "Unlocking Fair Use in the Generative AI Supply Chain: A Systematized\n  Literature Review", "author": "Amruta Mahuli and Asia Biega", "abstract": "  Through a systematization of generative AI (GenAI) stakeholder goals and\nexpectations, this work seeks to uncover what value different stakeholders see\nin their contributions to the GenAI supply line. This valuation enables us to\nunderstand whether fair use advocated by GenAI companies to train model\nprogresses the copyright law objective of promoting science and arts. While\nassessing the validity and efficacy of the fair use argument, we uncover\nresearch gaps and potential avenues for future works for researchers and\npolicymakers to address.\n", "link": "http://arxiv.org/abs/2408.00613v1", "date": "2024-08-01", "relevancy": 1.9137, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5042}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4641}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Fair%20Use%20in%20the%20Generative%20AI%20Supply%20Chain%3A%20A%20Systematized%0A%20%20Literature%20Review&body=Title%3A%20Unlocking%20Fair%20Use%20in%20the%20Generative%20AI%20Supply%20Chain%3A%20A%20Systematized%0A%20%20Literature%20Review%0AAuthor%3A%20Amruta%20Mahuli%20and%20Asia%20Biega%0AAbstract%3A%20%20%20Through%20a%20systematization%20of%20generative%20AI%20%28GenAI%29%20stakeholder%20goals%20and%0Aexpectations%2C%20this%20work%20seeks%20to%20uncover%20what%20value%20different%20stakeholders%20see%0Ain%20their%20contributions%20to%20the%20GenAI%20supply%20line.%20This%20valuation%20enables%20us%20to%0Aunderstand%20whether%20fair%20use%20advocated%20by%20GenAI%20companies%20to%20train%20model%0Aprogresses%20the%20copyright%20law%20objective%20of%20promoting%20science%20and%20arts.%20While%0Aassessing%20the%20validity%20and%20efficacy%20of%20the%20fair%20use%20argument%2C%20we%20uncover%0Aresearch%20gaps%20and%20potential%20avenues%20for%20future%20works%20for%20researchers%20and%0Apolicymakers%20to%20address.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Fair%2520Use%2520in%2520the%2520Generative%2520AI%2520Supply%2520Chain%253A%2520A%2520Systematized%250A%2520%2520Literature%2520Review%26entry.906535625%3DAmruta%2520Mahuli%2520and%2520Asia%2520Biega%26entry.1292438233%3D%2520%2520Through%2520a%2520systematization%2520of%2520generative%2520AI%2520%2528GenAI%2529%2520stakeholder%2520goals%2520and%250Aexpectations%252C%2520this%2520work%2520seeks%2520to%2520uncover%2520what%2520value%2520different%2520stakeholders%2520see%250Ain%2520their%2520contributions%2520to%2520the%2520GenAI%2520supply%2520line.%2520This%2520valuation%2520enables%2520us%2520to%250Aunderstand%2520whether%2520fair%2520use%2520advocated%2520by%2520GenAI%2520companies%2520to%2520train%2520model%250Aprogresses%2520the%2520copyright%2520law%2520objective%2520of%2520promoting%2520science%2520and%2520arts.%2520While%250Aassessing%2520the%2520validity%2520and%2520efficacy%2520of%2520the%2520fair%2520use%2520argument%252C%2520we%2520uncover%250Aresearch%2520gaps%2520and%2520potential%2520avenues%2520for%2520future%2520works%2520for%2520researchers%2520and%250Apolicymakers%2520to%2520address.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Fair%20Use%20in%20the%20Generative%20AI%20Supply%20Chain%3A%20A%20Systematized%0A%20%20Literature%20Review&entry.906535625=Amruta%20Mahuli%20and%20Asia%20Biega&entry.1292438233=%20%20Through%20a%20systematization%20of%20generative%20AI%20%28GenAI%29%20stakeholder%20goals%20and%0Aexpectations%2C%20this%20work%20seeks%20to%20uncover%20what%20value%20different%20stakeholders%20see%0Ain%20their%20contributions%20to%20the%20GenAI%20supply%20line.%20This%20valuation%20enables%20us%20to%0Aunderstand%20whether%20fair%20use%20advocated%20by%20GenAI%20companies%20to%20train%20model%0Aprogresses%20the%20copyright%20law%20objective%20of%20promoting%20science%20and%20arts.%20While%0Aassessing%20the%20validity%20and%20efficacy%20of%20the%20fair%20use%20argument%2C%20we%20uncover%0Aresearch%20gaps%20and%20potential%20avenues%20for%20future%20works%20for%20researchers%20and%0Apolicymakers%20to%20address.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00613v1&entry.124074799=Read"},
{"title": "A Cross-Domain Benchmark for Active Learning", "author": "Thorben Werner and Johannes Burchert and Maximilian Stubbemann and Lars Schmidt-Thieme", "abstract": "  Active Learning (AL) deals with identifying the most informative samples for\nlabeling to reduce data annotation costs for supervised learning tasks. AL\nresearch suffers from the fact that lifts from literature generalize poorly and\nthat only a small number of repetitions of experiments are conducted. To\novercome these obstacles, we propose \\emph{CDALBench}, the first active\nlearning benchmark which includes tasks in computer vision, natural language\nprocessing and tabular learning. Furthermore, by providing an efficient, greedy\noracle, \\emph{CDALBench} can be evaluated with 50 runs for each experiment. We\nshow, that both the cross-domain character and a large amount of repetitions\nare crucial for sophisticated evaluation of AL research. Concretely, we show\nthat the superiority of specific methods varies over the different domains,\nmaking it important to evaluate Active Learning with a cross-domain benchmark.\nAdditionally, we show that having a large amount of runs is crucial. With only\nconducting three runs as often done in the literature, the superiority of\nspecific methods can strongly vary with the specific runs. This effect is so\nstrong, that, depending on the seed, even a well-established method's\nperformance can be significantly better and significantly worse than random for\nthe same dataset.\n", "link": "http://arxiv.org/abs/2408.00426v1", "date": "2024-08-01", "relevancy": 1.9094, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.495}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cross-Domain%20Benchmark%20for%20Active%20Learning&body=Title%3A%20A%20Cross-Domain%20Benchmark%20for%20Active%20Learning%0AAuthor%3A%20Thorben%20Werner%20and%20Johannes%20Burchert%20and%20Maximilian%20Stubbemann%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20%20%20Active%20Learning%20%28AL%29%20deals%20with%20identifying%20the%20most%20informative%20samples%20for%0Alabeling%20to%20reduce%20data%20annotation%20costs%20for%20supervised%20learning%20tasks.%20AL%0Aresearch%20suffers%20from%20the%20fact%20that%20lifts%20from%20literature%20generalize%20poorly%20and%0Athat%20only%20a%20small%20number%20of%20repetitions%20of%20experiments%20are%20conducted.%20To%0Aovercome%20these%20obstacles%2C%20we%20propose%20%5Cemph%7BCDALBench%7D%2C%20the%20first%20active%0Alearning%20benchmark%20which%20includes%20tasks%20in%20computer%20vision%2C%20natural%20language%0Aprocessing%20and%20tabular%20learning.%20Furthermore%2C%20by%20providing%20an%20efficient%2C%20greedy%0Aoracle%2C%20%5Cemph%7BCDALBench%7D%20can%20be%20evaluated%20with%2050%20runs%20for%20each%20experiment.%20We%0Ashow%2C%20that%20both%20the%20cross-domain%20character%20and%20a%20large%20amount%20of%20repetitions%0Aare%20crucial%20for%20sophisticated%20evaluation%20of%20AL%20research.%20Concretely%2C%20we%20show%0Athat%20the%20superiority%20of%20specific%20methods%20varies%20over%20the%20different%20domains%2C%0Amaking%20it%20important%20to%20evaluate%20Active%20Learning%20with%20a%20cross-domain%20benchmark.%0AAdditionally%2C%20we%20show%20that%20having%20a%20large%20amount%20of%20runs%20is%20crucial.%20With%20only%0Aconducting%20three%20runs%20as%20often%20done%20in%20the%20literature%2C%20the%20superiority%20of%0Aspecific%20methods%20can%20strongly%20vary%20with%20the%20specific%20runs.%20This%20effect%20is%20so%0Astrong%2C%20that%2C%20depending%20on%20the%20seed%2C%20even%20a%20well-established%20method%27s%0Aperformance%20can%20be%20significantly%20better%20and%20significantly%20worse%20than%20random%20for%0Athe%20same%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cross-Domain%2520Benchmark%2520for%2520Active%2520Learning%26entry.906535625%3DThorben%2520Werner%2520and%2520Johannes%2520Burchert%2520and%2520Maximilian%2520Stubbemann%2520and%2520Lars%2520Schmidt-Thieme%26entry.1292438233%3D%2520%2520Active%2520Learning%2520%2528AL%2529%2520deals%2520with%2520identifying%2520the%2520most%2520informative%2520samples%2520for%250Alabeling%2520to%2520reduce%2520data%2520annotation%2520costs%2520for%2520supervised%2520learning%2520tasks.%2520AL%250Aresearch%2520suffers%2520from%2520the%2520fact%2520that%2520lifts%2520from%2520literature%2520generalize%2520poorly%2520and%250Athat%2520only%2520a%2520small%2520number%2520of%2520repetitions%2520of%2520experiments%2520are%2520conducted.%2520To%250Aovercome%2520these%2520obstacles%252C%2520we%2520propose%2520%255Cemph%257BCDALBench%257D%252C%2520the%2520first%2520active%250Alearning%2520benchmark%2520which%2520includes%2520tasks%2520in%2520computer%2520vision%252C%2520natural%2520language%250Aprocessing%2520and%2520tabular%2520learning.%2520Furthermore%252C%2520by%2520providing%2520an%2520efficient%252C%2520greedy%250Aoracle%252C%2520%255Cemph%257BCDALBench%257D%2520can%2520be%2520evaluated%2520with%252050%2520runs%2520for%2520each%2520experiment.%2520We%250Ashow%252C%2520that%2520both%2520the%2520cross-domain%2520character%2520and%2520a%2520large%2520amount%2520of%2520repetitions%250Aare%2520crucial%2520for%2520sophisticated%2520evaluation%2520of%2520AL%2520research.%2520Concretely%252C%2520we%2520show%250Athat%2520the%2520superiority%2520of%2520specific%2520methods%2520varies%2520over%2520the%2520different%2520domains%252C%250Amaking%2520it%2520important%2520to%2520evaluate%2520Active%2520Learning%2520with%2520a%2520cross-domain%2520benchmark.%250AAdditionally%252C%2520we%2520show%2520that%2520having%2520a%2520large%2520amount%2520of%2520runs%2520is%2520crucial.%2520With%2520only%250Aconducting%2520three%2520runs%2520as%2520often%2520done%2520in%2520the%2520literature%252C%2520the%2520superiority%2520of%250Aspecific%2520methods%2520can%2520strongly%2520vary%2520with%2520the%2520specific%2520runs.%2520This%2520effect%2520is%2520so%250Astrong%252C%2520that%252C%2520depending%2520on%2520the%2520seed%252C%2520even%2520a%2520well-established%2520method%2527s%250Aperformance%2520can%2520be%2520significantly%2520better%2520and%2520significantly%2520worse%2520than%2520random%2520for%250Athe%2520same%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cross-Domain%20Benchmark%20for%20Active%20Learning&entry.906535625=Thorben%20Werner%20and%20Johannes%20Burchert%20and%20Maximilian%20Stubbemann%20and%20Lars%20Schmidt-Thieme&entry.1292438233=%20%20Active%20Learning%20%28AL%29%20deals%20with%20identifying%20the%20most%20informative%20samples%20for%0Alabeling%20to%20reduce%20data%20annotation%20costs%20for%20supervised%20learning%20tasks.%20AL%0Aresearch%20suffers%20from%20the%20fact%20that%20lifts%20from%20literature%20generalize%20poorly%20and%0Athat%20only%20a%20small%20number%20of%20repetitions%20of%20experiments%20are%20conducted.%20To%0Aovercome%20these%20obstacles%2C%20we%20propose%20%5Cemph%7BCDALBench%7D%2C%20the%20first%20active%0Alearning%20benchmark%20which%20includes%20tasks%20in%20computer%20vision%2C%20natural%20language%0Aprocessing%20and%20tabular%20learning.%20Furthermore%2C%20by%20providing%20an%20efficient%2C%20greedy%0Aoracle%2C%20%5Cemph%7BCDALBench%7D%20can%20be%20evaluated%20with%2050%20runs%20for%20each%20experiment.%20We%0Ashow%2C%20that%20both%20the%20cross-domain%20character%20and%20a%20large%20amount%20of%20repetitions%0Aare%20crucial%20for%20sophisticated%20evaluation%20of%20AL%20research.%20Concretely%2C%20we%20show%0Athat%20the%20superiority%20of%20specific%20methods%20varies%20over%20the%20different%20domains%2C%0Amaking%20it%20important%20to%20evaluate%20Active%20Learning%20with%20a%20cross-domain%20benchmark.%0AAdditionally%2C%20we%20show%20that%20having%20a%20large%20amount%20of%20runs%20is%20crucial.%20With%20only%0Aconducting%20three%20runs%20as%20often%20done%20in%20the%20literature%2C%20the%20superiority%20of%0Aspecific%20methods%20can%20strongly%20vary%20with%20the%20specific%20runs.%20This%20effect%20is%20so%0Astrong%2C%20that%2C%20depending%20on%20the%20seed%2C%20even%20a%20well-established%20method%27s%0Aperformance%20can%20be%20significantly%20better%20and%20significantly%20worse%20than%20random%20for%0Athe%20same%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00426v1&entry.124074799=Read"},
{"title": "Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks", "author": "Xianliang Xu and Ting Du and Wang Kong and Ye Li and Zhongyi Huang", "abstract": "  First-order methods, such as gradient descent (GD) and stochastic gradient\ndescent (SGD) have been proven effective in training neural networks. In the\nsetting of over-parameterization, there is a line of work demonstrating that\nrandomly initialized (stochastic) gradient descent converges to a globally\noptimal solution at a linear convergence rate for the quadratic loss function.\nHowever, the learning rate of GD in training two-layer neural networks has a\npoor dependence on the sample size and the Gram matrix, resulting in a slow\ntraining process. In this paper, we show that for the $L^2$ regression\nproblems, the learning rate can be improved from $\\mathcal{O}(\\lambda_0/n^2)$\nto $\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, which implies that GD enjoys a\nfaster convergence rate. Moreover, we further generalize the method for GD in\ntraining two-layer Physics-Informed Neural Networks (PINNs), showing a similar\nimprovement for the learning rate. Although the improved learning rate depends\nmildly on the Gram matrix, we still need to set it small enough in practice due\nto the agnostic eigenvalues of the Gram matrix. More importantly, the\nconvergence rate relies on the least eigenvalue of the Gram matrix, leading to\nslow convergence. In this work, we provide the convergence analysis of natural\ngradient descent (NGD) in training two-layer PINNs. We show that the learning\nrate can be $\\mathcal{O}(1)$ and at this time, the convergence rate is\nindependent of the Gram matrix.\n", "link": "http://arxiv.org/abs/2408.00573v1", "date": "2024-08-01", "relevancy": 1.9089, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4697}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Ye%20Li%20and%20Zhongyi%20Huang%0AAbstract%3A%20%20%20First-order%20methods%2C%20such%20as%20gradient%20descent%20%28GD%29%20and%20stochastic%20gradient%0Adescent%20%28SGD%29%20have%20been%20proven%20effective%20in%20training%20neural%20networks.%20In%20the%0Asetting%20of%20over-parameterization%2C%20there%20is%20a%20line%20of%20work%20demonstrating%20that%0Arandomly%20initialized%20%28stochastic%29%20gradient%20descent%20converges%20to%20a%20globally%0Aoptimal%20solution%20at%20a%20linear%20convergence%20rate%20for%20the%20quadratic%20loss%20function.%0AHowever%2C%20the%20learning%20rate%20of%20GD%20in%20training%20two-layer%20neural%20networks%20has%20a%0Apoor%20dependence%20on%20the%20sample%20size%20and%20the%20Gram%20matrix%2C%20resulting%20in%20a%20slow%0Atraining%20process.%20In%20this%20paper%2C%20we%20show%20that%20for%20the%20%24L%5E2%24%20regression%0Aproblems%2C%20the%20learning%20rate%20can%20be%20improved%20from%20%24%5Cmathcal%7BO%7D%28%5Clambda_0/n%5E2%29%24%0Ato%20%24%5Cmathcal%7BO%7D%281/%5C%7C%5Cbm%7BH%7D%5E%7B%5Cinfty%7D%5C%7C_2%29%24%2C%20which%20implies%20that%20GD%20enjoys%20a%0Afaster%20convergence%20rate.%20Moreover%2C%20we%20further%20generalize%20the%20method%20for%20GD%20in%0Atraining%20two-layer%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20showing%20a%20similar%0Aimprovement%20for%20the%20learning%20rate.%20Although%20the%20improved%20learning%20rate%20depends%0Amildly%20on%20the%20Gram%20matrix%2C%20we%20still%20need%20to%20set%20it%20small%20enough%20in%20practice%20due%0Ato%20the%20agnostic%20eigenvalues%20of%20the%20Gram%20matrix.%20More%20importantly%2C%20the%0Aconvergence%20rate%20relies%20on%20the%20least%20eigenvalue%20of%20the%20Gram%20matrix%2C%20leading%20to%0Aslow%20convergence.%20In%20this%20work%2C%20we%20provide%20the%20convergence%20analysis%20of%20natural%0Agradient%20descent%20%28NGD%29%20in%20training%20two-layer%20PINNs.%20We%20show%20that%20the%20learning%0Arate%20can%20be%20%24%5Cmathcal%7BO%7D%281%29%24%20and%20at%20this%20time%2C%20the%20convergence%20rate%20is%0Aindependent%20of%20the%20Gram%20matrix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00573v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520Analysis%2520of%2520Natural%2520Gradient%2520Descent%2520for%2520Over-parameterized%250A%2520%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DXianliang%2520Xu%2520and%2520Ting%2520Du%2520and%2520Wang%2520Kong%2520and%2520Ye%2520Li%2520and%2520Zhongyi%2520Huang%26entry.1292438233%3D%2520%2520First-order%2520methods%252C%2520such%2520as%2520gradient%2520descent%2520%2528GD%2529%2520and%2520stochastic%2520gradient%250Adescent%2520%2528SGD%2529%2520have%2520been%2520proven%2520effective%2520in%2520training%2520neural%2520networks.%2520In%2520the%250Asetting%2520of%2520over-parameterization%252C%2520there%2520is%2520a%2520line%2520of%2520work%2520demonstrating%2520that%250Arandomly%2520initialized%2520%2528stochastic%2529%2520gradient%2520descent%2520converges%2520to%2520a%2520globally%250Aoptimal%2520solution%2520at%2520a%2520linear%2520convergence%2520rate%2520for%2520the%2520quadratic%2520loss%2520function.%250AHowever%252C%2520the%2520learning%2520rate%2520of%2520GD%2520in%2520training%2520two-layer%2520neural%2520networks%2520has%2520a%250Apoor%2520dependence%2520on%2520the%2520sample%2520size%2520and%2520the%2520Gram%2520matrix%252C%2520resulting%2520in%2520a%2520slow%250Atraining%2520process.%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520for%2520the%2520%2524L%255E2%2524%2520regression%250Aproblems%252C%2520the%2520learning%2520rate%2520can%2520be%2520improved%2520from%2520%2524%255Cmathcal%257BO%257D%2528%255Clambda_0/n%255E2%2529%2524%250Ato%2520%2524%255Cmathcal%257BO%257D%25281/%255C%257C%255Cbm%257BH%257D%255E%257B%255Cinfty%257D%255C%257C_2%2529%2524%252C%2520which%2520implies%2520that%2520GD%2520enjoys%2520a%250Afaster%2520convergence%2520rate.%2520Moreover%252C%2520we%2520further%2520generalize%2520the%2520method%2520for%2520GD%2520in%250Atraining%2520two-layer%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%252C%2520showing%2520a%2520similar%250Aimprovement%2520for%2520the%2520learning%2520rate.%2520Although%2520the%2520improved%2520learning%2520rate%2520depends%250Amildly%2520on%2520the%2520Gram%2520matrix%252C%2520we%2520still%2520need%2520to%2520set%2520it%2520small%2520enough%2520in%2520practice%2520due%250Ato%2520the%2520agnostic%2520eigenvalues%2520of%2520the%2520Gram%2520matrix.%2520More%2520importantly%252C%2520the%250Aconvergence%2520rate%2520relies%2520on%2520the%2520least%2520eigenvalue%2520of%2520the%2520Gram%2520matrix%252C%2520leading%2520to%250Aslow%2520convergence.%2520In%2520this%2520work%252C%2520we%2520provide%2520the%2520convergence%2520analysis%2520of%2520natural%250Agradient%2520descent%2520%2528NGD%2529%2520in%2520training%2520two-layer%2520PINNs.%2520We%2520show%2520that%2520the%2520learning%250Arate%2520can%2520be%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%2520and%2520at%2520this%2520time%252C%2520the%2520convergence%2520rate%2520is%250Aindependent%2520of%2520the%2520Gram%2520matrix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00573v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Analysis%20of%20Natural%20Gradient%20Descent%20for%20Over-parameterized%0A%20%20Physics-Informed%20Neural%20Networks&entry.906535625=Xianliang%20Xu%20and%20Ting%20Du%20and%20Wang%20Kong%20and%20Ye%20Li%20and%20Zhongyi%20Huang&entry.1292438233=%20%20First-order%20methods%2C%20such%20as%20gradient%20descent%20%28GD%29%20and%20stochastic%20gradient%0Adescent%20%28SGD%29%20have%20been%20proven%20effective%20in%20training%20neural%20networks.%20In%20the%0Asetting%20of%20over-parameterization%2C%20there%20is%20a%20line%20of%20work%20demonstrating%20that%0Arandomly%20initialized%20%28stochastic%29%20gradient%20descent%20converges%20to%20a%20globally%0Aoptimal%20solution%20at%20a%20linear%20convergence%20rate%20for%20the%20quadratic%20loss%20function.%0AHowever%2C%20the%20learning%20rate%20of%20GD%20in%20training%20two-layer%20neural%20networks%20has%20a%0Apoor%20dependence%20on%20the%20sample%20size%20and%20the%20Gram%20matrix%2C%20resulting%20in%20a%20slow%0Atraining%20process.%20In%20this%20paper%2C%20we%20show%20that%20for%20the%20%24L%5E2%24%20regression%0Aproblems%2C%20the%20learning%20rate%20can%20be%20improved%20from%20%24%5Cmathcal%7BO%7D%28%5Clambda_0/n%5E2%29%24%0Ato%20%24%5Cmathcal%7BO%7D%281/%5C%7C%5Cbm%7BH%7D%5E%7B%5Cinfty%7D%5C%7C_2%29%24%2C%20which%20implies%20that%20GD%20enjoys%20a%0Afaster%20convergence%20rate.%20Moreover%2C%20we%20further%20generalize%20the%20method%20for%20GD%20in%0Atraining%20two-layer%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%2C%20showing%20a%20similar%0Aimprovement%20for%20the%20learning%20rate.%20Although%20the%20improved%20learning%20rate%20depends%0Amildly%20on%20the%20Gram%20matrix%2C%20we%20still%20need%20to%20set%20it%20small%20enough%20in%20practice%20due%0Ato%20the%20agnostic%20eigenvalues%20of%20the%20Gram%20matrix.%20More%20importantly%2C%20the%0Aconvergence%20rate%20relies%20on%20the%20least%20eigenvalue%20of%20the%20Gram%20matrix%2C%20leading%20to%0Aslow%20convergence.%20In%20this%20work%2C%20we%20provide%20the%20convergence%20analysis%20of%20natural%0Agradient%20descent%20%28NGD%29%20in%20training%20two-layer%20PINNs.%20We%20show%20that%20the%20learning%0Arate%20can%20be%20%24%5Cmathcal%7BO%7D%281%29%24%20and%20at%20this%20time%2C%20the%20convergence%20rate%20is%0Aindependent%20of%20the%20Gram%20matrix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00573v1&entry.124074799=Read"},
{"title": "Infrequent Resolving Algorithm for Online Linear Programming", "author": "Guokai Li and Zizhuo Wang and Jingwei Zhang", "abstract": "  Online linear programming (OLP) has gained significant attention from both\nresearchers and practitioners due to its extensive applications, such as online\nauction, network revenue management and advertising. Existing OLP algorithms\nfall into two categories: LP-based algorithms and LP-free algorithms. The\nformer one typically guarantees better performance, even offering a constant\nregret, but requires solving a large number of LPs, which could be\ncomputationally expensive. In contrast, LP-free algorithm only requires\nfirst-order computations but induces a worse performance, lacking a constant\nregret bound. In this work, we bridge the gap between these two extremes by\nproposing an algorithm that achieves a constant regret while solving LPs only\n$O(\\log\\log T)$ times over the time horizon $T$. Moreover, when we are allowed\nto solve LPs only $M$ times, we propose an algorithm that can guarantee an\n$O\\left(T^{(1/2+\\epsilon)^{M-1}}\\right)$ regret. Furthermore, when the arrival\nprobabilities are known at the beginning, our algorithm can guarantee a\nconstant regret by solving LPs $O(\\log\\log T)$ times, and an\n$O\\left(T^{(1/2+\\epsilon)^{M}}\\right)$ regret by solving LPs only $M$ times.\nNumerical experiments are conducted to demonstrate the efficiency of the\nproposed algorithms.\n", "link": "http://arxiv.org/abs/2408.00465v1", "date": "2024-08-01", "relevancy": 1.9058, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3921}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3787}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infrequent%20Resolving%20Algorithm%20for%20Online%20Linear%20Programming&body=Title%3A%20Infrequent%20Resolving%20Algorithm%20for%20Online%20Linear%20Programming%0AAuthor%3A%20Guokai%20Li%20and%20Zizhuo%20Wang%20and%20Jingwei%20Zhang%0AAbstract%3A%20%20%20Online%20linear%20programming%20%28OLP%29%20has%20gained%20significant%20attention%20from%20both%0Aresearchers%20and%20practitioners%20due%20to%20its%20extensive%20applications%2C%20such%20as%20online%0Aauction%2C%20network%20revenue%20management%20and%20advertising.%20Existing%20OLP%20algorithms%0Afall%20into%20two%20categories%3A%20LP-based%20algorithms%20and%20LP-free%20algorithms.%20The%0Aformer%20one%20typically%20guarantees%20better%20performance%2C%20even%20offering%20a%20constant%0Aregret%2C%20but%20requires%20solving%20a%20large%20number%20of%20LPs%2C%20which%20could%20be%0Acomputationally%20expensive.%20In%20contrast%2C%20LP-free%20algorithm%20only%20requires%0Afirst-order%20computations%20but%20induces%20a%20worse%20performance%2C%20lacking%20a%20constant%0Aregret%20bound.%20In%20this%20work%2C%20we%20bridge%20the%20gap%20between%20these%20two%20extremes%20by%0Aproposing%20an%20algorithm%20that%20achieves%20a%20constant%20regret%20while%20solving%20LPs%20only%0A%24O%28%5Clog%5Clog%20T%29%24%20times%20over%20the%20time%20horizon%20%24T%24.%20Moreover%2C%20when%20we%20are%20allowed%0Ato%20solve%20LPs%20only%20%24M%24%20times%2C%20we%20propose%20an%20algorithm%20that%20can%20guarantee%20an%0A%24O%5Cleft%28T%5E%7B%281/2%2B%5Cepsilon%29%5E%7BM-1%7D%7D%5Cright%29%24%20regret.%20Furthermore%2C%20when%20the%20arrival%0Aprobabilities%20are%20known%20at%20the%20beginning%2C%20our%20algorithm%20can%20guarantee%20a%0Aconstant%20regret%20by%20solving%20LPs%20%24O%28%5Clog%5Clog%20T%29%24%20times%2C%20and%20an%0A%24O%5Cleft%28T%5E%7B%281/2%2B%5Cepsilon%29%5E%7BM%7D%7D%5Cright%29%24%20regret%20by%20solving%20LPs%20only%20%24M%24%20times.%0ANumerical%20experiments%20are%20conducted%20to%20demonstrate%20the%20efficiency%20of%20the%0Aproposed%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfrequent%2520Resolving%2520Algorithm%2520for%2520Online%2520Linear%2520Programming%26entry.906535625%3DGuokai%2520Li%2520and%2520Zizhuo%2520Wang%2520and%2520Jingwei%2520Zhang%26entry.1292438233%3D%2520%2520Online%2520linear%2520programming%2520%2528OLP%2529%2520has%2520gained%2520significant%2520attention%2520from%2520both%250Aresearchers%2520and%2520practitioners%2520due%2520to%2520its%2520extensive%2520applications%252C%2520such%2520as%2520online%250Aauction%252C%2520network%2520revenue%2520management%2520and%2520advertising.%2520Existing%2520OLP%2520algorithms%250Afall%2520into%2520two%2520categories%253A%2520LP-based%2520algorithms%2520and%2520LP-free%2520algorithms.%2520The%250Aformer%2520one%2520typically%2520guarantees%2520better%2520performance%252C%2520even%2520offering%2520a%2520constant%250Aregret%252C%2520but%2520requires%2520solving%2520a%2520large%2520number%2520of%2520LPs%252C%2520which%2520could%2520be%250Acomputationally%2520expensive.%2520In%2520contrast%252C%2520LP-free%2520algorithm%2520only%2520requires%250Afirst-order%2520computations%2520but%2520induces%2520a%2520worse%2520performance%252C%2520lacking%2520a%2520constant%250Aregret%2520bound.%2520In%2520this%2520work%252C%2520we%2520bridge%2520the%2520gap%2520between%2520these%2520two%2520extremes%2520by%250Aproposing%2520an%2520algorithm%2520that%2520achieves%2520a%2520constant%2520regret%2520while%2520solving%2520LPs%2520only%250A%2524O%2528%255Clog%255Clog%2520T%2529%2524%2520times%2520over%2520the%2520time%2520horizon%2520%2524T%2524.%2520Moreover%252C%2520when%2520we%2520are%2520allowed%250Ato%2520solve%2520LPs%2520only%2520%2524M%2524%2520times%252C%2520we%2520propose%2520an%2520algorithm%2520that%2520can%2520guarantee%2520an%250A%2524O%255Cleft%2528T%255E%257B%25281/2%252B%255Cepsilon%2529%255E%257BM-1%257D%257D%255Cright%2529%2524%2520regret.%2520Furthermore%252C%2520when%2520the%2520arrival%250Aprobabilities%2520are%2520known%2520at%2520the%2520beginning%252C%2520our%2520algorithm%2520can%2520guarantee%2520a%250Aconstant%2520regret%2520by%2520solving%2520LPs%2520%2524O%2528%255Clog%255Clog%2520T%2529%2524%2520times%252C%2520and%2520an%250A%2524O%255Cleft%2528T%255E%257B%25281/2%252B%255Cepsilon%2529%255E%257BM%257D%257D%255Cright%2529%2524%2520regret%2520by%2520solving%2520LPs%2520only%2520%2524M%2524%2520times.%250ANumerical%2520experiments%2520are%2520conducted%2520to%2520demonstrate%2520the%2520efficiency%2520of%2520the%250Aproposed%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infrequent%20Resolving%20Algorithm%20for%20Online%20Linear%20Programming&entry.906535625=Guokai%20Li%20and%20Zizhuo%20Wang%20and%20Jingwei%20Zhang&entry.1292438233=%20%20Online%20linear%20programming%20%28OLP%29%20has%20gained%20significant%20attention%20from%20both%0Aresearchers%20and%20practitioners%20due%20to%20its%20extensive%20applications%2C%20such%20as%20online%0Aauction%2C%20network%20revenue%20management%20and%20advertising.%20Existing%20OLP%20algorithms%0Afall%20into%20two%20categories%3A%20LP-based%20algorithms%20and%20LP-free%20algorithms.%20The%0Aformer%20one%20typically%20guarantees%20better%20performance%2C%20even%20offering%20a%20constant%0Aregret%2C%20but%20requires%20solving%20a%20large%20number%20of%20LPs%2C%20which%20could%20be%0Acomputationally%20expensive.%20In%20contrast%2C%20LP-free%20algorithm%20only%20requires%0Afirst-order%20computations%20but%20induces%20a%20worse%20performance%2C%20lacking%20a%20constant%0Aregret%20bound.%20In%20this%20work%2C%20we%20bridge%20the%20gap%20between%20these%20two%20extremes%20by%0Aproposing%20an%20algorithm%20that%20achieves%20a%20constant%20regret%20while%20solving%20LPs%20only%0A%24O%28%5Clog%5Clog%20T%29%24%20times%20over%20the%20time%20horizon%20%24T%24.%20Moreover%2C%20when%20we%20are%20allowed%0Ato%20solve%20LPs%20only%20%24M%24%20times%2C%20we%20propose%20an%20algorithm%20that%20can%20guarantee%20an%0A%24O%5Cleft%28T%5E%7B%281/2%2B%5Cepsilon%29%5E%7BM-1%7D%7D%5Cright%29%24%20regret.%20Furthermore%2C%20when%20the%20arrival%0Aprobabilities%20are%20known%20at%20the%20beginning%2C%20our%20algorithm%20can%20guarantee%20a%0Aconstant%20regret%20by%20solving%20LPs%20%24O%28%5Clog%5Clog%20T%29%24%20times%2C%20and%20an%0A%24O%5Cleft%28T%5E%7B%281/2%2B%5Cepsilon%29%5E%7BM%7D%7D%5Cright%29%24%20regret%20by%20solving%20LPs%20only%20%24M%24%20times.%0ANumerical%20experiments%20are%20conducted%20to%20demonstrate%20the%20efficiency%20of%20the%0Aproposed%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00465v1&entry.124074799=Read"},
{"title": "Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM", "author": "Xiaofeng Liu and Jonghye Woo and Chao Ma and Jinsong Ouyang and Georges El Fakhri", "abstract": "  Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.\n", "link": "http://arxiv.org/abs/2408.00706v1", "date": "2024-08-01", "relevancy": 1.9034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4812}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4766}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-supervised%20Brain%20Tumor%20Segmentation%20with%20Box-prompted%20MedSAM&body=Title%3A%20Point-supervised%20Brain%20Tumor%20Segmentation%20with%20Box-prompted%20MedSAM%0AAuthor%3A%20Xiaofeng%20Liu%20and%20Jonghye%20Woo%20and%20Chao%20Ma%20and%20Jinsong%20Ouyang%20and%20Georges%20El%20Fakhri%0AAbstract%3A%20%20%20Delineating%20lesions%20and%20anatomical%20structure%20is%20important%20for%20image-guided%0Ainterventions.%20Point-supervised%20medical%20image%20segmentation%20%28PSS%29%20has%20great%0Apotential%20to%20alleviate%20costly%20expert%20delineation%20labeling.%20However%2C%20due%20to%20the%0Alack%20of%20precise%20size%20and%20boundary%20guidance%2C%20the%20effectiveness%20of%20PSS%20often%0Afalls%20short%20of%20expectations.%20Although%20recent%20vision%20foundational%20models%2C%20such%0Aas%20the%20medical%20segment%20anything%20model%20%28MedSAM%29%2C%20have%20made%20significant%0Aadvancements%20in%20bounding-box-prompted%20segmentation%2C%20it%20is%20not%20straightforward%0Ato%20utilize%20point%20annotation%2C%20and%20is%20prone%20to%20semantic%20ambiguity.%20In%20this%0Apreliminary%20study%2C%20we%20introduce%20an%20iterative%20framework%20to%20facilitate%0Asemantic-aware%20point-supervised%20MedSAM.%20Specifically%2C%20the%20semantic%20box-prompt%0Agenerator%20%28SBPG%29%20module%20has%20the%20capacity%20to%20convert%20the%20point%20input%20into%0Apotential%20pseudo%20bounding%20box%20suggestions%2C%20which%20are%20explicitly%20refined%20by%20the%0Aprototype-based%20semantic%20similarity.%20This%20is%20then%20succeeded%20by%20a%20prompt-guided%0Aspatial%20refinement%20%28PGSR%29%20module%20that%20harnesses%20the%20exceptional%0Ageneralizability%20of%20MedSAM%20to%20infer%20the%20segmentation%20mask%2C%20which%20also%20updates%0Athe%20box%20proposal%20seed%20in%20SBPG.%20Performance%20can%20be%20progressively%20improved%20with%0Aadequate%20iterations.%20We%20conducted%20an%20evaluation%20on%20BraTS2018%20for%20the%0Asegmentation%20of%20whole%20brain%20tumors%20and%20demonstrated%20its%20superior%20performance%0Acompared%20to%20traditional%20PSS%20methods%20and%20on%20par%20with%20box-supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-supervised%2520Brain%2520Tumor%2520Segmentation%2520with%2520Box-prompted%2520MedSAM%26entry.906535625%3DXiaofeng%2520Liu%2520and%2520Jonghye%2520Woo%2520and%2520Chao%2520Ma%2520and%2520Jinsong%2520Ouyang%2520and%2520Georges%2520El%2520Fakhri%26entry.1292438233%3D%2520%2520Delineating%2520lesions%2520and%2520anatomical%2520structure%2520is%2520important%2520for%2520image-guided%250Ainterventions.%2520Point-supervised%2520medical%2520image%2520segmentation%2520%2528PSS%2529%2520has%2520great%250Apotential%2520to%2520alleviate%2520costly%2520expert%2520delineation%2520labeling.%2520However%252C%2520due%2520to%2520the%250Alack%2520of%2520precise%2520size%2520and%2520boundary%2520guidance%252C%2520the%2520effectiveness%2520of%2520PSS%2520often%250Afalls%2520short%2520of%2520expectations.%2520Although%2520recent%2520vision%2520foundational%2520models%252C%2520such%250Aas%2520the%2520medical%2520segment%2520anything%2520model%2520%2528MedSAM%2529%252C%2520have%2520made%2520significant%250Aadvancements%2520in%2520bounding-box-prompted%2520segmentation%252C%2520it%2520is%2520not%2520straightforward%250Ato%2520utilize%2520point%2520annotation%252C%2520and%2520is%2520prone%2520to%2520semantic%2520ambiguity.%2520In%2520this%250Apreliminary%2520study%252C%2520we%2520introduce%2520an%2520iterative%2520framework%2520to%2520facilitate%250Asemantic-aware%2520point-supervised%2520MedSAM.%2520Specifically%252C%2520the%2520semantic%2520box-prompt%250Agenerator%2520%2528SBPG%2529%2520module%2520has%2520the%2520capacity%2520to%2520convert%2520the%2520point%2520input%2520into%250Apotential%2520pseudo%2520bounding%2520box%2520suggestions%252C%2520which%2520are%2520explicitly%2520refined%2520by%2520the%250Aprototype-based%2520semantic%2520similarity.%2520This%2520is%2520then%2520succeeded%2520by%2520a%2520prompt-guided%250Aspatial%2520refinement%2520%2528PGSR%2529%2520module%2520that%2520harnesses%2520the%2520exceptional%250Ageneralizability%2520of%2520MedSAM%2520to%2520infer%2520the%2520segmentation%2520mask%252C%2520which%2520also%2520updates%250Athe%2520box%2520proposal%2520seed%2520in%2520SBPG.%2520Performance%2520can%2520be%2520progressively%2520improved%2520with%250Aadequate%2520iterations.%2520We%2520conducted%2520an%2520evaluation%2520on%2520BraTS2018%2520for%2520the%250Asegmentation%2520of%2520whole%2520brain%2520tumors%2520and%2520demonstrated%2520its%2520superior%2520performance%250Acompared%2520to%2520traditional%2520PSS%2520methods%2520and%2520on%2520par%2520with%2520box-supervised%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-supervised%20Brain%20Tumor%20Segmentation%20with%20Box-prompted%20MedSAM&entry.906535625=Xiaofeng%20Liu%20and%20Jonghye%20Woo%20and%20Chao%20Ma%20and%20Jinsong%20Ouyang%20and%20Georges%20El%20Fakhri&entry.1292438233=%20%20Delineating%20lesions%20and%20anatomical%20structure%20is%20important%20for%20image-guided%0Ainterventions.%20Point-supervised%20medical%20image%20segmentation%20%28PSS%29%20has%20great%0Apotential%20to%20alleviate%20costly%20expert%20delineation%20labeling.%20However%2C%20due%20to%20the%0Alack%20of%20precise%20size%20and%20boundary%20guidance%2C%20the%20effectiveness%20of%20PSS%20often%0Afalls%20short%20of%20expectations.%20Although%20recent%20vision%20foundational%20models%2C%20such%0Aas%20the%20medical%20segment%20anything%20model%20%28MedSAM%29%2C%20have%20made%20significant%0Aadvancements%20in%20bounding-box-prompted%20segmentation%2C%20it%20is%20not%20straightforward%0Ato%20utilize%20point%20annotation%2C%20and%20is%20prone%20to%20semantic%20ambiguity.%20In%20this%0Apreliminary%20study%2C%20we%20introduce%20an%20iterative%20framework%20to%20facilitate%0Asemantic-aware%20point-supervised%20MedSAM.%20Specifically%2C%20the%20semantic%20box-prompt%0Agenerator%20%28SBPG%29%20module%20has%20the%20capacity%20to%20convert%20the%20point%20input%20into%0Apotential%20pseudo%20bounding%20box%20suggestions%2C%20which%20are%20explicitly%20refined%20by%20the%0Aprototype-based%20semantic%20similarity.%20This%20is%20then%20succeeded%20by%20a%20prompt-guided%0Aspatial%20refinement%20%28PGSR%29%20module%20that%20harnesses%20the%20exceptional%0Ageneralizability%20of%20MedSAM%20to%20infer%20the%20segmentation%20mask%2C%20which%20also%20updates%0Athe%20box%20proposal%20seed%20in%20SBPG.%20Performance%20can%20be%20progressively%20improved%20with%0Aadequate%20iterations.%20We%20conducted%20an%20evaluation%20on%20BraTS2018%20for%20the%0Asegmentation%20of%20whole%20brain%20tumors%20and%20demonstrated%20its%20superior%20performance%0Acompared%20to%20traditional%20PSS%20methods%20and%20on%20par%20with%20box-supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00706v1&entry.124074799=Read"},
{"title": "HBot: A Chatbot for Healthcare Applications in Traditional Chinese\n  Medicine Based on Human Body 3D Visualization", "author": "Bolin Zhang and Zhiwei Yi and Jiahao Wang and Dianbo Sui and Zhiying Tu and Dianhui Chu", "abstract": "  The unique diagnosis and treatment techniques and remarkable clinical\nefficacy of traditional Chinese medicine (TCM) make it play an important role\nin the field of elderly care and healthcare, especially in the rehabilitation\nof some common chronic diseases of the elderly. Therefore, building a TCM\nchatbot for healthcare application will help users obtain consultation services\nin a direct and natural way. However, concepts such as acupuncture points\n(acupoints) and meridians involved in TCM always appear in the consultation,\nwhich cannot be displayed intuitively. To this end, we develop a\n\\textbf{h}ealthcare chat\\textbf{bot} (HBot) based on a human body model in 3D\nand knowledge graph, which provides conversational services such as knowledge\nQ\\&A, prescription recommendation, moxibustion therapy recommendation, and\nacupoint search. When specific acupoints are involved in the conversations\nbetween user and HBot, the 3D body will jump to the corresponding acupoints and\nhighlight them. Moreover, Hbot can also be used in training scenarios to\naccelerate the teaching process of TCM by intuitively displaying acupuncture\npoints and knowledge cards. The demonstration video is available at\nhttps://www.youtube.com/watch?v=UhQhutSKkTU . Our code and dataset are publicly\navailable at Gitee: https://gitee.com/plabrolin/interactive-3d-acup.git\n", "link": "http://arxiv.org/abs/2408.00481v1", "date": "2024-08-01", "relevancy": 1.8972, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4786}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4743}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HBot%3A%20A%20Chatbot%20for%20Healthcare%20Applications%20in%20Traditional%20Chinese%0A%20%20Medicine%20Based%20on%20Human%20Body%203D%20Visualization&body=Title%3A%20HBot%3A%20A%20Chatbot%20for%20Healthcare%20Applications%20in%20Traditional%20Chinese%0A%20%20Medicine%20Based%20on%20Human%20Body%203D%20Visualization%0AAuthor%3A%20Bolin%20Zhang%20and%20Zhiwei%20Yi%20and%20Jiahao%20Wang%20and%20Dianbo%20Sui%20and%20Zhiying%20Tu%20and%20Dianhui%20Chu%0AAbstract%3A%20%20%20The%20unique%20diagnosis%20and%20treatment%20techniques%20and%20remarkable%20clinical%0Aefficacy%20of%20traditional%20Chinese%20medicine%20%28TCM%29%20make%20it%20play%20an%20important%20role%0Ain%20the%20field%20of%20elderly%20care%20and%20healthcare%2C%20especially%20in%20the%20rehabilitation%0Aof%20some%20common%20chronic%20diseases%20of%20the%20elderly.%20Therefore%2C%20building%20a%20TCM%0Achatbot%20for%20healthcare%20application%20will%20help%20users%20obtain%20consultation%20services%0Ain%20a%20direct%20and%20natural%20way.%20However%2C%20concepts%20such%20as%20acupuncture%20points%0A%28acupoints%29%20and%20meridians%20involved%20in%20TCM%20always%20appear%20in%20the%20consultation%2C%0Awhich%20cannot%20be%20displayed%20intuitively.%20To%20this%20end%2C%20we%20develop%20a%0A%5Ctextbf%7Bh%7Dealthcare%20chat%5Ctextbf%7Bbot%7D%20%28HBot%29%20based%20on%20a%20human%20body%20model%20in%203D%0Aand%20knowledge%20graph%2C%20which%20provides%20conversational%20services%20such%20as%20knowledge%0AQ%5C%26A%2C%20prescription%20recommendation%2C%20moxibustion%20therapy%20recommendation%2C%20and%0Aacupoint%20search.%20When%20specific%20acupoints%20are%20involved%20in%20the%20conversations%0Abetween%20user%20and%20HBot%2C%20the%203D%20body%20will%20jump%20to%20the%20corresponding%20acupoints%20and%0Ahighlight%20them.%20Moreover%2C%20Hbot%20can%20also%20be%20used%20in%20training%20scenarios%20to%0Aaccelerate%20the%20teaching%20process%20of%20TCM%20by%20intuitively%20displaying%20acupuncture%0Apoints%20and%20knowledge%20cards.%20The%20demonstration%20video%20is%20available%20at%0Ahttps%3A//www.youtube.com/watch%3Fv%3DUhQhutSKkTU%20.%20Our%20code%20and%20dataset%20are%20publicly%0Aavailable%20at%20Gitee%3A%20https%3A//gitee.com/plabrolin/interactive-3d-acup.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHBot%253A%2520A%2520Chatbot%2520for%2520Healthcare%2520Applications%2520in%2520Traditional%2520Chinese%250A%2520%2520Medicine%2520Based%2520on%2520Human%2520Body%25203D%2520Visualization%26entry.906535625%3DBolin%2520Zhang%2520and%2520Zhiwei%2520Yi%2520and%2520Jiahao%2520Wang%2520and%2520Dianbo%2520Sui%2520and%2520Zhiying%2520Tu%2520and%2520Dianhui%2520Chu%26entry.1292438233%3D%2520%2520The%2520unique%2520diagnosis%2520and%2520treatment%2520techniques%2520and%2520remarkable%2520clinical%250Aefficacy%2520of%2520traditional%2520Chinese%2520medicine%2520%2528TCM%2529%2520make%2520it%2520play%2520an%2520important%2520role%250Ain%2520the%2520field%2520of%2520elderly%2520care%2520and%2520healthcare%252C%2520especially%2520in%2520the%2520rehabilitation%250Aof%2520some%2520common%2520chronic%2520diseases%2520of%2520the%2520elderly.%2520Therefore%252C%2520building%2520a%2520TCM%250Achatbot%2520for%2520healthcare%2520application%2520will%2520help%2520users%2520obtain%2520consultation%2520services%250Ain%2520a%2520direct%2520and%2520natural%2520way.%2520However%252C%2520concepts%2520such%2520as%2520acupuncture%2520points%250A%2528acupoints%2529%2520and%2520meridians%2520involved%2520in%2520TCM%2520always%2520appear%2520in%2520the%2520consultation%252C%250Awhich%2520cannot%2520be%2520displayed%2520intuitively.%2520To%2520this%2520end%252C%2520we%2520develop%2520a%250A%255Ctextbf%257Bh%257Dealthcare%2520chat%255Ctextbf%257Bbot%257D%2520%2528HBot%2529%2520based%2520on%2520a%2520human%2520body%2520model%2520in%25203D%250Aand%2520knowledge%2520graph%252C%2520which%2520provides%2520conversational%2520services%2520such%2520as%2520knowledge%250AQ%255C%2526A%252C%2520prescription%2520recommendation%252C%2520moxibustion%2520therapy%2520recommendation%252C%2520and%250Aacupoint%2520search.%2520When%2520specific%2520acupoints%2520are%2520involved%2520in%2520the%2520conversations%250Abetween%2520user%2520and%2520HBot%252C%2520the%25203D%2520body%2520will%2520jump%2520to%2520the%2520corresponding%2520acupoints%2520and%250Ahighlight%2520them.%2520Moreover%252C%2520Hbot%2520can%2520also%2520be%2520used%2520in%2520training%2520scenarios%2520to%250Aaccelerate%2520the%2520teaching%2520process%2520of%2520TCM%2520by%2520intuitively%2520displaying%2520acupuncture%250Apoints%2520and%2520knowledge%2520cards.%2520The%2520demonstration%2520video%2520is%2520available%2520at%250Ahttps%253A//www.youtube.com/watch%253Fv%253DUhQhutSKkTU%2520.%2520Our%2520code%2520and%2520dataset%2520are%2520publicly%250Aavailable%2520at%2520Gitee%253A%2520https%253A//gitee.com/plabrolin/interactive-3d-acup.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HBot%3A%20A%20Chatbot%20for%20Healthcare%20Applications%20in%20Traditional%20Chinese%0A%20%20Medicine%20Based%20on%20Human%20Body%203D%20Visualization&entry.906535625=Bolin%20Zhang%20and%20Zhiwei%20Yi%20and%20Jiahao%20Wang%20and%20Dianbo%20Sui%20and%20Zhiying%20Tu%20and%20Dianhui%20Chu&entry.1292438233=%20%20The%20unique%20diagnosis%20and%20treatment%20techniques%20and%20remarkable%20clinical%0Aefficacy%20of%20traditional%20Chinese%20medicine%20%28TCM%29%20make%20it%20play%20an%20important%20role%0Ain%20the%20field%20of%20elderly%20care%20and%20healthcare%2C%20especially%20in%20the%20rehabilitation%0Aof%20some%20common%20chronic%20diseases%20of%20the%20elderly.%20Therefore%2C%20building%20a%20TCM%0Achatbot%20for%20healthcare%20application%20will%20help%20users%20obtain%20consultation%20services%0Ain%20a%20direct%20and%20natural%20way.%20However%2C%20concepts%20such%20as%20acupuncture%20points%0A%28acupoints%29%20and%20meridians%20involved%20in%20TCM%20always%20appear%20in%20the%20consultation%2C%0Awhich%20cannot%20be%20displayed%20intuitively.%20To%20this%20end%2C%20we%20develop%20a%0A%5Ctextbf%7Bh%7Dealthcare%20chat%5Ctextbf%7Bbot%7D%20%28HBot%29%20based%20on%20a%20human%20body%20model%20in%203D%0Aand%20knowledge%20graph%2C%20which%20provides%20conversational%20services%20such%20as%20knowledge%0AQ%5C%26A%2C%20prescription%20recommendation%2C%20moxibustion%20therapy%20recommendation%2C%20and%0Aacupoint%20search.%20When%20specific%20acupoints%20are%20involved%20in%20the%20conversations%0Abetween%20user%20and%20HBot%2C%20the%203D%20body%20will%20jump%20to%20the%20corresponding%20acupoints%20and%0Ahighlight%20them.%20Moreover%2C%20Hbot%20can%20also%20be%20used%20in%20training%20scenarios%20to%0Aaccelerate%20the%20teaching%20process%20of%20TCM%20by%20intuitively%20displaying%20acupuncture%0Apoints%20and%20knowledge%20cards.%20The%20demonstration%20video%20is%20available%20at%0Ahttps%3A//www.youtube.com/watch%3Fv%3DUhQhutSKkTU%20.%20Our%20code%20and%20dataset%20are%20publicly%0Aavailable%20at%20Gitee%3A%20https%3A//gitee.com/plabrolin/interactive-3d-acup.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00481v1&entry.124074799=Read"},
{"title": "An introduction to reinforcement learning for neuroscience", "author": "Kristopher T. Jensen", "abstract": "  Reinforcement learning has a rich history in neuroscience, from early work on\ndopamine as a reward prediction error signal for temporal difference learning\n(Schultz et al., 1997) to recent work suggesting that dopamine could implement\na form of 'distributional reinforcement learning' popularized in deep learning\n(Dabney et al., 2020). Throughout this literature, there has been a tight link\nbetween theoretical advances in reinforcement learning and neuroscientific\nexperiments and findings. As a result, the theories describing our experimental\ndata have become increasingly complex and difficult to navigate. In this\nreview, we cover the basic theory underlying classical work in reinforcement\nlearning and build up to an introductory overview of methods in modern deep\nreinforcement learning that have found applications in systems neuroscience. We\nstart with an overview of the reinforcement learning problem and classical\ntemporal difference algorithms, followed by a discussion of 'model-free' and\n'model-based' reinforcement learning together with methods such as DYNA and\nsuccessor representations that fall in between these two extremes. Throughout\nthese sections, we highlight the close parallels between such machine learning\nmethods and related work in both experimental and theoretical neuroscience. We\nthen provide an introduction to deep reinforcement learning with examples of\nhow these methods have been used to model different learning phenomena in\nsystems neuroscience, such as meta-reinforcement learning (Wang et al., 2018)\nand distributional reinforcement learning (Dabney et al., 2020). Code that\nimplements the methods discussed in this work and generates the figures is also\nprovided.\n", "link": "http://arxiv.org/abs/2311.07315v2", "date": "2024-08-01", "relevancy": 1.8879, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.483}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4769}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20introduction%20to%20reinforcement%20learning%20for%20neuroscience&body=Title%3A%20An%20introduction%20to%20reinforcement%20learning%20for%20neuroscience%0AAuthor%3A%20Kristopher%20T.%20Jensen%0AAbstract%3A%20%20%20Reinforcement%20learning%20has%20a%20rich%20history%20in%20neuroscience%2C%20from%20early%20work%20on%0Adopamine%20as%20a%20reward%20prediction%20error%20signal%20for%20temporal%20difference%20learning%0A%28Schultz%20et%20al.%2C%201997%29%20to%20recent%20work%20suggesting%20that%20dopamine%20could%20implement%0Aa%20form%20of%20%27distributional%20reinforcement%20learning%27%20popularized%20in%20deep%20learning%0A%28Dabney%20et%20al.%2C%202020%29.%20Throughout%20this%20literature%2C%20there%20has%20been%20a%20tight%20link%0Abetween%20theoretical%20advances%20in%20reinforcement%20learning%20and%20neuroscientific%0Aexperiments%20and%20findings.%20As%20a%20result%2C%20the%20theories%20describing%20our%20experimental%0Adata%20have%20become%20increasingly%20complex%20and%20difficult%20to%20navigate.%20In%20this%0Areview%2C%20we%20cover%20the%20basic%20theory%20underlying%20classical%20work%20in%20reinforcement%0Alearning%20and%20build%20up%20to%20an%20introductory%20overview%20of%20methods%20in%20modern%20deep%0Areinforcement%20learning%20that%20have%20found%20applications%20in%20systems%20neuroscience.%20We%0Astart%20with%20an%20overview%20of%20the%20reinforcement%20learning%20problem%20and%20classical%0Atemporal%20difference%20algorithms%2C%20followed%20by%20a%20discussion%20of%20%27model-free%27%20and%0A%27model-based%27%20reinforcement%20learning%20together%20with%20methods%20such%20as%20DYNA%20and%0Asuccessor%20representations%20that%20fall%20in%20between%20these%20two%20extremes.%20Throughout%0Athese%20sections%2C%20we%20highlight%20the%20close%20parallels%20between%20such%20machine%20learning%0Amethods%20and%20related%20work%20in%20both%20experimental%20and%20theoretical%20neuroscience.%20We%0Athen%20provide%20an%20introduction%20to%20deep%20reinforcement%20learning%20with%20examples%20of%0Ahow%20these%20methods%20have%20been%20used%20to%20model%20different%20learning%20phenomena%20in%0Asystems%20neuroscience%2C%20such%20as%20meta-reinforcement%20learning%20%28Wang%20et%20al.%2C%202018%29%0Aand%20distributional%20reinforcement%20learning%20%28Dabney%20et%20al.%2C%202020%29.%20Code%20that%0Aimplements%20the%20methods%20discussed%20in%20this%20work%20and%20generates%20the%20figures%20is%20also%0Aprovided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520introduction%2520to%2520reinforcement%2520learning%2520for%2520neuroscience%26entry.906535625%3DKristopher%2520T.%2520Jensen%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520has%2520a%2520rich%2520history%2520in%2520neuroscience%252C%2520from%2520early%2520work%2520on%250Adopamine%2520as%2520a%2520reward%2520prediction%2520error%2520signal%2520for%2520temporal%2520difference%2520learning%250A%2528Schultz%2520et%2520al.%252C%25201997%2529%2520to%2520recent%2520work%2520suggesting%2520that%2520dopamine%2520could%2520implement%250Aa%2520form%2520of%2520%2527distributional%2520reinforcement%2520learning%2527%2520popularized%2520in%2520deep%2520learning%250A%2528Dabney%2520et%2520al.%252C%25202020%2529.%2520Throughout%2520this%2520literature%252C%2520there%2520has%2520been%2520a%2520tight%2520link%250Abetween%2520theoretical%2520advances%2520in%2520reinforcement%2520learning%2520and%2520neuroscientific%250Aexperiments%2520and%2520findings.%2520As%2520a%2520result%252C%2520the%2520theories%2520describing%2520our%2520experimental%250Adata%2520have%2520become%2520increasingly%2520complex%2520and%2520difficult%2520to%2520navigate.%2520In%2520this%250Areview%252C%2520we%2520cover%2520the%2520basic%2520theory%2520underlying%2520classical%2520work%2520in%2520reinforcement%250Alearning%2520and%2520build%2520up%2520to%2520an%2520introductory%2520overview%2520of%2520methods%2520in%2520modern%2520deep%250Areinforcement%2520learning%2520that%2520have%2520found%2520applications%2520in%2520systems%2520neuroscience.%2520We%250Astart%2520with%2520an%2520overview%2520of%2520the%2520reinforcement%2520learning%2520problem%2520and%2520classical%250Atemporal%2520difference%2520algorithms%252C%2520followed%2520by%2520a%2520discussion%2520of%2520%2527model-free%2527%2520and%250A%2527model-based%2527%2520reinforcement%2520learning%2520together%2520with%2520methods%2520such%2520as%2520DYNA%2520and%250Asuccessor%2520representations%2520that%2520fall%2520in%2520between%2520these%2520two%2520extremes.%2520Throughout%250Athese%2520sections%252C%2520we%2520highlight%2520the%2520close%2520parallels%2520between%2520such%2520machine%2520learning%250Amethods%2520and%2520related%2520work%2520in%2520both%2520experimental%2520and%2520theoretical%2520neuroscience.%2520We%250Athen%2520provide%2520an%2520introduction%2520to%2520deep%2520reinforcement%2520learning%2520with%2520examples%2520of%250Ahow%2520these%2520methods%2520have%2520been%2520used%2520to%2520model%2520different%2520learning%2520phenomena%2520in%250Asystems%2520neuroscience%252C%2520such%2520as%2520meta-reinforcement%2520learning%2520%2528Wang%2520et%2520al.%252C%25202018%2529%250Aand%2520distributional%2520reinforcement%2520learning%2520%2528Dabney%2520et%2520al.%252C%25202020%2529.%2520Code%2520that%250Aimplements%2520the%2520methods%2520discussed%2520in%2520this%2520work%2520and%2520generates%2520the%2520figures%2520is%2520also%250Aprovided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20introduction%20to%20reinforcement%20learning%20for%20neuroscience&entry.906535625=Kristopher%20T.%20Jensen&entry.1292438233=%20%20Reinforcement%20learning%20has%20a%20rich%20history%20in%20neuroscience%2C%20from%20early%20work%20on%0Adopamine%20as%20a%20reward%20prediction%20error%20signal%20for%20temporal%20difference%20learning%0A%28Schultz%20et%20al.%2C%201997%29%20to%20recent%20work%20suggesting%20that%20dopamine%20could%20implement%0Aa%20form%20of%20%27distributional%20reinforcement%20learning%27%20popularized%20in%20deep%20learning%0A%28Dabney%20et%20al.%2C%202020%29.%20Throughout%20this%20literature%2C%20there%20has%20been%20a%20tight%20link%0Abetween%20theoretical%20advances%20in%20reinforcement%20learning%20and%20neuroscientific%0Aexperiments%20and%20findings.%20As%20a%20result%2C%20the%20theories%20describing%20our%20experimental%0Adata%20have%20become%20increasingly%20complex%20and%20difficult%20to%20navigate.%20In%20this%0Areview%2C%20we%20cover%20the%20basic%20theory%20underlying%20classical%20work%20in%20reinforcement%0Alearning%20and%20build%20up%20to%20an%20introductory%20overview%20of%20methods%20in%20modern%20deep%0Areinforcement%20learning%20that%20have%20found%20applications%20in%20systems%20neuroscience.%20We%0Astart%20with%20an%20overview%20of%20the%20reinforcement%20learning%20problem%20and%20classical%0Atemporal%20difference%20algorithms%2C%20followed%20by%20a%20discussion%20of%20%27model-free%27%20and%0A%27model-based%27%20reinforcement%20learning%20together%20with%20methods%20such%20as%20DYNA%20and%0Asuccessor%20representations%20that%20fall%20in%20between%20these%20two%20extremes.%20Throughout%0Athese%20sections%2C%20we%20highlight%20the%20close%20parallels%20between%20such%20machine%20learning%0Amethods%20and%20related%20work%20in%20both%20experimental%20and%20theoretical%20neuroscience.%20We%0Athen%20provide%20an%20introduction%20to%20deep%20reinforcement%20learning%20with%20examples%20of%0Ahow%20these%20methods%20have%20been%20used%20to%20model%20different%20learning%20phenomena%20in%0Asystems%20neuroscience%2C%20such%20as%20meta-reinforcement%20learning%20%28Wang%20et%20al.%2C%202018%29%0Aand%20distributional%20reinforcement%20learning%20%28Dabney%20et%20al.%2C%202020%29.%20Code%20that%0Aimplements%20the%20methods%20discussed%20in%20this%20work%20and%20generates%20the%20figures%20is%20also%0Aprovided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07315v2&entry.124074799=Read"},
{"title": "Chance-Constrained Information-Theoretic Stochastic Model Predictive\n  Control with Safety Shielding", "author": "Ji yin and Panagiotis Tsiotras and Karl Berntorp", "abstract": "  This paper introduces a novel nonlinear stochastic model predictive control\npath integral (MPPI) method, which considers chance constraints on system\nstates. The proposed belief-space stochastic MPPI (BSS-MPPI) applies\nMonte-Carlo sampling to evaluate state distributions resulting from underlying\nsystematic disturbances, and utilizes a Control Barrier Function (CBF) inspired\nheuristic in belief space to fulfill the specified chance constraints. Compared\nto several previous stochastic predictive control methods, our approach applies\nto general nonlinear dynamics without requiring the computationally expensive\nsystem linearization step. Moreover, the BSS-MPPI controller can solve\noptimization problems without limiting the form of the objective function and\nchance constraints. By multi-threading the sampling process using a GPU, we can\nachieve fast real-time planning for time- and safety-critical tasks such as\nautonomous racing. Our results on a realistic race-car simulation study show\nsignificant reductions in constraint violation compared to some of the prior\nMPPI approaches, while being comparable in computation times.\n", "link": "http://arxiv.org/abs/2408.00494v1", "date": "2024-08-01", "relevancy": 1.8873, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5287}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chance-Constrained%20Information-Theoretic%20Stochastic%20Model%20Predictive%0A%20%20Control%20with%20Safety%20Shielding&body=Title%3A%20Chance-Constrained%20Information-Theoretic%20Stochastic%20Model%20Predictive%0A%20%20Control%20with%20Safety%20Shielding%0AAuthor%3A%20Ji%20yin%20and%20Panagiotis%20Tsiotras%20and%20Karl%20Berntorp%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20nonlinear%20stochastic%20model%20predictive%20control%0Apath%20integral%20%28MPPI%29%20method%2C%20which%20considers%20chance%20constraints%20on%20system%0Astates.%20The%20proposed%20belief-space%20stochastic%20MPPI%20%28BSS-MPPI%29%20applies%0AMonte-Carlo%20sampling%20to%20evaluate%20state%20distributions%20resulting%20from%20underlying%0Asystematic%20disturbances%2C%20and%20utilizes%20a%20Control%20Barrier%20Function%20%28CBF%29%20inspired%0Aheuristic%20in%20belief%20space%20to%20fulfill%20the%20specified%20chance%20constraints.%20Compared%0Ato%20several%20previous%20stochastic%20predictive%20control%20methods%2C%20our%20approach%20applies%0Ato%20general%20nonlinear%20dynamics%20without%20requiring%20the%20computationally%20expensive%0Asystem%20linearization%20step.%20Moreover%2C%20the%20BSS-MPPI%20controller%20can%20solve%0Aoptimization%20problems%20without%20limiting%20the%20form%20of%20the%20objective%20function%20and%0Achance%20constraints.%20By%20multi-threading%20the%20sampling%20process%20using%20a%20GPU%2C%20we%20can%0Aachieve%20fast%20real-time%20planning%20for%20time-%20and%20safety-critical%20tasks%20such%20as%0Aautonomous%20racing.%20Our%20results%20on%20a%20realistic%20race-car%20simulation%20study%20show%0Asignificant%20reductions%20in%20constraint%20violation%20compared%20to%20some%20of%20the%20prior%0AMPPI%20approaches%2C%20while%20being%20comparable%20in%20computation%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChance-Constrained%2520Information-Theoretic%2520Stochastic%2520Model%2520Predictive%250A%2520%2520Control%2520with%2520Safety%2520Shielding%26entry.906535625%3DJi%2520yin%2520and%2520Panagiotis%2520Tsiotras%2520and%2520Karl%2520Berntorp%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520nonlinear%2520stochastic%2520model%2520predictive%2520control%250Apath%2520integral%2520%2528MPPI%2529%2520method%252C%2520which%2520considers%2520chance%2520constraints%2520on%2520system%250Astates.%2520The%2520proposed%2520belief-space%2520stochastic%2520MPPI%2520%2528BSS-MPPI%2529%2520applies%250AMonte-Carlo%2520sampling%2520to%2520evaluate%2520state%2520distributions%2520resulting%2520from%2520underlying%250Asystematic%2520disturbances%252C%2520and%2520utilizes%2520a%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520inspired%250Aheuristic%2520in%2520belief%2520space%2520to%2520fulfill%2520the%2520specified%2520chance%2520constraints.%2520Compared%250Ato%2520several%2520previous%2520stochastic%2520predictive%2520control%2520methods%252C%2520our%2520approach%2520applies%250Ato%2520general%2520nonlinear%2520dynamics%2520without%2520requiring%2520the%2520computationally%2520expensive%250Asystem%2520linearization%2520step.%2520Moreover%252C%2520the%2520BSS-MPPI%2520controller%2520can%2520solve%250Aoptimization%2520problems%2520without%2520limiting%2520the%2520form%2520of%2520the%2520objective%2520function%2520and%250Achance%2520constraints.%2520By%2520multi-threading%2520the%2520sampling%2520process%2520using%2520a%2520GPU%252C%2520we%2520can%250Aachieve%2520fast%2520real-time%2520planning%2520for%2520time-%2520and%2520safety-critical%2520tasks%2520such%2520as%250Aautonomous%2520racing.%2520Our%2520results%2520on%2520a%2520realistic%2520race-car%2520simulation%2520study%2520show%250Asignificant%2520reductions%2520in%2520constraint%2520violation%2520compared%2520to%2520some%2520of%2520the%2520prior%250AMPPI%2520approaches%252C%2520while%2520being%2520comparable%2520in%2520computation%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chance-Constrained%20Information-Theoretic%20Stochastic%20Model%20Predictive%0A%20%20Control%20with%20Safety%20Shielding&entry.906535625=Ji%20yin%20and%20Panagiotis%20Tsiotras%20and%20Karl%20Berntorp&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20nonlinear%20stochastic%20model%20predictive%20control%0Apath%20integral%20%28MPPI%29%20method%2C%20which%20considers%20chance%20constraints%20on%20system%0Astates.%20The%20proposed%20belief-space%20stochastic%20MPPI%20%28BSS-MPPI%29%20applies%0AMonte-Carlo%20sampling%20to%20evaluate%20state%20distributions%20resulting%20from%20underlying%0Asystematic%20disturbances%2C%20and%20utilizes%20a%20Control%20Barrier%20Function%20%28CBF%29%20inspired%0Aheuristic%20in%20belief%20space%20to%20fulfill%20the%20specified%20chance%20constraints.%20Compared%0Ato%20several%20previous%20stochastic%20predictive%20control%20methods%2C%20our%20approach%20applies%0Ato%20general%20nonlinear%20dynamics%20without%20requiring%20the%20computationally%20expensive%0Asystem%20linearization%20step.%20Moreover%2C%20the%20BSS-MPPI%20controller%20can%20solve%0Aoptimization%20problems%20without%20limiting%20the%20form%20of%20the%20objective%20function%20and%0Achance%20constraints.%20By%20multi-threading%20the%20sampling%20process%20using%20a%20GPU%2C%20we%20can%0Aachieve%20fast%20real-time%20planning%20for%20time-%20and%20safety-critical%20tasks%20such%20as%0Aautonomous%20racing.%20Our%20results%20on%20a%20realistic%20race-car%20simulation%20study%20show%0Asignificant%20reductions%20in%20constraint%20violation%20compared%20to%20some%20of%20the%20prior%0AMPPI%20approaches%2C%20while%20being%20comparable%20in%20computation%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00494v1&entry.124074799=Read"},
{"title": "An AI-Enabled Framework Within Reach for Enhancing Healthcare\n  Sustainability and Fairness", "author": "Bin Huang and Changchen Zhao and Zimeng Liu and Shenda Hong and Baochang Zhang and Hao Lu and Zhijun Liu and Wenjin Wang and Hui Liu", "abstract": "  Good health and well-being is among key issues in the United Nations 2030\nSustainable Development Goals. The rising prevalence of large-scale infectious\ndiseases and the accelerated aging of the global population are driving the\ntransformation of healthcare technologies. In this context, establishing\nlarge-scale public health datasets, developing medical models, and creating\ndecision-making systems with a human-centric approach are of strategic\nsignificance. Recently, by leveraging the extraordinary number of accessible\ncameras, groundbreaking advancements have emerged in AI methods for\nphysiological signal monitoring and disease diagnosis using camera sensors.\nThese approaches, requiring no specialized medical equipment, offer convenient\nmanners of collecting large-scale medical data in response to public health\nevents. Therefore, we outline a prospective framework and heuristic vision for\na camera-based public health (CBPH) framework utilizing visual physiological\nmonitoring technology. The CBPH can be considered as a convenient and universal\nframework for public health, advancing the United Nations Sustainable\nDevelopment Goals, particularly in promoting the universality, sustainability,\nand equity of healthcare in low- and middle-income countries or regions.\nFurthermore, CBPH provides a comprehensive solution for building a large-scale\nand human-centric medical database, and a multi-task large medical model for\npublic health and medical scientific discoveries. It has a significant\npotential to revolutionize personal monitoring technologies, digital medicine,\ntelemedicine, and primary health care in public health. Therefore, it can be\ndeemed that the outcomes of this paper will contribute to the establishment of\na sustainable and fair framework for public health, which serves as a crucial\nbridge for advancing scientific discoveries in the realm of AI for medicine\n(AI4Medicine).\n", "link": "http://arxiv.org/abs/2406.07558v2", "date": "2024-08-01", "relevancy": 1.8746, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4713}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4685}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20AI-Enabled%20Framework%20Within%20Reach%20for%20Enhancing%20Healthcare%0A%20%20Sustainability%20and%20Fairness&body=Title%3A%20An%20AI-Enabled%20Framework%20Within%20Reach%20for%20Enhancing%20Healthcare%0A%20%20Sustainability%20and%20Fairness%0AAuthor%3A%20Bin%20Huang%20and%20Changchen%20Zhao%20and%20Zimeng%20Liu%20and%20Shenda%20Hong%20and%20Baochang%20Zhang%20and%20Hao%20Lu%20and%20Zhijun%20Liu%20and%20Wenjin%20Wang%20and%20Hui%20Liu%0AAbstract%3A%20%20%20Good%20health%20and%20well-being%20is%20among%20key%20issues%20in%20the%20United%20Nations%202030%0ASustainable%20Development%20Goals.%20The%20rising%20prevalence%20of%20large-scale%20infectious%0Adiseases%20and%20the%20accelerated%20aging%20of%20the%20global%20population%20are%20driving%20the%0Atransformation%20of%20healthcare%20technologies.%20In%20this%20context%2C%20establishing%0Alarge-scale%20public%20health%20datasets%2C%20developing%20medical%20models%2C%20and%20creating%0Adecision-making%20systems%20with%20a%20human-centric%20approach%20are%20of%20strategic%0Asignificance.%20Recently%2C%20by%20leveraging%20the%20extraordinary%20number%20of%20accessible%0Acameras%2C%20groundbreaking%20advancements%20have%20emerged%20in%20AI%20methods%20for%0Aphysiological%20signal%20monitoring%20and%20disease%20diagnosis%20using%20camera%20sensors.%0AThese%20approaches%2C%20requiring%20no%20specialized%20medical%20equipment%2C%20offer%20convenient%0Amanners%20of%20collecting%20large-scale%20medical%20data%20in%20response%20to%20public%20health%0Aevents.%20Therefore%2C%20we%20outline%20a%20prospective%20framework%20and%20heuristic%20vision%20for%0Aa%20camera-based%20public%20health%20%28CBPH%29%20framework%20utilizing%20visual%20physiological%0Amonitoring%20technology.%20The%20CBPH%20can%20be%20considered%20as%20a%20convenient%20and%20universal%0Aframework%20for%20public%20health%2C%20advancing%20the%20United%20Nations%20Sustainable%0ADevelopment%20Goals%2C%20particularly%20in%20promoting%20the%20universality%2C%20sustainability%2C%0Aand%20equity%20of%20healthcare%20in%20low-%20and%20middle-income%20countries%20or%20regions.%0AFurthermore%2C%20CBPH%20provides%20a%20comprehensive%20solution%20for%20building%20a%20large-scale%0Aand%20human-centric%20medical%20database%2C%20and%20a%20multi-task%20large%20medical%20model%20for%0Apublic%20health%20and%20medical%20scientific%20discoveries.%20It%20has%20a%20significant%0Apotential%20to%20revolutionize%20personal%20monitoring%20technologies%2C%20digital%20medicine%2C%0Atelemedicine%2C%20and%20primary%20health%20care%20in%20public%20health.%20Therefore%2C%20it%20can%20be%0Adeemed%20that%20the%20outcomes%20of%20this%20paper%20will%20contribute%20to%20the%20establishment%20of%0Aa%20sustainable%20and%20fair%20framework%20for%20public%20health%2C%20which%20serves%20as%20a%20crucial%0Abridge%20for%20advancing%20scientific%20discoveries%20in%20the%20realm%20of%20AI%20for%20medicine%0A%28AI4Medicine%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520AI-Enabled%2520Framework%2520Within%2520Reach%2520for%2520Enhancing%2520Healthcare%250A%2520%2520Sustainability%2520and%2520Fairness%26entry.906535625%3DBin%2520Huang%2520and%2520Changchen%2520Zhao%2520and%2520Zimeng%2520Liu%2520and%2520Shenda%2520Hong%2520and%2520Baochang%2520Zhang%2520and%2520Hao%2520Lu%2520and%2520Zhijun%2520Liu%2520and%2520Wenjin%2520Wang%2520and%2520Hui%2520Liu%26entry.1292438233%3D%2520%2520Good%2520health%2520and%2520well-being%2520is%2520among%2520key%2520issues%2520in%2520the%2520United%2520Nations%25202030%250ASustainable%2520Development%2520Goals.%2520The%2520rising%2520prevalence%2520of%2520large-scale%2520infectious%250Adiseases%2520and%2520the%2520accelerated%2520aging%2520of%2520the%2520global%2520population%2520are%2520driving%2520the%250Atransformation%2520of%2520healthcare%2520technologies.%2520In%2520this%2520context%252C%2520establishing%250Alarge-scale%2520public%2520health%2520datasets%252C%2520developing%2520medical%2520models%252C%2520and%2520creating%250Adecision-making%2520systems%2520with%2520a%2520human-centric%2520approach%2520are%2520of%2520strategic%250Asignificance.%2520Recently%252C%2520by%2520leveraging%2520the%2520extraordinary%2520number%2520of%2520accessible%250Acameras%252C%2520groundbreaking%2520advancements%2520have%2520emerged%2520in%2520AI%2520methods%2520for%250Aphysiological%2520signal%2520monitoring%2520and%2520disease%2520diagnosis%2520using%2520camera%2520sensors.%250AThese%2520approaches%252C%2520requiring%2520no%2520specialized%2520medical%2520equipment%252C%2520offer%2520convenient%250Amanners%2520of%2520collecting%2520large-scale%2520medical%2520data%2520in%2520response%2520to%2520public%2520health%250Aevents.%2520Therefore%252C%2520we%2520outline%2520a%2520prospective%2520framework%2520and%2520heuristic%2520vision%2520for%250Aa%2520camera-based%2520public%2520health%2520%2528CBPH%2529%2520framework%2520utilizing%2520visual%2520physiological%250Amonitoring%2520technology.%2520The%2520CBPH%2520can%2520be%2520considered%2520as%2520a%2520convenient%2520and%2520universal%250Aframework%2520for%2520public%2520health%252C%2520advancing%2520the%2520United%2520Nations%2520Sustainable%250ADevelopment%2520Goals%252C%2520particularly%2520in%2520promoting%2520the%2520universality%252C%2520sustainability%252C%250Aand%2520equity%2520of%2520healthcare%2520in%2520low-%2520and%2520middle-income%2520countries%2520or%2520regions.%250AFurthermore%252C%2520CBPH%2520provides%2520a%2520comprehensive%2520solution%2520for%2520building%2520a%2520large-scale%250Aand%2520human-centric%2520medical%2520database%252C%2520and%2520a%2520multi-task%2520large%2520medical%2520model%2520for%250Apublic%2520health%2520and%2520medical%2520scientific%2520discoveries.%2520It%2520has%2520a%2520significant%250Apotential%2520to%2520revolutionize%2520personal%2520monitoring%2520technologies%252C%2520digital%2520medicine%252C%250Atelemedicine%252C%2520and%2520primary%2520health%2520care%2520in%2520public%2520health.%2520Therefore%252C%2520it%2520can%2520be%250Adeemed%2520that%2520the%2520outcomes%2520of%2520this%2520paper%2520will%2520contribute%2520to%2520the%2520establishment%2520of%250Aa%2520sustainable%2520and%2520fair%2520framework%2520for%2520public%2520health%252C%2520which%2520serves%2520as%2520a%2520crucial%250Abridge%2520for%2520advancing%2520scientific%2520discoveries%2520in%2520the%2520realm%2520of%2520AI%2520for%2520medicine%250A%2528AI4Medicine%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20AI-Enabled%20Framework%20Within%20Reach%20for%20Enhancing%20Healthcare%0A%20%20Sustainability%20and%20Fairness&entry.906535625=Bin%20Huang%20and%20Changchen%20Zhao%20and%20Zimeng%20Liu%20and%20Shenda%20Hong%20and%20Baochang%20Zhang%20and%20Hao%20Lu%20and%20Zhijun%20Liu%20and%20Wenjin%20Wang%20and%20Hui%20Liu&entry.1292438233=%20%20Good%20health%20and%20well-being%20is%20among%20key%20issues%20in%20the%20United%20Nations%202030%0ASustainable%20Development%20Goals.%20The%20rising%20prevalence%20of%20large-scale%20infectious%0Adiseases%20and%20the%20accelerated%20aging%20of%20the%20global%20population%20are%20driving%20the%0Atransformation%20of%20healthcare%20technologies.%20In%20this%20context%2C%20establishing%0Alarge-scale%20public%20health%20datasets%2C%20developing%20medical%20models%2C%20and%20creating%0Adecision-making%20systems%20with%20a%20human-centric%20approach%20are%20of%20strategic%0Asignificance.%20Recently%2C%20by%20leveraging%20the%20extraordinary%20number%20of%20accessible%0Acameras%2C%20groundbreaking%20advancements%20have%20emerged%20in%20AI%20methods%20for%0Aphysiological%20signal%20monitoring%20and%20disease%20diagnosis%20using%20camera%20sensors.%0AThese%20approaches%2C%20requiring%20no%20specialized%20medical%20equipment%2C%20offer%20convenient%0Amanners%20of%20collecting%20large-scale%20medical%20data%20in%20response%20to%20public%20health%0Aevents.%20Therefore%2C%20we%20outline%20a%20prospective%20framework%20and%20heuristic%20vision%20for%0Aa%20camera-based%20public%20health%20%28CBPH%29%20framework%20utilizing%20visual%20physiological%0Amonitoring%20technology.%20The%20CBPH%20can%20be%20considered%20as%20a%20convenient%20and%20universal%0Aframework%20for%20public%20health%2C%20advancing%20the%20United%20Nations%20Sustainable%0ADevelopment%20Goals%2C%20particularly%20in%20promoting%20the%20universality%2C%20sustainability%2C%0Aand%20equity%20of%20healthcare%20in%20low-%20and%20middle-income%20countries%20or%20regions.%0AFurthermore%2C%20CBPH%20provides%20a%20comprehensive%20solution%20for%20building%20a%20large-scale%0Aand%20human-centric%20medical%20database%2C%20and%20a%20multi-task%20large%20medical%20model%20for%0Apublic%20health%20and%20medical%20scientific%20discoveries.%20It%20has%20a%20significant%0Apotential%20to%20revolutionize%20personal%20monitoring%20technologies%2C%20digital%20medicine%2C%0Atelemedicine%2C%20and%20primary%20health%20care%20in%20public%20health.%20Therefore%2C%20it%20can%20be%0Adeemed%20that%20the%20outcomes%20of%20this%20paper%20will%20contribute%20to%20the%20establishment%20of%0Aa%20sustainable%20and%20fair%20framework%20for%20public%20health%2C%20which%20serves%20as%20a%20crucial%0Abridge%20for%20advancing%20scientific%20discoveries%20in%20the%20realm%20of%20AI%20for%20medicine%0A%28AI4Medicine%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07558v2&entry.124074799=Read"},
{"title": "Learning in Multi-Objective Public Goods Games with Non-Linear Utilities", "author": "Nicole Orzan and Erman Acar and Davide Grossi and Patrick Mannion and Roxana R\u0103dulescu", "abstract": "  Addressing the question of how to achieve optimal decision-making under risk\nand uncertainty is crucial for enhancing the capabilities of artificial agents\nthat collaborate with or support humans. In this work, we address this question\nin the context of Public Goods Games. We study learning in a novel\nmulti-objective version of the Public Goods Game where agents have different\nrisk preferences, by means of multi-objective reinforcement learning. We\nintroduce a parametric non-linear utility function to model risk preferences at\nthe level of individual agents, over the collective and individual reward\ncomponents of the game. We study the interplay between such preference\nmodelling and environmental uncertainty on the incentive alignment level in the\ngame. We demonstrate how different combinations of individual preferences and\nenvironmental uncertainties sustain the emergence of cooperative patterns in\nnon-cooperative environments (i.e., where competitive strategies are dominant),\nwhile others sustain competitive patterns in cooperative environments (i.e.,\nwhere cooperative strategies are dominant).\n", "link": "http://arxiv.org/abs/2408.00682v1", "date": "2024-08-01", "relevancy": 1.8726, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5006}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4668}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20in%20Multi-Objective%20Public%20Goods%20Games%20with%20Non-Linear%20Utilities&body=Title%3A%20Learning%20in%20Multi-Objective%20Public%20Goods%20Games%20with%20Non-Linear%20Utilities%0AAuthor%3A%20Nicole%20Orzan%20and%20Erman%20Acar%20and%20Davide%20Grossi%20and%20Patrick%20Mannion%20and%20Roxana%20R%C4%83dulescu%0AAbstract%3A%20%20%20Addressing%20the%20question%20of%20how%20to%20achieve%20optimal%20decision-making%20under%20risk%0Aand%20uncertainty%20is%20crucial%20for%20enhancing%20the%20capabilities%20of%20artificial%20agents%0Athat%20collaborate%20with%20or%20support%20humans.%20In%20this%20work%2C%20we%20address%20this%20question%0Ain%20the%20context%20of%20Public%20Goods%20Games.%20We%20study%20learning%20in%20a%20novel%0Amulti-objective%20version%20of%20the%20Public%20Goods%20Game%20where%20agents%20have%20different%0Arisk%20preferences%2C%20by%20means%20of%20multi-objective%20reinforcement%20learning.%20We%0Aintroduce%20a%20parametric%20non-linear%20utility%20function%20to%20model%20risk%20preferences%20at%0Athe%20level%20of%20individual%20agents%2C%20over%20the%20collective%20and%20individual%20reward%0Acomponents%20of%20the%20game.%20We%20study%20the%20interplay%20between%20such%20preference%0Amodelling%20and%20environmental%20uncertainty%20on%20the%20incentive%20alignment%20level%20in%20the%0Agame.%20We%20demonstrate%20how%20different%20combinations%20of%20individual%20preferences%20and%0Aenvironmental%20uncertainties%20sustain%20the%20emergence%20of%20cooperative%20patterns%20in%0Anon-cooperative%20environments%20%28i.e.%2C%20where%20competitive%20strategies%20are%20dominant%29%2C%0Awhile%20others%20sustain%20competitive%20patterns%20in%20cooperative%20environments%20%28i.e.%2C%0Awhere%20cooperative%20strategies%20are%20dominant%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520in%2520Multi-Objective%2520Public%2520Goods%2520Games%2520with%2520Non-Linear%2520Utilities%26entry.906535625%3DNicole%2520Orzan%2520and%2520Erman%2520Acar%2520and%2520Davide%2520Grossi%2520and%2520Patrick%2520Mannion%2520and%2520Roxana%2520R%25C4%2583dulescu%26entry.1292438233%3D%2520%2520Addressing%2520the%2520question%2520of%2520how%2520to%2520achieve%2520optimal%2520decision-making%2520under%2520risk%250Aand%2520uncertainty%2520is%2520crucial%2520for%2520enhancing%2520the%2520capabilities%2520of%2520artificial%2520agents%250Athat%2520collaborate%2520with%2520or%2520support%2520humans.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520question%250Ain%2520the%2520context%2520of%2520Public%2520Goods%2520Games.%2520We%2520study%2520learning%2520in%2520a%2520novel%250Amulti-objective%2520version%2520of%2520the%2520Public%2520Goods%2520Game%2520where%2520agents%2520have%2520different%250Arisk%2520preferences%252C%2520by%2520means%2520of%2520multi-objective%2520reinforcement%2520learning.%2520We%250Aintroduce%2520a%2520parametric%2520non-linear%2520utility%2520function%2520to%2520model%2520risk%2520preferences%2520at%250Athe%2520level%2520of%2520individual%2520agents%252C%2520over%2520the%2520collective%2520and%2520individual%2520reward%250Acomponents%2520of%2520the%2520game.%2520We%2520study%2520the%2520interplay%2520between%2520such%2520preference%250Amodelling%2520and%2520environmental%2520uncertainty%2520on%2520the%2520incentive%2520alignment%2520level%2520in%2520the%250Agame.%2520We%2520demonstrate%2520how%2520different%2520combinations%2520of%2520individual%2520preferences%2520and%250Aenvironmental%2520uncertainties%2520sustain%2520the%2520emergence%2520of%2520cooperative%2520patterns%2520in%250Anon-cooperative%2520environments%2520%2528i.e.%252C%2520where%2520competitive%2520strategies%2520are%2520dominant%2529%252C%250Awhile%2520others%2520sustain%2520competitive%2520patterns%2520in%2520cooperative%2520environments%2520%2528i.e.%252C%250Awhere%2520cooperative%2520strategies%2520are%2520dominant%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20in%20Multi-Objective%20Public%20Goods%20Games%20with%20Non-Linear%20Utilities&entry.906535625=Nicole%20Orzan%20and%20Erman%20Acar%20and%20Davide%20Grossi%20and%20Patrick%20Mannion%20and%20Roxana%20R%C4%83dulescu&entry.1292438233=%20%20Addressing%20the%20question%20of%20how%20to%20achieve%20optimal%20decision-making%20under%20risk%0Aand%20uncertainty%20is%20crucial%20for%20enhancing%20the%20capabilities%20of%20artificial%20agents%0Athat%20collaborate%20with%20or%20support%20humans.%20In%20this%20work%2C%20we%20address%20this%20question%0Ain%20the%20context%20of%20Public%20Goods%20Games.%20We%20study%20learning%20in%20a%20novel%0Amulti-objective%20version%20of%20the%20Public%20Goods%20Game%20where%20agents%20have%20different%0Arisk%20preferences%2C%20by%20means%20of%20multi-objective%20reinforcement%20learning.%20We%0Aintroduce%20a%20parametric%20non-linear%20utility%20function%20to%20model%20risk%20preferences%20at%0Athe%20level%20of%20individual%20agents%2C%20over%20the%20collective%20and%20individual%20reward%0Acomponents%20of%20the%20game.%20We%20study%20the%20interplay%20between%20such%20preference%0Amodelling%20and%20environmental%20uncertainty%20on%20the%20incentive%20alignment%20level%20in%20the%0Agame.%20We%20demonstrate%20how%20different%20combinations%20of%20individual%20preferences%20and%0Aenvironmental%20uncertainties%20sustain%20the%20emergence%20of%20cooperative%20patterns%20in%0Anon-cooperative%20environments%20%28i.e.%2C%20where%20competitive%20strategies%20are%20dominant%29%2C%0Awhile%20others%20sustain%20competitive%20patterns%20in%20cooperative%20environments%20%28i.e.%2C%0Awhere%20cooperative%20strategies%20are%20dominant%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00682v1&entry.124074799=Read"},
{"title": "The Susceptibility of Example-Based Explainability Methods to Class\n  Outliers", "author": "Ikhtiyor Nematov and Dimitris Sacharidis and Tomer Sagi and Katja Hose", "abstract": "  This study explores the impact of class outliers on the effectiveness of\nexample-based explainability methods for black-box machine learning models. We\nreformulate existing explainability evaluation metrics, such as correctness and\nrelevance, specifically for example-based methods, and introduce a new metric,\ndistinguishability. Using these metrics, we highlight the shortcomings of\ncurrent example-based explainability methods, including those who attempt to\nsuppress class outliers. We conduct experiments on two datasets, a text\nclassification dataset and an image classification dataset, and evaluate the\nperformance of four state-of-the-art explainability methods. Our findings\nunderscore the need for robust techniques to tackle the challenges posed by\nclass outliers.\n", "link": "http://arxiv.org/abs/2407.20678v2", "date": "2024-08-01", "relevancy": 1.8677, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4754}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4668}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Susceptibility%20of%20Example-Based%20Explainability%20Methods%20to%20Class%0A%20%20Outliers&body=Title%3A%20The%20Susceptibility%20of%20Example-Based%20Explainability%20Methods%20to%20Class%0A%20%20Outliers%0AAuthor%3A%20Ikhtiyor%20Nematov%20and%20Dimitris%20Sacharidis%20and%20Tomer%20Sagi%20and%20Katja%20Hose%0AAbstract%3A%20%20%20This%20study%20explores%20the%20impact%20of%20class%20outliers%20on%20the%20effectiveness%20of%0Aexample-based%20explainability%20methods%20for%20black-box%20machine%20learning%20models.%20We%0Areformulate%20existing%20explainability%20evaluation%20metrics%2C%20such%20as%20correctness%20and%0Arelevance%2C%20specifically%20for%20example-based%20methods%2C%20and%20introduce%20a%20new%20metric%2C%0Adistinguishability.%20Using%20these%20metrics%2C%20we%20highlight%20the%20shortcomings%20of%0Acurrent%20example-based%20explainability%20methods%2C%20including%20those%20who%20attempt%20to%0Asuppress%20class%20outliers.%20We%20conduct%20experiments%20on%20two%20datasets%2C%20a%20text%0Aclassification%20dataset%20and%20an%20image%20classification%20dataset%2C%20and%20evaluate%20the%0Aperformance%20of%20four%20state-of-the-art%20explainability%20methods.%20Our%20findings%0Aunderscore%20the%20need%20for%20robust%20techniques%20to%20tackle%20the%20challenges%20posed%20by%0Aclass%20outliers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Susceptibility%2520of%2520Example-Based%2520Explainability%2520Methods%2520to%2520Class%250A%2520%2520Outliers%26entry.906535625%3DIkhtiyor%2520Nematov%2520and%2520Dimitris%2520Sacharidis%2520and%2520Tomer%2520Sagi%2520and%2520Katja%2520Hose%26entry.1292438233%3D%2520%2520This%2520study%2520explores%2520the%2520impact%2520of%2520class%2520outliers%2520on%2520the%2520effectiveness%2520of%250Aexample-based%2520explainability%2520methods%2520for%2520black-box%2520machine%2520learning%2520models.%2520We%250Areformulate%2520existing%2520explainability%2520evaluation%2520metrics%252C%2520such%2520as%2520correctness%2520and%250Arelevance%252C%2520specifically%2520for%2520example-based%2520methods%252C%2520and%2520introduce%2520a%2520new%2520metric%252C%250Adistinguishability.%2520Using%2520these%2520metrics%252C%2520we%2520highlight%2520the%2520shortcomings%2520of%250Acurrent%2520example-based%2520explainability%2520methods%252C%2520including%2520those%2520who%2520attempt%2520to%250Asuppress%2520class%2520outliers.%2520We%2520conduct%2520experiments%2520on%2520two%2520datasets%252C%2520a%2520text%250Aclassification%2520dataset%2520and%2520an%2520image%2520classification%2520dataset%252C%2520and%2520evaluate%2520the%250Aperformance%2520of%2520four%2520state-of-the-art%2520explainability%2520methods.%2520Our%2520findings%250Aunderscore%2520the%2520need%2520for%2520robust%2520techniques%2520to%2520tackle%2520the%2520challenges%2520posed%2520by%250Aclass%2520outliers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Susceptibility%20of%20Example-Based%20Explainability%20Methods%20to%20Class%0A%20%20Outliers&entry.906535625=Ikhtiyor%20Nematov%20and%20Dimitris%20Sacharidis%20and%20Tomer%20Sagi%20and%20Katja%20Hose&entry.1292438233=%20%20This%20study%20explores%20the%20impact%20of%20class%20outliers%20on%20the%20effectiveness%20of%0Aexample-based%20explainability%20methods%20for%20black-box%20machine%20learning%20models.%20We%0Areformulate%20existing%20explainability%20evaluation%20metrics%2C%20such%20as%20correctness%20and%0Arelevance%2C%20specifically%20for%20example-based%20methods%2C%20and%20introduce%20a%20new%20metric%2C%0Adistinguishability.%20Using%20these%20metrics%2C%20we%20highlight%20the%20shortcomings%20of%0Acurrent%20example-based%20explainability%20methods%2C%20including%20those%20who%20attempt%20to%0Asuppress%20class%20outliers.%20We%20conduct%20experiments%20on%20two%20datasets%2C%20a%20text%0Aclassification%20dataset%20and%20an%20image%20classification%20dataset%2C%20and%20evaluate%20the%0Aperformance%20of%20four%20state-of-the-art%20explainability%20methods.%20Our%20findings%0Aunderscore%20the%20need%20for%20robust%20techniques%20to%20tackle%20the%20challenges%20posed%20by%0Aclass%20outliers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20678v2&entry.124074799=Read"},
{"title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "author": "Mingcong Lu and Jiangcai Zhu and Wang Hao and Zheng Li and Shusheng Zhang and Kailai Shao and Chao Chen and Nan Li and Feng Wang and Xin Lu", "abstract": "  Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.\n", "link": "http://arxiv.org/abs/2408.00539v1", "date": "2024-08-01", "relevancy": 1.8537, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4829}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4544}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intermittent%20Semi-working%20Mask%3A%20A%20New%20Masking%20Paradigm%20for%20LLMs&body=Title%3A%20Intermittent%20Semi-working%20Mask%3A%20A%20New%20Masking%20Paradigm%20for%20LLMs%0AAuthor%3A%20Mingcong%20Lu%20and%20Jiangcai%20Zhu%20and%20Wang%20Hao%20and%20Zheng%20Li%20and%20Shusheng%20Zhang%20and%20Kailai%20Shao%20and%20Chao%20Chen%20and%20Nan%20Li%20and%20Feng%20Wang%20and%20Xin%20Lu%0AAbstract%3A%20%20%20Multi-turn%20dialogues%20are%20a%20key%20interaction%20method%20between%20humans%20and%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20as%20conversations%20extend%20over%20multiple%20rounds%2C%20keeping%0ALLMs%27%20high%20generation%20quality%20and%20low%20latency%20is%20a%20challenge.%20Mainstream%20LLMs%0Acan%20be%20grouped%20into%20two%20categories%20based%20on%20masking%20strategy%3A%20causal%20LLM%20and%0Aprefix%20LLM.%20Several%20works%20have%20demonstrated%20that%20prefix%20LLMs%20tend%20to%20outperform%0Acausal%20ones%20in%20scenarios%20that%20heavily%20depend%20on%20historical%20context%20such%20as%0Amulti-turn%20dialogues%20or%20in-context%20learning%2C%20thanks%20to%20their%20bidirectional%0Aattention%20on%20prefix%20sequences.%20However%2C%20prefix%20LLMs%20have%20an%20inherent%0Ainefficient%20training%20problem%20in%20multi-turn%20dialogue%20datasets.%20In%20addition%2C%20the%0Aattention%20mechanism%20of%20prefix%20LLM%20makes%20it%20unable%20to%20reuse%20Key-Value%20Cache%20%28KV%0ACache%29%20across%20dialogue%20rounds%20to%20reduce%20generation%20latency.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20masking%20scheme%20called%20Intermittent%20Semi-working%20Mask%20%28ISM%29%20to%0Aaddress%20these%20problems.%20Specifically%2C%20we%20apply%20alternate%20bidirectional%20and%0Aunidirectional%20attention%20on%20queries%20and%20answers%20in%20the%20dialogue%20history.%20In%0Athis%20way%2C%20ISM%20is%20able%20to%20maintain%20the%20high%20quality%20of%20prefix%20LLM%20and%20low%0Ageneration%20latency%20of%20causal%20LLM%2C%20simultaneously.%20Extensive%20experiments%0Aillustrate%20that%20our%20ISM%20achieves%20significant%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntermittent%2520Semi-working%2520Mask%253A%2520A%2520New%2520Masking%2520Paradigm%2520for%2520LLMs%26entry.906535625%3DMingcong%2520Lu%2520and%2520Jiangcai%2520Zhu%2520and%2520Wang%2520Hao%2520and%2520Zheng%2520Li%2520and%2520Shusheng%2520Zhang%2520and%2520Kailai%2520Shao%2520and%2520Chao%2520Chen%2520and%2520Nan%2520Li%2520and%2520Feng%2520Wang%2520and%2520Xin%2520Lu%26entry.1292438233%3D%2520%2520Multi-turn%2520dialogues%2520are%2520a%2520key%2520interaction%2520method%2520between%2520humans%2520and%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520as%2520conversations%2520extend%2520over%2520multiple%2520rounds%252C%2520keeping%250ALLMs%2527%2520high%2520generation%2520quality%2520and%2520low%2520latency%2520is%2520a%2520challenge.%2520Mainstream%2520LLMs%250Acan%2520be%2520grouped%2520into%2520two%2520categories%2520based%2520on%2520masking%2520strategy%253A%2520causal%2520LLM%2520and%250Aprefix%2520LLM.%2520Several%2520works%2520have%2520demonstrated%2520that%2520prefix%2520LLMs%2520tend%2520to%2520outperform%250Acausal%2520ones%2520in%2520scenarios%2520that%2520heavily%2520depend%2520on%2520historical%2520context%2520such%2520as%250Amulti-turn%2520dialogues%2520or%2520in-context%2520learning%252C%2520thanks%2520to%2520their%2520bidirectional%250Aattention%2520on%2520prefix%2520sequences.%2520However%252C%2520prefix%2520LLMs%2520have%2520an%2520inherent%250Ainefficient%2520training%2520problem%2520in%2520multi-turn%2520dialogue%2520datasets.%2520In%2520addition%252C%2520the%250Aattention%2520mechanism%2520of%2520prefix%2520LLM%2520makes%2520it%2520unable%2520to%2520reuse%2520Key-Value%2520Cache%2520%2528KV%250ACache%2529%2520across%2520dialogue%2520rounds%2520to%2520reduce%2520generation%2520latency.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520masking%2520scheme%2520called%2520Intermittent%2520Semi-working%2520Mask%2520%2528ISM%2529%2520to%250Aaddress%2520these%2520problems.%2520Specifically%252C%2520we%2520apply%2520alternate%2520bidirectional%2520and%250Aunidirectional%2520attention%2520on%2520queries%2520and%2520answers%2520in%2520the%2520dialogue%2520history.%2520In%250Athis%2520way%252C%2520ISM%2520is%2520able%2520to%2520maintain%2520the%2520high%2520quality%2520of%2520prefix%2520LLM%2520and%2520low%250Ageneration%2520latency%2520of%2520causal%2520LLM%252C%2520simultaneously.%2520Extensive%2520experiments%250Aillustrate%2520that%2520our%2520ISM%2520achieves%2520significant%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intermittent%20Semi-working%20Mask%3A%20A%20New%20Masking%20Paradigm%20for%20LLMs&entry.906535625=Mingcong%20Lu%20and%20Jiangcai%20Zhu%20and%20Wang%20Hao%20and%20Zheng%20Li%20and%20Shusheng%20Zhang%20and%20Kailai%20Shao%20and%20Chao%20Chen%20and%20Nan%20Li%20and%20Feng%20Wang%20and%20Xin%20Lu&entry.1292438233=%20%20Multi-turn%20dialogues%20are%20a%20key%20interaction%20method%20between%20humans%20and%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20as%20conversations%20extend%20over%20multiple%20rounds%2C%20keeping%0ALLMs%27%20high%20generation%20quality%20and%20low%20latency%20is%20a%20challenge.%20Mainstream%20LLMs%0Acan%20be%20grouped%20into%20two%20categories%20based%20on%20masking%20strategy%3A%20causal%20LLM%20and%0Aprefix%20LLM.%20Several%20works%20have%20demonstrated%20that%20prefix%20LLMs%20tend%20to%20outperform%0Acausal%20ones%20in%20scenarios%20that%20heavily%20depend%20on%20historical%20context%20such%20as%0Amulti-turn%20dialogues%20or%20in-context%20learning%2C%20thanks%20to%20their%20bidirectional%0Aattention%20on%20prefix%20sequences.%20However%2C%20prefix%20LLMs%20have%20an%20inherent%0Ainefficient%20training%20problem%20in%20multi-turn%20dialogue%20datasets.%20In%20addition%2C%20the%0Aattention%20mechanism%20of%20prefix%20LLM%20makes%20it%20unable%20to%20reuse%20Key-Value%20Cache%20%28KV%0ACache%29%20across%20dialogue%20rounds%20to%20reduce%20generation%20latency.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20masking%20scheme%20called%20Intermittent%20Semi-working%20Mask%20%28ISM%29%20to%0Aaddress%20these%20problems.%20Specifically%2C%20we%20apply%20alternate%20bidirectional%20and%0Aunidirectional%20attention%20on%20queries%20and%20answers%20in%20the%20dialogue%20history.%20In%0Athis%20way%2C%20ISM%20is%20able%20to%20maintain%20the%20high%20quality%20of%20prefix%20LLM%20and%20low%0Ageneration%20latency%20of%20causal%20LLM%2C%20simultaneously.%20Extensive%20experiments%0Aillustrate%20that%20our%20ISM%20achieves%20significant%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00539v1&entry.124074799=Read"},
{"title": "Tiered Reward: Designing Rewards for Specification and Fast Learning of\n  Desired Behavior", "author": "Zhiyuan Zhou and Shreyas Sundara Raman and Henry Sowerby and Michael L. Littman", "abstract": "  Reinforcement-learning agents seek to maximize a reward signal through\nenvironmental interactions. As humans, our job in the learning process is to\ndesign reward functions to express desired behavior and enable the agent to\nlearn such behavior swiftly. However, designing good reward functions to induce\nthe desired behavior is generally hard, let alone the question of which rewards\nmake learning fast. In this work, we introduce a family of a reward structures\nwe call Tiered Reward that addresses both of these questions. We consider the\nreward-design problem in tasks formulated as reaching desirable states and\navoiding undesirable states. To start, we propose a strict partial ordering of\nthe policy space to resolve trade-offs in behavior preference. We prefer\npolicies that reach the good states faster and with higher probability while\navoiding the bad states longer. Next, we introduce Tiered Reward, a class of\nenvironment-independent reward functions and show it is guaranteed to induce\npolicies that are Pareto-optimal according to our preference relation. Finally,\nwe demonstrate that Tiered Reward leads to fast learning with multiple tabular\nand deep reinforcement-learning algorithms.\n", "link": "http://arxiv.org/abs/2212.03733v3", "date": "2024-08-01", "relevancy": 1.8503, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4727}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4644}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiered%20Reward%3A%20Designing%20Rewards%20for%20Specification%20and%20Fast%20Learning%20of%0A%20%20Desired%20Behavior&body=Title%3A%20Tiered%20Reward%3A%20Designing%20Rewards%20for%20Specification%20and%20Fast%20Learning%20of%0A%20%20Desired%20Behavior%0AAuthor%3A%20Zhiyuan%20Zhou%20and%20Shreyas%20Sundara%20Raman%20and%20Henry%20Sowerby%20and%20Michael%20L.%20Littman%0AAbstract%3A%20%20%20Reinforcement-learning%20agents%20seek%20to%20maximize%20a%20reward%20signal%20through%0Aenvironmental%20interactions.%20As%20humans%2C%20our%20job%20in%20the%20learning%20process%20is%20to%0Adesign%20reward%20functions%20to%20express%20desired%20behavior%20and%20enable%20the%20agent%20to%0Alearn%20such%20behavior%20swiftly.%20However%2C%20designing%20good%20reward%20functions%20to%20induce%0Athe%20desired%20behavior%20is%20generally%20hard%2C%20let%20alone%20the%20question%20of%20which%20rewards%0Amake%20learning%20fast.%20In%20this%20work%2C%20we%20introduce%20a%20family%20of%20a%20reward%20structures%0Awe%20call%20Tiered%20Reward%20that%20addresses%20both%20of%20these%20questions.%20We%20consider%20the%0Areward-design%20problem%20in%20tasks%20formulated%20as%20reaching%20desirable%20states%20and%0Aavoiding%20undesirable%20states.%20To%20start%2C%20we%20propose%20a%20strict%20partial%20ordering%20of%0Athe%20policy%20space%20to%20resolve%20trade-offs%20in%20behavior%20preference.%20We%20prefer%0Apolicies%20that%20reach%20the%20good%20states%20faster%20and%20with%20higher%20probability%20while%0Aavoiding%20the%20bad%20states%20longer.%20Next%2C%20we%20introduce%20Tiered%20Reward%2C%20a%20class%20of%0Aenvironment-independent%20reward%20functions%20and%20show%20it%20is%20guaranteed%20to%20induce%0Apolicies%20that%20are%20Pareto-optimal%20according%20to%20our%20preference%20relation.%20Finally%2C%0Awe%20demonstrate%20that%20Tiered%20Reward%20leads%20to%20fast%20learning%20with%20multiple%20tabular%0Aand%20deep%20reinforcement-learning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.03733v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiered%2520Reward%253A%2520Designing%2520Rewards%2520for%2520Specification%2520and%2520Fast%2520Learning%2520of%250A%2520%2520Desired%2520Behavior%26entry.906535625%3DZhiyuan%2520Zhou%2520and%2520Shreyas%2520Sundara%2520Raman%2520and%2520Henry%2520Sowerby%2520and%2520Michael%2520L.%2520Littman%26entry.1292438233%3D%2520%2520Reinforcement-learning%2520agents%2520seek%2520to%2520maximize%2520a%2520reward%2520signal%2520through%250Aenvironmental%2520interactions.%2520As%2520humans%252C%2520our%2520job%2520in%2520the%2520learning%2520process%2520is%2520to%250Adesign%2520reward%2520functions%2520to%2520express%2520desired%2520behavior%2520and%2520enable%2520the%2520agent%2520to%250Alearn%2520such%2520behavior%2520swiftly.%2520However%252C%2520designing%2520good%2520reward%2520functions%2520to%2520induce%250Athe%2520desired%2520behavior%2520is%2520generally%2520hard%252C%2520let%2520alone%2520the%2520question%2520of%2520which%2520rewards%250Amake%2520learning%2520fast.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520family%2520of%2520a%2520reward%2520structures%250Awe%2520call%2520Tiered%2520Reward%2520that%2520addresses%2520both%2520of%2520these%2520questions.%2520We%2520consider%2520the%250Areward-design%2520problem%2520in%2520tasks%2520formulated%2520as%2520reaching%2520desirable%2520states%2520and%250Aavoiding%2520undesirable%2520states.%2520To%2520start%252C%2520we%2520propose%2520a%2520strict%2520partial%2520ordering%2520of%250Athe%2520policy%2520space%2520to%2520resolve%2520trade-offs%2520in%2520behavior%2520preference.%2520We%2520prefer%250Apolicies%2520that%2520reach%2520the%2520good%2520states%2520faster%2520and%2520with%2520higher%2520probability%2520while%250Aavoiding%2520the%2520bad%2520states%2520longer.%2520Next%252C%2520we%2520introduce%2520Tiered%2520Reward%252C%2520a%2520class%2520of%250Aenvironment-independent%2520reward%2520functions%2520and%2520show%2520it%2520is%2520guaranteed%2520to%2520induce%250Apolicies%2520that%2520are%2520Pareto-optimal%2520according%2520to%2520our%2520preference%2520relation.%2520Finally%252C%250Awe%2520demonstrate%2520that%2520Tiered%2520Reward%2520leads%2520to%2520fast%2520learning%2520with%2520multiple%2520tabular%250Aand%2520deep%2520reinforcement-learning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.03733v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiered%20Reward%3A%20Designing%20Rewards%20for%20Specification%20and%20Fast%20Learning%20of%0A%20%20Desired%20Behavior&entry.906535625=Zhiyuan%20Zhou%20and%20Shreyas%20Sundara%20Raman%20and%20Henry%20Sowerby%20and%20Michael%20L.%20Littman&entry.1292438233=%20%20Reinforcement-learning%20agents%20seek%20to%20maximize%20a%20reward%20signal%20through%0Aenvironmental%20interactions.%20As%20humans%2C%20our%20job%20in%20the%20learning%20process%20is%20to%0Adesign%20reward%20functions%20to%20express%20desired%20behavior%20and%20enable%20the%20agent%20to%0Alearn%20such%20behavior%20swiftly.%20However%2C%20designing%20good%20reward%20functions%20to%20induce%0Athe%20desired%20behavior%20is%20generally%20hard%2C%20let%20alone%20the%20question%20of%20which%20rewards%0Amake%20learning%20fast.%20In%20this%20work%2C%20we%20introduce%20a%20family%20of%20a%20reward%20structures%0Awe%20call%20Tiered%20Reward%20that%20addresses%20both%20of%20these%20questions.%20We%20consider%20the%0Areward-design%20problem%20in%20tasks%20formulated%20as%20reaching%20desirable%20states%20and%0Aavoiding%20undesirable%20states.%20To%20start%2C%20we%20propose%20a%20strict%20partial%20ordering%20of%0Athe%20policy%20space%20to%20resolve%20trade-offs%20in%20behavior%20preference.%20We%20prefer%0Apolicies%20that%20reach%20the%20good%20states%20faster%20and%20with%20higher%20probability%20while%0Aavoiding%20the%20bad%20states%20longer.%20Next%2C%20we%20introduce%20Tiered%20Reward%2C%20a%20class%20of%0Aenvironment-independent%20reward%20functions%20and%20show%20it%20is%20guaranteed%20to%20induce%0Apolicies%20that%20are%20Pareto-optimal%20according%20to%20our%20preference%20relation.%20Finally%2C%0Awe%20demonstrate%20that%20Tiered%20Reward%20leads%20to%20fast%20learning%20with%20multiple%20tabular%0Aand%20deep%20reinforcement-learning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.03733v3&entry.124074799=Read"},
{"title": "Analysis of Functional Insufficiencies and Triggering Conditions to\n  Improve the SOTIF of an MPC-based Trajectory Planner", "author": "Mirko Conrad and Georg Schildbach", "abstract": "  Automated and autonomous driving has made a significant technological leap\nover the past decade. In this process, the complexity of algorithms used for\nvehicle control has grown significantly. Model Predictive Control (MPC) is a\nprominent example, which has gained enormous popularity and is now widely used\nfor vehicle motion planning and control. However, safety concerns constrain its\npractical application, especially since traditional procedures of functional\nsafety (FS), with its universal standard ISO26262, reach their limits.\nConcomitantly, the new aspect of safety-of-the-intended-function (SOTIF) has\nmoved into the center of attention, whose standard, ISO21448, has only been\nreleased in 2022. Thus, experience with SOTIF is low and few case studies are\navailable in industry and research. Hence this paper aims to make two main\ncontributions: (1) an analysis of the SOTIF for a generic MPC-based trajectory\nplanner and (2) an interpretation and concrete application of the generic\nprocedures described in ISO21448 for determining functional insufficiencies\n(FIs) and triggering conditions (TCs). Particular novelties of the paper\ninclude an approach for the out-of-context development of SOTIF-related\nelements (SOTIF-EooC), a compilation of important FIs and TCs for a MPC-based\ntrajectory planner, and an optimized safety concept based on the identified FIs\nand TCs for the MPC-based trajectory planner.\n", "link": "http://arxiv.org/abs/2407.21569v2", "date": "2024-08-01", "relevancy": 1.8419, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5018}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4548}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Functional%20Insufficiencies%20and%20Triggering%20Conditions%20to%0A%20%20Improve%20the%20SOTIF%20of%20an%20MPC-based%20Trajectory%20Planner&body=Title%3A%20Analysis%20of%20Functional%20Insufficiencies%20and%20Triggering%20Conditions%20to%0A%20%20Improve%20the%20SOTIF%20of%20an%20MPC-based%20Trajectory%20Planner%0AAuthor%3A%20Mirko%20Conrad%20and%20Georg%20Schildbach%0AAbstract%3A%20%20%20Automated%20and%20autonomous%20driving%20has%20made%20a%20significant%20technological%20leap%0Aover%20the%20past%20decade.%20In%20this%20process%2C%20the%20complexity%20of%20algorithms%20used%20for%0Avehicle%20control%20has%20grown%20significantly.%20Model%20Predictive%20Control%20%28MPC%29%20is%20a%0Aprominent%20example%2C%20which%20has%20gained%20enormous%20popularity%20and%20is%20now%20widely%20used%0Afor%20vehicle%20motion%20planning%20and%20control.%20However%2C%20safety%20concerns%20constrain%20its%0Apractical%20application%2C%20especially%20since%20traditional%20procedures%20of%20functional%0Asafety%20%28FS%29%2C%20with%20its%20universal%20standard%20ISO26262%2C%20reach%20their%20limits.%0AConcomitantly%2C%20the%20new%20aspect%20of%20safety-of-the-intended-function%20%28SOTIF%29%20has%0Amoved%20into%20the%20center%20of%20attention%2C%20whose%20standard%2C%20ISO21448%2C%20has%20only%20been%0Areleased%20in%202022.%20Thus%2C%20experience%20with%20SOTIF%20is%20low%20and%20few%20case%20studies%20are%0Aavailable%20in%20industry%20and%20research.%20Hence%20this%20paper%20aims%20to%20make%20two%20main%0Acontributions%3A%20%281%29%20an%20analysis%20of%20the%20SOTIF%20for%20a%20generic%20MPC-based%20trajectory%0Aplanner%20and%20%282%29%20an%20interpretation%20and%20concrete%20application%20of%20the%20generic%0Aprocedures%20described%20in%20ISO21448%20for%20determining%20functional%20insufficiencies%0A%28FIs%29%20and%20triggering%20conditions%20%28TCs%29.%20Particular%20novelties%20of%20the%20paper%0Ainclude%20an%20approach%20for%20the%20out-of-context%20development%20of%20SOTIF-related%0Aelements%20%28SOTIF-EooC%29%2C%20a%20compilation%20of%20important%20FIs%20and%20TCs%20for%20a%20MPC-based%0Atrajectory%20planner%2C%20and%20an%20optimized%20safety%20concept%20based%20on%20the%20identified%20FIs%0Aand%20TCs%20for%20the%20MPC-based%20trajectory%20planner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Functional%2520Insufficiencies%2520and%2520Triggering%2520Conditions%2520to%250A%2520%2520Improve%2520the%2520SOTIF%2520of%2520an%2520MPC-based%2520Trajectory%2520Planner%26entry.906535625%3DMirko%2520Conrad%2520and%2520Georg%2520Schildbach%26entry.1292438233%3D%2520%2520Automated%2520and%2520autonomous%2520driving%2520has%2520made%2520a%2520significant%2520technological%2520leap%250Aover%2520the%2520past%2520decade.%2520In%2520this%2520process%252C%2520the%2520complexity%2520of%2520algorithms%2520used%2520for%250Avehicle%2520control%2520has%2520grown%2520significantly.%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520is%2520a%250Aprominent%2520example%252C%2520which%2520has%2520gained%2520enormous%2520popularity%2520and%2520is%2520now%2520widely%2520used%250Afor%2520vehicle%2520motion%2520planning%2520and%2520control.%2520However%252C%2520safety%2520concerns%2520constrain%2520its%250Apractical%2520application%252C%2520especially%2520since%2520traditional%2520procedures%2520of%2520functional%250Asafety%2520%2528FS%2529%252C%2520with%2520its%2520universal%2520standard%2520ISO26262%252C%2520reach%2520their%2520limits.%250AConcomitantly%252C%2520the%2520new%2520aspect%2520of%2520safety-of-the-intended-function%2520%2528SOTIF%2529%2520has%250Amoved%2520into%2520the%2520center%2520of%2520attention%252C%2520whose%2520standard%252C%2520ISO21448%252C%2520has%2520only%2520been%250Areleased%2520in%25202022.%2520Thus%252C%2520experience%2520with%2520SOTIF%2520is%2520low%2520and%2520few%2520case%2520studies%2520are%250Aavailable%2520in%2520industry%2520and%2520research.%2520Hence%2520this%2520paper%2520aims%2520to%2520make%2520two%2520main%250Acontributions%253A%2520%25281%2529%2520an%2520analysis%2520of%2520the%2520SOTIF%2520for%2520a%2520generic%2520MPC-based%2520trajectory%250Aplanner%2520and%2520%25282%2529%2520an%2520interpretation%2520and%2520concrete%2520application%2520of%2520the%2520generic%250Aprocedures%2520described%2520in%2520ISO21448%2520for%2520determining%2520functional%2520insufficiencies%250A%2528FIs%2529%2520and%2520triggering%2520conditions%2520%2528TCs%2529.%2520Particular%2520novelties%2520of%2520the%2520paper%250Ainclude%2520an%2520approach%2520for%2520the%2520out-of-context%2520development%2520of%2520SOTIF-related%250Aelements%2520%2528SOTIF-EooC%2529%252C%2520a%2520compilation%2520of%2520important%2520FIs%2520and%2520TCs%2520for%2520a%2520MPC-based%250Atrajectory%2520planner%252C%2520and%2520an%2520optimized%2520safety%2520concept%2520based%2520on%2520the%2520identified%2520FIs%250Aand%2520TCs%2520for%2520the%2520MPC-based%2520trajectory%2520planner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Functional%20Insufficiencies%20and%20Triggering%20Conditions%20to%0A%20%20Improve%20the%20SOTIF%20of%20an%20MPC-based%20Trajectory%20Planner&entry.906535625=Mirko%20Conrad%20and%20Georg%20Schildbach&entry.1292438233=%20%20Automated%20and%20autonomous%20driving%20has%20made%20a%20significant%20technological%20leap%0Aover%20the%20past%20decade.%20In%20this%20process%2C%20the%20complexity%20of%20algorithms%20used%20for%0Avehicle%20control%20has%20grown%20significantly.%20Model%20Predictive%20Control%20%28MPC%29%20is%20a%0Aprominent%20example%2C%20which%20has%20gained%20enormous%20popularity%20and%20is%20now%20widely%20used%0Afor%20vehicle%20motion%20planning%20and%20control.%20However%2C%20safety%20concerns%20constrain%20its%0Apractical%20application%2C%20especially%20since%20traditional%20procedures%20of%20functional%0Asafety%20%28FS%29%2C%20with%20its%20universal%20standard%20ISO26262%2C%20reach%20their%20limits.%0AConcomitantly%2C%20the%20new%20aspect%20of%20safety-of-the-intended-function%20%28SOTIF%29%20has%0Amoved%20into%20the%20center%20of%20attention%2C%20whose%20standard%2C%20ISO21448%2C%20has%20only%20been%0Areleased%20in%202022.%20Thus%2C%20experience%20with%20SOTIF%20is%20low%20and%20few%20case%20studies%20are%0Aavailable%20in%20industry%20and%20research.%20Hence%20this%20paper%20aims%20to%20make%20two%20main%0Acontributions%3A%20%281%29%20an%20analysis%20of%20the%20SOTIF%20for%20a%20generic%20MPC-based%20trajectory%0Aplanner%20and%20%282%29%20an%20interpretation%20and%20concrete%20application%20of%20the%20generic%0Aprocedures%20described%20in%20ISO21448%20for%20determining%20functional%20insufficiencies%0A%28FIs%29%20and%20triggering%20conditions%20%28TCs%29.%20Particular%20novelties%20of%20the%20paper%0Ainclude%20an%20approach%20for%20the%20out-of-context%20development%20of%20SOTIF-related%0Aelements%20%28SOTIF-EooC%29%2C%20a%20compilation%20of%20important%20FIs%20and%20TCs%20for%20a%20MPC-based%0Atrajectory%20planner%2C%20and%20an%20optimized%20safety%20concept%20based%20on%20the%20identified%20FIs%0Aand%20TCs%20for%20the%20MPC-based%20trajectory%20planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21569v2&entry.124074799=Read"},
{"title": "Tamper-Resistant Safeguards for Open-Weight LLMs", "author": "Rishub Tamirisa and Bhrugu Bharathi and Long Phan and Andy Zhou and Alice Gatti and Tarun Suresh and Maxwell Lin and Justin Wang and Rowan Wang and Ron Arel and Andy Zou and Dawn Song and Bo Li and Dan Hendrycks and Mantas Mazeika", "abstract": "  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n", "link": "http://arxiv.org/abs/2408.00761v1", "date": "2024-08-01", "relevancy": 1.8414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4876}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tamper-Resistant%20Safeguards%20for%20Open-Weight%20LLMs&body=Title%3A%20Tamper-Resistant%20Safeguards%20for%20Open-Weight%20LLMs%0AAuthor%3A%20Rishub%20Tamirisa%20and%20Bhrugu%20Bharathi%20and%20Long%20Phan%20and%20Andy%20Zhou%20and%20Alice%20Gatti%20and%20Tarun%20Suresh%20and%20Maxwell%20Lin%20and%20Justin%20Wang%20and%20Rowan%20Wang%20and%20Ron%20Arel%20and%20Andy%20Zou%20and%20Dawn%20Song%20and%20Bo%20Li%20and%20Dan%20Hendrycks%20and%20Mantas%20Mazeika%0AAbstract%3A%20%20%20Rapid%20advances%20in%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%0Araised%20widespread%20concerns%20regarding%20their%20potential%20for%20malicious%20use.%0AOpen-weight%20LLMs%20present%20unique%20challenges%2C%20as%20existing%20safeguards%20lack%0Arobustness%20to%20tampering%20attacks%20that%20modify%20model%20weights.%20For%20example%2C%20recent%0Aworks%20have%20demonstrated%20that%20refusal%20and%20unlearning%20safeguards%20can%20be%20trivially%0Aremoved%20with%20a%20few%20steps%20of%20fine-tuning.%20These%20vulnerabilities%20necessitate%20new%0Aapproaches%20for%20enabling%20the%20safe%20release%20of%20open-weight%20LLMs.%20We%20develop%20a%0Amethod%2C%20called%20TAR%2C%20for%20building%20tamper-resistant%20safeguards%20into%20open-weight%0ALLMs%20such%20that%20adversaries%20cannot%20remove%20the%20safeguards%20even%20after%20thousands%20of%0Asteps%20of%20fine-tuning.%20In%20extensive%20evaluations%20and%20red%20teaming%20analyses%2C%20we%0Afind%20that%20our%20method%20greatly%20improves%20tamper-resistance%20while%20preserving%20benign%0Acapabilities.%20Our%20results%20demonstrate%20that%20tamper-resistance%20is%20a%20tractable%0Aproblem%2C%20opening%20up%20a%20promising%20new%20avenue%20to%20improve%20the%20safety%20and%20security%0Aof%20open-weight%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTamper-Resistant%2520Safeguards%2520for%2520Open-Weight%2520LLMs%26entry.906535625%3DRishub%2520Tamirisa%2520and%2520Bhrugu%2520Bharathi%2520and%2520Long%2520Phan%2520and%2520Andy%2520Zhou%2520and%2520Alice%2520Gatti%2520and%2520Tarun%2520Suresh%2520and%2520Maxwell%2520Lin%2520and%2520Justin%2520Wang%2520and%2520Rowan%2520Wang%2520and%2520Ron%2520Arel%2520and%2520Andy%2520Zou%2520and%2520Dawn%2520Song%2520and%2520Bo%2520Li%2520and%2520Dan%2520Hendrycks%2520and%2520Mantas%2520Mazeika%26entry.1292438233%3D%2520%2520Rapid%2520advances%2520in%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Araised%2520widespread%2520concerns%2520regarding%2520their%2520potential%2520for%2520malicious%2520use.%250AOpen-weight%2520LLMs%2520present%2520unique%2520challenges%252C%2520as%2520existing%2520safeguards%2520lack%250Arobustness%2520to%2520tampering%2520attacks%2520that%2520modify%2520model%2520weights.%2520For%2520example%252C%2520recent%250Aworks%2520have%2520demonstrated%2520that%2520refusal%2520and%2520unlearning%2520safeguards%2520can%2520be%2520trivially%250Aremoved%2520with%2520a%2520few%2520steps%2520of%2520fine-tuning.%2520These%2520vulnerabilities%2520necessitate%2520new%250Aapproaches%2520for%2520enabling%2520the%2520safe%2520release%2520of%2520open-weight%2520LLMs.%2520We%2520develop%2520a%250Amethod%252C%2520called%2520TAR%252C%2520for%2520building%2520tamper-resistant%2520safeguards%2520into%2520open-weight%250ALLMs%2520such%2520that%2520adversaries%2520cannot%2520remove%2520the%2520safeguards%2520even%2520after%2520thousands%2520of%250Asteps%2520of%2520fine-tuning.%2520In%2520extensive%2520evaluations%2520and%2520red%2520teaming%2520analyses%252C%2520we%250Afind%2520that%2520our%2520method%2520greatly%2520improves%2520tamper-resistance%2520while%2520preserving%2520benign%250Acapabilities.%2520Our%2520results%2520demonstrate%2520that%2520tamper-resistance%2520is%2520a%2520tractable%250Aproblem%252C%2520opening%2520up%2520a%2520promising%2520new%2520avenue%2520to%2520improve%2520the%2520safety%2520and%2520security%250Aof%2520open-weight%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tamper-Resistant%20Safeguards%20for%20Open-Weight%20LLMs&entry.906535625=Rishub%20Tamirisa%20and%20Bhrugu%20Bharathi%20and%20Long%20Phan%20and%20Andy%20Zhou%20and%20Alice%20Gatti%20and%20Tarun%20Suresh%20and%20Maxwell%20Lin%20and%20Justin%20Wang%20and%20Rowan%20Wang%20and%20Ron%20Arel%20and%20Andy%20Zou%20and%20Dawn%20Song%20and%20Bo%20Li%20and%20Dan%20Hendrycks%20and%20Mantas%20Mazeika&entry.1292438233=%20%20Rapid%20advances%20in%20the%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20have%0Araised%20widespread%20concerns%20regarding%20their%20potential%20for%20malicious%20use.%0AOpen-weight%20LLMs%20present%20unique%20challenges%2C%20as%20existing%20safeguards%20lack%0Arobustness%20to%20tampering%20attacks%20that%20modify%20model%20weights.%20For%20example%2C%20recent%0Aworks%20have%20demonstrated%20that%20refusal%20and%20unlearning%20safeguards%20can%20be%20trivially%0Aremoved%20with%20a%20few%20steps%20of%20fine-tuning.%20These%20vulnerabilities%20necessitate%20new%0Aapproaches%20for%20enabling%20the%20safe%20release%20of%20open-weight%20LLMs.%20We%20develop%20a%0Amethod%2C%20called%20TAR%2C%20for%20building%20tamper-resistant%20safeguards%20into%20open-weight%0ALLMs%20such%20that%20adversaries%20cannot%20remove%20the%20safeguards%20even%20after%20thousands%20of%0Asteps%20of%20fine-tuning.%20In%20extensive%20evaluations%20and%20red%20teaming%20analyses%2C%20we%0Afind%20that%20our%20method%20greatly%20improves%20tamper-resistance%20while%20preserving%20benign%0Acapabilities.%20Our%20results%20demonstrate%20that%20tamper-resistance%20is%20a%20tractable%0Aproblem%2C%20opening%20up%20a%20promising%20new%20avenue%20to%20improve%20the%20safety%20and%20security%0Aof%20open-weight%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00761v1&entry.124074799=Read"},
{"title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention", "author": "Susung Hong", "abstract": "  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n", "link": "http://arxiv.org/abs/2408.00760v1", "date": "2024-08-01", "relevancy": 1.8385, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6269}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.611}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothed%20Energy%20Guidance%3A%20Guiding%20Diffusion%20Models%20with%20Reduced%20Energy%0A%20%20Curvature%20of%20Attention&body=Title%3A%20Smoothed%20Energy%20Guidance%3A%20Guiding%20Diffusion%20Models%20with%20Reduced%20Energy%0A%20%20Curvature%20of%20Attention%0AAuthor%3A%20Susung%20Hong%0AAbstract%3A%20%20%20Conditional%20diffusion%20models%20have%20shown%20remarkable%20success%20in%20visual%20content%0Ageneration%2C%20producing%20high-quality%20samples%20across%20various%20domains%2C%20largely%20due%0Ato%20classifier-free%20guidance%20%28CFG%29.%20Recent%20attempts%20to%20extend%20guidance%20to%0Aunconditional%20models%20have%20relied%20on%20heuristic%20techniques%2C%20resulting%20in%0Asuboptimal%20generation%20quality%20and%20unintended%20effects.%20In%20this%20work%2C%20we%20propose%0ASmoothed%20Energy%20Guidance%20%28SEG%29%2C%20a%20novel%20training-%20and%20condition-free%20approach%0Athat%20leverages%20the%20energy-based%20perspective%20of%20the%20self-attention%20mechanism%20to%0Aenhance%20image%20generation.%20By%20defining%20the%20energy%20of%20self-attention%2C%20we%0Aintroduce%20a%20method%20to%20reduce%20the%20curvature%20of%20the%20energy%20landscape%20of%20attention%0Aand%20use%20the%20output%20as%20the%20unconditional%20prediction.%20Practically%2C%20we%20control%20the%0Acurvature%20of%20the%20energy%20landscape%20by%20adjusting%20the%20Gaussian%20kernel%20parameter%0Awhile%20keeping%20the%20guidance%20scale%20parameter%20fixed.%20Additionally%2C%20we%20present%20a%0Aquery%20blurring%20method%20that%20is%20equivalent%20to%20blurring%20the%20entire%20attention%0Aweights%20without%20incurring%20quadratic%20complexity%20in%20the%20number%20of%20tokens.%20In%20our%0Aexperiments%2C%20SEG%20achieves%20a%20Pareto%20improvement%20in%20both%20quality%20and%20the%0Areduction%20of%20side%20effects.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/SusungHong/SEG-SDXL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothed%2520Energy%2520Guidance%253A%2520Guiding%2520Diffusion%2520Models%2520with%2520Reduced%2520Energy%250A%2520%2520Curvature%2520of%2520Attention%26entry.906535625%3DSusung%2520Hong%26entry.1292438233%3D%2520%2520Conditional%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520success%2520in%2520visual%2520content%250Ageneration%252C%2520producing%2520high-quality%2520samples%2520across%2520various%2520domains%252C%2520largely%2520due%250Ato%2520classifier-free%2520guidance%2520%2528CFG%2529.%2520Recent%2520attempts%2520to%2520extend%2520guidance%2520to%250Aunconditional%2520models%2520have%2520relied%2520on%2520heuristic%2520techniques%252C%2520resulting%2520in%250Asuboptimal%2520generation%2520quality%2520and%2520unintended%2520effects.%2520In%2520this%2520work%252C%2520we%2520propose%250ASmoothed%2520Energy%2520Guidance%2520%2528SEG%2529%252C%2520a%2520novel%2520training-%2520and%2520condition-free%2520approach%250Athat%2520leverages%2520the%2520energy-based%2520perspective%2520of%2520the%2520self-attention%2520mechanism%2520to%250Aenhance%2520image%2520generation.%2520By%2520defining%2520the%2520energy%2520of%2520self-attention%252C%2520we%250Aintroduce%2520a%2520method%2520to%2520reduce%2520the%2520curvature%2520of%2520the%2520energy%2520landscape%2520of%2520attention%250Aand%2520use%2520the%2520output%2520as%2520the%2520unconditional%2520prediction.%2520Practically%252C%2520we%2520control%2520the%250Acurvature%2520of%2520the%2520energy%2520landscape%2520by%2520adjusting%2520the%2520Gaussian%2520kernel%2520parameter%250Awhile%2520keeping%2520the%2520guidance%2520scale%2520parameter%2520fixed.%2520Additionally%252C%2520we%2520present%2520a%250Aquery%2520blurring%2520method%2520that%2520is%2520equivalent%2520to%2520blurring%2520the%2520entire%2520attention%250Aweights%2520without%2520incurring%2520quadratic%2520complexity%2520in%2520the%2520number%2520of%2520tokens.%2520In%2520our%250Aexperiments%252C%2520SEG%2520achieves%2520a%2520Pareto%2520improvement%2520in%2520both%2520quality%2520and%2520the%250Areduction%2520of%2520side%2520effects.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/SusungHong/SEG-SDXL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothed%20Energy%20Guidance%3A%20Guiding%20Diffusion%20Models%20with%20Reduced%20Energy%0A%20%20Curvature%20of%20Attention&entry.906535625=Susung%20Hong&entry.1292438233=%20%20Conditional%20diffusion%20models%20have%20shown%20remarkable%20success%20in%20visual%20content%0Ageneration%2C%20producing%20high-quality%20samples%20across%20various%20domains%2C%20largely%20due%0Ato%20classifier-free%20guidance%20%28CFG%29.%20Recent%20attempts%20to%20extend%20guidance%20to%0Aunconditional%20models%20have%20relied%20on%20heuristic%20techniques%2C%20resulting%20in%0Asuboptimal%20generation%20quality%20and%20unintended%20effects.%20In%20this%20work%2C%20we%20propose%0ASmoothed%20Energy%20Guidance%20%28SEG%29%2C%20a%20novel%20training-%20and%20condition-free%20approach%0Athat%20leverages%20the%20energy-based%20perspective%20of%20the%20self-attention%20mechanism%20to%0Aenhance%20image%20generation.%20By%20defining%20the%20energy%20of%20self-attention%2C%20we%0Aintroduce%20a%20method%20to%20reduce%20the%20curvature%20of%20the%20energy%20landscape%20of%20attention%0Aand%20use%20the%20output%20as%20the%20unconditional%20prediction.%20Practically%2C%20we%20control%20the%0Acurvature%20of%20the%20energy%20landscape%20by%20adjusting%20the%20Gaussian%20kernel%20parameter%0Awhile%20keeping%20the%20guidance%20scale%20parameter%20fixed.%20Additionally%2C%20we%20present%20a%0Aquery%20blurring%20method%20that%20is%20equivalent%20to%20blurring%20the%20entire%20attention%0Aweights%20without%20incurring%20quadratic%20complexity%20in%20the%20number%20of%20tokens.%20In%20our%0Aexperiments%2C%20SEG%20achieves%20a%20Pareto%20improvement%20in%20both%20quality%20and%20the%0Areduction%20of%20side%20effects.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/SusungHong/SEG-SDXL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00760v1&entry.124074799=Read"},
{"title": "Insurance Portfolio Pursuit with Reinforcement Learning", "author": "Edward James Young and Alistair Rogers and Elliott Tong and James Jordon", "abstract": "  When faced with a new customer, many factors contribute to an insurance\nfirm's decision of what offer to make to that customer. In addition to the\nexpected cost of providing the insurance, the firm must consider the other\noffers likely to be made to the customer, and how sensitive the customer is to\ndifferences in price. Moreover, firms often target a specific portfolio of\ncustomers that could depend on, e.g., age, location, and occupation. Given such\na target portfolio, firms may choose to modulate an individual customer's offer\nbased on whether the firm desires the customer within their portfolio. Given a\ntarget portfolio, we term the problem of modulating offers to achieve this\ntarget portfolio the portfolio pursuit problem. We give a formulation of\nportfolio pursuit as a sequential decision making problem, and devise a novel\nreinforcement learning algorithm for its solution. We test our method on a\ncomplex synthetic market environment, and demonstrate that it outperforms a\nbaseline method which mimics current industry approaches to portfolio pursuit.\n", "link": "http://arxiv.org/abs/2408.00713v1", "date": "2024-08-01", "relevancy": 1.8356, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4766}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4666}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insurance%20Portfolio%20Pursuit%20with%20Reinforcement%20Learning&body=Title%3A%20Insurance%20Portfolio%20Pursuit%20with%20Reinforcement%20Learning%0AAuthor%3A%20Edward%20James%20Young%20and%20Alistair%20Rogers%20and%20Elliott%20Tong%20and%20James%20Jordon%0AAbstract%3A%20%20%20When%20faced%20with%20a%20new%20customer%2C%20many%20factors%20contribute%20to%20an%20insurance%0Afirm%27s%20decision%20of%20what%20offer%20to%20make%20to%20that%20customer.%20In%20addition%20to%20the%0Aexpected%20cost%20of%20providing%20the%20insurance%2C%20the%20firm%20must%20consider%20the%20other%0Aoffers%20likely%20to%20be%20made%20to%20the%20customer%2C%20and%20how%20sensitive%20the%20customer%20is%20to%0Adifferences%20in%20price.%20Moreover%2C%20firms%20often%20target%20a%20specific%20portfolio%20of%0Acustomers%20that%20could%20depend%20on%2C%20e.g.%2C%20age%2C%20location%2C%20and%20occupation.%20Given%20such%0Aa%20target%20portfolio%2C%20firms%20may%20choose%20to%20modulate%20an%20individual%20customer%27s%20offer%0Abased%20on%20whether%20the%20firm%20desires%20the%20customer%20within%20their%20portfolio.%20Given%20a%0Atarget%20portfolio%2C%20we%20term%20the%20problem%20of%20modulating%20offers%20to%20achieve%20this%0Atarget%20portfolio%20the%20portfolio%20pursuit%20problem.%20We%20give%20a%20formulation%20of%0Aportfolio%20pursuit%20as%20a%20sequential%20decision%20making%20problem%2C%20and%20devise%20a%20novel%0Areinforcement%20learning%20algorithm%20for%20its%20solution.%20We%20test%20our%20method%20on%20a%0Acomplex%20synthetic%20market%20environment%2C%20and%20demonstrate%20that%20it%20outperforms%20a%0Abaseline%20method%20which%20mimics%20current%20industry%20approaches%20to%20portfolio%20pursuit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsurance%2520Portfolio%2520Pursuit%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DEdward%2520James%2520Young%2520and%2520Alistair%2520Rogers%2520and%2520Elliott%2520Tong%2520and%2520James%2520Jordon%26entry.1292438233%3D%2520%2520When%2520faced%2520with%2520a%2520new%2520customer%252C%2520many%2520factors%2520contribute%2520to%2520an%2520insurance%250Afirm%2527s%2520decision%2520of%2520what%2520offer%2520to%2520make%2520to%2520that%2520customer.%2520In%2520addition%2520to%2520the%250Aexpected%2520cost%2520of%2520providing%2520the%2520insurance%252C%2520the%2520firm%2520must%2520consider%2520the%2520other%250Aoffers%2520likely%2520to%2520be%2520made%2520to%2520the%2520customer%252C%2520and%2520how%2520sensitive%2520the%2520customer%2520is%2520to%250Adifferences%2520in%2520price.%2520Moreover%252C%2520firms%2520often%2520target%2520a%2520specific%2520portfolio%2520of%250Acustomers%2520that%2520could%2520depend%2520on%252C%2520e.g.%252C%2520age%252C%2520location%252C%2520and%2520occupation.%2520Given%2520such%250Aa%2520target%2520portfolio%252C%2520firms%2520may%2520choose%2520to%2520modulate%2520an%2520individual%2520customer%2527s%2520offer%250Abased%2520on%2520whether%2520the%2520firm%2520desires%2520the%2520customer%2520within%2520their%2520portfolio.%2520Given%2520a%250Atarget%2520portfolio%252C%2520we%2520term%2520the%2520problem%2520of%2520modulating%2520offers%2520to%2520achieve%2520this%250Atarget%2520portfolio%2520the%2520portfolio%2520pursuit%2520problem.%2520We%2520give%2520a%2520formulation%2520of%250Aportfolio%2520pursuit%2520as%2520a%2520sequential%2520decision%2520making%2520problem%252C%2520and%2520devise%2520a%2520novel%250Areinforcement%2520learning%2520algorithm%2520for%2520its%2520solution.%2520We%2520test%2520our%2520method%2520on%2520a%250Acomplex%2520synthetic%2520market%2520environment%252C%2520and%2520demonstrate%2520that%2520it%2520outperforms%2520a%250Abaseline%2520method%2520which%2520mimics%2520current%2520industry%2520approaches%2520to%2520portfolio%2520pursuit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insurance%20Portfolio%20Pursuit%20with%20Reinforcement%20Learning&entry.906535625=Edward%20James%20Young%20and%20Alistair%20Rogers%20and%20Elliott%20Tong%20and%20James%20Jordon&entry.1292438233=%20%20When%20faced%20with%20a%20new%20customer%2C%20many%20factors%20contribute%20to%20an%20insurance%0Afirm%27s%20decision%20of%20what%20offer%20to%20make%20to%20that%20customer.%20In%20addition%20to%20the%0Aexpected%20cost%20of%20providing%20the%20insurance%2C%20the%20firm%20must%20consider%20the%20other%0Aoffers%20likely%20to%20be%20made%20to%20the%20customer%2C%20and%20how%20sensitive%20the%20customer%20is%20to%0Adifferences%20in%20price.%20Moreover%2C%20firms%20often%20target%20a%20specific%20portfolio%20of%0Acustomers%20that%20could%20depend%20on%2C%20e.g.%2C%20age%2C%20location%2C%20and%20occupation.%20Given%20such%0Aa%20target%20portfolio%2C%20firms%20may%20choose%20to%20modulate%20an%20individual%20customer%27s%20offer%0Abased%20on%20whether%20the%20firm%20desires%20the%20customer%20within%20their%20portfolio.%20Given%20a%0Atarget%20portfolio%2C%20we%20term%20the%20problem%20of%20modulating%20offers%20to%20achieve%20this%0Atarget%20portfolio%20the%20portfolio%20pursuit%20problem.%20We%20give%20a%20formulation%20of%0Aportfolio%20pursuit%20as%20a%20sequential%20decision%20making%20problem%2C%20and%20devise%20a%20novel%0Areinforcement%20learning%20algorithm%20for%20its%20solution.%20We%20test%20our%20method%20on%20a%0Acomplex%20synthetic%20market%20environment%2C%20and%20demonstrate%20that%20it%20outperforms%20a%0Abaseline%20method%20which%20mimics%20current%20industry%20approaches%20to%20portfolio%20pursuit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00713v1&entry.124074799=Read"},
{"title": "Generative AI for Health Technology Assessment: Opportunities,\n  Challenges, and Policy Considerations", "author": "Rachael Fleurence and Jiang Bian and Xiaoyan Wang and Hua Xu and Dalia Dawoud and Mitch Higashi and Jagpreet Chhatwal", "abstract": "  This review introduces the transformative potential of generative Artificial\nIntelligence (AI) and foundation models, including large language models\n(LLMs), for health technology assessment (HTA). We explore their applications\nin four critical areas, evidence synthesis, evidence generation, clinical\ntrials and economic modeling: (1) Evidence synthesis: Generative AI has the\npotential to assist in automating literature reviews and meta-analyses by\nproposing search terms, screening abstracts, and extracting data with notable\naccuracy; (2) Evidence generation: These models can potentially facilitate\nautomating the process and analyze the increasingly available large collections\nof real-world data (RWD), including unstructured clinical notes and imaging,\nenhancing the speed and quality of real-world evidence (RWE) generation; (3)\nClinical trials: Generative AI can be used to optimize trial design, improve\npatient matching, and manage trial data more efficiently; and (4) Economic\nmodeling: Generative AI can also aid in the development of health economic\nmodels, from conceptualization to validation, thus streamlining the overall HTA\nprocess. Despite their promise, these technologies, while rapidly improving,\nare still nascent and continued careful evaluation in their applications to HTA\nis required. To ensure their responsible use and implementation, both\ndevelopers and users of research incorporating these tools, should familiarize\nthemselves with their current limitations, including the issues related to\nscientific validity, risk of bias, and consider equity and ethical\nimplications. We also surveyed the current policy landscape and provide\nsuggestions for HTA agencies on responsibly integrating generative AI into\ntheir workflows, emphasizing the importance of human oversight and the\nfast-evolving nature of these tools.\n", "link": "http://arxiv.org/abs/2407.11054v2", "date": "2024-08-01", "relevancy": 1.8352, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4912}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4592}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Health%20Technology%20Assessment%3A%20Opportunities%2C%0A%20%20Challenges%2C%20and%20Policy%20Considerations&body=Title%3A%20Generative%20AI%20for%20Health%20Technology%20Assessment%3A%20Opportunities%2C%0A%20%20Challenges%2C%20and%20Policy%20Considerations%0AAuthor%3A%20Rachael%20Fleurence%20and%20Jiang%20Bian%20and%20Xiaoyan%20Wang%20and%20Hua%20Xu%20and%20Dalia%20Dawoud%20and%20Mitch%20Higashi%20and%20Jagpreet%20Chhatwal%0AAbstract%3A%20%20%20This%20review%20introduces%20the%20transformative%20potential%20of%20generative%20Artificial%0AIntelligence%20%28AI%29%20and%20foundation%20models%2C%20including%20large%20language%20models%0A%28LLMs%29%2C%20for%20health%20technology%20assessment%20%28HTA%29.%20We%20explore%20their%20applications%0Ain%20four%20critical%20areas%2C%20evidence%20synthesis%2C%20evidence%20generation%2C%20clinical%0Atrials%20and%20economic%20modeling%3A%20%281%29%20Evidence%20synthesis%3A%20Generative%20AI%20has%20the%0Apotential%20to%20assist%20in%20automating%20literature%20reviews%20and%20meta-analyses%20by%0Aproposing%20search%20terms%2C%20screening%20abstracts%2C%20and%20extracting%20data%20with%20notable%0Aaccuracy%3B%20%282%29%20Evidence%20generation%3A%20These%20models%20can%20potentially%20facilitate%0Aautomating%20the%20process%20and%20analyze%20the%20increasingly%20available%20large%20collections%0Aof%20real-world%20data%20%28RWD%29%2C%20including%20unstructured%20clinical%20notes%20and%20imaging%2C%0Aenhancing%20the%20speed%20and%20quality%20of%20real-world%20evidence%20%28RWE%29%20generation%3B%20%283%29%0AClinical%20trials%3A%20Generative%20AI%20can%20be%20used%20to%20optimize%20trial%20design%2C%20improve%0Apatient%20matching%2C%20and%20manage%20trial%20data%20more%20efficiently%3B%20and%20%284%29%20Economic%0Amodeling%3A%20Generative%20AI%20can%20also%20aid%20in%20the%20development%20of%20health%20economic%0Amodels%2C%20from%20conceptualization%20to%20validation%2C%20thus%20streamlining%20the%20overall%20HTA%0Aprocess.%20Despite%20their%20promise%2C%20these%20technologies%2C%20while%20rapidly%20improving%2C%0Aare%20still%20nascent%20and%20continued%20careful%20evaluation%20in%20their%20applications%20to%20HTA%0Ais%20required.%20To%20ensure%20their%20responsible%20use%20and%20implementation%2C%20both%0Adevelopers%20and%20users%20of%20research%20incorporating%20these%20tools%2C%20should%20familiarize%0Athemselves%20with%20their%20current%20limitations%2C%20including%20the%20issues%20related%20to%0Ascientific%20validity%2C%20risk%20of%20bias%2C%20and%20consider%20equity%20and%20ethical%0Aimplications.%20We%20also%20surveyed%20the%20current%20policy%20landscape%20and%20provide%0Asuggestions%20for%20HTA%20agencies%20on%20responsibly%20integrating%20generative%20AI%20into%0Atheir%20workflows%2C%20emphasizing%20the%20importance%20of%20human%20oversight%20and%20the%0Afast-evolving%20nature%20of%20these%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Health%2520Technology%2520Assessment%253A%2520Opportunities%252C%250A%2520%2520Challenges%252C%2520and%2520Policy%2520Considerations%26entry.906535625%3DRachael%2520Fleurence%2520and%2520Jiang%2520Bian%2520and%2520Xiaoyan%2520Wang%2520and%2520Hua%2520Xu%2520and%2520Dalia%2520Dawoud%2520and%2520Mitch%2520Higashi%2520and%2520Jagpreet%2520Chhatwal%26entry.1292438233%3D%2520%2520This%2520review%2520introduces%2520the%2520transformative%2520potential%2520of%2520generative%2520Artificial%250AIntelligence%2520%2528AI%2529%2520and%2520foundation%2520models%252C%2520including%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520for%2520health%2520technology%2520assessment%2520%2528HTA%2529.%2520We%2520explore%2520their%2520applications%250Ain%2520four%2520critical%2520areas%252C%2520evidence%2520synthesis%252C%2520evidence%2520generation%252C%2520clinical%250Atrials%2520and%2520economic%2520modeling%253A%2520%25281%2529%2520Evidence%2520synthesis%253A%2520Generative%2520AI%2520has%2520the%250Apotential%2520to%2520assist%2520in%2520automating%2520literature%2520reviews%2520and%2520meta-analyses%2520by%250Aproposing%2520search%2520terms%252C%2520screening%2520abstracts%252C%2520and%2520extracting%2520data%2520with%2520notable%250Aaccuracy%253B%2520%25282%2529%2520Evidence%2520generation%253A%2520These%2520models%2520can%2520potentially%2520facilitate%250Aautomating%2520the%2520process%2520and%2520analyze%2520the%2520increasingly%2520available%2520large%2520collections%250Aof%2520real-world%2520data%2520%2528RWD%2529%252C%2520including%2520unstructured%2520clinical%2520notes%2520and%2520imaging%252C%250Aenhancing%2520the%2520speed%2520and%2520quality%2520of%2520real-world%2520evidence%2520%2528RWE%2529%2520generation%253B%2520%25283%2529%250AClinical%2520trials%253A%2520Generative%2520AI%2520can%2520be%2520used%2520to%2520optimize%2520trial%2520design%252C%2520improve%250Apatient%2520matching%252C%2520and%2520manage%2520trial%2520data%2520more%2520efficiently%253B%2520and%2520%25284%2529%2520Economic%250Amodeling%253A%2520Generative%2520AI%2520can%2520also%2520aid%2520in%2520the%2520development%2520of%2520health%2520economic%250Amodels%252C%2520from%2520conceptualization%2520to%2520validation%252C%2520thus%2520streamlining%2520the%2520overall%2520HTA%250Aprocess.%2520Despite%2520their%2520promise%252C%2520these%2520technologies%252C%2520while%2520rapidly%2520improving%252C%250Aare%2520still%2520nascent%2520and%2520continued%2520careful%2520evaluation%2520in%2520their%2520applications%2520to%2520HTA%250Ais%2520required.%2520To%2520ensure%2520their%2520responsible%2520use%2520and%2520implementation%252C%2520both%250Adevelopers%2520and%2520users%2520of%2520research%2520incorporating%2520these%2520tools%252C%2520should%2520familiarize%250Athemselves%2520with%2520their%2520current%2520limitations%252C%2520including%2520the%2520issues%2520related%2520to%250Ascientific%2520validity%252C%2520risk%2520of%2520bias%252C%2520and%2520consider%2520equity%2520and%2520ethical%250Aimplications.%2520We%2520also%2520surveyed%2520the%2520current%2520policy%2520landscape%2520and%2520provide%250Asuggestions%2520for%2520HTA%2520agencies%2520on%2520responsibly%2520integrating%2520generative%2520AI%2520into%250Atheir%2520workflows%252C%2520emphasizing%2520the%2520importance%2520of%2520human%2520oversight%2520and%2520the%250Afast-evolving%2520nature%2520of%2520these%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Health%20Technology%20Assessment%3A%20Opportunities%2C%0A%20%20Challenges%2C%20and%20Policy%20Considerations&entry.906535625=Rachael%20Fleurence%20and%20Jiang%20Bian%20and%20Xiaoyan%20Wang%20and%20Hua%20Xu%20and%20Dalia%20Dawoud%20and%20Mitch%20Higashi%20and%20Jagpreet%20Chhatwal&entry.1292438233=%20%20This%20review%20introduces%20the%20transformative%20potential%20of%20generative%20Artificial%0AIntelligence%20%28AI%29%20and%20foundation%20models%2C%20including%20large%20language%20models%0A%28LLMs%29%2C%20for%20health%20technology%20assessment%20%28HTA%29.%20We%20explore%20their%20applications%0Ain%20four%20critical%20areas%2C%20evidence%20synthesis%2C%20evidence%20generation%2C%20clinical%0Atrials%20and%20economic%20modeling%3A%20%281%29%20Evidence%20synthesis%3A%20Generative%20AI%20has%20the%0Apotential%20to%20assist%20in%20automating%20literature%20reviews%20and%20meta-analyses%20by%0Aproposing%20search%20terms%2C%20screening%20abstracts%2C%20and%20extracting%20data%20with%20notable%0Aaccuracy%3B%20%282%29%20Evidence%20generation%3A%20These%20models%20can%20potentially%20facilitate%0Aautomating%20the%20process%20and%20analyze%20the%20increasingly%20available%20large%20collections%0Aof%20real-world%20data%20%28RWD%29%2C%20including%20unstructured%20clinical%20notes%20and%20imaging%2C%0Aenhancing%20the%20speed%20and%20quality%20of%20real-world%20evidence%20%28RWE%29%20generation%3B%20%283%29%0AClinical%20trials%3A%20Generative%20AI%20can%20be%20used%20to%20optimize%20trial%20design%2C%20improve%0Apatient%20matching%2C%20and%20manage%20trial%20data%20more%20efficiently%3B%20and%20%284%29%20Economic%0Amodeling%3A%20Generative%20AI%20can%20also%20aid%20in%20the%20development%20of%20health%20economic%0Amodels%2C%20from%20conceptualization%20to%20validation%2C%20thus%20streamlining%20the%20overall%20HTA%0Aprocess.%20Despite%20their%20promise%2C%20these%20technologies%2C%20while%20rapidly%20improving%2C%0Aare%20still%20nascent%20and%20continued%20careful%20evaluation%20in%20their%20applications%20to%20HTA%0Ais%20required.%20To%20ensure%20their%20responsible%20use%20and%20implementation%2C%20both%0Adevelopers%20and%20users%20of%20research%20incorporating%20these%20tools%2C%20should%20familiarize%0Athemselves%20with%20their%20current%20limitations%2C%20including%20the%20issues%20related%20to%0Ascientific%20validity%2C%20risk%20of%20bias%2C%20and%20consider%20equity%20and%20ethical%0Aimplications.%20We%20also%20surveyed%20the%20current%20policy%20landscape%20and%20provide%0Asuggestions%20for%20HTA%20agencies%20on%20responsibly%20integrating%20generative%20AI%20into%0Atheir%20workflows%2C%20emphasizing%20the%20importance%20of%20human%20oversight%20and%20the%0Afast-evolving%20nature%20of%20these%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11054v2&entry.124074799=Read"},
{"title": "Neyman-Pearson Multi-class Classification via Cost-sensitive Learning", "author": "Ye Tian and Yang Feng", "abstract": "  Most existing classification methods aim to minimize the overall\nmisclassification error rate. However, in applications such as loan default\nprediction, different types of errors can have varying consequences. To address\nthis asymmetry issue, two popular paradigms have been developed: the\nNeyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous\nstudies on the NP paradigm have primarily focused on the binary case, while the\nmulti-class NP problem poses a greater challenge due to its unknown\nfeasibility. In this work, we tackle the multi-class NP problem by establishing\na connection with the CS problem via strong duality and propose two algorithms.\nWe extend the concept of NP oracle inequalities, crucial in binary\nclassifications, to NP oracle properties in the multi-class context. Our\nalgorithms satisfy these NP oracle properties under certain conditions.\nFurthermore, we develop practical algorithms to assess the feasibility and\nstrong duality in multi-class NP problems, which can offer practitioners the\nlandscape of a multi-class NP problem with various target error levels.\nSimulations and real data studies validate the effectiveness of our algorithms.\nTo our knowledge, this is the first study to address the multi-class NP problem\nwith theoretical guarantees. The proposed algorithms have been implemented in\nthe R package \\texttt{npcs}, which is available on CRAN.\n", "link": "http://arxiv.org/abs/2111.04597v4", "date": "2024-08-01", "relevancy": 1.826, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5308}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4481}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neyman-Pearson%20Multi-class%20Classification%20via%20Cost-sensitive%20Learning&body=Title%3A%20Neyman-Pearson%20Multi-class%20Classification%20via%20Cost-sensitive%20Learning%0AAuthor%3A%20Ye%20Tian%20and%20Yang%20Feng%0AAbstract%3A%20%20%20Most%20existing%20classification%20methods%20aim%20to%20minimize%20the%20overall%0Amisclassification%20error%20rate.%20However%2C%20in%20applications%20such%20as%20loan%20default%0Aprediction%2C%20different%20types%20of%20errors%20can%20have%20varying%20consequences.%20To%20address%0Athis%20asymmetry%20issue%2C%20two%20popular%20paradigms%20have%20been%20developed%3A%20the%0ANeyman-Pearson%20%28NP%29%20paradigm%20and%20the%20cost-sensitive%20%28CS%29%20paradigm.%20Previous%0Astudies%20on%20the%20NP%20paradigm%20have%20primarily%20focused%20on%20the%20binary%20case%2C%20while%20the%0Amulti-class%20NP%20problem%20poses%20a%20greater%20challenge%20due%20to%20its%20unknown%0Afeasibility.%20In%20this%20work%2C%20we%20tackle%20the%20multi-class%20NP%20problem%20by%20establishing%0Aa%20connection%20with%20the%20CS%20problem%20via%20strong%20duality%20and%20propose%20two%20algorithms.%0AWe%20extend%20the%20concept%20of%20NP%20oracle%20inequalities%2C%20crucial%20in%20binary%0Aclassifications%2C%20to%20NP%20oracle%20properties%20in%20the%20multi-class%20context.%20Our%0Aalgorithms%20satisfy%20these%20NP%20oracle%20properties%20under%20certain%20conditions.%0AFurthermore%2C%20we%20develop%20practical%20algorithms%20to%20assess%20the%20feasibility%20and%0Astrong%20duality%20in%20multi-class%20NP%20problems%2C%20which%20can%20offer%20practitioners%20the%0Alandscape%20of%20a%20multi-class%20NP%20problem%20with%20various%20target%20error%20levels.%0ASimulations%20and%20real%20data%20studies%20validate%20the%20effectiveness%20of%20our%20algorithms.%0ATo%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%20address%20the%20multi-class%20NP%20problem%0Awith%20theoretical%20guarantees.%20The%20proposed%20algorithms%20have%20been%20implemented%20in%0Athe%20R%20package%20%5Ctexttt%7Bnpcs%7D%2C%20which%20is%20available%20on%20CRAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2111.04597v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeyman-Pearson%2520Multi-class%2520Classification%2520via%2520Cost-sensitive%2520Learning%26entry.906535625%3DYe%2520Tian%2520and%2520Yang%2520Feng%26entry.1292438233%3D%2520%2520Most%2520existing%2520classification%2520methods%2520aim%2520to%2520minimize%2520the%2520overall%250Amisclassification%2520error%2520rate.%2520However%252C%2520in%2520applications%2520such%2520as%2520loan%2520default%250Aprediction%252C%2520different%2520types%2520of%2520errors%2520can%2520have%2520varying%2520consequences.%2520To%2520address%250Athis%2520asymmetry%2520issue%252C%2520two%2520popular%2520paradigms%2520have%2520been%2520developed%253A%2520the%250ANeyman-Pearson%2520%2528NP%2529%2520paradigm%2520and%2520the%2520cost-sensitive%2520%2528CS%2529%2520paradigm.%2520Previous%250Astudies%2520on%2520the%2520NP%2520paradigm%2520have%2520primarily%2520focused%2520on%2520the%2520binary%2520case%252C%2520while%2520the%250Amulti-class%2520NP%2520problem%2520poses%2520a%2520greater%2520challenge%2520due%2520to%2520its%2520unknown%250Afeasibility.%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520multi-class%2520NP%2520problem%2520by%2520establishing%250Aa%2520connection%2520with%2520the%2520CS%2520problem%2520via%2520strong%2520duality%2520and%2520propose%2520two%2520algorithms.%250AWe%2520extend%2520the%2520concept%2520of%2520NP%2520oracle%2520inequalities%252C%2520crucial%2520in%2520binary%250Aclassifications%252C%2520to%2520NP%2520oracle%2520properties%2520in%2520the%2520multi-class%2520context.%2520Our%250Aalgorithms%2520satisfy%2520these%2520NP%2520oracle%2520properties%2520under%2520certain%2520conditions.%250AFurthermore%252C%2520we%2520develop%2520practical%2520algorithms%2520to%2520assess%2520the%2520feasibility%2520and%250Astrong%2520duality%2520in%2520multi-class%2520NP%2520problems%252C%2520which%2520can%2520offer%2520practitioners%2520the%250Alandscape%2520of%2520a%2520multi-class%2520NP%2520problem%2520with%2520various%2520target%2520error%2520levels.%250ASimulations%2520and%2520real%2520data%2520studies%2520validate%2520the%2520effectiveness%2520of%2520our%2520algorithms.%250ATo%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520study%2520to%2520address%2520the%2520multi-class%2520NP%2520problem%250Awith%2520theoretical%2520guarantees.%2520The%2520proposed%2520algorithms%2520have%2520been%2520implemented%2520in%250Athe%2520R%2520package%2520%255Ctexttt%257Bnpcs%257D%252C%2520which%2520is%2520available%2520on%2520CRAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2111.04597v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neyman-Pearson%20Multi-class%20Classification%20via%20Cost-sensitive%20Learning&entry.906535625=Ye%20Tian%20and%20Yang%20Feng&entry.1292438233=%20%20Most%20existing%20classification%20methods%20aim%20to%20minimize%20the%20overall%0Amisclassification%20error%20rate.%20However%2C%20in%20applications%20such%20as%20loan%20default%0Aprediction%2C%20different%20types%20of%20errors%20can%20have%20varying%20consequences.%20To%20address%0Athis%20asymmetry%20issue%2C%20two%20popular%20paradigms%20have%20been%20developed%3A%20the%0ANeyman-Pearson%20%28NP%29%20paradigm%20and%20the%20cost-sensitive%20%28CS%29%20paradigm.%20Previous%0Astudies%20on%20the%20NP%20paradigm%20have%20primarily%20focused%20on%20the%20binary%20case%2C%20while%20the%0Amulti-class%20NP%20problem%20poses%20a%20greater%20challenge%20due%20to%20its%20unknown%0Afeasibility.%20In%20this%20work%2C%20we%20tackle%20the%20multi-class%20NP%20problem%20by%20establishing%0Aa%20connection%20with%20the%20CS%20problem%20via%20strong%20duality%20and%20propose%20two%20algorithms.%0AWe%20extend%20the%20concept%20of%20NP%20oracle%20inequalities%2C%20crucial%20in%20binary%0Aclassifications%2C%20to%20NP%20oracle%20properties%20in%20the%20multi-class%20context.%20Our%0Aalgorithms%20satisfy%20these%20NP%20oracle%20properties%20under%20certain%20conditions.%0AFurthermore%2C%20we%20develop%20practical%20algorithms%20to%20assess%20the%20feasibility%20and%0Astrong%20duality%20in%20multi-class%20NP%20problems%2C%20which%20can%20offer%20practitioners%20the%0Alandscape%20of%20a%20multi-class%20NP%20problem%20with%20various%20target%20error%20levels.%0ASimulations%20and%20real%20data%20studies%20validate%20the%20effectiveness%20of%20our%20algorithms.%0ATo%20our%20knowledge%2C%20this%20is%20the%20first%20study%20to%20address%20the%20multi-class%20NP%20problem%0Awith%20theoretical%20guarantees.%20The%20proposed%20algorithms%20have%20been%20implemented%20in%0Athe%20R%20package%20%5Ctexttt%7Bnpcs%7D%2C%20which%20is%20available%20on%20CRAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.04597v4&entry.124074799=Read"},
{"title": "CERT-ED: Certifiably Robust Text Classification for Edit Distance", "author": "Zhuoqun Huang and Neil G Marchant and Olga Ohrimenko and Benjamin I. P. Rubinstein", "abstract": "  With the growing integration of AI in daily life, ensuring the robustness of\nsystems to inference-time attacks is crucial. Among the approaches for\ncertifying robustness to such adversarial examples, randomized smoothing has\nemerged as highly promising due to its nature as a wrapper around arbitrary\nblack-box models. Previous work on randomized smoothing in natural language\nprocessing has primarily focused on specific subsets of edit distance\noperations, such as synonym substitution or word insertion, without exploring\nthe certification of all edit operations. In this paper, we adapt Randomized\nDeletion (Huang et al., 2023) and propose, CERTified Edit Distance defense\n(CERT-ED) for natural language classification. Through comprehensive\nexperiments, we demonstrate that CERT-ED outperforms the existing Hamming\ndistance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of\nboth accuracy and the cardinality of the certificate. By covering various\nthreat models, including 5 direct and 5 transfer attacks, our method improves\nempirical robustness in 38 out of 50 settings.\n", "link": "http://arxiv.org/abs/2408.00728v1", "date": "2024-08-01", "relevancy": 1.8232, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5158}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.444}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CERT-ED%3A%20Certifiably%20Robust%20Text%20Classification%20for%20Edit%20Distance&body=Title%3A%20CERT-ED%3A%20Certifiably%20Robust%20Text%20Classification%20for%20Edit%20Distance%0AAuthor%3A%20Zhuoqun%20Huang%20and%20Neil%20G%20Marchant%20and%20Olga%20Ohrimenko%20and%20Benjamin%20I.%20P.%20Rubinstein%0AAbstract%3A%20%20%20With%20the%20growing%20integration%20of%20AI%20in%20daily%20life%2C%20ensuring%20the%20robustness%20of%0Asystems%20to%20inference-time%20attacks%20is%20crucial.%20Among%20the%20approaches%20for%0Acertifying%20robustness%20to%20such%20adversarial%20examples%2C%20randomized%20smoothing%20has%0Aemerged%20as%20highly%20promising%20due%20to%20its%20nature%20as%20a%20wrapper%20around%20arbitrary%0Ablack-box%20models.%20Previous%20work%20on%20randomized%20smoothing%20in%20natural%20language%0Aprocessing%20has%20primarily%20focused%20on%20specific%20subsets%20of%20edit%20distance%0Aoperations%2C%20such%20as%20synonym%20substitution%20or%20word%20insertion%2C%20without%20exploring%0Athe%20certification%20of%20all%20edit%20operations.%20In%20this%20paper%2C%20we%20adapt%20Randomized%0ADeletion%20%28Huang%20et%20al.%2C%202023%29%20and%20propose%2C%20CERTified%20Edit%20Distance%20defense%0A%28CERT-ED%29%20for%20natural%20language%20classification.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20CERT-ED%20outperforms%20the%20existing%20Hamming%0Adistance%20method%20RanMASK%20%28Zeng%20et%20al.%2C%202023%29%20in%204%20out%20of%205%20datasets%20in%20terms%20of%0Aboth%20accuracy%20and%20the%20cardinality%20of%20the%20certificate.%20By%20covering%20various%0Athreat%20models%2C%20including%205%20direct%20and%205%20transfer%20attacks%2C%20our%20method%20improves%0Aempirical%20robustness%20in%2038%20out%20of%2050%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCERT-ED%253A%2520Certifiably%2520Robust%2520Text%2520Classification%2520for%2520Edit%2520Distance%26entry.906535625%3DZhuoqun%2520Huang%2520and%2520Neil%2520G%2520Marchant%2520and%2520Olga%2520Ohrimenko%2520and%2520Benjamin%2520I.%2520P.%2520Rubinstein%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520integration%2520of%2520AI%2520in%2520daily%2520life%252C%2520ensuring%2520the%2520robustness%2520of%250Asystems%2520to%2520inference-time%2520attacks%2520is%2520crucial.%2520Among%2520the%2520approaches%2520for%250Acertifying%2520robustness%2520to%2520such%2520adversarial%2520examples%252C%2520randomized%2520smoothing%2520has%250Aemerged%2520as%2520highly%2520promising%2520due%2520to%2520its%2520nature%2520as%2520a%2520wrapper%2520around%2520arbitrary%250Ablack-box%2520models.%2520Previous%2520work%2520on%2520randomized%2520smoothing%2520in%2520natural%2520language%250Aprocessing%2520has%2520primarily%2520focused%2520on%2520specific%2520subsets%2520of%2520edit%2520distance%250Aoperations%252C%2520such%2520as%2520synonym%2520substitution%2520or%2520word%2520insertion%252C%2520without%2520exploring%250Athe%2520certification%2520of%2520all%2520edit%2520operations.%2520In%2520this%2520paper%252C%2520we%2520adapt%2520Randomized%250ADeletion%2520%2528Huang%2520et%2520al.%252C%25202023%2529%2520and%2520propose%252C%2520CERTified%2520Edit%2520Distance%2520defense%250A%2528CERT-ED%2529%2520for%2520natural%2520language%2520classification.%2520Through%2520comprehensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520CERT-ED%2520outperforms%2520the%2520existing%2520Hamming%250Adistance%2520method%2520RanMASK%2520%2528Zeng%2520et%2520al.%252C%25202023%2529%2520in%25204%2520out%2520of%25205%2520datasets%2520in%2520terms%2520of%250Aboth%2520accuracy%2520and%2520the%2520cardinality%2520of%2520the%2520certificate.%2520By%2520covering%2520various%250Athreat%2520models%252C%2520including%25205%2520direct%2520and%25205%2520transfer%2520attacks%252C%2520our%2520method%2520improves%250Aempirical%2520robustness%2520in%252038%2520out%2520of%252050%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CERT-ED%3A%20Certifiably%20Robust%20Text%20Classification%20for%20Edit%20Distance&entry.906535625=Zhuoqun%20Huang%20and%20Neil%20G%20Marchant%20and%20Olga%20Ohrimenko%20and%20Benjamin%20I.%20P.%20Rubinstein&entry.1292438233=%20%20With%20the%20growing%20integration%20of%20AI%20in%20daily%20life%2C%20ensuring%20the%20robustness%20of%0Asystems%20to%20inference-time%20attacks%20is%20crucial.%20Among%20the%20approaches%20for%0Acertifying%20robustness%20to%20such%20adversarial%20examples%2C%20randomized%20smoothing%20has%0Aemerged%20as%20highly%20promising%20due%20to%20its%20nature%20as%20a%20wrapper%20around%20arbitrary%0Ablack-box%20models.%20Previous%20work%20on%20randomized%20smoothing%20in%20natural%20language%0Aprocessing%20has%20primarily%20focused%20on%20specific%20subsets%20of%20edit%20distance%0Aoperations%2C%20such%20as%20synonym%20substitution%20or%20word%20insertion%2C%20without%20exploring%0Athe%20certification%20of%20all%20edit%20operations.%20In%20this%20paper%2C%20we%20adapt%20Randomized%0ADeletion%20%28Huang%20et%20al.%2C%202023%29%20and%20propose%2C%20CERTified%20Edit%20Distance%20defense%0A%28CERT-ED%29%20for%20natural%20language%20classification.%20Through%20comprehensive%0Aexperiments%2C%20we%20demonstrate%20that%20CERT-ED%20outperforms%20the%20existing%20Hamming%0Adistance%20method%20RanMASK%20%28Zeng%20et%20al.%2C%202023%29%20in%204%20out%20of%205%20datasets%20in%20terms%20of%0Aboth%20accuracy%20and%20the%20cardinality%20of%20the%20certificate.%20By%20covering%20various%0Athreat%20models%2C%20including%205%20direct%20and%205%20transfer%20attacks%2C%20our%20method%20improves%0Aempirical%20robustness%20in%2038%20out%20of%2050%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00728v1&entry.124074799=Read"},
{"title": "Ontological Relations from Word Embeddings", "author": "Mathieu d'Aquin and Emmanuel Nauer", "abstract": "  It has been reliably shown that the similarity of word embeddings obtained\nfrom popular neural models such as BERT approximates effectively a form of\nsemantic similarity of the meaning of those words. It is therefore natural to\nwonder if those embeddings contain enough information to be able to connect\nthose meanings through ontological relationships such as the one of\nsubsumption. If so, large knowledge models could be built that are capable of\nsemantically relating terms based on the information encapsulated in word\nembeddings produced by pre-trained models, with implications not only for\nontologies (ontology matching, ontology evolution, etc.) but also on the\nability to integrate ontological knowledge in neural models. In this paper, we\ntest how embeddings produced by several pre-trained models can be used to\npredict relations existing between classes and properties of popular\nupper-level and general ontologies. We show that even a simple feed-forward\narchitecture on top of those embeddings can achieve promising accuracies, with\nvarying generalisation abilities depending on the input data. To achieve that,\nwe produce a dataset that can be used to further enhance those models, opening\nnew possibilities for applications integrating knowledge from web ontologies.\n", "link": "http://arxiv.org/abs/2408.00444v1", "date": "2024-08-01", "relevancy": 1.8133, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4548}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ontological%20Relations%20from%20Word%20Embeddings&body=Title%3A%20Ontological%20Relations%20from%20Word%20Embeddings%0AAuthor%3A%20Mathieu%20d%27Aquin%20and%20Emmanuel%20Nauer%0AAbstract%3A%20%20%20It%20has%20been%20reliably%20shown%20that%20the%20similarity%20of%20word%20embeddings%20obtained%0Afrom%20popular%20neural%20models%20such%20as%20BERT%20approximates%20effectively%20a%20form%20of%0Asemantic%20similarity%20of%20the%20meaning%20of%20those%20words.%20It%20is%20therefore%20natural%20to%0Awonder%20if%20those%20embeddings%20contain%20enough%20information%20to%20be%20able%20to%20connect%0Athose%20meanings%20through%20ontological%20relationships%20such%20as%20the%20one%20of%0Asubsumption.%20If%20so%2C%20large%20knowledge%20models%20could%20be%20built%20that%20are%20capable%20of%0Asemantically%20relating%20terms%20based%20on%20the%20information%20encapsulated%20in%20word%0Aembeddings%20produced%20by%20pre-trained%20models%2C%20with%20implications%20not%20only%20for%0Aontologies%20%28ontology%20matching%2C%20ontology%20evolution%2C%20etc.%29%20but%20also%20on%20the%0Aability%20to%20integrate%20ontological%20knowledge%20in%20neural%20models.%20In%20this%20paper%2C%20we%0Atest%20how%20embeddings%20produced%20by%20several%20pre-trained%20models%20can%20be%20used%20to%0Apredict%20relations%20existing%20between%20classes%20and%20properties%20of%20popular%0Aupper-level%20and%20general%20ontologies.%20We%20show%20that%20even%20a%20simple%20feed-forward%0Aarchitecture%20on%20top%20of%20those%20embeddings%20can%20achieve%20promising%20accuracies%2C%20with%0Avarying%20generalisation%20abilities%20depending%20on%20the%20input%20data.%20To%20achieve%20that%2C%0Awe%20produce%20a%20dataset%20that%20can%20be%20used%20to%20further%20enhance%20those%20models%2C%20opening%0Anew%20possibilities%20for%20applications%20integrating%20knowledge%20from%20web%20ontologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOntological%2520Relations%2520from%2520Word%2520Embeddings%26entry.906535625%3DMathieu%2520d%2527Aquin%2520and%2520Emmanuel%2520Nauer%26entry.1292438233%3D%2520%2520It%2520has%2520been%2520reliably%2520shown%2520that%2520the%2520similarity%2520of%2520word%2520embeddings%2520obtained%250Afrom%2520popular%2520neural%2520models%2520such%2520as%2520BERT%2520approximates%2520effectively%2520a%2520form%2520of%250Asemantic%2520similarity%2520of%2520the%2520meaning%2520of%2520those%2520words.%2520It%2520is%2520therefore%2520natural%2520to%250Awonder%2520if%2520those%2520embeddings%2520contain%2520enough%2520information%2520to%2520be%2520able%2520to%2520connect%250Athose%2520meanings%2520through%2520ontological%2520relationships%2520such%2520as%2520the%2520one%2520of%250Asubsumption.%2520If%2520so%252C%2520large%2520knowledge%2520models%2520could%2520be%2520built%2520that%2520are%2520capable%2520of%250Asemantically%2520relating%2520terms%2520based%2520on%2520the%2520information%2520encapsulated%2520in%2520word%250Aembeddings%2520produced%2520by%2520pre-trained%2520models%252C%2520with%2520implications%2520not%2520only%2520for%250Aontologies%2520%2528ontology%2520matching%252C%2520ontology%2520evolution%252C%2520etc.%2529%2520but%2520also%2520on%2520the%250Aability%2520to%2520integrate%2520ontological%2520knowledge%2520in%2520neural%2520models.%2520In%2520this%2520paper%252C%2520we%250Atest%2520how%2520embeddings%2520produced%2520by%2520several%2520pre-trained%2520models%2520can%2520be%2520used%2520to%250Apredict%2520relations%2520existing%2520between%2520classes%2520and%2520properties%2520of%2520popular%250Aupper-level%2520and%2520general%2520ontologies.%2520We%2520show%2520that%2520even%2520a%2520simple%2520feed-forward%250Aarchitecture%2520on%2520top%2520of%2520those%2520embeddings%2520can%2520achieve%2520promising%2520accuracies%252C%2520with%250Avarying%2520generalisation%2520abilities%2520depending%2520on%2520the%2520input%2520data.%2520To%2520achieve%2520that%252C%250Awe%2520produce%2520a%2520dataset%2520that%2520can%2520be%2520used%2520to%2520further%2520enhance%2520those%2520models%252C%2520opening%250Anew%2520possibilities%2520for%2520applications%2520integrating%2520knowledge%2520from%2520web%2520ontologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ontological%20Relations%20from%20Word%20Embeddings&entry.906535625=Mathieu%20d%27Aquin%20and%20Emmanuel%20Nauer&entry.1292438233=%20%20It%20has%20been%20reliably%20shown%20that%20the%20similarity%20of%20word%20embeddings%20obtained%0Afrom%20popular%20neural%20models%20such%20as%20BERT%20approximates%20effectively%20a%20form%20of%0Asemantic%20similarity%20of%20the%20meaning%20of%20those%20words.%20It%20is%20therefore%20natural%20to%0Awonder%20if%20those%20embeddings%20contain%20enough%20information%20to%20be%20able%20to%20connect%0Athose%20meanings%20through%20ontological%20relationships%20such%20as%20the%20one%20of%0Asubsumption.%20If%20so%2C%20large%20knowledge%20models%20could%20be%20built%20that%20are%20capable%20of%0Asemantically%20relating%20terms%20based%20on%20the%20information%20encapsulated%20in%20word%0Aembeddings%20produced%20by%20pre-trained%20models%2C%20with%20implications%20not%20only%20for%0Aontologies%20%28ontology%20matching%2C%20ontology%20evolution%2C%20etc.%29%20but%20also%20on%20the%0Aability%20to%20integrate%20ontological%20knowledge%20in%20neural%20models.%20In%20this%20paper%2C%20we%0Atest%20how%20embeddings%20produced%20by%20several%20pre-trained%20models%20can%20be%20used%20to%0Apredict%20relations%20existing%20between%20classes%20and%20properties%20of%20popular%0Aupper-level%20and%20general%20ontologies.%20We%20show%20that%20even%20a%20simple%20feed-forward%0Aarchitecture%20on%20top%20of%20those%20embeddings%20can%20achieve%20promising%20accuracies%2C%20with%0Avarying%20generalisation%20abilities%20depending%20on%20the%20input%20data.%20To%20achieve%20that%2C%0Awe%20produce%20a%20dataset%20that%20can%20be%20used%20to%20further%20enhance%20those%20models%2C%20opening%0Anew%20possibilities%20for%20applications%20integrating%20knowledge%20from%20web%20ontologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00444v1&entry.124074799=Read"},
{"title": "Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy\n  Structure Prior", "author": "Kechun Xu and Zhongxiang Zhou and Jun Wu and Haojian Lu and Rong Xiong and Yue Wang", "abstract": "  We focus on the task of unknown object rearrangement, where a robot is\nsupposed to re-configure the objects into a desired goal configuration\nspecified by an RGB-D image. Recent works explore unknown object rearrangement\nsystems by incorporating learning-based perception modules. However, they are\nsensitive to perception error, and pay less attention to task-level\nperformance. In this paper, we aim to develop an effective system for unknown\nobject rearrangement amidst perception noise. We theoretically reveal the noisy\nperception impacts grasp and place in a decoupled way, and show such a\ndecoupled structure is valuable to improve task optimality. We propose GSP, a\ndual-loop system with the decoupled structure as prior. For the inner loop, we\nlearn a see policy for self-confident in-hand object matching. For the outer\nloop, we learn a grasp policy aware of object matching and grasp capability\nguided by task-level rewards. We leverage the foundation model CLIP for object\nmatching, policy learning and self-termination. A series of experiments\nindicate that GSP can conduct unknown object rearrangement with higher\ncompletion rates and fewer steps.\n", "link": "http://arxiv.org/abs/2402.15402v2", "date": "2024-08-01", "relevancy": 1.8123, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6326}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grasp%2C%20See%20and%20Place%3A%20Efficient%20Unknown%20Object%20Rearrangement%20with%20Policy%0A%20%20Structure%20Prior&body=Title%3A%20Grasp%2C%20See%20and%20Place%3A%20Efficient%20Unknown%20Object%20Rearrangement%20with%20Policy%0A%20%20Structure%20Prior%0AAuthor%3A%20Kechun%20Xu%20and%20Zhongxiang%20Zhou%20and%20Jun%20Wu%20and%20Haojian%20Lu%20and%20Rong%20Xiong%20and%20Yue%20Wang%0AAbstract%3A%20%20%20We%20focus%20on%20the%20task%20of%20unknown%20object%20rearrangement%2C%20where%20a%20robot%20is%0Asupposed%20to%20re-configure%20the%20objects%20into%20a%20desired%20goal%20configuration%0Aspecified%20by%20an%20RGB-D%20image.%20Recent%20works%20explore%20unknown%20object%20rearrangement%0Asystems%20by%20incorporating%20learning-based%20perception%20modules.%20However%2C%20they%20are%0Asensitive%20to%20perception%20error%2C%20and%20pay%20less%20attention%20to%20task-level%0Aperformance.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20an%20effective%20system%20for%20unknown%0Aobject%20rearrangement%20amidst%20perception%20noise.%20We%20theoretically%20reveal%20the%20noisy%0Aperception%20impacts%20grasp%20and%20place%20in%20a%20decoupled%20way%2C%20and%20show%20such%20a%0Adecoupled%20structure%20is%20valuable%20to%20improve%20task%20optimality.%20We%20propose%20GSP%2C%20a%0Adual-loop%20system%20with%20the%20decoupled%20structure%20as%20prior.%20For%20the%20inner%20loop%2C%20we%0Alearn%20a%20see%20policy%20for%20self-confident%20in-hand%20object%20matching.%20For%20the%20outer%0Aloop%2C%20we%20learn%20a%20grasp%20policy%20aware%20of%20object%20matching%20and%20grasp%20capability%0Aguided%20by%20task-level%20rewards.%20We%20leverage%20the%20foundation%20model%20CLIP%20for%20object%0Amatching%2C%20policy%20learning%20and%20self-termination.%20A%20series%20of%20experiments%0Aindicate%20that%20GSP%20can%20conduct%20unknown%20object%20rearrangement%20with%20higher%0Acompletion%20rates%20and%20fewer%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15402v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrasp%252C%2520See%2520and%2520Place%253A%2520Efficient%2520Unknown%2520Object%2520Rearrangement%2520with%2520Policy%250A%2520%2520Structure%2520Prior%26entry.906535625%3DKechun%2520Xu%2520and%2520Zhongxiang%2520Zhou%2520and%2520Jun%2520Wu%2520and%2520Haojian%2520Lu%2520and%2520Rong%2520Xiong%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520We%2520focus%2520on%2520the%2520task%2520of%2520unknown%2520object%2520rearrangement%252C%2520where%2520a%2520robot%2520is%250Asupposed%2520to%2520re-configure%2520the%2520objects%2520into%2520a%2520desired%2520goal%2520configuration%250Aspecified%2520by%2520an%2520RGB-D%2520image.%2520Recent%2520works%2520explore%2520unknown%2520object%2520rearrangement%250Asystems%2520by%2520incorporating%2520learning-based%2520perception%2520modules.%2520However%252C%2520they%2520are%250Asensitive%2520to%2520perception%2520error%252C%2520and%2520pay%2520less%2520attention%2520to%2520task-level%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520develop%2520an%2520effective%2520system%2520for%2520unknown%250Aobject%2520rearrangement%2520amidst%2520perception%2520noise.%2520We%2520theoretically%2520reveal%2520the%2520noisy%250Aperception%2520impacts%2520grasp%2520and%2520place%2520in%2520a%2520decoupled%2520way%252C%2520and%2520show%2520such%2520a%250Adecoupled%2520structure%2520is%2520valuable%2520to%2520improve%2520task%2520optimality.%2520We%2520propose%2520GSP%252C%2520a%250Adual-loop%2520system%2520with%2520the%2520decoupled%2520structure%2520as%2520prior.%2520For%2520the%2520inner%2520loop%252C%2520we%250Alearn%2520a%2520see%2520policy%2520for%2520self-confident%2520in-hand%2520object%2520matching.%2520For%2520the%2520outer%250Aloop%252C%2520we%2520learn%2520a%2520grasp%2520policy%2520aware%2520of%2520object%2520matching%2520and%2520grasp%2520capability%250Aguided%2520by%2520task-level%2520rewards.%2520We%2520leverage%2520the%2520foundation%2520model%2520CLIP%2520for%2520object%250Amatching%252C%2520policy%2520learning%2520and%2520self-termination.%2520A%2520series%2520of%2520experiments%250Aindicate%2520that%2520GSP%2520can%2520conduct%2520unknown%2520object%2520rearrangement%2520with%2520higher%250Acompletion%2520rates%2520and%2520fewer%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15402v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grasp%2C%20See%20and%20Place%3A%20Efficient%20Unknown%20Object%20Rearrangement%20with%20Policy%0A%20%20Structure%20Prior&entry.906535625=Kechun%20Xu%20and%20Zhongxiang%20Zhou%20and%20Jun%20Wu%20and%20Haojian%20Lu%20and%20Rong%20Xiong%20and%20Yue%20Wang&entry.1292438233=%20%20We%20focus%20on%20the%20task%20of%20unknown%20object%20rearrangement%2C%20where%20a%20robot%20is%0Asupposed%20to%20re-configure%20the%20objects%20into%20a%20desired%20goal%20configuration%0Aspecified%20by%20an%20RGB-D%20image.%20Recent%20works%20explore%20unknown%20object%20rearrangement%0Asystems%20by%20incorporating%20learning-based%20perception%20modules.%20However%2C%20they%20are%0Asensitive%20to%20perception%20error%2C%20and%20pay%20less%20attention%20to%20task-level%0Aperformance.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20an%20effective%20system%20for%20unknown%0Aobject%20rearrangement%20amidst%20perception%20noise.%20We%20theoretically%20reveal%20the%20noisy%0Aperception%20impacts%20grasp%20and%20place%20in%20a%20decoupled%20way%2C%20and%20show%20such%20a%0Adecoupled%20structure%20is%20valuable%20to%20improve%20task%20optimality.%20We%20propose%20GSP%2C%20a%0Adual-loop%20system%20with%20the%20decoupled%20structure%20as%20prior.%20For%20the%20inner%20loop%2C%20we%0Alearn%20a%20see%20policy%20for%20self-confident%20in-hand%20object%20matching.%20For%20the%20outer%0Aloop%2C%20we%20learn%20a%20grasp%20policy%20aware%20of%20object%20matching%20and%20grasp%20capability%0Aguided%20by%20task-level%20rewards.%20We%20leverage%20the%20foundation%20model%20CLIP%20for%20object%0Amatching%2C%20policy%20learning%20and%20self-termination.%20A%20series%20of%20experiments%0Aindicate%20that%20GSP%20can%20conduct%20unknown%20object%20rearrangement%20with%20higher%0Acompletion%20rates%20and%20fewer%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15402v2&entry.124074799=Read"},
{"title": "Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities", "author": "Joshua Rosaler and Dhruv Desai and Bhaskarjit Sarmah and Dimitrios Vamvourellis and Deran Onay and Dhagash Mehta and Stefano Pasquali", "abstract": "  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n", "link": "http://arxiv.org/abs/2310.12428v2", "date": "2024-08-01", "relevancy": 1.8119, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.494}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4453}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Local%20Explainability%20and%20Trust%20Scores%20with%20Random%20Forest%0A%20%20Proximities&body=Title%3A%20Enhanced%20Local%20Explainability%20and%20Trust%20Scores%20with%20Random%20Forest%0A%20%20Proximities%0AAuthor%3A%20Joshua%20Rosaler%20and%20Dhruv%20Desai%20and%20Bhaskarjit%20Sarmah%20and%20Dimitrios%20Vamvourellis%20and%20Deran%20Onay%20and%20Dhagash%20Mehta%20and%20Stefano%20Pasquali%0AAbstract%3A%20%20%20We%20initiate%20a%20novel%20approach%20to%20explain%20the%20predictions%20and%20out%20of%20sample%0Aperformance%20of%20random%20forest%20%28RF%29%20regression%20and%20classification%20models%20by%0Aexploiting%20the%20fact%20that%20any%20RF%20can%20be%20mathematically%20formulated%20as%20an%20adaptive%0Aweighted%20K%20nearest-neighbors%20model.%20Specifically%2C%20we%20employ%20a%20recent%20result%0Athat%2C%20for%20both%20regression%20and%20classification%20tasks%2C%20any%20RF%20prediction%20can%20be%0Arewritten%20exactly%20as%20a%20weighted%20sum%20of%20the%20training%20targets%2C%20where%20the%20weights%0Aare%20RF%20proximities%20between%20the%20corresponding%20pairs%20of%20data%20points.%20We%20show%20that%0Athis%20linearity%20facilitates%20a%20local%20notion%20of%20explainability%20of%20RF%20predictions%0Athat%20generates%20attributions%20for%20any%20model%20prediction%20across%20observations%20in%20the%0Atraining%20set%2C%20and%20thereby%20complements%20established%20feature-based%20methods%20like%0ASHAP%2C%20which%20generate%20attributions%20for%20a%20model%20prediction%20across%20input%20features.%0AWe%20show%20how%20this%20proximity-based%20approach%20to%20explainability%20can%20be%20used%20in%0Aconjunction%20with%20SHAP%20to%20explain%20not%20just%20the%20model%20predictions%2C%20but%20also%0Aout-of-sample%20performance%2C%20in%20the%20sense%20that%20proximities%20furnish%20a%20novel%20means%0Aof%20assessing%20when%20a%20given%20model%20prediction%20is%20more%20or%20less%20likely%20to%20be%0Acorrect.%20We%20demonstrate%20this%20approach%20in%20the%20modeling%20of%20US%20corporate%20bond%0Aprices%20and%20returns%20in%20both%20regression%20and%20classification%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12428v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Local%2520Explainability%2520and%2520Trust%2520Scores%2520with%2520Random%2520Forest%250A%2520%2520Proximities%26entry.906535625%3DJoshua%2520Rosaler%2520and%2520Dhruv%2520Desai%2520and%2520Bhaskarjit%2520Sarmah%2520and%2520Dimitrios%2520Vamvourellis%2520and%2520Deran%2520Onay%2520and%2520Dhagash%2520Mehta%2520and%2520Stefano%2520Pasquali%26entry.1292438233%3D%2520%2520We%2520initiate%2520a%2520novel%2520approach%2520to%2520explain%2520the%2520predictions%2520and%2520out%2520of%2520sample%250Aperformance%2520of%2520random%2520forest%2520%2528RF%2529%2520regression%2520and%2520classification%2520models%2520by%250Aexploiting%2520the%2520fact%2520that%2520any%2520RF%2520can%2520be%2520mathematically%2520formulated%2520as%2520an%2520adaptive%250Aweighted%2520K%2520nearest-neighbors%2520model.%2520Specifically%252C%2520we%2520employ%2520a%2520recent%2520result%250Athat%252C%2520for%2520both%2520regression%2520and%2520classification%2520tasks%252C%2520any%2520RF%2520prediction%2520can%2520be%250Arewritten%2520exactly%2520as%2520a%2520weighted%2520sum%2520of%2520the%2520training%2520targets%252C%2520where%2520the%2520weights%250Aare%2520RF%2520proximities%2520between%2520the%2520corresponding%2520pairs%2520of%2520data%2520points.%2520We%2520show%2520that%250Athis%2520linearity%2520facilitates%2520a%2520local%2520notion%2520of%2520explainability%2520of%2520RF%2520predictions%250Athat%2520generates%2520attributions%2520for%2520any%2520model%2520prediction%2520across%2520observations%2520in%2520the%250Atraining%2520set%252C%2520and%2520thereby%2520complements%2520established%2520feature-based%2520methods%2520like%250ASHAP%252C%2520which%2520generate%2520attributions%2520for%2520a%2520model%2520prediction%2520across%2520input%2520features.%250AWe%2520show%2520how%2520this%2520proximity-based%2520approach%2520to%2520explainability%2520can%2520be%2520used%2520in%250Aconjunction%2520with%2520SHAP%2520to%2520explain%2520not%2520just%2520the%2520model%2520predictions%252C%2520but%2520also%250Aout-of-sample%2520performance%252C%2520in%2520the%2520sense%2520that%2520proximities%2520furnish%2520a%2520novel%2520means%250Aof%2520assessing%2520when%2520a%2520given%2520model%2520prediction%2520is%2520more%2520or%2520less%2520likely%2520to%2520be%250Acorrect.%2520We%2520demonstrate%2520this%2520approach%2520in%2520the%2520modeling%2520of%2520US%2520corporate%2520bond%250Aprices%2520and%2520returns%2520in%2520both%2520regression%2520and%2520classification%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12428v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Local%20Explainability%20and%20Trust%20Scores%20with%20Random%20Forest%0A%20%20Proximities&entry.906535625=Joshua%20Rosaler%20and%20Dhruv%20Desai%20and%20Bhaskarjit%20Sarmah%20and%20Dimitrios%20Vamvourellis%20and%20Deran%20Onay%20and%20Dhagash%20Mehta%20and%20Stefano%20Pasquali&entry.1292438233=%20%20We%20initiate%20a%20novel%20approach%20to%20explain%20the%20predictions%20and%20out%20of%20sample%0Aperformance%20of%20random%20forest%20%28RF%29%20regression%20and%20classification%20models%20by%0Aexploiting%20the%20fact%20that%20any%20RF%20can%20be%20mathematically%20formulated%20as%20an%20adaptive%0Aweighted%20K%20nearest-neighbors%20model.%20Specifically%2C%20we%20employ%20a%20recent%20result%0Athat%2C%20for%20both%20regression%20and%20classification%20tasks%2C%20any%20RF%20prediction%20can%20be%0Arewritten%20exactly%20as%20a%20weighted%20sum%20of%20the%20training%20targets%2C%20where%20the%20weights%0Aare%20RF%20proximities%20between%20the%20corresponding%20pairs%20of%20data%20points.%20We%20show%20that%0Athis%20linearity%20facilitates%20a%20local%20notion%20of%20explainability%20of%20RF%20predictions%0Athat%20generates%20attributions%20for%20any%20model%20prediction%20across%20observations%20in%20the%0Atraining%20set%2C%20and%20thereby%20complements%20established%20feature-based%20methods%20like%0ASHAP%2C%20which%20generate%20attributions%20for%20a%20model%20prediction%20across%20input%20features.%0AWe%20show%20how%20this%20proximity-based%20approach%20to%20explainability%20can%20be%20used%20in%0Aconjunction%20with%20SHAP%20to%20explain%20not%20just%20the%20model%20predictions%2C%20but%20also%0Aout-of-sample%20performance%2C%20in%20the%20sense%20that%20proximities%20furnish%20a%20novel%20means%0Aof%20assessing%20when%20a%20given%20model%20prediction%20is%20more%20or%20less%20likely%20to%20be%0Acorrect.%20We%20demonstrate%20this%20approach%20in%20the%20modeling%20of%20US%20corporate%20bond%0Aprices%20and%20returns%20in%20both%20regression%20and%20classification%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12428v2&entry.124074799=Read"},
{"title": "Learning to Embed Distributions via Maximum Kernel Entropy", "author": "Oleksii Kachaiev and Stefano Recanatesi", "abstract": "  Empirical data can often be considered as samples from a set of probability\ndistributions. Kernel methods have emerged as a natural approach for learning\nto classify these distributions. Although numerous kernels between\ndistributions have been proposed, applying kernel methods to distribution\nregression tasks remains challenging, primarily because selecting a suitable\nkernel is not straightforward. Surprisingly, the question of learning a\ndata-dependent distribution kernel has received little attention. In this\npaper, we propose a novel objective for the unsupervised learning of\ndata-dependent distribution kernel, based on the principle of entropy\nmaximization in the space of probability measure embeddings. We examine the\ntheoretical properties of the latent embedding space induced by our objective,\ndemonstrating that its geometric structure is well-suited for solving\ndownstream discriminative tasks. Finally, we demonstrate the performance of the\nlearned kernel across different modalities.\n", "link": "http://arxiv.org/abs/2408.00549v1", "date": "2024-08-01", "relevancy": 1.8108, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4627}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4497}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Embed%20Distributions%20via%20Maximum%20Kernel%20Entropy&body=Title%3A%20Learning%20to%20Embed%20Distributions%20via%20Maximum%20Kernel%20Entropy%0AAuthor%3A%20Oleksii%20Kachaiev%20and%20Stefano%20Recanatesi%0AAbstract%3A%20%20%20Empirical%20data%20can%20often%20be%20considered%20as%20samples%20from%20a%20set%20of%20probability%0Adistributions.%20Kernel%20methods%20have%20emerged%20as%20a%20natural%20approach%20for%20learning%0Ato%20classify%20these%20distributions.%20Although%20numerous%20kernels%20between%0Adistributions%20have%20been%20proposed%2C%20applying%20kernel%20methods%20to%20distribution%0Aregression%20tasks%20remains%20challenging%2C%20primarily%20because%20selecting%20a%20suitable%0Akernel%20is%20not%20straightforward.%20Surprisingly%2C%20the%20question%20of%20learning%20a%0Adata-dependent%20distribution%20kernel%20has%20received%20little%20attention.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20objective%20for%20the%20unsupervised%20learning%20of%0Adata-dependent%20distribution%20kernel%2C%20based%20on%20the%20principle%20of%20entropy%0Amaximization%20in%20the%20space%20of%20probability%20measure%20embeddings.%20We%20examine%20the%0Atheoretical%20properties%20of%20the%20latent%20embedding%20space%20induced%20by%20our%20objective%2C%0Ademonstrating%20that%20its%20geometric%20structure%20is%20well-suited%20for%20solving%0Adownstream%20discriminative%20tasks.%20Finally%2C%20we%20demonstrate%20the%20performance%20of%20the%0Alearned%20kernel%20across%20different%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Embed%2520Distributions%2520via%2520Maximum%2520Kernel%2520Entropy%26entry.906535625%3DOleksii%2520Kachaiev%2520and%2520Stefano%2520Recanatesi%26entry.1292438233%3D%2520%2520Empirical%2520data%2520can%2520often%2520be%2520considered%2520as%2520samples%2520from%2520a%2520set%2520of%2520probability%250Adistributions.%2520Kernel%2520methods%2520have%2520emerged%2520as%2520a%2520natural%2520approach%2520for%2520learning%250Ato%2520classify%2520these%2520distributions.%2520Although%2520numerous%2520kernels%2520between%250Adistributions%2520have%2520been%2520proposed%252C%2520applying%2520kernel%2520methods%2520to%2520distribution%250Aregression%2520tasks%2520remains%2520challenging%252C%2520primarily%2520because%2520selecting%2520a%2520suitable%250Akernel%2520is%2520not%2520straightforward.%2520Surprisingly%252C%2520the%2520question%2520of%2520learning%2520a%250Adata-dependent%2520distribution%2520kernel%2520has%2520received%2520little%2520attention.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520objective%2520for%2520the%2520unsupervised%2520learning%2520of%250Adata-dependent%2520distribution%2520kernel%252C%2520based%2520on%2520the%2520principle%2520of%2520entropy%250Amaximization%2520in%2520the%2520space%2520of%2520probability%2520measure%2520embeddings.%2520We%2520examine%2520the%250Atheoretical%2520properties%2520of%2520the%2520latent%2520embedding%2520space%2520induced%2520by%2520our%2520objective%252C%250Ademonstrating%2520that%2520its%2520geometric%2520structure%2520is%2520well-suited%2520for%2520solving%250Adownstream%2520discriminative%2520tasks.%2520Finally%252C%2520we%2520demonstrate%2520the%2520performance%2520of%2520the%250Alearned%2520kernel%2520across%2520different%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Embed%20Distributions%20via%20Maximum%20Kernel%20Entropy&entry.906535625=Oleksii%20Kachaiev%20and%20Stefano%20Recanatesi&entry.1292438233=%20%20Empirical%20data%20can%20often%20be%20considered%20as%20samples%20from%20a%20set%20of%20probability%0Adistributions.%20Kernel%20methods%20have%20emerged%20as%20a%20natural%20approach%20for%20learning%0Ato%20classify%20these%20distributions.%20Although%20numerous%20kernels%20between%0Adistributions%20have%20been%20proposed%2C%20applying%20kernel%20methods%20to%20distribution%0Aregression%20tasks%20remains%20challenging%2C%20primarily%20because%20selecting%20a%20suitable%0Akernel%20is%20not%20straightforward.%20Surprisingly%2C%20the%20question%20of%20learning%20a%0Adata-dependent%20distribution%20kernel%20has%20received%20little%20attention.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20objective%20for%20the%20unsupervised%20learning%20of%0Adata-dependent%20distribution%20kernel%2C%20based%20on%20the%20principle%20of%20entropy%0Amaximization%20in%20the%20space%20of%20probability%20measure%20embeddings.%20We%20examine%20the%0Atheoretical%20properties%20of%20the%20latent%20embedding%20space%20induced%20by%20our%20objective%2C%0Ademonstrating%20that%20its%20geometric%20structure%20is%20well-suited%20for%20solving%0Adownstream%20discriminative%20tasks.%20Finally%2C%20we%20demonstrate%20the%20performance%20of%20the%0Alearned%20kernel%20across%20different%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00549v1&entry.124074799=Read"},
{"title": "Predicting the Geolocation of Tweets Using transformer models on\n  Customized Data", "author": "Kateryna Lutsai and Christoph H. Lampert", "abstract": "  This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context. Our source code and data are\navailable at https://github.com/K4TEL/geo-twitter.git\n", "link": "http://arxiv.org/abs/2303.07865v4", "date": "2024-08-01", "relevancy": 1.8035, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4951}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4451}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20the%20Geolocation%20of%20Tweets%20Using%20transformer%20models%20on%0A%20%20Customized%20Data&body=Title%3A%20Predicting%20the%20Geolocation%20of%20Tweets%20Using%20transformer%20models%20on%0A%20%20Customized%20Data%0AAuthor%3A%20Kateryna%20Lutsai%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20This%20research%20is%20aimed%20to%20solve%20the%20tweet/user%20geolocation%20prediction%20task%0Aand%20provide%20a%20flexible%20methodology%20for%20the%20geotagging%20of%20textual%20big%20data.%20The%0Asuggested%20approach%20implements%20neural%20networks%20for%20natural%20language%20processing%0A%28NLP%29%20to%20estimate%20the%20location%20as%20coordinate%20pairs%20%28longitude%2C%20latitude%29%20and%0Atwo-dimensional%20Gaussian%20Mixture%20Models%20%28GMMs%29.%20The%20scope%20of%20proposed%20models%0Ahas%20been%20finetuned%20on%20a%20Twitter%20dataset%20using%20pretrained%20Bidirectional%20Encoder%0ARepresentations%20from%20Transformers%20%28BERT%29%20as%20base%20models.%20Performance%20metrics%0Ashow%20a%20median%20error%20of%20fewer%20than%2030%20km%20on%20a%20worldwide-level%2C%20and%20fewer%20than%2015%0Akm%20on%20the%20US-level%20datasets%20for%20the%20models%20trained%20and%20evaluated%20on%20text%0Afeatures%20of%20tweets%27%20content%20and%20metadata%20context.%20Our%20source%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/K4TEL/geo-twitter.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.07865v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520the%2520Geolocation%2520of%2520Tweets%2520Using%2520transformer%2520models%2520on%250A%2520%2520Customized%2520Data%26entry.906535625%3DKateryna%2520Lutsai%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520This%2520research%2520is%2520aimed%2520to%2520solve%2520the%2520tweet/user%2520geolocation%2520prediction%2520task%250Aand%2520provide%2520a%2520flexible%2520methodology%2520for%2520the%2520geotagging%2520of%2520textual%2520big%2520data.%2520The%250Asuggested%2520approach%2520implements%2520neural%2520networks%2520for%2520natural%2520language%2520processing%250A%2528NLP%2529%2520to%2520estimate%2520the%2520location%2520as%2520coordinate%2520pairs%2520%2528longitude%252C%2520latitude%2529%2520and%250Atwo-dimensional%2520Gaussian%2520Mixture%2520Models%2520%2528GMMs%2529.%2520The%2520scope%2520of%2520proposed%2520models%250Ahas%2520been%2520finetuned%2520on%2520a%2520Twitter%2520dataset%2520using%2520pretrained%2520Bidirectional%2520Encoder%250ARepresentations%2520from%2520Transformers%2520%2528BERT%2529%2520as%2520base%2520models.%2520Performance%2520metrics%250Ashow%2520a%2520median%2520error%2520of%2520fewer%2520than%252030%2520km%2520on%2520a%2520worldwide-level%252C%2520and%2520fewer%2520than%252015%250Akm%2520on%2520the%2520US-level%2520datasets%2520for%2520the%2520models%2520trained%2520and%2520evaluated%2520on%2520text%250Afeatures%2520of%2520tweets%2527%2520content%2520and%2520metadata%2520context.%2520Our%2520source%2520code%2520and%2520data%2520are%250Aavailable%2520at%2520https%253A//github.com/K4TEL/geo-twitter.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.07865v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20the%20Geolocation%20of%20Tweets%20Using%20transformer%20models%20on%0A%20%20Customized%20Data&entry.906535625=Kateryna%20Lutsai%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20This%20research%20is%20aimed%20to%20solve%20the%20tweet/user%20geolocation%20prediction%20task%0Aand%20provide%20a%20flexible%20methodology%20for%20the%20geotagging%20of%20textual%20big%20data.%20The%0Asuggested%20approach%20implements%20neural%20networks%20for%20natural%20language%20processing%0A%28NLP%29%20to%20estimate%20the%20location%20as%20coordinate%20pairs%20%28longitude%2C%20latitude%29%20and%0Atwo-dimensional%20Gaussian%20Mixture%20Models%20%28GMMs%29.%20The%20scope%20of%20proposed%20models%0Ahas%20been%20finetuned%20on%20a%20Twitter%20dataset%20using%20pretrained%20Bidirectional%20Encoder%0ARepresentations%20from%20Transformers%20%28BERT%29%20as%20base%20models.%20Performance%20metrics%0Ashow%20a%20median%20error%20of%20fewer%20than%2030%20km%20on%20a%20worldwide-level%2C%20and%20fewer%20than%2015%0Akm%20on%20the%20US-level%20datasets%20for%20the%20models%20trained%20and%20evaluated%20on%20text%0Afeatures%20of%20tweets%27%20content%20and%20metadata%20context.%20Our%20source%20code%20and%20data%20are%0Aavailable%20at%20https%3A//github.com/K4TEL/geo-twitter.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.07865v4&entry.124074799=Read"},
{"title": "MoE-Infinity: Offloading-Efficient MoE Model Serving", "author": "Leyang Xue and Yao Fu and Zhan Lu and Luo Mai and Mahesh Marina", "abstract": "  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n", "link": "http://arxiv.org/abs/2401.14361v2", "date": "2024-08-01", "relevancy": 1.7974, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4803}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoE-Infinity%3A%20Offloading-Efficient%20MoE%20Model%20Serving&body=Title%3A%20MoE-Infinity%3A%20Offloading-Efficient%20MoE%20Model%20Serving%0AAuthor%3A%20Leyang%20Xue%20and%20Yao%20Fu%20and%20Zhan%20Lu%20and%20Luo%20Mai%20and%20Mahesh%20Marina%0AAbstract%3A%20%20%20This%20paper%20presents%20MoE-Infinity%2C%20an%20offloading-efficient%20serving%20system%20for%0Asparse%20mixture-of-experts%20%28MoE%29%20models.%20To%20optimize%20offloading%2C%20MoE-Infinity%0Aachieves%20novel%20request-level%20tracing%20for%20expert%20activation%2C%20capturing%20MoE%27s%0Asparse%20execution%20patterns%20such%20as%20selective%20activation%2C%20group%20activation%2C%20and%0Askewed%20reuse.%20Leveraging%20the%20request-level%20trace%2C%20MoE-Infinity%20performs%0Aeffective%20expert%20prefetching%20and%20expert%20caching%2C%20achieving%20high%20efficiency%20in%0Atransferring%20model%20parameters%20from%20host%20memory%20to%20GPU%20memory.%20Experimental%0Aresults%20demonstrate%20that%20MoE-Infinity%20achieves%20low%20latency%20comparable%20to%0Aexpensive%20full-GPU%20deployments%2C%20which%20require%20up%20to%204X%20more%20GPU%20resources%20than%0AMoE-Infinity.%20Compared%20to%20offloading-supporting%20LLM%20serving%20systems%20such%20as%0ADeepSpeed-Inference%2C%20Llama.cpp%2C%20Mixtral%20Offloading%2C%20and%20BrainStorm%2C%0AMoE-Infinity%20exhibits%20superior%20latency%20performance%2C%20providing%202-20X%0Aimprovements%20when%20serving%20various%20MoE%20models%20for%20a%20large%20collection%20of%20LLM%0Atasks.%20MoE-Infinity%27s%20source%20code%20is%20publicly%20available%20a%0Ahttps%3A//github.com/TorchMoE/MoE-Infinity%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoE-Infinity%253A%2520Offloading-Efficient%2520MoE%2520Model%2520Serving%26entry.906535625%3DLeyang%2520Xue%2520and%2520Yao%2520Fu%2520and%2520Zhan%2520Lu%2520and%2520Luo%2520Mai%2520and%2520Mahesh%2520Marina%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520MoE-Infinity%252C%2520an%2520offloading-efficient%2520serving%2520system%2520for%250Asparse%2520mixture-of-experts%2520%2528MoE%2529%2520models.%2520To%2520optimize%2520offloading%252C%2520MoE-Infinity%250Aachieves%2520novel%2520request-level%2520tracing%2520for%2520expert%2520activation%252C%2520capturing%2520MoE%2527s%250Asparse%2520execution%2520patterns%2520such%2520as%2520selective%2520activation%252C%2520group%2520activation%252C%2520and%250Askewed%2520reuse.%2520Leveraging%2520the%2520request-level%2520trace%252C%2520MoE-Infinity%2520performs%250Aeffective%2520expert%2520prefetching%2520and%2520expert%2520caching%252C%2520achieving%2520high%2520efficiency%2520in%250Atransferring%2520model%2520parameters%2520from%2520host%2520memory%2520to%2520GPU%2520memory.%2520Experimental%250Aresults%2520demonstrate%2520that%2520MoE-Infinity%2520achieves%2520low%2520latency%2520comparable%2520to%250Aexpensive%2520full-GPU%2520deployments%252C%2520which%2520require%2520up%2520to%25204X%2520more%2520GPU%2520resources%2520than%250AMoE-Infinity.%2520Compared%2520to%2520offloading-supporting%2520LLM%2520serving%2520systems%2520such%2520as%250ADeepSpeed-Inference%252C%2520Llama.cpp%252C%2520Mixtral%2520Offloading%252C%2520and%2520BrainStorm%252C%250AMoE-Infinity%2520exhibits%2520superior%2520latency%2520performance%252C%2520providing%25202-20X%250Aimprovements%2520when%2520serving%2520various%2520MoE%2520models%2520for%2520a%2520large%2520collection%2520of%2520LLM%250Atasks.%2520MoE-Infinity%2527s%2520source%2520code%2520is%2520publicly%2520available%2520a%250Ahttps%253A//github.com/TorchMoE/MoE-Infinity%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoE-Infinity%3A%20Offloading-Efficient%20MoE%20Model%20Serving&entry.906535625=Leyang%20Xue%20and%20Yao%20Fu%20and%20Zhan%20Lu%20and%20Luo%20Mai%20and%20Mahesh%20Marina&entry.1292438233=%20%20This%20paper%20presents%20MoE-Infinity%2C%20an%20offloading-efficient%20serving%20system%20for%0Asparse%20mixture-of-experts%20%28MoE%29%20models.%20To%20optimize%20offloading%2C%20MoE-Infinity%0Aachieves%20novel%20request-level%20tracing%20for%20expert%20activation%2C%20capturing%20MoE%27s%0Asparse%20execution%20patterns%20such%20as%20selective%20activation%2C%20group%20activation%2C%20and%0Askewed%20reuse.%20Leveraging%20the%20request-level%20trace%2C%20MoE-Infinity%20performs%0Aeffective%20expert%20prefetching%20and%20expert%20caching%2C%20achieving%20high%20efficiency%20in%0Atransferring%20model%20parameters%20from%20host%20memory%20to%20GPU%20memory.%20Experimental%0Aresults%20demonstrate%20that%20MoE-Infinity%20achieves%20low%20latency%20comparable%20to%0Aexpensive%20full-GPU%20deployments%2C%20which%20require%20up%20to%204X%20more%20GPU%20resources%20than%0AMoE-Infinity.%20Compared%20to%20offloading-supporting%20LLM%20serving%20systems%20such%20as%0ADeepSpeed-Inference%2C%20Llama.cpp%2C%20Mixtral%20Offloading%2C%20and%20BrainStorm%2C%0AMoE-Infinity%20exhibits%20superior%20latency%20performance%2C%20providing%202-20X%0Aimprovements%20when%20serving%20various%20MoE%20models%20for%20a%20large%20collection%20of%20LLM%0Atasks.%20MoE-Infinity%27s%20source%20code%20is%20publicly%20available%20a%0Ahttps%3A//github.com/TorchMoE/MoE-Infinity%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14361v2&entry.124074799=Read"},
{"title": "Contextual Bandits with Packing and Covering Constraints: A Modular\n  Lagrangian Approach via Regression", "author": "Aleksandrs Slivkins and Xingyu Zhou and Karthik Abinav Sankararaman and Dylan J. Foster", "abstract": "  We consider contextual bandits with linear constraints (CBwLC), a variant of\ncontextual bandits in which the algorithm consumes multiple resources subject\nto linear constraints on total consumption. This problem generalizes contextual\nbandits with knapsacks (CBwK), allowing for packing and covering constraints,\nas well as positive and negative resource consumption. We provide the first\nalgorithm for CBwLC (or CBwK) that is based on regression oracles. The\nalgorithm is simple, computationally efficient, and statistically optimal under\nmild assumptions. Further, we provide the first vanishing-regret guarantees for\nCBwLC (or CBwK) that extend beyond the stochastic environment. We side-step\nstrong impossibility results from prior work by identifying a weaker (and,\narguably, fairer) benchmark to compare against. Our algorithm builds on\nLagrangeBwK (Immorlica et al., FOCS 2019), a Lagrangian-based technique for\nCBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based\ntechnique for contextual bandits. Our analysis leverages the inherent\nmodularity of both techniques.\n", "link": "http://arxiv.org/abs/2211.07484v6", "date": "2024-08-01", "relevancy": 1.7883, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4666}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4464}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Bandits%20with%20Packing%20and%20Covering%20Constraints%3A%20A%20Modular%0A%20%20Lagrangian%20Approach%20via%20Regression&body=Title%3A%20Contextual%20Bandits%20with%20Packing%20and%20Covering%20Constraints%3A%20A%20Modular%0A%20%20Lagrangian%20Approach%20via%20Regression%0AAuthor%3A%20Aleksandrs%20Slivkins%20and%20Xingyu%20Zhou%20and%20Karthik%20Abinav%20Sankararaman%20and%20Dylan%20J.%20Foster%0AAbstract%3A%20%20%20We%20consider%20contextual%20bandits%20with%20linear%20constraints%20%28CBwLC%29%2C%20a%20variant%20of%0Acontextual%20bandits%20in%20which%20the%20algorithm%20consumes%20multiple%20resources%20subject%0Ato%20linear%20constraints%20on%20total%20consumption.%20This%20problem%20generalizes%20contextual%0Abandits%20with%20knapsacks%20%28CBwK%29%2C%20allowing%20for%20packing%20and%20covering%20constraints%2C%0Aas%20well%20as%20positive%20and%20negative%20resource%20consumption.%20We%20provide%20the%20first%0Aalgorithm%20for%20CBwLC%20%28or%20CBwK%29%20that%20is%20based%20on%20regression%20oracles.%20The%0Aalgorithm%20is%20simple%2C%20computationally%20efficient%2C%20and%20statistically%20optimal%20under%0Amild%20assumptions.%20Further%2C%20we%20provide%20the%20first%20vanishing-regret%20guarantees%20for%0ACBwLC%20%28or%20CBwK%29%20that%20extend%20beyond%20the%20stochastic%20environment.%20We%20side-step%0Astrong%20impossibility%20results%20from%20prior%20work%20by%20identifying%20a%20weaker%20%28and%2C%0Aarguably%2C%20fairer%29%20benchmark%20to%20compare%20against.%20Our%20algorithm%20builds%20on%0ALagrangeBwK%20%28Immorlica%20et%20al.%2C%20FOCS%202019%29%2C%20a%20Lagrangian-based%20technique%20for%0ACBwK%2C%20and%20SquareCB%20%28Foster%20and%20Rakhlin%2C%20ICML%202020%29%2C%20a%20regression-based%0Atechnique%20for%20contextual%20bandits.%20Our%20analysis%20leverages%20the%20inherent%0Amodularity%20of%20both%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.07484v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Bandits%2520with%2520Packing%2520and%2520Covering%2520Constraints%253A%2520A%2520Modular%250A%2520%2520Lagrangian%2520Approach%2520via%2520Regression%26entry.906535625%3DAleksandrs%2520Slivkins%2520and%2520Xingyu%2520Zhou%2520and%2520Karthik%2520Abinav%2520Sankararaman%2520and%2520Dylan%2520J.%2520Foster%26entry.1292438233%3D%2520%2520We%2520consider%2520contextual%2520bandits%2520with%2520linear%2520constraints%2520%2528CBwLC%2529%252C%2520a%2520variant%2520of%250Acontextual%2520bandits%2520in%2520which%2520the%2520algorithm%2520consumes%2520multiple%2520resources%2520subject%250Ato%2520linear%2520constraints%2520on%2520total%2520consumption.%2520This%2520problem%2520generalizes%2520contextual%250Abandits%2520with%2520knapsacks%2520%2528CBwK%2529%252C%2520allowing%2520for%2520packing%2520and%2520covering%2520constraints%252C%250Aas%2520well%2520as%2520positive%2520and%2520negative%2520resource%2520consumption.%2520We%2520provide%2520the%2520first%250Aalgorithm%2520for%2520CBwLC%2520%2528or%2520CBwK%2529%2520that%2520is%2520based%2520on%2520regression%2520oracles.%2520The%250Aalgorithm%2520is%2520simple%252C%2520computationally%2520efficient%252C%2520and%2520statistically%2520optimal%2520under%250Amild%2520assumptions.%2520Further%252C%2520we%2520provide%2520the%2520first%2520vanishing-regret%2520guarantees%2520for%250ACBwLC%2520%2528or%2520CBwK%2529%2520that%2520extend%2520beyond%2520the%2520stochastic%2520environment.%2520We%2520side-step%250Astrong%2520impossibility%2520results%2520from%2520prior%2520work%2520by%2520identifying%2520a%2520weaker%2520%2528and%252C%250Aarguably%252C%2520fairer%2529%2520benchmark%2520to%2520compare%2520against.%2520Our%2520algorithm%2520builds%2520on%250ALagrangeBwK%2520%2528Immorlica%2520et%2520al.%252C%2520FOCS%25202019%2529%252C%2520a%2520Lagrangian-based%2520technique%2520for%250ACBwK%252C%2520and%2520SquareCB%2520%2528Foster%2520and%2520Rakhlin%252C%2520ICML%25202020%2529%252C%2520a%2520regression-based%250Atechnique%2520for%2520contextual%2520bandits.%2520Our%2520analysis%2520leverages%2520the%2520inherent%250Amodularity%2520of%2520both%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.07484v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Bandits%20with%20Packing%20and%20Covering%20Constraints%3A%20A%20Modular%0A%20%20Lagrangian%20Approach%20via%20Regression&entry.906535625=Aleksandrs%20Slivkins%20and%20Xingyu%20Zhou%20and%20Karthik%20Abinav%20Sankararaman%20and%20Dylan%20J.%20Foster&entry.1292438233=%20%20We%20consider%20contextual%20bandits%20with%20linear%20constraints%20%28CBwLC%29%2C%20a%20variant%20of%0Acontextual%20bandits%20in%20which%20the%20algorithm%20consumes%20multiple%20resources%20subject%0Ato%20linear%20constraints%20on%20total%20consumption.%20This%20problem%20generalizes%20contextual%0Abandits%20with%20knapsacks%20%28CBwK%29%2C%20allowing%20for%20packing%20and%20covering%20constraints%2C%0Aas%20well%20as%20positive%20and%20negative%20resource%20consumption.%20We%20provide%20the%20first%0Aalgorithm%20for%20CBwLC%20%28or%20CBwK%29%20that%20is%20based%20on%20regression%20oracles.%20The%0Aalgorithm%20is%20simple%2C%20computationally%20efficient%2C%20and%20statistically%20optimal%20under%0Amild%20assumptions.%20Further%2C%20we%20provide%20the%20first%20vanishing-regret%20guarantees%20for%0ACBwLC%20%28or%20CBwK%29%20that%20extend%20beyond%20the%20stochastic%20environment.%20We%20side-step%0Astrong%20impossibility%20results%20from%20prior%20work%20by%20identifying%20a%20weaker%20%28and%2C%0Aarguably%2C%20fairer%29%20benchmark%20to%20compare%20against.%20Our%20algorithm%20builds%20on%0ALagrangeBwK%20%28Immorlica%20et%20al.%2C%20FOCS%202019%29%2C%20a%20Lagrangian-based%20technique%20for%0ACBwK%2C%20and%20SquareCB%20%28Foster%20and%20Rakhlin%2C%20ICML%202020%29%2C%20a%20regression-based%0Atechnique%20for%20contextual%20bandits.%20Our%20analysis%20leverages%20the%20inherent%0Amodularity%20of%20both%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.07484v6&entry.124074799=Read"},
{"title": "Enhancing Multistep Prediction of Multivariate Market Indices Using\n  Weighted Optical Reservoir Computing", "author": "Fang Wang and Ting Bu and Yuping Huang", "abstract": "  We propose and experimentally demonstrate an innovative stock index\nprediction method using a weighted optical reservoir computing system. We\nconstruct fundamental market data combined with macroeconomic data and\ntechnical indicators to capture the broader behavior of the stock market. Our\napproach shows significant higher performance than state-of-the-art methods\nsuch as linear regression, decision trees, and neural network architectures\nincluding long short-term memory. It captures well the market's high volatility\nand nonlinear behaviors despite limited data, demonstrating great potential for\nreal-time, parallel, multi-dimensional data processing and predictions.\n", "link": "http://arxiv.org/abs/2408.00652v1", "date": "2024-08-01", "relevancy": 0.9694, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5078}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.478}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Multistep%20Prediction%20of%20Multivariate%20Market%20Indices%20Using%0A%20%20Weighted%20Optical%20Reservoir%20Computing&body=Title%3A%20Enhancing%20Multistep%20Prediction%20of%20Multivariate%20Market%20Indices%20Using%0A%20%20Weighted%20Optical%20Reservoir%20Computing%0AAuthor%3A%20Fang%20Wang%20and%20Ting%20Bu%20and%20Yuping%20Huang%0AAbstract%3A%20%20%20We%20propose%20and%20experimentally%20demonstrate%20an%20innovative%20stock%20index%0Aprediction%20method%20using%20a%20weighted%20optical%20reservoir%20computing%20system.%20We%0Aconstruct%20fundamental%20market%20data%20combined%20with%20macroeconomic%20data%20and%0Atechnical%20indicators%20to%20capture%20the%20broader%20behavior%20of%20the%20stock%20market.%20Our%0Aapproach%20shows%20significant%20higher%20performance%20than%20state-of-the-art%20methods%0Asuch%20as%20linear%20regression%2C%20decision%20trees%2C%20and%20neural%20network%20architectures%0Aincluding%20long%20short-term%20memory.%20It%20captures%20well%20the%20market%27s%20high%20volatility%0Aand%20nonlinear%20behaviors%20despite%20limited%20data%2C%20demonstrating%20great%20potential%20for%0Areal-time%2C%20parallel%2C%20multi-dimensional%20data%20processing%20and%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Multistep%2520Prediction%2520of%2520Multivariate%2520Market%2520Indices%2520Using%250A%2520%2520Weighted%2520Optical%2520Reservoir%2520Computing%26entry.906535625%3DFang%2520Wang%2520and%2520Ting%2520Bu%2520and%2520Yuping%2520Huang%26entry.1292438233%3D%2520%2520We%2520propose%2520and%2520experimentally%2520demonstrate%2520an%2520innovative%2520stock%2520index%250Aprediction%2520method%2520using%2520a%2520weighted%2520optical%2520reservoir%2520computing%2520system.%2520We%250Aconstruct%2520fundamental%2520market%2520data%2520combined%2520with%2520macroeconomic%2520data%2520and%250Atechnical%2520indicators%2520to%2520capture%2520the%2520broader%2520behavior%2520of%2520the%2520stock%2520market.%2520Our%250Aapproach%2520shows%2520significant%2520higher%2520performance%2520than%2520state-of-the-art%2520methods%250Asuch%2520as%2520linear%2520regression%252C%2520decision%2520trees%252C%2520and%2520neural%2520network%2520architectures%250Aincluding%2520long%2520short-term%2520memory.%2520It%2520captures%2520well%2520the%2520market%2527s%2520high%2520volatility%250Aand%2520nonlinear%2520behaviors%2520despite%2520limited%2520data%252C%2520demonstrating%2520great%2520potential%2520for%250Areal-time%252C%2520parallel%252C%2520multi-dimensional%2520data%2520processing%2520and%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Multistep%20Prediction%20of%20Multivariate%20Market%20Indices%20Using%0A%20%20Weighted%20Optical%20Reservoir%20Computing&entry.906535625=Fang%20Wang%20and%20Ting%20Bu%20and%20Yuping%20Huang&entry.1292438233=%20%20We%20propose%20and%20experimentally%20demonstrate%20an%20innovative%20stock%20index%0Aprediction%20method%20using%20a%20weighted%20optical%20reservoir%20computing%20system.%20We%0Aconstruct%20fundamental%20market%20data%20combined%20with%20macroeconomic%20data%20and%0Atechnical%20indicators%20to%20capture%20the%20broader%20behavior%20of%20the%20stock%20market.%20Our%0Aapproach%20shows%20significant%20higher%20performance%20than%20state-of-the-art%20methods%0Asuch%20as%20linear%20regression%2C%20decision%20trees%2C%20and%20neural%20network%20architectures%0Aincluding%20long%20short-term%20memory.%20It%20captures%20well%20the%20market%27s%20high%20volatility%0Aand%20nonlinear%20behaviors%20despite%20limited%20data%2C%20demonstrating%20great%20potential%20for%0Areal-time%2C%20parallel%2C%20multi-dimensional%20data%20processing%20and%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00652v1&entry.124074799=Read"},
{"title": "Using CSNNs to Perform Event-based Data Processing & Classification on\n  ASL-DVS", "author": "Ria Patel and Sujit Tripathy and Zachary Sublett and Seoyoung An and Riya Patel", "abstract": "  Recent advancements in bio-inspired visual sensing and neuromorphic computing\nhave led to the development of various highly efficient bio-inspired solutions\nwith real-world applications. One notable application integrates event-based\ncameras with spiking neural networks (SNNs) to process event-based sequences\nthat are asynchronous and sparse, making them difficult to handle. In this\nproject, we develop a convolutional spiking neural network (CSNN) architecture\nthat leverages convolutional operations and recurrent properties of a spiking\nneuron to learn the spatial and temporal relations in the ASL-DVS gesture\ndataset. The ASL-DVS gesture dataset is a neuromorphic dataset containing hand\ngestures when displaying 24 letters (A to Y, excluding J and Z due to the\nnature of their symbols) from the American Sign Language (ASL). We performed\nclassification on a pre-processed subset of the full ASL-DVS dataset to\nidentify letter signs and achieved 100\\% training accuracy. Specifically, this\nwas achieved by training in the Google Cloud compute platform while using a\nlearning rate of 0.0005, batch size of 25 (total of 20 batches), 200\niterations, and 10 epochs.\n", "link": "http://arxiv.org/abs/2408.00611v1", "date": "2024-08-01", "relevancy": 1.4857, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5046}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20CSNNs%20to%20Perform%20Event-based%20Data%20Processing%20%26%20Classification%20on%0A%20%20ASL-DVS&body=Title%3A%20Using%20CSNNs%20to%20Perform%20Event-based%20Data%20Processing%20%26%20Classification%20on%0A%20%20ASL-DVS%0AAuthor%3A%20Ria%20Patel%20and%20Sujit%20Tripathy%20and%20Zachary%20Sublett%20and%20Seoyoung%20An%20and%20Riya%20Patel%0AAbstract%3A%20%20%20Recent%20advancements%20in%20bio-inspired%20visual%20sensing%20and%20neuromorphic%20computing%0Ahave%20led%20to%20the%20development%20of%20various%20highly%20efficient%20bio-inspired%20solutions%0Awith%20real-world%20applications.%20One%20notable%20application%20integrates%20event-based%0Acameras%20with%20spiking%20neural%20networks%20%28SNNs%29%20to%20process%20event-based%20sequences%0Athat%20are%20asynchronous%20and%20sparse%2C%20making%20them%20difficult%20to%20handle.%20In%20this%0Aproject%2C%20we%20develop%20a%20convolutional%20spiking%20neural%20network%20%28CSNN%29%20architecture%0Athat%20leverages%20convolutional%20operations%20and%20recurrent%20properties%20of%20a%20spiking%0Aneuron%20to%20learn%20the%20spatial%20and%20temporal%20relations%20in%20the%20ASL-DVS%20gesture%0Adataset.%20The%20ASL-DVS%20gesture%20dataset%20is%20a%20neuromorphic%20dataset%20containing%20hand%0Agestures%20when%20displaying%2024%20letters%20%28A%20to%20Y%2C%20excluding%20J%20and%20Z%20due%20to%20the%0Anature%20of%20their%20symbols%29%20from%20the%20American%20Sign%20Language%20%28ASL%29.%20We%20performed%0Aclassification%20on%20a%20pre-processed%20subset%20of%20the%20full%20ASL-DVS%20dataset%20to%0Aidentify%20letter%20signs%20and%20achieved%20100%5C%25%20training%20accuracy.%20Specifically%2C%20this%0Awas%20achieved%20by%20training%20in%20the%20Google%20Cloud%20compute%20platform%20while%20using%20a%0Alearning%20rate%20of%200.0005%2C%20batch%20size%20of%2025%20%28total%20of%2020%20batches%29%2C%20200%0Aiterations%2C%20and%2010%20epochs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520CSNNs%2520to%2520Perform%2520Event-based%2520Data%2520Processing%2520%2526%2520Classification%2520on%250A%2520%2520ASL-DVS%26entry.906535625%3DRia%2520Patel%2520and%2520Sujit%2520Tripathy%2520and%2520Zachary%2520Sublett%2520and%2520Seoyoung%2520An%2520and%2520Riya%2520Patel%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520bio-inspired%2520visual%2520sensing%2520and%2520neuromorphic%2520computing%250Ahave%2520led%2520to%2520the%2520development%2520of%2520various%2520highly%2520efficient%2520bio-inspired%2520solutions%250Awith%2520real-world%2520applications.%2520One%2520notable%2520application%2520integrates%2520event-based%250Acameras%2520with%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520to%2520process%2520event-based%2520sequences%250Athat%2520are%2520asynchronous%2520and%2520sparse%252C%2520making%2520them%2520difficult%2520to%2520handle.%2520In%2520this%250Aproject%252C%2520we%2520develop%2520a%2520convolutional%2520spiking%2520neural%2520network%2520%2528CSNN%2529%2520architecture%250Athat%2520leverages%2520convolutional%2520operations%2520and%2520recurrent%2520properties%2520of%2520a%2520spiking%250Aneuron%2520to%2520learn%2520the%2520spatial%2520and%2520temporal%2520relations%2520in%2520the%2520ASL-DVS%2520gesture%250Adataset.%2520The%2520ASL-DVS%2520gesture%2520dataset%2520is%2520a%2520neuromorphic%2520dataset%2520containing%2520hand%250Agestures%2520when%2520displaying%252024%2520letters%2520%2528A%2520to%2520Y%252C%2520excluding%2520J%2520and%2520Z%2520due%2520to%2520the%250Anature%2520of%2520their%2520symbols%2529%2520from%2520the%2520American%2520Sign%2520Language%2520%2528ASL%2529.%2520We%2520performed%250Aclassification%2520on%2520a%2520pre-processed%2520subset%2520of%2520the%2520full%2520ASL-DVS%2520dataset%2520to%250Aidentify%2520letter%2520signs%2520and%2520achieved%2520100%255C%2525%2520training%2520accuracy.%2520Specifically%252C%2520this%250Awas%2520achieved%2520by%2520training%2520in%2520the%2520Google%2520Cloud%2520compute%2520platform%2520while%2520using%2520a%250Alearning%2520rate%2520of%25200.0005%252C%2520batch%2520size%2520of%252025%2520%2528total%2520of%252020%2520batches%2529%252C%2520200%250Aiterations%252C%2520and%252010%2520epochs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20CSNNs%20to%20Perform%20Event-based%20Data%20Processing%20%26%20Classification%20on%0A%20%20ASL-DVS&entry.906535625=Ria%20Patel%20and%20Sujit%20Tripathy%20and%20Zachary%20Sublett%20and%20Seoyoung%20An%20and%20Riya%20Patel&entry.1292438233=%20%20Recent%20advancements%20in%20bio-inspired%20visual%20sensing%20and%20neuromorphic%20computing%0Ahave%20led%20to%20the%20development%20of%20various%20highly%20efficient%20bio-inspired%20solutions%0Awith%20real-world%20applications.%20One%20notable%20application%20integrates%20event-based%0Acameras%20with%20spiking%20neural%20networks%20%28SNNs%29%20to%20process%20event-based%20sequences%0Athat%20are%20asynchronous%20and%20sparse%2C%20making%20them%20difficult%20to%20handle.%20In%20this%0Aproject%2C%20we%20develop%20a%20convolutional%20spiking%20neural%20network%20%28CSNN%29%20architecture%0Athat%20leverages%20convolutional%20operations%20and%20recurrent%20properties%20of%20a%20spiking%0Aneuron%20to%20learn%20the%20spatial%20and%20temporal%20relations%20in%20the%20ASL-DVS%20gesture%0Adataset.%20The%20ASL-DVS%20gesture%20dataset%20is%20a%20neuromorphic%20dataset%20containing%20hand%0Agestures%20when%20displaying%2024%20letters%20%28A%20to%20Y%2C%20excluding%20J%20and%20Z%20due%20to%20the%0Anature%20of%20their%20symbols%29%20from%20the%20American%20Sign%20Language%20%28ASL%29.%20We%20performed%0Aclassification%20on%20a%20pre-processed%20subset%20of%20the%20full%20ASL-DVS%20dataset%20to%0Aidentify%20letter%20signs%20and%20achieved%20100%5C%25%20training%20accuracy.%20Specifically%2C%20this%0Awas%20achieved%20by%20training%20in%20the%20Google%20Cloud%20compute%20platform%20while%20using%20a%0Alearning%20rate%20of%200.0005%2C%20batch%20size%20of%2025%20%28total%20of%2020%20batches%29%2C%20200%0Aiterations%2C%20and%2010%20epochs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00611v1&entry.124074799=Read"},
{"title": "A causal intervention framework for synthesizing mobility data and\n  evaluating predictive neural networks", "author": "Ye Hong and Yanan Xin and Simon Dirmeier and Fernando Perez-Cruz and Martin Raubal", "abstract": "  Deep neural networks are increasingly utilized in mobility prediction tasks,\nyet their intricate internal workings pose challenges for interpretability,\nespecially in comprehending how various aspects of mobility behavior affect\npredictions. This study introduces a causal intervention framework to assess\nthe impact of mobility-related factors on neural networks designed for next\nlocation prediction -- a task focusing on predicting the immediate next\nlocation of an individual. To achieve this, we employ individual mobility\nmodels to synthesize location visit sequences and control behavior dynamics by\nintervening in their data generation process. We evaluate the interventional\nlocation sequences using mobility metrics and input them into well-trained\nnetworks to analyze performance variations. The results demonstrate the\neffectiveness in producing location sequences with distinct mobility behaviors,\nthereby facilitating the simulation of diverse yet realistic spatial and\ntemporal changes. These changes result in performance fluctuations in next\nlocation prediction networks, revealing impacts of critical mobility behavior\nfactors, including sequential patterns in location transitions, proclivity for\nexploring new locations, and preferences in location choices at population and\nindividual levels. The gained insights hold value for the real-world\napplication of mobility prediction networks, and the framework is expected to\npromote the use of causal inference to enhance the interpretability and\nrobustness of neural networks in mobility applications.\n", "link": "http://arxiv.org/abs/2311.11749v3", "date": "2024-08-01", "relevancy": 1.5014, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5531}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4882}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20causal%20intervention%20framework%20for%20synthesizing%20mobility%20data%20and%0A%20%20evaluating%20predictive%20neural%20networks&body=Title%3A%20A%20causal%20intervention%20framework%20for%20synthesizing%20mobility%20data%20and%0A%20%20evaluating%20predictive%20neural%20networks%0AAuthor%3A%20Ye%20Hong%20and%20Yanan%20Xin%20and%20Simon%20Dirmeier%20and%20Fernando%20Perez-Cruz%20and%20Martin%20Raubal%0AAbstract%3A%20%20%20Deep%20neural%20networks%20are%20increasingly%20utilized%20in%20mobility%20prediction%20tasks%2C%0Ayet%20their%20intricate%20internal%20workings%20pose%20challenges%20for%20interpretability%2C%0Aespecially%20in%20comprehending%20how%20various%20aspects%20of%20mobility%20behavior%20affect%0Apredictions.%20This%20study%20introduces%20a%20causal%20intervention%20framework%20to%20assess%0Athe%20impact%20of%20mobility-related%20factors%20on%20neural%20networks%20designed%20for%20next%0Alocation%20prediction%20--%20a%20task%20focusing%20on%20predicting%20the%20immediate%20next%0Alocation%20of%20an%20individual.%20To%20achieve%20this%2C%20we%20employ%20individual%20mobility%0Amodels%20to%20synthesize%20location%20visit%20sequences%20and%20control%20behavior%20dynamics%20by%0Aintervening%20in%20their%20data%20generation%20process.%20We%20evaluate%20the%20interventional%0Alocation%20sequences%20using%20mobility%20metrics%20and%20input%20them%20into%20well-trained%0Anetworks%20to%20analyze%20performance%20variations.%20The%20results%20demonstrate%20the%0Aeffectiveness%20in%20producing%20location%20sequences%20with%20distinct%20mobility%20behaviors%2C%0Athereby%20facilitating%20the%20simulation%20of%20diverse%20yet%20realistic%20spatial%20and%0Atemporal%20changes.%20These%20changes%20result%20in%20performance%20fluctuations%20in%20next%0Alocation%20prediction%20networks%2C%20revealing%20impacts%20of%20critical%20mobility%20behavior%0Afactors%2C%20including%20sequential%20patterns%20in%20location%20transitions%2C%20proclivity%20for%0Aexploring%20new%20locations%2C%20and%20preferences%20in%20location%20choices%20at%20population%20and%0Aindividual%20levels.%20The%20gained%20insights%20hold%20value%20for%20the%20real-world%0Aapplication%20of%20mobility%20prediction%20networks%2C%20and%20the%20framework%20is%20expected%20to%0Apromote%20the%20use%20of%20causal%20inference%20to%20enhance%20the%20interpretability%20and%0Arobustness%20of%20neural%20networks%20in%20mobility%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11749v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520causal%2520intervention%2520framework%2520for%2520synthesizing%2520mobility%2520data%2520and%250A%2520%2520evaluating%2520predictive%2520neural%2520networks%26entry.906535625%3DYe%2520Hong%2520and%2520Yanan%2520Xin%2520and%2520Simon%2520Dirmeier%2520and%2520Fernando%2520Perez-Cruz%2520and%2520Martin%2520Raubal%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520are%2520increasingly%2520utilized%2520in%2520mobility%2520prediction%2520tasks%252C%250Ayet%2520their%2520intricate%2520internal%2520workings%2520pose%2520challenges%2520for%2520interpretability%252C%250Aespecially%2520in%2520comprehending%2520how%2520various%2520aspects%2520of%2520mobility%2520behavior%2520affect%250Apredictions.%2520This%2520study%2520introduces%2520a%2520causal%2520intervention%2520framework%2520to%2520assess%250Athe%2520impact%2520of%2520mobility-related%2520factors%2520on%2520neural%2520networks%2520designed%2520for%2520next%250Alocation%2520prediction%2520--%2520a%2520task%2520focusing%2520on%2520predicting%2520the%2520immediate%2520next%250Alocation%2520of%2520an%2520individual.%2520To%2520achieve%2520this%252C%2520we%2520employ%2520individual%2520mobility%250Amodels%2520to%2520synthesize%2520location%2520visit%2520sequences%2520and%2520control%2520behavior%2520dynamics%2520by%250Aintervening%2520in%2520their%2520data%2520generation%2520process.%2520We%2520evaluate%2520the%2520interventional%250Alocation%2520sequences%2520using%2520mobility%2520metrics%2520and%2520input%2520them%2520into%2520well-trained%250Anetworks%2520to%2520analyze%2520performance%2520variations.%2520The%2520results%2520demonstrate%2520the%250Aeffectiveness%2520in%2520producing%2520location%2520sequences%2520with%2520distinct%2520mobility%2520behaviors%252C%250Athereby%2520facilitating%2520the%2520simulation%2520of%2520diverse%2520yet%2520realistic%2520spatial%2520and%250Atemporal%2520changes.%2520These%2520changes%2520result%2520in%2520performance%2520fluctuations%2520in%2520next%250Alocation%2520prediction%2520networks%252C%2520revealing%2520impacts%2520of%2520critical%2520mobility%2520behavior%250Afactors%252C%2520including%2520sequential%2520patterns%2520in%2520location%2520transitions%252C%2520proclivity%2520for%250Aexploring%2520new%2520locations%252C%2520and%2520preferences%2520in%2520location%2520choices%2520at%2520population%2520and%250Aindividual%2520levels.%2520The%2520gained%2520insights%2520hold%2520value%2520for%2520the%2520real-world%250Aapplication%2520of%2520mobility%2520prediction%2520networks%252C%2520and%2520the%2520framework%2520is%2520expected%2520to%250Apromote%2520the%2520use%2520of%2520causal%2520inference%2520to%2520enhance%2520the%2520interpretability%2520and%250Arobustness%2520of%2520neural%2520networks%2520in%2520mobility%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11749v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20causal%20intervention%20framework%20for%20synthesizing%20mobility%20data%20and%0A%20%20evaluating%20predictive%20neural%20networks&entry.906535625=Ye%20Hong%20and%20Yanan%20Xin%20and%20Simon%20Dirmeier%20and%20Fernando%20Perez-Cruz%20and%20Martin%20Raubal&entry.1292438233=%20%20Deep%20neural%20networks%20are%20increasingly%20utilized%20in%20mobility%20prediction%20tasks%2C%0Ayet%20their%20intricate%20internal%20workings%20pose%20challenges%20for%20interpretability%2C%0Aespecially%20in%20comprehending%20how%20various%20aspects%20of%20mobility%20behavior%20affect%0Apredictions.%20This%20study%20introduces%20a%20causal%20intervention%20framework%20to%20assess%0Athe%20impact%20of%20mobility-related%20factors%20on%20neural%20networks%20designed%20for%20next%0Alocation%20prediction%20--%20a%20task%20focusing%20on%20predicting%20the%20immediate%20next%0Alocation%20of%20an%20individual.%20To%20achieve%20this%2C%20we%20employ%20individual%20mobility%0Amodels%20to%20synthesize%20location%20visit%20sequences%20and%20control%20behavior%20dynamics%20by%0Aintervening%20in%20their%20data%20generation%20process.%20We%20evaluate%20the%20interventional%0Alocation%20sequences%20using%20mobility%20metrics%20and%20input%20them%20into%20well-trained%0Anetworks%20to%20analyze%20performance%20variations.%20The%20results%20demonstrate%20the%0Aeffectiveness%20in%20producing%20location%20sequences%20with%20distinct%20mobility%20behaviors%2C%0Athereby%20facilitating%20the%20simulation%20of%20diverse%20yet%20realistic%20spatial%20and%0Atemporal%20changes.%20These%20changes%20result%20in%20performance%20fluctuations%20in%20next%0Alocation%20prediction%20networks%2C%20revealing%20impacts%20of%20critical%20mobility%20behavior%0Afactors%2C%20including%20sequential%20patterns%20in%20location%20transitions%2C%20proclivity%20for%0Aexploring%20new%20locations%2C%20and%20preferences%20in%20location%20choices%20at%20population%20and%0Aindividual%20levels.%20The%20gained%20insights%20hold%20value%20for%20the%20real-world%0Aapplication%20of%20mobility%20prediction%20networks%2C%20and%20the%20framework%20is%20expected%20to%0Apromote%20the%20use%20of%20causal%20inference%20to%20enhance%20the%20interpretability%20and%0Arobustness%20of%20neural%20networks%20in%20mobility%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11749v3&entry.124074799=Read"},
{"title": "SentenceVAE: Faster, Longer and More Accurate Inference with\n  Next-sentence Prediction for Large Language Models", "author": "Hongjun An and Yifan Chen and Xiaozhen Qiao and Zhe Sun and Xuelong Li", "abstract": "  Contemporary large language models (LLMs) predominantly utilize a next-token\nprediction method for inference, which significantly impedes their processing\nspeed. In this paper, we introduce a novel inference methodology termed\nnext-sentence prediction, aimed at enhancing the inference efficiency of LLMs.\nWe present SentenceVAE, a tiny model consisting of an encoder and a decoder.\nThe encoder effectively condenses the information within a sentence into a\nsingular token, while the decoder reconstructs this compressed data back into\nits original sentential form. By integrating SentenceVAE into the input and\noutput layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference approach, markedly accelerating inference\nspeeds. SentenceVAE also maintains the integrity of the original semantic\ncontent by segmenting the text into sentences, thereby preserving accuracy\nwhile boosting inference speeds. Compared to traditional LLMs, SLLMs process\nfewer tokens over equivalent context lengths, significantly reducing memory\ndemands for Self-Attention computations and facilitating the handling of longer\ncontexts. Our experimental findings reveal that this method can increase\ninference speeds by 204~365%, reduce perplexity (PPL) to 46~75% of its original\nmetric, and decrease memory overhead by 86~91% for the same context length. The\nadvantages of this approach are further amplified with increases in model\nparameters.\n", "link": "http://arxiv.org/abs/2408.00655v1", "date": "2024-08-01", "relevancy": 1.3732, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SentenceVAE%3A%20Faster%2C%20Longer%20and%20More%20Accurate%20Inference%20with%0A%20%20Next-sentence%20Prediction%20for%20Large%20Language%20Models&body=Title%3A%20SentenceVAE%3A%20Faster%2C%20Longer%20and%20More%20Accurate%20Inference%20with%0A%20%20Next-sentence%20Prediction%20for%20Large%20Language%20Models%0AAuthor%3A%20Hongjun%20An%20and%20Yifan%20Chen%20and%20Xiaozhen%20Qiao%20and%20Zhe%20Sun%20and%20Xuelong%20Li%0AAbstract%3A%20%20%20Contemporary%20large%20language%20models%20%28LLMs%29%20predominantly%20utilize%20a%20next-token%0Aprediction%20method%20for%20inference%2C%20which%20significantly%20impedes%20their%20processing%0Aspeed.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20inference%20methodology%20termed%0Anext-sentence%20prediction%2C%20aimed%20at%20enhancing%20the%20inference%20efficiency%20of%20LLMs.%0AWe%20present%20SentenceVAE%2C%20a%20tiny%20model%20consisting%20of%20an%20encoder%20and%20a%20decoder.%0AThe%20encoder%20effectively%20condenses%20the%20information%20within%20a%20sentence%20into%20a%0Asingular%20token%2C%20while%20the%20decoder%20reconstructs%20this%20compressed%20data%20back%20into%0Aits%20original%20sentential%20form.%20By%20integrating%20SentenceVAE%20into%20the%20input%20and%0Aoutput%20layers%20of%20LLMs%2C%20we%20develop%20Sentence-level%20LLMs%20%28SLLMs%29%20that%20employ%20a%0Asentence-by-sentence%20inference%20approach%2C%20markedly%20accelerating%20inference%0Aspeeds.%20SentenceVAE%20also%20maintains%20the%20integrity%20of%20the%20original%20semantic%0Acontent%20by%20segmenting%20the%20text%20into%20sentences%2C%20thereby%20preserving%20accuracy%0Awhile%20boosting%20inference%20speeds.%20Compared%20to%20traditional%20LLMs%2C%20SLLMs%20process%0Afewer%20tokens%20over%20equivalent%20context%20lengths%2C%20significantly%20reducing%20memory%0Ademands%20for%20Self-Attention%20computations%20and%20facilitating%20the%20handling%20of%20longer%0Acontexts.%20Our%20experimental%20findings%20reveal%20that%20this%20method%20can%20increase%0Ainference%20speeds%20by%20204~365%25%2C%20reduce%20perplexity%20%28PPL%29%20to%2046~75%25%20of%20its%20original%0Ametric%2C%20and%20decrease%20memory%20overhead%20by%2086~91%25%20for%20the%20same%20context%20length.%20The%0Aadvantages%20of%20this%20approach%20are%20further%20amplified%20with%20increases%20in%20model%0Aparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSentenceVAE%253A%2520Faster%252C%2520Longer%2520and%2520More%2520Accurate%2520Inference%2520with%250A%2520%2520Next-sentence%2520Prediction%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DHongjun%2520An%2520and%2520Yifan%2520Chen%2520and%2520Xiaozhen%2520Qiao%2520and%2520Zhe%2520Sun%2520and%2520Xuelong%2520Li%26entry.1292438233%3D%2520%2520Contemporary%2520large%2520language%2520models%2520%2528LLMs%2529%2520predominantly%2520utilize%2520a%2520next-token%250Aprediction%2520method%2520for%2520inference%252C%2520which%2520significantly%2520impedes%2520their%2520processing%250Aspeed.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520inference%2520methodology%2520termed%250Anext-sentence%2520prediction%252C%2520aimed%2520at%2520enhancing%2520the%2520inference%2520efficiency%2520of%2520LLMs.%250AWe%2520present%2520SentenceVAE%252C%2520a%2520tiny%2520model%2520consisting%2520of%2520an%2520encoder%2520and%2520a%2520decoder.%250AThe%2520encoder%2520effectively%2520condenses%2520the%2520information%2520within%2520a%2520sentence%2520into%2520a%250Asingular%2520token%252C%2520while%2520the%2520decoder%2520reconstructs%2520this%2520compressed%2520data%2520back%2520into%250Aits%2520original%2520sentential%2520form.%2520By%2520integrating%2520SentenceVAE%2520into%2520the%2520input%2520and%250Aoutput%2520layers%2520of%2520LLMs%252C%2520we%2520develop%2520Sentence-level%2520LLMs%2520%2528SLLMs%2529%2520that%2520employ%2520a%250Asentence-by-sentence%2520inference%2520approach%252C%2520markedly%2520accelerating%2520inference%250Aspeeds.%2520SentenceVAE%2520also%2520maintains%2520the%2520integrity%2520of%2520the%2520original%2520semantic%250Acontent%2520by%2520segmenting%2520the%2520text%2520into%2520sentences%252C%2520thereby%2520preserving%2520accuracy%250Awhile%2520boosting%2520inference%2520speeds.%2520Compared%2520to%2520traditional%2520LLMs%252C%2520SLLMs%2520process%250Afewer%2520tokens%2520over%2520equivalent%2520context%2520lengths%252C%2520significantly%2520reducing%2520memory%250Ademands%2520for%2520Self-Attention%2520computations%2520and%2520facilitating%2520the%2520handling%2520of%2520longer%250Acontexts.%2520Our%2520experimental%2520findings%2520reveal%2520that%2520this%2520method%2520can%2520increase%250Ainference%2520speeds%2520by%2520204~365%2525%252C%2520reduce%2520perplexity%2520%2528PPL%2529%2520to%252046~75%2525%2520of%2520its%2520original%250Ametric%252C%2520and%2520decrease%2520memory%2520overhead%2520by%252086~91%2525%2520for%2520the%2520same%2520context%2520length.%2520The%250Aadvantages%2520of%2520this%2520approach%2520are%2520further%2520amplified%2520with%2520increases%2520in%2520model%250Aparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SentenceVAE%3A%20Faster%2C%20Longer%20and%20More%20Accurate%20Inference%20with%0A%20%20Next-sentence%20Prediction%20for%20Large%20Language%20Models&entry.906535625=Hongjun%20An%20and%20Yifan%20Chen%20and%20Xiaozhen%20Qiao%20and%20Zhe%20Sun%20and%20Xuelong%20Li&entry.1292438233=%20%20Contemporary%20large%20language%20models%20%28LLMs%29%20predominantly%20utilize%20a%20next-token%0Aprediction%20method%20for%20inference%2C%20which%20significantly%20impedes%20their%20processing%0Aspeed.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20inference%20methodology%20termed%0Anext-sentence%20prediction%2C%20aimed%20at%20enhancing%20the%20inference%20efficiency%20of%20LLMs.%0AWe%20present%20SentenceVAE%2C%20a%20tiny%20model%20consisting%20of%20an%20encoder%20and%20a%20decoder.%0AThe%20encoder%20effectively%20condenses%20the%20information%20within%20a%20sentence%20into%20a%0Asingular%20token%2C%20while%20the%20decoder%20reconstructs%20this%20compressed%20data%20back%20into%0Aits%20original%20sentential%20form.%20By%20integrating%20SentenceVAE%20into%20the%20input%20and%0Aoutput%20layers%20of%20LLMs%2C%20we%20develop%20Sentence-level%20LLMs%20%28SLLMs%29%20that%20employ%20a%0Asentence-by-sentence%20inference%20approach%2C%20markedly%20accelerating%20inference%0Aspeeds.%20SentenceVAE%20also%20maintains%20the%20integrity%20of%20the%20original%20semantic%0Acontent%20by%20segmenting%20the%20text%20into%20sentences%2C%20thereby%20preserving%20accuracy%0Awhile%20boosting%20inference%20speeds.%20Compared%20to%20traditional%20LLMs%2C%20SLLMs%20process%0Afewer%20tokens%20over%20equivalent%20context%20lengths%2C%20significantly%20reducing%20memory%0Ademands%20for%20Self-Attention%20computations%20and%20facilitating%20the%20handling%20of%20longer%0Acontexts.%20Our%20experimental%20findings%20reveal%20that%20this%20method%20can%20increase%0Ainference%20speeds%20by%20204~365%25%2C%20reduce%20perplexity%20%28PPL%29%20to%2046~75%25%20of%20its%20original%0Ametric%2C%20and%20decrease%20memory%20overhead%20by%2086~91%25%20for%20the%20same%20context%20length.%20The%0Aadvantages%20of%20this%20approach%20are%20further%20amplified%20with%20increases%20in%20model%0Aparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00655v1&entry.124074799=Read"},
{"title": "DATENeRF: Depth-Aware Text-based Editing of NeRFs", "author": "Sara Rojas and Julien Philip and Kai Zhang and Sai Bi and Fujun Luan and Bernard Ghanem and Kalyan Sunkavall", "abstract": "  Recent advancements in diffusion models have shown remarkable proficiency in\nediting 2D images based on text prompts. However, extending these techniques to\nedit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual\n2D frames can result in inconsistencies across multiple views. Our crucial\ninsight is that a NeRF scene's geometry can serve as a bridge to integrate\nthese 2D edits. Utilizing this geometry, we employ a depth-conditioned\nControlNet to enhance the coherence of each 2D image modification. Moreover, we\nintroduce an inpainting approach that leverages the depth information of NeRF\nscenes to distribute 2D edits across different images, ensuring robustness\nagainst errors and resampling challenges. Our results reveal that this\nmethodology achieves more consistent, lifelike, and detailed edits than\nexisting leading methods for text-driven NeRF scene editing.\n", "link": "http://arxiv.org/abs/2404.04526v2", "date": "2024-08-01", "relevancy": 1.115, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5746}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5528}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DATENeRF%3A%20Depth-Aware%20Text-based%20Editing%20of%20NeRFs&body=Title%3A%20DATENeRF%3A%20Depth-Aware%20Text-based%20Editing%20of%20NeRFs%0AAuthor%3A%20Sara%20Rojas%20and%20Julien%20Philip%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Fujun%20Luan%20and%20Bernard%20Ghanem%20and%20Kalyan%20Sunkavall%0AAbstract%3A%20%20%20Recent%20advancements%20in%20diffusion%20models%20have%20shown%20remarkable%20proficiency%20in%0Aediting%202D%20images%20based%20on%20text%20prompts.%20However%2C%20extending%20these%20techniques%20to%0Aedit%20scenes%20in%20Neural%20Radiance%20Fields%20%28NeRF%29%20is%20complex%2C%20as%20editing%20individual%0A2D%20frames%20can%20result%20in%20inconsistencies%20across%20multiple%20views.%20Our%20crucial%0Ainsight%20is%20that%20a%20NeRF%20scene%27s%20geometry%20can%20serve%20as%20a%20bridge%20to%20integrate%0Athese%202D%20edits.%20Utilizing%20this%20geometry%2C%20we%20employ%20a%20depth-conditioned%0AControlNet%20to%20enhance%20the%20coherence%20of%20each%202D%20image%20modification.%20Moreover%2C%20we%0Aintroduce%20an%20inpainting%20approach%20that%20leverages%20the%20depth%20information%20of%20NeRF%0Ascenes%20to%20distribute%202D%20edits%20across%20different%20images%2C%20ensuring%20robustness%0Aagainst%20errors%20and%20resampling%20challenges.%20Our%20results%20reveal%20that%20this%0Amethodology%20achieves%20more%20consistent%2C%20lifelike%2C%20and%20detailed%20edits%20than%0Aexisting%20leading%20methods%20for%20text-driven%20NeRF%20scene%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDATENeRF%253A%2520Depth-Aware%2520Text-based%2520Editing%2520of%2520NeRFs%26entry.906535625%3DSara%2520Rojas%2520and%2520Julien%2520Philip%2520and%2520Kai%2520Zhang%2520and%2520Sai%2520Bi%2520and%2520Fujun%2520Luan%2520and%2520Bernard%2520Ghanem%2520and%2520Kalyan%2520Sunkavall%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520diffusion%2520models%2520have%2520shown%2520remarkable%2520proficiency%2520in%250Aediting%25202D%2520images%2520based%2520on%2520text%2520prompts.%2520However%252C%2520extending%2520these%2520techniques%2520to%250Aedit%2520scenes%2520in%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520is%2520complex%252C%2520as%2520editing%2520individual%250A2D%2520frames%2520can%2520result%2520in%2520inconsistencies%2520across%2520multiple%2520views.%2520Our%2520crucial%250Ainsight%2520is%2520that%2520a%2520NeRF%2520scene%2527s%2520geometry%2520can%2520serve%2520as%2520a%2520bridge%2520to%2520integrate%250Athese%25202D%2520edits.%2520Utilizing%2520this%2520geometry%252C%2520we%2520employ%2520a%2520depth-conditioned%250AControlNet%2520to%2520enhance%2520the%2520coherence%2520of%2520each%25202D%2520image%2520modification.%2520Moreover%252C%2520we%250Aintroduce%2520an%2520inpainting%2520approach%2520that%2520leverages%2520the%2520depth%2520information%2520of%2520NeRF%250Ascenes%2520to%2520distribute%25202D%2520edits%2520across%2520different%2520images%252C%2520ensuring%2520robustness%250Aagainst%2520errors%2520and%2520resampling%2520challenges.%2520Our%2520results%2520reveal%2520that%2520this%250Amethodology%2520achieves%2520more%2520consistent%252C%2520lifelike%252C%2520and%2520detailed%2520edits%2520than%250Aexisting%2520leading%2520methods%2520for%2520text-driven%2520NeRF%2520scene%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DATENeRF%3A%20Depth-Aware%20Text-based%20Editing%20of%20NeRFs&entry.906535625=Sara%20Rojas%20and%20Julien%20Philip%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Fujun%20Luan%20and%20Bernard%20Ghanem%20and%20Kalyan%20Sunkavall&entry.1292438233=%20%20Recent%20advancements%20in%20diffusion%20models%20have%20shown%20remarkable%20proficiency%20in%0Aediting%202D%20images%20based%20on%20text%20prompts.%20However%2C%20extending%20these%20techniques%20to%0Aedit%20scenes%20in%20Neural%20Radiance%20Fields%20%28NeRF%29%20is%20complex%2C%20as%20editing%20individual%0A2D%20frames%20can%20result%20in%20inconsistencies%20across%20multiple%20views.%20Our%20crucial%0Ainsight%20is%20that%20a%20NeRF%20scene%27s%20geometry%20can%20serve%20as%20a%20bridge%20to%20integrate%0Athese%202D%20edits.%20Utilizing%20this%20geometry%2C%20we%20employ%20a%20depth-conditioned%0AControlNet%20to%20enhance%20the%20coherence%20of%20each%202D%20image%20modification.%20Moreover%2C%20we%0Aintroduce%20an%20inpainting%20approach%20that%20leverages%20the%20depth%20information%20of%20NeRF%0Ascenes%20to%20distribute%202D%20edits%20across%20different%20images%2C%20ensuring%20robustness%0Aagainst%20errors%20and%20resampling%20challenges.%20Our%20results%20reveal%20that%20this%0Amethodology%20achieves%20more%20consistent%2C%20lifelike%2C%20and%20detailed%20edits%20than%0Aexisting%20leading%20methods%20for%20text-driven%20NeRF%20scene%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04526v2&entry.124074799=Read"},
{"title": "Empowering Robot Path Planning with Large Language Models: osmAG Map\n  Topology & Hierarchy Comprehension with LLMs", "author": "Fujing Xie and S\u00f6ren Schwertfeger", "abstract": "  Large Language Models (LLMs) have demonstrated great potential in robotic\napplications by providing essential general knowledge. Mobile robots rely on\nmap comprehension for tasks like localization and navigation. In this paper, we\nexplore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a\ntext-based hierarchical, topometric semantic map representation utilizing\npolygons to demark areas such as rooms or buildings. Our experiments\ndemonstrate that with the right map representation, LLMs can effectively\ncomprehend Area Graph's topology and hierarchy. After straightforward\nfine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.\nOur dataset, dataset generation code, fine-tuned LoRA adapters can be accessed\nat https://github.com/xiefujing/LLM-osmAG-Comprehension.\n", "link": "http://arxiv.org/abs/2403.08228v2", "date": "2024-08-01", "relevancy": 1.6618, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5598}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5596}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Robot%20Path%20Planning%20with%20Large%20Language%20Models%3A%20osmAG%20Map%0A%20%20Topology%20%26%20Hierarchy%20Comprehension%20with%20LLMs&body=Title%3A%20Empowering%20Robot%20Path%20Planning%20with%20Large%20Language%20Models%3A%20osmAG%20Map%0A%20%20Topology%20%26%20Hierarchy%20Comprehension%20with%20LLMs%0AAuthor%3A%20Fujing%20Xie%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20great%20potential%20in%20robotic%0Aapplications%20by%20providing%20essential%20general%20knowledge.%20Mobile%20robots%20rely%20on%0Amap%20comprehension%20for%20tasks%20like%20localization%20and%20navigation.%20In%20this%20paper%2C%20we%0Aexplore%20enabling%20LLMs%20to%20comprehend%20the%20topology%20and%20hierarchy%20of%20Area%20Graph%2C%20a%0Atext-based%20hierarchical%2C%20topometric%20semantic%20map%20representation%20utilizing%0Apolygons%20to%20demark%20areas%20such%20as%20rooms%20or%20buildings.%20Our%20experiments%0Ademonstrate%20that%20with%20the%20right%20map%20representation%2C%20LLMs%20can%20effectively%0Acomprehend%20Area%20Graph%27s%20topology%20and%20hierarchy.%20After%20straightforward%0Afine-tuning%2C%20the%20LLaMA2%20models%20exceeded%20ChatGPT-3.5%20in%20mastering%20these%20aspects.%0AOur%20dataset%2C%20dataset%20generation%20code%2C%20fine-tuned%20LoRA%20adapters%20can%20be%20accessed%0Aat%20https%3A//github.com/xiefujing/LLM-osmAG-Comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Robot%2520Path%2520Planning%2520with%2520Large%2520Language%2520Models%253A%2520osmAG%2520Map%250A%2520%2520Topology%2520%2526%2520Hierarchy%2520Comprehension%2520with%2520LLMs%26entry.906535625%3DFujing%2520Xie%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520great%2520potential%2520in%2520robotic%250Aapplications%2520by%2520providing%2520essential%2520general%2520knowledge.%2520Mobile%2520robots%2520rely%2520on%250Amap%2520comprehension%2520for%2520tasks%2520like%2520localization%2520and%2520navigation.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520enabling%2520LLMs%2520to%2520comprehend%2520the%2520topology%2520and%2520hierarchy%2520of%2520Area%2520Graph%252C%2520a%250Atext-based%2520hierarchical%252C%2520topometric%2520semantic%2520map%2520representation%2520utilizing%250Apolygons%2520to%2520demark%2520areas%2520such%2520as%2520rooms%2520or%2520buildings.%2520Our%2520experiments%250Ademonstrate%2520that%2520with%2520the%2520right%2520map%2520representation%252C%2520LLMs%2520can%2520effectively%250Acomprehend%2520Area%2520Graph%2527s%2520topology%2520and%2520hierarchy.%2520After%2520straightforward%250Afine-tuning%252C%2520the%2520LLaMA2%2520models%2520exceeded%2520ChatGPT-3.5%2520in%2520mastering%2520these%2520aspects.%250AOur%2520dataset%252C%2520dataset%2520generation%2520code%252C%2520fine-tuned%2520LoRA%2520adapters%2520can%2520be%2520accessed%250Aat%2520https%253A//github.com/xiefujing/LLM-osmAG-Comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Robot%20Path%20Planning%20with%20Large%20Language%20Models%3A%20osmAG%20Map%0A%20%20Topology%20%26%20Hierarchy%20Comprehension%20with%20LLMs&entry.906535625=Fujing%20Xie%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20great%20potential%20in%20robotic%0Aapplications%20by%20providing%20essential%20general%20knowledge.%20Mobile%20robots%20rely%20on%0Amap%20comprehension%20for%20tasks%20like%20localization%20and%20navigation.%20In%20this%20paper%2C%20we%0Aexplore%20enabling%20LLMs%20to%20comprehend%20the%20topology%20and%20hierarchy%20of%20Area%20Graph%2C%20a%0Atext-based%20hierarchical%2C%20topometric%20semantic%20map%20representation%20utilizing%0Apolygons%20to%20demark%20areas%20such%20as%20rooms%20or%20buildings.%20Our%20experiments%0Ademonstrate%20that%20with%20the%20right%20map%20representation%2C%20LLMs%20can%20effectively%0Acomprehend%20Area%20Graph%27s%20topology%20and%20hierarchy.%20After%20straightforward%0Afine-tuning%2C%20the%20LLaMA2%20models%20exceeded%20ChatGPT-3.5%20in%20mastering%20these%20aspects.%0AOur%20dataset%2C%20dataset%20generation%20code%2C%20fine-tuned%20LoRA%20adapters%20can%20be%20accessed%0Aat%20https%3A//github.com/xiefujing/LLM-osmAG-Comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08228v2&entry.124074799=Read"},
{"title": "Towards Assessing Data Replication in Music Generation with Music\n  Similarity Metrics on Raw Audio", "author": "Roser Batlle-Roca and Wei-Hisang Liao and Xavier Serra and Yuki Mitsufuji and Emilia G\u00f3mez", "abstract": "  Recent advancements in music generation are raising multiple concerns about\nthe implications of AI in creative music processes, current business models and\nimpacts related to intellectual property management. A relevant discussion and\nrelated technical challenge is the potential replication and plagiarism of the\ntraining set in AI-generated music, which could lead to misuse of data and\nintellectual property rights violations. To tackle this issue, we present the\nMusic Replication Assessment (MiRA) tool: a model-independent open evaluation\nmethod based on diverse audio music similarity metrics to assess data\nreplication. We evaluate the ability of five metrics to identify exact\nreplication by conducting a controlled replication experiment in different\nmusic genres using synthetic samples. Our results show that the proposed\nmethodology can estimate exact data replication with a proportion higher than\n10%. By introducing the MiRA tool, we intend to encourage the open evaluation\nof music-generative models by researchers, developers, and users concerning\ndata replication, highlighting the importance of the ethical, social, legal,\nand economic consequences. Code and examples are available for reproducibility\npurposes.\n", "link": "http://arxiv.org/abs/2407.14364v2", "date": "2024-08-01", "relevancy": 1.5824, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4033}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Assessing%20Data%20Replication%20in%20Music%20Generation%20with%20Music%0A%20%20Similarity%20Metrics%20on%20Raw%20Audio&body=Title%3A%20Towards%20Assessing%20Data%20Replication%20in%20Music%20Generation%20with%20Music%0A%20%20Similarity%20Metrics%20on%20Raw%20Audio%0AAuthor%3A%20Roser%20Batlle-Roca%20and%20Wei-Hisang%20Liao%20and%20Xavier%20Serra%20and%20Yuki%20Mitsufuji%20and%20Emilia%20G%C3%B3mez%0AAbstract%3A%20%20%20Recent%20advancements%20in%20music%20generation%20are%20raising%20multiple%20concerns%20about%0Athe%20implications%20of%20AI%20in%20creative%20music%20processes%2C%20current%20business%20models%20and%0Aimpacts%20related%20to%20intellectual%20property%20management.%20A%20relevant%20discussion%20and%0Arelated%20technical%20challenge%20is%20the%20potential%20replication%20and%20plagiarism%20of%20the%0Atraining%20set%20in%20AI-generated%20music%2C%20which%20could%20lead%20to%20misuse%20of%20data%20and%0Aintellectual%20property%20rights%20violations.%20To%20tackle%20this%20issue%2C%20we%20present%20the%0AMusic%20Replication%20Assessment%20%28MiRA%29%20tool%3A%20a%20model-independent%20open%20evaluation%0Amethod%20based%20on%20diverse%20audio%20music%20similarity%20metrics%20to%20assess%20data%0Areplication.%20We%20evaluate%20the%20ability%20of%20five%20metrics%20to%20identify%20exact%0Areplication%20by%20conducting%20a%20controlled%20replication%20experiment%20in%20different%0Amusic%20genres%20using%20synthetic%20samples.%20Our%20results%20show%20that%20the%20proposed%0Amethodology%20can%20estimate%20exact%20data%20replication%20with%20a%20proportion%20higher%20than%0A10%25.%20By%20introducing%20the%20MiRA%20tool%2C%20we%20intend%20to%20encourage%20the%20open%20evaluation%0Aof%20music-generative%20models%20by%20researchers%2C%20developers%2C%20and%20users%20concerning%0Adata%20replication%2C%20highlighting%20the%20importance%20of%20the%20ethical%2C%20social%2C%20legal%2C%0Aand%20economic%20consequences.%20Code%20and%20examples%20are%20available%20for%20reproducibility%0Apurposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14364v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Assessing%2520Data%2520Replication%2520in%2520Music%2520Generation%2520with%2520Music%250A%2520%2520Similarity%2520Metrics%2520on%2520Raw%2520Audio%26entry.906535625%3DRoser%2520Batlle-Roca%2520and%2520Wei-Hisang%2520Liao%2520and%2520Xavier%2520Serra%2520and%2520Yuki%2520Mitsufuji%2520and%2520Emilia%2520G%25C3%25B3mez%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520music%2520generation%2520are%2520raising%2520multiple%2520concerns%2520about%250Athe%2520implications%2520of%2520AI%2520in%2520creative%2520music%2520processes%252C%2520current%2520business%2520models%2520and%250Aimpacts%2520related%2520to%2520intellectual%2520property%2520management.%2520A%2520relevant%2520discussion%2520and%250Arelated%2520technical%2520challenge%2520is%2520the%2520potential%2520replication%2520and%2520plagiarism%2520of%2520the%250Atraining%2520set%2520in%2520AI-generated%2520music%252C%2520which%2520could%2520lead%2520to%2520misuse%2520of%2520data%2520and%250Aintellectual%2520property%2520rights%2520violations.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520present%2520the%250AMusic%2520Replication%2520Assessment%2520%2528MiRA%2529%2520tool%253A%2520a%2520model-independent%2520open%2520evaluation%250Amethod%2520based%2520on%2520diverse%2520audio%2520music%2520similarity%2520metrics%2520to%2520assess%2520data%250Areplication.%2520We%2520evaluate%2520the%2520ability%2520of%2520five%2520metrics%2520to%2520identify%2520exact%250Areplication%2520by%2520conducting%2520a%2520controlled%2520replication%2520experiment%2520in%2520different%250Amusic%2520genres%2520using%2520synthetic%2520samples.%2520Our%2520results%2520show%2520that%2520the%2520proposed%250Amethodology%2520can%2520estimate%2520exact%2520data%2520replication%2520with%2520a%2520proportion%2520higher%2520than%250A10%2525.%2520By%2520introducing%2520the%2520MiRA%2520tool%252C%2520we%2520intend%2520to%2520encourage%2520the%2520open%2520evaluation%250Aof%2520music-generative%2520models%2520by%2520researchers%252C%2520developers%252C%2520and%2520users%2520concerning%250Adata%2520replication%252C%2520highlighting%2520the%2520importance%2520of%2520the%2520ethical%252C%2520social%252C%2520legal%252C%250Aand%2520economic%2520consequences.%2520Code%2520and%2520examples%2520are%2520available%2520for%2520reproducibility%250Apurposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14364v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Assessing%20Data%20Replication%20in%20Music%20Generation%20with%20Music%0A%20%20Similarity%20Metrics%20on%20Raw%20Audio&entry.906535625=Roser%20Batlle-Roca%20and%20Wei-Hisang%20Liao%20and%20Xavier%20Serra%20and%20Yuki%20Mitsufuji%20and%20Emilia%20G%C3%B3mez&entry.1292438233=%20%20Recent%20advancements%20in%20music%20generation%20are%20raising%20multiple%20concerns%20about%0Athe%20implications%20of%20AI%20in%20creative%20music%20processes%2C%20current%20business%20models%20and%0Aimpacts%20related%20to%20intellectual%20property%20management.%20A%20relevant%20discussion%20and%0Arelated%20technical%20challenge%20is%20the%20potential%20replication%20and%20plagiarism%20of%20the%0Atraining%20set%20in%20AI-generated%20music%2C%20which%20could%20lead%20to%20misuse%20of%20data%20and%0Aintellectual%20property%20rights%20violations.%20To%20tackle%20this%20issue%2C%20we%20present%20the%0AMusic%20Replication%20Assessment%20%28MiRA%29%20tool%3A%20a%20model-independent%20open%20evaluation%0Amethod%20based%20on%20diverse%20audio%20music%20similarity%20metrics%20to%20assess%20data%0Areplication.%20We%20evaluate%20the%20ability%20of%20five%20metrics%20to%20identify%20exact%0Areplication%20by%20conducting%20a%20controlled%20replication%20experiment%20in%20different%0Amusic%20genres%20using%20synthetic%20samples.%20Our%20results%20show%20that%20the%20proposed%0Amethodology%20can%20estimate%20exact%20data%20replication%20with%20a%20proportion%20higher%20than%0A10%25.%20By%20introducing%20the%20MiRA%20tool%2C%20we%20intend%20to%20encourage%20the%20open%20evaluation%0Aof%20music-generative%20models%20by%20researchers%2C%20developers%2C%20and%20users%20concerning%0Adata%20replication%2C%20highlighting%20the%20importance%20of%20the%20ethical%2C%20social%2C%20legal%2C%0Aand%20economic%20consequences.%20Code%20and%20examples%20are%20available%20for%20reproducibility%0Apurposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14364v2&entry.124074799=Read"},
{"title": "ExpertAF: Expert Actionable Feedback from Video", "author": "Kumar Ashutosh and Tushar Nagarajan and Georgios Pavlakos and Kris Kitani and Kristen Grauman", "abstract": "  Feedback is essential for learning a new skill or improving one's current\nskill-level. However, current methods for skill-assessment from video only\nprovide scores or compare demonstrations, leaving the burden of knowing what to\ndo differently on the user. We introduce a novel method to generate actionable\nfeedback from video of a person doing a physical activity, such as basketball\nor soccer. Our method takes a video demonstration and its accompanying 3D body\npose and generates (1) free-form expert commentary describing what the person\nis doing well and what they could improve, and (2) a visual expert\ndemonstration that incorporates the required corrections. We show how to\nleverage Ego-Exo4D's videos of skilled activity and expert commentary together\nwith a strong language model to create a weakly-supervised training dataset for\nthis task, and we devise a multimodal video-language model to infer coaching\nfeedback. Our method is able to reason across multi-modal input combinations to\noutput full-spectrum, actionable coaching -- expert commentary, expert video\nretrieval, and the first-of-its-kind expert pose generation -- outperforming\nstrong vision-language models on both established metrics and human preference\nstudies.\n", "link": "http://arxiv.org/abs/2408.00672v1", "date": "2024-08-01", "relevancy": 1.098, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5662}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExpertAF%3A%20Expert%20Actionable%20Feedback%20from%20Video&body=Title%3A%20ExpertAF%3A%20Expert%20Actionable%20Feedback%20from%20Video%0AAuthor%3A%20Kumar%20Ashutosh%20and%20Tushar%20Nagarajan%20and%20Georgios%20Pavlakos%20and%20Kris%20Kitani%20and%20Kristen%20Grauman%0AAbstract%3A%20%20%20Feedback%20is%20essential%20for%20learning%20a%20new%20skill%20or%20improving%20one%27s%20current%0Askill-level.%20However%2C%20current%20methods%20for%20skill-assessment%20from%20video%20only%0Aprovide%20scores%20or%20compare%20demonstrations%2C%20leaving%20the%20burden%20of%20knowing%20what%20to%0Ado%20differently%20on%20the%20user.%20We%20introduce%20a%20novel%20method%20to%20generate%20actionable%0Afeedback%20from%20video%20of%20a%20person%20doing%20a%20physical%20activity%2C%20such%20as%20basketball%0Aor%20soccer.%20Our%20method%20takes%20a%20video%20demonstration%20and%20its%20accompanying%203D%20body%0Apose%20and%20generates%20%281%29%20free-form%20expert%20commentary%20describing%20what%20the%20person%0Ais%20doing%20well%20and%20what%20they%20could%20improve%2C%20and%20%282%29%20a%20visual%20expert%0Ademonstration%20that%20incorporates%20the%20required%20corrections.%20We%20show%20how%20to%0Aleverage%20Ego-Exo4D%27s%20videos%20of%20skilled%20activity%20and%20expert%20commentary%20together%0Awith%20a%20strong%20language%20model%20to%20create%20a%20weakly-supervised%20training%20dataset%20for%0Athis%20task%2C%20and%20we%20devise%20a%20multimodal%20video-language%20model%20to%20infer%20coaching%0Afeedback.%20Our%20method%20is%20able%20to%20reason%20across%20multi-modal%20input%20combinations%20to%0Aoutput%20full-spectrum%2C%20actionable%20coaching%20--%20expert%20commentary%2C%20expert%20video%0Aretrieval%2C%20and%20the%20first-of-its-kind%20expert%20pose%20generation%20--%20outperforming%0Astrong%20vision-language%20models%20on%20both%20established%20metrics%20and%20human%20preference%0Astudies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpertAF%253A%2520Expert%2520Actionable%2520Feedback%2520from%2520Video%26entry.906535625%3DKumar%2520Ashutosh%2520and%2520Tushar%2520Nagarajan%2520and%2520Georgios%2520Pavlakos%2520and%2520Kris%2520Kitani%2520and%2520Kristen%2520Grauman%26entry.1292438233%3D%2520%2520Feedback%2520is%2520essential%2520for%2520learning%2520a%2520new%2520skill%2520or%2520improving%2520one%2527s%2520current%250Askill-level.%2520However%252C%2520current%2520methods%2520for%2520skill-assessment%2520from%2520video%2520only%250Aprovide%2520scores%2520or%2520compare%2520demonstrations%252C%2520leaving%2520the%2520burden%2520of%2520knowing%2520what%2520to%250Ado%2520differently%2520on%2520the%2520user.%2520We%2520introduce%2520a%2520novel%2520method%2520to%2520generate%2520actionable%250Afeedback%2520from%2520video%2520of%2520a%2520person%2520doing%2520a%2520physical%2520activity%252C%2520such%2520as%2520basketball%250Aor%2520soccer.%2520Our%2520method%2520takes%2520a%2520video%2520demonstration%2520and%2520its%2520accompanying%25203D%2520body%250Apose%2520and%2520generates%2520%25281%2529%2520free-form%2520expert%2520commentary%2520describing%2520what%2520the%2520person%250Ais%2520doing%2520well%2520and%2520what%2520they%2520could%2520improve%252C%2520and%2520%25282%2529%2520a%2520visual%2520expert%250Ademonstration%2520that%2520incorporates%2520the%2520required%2520corrections.%2520We%2520show%2520how%2520to%250Aleverage%2520Ego-Exo4D%2527s%2520videos%2520of%2520skilled%2520activity%2520and%2520expert%2520commentary%2520together%250Awith%2520a%2520strong%2520language%2520model%2520to%2520create%2520a%2520weakly-supervised%2520training%2520dataset%2520for%250Athis%2520task%252C%2520and%2520we%2520devise%2520a%2520multimodal%2520video-language%2520model%2520to%2520infer%2520coaching%250Afeedback.%2520Our%2520method%2520is%2520able%2520to%2520reason%2520across%2520multi-modal%2520input%2520combinations%2520to%250Aoutput%2520full-spectrum%252C%2520actionable%2520coaching%2520--%2520expert%2520commentary%252C%2520expert%2520video%250Aretrieval%252C%2520and%2520the%2520first-of-its-kind%2520expert%2520pose%2520generation%2520--%2520outperforming%250Astrong%2520vision-language%2520models%2520on%2520both%2520established%2520metrics%2520and%2520human%2520preference%250Astudies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpertAF%3A%20Expert%20Actionable%20Feedback%20from%20Video&entry.906535625=Kumar%20Ashutosh%20and%20Tushar%20Nagarajan%20and%20Georgios%20Pavlakos%20and%20Kris%20Kitani%20and%20Kristen%20Grauman&entry.1292438233=%20%20Feedback%20is%20essential%20for%20learning%20a%20new%20skill%20or%20improving%20one%27s%20current%0Askill-level.%20However%2C%20current%20methods%20for%20skill-assessment%20from%20video%20only%0Aprovide%20scores%20or%20compare%20demonstrations%2C%20leaving%20the%20burden%20of%20knowing%20what%20to%0Ado%20differently%20on%20the%20user.%20We%20introduce%20a%20novel%20method%20to%20generate%20actionable%0Afeedback%20from%20video%20of%20a%20person%20doing%20a%20physical%20activity%2C%20such%20as%20basketball%0Aor%20soccer.%20Our%20method%20takes%20a%20video%20demonstration%20and%20its%20accompanying%203D%20body%0Apose%20and%20generates%20%281%29%20free-form%20expert%20commentary%20describing%20what%20the%20person%0Ais%20doing%20well%20and%20what%20they%20could%20improve%2C%20and%20%282%29%20a%20visual%20expert%0Ademonstration%20that%20incorporates%20the%20required%20corrections.%20We%20show%20how%20to%0Aleverage%20Ego-Exo4D%27s%20videos%20of%20skilled%20activity%20and%20expert%20commentary%20together%0Awith%20a%20strong%20language%20model%20to%20create%20a%20weakly-supervised%20training%20dataset%20for%0Athis%20task%2C%20and%20we%20devise%20a%20multimodal%20video-language%20model%20to%20infer%20coaching%0Afeedback.%20Our%20method%20is%20able%20to%20reason%20across%20multi-modal%20input%20combinations%20to%0Aoutput%20full-spectrum%2C%20actionable%20coaching%20--%20expert%20commentary%2C%20expert%20video%0Aretrieval%2C%20and%20the%20first-of-its-kind%20expert%20pose%20generation%20--%20outperforming%0Astrong%20vision-language%20models%20on%20both%20established%20metrics%20and%20human%20preference%0Astudies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00672v1&entry.124074799=Read"},
{"title": "Identifying the Hierarchical Emotional Areas in the Human Brain Through\n  Information Fusion", "author": "Zhongyu Huang and Changde Du and Chaozhuo Li and Kaicheng Fu and Huiguang He", "abstract": "  The brain basis of emotion has consistently received widespread attention,\nattracting a large number of studies to explore this cutting-edge topic.\nHowever, the methods employed in these studies typically only model the\npairwise relationship between two brain regions, while neglecting the\ninteractions and information fusion among multiple brain\nregions$\\unicode{x2014}$one of the key ideas of the psychological\nconstructionist hypothesis. To overcome the limitations of traditional methods,\nthis study provides an in-depth theoretical analysis of how to maximize\ninteractions and information fusion among brain regions. Building on the\nresults of this analysis, we propose to identify the hierarchical emotional\nareas in the human brain through multi-source information fusion and graph\nmachine learning methods. Comprehensive experiments reveal that the identified\nhierarchical emotional areas, from lower to higher levels, primarily facilitate\nthe fundamental process of emotion perception, the construction of basic\npsychological operations, and the coordination and integration of these\noperations. Overall, our findings provide unique insights into the brain\nmechanisms underlying specific emotions based on the psychological\nconstructionist hypothesis.\n", "link": "http://arxiv.org/abs/2408.00525v1", "date": "2024-08-01", "relevancy": 1.7307, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4698}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4332}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20the%20Hierarchical%20Emotional%20Areas%20in%20the%20Human%20Brain%20Through%0A%20%20Information%20Fusion&body=Title%3A%20Identifying%20the%20Hierarchical%20Emotional%20Areas%20in%20the%20Human%20Brain%20Through%0A%20%20Information%20Fusion%0AAuthor%3A%20Zhongyu%20Huang%20and%20Changde%20Du%20and%20Chaozhuo%20Li%20and%20Kaicheng%20Fu%20and%20Huiguang%20He%0AAbstract%3A%20%20%20The%20brain%20basis%20of%20emotion%20has%20consistently%20received%20widespread%20attention%2C%0Aattracting%20a%20large%20number%20of%20studies%20to%20explore%20this%20cutting-edge%20topic.%0AHowever%2C%20the%20methods%20employed%20in%20these%20studies%20typically%20only%20model%20the%0Apairwise%20relationship%20between%20two%20brain%20regions%2C%20while%20neglecting%20the%0Ainteractions%20and%20information%20fusion%20among%20multiple%20brain%0Aregions%24%5Cunicode%7Bx2014%7D%24one%20of%20the%20key%20ideas%20of%20the%20psychological%0Aconstructionist%20hypothesis.%20To%20overcome%20the%20limitations%20of%20traditional%20methods%2C%0Athis%20study%20provides%20an%20in-depth%20theoretical%20analysis%20of%20how%20to%20maximize%0Ainteractions%20and%20information%20fusion%20among%20brain%20regions.%20Building%20on%20the%0Aresults%20of%20this%20analysis%2C%20we%20propose%20to%20identify%20the%20hierarchical%20emotional%0Aareas%20in%20the%20human%20brain%20through%20multi-source%20information%20fusion%20and%20graph%0Amachine%20learning%20methods.%20Comprehensive%20experiments%20reveal%20that%20the%20identified%0Ahierarchical%20emotional%20areas%2C%20from%20lower%20to%20higher%20levels%2C%20primarily%20facilitate%0Athe%20fundamental%20process%20of%20emotion%20perception%2C%20the%20construction%20of%20basic%0Apsychological%20operations%2C%20and%20the%20coordination%20and%20integration%20of%20these%0Aoperations.%20Overall%2C%20our%20findings%20provide%20unique%20insights%20into%20the%20brain%0Amechanisms%20underlying%20specific%20emotions%20based%20on%20the%20psychological%0Aconstructionist%20hypothesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520the%2520Hierarchical%2520Emotional%2520Areas%2520in%2520the%2520Human%2520Brain%2520Through%250A%2520%2520Information%2520Fusion%26entry.906535625%3DZhongyu%2520Huang%2520and%2520Changde%2520Du%2520and%2520Chaozhuo%2520Li%2520and%2520Kaicheng%2520Fu%2520and%2520Huiguang%2520He%26entry.1292438233%3D%2520%2520The%2520brain%2520basis%2520of%2520emotion%2520has%2520consistently%2520received%2520widespread%2520attention%252C%250Aattracting%2520a%2520large%2520number%2520of%2520studies%2520to%2520explore%2520this%2520cutting-edge%2520topic.%250AHowever%252C%2520the%2520methods%2520employed%2520in%2520these%2520studies%2520typically%2520only%2520model%2520the%250Apairwise%2520relationship%2520between%2520two%2520brain%2520regions%252C%2520while%2520neglecting%2520the%250Ainteractions%2520and%2520information%2520fusion%2520among%2520multiple%2520brain%250Aregions%2524%255Cunicode%257Bx2014%257D%2524one%2520of%2520the%2520key%2520ideas%2520of%2520the%2520psychological%250Aconstructionist%2520hypothesis.%2520To%2520overcome%2520the%2520limitations%2520of%2520traditional%2520methods%252C%250Athis%2520study%2520provides%2520an%2520in-depth%2520theoretical%2520analysis%2520of%2520how%2520to%2520maximize%250Ainteractions%2520and%2520information%2520fusion%2520among%2520brain%2520regions.%2520Building%2520on%2520the%250Aresults%2520of%2520this%2520analysis%252C%2520we%2520propose%2520to%2520identify%2520the%2520hierarchical%2520emotional%250Aareas%2520in%2520the%2520human%2520brain%2520through%2520multi-source%2520information%2520fusion%2520and%2520graph%250Amachine%2520learning%2520methods.%2520Comprehensive%2520experiments%2520reveal%2520that%2520the%2520identified%250Ahierarchical%2520emotional%2520areas%252C%2520from%2520lower%2520to%2520higher%2520levels%252C%2520primarily%2520facilitate%250Athe%2520fundamental%2520process%2520of%2520emotion%2520perception%252C%2520the%2520construction%2520of%2520basic%250Apsychological%2520operations%252C%2520and%2520the%2520coordination%2520and%2520integration%2520of%2520these%250Aoperations.%2520Overall%252C%2520our%2520findings%2520provide%2520unique%2520insights%2520into%2520the%2520brain%250Amechanisms%2520underlying%2520specific%2520emotions%2520based%2520on%2520the%2520psychological%250Aconstructionist%2520hypothesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20the%20Hierarchical%20Emotional%20Areas%20in%20the%20Human%20Brain%20Through%0A%20%20Information%20Fusion&entry.906535625=Zhongyu%20Huang%20and%20Changde%20Du%20and%20Chaozhuo%20Li%20and%20Kaicheng%20Fu%20and%20Huiguang%20He&entry.1292438233=%20%20The%20brain%20basis%20of%20emotion%20has%20consistently%20received%20widespread%20attention%2C%0Aattracting%20a%20large%20number%20of%20studies%20to%20explore%20this%20cutting-edge%20topic.%0AHowever%2C%20the%20methods%20employed%20in%20these%20studies%20typically%20only%20model%20the%0Apairwise%20relationship%20between%20two%20brain%20regions%2C%20while%20neglecting%20the%0Ainteractions%20and%20information%20fusion%20among%20multiple%20brain%0Aregions%24%5Cunicode%7Bx2014%7D%24one%20of%20the%20key%20ideas%20of%20the%20psychological%0Aconstructionist%20hypothesis.%20To%20overcome%20the%20limitations%20of%20traditional%20methods%2C%0Athis%20study%20provides%20an%20in-depth%20theoretical%20analysis%20of%20how%20to%20maximize%0Ainteractions%20and%20information%20fusion%20among%20brain%20regions.%20Building%20on%20the%0Aresults%20of%20this%20analysis%2C%20we%20propose%20to%20identify%20the%20hierarchical%20emotional%0Aareas%20in%20the%20human%20brain%20through%20multi-source%20information%20fusion%20and%20graph%0Amachine%20learning%20methods.%20Comprehensive%20experiments%20reveal%20that%20the%20identified%0Ahierarchical%20emotional%20areas%2C%20from%20lower%20to%20higher%20levels%2C%20primarily%20facilitate%0Athe%20fundamental%20process%20of%20emotion%20perception%2C%20the%20construction%20of%20basic%0Apsychological%20operations%2C%20and%20the%20coordination%20and%20integration%20of%20these%0Aoperations.%20Overall%2C%20our%20findings%20provide%20unique%20insights%20into%20the%20brain%0Amechanisms%20underlying%20specific%20emotions%20based%20on%20the%20psychological%0Aconstructionist%20hypothesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00525v1&entry.124074799=Read"},
{"title": "Application of Transformers for Nonlinear Channel Compensation in\n  Optical Systems", "author": "Behnam Behinaein Hamgini and Hossein Najafi and Ali Bakhshali and Zhuhong Zhang", "abstract": "  In this paper, we introduce a new nonlinear optical channel equalizer based\non Transformers. By leveraging parallel computation and attending directly to\nthe memory across a sequence of symbols, we show that Transformers can be used\neffectively for nonlinear compensation (NLC) in coherent long-haul transmission\nsystems. For this application, we present an implementation of the encoder part\nof the Transformer and analyze its performance over a wide range of different\nhyper-parameters. It is shown that by proper embeddings and processing blocks\nof symbols at each iteration and also carefully selecting subsets of the\nencoder's output to be processed together, an efficient nonlinear equalization\ncan be achieved for different complexity constraints. To reduce the\ncomputational complexity of the attention mechanism, we further propose the use\nof a physic-informed mask inspired by nonlinear perturbation theory. We also\ncompare the Transformer-NLC with digital back-propagation (DBP) under different\ntransmission scenarios in order to demonstrate the flexibility and\ngeneralizability of the proposed data-driven solution.\n", "link": "http://arxiv.org/abs/2304.13119v3", "date": "2024-08-01", "relevancy": 1.7872, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5171}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4534}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Transformers%20for%20Nonlinear%20Channel%20Compensation%20in%0A%20%20Optical%20Systems&body=Title%3A%20Application%20of%20Transformers%20for%20Nonlinear%20Channel%20Compensation%20in%0A%20%20Optical%20Systems%0AAuthor%3A%20Behnam%20Behinaein%20Hamgini%20and%20Hossein%20Najafi%20and%20Ali%20Bakhshali%20and%20Zhuhong%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20nonlinear%20optical%20channel%20equalizer%20based%0Aon%20Transformers.%20By%20leveraging%20parallel%20computation%20and%20attending%20directly%20to%0Athe%20memory%20across%20a%20sequence%20of%20symbols%2C%20we%20show%20that%20Transformers%20can%20be%20used%0Aeffectively%20for%20nonlinear%20compensation%20%28NLC%29%20in%20coherent%20long-haul%20transmission%0Asystems.%20For%20this%20application%2C%20we%20present%20an%20implementation%20of%20the%20encoder%20part%0Aof%20the%20Transformer%20and%20analyze%20its%20performance%20over%20a%20wide%20range%20of%20different%0Ahyper-parameters.%20It%20is%20shown%20that%20by%20proper%20embeddings%20and%20processing%20blocks%0Aof%20symbols%20at%20each%20iteration%20and%20also%20carefully%20selecting%20subsets%20of%20the%0Aencoder%27s%20output%20to%20be%20processed%20together%2C%20an%20efficient%20nonlinear%20equalization%0Acan%20be%20achieved%20for%20different%20complexity%20constraints.%20To%20reduce%20the%0Acomputational%20complexity%20of%20the%20attention%20mechanism%2C%20we%20further%20propose%20the%20use%0Aof%20a%20physic-informed%20mask%20inspired%20by%20nonlinear%20perturbation%20theory.%20We%20also%0Acompare%20the%20Transformer-NLC%20with%20digital%20back-propagation%20%28DBP%29%20under%20different%0Atransmission%20scenarios%20in%20order%20to%20demonstrate%20the%20flexibility%20and%0Ageneralizability%20of%20the%20proposed%20data-driven%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.13119v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Transformers%2520for%2520Nonlinear%2520Channel%2520Compensation%2520in%250A%2520%2520Optical%2520Systems%26entry.906535625%3DBehnam%2520Behinaein%2520Hamgini%2520and%2520Hossein%2520Najafi%2520and%2520Ali%2520Bakhshali%2520and%2520Zhuhong%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520nonlinear%2520optical%2520channel%2520equalizer%2520based%250Aon%2520Transformers.%2520By%2520leveraging%2520parallel%2520computation%2520and%2520attending%2520directly%2520to%250Athe%2520memory%2520across%2520a%2520sequence%2520of%2520symbols%252C%2520we%2520show%2520that%2520Transformers%2520can%2520be%2520used%250Aeffectively%2520for%2520nonlinear%2520compensation%2520%2528NLC%2529%2520in%2520coherent%2520long-haul%2520transmission%250Asystems.%2520For%2520this%2520application%252C%2520we%2520present%2520an%2520implementation%2520of%2520the%2520encoder%2520part%250Aof%2520the%2520Transformer%2520and%2520analyze%2520its%2520performance%2520over%2520a%2520wide%2520range%2520of%2520different%250Ahyper-parameters.%2520It%2520is%2520shown%2520that%2520by%2520proper%2520embeddings%2520and%2520processing%2520blocks%250Aof%2520symbols%2520at%2520each%2520iteration%2520and%2520also%2520carefully%2520selecting%2520subsets%2520of%2520the%250Aencoder%2527s%2520output%2520to%2520be%2520processed%2520together%252C%2520an%2520efficient%2520nonlinear%2520equalization%250Acan%2520be%2520achieved%2520for%2520different%2520complexity%2520constraints.%2520To%2520reduce%2520the%250Acomputational%2520complexity%2520of%2520the%2520attention%2520mechanism%252C%2520we%2520further%2520propose%2520the%2520use%250Aof%2520a%2520physic-informed%2520mask%2520inspired%2520by%2520nonlinear%2520perturbation%2520theory.%2520We%2520also%250Acompare%2520the%2520Transformer-NLC%2520with%2520digital%2520back-propagation%2520%2528DBP%2529%2520under%2520different%250Atransmission%2520scenarios%2520in%2520order%2520to%2520demonstrate%2520the%2520flexibility%2520and%250Ageneralizability%2520of%2520the%2520proposed%2520data-driven%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.13119v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Transformers%20for%20Nonlinear%20Channel%20Compensation%20in%0A%20%20Optical%20Systems&entry.906535625=Behnam%20Behinaein%20Hamgini%20and%20Hossein%20Najafi%20and%20Ali%20Bakhshali%20and%20Zhuhong%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20new%20nonlinear%20optical%20channel%20equalizer%20based%0Aon%20Transformers.%20By%20leveraging%20parallel%20computation%20and%20attending%20directly%20to%0Athe%20memory%20across%20a%20sequence%20of%20symbols%2C%20we%20show%20that%20Transformers%20can%20be%20used%0Aeffectively%20for%20nonlinear%20compensation%20%28NLC%29%20in%20coherent%20long-haul%20transmission%0Asystems.%20For%20this%20application%2C%20we%20present%20an%20implementation%20of%20the%20encoder%20part%0Aof%20the%20Transformer%20and%20analyze%20its%20performance%20over%20a%20wide%20range%20of%20different%0Ahyper-parameters.%20It%20is%20shown%20that%20by%20proper%20embeddings%20and%20processing%20blocks%0Aof%20symbols%20at%20each%20iteration%20and%20also%20carefully%20selecting%20subsets%20of%20the%0Aencoder%27s%20output%20to%20be%20processed%20together%2C%20an%20efficient%20nonlinear%20equalization%0Acan%20be%20achieved%20for%20different%20complexity%20constraints.%20To%20reduce%20the%0Acomputational%20complexity%20of%20the%20attention%20mechanism%2C%20we%20further%20propose%20the%20use%0Aof%20a%20physic-informed%20mask%20inspired%20by%20nonlinear%20perturbation%20theory.%20We%20also%0Acompare%20the%20Transformer-NLC%20with%20digital%20back-propagation%20%28DBP%29%20under%20different%0Atransmission%20scenarios%20in%20order%20to%20demonstrate%20the%20flexibility%20and%0Ageneralizability%20of%20the%20proposed%20data-driven%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.13119v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


