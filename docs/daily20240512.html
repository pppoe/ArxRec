<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240509.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap", "author": "Mingrui Li and Jingwei Huang and Lei Sun and Aaron Xuxiang Tian and Tianchen Deng and Hongyu Wang", "abstract": "  Gaussian Splatting has garnered widespread attention due to its exceptional\nperformance. Consequently, SLAM systems based on Gaussian Splatting have\nemerged, leveraging its capabilities for rapid real-time rendering and\nhigh-fidelity mapping. However, current Gaussian Splatting SLAM systems usually\nstruggle with large scene representation and lack effective loop closure\nadjustments and scene generalization capabilities. To address these issues, we\nintroduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance\nfield submaps for progressive scene expression, effectively integrating the\nstrengths of neural radiance fields and 3D Gaussian Splatting. We have\ndeveloped neural implicit submaps as supervision and achieve high-quality scene\nexpression and online loop closure adjustments through Gaussian rendering of\nfused submaps. Our results on multiple real-world scenes and large-scale scene\ndatasets demonstrate that our method can achieve accurate gap filling and\nhigh-quality scene expression, supporting both monocular, stereo, and RGB-D\ninputs, and achieving state-of-the-art scene reconstruction and tracking\nperformance.\n", "link": "http://arxiv.org/abs/2405.05702v1", "date": "2024-05-09", "relevancy": 3.3885, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8062}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6431}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap&body=Title%3A%20NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap%0AAuthor%3A%20Mingrui%20Li%20and%20Jingwei%20Huang%20and%20Lei%20Sun%20and%20Aaron%20Xuxiang%20Tian%20and%20Tianchen%20Deng%20and%20Hongyu%20Wang%0AAbstract%3A%20%20%20Gaussian%20Splatting%20has%20garnered%20widespread%20attention%20due%20to%20its%20exceptional%0Aperformance.%20Consequently%2C%20SLAM%20systems%20based%20on%20Gaussian%20Splatting%20have%0Aemerged%2C%20leveraging%20its%20capabilities%20for%20rapid%20real-time%20rendering%20and%0Ahigh-fidelity%20mapping.%20However%2C%20current%20Gaussian%20Splatting%20SLAM%20systems%20usually%0Astruggle%20with%20large%20scene%20representation%20and%20lack%20effective%20loop%20closure%0Aadjustments%20and%20scene%20generalization%20capabilities.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20NGM-SLAM%2C%20the%20first%20GS-SLAM%20system%20that%20utilizes%20neural%20radiance%0Afield%20submaps%20for%20progressive%20scene%20expression%2C%20effectively%20integrating%20the%0Astrengths%20of%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting.%20We%20have%0Adeveloped%20neural%20implicit%20submaps%20as%20supervision%20and%20achieve%20high-quality%20scene%0Aexpression%20and%20online%20loop%20closure%20adjustments%20through%20Gaussian%20rendering%20of%0Afused%20submaps.%20Our%20results%20on%20multiple%20real-world%20scenes%20and%20large-scale%20scene%0Adatasets%20demonstrate%20that%20our%20method%20can%20achieve%20accurate%20gap%20filling%20and%0Ahigh-quality%20scene%20expression%2C%20supporting%20both%20monocular%2C%20stereo%2C%20and%20RGB-D%0Ainputs%2C%20and%20achieving%20state-of-the-art%20scene%20reconstruction%20and%20tracking%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNGM-SLAM%253A%2520Gaussian%2520Splatting%2520SLAM%2520with%2520Radiance%2520Field%2520Submap%26entry.906535625%3DMingrui%2520Li%2520and%2520Jingwei%2520Huang%2520and%2520Lei%2520Sun%2520and%2520Aaron%2520Xuxiang%2520Tian%2520and%2520Tianchen%2520Deng%2520and%2520Hongyu%2520Wang%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520has%2520garnered%2520widespread%2520attention%2520due%2520to%2520its%2520exceptional%250Aperformance.%2520Consequently%252C%2520SLAM%2520systems%2520based%2520on%2520Gaussian%2520Splatting%2520have%250Aemerged%252C%2520leveraging%2520its%2520capabilities%2520for%2520rapid%2520real-time%2520rendering%2520and%250Ahigh-fidelity%2520mapping.%2520However%252C%2520current%2520Gaussian%2520Splatting%2520SLAM%2520systems%2520usually%250Astruggle%2520with%2520large%2520scene%2520representation%2520and%2520lack%2520effective%2520loop%2520closure%250Aadjustments%2520and%2520scene%2520generalization%2520capabilities.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520NGM-SLAM%252C%2520the%2520first%2520GS-SLAM%2520system%2520that%2520utilizes%2520neural%2520radiance%250Afield%2520submaps%2520for%2520progressive%2520scene%2520expression%252C%2520effectively%2520integrating%2520the%250Astrengths%2520of%2520neural%2520radiance%2520fields%2520and%25203D%2520Gaussian%2520Splatting.%2520We%2520have%250Adeveloped%2520neural%2520implicit%2520submaps%2520as%2520supervision%2520and%2520achieve%2520high-quality%2520scene%250Aexpression%2520and%2520online%2520loop%2520closure%2520adjustments%2520through%2520Gaussian%2520rendering%2520of%250Afused%2520submaps.%2520Our%2520results%2520on%2520multiple%2520real-world%2520scenes%2520and%2520large-scale%2520scene%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520can%2520achieve%2520accurate%2520gap%2520filling%2520and%250Ahigh-quality%2520scene%2520expression%252C%2520supporting%2520both%2520monocular%252C%2520stereo%252C%2520and%2520RGB-D%250Ainputs%252C%2520and%2520achieving%2520state-of-the-art%2520scene%2520reconstruction%2520and%2520tracking%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NGM-SLAM%3A%20Gaussian%20Splatting%20SLAM%20with%20Radiance%20Field%20Submap&entry.906535625=Mingrui%20Li%20and%20Jingwei%20Huang%20and%20Lei%20Sun%20and%20Aaron%20Xuxiang%20Tian%20and%20Tianchen%20Deng%20and%20Hongyu%20Wang&entry.1292438233=%20%20Gaussian%20Splatting%20has%20garnered%20widespread%20attention%20due%20to%20its%20exceptional%0Aperformance.%20Consequently%2C%20SLAM%20systems%20based%20on%20Gaussian%20Splatting%20have%0Aemerged%2C%20leveraging%20its%20capabilities%20for%20rapid%20real-time%20rendering%20and%0Ahigh-fidelity%20mapping.%20However%2C%20current%20Gaussian%20Splatting%20SLAM%20systems%20usually%0Astruggle%20with%20large%20scene%20representation%20and%20lack%20effective%20loop%20closure%0Aadjustments%20and%20scene%20generalization%20capabilities.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20NGM-SLAM%2C%20the%20first%20GS-SLAM%20system%20that%20utilizes%20neural%20radiance%0Afield%20submaps%20for%20progressive%20scene%20expression%2C%20effectively%20integrating%20the%0Astrengths%20of%20neural%20radiance%20fields%20and%203D%20Gaussian%20Splatting.%20We%20have%0Adeveloped%20neural%20implicit%20submaps%20as%20supervision%20and%20achieve%20high-quality%20scene%0Aexpression%20and%20online%20loop%20closure%20adjustments%20through%20Gaussian%20rendering%20of%0Afused%20submaps.%20Our%20results%20on%20multiple%20real-world%20scenes%20and%20large-scale%20scene%0Adatasets%20demonstrate%20that%20our%20method%20can%20achieve%20accurate%20gap%20filling%20and%0Ahigh-quality%20scene%20expression%2C%20supporting%20both%20monocular%2C%20stereo%2C%20and%20RGB-D%0Ainputs%2C%20and%20achieving%20state-of-the-art%20scene%20reconstruction%20and%20tracking%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05702v1&entry.124074799=Read"},
{"title": "NeRFFaceSpeech: One-shot Audio-diven 3D Talking Head Synthesis via\n  Generative Prior", "author": "Gihoon Kim and Kwanggyoon Seo and Sihun Cha and Junyong Noh", "abstract": "  Audio-driven talking head generation is advancing from 2D to 3D content.\nNotably, Neural Radiance Field (NeRF) is in the spotlight as a means to\nsynthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based\napproach typically requires a large number of paired audio-visual data for each\nidentity, thereby limiting the scalability of the method. Although there have\nbeen attempts to generate audio-driven 3D talking head animations with a single\nimage, the results are often unsatisfactory due to insufficient information on\nobscured regions in the image. In this paper, we mainly focus on addressing the\noverlooked aspect of 3D consistency in the one-shot, audio-driven domain, where\nfacial animations are synthesized primarily in front-facing perspectives. We\npropose a novel method, NeRFFaceSpeech, which enables to produce high-quality\n3D-aware talking head. Using prior knowledge of generative models combined with\nNeRF, our method can craft a 3D-consistent facial feature space corresponding\nto a single image. Our spatial synchronization method employs audio-correlated\nvertex dynamics of a parametric face model to transform static image features\ninto dynamic visuals through ray deformation, ensuring realistic 3D facial\nmotion. Moreover, we introduce LipaintNet that can replenish the lacking\ninformation in the inner-mouth area, which can not be obtained from a given\nsingle image. The network is trained in a self-supervised manner by utilizing\nthe generative capabilities without additional data. The comprehensive\nexperiments demonstrate the superiority of our method in generating\naudio-driven talking heads from a single image with enhanced 3D consistency\ncompared to previous approaches. In addition, we introduce a quantitative way\nof measuring the robustness of a model against pose changes for the first time,\nwhich has been possible only qualitatively.\n", "link": "http://arxiv.org/abs/2405.05749v1", "date": "2024-05-09", "relevancy": 3.0131, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6148}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6148}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRFFaceSpeech%3A%20One-shot%20Audio-diven%203D%20Talking%20Head%20Synthesis%20via%0A%20%20Generative%20Prior&body=Title%3A%20NeRFFaceSpeech%3A%20One-shot%20Audio-diven%203D%20Talking%20Head%20Synthesis%20via%0A%20%20Generative%20Prior%0AAuthor%3A%20Gihoon%20Kim%20and%20Kwanggyoon%20Seo%20and%20Sihun%20Cha%20and%20Junyong%20Noh%0AAbstract%3A%20%20%20Audio-driven%20talking%20head%20generation%20is%20advancing%20from%202D%20to%203D%20content.%0ANotably%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20is%20in%20the%20spotlight%20as%20a%20means%20to%0Asynthesize%20high-quality%203D%20talking%20head%20outputs.%20Unfortunately%2C%20this%20NeRF-based%0Aapproach%20typically%20requires%20a%20large%20number%20of%20paired%20audio-visual%20data%20for%20each%0Aidentity%2C%20thereby%20limiting%20the%20scalability%20of%20the%20method.%20Although%20there%20have%0Abeen%20attempts%20to%20generate%20audio-driven%203D%20talking%20head%20animations%20with%20a%20single%0Aimage%2C%20the%20results%20are%20often%20unsatisfactory%20due%20to%20insufficient%20information%20on%0Aobscured%20regions%20in%20the%20image.%20In%20this%20paper%2C%20we%20mainly%20focus%20on%20addressing%20the%0Aoverlooked%20aspect%20of%203D%20consistency%20in%20the%20one-shot%2C%20audio-driven%20domain%2C%20where%0Afacial%20animations%20are%20synthesized%20primarily%20in%20front-facing%20perspectives.%20We%0Apropose%20a%20novel%20method%2C%20NeRFFaceSpeech%2C%20which%20enables%20to%20produce%20high-quality%0A3D-aware%20talking%20head.%20Using%20prior%20knowledge%20of%20generative%20models%20combined%20with%0ANeRF%2C%20our%20method%20can%20craft%20a%203D-consistent%20facial%20feature%20space%20corresponding%0Ato%20a%20single%20image.%20Our%20spatial%20synchronization%20method%20employs%20audio-correlated%0Avertex%20dynamics%20of%20a%20parametric%20face%20model%20to%20transform%20static%20image%20features%0Ainto%20dynamic%20visuals%20through%20ray%20deformation%2C%20ensuring%20realistic%203D%20facial%0Amotion.%20Moreover%2C%20we%20introduce%20LipaintNet%20that%20can%20replenish%20the%20lacking%0Ainformation%20in%20the%20inner-mouth%20area%2C%20which%20can%20not%20be%20obtained%20from%20a%20given%0Asingle%20image.%20The%20network%20is%20trained%20in%20a%20self-supervised%20manner%20by%20utilizing%0Athe%20generative%20capabilities%20without%20additional%20data.%20The%20comprehensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20in%20generating%0Aaudio-driven%20talking%20heads%20from%20a%20single%20image%20with%20enhanced%203D%20consistency%0Acompared%20to%20previous%20approaches.%20In%20addition%2C%20we%20introduce%20a%20quantitative%20way%0Aof%20measuring%20the%20robustness%20of%20a%20model%20against%20pose%20changes%20for%20the%20first%20time%2C%0Awhich%20has%20been%20possible%20only%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRFFaceSpeech%253A%2520One-shot%2520Audio-diven%25203D%2520Talking%2520Head%2520Synthesis%2520via%250A%2520%2520Generative%2520Prior%26entry.906535625%3DGihoon%2520Kim%2520and%2520Kwanggyoon%2520Seo%2520and%2520Sihun%2520Cha%2520and%2520Junyong%2520Noh%26entry.1292438233%3D%2520%2520Audio-driven%2520talking%2520head%2520generation%2520is%2520advancing%2520from%25202D%2520to%25203D%2520content.%250ANotably%252C%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520is%2520in%2520the%2520spotlight%2520as%2520a%2520means%2520to%250Asynthesize%2520high-quality%25203D%2520talking%2520head%2520outputs.%2520Unfortunately%252C%2520this%2520NeRF-based%250Aapproach%2520typically%2520requires%2520a%2520large%2520number%2520of%2520paired%2520audio-visual%2520data%2520for%2520each%250Aidentity%252C%2520thereby%2520limiting%2520the%2520scalability%2520of%2520the%2520method.%2520Although%2520there%2520have%250Abeen%2520attempts%2520to%2520generate%2520audio-driven%25203D%2520talking%2520head%2520animations%2520with%2520a%2520single%250Aimage%252C%2520the%2520results%2520are%2520often%2520unsatisfactory%2520due%2520to%2520insufficient%2520information%2520on%250Aobscured%2520regions%2520in%2520the%2520image.%2520In%2520this%2520paper%252C%2520we%2520mainly%2520focus%2520on%2520addressing%2520the%250Aoverlooked%2520aspect%2520of%25203D%2520consistency%2520in%2520the%2520one-shot%252C%2520audio-driven%2520domain%252C%2520where%250Afacial%2520animations%2520are%2520synthesized%2520primarily%2520in%2520front-facing%2520perspectives.%2520We%250Apropose%2520a%2520novel%2520method%252C%2520NeRFFaceSpeech%252C%2520which%2520enables%2520to%2520produce%2520high-quality%250A3D-aware%2520talking%2520head.%2520Using%2520prior%2520knowledge%2520of%2520generative%2520models%2520combined%2520with%250ANeRF%252C%2520our%2520method%2520can%2520craft%2520a%25203D-consistent%2520facial%2520feature%2520space%2520corresponding%250Ato%2520a%2520single%2520image.%2520Our%2520spatial%2520synchronization%2520method%2520employs%2520audio-correlated%250Avertex%2520dynamics%2520of%2520a%2520parametric%2520face%2520model%2520to%2520transform%2520static%2520image%2520features%250Ainto%2520dynamic%2520visuals%2520through%2520ray%2520deformation%252C%2520ensuring%2520realistic%25203D%2520facial%250Amotion.%2520Moreover%252C%2520we%2520introduce%2520LipaintNet%2520that%2520can%2520replenish%2520the%2520lacking%250Ainformation%2520in%2520the%2520inner-mouth%2520area%252C%2520which%2520can%2520not%2520be%2520obtained%2520from%2520a%2520given%250Asingle%2520image.%2520The%2520network%2520is%2520trained%2520in%2520a%2520self-supervised%2520manner%2520by%2520utilizing%250Athe%2520generative%2520capabilities%2520without%2520additional%2520data.%2520The%2520comprehensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520in%2520generating%250Aaudio-driven%2520talking%2520heads%2520from%2520a%2520single%2520image%2520with%2520enhanced%25203D%2520consistency%250Acompared%2520to%2520previous%2520approaches.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520quantitative%2520way%250Aof%2520measuring%2520the%2520robustness%2520of%2520a%2520model%2520against%2520pose%2520changes%2520for%2520the%2520first%2520time%252C%250Awhich%2520has%2520been%2520possible%2520only%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRFFaceSpeech%3A%20One-shot%20Audio-diven%203D%20Talking%20Head%20Synthesis%20via%0A%20%20Generative%20Prior&entry.906535625=Gihoon%20Kim%20and%20Kwanggyoon%20Seo%20and%20Sihun%20Cha%20and%20Junyong%20Noh&entry.1292438233=%20%20Audio-driven%20talking%20head%20generation%20is%20advancing%20from%202D%20to%203D%20content.%0ANotably%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20is%20in%20the%20spotlight%20as%20a%20means%20to%0Asynthesize%20high-quality%203D%20talking%20head%20outputs.%20Unfortunately%2C%20this%20NeRF-based%0Aapproach%20typically%20requires%20a%20large%20number%20of%20paired%20audio-visual%20data%20for%20each%0Aidentity%2C%20thereby%20limiting%20the%20scalability%20of%20the%20method.%20Although%20there%20have%0Abeen%20attempts%20to%20generate%20audio-driven%203D%20talking%20head%20animations%20with%20a%20single%0Aimage%2C%20the%20results%20are%20often%20unsatisfactory%20due%20to%20insufficient%20information%20on%0Aobscured%20regions%20in%20the%20image.%20In%20this%20paper%2C%20we%20mainly%20focus%20on%20addressing%20the%0Aoverlooked%20aspect%20of%203D%20consistency%20in%20the%20one-shot%2C%20audio-driven%20domain%2C%20where%0Afacial%20animations%20are%20synthesized%20primarily%20in%20front-facing%20perspectives.%20We%0Apropose%20a%20novel%20method%2C%20NeRFFaceSpeech%2C%20which%20enables%20to%20produce%20high-quality%0A3D-aware%20talking%20head.%20Using%20prior%20knowledge%20of%20generative%20models%20combined%20with%0ANeRF%2C%20our%20method%20can%20craft%20a%203D-consistent%20facial%20feature%20space%20corresponding%0Ato%20a%20single%20image.%20Our%20spatial%20synchronization%20method%20employs%20audio-correlated%0Avertex%20dynamics%20of%20a%20parametric%20face%20model%20to%20transform%20static%20image%20features%0Ainto%20dynamic%20visuals%20through%20ray%20deformation%2C%20ensuring%20realistic%203D%20facial%0Amotion.%20Moreover%2C%20we%20introduce%20LipaintNet%20that%20can%20replenish%20the%20lacking%0Ainformation%20in%20the%20inner-mouth%20area%2C%20which%20can%20not%20be%20obtained%20from%20a%20given%0Asingle%20image.%20The%20network%20is%20trained%20in%20a%20self-supervised%20manner%20by%20utilizing%0Athe%20generative%20capabilities%20without%20additional%20data.%20The%20comprehensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20in%20generating%0Aaudio-driven%20talking%20heads%20from%20a%20single%20image%20with%20enhanced%203D%20consistency%0Acompared%20to%20previous%20approaches.%20In%20addition%2C%20we%20introduce%20a%20quantitative%20way%0Aof%20measuring%20the%20robustness%20of%20a%20model%20against%20pose%20changes%20for%20the%20first%20time%2C%0Awhich%20has%20been%20possible%20only%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05749v1&entry.124074799=Read"},
{"title": "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian\n  Representation", "author": "Sitian Shen and Jing Xu and Yuheng Yuan and Xingyi Yang and Qiuhong Shen and Xinchao Wang", "abstract": "  User-friendly 3D object editing is a challenging task that has attracted\nsignificant attention recently. The limitations of direct 3D object editing\nwithout 2D prior knowledge have prompted increased attention towards utilizing\n2D generative models for 3D editing. While existing methods like Instruct\nNeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly\ndue to semantic guided editing. In the realm of 3D representation, 3D Gaussian\nSplatting emerges as a promising approach for its efficiency and natural\nexplicit property, facilitating precise editing tasks. Building upon these\ninsights, we propose DragGaussian, a 3D object drag-editing framework based on\n3D Gaussian Splatting, leveraging diffusion models for interactive image\nediting with open-vocabulary input. This framework enables users to perform\ndrag-based editing on pre-trained 3D Gaussian object models, producing modified\n2D images through multi-view consistent editing. Our contributions include the\nintroduction of a new task, the development of DragGaussian for interactive\npoint-based 3D editing, and comprehensive validation of its effectiveness\nthrough qualitative and quantitative experiments.\n", "link": "http://arxiv.org/abs/2405.05800v1", "date": "2024-05-09", "relevancy": 2.9989, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6104}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6059}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DragGaussian%3A%20Enabling%20Drag-style%20Manipulation%20on%203D%20Gaussian%0A%20%20Representation&body=Title%3A%20DragGaussian%3A%20Enabling%20Drag-style%20Manipulation%20on%203D%20Gaussian%0A%20%20Representation%0AAuthor%3A%20Sitian%20Shen%20and%20Jing%20Xu%20and%20Yuheng%20Yuan%20and%20Xingyi%20Yang%20and%20Qiuhong%20Shen%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20User-friendly%203D%20object%20editing%20is%20a%20challenging%20task%20that%20has%20attracted%0Asignificant%20attention%20recently.%20The%20limitations%20of%20direct%203D%20object%20editing%0Awithout%202D%20prior%20knowledge%20have%20prompted%20increased%20attention%20towards%20utilizing%0A2D%20generative%20models%20for%203D%20editing.%20While%20existing%20methods%20like%20Instruct%0ANeRF-to-NeRF%20offer%20a%20solution%2C%20they%20often%20lack%20user-friendliness%2C%20particularly%0Adue%20to%20semantic%20guided%20editing.%20In%20the%20realm%20of%203D%20representation%2C%203D%20Gaussian%0ASplatting%20emerges%20as%20a%20promising%20approach%20for%20its%20efficiency%20and%20natural%0Aexplicit%20property%2C%20facilitating%20precise%20editing%20tasks.%20Building%20upon%20these%0Ainsights%2C%20we%20propose%20DragGaussian%2C%20a%203D%20object%20drag-editing%20framework%20based%20on%0A3D%20Gaussian%20Splatting%2C%20leveraging%20diffusion%20models%20for%20interactive%20image%0Aediting%20with%20open-vocabulary%20input.%20This%20framework%20enables%20users%20to%20perform%0Adrag-based%20editing%20on%20pre-trained%203D%20Gaussian%20object%20models%2C%20producing%20modified%0A2D%20images%20through%20multi-view%20consistent%20editing.%20Our%20contributions%20include%20the%0Aintroduction%20of%20a%20new%20task%2C%20the%20development%20of%20DragGaussian%20for%20interactive%0Apoint-based%203D%20editing%2C%20and%20comprehensive%20validation%20of%20its%20effectiveness%0Athrough%20qualitative%20and%20quantitative%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDragGaussian%253A%2520Enabling%2520Drag-style%2520Manipulation%2520on%25203D%2520Gaussian%250A%2520%2520Representation%26entry.906535625%3DSitian%2520Shen%2520and%2520Jing%2520Xu%2520and%2520Yuheng%2520Yuan%2520and%2520Xingyi%2520Yang%2520and%2520Qiuhong%2520Shen%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520User-friendly%25203D%2520object%2520editing%2520is%2520a%2520challenging%2520task%2520that%2520has%2520attracted%250Asignificant%2520attention%2520recently.%2520The%2520limitations%2520of%2520direct%25203D%2520object%2520editing%250Awithout%25202D%2520prior%2520knowledge%2520have%2520prompted%2520increased%2520attention%2520towards%2520utilizing%250A2D%2520generative%2520models%2520for%25203D%2520editing.%2520While%2520existing%2520methods%2520like%2520Instruct%250ANeRF-to-NeRF%2520offer%2520a%2520solution%252C%2520they%2520often%2520lack%2520user-friendliness%252C%2520particularly%250Adue%2520to%2520semantic%2520guided%2520editing.%2520In%2520the%2520realm%2520of%25203D%2520representation%252C%25203D%2520Gaussian%250ASplatting%2520emerges%2520as%2520a%2520promising%2520approach%2520for%2520its%2520efficiency%2520and%2520natural%250Aexplicit%2520property%252C%2520facilitating%2520precise%2520editing%2520tasks.%2520Building%2520upon%2520these%250Ainsights%252C%2520we%2520propose%2520DragGaussian%252C%2520a%25203D%2520object%2520drag-editing%2520framework%2520based%2520on%250A3D%2520Gaussian%2520Splatting%252C%2520leveraging%2520diffusion%2520models%2520for%2520interactive%2520image%250Aediting%2520with%2520open-vocabulary%2520input.%2520This%2520framework%2520enables%2520users%2520to%2520perform%250Adrag-based%2520editing%2520on%2520pre-trained%25203D%2520Gaussian%2520object%2520models%252C%2520producing%2520modified%250A2D%2520images%2520through%2520multi-view%2520consistent%2520editing.%2520Our%2520contributions%2520include%2520the%250Aintroduction%2520of%2520a%2520new%2520task%252C%2520the%2520development%2520of%2520DragGaussian%2520for%2520interactive%250Apoint-based%25203D%2520editing%252C%2520and%2520comprehensive%2520validation%2520of%2520its%2520effectiveness%250Athrough%2520qualitative%2520and%2520quantitative%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DragGaussian%3A%20Enabling%20Drag-style%20Manipulation%20on%203D%20Gaussian%0A%20%20Representation&entry.906535625=Sitian%20Shen%20and%20Jing%20Xu%20and%20Yuheng%20Yuan%20and%20Xingyi%20Yang%20and%20Qiuhong%20Shen%20and%20Xinchao%20Wang&entry.1292438233=%20%20User-friendly%203D%20object%20editing%20is%20a%20challenging%20task%20that%20has%20attracted%0Asignificant%20attention%20recently.%20The%20limitations%20of%20direct%203D%20object%20editing%0Awithout%202D%20prior%20knowledge%20have%20prompted%20increased%20attention%20towards%20utilizing%0A2D%20generative%20models%20for%203D%20editing.%20While%20existing%20methods%20like%20Instruct%0ANeRF-to-NeRF%20offer%20a%20solution%2C%20they%20often%20lack%20user-friendliness%2C%20particularly%0Adue%20to%20semantic%20guided%20editing.%20In%20the%20realm%20of%203D%20representation%2C%203D%20Gaussian%0ASplatting%20emerges%20as%20a%20promising%20approach%20for%20its%20efficiency%20and%20natural%0Aexplicit%20property%2C%20facilitating%20precise%20editing%20tasks.%20Building%20upon%20these%0Ainsights%2C%20we%20propose%20DragGaussian%2C%20a%203D%20object%20drag-editing%20framework%20based%20on%0A3D%20Gaussian%20Splatting%2C%20leveraging%20diffusion%20models%20for%20interactive%20image%0Aediting%20with%20open-vocabulary%20input.%20This%20framework%20enables%20users%20to%20perform%0Adrag-based%20editing%20on%20pre-trained%203D%20Gaussian%20object%20models%2C%20producing%20modified%0A2D%20images%20through%20multi-view%20consistent%20editing.%20Our%20contributions%20include%20the%0Aintroduction%20of%20a%20new%20task%2C%20the%20development%20of%20DragGaussian%20for%20interactive%0Apoint-based%203D%20editing%2C%20and%20comprehensive%20validation%20of%20its%20effectiveness%0Athrough%20qualitative%20and%20quantitative%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05800v1&entry.124074799=Read"},
{"title": "Free-Moving Object Reconstruction and Pose Estimation with Virtual\n  Camera", "author": "Haixin Shi and Yinlin Hu and Daniel Koguciuk and Juan-Ting Lin and Mathieu Salzmann and David Ferstl", "abstract": "  We propose an approach for reconstructing free-moving object from a monocular\nRGB video. Most existing methods either assume scene prior, hand pose prior,\nobject category pose prior, or rely on local optimization with multiple\nsequence segments. We propose a method that allows free interaction with the\nobject in front of a moving camera without relying on any prior, and optimizes\nthe sequence globally without any segments. We progressively optimize the\nobject shape and pose simultaneously based on an implicit neural\nrepresentation. A key aspect of our method is a virtual camera system that\nreduces the search space of the optimization significantly. We evaluate our\nmethod on the standard HO3D dataset and a collection of egocentric RGB\nsequences captured with a head-mounted device. We demonstrate that our approach\noutperforms most methods significantly, and is on par with recent techniques\nthat assume prior information.\n", "link": "http://arxiv.org/abs/2405.05858v1", "date": "2024-05-09", "relevancy": 2.9427, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6033}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-Moving%20Object%20Reconstruction%20and%20Pose%20Estimation%20with%20Virtual%0A%20%20Camera&body=Title%3A%20Free-Moving%20Object%20Reconstruction%20and%20Pose%20Estimation%20with%20Virtual%0A%20%20Camera%0AAuthor%3A%20Haixin%20Shi%20and%20Yinlin%20Hu%20and%20Daniel%20Koguciuk%20and%20Juan-Ting%20Lin%20and%20Mathieu%20Salzmann%20and%20David%20Ferstl%0AAbstract%3A%20%20%20We%20propose%20an%20approach%20for%20reconstructing%20free-moving%20object%20from%20a%20monocular%0ARGB%20video.%20Most%20existing%20methods%20either%20assume%20scene%20prior%2C%20hand%20pose%20prior%2C%0Aobject%20category%20pose%20prior%2C%20or%20rely%20on%20local%20optimization%20with%20multiple%0Asequence%20segments.%20We%20propose%20a%20method%20that%20allows%20free%20interaction%20with%20the%0Aobject%20in%20front%20of%20a%20moving%20camera%20without%20relying%20on%20any%20prior%2C%20and%20optimizes%0Athe%20sequence%20globally%20without%20any%20segments.%20We%20progressively%20optimize%20the%0Aobject%20shape%20and%20pose%20simultaneously%20based%20on%20an%20implicit%20neural%0Arepresentation.%20A%20key%20aspect%20of%20our%20method%20is%20a%20virtual%20camera%20system%20that%0Areduces%20the%20search%20space%20of%20the%20optimization%20significantly.%20We%20evaluate%20our%0Amethod%20on%20the%20standard%20HO3D%20dataset%20and%20a%20collection%20of%20egocentric%20RGB%0Asequences%20captured%20with%20a%20head-mounted%20device.%20We%20demonstrate%20that%20our%20approach%0Aoutperforms%20most%20methods%20significantly%2C%20and%20is%20on%20par%20with%20recent%20techniques%0Athat%20assume%20prior%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-Moving%2520Object%2520Reconstruction%2520and%2520Pose%2520Estimation%2520with%2520Virtual%250A%2520%2520Camera%26entry.906535625%3DHaixin%2520Shi%2520and%2520Yinlin%2520Hu%2520and%2520Daniel%2520Koguciuk%2520and%2520Juan-Ting%2520Lin%2520and%2520Mathieu%2520Salzmann%2520and%2520David%2520Ferstl%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520approach%2520for%2520reconstructing%2520free-moving%2520object%2520from%2520a%2520monocular%250ARGB%2520video.%2520Most%2520existing%2520methods%2520either%2520assume%2520scene%2520prior%252C%2520hand%2520pose%2520prior%252C%250Aobject%2520category%2520pose%2520prior%252C%2520or%2520rely%2520on%2520local%2520optimization%2520with%2520multiple%250Asequence%2520segments.%2520We%2520propose%2520a%2520method%2520that%2520allows%2520free%2520interaction%2520with%2520the%250Aobject%2520in%2520front%2520of%2520a%2520moving%2520camera%2520without%2520relying%2520on%2520any%2520prior%252C%2520and%2520optimizes%250Athe%2520sequence%2520globally%2520without%2520any%2520segments.%2520We%2520progressively%2520optimize%2520the%250Aobject%2520shape%2520and%2520pose%2520simultaneously%2520based%2520on%2520an%2520implicit%2520neural%250Arepresentation.%2520A%2520key%2520aspect%2520of%2520our%2520method%2520is%2520a%2520virtual%2520camera%2520system%2520that%250Areduces%2520the%2520search%2520space%2520of%2520the%2520optimization%2520significantly.%2520We%2520evaluate%2520our%250Amethod%2520on%2520the%2520standard%2520HO3D%2520dataset%2520and%2520a%2520collection%2520of%2520egocentric%2520RGB%250Asequences%2520captured%2520with%2520a%2520head-mounted%2520device.%2520We%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520most%2520methods%2520significantly%252C%2520and%2520is%2520on%2520par%2520with%2520recent%2520techniques%250Athat%2520assume%2520prior%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-Moving%20Object%20Reconstruction%20and%20Pose%20Estimation%20with%20Virtual%0A%20%20Camera&entry.906535625=Haixin%20Shi%20and%20Yinlin%20Hu%20and%20Daniel%20Koguciuk%20and%20Juan-Ting%20Lin%20and%20Mathieu%20Salzmann%20and%20David%20Ferstl&entry.1292438233=%20%20We%20propose%20an%20approach%20for%20reconstructing%20free-moving%20object%20from%20a%20monocular%0ARGB%20video.%20Most%20existing%20methods%20either%20assume%20scene%20prior%2C%20hand%20pose%20prior%2C%0Aobject%20category%20pose%20prior%2C%20or%20rely%20on%20local%20optimization%20with%20multiple%0Asequence%20segments.%20We%20propose%20a%20method%20that%20allows%20free%20interaction%20with%20the%0Aobject%20in%20front%20of%20a%20moving%20camera%20without%20relying%20on%20any%20prior%2C%20and%20optimizes%0Athe%20sequence%20globally%20without%20any%20segments.%20We%20progressively%20optimize%20the%0Aobject%20shape%20and%20pose%20simultaneously%20based%20on%20an%20implicit%20neural%0Arepresentation.%20A%20key%20aspect%20of%20our%20method%20is%20a%20virtual%20camera%20system%20that%0Areduces%20the%20search%20space%20of%20the%20optimization%20significantly.%20We%20evaluate%20our%0Amethod%20on%20the%20standard%20HO3D%20dataset%20and%20a%20collection%20of%20egocentric%20RGB%0Asequences%20captured%20with%20a%20head-mounted%20device.%20We%20demonstrate%20that%20our%20approach%0Aoutperforms%20most%20methods%20significantly%2C%20and%20is%20on%20par%20with%20recent%20techniques%0Athat%20assume%20prior%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05858v1&entry.124074799=Read"},
{"title": "Self-Supervised Pre-training with Symmetric Superimposition Modeling for\n  Scene Text Recognition", "author": "Zuan Gao and Yuxin Wang and Yadong Qu and Boqiang Zhang and Zixiao Wang and Jianjun Xu and Hongtao Xie", "abstract": "  In text recognition, self-supervised pre-training emerges as a good solution\nto reduce dependence on expansive annotated real data. Previous studies\nprimarily focus on local visual representation by leveraging mask image\nmodeling or sequence contrastive learning. However, they omit modeling the\nlinguistic information in text images, which is crucial for recognizing text.\nTo simultaneously capture local character features and linguistic information\nin visual space, we propose Symmetric Superimposition Modeling (SSM). The\nobjective of SSM is to reconstruct the direction-specific pixel and feature\nsignals from the symmetrically superimposed input. Specifically, we add the\noriginal image with its inverted views to create the symmetrically superimposed\ninputs. At the pixel level, we reconstruct the original and inverted images to\ncapture character shapes and texture-level linguistic context. At the feature\nlevel, we reconstruct the feature of the same original image and inverted image\nwith different augmentations to model the semantic-level linguistic context and\nthe local character discrimination. In our design, we disrupt the character\nshape and linguistic rules. Consequently, the dual-level reconstruction\nfacilitates understanding character shapes and linguistic information from the\nperspective of visual texture and feature semantics. Experiments on various\ntext recognition benchmarks demonstrate the effectiveness and generality of\nSSM, with 4.1% average performance gains and 86.6% new state-of-the-art average\nword accuracy on Union14M benchmarks.\n", "link": "http://arxiv.org/abs/2405.05841v1", "date": "2024-05-09", "relevancy": 2.872, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Pre-training%20with%20Symmetric%20Superimposition%20Modeling%20for%0A%20%20Scene%20Text%20Recognition&body=Title%3A%20Self-Supervised%20Pre-training%20with%20Symmetric%20Superimposition%20Modeling%20for%0A%20%20Scene%20Text%20Recognition%0AAuthor%3A%20Zuan%20Gao%20and%20Yuxin%20Wang%20and%20Yadong%20Qu%20and%20Boqiang%20Zhang%20and%20Zixiao%20Wang%20and%20Jianjun%20Xu%20and%20Hongtao%20Xie%0AAbstract%3A%20%20%20In%20text%20recognition%2C%20self-supervised%20pre-training%20emerges%20as%20a%20good%20solution%0Ato%20reduce%20dependence%20on%20expansive%20annotated%20real%20data.%20Previous%20studies%0Aprimarily%20focus%20on%20local%20visual%20representation%20by%20leveraging%20mask%20image%0Amodeling%20or%20sequence%20contrastive%20learning.%20However%2C%20they%20omit%20modeling%20the%0Alinguistic%20information%20in%20text%20images%2C%20which%20is%20crucial%20for%20recognizing%20text.%0ATo%20simultaneously%20capture%20local%20character%20features%20and%20linguistic%20information%0Ain%20visual%20space%2C%20we%20propose%20Symmetric%20Superimposition%20Modeling%20%28SSM%29.%20The%0Aobjective%20of%20SSM%20is%20to%20reconstruct%20the%20direction-specific%20pixel%20and%20feature%0Asignals%20from%20the%20symmetrically%20superimposed%20input.%20Specifically%2C%20we%20add%20the%0Aoriginal%20image%20with%20its%20inverted%20views%20to%20create%20the%20symmetrically%20superimposed%0Ainputs.%20At%20the%20pixel%20level%2C%20we%20reconstruct%20the%20original%20and%20inverted%20images%20to%0Acapture%20character%20shapes%20and%20texture-level%20linguistic%20context.%20At%20the%20feature%0Alevel%2C%20we%20reconstruct%20the%20feature%20of%20the%20same%20original%20image%20and%20inverted%20image%0Awith%20different%20augmentations%20to%20model%20the%20semantic-level%20linguistic%20context%20and%0Athe%20local%20character%20discrimination.%20In%20our%20design%2C%20we%20disrupt%20the%20character%0Ashape%20and%20linguistic%20rules.%20Consequently%2C%20the%20dual-level%20reconstruction%0Afacilitates%20understanding%20character%20shapes%20and%20linguistic%20information%20from%20the%0Aperspective%20of%20visual%20texture%20and%20feature%20semantics.%20Experiments%20on%20various%0Atext%20recognition%20benchmarks%20demonstrate%20the%20effectiveness%20and%20generality%20of%0ASSM%2C%20with%204.1%25%20average%20performance%20gains%20and%2086.6%25%20new%20state-of-the-art%20average%0Aword%20accuracy%20on%20Union14M%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Pre-training%2520with%2520Symmetric%2520Superimposition%2520Modeling%2520for%250A%2520%2520Scene%2520Text%2520Recognition%26entry.906535625%3DZuan%2520Gao%2520and%2520Yuxin%2520Wang%2520and%2520Yadong%2520Qu%2520and%2520Boqiang%2520Zhang%2520and%2520Zixiao%2520Wang%2520and%2520Jianjun%2520Xu%2520and%2520Hongtao%2520Xie%26entry.1292438233%3D%2520%2520In%2520text%2520recognition%252C%2520self-supervised%2520pre-training%2520emerges%2520as%2520a%2520good%2520solution%250Ato%2520reduce%2520dependence%2520on%2520expansive%2520annotated%2520real%2520data.%2520Previous%2520studies%250Aprimarily%2520focus%2520on%2520local%2520visual%2520representation%2520by%2520leveraging%2520mask%2520image%250Amodeling%2520or%2520sequence%2520contrastive%2520learning.%2520However%252C%2520they%2520omit%2520modeling%2520the%250Alinguistic%2520information%2520in%2520text%2520images%252C%2520which%2520is%2520crucial%2520for%2520recognizing%2520text.%250ATo%2520simultaneously%2520capture%2520local%2520character%2520features%2520and%2520linguistic%2520information%250Ain%2520visual%2520space%252C%2520we%2520propose%2520Symmetric%2520Superimposition%2520Modeling%2520%2528SSM%2529.%2520The%250Aobjective%2520of%2520SSM%2520is%2520to%2520reconstruct%2520the%2520direction-specific%2520pixel%2520and%2520feature%250Asignals%2520from%2520the%2520symmetrically%2520superimposed%2520input.%2520Specifically%252C%2520we%2520add%2520the%250Aoriginal%2520image%2520with%2520its%2520inverted%2520views%2520to%2520create%2520the%2520symmetrically%2520superimposed%250Ainputs.%2520At%2520the%2520pixel%2520level%252C%2520we%2520reconstruct%2520the%2520original%2520and%2520inverted%2520images%2520to%250Acapture%2520character%2520shapes%2520and%2520texture-level%2520linguistic%2520context.%2520At%2520the%2520feature%250Alevel%252C%2520we%2520reconstruct%2520the%2520feature%2520of%2520the%2520same%2520original%2520image%2520and%2520inverted%2520image%250Awith%2520different%2520augmentations%2520to%2520model%2520the%2520semantic-level%2520linguistic%2520context%2520and%250Athe%2520local%2520character%2520discrimination.%2520In%2520our%2520design%252C%2520we%2520disrupt%2520the%2520character%250Ashape%2520and%2520linguistic%2520rules.%2520Consequently%252C%2520the%2520dual-level%2520reconstruction%250Afacilitates%2520understanding%2520character%2520shapes%2520and%2520linguistic%2520information%2520from%2520the%250Aperspective%2520of%2520visual%2520texture%2520and%2520feature%2520semantics.%2520Experiments%2520on%2520various%250Atext%2520recognition%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520and%2520generality%2520of%250ASSM%252C%2520with%25204.1%2525%2520average%2520performance%2520gains%2520and%252086.6%2525%2520new%2520state-of-the-art%2520average%250Aword%2520accuracy%2520on%2520Union14M%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Pre-training%20with%20Symmetric%20Superimposition%20Modeling%20for%0A%20%20Scene%20Text%20Recognition&entry.906535625=Zuan%20Gao%20and%20Yuxin%20Wang%20and%20Yadong%20Qu%20and%20Boqiang%20Zhang%20and%20Zixiao%20Wang%20and%20Jianjun%20Xu%20and%20Hongtao%20Xie&entry.1292438233=%20%20In%20text%20recognition%2C%20self-supervised%20pre-training%20emerges%20as%20a%20good%20solution%0Ato%20reduce%20dependence%20on%20expansive%20annotated%20real%20data.%20Previous%20studies%0Aprimarily%20focus%20on%20local%20visual%20representation%20by%20leveraging%20mask%20image%0Amodeling%20or%20sequence%20contrastive%20learning.%20However%2C%20they%20omit%20modeling%20the%0Alinguistic%20information%20in%20text%20images%2C%20which%20is%20crucial%20for%20recognizing%20text.%0ATo%20simultaneously%20capture%20local%20character%20features%20and%20linguistic%20information%0Ain%20visual%20space%2C%20we%20propose%20Symmetric%20Superimposition%20Modeling%20%28SSM%29.%20The%0Aobjective%20of%20SSM%20is%20to%20reconstruct%20the%20direction-specific%20pixel%20and%20feature%0Asignals%20from%20the%20symmetrically%20superimposed%20input.%20Specifically%2C%20we%20add%20the%0Aoriginal%20image%20with%20its%20inverted%20views%20to%20create%20the%20symmetrically%20superimposed%0Ainputs.%20At%20the%20pixel%20level%2C%20we%20reconstruct%20the%20original%20and%20inverted%20images%20to%0Acapture%20character%20shapes%20and%20texture-level%20linguistic%20context.%20At%20the%20feature%0Alevel%2C%20we%20reconstruct%20the%20feature%20of%20the%20same%20original%20image%20and%20inverted%20image%0Awith%20different%20augmentations%20to%20model%20the%20semantic-level%20linguistic%20context%20and%0Athe%20local%20character%20discrimination.%20In%20our%20design%2C%20we%20disrupt%20the%20character%0Ashape%20and%20linguistic%20rules.%20Consequently%2C%20the%20dual-level%20reconstruction%0Afacilitates%20understanding%20character%20shapes%20and%20linguistic%20information%20from%20the%0Aperspective%20of%20visual%20texture%20and%20feature%20semantics.%20Experiments%20on%20various%0Atext%20recognition%20benchmarks%20demonstrate%20the%20effectiveness%20and%20generality%20of%0ASSM%2C%20with%204.1%25%20average%20performance%20gains%20and%2086.6%25%20new%20state-of-the-art%20average%0Aword%20accuracy%20on%20Union14M%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05841v1&entry.124074799=Read"},
{"title": "FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic\n  Gaussian Splatting", "author": "Yikun Ma and Dandan Zhan and Zhi Jin", "abstract": "  Text-driven 3D indoor scene generation holds broad applications, ranging from\ngaming and smart homes to AR/VR applications. Fast and high-fidelity scene\ngeneration is paramount for ensuring user-friendly experiences. However,\nexisting methods are characterized by lengthy generation processes or\nnecessitate the intricate manual specification of motion parameters, which\nintroduces inconvenience for users. Furthermore, these methods often rely on\nnarrow-field viewpoint iterative generations, compromising global consistency\nand overall scene quality. To address these issues, we propose FastScene, a\nframework for fast and higher-quality 3D scene generation, while maintaining\nthe scene consistency. Specifically, given a text prompt, we generate a\npanorama and estimate its depth, since the panorama encompasses information\nabout the entire scene and exhibits explicit geometric constraints. To obtain\nhigh-quality novel views, we introduce the Coarse View Synthesis (CVS) and\nProgressive Novel View Inpainting (PNVI) strategies, ensuring both scene\nconsistency and view quality. Subsequently, we utilize Multi-View Projection\n(MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for\nscene reconstruction. Comprehensive experiments demonstrate FastScene surpasses\nother methods in both generation speed and quality with better scene\nconsistency. Notably, guided only by a text prompt, FastScene can generate a 3D\nscene within a mere 15 minutes, which is at least one hour faster than\nstate-of-the-art methods, making it a paradigm for user-friendly scene\ngeneration.\n", "link": "http://arxiv.org/abs/2405.05768v1", "date": "2024-05-09", "relevancy": 2.8695, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6222}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5718}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastScene%3A%20Text-Driven%20Fast%203D%20Indoor%20Scene%20Generation%20via%20Panoramic%0A%20%20Gaussian%20Splatting&body=Title%3A%20FastScene%3A%20Text-Driven%20Fast%203D%20Indoor%20Scene%20Generation%20via%20Panoramic%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Yikun%20Ma%20and%20Dandan%20Zhan%20and%20Zhi%20Jin%0AAbstract%3A%20%20%20Text-driven%203D%20indoor%20scene%20generation%20holds%20broad%20applications%2C%20ranging%20from%0Agaming%20and%20smart%20homes%20to%20AR/VR%20applications.%20Fast%20and%20high-fidelity%20scene%0Ageneration%20is%20paramount%20for%20ensuring%20user-friendly%20experiences.%20However%2C%0Aexisting%20methods%20are%20characterized%20by%20lengthy%20generation%20processes%20or%0Anecessitate%20the%20intricate%20manual%20specification%20of%20motion%20parameters%2C%20which%0Aintroduces%20inconvenience%20for%20users.%20Furthermore%2C%20these%20methods%20often%20rely%20on%0Anarrow-field%20viewpoint%20iterative%20generations%2C%20compromising%20global%20consistency%0Aand%20overall%20scene%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20FastScene%2C%20a%0Aframework%20for%20fast%20and%20higher-quality%203D%20scene%20generation%2C%20while%20maintaining%0Athe%20scene%20consistency.%20Specifically%2C%20given%20a%20text%20prompt%2C%20we%20generate%20a%0Apanorama%20and%20estimate%20its%20depth%2C%20since%20the%20panorama%20encompasses%20information%0Aabout%20the%20entire%20scene%20and%20exhibits%20explicit%20geometric%20constraints.%20To%20obtain%0Ahigh-quality%20novel%20views%2C%20we%20introduce%20the%20Coarse%20View%20Synthesis%20%28CVS%29%20and%0AProgressive%20Novel%20View%20Inpainting%20%28PNVI%29%20strategies%2C%20ensuring%20both%20scene%0Aconsistency%20and%20view%20quality.%20Subsequently%2C%20we%20utilize%20Multi-View%20Projection%0A%28MVP%29%20to%20form%20perspective%20views%2C%20and%20apply%203D%20Gaussian%20Splatting%20%283DGS%29%20for%0Ascene%20reconstruction.%20Comprehensive%20experiments%20demonstrate%20FastScene%20surpasses%0Aother%20methods%20in%20both%20generation%20speed%20and%20quality%20with%20better%20scene%0Aconsistency.%20Notably%2C%20guided%20only%20by%20a%20text%20prompt%2C%20FastScene%20can%20generate%20a%203D%0Ascene%20within%20a%20mere%2015%20minutes%2C%20which%20is%20at%20least%20one%20hour%20faster%20than%0Astate-of-the-art%20methods%2C%20making%20it%20a%20paradigm%20for%20user-friendly%20scene%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastScene%253A%2520Text-Driven%2520Fast%25203D%2520Indoor%2520Scene%2520Generation%2520via%2520Panoramic%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DYikun%2520Ma%2520and%2520Dandan%2520Zhan%2520and%2520Zhi%2520Jin%26entry.1292438233%3D%2520%2520Text-driven%25203D%2520indoor%2520scene%2520generation%2520holds%2520broad%2520applications%252C%2520ranging%2520from%250Agaming%2520and%2520smart%2520homes%2520to%2520AR/VR%2520applications.%2520Fast%2520and%2520high-fidelity%2520scene%250Ageneration%2520is%2520paramount%2520for%2520ensuring%2520user-friendly%2520experiences.%2520However%252C%250Aexisting%2520methods%2520are%2520characterized%2520by%2520lengthy%2520generation%2520processes%2520or%250Anecessitate%2520the%2520intricate%2520manual%2520specification%2520of%2520motion%2520parameters%252C%2520which%250Aintroduces%2520inconvenience%2520for%2520users.%2520Furthermore%252C%2520these%2520methods%2520often%2520rely%2520on%250Anarrow-field%2520viewpoint%2520iterative%2520generations%252C%2520compromising%2520global%2520consistency%250Aand%2520overall%2520scene%2520quality.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520FastScene%252C%2520a%250Aframework%2520for%2520fast%2520and%2520higher-quality%25203D%2520scene%2520generation%252C%2520while%2520maintaining%250Athe%2520scene%2520consistency.%2520Specifically%252C%2520given%2520a%2520text%2520prompt%252C%2520we%2520generate%2520a%250Apanorama%2520and%2520estimate%2520its%2520depth%252C%2520since%2520the%2520panorama%2520encompasses%2520information%250Aabout%2520the%2520entire%2520scene%2520and%2520exhibits%2520explicit%2520geometric%2520constraints.%2520To%2520obtain%250Ahigh-quality%2520novel%2520views%252C%2520we%2520introduce%2520the%2520Coarse%2520View%2520Synthesis%2520%2528CVS%2529%2520and%250AProgressive%2520Novel%2520View%2520Inpainting%2520%2528PNVI%2529%2520strategies%252C%2520ensuring%2520both%2520scene%250Aconsistency%2520and%2520view%2520quality.%2520Subsequently%252C%2520we%2520utilize%2520Multi-View%2520Projection%250A%2528MVP%2529%2520to%2520form%2520perspective%2520views%252C%2520and%2520apply%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520for%250Ascene%2520reconstruction.%2520Comprehensive%2520experiments%2520demonstrate%2520FastScene%2520surpasses%250Aother%2520methods%2520in%2520both%2520generation%2520speed%2520and%2520quality%2520with%2520better%2520scene%250Aconsistency.%2520Notably%252C%2520guided%2520only%2520by%2520a%2520text%2520prompt%252C%2520FastScene%2520can%2520generate%2520a%25203D%250Ascene%2520within%2520a%2520mere%252015%2520minutes%252C%2520which%2520is%2520at%2520least%2520one%2520hour%2520faster%2520than%250Astate-of-the-art%2520methods%252C%2520making%2520it%2520a%2520paradigm%2520for%2520user-friendly%2520scene%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastScene%3A%20Text-Driven%20Fast%203D%20Indoor%20Scene%20Generation%20via%20Panoramic%0A%20%20Gaussian%20Splatting&entry.906535625=Yikun%20Ma%20and%20Dandan%20Zhan%20and%20Zhi%20Jin&entry.1292438233=%20%20Text-driven%203D%20indoor%20scene%20generation%20holds%20broad%20applications%2C%20ranging%20from%0Agaming%20and%20smart%20homes%20to%20AR/VR%20applications.%20Fast%20and%20high-fidelity%20scene%0Ageneration%20is%20paramount%20for%20ensuring%20user-friendly%20experiences.%20However%2C%0Aexisting%20methods%20are%20characterized%20by%20lengthy%20generation%20processes%20or%0Anecessitate%20the%20intricate%20manual%20specification%20of%20motion%20parameters%2C%20which%0Aintroduces%20inconvenience%20for%20users.%20Furthermore%2C%20these%20methods%20often%20rely%20on%0Anarrow-field%20viewpoint%20iterative%20generations%2C%20compromising%20global%20consistency%0Aand%20overall%20scene%20quality.%20To%20address%20these%20issues%2C%20we%20propose%20FastScene%2C%20a%0Aframework%20for%20fast%20and%20higher-quality%203D%20scene%20generation%2C%20while%20maintaining%0Athe%20scene%20consistency.%20Specifically%2C%20given%20a%20text%20prompt%2C%20we%20generate%20a%0Apanorama%20and%20estimate%20its%20depth%2C%20since%20the%20panorama%20encompasses%20information%0Aabout%20the%20entire%20scene%20and%20exhibits%20explicit%20geometric%20constraints.%20To%20obtain%0Ahigh-quality%20novel%20views%2C%20we%20introduce%20the%20Coarse%20View%20Synthesis%20%28CVS%29%20and%0AProgressive%20Novel%20View%20Inpainting%20%28PNVI%29%20strategies%2C%20ensuring%20both%20scene%0Aconsistency%20and%20view%20quality.%20Subsequently%2C%20we%20utilize%20Multi-View%20Projection%0A%28MVP%29%20to%20form%20perspective%20views%2C%20and%20apply%203D%20Gaussian%20Splatting%20%283DGS%29%20for%0Ascene%20reconstruction.%20Comprehensive%20experiments%20demonstrate%20FastScene%20surpasses%0Aother%20methods%20in%20both%20generation%20speed%20and%20quality%20with%20better%20scene%0Aconsistency.%20Notably%2C%20guided%20only%20by%20a%20text%20prompt%2C%20FastScene%20can%20generate%20a%203D%0Ascene%20within%20a%20mere%2015%20minutes%2C%20which%20is%20at%20least%20one%20hour%20faster%20than%0Astate-of-the-art%20methods%2C%20making%20it%20a%20paradigm%20for%20user-friendly%20scene%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05768v1&entry.124074799=Read"},
{"title": "RPBG: Towards Robust Neural Point-based Graphics in the Wild", "author": "Qingtian Zhu and Zizhuang Wei and Zhongtian Zheng and Yifan Zhan and Zhuyu Yao and Jiawang Zhang and Kejian Wu and Yinqiang Zheng", "abstract": "  Point-based representations have recently gained popularity in novel view\nsynthesis, for their unique advantages, e.g., intuitive geometric\nrepresentation, simple manipulation, and faster convergence. However, based on\nour observation, these point-based neural re-rendering methods are only\nexpected to perform well under ideal conditions and suffer from noisy, patchy\npoints and unbounded scenes, which are challenging to handle but defacto common\nin real applications. To this end, we revisit one such influential method,\nknown as Neural Point-based Graphics (NPBG), as our baseline, and propose\nRobust Point-based Graphics (RPBG). We in-depth analyze the factors that\nprevent NPBG from achieving satisfactory renderings on generic datasets, and\naccordingly reform the pipeline to make it more robust to varying datasets\nin-the-wild. Inspired by the practices in image restoration, we greatly enhance\nthe neural renderer to enable the attention-based correction of point\nvisibility and the inpainting of incomplete rasterization, with only acceptable\noverheads. We also seek for a simple and lightweight alternative for\nenvironment modeling and an iterative method to alleviate the problem of poor\ngeometry. By thorough evaluation on a wide range of datasets with different\nshooting conditions and camera trajectories, RPBG stably outperforms the\nbaseline by a large margin, and exhibits its great robustness over\nstate-of-the-art NeRF-based variants. Code available at\nhttps://github.com/QT-Zhu/RPBG.\n", "link": "http://arxiv.org/abs/2405.05663v1", "date": "2024-05-09", "relevancy": 2.8616, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6131}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.577}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RPBG%3A%20Towards%20Robust%20Neural%20Point-based%20Graphics%20in%20the%20Wild&body=Title%3A%20RPBG%3A%20Towards%20Robust%20Neural%20Point-based%20Graphics%20in%20the%20Wild%0AAuthor%3A%20Qingtian%20Zhu%20and%20Zizhuang%20Wei%20and%20Zhongtian%20Zheng%20and%20Yifan%20Zhan%20and%20Zhuyu%20Yao%20and%20Jiawang%20Zhang%20and%20Kejian%20Wu%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Point-based%20representations%20have%20recently%20gained%20popularity%20in%20novel%20view%0Asynthesis%2C%20for%20their%20unique%20advantages%2C%20e.g.%2C%20intuitive%20geometric%0Arepresentation%2C%20simple%20manipulation%2C%20and%20faster%20convergence.%20However%2C%20based%20on%0Aour%20observation%2C%20these%20point-based%20neural%20re-rendering%20methods%20are%20only%0Aexpected%20to%20perform%20well%20under%20ideal%20conditions%20and%20suffer%20from%20noisy%2C%20patchy%0Apoints%20and%20unbounded%20scenes%2C%20which%20are%20challenging%20to%20handle%20but%20defacto%20common%0Ain%20real%20applications.%20To%20this%20end%2C%20we%20revisit%20one%20such%20influential%20method%2C%0Aknown%20as%20Neural%20Point-based%20Graphics%20%28NPBG%29%2C%20as%20our%20baseline%2C%20and%20propose%0ARobust%20Point-based%20Graphics%20%28RPBG%29.%20We%20in-depth%20analyze%20the%20factors%20that%0Aprevent%20NPBG%20from%20achieving%20satisfactory%20renderings%20on%20generic%20datasets%2C%20and%0Aaccordingly%20reform%20the%20pipeline%20to%20make%20it%20more%20robust%20to%20varying%20datasets%0Ain-the-wild.%20Inspired%20by%20the%20practices%20in%20image%20restoration%2C%20we%20greatly%20enhance%0Athe%20neural%20renderer%20to%20enable%20the%20attention-based%20correction%20of%20point%0Avisibility%20and%20the%20inpainting%20of%20incomplete%20rasterization%2C%20with%20only%20acceptable%0Aoverheads.%20We%20also%20seek%20for%20a%20simple%20and%20lightweight%20alternative%20for%0Aenvironment%20modeling%20and%20an%20iterative%20method%20to%20alleviate%20the%20problem%20of%20poor%0Ageometry.%20By%20thorough%20evaluation%20on%20a%20wide%20range%20of%20datasets%20with%20different%0Ashooting%20conditions%20and%20camera%20trajectories%2C%20RPBG%20stably%20outperforms%20the%0Abaseline%20by%20a%20large%20margin%2C%20and%20exhibits%20its%20great%20robustness%20over%0Astate-of-the-art%20NeRF-based%20variants.%20Code%20available%20at%0Ahttps%3A//github.com/QT-Zhu/RPBG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRPBG%253A%2520Towards%2520Robust%2520Neural%2520Point-based%2520Graphics%2520in%2520the%2520Wild%26entry.906535625%3DQingtian%2520Zhu%2520and%2520Zizhuang%2520Wei%2520and%2520Zhongtian%2520Zheng%2520and%2520Yifan%2520Zhan%2520and%2520Zhuyu%2520Yao%2520and%2520Jiawang%2520Zhang%2520and%2520Kejian%2520Wu%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Point-based%2520representations%2520have%2520recently%2520gained%2520popularity%2520in%2520novel%2520view%250Asynthesis%252C%2520for%2520their%2520unique%2520advantages%252C%2520e.g.%252C%2520intuitive%2520geometric%250Arepresentation%252C%2520simple%2520manipulation%252C%2520and%2520faster%2520convergence.%2520However%252C%2520based%2520on%250Aour%2520observation%252C%2520these%2520point-based%2520neural%2520re-rendering%2520methods%2520are%2520only%250Aexpected%2520to%2520perform%2520well%2520under%2520ideal%2520conditions%2520and%2520suffer%2520from%2520noisy%252C%2520patchy%250Apoints%2520and%2520unbounded%2520scenes%252C%2520which%2520are%2520challenging%2520to%2520handle%2520but%2520defacto%2520common%250Ain%2520real%2520applications.%2520To%2520this%2520end%252C%2520we%2520revisit%2520one%2520such%2520influential%2520method%252C%250Aknown%2520as%2520Neural%2520Point-based%2520Graphics%2520%2528NPBG%2529%252C%2520as%2520our%2520baseline%252C%2520and%2520propose%250ARobust%2520Point-based%2520Graphics%2520%2528RPBG%2529.%2520We%2520in-depth%2520analyze%2520the%2520factors%2520that%250Aprevent%2520NPBG%2520from%2520achieving%2520satisfactory%2520renderings%2520on%2520generic%2520datasets%252C%2520and%250Aaccordingly%2520reform%2520the%2520pipeline%2520to%2520make%2520it%2520more%2520robust%2520to%2520varying%2520datasets%250Ain-the-wild.%2520Inspired%2520by%2520the%2520practices%2520in%2520image%2520restoration%252C%2520we%2520greatly%2520enhance%250Athe%2520neural%2520renderer%2520to%2520enable%2520the%2520attention-based%2520correction%2520of%2520point%250Avisibility%2520and%2520the%2520inpainting%2520of%2520incomplete%2520rasterization%252C%2520with%2520only%2520acceptable%250Aoverheads.%2520We%2520also%2520seek%2520for%2520a%2520simple%2520and%2520lightweight%2520alternative%2520for%250Aenvironment%2520modeling%2520and%2520an%2520iterative%2520method%2520to%2520alleviate%2520the%2520problem%2520of%2520poor%250Ageometry.%2520By%2520thorough%2520evaluation%2520on%2520a%2520wide%2520range%2520of%2520datasets%2520with%2520different%250Ashooting%2520conditions%2520and%2520camera%2520trajectories%252C%2520RPBG%2520stably%2520outperforms%2520the%250Abaseline%2520by%2520a%2520large%2520margin%252C%2520and%2520exhibits%2520its%2520great%2520robustness%2520over%250Astate-of-the-art%2520NeRF-based%2520variants.%2520Code%2520available%2520at%250Ahttps%253A//github.com/QT-Zhu/RPBG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RPBG%3A%20Towards%20Robust%20Neural%20Point-based%20Graphics%20in%20the%20Wild&entry.906535625=Qingtian%20Zhu%20and%20Zizhuang%20Wei%20and%20Zhongtian%20Zheng%20and%20Yifan%20Zhan%20and%20Zhuyu%20Yao%20and%20Jiawang%20Zhang%20and%20Kejian%20Wu%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Point-based%20representations%20have%20recently%20gained%20popularity%20in%20novel%20view%0Asynthesis%2C%20for%20their%20unique%20advantages%2C%20e.g.%2C%20intuitive%20geometric%0Arepresentation%2C%20simple%20manipulation%2C%20and%20faster%20convergence.%20However%2C%20based%20on%0Aour%20observation%2C%20these%20point-based%20neural%20re-rendering%20methods%20are%20only%0Aexpected%20to%20perform%20well%20under%20ideal%20conditions%20and%20suffer%20from%20noisy%2C%20patchy%0Apoints%20and%20unbounded%20scenes%2C%20which%20are%20challenging%20to%20handle%20but%20defacto%20common%0Ain%20real%20applications.%20To%20this%20end%2C%20we%20revisit%20one%20such%20influential%20method%2C%0Aknown%20as%20Neural%20Point-based%20Graphics%20%28NPBG%29%2C%20as%20our%20baseline%2C%20and%20propose%0ARobust%20Point-based%20Graphics%20%28RPBG%29.%20We%20in-depth%20analyze%20the%20factors%20that%0Aprevent%20NPBG%20from%20achieving%20satisfactory%20renderings%20on%20generic%20datasets%2C%20and%0Aaccordingly%20reform%20the%20pipeline%20to%20make%20it%20more%20robust%20to%20varying%20datasets%0Ain-the-wild.%20Inspired%20by%20the%20practices%20in%20image%20restoration%2C%20we%20greatly%20enhance%0Athe%20neural%20renderer%20to%20enable%20the%20attention-based%20correction%20of%20point%0Avisibility%20and%20the%20inpainting%20of%20incomplete%20rasterization%2C%20with%20only%20acceptable%0Aoverheads.%20We%20also%20seek%20for%20a%20simple%20and%20lightweight%20alternative%20for%0Aenvironment%20modeling%20and%20an%20iterative%20method%20to%20alleviate%20the%20problem%20of%20poor%0Ageometry.%20By%20thorough%20evaluation%20on%20a%20wide%20range%20of%20datasets%20with%20different%0Ashooting%20conditions%20and%20camera%20trajectories%2C%20RPBG%20stably%20outperforms%20the%0Abaseline%20by%20a%20large%20margin%2C%20and%20exhibits%20its%20great%20robustness%20over%0Astate-of-the-art%20NeRF-based%20variants.%20Code%20available%20at%0Ahttps%3A//github.com/QT-Zhu/RPBG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05663v1&entry.124074799=Read"},
{"title": "DGMamba: Domain Generalization via Generalized State Space Model", "author": "Shaocong Long and Qianyu Zhou and Xiangtai Li and Xuequan Lu and Chenhao Ying and Yuan Luo and Lizhuang Ma and Shuicheng Yan", "abstract": "  Domain generalization~(DG) aims at solving distribution shift problems in\nvarious scenes. Existing approaches are based on Convolution Neural Networks\n(CNNs) or Vision Transformers (ViTs), which suffer from limited receptive\nfields or quadratic complexities issues. Mamba, as an emerging state space\nmodel (SSM), possesses superior linear complexity and global receptive fields.\nDespite this, it can hardly be applied to DG to address distribution shifts,\ndue to the hidden state issues and inappropriate scan mechanisms. In this\npaper, we propose a novel framework for DG, named DGMamba, that excels in\nstrong generalizability toward unseen domains and meanwhile has the advantages\nof global receptive fields, and efficient linear complexity. Our DGMamba\ncompromises two core components: Hidden State Suppressing~(HSS) and\nSemantic-aware Patch refining~(SPR). In particular, HSS is introduced to\nmitigate the influence of hidden states associated with domain-specific\nfeatures during output prediction. SPR strives to encourage the model to\nconcentrate more on objects rather than context, consisting of two designs:\nPrior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely,\nPFS aims to shuffle the non-semantic patches within images, creating more\nflexible and effective sequences from images, and DCI is designed to regularize\nMamba with the combination of mismatched non-semantic and semantic information\nby fusing patches among domains. Extensive experiments on four commonly used DG\nbenchmarks demonstrate that the proposed DGMamba achieves remarkably superior\nresults to state-of-the-art models. The code will be made publicly available.\n", "link": "http://arxiv.org/abs/2404.07794v2", "date": "2024-05-09", "relevancy": 2.8142, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5767}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5719}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGMamba%3A%20Domain%20Generalization%20via%20Generalized%20State%20Space%20Model&body=Title%3A%20DGMamba%3A%20Domain%20Generalization%20via%20Generalized%20State%20Space%20Model%0AAuthor%3A%20Shaocong%20Long%20and%20Qianyu%20Zhou%20and%20Xiangtai%20Li%20and%20Xuequan%20Lu%20and%20Chenhao%20Ying%20and%20Yuan%20Luo%20and%20Lizhuang%20Ma%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Domain%20generalization~%28DG%29%20aims%20at%20solving%20distribution%20shift%20problems%20in%0Avarious%20scenes.%20Existing%20approaches%20are%20based%20on%20Convolution%20Neural%20Networks%0A%28CNNs%29%20or%20Vision%20Transformers%20%28ViTs%29%2C%20which%20suffer%20from%20limited%20receptive%0Afields%20or%20quadratic%20complexities%20issues.%20Mamba%2C%20as%20an%20emerging%20state%20space%0Amodel%20%28SSM%29%2C%20possesses%20superior%20linear%20complexity%20and%20global%20receptive%20fields.%0ADespite%20this%2C%20it%20can%20hardly%20be%20applied%20to%20DG%20to%20address%20distribution%20shifts%2C%0Adue%20to%20the%20hidden%20state%20issues%20and%20inappropriate%20scan%20mechanisms.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%20for%20DG%2C%20named%20DGMamba%2C%20that%20excels%20in%0Astrong%20generalizability%20toward%20unseen%20domains%20and%20meanwhile%20has%20the%20advantages%0Aof%20global%20receptive%20fields%2C%20and%20efficient%20linear%20complexity.%20Our%20DGMamba%0Acompromises%20two%20core%20components%3A%20Hidden%20State%20Suppressing~%28HSS%29%20and%0ASemantic-aware%20Patch%20refining~%28SPR%29.%20In%20particular%2C%20HSS%20is%20introduced%20to%0Amitigate%20the%20influence%20of%20hidden%20states%20associated%20with%20domain-specific%0Afeatures%20during%20output%20prediction.%20SPR%20strives%20to%20encourage%20the%20model%20to%0Aconcentrate%20more%20on%20objects%20rather%20than%20context%2C%20consisting%20of%20two%20designs%3A%0APrior-Free%20Scanning~%28PFS%29%2C%20and%20Domain%20Context%20Interchange~%28DCI%29.%20Concretely%2C%0APFS%20aims%20to%20shuffle%20the%20non-semantic%20patches%20within%20images%2C%20creating%20more%0Aflexible%20and%20effective%20sequences%20from%20images%2C%20and%20DCI%20is%20designed%20to%20regularize%0AMamba%20with%20the%20combination%20of%20mismatched%20non-semantic%20and%20semantic%20information%0Aby%20fusing%20patches%20among%20domains.%20Extensive%20experiments%20on%20four%20commonly%20used%20DG%0Abenchmarks%20demonstrate%20that%20the%20proposed%20DGMamba%20achieves%20remarkably%20superior%0Aresults%20to%20state-of-the-art%20models.%20The%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGMamba%253A%2520Domain%2520Generalization%2520via%2520Generalized%2520State%2520Space%2520Model%26entry.906535625%3DShaocong%2520Long%2520and%2520Qianyu%2520Zhou%2520and%2520Xiangtai%2520Li%2520and%2520Xuequan%2520Lu%2520and%2520Chenhao%2520Ying%2520and%2520Yuan%2520Luo%2520and%2520Lizhuang%2520Ma%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Domain%2520generalization~%2528DG%2529%2520aims%2520at%2520solving%2520distribution%2520shift%2520problems%2520in%250Avarious%2520scenes.%2520Existing%2520approaches%2520are%2520based%2520on%2520Convolution%2520Neural%2520Networks%250A%2528CNNs%2529%2520or%2520Vision%2520Transformers%2520%2528ViTs%2529%252C%2520which%2520suffer%2520from%2520limited%2520receptive%250Afields%2520or%2520quadratic%2520complexities%2520issues.%2520Mamba%252C%2520as%2520an%2520emerging%2520state%2520space%250Amodel%2520%2528SSM%2529%252C%2520possesses%2520superior%2520linear%2520complexity%2520and%2520global%2520receptive%2520fields.%250ADespite%2520this%252C%2520it%2520can%2520hardly%2520be%2520applied%2520to%2520DG%2520to%2520address%2520distribution%2520shifts%252C%250Adue%2520to%2520the%2520hidden%2520state%2520issues%2520and%2520inappropriate%2520scan%2520mechanisms.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520DG%252C%2520named%2520DGMamba%252C%2520that%2520excels%2520in%250Astrong%2520generalizability%2520toward%2520unseen%2520domains%2520and%2520meanwhile%2520has%2520the%2520advantages%250Aof%2520global%2520receptive%2520fields%252C%2520and%2520efficient%2520linear%2520complexity.%2520Our%2520DGMamba%250Acompromises%2520two%2520core%2520components%253A%2520Hidden%2520State%2520Suppressing~%2528HSS%2529%2520and%250ASemantic-aware%2520Patch%2520refining~%2528SPR%2529.%2520In%2520particular%252C%2520HSS%2520is%2520introduced%2520to%250Amitigate%2520the%2520influence%2520of%2520hidden%2520states%2520associated%2520with%2520domain-specific%250Afeatures%2520during%2520output%2520prediction.%2520SPR%2520strives%2520to%2520encourage%2520the%2520model%2520to%250Aconcentrate%2520more%2520on%2520objects%2520rather%2520than%2520context%252C%2520consisting%2520of%2520two%2520designs%253A%250APrior-Free%2520Scanning~%2528PFS%2529%252C%2520and%2520Domain%2520Context%2520Interchange~%2528DCI%2529.%2520Concretely%252C%250APFS%2520aims%2520to%2520shuffle%2520the%2520non-semantic%2520patches%2520within%2520images%252C%2520creating%2520more%250Aflexible%2520and%2520effective%2520sequences%2520from%2520images%252C%2520and%2520DCI%2520is%2520designed%2520to%2520regularize%250AMamba%2520with%2520the%2520combination%2520of%2520mismatched%2520non-semantic%2520and%2520semantic%2520information%250Aby%2520fusing%2520patches%2520among%2520domains.%2520Extensive%2520experiments%2520on%2520four%2520commonly%2520used%2520DG%250Abenchmarks%2520demonstrate%2520that%2520the%2520proposed%2520DGMamba%2520achieves%2520remarkably%2520superior%250Aresults%2520to%2520state-of-the-art%2520models.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGMamba%3A%20Domain%20Generalization%20via%20Generalized%20State%20Space%20Model&entry.906535625=Shaocong%20Long%20and%20Qianyu%20Zhou%20and%20Xiangtai%20Li%20and%20Xuequan%20Lu%20and%20Chenhao%20Ying%20and%20Yuan%20Luo%20and%20Lizhuang%20Ma%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Domain%20generalization~%28DG%29%20aims%20at%20solving%20distribution%20shift%20problems%20in%0Avarious%20scenes.%20Existing%20approaches%20are%20based%20on%20Convolution%20Neural%20Networks%0A%28CNNs%29%20or%20Vision%20Transformers%20%28ViTs%29%2C%20which%20suffer%20from%20limited%20receptive%0Afields%20or%20quadratic%20complexities%20issues.%20Mamba%2C%20as%20an%20emerging%20state%20space%0Amodel%20%28SSM%29%2C%20possesses%20superior%20linear%20complexity%20and%20global%20receptive%20fields.%0ADespite%20this%2C%20it%20can%20hardly%20be%20applied%20to%20DG%20to%20address%20distribution%20shifts%2C%0Adue%20to%20the%20hidden%20state%20issues%20and%20inappropriate%20scan%20mechanisms.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%20for%20DG%2C%20named%20DGMamba%2C%20that%20excels%20in%0Astrong%20generalizability%20toward%20unseen%20domains%20and%20meanwhile%20has%20the%20advantages%0Aof%20global%20receptive%20fields%2C%20and%20efficient%20linear%20complexity.%20Our%20DGMamba%0Acompromises%20two%20core%20components%3A%20Hidden%20State%20Suppressing~%28HSS%29%20and%0ASemantic-aware%20Patch%20refining~%28SPR%29.%20In%20particular%2C%20HSS%20is%20introduced%20to%0Amitigate%20the%20influence%20of%20hidden%20states%20associated%20with%20domain-specific%0Afeatures%20during%20output%20prediction.%20SPR%20strives%20to%20encourage%20the%20model%20to%0Aconcentrate%20more%20on%20objects%20rather%20than%20context%2C%20consisting%20of%20two%20designs%3A%0APrior-Free%20Scanning~%28PFS%29%2C%20and%20Domain%20Context%20Interchange~%28DCI%29.%20Concretely%2C%0APFS%20aims%20to%20shuffle%20the%20non-semantic%20patches%20within%20images%2C%20creating%20more%0Aflexible%20and%20effective%20sequences%20from%20images%2C%20and%20DCI%20is%20designed%20to%20regularize%0AMamba%20with%20the%20combination%20of%20mismatched%20non-semantic%20and%20semantic%20information%0Aby%20fusing%20patches%20among%20domains.%20Extensive%20experiments%20on%20four%20commonly%20used%20DG%0Abenchmarks%20demonstrate%20that%20the%20proposed%20DGMamba%20achieves%20remarkably%20superior%0Aresults%20to%20state-of-the-art%20models.%20The%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07794v2&entry.124074799=Read"},
{"title": "Robust and Explainable Fine-Grained Visual Classification with Transfer\n  Learning: A Dual-Carriageway Framework", "author": "Zheming Zuo and Joseph Smith and Jonathan Stonehouse and Boguslaw Obara", "abstract": "  In the realm of practical fine-grained visual classification applications\nrooted in deep learning, a common scenario involves training a model using a\npre-existing dataset. Subsequently, a new dataset becomes available, prompting\nthe desire to make a pivotal decision for achieving enhanced and leveraged\ninference performance on both sides: Should one opt to train datasets from\nscratch or fine-tune the model trained on the initial dataset using the newly\nreleased dataset? The existing literature reveals a lack of methods to\nsystematically determine the optimal training strategy, necessitating\nexplainability. To this end, we present an automatic best-suit training\nsolution searching framework, the Dual-Carriageway Framework (DCF), to fill\nthis gap. DCF benefits from the design of a dual-direction search (starting\nfrom the pre-existing or the newly released dataset) where five different\ntraining settings are enforced. In addition, DCF is not only capable of\nfiguring out the optimal training strategy with the capability of avoiding\noverfitting but also yields built-in quantitative and visual explanations\nderived from the actual input and weights of the trained model. We validated\nDCF's effectiveness through experiments with three convolutional neural\nnetworks (ResNet18, ResNet34 and Inception-v3) on two temporally continued\ncommercial product datasets. Results showed fine-tuning pathways outperformed\ntraining-from-scratch ones by up to 2.13% and 1.23% on the pre-existing and new\ndatasets, respectively, in terms of mean accuracy. Furthermore, DCF identified\nreflection padding as the superior padding method, enhancing testing accuracy\nby 3.72% on average. This framework stands out for its potential to guide the\ndevelopment of robust and explainable AI solutions in fine-grained visual\nclassification tasks.\n", "link": "http://arxiv.org/abs/2405.05853v1", "date": "2024-05-09", "relevancy": 2.8088, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Explainable%20Fine-Grained%20Visual%20Classification%20with%20Transfer%0A%20%20Learning%3A%20A%20Dual-Carriageway%20Framework&body=Title%3A%20Robust%20and%20Explainable%20Fine-Grained%20Visual%20Classification%20with%20Transfer%0A%20%20Learning%3A%20A%20Dual-Carriageway%20Framework%0AAuthor%3A%20Zheming%20Zuo%20and%20Joseph%20Smith%20and%20Jonathan%20Stonehouse%20and%20Boguslaw%20Obara%0AAbstract%3A%20%20%20In%20the%20realm%20of%20practical%20fine-grained%20visual%20classification%20applications%0Arooted%20in%20deep%20learning%2C%20a%20common%20scenario%20involves%20training%20a%20model%20using%20a%0Apre-existing%20dataset.%20Subsequently%2C%20a%20new%20dataset%20becomes%20available%2C%20prompting%0Athe%20desire%20to%20make%20a%20pivotal%20decision%20for%20achieving%20enhanced%20and%20leveraged%0Ainference%20performance%20on%20both%20sides%3A%20Should%20one%20opt%20to%20train%20datasets%20from%0Ascratch%20or%20fine-tune%20the%20model%20trained%20on%20the%20initial%20dataset%20using%20the%20newly%0Areleased%20dataset%3F%20The%20existing%20literature%20reveals%20a%20lack%20of%20methods%20to%0Asystematically%20determine%20the%20optimal%20training%20strategy%2C%20necessitating%0Aexplainability.%20To%20this%20end%2C%20we%20present%20an%20automatic%20best-suit%20training%0Asolution%20searching%20framework%2C%20the%20Dual-Carriageway%20Framework%20%28DCF%29%2C%20to%20fill%0Athis%20gap.%20DCF%20benefits%20from%20the%20design%20of%20a%20dual-direction%20search%20%28starting%0Afrom%20the%20pre-existing%20or%20the%20newly%20released%20dataset%29%20where%20five%20different%0Atraining%20settings%20are%20enforced.%20In%20addition%2C%20DCF%20is%20not%20only%20capable%20of%0Afiguring%20out%20the%20optimal%20training%20strategy%20with%20the%20capability%20of%20avoiding%0Aoverfitting%20but%20also%20yields%20built-in%20quantitative%20and%20visual%20explanations%0Aderived%20from%20the%20actual%20input%20and%20weights%20of%20the%20trained%20model.%20We%20validated%0ADCF%27s%20effectiveness%20through%20experiments%20with%20three%20convolutional%20neural%0Anetworks%20%28ResNet18%2C%20ResNet34%20and%20Inception-v3%29%20on%20two%20temporally%20continued%0Acommercial%20product%20datasets.%20Results%20showed%20fine-tuning%20pathways%20outperformed%0Atraining-from-scratch%20ones%20by%20up%20to%202.13%25%20and%201.23%25%20on%20the%20pre-existing%20and%20new%0Adatasets%2C%20respectively%2C%20in%20terms%20of%20mean%20accuracy.%20Furthermore%2C%20DCF%20identified%0Areflection%20padding%20as%20the%20superior%20padding%20method%2C%20enhancing%20testing%20accuracy%0Aby%203.72%25%20on%20average.%20This%20framework%20stands%20out%20for%20its%20potential%20to%20guide%20the%0Adevelopment%20of%20robust%20and%20explainable%20AI%20solutions%20in%20fine-grained%20visual%0Aclassification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Explainable%2520Fine-Grained%2520Visual%2520Classification%2520with%2520Transfer%250A%2520%2520Learning%253A%2520A%2520Dual-Carriageway%2520Framework%26entry.906535625%3DZheming%2520Zuo%2520and%2520Joseph%2520Smith%2520and%2520Jonathan%2520Stonehouse%2520and%2520Boguslaw%2520Obara%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520practical%2520fine-grained%2520visual%2520classification%2520applications%250Arooted%2520in%2520deep%2520learning%252C%2520a%2520common%2520scenario%2520involves%2520training%2520a%2520model%2520using%2520a%250Apre-existing%2520dataset.%2520Subsequently%252C%2520a%2520new%2520dataset%2520becomes%2520available%252C%2520prompting%250Athe%2520desire%2520to%2520make%2520a%2520pivotal%2520decision%2520for%2520achieving%2520enhanced%2520and%2520leveraged%250Ainference%2520performance%2520on%2520both%2520sides%253A%2520Should%2520one%2520opt%2520to%2520train%2520datasets%2520from%250Ascratch%2520or%2520fine-tune%2520the%2520model%2520trained%2520on%2520the%2520initial%2520dataset%2520using%2520the%2520newly%250Areleased%2520dataset%253F%2520The%2520existing%2520literature%2520reveals%2520a%2520lack%2520of%2520methods%2520to%250Asystematically%2520determine%2520the%2520optimal%2520training%2520strategy%252C%2520necessitating%250Aexplainability.%2520To%2520this%2520end%252C%2520we%2520present%2520an%2520automatic%2520best-suit%2520training%250Asolution%2520searching%2520framework%252C%2520the%2520Dual-Carriageway%2520Framework%2520%2528DCF%2529%252C%2520to%2520fill%250Athis%2520gap.%2520DCF%2520benefits%2520from%2520the%2520design%2520of%2520a%2520dual-direction%2520search%2520%2528starting%250Afrom%2520the%2520pre-existing%2520or%2520the%2520newly%2520released%2520dataset%2529%2520where%2520five%2520different%250Atraining%2520settings%2520are%2520enforced.%2520In%2520addition%252C%2520DCF%2520is%2520not%2520only%2520capable%2520of%250Afiguring%2520out%2520the%2520optimal%2520training%2520strategy%2520with%2520the%2520capability%2520of%2520avoiding%250Aoverfitting%2520but%2520also%2520yields%2520built-in%2520quantitative%2520and%2520visual%2520explanations%250Aderived%2520from%2520the%2520actual%2520input%2520and%2520weights%2520of%2520the%2520trained%2520model.%2520We%2520validated%250ADCF%2527s%2520effectiveness%2520through%2520experiments%2520with%2520three%2520convolutional%2520neural%250Anetworks%2520%2528ResNet18%252C%2520ResNet34%2520and%2520Inception-v3%2529%2520on%2520two%2520temporally%2520continued%250Acommercial%2520product%2520datasets.%2520Results%2520showed%2520fine-tuning%2520pathways%2520outperformed%250Atraining-from-scratch%2520ones%2520by%2520up%2520to%25202.13%2525%2520and%25201.23%2525%2520on%2520the%2520pre-existing%2520and%2520new%250Adatasets%252C%2520respectively%252C%2520in%2520terms%2520of%2520mean%2520accuracy.%2520Furthermore%252C%2520DCF%2520identified%250Areflection%2520padding%2520as%2520the%2520superior%2520padding%2520method%252C%2520enhancing%2520testing%2520accuracy%250Aby%25203.72%2525%2520on%2520average.%2520This%2520framework%2520stands%2520out%2520for%2520its%2520potential%2520to%2520guide%2520the%250Adevelopment%2520of%2520robust%2520and%2520explainable%2520AI%2520solutions%2520in%2520fine-grained%2520visual%250Aclassification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Explainable%20Fine-Grained%20Visual%20Classification%20with%20Transfer%0A%20%20Learning%3A%20A%20Dual-Carriageway%20Framework&entry.906535625=Zheming%20Zuo%20and%20Joseph%20Smith%20and%20Jonathan%20Stonehouse%20and%20Boguslaw%20Obara&entry.1292438233=%20%20In%20the%20realm%20of%20practical%20fine-grained%20visual%20classification%20applications%0Arooted%20in%20deep%20learning%2C%20a%20common%20scenario%20involves%20training%20a%20model%20using%20a%0Apre-existing%20dataset.%20Subsequently%2C%20a%20new%20dataset%20becomes%20available%2C%20prompting%0Athe%20desire%20to%20make%20a%20pivotal%20decision%20for%20achieving%20enhanced%20and%20leveraged%0Ainference%20performance%20on%20both%20sides%3A%20Should%20one%20opt%20to%20train%20datasets%20from%0Ascratch%20or%20fine-tune%20the%20model%20trained%20on%20the%20initial%20dataset%20using%20the%20newly%0Areleased%20dataset%3F%20The%20existing%20literature%20reveals%20a%20lack%20of%20methods%20to%0Asystematically%20determine%20the%20optimal%20training%20strategy%2C%20necessitating%0Aexplainability.%20To%20this%20end%2C%20we%20present%20an%20automatic%20best-suit%20training%0Asolution%20searching%20framework%2C%20the%20Dual-Carriageway%20Framework%20%28DCF%29%2C%20to%20fill%0Athis%20gap.%20DCF%20benefits%20from%20the%20design%20of%20a%20dual-direction%20search%20%28starting%0Afrom%20the%20pre-existing%20or%20the%20newly%20released%20dataset%29%20where%20five%20different%0Atraining%20settings%20are%20enforced.%20In%20addition%2C%20DCF%20is%20not%20only%20capable%20of%0Afiguring%20out%20the%20optimal%20training%20strategy%20with%20the%20capability%20of%20avoiding%0Aoverfitting%20but%20also%20yields%20built-in%20quantitative%20and%20visual%20explanations%0Aderived%20from%20the%20actual%20input%20and%20weights%20of%20the%20trained%20model.%20We%20validated%0ADCF%27s%20effectiveness%20through%20experiments%20with%20three%20convolutional%20neural%0Anetworks%20%28ResNet18%2C%20ResNet34%20and%20Inception-v3%29%20on%20two%20temporally%20continued%0Acommercial%20product%20datasets.%20Results%20showed%20fine-tuning%20pathways%20outperformed%0Atraining-from-scratch%20ones%20by%20up%20to%202.13%25%20and%201.23%25%20on%20the%20pre-existing%20and%20new%0Adatasets%2C%20respectively%2C%20in%20terms%20of%20mean%20accuracy.%20Furthermore%2C%20DCF%20identified%0Areflection%20padding%20as%20the%20superior%20padding%20method%2C%20enhancing%20testing%20accuracy%0Aby%203.72%25%20on%20average.%20This%20framework%20stands%20out%20for%20its%20potential%20to%20guide%20the%0Adevelopment%20of%20robust%20and%20explainable%20AI%20solutions%20in%20fine-grained%20visual%0Aclassification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05853v1&entry.124074799=Read"},
{"title": "DRSI-Net: Dual-Residual Spatial Interaction Network for Multi-Person\n  Pose Estimation", "author": "Shang Wu and Bin Wang", "abstract": "  Multi-person pose estimation (MPPE), which aims to locate the key points for\nall persons in the frames, is an active research branch of computer vision.\nVariable human poses and complex scenes make MPPE dependent on local details\nand global structures; their absence may cause key point feature misalignment.\nIn this case, high-order spatial interactions that can effectively link the\nlocal and global information of features are particularly important. However,\nmost methods do not include spatial interactions. A few methods have low-order\nspatial interactions, but achieving a good balance between accuracy and\ncomplexity is challenging. To address the above problems, a dual-residual\nspatial interaction network (DRSI-Net) for MPPE with high accuracy and low\ncomplexity is proposed herein. Compared to other methods, DRSI-Net recursively\nperforms residual spatial information interactions on the neighbouring features\nso that more useful spatial information can be retained and more similarities\ncan be obtained between shallow and deep extracted features. The channel and\nspatial dual attention mechanism introduced in the multi-scale feature fusion\nalso helps the network to adaptively focus on features relevant to the target\nkey points and further refine the generated poses. Simultaneously, by\noptimising the interactive channel dimensions and dividing the gradient flow,\nthe spatial interaction module is designed to be lightweight, thus reducing the\ncomplexity of the network. According to the experimental results on the COCO\ndataset, the proposed DRSI-Net outperforms other state-of-the-art methods in\naccuracy and complexity.\n", "link": "http://arxiv.org/abs/2402.16640v2", "date": "2024-05-09", "relevancy": 2.8086, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6149}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5354}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRSI-Net%3A%20Dual-Residual%20Spatial%20Interaction%20Network%20for%20Multi-Person%0A%20%20Pose%20Estimation&body=Title%3A%20DRSI-Net%3A%20Dual-Residual%20Spatial%20Interaction%20Network%20for%20Multi-Person%0A%20%20Pose%20Estimation%0AAuthor%3A%20Shang%20Wu%20and%20Bin%20Wang%0AAbstract%3A%20%20%20Multi-person%20pose%20estimation%20%28MPPE%29%2C%20which%20aims%20to%20locate%20the%20key%20points%20for%0Aall%20persons%20in%20the%20frames%2C%20is%20an%20active%20research%20branch%20of%20computer%20vision.%0AVariable%20human%20poses%20and%20complex%20scenes%20make%20MPPE%20dependent%20on%20local%20details%0Aand%20global%20structures%3B%20their%20absence%20may%20cause%20key%20point%20feature%20misalignment.%0AIn%20this%20case%2C%20high-order%20spatial%20interactions%20that%20can%20effectively%20link%20the%0Alocal%20and%20global%20information%20of%20features%20are%20particularly%20important.%20However%2C%0Amost%20methods%20do%20not%20include%20spatial%20interactions.%20A%20few%20methods%20have%20low-order%0Aspatial%20interactions%2C%20but%20achieving%20a%20good%20balance%20between%20accuracy%20and%0Acomplexity%20is%20challenging.%20To%20address%20the%20above%20problems%2C%20a%20dual-residual%0Aspatial%20interaction%20network%20%28DRSI-Net%29%20for%20MPPE%20with%20high%20accuracy%20and%20low%0Acomplexity%20is%20proposed%20herein.%20Compared%20to%20other%20methods%2C%20DRSI-Net%20recursively%0Aperforms%20residual%20spatial%20information%20interactions%20on%20the%20neighbouring%20features%0Aso%20that%20more%20useful%20spatial%20information%20can%20be%20retained%20and%20more%20similarities%0Acan%20be%20obtained%20between%20shallow%20and%20deep%20extracted%20features.%20The%20channel%20and%0Aspatial%20dual%20attention%20mechanism%20introduced%20in%20the%20multi-scale%20feature%20fusion%0Aalso%20helps%20the%20network%20to%20adaptively%20focus%20on%20features%20relevant%20to%20the%20target%0Akey%20points%20and%20further%20refine%20the%20generated%20poses.%20Simultaneously%2C%20by%0Aoptimising%20the%20interactive%20channel%20dimensions%20and%20dividing%20the%20gradient%20flow%2C%0Athe%20spatial%20interaction%20module%20is%20designed%20to%20be%20lightweight%2C%20thus%20reducing%20the%0Acomplexity%20of%20the%20network.%20According%20to%20the%20experimental%20results%20on%20the%20COCO%0Adataset%2C%20the%20proposed%20DRSI-Net%20outperforms%20other%20state-of-the-art%20methods%20in%0Aaccuracy%20and%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRSI-Net%253A%2520Dual-Residual%2520Spatial%2520Interaction%2520Network%2520for%2520Multi-Person%250A%2520%2520Pose%2520Estimation%26entry.906535625%3DShang%2520Wu%2520and%2520Bin%2520Wang%26entry.1292438233%3D%2520%2520Multi-person%2520pose%2520estimation%2520%2528MPPE%2529%252C%2520which%2520aims%2520to%2520locate%2520the%2520key%2520points%2520for%250Aall%2520persons%2520in%2520the%2520frames%252C%2520is%2520an%2520active%2520research%2520branch%2520of%2520computer%2520vision.%250AVariable%2520human%2520poses%2520and%2520complex%2520scenes%2520make%2520MPPE%2520dependent%2520on%2520local%2520details%250Aand%2520global%2520structures%253B%2520their%2520absence%2520may%2520cause%2520key%2520point%2520feature%2520misalignment.%250AIn%2520this%2520case%252C%2520high-order%2520spatial%2520interactions%2520that%2520can%2520effectively%2520link%2520the%250Alocal%2520and%2520global%2520information%2520of%2520features%2520are%2520particularly%2520important.%2520However%252C%250Amost%2520methods%2520do%2520not%2520include%2520spatial%2520interactions.%2520A%2520few%2520methods%2520have%2520low-order%250Aspatial%2520interactions%252C%2520but%2520achieving%2520a%2520good%2520balance%2520between%2520accuracy%2520and%250Acomplexity%2520is%2520challenging.%2520To%2520address%2520the%2520above%2520problems%252C%2520a%2520dual-residual%250Aspatial%2520interaction%2520network%2520%2528DRSI-Net%2529%2520for%2520MPPE%2520with%2520high%2520accuracy%2520and%2520low%250Acomplexity%2520is%2520proposed%2520herein.%2520Compared%2520to%2520other%2520methods%252C%2520DRSI-Net%2520recursively%250Aperforms%2520residual%2520spatial%2520information%2520interactions%2520on%2520the%2520neighbouring%2520features%250Aso%2520that%2520more%2520useful%2520spatial%2520information%2520can%2520be%2520retained%2520and%2520more%2520similarities%250Acan%2520be%2520obtained%2520between%2520shallow%2520and%2520deep%2520extracted%2520features.%2520The%2520channel%2520and%250Aspatial%2520dual%2520attention%2520mechanism%2520introduced%2520in%2520the%2520multi-scale%2520feature%2520fusion%250Aalso%2520helps%2520the%2520network%2520to%2520adaptively%2520focus%2520on%2520features%2520relevant%2520to%2520the%2520target%250Akey%2520points%2520and%2520further%2520refine%2520the%2520generated%2520poses.%2520Simultaneously%252C%2520by%250Aoptimising%2520the%2520interactive%2520channel%2520dimensions%2520and%2520dividing%2520the%2520gradient%2520flow%252C%250Athe%2520spatial%2520interaction%2520module%2520is%2520designed%2520to%2520be%2520lightweight%252C%2520thus%2520reducing%2520the%250Acomplexity%2520of%2520the%2520network.%2520According%2520to%2520the%2520experimental%2520results%2520on%2520the%2520COCO%250Adataset%252C%2520the%2520proposed%2520DRSI-Net%2520outperforms%2520other%2520state-of-the-art%2520methods%2520in%250Aaccuracy%2520and%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRSI-Net%3A%20Dual-Residual%20Spatial%20Interaction%20Network%20for%20Multi-Person%0A%20%20Pose%20Estimation&entry.906535625=Shang%20Wu%20and%20Bin%20Wang&entry.1292438233=%20%20Multi-person%20pose%20estimation%20%28MPPE%29%2C%20which%20aims%20to%20locate%20the%20key%20points%20for%0Aall%20persons%20in%20the%20frames%2C%20is%20an%20active%20research%20branch%20of%20computer%20vision.%0AVariable%20human%20poses%20and%20complex%20scenes%20make%20MPPE%20dependent%20on%20local%20details%0Aand%20global%20structures%3B%20their%20absence%20may%20cause%20key%20point%20feature%20misalignment.%0AIn%20this%20case%2C%20high-order%20spatial%20interactions%20that%20can%20effectively%20link%20the%0Alocal%20and%20global%20information%20of%20features%20are%20particularly%20important.%20However%2C%0Amost%20methods%20do%20not%20include%20spatial%20interactions.%20A%20few%20methods%20have%20low-order%0Aspatial%20interactions%2C%20but%20achieving%20a%20good%20balance%20between%20accuracy%20and%0Acomplexity%20is%20challenging.%20To%20address%20the%20above%20problems%2C%20a%20dual-residual%0Aspatial%20interaction%20network%20%28DRSI-Net%29%20for%20MPPE%20with%20high%20accuracy%20and%20low%0Acomplexity%20is%20proposed%20herein.%20Compared%20to%20other%20methods%2C%20DRSI-Net%20recursively%0Aperforms%20residual%20spatial%20information%20interactions%20on%20the%20neighbouring%20features%0Aso%20that%20more%20useful%20spatial%20information%20can%20be%20retained%20and%20more%20similarities%0Acan%20be%20obtained%20between%20shallow%20and%20deep%20extracted%20features.%20The%20channel%20and%0Aspatial%20dual%20attention%20mechanism%20introduced%20in%20the%20multi-scale%20feature%20fusion%0Aalso%20helps%20the%20network%20to%20adaptively%20focus%20on%20features%20relevant%20to%20the%20target%0Akey%20points%20and%20further%20refine%20the%20generated%20poses.%20Simultaneously%2C%20by%0Aoptimising%20the%20interactive%20channel%20dimensions%20and%20dividing%20the%20gradient%20flow%2C%0Athe%20spatial%20interaction%20module%20is%20designed%20to%20be%20lightweight%2C%20thus%20reducing%20the%0Acomplexity%20of%20the%20network.%20According%20to%20the%20experimental%20results%20on%20the%20COCO%0Adataset%2C%20the%20proposed%20DRSI-Net%20outperforms%20other%20state-of-the-art%20methods%20in%0Aaccuracy%20and%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16640v2&entry.124074799=Read"},
{"title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal\n  for Rapid Inference", "author": "Zhihang Lin and Mingbao Lin and Luxi Lin and Rongrong Ji", "abstract": "  Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nnot necessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for vision tokens withdrawal, we initially analyze\na limited set of tiny datasets and choose the first layer that meets the\nKullback-Leibler divergence criterion. Our VTW approach can cut computational\noverhead by over 40\\% across diverse multimodal tasks while maintaining\nperformance. Our code is released at https://github.com/lzhxmu/VTW.\n", "link": "http://arxiv.org/abs/2405.05803v1", "date": "2024-05-09", "relevancy": 2.7183, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5527}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5455}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Multimodal%20Large%20Language%20Models%20with%20Visual%20Tokens%20Withdrawal%0A%20%20for%20Rapid%20Inference&body=Title%3A%20Boosting%20Multimodal%20Large%20Language%20Models%20with%20Visual%20Tokens%20Withdrawal%0A%20%20for%20Rapid%20Inference%0AAuthor%3A%20Zhihang%20Lin%20and%20Mingbao%20Lin%20and%20Luxi%20Lin%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20demand%20considerable%20computations%20for%0Ainference%20due%20to%20the%20extensive%20parameters%20and%20the%20additional%20input%20tokens%0Aneeded%20for%20visual%20information%20representation.%20Herein%2C%20we%20introduce%20Visual%0ATokens%20Withdrawal%20%28VTW%29%2C%20a%20plug-and-play%20module%20to%20boost%20MLLMs%20for%20rapid%0Ainference.%20Our%20approach%20is%20inspired%20by%20two%20intriguing%20phenomena%20we%20have%0Aobserved%3A%20%281%29%20the%20attention%20sink%20phenomenon%20that%20is%20prevalent%20in%20LLMs%20also%0Apersists%20in%20MLLMs%2C%20suggesting%20that%20initial%20tokens%20and%20nearest%20tokens%20receive%0Athe%20majority%20of%20attention%2C%20while%20middle%20vision%20tokens%20garner%20minimal%20attention%0Ain%20deep%20layers%3B%20%282%29%20the%20presence%20of%20information%20migration%2C%20which%20implies%20that%0Avisual%20information%20is%20transferred%20to%20subsequent%20text%20tokens%20within%20the%20first%0Afew%20layers%20of%20MLLMs.%20As%20per%20our%20findings%2C%20we%20conclude%20that%20vision%20tokens%20are%0Anot%20necessary%20in%20the%20deep%20layers%20of%20MLLMs.%20Thus%2C%20we%20strategically%20withdraw%20them%0Aat%20a%20certain%20layer%2C%20enabling%20only%20text%20tokens%20to%20engage%20in%20subsequent%20layers.%0ATo%20pinpoint%20the%20ideal%20layer%20for%20vision%20tokens%20withdrawal%2C%20we%20initially%20analyze%0Aa%20limited%20set%20of%20tiny%20datasets%20and%20choose%20the%20first%20layer%20that%20meets%20the%0AKullback-Leibler%20divergence%20criterion.%20Our%20VTW%20approach%20can%20cut%20computational%0Aoverhead%20by%20over%2040%5C%25%20across%20diverse%20multimodal%20tasks%20while%20maintaining%0Aperformance.%20Our%20code%20is%20released%20at%20https%3A//github.com/lzhxmu/VTW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Multimodal%2520Large%2520Language%2520Models%2520with%2520Visual%2520Tokens%2520Withdrawal%250A%2520%2520for%2520Rapid%2520Inference%26entry.906535625%3DZhihang%2520Lin%2520and%2520Mingbao%2520Lin%2520and%2520Luxi%2520Lin%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520demand%2520considerable%2520computations%2520for%250Ainference%2520due%2520to%2520the%2520extensive%2520parameters%2520and%2520the%2520additional%2520input%2520tokens%250Aneeded%2520for%2520visual%2520information%2520representation.%2520Herein%252C%2520we%2520introduce%2520Visual%250ATokens%2520Withdrawal%2520%2528VTW%2529%252C%2520a%2520plug-and-play%2520module%2520to%2520boost%2520MLLMs%2520for%2520rapid%250Ainference.%2520Our%2520approach%2520is%2520inspired%2520by%2520two%2520intriguing%2520phenomena%2520we%2520have%250Aobserved%253A%2520%25281%2529%2520the%2520attention%2520sink%2520phenomenon%2520that%2520is%2520prevalent%2520in%2520LLMs%2520also%250Apersists%2520in%2520MLLMs%252C%2520suggesting%2520that%2520initial%2520tokens%2520and%2520nearest%2520tokens%2520receive%250Athe%2520majority%2520of%2520attention%252C%2520while%2520middle%2520vision%2520tokens%2520garner%2520minimal%2520attention%250Ain%2520deep%2520layers%253B%2520%25282%2529%2520the%2520presence%2520of%2520information%2520migration%252C%2520which%2520implies%2520that%250Avisual%2520information%2520is%2520transferred%2520to%2520subsequent%2520text%2520tokens%2520within%2520the%2520first%250Afew%2520layers%2520of%2520MLLMs.%2520As%2520per%2520our%2520findings%252C%2520we%2520conclude%2520that%2520vision%2520tokens%2520are%250Anot%2520necessary%2520in%2520the%2520deep%2520layers%2520of%2520MLLMs.%2520Thus%252C%2520we%2520strategically%2520withdraw%2520them%250Aat%2520a%2520certain%2520layer%252C%2520enabling%2520only%2520text%2520tokens%2520to%2520engage%2520in%2520subsequent%2520layers.%250ATo%2520pinpoint%2520the%2520ideal%2520layer%2520for%2520vision%2520tokens%2520withdrawal%252C%2520we%2520initially%2520analyze%250Aa%2520limited%2520set%2520of%2520tiny%2520datasets%2520and%2520choose%2520the%2520first%2520layer%2520that%2520meets%2520the%250AKullback-Leibler%2520divergence%2520criterion.%2520Our%2520VTW%2520approach%2520can%2520cut%2520computational%250Aoverhead%2520by%2520over%252040%255C%2525%2520across%2520diverse%2520multimodal%2520tasks%2520while%2520maintaining%250Aperformance.%2520Our%2520code%2520is%2520released%2520at%2520https%253A//github.com/lzhxmu/VTW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Multimodal%20Large%20Language%20Models%20with%20Visual%20Tokens%20Withdrawal%0A%20%20for%20Rapid%20Inference&entry.906535625=Zhihang%20Lin%20and%20Mingbao%20Lin%20and%20Luxi%20Lin%20and%20Rongrong%20Ji&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20demand%20considerable%20computations%20for%0Ainference%20due%20to%20the%20extensive%20parameters%20and%20the%20additional%20input%20tokens%0Aneeded%20for%20visual%20information%20representation.%20Herein%2C%20we%20introduce%20Visual%0ATokens%20Withdrawal%20%28VTW%29%2C%20a%20plug-and-play%20module%20to%20boost%20MLLMs%20for%20rapid%0Ainference.%20Our%20approach%20is%20inspired%20by%20two%20intriguing%20phenomena%20we%20have%0Aobserved%3A%20%281%29%20the%20attention%20sink%20phenomenon%20that%20is%20prevalent%20in%20LLMs%20also%0Apersists%20in%20MLLMs%2C%20suggesting%20that%20initial%20tokens%20and%20nearest%20tokens%20receive%0Athe%20majority%20of%20attention%2C%20while%20middle%20vision%20tokens%20garner%20minimal%20attention%0Ain%20deep%20layers%3B%20%282%29%20the%20presence%20of%20information%20migration%2C%20which%20implies%20that%0Avisual%20information%20is%20transferred%20to%20subsequent%20text%20tokens%20within%20the%20first%0Afew%20layers%20of%20MLLMs.%20As%20per%20our%20findings%2C%20we%20conclude%20that%20vision%20tokens%20are%0Anot%20necessary%20in%20the%20deep%20layers%20of%20MLLMs.%20Thus%2C%20we%20strategically%20withdraw%20them%0Aat%20a%20certain%20layer%2C%20enabling%20only%20text%20tokens%20to%20engage%20in%20subsequent%20layers.%0ATo%20pinpoint%20the%20ideal%20layer%20for%20vision%20tokens%20withdrawal%2C%20we%20initially%20analyze%0Aa%20limited%20set%20of%20tiny%20datasets%20and%20choose%20the%20first%20layer%20that%20meets%20the%0AKullback-Leibler%20divergence%20criterion.%20Our%20VTW%20approach%20can%20cut%20computational%0Aoverhead%20by%20over%2040%5C%25%20across%20diverse%20multimodal%20tasks%20while%20maintaining%0Aperformance.%20Our%20code%20is%20released%20at%20https%3A//github.com/lzhxmu/VTW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05803v1&entry.124074799=Read"},
{"title": "FlockGPT: Guiding UAV Flocking with Linguistic Orchestration", "author": "Artem Lykov and Sausar Karaf and Mikhail Martynov and Valerii Serpiva and Aleksey Fedoseev and Mikhail Konenkov and Dzmitry Tsetserukou", "abstract": "  This article presents the world's first rapid drone flocking control using\nnatural language through generative AI. The described approach enables the\nintuitive orchestration of a flock of any size to achieve the desired geometry.\nThe key feature of the method is the development of a new interface based on\nLarge Language Models to communicate with the user and to generate the target\ngeometry descriptions. Users can interactively modify or provide comments\nduring the construction of the flock geometry model. By combining flocking\ntechnology and defining the target surface using a signed distance function,\nsmooth and adaptive movement of the drone swarm between target states is\nachieved.\n  Our user study on FlockGPT confirmed a high level of intuitive control over\ndrone flocking by users. Subjects who had never previously controlled a swarm\nof drones were able to construct complex figures in just a few iterations and\nwere able to accurately distinguish the formed swarm drone figures. The results\nrevealed a high recognition rate for six different geometric patterns generated\nthrough the LLM-based interface and performed by a simulated drone flock (mean\nof 80% with a maximum of 93\\% for cube and tetrahedron patterns). Users\ncommented on low temporal demand (19.2 score in NASA-TLX), high performance (26\nscore in NASA-TLX), attractiveness (1.94 UEQ score), and hedonic quality (1.81\nUEQ score) of the developed system. The FlockGPT demo code repository can be\nfound at: coming soon\n", "link": "http://arxiv.org/abs/2405.05872v1", "date": "2024-05-09", "relevancy": 2.6131, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5321}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5268}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlockGPT%3A%20Guiding%20UAV%20Flocking%20with%20Linguistic%20Orchestration&body=Title%3A%20FlockGPT%3A%20Guiding%20UAV%20Flocking%20with%20Linguistic%20Orchestration%0AAuthor%3A%20Artem%20Lykov%20and%20Sausar%20Karaf%20and%20Mikhail%20Martynov%20and%20Valerii%20Serpiva%20and%20Aleksey%20Fedoseev%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20article%20presents%20the%20world%27s%20first%20rapid%20drone%20flocking%20control%20using%0Anatural%20language%20through%20generative%20AI.%20The%20described%20approach%20enables%20the%0Aintuitive%20orchestration%20of%20a%20flock%20of%20any%20size%20to%20achieve%20the%20desired%20geometry.%0AThe%20key%20feature%20of%20the%20method%20is%20the%20development%20of%20a%20new%20interface%20based%20on%0ALarge%20Language%20Models%20to%20communicate%20with%20the%20user%20and%20to%20generate%20the%20target%0Ageometry%20descriptions.%20Users%20can%20interactively%20modify%20or%20provide%20comments%0Aduring%20the%20construction%20of%20the%20flock%20geometry%20model.%20By%20combining%20flocking%0Atechnology%20and%20defining%20the%20target%20surface%20using%20a%20signed%20distance%20function%2C%0Asmooth%20and%20adaptive%20movement%20of%20the%20drone%20swarm%20between%20target%20states%20is%0Aachieved.%0A%20%20Our%20user%20study%20on%20FlockGPT%20confirmed%20a%20high%20level%20of%20intuitive%20control%20over%0Adrone%20flocking%20by%20users.%20Subjects%20who%20had%20never%20previously%20controlled%20a%20swarm%0Aof%20drones%20were%20able%20to%20construct%20complex%20figures%20in%20just%20a%20few%20iterations%20and%0Awere%20able%20to%20accurately%20distinguish%20the%20formed%20swarm%20drone%20figures.%20The%20results%0Arevealed%20a%20high%20recognition%20rate%20for%20six%20different%20geometric%20patterns%20generated%0Athrough%20the%20LLM-based%20interface%20and%20performed%20by%20a%20simulated%20drone%20flock%20%28mean%0Aof%2080%25%20with%20a%20maximum%20of%2093%5C%25%20for%20cube%20and%20tetrahedron%20patterns%29.%20Users%0Acommented%20on%20low%20temporal%20demand%20%2819.2%20score%20in%20NASA-TLX%29%2C%20high%20performance%20%2826%0Ascore%20in%20NASA-TLX%29%2C%20attractiveness%20%281.94%20UEQ%20score%29%2C%20and%20hedonic%20quality%20%281.81%0AUEQ%20score%29%20of%20the%20developed%20system.%20The%20FlockGPT%20demo%20code%20repository%20can%20be%0Afound%20at%3A%20coming%20soon%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlockGPT%253A%2520Guiding%2520UAV%2520Flocking%2520with%2520Linguistic%2520Orchestration%26entry.906535625%3DArtem%2520Lykov%2520and%2520Sausar%2520Karaf%2520and%2520Mikhail%2520Martynov%2520and%2520Valerii%2520Serpiva%2520and%2520Aleksey%2520Fedoseev%2520and%2520Mikhail%2520Konenkov%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520article%2520presents%2520the%2520world%2527s%2520first%2520rapid%2520drone%2520flocking%2520control%2520using%250Anatural%2520language%2520through%2520generative%2520AI.%2520The%2520described%2520approach%2520enables%2520the%250Aintuitive%2520orchestration%2520of%2520a%2520flock%2520of%2520any%2520size%2520to%2520achieve%2520the%2520desired%2520geometry.%250AThe%2520key%2520feature%2520of%2520the%2520method%2520is%2520the%2520development%2520of%2520a%2520new%2520interface%2520based%2520on%250ALarge%2520Language%2520Models%2520to%2520communicate%2520with%2520the%2520user%2520and%2520to%2520generate%2520the%2520target%250Ageometry%2520descriptions.%2520Users%2520can%2520interactively%2520modify%2520or%2520provide%2520comments%250Aduring%2520the%2520construction%2520of%2520the%2520flock%2520geometry%2520model.%2520By%2520combining%2520flocking%250Atechnology%2520and%2520defining%2520the%2520target%2520surface%2520using%2520a%2520signed%2520distance%2520function%252C%250Asmooth%2520and%2520adaptive%2520movement%2520of%2520the%2520drone%2520swarm%2520between%2520target%2520states%2520is%250Aachieved.%250A%2520%2520Our%2520user%2520study%2520on%2520FlockGPT%2520confirmed%2520a%2520high%2520level%2520of%2520intuitive%2520control%2520over%250Adrone%2520flocking%2520by%2520users.%2520Subjects%2520who%2520had%2520never%2520previously%2520controlled%2520a%2520swarm%250Aof%2520drones%2520were%2520able%2520to%2520construct%2520complex%2520figures%2520in%2520just%2520a%2520few%2520iterations%2520and%250Awere%2520able%2520to%2520accurately%2520distinguish%2520the%2520formed%2520swarm%2520drone%2520figures.%2520The%2520results%250Arevealed%2520a%2520high%2520recognition%2520rate%2520for%2520six%2520different%2520geometric%2520patterns%2520generated%250Athrough%2520the%2520LLM-based%2520interface%2520and%2520performed%2520by%2520a%2520simulated%2520drone%2520flock%2520%2528mean%250Aof%252080%2525%2520with%2520a%2520maximum%2520of%252093%255C%2525%2520for%2520cube%2520and%2520tetrahedron%2520patterns%2529.%2520Users%250Acommented%2520on%2520low%2520temporal%2520demand%2520%252819.2%2520score%2520in%2520NASA-TLX%2529%252C%2520high%2520performance%2520%252826%250Ascore%2520in%2520NASA-TLX%2529%252C%2520attractiveness%2520%25281.94%2520UEQ%2520score%2529%252C%2520and%2520hedonic%2520quality%2520%25281.81%250AUEQ%2520score%2529%2520of%2520the%2520developed%2520system.%2520The%2520FlockGPT%2520demo%2520code%2520repository%2520can%2520be%250Afound%2520at%253A%2520coming%2520soon%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlockGPT%3A%20Guiding%20UAV%20Flocking%20with%20Linguistic%20Orchestration&entry.906535625=Artem%20Lykov%20and%20Sausar%20Karaf%20and%20Mikhail%20Martynov%20and%20Valerii%20Serpiva%20and%20Aleksey%20Fedoseev%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20article%20presents%20the%20world%27s%20first%20rapid%20drone%20flocking%20control%20using%0Anatural%20language%20through%20generative%20AI.%20The%20described%20approach%20enables%20the%0Aintuitive%20orchestration%20of%20a%20flock%20of%20any%20size%20to%20achieve%20the%20desired%20geometry.%0AThe%20key%20feature%20of%20the%20method%20is%20the%20development%20of%20a%20new%20interface%20based%20on%0ALarge%20Language%20Models%20to%20communicate%20with%20the%20user%20and%20to%20generate%20the%20target%0Ageometry%20descriptions.%20Users%20can%20interactively%20modify%20or%20provide%20comments%0Aduring%20the%20construction%20of%20the%20flock%20geometry%20model.%20By%20combining%20flocking%0Atechnology%20and%20defining%20the%20target%20surface%20using%20a%20signed%20distance%20function%2C%0Asmooth%20and%20adaptive%20movement%20of%20the%20drone%20swarm%20between%20target%20states%20is%0Aachieved.%0A%20%20Our%20user%20study%20on%20FlockGPT%20confirmed%20a%20high%20level%20of%20intuitive%20control%20over%0Adrone%20flocking%20by%20users.%20Subjects%20who%20had%20never%20previously%20controlled%20a%20swarm%0Aof%20drones%20were%20able%20to%20construct%20complex%20figures%20in%20just%20a%20few%20iterations%20and%0Awere%20able%20to%20accurately%20distinguish%20the%20formed%20swarm%20drone%20figures.%20The%20results%0Arevealed%20a%20high%20recognition%20rate%20for%20six%20different%20geometric%20patterns%20generated%0Athrough%20the%20LLM-based%20interface%20and%20performed%20by%20a%20simulated%20drone%20flock%20%28mean%0Aof%2080%25%20with%20a%20maximum%20of%2093%5C%25%20for%20cube%20and%20tetrahedron%20patterns%29.%20Users%0Acommented%20on%20low%20temporal%20demand%20%2819.2%20score%20in%20NASA-TLX%29%2C%20high%20performance%20%2826%0Ascore%20in%20NASA-TLX%29%2C%20attractiveness%20%281.94%20UEQ%20score%29%2C%20and%20hedonic%20quality%20%281.81%0AUEQ%20score%29%20of%20the%20developed%20system.%20The%20FlockGPT%20demo%20code%20repository%20can%20be%0Afound%20at%3A%20coming%20soon%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05872v1&entry.124074799=Read"},
{"title": "T-Rep: Representation Learning for Time Series using Time-Embeddings", "author": "Archibald Fraikin and Adrien Bennetot and St\u00e9phanie Allassonni\u00e8re", "abstract": "  Multivariate time series present challenges to standard machine learning\ntechniques, as they are often unlabeled, high dimensional, noisy, and contain\nmissing data. To address this, we propose T-Rep, a self-supervised method to\nlearn time series representations at a timestep granularity. T-Rep learns\nvector embeddings of time alongside its feature extractor, to extract temporal\nfeatures such as trend, periodicity, or distribution shifts from the signal.\nThese time-embeddings are leveraged in pretext tasks, to incorporate smooth and\nfine-grained temporal dependencies in the representations, as well as reinforce\nrobustness to missing data. We evaluate T-Rep on downstream classification,\nforecasting, and anomaly detection tasks. It is compared to existing\nself-supervised algorithms for time series, which it outperforms in all three\ntasks. We test T-Rep in missing data regimes, where it proves more resilient\nthan its counterparts. Finally, we provide latent space visualisation\nexperiments, highlighting the interpretability of the learned representations.\n", "link": "http://arxiv.org/abs/2310.04486v3", "date": "2024-05-09", "relevancy": 2.5349, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-Rep%3A%20Representation%20Learning%20for%20Time%20Series%20using%20Time-Embeddings&body=Title%3A%20T-Rep%3A%20Representation%20Learning%20for%20Time%20Series%20using%20Time-Embeddings%0AAuthor%3A%20Archibald%20Fraikin%20and%20Adrien%20Bennetot%20and%20St%C3%A9phanie%20Allassonni%C3%A8re%0AAbstract%3A%20%20%20Multivariate%20time%20series%20present%20challenges%20to%20standard%20machine%20learning%0Atechniques%2C%20as%20they%20are%20often%20unlabeled%2C%20high%20dimensional%2C%20noisy%2C%20and%20contain%0Amissing%20data.%20To%20address%20this%2C%20we%20propose%20T-Rep%2C%20a%20self-supervised%20method%20to%0Alearn%20time%20series%20representations%20at%20a%20timestep%20granularity.%20T-Rep%20learns%0Avector%20embeddings%20of%20time%20alongside%20its%20feature%20extractor%2C%20to%20extract%20temporal%0Afeatures%20such%20as%20trend%2C%20periodicity%2C%20or%20distribution%20shifts%20from%20the%20signal.%0AThese%20time-embeddings%20are%20leveraged%20in%20pretext%20tasks%2C%20to%20incorporate%20smooth%20and%0Afine-grained%20temporal%20dependencies%20in%20the%20representations%2C%20as%20well%20as%20reinforce%0Arobustness%20to%20missing%20data.%20We%20evaluate%20T-Rep%20on%20downstream%20classification%2C%0Aforecasting%2C%20and%20anomaly%20detection%20tasks.%20It%20is%20compared%20to%20existing%0Aself-supervised%20algorithms%20for%20time%20series%2C%20which%20it%20outperforms%20in%20all%20three%0Atasks.%20We%20test%20T-Rep%20in%20missing%20data%20regimes%2C%20where%20it%20proves%20more%20resilient%0Athan%20its%20counterparts.%20Finally%2C%20we%20provide%20latent%20space%20visualisation%0Aexperiments%2C%20highlighting%20the%20interpretability%20of%20the%20learned%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.04486v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-Rep%253A%2520Representation%2520Learning%2520for%2520Time%2520Series%2520using%2520Time-Embeddings%26entry.906535625%3DArchibald%2520Fraikin%2520and%2520Adrien%2520Bennetot%2520and%2520St%25C3%25A9phanie%2520Allassonni%25C3%25A8re%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520present%2520challenges%2520to%2520standard%2520machine%2520learning%250Atechniques%252C%2520as%2520they%2520are%2520often%2520unlabeled%252C%2520high%2520dimensional%252C%2520noisy%252C%2520and%2520contain%250Amissing%2520data.%2520To%2520address%2520this%252C%2520we%2520propose%2520T-Rep%252C%2520a%2520self-supervised%2520method%2520to%250Alearn%2520time%2520series%2520representations%2520at%2520a%2520timestep%2520granularity.%2520T-Rep%2520learns%250Avector%2520embeddings%2520of%2520time%2520alongside%2520its%2520feature%2520extractor%252C%2520to%2520extract%2520temporal%250Afeatures%2520such%2520as%2520trend%252C%2520periodicity%252C%2520or%2520distribution%2520shifts%2520from%2520the%2520signal.%250AThese%2520time-embeddings%2520are%2520leveraged%2520in%2520pretext%2520tasks%252C%2520to%2520incorporate%2520smooth%2520and%250Afine-grained%2520temporal%2520dependencies%2520in%2520the%2520representations%252C%2520as%2520well%2520as%2520reinforce%250Arobustness%2520to%2520missing%2520data.%2520We%2520evaluate%2520T-Rep%2520on%2520downstream%2520classification%252C%250Aforecasting%252C%2520and%2520anomaly%2520detection%2520tasks.%2520It%2520is%2520compared%2520to%2520existing%250Aself-supervised%2520algorithms%2520for%2520time%2520series%252C%2520which%2520it%2520outperforms%2520in%2520all%2520three%250Atasks.%2520We%2520test%2520T-Rep%2520in%2520missing%2520data%2520regimes%252C%2520where%2520it%2520proves%2520more%2520resilient%250Athan%2520its%2520counterparts.%2520Finally%252C%2520we%2520provide%2520latent%2520space%2520visualisation%250Aexperiments%252C%2520highlighting%2520the%2520interpretability%2520of%2520the%2520learned%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.04486v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-Rep%3A%20Representation%20Learning%20for%20Time%20Series%20using%20Time-Embeddings&entry.906535625=Archibald%20Fraikin%20and%20Adrien%20Bennetot%20and%20St%C3%A9phanie%20Allassonni%C3%A8re&entry.1292438233=%20%20Multivariate%20time%20series%20present%20challenges%20to%20standard%20machine%20learning%0Atechniques%2C%20as%20they%20are%20often%20unlabeled%2C%20high%20dimensional%2C%20noisy%2C%20and%20contain%0Amissing%20data.%20To%20address%20this%2C%20we%20propose%20T-Rep%2C%20a%20self-supervised%20method%20to%0Alearn%20time%20series%20representations%20at%20a%20timestep%20granularity.%20T-Rep%20learns%0Avector%20embeddings%20of%20time%20alongside%20its%20feature%20extractor%2C%20to%20extract%20temporal%0Afeatures%20such%20as%20trend%2C%20periodicity%2C%20or%20distribution%20shifts%20from%20the%20signal.%0AThese%20time-embeddings%20are%20leveraged%20in%20pretext%20tasks%2C%20to%20incorporate%20smooth%20and%0Afine-grained%20temporal%20dependencies%20in%20the%20representations%2C%20as%20well%20as%20reinforce%0Arobustness%20to%20missing%20data.%20We%20evaluate%20T-Rep%20on%20downstream%20classification%2C%0Aforecasting%2C%20and%20anomaly%20detection%20tasks.%20It%20is%20compared%20to%20existing%0Aself-supervised%20algorithms%20for%20time%20series%2C%20which%20it%20outperforms%20in%20all%20three%0Atasks.%20We%20test%20T-Rep%20in%20missing%20data%20regimes%2C%20where%20it%20proves%20more%20resilient%0Athan%20its%20counterparts.%20Finally%2C%20we%20provide%20latent%20space%20visualisation%0Aexperiments%2C%20highlighting%20the%20interpretability%20of%20the%20learned%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.04486v3&entry.124074799=Read"},
{"title": "Frame Interpolation with Consecutive Brownian Bridge Diffusion", "author": "Zonglin Lyu and Ming Li and Jianbo Jiao and Chen Chen", "abstract": "  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a\ndiffusion-based conditional image generation problem, synthesizing the\nintermediate frame given a random noise and neighboring frames. Due to the\nrelatively high resolution of videos, Latent Diffusion Models (LDMs) are\nemployed as the conditional generation model, where the autoencoder compresses\nimages into latent representations for diffusion and then reconstructs images\nfrom these latent representations. Such a formulation poses a crucial\nchallenge: VFI expects that the output is deterministically equal to the ground\ntruth intermediate frame, but LDMs randomly generate a diverse set of different\nimages when the model runs multiple times. The reason for the diverse\ngeneration is that the cumulative variance (variance accumulated at each step\nof generation) of generated latent representations in LDMs is large. This makes\nthe sampling trajectory random, resulting in diverse rather than deterministic\ngenerations. To address this problem, we propose our unique solution: Frame\nInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, we\npropose consecutive Brownian Bridge diffusion that takes a deterministic\ninitial value as input, resulting in a much smaller cumulative variance of\ngenerated latent representations. Our experiments suggest that our method can\nimprove together with the improvement of the autoencoder and achieve\nstate-of-the-art performance in VFI, leaving strong potential for further\nenhancement.\n", "link": "http://arxiv.org/abs/2405.05953v1", "date": "2024-05-09", "relevancy": 2.5008, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6468}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6161}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion&body=Title%3A%20Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion%0AAuthor%3A%20Zonglin%20Lyu%20and%20Ming%20Li%20and%20Jianbo%20Jiao%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Recent%20work%20in%20Video%20Frame%20Interpolation%20%28VFI%29%20tries%20to%20formulate%20VFI%20as%20a%0Adiffusion-based%20conditional%20image%20generation%20problem%2C%20synthesizing%20the%0Aintermediate%20frame%20given%20a%20random%20noise%20and%20neighboring%20frames.%20Due%20to%20the%0Arelatively%20high%20resolution%20of%20videos%2C%20Latent%20Diffusion%20Models%20%28LDMs%29%20are%0Aemployed%20as%20the%20conditional%20generation%20model%2C%20where%20the%20autoencoder%20compresses%0Aimages%20into%20latent%20representations%20for%20diffusion%20and%20then%20reconstructs%20images%0Afrom%20these%20latent%20representations.%20Such%20a%20formulation%20poses%20a%20crucial%0Achallenge%3A%20VFI%20expects%20that%20the%20output%20is%20deterministically%20equal%20to%20the%20ground%0Atruth%20intermediate%20frame%2C%20but%20LDMs%20randomly%20generate%20a%20diverse%20set%20of%20different%0Aimages%20when%20the%20model%20runs%20multiple%20times.%20The%20reason%20for%20the%20diverse%0Ageneration%20is%20that%20the%20cumulative%20variance%20%28variance%20accumulated%20at%20each%20step%0Aof%20generation%29%20of%20generated%20latent%20representations%20in%20LDMs%20is%20large.%20This%20makes%0Athe%20sampling%20trajectory%20random%2C%20resulting%20in%20diverse%20rather%20than%20deterministic%0Agenerations.%20To%20address%20this%20problem%2C%20we%20propose%20our%20unique%20solution%3A%20Frame%0AInterpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion.%20Specifically%2C%20we%0Apropose%20consecutive%20Brownian%20Bridge%20diffusion%20that%20takes%20a%20deterministic%0Ainitial%20value%20as%20input%2C%20resulting%20in%20a%20much%20smaller%20cumulative%20variance%20of%0Agenerated%20latent%20representations.%20Our%20experiments%20suggest%20that%20our%20method%20can%0Aimprove%20together%20with%20the%20improvement%20of%20the%20autoencoder%20and%20achieve%0Astate-of-the-art%20performance%20in%20VFI%2C%20leaving%20strong%20potential%20for%20further%0Aenhancement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrame%2520Interpolation%2520with%2520Consecutive%2520Brownian%2520Bridge%2520Diffusion%26entry.906535625%3DZonglin%2520Lyu%2520and%2520Ming%2520Li%2520and%2520Jianbo%2520Jiao%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520work%2520in%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520tries%2520to%2520formulate%2520VFI%2520as%2520a%250Adiffusion-based%2520conditional%2520image%2520generation%2520problem%252C%2520synthesizing%2520the%250Aintermediate%2520frame%2520given%2520a%2520random%2520noise%2520and%2520neighboring%2520frames.%2520Due%2520to%2520the%250Arelatively%2520high%2520resolution%2520of%2520videos%252C%2520Latent%2520Diffusion%2520Models%2520%2528LDMs%2529%2520are%250Aemployed%2520as%2520the%2520conditional%2520generation%2520model%252C%2520where%2520the%2520autoencoder%2520compresses%250Aimages%2520into%2520latent%2520representations%2520for%2520diffusion%2520and%2520then%2520reconstructs%2520images%250Afrom%2520these%2520latent%2520representations.%2520Such%2520a%2520formulation%2520poses%2520a%2520crucial%250Achallenge%253A%2520VFI%2520expects%2520that%2520the%2520output%2520is%2520deterministically%2520equal%2520to%2520the%2520ground%250Atruth%2520intermediate%2520frame%252C%2520but%2520LDMs%2520randomly%2520generate%2520a%2520diverse%2520set%2520of%2520different%250Aimages%2520when%2520the%2520model%2520runs%2520multiple%2520times.%2520The%2520reason%2520for%2520the%2520diverse%250Ageneration%2520is%2520that%2520the%2520cumulative%2520variance%2520%2528variance%2520accumulated%2520at%2520each%2520step%250Aof%2520generation%2529%2520of%2520generated%2520latent%2520representations%2520in%2520LDMs%2520is%2520large.%2520This%2520makes%250Athe%2520sampling%2520trajectory%2520random%252C%2520resulting%2520in%2520diverse%2520rather%2520than%2520deterministic%250Agenerations.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520our%2520unique%2520solution%253A%2520Frame%250AInterpolation%2520with%2520Consecutive%2520Brownian%2520Bridge%2520Diffusion.%2520Specifically%252C%2520we%250Apropose%2520consecutive%2520Brownian%2520Bridge%2520diffusion%2520that%2520takes%2520a%2520deterministic%250Ainitial%2520value%2520as%2520input%252C%2520resulting%2520in%2520a%2520much%2520smaller%2520cumulative%2520variance%2520of%250Agenerated%2520latent%2520representations.%2520Our%2520experiments%2520suggest%2520that%2520our%2520method%2520can%250Aimprove%2520together%2520with%2520the%2520improvement%2520of%2520the%2520autoencoder%2520and%2520achieve%250Astate-of-the-art%2520performance%2520in%2520VFI%252C%2520leaving%2520strong%2520potential%2520for%2520further%250Aenhancement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frame%20Interpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion&entry.906535625=Zonglin%20Lyu%20and%20Ming%20Li%20and%20Jianbo%20Jiao%20and%20Chen%20Chen&entry.1292438233=%20%20Recent%20work%20in%20Video%20Frame%20Interpolation%20%28VFI%29%20tries%20to%20formulate%20VFI%20as%20a%0Adiffusion-based%20conditional%20image%20generation%20problem%2C%20synthesizing%20the%0Aintermediate%20frame%20given%20a%20random%20noise%20and%20neighboring%20frames.%20Due%20to%20the%0Arelatively%20high%20resolution%20of%20videos%2C%20Latent%20Diffusion%20Models%20%28LDMs%29%20are%0Aemployed%20as%20the%20conditional%20generation%20model%2C%20where%20the%20autoencoder%20compresses%0Aimages%20into%20latent%20representations%20for%20diffusion%20and%20then%20reconstructs%20images%0Afrom%20these%20latent%20representations.%20Such%20a%20formulation%20poses%20a%20crucial%0Achallenge%3A%20VFI%20expects%20that%20the%20output%20is%20deterministically%20equal%20to%20the%20ground%0Atruth%20intermediate%20frame%2C%20but%20LDMs%20randomly%20generate%20a%20diverse%20set%20of%20different%0Aimages%20when%20the%20model%20runs%20multiple%20times.%20The%20reason%20for%20the%20diverse%0Ageneration%20is%20that%20the%20cumulative%20variance%20%28variance%20accumulated%20at%20each%20step%0Aof%20generation%29%20of%20generated%20latent%20representations%20in%20LDMs%20is%20large.%20This%20makes%0Athe%20sampling%20trajectory%20random%2C%20resulting%20in%20diverse%20rather%20than%20deterministic%0Agenerations.%20To%20address%20this%20problem%2C%20we%20propose%20our%20unique%20solution%3A%20Frame%0AInterpolation%20with%20Consecutive%20Brownian%20Bridge%20Diffusion.%20Specifically%2C%20we%0Apropose%20consecutive%20Brownian%20Bridge%20diffusion%20that%20takes%20a%20deterministic%0Ainitial%20value%20as%20input%2C%20resulting%20in%20a%20much%20smaller%20cumulative%20variance%20of%0Agenerated%20latent%20representations.%20Our%20experiments%20suggest%20that%20our%20method%20can%0Aimprove%20together%20with%20the%20improvement%20of%20the%20autoencoder%20and%20achieve%0Astate-of-the-art%20performance%20in%20VFI%2C%20leaving%20strong%20potential%20for%20further%0Aenhancement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05953v1&entry.124074799=Read"},
{"title": "Design and Evaluation of a Generic Visual SLAM Framework for\n  Multi-Camera Systems", "author": "Pushyami Kaveti and Shankara Narayanan Vaidyanathan and Arvind Thamilchelvan and Hanumant Singh", "abstract": "  Multi-camera systems have been shown to improve the accuracy and robustness\nof SLAM estimates, yet state-of-the-art SLAM systems predominantly support\nmonocular or stereo setups. This paper presents a generic sparse visual SLAM\nframework capable of running on any number of cameras and in any arrangement.\nOur SLAM system uses the generalized camera model, which allows us to represent\nan arbitrary multi-camera system as a single imaging device. Additionally, it\ntakes advantage of the overlapping fields of view (FoV) by extracting\ncross-matched features across cameras in the rig. This limits the linear rise\nin the number of features with the number of cameras and keeps the\ncomputational load in check while enabling an accurate representation of the\nscene. We evaluate our method in terms of accuracy, robustness, and run time on\nindoor and outdoor datasets that include challenging real-world scenarios such\nas narrow corridors, featureless spaces, and dynamic objects. We show that our\nsystem can adapt to different camera configurations and allows real-time\nexecution for typical robotic applications. Finally, we benchmark the impact of\nthe critical design parameters - the number of cameras and the overlap between\ntheir FoV that define the camera configuration for SLAM. All our software and\ndatasets are freely available for further research.\n", "link": "http://arxiv.org/abs/2210.07315v2", "date": "2024-05-09", "relevancy": 2.4492, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6438}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20and%20Evaluation%20of%20a%20Generic%20Visual%20SLAM%20Framework%20for%0A%20%20Multi-Camera%20Systems&body=Title%3A%20Design%20and%20Evaluation%20of%20a%20Generic%20Visual%20SLAM%20Framework%20for%0A%20%20Multi-Camera%20Systems%0AAuthor%3A%20Pushyami%20Kaveti%20and%20Shankara%20Narayanan%20Vaidyanathan%20and%20Arvind%20Thamilchelvan%20and%20Hanumant%20Singh%0AAbstract%3A%20%20%20Multi-camera%20systems%20have%20been%20shown%20to%20improve%20the%20accuracy%20and%20robustness%0Aof%20SLAM%20estimates%2C%20yet%20state-of-the-art%20SLAM%20systems%20predominantly%20support%0Amonocular%20or%20stereo%20setups.%20This%20paper%20presents%20a%20generic%20sparse%20visual%20SLAM%0Aframework%20capable%20of%20running%20on%20any%20number%20of%20cameras%20and%20in%20any%20arrangement.%0AOur%20SLAM%20system%20uses%20the%20generalized%20camera%20model%2C%20which%20allows%20us%20to%20represent%0Aan%20arbitrary%20multi-camera%20system%20as%20a%20single%20imaging%20device.%20Additionally%2C%20it%0Atakes%20advantage%20of%20the%20overlapping%20fields%20of%20view%20%28FoV%29%20by%20extracting%0Across-matched%20features%20across%20cameras%20in%20the%20rig.%20This%20limits%20the%20linear%20rise%0Ain%20the%20number%20of%20features%20with%20the%20number%20of%20cameras%20and%20keeps%20the%0Acomputational%20load%20in%20check%20while%20enabling%20an%20accurate%20representation%20of%20the%0Ascene.%20We%20evaluate%20our%20method%20in%20terms%20of%20accuracy%2C%20robustness%2C%20and%20run%20time%20on%0Aindoor%20and%20outdoor%20datasets%20that%20include%20challenging%20real-world%20scenarios%20such%0Aas%20narrow%20corridors%2C%20featureless%20spaces%2C%20and%20dynamic%20objects.%20We%20show%20that%20our%0Asystem%20can%20adapt%20to%20different%20camera%20configurations%20and%20allows%20real-time%0Aexecution%20for%20typical%20robotic%20applications.%20Finally%2C%20we%20benchmark%20the%20impact%20of%0Athe%20critical%20design%20parameters%20-%20the%20number%20of%20cameras%20and%20the%20overlap%20between%0Atheir%20FoV%20that%20define%20the%20camera%20configuration%20for%20SLAM.%20All%20our%20software%20and%0Adatasets%20are%20freely%20available%20for%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.07315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520and%2520Evaluation%2520of%2520a%2520Generic%2520Visual%2520SLAM%2520Framework%2520for%250A%2520%2520Multi-Camera%2520Systems%26entry.906535625%3DPushyami%2520Kaveti%2520and%2520Shankara%2520Narayanan%2520Vaidyanathan%2520and%2520Arvind%2520Thamilchelvan%2520and%2520Hanumant%2520Singh%26entry.1292438233%3D%2520%2520Multi-camera%2520systems%2520have%2520been%2520shown%2520to%2520improve%2520the%2520accuracy%2520and%2520robustness%250Aof%2520SLAM%2520estimates%252C%2520yet%2520state-of-the-art%2520SLAM%2520systems%2520predominantly%2520support%250Amonocular%2520or%2520stereo%2520setups.%2520This%2520paper%2520presents%2520a%2520generic%2520sparse%2520visual%2520SLAM%250Aframework%2520capable%2520of%2520running%2520on%2520any%2520number%2520of%2520cameras%2520and%2520in%2520any%2520arrangement.%250AOur%2520SLAM%2520system%2520uses%2520the%2520generalized%2520camera%2520model%252C%2520which%2520allows%2520us%2520to%2520represent%250Aan%2520arbitrary%2520multi-camera%2520system%2520as%2520a%2520single%2520imaging%2520device.%2520Additionally%252C%2520it%250Atakes%2520advantage%2520of%2520the%2520overlapping%2520fields%2520of%2520view%2520%2528FoV%2529%2520by%2520extracting%250Across-matched%2520features%2520across%2520cameras%2520in%2520the%2520rig.%2520This%2520limits%2520the%2520linear%2520rise%250Ain%2520the%2520number%2520of%2520features%2520with%2520the%2520number%2520of%2520cameras%2520and%2520keeps%2520the%250Acomputational%2520load%2520in%2520check%2520while%2520enabling%2520an%2520accurate%2520representation%2520of%2520the%250Ascene.%2520We%2520evaluate%2520our%2520method%2520in%2520terms%2520of%2520accuracy%252C%2520robustness%252C%2520and%2520run%2520time%2520on%250Aindoor%2520and%2520outdoor%2520datasets%2520that%2520include%2520challenging%2520real-world%2520scenarios%2520such%250Aas%2520narrow%2520corridors%252C%2520featureless%2520spaces%252C%2520and%2520dynamic%2520objects.%2520We%2520show%2520that%2520our%250Asystem%2520can%2520adapt%2520to%2520different%2520camera%2520configurations%2520and%2520allows%2520real-time%250Aexecution%2520for%2520typical%2520robotic%2520applications.%2520Finally%252C%2520we%2520benchmark%2520the%2520impact%2520of%250Athe%2520critical%2520design%2520parameters%2520-%2520the%2520number%2520of%2520cameras%2520and%2520the%2520overlap%2520between%250Atheir%2520FoV%2520that%2520define%2520the%2520camera%2520configuration%2520for%2520SLAM.%2520All%2520our%2520software%2520and%250Adatasets%2520are%2520freely%2520available%2520for%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.07315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20and%20Evaluation%20of%20a%20Generic%20Visual%20SLAM%20Framework%20for%0A%20%20Multi-Camera%20Systems&entry.906535625=Pushyami%20Kaveti%20and%20Shankara%20Narayanan%20Vaidyanathan%20and%20Arvind%20Thamilchelvan%20and%20Hanumant%20Singh&entry.1292438233=%20%20Multi-camera%20systems%20have%20been%20shown%20to%20improve%20the%20accuracy%20and%20robustness%0Aof%20SLAM%20estimates%2C%20yet%20state-of-the-art%20SLAM%20systems%20predominantly%20support%0Amonocular%20or%20stereo%20setups.%20This%20paper%20presents%20a%20generic%20sparse%20visual%20SLAM%0Aframework%20capable%20of%20running%20on%20any%20number%20of%20cameras%20and%20in%20any%20arrangement.%0AOur%20SLAM%20system%20uses%20the%20generalized%20camera%20model%2C%20which%20allows%20us%20to%20represent%0Aan%20arbitrary%20multi-camera%20system%20as%20a%20single%20imaging%20device.%20Additionally%2C%20it%0Atakes%20advantage%20of%20the%20overlapping%20fields%20of%20view%20%28FoV%29%20by%20extracting%0Across-matched%20features%20across%20cameras%20in%20the%20rig.%20This%20limits%20the%20linear%20rise%0Ain%20the%20number%20of%20features%20with%20the%20number%20of%20cameras%20and%20keeps%20the%0Acomputational%20load%20in%20check%20while%20enabling%20an%20accurate%20representation%20of%20the%0Ascene.%20We%20evaluate%20our%20method%20in%20terms%20of%20accuracy%2C%20robustness%2C%20and%20run%20time%20on%0Aindoor%20and%20outdoor%20datasets%20that%20include%20challenging%20real-world%20scenarios%20such%0Aas%20narrow%20corridors%2C%20featureless%20spaces%2C%20and%20dynamic%20objects.%20We%20show%20that%20our%0Asystem%20can%20adapt%20to%20different%20camera%20configurations%20and%20allows%20real-time%0Aexecution%20for%20typical%20robotic%20applications.%20Finally%2C%20we%20benchmark%20the%20impact%20of%0Athe%20critical%20design%20parameters%20-%20the%20number%20of%20cameras%20and%20the%20overlap%20between%0Atheir%20FoV%20that%20define%20the%20camera%20configuration%20for%20SLAM.%20All%20our%20software%20and%0Adatasets%20are%20freely%20available%20for%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.07315v2&entry.124074799=Read"},
{"title": "NeuRSS: Enhancing AUV Localization and Bathymetric Mapping with Neural\n  Rendering for Sidescan SLAM", "author": "Yiping Xie and Jun Zhang and Nils Bore and John Folkesson", "abstract": "  Implicit neural representations and neural rendering have gained increasing\nattention for bathymetry estimation from sidescan sonar (SSS). These methods\nincorporate multiple observations of the same place from SSS data to constrain\nthe elevation estimate, converging to a globally-consistent bathymetric model.\nHowever, the quality and precision of the bathymetric estimate are limited by\nthe positioning accuracy of the autonomous underwater vehicle (AUV) equipped\nwith the sonar. The global positioning estimate of the AUV relying on dead\nreckoning (DR) has an unbounded error due to the absence of a geo-reference\nsystem like GPS underwater. To address this challenge, we propose in this\nletter a modern and scalable framework, NeuRSS, for SSS SLAM based on DR and\nloop closures (LCs) over large timescales, with an elevation prior provided by\nthe bathymetric estimate using neural rendering from SSS. This framework is an\niterative procedure that improves localization and bathymetric mapping.\nInitially, the bathymetry estimated from SSS using the DR estimate, though\ncrude, can provide an important elevation prior in the nonlinear least-squares\n(NLS) optimization that estimates the relative pose between two loop-closure\nvertices in a pose graph. Subsequently, the global pose estimate from the SLAM\ncomponent improves the positioning estimate of the vehicle, thus improving the\nbathymetry estimation. We validate our localization and mapping approach on two\nlarge surveys collected with a surface vessel and an AUV, respectively. We\nevaluate their localization results against the ground truth and compare the\nbathymetry estimation against data collected with multibeam echo sounders\n(MBES).\n", "link": "http://arxiv.org/abs/2405.05807v1", "date": "2024-05-09", "relevancy": 2.4364, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6399}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6031}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuRSS%3A%20Enhancing%20AUV%20Localization%20and%20Bathymetric%20Mapping%20with%20Neural%0A%20%20Rendering%20for%20Sidescan%20SLAM&body=Title%3A%20NeuRSS%3A%20Enhancing%20AUV%20Localization%20and%20Bathymetric%20Mapping%20with%20Neural%0A%20%20Rendering%20for%20Sidescan%20SLAM%0AAuthor%3A%20Yiping%20Xie%20and%20Jun%20Zhang%20and%20Nils%20Bore%20and%20John%20Folkesson%0AAbstract%3A%20%20%20Implicit%20neural%20representations%20and%20neural%20rendering%20have%20gained%20increasing%0Aattention%20for%20bathymetry%20estimation%20from%20sidescan%20sonar%20%28SSS%29.%20These%20methods%0Aincorporate%20multiple%20observations%20of%20the%20same%20place%20from%20SSS%20data%20to%20constrain%0Athe%20elevation%20estimate%2C%20converging%20to%20a%20globally-consistent%20bathymetric%20model.%0AHowever%2C%20the%20quality%20and%20precision%20of%20the%20bathymetric%20estimate%20are%20limited%20by%0Athe%20positioning%20accuracy%20of%20the%20autonomous%20underwater%20vehicle%20%28AUV%29%20equipped%0Awith%20the%20sonar.%20The%20global%20positioning%20estimate%20of%20the%20AUV%20relying%20on%20dead%0Areckoning%20%28DR%29%20has%20an%20unbounded%20error%20due%20to%20the%20absence%20of%20a%20geo-reference%0Asystem%20like%20GPS%20underwater.%20To%20address%20this%20challenge%2C%20we%20propose%20in%20this%0Aletter%20a%20modern%20and%20scalable%20framework%2C%20NeuRSS%2C%20for%20SSS%20SLAM%20based%20on%20DR%20and%0Aloop%20closures%20%28LCs%29%20over%20large%20timescales%2C%20with%20an%20elevation%20prior%20provided%20by%0Athe%20bathymetric%20estimate%20using%20neural%20rendering%20from%20SSS.%20This%20framework%20is%20an%0Aiterative%20procedure%20that%20improves%20localization%20and%20bathymetric%20mapping.%0AInitially%2C%20the%20bathymetry%20estimated%20from%20SSS%20using%20the%20DR%20estimate%2C%20though%0Acrude%2C%20can%20provide%20an%20important%20elevation%20prior%20in%20the%20nonlinear%20least-squares%0A%28NLS%29%20optimization%20that%20estimates%20the%20relative%20pose%20between%20two%20loop-closure%0Avertices%20in%20a%20pose%20graph.%20Subsequently%2C%20the%20global%20pose%20estimate%20from%20the%20SLAM%0Acomponent%20improves%20the%20positioning%20estimate%20of%20the%20vehicle%2C%20thus%20improving%20the%0Abathymetry%20estimation.%20We%20validate%20our%20localization%20and%20mapping%20approach%20on%20two%0Alarge%20surveys%20collected%20with%20a%20surface%20vessel%20and%20an%20AUV%2C%20respectively.%20We%0Aevaluate%20their%20localization%20results%20against%20the%20ground%20truth%20and%20compare%20the%0Abathymetry%20estimation%20against%20data%20collected%20with%20multibeam%20echo%20sounders%0A%28MBES%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05807v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuRSS%253A%2520Enhancing%2520AUV%2520Localization%2520and%2520Bathymetric%2520Mapping%2520with%2520Neural%250A%2520%2520Rendering%2520for%2520Sidescan%2520SLAM%26entry.906535625%3DYiping%2520Xie%2520and%2520Jun%2520Zhang%2520and%2520Nils%2520Bore%2520and%2520John%2520Folkesson%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representations%2520and%2520neural%2520rendering%2520have%2520gained%2520increasing%250Aattention%2520for%2520bathymetry%2520estimation%2520from%2520sidescan%2520sonar%2520%2528SSS%2529.%2520These%2520methods%250Aincorporate%2520multiple%2520observations%2520of%2520the%2520same%2520place%2520from%2520SSS%2520data%2520to%2520constrain%250Athe%2520elevation%2520estimate%252C%2520converging%2520to%2520a%2520globally-consistent%2520bathymetric%2520model.%250AHowever%252C%2520the%2520quality%2520and%2520precision%2520of%2520the%2520bathymetric%2520estimate%2520are%2520limited%2520by%250Athe%2520positioning%2520accuracy%2520of%2520the%2520autonomous%2520underwater%2520vehicle%2520%2528AUV%2529%2520equipped%250Awith%2520the%2520sonar.%2520The%2520global%2520positioning%2520estimate%2520of%2520the%2520AUV%2520relying%2520on%2520dead%250Areckoning%2520%2528DR%2529%2520has%2520an%2520unbounded%2520error%2520due%2520to%2520the%2520absence%2520of%2520a%2520geo-reference%250Asystem%2520like%2520GPS%2520underwater.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520in%2520this%250Aletter%2520a%2520modern%2520and%2520scalable%2520framework%252C%2520NeuRSS%252C%2520for%2520SSS%2520SLAM%2520based%2520on%2520DR%2520and%250Aloop%2520closures%2520%2528LCs%2529%2520over%2520large%2520timescales%252C%2520with%2520an%2520elevation%2520prior%2520provided%2520by%250Athe%2520bathymetric%2520estimate%2520using%2520neural%2520rendering%2520from%2520SSS.%2520This%2520framework%2520is%2520an%250Aiterative%2520procedure%2520that%2520improves%2520localization%2520and%2520bathymetric%2520mapping.%250AInitially%252C%2520the%2520bathymetry%2520estimated%2520from%2520SSS%2520using%2520the%2520DR%2520estimate%252C%2520though%250Acrude%252C%2520can%2520provide%2520an%2520important%2520elevation%2520prior%2520in%2520the%2520nonlinear%2520least-squares%250A%2528NLS%2529%2520optimization%2520that%2520estimates%2520the%2520relative%2520pose%2520between%2520two%2520loop-closure%250Avertices%2520in%2520a%2520pose%2520graph.%2520Subsequently%252C%2520the%2520global%2520pose%2520estimate%2520from%2520the%2520SLAM%250Acomponent%2520improves%2520the%2520positioning%2520estimate%2520of%2520the%2520vehicle%252C%2520thus%2520improving%2520the%250Abathymetry%2520estimation.%2520We%2520validate%2520our%2520localization%2520and%2520mapping%2520approach%2520on%2520two%250Alarge%2520surveys%2520collected%2520with%2520a%2520surface%2520vessel%2520and%2520an%2520AUV%252C%2520respectively.%2520We%250Aevaluate%2520their%2520localization%2520results%2520against%2520the%2520ground%2520truth%2520and%2520compare%2520the%250Abathymetry%2520estimation%2520against%2520data%2520collected%2520with%2520multibeam%2520echo%2520sounders%250A%2528MBES%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05807v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuRSS%3A%20Enhancing%20AUV%20Localization%20and%20Bathymetric%20Mapping%20with%20Neural%0A%20%20Rendering%20for%20Sidescan%20SLAM&entry.906535625=Yiping%20Xie%20and%20Jun%20Zhang%20and%20Nils%20Bore%20and%20John%20Folkesson&entry.1292438233=%20%20Implicit%20neural%20representations%20and%20neural%20rendering%20have%20gained%20increasing%0Aattention%20for%20bathymetry%20estimation%20from%20sidescan%20sonar%20%28SSS%29.%20These%20methods%0Aincorporate%20multiple%20observations%20of%20the%20same%20place%20from%20SSS%20data%20to%20constrain%0Athe%20elevation%20estimate%2C%20converging%20to%20a%20globally-consistent%20bathymetric%20model.%0AHowever%2C%20the%20quality%20and%20precision%20of%20the%20bathymetric%20estimate%20are%20limited%20by%0Athe%20positioning%20accuracy%20of%20the%20autonomous%20underwater%20vehicle%20%28AUV%29%20equipped%0Awith%20the%20sonar.%20The%20global%20positioning%20estimate%20of%20the%20AUV%20relying%20on%20dead%0Areckoning%20%28DR%29%20has%20an%20unbounded%20error%20due%20to%20the%20absence%20of%20a%20geo-reference%0Asystem%20like%20GPS%20underwater.%20To%20address%20this%20challenge%2C%20we%20propose%20in%20this%0Aletter%20a%20modern%20and%20scalable%20framework%2C%20NeuRSS%2C%20for%20SSS%20SLAM%20based%20on%20DR%20and%0Aloop%20closures%20%28LCs%29%20over%20large%20timescales%2C%20with%20an%20elevation%20prior%20provided%20by%0Athe%20bathymetric%20estimate%20using%20neural%20rendering%20from%20SSS.%20This%20framework%20is%20an%0Aiterative%20procedure%20that%20improves%20localization%20and%20bathymetric%20mapping.%0AInitially%2C%20the%20bathymetry%20estimated%20from%20SSS%20using%20the%20DR%20estimate%2C%20though%0Acrude%2C%20can%20provide%20an%20important%20elevation%20prior%20in%20the%20nonlinear%20least-squares%0A%28NLS%29%20optimization%20that%20estimates%20the%20relative%20pose%20between%20two%20loop-closure%0Avertices%20in%20a%20pose%20graph.%20Subsequently%2C%20the%20global%20pose%20estimate%20from%20the%20SLAM%0Acomponent%20improves%20the%20positioning%20estimate%20of%20the%20vehicle%2C%20thus%20improving%20the%0Abathymetry%20estimation.%20We%20validate%20our%20localization%20and%20mapping%20approach%20on%20two%0Alarge%20surveys%20collected%20with%20a%20surface%20vessel%20and%20an%20AUV%2C%20respectively.%20We%0Aevaluate%20their%20localization%20results%20against%20the%20ground%20truth%20and%20compare%20the%0Abathymetry%20estimation%20against%20data%20collected%20with%20multibeam%20echo%20sounders%0A%28MBES%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05807v1&entry.124074799=Read"},
{"title": "Bridging Lottery ticket and Grokking: Is Weight Norm Sufficient to\n  Explain Delayed Generalization?", "author": "Gouki Minegishi and Yusuke Iwasawa and Yutaka Matsuo", "abstract": "  Grokking is one of the most surprising puzzles in neural network\ngeneralization: a network first reaches a memorization solution with perfect\ntraining accuracy and poor generalization, but with further training, it\nreaches a perfectly generalized solution. We aim to analyze the mechanism of\ngrokking from the lottery ticket hypothesis, identifying the process to find\nthe lottery tickets (good sparse subnetworks) as the key to describing the\ntransitional phase between memorization and generalization. We refer to these\nsubnetworks as ''Grokking tickets'', which is identified via magnitude pruning\nafter perfect generalization. First, using ''Grokking tickets'', we show that\nthe lottery tickets drastically accelerate grokking compared to the dense\nnetworks on various configurations (MLP and Transformer, and an arithmetic and\nimage classification tasks). Additionally, to verify that ''Grokking ticket''\nare a more critical factor than weight norms, we compared the ''good''\nsubnetworks with a dense network having the same L1 and L2 norms. Results show\nthat the subnetworks generalize faster than the controlled dense model. In\nfurther investigations, we discovered that at an appropriate pruning rate,\ngrokking can be achieved even without weight decay. We also show that speedup\ndoes not happen when using tickets identified at the memorization solution or\ntransition between memorization and generalization or when pruning networks at\nthe initialization (Random pruning, Grasp, SNIP, and Synflow). The results\nindicate that the weight norm of network parameters is not enough to explain\nthe process of grokking, but the importance of finding good subnetworks to\ndescribe the transition from memorization to generalization. The implementation\ncode can be accessed via this link:\n\\url{https://github.com/gouki510/Grokking-Tickets}.\n", "link": "http://arxiv.org/abs/2310.19470v2", "date": "2024-05-09", "relevancy": 2.4346, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Lottery%20ticket%20and%20Grokking%3A%20Is%20Weight%20Norm%20Sufficient%20to%0A%20%20Explain%20Delayed%20Generalization%3F&body=Title%3A%20Bridging%20Lottery%20ticket%20and%20Grokking%3A%20Is%20Weight%20Norm%20Sufficient%20to%0A%20%20Explain%20Delayed%20Generalization%3F%0AAuthor%3A%20Gouki%20Minegishi%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo%0AAbstract%3A%20%20%20Grokking%20is%20one%20of%20the%20most%20surprising%20puzzles%20in%20neural%20network%0Ageneralization%3A%20a%20network%20first%20reaches%20a%20memorization%20solution%20with%20perfect%0Atraining%20accuracy%20and%20poor%20generalization%2C%20but%20with%20further%20training%2C%20it%0Areaches%20a%20perfectly%20generalized%20solution.%20We%20aim%20to%20analyze%20the%20mechanism%20of%0Agrokking%20from%20the%20lottery%20ticket%20hypothesis%2C%20identifying%20the%20process%20to%20find%0Athe%20lottery%20tickets%20%28good%20sparse%20subnetworks%29%20as%20the%20key%20to%20describing%20the%0Atransitional%20phase%20between%20memorization%20and%20generalization.%20We%20refer%20to%20these%0Asubnetworks%20as%20%27%27Grokking%20tickets%27%27%2C%20which%20is%20identified%20via%20magnitude%20pruning%0Aafter%20perfect%20generalization.%20First%2C%20using%20%27%27Grokking%20tickets%27%27%2C%20we%20show%20that%0Athe%20lottery%20tickets%20drastically%20accelerate%20grokking%20compared%20to%20the%20dense%0Anetworks%20on%20various%20configurations%20%28MLP%20and%20Transformer%2C%20and%20an%20arithmetic%20and%0Aimage%20classification%20tasks%29.%20Additionally%2C%20to%20verify%20that%20%27%27Grokking%20ticket%27%27%0Aare%20a%20more%20critical%20factor%20than%20weight%20norms%2C%20we%20compared%20the%20%27%27good%27%27%0Asubnetworks%20with%20a%20dense%20network%20having%20the%20same%20L1%20and%20L2%20norms.%20Results%20show%0Athat%20the%20subnetworks%20generalize%20faster%20than%20the%20controlled%20dense%20model.%20In%0Afurther%20investigations%2C%20we%20discovered%20that%20at%20an%20appropriate%20pruning%20rate%2C%0Agrokking%20can%20be%20achieved%20even%20without%20weight%20decay.%20We%20also%20show%20that%20speedup%0Adoes%20not%20happen%20when%20using%20tickets%20identified%20at%20the%20memorization%20solution%20or%0Atransition%20between%20memorization%20and%20generalization%20or%20when%20pruning%20networks%20at%0Athe%20initialization%20%28Random%20pruning%2C%20Grasp%2C%20SNIP%2C%20and%20Synflow%29.%20The%20results%0Aindicate%20that%20the%20weight%20norm%20of%20network%20parameters%20is%20not%20enough%20to%20explain%0Athe%20process%20of%20grokking%2C%20but%20the%20importance%20of%20finding%20good%20subnetworks%20to%0Adescribe%20the%20transition%20from%20memorization%20to%20generalization.%20The%20implementation%0Acode%20can%20be%20accessed%20via%20this%20link%3A%0A%5Curl%7Bhttps%3A//github.com/gouki510/Grokking-Tickets%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Lottery%2520ticket%2520and%2520Grokking%253A%2520Is%2520Weight%2520Norm%2520Sufficient%2520to%250A%2520%2520Explain%2520Delayed%2520Generalization%253F%26entry.906535625%3DGouki%2520Minegishi%2520and%2520Yusuke%2520Iwasawa%2520and%2520Yutaka%2520Matsuo%26entry.1292438233%3D%2520%2520Grokking%2520is%2520one%2520of%2520the%2520most%2520surprising%2520puzzles%2520in%2520neural%2520network%250Ageneralization%253A%2520a%2520network%2520first%2520reaches%2520a%2520memorization%2520solution%2520with%2520perfect%250Atraining%2520accuracy%2520and%2520poor%2520generalization%252C%2520but%2520with%2520further%2520training%252C%2520it%250Areaches%2520a%2520perfectly%2520generalized%2520solution.%2520We%2520aim%2520to%2520analyze%2520the%2520mechanism%2520of%250Agrokking%2520from%2520the%2520lottery%2520ticket%2520hypothesis%252C%2520identifying%2520the%2520process%2520to%2520find%250Athe%2520lottery%2520tickets%2520%2528good%2520sparse%2520subnetworks%2529%2520as%2520the%2520key%2520to%2520describing%2520the%250Atransitional%2520phase%2520between%2520memorization%2520and%2520generalization.%2520We%2520refer%2520to%2520these%250Asubnetworks%2520as%2520%2527%2527Grokking%2520tickets%2527%2527%252C%2520which%2520is%2520identified%2520via%2520magnitude%2520pruning%250Aafter%2520perfect%2520generalization.%2520First%252C%2520using%2520%2527%2527Grokking%2520tickets%2527%2527%252C%2520we%2520show%2520that%250Athe%2520lottery%2520tickets%2520drastically%2520accelerate%2520grokking%2520compared%2520to%2520the%2520dense%250Anetworks%2520on%2520various%2520configurations%2520%2528MLP%2520and%2520Transformer%252C%2520and%2520an%2520arithmetic%2520and%250Aimage%2520classification%2520tasks%2529.%2520Additionally%252C%2520to%2520verify%2520that%2520%2527%2527Grokking%2520ticket%2527%2527%250Aare%2520a%2520more%2520critical%2520factor%2520than%2520weight%2520norms%252C%2520we%2520compared%2520the%2520%2527%2527good%2527%2527%250Asubnetworks%2520with%2520a%2520dense%2520network%2520having%2520the%2520same%2520L1%2520and%2520L2%2520norms.%2520Results%2520show%250Athat%2520the%2520subnetworks%2520generalize%2520faster%2520than%2520the%2520controlled%2520dense%2520model.%2520In%250Afurther%2520investigations%252C%2520we%2520discovered%2520that%2520at%2520an%2520appropriate%2520pruning%2520rate%252C%250Agrokking%2520can%2520be%2520achieved%2520even%2520without%2520weight%2520decay.%2520We%2520also%2520show%2520that%2520speedup%250Adoes%2520not%2520happen%2520when%2520using%2520tickets%2520identified%2520at%2520the%2520memorization%2520solution%2520or%250Atransition%2520between%2520memorization%2520and%2520generalization%2520or%2520when%2520pruning%2520networks%2520at%250Athe%2520initialization%2520%2528Random%2520pruning%252C%2520Grasp%252C%2520SNIP%252C%2520and%2520Synflow%2529.%2520The%2520results%250Aindicate%2520that%2520the%2520weight%2520norm%2520of%2520network%2520parameters%2520is%2520not%2520enough%2520to%2520explain%250Athe%2520process%2520of%2520grokking%252C%2520but%2520the%2520importance%2520of%2520finding%2520good%2520subnetworks%2520to%250Adescribe%2520the%2520transition%2520from%2520memorization%2520to%2520generalization.%2520The%2520implementation%250Acode%2520can%2520be%2520accessed%2520via%2520this%2520link%253A%250A%255Curl%257Bhttps%253A//github.com/gouki510/Grokking-Tickets%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Lottery%20ticket%20and%20Grokking%3A%20Is%20Weight%20Norm%20Sufficient%20to%0A%20%20Explain%20Delayed%20Generalization%3F&entry.906535625=Gouki%20Minegishi%20and%20Yusuke%20Iwasawa%20and%20Yutaka%20Matsuo&entry.1292438233=%20%20Grokking%20is%20one%20of%20the%20most%20surprising%20puzzles%20in%20neural%20network%0Ageneralization%3A%20a%20network%20first%20reaches%20a%20memorization%20solution%20with%20perfect%0Atraining%20accuracy%20and%20poor%20generalization%2C%20but%20with%20further%20training%2C%20it%0Areaches%20a%20perfectly%20generalized%20solution.%20We%20aim%20to%20analyze%20the%20mechanism%20of%0Agrokking%20from%20the%20lottery%20ticket%20hypothesis%2C%20identifying%20the%20process%20to%20find%0Athe%20lottery%20tickets%20%28good%20sparse%20subnetworks%29%20as%20the%20key%20to%20describing%20the%0Atransitional%20phase%20between%20memorization%20and%20generalization.%20We%20refer%20to%20these%0Asubnetworks%20as%20%27%27Grokking%20tickets%27%27%2C%20which%20is%20identified%20via%20magnitude%20pruning%0Aafter%20perfect%20generalization.%20First%2C%20using%20%27%27Grokking%20tickets%27%27%2C%20we%20show%20that%0Athe%20lottery%20tickets%20drastically%20accelerate%20grokking%20compared%20to%20the%20dense%0Anetworks%20on%20various%20configurations%20%28MLP%20and%20Transformer%2C%20and%20an%20arithmetic%20and%0Aimage%20classification%20tasks%29.%20Additionally%2C%20to%20verify%20that%20%27%27Grokking%20ticket%27%27%0Aare%20a%20more%20critical%20factor%20than%20weight%20norms%2C%20we%20compared%20the%20%27%27good%27%27%0Asubnetworks%20with%20a%20dense%20network%20having%20the%20same%20L1%20and%20L2%20norms.%20Results%20show%0Athat%20the%20subnetworks%20generalize%20faster%20than%20the%20controlled%20dense%20model.%20In%0Afurther%20investigations%2C%20we%20discovered%20that%20at%20an%20appropriate%20pruning%20rate%2C%0Agrokking%20can%20be%20achieved%20even%20without%20weight%20decay.%20We%20also%20show%20that%20speedup%0Adoes%20not%20happen%20when%20using%20tickets%20identified%20at%20the%20memorization%20solution%20or%0Atransition%20between%20memorization%20and%20generalization%20or%20when%20pruning%20networks%20at%0Athe%20initialization%20%28Random%20pruning%2C%20Grasp%2C%20SNIP%2C%20and%20Synflow%29.%20The%20results%0Aindicate%20that%20the%20weight%20norm%20of%20network%20parameters%20is%20not%20enough%20to%20explain%0Athe%20process%20of%20grokking%2C%20but%20the%20importance%20of%20finding%20good%20subnetworks%20to%0Adescribe%20the%20transition%20from%20memorization%20to%20generalization.%20The%20implementation%0Acode%20can%20be%20accessed%20via%20this%20link%3A%0A%5Curl%7Bhttps%3A//github.com/gouki510/Grokking-Tickets%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19470v2&entry.124074799=Read"},
{"title": "CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks", "author": " Nick and  Nikzad and Yongsheng Gao and Jun Zhou", "abstract": "  In recent years, convolutional neural networks (CNNs) with channel-wise\nfeature refining mechanisms have brought noticeable benefits to modelling\nchannel dependencies. However, current attention paradigms fail to infer an\noptimal channel descriptor capable of simultaneously exploiting statistical and\nspatial relationships among feature maps. In this paper, to overcome this\nshortcoming, we present a novel channel-wise spatially autocorrelated (CSA)\nattention mechanism. Inspired by geographical analysis, the proposed CSA\nexploits the spatial relationships between channels of feature maps to produce\nan effective channel descriptor. To the best of our knowledge, this is the f\nirst time that the concept of geographical spatial analysis is utilized in deep\nCNNs. The proposed CSA imposes negligible learning parameters and light\ncomputational overhead to the deep model, making it a powerful yet efficient\nattention module of choice. We validate the effectiveness of the proposed CSA\nnetworks (CSA-Nets) through extensive experiments and analysis on ImageNet, and\nMS COCO benchmark datasets for image classification, object detection, and\ninstance segmentation. The experimental results demonstrate that CSA-Nets are\nable to consistently achieve competitive performance and superior\ngeneralization than several state-of-the-art attention-based CNNs over\ndifferent benchmark tasks and datasets.\n", "link": "http://arxiv.org/abs/2405.05755v1", "date": "2024-05-09", "relevancy": 2.4239, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4904}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4883}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSA-Net%3A%20Channel-wise%20Spatially%20Autocorrelated%20Attention%20Networks&body=Title%3A%20CSA-Net%3A%20Channel-wise%20Spatially%20Autocorrelated%20Attention%20Networks%0AAuthor%3A%20%20Nick%20and%20%20Nikzad%20and%20Yongsheng%20Gao%20and%20Jun%20Zhou%0AAbstract%3A%20%20%20In%20recent%20years%2C%20convolutional%20neural%20networks%20%28CNNs%29%20with%20channel-wise%0Afeature%20refining%20mechanisms%20have%20brought%20noticeable%20benefits%20to%20modelling%0Achannel%20dependencies.%20However%2C%20current%20attention%20paradigms%20fail%20to%20infer%20an%0Aoptimal%20channel%20descriptor%20capable%20of%20simultaneously%20exploiting%20statistical%20and%0Aspatial%20relationships%20among%20feature%20maps.%20In%20this%20paper%2C%20to%20overcome%20this%0Ashortcoming%2C%20we%20present%20a%20novel%20channel-wise%20spatially%20autocorrelated%20%28CSA%29%0Aattention%20mechanism.%20Inspired%20by%20geographical%20analysis%2C%20the%20proposed%20CSA%0Aexploits%20the%20spatial%20relationships%20between%20channels%20of%20feature%20maps%20to%20produce%0Aan%20effective%20channel%20descriptor.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20f%0Airst%20time%20that%20the%20concept%20of%20geographical%20spatial%20analysis%20is%20utilized%20in%20deep%0ACNNs.%20The%20proposed%20CSA%20imposes%20negligible%20learning%20parameters%20and%20light%0Acomputational%20overhead%20to%20the%20deep%20model%2C%20making%20it%20a%20powerful%20yet%20efficient%0Aattention%20module%20of%20choice.%20We%20validate%20the%20effectiveness%20of%20the%20proposed%20CSA%0Anetworks%20%28CSA-Nets%29%20through%20extensive%20experiments%20and%20analysis%20on%20ImageNet%2C%20and%0AMS%20COCO%20benchmark%20datasets%20for%20image%20classification%2C%20object%20detection%2C%20and%0Ainstance%20segmentation.%20The%20experimental%20results%20demonstrate%20that%20CSA-Nets%20are%0Aable%20to%20consistently%20achieve%20competitive%20performance%20and%20superior%0Ageneralization%20than%20several%20state-of-the-art%20attention-based%20CNNs%20over%0Adifferent%20benchmark%20tasks%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSA-Net%253A%2520Channel-wise%2520Spatially%2520Autocorrelated%2520Attention%2520Networks%26entry.906535625%3D%2520Nick%2520and%2520%2520Nikzad%2520and%2520Yongsheng%2520Gao%2520and%2520Jun%2520Zhou%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520with%2520channel-wise%250Afeature%2520refining%2520mechanisms%2520have%2520brought%2520noticeable%2520benefits%2520to%2520modelling%250Achannel%2520dependencies.%2520However%252C%2520current%2520attention%2520paradigms%2520fail%2520to%2520infer%2520an%250Aoptimal%2520channel%2520descriptor%2520capable%2520of%2520simultaneously%2520exploiting%2520statistical%2520and%250Aspatial%2520relationships%2520among%2520feature%2520maps.%2520In%2520this%2520paper%252C%2520to%2520overcome%2520this%250Ashortcoming%252C%2520we%2520present%2520a%2520novel%2520channel-wise%2520spatially%2520autocorrelated%2520%2528CSA%2529%250Aattention%2520mechanism.%2520Inspired%2520by%2520geographical%2520analysis%252C%2520the%2520proposed%2520CSA%250Aexploits%2520the%2520spatial%2520relationships%2520between%2520channels%2520of%2520feature%2520maps%2520to%2520produce%250Aan%2520effective%2520channel%2520descriptor.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520f%250Airst%2520time%2520that%2520the%2520concept%2520of%2520geographical%2520spatial%2520analysis%2520is%2520utilized%2520in%2520deep%250ACNNs.%2520The%2520proposed%2520CSA%2520imposes%2520negligible%2520learning%2520parameters%2520and%2520light%250Acomputational%2520overhead%2520to%2520the%2520deep%2520model%252C%2520making%2520it%2520a%2520powerful%2520yet%2520efficient%250Aattention%2520module%2520of%2520choice.%2520We%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520CSA%250Anetworks%2520%2528CSA-Nets%2529%2520through%2520extensive%2520experiments%2520and%2520analysis%2520on%2520ImageNet%252C%2520and%250AMS%2520COCO%2520benchmark%2520datasets%2520for%2520image%2520classification%252C%2520object%2520detection%252C%2520and%250Ainstance%2520segmentation.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520CSA-Nets%2520are%250Aable%2520to%2520consistently%2520achieve%2520competitive%2520performance%2520and%2520superior%250Ageneralization%2520than%2520several%2520state-of-the-art%2520attention-based%2520CNNs%2520over%250Adifferent%2520benchmark%2520tasks%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSA-Net%3A%20Channel-wise%20Spatially%20Autocorrelated%20Attention%20Networks&entry.906535625=%20Nick%20and%20%20Nikzad%20and%20Yongsheng%20Gao%20and%20Jun%20Zhou&entry.1292438233=%20%20In%20recent%20years%2C%20convolutional%20neural%20networks%20%28CNNs%29%20with%20channel-wise%0Afeature%20refining%20mechanisms%20have%20brought%20noticeable%20benefits%20to%20modelling%0Achannel%20dependencies.%20However%2C%20current%20attention%20paradigms%20fail%20to%20infer%20an%0Aoptimal%20channel%20descriptor%20capable%20of%20simultaneously%20exploiting%20statistical%20and%0Aspatial%20relationships%20among%20feature%20maps.%20In%20this%20paper%2C%20to%20overcome%20this%0Ashortcoming%2C%20we%20present%20a%20novel%20channel-wise%20spatially%20autocorrelated%20%28CSA%29%0Aattention%20mechanism.%20Inspired%20by%20geographical%20analysis%2C%20the%20proposed%20CSA%0Aexploits%20the%20spatial%20relationships%20between%20channels%20of%20feature%20maps%20to%20produce%0Aan%20effective%20channel%20descriptor.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20f%0Airst%20time%20that%20the%20concept%20of%20geographical%20spatial%20analysis%20is%20utilized%20in%20deep%0ACNNs.%20The%20proposed%20CSA%20imposes%20negligible%20learning%20parameters%20and%20light%0Acomputational%20overhead%20to%20the%20deep%20model%2C%20making%20it%20a%20powerful%20yet%20efficient%0Aattention%20module%20of%20choice.%20We%20validate%20the%20effectiveness%20of%20the%20proposed%20CSA%0Anetworks%20%28CSA-Nets%29%20through%20extensive%20experiments%20and%20analysis%20on%20ImageNet%2C%20and%0AMS%20COCO%20benchmark%20datasets%20for%20image%20classification%2C%20object%20detection%2C%20and%0Ainstance%20segmentation.%20The%20experimental%20results%20demonstrate%20that%20CSA-Nets%20are%0Aable%20to%20consistently%20achieve%20competitive%20performance%20and%20superior%0Ageneralization%20than%20several%20state-of-the-art%20attention-based%20CNNs%20over%0Adifferent%20benchmark%20tasks%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05755v1&entry.124074799=Read"},
{"title": "Trustworthy AI-Generative Content in Intelligent 6G Network:\n  Adversarial, Privacy, and Fairness", "author": "Siyuan Li and Xi Lin and Yaju Liu and Jianhua Li", "abstract": "  AI-generated content (AIGC) models, represented by large language models\n(LLM), have brought revolutionary changes to the content generation fields. The\nhigh-speed and extensive 6G technology is an ideal platform for providing\npowerful AIGC mobile service applications, while future 6G mobile networks also\nneed to support intelligent and personalized mobile generation services.\nHowever, the significant ethical and security issues of current AIGC models,\nsuch as adversarial attacks, privacy, and fairness, greatly affect the\ncredibility of 6G intelligent networks, especially in ensuring secure, private,\nand fair AIGC applications. In this paper, we propose TrustGAIN, a novel\nparadigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale\nAIGC services in future 6G networks. We first discuss the adversarial attacks\nand privacy threats faced by AIGC systems in 6G networks, as well as the\ncorresponding protection issues. Subsequently, we emphasize the importance of\nensuring the unbiasedness and fairness of the mobile generative service in\nfuture intelligent networks. In particular, we conduct a use case to\ndemonstrate that TrustGAIN can effectively guide the resistance against\nmalicious or generated false information. We believe that TrustGAIN is a\nnecessary paradigm for intelligent and trustworthy 6G networks to support AIGC\nservices, ensuring the security, privacy, and fairness of AIGC network\nservices.\n", "link": "http://arxiv.org/abs/2405.05930v1", "date": "2024-05-09", "relevancy": 2.4192, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4909}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4892}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20AI-Generative%20Content%20in%20Intelligent%206G%20Network%3A%0A%20%20Adversarial%2C%20Privacy%2C%20and%20Fairness&body=Title%3A%20Trustworthy%20AI-Generative%20Content%20in%20Intelligent%206G%20Network%3A%0A%20%20Adversarial%2C%20Privacy%2C%20and%20Fairness%0AAuthor%3A%20Siyuan%20Li%20and%20Xi%20Lin%20and%20Yaju%20Liu%20and%20Jianhua%20Li%0AAbstract%3A%20%20%20AI-generated%20content%20%28AIGC%29%20models%2C%20represented%20by%20large%20language%20models%0A%28LLM%29%2C%20have%20brought%20revolutionary%20changes%20to%20the%20content%20generation%20fields.%20The%0Ahigh-speed%20and%20extensive%206G%20technology%20is%20an%20ideal%20platform%20for%20providing%0Apowerful%20AIGC%20mobile%20service%20applications%2C%20while%20future%206G%20mobile%20networks%20also%0Aneed%20to%20support%20intelligent%20and%20personalized%20mobile%20generation%20services.%0AHowever%2C%20the%20significant%20ethical%20and%20security%20issues%20of%20current%20AIGC%20models%2C%0Asuch%20as%20adversarial%20attacks%2C%20privacy%2C%20and%20fairness%2C%20greatly%20affect%20the%0Acredibility%20of%206G%20intelligent%20networks%2C%20especially%20in%20ensuring%20secure%2C%20private%2C%0Aand%20fair%20AIGC%20applications.%20In%20this%20paper%2C%20we%20propose%20TrustGAIN%2C%20a%20novel%0Aparadigm%20for%20trustworthy%20AIGC%20in%206G%20networks%2C%20to%20ensure%20trustworthy%20large-scale%0AAIGC%20services%20in%20future%206G%20networks.%20We%20first%20discuss%20the%20adversarial%20attacks%0Aand%20privacy%20threats%20faced%20by%20AIGC%20systems%20in%206G%20networks%2C%20as%20well%20as%20the%0Acorresponding%20protection%20issues.%20Subsequently%2C%20we%20emphasize%20the%20importance%20of%0Aensuring%20the%20unbiasedness%20and%20fairness%20of%20the%20mobile%20generative%20service%20in%0Afuture%20intelligent%20networks.%20In%20particular%2C%20we%20conduct%20a%20use%20case%20to%0Ademonstrate%20that%20TrustGAIN%20can%20effectively%20guide%20the%20resistance%20against%0Amalicious%20or%20generated%20false%20information.%20We%20believe%20that%20TrustGAIN%20is%20a%0Anecessary%20paradigm%20for%20intelligent%20and%20trustworthy%206G%20networks%20to%20support%20AIGC%0Aservices%2C%20ensuring%20the%20security%2C%20privacy%2C%20and%20fairness%20of%20AIGC%20network%0Aservices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%2520AI-Generative%2520Content%2520in%2520Intelligent%25206G%2520Network%253A%250A%2520%2520Adversarial%252C%2520Privacy%252C%2520and%2520Fairness%26entry.906535625%3DSiyuan%2520Li%2520and%2520Xi%2520Lin%2520and%2520Yaju%2520Liu%2520and%2520Jianhua%2520Li%26entry.1292438233%3D%2520%2520AI-generated%2520content%2520%2528AIGC%2529%2520models%252C%2520represented%2520by%2520large%2520language%2520models%250A%2528LLM%2529%252C%2520have%2520brought%2520revolutionary%2520changes%2520to%2520the%2520content%2520generation%2520fields.%2520The%250Ahigh-speed%2520and%2520extensive%25206G%2520technology%2520is%2520an%2520ideal%2520platform%2520for%2520providing%250Apowerful%2520AIGC%2520mobile%2520service%2520applications%252C%2520while%2520future%25206G%2520mobile%2520networks%2520also%250Aneed%2520to%2520support%2520intelligent%2520and%2520personalized%2520mobile%2520generation%2520services.%250AHowever%252C%2520the%2520significant%2520ethical%2520and%2520security%2520issues%2520of%2520current%2520AIGC%2520models%252C%250Asuch%2520as%2520adversarial%2520attacks%252C%2520privacy%252C%2520and%2520fairness%252C%2520greatly%2520affect%2520the%250Acredibility%2520of%25206G%2520intelligent%2520networks%252C%2520especially%2520in%2520ensuring%2520secure%252C%2520private%252C%250Aand%2520fair%2520AIGC%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520TrustGAIN%252C%2520a%2520novel%250Aparadigm%2520for%2520trustworthy%2520AIGC%2520in%25206G%2520networks%252C%2520to%2520ensure%2520trustworthy%2520large-scale%250AAIGC%2520services%2520in%2520future%25206G%2520networks.%2520We%2520first%2520discuss%2520the%2520adversarial%2520attacks%250Aand%2520privacy%2520threats%2520faced%2520by%2520AIGC%2520systems%2520in%25206G%2520networks%252C%2520as%2520well%2520as%2520the%250Acorresponding%2520protection%2520issues.%2520Subsequently%252C%2520we%2520emphasize%2520the%2520importance%2520of%250Aensuring%2520the%2520unbiasedness%2520and%2520fairness%2520of%2520the%2520mobile%2520generative%2520service%2520in%250Afuture%2520intelligent%2520networks.%2520In%2520particular%252C%2520we%2520conduct%2520a%2520use%2520case%2520to%250Ademonstrate%2520that%2520TrustGAIN%2520can%2520effectively%2520guide%2520the%2520resistance%2520against%250Amalicious%2520or%2520generated%2520false%2520information.%2520We%2520believe%2520that%2520TrustGAIN%2520is%2520a%250Anecessary%2520paradigm%2520for%2520intelligent%2520and%2520trustworthy%25206G%2520networks%2520to%2520support%2520AIGC%250Aservices%252C%2520ensuring%2520the%2520security%252C%2520privacy%252C%2520and%2520fairness%2520of%2520AIGC%2520network%250Aservices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20AI-Generative%20Content%20in%20Intelligent%206G%20Network%3A%0A%20%20Adversarial%2C%20Privacy%2C%20and%20Fairness&entry.906535625=Siyuan%20Li%20and%20Xi%20Lin%20and%20Yaju%20Liu%20and%20Jianhua%20Li&entry.1292438233=%20%20AI-generated%20content%20%28AIGC%29%20models%2C%20represented%20by%20large%20language%20models%0A%28LLM%29%2C%20have%20brought%20revolutionary%20changes%20to%20the%20content%20generation%20fields.%20The%0Ahigh-speed%20and%20extensive%206G%20technology%20is%20an%20ideal%20platform%20for%20providing%0Apowerful%20AIGC%20mobile%20service%20applications%2C%20while%20future%206G%20mobile%20networks%20also%0Aneed%20to%20support%20intelligent%20and%20personalized%20mobile%20generation%20services.%0AHowever%2C%20the%20significant%20ethical%20and%20security%20issues%20of%20current%20AIGC%20models%2C%0Asuch%20as%20adversarial%20attacks%2C%20privacy%2C%20and%20fairness%2C%20greatly%20affect%20the%0Acredibility%20of%206G%20intelligent%20networks%2C%20especially%20in%20ensuring%20secure%2C%20private%2C%0Aand%20fair%20AIGC%20applications.%20In%20this%20paper%2C%20we%20propose%20TrustGAIN%2C%20a%20novel%0Aparadigm%20for%20trustworthy%20AIGC%20in%206G%20networks%2C%20to%20ensure%20trustworthy%20large-scale%0AAIGC%20services%20in%20future%206G%20networks.%20We%20first%20discuss%20the%20adversarial%20attacks%0Aand%20privacy%20threats%20faced%20by%20AIGC%20systems%20in%206G%20networks%2C%20as%20well%20as%20the%0Acorresponding%20protection%20issues.%20Subsequently%2C%20we%20emphasize%20the%20importance%20of%0Aensuring%20the%20unbiasedness%20and%20fairness%20of%20the%20mobile%20generative%20service%20in%0Afuture%20intelligent%20networks.%20In%20particular%2C%20we%20conduct%20a%20use%20case%20to%0Ademonstrate%20that%20TrustGAIN%20can%20effectively%20guide%20the%20resistance%20against%0Amalicious%20or%20generated%20false%20information.%20We%20believe%20that%20TrustGAIN%20is%20a%0Anecessary%20paradigm%20for%20intelligent%20and%20trustworthy%206G%20networks%20to%20support%20AIGC%0Aservices%2C%20ensuring%20the%20security%2C%20privacy%2C%20and%20fairness%20of%20AIGC%20network%0Aservices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05930v1&entry.124074799=Read"},
{"title": "StableMoFusion: Towards Robust and Efficient Diffusion-based Motion\n  Generation Framework", "author": "Yiheng Huang and Hui Yang and Chuanchen Luo and Yuxi Wang and Shibiao Xu and Zhaoxiang Zhang and Man Zhang and Junran Peng", "abstract": "  Thanks to the powerful generative capacity of diffusion models, recent years\nhave witnessed rapid progress in human motion generation. Existing\ndiffusion-based methods employ disparate network architectures and training\nstrategies. The effect of the design of each component is still unclear. In\naddition, the iterative denoising process consumes considerable computational\noverhead, which is prohibitive for real-time scenarios such as virtual\ncharacters and humanoid robots. For this reason, we first conduct a\ncomprehensive investigation into network architectures, training strategies,\nand inference processs. Based on the profound analysis, we tailor each\ncomponent for efficient high-quality human motion generation. Despite the\npromising performance, the tailored model still suffers from foot skating which\nis an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we\nidentify foot-ground contact and correct foot motions along the denoising\nprocess. By organically combining these well-designed components together, we\npresent StableMoFusion, a robust and efficient framework for human motion\ngeneration. Extensive experimental results show that our StableMoFusion\nperforms favorably against current state-of-the-art methods. Project page:\nhttps://h-y1heng.github.io/StableMoFusion-page/\n", "link": "http://arxiv.org/abs/2405.05691v1", "date": "2024-05-09", "relevancy": 2.4127, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6117}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6077}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableMoFusion%3A%20Towards%20Robust%20and%20Efficient%20Diffusion-based%20Motion%0A%20%20Generation%20Framework&body=Title%3A%20StableMoFusion%3A%20Towards%20Robust%20and%20Efficient%20Diffusion-based%20Motion%0A%20%20Generation%20Framework%0AAuthor%3A%20Yiheng%20Huang%20and%20Hui%20Yang%20and%20Chuanchen%20Luo%20and%20Yuxi%20Wang%20and%20Shibiao%20Xu%20and%20Zhaoxiang%20Zhang%20and%20Man%20Zhang%20and%20Junran%20Peng%0AAbstract%3A%20%20%20Thanks%20to%20the%20powerful%20generative%20capacity%20of%20diffusion%20models%2C%20recent%20years%0Ahave%20witnessed%20rapid%20progress%20in%20human%20motion%20generation.%20Existing%0Adiffusion-based%20methods%20employ%20disparate%20network%20architectures%20and%20training%0Astrategies.%20The%20effect%20of%20the%20design%20of%20each%20component%20is%20still%20unclear.%20In%0Aaddition%2C%20the%20iterative%20denoising%20process%20consumes%20considerable%20computational%0Aoverhead%2C%20which%20is%20prohibitive%20for%20real-time%20scenarios%20such%20as%20virtual%0Acharacters%20and%20humanoid%20robots.%20For%20this%20reason%2C%20we%20first%20conduct%20a%0Acomprehensive%20investigation%20into%20network%20architectures%2C%20training%20strategies%2C%0Aand%20inference%20processs.%20Based%20on%20the%20profound%20analysis%2C%20we%20tailor%20each%0Acomponent%20for%20efficient%20high-quality%20human%20motion%20generation.%20Despite%20the%0Apromising%20performance%2C%20the%20tailored%20model%20still%20suffers%20from%20foot%20skating%20which%0Ais%20an%20ubiquitous%20issue%20in%20diffusion-based%20solutions.%20To%20eliminate%20footskate%2C%20we%0Aidentify%20foot-ground%20contact%20and%20correct%20foot%20motions%20along%20the%20denoising%0Aprocess.%20By%20organically%20combining%20these%20well-designed%20components%20together%2C%20we%0Apresent%20StableMoFusion%2C%20a%20robust%20and%20efficient%20framework%20for%20human%20motion%0Ageneration.%20Extensive%20experimental%20results%20show%20that%20our%20StableMoFusion%0Aperforms%20favorably%20against%20current%20state-of-the-art%20methods.%20Project%20page%3A%0Ahttps%3A//h-y1heng.github.io/StableMoFusion-page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableMoFusion%253A%2520Towards%2520Robust%2520and%2520Efficient%2520Diffusion-based%2520Motion%250A%2520%2520Generation%2520Framework%26entry.906535625%3DYiheng%2520Huang%2520and%2520Hui%2520Yang%2520and%2520Chuanchen%2520Luo%2520and%2520Yuxi%2520Wang%2520and%2520Shibiao%2520Xu%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Man%2520Zhang%2520and%2520Junran%2520Peng%26entry.1292438233%3D%2520%2520Thanks%2520to%2520the%2520powerful%2520generative%2520capacity%2520of%2520diffusion%2520models%252C%2520recent%2520years%250Ahave%2520witnessed%2520rapid%2520progress%2520in%2520human%2520motion%2520generation.%2520Existing%250Adiffusion-based%2520methods%2520employ%2520disparate%2520network%2520architectures%2520and%2520training%250Astrategies.%2520The%2520effect%2520of%2520the%2520design%2520of%2520each%2520component%2520is%2520still%2520unclear.%2520In%250Aaddition%252C%2520the%2520iterative%2520denoising%2520process%2520consumes%2520considerable%2520computational%250Aoverhead%252C%2520which%2520is%2520prohibitive%2520for%2520real-time%2520scenarios%2520such%2520as%2520virtual%250Acharacters%2520and%2520humanoid%2520robots.%2520For%2520this%2520reason%252C%2520we%2520first%2520conduct%2520a%250Acomprehensive%2520investigation%2520into%2520network%2520architectures%252C%2520training%2520strategies%252C%250Aand%2520inference%2520processs.%2520Based%2520on%2520the%2520profound%2520analysis%252C%2520we%2520tailor%2520each%250Acomponent%2520for%2520efficient%2520high-quality%2520human%2520motion%2520generation.%2520Despite%2520the%250Apromising%2520performance%252C%2520the%2520tailored%2520model%2520still%2520suffers%2520from%2520foot%2520skating%2520which%250Ais%2520an%2520ubiquitous%2520issue%2520in%2520diffusion-based%2520solutions.%2520To%2520eliminate%2520footskate%252C%2520we%250Aidentify%2520foot-ground%2520contact%2520and%2520correct%2520foot%2520motions%2520along%2520the%2520denoising%250Aprocess.%2520By%2520organically%2520combining%2520these%2520well-designed%2520components%2520together%252C%2520we%250Apresent%2520StableMoFusion%252C%2520a%2520robust%2520and%2520efficient%2520framework%2520for%2520human%2520motion%250Ageneration.%2520Extensive%2520experimental%2520results%2520show%2520that%2520our%2520StableMoFusion%250Aperforms%2520favorably%2520against%2520current%2520state-of-the-art%2520methods.%2520Project%2520page%253A%250Ahttps%253A//h-y1heng.github.io/StableMoFusion-page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableMoFusion%3A%20Towards%20Robust%20and%20Efficient%20Diffusion-based%20Motion%0A%20%20Generation%20Framework&entry.906535625=Yiheng%20Huang%20and%20Hui%20Yang%20and%20Chuanchen%20Luo%20and%20Yuxi%20Wang%20and%20Shibiao%20Xu%20and%20Zhaoxiang%20Zhang%20and%20Man%20Zhang%20and%20Junran%20Peng&entry.1292438233=%20%20Thanks%20to%20the%20powerful%20generative%20capacity%20of%20diffusion%20models%2C%20recent%20years%0Ahave%20witnessed%20rapid%20progress%20in%20human%20motion%20generation.%20Existing%0Adiffusion-based%20methods%20employ%20disparate%20network%20architectures%20and%20training%0Astrategies.%20The%20effect%20of%20the%20design%20of%20each%20component%20is%20still%20unclear.%20In%0Aaddition%2C%20the%20iterative%20denoising%20process%20consumes%20considerable%20computational%0Aoverhead%2C%20which%20is%20prohibitive%20for%20real-time%20scenarios%20such%20as%20virtual%0Acharacters%20and%20humanoid%20robots.%20For%20this%20reason%2C%20we%20first%20conduct%20a%0Acomprehensive%20investigation%20into%20network%20architectures%2C%20training%20strategies%2C%0Aand%20inference%20processs.%20Based%20on%20the%20profound%20analysis%2C%20we%20tailor%20each%0Acomponent%20for%20efficient%20high-quality%20human%20motion%20generation.%20Despite%20the%0Apromising%20performance%2C%20the%20tailored%20model%20still%20suffers%20from%20foot%20skating%20which%0Ais%20an%20ubiquitous%20issue%20in%20diffusion-based%20solutions.%20To%20eliminate%20footskate%2C%20we%0Aidentify%20foot-ground%20contact%20and%20correct%20foot%20motions%20along%20the%20denoising%0Aprocess.%20By%20organically%20combining%20these%20well-designed%20components%20together%2C%20we%0Apresent%20StableMoFusion%2C%20a%20robust%20and%20efficient%20framework%20for%20human%20motion%0Ageneration.%20Extensive%20experimental%20results%20show%20that%20our%20StableMoFusion%0Aperforms%20favorably%20against%20current%20state-of-the-art%20methods.%20Project%20page%3A%0Ahttps%3A//h-y1heng.github.io/StableMoFusion-page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05691v1&entry.124074799=Read"},
{"title": "MFA-Net: Multi-Scale feature fusion attention network for liver tumor\n  segmentation", "author": "Yanli Yuan and Bingbing Wang and Chuan Zhang and Jingyi Xu and Ximeng Liu and Liehuang Zhu", "abstract": "  Segmentation of organs of interest in medical CT images is beneficial for\ndiagnosis of diseases. Though recent methods based on Fully Convolutional\nNeural Networks (F-CNNs) have shown success in many segmentation tasks, fusing\nfeatures from images with different scales is still a challenge: (1) Due to the\nlack of spatial awareness, F-CNNs share the same weights at different spatial\nlocations. (2) F-CNNs can only obtain surrounding information through local\nreceptive fields. To address the above challenge, we propose a new segmentation\nframework based on attention mechanisms, named MFA-Net (Multi-Scale Feature\nFusion Attention Network). The proposed framework can learn more meaningful\nfeature maps among multiple scales and result in more accurate automatic\nsegmentation. We compare our proposed MFA-Net with SOTA methods on two 2D liver\nCT datasets. The experimental results show that our MFA-Net produces more\nprecise segmentation on images with different scales.\n", "link": "http://arxiv.org/abs/2405.04064v2", "date": "2024-05-09", "relevancy": 2.4075, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4799}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MFA-Net%3A%20Multi-Scale%20feature%20fusion%20attention%20network%20for%20liver%20tumor%0A%20%20segmentation&body=Title%3A%20MFA-Net%3A%20Multi-Scale%20feature%20fusion%20attention%20network%20for%20liver%20tumor%0A%20%20segmentation%0AAuthor%3A%20Yanli%20Yuan%20and%20Bingbing%20Wang%20and%20Chuan%20Zhang%20and%20Jingyi%20Xu%20and%20Ximeng%20Liu%20and%20Liehuang%20Zhu%0AAbstract%3A%20%20%20Segmentation%20of%20organs%20of%20interest%20in%20medical%20CT%20images%20is%20beneficial%20for%0Adiagnosis%20of%20diseases.%20Though%20recent%20methods%20based%20on%20Fully%20Convolutional%0ANeural%20Networks%20%28F-CNNs%29%20have%20shown%20success%20in%20many%20segmentation%20tasks%2C%20fusing%0Afeatures%20from%20images%20with%20different%20scales%20is%20still%20a%20challenge%3A%20%281%29%20Due%20to%20the%0Alack%20of%20spatial%20awareness%2C%20F-CNNs%20share%20the%20same%20weights%20at%20different%20spatial%0Alocations.%20%282%29%20F-CNNs%20can%20only%20obtain%20surrounding%20information%20through%20local%0Areceptive%20fields.%20To%20address%20the%20above%20challenge%2C%20we%20propose%20a%20new%20segmentation%0Aframework%20based%20on%20attention%20mechanisms%2C%20named%20MFA-Net%20%28Multi-Scale%20Feature%0AFusion%20Attention%20Network%29.%20The%20proposed%20framework%20can%20learn%20more%20meaningful%0Afeature%20maps%20among%20multiple%20scales%20and%20result%20in%20more%20accurate%20automatic%0Asegmentation.%20We%20compare%20our%20proposed%20MFA-Net%20with%20SOTA%20methods%20on%20two%202D%20liver%0ACT%20datasets.%20The%20experimental%20results%20show%20that%20our%20MFA-Net%20produces%20more%0Aprecise%20segmentation%20on%20images%20with%20different%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMFA-Net%253A%2520Multi-Scale%2520feature%2520fusion%2520attention%2520network%2520for%2520liver%2520tumor%250A%2520%2520segmentation%26entry.906535625%3DYanli%2520Yuan%2520and%2520Bingbing%2520Wang%2520and%2520Chuan%2520Zhang%2520and%2520Jingyi%2520Xu%2520and%2520Ximeng%2520Liu%2520and%2520Liehuang%2520Zhu%26entry.1292438233%3D%2520%2520Segmentation%2520of%2520organs%2520of%2520interest%2520in%2520medical%2520CT%2520images%2520is%2520beneficial%2520for%250Adiagnosis%2520of%2520diseases.%2520Though%2520recent%2520methods%2520based%2520on%2520Fully%2520Convolutional%250ANeural%2520Networks%2520%2528F-CNNs%2529%2520have%2520shown%2520success%2520in%2520many%2520segmentation%2520tasks%252C%2520fusing%250Afeatures%2520from%2520images%2520with%2520different%2520scales%2520is%2520still%2520a%2520challenge%253A%2520%25281%2529%2520Due%2520to%2520the%250Alack%2520of%2520spatial%2520awareness%252C%2520F-CNNs%2520share%2520the%2520same%2520weights%2520at%2520different%2520spatial%250Alocations.%2520%25282%2529%2520F-CNNs%2520can%2520only%2520obtain%2520surrounding%2520information%2520through%2520local%250Areceptive%2520fields.%2520To%2520address%2520the%2520above%2520challenge%252C%2520we%2520propose%2520a%2520new%2520segmentation%250Aframework%2520based%2520on%2520attention%2520mechanisms%252C%2520named%2520MFA-Net%2520%2528Multi-Scale%2520Feature%250AFusion%2520Attention%2520Network%2529.%2520The%2520proposed%2520framework%2520can%2520learn%2520more%2520meaningful%250Afeature%2520maps%2520among%2520multiple%2520scales%2520and%2520result%2520in%2520more%2520accurate%2520automatic%250Asegmentation.%2520We%2520compare%2520our%2520proposed%2520MFA-Net%2520with%2520SOTA%2520methods%2520on%2520two%25202D%2520liver%250ACT%2520datasets.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520MFA-Net%2520produces%2520more%250Aprecise%2520segmentation%2520on%2520images%2520with%2520different%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MFA-Net%3A%20Multi-Scale%20feature%20fusion%20attention%20network%20for%20liver%20tumor%0A%20%20segmentation&entry.906535625=Yanli%20Yuan%20and%20Bingbing%20Wang%20and%20Chuan%20Zhang%20and%20Jingyi%20Xu%20and%20Ximeng%20Liu%20and%20Liehuang%20Zhu&entry.1292438233=%20%20Segmentation%20of%20organs%20of%20interest%20in%20medical%20CT%20images%20is%20beneficial%20for%0Adiagnosis%20of%20diseases.%20Though%20recent%20methods%20based%20on%20Fully%20Convolutional%0ANeural%20Networks%20%28F-CNNs%29%20have%20shown%20success%20in%20many%20segmentation%20tasks%2C%20fusing%0Afeatures%20from%20images%20with%20different%20scales%20is%20still%20a%20challenge%3A%20%281%29%20Due%20to%20the%0Alack%20of%20spatial%20awareness%2C%20F-CNNs%20share%20the%20same%20weights%20at%20different%20spatial%0Alocations.%20%282%29%20F-CNNs%20can%20only%20obtain%20surrounding%20information%20through%20local%0Areceptive%20fields.%20To%20address%20the%20above%20challenge%2C%20we%20propose%20a%20new%20segmentation%0Aframework%20based%20on%20attention%20mechanisms%2C%20named%20MFA-Net%20%28Multi-Scale%20Feature%0AFusion%20Attention%20Network%29.%20The%20proposed%20framework%20can%20learn%20more%20meaningful%0Afeature%20maps%20among%20multiple%20scales%20and%20result%20in%20more%20accurate%20automatic%0Asegmentation.%20We%20compare%20our%20proposed%20MFA-Net%20with%20SOTA%20methods%20on%20two%202D%20liver%0ACT%20datasets.%20The%20experimental%20results%20show%20that%20our%20MFA-Net%20produces%20more%0Aprecise%20segmentation%20on%20images%20with%20different%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04064v2&entry.124074799=Read"},
{"title": "Creative Beam Search: LLM-as-a-Judge For Improving Response Generation", "author": "Giorgio Franceschelli and Mirco Musolesi", "abstract": "  Large language models are revolutionizing several areas, including artificial\ncreativity. However, the process of generation in machines profoundly diverges\nfrom that observed in humans. In particular, machine generation is\ncharacterized by a lack of intentionality and an underlying creative process.\nWe propose a method called Creative Beam Search that uses Diverse Beam Search\nand LLM-as-a-Judge to perform response generation and response validation. The\nresults of a qualitative experiment show how our approach can provide better\noutput than standard sampling techniques. We also show that the response\nvalidation step is a necessary complement to the response generation step.\n", "link": "http://arxiv.org/abs/2405.00099v2", "date": "2024-05-09", "relevancy": 2.3512, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4872}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4631}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creative%20Beam%20Search%3A%20LLM-as-a-Judge%20For%20Improving%20Response%20Generation&body=Title%3A%20Creative%20Beam%20Search%3A%20LLM-as-a-Judge%20For%20Improving%20Response%20Generation%0AAuthor%3A%20Giorgio%20Franceschelli%20and%20Mirco%20Musolesi%0AAbstract%3A%20%20%20Large%20language%20models%20are%20revolutionizing%20several%20areas%2C%20including%20artificial%0Acreativity.%20However%2C%20the%20process%20of%20generation%20in%20machines%20profoundly%20diverges%0Afrom%20that%20observed%20in%20humans.%20In%20particular%2C%20machine%20generation%20is%0Acharacterized%20by%20a%20lack%20of%20intentionality%20and%20an%20underlying%20creative%20process.%0AWe%20propose%20a%20method%20called%20Creative%20Beam%20Search%20that%20uses%20Diverse%20Beam%20Search%0Aand%20LLM-as-a-Judge%20to%20perform%20response%20generation%20and%20response%20validation.%20The%0Aresults%20of%20a%20qualitative%20experiment%20show%20how%20our%20approach%20can%20provide%20better%0Aoutput%20than%20standard%20sampling%20techniques.%20We%20also%20show%20that%20the%20response%0Avalidation%20step%20is%20a%20necessary%20complement%20to%20the%20response%20generation%20step.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00099v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreative%2520Beam%2520Search%253A%2520LLM-as-a-Judge%2520For%2520Improving%2520Response%2520Generation%26entry.906535625%3DGiorgio%2520Franceschelli%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520revolutionizing%2520several%2520areas%252C%2520including%2520artificial%250Acreativity.%2520However%252C%2520the%2520process%2520of%2520generation%2520in%2520machines%2520profoundly%2520diverges%250Afrom%2520that%2520observed%2520in%2520humans.%2520In%2520particular%252C%2520machine%2520generation%2520is%250Acharacterized%2520by%2520a%2520lack%2520of%2520intentionality%2520and%2520an%2520underlying%2520creative%2520process.%250AWe%2520propose%2520a%2520method%2520called%2520Creative%2520Beam%2520Search%2520that%2520uses%2520Diverse%2520Beam%2520Search%250Aand%2520LLM-as-a-Judge%2520to%2520perform%2520response%2520generation%2520and%2520response%2520validation.%2520The%250Aresults%2520of%2520a%2520qualitative%2520experiment%2520show%2520how%2520our%2520approach%2520can%2520provide%2520better%250Aoutput%2520than%2520standard%2520sampling%2520techniques.%2520We%2520also%2520show%2520that%2520the%2520response%250Avalidation%2520step%2520is%2520a%2520necessary%2520complement%2520to%2520the%2520response%2520generation%2520step.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00099v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creative%20Beam%20Search%3A%20LLM-as-a-Judge%20For%20Improving%20Response%20Generation&entry.906535625=Giorgio%20Franceschelli%20and%20Mirco%20Musolesi&entry.1292438233=%20%20Large%20language%20models%20are%20revolutionizing%20several%20areas%2C%20including%20artificial%0Acreativity.%20However%2C%20the%20process%20of%20generation%20in%20machines%20profoundly%20diverges%0Afrom%20that%20observed%20in%20humans.%20In%20particular%2C%20machine%20generation%20is%0Acharacterized%20by%20a%20lack%20of%20intentionality%20and%20an%20underlying%20creative%20process.%0AWe%20propose%20a%20method%20called%20Creative%20Beam%20Search%20that%20uses%20Diverse%20Beam%20Search%0Aand%20LLM-as-a-Judge%20to%20perform%20response%20generation%20and%20response%20validation.%20The%0Aresults%20of%20a%20qualitative%20experiment%20show%20how%20our%20approach%20can%20provide%20better%0Aoutput%20than%20standard%20sampling%20techniques.%20We%20also%20show%20that%20the%20response%0Avalidation%20step%20is%20a%20necessary%20complement%20to%20the%20response%20generation%20step.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00099v2&entry.124074799=Read"},
{"title": "An Embodied Generalist Agent in 3D World", "author": "Jiangyong Huang and Silong Yong and Xiaojian Ma and Xiongkun Linghu and Puhao Li and Yan Wang and Qing Li and Song-Chun Zhu and Baoxiong Jia and Siyuan Huang", "abstract": "  Leveraging massive knowledge from large language models (LLMs), recent\nmachine learning models show notable successes in general-purpose task solving\nin diverse domains such as computer vision and robotics. However, several\nsignificant challenges remain: (i) most of these models rely on 2D images yet\nexhibit a limited capacity for 3D input; (ii) these models rarely explore the\ntasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning\nand acting. We argue these limitations significantly hinder current models from\nperforming real-world tasks and approaching general intelligence. To this end,\nwe introduce LEO, an embodied multi-modal generalist agent that excels in\nperceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is\ntrained with a unified task interface, model architecture, and objective in two\nstages: (i) 3D vision-language (VL) alignment and (ii) 3D\nvision-language-action (VLA) instruction tuning. We collect large-scale\ndatasets comprising diverse object-level and scene-level tasks, which require\nconsiderable understanding of and interaction with the 3D world. Moreover, we\nmeticulously design an LLM-assisted pipeline to produce high-quality 3D VL\ndata. Through extensive experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, navigation and manipulation. Our ablative\nstudies and scaling analyses further provide valuable insights for developing\nfuture embodied generalist agents. Code and data are available on project page.\n", "link": "http://arxiv.org/abs/2311.12871v3", "date": "2024-05-09", "relevancy": 2.346, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6114}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Embodied%20Generalist%20Agent%20in%203D%20World&body=Title%3A%20An%20Embodied%20Generalist%20Agent%20in%203D%20World%0AAuthor%3A%20Jiangyong%20Huang%20and%20Silong%20Yong%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Puhao%20Li%20and%20Yan%20Wang%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Leveraging%20massive%20knowledge%20from%20large%20language%20models%20%28LLMs%29%2C%20recent%0Amachine%20learning%20models%20show%20notable%20successes%20in%20general-purpose%20task%20solving%0Ain%20diverse%20domains%20such%20as%20computer%20vision%20and%20robotics.%20However%2C%20several%0Asignificant%20challenges%20remain%3A%20%28i%29%20most%20of%20these%20models%20rely%20on%202D%20images%20yet%0Aexhibit%20a%20limited%20capacity%20for%203D%20input%3B%20%28ii%29%20these%20models%20rarely%20explore%20the%0Atasks%20inherently%20defined%20in%203D%20world%2C%20e.g.%2C%203D%20grounding%2C%20embodied%20reasoning%0Aand%20acting.%20We%20argue%20these%20limitations%20significantly%20hinder%20current%20models%20from%0Aperforming%20real-world%20tasks%20and%20approaching%20general%20intelligence.%20To%20this%20end%2C%0Awe%20introduce%20LEO%2C%20an%20embodied%20multi-modal%20generalist%20agent%20that%20excels%20in%0Aperceiving%2C%20grounding%2C%20reasoning%2C%20planning%2C%20and%20acting%20in%20the%203D%20world.%20LEO%20is%0Atrained%20with%20a%20unified%20task%20interface%2C%20model%20architecture%2C%20and%20objective%20in%20two%0Astages%3A%20%28i%29%203D%20vision-language%20%28VL%29%20alignment%20and%20%28ii%29%203D%0Avision-language-action%20%28VLA%29%20instruction%20tuning.%20We%20collect%20large-scale%0Adatasets%20comprising%20diverse%20object-level%20and%20scene-level%20tasks%2C%20which%20require%0Aconsiderable%20understanding%20of%20and%20interaction%20with%20the%203D%20world.%20Moreover%2C%20we%0Ameticulously%20design%20an%20LLM-assisted%20pipeline%20to%20produce%20high-quality%203D%20VL%0Adata.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20LEO%27s%20remarkable%0Aproficiency%20across%20a%20wide%20spectrum%20of%20tasks%2C%20including%203D%20captioning%2C%20question%0Aanswering%2C%20embodied%20reasoning%2C%20navigation%20and%20manipulation.%20Our%20ablative%0Astudies%20and%20scaling%20analyses%20further%20provide%20valuable%20insights%20for%20developing%0Afuture%20embodied%20generalist%20agents.%20Code%20and%20data%20are%20available%20on%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12871v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Embodied%2520Generalist%2520Agent%2520in%25203D%2520World%26entry.906535625%3DJiangyong%2520Huang%2520and%2520Silong%2520Yong%2520and%2520Xiaojian%2520Ma%2520and%2520Xiongkun%2520Linghu%2520and%2520Puhao%2520Li%2520and%2520Yan%2520Wang%2520and%2520Qing%2520Li%2520and%2520Song-Chun%2520Zhu%2520and%2520Baoxiong%2520Jia%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Leveraging%2520massive%2520knowledge%2520from%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520recent%250Amachine%2520learning%2520models%2520show%2520notable%2520successes%2520in%2520general-purpose%2520task%2520solving%250Ain%2520diverse%2520domains%2520such%2520as%2520computer%2520vision%2520and%2520robotics.%2520However%252C%2520several%250Asignificant%2520challenges%2520remain%253A%2520%2528i%2529%2520most%2520of%2520these%2520models%2520rely%2520on%25202D%2520images%2520yet%250Aexhibit%2520a%2520limited%2520capacity%2520for%25203D%2520input%253B%2520%2528ii%2529%2520these%2520models%2520rarely%2520explore%2520the%250Atasks%2520inherently%2520defined%2520in%25203D%2520world%252C%2520e.g.%252C%25203D%2520grounding%252C%2520embodied%2520reasoning%250Aand%2520acting.%2520We%2520argue%2520these%2520limitations%2520significantly%2520hinder%2520current%2520models%2520from%250Aperforming%2520real-world%2520tasks%2520and%2520approaching%2520general%2520intelligence.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520LEO%252C%2520an%2520embodied%2520multi-modal%2520generalist%2520agent%2520that%2520excels%2520in%250Aperceiving%252C%2520grounding%252C%2520reasoning%252C%2520planning%252C%2520and%2520acting%2520in%2520the%25203D%2520world.%2520LEO%2520is%250Atrained%2520with%2520a%2520unified%2520task%2520interface%252C%2520model%2520architecture%252C%2520and%2520objective%2520in%2520two%250Astages%253A%2520%2528i%2529%25203D%2520vision-language%2520%2528VL%2529%2520alignment%2520and%2520%2528ii%2529%25203D%250Avision-language-action%2520%2528VLA%2529%2520instruction%2520tuning.%2520We%2520collect%2520large-scale%250Adatasets%2520comprising%2520diverse%2520object-level%2520and%2520scene-level%2520tasks%252C%2520which%2520require%250Aconsiderable%2520understanding%2520of%2520and%2520interaction%2520with%2520the%25203D%2520world.%2520Moreover%252C%2520we%250Ameticulously%2520design%2520an%2520LLM-assisted%2520pipeline%2520to%2520produce%2520high-quality%25203D%2520VL%250Adata.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520LEO%2527s%2520remarkable%250Aproficiency%2520across%2520a%2520wide%2520spectrum%2520of%2520tasks%252C%2520including%25203D%2520captioning%252C%2520question%250Aanswering%252C%2520embodied%2520reasoning%252C%2520navigation%2520and%2520manipulation.%2520Our%2520ablative%250Astudies%2520and%2520scaling%2520analyses%2520further%2520provide%2520valuable%2520insights%2520for%2520developing%250Afuture%2520embodied%2520generalist%2520agents.%2520Code%2520and%2520data%2520are%2520available%2520on%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12871v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Embodied%20Generalist%20Agent%20in%203D%20World&entry.906535625=Jiangyong%20Huang%20and%20Silong%20Yong%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Puhao%20Li%20and%20Yan%20Wang%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang&entry.1292438233=%20%20Leveraging%20massive%20knowledge%20from%20large%20language%20models%20%28LLMs%29%2C%20recent%0Amachine%20learning%20models%20show%20notable%20successes%20in%20general-purpose%20task%20solving%0Ain%20diverse%20domains%20such%20as%20computer%20vision%20and%20robotics.%20However%2C%20several%0Asignificant%20challenges%20remain%3A%20%28i%29%20most%20of%20these%20models%20rely%20on%202D%20images%20yet%0Aexhibit%20a%20limited%20capacity%20for%203D%20input%3B%20%28ii%29%20these%20models%20rarely%20explore%20the%0Atasks%20inherently%20defined%20in%203D%20world%2C%20e.g.%2C%203D%20grounding%2C%20embodied%20reasoning%0Aand%20acting.%20We%20argue%20these%20limitations%20significantly%20hinder%20current%20models%20from%0Aperforming%20real-world%20tasks%20and%20approaching%20general%20intelligence.%20To%20this%20end%2C%0Awe%20introduce%20LEO%2C%20an%20embodied%20multi-modal%20generalist%20agent%20that%20excels%20in%0Aperceiving%2C%20grounding%2C%20reasoning%2C%20planning%2C%20and%20acting%20in%20the%203D%20world.%20LEO%20is%0Atrained%20with%20a%20unified%20task%20interface%2C%20model%20architecture%2C%20and%20objective%20in%20two%0Astages%3A%20%28i%29%203D%20vision-language%20%28VL%29%20alignment%20and%20%28ii%29%203D%0Avision-language-action%20%28VLA%29%20instruction%20tuning.%20We%20collect%20large-scale%0Adatasets%20comprising%20diverse%20object-level%20and%20scene-level%20tasks%2C%20which%20require%0Aconsiderable%20understanding%20of%20and%20interaction%20with%20the%203D%20world.%20Moreover%2C%20we%0Ameticulously%20design%20an%20LLM-assisted%20pipeline%20to%20produce%20high-quality%203D%20VL%0Adata.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20LEO%27s%20remarkable%0Aproficiency%20across%20a%20wide%20spectrum%20of%20tasks%2C%20including%203D%20captioning%2C%20question%0Aanswering%2C%20embodied%20reasoning%2C%20navigation%20and%20manipulation.%20Our%20ablative%0Astudies%20and%20scaling%20analyses%20further%20provide%20valuable%20insights%20for%20developing%0Afuture%20embodied%20generalist%20agents.%20Code%20and%20data%20are%20available%20on%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12871v3&entry.124074799=Read"},
{"title": "Deploying Graph Neural Networks in Wireless Networks: A Link Stability\n  Viewpoint", "author": "Jun Li and Weiwei Zhang and Kang Wei and Guangji Chen and Long Shi and Wen Chen", "abstract": "  As an emerging artificial intelligence technology, graph neural networks\n(GNNs) have exhibited promising performance across a wide range of\ngraph-related applications. However, information exchanges among neighbor nodes\nin GNN pose new challenges in the resource-constrained scenario, especially in\nwireless systems. In practical wireless systems, the communication links among\nnodes are usually unreliable due to wireless fading and receiver noise,\nconsequently resulting in performance degradation of GNNs. To improve the\nlearning performance of GNNs, we aim to maximize the number of long-term\naverage (LTA) communication links by the optimized power control under energy\nconsumption constraints. Using the Lyapunov optimization method, we first\ntransform the intractable long-term problem into a deterministic problem in\neach time slot by converting the long-term energy constraints into the\nobjective function. In spite of this non-convex combinatorial optimization\nproblem, we address this problem via equivalently solving a sequence of convex\nfeasibility problems together with a greedy based solver. Simulation results\ndemonstrate the superiority of our proposed scheme over the baselines.\n", "link": "http://arxiv.org/abs/2405.05802v1", "date": "2024-05-09", "relevancy": 2.3227, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4997}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4502}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deploying%20Graph%20Neural%20Networks%20in%20Wireless%20Networks%3A%20A%20Link%20Stability%0A%20%20Viewpoint&body=Title%3A%20Deploying%20Graph%20Neural%20Networks%20in%20Wireless%20Networks%3A%20A%20Link%20Stability%0A%20%20Viewpoint%0AAuthor%3A%20Jun%20Li%20and%20Weiwei%20Zhang%20and%20Kang%20Wei%20and%20Guangji%20Chen%20and%20Long%20Shi%20and%20Wen%20Chen%0AAbstract%3A%20%20%20As%20an%20emerging%20artificial%20intelligence%20technology%2C%20graph%20neural%20networks%0A%28GNNs%29%20have%20exhibited%20promising%20performance%20across%20a%20wide%20range%20of%0Agraph-related%20applications.%20However%2C%20information%20exchanges%20among%20neighbor%20nodes%0Ain%20GNN%20pose%20new%20challenges%20in%20the%20resource-constrained%20scenario%2C%20especially%20in%0Awireless%20systems.%20In%20practical%20wireless%20systems%2C%20the%20communication%20links%20among%0Anodes%20are%20usually%20unreliable%20due%20to%20wireless%20fading%20and%20receiver%20noise%2C%0Aconsequently%20resulting%20in%20performance%20degradation%20of%20GNNs.%20To%20improve%20the%0Alearning%20performance%20of%20GNNs%2C%20we%20aim%20to%20maximize%20the%20number%20of%20long-term%0Aaverage%20%28LTA%29%20communication%20links%20by%20the%20optimized%20power%20control%20under%20energy%0Aconsumption%20constraints.%20Using%20the%20Lyapunov%20optimization%20method%2C%20we%20first%0Atransform%20the%20intractable%20long-term%20problem%20into%20a%20deterministic%20problem%20in%0Aeach%20time%20slot%20by%20converting%20the%20long-term%20energy%20constraints%20into%20the%0Aobjective%20function.%20In%20spite%20of%20this%20non-convex%20combinatorial%20optimization%0Aproblem%2C%20we%20address%20this%20problem%20via%20equivalently%20solving%20a%20sequence%20of%20convex%0Afeasibility%20problems%20together%20with%20a%20greedy%20based%20solver.%20Simulation%20results%0Ademonstrate%20the%20superiority%20of%20our%20proposed%20scheme%20over%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeploying%2520Graph%2520Neural%2520Networks%2520in%2520Wireless%2520Networks%253A%2520A%2520Link%2520Stability%250A%2520%2520Viewpoint%26entry.906535625%3DJun%2520Li%2520and%2520Weiwei%2520Zhang%2520and%2520Kang%2520Wei%2520and%2520Guangji%2520Chen%2520and%2520Long%2520Shi%2520and%2520Wen%2520Chen%26entry.1292438233%3D%2520%2520As%2520an%2520emerging%2520artificial%2520intelligence%2520technology%252C%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520have%2520exhibited%2520promising%2520performance%2520across%2520a%2520wide%2520range%2520of%250Agraph-related%2520applications.%2520However%252C%2520information%2520exchanges%2520among%2520neighbor%2520nodes%250Ain%2520GNN%2520pose%2520new%2520challenges%2520in%2520the%2520resource-constrained%2520scenario%252C%2520especially%2520in%250Awireless%2520systems.%2520In%2520practical%2520wireless%2520systems%252C%2520the%2520communication%2520links%2520among%250Anodes%2520are%2520usually%2520unreliable%2520due%2520to%2520wireless%2520fading%2520and%2520receiver%2520noise%252C%250Aconsequently%2520resulting%2520in%2520performance%2520degradation%2520of%2520GNNs.%2520To%2520improve%2520the%250Alearning%2520performance%2520of%2520GNNs%252C%2520we%2520aim%2520to%2520maximize%2520the%2520number%2520of%2520long-term%250Aaverage%2520%2528LTA%2529%2520communication%2520links%2520by%2520the%2520optimized%2520power%2520control%2520under%2520energy%250Aconsumption%2520constraints.%2520Using%2520the%2520Lyapunov%2520optimization%2520method%252C%2520we%2520first%250Atransform%2520the%2520intractable%2520long-term%2520problem%2520into%2520a%2520deterministic%2520problem%2520in%250Aeach%2520time%2520slot%2520by%2520converting%2520the%2520long-term%2520energy%2520constraints%2520into%2520the%250Aobjective%2520function.%2520In%2520spite%2520of%2520this%2520non-convex%2520combinatorial%2520optimization%250Aproblem%252C%2520we%2520address%2520this%2520problem%2520via%2520equivalently%2520solving%2520a%2520sequence%2520of%2520convex%250Afeasibility%2520problems%2520together%2520with%2520a%2520greedy%2520based%2520solver.%2520Simulation%2520results%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520scheme%2520over%2520the%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deploying%20Graph%20Neural%20Networks%20in%20Wireless%20Networks%3A%20A%20Link%20Stability%0A%20%20Viewpoint&entry.906535625=Jun%20Li%20and%20Weiwei%20Zhang%20and%20Kang%20Wei%20and%20Guangji%20Chen%20and%20Long%20Shi%20and%20Wen%20Chen&entry.1292438233=%20%20As%20an%20emerging%20artificial%20intelligence%20technology%2C%20graph%20neural%20networks%0A%28GNNs%29%20have%20exhibited%20promising%20performance%20across%20a%20wide%20range%20of%0Agraph-related%20applications.%20However%2C%20information%20exchanges%20among%20neighbor%20nodes%0Ain%20GNN%20pose%20new%20challenges%20in%20the%20resource-constrained%20scenario%2C%20especially%20in%0Awireless%20systems.%20In%20practical%20wireless%20systems%2C%20the%20communication%20links%20among%0Anodes%20are%20usually%20unreliable%20due%20to%20wireless%20fading%20and%20receiver%20noise%2C%0Aconsequently%20resulting%20in%20performance%20degradation%20of%20GNNs.%20To%20improve%20the%0Alearning%20performance%20of%20GNNs%2C%20we%20aim%20to%20maximize%20the%20number%20of%20long-term%0Aaverage%20%28LTA%29%20communication%20links%20by%20the%20optimized%20power%20control%20under%20energy%0Aconsumption%20constraints.%20Using%20the%20Lyapunov%20optimization%20method%2C%20we%20first%0Atransform%20the%20intractable%20long-term%20problem%20into%20a%20deterministic%20problem%20in%0Aeach%20time%20slot%20by%20converting%20the%20long-term%20energy%20constraints%20into%20the%0Aobjective%20function.%20In%20spite%20of%20this%20non-convex%20combinatorial%20optimization%0Aproblem%2C%20we%20address%20this%20problem%20via%20equivalently%20solving%20a%20sequence%20of%20convex%0Afeasibility%20problems%20together%20with%20a%20greedy%20based%20solver.%20Simulation%20results%0Ademonstrate%20the%20superiority%20of%20our%20proposed%20scheme%20over%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05802v1&entry.124074799=Read"},
{"title": "Co-driver: VLM-based Autonomous Driving Assistant with Human-like\n  Behavior and Understanding for Complex Road Scenes", "author": "Ziang Guo and Artem Lykov and Zakhar Yagudin and Mikhail Konenkov and Dzmitry Tsetserukou", "abstract": "  Recent research about Large Language Model based autonomous driving solutions\nshows a promising picture in planning and control fields. However, heavy\ncomputational resources and hallucinations of Large Language Models continue to\nhinder the tasks of predicting precise trajectories and instructing control\nsignals. To address this problem, we propose Co-driver, a novel autonomous\ndriving assistant system to empower autonomous vehicles with adjustable driving\nbehaviors based on the understanding of road scenes. A pipeline involving the\nCARLA simulator and Robot Operating System 2 (ROS2) verifying the effectiveness\nof our system is presented, utilizing a single Nvidia 4090 24G GPU while\nexploiting the capacity of textual output of the Visual Language Model.\nBesides, we also contribute a dataset containing an image set and a\ncorresponding prompt set for fine-tuning the Visual Language Model module of\nour system. In the real-world driving dataset, our system achieved 96.16%\nsuccess rate in night scenes and 89.7% in gloomy scenes regarding reasonable\npredictions. Our Co-driver dataset will be released at\nhttps://github.com/ZionGo6/Co-driver.\n", "link": "http://arxiv.org/abs/2405.05885v1", "date": "2024-05-09", "relevancy": 2.2718, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5753}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5714}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-driver%3A%20VLM-based%20Autonomous%20Driving%20Assistant%20with%20Human-like%0A%20%20Behavior%20and%20Understanding%20for%20Complex%20Road%20Scenes&body=Title%3A%20Co-driver%3A%20VLM-based%20Autonomous%20Driving%20Assistant%20with%20Human-like%0A%20%20Behavior%20and%20Understanding%20for%20Complex%20Road%20Scenes%0AAuthor%3A%20Ziang%20Guo%20and%20Artem%20Lykov%20and%20Zakhar%20Yagudin%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20Recent%20research%20about%20Large%20Language%20Model%20based%20autonomous%20driving%20solutions%0Ashows%20a%20promising%20picture%20in%20planning%20and%20control%20fields.%20However%2C%20heavy%0Acomputational%20resources%20and%20hallucinations%20of%20Large%20Language%20Models%20continue%20to%0Ahinder%20the%20tasks%20of%20predicting%20precise%20trajectories%20and%20instructing%20control%0Asignals.%20To%20address%20this%20problem%2C%20we%20propose%20Co-driver%2C%20a%20novel%20autonomous%0Adriving%20assistant%20system%20to%20empower%20autonomous%20vehicles%20with%20adjustable%20driving%0Abehaviors%20based%20on%20the%20understanding%20of%20road%20scenes.%20A%20pipeline%20involving%20the%0ACARLA%20simulator%20and%20Robot%20Operating%20System%202%20%28ROS2%29%20verifying%20the%20effectiveness%0Aof%20our%20system%20is%20presented%2C%20utilizing%20a%20single%20Nvidia%204090%2024G%20GPU%20while%0Aexploiting%20the%20capacity%20of%20textual%20output%20of%20the%20Visual%20Language%20Model.%0ABesides%2C%20we%20also%20contribute%20a%20dataset%20containing%20an%20image%20set%20and%20a%0Acorresponding%20prompt%20set%20for%20fine-tuning%20the%20Visual%20Language%20Model%20module%20of%0Aour%20system.%20In%20the%20real-world%20driving%20dataset%2C%20our%20system%20achieved%2096.16%25%0Asuccess%20rate%20in%20night%20scenes%20and%2089.7%25%20in%20gloomy%20scenes%20regarding%20reasonable%0Apredictions.%20Our%20Co-driver%20dataset%20will%20be%20released%20at%0Ahttps%3A//github.com/ZionGo6/Co-driver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-driver%253A%2520VLM-based%2520Autonomous%2520Driving%2520Assistant%2520with%2520Human-like%250A%2520%2520Behavior%2520and%2520Understanding%2520for%2520Complex%2520Road%2520Scenes%26entry.906535625%3DZiang%2520Guo%2520and%2520Artem%2520Lykov%2520and%2520Zakhar%2520Yagudin%2520and%2520Mikhail%2520Konenkov%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520Recent%2520research%2520about%2520Large%2520Language%2520Model%2520based%2520autonomous%2520driving%2520solutions%250Ashows%2520a%2520promising%2520picture%2520in%2520planning%2520and%2520control%2520fields.%2520However%252C%2520heavy%250Acomputational%2520resources%2520and%2520hallucinations%2520of%2520Large%2520Language%2520Models%2520continue%2520to%250Ahinder%2520the%2520tasks%2520of%2520predicting%2520precise%2520trajectories%2520and%2520instructing%2520control%250Asignals.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Co-driver%252C%2520a%2520novel%2520autonomous%250Adriving%2520assistant%2520system%2520to%2520empower%2520autonomous%2520vehicles%2520with%2520adjustable%2520driving%250Abehaviors%2520based%2520on%2520the%2520understanding%2520of%2520road%2520scenes.%2520A%2520pipeline%2520involving%2520the%250ACARLA%2520simulator%2520and%2520Robot%2520Operating%2520System%25202%2520%2528ROS2%2529%2520verifying%2520the%2520effectiveness%250Aof%2520our%2520system%2520is%2520presented%252C%2520utilizing%2520a%2520single%2520Nvidia%25204090%252024G%2520GPU%2520while%250Aexploiting%2520the%2520capacity%2520of%2520textual%2520output%2520of%2520the%2520Visual%2520Language%2520Model.%250ABesides%252C%2520we%2520also%2520contribute%2520a%2520dataset%2520containing%2520an%2520image%2520set%2520and%2520a%250Acorresponding%2520prompt%2520set%2520for%2520fine-tuning%2520the%2520Visual%2520Language%2520Model%2520module%2520of%250Aour%2520system.%2520In%2520the%2520real-world%2520driving%2520dataset%252C%2520our%2520system%2520achieved%252096.16%2525%250Asuccess%2520rate%2520in%2520night%2520scenes%2520and%252089.7%2525%2520in%2520gloomy%2520scenes%2520regarding%2520reasonable%250Apredictions.%2520Our%2520Co-driver%2520dataset%2520will%2520be%2520released%2520at%250Ahttps%253A//github.com/ZionGo6/Co-driver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-driver%3A%20VLM-based%20Autonomous%20Driving%20Assistant%20with%20Human-like%0A%20%20Behavior%20and%20Understanding%20for%20Complex%20Road%20Scenes&entry.906535625=Ziang%20Guo%20and%20Artem%20Lykov%20and%20Zakhar%20Yagudin%20and%20Mikhail%20Konenkov%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20Recent%20research%20about%20Large%20Language%20Model%20based%20autonomous%20driving%20solutions%0Ashows%20a%20promising%20picture%20in%20planning%20and%20control%20fields.%20However%2C%20heavy%0Acomputational%20resources%20and%20hallucinations%20of%20Large%20Language%20Models%20continue%20to%0Ahinder%20the%20tasks%20of%20predicting%20precise%20trajectories%20and%20instructing%20control%0Asignals.%20To%20address%20this%20problem%2C%20we%20propose%20Co-driver%2C%20a%20novel%20autonomous%0Adriving%20assistant%20system%20to%20empower%20autonomous%20vehicles%20with%20adjustable%20driving%0Abehaviors%20based%20on%20the%20understanding%20of%20road%20scenes.%20A%20pipeline%20involving%20the%0ACARLA%20simulator%20and%20Robot%20Operating%20System%202%20%28ROS2%29%20verifying%20the%20effectiveness%0Aof%20our%20system%20is%20presented%2C%20utilizing%20a%20single%20Nvidia%204090%2024G%20GPU%20while%0Aexploiting%20the%20capacity%20of%20textual%20output%20of%20the%20Visual%20Language%20Model.%0ABesides%2C%20we%20also%20contribute%20a%20dataset%20containing%20an%20image%20set%20and%20a%0Acorresponding%20prompt%20set%20for%20fine-tuning%20the%20Visual%20Language%20Model%20module%20of%0Aour%20system.%20In%20the%20real-world%20driving%20dataset%2C%20our%20system%20achieved%2096.16%25%0Asuccess%20rate%20in%20night%20scenes%20and%2089.7%25%20in%20gloomy%20scenes%20regarding%20reasonable%0Apredictions.%20Our%20Co-driver%20dataset%20will%20be%20released%20at%0Ahttps%3A//github.com/ZionGo6/Co-driver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05885v1&entry.124074799=Read"},
{"title": "MAD-ICP: It Is All About Matching Data -- Robust and Informed LiDAR\n  Odometry", "author": "Simone Ferrari and Luca Di Giammarino and Leonardo Brizi and Giorgio Grisetti", "abstract": "  LiDAR odometry is the task of estimating the ego-motion of the sensor from\nsequential laser scans. This problem has been addressed by the community for\nmore than two decades, and many effective solutions are available nowadays.\nMost of these systems implicitly rely on assumptions about the operating\nenvironment, the sensor used, and motion pattern. When these assumptions are\nviolated, several well-known systems tend to perform poorly. This paper\npresents a LiDAR odometry system that can overcome these limitations and\noperate well under different operating conditions while achieving performance\ncomparable with domain-specific methods. Our algorithm follows the well-known\nICP paradigm that leverages a PCA-based kd-tree implementation that is used to\nextract structural information about the clouds being registered and to compute\nthe minimization metric for the alignment. The drift is bound by managing the\nlocal map based on the estimated uncertainty of the tracked pose. To benefit\nthe community, we release an open-source C++ anytime real-time implementation.\n", "link": "http://arxiv.org/abs/2405.05828v1", "date": "2024-05-09", "relevancy": 2.2574, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5951}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.57}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAD-ICP%3A%20It%20Is%20All%20About%20Matching%20Data%20--%20Robust%20and%20Informed%20LiDAR%0A%20%20Odometry&body=Title%3A%20MAD-ICP%3A%20It%20Is%20All%20About%20Matching%20Data%20--%20Robust%20and%20Informed%20LiDAR%0A%20%20Odometry%0AAuthor%3A%20Simone%20Ferrari%20and%20Luca%20Di%20Giammarino%20and%20Leonardo%20Brizi%20and%20Giorgio%20Grisetti%0AAbstract%3A%20%20%20LiDAR%20odometry%20is%20the%20task%20of%20estimating%20the%20ego-motion%20of%20the%20sensor%20from%0Asequential%20laser%20scans.%20This%20problem%20has%20been%20addressed%20by%20the%20community%20for%0Amore%20than%20two%20decades%2C%20and%20many%20effective%20solutions%20are%20available%20nowadays.%0AMost%20of%20these%20systems%20implicitly%20rely%20on%20assumptions%20about%20the%20operating%0Aenvironment%2C%20the%20sensor%20used%2C%20and%20motion%20pattern.%20When%20these%20assumptions%20are%0Aviolated%2C%20several%20well-known%20systems%20tend%20to%20perform%20poorly.%20This%20paper%0Apresents%20a%20LiDAR%20odometry%20system%20that%20can%20overcome%20these%20limitations%20and%0Aoperate%20well%20under%20different%20operating%20conditions%20while%20achieving%20performance%0Acomparable%20with%20domain-specific%20methods.%20Our%20algorithm%20follows%20the%20well-known%0AICP%20paradigm%20that%20leverages%20a%20PCA-based%20kd-tree%20implementation%20that%20is%20used%20to%0Aextract%20structural%20information%20about%20the%20clouds%20being%20registered%20and%20to%20compute%0Athe%20minimization%20metric%20for%20the%20alignment.%20The%20drift%20is%20bound%20by%20managing%20the%0Alocal%20map%20based%20on%20the%20estimated%20uncertainty%20of%20the%20tracked%20pose.%20To%20benefit%0Athe%20community%2C%20we%20release%20an%20open-source%20C%2B%2B%20anytime%20real-time%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAD-ICP%253A%2520It%2520Is%2520All%2520About%2520Matching%2520Data%2520--%2520Robust%2520and%2520Informed%2520LiDAR%250A%2520%2520Odometry%26entry.906535625%3DSimone%2520Ferrari%2520and%2520Luca%2520Di%2520Giammarino%2520and%2520Leonardo%2520Brizi%2520and%2520Giorgio%2520Grisetti%26entry.1292438233%3D%2520%2520LiDAR%2520odometry%2520is%2520the%2520task%2520of%2520estimating%2520the%2520ego-motion%2520of%2520the%2520sensor%2520from%250Asequential%2520laser%2520scans.%2520This%2520problem%2520has%2520been%2520addressed%2520by%2520the%2520community%2520for%250Amore%2520than%2520two%2520decades%252C%2520and%2520many%2520effective%2520solutions%2520are%2520available%2520nowadays.%250AMost%2520of%2520these%2520systems%2520implicitly%2520rely%2520on%2520assumptions%2520about%2520the%2520operating%250Aenvironment%252C%2520the%2520sensor%2520used%252C%2520and%2520motion%2520pattern.%2520When%2520these%2520assumptions%2520are%250Aviolated%252C%2520several%2520well-known%2520systems%2520tend%2520to%2520perform%2520poorly.%2520This%2520paper%250Apresents%2520a%2520LiDAR%2520odometry%2520system%2520that%2520can%2520overcome%2520these%2520limitations%2520and%250Aoperate%2520well%2520under%2520different%2520operating%2520conditions%2520while%2520achieving%2520performance%250Acomparable%2520with%2520domain-specific%2520methods.%2520Our%2520algorithm%2520follows%2520the%2520well-known%250AICP%2520paradigm%2520that%2520leverages%2520a%2520PCA-based%2520kd-tree%2520implementation%2520that%2520is%2520used%2520to%250Aextract%2520structural%2520information%2520about%2520the%2520clouds%2520being%2520registered%2520and%2520to%2520compute%250Athe%2520minimization%2520metric%2520for%2520the%2520alignment.%2520The%2520drift%2520is%2520bound%2520by%2520managing%2520the%250Alocal%2520map%2520based%2520on%2520the%2520estimated%2520uncertainty%2520of%2520the%2520tracked%2520pose.%2520To%2520benefit%250Athe%2520community%252C%2520we%2520release%2520an%2520open-source%2520C%252B%252B%2520anytime%2520real-time%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAD-ICP%3A%20It%20Is%20All%20About%20Matching%20Data%20--%20Robust%20and%20Informed%20LiDAR%0A%20%20Odometry&entry.906535625=Simone%20Ferrari%20and%20Luca%20Di%20Giammarino%20and%20Leonardo%20Brizi%20and%20Giorgio%20Grisetti&entry.1292438233=%20%20LiDAR%20odometry%20is%20the%20task%20of%20estimating%20the%20ego-motion%20of%20the%20sensor%20from%0Asequential%20laser%20scans.%20This%20problem%20has%20been%20addressed%20by%20the%20community%20for%0Amore%20than%20two%20decades%2C%20and%20many%20effective%20solutions%20are%20available%20nowadays.%0AMost%20of%20these%20systems%20implicitly%20rely%20on%20assumptions%20about%20the%20operating%0Aenvironment%2C%20the%20sensor%20used%2C%20and%20motion%20pattern.%20When%20these%20assumptions%20are%0Aviolated%2C%20several%20well-known%20systems%20tend%20to%20perform%20poorly.%20This%20paper%0Apresents%20a%20LiDAR%20odometry%20system%20that%20can%20overcome%20these%20limitations%20and%0Aoperate%20well%20under%20different%20operating%20conditions%20while%20achieving%20performance%0Acomparable%20with%20domain-specific%20methods.%20Our%20algorithm%20follows%20the%20well-known%0AICP%20paradigm%20that%20leverages%20a%20PCA-based%20kd-tree%20implementation%20that%20is%20used%20to%0Aextract%20structural%20information%20about%20the%20clouds%20being%20registered%20and%20to%20compute%0Athe%20minimization%20metric%20for%20the%20alignment.%20The%20drift%20is%20bound%20by%20managing%20the%0Alocal%20map%20based%20on%20the%20estimated%20uncertainty%20of%20the%20tracked%20pose.%20To%20benefit%0Athe%20community%2C%20we%20release%20an%20open-source%20C%2B%2B%20anytime%20real-time%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05828v1&entry.124074799=Read"},
{"title": "Probing Multimodal LLMs as World Models for Driving", "author": "Shiva Sreeram and Tsun-Hsuan Wang and Alaa Maalouf and Guy Rosman and Sertac Karaman and Daniela Rus", "abstract": "  We provide a sober look at the application of Multimodal Large Language\nModels (MLLMs) within the domain of autonomous driving and challenge/verify\nsome common assumptions, focusing on their ability to reason and interpret\ndynamic driving scenarios through sequences of images/frames in a closed-loop\ncontrol environment. Despite the significant advancements in MLLMs like GPT-4V,\ntheir performance in complex, dynamic driving environments remains largely\nuntested and presents a wide area of exploration. We conduct a comprehensive\nexperimental study to evaluate the capability of various MLLMs as world models\nfor driving from the perspective of a fixed in-car camera. Our findings reveal\nthat, while these models proficiently interpret individual images, they\nstruggle significantly with synthesizing coherent narratives or logical\nsequences across frames depicting dynamic behavior. The experiments demonstrate\nconsiderable inaccuracies in predicting (i) basic vehicle dynamics\n(forward/backward, acceleration/deceleration, turning right or left), (ii)\ninteractions with other road actors (e.g., identifying speeding cars or heavy\ntraffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning,\nsuggesting biases in the models' training data. To enable this experimental\nstudy we introduce a specialized simulator, DriveSim, designed to generate\ndiverse driving scenarios, providing a platform for evaluating MLLMs in the\nrealms of driving. Additionally, we contribute the full open-source code and a\nnew dataset, \"Eval-LLM-Drive\", for evaluating MLLMs in driving. Our results\nhighlight a critical gap in the current capabilities of state-of-the-art MLLMs,\nunderscoring the need for enhanced foundation models to improve their\napplicability in real-world dynamic environments.\n", "link": "http://arxiv.org/abs/2405.05956v1", "date": "2024-05-09", "relevancy": 2.2507, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20Multimodal%20LLMs%20as%20World%20Models%20for%20Driving&body=Title%3A%20Probing%20Multimodal%20LLMs%20as%20World%20Models%20for%20Driving%0AAuthor%3A%20Shiva%20Sreeram%20and%20Tsun-Hsuan%20Wang%20and%20Alaa%20Maalouf%20and%20Guy%20Rosman%20and%20Sertac%20Karaman%20and%20Daniela%20Rus%0AAbstract%3A%20%20%20We%20provide%20a%20sober%20look%20at%20the%20application%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20within%20the%20domain%20of%20autonomous%20driving%20and%20challenge/verify%0Asome%20common%20assumptions%2C%20focusing%20on%20their%20ability%20to%20reason%20and%20interpret%0Adynamic%20driving%20scenarios%20through%20sequences%20of%20images/frames%20in%20a%20closed-loop%0Acontrol%20environment.%20Despite%20the%20significant%20advancements%20in%20MLLMs%20like%20GPT-4V%2C%0Atheir%20performance%20in%20complex%2C%20dynamic%20driving%20environments%20remains%20largely%0Auntested%20and%20presents%20a%20wide%20area%20of%20exploration.%20We%20conduct%20a%20comprehensive%0Aexperimental%20study%20to%20evaluate%20the%20capability%20of%20various%20MLLMs%20as%20world%20models%0Afor%20driving%20from%20the%20perspective%20of%20a%20fixed%20in-car%20camera.%20Our%20findings%20reveal%0Athat%2C%20while%20these%20models%20proficiently%20interpret%20individual%20images%2C%20they%0Astruggle%20significantly%20with%20synthesizing%20coherent%20narratives%20or%20logical%0Asequences%20across%20frames%20depicting%20dynamic%20behavior.%20The%20experiments%20demonstrate%0Aconsiderable%20inaccuracies%20in%20predicting%20%28i%29%20basic%20vehicle%20dynamics%0A%28forward/backward%2C%20acceleration/deceleration%2C%20turning%20right%20or%20left%29%2C%20%28ii%29%0Ainteractions%20with%20other%20road%20actors%20%28e.g.%2C%20identifying%20speeding%20cars%20or%20heavy%0Atraffic%29%2C%20%28iii%29%20trajectory%20planning%2C%20and%20%28iv%29%20open-set%20dynamic%20scene%20reasoning%2C%0Asuggesting%20biases%20in%20the%20models%27%20training%20data.%20To%20enable%20this%20experimental%0Astudy%20we%20introduce%20a%20specialized%20simulator%2C%20DriveSim%2C%20designed%20to%20generate%0Adiverse%20driving%20scenarios%2C%20providing%20a%20platform%20for%20evaluating%20MLLMs%20in%20the%0Arealms%20of%20driving.%20Additionally%2C%20we%20contribute%20the%20full%20open-source%20code%20and%20a%0Anew%20dataset%2C%20%22Eval-LLM-Drive%22%2C%20for%20evaluating%20MLLMs%20in%20driving.%20Our%20results%0Ahighlight%20a%20critical%20gap%20in%20the%20current%20capabilities%20of%20state-of-the-art%20MLLMs%2C%0Aunderscoring%20the%20need%20for%20enhanced%20foundation%20models%20to%20improve%20their%0Aapplicability%20in%20real-world%20dynamic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520Multimodal%2520LLMs%2520as%2520World%2520Models%2520for%2520Driving%26entry.906535625%3DShiva%2520Sreeram%2520and%2520Tsun-Hsuan%2520Wang%2520and%2520Alaa%2520Maalouf%2520and%2520Guy%2520Rosman%2520and%2520Sertac%2520Karaman%2520and%2520Daniela%2520Rus%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520sober%2520look%2520at%2520the%2520application%2520of%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520within%2520the%2520domain%2520of%2520autonomous%2520driving%2520and%2520challenge/verify%250Asome%2520common%2520assumptions%252C%2520focusing%2520on%2520their%2520ability%2520to%2520reason%2520and%2520interpret%250Adynamic%2520driving%2520scenarios%2520through%2520sequences%2520of%2520images/frames%2520in%2520a%2520closed-loop%250Acontrol%2520environment.%2520Despite%2520the%2520significant%2520advancements%2520in%2520MLLMs%2520like%2520GPT-4V%252C%250Atheir%2520performance%2520in%2520complex%252C%2520dynamic%2520driving%2520environments%2520remains%2520largely%250Auntested%2520and%2520presents%2520a%2520wide%2520area%2520of%2520exploration.%2520We%2520conduct%2520a%2520comprehensive%250Aexperimental%2520study%2520to%2520evaluate%2520the%2520capability%2520of%2520various%2520MLLMs%2520as%2520world%2520models%250Afor%2520driving%2520from%2520the%2520perspective%2520of%2520a%2520fixed%2520in-car%2520camera.%2520Our%2520findings%2520reveal%250Athat%252C%2520while%2520these%2520models%2520proficiently%2520interpret%2520individual%2520images%252C%2520they%250Astruggle%2520significantly%2520with%2520synthesizing%2520coherent%2520narratives%2520or%2520logical%250Asequences%2520across%2520frames%2520depicting%2520dynamic%2520behavior.%2520The%2520experiments%2520demonstrate%250Aconsiderable%2520inaccuracies%2520in%2520predicting%2520%2528i%2529%2520basic%2520vehicle%2520dynamics%250A%2528forward/backward%252C%2520acceleration/deceleration%252C%2520turning%2520right%2520or%2520left%2529%252C%2520%2528ii%2529%250Ainteractions%2520with%2520other%2520road%2520actors%2520%2528e.g.%252C%2520identifying%2520speeding%2520cars%2520or%2520heavy%250Atraffic%2529%252C%2520%2528iii%2529%2520trajectory%2520planning%252C%2520and%2520%2528iv%2529%2520open-set%2520dynamic%2520scene%2520reasoning%252C%250Asuggesting%2520biases%2520in%2520the%2520models%2527%2520training%2520data.%2520To%2520enable%2520this%2520experimental%250Astudy%2520we%2520introduce%2520a%2520specialized%2520simulator%252C%2520DriveSim%252C%2520designed%2520to%2520generate%250Adiverse%2520driving%2520scenarios%252C%2520providing%2520a%2520platform%2520for%2520evaluating%2520MLLMs%2520in%2520the%250Arealms%2520of%2520driving.%2520Additionally%252C%2520we%2520contribute%2520the%2520full%2520open-source%2520code%2520and%2520a%250Anew%2520dataset%252C%2520%2522Eval-LLM-Drive%2522%252C%2520for%2520evaluating%2520MLLMs%2520in%2520driving.%2520Our%2520results%250Ahighlight%2520a%2520critical%2520gap%2520in%2520the%2520current%2520capabilities%2520of%2520state-of-the-art%2520MLLMs%252C%250Aunderscoring%2520the%2520need%2520for%2520enhanced%2520foundation%2520models%2520to%2520improve%2520their%250Aapplicability%2520in%2520real-world%2520dynamic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20Multimodal%20LLMs%20as%20World%20Models%20for%20Driving&entry.906535625=Shiva%20Sreeram%20and%20Tsun-Hsuan%20Wang%20and%20Alaa%20Maalouf%20and%20Guy%20Rosman%20and%20Sertac%20Karaman%20and%20Daniela%20Rus&entry.1292438233=%20%20We%20provide%20a%20sober%20look%20at%20the%20application%20of%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29%20within%20the%20domain%20of%20autonomous%20driving%20and%20challenge/verify%0Asome%20common%20assumptions%2C%20focusing%20on%20their%20ability%20to%20reason%20and%20interpret%0Adynamic%20driving%20scenarios%20through%20sequences%20of%20images/frames%20in%20a%20closed-loop%0Acontrol%20environment.%20Despite%20the%20significant%20advancements%20in%20MLLMs%20like%20GPT-4V%2C%0Atheir%20performance%20in%20complex%2C%20dynamic%20driving%20environments%20remains%20largely%0Auntested%20and%20presents%20a%20wide%20area%20of%20exploration.%20We%20conduct%20a%20comprehensive%0Aexperimental%20study%20to%20evaluate%20the%20capability%20of%20various%20MLLMs%20as%20world%20models%0Afor%20driving%20from%20the%20perspective%20of%20a%20fixed%20in-car%20camera.%20Our%20findings%20reveal%0Athat%2C%20while%20these%20models%20proficiently%20interpret%20individual%20images%2C%20they%0Astruggle%20significantly%20with%20synthesizing%20coherent%20narratives%20or%20logical%0Asequences%20across%20frames%20depicting%20dynamic%20behavior.%20The%20experiments%20demonstrate%0Aconsiderable%20inaccuracies%20in%20predicting%20%28i%29%20basic%20vehicle%20dynamics%0A%28forward/backward%2C%20acceleration/deceleration%2C%20turning%20right%20or%20left%29%2C%20%28ii%29%0Ainteractions%20with%20other%20road%20actors%20%28e.g.%2C%20identifying%20speeding%20cars%20or%20heavy%0Atraffic%29%2C%20%28iii%29%20trajectory%20planning%2C%20and%20%28iv%29%20open-set%20dynamic%20scene%20reasoning%2C%0Asuggesting%20biases%20in%20the%20models%27%20training%20data.%20To%20enable%20this%20experimental%0Astudy%20we%20introduce%20a%20specialized%20simulator%2C%20DriveSim%2C%20designed%20to%20generate%0Adiverse%20driving%20scenarios%2C%20providing%20a%20platform%20for%20evaluating%20MLLMs%20in%20the%0Arealms%20of%20driving.%20Additionally%2C%20we%20contribute%20the%20full%20open-source%20code%20and%20a%0Anew%20dataset%2C%20%22Eval-LLM-Drive%22%2C%20for%20evaluating%20MLLMs%20in%20driving.%20Our%20results%0Ahighlight%20a%20critical%20gap%20in%20the%20current%20capabilities%20of%20state-of-the-art%20MLLMs%2C%0Aunderscoring%20the%20need%20for%20enhanced%20foundation%20models%20to%20improve%20their%0Aapplicability%20in%20real-world%20dynamic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05956v1&entry.124074799=Read"},
{"title": "Semi-Autonomous Laparoscopic Robot Docking with Learned Hand-Eye\n  Information Fusion", "author": "Huanyu Tian and Martin Huber and Christopher E. Mower and Zhe Han and Changsheng Li and Xingguang Duan and Christos Bergeles", "abstract": "  In this study, we introduce a novel shared-control system for key-hole\ndocking operations, combining a commercial camera with occlusion-robust pose\nestimation and a hand-eye information fusion technique. This system is used to\nenhance docking precision and force-compliance safety. To train a hand-eye\ninformation fusion network model, we generated a self-supervised dataset using\nthis docking system. After training, our pose estimation method showed improved\naccuracy compared to traditional methods, including observation-only\napproaches, hand-eye calibration, and conventional state estimation filters. In\nreal-world phantom experiments, our approach demonstrated its effectiveness\nwith reduced position dispersion (1.23\\pm 0.81 mm vs. 2.47 \\pm 1.22 mm) and\nforce dispersion (0.78\\pm 0.57 N vs. 1.15 \\pm 0.97 N) compared to the control\ngroup. These advancements in semi-autonomy co-manipulation scenarios enhance\ninteraction and stability. The study presents an anti-interference, steady, and\nprecision solution with potential applications extending beyond laparoscopic\nsurgery to other minimally invasive procedures.\n", "link": "http://arxiv.org/abs/2405.05817v1", "date": "2024-05-09", "relevancy": 2.2057, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5863}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Autonomous%20Laparoscopic%20Robot%20Docking%20with%20Learned%20Hand-Eye%0A%20%20Information%20Fusion&body=Title%3A%20Semi-Autonomous%20Laparoscopic%20Robot%20Docking%20with%20Learned%20Hand-Eye%0A%20%20Information%20Fusion%0AAuthor%3A%20Huanyu%20Tian%20and%20Martin%20Huber%20and%20Christopher%20E.%20Mower%20and%20Zhe%20Han%20and%20Changsheng%20Li%20and%20Xingguang%20Duan%20and%20Christos%20Bergeles%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20a%20novel%20shared-control%20system%20for%20key-hole%0Adocking%20operations%2C%20combining%20a%20commercial%20camera%20with%20occlusion-robust%20pose%0Aestimation%20and%20a%20hand-eye%20information%20fusion%20technique.%20This%20system%20is%20used%20to%0Aenhance%20docking%20precision%20and%20force-compliance%20safety.%20To%20train%20a%20hand-eye%0Ainformation%20fusion%20network%20model%2C%20we%20generated%20a%20self-supervised%20dataset%20using%0Athis%20docking%20system.%20After%20training%2C%20our%20pose%20estimation%20method%20showed%20improved%0Aaccuracy%20compared%20to%20traditional%20methods%2C%20including%20observation-only%0Aapproaches%2C%20hand-eye%20calibration%2C%20and%20conventional%20state%20estimation%20filters.%20In%0Areal-world%20phantom%20experiments%2C%20our%20approach%20demonstrated%20its%20effectiveness%0Awith%20reduced%20position%20dispersion%20%281.23%5Cpm%200.81%20mm%20vs.%202.47%20%5Cpm%201.22%20mm%29%20and%0Aforce%20dispersion%20%280.78%5Cpm%200.57%20N%20vs.%201.15%20%5Cpm%200.97%20N%29%20compared%20to%20the%20control%0Agroup.%20These%20advancements%20in%20semi-autonomy%20co-manipulation%20scenarios%20enhance%0Ainteraction%20and%20stability.%20The%20study%20presents%20an%20anti-interference%2C%20steady%2C%20and%0Aprecision%20solution%20with%20potential%20applications%20extending%20beyond%20laparoscopic%0Asurgery%20to%20other%20minimally%20invasive%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Autonomous%2520Laparoscopic%2520Robot%2520Docking%2520with%2520Learned%2520Hand-Eye%250A%2520%2520Information%2520Fusion%26entry.906535625%3DHuanyu%2520Tian%2520and%2520Martin%2520Huber%2520and%2520Christopher%2520E.%2520Mower%2520and%2520Zhe%2520Han%2520and%2520Changsheng%2520Li%2520and%2520Xingguang%2520Duan%2520and%2520Christos%2520Bergeles%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520shared-control%2520system%2520for%2520key-hole%250Adocking%2520operations%252C%2520combining%2520a%2520commercial%2520camera%2520with%2520occlusion-robust%2520pose%250Aestimation%2520and%2520a%2520hand-eye%2520information%2520fusion%2520technique.%2520This%2520system%2520is%2520used%2520to%250Aenhance%2520docking%2520precision%2520and%2520force-compliance%2520safety.%2520To%2520train%2520a%2520hand-eye%250Ainformation%2520fusion%2520network%2520model%252C%2520we%2520generated%2520a%2520self-supervised%2520dataset%2520using%250Athis%2520docking%2520system.%2520After%2520training%252C%2520our%2520pose%2520estimation%2520method%2520showed%2520improved%250Aaccuracy%2520compared%2520to%2520traditional%2520methods%252C%2520including%2520observation-only%250Aapproaches%252C%2520hand-eye%2520calibration%252C%2520and%2520conventional%2520state%2520estimation%2520filters.%2520In%250Areal-world%2520phantom%2520experiments%252C%2520our%2520approach%2520demonstrated%2520its%2520effectiveness%250Awith%2520reduced%2520position%2520dispersion%2520%25281.23%255Cpm%25200.81%2520mm%2520vs.%25202.47%2520%255Cpm%25201.22%2520mm%2529%2520and%250Aforce%2520dispersion%2520%25280.78%255Cpm%25200.57%2520N%2520vs.%25201.15%2520%255Cpm%25200.97%2520N%2529%2520compared%2520to%2520the%2520control%250Agroup.%2520These%2520advancements%2520in%2520semi-autonomy%2520co-manipulation%2520scenarios%2520enhance%250Ainteraction%2520and%2520stability.%2520The%2520study%2520presents%2520an%2520anti-interference%252C%2520steady%252C%2520and%250Aprecision%2520solution%2520with%2520potential%2520applications%2520extending%2520beyond%2520laparoscopic%250Asurgery%2520to%2520other%2520minimally%2520invasive%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Autonomous%20Laparoscopic%20Robot%20Docking%20with%20Learned%20Hand-Eye%0A%20%20Information%20Fusion&entry.906535625=Huanyu%20Tian%20and%20Martin%20Huber%20and%20Christopher%20E.%20Mower%20and%20Zhe%20Han%20and%20Changsheng%20Li%20and%20Xingguang%20Duan%20and%20Christos%20Bergeles&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20a%20novel%20shared-control%20system%20for%20key-hole%0Adocking%20operations%2C%20combining%20a%20commercial%20camera%20with%20occlusion-robust%20pose%0Aestimation%20and%20a%20hand-eye%20information%20fusion%20technique.%20This%20system%20is%20used%20to%0Aenhance%20docking%20precision%20and%20force-compliance%20safety.%20To%20train%20a%20hand-eye%0Ainformation%20fusion%20network%20model%2C%20we%20generated%20a%20self-supervised%20dataset%20using%0Athis%20docking%20system.%20After%20training%2C%20our%20pose%20estimation%20method%20showed%20improved%0Aaccuracy%20compared%20to%20traditional%20methods%2C%20including%20observation-only%0Aapproaches%2C%20hand-eye%20calibration%2C%20and%20conventional%20state%20estimation%20filters.%20In%0Areal-world%20phantom%20experiments%2C%20our%20approach%20demonstrated%20its%20effectiveness%0Awith%20reduced%20position%20dispersion%20%281.23%5Cpm%200.81%20mm%20vs.%202.47%20%5Cpm%201.22%20mm%29%20and%0Aforce%20dispersion%20%280.78%5Cpm%200.57%20N%20vs.%201.15%20%5Cpm%200.97%20N%29%20compared%20to%20the%20control%0Agroup.%20These%20advancements%20in%20semi-autonomy%20co-manipulation%20scenarios%20enhance%0Ainteraction%20and%20stability.%20The%20study%20presents%20an%20anti-interference%2C%20steady%2C%20and%0Aprecision%20solution%20with%20potential%20applications%20extending%20beyond%20laparoscopic%0Asurgery%20to%20other%20minimally%20invasive%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05817v1&entry.124074799=Read"},
{"title": "Similarity Guided Multimodal Fusion Transformer for Semantic Location\n  Prediction in Social Media", "author": "Zhizhen Zhang and Ning Wang and Haojie Li and Zhihui Wang", "abstract": "  The purpose of semantic location prediction is to extract relevant semantic\nlocation information from multimodal social media posts, offering a more\ncontextual understanding of daily activities compared to GPS coordinates.\nHowever, this task becomes challenging due to the presence of noise and\nirrelevant information in \"text-image\" pairs. Existing methods suffer from\ninsufficient feature representations and fail to consider the comprehensive\nintegration of similarity at different granularities, making it difficult to\nfilter out noise and irrelevant information. To address these challenges, we\npropose a Similarity-Guided Multimodal Fusion Transformer (SG-MFT) for\npredicting social users' semantic locations. First, we utilize a pre-trained\nlarge-scale vision-language model to extract high-quality feature\nrepresentations from social media posts. Then, we introduce a Similarity-Guided\nInteraction Module (SIM) to alleviate modality heterogeneity and noise\ninterference by incorporating coarse-grained and fine-grained similarity\nguidance for modality interactions. Specifically, we propose a novel\nsimilarity-aware feature interpolation attention mechanism at the coarse level,\nleveraging modality-wise similarity to mitigate heterogeneity and reduce noise\nwithin each modality. Meanwhile, we employ a similarity-aware feed-forward\nblock at the fine level, utilizing element-wise similarity to further mitigate\nthe impact of modality heterogeneity. Building upon pre-processed features with\nminimal noise and modal interference, we propose a Similarity-aware Feature\nFusion Module (SFM) to fuse two modalities with cross-attention mechanism.\nComprehensive experimental results demonstrate the superior performance of our\nproposed method in handling modality imbalance while maintaining efficient\nfusion effectiveness.\n", "link": "http://arxiv.org/abs/2405.05760v1", "date": "2024-05-09", "relevancy": 2.194, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Similarity%20Guided%20Multimodal%20Fusion%20Transformer%20for%20Semantic%20Location%0A%20%20Prediction%20in%20Social%20Media&body=Title%3A%20Similarity%20Guided%20Multimodal%20Fusion%20Transformer%20for%20Semantic%20Location%0A%20%20Prediction%20in%20Social%20Media%0AAuthor%3A%20Zhizhen%20Zhang%20and%20Ning%20Wang%20and%20Haojie%20Li%20and%20Zhihui%20Wang%0AAbstract%3A%20%20%20The%20purpose%20of%20semantic%20location%20prediction%20is%20to%20extract%20relevant%20semantic%0Alocation%20information%20from%20multimodal%20social%20media%20posts%2C%20offering%20a%20more%0Acontextual%20understanding%20of%20daily%20activities%20compared%20to%20GPS%20coordinates.%0AHowever%2C%20this%20task%20becomes%20challenging%20due%20to%20the%20presence%20of%20noise%20and%0Airrelevant%20information%20in%20%22text-image%22%20pairs.%20Existing%20methods%20suffer%20from%0Ainsufficient%20feature%20representations%20and%20fail%20to%20consider%20the%20comprehensive%0Aintegration%20of%20similarity%20at%20different%20granularities%2C%20making%20it%20difficult%20to%0Afilter%20out%20noise%20and%20irrelevant%20information.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20Similarity-Guided%20Multimodal%20Fusion%20Transformer%20%28SG-MFT%29%20for%0Apredicting%20social%20users%27%20semantic%20locations.%20First%2C%20we%20utilize%20a%20pre-trained%0Alarge-scale%20vision-language%20model%20to%20extract%20high-quality%20feature%0Arepresentations%20from%20social%20media%20posts.%20Then%2C%20we%20introduce%20a%20Similarity-Guided%0AInteraction%20Module%20%28SIM%29%20to%20alleviate%20modality%20heterogeneity%20and%20noise%0Ainterference%20by%20incorporating%20coarse-grained%20and%20fine-grained%20similarity%0Aguidance%20for%20modality%20interactions.%20Specifically%2C%20we%20propose%20a%20novel%0Asimilarity-aware%20feature%20interpolation%20attention%20mechanism%20at%20the%20coarse%20level%2C%0Aleveraging%20modality-wise%20similarity%20to%20mitigate%20heterogeneity%20and%20reduce%20noise%0Awithin%20each%20modality.%20Meanwhile%2C%20we%20employ%20a%20similarity-aware%20feed-forward%0Ablock%20at%20the%20fine%20level%2C%20utilizing%20element-wise%20similarity%20to%20further%20mitigate%0Athe%20impact%20of%20modality%20heterogeneity.%20Building%20upon%20pre-processed%20features%20with%0Aminimal%20noise%20and%20modal%20interference%2C%20we%20propose%20a%20Similarity-aware%20Feature%0AFusion%20Module%20%28SFM%29%20to%20fuse%20two%20modalities%20with%20cross-attention%20mechanism.%0AComprehensive%20experimental%20results%20demonstrate%20the%20superior%20performance%20of%20our%0Aproposed%20method%20in%20handling%20modality%20imbalance%20while%20maintaining%20efficient%0Afusion%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimilarity%2520Guided%2520Multimodal%2520Fusion%2520Transformer%2520for%2520Semantic%2520Location%250A%2520%2520Prediction%2520in%2520Social%2520Media%26entry.906535625%3DZhizhen%2520Zhang%2520and%2520Ning%2520Wang%2520and%2520Haojie%2520Li%2520and%2520Zhihui%2520Wang%26entry.1292438233%3D%2520%2520The%2520purpose%2520of%2520semantic%2520location%2520prediction%2520is%2520to%2520extract%2520relevant%2520semantic%250Alocation%2520information%2520from%2520multimodal%2520social%2520media%2520posts%252C%2520offering%2520a%2520more%250Acontextual%2520understanding%2520of%2520daily%2520activities%2520compared%2520to%2520GPS%2520coordinates.%250AHowever%252C%2520this%2520task%2520becomes%2520challenging%2520due%2520to%2520the%2520presence%2520of%2520noise%2520and%250Airrelevant%2520information%2520in%2520%2522text-image%2522%2520pairs.%2520Existing%2520methods%2520suffer%2520from%250Ainsufficient%2520feature%2520representations%2520and%2520fail%2520to%2520consider%2520the%2520comprehensive%250Aintegration%2520of%2520similarity%2520at%2520different%2520granularities%252C%2520making%2520it%2520difficult%2520to%250Afilter%2520out%2520noise%2520and%2520irrelevant%2520information.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520Similarity-Guided%2520Multimodal%2520Fusion%2520Transformer%2520%2528SG-MFT%2529%2520for%250Apredicting%2520social%2520users%2527%2520semantic%2520locations.%2520First%252C%2520we%2520utilize%2520a%2520pre-trained%250Alarge-scale%2520vision-language%2520model%2520to%2520extract%2520high-quality%2520feature%250Arepresentations%2520from%2520social%2520media%2520posts.%2520Then%252C%2520we%2520introduce%2520a%2520Similarity-Guided%250AInteraction%2520Module%2520%2528SIM%2529%2520to%2520alleviate%2520modality%2520heterogeneity%2520and%2520noise%250Ainterference%2520by%2520incorporating%2520coarse-grained%2520and%2520fine-grained%2520similarity%250Aguidance%2520for%2520modality%2520interactions.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%250Asimilarity-aware%2520feature%2520interpolation%2520attention%2520mechanism%2520at%2520the%2520coarse%2520level%252C%250Aleveraging%2520modality-wise%2520similarity%2520to%2520mitigate%2520heterogeneity%2520and%2520reduce%2520noise%250Awithin%2520each%2520modality.%2520Meanwhile%252C%2520we%2520employ%2520a%2520similarity-aware%2520feed-forward%250Ablock%2520at%2520the%2520fine%2520level%252C%2520utilizing%2520element-wise%2520similarity%2520to%2520further%2520mitigate%250Athe%2520impact%2520of%2520modality%2520heterogeneity.%2520Building%2520upon%2520pre-processed%2520features%2520with%250Aminimal%2520noise%2520and%2520modal%2520interference%252C%2520we%2520propose%2520a%2520Similarity-aware%2520Feature%250AFusion%2520Module%2520%2528SFM%2529%2520to%2520fuse%2520two%2520modalities%2520with%2520cross-attention%2520mechanism.%250AComprehensive%2520experimental%2520results%2520demonstrate%2520the%2520superior%2520performance%2520of%2520our%250Aproposed%2520method%2520in%2520handling%2520modality%2520imbalance%2520while%2520maintaining%2520efficient%250Afusion%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Similarity%20Guided%20Multimodal%20Fusion%20Transformer%20for%20Semantic%20Location%0A%20%20Prediction%20in%20Social%20Media&entry.906535625=Zhizhen%20Zhang%20and%20Ning%20Wang%20and%20Haojie%20Li%20and%20Zhihui%20Wang&entry.1292438233=%20%20The%20purpose%20of%20semantic%20location%20prediction%20is%20to%20extract%20relevant%20semantic%0Alocation%20information%20from%20multimodal%20social%20media%20posts%2C%20offering%20a%20more%0Acontextual%20understanding%20of%20daily%20activities%20compared%20to%20GPS%20coordinates.%0AHowever%2C%20this%20task%20becomes%20challenging%20due%20to%20the%20presence%20of%20noise%20and%0Airrelevant%20information%20in%20%22text-image%22%20pairs.%20Existing%20methods%20suffer%20from%0Ainsufficient%20feature%20representations%20and%20fail%20to%20consider%20the%20comprehensive%0Aintegration%20of%20similarity%20at%20different%20granularities%2C%20making%20it%20difficult%20to%0Afilter%20out%20noise%20and%20irrelevant%20information.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20Similarity-Guided%20Multimodal%20Fusion%20Transformer%20%28SG-MFT%29%20for%0Apredicting%20social%20users%27%20semantic%20locations.%20First%2C%20we%20utilize%20a%20pre-trained%0Alarge-scale%20vision-language%20model%20to%20extract%20high-quality%20feature%0Arepresentations%20from%20social%20media%20posts.%20Then%2C%20we%20introduce%20a%20Similarity-Guided%0AInteraction%20Module%20%28SIM%29%20to%20alleviate%20modality%20heterogeneity%20and%20noise%0Ainterference%20by%20incorporating%20coarse-grained%20and%20fine-grained%20similarity%0Aguidance%20for%20modality%20interactions.%20Specifically%2C%20we%20propose%20a%20novel%0Asimilarity-aware%20feature%20interpolation%20attention%20mechanism%20at%20the%20coarse%20level%2C%0Aleveraging%20modality-wise%20similarity%20to%20mitigate%20heterogeneity%20and%20reduce%20noise%0Awithin%20each%20modality.%20Meanwhile%2C%20we%20employ%20a%20similarity-aware%20feed-forward%0Ablock%20at%20the%20fine%20level%2C%20utilizing%20element-wise%20similarity%20to%20further%20mitigate%0Athe%20impact%20of%20modality%20heterogeneity.%20Building%20upon%20pre-processed%20features%20with%0Aminimal%20noise%20and%20modal%20interference%2C%20we%20propose%20a%20Similarity-aware%20Feature%0AFusion%20Module%20%28SFM%29%20to%20fuse%20two%20modalities%20with%20cross-attention%20mechanism.%0AComprehensive%20experimental%20results%20demonstrate%20the%20superior%20performance%20of%20our%0Aproposed%20method%20in%20handling%20modality%20imbalance%20while%20maintaining%20efficient%0Afusion%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05760v1&entry.124074799=Read"},
{"title": "Composite Distributed Learning and Synchronization of Nonlinear\n  Multi-Agent Systems with Complete Uncertain Dynamics", "author": "Emadodin Jandaghi and Dalton L. Stein and Adam Hoburg and Paolo Stegagno and Mingxi Zhou and Chengzhi Yuan", "abstract": "  This paper addresses the problem of composite synchronization and learning\ncontrol in a network of multi-agent robotic manipulator systems with\nheterogeneous nonlinear uncertainties under a leader-follower framework. A\nnovel two-layer distributed adaptive learning control strategy is introduced,\ncomprising a first-layer distributed cooperative estimator and a second-layer\ndecentralized deterministic learning controller. The first layer is to\nfacilitate each robotic agent's estimation of the leader's information. The\nsecond layer is responsible for both controlling individual robot agents to\ntrack desired reference trajectories and accurately identifying/learning their\nnonlinear uncertain dynamics. The proposed distributed learning control scheme\nrepresents an advancement in the existing literature due to its ability to\nmanage robotic agents with completely uncertain dynamics including uncertain\nmass matrices. This allows the robotic control to be environment-independent\nwhich can be used in various settings, from underwater to space where\nidentifying system dynamics parameters is challenging. The stability and\nparameter convergence of the closed-loop system are rigorously analyzed using\nthe Lyapunov method. Numerical simulations validate the effectiveness of the\nproposed scheme.\n", "link": "http://arxiv.org/abs/2403.00987v3", "date": "2024-05-09", "relevancy": 2.1938, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5799}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5347}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Composite%20Distributed%20Learning%20and%20Synchronization%20of%20Nonlinear%0A%20%20Multi-Agent%20Systems%20with%20Complete%20Uncertain%20Dynamics&body=Title%3A%20Composite%20Distributed%20Learning%20and%20Synchronization%20of%20Nonlinear%0A%20%20Multi-Agent%20Systems%20with%20Complete%20Uncertain%20Dynamics%0AAuthor%3A%20Emadodin%20Jandaghi%20and%20Dalton%20L.%20Stein%20and%20Adam%20Hoburg%20and%20Paolo%20Stegagno%20and%20Mingxi%20Zhou%20and%20Chengzhi%20Yuan%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20composite%20synchronization%20and%20learning%0Acontrol%20in%20a%20network%20of%20multi-agent%20robotic%20manipulator%20systems%20with%0Aheterogeneous%20nonlinear%20uncertainties%20under%20a%20leader-follower%20framework.%20A%0Anovel%20two-layer%20distributed%20adaptive%20learning%20control%20strategy%20is%20introduced%2C%0Acomprising%20a%20first-layer%20distributed%20cooperative%20estimator%20and%20a%20second-layer%0Adecentralized%20deterministic%20learning%20controller.%20The%20first%20layer%20is%20to%0Afacilitate%20each%20robotic%20agent%27s%20estimation%20of%20the%20leader%27s%20information.%20The%0Asecond%20layer%20is%20responsible%20for%20both%20controlling%20individual%20robot%20agents%20to%0Atrack%20desired%20reference%20trajectories%20and%20accurately%20identifying/learning%20their%0Anonlinear%20uncertain%20dynamics.%20The%20proposed%20distributed%20learning%20control%20scheme%0Arepresents%20an%20advancement%20in%20the%20existing%20literature%20due%20to%20its%20ability%20to%0Amanage%20robotic%20agents%20with%20completely%20uncertain%20dynamics%20including%20uncertain%0Amass%20matrices.%20This%20allows%20the%20robotic%20control%20to%20be%20environment-independent%0Awhich%20can%20be%20used%20in%20various%20settings%2C%20from%20underwater%20to%20space%20where%0Aidentifying%20system%20dynamics%20parameters%20is%20challenging.%20The%20stability%20and%0Aparameter%20convergence%20of%20the%20closed-loop%20system%20are%20rigorously%20analyzed%20using%0Athe%20Lyapunov%20method.%20Numerical%20simulations%20validate%20the%20effectiveness%20of%20the%0Aproposed%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00987v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComposite%2520Distributed%2520Learning%2520and%2520Synchronization%2520of%2520Nonlinear%250A%2520%2520Multi-Agent%2520Systems%2520with%2520Complete%2520Uncertain%2520Dynamics%26entry.906535625%3DEmadodin%2520Jandaghi%2520and%2520Dalton%2520L.%2520Stein%2520and%2520Adam%2520Hoburg%2520and%2520Paolo%2520Stegagno%2520and%2520Mingxi%2520Zhou%2520and%2520Chengzhi%2520Yuan%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520composite%2520synchronization%2520and%2520learning%250Acontrol%2520in%2520a%2520network%2520of%2520multi-agent%2520robotic%2520manipulator%2520systems%2520with%250Aheterogeneous%2520nonlinear%2520uncertainties%2520under%2520a%2520leader-follower%2520framework.%2520A%250Anovel%2520two-layer%2520distributed%2520adaptive%2520learning%2520control%2520strategy%2520is%2520introduced%252C%250Acomprising%2520a%2520first-layer%2520distributed%2520cooperative%2520estimator%2520and%2520a%2520second-layer%250Adecentralized%2520deterministic%2520learning%2520controller.%2520The%2520first%2520layer%2520is%2520to%250Afacilitate%2520each%2520robotic%2520agent%2527s%2520estimation%2520of%2520the%2520leader%2527s%2520information.%2520The%250Asecond%2520layer%2520is%2520responsible%2520for%2520both%2520controlling%2520individual%2520robot%2520agents%2520to%250Atrack%2520desired%2520reference%2520trajectories%2520and%2520accurately%2520identifying/learning%2520their%250Anonlinear%2520uncertain%2520dynamics.%2520The%2520proposed%2520distributed%2520learning%2520control%2520scheme%250Arepresents%2520an%2520advancement%2520in%2520the%2520existing%2520literature%2520due%2520to%2520its%2520ability%2520to%250Amanage%2520robotic%2520agents%2520with%2520completely%2520uncertain%2520dynamics%2520including%2520uncertain%250Amass%2520matrices.%2520This%2520allows%2520the%2520robotic%2520control%2520to%2520be%2520environment-independent%250Awhich%2520can%2520be%2520used%2520in%2520various%2520settings%252C%2520from%2520underwater%2520to%2520space%2520where%250Aidentifying%2520system%2520dynamics%2520parameters%2520is%2520challenging.%2520The%2520stability%2520and%250Aparameter%2520convergence%2520of%2520the%2520closed-loop%2520system%2520are%2520rigorously%2520analyzed%2520using%250Athe%2520Lyapunov%2520method.%2520Numerical%2520simulations%2520validate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00987v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Composite%20Distributed%20Learning%20and%20Synchronization%20of%20Nonlinear%0A%20%20Multi-Agent%20Systems%20with%20Complete%20Uncertain%20Dynamics&entry.906535625=Emadodin%20Jandaghi%20and%20Dalton%20L.%20Stein%20and%20Adam%20Hoburg%20and%20Paolo%20Stegagno%20and%20Mingxi%20Zhou%20and%20Chengzhi%20Yuan&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20composite%20synchronization%20and%20learning%0Acontrol%20in%20a%20network%20of%20multi-agent%20robotic%20manipulator%20systems%20with%0Aheterogeneous%20nonlinear%20uncertainties%20under%20a%20leader-follower%20framework.%20A%0Anovel%20two-layer%20distributed%20adaptive%20learning%20control%20strategy%20is%20introduced%2C%0Acomprising%20a%20first-layer%20distributed%20cooperative%20estimator%20and%20a%20second-layer%0Adecentralized%20deterministic%20learning%20controller.%20The%20first%20layer%20is%20to%0Afacilitate%20each%20robotic%20agent%27s%20estimation%20of%20the%20leader%27s%20information.%20The%0Asecond%20layer%20is%20responsible%20for%20both%20controlling%20individual%20robot%20agents%20to%0Atrack%20desired%20reference%20trajectories%20and%20accurately%20identifying/learning%20their%0Anonlinear%20uncertain%20dynamics.%20The%20proposed%20distributed%20learning%20control%20scheme%0Arepresents%20an%20advancement%20in%20the%20existing%20literature%20due%20to%20its%20ability%20to%0Amanage%20robotic%20agents%20with%20completely%20uncertain%20dynamics%20including%20uncertain%0Amass%20matrices.%20This%20allows%20the%20robotic%20control%20to%20be%20environment-independent%0Awhich%20can%20be%20used%20in%20various%20settings%2C%20from%20underwater%20to%20space%20where%0Aidentifying%20system%20dynamics%20parameters%20is%20challenging.%20The%20stability%20and%0Aparameter%20convergence%20of%20the%20closed-loop%20system%20are%20rigorously%20analyzed%20using%0Athe%20Lyapunov%20method.%20Numerical%20simulations%20validate%20the%20effectiveness%20of%20the%0Aproposed%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00987v3&entry.124074799=Read"},
{"title": "A Universal Growth Rate for Learning with Smooth Surrogate Losses", "author": "Anqi Mao and Mehryar Mohri and Yutao Zhong", "abstract": "  This paper presents a comprehensive analysis of the growth rate of\n$H$-consistency bounds (and excess error bounds) for various surrogate losses\nused in classification. We prove a square-root growth rate near zero for smooth\nmargin-based surrogate losses in binary classification, providing both upper\nand lower bounds under mild assumptions. This result also translates to excess\nerror bounds. Our lower bound requires weaker conditions than those in previous\nwork for excess error bounds, and our upper bound is entirely novel. Moreover,\nwe extend this analysis to multi-class classification with a series of novel\nresults, demonstrating a universal square-root growth rate for smooth comp-sum\nand constrained losses, covering common choices for training neural networks in\nmulti-class classification. Given this universal rate, we turn to the question\nof choosing among different surrogate losses. We first examine how\n$H$-consistency bounds vary across surrogates based on the number of classes.\nNext, ignoring constants and focusing on behavior near zero, we identify\nminimizability gaps as the key differentiating factor in these bounds. Thus, we\nthoroughly analyze these gaps, to guide surrogate loss selection, covering:\ncomparisons across different comp-sum losses, conditions where gaps become\nzero, and general conditions leading to small gaps. Additionally, we\ndemonstrate the key role of minimizability gaps in comparing excess error\nbounds and $H$-consistency bounds.\n", "link": "http://arxiv.org/abs/2405.05968v1", "date": "2024-05-09", "relevancy": 2.1509, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4443}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4257}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Growth%20Rate%20for%20Learning%20with%20Smooth%20Surrogate%20Losses&body=Title%3A%20A%20Universal%20Growth%20Rate%20for%20Learning%20with%20Smooth%20Surrogate%20Losses%0AAuthor%3A%20Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20analysis%20of%20the%20growth%20rate%20of%0A%24H%24-consistency%20bounds%20%28and%20excess%20error%20bounds%29%20for%20various%20surrogate%20losses%0Aused%20in%20classification.%20We%20prove%20a%20square-root%20growth%20rate%20near%20zero%20for%20smooth%0Amargin-based%20surrogate%20losses%20in%20binary%20classification%2C%20providing%20both%20upper%0Aand%20lower%20bounds%20under%20mild%20assumptions.%20This%20result%20also%20translates%20to%20excess%0Aerror%20bounds.%20Our%20lower%20bound%20requires%20weaker%20conditions%20than%20those%20in%20previous%0Awork%20for%20excess%20error%20bounds%2C%20and%20our%20upper%20bound%20is%20entirely%20novel.%20Moreover%2C%0Awe%20extend%20this%20analysis%20to%20multi-class%20classification%20with%20a%20series%20of%20novel%0Aresults%2C%20demonstrating%20a%20universal%20square-root%20growth%20rate%20for%20smooth%20comp-sum%0Aand%20constrained%20losses%2C%20covering%20common%20choices%20for%20training%20neural%20networks%20in%0Amulti-class%20classification.%20Given%20this%20universal%20rate%2C%20we%20turn%20to%20the%20question%0Aof%20choosing%20among%20different%20surrogate%20losses.%20We%20first%20examine%20how%0A%24H%24-consistency%20bounds%20vary%20across%20surrogates%20based%20on%20the%20number%20of%20classes.%0ANext%2C%20ignoring%20constants%20and%20focusing%20on%20behavior%20near%20zero%2C%20we%20identify%0Aminimizability%20gaps%20as%20the%20key%20differentiating%20factor%20in%20these%20bounds.%20Thus%2C%20we%0Athoroughly%20analyze%20these%20gaps%2C%20to%20guide%20surrogate%20loss%20selection%2C%20covering%3A%0Acomparisons%20across%20different%20comp-sum%20losses%2C%20conditions%20where%20gaps%20become%0Azero%2C%20and%20general%20conditions%20leading%20to%20small%20gaps.%20Additionally%2C%20we%0Ademonstrate%20the%20key%20role%20of%20minimizability%20gaps%20in%20comparing%20excess%20error%0Abounds%20and%20%24H%24-consistency%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05968v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Universal%2520Growth%2520Rate%2520for%2520Learning%2520with%2520Smooth%2520Surrogate%2520Losses%26entry.906535625%3DAnqi%2520Mao%2520and%2520Mehryar%2520Mohri%2520and%2520Yutao%2520Zhong%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520analysis%2520of%2520the%2520growth%2520rate%2520of%250A%2524H%2524-consistency%2520bounds%2520%2528and%2520excess%2520error%2520bounds%2529%2520for%2520various%2520surrogate%2520losses%250Aused%2520in%2520classification.%2520We%2520prove%2520a%2520square-root%2520growth%2520rate%2520near%2520zero%2520for%2520smooth%250Amargin-based%2520surrogate%2520losses%2520in%2520binary%2520classification%252C%2520providing%2520both%2520upper%250Aand%2520lower%2520bounds%2520under%2520mild%2520assumptions.%2520This%2520result%2520also%2520translates%2520to%2520excess%250Aerror%2520bounds.%2520Our%2520lower%2520bound%2520requires%2520weaker%2520conditions%2520than%2520those%2520in%2520previous%250Awork%2520for%2520excess%2520error%2520bounds%252C%2520and%2520our%2520upper%2520bound%2520is%2520entirely%2520novel.%2520Moreover%252C%250Awe%2520extend%2520this%2520analysis%2520to%2520multi-class%2520classification%2520with%2520a%2520series%2520of%2520novel%250Aresults%252C%2520demonstrating%2520a%2520universal%2520square-root%2520growth%2520rate%2520for%2520smooth%2520comp-sum%250Aand%2520constrained%2520losses%252C%2520covering%2520common%2520choices%2520for%2520training%2520neural%2520networks%2520in%250Amulti-class%2520classification.%2520Given%2520this%2520universal%2520rate%252C%2520we%2520turn%2520to%2520the%2520question%250Aof%2520choosing%2520among%2520different%2520surrogate%2520losses.%2520We%2520first%2520examine%2520how%250A%2524H%2524-consistency%2520bounds%2520vary%2520across%2520surrogates%2520based%2520on%2520the%2520number%2520of%2520classes.%250ANext%252C%2520ignoring%2520constants%2520and%2520focusing%2520on%2520behavior%2520near%2520zero%252C%2520we%2520identify%250Aminimizability%2520gaps%2520as%2520the%2520key%2520differentiating%2520factor%2520in%2520these%2520bounds.%2520Thus%252C%2520we%250Athoroughly%2520analyze%2520these%2520gaps%252C%2520to%2520guide%2520surrogate%2520loss%2520selection%252C%2520covering%253A%250Acomparisons%2520across%2520different%2520comp-sum%2520losses%252C%2520conditions%2520where%2520gaps%2520become%250Azero%252C%2520and%2520general%2520conditions%2520leading%2520to%2520small%2520gaps.%2520Additionally%252C%2520we%250Ademonstrate%2520the%2520key%2520role%2520of%2520minimizability%2520gaps%2520in%2520comparing%2520excess%2520error%250Abounds%2520and%2520%2524H%2524-consistency%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05968v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Growth%20Rate%20for%20Learning%20with%20Smooth%20Surrogate%20Losses&entry.906535625=Anqi%20Mao%20and%20Mehryar%20Mohri%20and%20Yutao%20Zhong&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20analysis%20of%20the%20growth%20rate%20of%0A%24H%24-consistency%20bounds%20%28and%20excess%20error%20bounds%29%20for%20various%20surrogate%20losses%0Aused%20in%20classification.%20We%20prove%20a%20square-root%20growth%20rate%20near%20zero%20for%20smooth%0Amargin-based%20surrogate%20losses%20in%20binary%20classification%2C%20providing%20both%20upper%0Aand%20lower%20bounds%20under%20mild%20assumptions.%20This%20result%20also%20translates%20to%20excess%0Aerror%20bounds.%20Our%20lower%20bound%20requires%20weaker%20conditions%20than%20those%20in%20previous%0Awork%20for%20excess%20error%20bounds%2C%20and%20our%20upper%20bound%20is%20entirely%20novel.%20Moreover%2C%0Awe%20extend%20this%20analysis%20to%20multi-class%20classification%20with%20a%20series%20of%20novel%0Aresults%2C%20demonstrating%20a%20universal%20square-root%20growth%20rate%20for%20smooth%20comp-sum%0Aand%20constrained%20losses%2C%20covering%20common%20choices%20for%20training%20neural%20networks%20in%0Amulti-class%20classification.%20Given%20this%20universal%20rate%2C%20we%20turn%20to%20the%20question%0Aof%20choosing%20among%20different%20surrogate%20losses.%20We%20first%20examine%20how%0A%24H%24-consistency%20bounds%20vary%20across%20surrogates%20based%20on%20the%20number%20of%20classes.%0ANext%2C%20ignoring%20constants%20and%20focusing%20on%20behavior%20near%20zero%2C%20we%20identify%0Aminimizability%20gaps%20as%20the%20key%20differentiating%20factor%20in%20these%20bounds.%20Thus%2C%20we%0Athoroughly%20analyze%20these%20gaps%2C%20to%20guide%20surrogate%20loss%20selection%2C%20covering%3A%0Acomparisons%20across%20different%20comp-sum%20losses%2C%20conditions%20where%20gaps%20become%0Azero%2C%20and%20general%20conditions%20leading%20to%20small%20gaps.%20Additionally%2C%20we%0Ademonstrate%20the%20key%20role%20of%20minimizability%20gaps%20in%20comparing%20excess%20error%0Abounds%20and%20%24H%24-consistency%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05968v1&entry.124074799=Read"},
{"title": "Fast and Controllable Post-training Sparsity: Learning Optimal Sparsity\n  Allocation with Global Constraint in Minutes", "author": "Ruihao Gong and Yang Yong and Zining Wang and Jinyang Guo and Xiuying Wei and Yuqing Ma and Xianglong Liu", "abstract": "  Neural network sparsity has attracted many research interests due to its\nsimilarity to biological schemes and high energy efficiency. However, existing\nmethods depend on long-time training or fine-tuning, which prevents large-scale\napplications. Recently, some works focusing on post-training sparsity (PTS)\nhave emerged. They get rid of the high training cost but usually suffer from\ndistinct accuracy degradation due to neglect of the reasonable sparsity rate at\neach layer. Previous methods for finding sparsity rates mainly focus on the\ntraining-aware scenario, which usually fails to converge stably under the PTS\nsetting with limited data and much less training cost. In this paper, we\npropose a fast and controllable post-training sparsity (FCPTS) framework. By\nincorporating a differentiable bridge function and a controllable optimization\nobjective, our method allows for rapid and accurate sparsity allocation\nlearning in minutes, with the added assurance of convergence to a predetermined\nglobal sparsity rate. Equipped with these techniques, we can surpass the\nstate-of-the-art methods by a large margin, e.g., over 30\\% improvement for\nResNet-50 on ImageNet under the sparsity rate of 80\\%. Our plug-and-play code\nand supplementary materials are open-sourced at\nhttps://github.com/ModelTC/FCPTS.\n", "link": "http://arxiv.org/abs/2405.05808v1", "date": "2024-05-09", "relevancy": 2.1488, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5635}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5347}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Controllable%20Post-training%20Sparsity%3A%20Learning%20Optimal%20Sparsity%0A%20%20Allocation%20with%20Global%20Constraint%20in%20Minutes&body=Title%3A%20Fast%20and%20Controllable%20Post-training%20Sparsity%3A%20Learning%20Optimal%20Sparsity%0A%20%20Allocation%20with%20Global%20Constraint%20in%20Minutes%0AAuthor%3A%20Ruihao%20Gong%20and%20Yang%20Yong%20and%20Zining%20Wang%20and%20Jinyang%20Guo%20and%20Xiuying%20Wei%20and%20Yuqing%20Ma%20and%20Xianglong%20Liu%0AAbstract%3A%20%20%20Neural%20network%20sparsity%20has%20attracted%20many%20research%20interests%20due%20to%20its%0Asimilarity%20to%20biological%20schemes%20and%20high%20energy%20efficiency.%20However%2C%20existing%0Amethods%20depend%20on%20long-time%20training%20or%20fine-tuning%2C%20which%20prevents%20large-scale%0Aapplications.%20Recently%2C%20some%20works%20focusing%20on%20post-training%20sparsity%20%28PTS%29%0Ahave%20emerged.%20They%20get%20rid%20of%20the%20high%20training%20cost%20but%20usually%20suffer%20from%0Adistinct%20accuracy%20degradation%20due%20to%20neglect%20of%20the%20reasonable%20sparsity%20rate%20at%0Aeach%20layer.%20Previous%20methods%20for%20finding%20sparsity%20rates%20mainly%20focus%20on%20the%0Atraining-aware%20scenario%2C%20which%20usually%20fails%20to%20converge%20stably%20under%20the%20PTS%0Asetting%20with%20limited%20data%20and%20much%20less%20training%20cost.%20In%20this%20paper%2C%20we%0Apropose%20a%20fast%20and%20controllable%20post-training%20sparsity%20%28FCPTS%29%20framework.%20By%0Aincorporating%20a%20differentiable%20bridge%20function%20and%20a%20controllable%20optimization%0Aobjective%2C%20our%20method%20allows%20for%20rapid%20and%20accurate%20sparsity%20allocation%0Alearning%20in%20minutes%2C%20with%20the%20added%20assurance%20of%20convergence%20to%20a%20predetermined%0Aglobal%20sparsity%20rate.%20Equipped%20with%20these%20techniques%2C%20we%20can%20surpass%20the%0Astate-of-the-art%20methods%20by%20a%20large%20margin%2C%20e.g.%2C%20over%2030%5C%25%20improvement%20for%0AResNet-50%20on%20ImageNet%20under%20the%20sparsity%20rate%20of%2080%5C%25.%20Our%20plug-and-play%20code%0Aand%20supplementary%20materials%20are%20open-sourced%20at%0Ahttps%3A//github.com/ModelTC/FCPTS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Controllable%2520Post-training%2520Sparsity%253A%2520Learning%2520Optimal%2520Sparsity%250A%2520%2520Allocation%2520with%2520Global%2520Constraint%2520in%2520Minutes%26entry.906535625%3DRuihao%2520Gong%2520and%2520Yang%2520Yong%2520and%2520Zining%2520Wang%2520and%2520Jinyang%2520Guo%2520and%2520Xiuying%2520Wei%2520and%2520Yuqing%2520Ma%2520and%2520Xianglong%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520network%2520sparsity%2520has%2520attracted%2520many%2520research%2520interests%2520due%2520to%2520its%250Asimilarity%2520to%2520biological%2520schemes%2520and%2520high%2520energy%2520efficiency.%2520However%252C%2520existing%250Amethods%2520depend%2520on%2520long-time%2520training%2520or%2520fine-tuning%252C%2520which%2520prevents%2520large-scale%250Aapplications.%2520Recently%252C%2520some%2520works%2520focusing%2520on%2520post-training%2520sparsity%2520%2528PTS%2529%250Ahave%2520emerged.%2520They%2520get%2520rid%2520of%2520the%2520high%2520training%2520cost%2520but%2520usually%2520suffer%2520from%250Adistinct%2520accuracy%2520degradation%2520due%2520to%2520neglect%2520of%2520the%2520reasonable%2520sparsity%2520rate%2520at%250Aeach%2520layer.%2520Previous%2520methods%2520for%2520finding%2520sparsity%2520rates%2520mainly%2520focus%2520on%2520the%250Atraining-aware%2520scenario%252C%2520which%2520usually%2520fails%2520to%2520converge%2520stably%2520under%2520the%2520PTS%250Asetting%2520with%2520limited%2520data%2520and%2520much%2520less%2520training%2520cost.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520fast%2520and%2520controllable%2520post-training%2520sparsity%2520%2528FCPTS%2529%2520framework.%2520By%250Aincorporating%2520a%2520differentiable%2520bridge%2520function%2520and%2520a%2520controllable%2520optimization%250Aobjective%252C%2520our%2520method%2520allows%2520for%2520rapid%2520and%2520accurate%2520sparsity%2520allocation%250Alearning%2520in%2520minutes%252C%2520with%2520the%2520added%2520assurance%2520of%2520convergence%2520to%2520a%2520predetermined%250Aglobal%2520sparsity%2520rate.%2520Equipped%2520with%2520these%2520techniques%252C%2520we%2520can%2520surpass%2520the%250Astate-of-the-art%2520methods%2520by%2520a%2520large%2520margin%252C%2520e.g.%252C%2520over%252030%255C%2525%2520improvement%2520for%250AResNet-50%2520on%2520ImageNet%2520under%2520the%2520sparsity%2520rate%2520of%252080%255C%2525.%2520Our%2520plug-and-play%2520code%250Aand%2520supplementary%2520materials%2520are%2520open-sourced%2520at%250Ahttps%253A//github.com/ModelTC/FCPTS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Controllable%20Post-training%20Sparsity%3A%20Learning%20Optimal%20Sparsity%0A%20%20Allocation%20with%20Global%20Constraint%20in%20Minutes&entry.906535625=Ruihao%20Gong%20and%20Yang%20Yong%20and%20Zining%20Wang%20and%20Jinyang%20Guo%20and%20Xiuying%20Wei%20and%20Yuqing%20Ma%20and%20Xianglong%20Liu&entry.1292438233=%20%20Neural%20network%20sparsity%20has%20attracted%20many%20research%20interests%20due%20to%20its%0Asimilarity%20to%20biological%20schemes%20and%20high%20energy%20efficiency.%20However%2C%20existing%0Amethods%20depend%20on%20long-time%20training%20or%20fine-tuning%2C%20which%20prevents%20large-scale%0Aapplications.%20Recently%2C%20some%20works%20focusing%20on%20post-training%20sparsity%20%28PTS%29%0Ahave%20emerged.%20They%20get%20rid%20of%20the%20high%20training%20cost%20but%20usually%20suffer%20from%0Adistinct%20accuracy%20degradation%20due%20to%20neglect%20of%20the%20reasonable%20sparsity%20rate%20at%0Aeach%20layer.%20Previous%20methods%20for%20finding%20sparsity%20rates%20mainly%20focus%20on%20the%0Atraining-aware%20scenario%2C%20which%20usually%20fails%20to%20converge%20stably%20under%20the%20PTS%0Asetting%20with%20limited%20data%20and%20much%20less%20training%20cost.%20In%20this%20paper%2C%20we%0Apropose%20a%20fast%20and%20controllable%20post-training%20sparsity%20%28FCPTS%29%20framework.%20By%0Aincorporating%20a%20differentiable%20bridge%20function%20and%20a%20controllable%20optimization%0Aobjective%2C%20our%20method%20allows%20for%20rapid%20and%20accurate%20sparsity%20allocation%0Alearning%20in%20minutes%2C%20with%20the%20added%20assurance%20of%20convergence%20to%20a%20predetermined%0Aglobal%20sparsity%20rate.%20Equipped%20with%20these%20techniques%2C%20we%20can%20surpass%20the%0Astate-of-the-art%20methods%20by%20a%20large%20margin%2C%20e.g.%2C%20over%2030%5C%25%20improvement%20for%0AResNet-50%20on%20ImageNet%20under%20the%20sparsity%20rate%20of%2080%5C%25.%20Our%20plug-and-play%20code%0Aand%20supplementary%20materials%20are%20open-sourced%20at%0Ahttps%3A//github.com/ModelTC/FCPTS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05808v1&entry.124074799=Read"},
{"title": "Going Forward-Forward in Distributed Deep Learning", "author": "Ege Aktemur and Ege Zorlutuna and Kaan Bilgili and Tacettin Emre Bok and Berrin Yanikoglu and Suha Orhun Mutluergil", "abstract": "  We introduce a new approach in distributed deep learning, utilizing Geoffrey\nHinton's Forward-Forward (FF) algorithm to speed up the training of neural\nnetworks in distributed computing environments. Unlike traditional methods that\nrely on forward and backward passes, the FF algorithm employs a dual forward\npass strategy, significantly diverging from the conventional backpropagation\nprocess. This novel method aligns more closely with the human brain's\nprocessing mechanisms, potentially offering a more efficient and biologically\nplausible approach to neural network training. Our research explores different\nimplementations of the FF algorithm in distributed settings, to explore its\ncapacity for parallelization. While the original FF algorithm focused on its\nability to match the performance of the backpropagation algorithm, the\nparallelism aims to reduce training times and resource consumption, thereby\naddressing the long training times associated with the training of deep neural\nnetworks. Our evaluation shows a 3.75 times speed up on MNIST dataset without\ncompromising accuracy when training a four-layer network with four compute\nnodes. The integration of the FF algorithm into distributed deep learning\nrepresents a significant step forward in the field, potentially revolutionizing\nthe way neural networks are trained in distributed environments.\n", "link": "http://arxiv.org/abs/2404.08573v2", "date": "2024-05-09", "relevancy": 2.1408, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5623}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5178}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Going%20Forward-Forward%20in%20Distributed%20Deep%20Learning&body=Title%3A%20Going%20Forward-Forward%20in%20Distributed%20Deep%20Learning%0AAuthor%3A%20Ege%20Aktemur%20and%20Ege%20Zorlutuna%20and%20Kaan%20Bilgili%20and%20Tacettin%20Emre%20Bok%20and%20Berrin%20Yanikoglu%20and%20Suha%20Orhun%20Mutluergil%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20approach%20in%20distributed%20deep%20learning%2C%20utilizing%20Geoffrey%0AHinton%27s%20Forward-Forward%20%28FF%29%20algorithm%20to%20speed%20up%20the%20training%20of%20neural%0Anetworks%20in%20distributed%20computing%20environments.%20Unlike%20traditional%20methods%20that%0Arely%20on%20forward%20and%20backward%20passes%2C%20the%20FF%20algorithm%20employs%20a%20dual%20forward%0Apass%20strategy%2C%20significantly%20diverging%20from%20the%20conventional%20backpropagation%0Aprocess.%20This%20novel%20method%20aligns%20more%20closely%20with%20the%20human%20brain%27s%0Aprocessing%20mechanisms%2C%20potentially%20offering%20a%20more%20efficient%20and%20biologically%0Aplausible%20approach%20to%20neural%20network%20training.%20Our%20research%20explores%20different%0Aimplementations%20of%20the%20FF%20algorithm%20in%20distributed%20settings%2C%20to%20explore%20its%0Acapacity%20for%20parallelization.%20While%20the%20original%20FF%20algorithm%20focused%20on%20its%0Aability%20to%20match%20the%20performance%20of%20the%20backpropagation%20algorithm%2C%20the%0Aparallelism%20aims%20to%20reduce%20training%20times%20and%20resource%20consumption%2C%20thereby%0Aaddressing%20the%20long%20training%20times%20associated%20with%20the%20training%20of%20deep%20neural%0Anetworks.%20Our%20evaluation%20shows%20a%203.75%20times%20speed%20up%20on%20MNIST%20dataset%20without%0Acompromising%20accuracy%20when%20training%20a%20four-layer%20network%20with%20four%20compute%0Anodes.%20The%20integration%20of%20the%20FF%20algorithm%20into%20distributed%20deep%20learning%0Arepresents%20a%20significant%20step%20forward%20in%20the%20field%2C%20potentially%20revolutionizing%0Athe%20way%20neural%20networks%20are%20trained%20in%20distributed%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoing%2520Forward-Forward%2520in%2520Distributed%2520Deep%2520Learning%26entry.906535625%3DEge%2520Aktemur%2520and%2520Ege%2520Zorlutuna%2520and%2520Kaan%2520Bilgili%2520and%2520Tacettin%2520Emre%2520Bok%2520and%2520Berrin%2520Yanikoglu%2520and%2520Suha%2520Orhun%2520Mutluergil%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520approach%2520in%2520distributed%2520deep%2520learning%252C%2520utilizing%2520Geoffrey%250AHinton%2527s%2520Forward-Forward%2520%2528FF%2529%2520algorithm%2520to%2520speed%2520up%2520the%2520training%2520of%2520neural%250Anetworks%2520in%2520distributed%2520computing%2520environments.%2520Unlike%2520traditional%2520methods%2520that%250Arely%2520on%2520forward%2520and%2520backward%2520passes%252C%2520the%2520FF%2520algorithm%2520employs%2520a%2520dual%2520forward%250Apass%2520strategy%252C%2520significantly%2520diverging%2520from%2520the%2520conventional%2520backpropagation%250Aprocess.%2520This%2520novel%2520method%2520aligns%2520more%2520closely%2520with%2520the%2520human%2520brain%2527s%250Aprocessing%2520mechanisms%252C%2520potentially%2520offering%2520a%2520more%2520efficient%2520and%2520biologically%250Aplausible%2520approach%2520to%2520neural%2520network%2520training.%2520Our%2520research%2520explores%2520different%250Aimplementations%2520of%2520the%2520FF%2520algorithm%2520in%2520distributed%2520settings%252C%2520to%2520explore%2520its%250Acapacity%2520for%2520parallelization.%2520While%2520the%2520original%2520FF%2520algorithm%2520focused%2520on%2520its%250Aability%2520to%2520match%2520the%2520performance%2520of%2520the%2520backpropagation%2520algorithm%252C%2520the%250Aparallelism%2520aims%2520to%2520reduce%2520training%2520times%2520and%2520resource%2520consumption%252C%2520thereby%250Aaddressing%2520the%2520long%2520training%2520times%2520associated%2520with%2520the%2520training%2520of%2520deep%2520neural%250Anetworks.%2520Our%2520evaluation%2520shows%2520a%25203.75%2520times%2520speed%2520up%2520on%2520MNIST%2520dataset%2520without%250Acompromising%2520accuracy%2520when%2520training%2520a%2520four-layer%2520network%2520with%2520four%2520compute%250Anodes.%2520The%2520integration%2520of%2520the%2520FF%2520algorithm%2520into%2520distributed%2520deep%2520learning%250Arepresents%2520a%2520significant%2520step%2520forward%2520in%2520the%2520field%252C%2520potentially%2520revolutionizing%250Athe%2520way%2520neural%2520networks%2520are%2520trained%2520in%2520distributed%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Going%20Forward-Forward%20in%20Distributed%20Deep%20Learning&entry.906535625=Ege%20Aktemur%20and%20Ege%20Zorlutuna%20and%20Kaan%20Bilgili%20and%20Tacettin%20Emre%20Bok%20and%20Berrin%20Yanikoglu%20and%20Suha%20Orhun%20Mutluergil&entry.1292438233=%20%20We%20introduce%20a%20new%20approach%20in%20distributed%20deep%20learning%2C%20utilizing%20Geoffrey%0AHinton%27s%20Forward-Forward%20%28FF%29%20algorithm%20to%20speed%20up%20the%20training%20of%20neural%0Anetworks%20in%20distributed%20computing%20environments.%20Unlike%20traditional%20methods%20that%0Arely%20on%20forward%20and%20backward%20passes%2C%20the%20FF%20algorithm%20employs%20a%20dual%20forward%0Apass%20strategy%2C%20significantly%20diverging%20from%20the%20conventional%20backpropagation%0Aprocess.%20This%20novel%20method%20aligns%20more%20closely%20with%20the%20human%20brain%27s%0Aprocessing%20mechanisms%2C%20potentially%20offering%20a%20more%20efficient%20and%20biologically%0Aplausible%20approach%20to%20neural%20network%20training.%20Our%20research%20explores%20different%0Aimplementations%20of%20the%20FF%20algorithm%20in%20distributed%20settings%2C%20to%20explore%20its%0Acapacity%20for%20parallelization.%20While%20the%20original%20FF%20algorithm%20focused%20on%20its%0Aability%20to%20match%20the%20performance%20of%20the%20backpropagation%20algorithm%2C%20the%0Aparallelism%20aims%20to%20reduce%20training%20times%20and%20resource%20consumption%2C%20thereby%0Aaddressing%20the%20long%20training%20times%20associated%20with%20the%20training%20of%20deep%20neural%0Anetworks.%20Our%20evaluation%20shows%20a%203.75%20times%20speed%20up%20on%20MNIST%20dataset%20without%0Acompromising%20accuracy%20when%20training%20a%20four-layer%20network%20with%20four%20compute%0Anodes.%20The%20integration%20of%20the%20FF%20algorithm%20into%20distributed%20deep%20learning%0Arepresents%20a%20significant%20step%20forward%20in%20the%20field%2C%20potentially%20revolutionizing%0Athe%20way%20neural%20networks%20are%20trained%20in%20distributed%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08573v2&entry.124074799=Read"},
{"title": "Link Stealing Attacks Against Inductive Graph Neural Networks", "author": "Yixin Wu and Xinlei He and Pascal Berrang and Mathias Humbert and Michael Backes and Neil Zhenqiang Gong and Yang Zhang", "abstract": "  A graph neural network (GNN) is a type of neural network that is specifically\ndesigned to process graph-structured data. Typically, GNNs can be implemented\nin two settings, including the transductive setting and the inductive setting.\nIn the transductive setting, the trained model can only predict the labels of\nnodes that were observed at the training time. In the inductive setting, the\ntrained model can be generalized to new nodes/graphs. Due to its flexibility,\nthe inductive setting is the most popular GNN setting at the moment. Previous\nwork has shown that transductive GNNs are vulnerable to a series of privacy\nattacks. However, a comprehensive privacy analysis of inductive GNN models is\nstill missing. This paper fills the gap by conducting a systematic privacy\nanalysis of inductive GNNs through the lens of link stealing attacks, one of\nthe most popular attacks that are specifically designed for GNNs. We propose\ntwo types of link stealing attacks, i.e., posterior-only attacks and combined\nattacks. We define threat models of the posterior-only attacks with respect to\nnode topology and the combined attacks by considering combinations of\nposteriors, node attributes, and graph features. Extensive evaluation on six\nreal-world datasets demonstrates that inductive GNNs leak rich information that\nenables link stealing attacks with advantageous properties. Even attacks with\nno knowledge about graph structures can be effective. We also show that our\nattacks are robust to different node similarities and different graph features.\nAs a counterpart, we investigate two possible defenses and discover they are\nineffective against our attacks, which calls for more effective defenses.\n", "link": "http://arxiv.org/abs/2405.05784v1", "date": "2024-05-09", "relevancy": 2.1216, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.452}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4234}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Link%20Stealing%20Attacks%20Against%20Inductive%20Graph%20Neural%20Networks&body=Title%3A%20Link%20Stealing%20Attacks%20Against%20Inductive%20Graph%20Neural%20Networks%0AAuthor%3A%20Yixin%20Wu%20and%20Xinlei%20He%20and%20Pascal%20Berrang%20and%20Mathias%20Humbert%20and%20Michael%20Backes%20and%20Neil%20Zhenqiang%20Gong%20and%20Yang%20Zhang%0AAbstract%3A%20%20%20A%20graph%20neural%20network%20%28GNN%29%20is%20a%20type%20of%20neural%20network%20that%20is%20specifically%0Adesigned%20to%20process%20graph-structured%20data.%20Typically%2C%20GNNs%20can%20be%20implemented%0Ain%20two%20settings%2C%20including%20the%20transductive%20setting%20and%20the%20inductive%20setting.%0AIn%20the%20transductive%20setting%2C%20the%20trained%20model%20can%20only%20predict%20the%20labels%20of%0Anodes%20that%20were%20observed%20at%20the%20training%20time.%20In%20the%20inductive%20setting%2C%20the%0Atrained%20model%20can%20be%20generalized%20to%20new%20nodes/graphs.%20Due%20to%20its%20flexibility%2C%0Athe%20inductive%20setting%20is%20the%20most%20popular%20GNN%20setting%20at%20the%20moment.%20Previous%0Awork%20has%20shown%20that%20transductive%20GNNs%20are%20vulnerable%20to%20a%20series%20of%20privacy%0Aattacks.%20However%2C%20a%20comprehensive%20privacy%20analysis%20of%20inductive%20GNN%20models%20is%0Astill%20missing.%20This%20paper%20fills%20the%20gap%20by%20conducting%20a%20systematic%20privacy%0Aanalysis%20of%20inductive%20GNNs%20through%20the%20lens%20of%20link%20stealing%20attacks%2C%20one%20of%0Athe%20most%20popular%20attacks%20that%20are%20specifically%20designed%20for%20GNNs.%20We%20propose%0Atwo%20types%20of%20link%20stealing%20attacks%2C%20i.e.%2C%20posterior-only%20attacks%20and%20combined%0Aattacks.%20We%20define%20threat%20models%20of%20the%20posterior-only%20attacks%20with%20respect%20to%0Anode%20topology%20and%20the%20combined%20attacks%20by%20considering%20combinations%20of%0Aposteriors%2C%20node%20attributes%2C%20and%20graph%20features.%20Extensive%20evaluation%20on%20six%0Areal-world%20datasets%20demonstrates%20that%20inductive%20GNNs%20leak%20rich%20information%20that%0Aenables%20link%20stealing%20attacks%20with%20advantageous%20properties.%20Even%20attacks%20with%0Ano%20knowledge%20about%20graph%20structures%20can%20be%20effective.%20We%20also%20show%20that%20our%0Aattacks%20are%20robust%20to%20different%20node%20similarities%20and%20different%20graph%20features.%0AAs%20a%20counterpart%2C%20we%20investigate%20two%20possible%20defenses%20and%20discover%20they%20are%0Aineffective%20against%20our%20attacks%2C%20which%20calls%20for%20more%20effective%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLink%2520Stealing%2520Attacks%2520Against%2520Inductive%2520Graph%2520Neural%2520Networks%26entry.906535625%3DYixin%2520Wu%2520and%2520Xinlei%2520He%2520and%2520Pascal%2520Berrang%2520and%2520Mathias%2520Humbert%2520and%2520Michael%2520Backes%2520and%2520Neil%2520Zhenqiang%2520Gong%2520and%2520Yang%2520Zhang%26entry.1292438233%3D%2520%2520A%2520graph%2520neural%2520network%2520%2528GNN%2529%2520is%2520a%2520type%2520of%2520neural%2520network%2520that%2520is%2520specifically%250Adesigned%2520to%2520process%2520graph-structured%2520data.%2520Typically%252C%2520GNNs%2520can%2520be%2520implemented%250Ain%2520two%2520settings%252C%2520including%2520the%2520transductive%2520setting%2520and%2520the%2520inductive%2520setting.%250AIn%2520the%2520transductive%2520setting%252C%2520the%2520trained%2520model%2520can%2520only%2520predict%2520the%2520labels%2520of%250Anodes%2520that%2520were%2520observed%2520at%2520the%2520training%2520time.%2520In%2520the%2520inductive%2520setting%252C%2520the%250Atrained%2520model%2520can%2520be%2520generalized%2520to%2520new%2520nodes/graphs.%2520Due%2520to%2520its%2520flexibility%252C%250Athe%2520inductive%2520setting%2520is%2520the%2520most%2520popular%2520GNN%2520setting%2520at%2520the%2520moment.%2520Previous%250Awork%2520has%2520shown%2520that%2520transductive%2520GNNs%2520are%2520vulnerable%2520to%2520a%2520series%2520of%2520privacy%250Aattacks.%2520However%252C%2520a%2520comprehensive%2520privacy%2520analysis%2520of%2520inductive%2520GNN%2520models%2520is%250Astill%2520missing.%2520This%2520paper%2520fills%2520the%2520gap%2520by%2520conducting%2520a%2520systematic%2520privacy%250Aanalysis%2520of%2520inductive%2520GNNs%2520through%2520the%2520lens%2520of%2520link%2520stealing%2520attacks%252C%2520one%2520of%250Athe%2520most%2520popular%2520attacks%2520that%2520are%2520specifically%2520designed%2520for%2520GNNs.%2520We%2520propose%250Atwo%2520types%2520of%2520link%2520stealing%2520attacks%252C%2520i.e.%252C%2520posterior-only%2520attacks%2520and%2520combined%250Aattacks.%2520We%2520define%2520threat%2520models%2520of%2520the%2520posterior-only%2520attacks%2520with%2520respect%2520to%250Anode%2520topology%2520and%2520the%2520combined%2520attacks%2520by%2520considering%2520combinations%2520of%250Aposteriors%252C%2520node%2520attributes%252C%2520and%2520graph%2520features.%2520Extensive%2520evaluation%2520on%2520six%250Areal-world%2520datasets%2520demonstrates%2520that%2520inductive%2520GNNs%2520leak%2520rich%2520information%2520that%250Aenables%2520link%2520stealing%2520attacks%2520with%2520advantageous%2520properties.%2520Even%2520attacks%2520with%250Ano%2520knowledge%2520about%2520graph%2520structures%2520can%2520be%2520effective.%2520We%2520also%2520show%2520that%2520our%250Aattacks%2520are%2520robust%2520to%2520different%2520node%2520similarities%2520and%2520different%2520graph%2520features.%250AAs%2520a%2520counterpart%252C%2520we%2520investigate%2520two%2520possible%2520defenses%2520and%2520discover%2520they%2520are%250Aineffective%2520against%2520our%2520attacks%252C%2520which%2520calls%2520for%2520more%2520effective%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Link%20Stealing%20Attacks%20Against%20Inductive%20Graph%20Neural%20Networks&entry.906535625=Yixin%20Wu%20and%20Xinlei%20He%20and%20Pascal%20Berrang%20and%20Mathias%20Humbert%20and%20Michael%20Backes%20and%20Neil%20Zhenqiang%20Gong%20and%20Yang%20Zhang&entry.1292438233=%20%20A%20graph%20neural%20network%20%28GNN%29%20is%20a%20type%20of%20neural%20network%20that%20is%20specifically%0Adesigned%20to%20process%20graph-structured%20data.%20Typically%2C%20GNNs%20can%20be%20implemented%0Ain%20two%20settings%2C%20including%20the%20transductive%20setting%20and%20the%20inductive%20setting.%0AIn%20the%20transductive%20setting%2C%20the%20trained%20model%20can%20only%20predict%20the%20labels%20of%0Anodes%20that%20were%20observed%20at%20the%20training%20time.%20In%20the%20inductive%20setting%2C%20the%0Atrained%20model%20can%20be%20generalized%20to%20new%20nodes/graphs.%20Due%20to%20its%20flexibility%2C%0Athe%20inductive%20setting%20is%20the%20most%20popular%20GNN%20setting%20at%20the%20moment.%20Previous%0Awork%20has%20shown%20that%20transductive%20GNNs%20are%20vulnerable%20to%20a%20series%20of%20privacy%0Aattacks.%20However%2C%20a%20comprehensive%20privacy%20analysis%20of%20inductive%20GNN%20models%20is%0Astill%20missing.%20This%20paper%20fills%20the%20gap%20by%20conducting%20a%20systematic%20privacy%0Aanalysis%20of%20inductive%20GNNs%20through%20the%20lens%20of%20link%20stealing%20attacks%2C%20one%20of%0Athe%20most%20popular%20attacks%20that%20are%20specifically%20designed%20for%20GNNs.%20We%20propose%0Atwo%20types%20of%20link%20stealing%20attacks%2C%20i.e.%2C%20posterior-only%20attacks%20and%20combined%0Aattacks.%20We%20define%20threat%20models%20of%20the%20posterior-only%20attacks%20with%20respect%20to%0Anode%20topology%20and%20the%20combined%20attacks%20by%20considering%20combinations%20of%0Aposteriors%2C%20node%20attributes%2C%20and%20graph%20features.%20Extensive%20evaluation%20on%20six%0Areal-world%20datasets%20demonstrates%20that%20inductive%20GNNs%20leak%20rich%20information%20that%0Aenables%20link%20stealing%20attacks%20with%20advantageous%20properties.%20Even%20attacks%20with%0Ano%20knowledge%20about%20graph%20structures%20can%20be%20effective.%20We%20also%20show%20that%20our%0Aattacks%20are%20robust%20to%20different%20node%20similarities%20and%20different%20graph%20features.%0AAs%20a%20counterpart%2C%20we%20investigate%20two%20possible%20defenses%20and%20discover%20they%20are%0Aineffective%20against%20our%20attacks%2C%20which%20calls%20for%20more%20effective%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05784v1&entry.124074799=Read"},
{"title": "Informed Decision-Making through Advancements in Open Set Recognition\n  and Unknown Sample Detection", "author": "Atefeh Mahdavi and Marco Carvalho", "abstract": "  Machine learning-based techniques open up many opportunities and improvements\nto derive deeper and more practical insights from data that can help businesses\nmake informed decisions. However, the majority of these techniques focus on the\nconventional closed-set scenario, in which the label spaces for the training\nand test sets are identical. Open set recognition (OSR) aims to bring\nclassification tasks in a situation that is more like reality, which focuses on\nclassifying the known classes as well as handling unknown classes effectively.\nIn such an open-set problem the gathered samples in the training set cannot\nencompass all the classes and the system needs to identify unknown samples at\ntest time. On the other hand, building an accurate and comprehensive model in a\nreal dynamic environment presents a number of obstacles, because it is\nprohibitively expensive to train for every possible example of unknown items,\nand the model may fail when tested in testbeds. This study provides an\nalgorithm exploring a new representation of feature space to improve\nclassification in OSR tasks. The efficacy and efficiency of business processes\nand decision-making can be improved by integrating OSR, which offers more\nprecise and insightful predictions of outcomes. We demonstrate the performance\nof the proposed method on three established datasets. The results indicate that\nthe proposed model outperforms the baseline methods in accuracy and F1-score.\n", "link": "http://arxiv.org/abs/2405.05836v1", "date": "2024-05-09", "relevancy": 2.1107, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5412}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5256}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Informed%20Decision-Making%20through%20Advancements%20in%20Open%20Set%20Recognition%0A%20%20and%20Unknown%20Sample%20Detection&body=Title%3A%20Informed%20Decision-Making%20through%20Advancements%20in%20Open%20Set%20Recognition%0A%20%20and%20Unknown%20Sample%20Detection%0AAuthor%3A%20Atefeh%20Mahdavi%20and%20Marco%20Carvalho%0AAbstract%3A%20%20%20Machine%20learning-based%20techniques%20open%20up%20many%20opportunities%20and%20improvements%0Ato%20derive%20deeper%20and%20more%20practical%20insights%20from%20data%20that%20can%20help%20businesses%0Amake%20informed%20decisions.%20However%2C%20the%20majority%20of%20these%20techniques%20focus%20on%20the%0Aconventional%20closed-set%20scenario%2C%20in%20which%20the%20label%20spaces%20for%20the%20training%0Aand%20test%20sets%20are%20identical.%20Open%20set%20recognition%20%28OSR%29%20aims%20to%20bring%0Aclassification%20tasks%20in%20a%20situation%20that%20is%20more%20like%20reality%2C%20which%20focuses%20on%0Aclassifying%20the%20known%20classes%20as%20well%20as%20handling%20unknown%20classes%20effectively.%0AIn%20such%20an%20open-set%20problem%20the%20gathered%20samples%20in%20the%20training%20set%20cannot%0Aencompass%20all%20the%20classes%20and%20the%20system%20needs%20to%20identify%20unknown%20samples%20at%0Atest%20time.%20On%20the%20other%20hand%2C%20building%20an%20accurate%20and%20comprehensive%20model%20in%20a%0Areal%20dynamic%20environment%20presents%20a%20number%20of%20obstacles%2C%20because%20it%20is%0Aprohibitively%20expensive%20to%20train%20for%20every%20possible%20example%20of%20unknown%20items%2C%0Aand%20the%20model%20may%20fail%20when%20tested%20in%20testbeds.%20This%20study%20provides%20an%0Aalgorithm%20exploring%20a%20new%20representation%20of%20feature%20space%20to%20improve%0Aclassification%20in%20OSR%20tasks.%20The%20efficacy%20and%20efficiency%20of%20business%20processes%0Aand%20decision-making%20can%20be%20improved%20by%20integrating%20OSR%2C%20which%20offers%20more%0Aprecise%20and%20insightful%20predictions%20of%20outcomes.%20We%20demonstrate%20the%20performance%0Aof%20the%20proposed%20method%20on%20three%20established%20datasets.%20The%20results%20indicate%20that%0Athe%20proposed%20model%20outperforms%20the%20baseline%20methods%20in%20accuracy%20and%20F1-score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformed%2520Decision-Making%2520through%2520Advancements%2520in%2520Open%2520Set%2520Recognition%250A%2520%2520and%2520Unknown%2520Sample%2520Detection%26entry.906535625%3DAtefeh%2520Mahdavi%2520and%2520Marco%2520Carvalho%26entry.1292438233%3D%2520%2520Machine%2520learning-based%2520techniques%2520open%2520up%2520many%2520opportunities%2520and%2520improvements%250Ato%2520derive%2520deeper%2520and%2520more%2520practical%2520insights%2520from%2520data%2520that%2520can%2520help%2520businesses%250Amake%2520informed%2520decisions.%2520However%252C%2520the%2520majority%2520of%2520these%2520techniques%2520focus%2520on%2520the%250Aconventional%2520closed-set%2520scenario%252C%2520in%2520which%2520the%2520label%2520spaces%2520for%2520the%2520training%250Aand%2520test%2520sets%2520are%2520identical.%2520Open%2520set%2520recognition%2520%2528OSR%2529%2520aims%2520to%2520bring%250Aclassification%2520tasks%2520in%2520a%2520situation%2520that%2520is%2520more%2520like%2520reality%252C%2520which%2520focuses%2520on%250Aclassifying%2520the%2520known%2520classes%2520as%2520well%2520as%2520handling%2520unknown%2520classes%2520effectively.%250AIn%2520such%2520an%2520open-set%2520problem%2520the%2520gathered%2520samples%2520in%2520the%2520training%2520set%2520cannot%250Aencompass%2520all%2520the%2520classes%2520and%2520the%2520system%2520needs%2520to%2520identify%2520unknown%2520samples%2520at%250Atest%2520time.%2520On%2520the%2520other%2520hand%252C%2520building%2520an%2520accurate%2520and%2520comprehensive%2520model%2520in%2520a%250Areal%2520dynamic%2520environment%2520presents%2520a%2520number%2520of%2520obstacles%252C%2520because%2520it%2520is%250Aprohibitively%2520expensive%2520to%2520train%2520for%2520every%2520possible%2520example%2520of%2520unknown%2520items%252C%250Aand%2520the%2520model%2520may%2520fail%2520when%2520tested%2520in%2520testbeds.%2520This%2520study%2520provides%2520an%250Aalgorithm%2520exploring%2520a%2520new%2520representation%2520of%2520feature%2520space%2520to%2520improve%250Aclassification%2520in%2520OSR%2520tasks.%2520The%2520efficacy%2520and%2520efficiency%2520of%2520business%2520processes%250Aand%2520decision-making%2520can%2520be%2520improved%2520by%2520integrating%2520OSR%252C%2520which%2520offers%2520more%250Aprecise%2520and%2520insightful%2520predictions%2520of%2520outcomes.%2520We%2520demonstrate%2520the%2520performance%250Aof%2520the%2520proposed%2520method%2520on%2520three%2520established%2520datasets.%2520The%2520results%2520indicate%2520that%250Athe%2520proposed%2520model%2520outperforms%2520the%2520baseline%2520methods%2520in%2520accuracy%2520and%2520F1-score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Informed%20Decision-Making%20through%20Advancements%20in%20Open%20Set%20Recognition%0A%20%20and%20Unknown%20Sample%20Detection&entry.906535625=Atefeh%20Mahdavi%20and%20Marco%20Carvalho&entry.1292438233=%20%20Machine%20learning-based%20techniques%20open%20up%20many%20opportunities%20and%20improvements%0Ato%20derive%20deeper%20and%20more%20practical%20insights%20from%20data%20that%20can%20help%20businesses%0Amake%20informed%20decisions.%20However%2C%20the%20majority%20of%20these%20techniques%20focus%20on%20the%0Aconventional%20closed-set%20scenario%2C%20in%20which%20the%20label%20spaces%20for%20the%20training%0Aand%20test%20sets%20are%20identical.%20Open%20set%20recognition%20%28OSR%29%20aims%20to%20bring%0Aclassification%20tasks%20in%20a%20situation%20that%20is%20more%20like%20reality%2C%20which%20focuses%20on%0Aclassifying%20the%20known%20classes%20as%20well%20as%20handling%20unknown%20classes%20effectively.%0AIn%20such%20an%20open-set%20problem%20the%20gathered%20samples%20in%20the%20training%20set%20cannot%0Aencompass%20all%20the%20classes%20and%20the%20system%20needs%20to%20identify%20unknown%20samples%20at%0Atest%20time.%20On%20the%20other%20hand%2C%20building%20an%20accurate%20and%20comprehensive%20model%20in%20a%0Areal%20dynamic%20environment%20presents%20a%20number%20of%20obstacles%2C%20because%20it%20is%0Aprohibitively%20expensive%20to%20train%20for%20every%20possible%20example%20of%20unknown%20items%2C%0Aand%20the%20model%20may%20fail%20when%20tested%20in%20testbeds.%20This%20study%20provides%20an%0Aalgorithm%20exploring%20a%20new%20representation%20of%20feature%20space%20to%20improve%0Aclassification%20in%20OSR%20tasks.%20The%20efficacy%20and%20efficiency%20of%20business%20processes%0Aand%20decision-making%20can%20be%20improved%20by%20integrating%20OSR%2C%20which%20offers%20more%0Aprecise%20and%20insightful%20predictions%20of%20outcomes.%20We%20demonstrate%20the%20performance%0Aof%20the%20proposed%20method%20on%20three%20established%20datasets.%20The%20results%20indicate%20that%0Athe%20proposed%20model%20outperforms%20the%20baseline%20methods%20in%20accuracy%20and%20F1-score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05836v1&entry.124074799=Read"},
{"title": "V2X Cooperative Perception for Autonomous Driving: Recent Advances and\n  Challenges", "author": "Tao Huang and Jianan Liu and Xi Zhou and Dinh C. Nguyen and Mostafa Rahimi Azghadi and Yuxuan Xia and Qing-Long Han and Sumei Sun", "abstract": "  Accurate perception is essential for advancing autonomous driving and\naddressing safety challenges in modern transportation systems. Despite\nsignificant advancements in computer vision for object recognition, current\nperception methods still face difficulties in complex real-world traffic\nenvironments. Challenges such as physical occlusion and limited sensor field of\nview persist for individual vehicle systems. Cooperative Perception (CP) with\nVehicle-to-Everything (V2X) technologies has emerged as a solution to overcome\nthese obstacles and enhance driving automation systems. While some research has\nexplored CP's fundamental architecture and critical components, there remains a\nlack of comprehensive summaries of the latest innovations, particularly in the\ncontext of V2X communication technologies. To address this gap, this paper\nprovides a comprehensive overview of the evolution of CP technologies, spanning\nfrom early explorations to recent developments, including advancements in V2X\ncommunication technologies. Additionally, a contemporary generic framework is\nalso proposed to illustrate the V2X-based CP workflow, aiding in the structured\nunderstanding of CP system components. Furthermore, this paper categorizes\nprevailing V2X-based CP methodologies based on the critical issues they\naddress. An extensive literature review is conducted within this taxonomy,\nevaluating existing datasets and simulators. Finally, open challenges and\nfuture directions in CP for autonomous driving are discussed by considering\nboth perception and V2X communication advancements.\n", "link": "http://arxiv.org/abs/2310.03525v3", "date": "2024-05-09", "relevancy": 2.0984, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5373}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.531}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2X%20Cooperative%20Perception%20for%20Autonomous%20Driving%3A%20Recent%20Advances%20and%0A%20%20Challenges&body=Title%3A%20V2X%20Cooperative%20Perception%20for%20Autonomous%20Driving%3A%20Recent%20Advances%20and%0A%20%20Challenges%0AAuthor%3A%20Tao%20Huang%20and%20Jianan%20Liu%20and%20Xi%20Zhou%20and%20Dinh%20C.%20Nguyen%20and%20Mostafa%20Rahimi%20Azghadi%20and%20Yuxuan%20Xia%20and%20Qing-Long%20Han%20and%20Sumei%20Sun%0AAbstract%3A%20%20%20Accurate%20perception%20is%20essential%20for%20advancing%20autonomous%20driving%20and%0Aaddressing%20safety%20challenges%20in%20modern%20transportation%20systems.%20Despite%0Asignificant%20advancements%20in%20computer%20vision%20for%20object%20recognition%2C%20current%0Aperception%20methods%20still%20face%20difficulties%20in%20complex%20real-world%20traffic%0Aenvironments.%20Challenges%20such%20as%20physical%20occlusion%20and%20limited%20sensor%20field%20of%0Aview%20persist%20for%20individual%20vehicle%20systems.%20Cooperative%20Perception%20%28CP%29%20with%0AVehicle-to-Everything%20%28V2X%29%20technologies%20has%20emerged%20as%20a%20solution%20to%20overcome%0Athese%20obstacles%20and%20enhance%20driving%20automation%20systems.%20While%20some%20research%20has%0Aexplored%20CP%27s%20fundamental%20architecture%20and%20critical%20components%2C%20there%20remains%20a%0Alack%20of%20comprehensive%20summaries%20of%20the%20latest%20innovations%2C%20particularly%20in%20the%0Acontext%20of%20V2X%20communication%20technologies.%20To%20address%20this%20gap%2C%20this%20paper%0Aprovides%20a%20comprehensive%20overview%20of%20the%20evolution%20of%20CP%20technologies%2C%20spanning%0Afrom%20early%20explorations%20to%20recent%20developments%2C%20including%20advancements%20in%20V2X%0Acommunication%20technologies.%20Additionally%2C%20a%20contemporary%20generic%20framework%20is%0Aalso%20proposed%20to%20illustrate%20the%20V2X-based%20CP%20workflow%2C%20aiding%20in%20the%20structured%0Aunderstanding%20of%20CP%20system%20components.%20Furthermore%2C%20this%20paper%20categorizes%0Aprevailing%20V2X-based%20CP%20methodologies%20based%20on%20the%20critical%20issues%20they%0Aaddress.%20An%20extensive%20literature%20review%20is%20conducted%20within%20this%20taxonomy%2C%0Aevaluating%20existing%20datasets%20and%20simulators.%20Finally%2C%20open%20challenges%20and%0Afuture%20directions%20in%20CP%20for%20autonomous%20driving%20are%20discussed%20by%20considering%0Aboth%20perception%20and%20V2X%20communication%20advancements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03525v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2X%2520Cooperative%2520Perception%2520for%2520Autonomous%2520Driving%253A%2520Recent%2520Advances%2520and%250A%2520%2520Challenges%26entry.906535625%3DTao%2520Huang%2520and%2520Jianan%2520Liu%2520and%2520Xi%2520Zhou%2520and%2520Dinh%2520C.%2520Nguyen%2520and%2520Mostafa%2520Rahimi%2520Azghadi%2520and%2520Yuxuan%2520Xia%2520and%2520Qing-Long%2520Han%2520and%2520Sumei%2520Sun%26entry.1292438233%3D%2520%2520Accurate%2520perception%2520is%2520essential%2520for%2520advancing%2520autonomous%2520driving%2520and%250Aaddressing%2520safety%2520challenges%2520in%2520modern%2520transportation%2520systems.%2520Despite%250Asignificant%2520advancements%2520in%2520computer%2520vision%2520for%2520object%2520recognition%252C%2520current%250Aperception%2520methods%2520still%2520face%2520difficulties%2520in%2520complex%2520real-world%2520traffic%250Aenvironments.%2520Challenges%2520such%2520as%2520physical%2520occlusion%2520and%2520limited%2520sensor%2520field%2520of%250Aview%2520persist%2520for%2520individual%2520vehicle%2520systems.%2520Cooperative%2520Perception%2520%2528CP%2529%2520with%250AVehicle-to-Everything%2520%2528V2X%2529%2520technologies%2520has%2520emerged%2520as%2520a%2520solution%2520to%2520overcome%250Athese%2520obstacles%2520and%2520enhance%2520driving%2520automation%2520systems.%2520While%2520some%2520research%2520has%250Aexplored%2520CP%2527s%2520fundamental%2520architecture%2520and%2520critical%2520components%252C%2520there%2520remains%2520a%250Alack%2520of%2520comprehensive%2520summaries%2520of%2520the%2520latest%2520innovations%252C%2520particularly%2520in%2520the%250Acontext%2520of%2520V2X%2520communication%2520technologies.%2520To%2520address%2520this%2520gap%252C%2520this%2520paper%250Aprovides%2520a%2520comprehensive%2520overview%2520of%2520the%2520evolution%2520of%2520CP%2520technologies%252C%2520spanning%250Afrom%2520early%2520explorations%2520to%2520recent%2520developments%252C%2520including%2520advancements%2520in%2520V2X%250Acommunication%2520technologies.%2520Additionally%252C%2520a%2520contemporary%2520generic%2520framework%2520is%250Aalso%2520proposed%2520to%2520illustrate%2520the%2520V2X-based%2520CP%2520workflow%252C%2520aiding%2520in%2520the%2520structured%250Aunderstanding%2520of%2520CP%2520system%2520components.%2520Furthermore%252C%2520this%2520paper%2520categorizes%250Aprevailing%2520V2X-based%2520CP%2520methodologies%2520based%2520on%2520the%2520critical%2520issues%2520they%250Aaddress.%2520An%2520extensive%2520literature%2520review%2520is%2520conducted%2520within%2520this%2520taxonomy%252C%250Aevaluating%2520existing%2520datasets%2520and%2520simulators.%2520Finally%252C%2520open%2520challenges%2520and%250Afuture%2520directions%2520in%2520CP%2520for%2520autonomous%2520driving%2520are%2520discussed%2520by%2520considering%250Aboth%2520perception%2520and%2520V2X%2520communication%2520advancements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03525v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2X%20Cooperative%20Perception%20for%20Autonomous%20Driving%3A%20Recent%20Advances%20and%0A%20%20Challenges&entry.906535625=Tao%20Huang%20and%20Jianan%20Liu%20and%20Xi%20Zhou%20and%20Dinh%20C.%20Nguyen%20and%20Mostafa%20Rahimi%20Azghadi%20and%20Yuxuan%20Xia%20and%20Qing-Long%20Han%20and%20Sumei%20Sun&entry.1292438233=%20%20Accurate%20perception%20is%20essential%20for%20advancing%20autonomous%20driving%20and%0Aaddressing%20safety%20challenges%20in%20modern%20transportation%20systems.%20Despite%0Asignificant%20advancements%20in%20computer%20vision%20for%20object%20recognition%2C%20current%0Aperception%20methods%20still%20face%20difficulties%20in%20complex%20real-world%20traffic%0Aenvironments.%20Challenges%20such%20as%20physical%20occlusion%20and%20limited%20sensor%20field%20of%0Aview%20persist%20for%20individual%20vehicle%20systems.%20Cooperative%20Perception%20%28CP%29%20with%0AVehicle-to-Everything%20%28V2X%29%20technologies%20has%20emerged%20as%20a%20solution%20to%20overcome%0Athese%20obstacles%20and%20enhance%20driving%20automation%20systems.%20While%20some%20research%20has%0Aexplored%20CP%27s%20fundamental%20architecture%20and%20critical%20components%2C%20there%20remains%20a%0Alack%20of%20comprehensive%20summaries%20of%20the%20latest%20innovations%2C%20particularly%20in%20the%0Acontext%20of%20V2X%20communication%20technologies.%20To%20address%20this%20gap%2C%20this%20paper%0Aprovides%20a%20comprehensive%20overview%20of%20the%20evolution%20of%20CP%20technologies%2C%20spanning%0Afrom%20early%20explorations%20to%20recent%20developments%2C%20including%20advancements%20in%20V2X%0Acommunication%20technologies.%20Additionally%2C%20a%20contemporary%20generic%20framework%20is%0Aalso%20proposed%20to%20illustrate%20the%20V2X-based%20CP%20workflow%2C%20aiding%20in%20the%20structured%0Aunderstanding%20of%20CP%20system%20components.%20Furthermore%2C%20this%20paper%20categorizes%0Aprevailing%20V2X-based%20CP%20methodologies%20based%20on%20the%20critical%20issues%20they%0Aaddress.%20An%20extensive%20literature%20review%20is%20conducted%20within%20this%20taxonomy%2C%0Aevaluating%20existing%20datasets%20and%20simulators.%20Finally%2C%20open%20challenges%20and%0Afuture%20directions%20in%20CP%20for%20autonomous%20driving%20are%20discussed%20by%20considering%0Aboth%20perception%20and%20V2X%20communication%20advancements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03525v3&entry.124074799=Read"},
{"title": "Efficient Pretraining Model based on Multi-Scale Local Visual Field\n  Feature Reconstruction for PCB CT Image Element Segmentation", "author": "Chen Chen and Kai Qiao and Jie Yang and Jian Chen and Bin Yan", "abstract": "  Element segmentation is a key step in nondestructive testing of Printed\nCircuit Boards (PCB) based on Computed Tomography (CT) technology. In recent\nyears, the rapid development of self-supervised pretraining technology can\nobtain general image features without labeled samples, and then use a small\namount of labeled samples to solve downstream tasks, which has a good potential\nin PCB element segmentation. At present, Masked Image Modeling (MIM)\npretraining model has been initially applied in PCB CT image element\nsegmentation. However, due to the small and regular size of PCB elements such\nas vias, wires, and pads, the global visual field has redundancy for a single\nelement reconstruction, which may damage the performance of the model. Based on\nthis issue, we propose an efficient pretraining model based on multi-scale\nlocal visual field feature reconstruction for PCB CT image element segmentation\n(EMLR-seg). In this model, the teacher-guided MIM pretraining model is\nintroduced into PCB CT image element segmentation for the first time, and a\nmulti-scale local visual field extraction (MVE) module is proposed to reduce\nredundancy by focusing on local visual fields. At the same time, a simple\n4-Transformer-blocks decoder is used. Experiments show that EMLR-seg can\nachieve 88.6% mIoU on the PCB CT image dataset we proposed, which exceeds 1.2%\nby the baseline model, and the training time is reduced by 29.6 hours, a\nreduction of 17.4% under the same experimental condition, which reflects the\nadvantage of EMLR-seg in terms of performance and efficiency.\n", "link": "http://arxiv.org/abs/2405.05745v1", "date": "2024-05-09", "relevancy": 2.0952, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5377}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5214}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Pretraining%20Model%20based%20on%20Multi-Scale%20Local%20Visual%20Field%0A%20%20Feature%20Reconstruction%20for%20PCB%20CT%20Image%20Element%20Segmentation&body=Title%3A%20Efficient%20Pretraining%20Model%20based%20on%20Multi-Scale%20Local%20Visual%20Field%0A%20%20Feature%20Reconstruction%20for%20PCB%20CT%20Image%20Element%20Segmentation%0AAuthor%3A%20Chen%20Chen%20and%20Kai%20Qiao%20and%20Jie%20Yang%20and%20Jian%20Chen%20and%20Bin%20Yan%0AAbstract%3A%20%20%20Element%20segmentation%20is%20a%20key%20step%20in%20nondestructive%20testing%20of%20Printed%0ACircuit%20Boards%20%28PCB%29%20based%20on%20Computed%20Tomography%20%28CT%29%20technology.%20In%20recent%0Ayears%2C%20the%20rapid%20development%20of%20self-supervised%20pretraining%20technology%20can%0Aobtain%20general%20image%20features%20without%20labeled%20samples%2C%20and%20then%20use%20a%20small%0Aamount%20of%20labeled%20samples%20to%20solve%20downstream%20tasks%2C%20which%20has%20a%20good%20potential%0Ain%20PCB%20element%20segmentation.%20At%20present%2C%20Masked%20Image%20Modeling%20%28MIM%29%0Apretraining%20model%20has%20been%20initially%20applied%20in%20PCB%20CT%20image%20element%0Asegmentation.%20However%2C%20due%20to%20the%20small%20and%20regular%20size%20of%20PCB%20elements%20such%0Aas%20vias%2C%20wires%2C%20and%20pads%2C%20the%20global%20visual%20field%20has%20redundancy%20for%20a%20single%0Aelement%20reconstruction%2C%20which%20may%20damage%20the%20performance%20of%20the%20model.%20Based%20on%0Athis%20issue%2C%20we%20propose%20an%20efficient%20pretraining%20model%20based%20on%20multi-scale%0Alocal%20visual%20field%20feature%20reconstruction%20for%20PCB%20CT%20image%20element%20segmentation%0A%28EMLR-seg%29.%20In%20this%20model%2C%20the%20teacher-guided%20MIM%20pretraining%20model%20is%0Aintroduced%20into%20PCB%20CT%20image%20element%20segmentation%20for%20the%20first%20time%2C%20and%20a%0Amulti-scale%20local%20visual%20field%20extraction%20%28MVE%29%20module%20is%20proposed%20to%20reduce%0Aredundancy%20by%20focusing%20on%20local%20visual%20fields.%20At%20the%20same%20time%2C%20a%20simple%0A4-Transformer-blocks%20decoder%20is%20used.%20Experiments%20show%20that%20EMLR-seg%20can%0Aachieve%2088.6%25%20mIoU%20on%20the%20PCB%20CT%20image%20dataset%20we%20proposed%2C%20which%20exceeds%201.2%25%0Aby%20the%20baseline%20model%2C%20and%20the%20training%20time%20is%20reduced%20by%2029.6%20hours%2C%20a%0Areduction%20of%2017.4%25%20under%20the%20same%20experimental%20condition%2C%20which%20reflects%20the%0Aadvantage%20of%20EMLR-seg%20in%20terms%20of%20performance%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Pretraining%2520Model%2520based%2520on%2520Multi-Scale%2520Local%2520Visual%2520Field%250A%2520%2520Feature%2520Reconstruction%2520for%2520PCB%2520CT%2520Image%2520Element%2520Segmentation%26entry.906535625%3DChen%2520Chen%2520and%2520Kai%2520Qiao%2520and%2520Jie%2520Yang%2520and%2520Jian%2520Chen%2520and%2520Bin%2520Yan%26entry.1292438233%3D%2520%2520Element%2520segmentation%2520is%2520a%2520key%2520step%2520in%2520nondestructive%2520testing%2520of%2520Printed%250ACircuit%2520Boards%2520%2528PCB%2529%2520based%2520on%2520Computed%2520Tomography%2520%2528CT%2529%2520technology.%2520In%2520recent%250Ayears%252C%2520the%2520rapid%2520development%2520of%2520self-supervised%2520pretraining%2520technology%2520can%250Aobtain%2520general%2520image%2520features%2520without%2520labeled%2520samples%252C%2520and%2520then%2520use%2520a%2520small%250Aamount%2520of%2520labeled%2520samples%2520to%2520solve%2520downstream%2520tasks%252C%2520which%2520has%2520a%2520good%2520potential%250Ain%2520PCB%2520element%2520segmentation.%2520At%2520present%252C%2520Masked%2520Image%2520Modeling%2520%2528MIM%2529%250Apretraining%2520model%2520has%2520been%2520initially%2520applied%2520in%2520PCB%2520CT%2520image%2520element%250Asegmentation.%2520However%252C%2520due%2520to%2520the%2520small%2520and%2520regular%2520size%2520of%2520PCB%2520elements%2520such%250Aas%2520vias%252C%2520wires%252C%2520and%2520pads%252C%2520the%2520global%2520visual%2520field%2520has%2520redundancy%2520for%2520a%2520single%250Aelement%2520reconstruction%252C%2520which%2520may%2520damage%2520the%2520performance%2520of%2520the%2520model.%2520Based%2520on%250Athis%2520issue%252C%2520we%2520propose%2520an%2520efficient%2520pretraining%2520model%2520based%2520on%2520multi-scale%250Alocal%2520visual%2520field%2520feature%2520reconstruction%2520for%2520PCB%2520CT%2520image%2520element%2520segmentation%250A%2528EMLR-seg%2529.%2520In%2520this%2520model%252C%2520the%2520teacher-guided%2520MIM%2520pretraining%2520model%2520is%250Aintroduced%2520into%2520PCB%2520CT%2520image%2520element%2520segmentation%2520for%2520the%2520first%2520time%252C%2520and%2520a%250Amulti-scale%2520local%2520visual%2520field%2520extraction%2520%2528MVE%2529%2520module%2520is%2520proposed%2520to%2520reduce%250Aredundancy%2520by%2520focusing%2520on%2520local%2520visual%2520fields.%2520At%2520the%2520same%2520time%252C%2520a%2520simple%250A4-Transformer-blocks%2520decoder%2520is%2520used.%2520Experiments%2520show%2520that%2520EMLR-seg%2520can%250Aachieve%252088.6%2525%2520mIoU%2520on%2520the%2520PCB%2520CT%2520image%2520dataset%2520we%2520proposed%252C%2520which%2520exceeds%25201.2%2525%250Aby%2520the%2520baseline%2520model%252C%2520and%2520the%2520training%2520time%2520is%2520reduced%2520by%252029.6%2520hours%252C%2520a%250Areduction%2520of%252017.4%2525%2520under%2520the%2520same%2520experimental%2520condition%252C%2520which%2520reflects%2520the%250Aadvantage%2520of%2520EMLR-seg%2520in%2520terms%2520of%2520performance%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Pretraining%20Model%20based%20on%20Multi-Scale%20Local%20Visual%20Field%0A%20%20Feature%20Reconstruction%20for%20PCB%20CT%20Image%20Element%20Segmentation&entry.906535625=Chen%20Chen%20and%20Kai%20Qiao%20and%20Jie%20Yang%20and%20Jian%20Chen%20and%20Bin%20Yan&entry.1292438233=%20%20Element%20segmentation%20is%20a%20key%20step%20in%20nondestructive%20testing%20of%20Printed%0ACircuit%20Boards%20%28PCB%29%20based%20on%20Computed%20Tomography%20%28CT%29%20technology.%20In%20recent%0Ayears%2C%20the%20rapid%20development%20of%20self-supervised%20pretraining%20technology%20can%0Aobtain%20general%20image%20features%20without%20labeled%20samples%2C%20and%20then%20use%20a%20small%0Aamount%20of%20labeled%20samples%20to%20solve%20downstream%20tasks%2C%20which%20has%20a%20good%20potential%0Ain%20PCB%20element%20segmentation.%20At%20present%2C%20Masked%20Image%20Modeling%20%28MIM%29%0Apretraining%20model%20has%20been%20initially%20applied%20in%20PCB%20CT%20image%20element%0Asegmentation.%20However%2C%20due%20to%20the%20small%20and%20regular%20size%20of%20PCB%20elements%20such%0Aas%20vias%2C%20wires%2C%20and%20pads%2C%20the%20global%20visual%20field%20has%20redundancy%20for%20a%20single%0Aelement%20reconstruction%2C%20which%20may%20damage%20the%20performance%20of%20the%20model.%20Based%20on%0Athis%20issue%2C%20we%20propose%20an%20efficient%20pretraining%20model%20based%20on%20multi-scale%0Alocal%20visual%20field%20feature%20reconstruction%20for%20PCB%20CT%20image%20element%20segmentation%0A%28EMLR-seg%29.%20In%20this%20model%2C%20the%20teacher-guided%20MIM%20pretraining%20model%20is%0Aintroduced%20into%20PCB%20CT%20image%20element%20segmentation%20for%20the%20first%20time%2C%20and%20a%0Amulti-scale%20local%20visual%20field%20extraction%20%28MVE%29%20module%20is%20proposed%20to%20reduce%0Aredundancy%20by%20focusing%20on%20local%20visual%20fields.%20At%20the%20same%20time%2C%20a%20simple%0A4-Transformer-blocks%20decoder%20is%20used.%20Experiments%20show%20that%20EMLR-seg%20can%0Aachieve%2088.6%25%20mIoU%20on%20the%20PCB%20CT%20image%20dataset%20we%20proposed%2C%20which%20exceeds%201.2%25%0Aby%20the%20baseline%20model%2C%20and%20the%20training%20time%20is%20reduced%20by%2029.6%20hours%2C%20a%0Areduction%20of%2017.4%25%20under%20the%20same%20experimental%20condition%2C%20which%20reflects%20the%0Aadvantage%20of%20EMLR-seg%20in%20terms%20of%20performance%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05745v1&entry.124074799=Read"},
{"title": "Passive Obstacle Aware Control to Follow Desired Velocities", "author": "Lukas Huber and Trinca Thibaud and Jean-Jacques Slotine and Aude Billard", "abstract": "  Evaluating and updating the obstacle avoidance velocity for an autonomous\nrobot in real-time ensures robustness against noise and disturbances. A passive\ndamping controller can obtain the desired motion with a torque-controlled\nrobot, which remains compliant and ensures a safe response to external\nperturbations. Here, we propose a novel approach for designing the passive\ncontrol policy. Our algorithm complies with obstacle-free zones while\ntransitioning to increased damping near obstacles to ensure collision\navoidance. This approach ensures stability across diverse scenarios,\neffectively mitigating disturbances. Validation on a 7DoF robot arm\ndemonstrates superior collision rejection capabilities compared to the\nbaseline, underlining its practicality for real-world applications. Our\nobstacle-aware damping controller represents a substantial advancement in\nsecure robot control within complex and uncertain environments.\n", "link": "http://arxiv.org/abs/2405.05669v1", "date": "2024-05-09", "relevancy": 2.0923, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5422}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Passive%20Obstacle%20Aware%20Control%20to%20Follow%20Desired%20Velocities&body=Title%3A%20Passive%20Obstacle%20Aware%20Control%20to%20Follow%20Desired%20Velocities%0AAuthor%3A%20Lukas%20Huber%20and%20Trinca%20Thibaud%20and%20Jean-Jacques%20Slotine%20and%20Aude%20Billard%0AAbstract%3A%20%20%20Evaluating%20and%20updating%20the%20obstacle%20avoidance%20velocity%20for%20an%20autonomous%0Arobot%20in%20real-time%20ensures%20robustness%20against%20noise%20and%20disturbances.%20A%20passive%0Adamping%20controller%20can%20obtain%20the%20desired%20motion%20with%20a%20torque-controlled%0Arobot%2C%20which%20remains%20compliant%20and%20ensures%20a%20safe%20response%20to%20external%0Aperturbations.%20Here%2C%20we%20propose%20a%20novel%20approach%20for%20designing%20the%20passive%0Acontrol%20policy.%20Our%20algorithm%20complies%20with%20obstacle-free%20zones%20while%0Atransitioning%20to%20increased%20damping%20near%20obstacles%20to%20ensure%20collision%0Aavoidance.%20This%20approach%20ensures%20stability%20across%20diverse%20scenarios%2C%0Aeffectively%20mitigating%20disturbances.%20Validation%20on%20a%207DoF%20robot%20arm%0Ademonstrates%20superior%20collision%20rejection%20capabilities%20compared%20to%20the%0Abaseline%2C%20underlining%20its%20practicality%20for%20real-world%20applications.%20Our%0Aobstacle-aware%20damping%20controller%20represents%20a%20substantial%20advancement%20in%0Asecure%20robot%20control%20within%20complex%20and%20uncertain%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPassive%2520Obstacle%2520Aware%2520Control%2520to%2520Follow%2520Desired%2520Velocities%26entry.906535625%3DLukas%2520Huber%2520and%2520Trinca%2520Thibaud%2520and%2520Jean-Jacques%2520Slotine%2520and%2520Aude%2520Billard%26entry.1292438233%3D%2520%2520Evaluating%2520and%2520updating%2520the%2520obstacle%2520avoidance%2520velocity%2520for%2520an%2520autonomous%250Arobot%2520in%2520real-time%2520ensures%2520robustness%2520against%2520noise%2520and%2520disturbances.%2520A%2520passive%250Adamping%2520controller%2520can%2520obtain%2520the%2520desired%2520motion%2520with%2520a%2520torque-controlled%250Arobot%252C%2520which%2520remains%2520compliant%2520and%2520ensures%2520a%2520safe%2520response%2520to%2520external%250Aperturbations.%2520Here%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520designing%2520the%2520passive%250Acontrol%2520policy.%2520Our%2520algorithm%2520complies%2520with%2520obstacle-free%2520zones%2520while%250Atransitioning%2520to%2520increased%2520damping%2520near%2520obstacles%2520to%2520ensure%2520collision%250Aavoidance.%2520This%2520approach%2520ensures%2520stability%2520across%2520diverse%2520scenarios%252C%250Aeffectively%2520mitigating%2520disturbances.%2520Validation%2520on%2520a%25207DoF%2520robot%2520arm%250Ademonstrates%2520superior%2520collision%2520rejection%2520capabilities%2520compared%2520to%2520the%250Abaseline%252C%2520underlining%2520its%2520practicality%2520for%2520real-world%2520applications.%2520Our%250Aobstacle-aware%2520damping%2520controller%2520represents%2520a%2520substantial%2520advancement%2520in%250Asecure%2520robot%2520control%2520within%2520complex%2520and%2520uncertain%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Passive%20Obstacle%20Aware%20Control%20to%20Follow%20Desired%20Velocities&entry.906535625=Lukas%20Huber%20and%20Trinca%20Thibaud%20and%20Jean-Jacques%20Slotine%20and%20Aude%20Billard&entry.1292438233=%20%20Evaluating%20and%20updating%20the%20obstacle%20avoidance%20velocity%20for%20an%20autonomous%0Arobot%20in%20real-time%20ensures%20robustness%20against%20noise%20and%20disturbances.%20A%20passive%0Adamping%20controller%20can%20obtain%20the%20desired%20motion%20with%20a%20torque-controlled%0Arobot%2C%20which%20remains%20compliant%20and%20ensures%20a%20safe%20response%20to%20external%0Aperturbations.%20Here%2C%20we%20propose%20a%20novel%20approach%20for%20designing%20the%20passive%0Acontrol%20policy.%20Our%20algorithm%20complies%20with%20obstacle-free%20zones%20while%0Atransitioning%20to%20increased%20damping%20near%20obstacles%20to%20ensure%20collision%0Aavoidance.%20This%20approach%20ensures%20stability%20across%20diverse%20scenarios%2C%0Aeffectively%20mitigating%20disturbances.%20Validation%20on%20a%207DoF%20robot%20arm%0Ademonstrates%20superior%20collision%20rejection%20capabilities%20compared%20to%20the%0Abaseline%2C%20underlining%20its%20practicality%20for%20real-world%20applications.%20Our%0Aobstacle-aware%20damping%20controller%20represents%20a%20substantial%20advancement%20in%0Asecure%20robot%20control%20within%20complex%20and%20uncertain%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05669v1&entry.124074799=Read"},
{"title": "Mask-TS Net: Mask Temperature Scaling Uncertainty Calibration for Polyp\n  Segmentation", "author": "Yudian Zhang and Chenhao Xu and Kaiye Xu and Haijiang Zhu", "abstract": "  Lots of popular calibration methods in medical images focus on\nclassification, but there are few comparable studies on semantic segmentation.\nIn polyp segmentation of medical images, we find most diseased area occupies\nonly a small portion of the entire image, resulting in previous models being\nnot well-calibrated for lesion regions but well-calibrated for background,\ndespite their seemingly better Expected Calibration Error (ECE) scores overall.\nTherefore, we proposed four-branches calibration network with Mask-Loss and\nMask-TS strategies to more focus on the scaling of logits within potential\nlesion regions, which serves to mitigate the influence of background\ninterference. In the experiments, we compare the existing calibration methods\nwith the proposed Mask Temperature Scaling (Mask-TS). The results indicate that\nthe proposed calibration network outperforms other methods both qualitatively\nand quantitatively.\n", "link": "http://arxiv.org/abs/2405.05830v1", "date": "2024-05-09", "relevancy": 2.0889, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5333}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5313}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask-TS%20Net%3A%20Mask%20Temperature%20Scaling%20Uncertainty%20Calibration%20for%20Polyp%0A%20%20Segmentation&body=Title%3A%20Mask-TS%20Net%3A%20Mask%20Temperature%20Scaling%20Uncertainty%20Calibration%20for%20Polyp%0A%20%20Segmentation%0AAuthor%3A%20Yudian%20Zhang%20and%20Chenhao%20Xu%20and%20Kaiye%20Xu%20and%20Haijiang%20Zhu%0AAbstract%3A%20%20%20Lots%20of%20popular%20calibration%20methods%20in%20medical%20images%20focus%20on%0Aclassification%2C%20but%20there%20are%20few%20comparable%20studies%20on%20semantic%20segmentation.%0AIn%20polyp%20segmentation%20of%20medical%20images%2C%20we%20find%20most%20diseased%20area%20occupies%0Aonly%20a%20small%20portion%20of%20the%20entire%20image%2C%20resulting%20in%20previous%20models%20being%0Anot%20well-calibrated%20for%20lesion%20regions%20but%20well-calibrated%20for%20background%2C%0Adespite%20their%20seemingly%20better%20Expected%20Calibration%20Error%20%28ECE%29%20scores%20overall.%0ATherefore%2C%20we%20proposed%20four-branches%20calibration%20network%20with%20Mask-Loss%20and%0AMask-TS%20strategies%20to%20more%20focus%20on%20the%20scaling%20of%20logits%20within%20potential%0Alesion%20regions%2C%20which%20serves%20to%20mitigate%20the%20influence%20of%20background%0Ainterference.%20In%20the%20experiments%2C%20we%20compare%20the%20existing%20calibration%20methods%0Awith%20the%20proposed%20Mask%20Temperature%20Scaling%20%28Mask-TS%29.%20The%20results%20indicate%20that%0Athe%20proposed%20calibration%20network%20outperforms%20other%20methods%20both%20qualitatively%0Aand%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask-TS%2520Net%253A%2520Mask%2520Temperature%2520Scaling%2520Uncertainty%2520Calibration%2520for%2520Polyp%250A%2520%2520Segmentation%26entry.906535625%3DYudian%2520Zhang%2520and%2520Chenhao%2520Xu%2520and%2520Kaiye%2520Xu%2520and%2520Haijiang%2520Zhu%26entry.1292438233%3D%2520%2520Lots%2520of%2520popular%2520calibration%2520methods%2520in%2520medical%2520images%2520focus%2520on%250Aclassification%252C%2520but%2520there%2520are%2520few%2520comparable%2520studies%2520on%2520semantic%2520segmentation.%250AIn%2520polyp%2520segmentation%2520of%2520medical%2520images%252C%2520we%2520find%2520most%2520diseased%2520area%2520occupies%250Aonly%2520a%2520small%2520portion%2520of%2520the%2520entire%2520image%252C%2520resulting%2520in%2520previous%2520models%2520being%250Anot%2520well-calibrated%2520for%2520lesion%2520regions%2520but%2520well-calibrated%2520for%2520background%252C%250Adespite%2520their%2520seemingly%2520better%2520Expected%2520Calibration%2520Error%2520%2528ECE%2529%2520scores%2520overall.%250ATherefore%252C%2520we%2520proposed%2520four-branches%2520calibration%2520network%2520with%2520Mask-Loss%2520and%250AMask-TS%2520strategies%2520to%2520more%2520focus%2520on%2520the%2520scaling%2520of%2520logits%2520within%2520potential%250Alesion%2520regions%252C%2520which%2520serves%2520to%2520mitigate%2520the%2520influence%2520of%2520background%250Ainterference.%2520In%2520the%2520experiments%252C%2520we%2520compare%2520the%2520existing%2520calibration%2520methods%250Awith%2520the%2520proposed%2520Mask%2520Temperature%2520Scaling%2520%2528Mask-TS%2529.%2520The%2520results%2520indicate%2520that%250Athe%2520proposed%2520calibration%2520network%2520outperforms%2520other%2520methods%2520both%2520qualitatively%250Aand%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask-TS%20Net%3A%20Mask%20Temperature%20Scaling%20Uncertainty%20Calibration%20for%20Polyp%0A%20%20Segmentation&entry.906535625=Yudian%20Zhang%20and%20Chenhao%20Xu%20and%20Kaiye%20Xu%20and%20Haijiang%20Zhu&entry.1292438233=%20%20Lots%20of%20popular%20calibration%20methods%20in%20medical%20images%20focus%20on%0Aclassification%2C%20but%20there%20are%20few%20comparable%20studies%20on%20semantic%20segmentation.%0AIn%20polyp%20segmentation%20of%20medical%20images%2C%20we%20find%20most%20diseased%20area%20occupies%0Aonly%20a%20small%20portion%20of%20the%20entire%20image%2C%20resulting%20in%20previous%20models%20being%0Anot%20well-calibrated%20for%20lesion%20regions%20but%20well-calibrated%20for%20background%2C%0Adespite%20their%20seemingly%20better%20Expected%20Calibration%20Error%20%28ECE%29%20scores%20overall.%0ATherefore%2C%20we%20proposed%20four-branches%20calibration%20network%20with%20Mask-Loss%20and%0AMask-TS%20strategies%20to%20more%20focus%20on%20the%20scaling%20of%20logits%20within%20potential%0Alesion%20regions%2C%20which%20serves%20to%20mitigate%20the%20influence%20of%20background%0Ainterference.%20In%20the%20experiments%2C%20we%20compare%20the%20existing%20calibration%20methods%0Awith%20the%20proposed%20Mask%20Temperature%20Scaling%20%28Mask-TS%29.%20The%20results%20indicate%20that%0Athe%20proposed%20calibration%20network%20outperforms%20other%20methods%20both%20qualitatively%0Aand%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05830v1&entry.124074799=Read"},
{"title": "Multi-Stream Keypoint Attention Network for Sign Language Recognition\n  and Translation", "author": "Mo Guan and Yan Wang and Guangkun Ma and Jiarui Liu and Mingzu Sun", "abstract": "  Sign language serves as a non-vocal means of communication, transmitting\ninformation and significance through gestures, facial expressions, and bodily\nmovements. The majority of current approaches for sign language recognition\n(SLR) and translation rely on RGB video inputs, which are vulnerable to\nfluctuations in the background. Employing a keypoint-based strategy not only\nmitigates the effects of background alterations but also substantially\ndiminishes the computational demands of the model. Nevertheless, contemporary\nkeypoint-based methodologies fail to fully harness the implicit knowledge\nembedded in keypoint sequences. To tackle this challenge, our inspiration is\nderived from the human cognition mechanism, which discerns sign language by\nanalyzing the interplay between gesture configurations and supplementary\nelements. We propose a multi-stream keypoint attention network to depict a\nsequence of keypoints produced by a readily available keypoint estimator. In\norder to facilitate interaction across multiple streams, we investigate diverse\nmethodologies such as keypoint fusion strategies, head fusion, and\nself-distillation. The resulting framework is denoted as MSKA-SLR, which is\nexpanded into a sign language translation (SLT) model through the\nstraightforward addition of an extra translation network. We carry out\ncomprehensive experiments on well-known benchmarks like Phoenix-2014,\nPhoenix-2014T, and CSL-Daily to showcase the efficacy of our methodology.\nNotably, we have attained a novel state-of-the-art performance in the sign\nlanguage translation task of Phoenix-2014T. The code and models can be accessed\nat: https://github.com/sutwangyan/MSKA.\n", "link": "http://arxiv.org/abs/2405.05672v1", "date": "2024-05-09", "relevancy": 2.0884, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5283}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5255}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Stream%20Keypoint%20Attention%20Network%20for%20Sign%20Language%20Recognition%0A%20%20and%20Translation&body=Title%3A%20Multi-Stream%20Keypoint%20Attention%20Network%20for%20Sign%20Language%20Recognition%0A%20%20and%20Translation%0AAuthor%3A%20Mo%20Guan%20and%20Yan%20Wang%20and%20Guangkun%20Ma%20and%20Jiarui%20Liu%20and%20Mingzu%20Sun%0AAbstract%3A%20%20%20Sign%20language%20serves%20as%20a%20non-vocal%20means%20of%20communication%2C%20transmitting%0Ainformation%20and%20significance%20through%20gestures%2C%20facial%20expressions%2C%20and%20bodily%0Amovements.%20The%20majority%20of%20current%20approaches%20for%20sign%20language%20recognition%0A%28SLR%29%20and%20translation%20rely%20on%20RGB%20video%20inputs%2C%20which%20are%20vulnerable%20to%0Afluctuations%20in%20the%20background.%20Employing%20a%20keypoint-based%20strategy%20not%20only%0Amitigates%20the%20effects%20of%20background%20alterations%20but%20also%20substantially%0Adiminishes%20the%20computational%20demands%20of%20the%20model.%20Nevertheless%2C%20contemporary%0Akeypoint-based%20methodologies%20fail%20to%20fully%20harness%20the%20implicit%20knowledge%0Aembedded%20in%20keypoint%20sequences.%20To%20tackle%20this%20challenge%2C%20our%20inspiration%20is%0Aderived%20from%20the%20human%20cognition%20mechanism%2C%20which%20discerns%20sign%20language%20by%0Aanalyzing%20the%20interplay%20between%20gesture%20configurations%20and%20supplementary%0Aelements.%20We%20propose%20a%20multi-stream%20keypoint%20attention%20network%20to%20depict%20a%0Asequence%20of%20keypoints%20produced%20by%20a%20readily%20available%20keypoint%20estimator.%20In%0Aorder%20to%20facilitate%20interaction%20across%20multiple%20streams%2C%20we%20investigate%20diverse%0Amethodologies%20such%20as%20keypoint%20fusion%20strategies%2C%20head%20fusion%2C%20and%0Aself-distillation.%20The%20resulting%20framework%20is%20denoted%20as%20MSKA-SLR%2C%20which%20is%0Aexpanded%20into%20a%20sign%20language%20translation%20%28SLT%29%20model%20through%20the%0Astraightforward%20addition%20of%20an%20extra%20translation%20network.%20We%20carry%20out%0Acomprehensive%20experiments%20on%20well-known%20benchmarks%20like%20Phoenix-2014%2C%0APhoenix-2014T%2C%20and%20CSL-Daily%20to%20showcase%20the%20efficacy%20of%20our%20methodology.%0ANotably%2C%20we%20have%20attained%20a%20novel%20state-of-the-art%20performance%20in%20the%20sign%0Alanguage%20translation%20task%20of%20Phoenix-2014T.%20The%20code%20and%20models%20can%20be%20accessed%0Aat%3A%20https%3A//github.com/sutwangyan/MSKA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Stream%2520Keypoint%2520Attention%2520Network%2520for%2520Sign%2520Language%2520Recognition%250A%2520%2520and%2520Translation%26entry.906535625%3DMo%2520Guan%2520and%2520Yan%2520Wang%2520and%2520Guangkun%2520Ma%2520and%2520Jiarui%2520Liu%2520and%2520Mingzu%2520Sun%26entry.1292438233%3D%2520%2520Sign%2520language%2520serves%2520as%2520a%2520non-vocal%2520means%2520of%2520communication%252C%2520transmitting%250Ainformation%2520and%2520significance%2520through%2520gestures%252C%2520facial%2520expressions%252C%2520and%2520bodily%250Amovements.%2520The%2520majority%2520of%2520current%2520approaches%2520for%2520sign%2520language%2520recognition%250A%2528SLR%2529%2520and%2520translation%2520rely%2520on%2520RGB%2520video%2520inputs%252C%2520which%2520are%2520vulnerable%2520to%250Afluctuations%2520in%2520the%2520background.%2520Employing%2520a%2520keypoint-based%2520strategy%2520not%2520only%250Amitigates%2520the%2520effects%2520of%2520background%2520alterations%2520but%2520also%2520substantially%250Adiminishes%2520the%2520computational%2520demands%2520of%2520the%2520model.%2520Nevertheless%252C%2520contemporary%250Akeypoint-based%2520methodologies%2520fail%2520to%2520fully%2520harness%2520the%2520implicit%2520knowledge%250Aembedded%2520in%2520keypoint%2520sequences.%2520To%2520tackle%2520this%2520challenge%252C%2520our%2520inspiration%2520is%250Aderived%2520from%2520the%2520human%2520cognition%2520mechanism%252C%2520which%2520discerns%2520sign%2520language%2520by%250Aanalyzing%2520the%2520interplay%2520between%2520gesture%2520configurations%2520and%2520supplementary%250Aelements.%2520We%2520propose%2520a%2520multi-stream%2520keypoint%2520attention%2520network%2520to%2520depict%2520a%250Asequence%2520of%2520keypoints%2520produced%2520by%2520a%2520readily%2520available%2520keypoint%2520estimator.%2520In%250Aorder%2520to%2520facilitate%2520interaction%2520across%2520multiple%2520streams%252C%2520we%2520investigate%2520diverse%250Amethodologies%2520such%2520as%2520keypoint%2520fusion%2520strategies%252C%2520head%2520fusion%252C%2520and%250Aself-distillation.%2520The%2520resulting%2520framework%2520is%2520denoted%2520as%2520MSKA-SLR%252C%2520which%2520is%250Aexpanded%2520into%2520a%2520sign%2520language%2520translation%2520%2528SLT%2529%2520model%2520through%2520the%250Astraightforward%2520addition%2520of%2520an%2520extra%2520translation%2520network.%2520We%2520carry%2520out%250Acomprehensive%2520experiments%2520on%2520well-known%2520benchmarks%2520like%2520Phoenix-2014%252C%250APhoenix-2014T%252C%2520and%2520CSL-Daily%2520to%2520showcase%2520the%2520efficacy%2520of%2520our%2520methodology.%250ANotably%252C%2520we%2520have%2520attained%2520a%2520novel%2520state-of-the-art%2520performance%2520in%2520the%2520sign%250Alanguage%2520translation%2520task%2520of%2520Phoenix-2014T.%2520The%2520code%2520and%2520models%2520can%2520be%2520accessed%250Aat%253A%2520https%253A//github.com/sutwangyan/MSKA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Stream%20Keypoint%20Attention%20Network%20for%20Sign%20Language%20Recognition%0A%20%20and%20Translation&entry.906535625=Mo%20Guan%20and%20Yan%20Wang%20and%20Guangkun%20Ma%20and%20Jiarui%20Liu%20and%20Mingzu%20Sun&entry.1292438233=%20%20Sign%20language%20serves%20as%20a%20non-vocal%20means%20of%20communication%2C%20transmitting%0Ainformation%20and%20significance%20through%20gestures%2C%20facial%20expressions%2C%20and%20bodily%0Amovements.%20The%20majority%20of%20current%20approaches%20for%20sign%20language%20recognition%0A%28SLR%29%20and%20translation%20rely%20on%20RGB%20video%20inputs%2C%20which%20are%20vulnerable%20to%0Afluctuations%20in%20the%20background.%20Employing%20a%20keypoint-based%20strategy%20not%20only%0Amitigates%20the%20effects%20of%20background%20alterations%20but%20also%20substantially%0Adiminishes%20the%20computational%20demands%20of%20the%20model.%20Nevertheless%2C%20contemporary%0Akeypoint-based%20methodologies%20fail%20to%20fully%20harness%20the%20implicit%20knowledge%0Aembedded%20in%20keypoint%20sequences.%20To%20tackle%20this%20challenge%2C%20our%20inspiration%20is%0Aderived%20from%20the%20human%20cognition%20mechanism%2C%20which%20discerns%20sign%20language%20by%0Aanalyzing%20the%20interplay%20between%20gesture%20configurations%20and%20supplementary%0Aelements.%20We%20propose%20a%20multi-stream%20keypoint%20attention%20network%20to%20depict%20a%0Asequence%20of%20keypoints%20produced%20by%20a%20readily%20available%20keypoint%20estimator.%20In%0Aorder%20to%20facilitate%20interaction%20across%20multiple%20streams%2C%20we%20investigate%20diverse%0Amethodologies%20such%20as%20keypoint%20fusion%20strategies%2C%20head%20fusion%2C%20and%0Aself-distillation.%20The%20resulting%20framework%20is%20denoted%20as%20MSKA-SLR%2C%20which%20is%0Aexpanded%20into%20a%20sign%20language%20translation%20%28SLT%29%20model%20through%20the%0Astraightforward%20addition%20of%20an%20extra%20translation%20network.%20We%20carry%20out%0Acomprehensive%20experiments%20on%20well-known%20benchmarks%20like%20Phoenix-2014%2C%0APhoenix-2014T%2C%20and%20CSL-Daily%20to%20showcase%20the%20efficacy%20of%20our%20methodology.%0ANotably%2C%20we%20have%20attained%20a%20novel%20state-of-the-art%20performance%20in%20the%20sign%0Alanguage%20translation%20task%20of%20Phoenix-2014T.%20The%20code%20and%20models%20can%20be%20accessed%0Aat%3A%20https%3A//github.com/sutwangyan/MSKA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05672v1&entry.124074799=Read"},
{"title": "A Framework of SO(3)-equivariant Non-linear Representation Learning and\n  its Application to Electronic-Structure Hamiltonian Prediction", "author": "Shi Yin and Xinyang Pan and Fengyan Wang and Feng Wu and Lixin He", "abstract": "  We present both a theoretical and a methodological framework that addresses a\ncritical challenge in applying deep learning to physical systems: the\nreconciliation of non-linear expressiveness with SO(3)-equivariance in\npredictions of SO(3)-equivariant quantities, such as the electronic-structure\nHamiltonian. Inspired by covariant theory in physics, we address this problem\nby exploring the mathematical relationships between SO(3)-invariant and\nSO(3)-equivariant quantities and their representations. We first construct\ntheoretical SO(3)-invariant quantities derived from the SO(3)-equivariant\nregression targets, and use these invariant quantities as supervisory labels to\nguide the learning of high-quality SO(3)-invariant features. Given that\nSO(3)-invariance is preserved under non-linear operations, the encoding process\nfor invariant features can extensively utilize non-linear mappings, thereby\nfully capturing the non-linear patterns inherent in physical systems. Building\non this foundation, we propose a gradient-based mechanism to induce\nSO(3)-equivariant encodings of various degrees from the learned SO(3)-invariant\nfeatures. This mechanism can incorporate non-linear expressive capabilities\ninto SO(3)-equivariant representations, while theoretically preserving their\nequivariant properties as we prove. Our approach offers a promising general\nsolution to the critical dilemma between equivariance and non-linear\nexpressiveness in deep learning methodologies. We apply our theory and method\nto the electronic-structure Hamiltonian prediction tasks, demonstrating\nstate-of-the-art performance across six benchmark databases.\n", "link": "http://arxiv.org/abs/2405.05722v1", "date": "2024-05-09", "relevancy": 2.086, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5275}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5217}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20of%20SO%283%29-equivariant%20Non-linear%20Representation%20Learning%20and%0A%20%20its%20Application%20to%20Electronic-Structure%20Hamiltonian%20Prediction&body=Title%3A%20A%20Framework%20of%20SO%283%29-equivariant%20Non-linear%20Representation%20Learning%20and%0A%20%20its%20Application%20to%20Electronic-Structure%20Hamiltonian%20Prediction%0AAuthor%3A%20Shi%20Yin%20and%20Xinyang%20Pan%20and%20Fengyan%20Wang%20and%20Feng%20Wu%20and%20Lixin%20He%0AAbstract%3A%20%20%20We%20present%20both%20a%20theoretical%20and%20a%20methodological%20framework%20that%20addresses%20a%0Acritical%20challenge%20in%20applying%20deep%20learning%20to%20physical%20systems%3A%20the%0Areconciliation%20of%20non-linear%20expressiveness%20with%20SO%283%29-equivariance%20in%0Apredictions%20of%20SO%283%29-equivariant%20quantities%2C%20such%20as%20the%20electronic-structure%0AHamiltonian.%20Inspired%20by%20covariant%20theory%20in%20physics%2C%20we%20address%20this%20problem%0Aby%20exploring%20the%20mathematical%20relationships%20between%20SO%283%29-invariant%20and%0ASO%283%29-equivariant%20quantities%20and%20their%20representations.%20We%20first%20construct%0Atheoretical%20SO%283%29-invariant%20quantities%20derived%20from%20the%20SO%283%29-equivariant%0Aregression%20targets%2C%20and%20use%20these%20invariant%20quantities%20as%20supervisory%20labels%20to%0Aguide%20the%20learning%20of%20high-quality%20SO%283%29-invariant%20features.%20Given%20that%0ASO%283%29-invariance%20is%20preserved%20under%20non-linear%20operations%2C%20the%20encoding%20process%0Afor%20invariant%20features%20can%20extensively%20utilize%20non-linear%20mappings%2C%20thereby%0Afully%20capturing%20the%20non-linear%20patterns%20inherent%20in%20physical%20systems.%20Building%0Aon%20this%20foundation%2C%20we%20propose%20a%20gradient-based%20mechanism%20to%20induce%0ASO%283%29-equivariant%20encodings%20of%20various%20degrees%20from%20the%20learned%20SO%283%29-invariant%0Afeatures.%20This%20mechanism%20can%20incorporate%20non-linear%20expressive%20capabilities%0Ainto%20SO%283%29-equivariant%20representations%2C%20while%20theoretically%20preserving%20their%0Aequivariant%20properties%20as%20we%20prove.%20Our%20approach%20offers%20a%20promising%20general%0Asolution%20to%20the%20critical%20dilemma%20between%20equivariance%20and%20non-linear%0Aexpressiveness%20in%20deep%20learning%20methodologies.%20We%20apply%20our%20theory%20and%20method%0Ato%20the%20electronic-structure%20Hamiltonian%20prediction%20tasks%2C%20demonstrating%0Astate-of-the-art%20performance%20across%20six%20benchmark%20databases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520of%2520SO%25283%2529-equivariant%2520Non-linear%2520Representation%2520Learning%2520and%250A%2520%2520its%2520Application%2520to%2520Electronic-Structure%2520Hamiltonian%2520Prediction%26entry.906535625%3DShi%2520Yin%2520and%2520Xinyang%2520Pan%2520and%2520Fengyan%2520Wang%2520and%2520Feng%2520Wu%2520and%2520Lixin%2520He%26entry.1292438233%3D%2520%2520We%2520present%2520both%2520a%2520theoretical%2520and%2520a%2520methodological%2520framework%2520that%2520addresses%2520a%250Acritical%2520challenge%2520in%2520applying%2520deep%2520learning%2520to%2520physical%2520systems%253A%2520the%250Areconciliation%2520of%2520non-linear%2520expressiveness%2520with%2520SO%25283%2529-equivariance%2520in%250Apredictions%2520of%2520SO%25283%2529-equivariant%2520quantities%252C%2520such%2520as%2520the%2520electronic-structure%250AHamiltonian.%2520Inspired%2520by%2520covariant%2520theory%2520in%2520physics%252C%2520we%2520address%2520this%2520problem%250Aby%2520exploring%2520the%2520mathematical%2520relationships%2520between%2520SO%25283%2529-invariant%2520and%250ASO%25283%2529-equivariant%2520quantities%2520and%2520their%2520representations.%2520We%2520first%2520construct%250Atheoretical%2520SO%25283%2529-invariant%2520quantities%2520derived%2520from%2520the%2520SO%25283%2529-equivariant%250Aregression%2520targets%252C%2520and%2520use%2520these%2520invariant%2520quantities%2520as%2520supervisory%2520labels%2520to%250Aguide%2520the%2520learning%2520of%2520high-quality%2520SO%25283%2529-invariant%2520features.%2520Given%2520that%250ASO%25283%2529-invariance%2520is%2520preserved%2520under%2520non-linear%2520operations%252C%2520the%2520encoding%2520process%250Afor%2520invariant%2520features%2520can%2520extensively%2520utilize%2520non-linear%2520mappings%252C%2520thereby%250Afully%2520capturing%2520the%2520non-linear%2520patterns%2520inherent%2520in%2520physical%2520systems.%2520Building%250Aon%2520this%2520foundation%252C%2520we%2520propose%2520a%2520gradient-based%2520mechanism%2520to%2520induce%250ASO%25283%2529-equivariant%2520encodings%2520of%2520various%2520degrees%2520from%2520the%2520learned%2520SO%25283%2529-invariant%250Afeatures.%2520This%2520mechanism%2520can%2520incorporate%2520non-linear%2520expressive%2520capabilities%250Ainto%2520SO%25283%2529-equivariant%2520representations%252C%2520while%2520theoretically%2520preserving%2520their%250Aequivariant%2520properties%2520as%2520we%2520prove.%2520Our%2520approach%2520offers%2520a%2520promising%2520general%250Asolution%2520to%2520the%2520critical%2520dilemma%2520between%2520equivariance%2520and%2520non-linear%250Aexpressiveness%2520in%2520deep%2520learning%2520methodologies.%2520We%2520apply%2520our%2520theory%2520and%2520method%250Ato%2520the%2520electronic-structure%2520Hamiltonian%2520prediction%2520tasks%252C%2520demonstrating%250Astate-of-the-art%2520performance%2520across%2520six%2520benchmark%2520databases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20of%20SO%283%29-equivariant%20Non-linear%20Representation%20Learning%20and%0A%20%20its%20Application%20to%20Electronic-Structure%20Hamiltonian%20Prediction&entry.906535625=Shi%20Yin%20and%20Xinyang%20Pan%20and%20Fengyan%20Wang%20and%20Feng%20Wu%20and%20Lixin%20He&entry.1292438233=%20%20We%20present%20both%20a%20theoretical%20and%20a%20methodological%20framework%20that%20addresses%20a%0Acritical%20challenge%20in%20applying%20deep%20learning%20to%20physical%20systems%3A%20the%0Areconciliation%20of%20non-linear%20expressiveness%20with%20SO%283%29-equivariance%20in%0Apredictions%20of%20SO%283%29-equivariant%20quantities%2C%20such%20as%20the%20electronic-structure%0AHamiltonian.%20Inspired%20by%20covariant%20theory%20in%20physics%2C%20we%20address%20this%20problem%0Aby%20exploring%20the%20mathematical%20relationships%20between%20SO%283%29-invariant%20and%0ASO%283%29-equivariant%20quantities%20and%20their%20representations.%20We%20first%20construct%0Atheoretical%20SO%283%29-invariant%20quantities%20derived%20from%20the%20SO%283%29-equivariant%0Aregression%20targets%2C%20and%20use%20these%20invariant%20quantities%20as%20supervisory%20labels%20to%0Aguide%20the%20learning%20of%20high-quality%20SO%283%29-invariant%20features.%20Given%20that%0ASO%283%29-invariance%20is%20preserved%20under%20non-linear%20operations%2C%20the%20encoding%20process%0Afor%20invariant%20features%20can%20extensively%20utilize%20non-linear%20mappings%2C%20thereby%0Afully%20capturing%20the%20non-linear%20patterns%20inherent%20in%20physical%20systems.%20Building%0Aon%20this%20foundation%2C%20we%20propose%20a%20gradient-based%20mechanism%20to%20induce%0ASO%283%29-equivariant%20encodings%20of%20various%20degrees%20from%20the%20learned%20SO%283%29-invariant%0Afeatures.%20This%20mechanism%20can%20incorporate%20non-linear%20expressive%20capabilities%0Ainto%20SO%283%29-equivariant%20representations%2C%20while%20theoretically%20preserving%20their%0Aequivariant%20properties%20as%20we%20prove.%20Our%20approach%20offers%20a%20promising%20general%0Asolution%20to%20the%20critical%20dilemma%20between%20equivariance%20and%20non-linear%0Aexpressiveness%20in%20deep%20learning%20methodologies.%20We%20apply%20our%20theory%20and%20method%0Ato%20the%20electronic-structure%20Hamiltonian%20prediction%20tasks%2C%20demonstrating%0Astate-of-the-art%20performance%20across%20six%20benchmark%20databases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05722v1&entry.124074799=Read"},
{"title": "ExACT: An End-to-End Autonomous Excavator System Using Action Chunking\n  With Transformers", "author": "Liangliang Chen and Shiyu Jin and Haoyu Wang and Liangjun Zhang", "abstract": "  Excavators are crucial for diverse tasks such as construction and mining,\nwhile autonomous excavator systems enhance safety and efficiency, address labor\nshortages, and improve human working conditions. Different from the existing\nmodularized approaches, this paper introduces ExACT, an end-to-end autonomous\nexcavator system that processes raw LiDAR, camera data, and joint positions to\ncontrol excavator valves directly. Utilizing the Action Chunking with\nTransformers (ACT) architecture, ExACT employs imitation learning to take\nobservations from multi-modal sensors as inputs and generate actionable\nsequences. In our experiment, we build a simulator based on the captured\nreal-world data to model the relations between excavator valve states and joint\nvelocities. With a few human-operated demonstration data trajectories, ExACT\ndemonstrates the capability of completing different excavation tasks, including\nreaching, digging and dumping through imitation learning in validations with\nthe simulator. To the best of our knowledge, ExACT represents the first\ninstance towards building an end-to-end autonomous excavator system via\nimitation learning methods with a minimal set of human demonstrations. The\nvideo about this work can be accessed at https://youtu.be/NmzR_Rf-aEk.\n", "link": "http://arxiv.org/abs/2405.05861v1", "date": "2024-05-09", "relevancy": 2.0838, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6347}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5001}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExACT%3A%20An%20End-to-End%20Autonomous%20Excavator%20System%20Using%20Action%20Chunking%0A%20%20With%20Transformers&body=Title%3A%20ExACT%3A%20An%20End-to-End%20Autonomous%20Excavator%20System%20Using%20Action%20Chunking%0A%20%20With%20Transformers%0AAuthor%3A%20Liangliang%20Chen%20and%20Shiyu%20Jin%20and%20Haoyu%20Wang%20and%20Liangjun%20Zhang%0AAbstract%3A%20%20%20Excavators%20are%20crucial%20for%20diverse%20tasks%20such%20as%20construction%20and%20mining%2C%0Awhile%20autonomous%20excavator%20systems%20enhance%20safety%20and%20efficiency%2C%20address%20labor%0Ashortages%2C%20and%20improve%20human%20working%20conditions.%20Different%20from%20the%20existing%0Amodularized%20approaches%2C%20this%20paper%20introduces%20ExACT%2C%20an%20end-to-end%20autonomous%0Aexcavator%20system%20that%20processes%20raw%20LiDAR%2C%20camera%20data%2C%20and%20joint%20positions%20to%0Acontrol%20excavator%20valves%20directly.%20Utilizing%20the%20Action%20Chunking%20with%0ATransformers%20%28ACT%29%20architecture%2C%20ExACT%20employs%20imitation%20learning%20to%20take%0Aobservations%20from%20multi-modal%20sensors%20as%20inputs%20and%20generate%20actionable%0Asequences.%20In%20our%20experiment%2C%20we%20build%20a%20simulator%20based%20on%20the%20captured%0Areal-world%20data%20to%20model%20the%20relations%20between%20excavator%20valve%20states%20and%20joint%0Avelocities.%20With%20a%20few%20human-operated%20demonstration%20data%20trajectories%2C%20ExACT%0Ademonstrates%20the%20capability%20of%20completing%20different%20excavation%20tasks%2C%20including%0Areaching%2C%20digging%20and%20dumping%20through%20imitation%20learning%20in%20validations%20with%0Athe%20simulator.%20To%20the%20best%20of%20our%20knowledge%2C%20ExACT%20represents%20the%20first%0Ainstance%20towards%20building%20an%20end-to-end%20autonomous%20excavator%20system%20via%0Aimitation%20learning%20methods%20with%20a%20minimal%20set%20of%20human%20demonstrations.%20The%0Avideo%20about%20this%20work%20can%20be%20accessed%20at%20https%3A//youtu.be/NmzR_Rf-aEk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExACT%253A%2520An%2520End-to-End%2520Autonomous%2520Excavator%2520System%2520Using%2520Action%2520Chunking%250A%2520%2520With%2520Transformers%26entry.906535625%3DLiangliang%2520Chen%2520and%2520Shiyu%2520Jin%2520and%2520Haoyu%2520Wang%2520and%2520Liangjun%2520Zhang%26entry.1292438233%3D%2520%2520Excavators%2520are%2520crucial%2520for%2520diverse%2520tasks%2520such%2520as%2520construction%2520and%2520mining%252C%250Awhile%2520autonomous%2520excavator%2520systems%2520enhance%2520safety%2520and%2520efficiency%252C%2520address%2520labor%250Ashortages%252C%2520and%2520improve%2520human%2520working%2520conditions.%2520Different%2520from%2520the%2520existing%250Amodularized%2520approaches%252C%2520this%2520paper%2520introduces%2520ExACT%252C%2520an%2520end-to-end%2520autonomous%250Aexcavator%2520system%2520that%2520processes%2520raw%2520LiDAR%252C%2520camera%2520data%252C%2520and%2520joint%2520positions%2520to%250Acontrol%2520excavator%2520valves%2520directly.%2520Utilizing%2520the%2520Action%2520Chunking%2520with%250ATransformers%2520%2528ACT%2529%2520architecture%252C%2520ExACT%2520employs%2520imitation%2520learning%2520to%2520take%250Aobservations%2520from%2520multi-modal%2520sensors%2520as%2520inputs%2520and%2520generate%2520actionable%250Asequences.%2520In%2520our%2520experiment%252C%2520we%2520build%2520a%2520simulator%2520based%2520on%2520the%2520captured%250Areal-world%2520data%2520to%2520model%2520the%2520relations%2520between%2520excavator%2520valve%2520states%2520and%2520joint%250Avelocities.%2520With%2520a%2520few%2520human-operated%2520demonstration%2520data%2520trajectories%252C%2520ExACT%250Ademonstrates%2520the%2520capability%2520of%2520completing%2520different%2520excavation%2520tasks%252C%2520including%250Areaching%252C%2520digging%2520and%2520dumping%2520through%2520imitation%2520learning%2520in%2520validations%2520with%250Athe%2520simulator.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520ExACT%2520represents%2520the%2520first%250Ainstance%2520towards%2520building%2520an%2520end-to-end%2520autonomous%2520excavator%2520system%2520via%250Aimitation%2520learning%2520methods%2520with%2520a%2520minimal%2520set%2520of%2520human%2520demonstrations.%2520The%250Avideo%2520about%2520this%2520work%2520can%2520be%2520accessed%2520at%2520https%253A//youtu.be/NmzR_Rf-aEk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExACT%3A%20An%20End-to-End%20Autonomous%20Excavator%20System%20Using%20Action%20Chunking%0A%20%20With%20Transformers&entry.906535625=Liangliang%20Chen%20and%20Shiyu%20Jin%20and%20Haoyu%20Wang%20and%20Liangjun%20Zhang&entry.1292438233=%20%20Excavators%20are%20crucial%20for%20diverse%20tasks%20such%20as%20construction%20and%20mining%2C%0Awhile%20autonomous%20excavator%20systems%20enhance%20safety%20and%20efficiency%2C%20address%20labor%0Ashortages%2C%20and%20improve%20human%20working%20conditions.%20Different%20from%20the%20existing%0Amodularized%20approaches%2C%20this%20paper%20introduces%20ExACT%2C%20an%20end-to-end%20autonomous%0Aexcavator%20system%20that%20processes%20raw%20LiDAR%2C%20camera%20data%2C%20and%20joint%20positions%20to%0Acontrol%20excavator%20valves%20directly.%20Utilizing%20the%20Action%20Chunking%20with%0ATransformers%20%28ACT%29%20architecture%2C%20ExACT%20employs%20imitation%20learning%20to%20take%0Aobservations%20from%20multi-modal%20sensors%20as%20inputs%20and%20generate%20actionable%0Asequences.%20In%20our%20experiment%2C%20we%20build%20a%20simulator%20based%20on%20the%20captured%0Areal-world%20data%20to%20model%20the%20relations%20between%20excavator%20valve%20states%20and%20joint%0Avelocities.%20With%20a%20few%20human-operated%20demonstration%20data%20trajectories%2C%20ExACT%0Ademonstrates%20the%20capability%20of%20completing%20different%20excavation%20tasks%2C%20including%0Areaching%2C%20digging%20and%20dumping%20through%20imitation%20learning%20in%20validations%20with%0Athe%20simulator.%20To%20the%20best%20of%20our%20knowledge%2C%20ExACT%20represents%20the%20first%0Ainstance%20towards%20building%20an%20end-to-end%20autonomous%20excavator%20system%20via%0Aimitation%20learning%20methods%20with%20a%20minimal%20set%20of%20human%20demonstrations.%20The%0Avideo%20about%20this%20work%20can%20be%20accessed%20at%20https%3A//youtu.be/NmzR_Rf-aEk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05861v1&entry.124074799=Read"},
{"title": "How Quality Affects Deep Neural Networks in Fine-Grained Image\n  Classification", "author": "Joseph Smith and Zheming Zuo and Jonathan Stonehouse and Boguslaw Obara", "abstract": "  In this paper, we propose a No-Reference Image Quality Assessment (NRIQA)\nguided cut-off point selection (CPS) strategy to enhance the performance of a\nfine-grained classification system. Scores given by existing NRIQA methods on\nthe same image may vary and not be as independent of natural image\naugmentations as expected, which weakens their connection and explainability to\nfine-grained image classification. Taking the three most commonly adopted image\naugmentation configurations -- cropping, rotating, and blurring -- as the entry\npoint, we formulate a two-step mechanism for selecting the most discriminative\nsubset from a given image dataset by considering both the confidence of model\npredictions and the density distribution of image qualities over several NRIQA\nmethods. Concretely, the cut-off points yielded by those methods are aggregated\nvia majority voting to inform the process of image subset selection. The\nefficacy and efficiency of such a mechanism have been confirmed by comparing\nthe models being trained on high-quality images against a combination of high-\nand low-quality ones, with a range of 0.7% to 4.2% improvement on a commercial\nproduct dataset in terms of mean accuracy through four deep neural classifiers.\nThe robustness of the mechanism has been proven by the observations that all\nthe selected high-quality images can work jointly with 70% low-quality images\nwith 1.3% of classification precision sacrificed when using ResNet34 in an\nablation study.\n", "link": "http://arxiv.org/abs/2405.05742v1", "date": "2024-05-09", "relevancy": 2.08, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5405}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5056}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Quality%20Affects%20Deep%20Neural%20Networks%20in%20Fine-Grained%20Image%0A%20%20Classification&body=Title%3A%20How%20Quality%20Affects%20Deep%20Neural%20Networks%20in%20Fine-Grained%20Image%0A%20%20Classification%0AAuthor%3A%20Joseph%20Smith%20and%20Zheming%20Zuo%20and%20Jonathan%20Stonehouse%20and%20Boguslaw%20Obara%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20No-Reference%20Image%20Quality%20Assessment%20%28NRIQA%29%0Aguided%20cut-off%20point%20selection%20%28CPS%29%20strategy%20to%20enhance%20the%20performance%20of%20a%0Afine-grained%20classification%20system.%20Scores%20given%20by%20existing%20NRIQA%20methods%20on%0Athe%20same%20image%20may%20vary%20and%20not%20be%20as%20independent%20of%20natural%20image%0Aaugmentations%20as%20expected%2C%20which%20weakens%20their%20connection%20and%20explainability%20to%0Afine-grained%20image%20classification.%20Taking%20the%20three%20most%20commonly%20adopted%20image%0Aaugmentation%20configurations%20--%20cropping%2C%20rotating%2C%20and%20blurring%20--%20as%20the%20entry%0Apoint%2C%20we%20formulate%20a%20two-step%20mechanism%20for%20selecting%20the%20most%20discriminative%0Asubset%20from%20a%20given%20image%20dataset%20by%20considering%20both%20the%20confidence%20of%20model%0Apredictions%20and%20the%20density%20distribution%20of%20image%20qualities%20over%20several%20NRIQA%0Amethods.%20Concretely%2C%20the%20cut-off%20points%20yielded%20by%20those%20methods%20are%20aggregated%0Avia%20majority%20voting%20to%20inform%20the%20process%20of%20image%20subset%20selection.%20The%0Aefficacy%20and%20efficiency%20of%20such%20a%20mechanism%20have%20been%20confirmed%20by%20comparing%0Athe%20models%20being%20trained%20on%20high-quality%20images%20against%20a%20combination%20of%20high-%0Aand%20low-quality%20ones%2C%20with%20a%20range%20of%200.7%25%20to%204.2%25%20improvement%20on%20a%20commercial%0Aproduct%20dataset%20in%20terms%20of%20mean%20accuracy%20through%20four%20deep%20neural%20classifiers.%0AThe%20robustness%20of%20the%20mechanism%20has%20been%20proven%20by%20the%20observations%20that%20all%0Athe%20selected%20high-quality%20images%20can%20work%20jointly%20with%2070%25%20low-quality%20images%0Awith%201.3%25%20of%20classification%20precision%20sacrificed%20when%20using%20ResNet34%20in%20an%0Aablation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Quality%2520Affects%2520Deep%2520Neural%2520Networks%2520in%2520Fine-Grained%2520Image%250A%2520%2520Classification%26entry.906535625%3DJoseph%2520Smith%2520and%2520Zheming%2520Zuo%2520and%2520Jonathan%2520Stonehouse%2520and%2520Boguslaw%2520Obara%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520No-Reference%2520Image%2520Quality%2520Assessment%2520%2528NRIQA%2529%250Aguided%2520cut-off%2520point%2520selection%2520%2528CPS%2529%2520strategy%2520to%2520enhance%2520the%2520performance%2520of%2520a%250Afine-grained%2520classification%2520system.%2520Scores%2520given%2520by%2520existing%2520NRIQA%2520methods%2520on%250Athe%2520same%2520image%2520may%2520vary%2520and%2520not%2520be%2520as%2520independent%2520of%2520natural%2520image%250Aaugmentations%2520as%2520expected%252C%2520which%2520weakens%2520their%2520connection%2520and%2520explainability%2520to%250Afine-grained%2520image%2520classification.%2520Taking%2520the%2520three%2520most%2520commonly%2520adopted%2520image%250Aaugmentation%2520configurations%2520--%2520cropping%252C%2520rotating%252C%2520and%2520blurring%2520--%2520as%2520the%2520entry%250Apoint%252C%2520we%2520formulate%2520a%2520two-step%2520mechanism%2520for%2520selecting%2520the%2520most%2520discriminative%250Asubset%2520from%2520a%2520given%2520image%2520dataset%2520by%2520considering%2520both%2520the%2520confidence%2520of%2520model%250Apredictions%2520and%2520the%2520density%2520distribution%2520of%2520image%2520qualities%2520over%2520several%2520NRIQA%250Amethods.%2520Concretely%252C%2520the%2520cut-off%2520points%2520yielded%2520by%2520those%2520methods%2520are%2520aggregated%250Avia%2520majority%2520voting%2520to%2520inform%2520the%2520process%2520of%2520image%2520subset%2520selection.%2520The%250Aefficacy%2520and%2520efficiency%2520of%2520such%2520a%2520mechanism%2520have%2520been%2520confirmed%2520by%2520comparing%250Athe%2520models%2520being%2520trained%2520on%2520high-quality%2520images%2520against%2520a%2520combination%2520of%2520high-%250Aand%2520low-quality%2520ones%252C%2520with%2520a%2520range%2520of%25200.7%2525%2520to%25204.2%2525%2520improvement%2520on%2520a%2520commercial%250Aproduct%2520dataset%2520in%2520terms%2520of%2520mean%2520accuracy%2520through%2520four%2520deep%2520neural%2520classifiers.%250AThe%2520robustness%2520of%2520the%2520mechanism%2520has%2520been%2520proven%2520by%2520the%2520observations%2520that%2520all%250Athe%2520selected%2520high-quality%2520images%2520can%2520work%2520jointly%2520with%252070%2525%2520low-quality%2520images%250Awith%25201.3%2525%2520of%2520classification%2520precision%2520sacrificed%2520when%2520using%2520ResNet34%2520in%2520an%250Aablation%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Quality%20Affects%20Deep%20Neural%20Networks%20in%20Fine-Grained%20Image%0A%20%20Classification&entry.906535625=Joseph%20Smith%20and%20Zheming%20Zuo%20and%20Jonathan%20Stonehouse%20and%20Boguslaw%20Obara&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20No-Reference%20Image%20Quality%20Assessment%20%28NRIQA%29%0Aguided%20cut-off%20point%20selection%20%28CPS%29%20strategy%20to%20enhance%20the%20performance%20of%20a%0Afine-grained%20classification%20system.%20Scores%20given%20by%20existing%20NRIQA%20methods%20on%0Athe%20same%20image%20may%20vary%20and%20not%20be%20as%20independent%20of%20natural%20image%0Aaugmentations%20as%20expected%2C%20which%20weakens%20their%20connection%20and%20explainability%20to%0Afine-grained%20image%20classification.%20Taking%20the%20three%20most%20commonly%20adopted%20image%0Aaugmentation%20configurations%20--%20cropping%2C%20rotating%2C%20and%20blurring%20--%20as%20the%20entry%0Apoint%2C%20we%20formulate%20a%20two-step%20mechanism%20for%20selecting%20the%20most%20discriminative%0Asubset%20from%20a%20given%20image%20dataset%20by%20considering%20both%20the%20confidence%20of%20model%0Apredictions%20and%20the%20density%20distribution%20of%20image%20qualities%20over%20several%20NRIQA%0Amethods.%20Concretely%2C%20the%20cut-off%20points%20yielded%20by%20those%20methods%20are%20aggregated%0Avia%20majority%20voting%20to%20inform%20the%20process%20of%20image%20subset%20selection.%20The%0Aefficacy%20and%20efficiency%20of%20such%20a%20mechanism%20have%20been%20confirmed%20by%20comparing%0Athe%20models%20being%20trained%20on%20high-quality%20images%20against%20a%20combination%20of%20high-%0Aand%20low-quality%20ones%2C%20with%20a%20range%20of%200.7%25%20to%204.2%25%20improvement%20on%20a%20commercial%0Aproduct%20dataset%20in%20terms%20of%20mean%20accuracy%20through%20four%20deep%20neural%20classifiers.%0AThe%20robustness%20of%20the%20mechanism%20has%20been%20proven%20by%20the%20observations%20that%20all%0Athe%20selected%20high-quality%20images%20can%20work%20jointly%20with%2070%25%20low-quality%20images%0Awith%201.3%25%20of%20classification%20precision%20sacrificed%20when%20using%20ResNet34%20in%20an%0Aablation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05742v1&entry.124074799=Read"},
{"title": "Fusing Models with Complementary Expertise", "author": "Hongyi Wang and Felipe Maia Polo and Yuekai Sun and Souvik Kundu and Eric Xing and Mikhail Yurochkin", "abstract": "  Training AI models that generalize across tasks and domains has long been\namong the open problems driving AI research. The emergence of Foundation Models\nmade it easier to obtain expert models for a given task, but the heterogeneity\nof data that may be encountered at test time often means that any single expert\nis insufficient. We consider the Fusion of Experts (FoE) problem of fusing\noutputs of expert models with complementary knowledge of the data distribution\nand formulate it as an instance of supervised learning. Our method is\napplicable to both discriminative and generative tasks and leads to significant\nperformance improvements in image and text classification, text summarization,\nmultiple-choice QA, and automatic evaluation of generated text. We also extend\nour method to the \"frugal\" setting where it is desired to reduce the number of\nexpert model evaluations at test time. Our implementation is publicly available\nat https://github.com/hwang595/FoE-ICLR2024.\n", "link": "http://arxiv.org/abs/2310.01542v2", "date": "2024-05-09", "relevancy": 2.0773, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5235}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusing%20Models%20with%20Complementary%20Expertise&body=Title%3A%20Fusing%20Models%20with%20Complementary%20Expertise%0AAuthor%3A%20Hongyi%20Wang%20and%20Felipe%20Maia%20Polo%20and%20Yuekai%20Sun%20and%20Souvik%20Kundu%20and%20Eric%20Xing%20and%20Mikhail%20Yurochkin%0AAbstract%3A%20%20%20Training%20AI%20models%20that%20generalize%20across%20tasks%20and%20domains%20has%20long%20been%0Aamong%20the%20open%20problems%20driving%20AI%20research.%20The%20emergence%20of%20Foundation%20Models%0Amade%20it%20easier%20to%20obtain%20expert%20models%20for%20a%20given%20task%2C%20but%20the%20heterogeneity%0Aof%20data%20that%20may%20be%20encountered%20at%20test%20time%20often%20means%20that%20any%20single%20expert%0Ais%20insufficient.%20We%20consider%20the%20Fusion%20of%20Experts%20%28FoE%29%20problem%20of%20fusing%0Aoutputs%20of%20expert%20models%20with%20complementary%20knowledge%20of%20the%20data%20distribution%0Aand%20formulate%20it%20as%20an%20instance%20of%20supervised%20learning.%20Our%20method%20is%0Aapplicable%20to%20both%20discriminative%20and%20generative%20tasks%20and%20leads%20to%20significant%0Aperformance%20improvements%20in%20image%20and%20text%20classification%2C%20text%20summarization%2C%0Amultiple-choice%20QA%2C%20and%20automatic%20evaluation%20of%20generated%20text.%20We%20also%20extend%0Aour%20method%20to%20the%20%22frugal%22%20setting%20where%20it%20is%20desired%20to%20reduce%20the%20number%20of%0Aexpert%20model%20evaluations%20at%20test%20time.%20Our%20implementation%20is%20publicly%20available%0Aat%20https%3A//github.com/hwang595/FoE-ICLR2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusing%2520Models%2520with%2520Complementary%2520Expertise%26entry.906535625%3DHongyi%2520Wang%2520and%2520Felipe%2520Maia%2520Polo%2520and%2520Yuekai%2520Sun%2520and%2520Souvik%2520Kundu%2520and%2520Eric%2520Xing%2520and%2520Mikhail%2520Yurochkin%26entry.1292438233%3D%2520%2520Training%2520AI%2520models%2520that%2520generalize%2520across%2520tasks%2520and%2520domains%2520has%2520long%2520been%250Aamong%2520the%2520open%2520problems%2520driving%2520AI%2520research.%2520The%2520emergence%2520of%2520Foundation%2520Models%250Amade%2520it%2520easier%2520to%2520obtain%2520expert%2520models%2520for%2520a%2520given%2520task%252C%2520but%2520the%2520heterogeneity%250Aof%2520data%2520that%2520may%2520be%2520encountered%2520at%2520test%2520time%2520often%2520means%2520that%2520any%2520single%2520expert%250Ais%2520insufficient.%2520We%2520consider%2520the%2520Fusion%2520of%2520Experts%2520%2528FoE%2529%2520problem%2520of%2520fusing%250Aoutputs%2520of%2520expert%2520models%2520with%2520complementary%2520knowledge%2520of%2520the%2520data%2520distribution%250Aand%2520formulate%2520it%2520as%2520an%2520instance%2520of%2520supervised%2520learning.%2520Our%2520method%2520is%250Aapplicable%2520to%2520both%2520discriminative%2520and%2520generative%2520tasks%2520and%2520leads%2520to%2520significant%250Aperformance%2520improvements%2520in%2520image%2520and%2520text%2520classification%252C%2520text%2520summarization%252C%250Amultiple-choice%2520QA%252C%2520and%2520automatic%2520evaluation%2520of%2520generated%2520text.%2520We%2520also%2520extend%250Aour%2520method%2520to%2520the%2520%2522frugal%2522%2520setting%2520where%2520it%2520is%2520desired%2520to%2520reduce%2520the%2520number%2520of%250Aexpert%2520model%2520evaluations%2520at%2520test%2520time.%2520Our%2520implementation%2520is%2520publicly%2520available%250Aat%2520https%253A//github.com/hwang595/FoE-ICLR2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20Models%20with%20Complementary%20Expertise&entry.906535625=Hongyi%20Wang%20and%20Felipe%20Maia%20Polo%20and%20Yuekai%20Sun%20and%20Souvik%20Kundu%20and%20Eric%20Xing%20and%20Mikhail%20Yurochkin&entry.1292438233=%20%20Training%20AI%20models%20that%20generalize%20across%20tasks%20and%20domains%20has%20long%20been%0Aamong%20the%20open%20problems%20driving%20AI%20research.%20The%20emergence%20of%20Foundation%20Models%0Amade%20it%20easier%20to%20obtain%20expert%20models%20for%20a%20given%20task%2C%20but%20the%20heterogeneity%0Aof%20data%20that%20may%20be%20encountered%20at%20test%20time%20often%20means%20that%20any%20single%20expert%0Ais%20insufficient.%20We%20consider%20the%20Fusion%20of%20Experts%20%28FoE%29%20problem%20of%20fusing%0Aoutputs%20of%20expert%20models%20with%20complementary%20knowledge%20of%20the%20data%20distribution%0Aand%20formulate%20it%20as%20an%20instance%20of%20supervised%20learning.%20Our%20method%20is%0Aapplicable%20to%20both%20discriminative%20and%20generative%20tasks%20and%20leads%20to%20significant%0Aperformance%20improvements%20in%20image%20and%20text%20classification%2C%20text%20summarization%2C%0Amultiple-choice%20QA%2C%20and%20automatic%20evaluation%20of%20generated%20text.%20We%20also%20extend%0Aour%20method%20to%20the%20%22frugal%22%20setting%20where%20it%20is%20desired%20to%20reduce%20the%20number%20of%0Aexpert%20model%20evaluations%20at%20test%20time.%20Our%20implementation%20is%20publicly%20available%0Aat%20https%3A//github.com/hwang595/FoE-ICLR2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01542v2&entry.124074799=Read"},
{"title": "Batched Stochastic Bandit for Nondegenerate Functions", "author": "Yu Liu and Yunlu Shu and Tianyu Wang", "abstract": "  This paper studies batched bandit learning problems for nondegenerate\nfunctions. We introduce an algorithm that solves the batched bandit problem for\nnondegenerate functions near-optimally. More specifically, we introduce an\nalgorithm, called Geometric Narrowing (GN), whose regret bound is of order\n$\\widetilde{{\\mathcal{O}}} ( A_{+}^d \\sqrt{T} )$. In addition, GN only needs\n$\\mathcal{O} (\\log \\log T)$ batches to achieve this regret. We also provide\nlower bound analysis for this problem. More specifically, we prove that over\nsome (compact) doubling metric space of doubling dimension $d$: 1. For any\npolicy $\\pi$, there exists a problem instance on which $\\pi$ admits a regret of\norder ${\\Omega} ( A_-^d \\sqrt{T})$; 2. No policy can achieve a regret of order\n$ A_-^d \\sqrt{T} $ over all problem instances, using less than $ \\Omega ( \\log\n\\log T ) $ rounds of communications. Our lower bound analysis shows that the GN\nalgorithm achieves near optimal regret with minimal number of batches.\n", "link": "http://arxiv.org/abs/2405.05733v1", "date": "2024-05-09", "relevancy": 2.071, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4207}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4161}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Batched%20Stochastic%20Bandit%20for%20Nondegenerate%20Functions&body=Title%3A%20Batched%20Stochastic%20Bandit%20for%20Nondegenerate%20Functions%0AAuthor%3A%20Yu%20Liu%20and%20Yunlu%20Shu%20and%20Tianyu%20Wang%0AAbstract%3A%20%20%20This%20paper%20studies%20batched%20bandit%20learning%20problems%20for%20nondegenerate%0Afunctions.%20We%20introduce%20an%20algorithm%20that%20solves%20the%20batched%20bandit%20problem%20for%0Anondegenerate%20functions%20near-optimally.%20More%20specifically%2C%20we%20introduce%20an%0Aalgorithm%2C%20called%20Geometric%20Narrowing%20%28GN%29%2C%20whose%20regret%20bound%20is%20of%20order%0A%24%5Cwidetilde%7B%7B%5Cmathcal%7BO%7D%7D%7D%20%28%20A_%7B%2B%7D%5Ed%20%5Csqrt%7BT%7D%20%29%24.%20In%20addition%2C%20GN%20only%20needs%0A%24%5Cmathcal%7BO%7D%20%28%5Clog%20%5Clog%20T%29%24%20batches%20to%20achieve%20this%20regret.%20We%20also%20provide%0Alower%20bound%20analysis%20for%20this%20problem.%20More%20specifically%2C%20we%20prove%20that%20over%0Asome%20%28compact%29%20doubling%20metric%20space%20of%20doubling%20dimension%20%24d%24%3A%201.%20For%20any%0Apolicy%20%24%5Cpi%24%2C%20there%20exists%20a%20problem%20instance%20on%20which%20%24%5Cpi%24%20admits%20a%20regret%20of%0Aorder%20%24%7B%5COmega%7D%20%28%20A_-%5Ed%20%5Csqrt%7BT%7D%29%24%3B%202.%20No%20policy%20can%20achieve%20a%20regret%20of%20order%0A%24%20A_-%5Ed%20%5Csqrt%7BT%7D%20%24%20over%20all%20problem%20instances%2C%20using%20less%20than%20%24%20%5COmega%20%28%20%5Clog%0A%5Clog%20T%20%29%20%24%20rounds%20of%20communications.%20Our%20lower%20bound%20analysis%20shows%20that%20the%20GN%0Aalgorithm%20achieves%20near%20optimal%20regret%20with%20minimal%20number%20of%20batches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBatched%2520Stochastic%2520Bandit%2520for%2520Nondegenerate%2520Functions%26entry.906535625%3DYu%2520Liu%2520and%2520Yunlu%2520Shu%2520and%2520Tianyu%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520batched%2520bandit%2520learning%2520problems%2520for%2520nondegenerate%250Afunctions.%2520We%2520introduce%2520an%2520algorithm%2520that%2520solves%2520the%2520batched%2520bandit%2520problem%2520for%250Anondegenerate%2520functions%2520near-optimally.%2520More%2520specifically%252C%2520we%2520introduce%2520an%250Aalgorithm%252C%2520called%2520Geometric%2520Narrowing%2520%2528GN%2529%252C%2520whose%2520regret%2520bound%2520is%2520of%2520order%250A%2524%255Cwidetilde%257B%257B%255Cmathcal%257BO%257D%257D%257D%2520%2528%2520A_%257B%252B%257D%255Ed%2520%255Csqrt%257BT%257D%2520%2529%2524.%2520In%2520addition%252C%2520GN%2520only%2520needs%250A%2524%255Cmathcal%257BO%257D%2520%2528%255Clog%2520%255Clog%2520T%2529%2524%2520batches%2520to%2520achieve%2520this%2520regret.%2520We%2520also%2520provide%250Alower%2520bound%2520analysis%2520for%2520this%2520problem.%2520More%2520specifically%252C%2520we%2520prove%2520that%2520over%250Asome%2520%2528compact%2529%2520doubling%2520metric%2520space%2520of%2520doubling%2520dimension%2520%2524d%2524%253A%25201.%2520For%2520any%250Apolicy%2520%2524%255Cpi%2524%252C%2520there%2520exists%2520a%2520problem%2520instance%2520on%2520which%2520%2524%255Cpi%2524%2520admits%2520a%2520regret%2520of%250Aorder%2520%2524%257B%255COmega%257D%2520%2528%2520A_-%255Ed%2520%255Csqrt%257BT%257D%2529%2524%253B%25202.%2520No%2520policy%2520can%2520achieve%2520a%2520regret%2520of%2520order%250A%2524%2520A_-%255Ed%2520%255Csqrt%257BT%257D%2520%2524%2520over%2520all%2520problem%2520instances%252C%2520using%2520less%2520than%2520%2524%2520%255COmega%2520%2528%2520%255Clog%250A%255Clog%2520T%2520%2529%2520%2524%2520rounds%2520of%2520communications.%2520Our%2520lower%2520bound%2520analysis%2520shows%2520that%2520the%2520GN%250Aalgorithm%2520achieves%2520near%2520optimal%2520regret%2520with%2520minimal%2520number%2520of%2520batches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Batched%20Stochastic%20Bandit%20for%20Nondegenerate%20Functions&entry.906535625=Yu%20Liu%20and%20Yunlu%20Shu%20and%20Tianyu%20Wang&entry.1292438233=%20%20This%20paper%20studies%20batched%20bandit%20learning%20problems%20for%20nondegenerate%0Afunctions.%20We%20introduce%20an%20algorithm%20that%20solves%20the%20batched%20bandit%20problem%20for%0Anondegenerate%20functions%20near-optimally.%20More%20specifically%2C%20we%20introduce%20an%0Aalgorithm%2C%20called%20Geometric%20Narrowing%20%28GN%29%2C%20whose%20regret%20bound%20is%20of%20order%0A%24%5Cwidetilde%7B%7B%5Cmathcal%7BO%7D%7D%7D%20%28%20A_%7B%2B%7D%5Ed%20%5Csqrt%7BT%7D%20%29%24.%20In%20addition%2C%20GN%20only%20needs%0A%24%5Cmathcal%7BO%7D%20%28%5Clog%20%5Clog%20T%29%24%20batches%20to%20achieve%20this%20regret.%20We%20also%20provide%0Alower%20bound%20analysis%20for%20this%20problem.%20More%20specifically%2C%20we%20prove%20that%20over%0Asome%20%28compact%29%20doubling%20metric%20space%20of%20doubling%20dimension%20%24d%24%3A%201.%20For%20any%0Apolicy%20%24%5Cpi%24%2C%20there%20exists%20a%20problem%20instance%20on%20which%20%24%5Cpi%24%20admits%20a%20regret%20of%0Aorder%20%24%7B%5COmega%7D%20%28%20A_-%5Ed%20%5Csqrt%7BT%7D%29%24%3B%202.%20No%20policy%20can%20achieve%20a%20regret%20of%20order%0A%24%20A_-%5Ed%20%5Csqrt%7BT%7D%20%24%20over%20all%20problem%20instances%2C%20using%20less%20than%20%24%20%5COmega%20%28%20%5Clog%0A%5Clog%20T%20%29%20%24%20rounds%20of%20communications.%20Our%20lower%20bound%20analysis%20shows%20that%20the%20GN%0Aalgorithm%20achieves%20near%20optimal%20regret%20with%20minimal%20number%20of%20batches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05733v1&entry.124074799=Read"},
{"title": "Designed Dithering Sign Activation for Binary Neural Networks", "author": "Brayan Monroy and Juan Estupi\u00f1an and Tatiana Gelvez-Barrera and Jorge Bacca and Henry Arguello", "abstract": "  Binary Neural Networks emerged as a cost-effective and energy-efficient\nsolution for computer vision tasks by binarizing either network weights or\nactivations. However, common binary activations, such as the Sign activation\nfunction, abruptly binarize the values with a single threshold, losing\nfine-grained details in the feature outputs. This work proposes an activation\nthat applies multiple thresholds following dithering principles, shifting the\nSign activation function for each pixel according to a spatially periodic\nthreshold kernel. Unlike literature methods, the shifting is defined jointly\nfor a set of adjacent pixels, taking advantage of spatial correlations.\nExperiments over the classification task demonstrate the effectiveness of the\ndesigned dithering Sign activation function as an alternative activation for\nbinary neural networks, without increasing the computational cost. Further,\nDeSign balances the preservation of details with the efficiency of binary\noperations.\n", "link": "http://arxiv.org/abs/2405.02220v2", "date": "2024-05-09", "relevancy": 2.0428, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5154}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5095}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designed%20Dithering%20Sign%20Activation%20for%20Binary%20Neural%20Networks&body=Title%3A%20Designed%20Dithering%20Sign%20Activation%20for%20Binary%20Neural%20Networks%0AAuthor%3A%20Brayan%20Monroy%20and%20Juan%20Estupi%C3%B1an%20and%20Tatiana%20Gelvez-Barrera%20and%20Jorge%20Bacca%20and%20Henry%20Arguello%0AAbstract%3A%20%20%20Binary%20Neural%20Networks%20emerged%20as%20a%20cost-effective%20and%20energy-efficient%0Asolution%20for%20computer%20vision%20tasks%20by%20binarizing%20either%20network%20weights%20or%0Aactivations.%20However%2C%20common%20binary%20activations%2C%20such%20as%20the%20Sign%20activation%0Afunction%2C%20abruptly%20binarize%20the%20values%20with%20a%20single%20threshold%2C%20losing%0Afine-grained%20details%20in%20the%20feature%20outputs.%20This%20work%20proposes%20an%20activation%0Athat%20applies%20multiple%20thresholds%20following%20dithering%20principles%2C%20shifting%20the%0ASign%20activation%20function%20for%20each%20pixel%20according%20to%20a%20spatially%20periodic%0Athreshold%20kernel.%20Unlike%20literature%20methods%2C%20the%20shifting%20is%20defined%20jointly%0Afor%20a%20set%20of%20adjacent%20pixels%2C%20taking%20advantage%20of%20spatial%20correlations.%0AExperiments%20over%20the%20classification%20task%20demonstrate%20the%20effectiveness%20of%20the%0Adesigned%20dithering%20Sign%20activation%20function%20as%20an%20alternative%20activation%20for%0Abinary%20neural%20networks%2C%20without%20increasing%20the%20computational%20cost.%20Further%2C%0ADeSign%20balances%20the%20preservation%20of%20details%20with%20the%20efficiency%20of%20binary%0Aoperations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigned%2520Dithering%2520Sign%2520Activation%2520for%2520Binary%2520Neural%2520Networks%26entry.906535625%3DBrayan%2520Monroy%2520and%2520Juan%2520Estupi%25C3%25B1an%2520and%2520Tatiana%2520Gelvez-Barrera%2520and%2520Jorge%2520Bacca%2520and%2520Henry%2520Arguello%26entry.1292438233%3D%2520%2520Binary%2520Neural%2520Networks%2520emerged%2520as%2520a%2520cost-effective%2520and%2520energy-efficient%250Asolution%2520for%2520computer%2520vision%2520tasks%2520by%2520binarizing%2520either%2520network%2520weights%2520or%250Aactivations.%2520However%252C%2520common%2520binary%2520activations%252C%2520such%2520as%2520the%2520Sign%2520activation%250Afunction%252C%2520abruptly%2520binarize%2520the%2520values%2520with%2520a%2520single%2520threshold%252C%2520losing%250Afine-grained%2520details%2520in%2520the%2520feature%2520outputs.%2520This%2520work%2520proposes%2520an%2520activation%250Athat%2520applies%2520multiple%2520thresholds%2520following%2520dithering%2520principles%252C%2520shifting%2520the%250ASign%2520activation%2520function%2520for%2520each%2520pixel%2520according%2520to%2520a%2520spatially%2520periodic%250Athreshold%2520kernel.%2520Unlike%2520literature%2520methods%252C%2520the%2520shifting%2520is%2520defined%2520jointly%250Afor%2520a%2520set%2520of%2520adjacent%2520pixels%252C%2520taking%2520advantage%2520of%2520spatial%2520correlations.%250AExperiments%2520over%2520the%2520classification%2520task%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Adesigned%2520dithering%2520Sign%2520activation%2520function%2520as%2520an%2520alternative%2520activation%2520for%250Abinary%2520neural%2520networks%252C%2520without%2520increasing%2520the%2520computational%2520cost.%2520Further%252C%250ADeSign%2520balances%2520the%2520preservation%2520of%2520details%2520with%2520the%2520efficiency%2520of%2520binary%250Aoperations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designed%20Dithering%20Sign%20Activation%20for%20Binary%20Neural%20Networks&entry.906535625=Brayan%20Monroy%20and%20Juan%20Estupi%C3%B1an%20and%20Tatiana%20Gelvez-Barrera%20and%20Jorge%20Bacca%20and%20Henry%20Arguello&entry.1292438233=%20%20Binary%20Neural%20Networks%20emerged%20as%20a%20cost-effective%20and%20energy-efficient%0Asolution%20for%20computer%20vision%20tasks%20by%20binarizing%20either%20network%20weights%20or%0Aactivations.%20However%2C%20common%20binary%20activations%2C%20such%20as%20the%20Sign%20activation%0Afunction%2C%20abruptly%20binarize%20the%20values%20with%20a%20single%20threshold%2C%20losing%0Afine-grained%20details%20in%20the%20feature%20outputs.%20This%20work%20proposes%20an%20activation%0Athat%20applies%20multiple%20thresholds%20following%20dithering%20principles%2C%20shifting%20the%0ASign%20activation%20function%20for%20each%20pixel%20according%20to%20a%20spatially%20periodic%0Athreshold%20kernel.%20Unlike%20literature%20methods%2C%20the%20shifting%20is%20defined%20jointly%0Afor%20a%20set%20of%20adjacent%20pixels%2C%20taking%20advantage%20of%20spatial%20correlations.%0AExperiments%20over%20the%20classification%20task%20demonstrate%20the%20effectiveness%20of%20the%0Adesigned%20dithering%20Sign%20activation%20function%20as%20an%20alternative%20activation%20for%0Abinary%20neural%20networks%2C%20without%20increasing%20the%20computational%20cost.%20Further%2C%0ADeSign%20balances%20the%20preservation%20of%20details%20with%20the%20efficiency%20of%20binary%0Aoperations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02220v2&entry.124074799=Read"},
{"title": "Exploiting Autoencoder's Weakness to Generate Pseudo Anomalies", "author": "Marcella Astrid and Muhammad Zaigham Zaheer and Djamila Aouada and Seung-Ik Lee", "abstract": "  Due to the rare occurrence of anomalous events, a typical approach to anomaly\ndetection is to train an autoencoder (AE) with normal data only so that it\nlearns the patterns or representations of the normal training data. At test\ntime, the trained AE is expected to well reconstruct normal but to poorly\nreconstruct anomalous data. However, contrary to the expectation, anomalous\ndata is often well reconstructed as well. In order to further separate the\nreconstruction quality between normal and anomalous data, we propose creating\npseudo anomalies from learned adaptive noise by exploiting the aforementioned\nweakness of AE, i.e., reconstructing anomalies too well. The generated noise is\nadded to the normal data to create pseudo anomalies. Extensive experiments on\nPed2, Avenue, ShanghaiTech, CIFAR-10, and KDDCUP datasets demonstrate the\neffectiveness and generic applicability of our approach in improving the\ndiscriminative capability of AEs for anomaly detection.\n", "link": "http://arxiv.org/abs/2405.05886v1", "date": "2024-05-09", "relevancy": 2.0422, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.527}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5029}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Autoencoder%27s%20Weakness%20to%20Generate%20Pseudo%20Anomalies&body=Title%3A%20Exploiting%20Autoencoder%27s%20Weakness%20to%20Generate%20Pseudo%20Anomalies%0AAuthor%3A%20Marcella%20Astrid%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Djamila%20Aouada%20and%20Seung-Ik%20Lee%0AAbstract%3A%20%20%20Due%20to%20the%20rare%20occurrence%20of%20anomalous%20events%2C%20a%20typical%20approach%20to%20anomaly%0Adetection%20is%20to%20train%20an%20autoencoder%20%28AE%29%20with%20normal%20data%20only%20so%20that%20it%0Alearns%20the%20patterns%20or%20representations%20of%20the%20normal%20training%20data.%20At%20test%0Atime%2C%20the%20trained%20AE%20is%20expected%20to%20well%20reconstruct%20normal%20but%20to%20poorly%0Areconstruct%20anomalous%20data.%20However%2C%20contrary%20to%20the%20expectation%2C%20anomalous%0Adata%20is%20often%20well%20reconstructed%20as%20well.%20In%20order%20to%20further%20separate%20the%0Areconstruction%20quality%20between%20normal%20and%20anomalous%20data%2C%20we%20propose%20creating%0Apseudo%20anomalies%20from%20learned%20adaptive%20noise%20by%20exploiting%20the%20aforementioned%0Aweakness%20of%20AE%2C%20i.e.%2C%20reconstructing%20anomalies%20too%20well.%20The%20generated%20noise%20is%0Aadded%20to%20the%20normal%20data%20to%20create%20pseudo%20anomalies.%20Extensive%20experiments%20on%0APed2%2C%20Avenue%2C%20ShanghaiTech%2C%20CIFAR-10%2C%20and%20KDDCUP%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generic%20applicability%20of%20our%20approach%20in%20improving%20the%0Adiscriminative%20capability%20of%20AEs%20for%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Autoencoder%2527s%2520Weakness%2520to%2520Generate%2520Pseudo%2520Anomalies%26entry.906535625%3DMarcella%2520Astrid%2520and%2520Muhammad%2520Zaigham%2520Zaheer%2520and%2520Djamila%2520Aouada%2520and%2520Seung-Ik%2520Lee%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520rare%2520occurrence%2520of%2520anomalous%2520events%252C%2520a%2520typical%2520approach%2520to%2520anomaly%250Adetection%2520is%2520to%2520train%2520an%2520autoencoder%2520%2528AE%2529%2520with%2520normal%2520data%2520only%2520so%2520that%2520it%250Alearns%2520the%2520patterns%2520or%2520representations%2520of%2520the%2520normal%2520training%2520data.%2520At%2520test%250Atime%252C%2520the%2520trained%2520AE%2520is%2520expected%2520to%2520well%2520reconstruct%2520normal%2520but%2520to%2520poorly%250Areconstruct%2520anomalous%2520data.%2520However%252C%2520contrary%2520to%2520the%2520expectation%252C%2520anomalous%250Adata%2520is%2520often%2520well%2520reconstructed%2520as%2520well.%2520In%2520order%2520to%2520further%2520separate%2520the%250Areconstruction%2520quality%2520between%2520normal%2520and%2520anomalous%2520data%252C%2520we%2520propose%2520creating%250Apseudo%2520anomalies%2520from%2520learned%2520adaptive%2520noise%2520by%2520exploiting%2520the%2520aforementioned%250Aweakness%2520of%2520AE%252C%2520i.e.%252C%2520reconstructing%2520anomalies%2520too%2520well.%2520The%2520generated%2520noise%2520is%250Aadded%2520to%2520the%2520normal%2520data%2520to%2520create%2520pseudo%2520anomalies.%2520Extensive%2520experiments%2520on%250APed2%252C%2520Avenue%252C%2520ShanghaiTech%252C%2520CIFAR-10%252C%2520and%2520KDDCUP%2520datasets%2520demonstrate%2520the%250Aeffectiveness%2520and%2520generic%2520applicability%2520of%2520our%2520approach%2520in%2520improving%2520the%250Adiscriminative%2520capability%2520of%2520AEs%2520for%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Autoencoder%27s%20Weakness%20to%20Generate%20Pseudo%20Anomalies&entry.906535625=Marcella%20Astrid%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Djamila%20Aouada%20and%20Seung-Ik%20Lee&entry.1292438233=%20%20Due%20to%20the%20rare%20occurrence%20of%20anomalous%20events%2C%20a%20typical%20approach%20to%20anomaly%0Adetection%20is%20to%20train%20an%20autoencoder%20%28AE%29%20with%20normal%20data%20only%20so%20that%20it%0Alearns%20the%20patterns%20or%20representations%20of%20the%20normal%20training%20data.%20At%20test%0Atime%2C%20the%20trained%20AE%20is%20expected%20to%20well%20reconstruct%20normal%20but%20to%20poorly%0Areconstruct%20anomalous%20data.%20However%2C%20contrary%20to%20the%20expectation%2C%20anomalous%0Adata%20is%20often%20well%20reconstructed%20as%20well.%20In%20order%20to%20further%20separate%20the%0Areconstruction%20quality%20between%20normal%20and%20anomalous%20data%2C%20we%20propose%20creating%0Apseudo%20anomalies%20from%20learned%20adaptive%20noise%20by%20exploiting%20the%20aforementioned%0Aweakness%20of%20AE%2C%20i.e.%2C%20reconstructing%20anomalies%20too%20well.%20The%20generated%20noise%20is%0Aadded%20to%20the%20normal%20data%20to%20create%20pseudo%20anomalies.%20Extensive%20experiments%20on%0APed2%2C%20Avenue%2C%20ShanghaiTech%2C%20CIFAR-10%2C%20and%20KDDCUP%20datasets%20demonstrate%20the%0Aeffectiveness%20and%20generic%20applicability%20of%20our%20approach%20in%20improving%20the%0Adiscriminative%20capability%20of%20AEs%20for%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05886v1&entry.124074799=Read"},
{"title": "A Robust eLORETA Technique for Localization of Brain Sources in the\n  Presence of Forward Model Uncertainties", "author": "A. Noroozi and M. Ravan and B. Razavi and R. S. Fisher and Y. Law and M. S. Hasan", "abstract": "  In this paper, we present a robust version of the well-known exact\nlow-resolution electromagnetic tomography (eLORETA) technique, named ReLORETA,\nto localize brain sources in the presence of different forward model\nuncertainties. Methods: We first assume that the true lead field matrix is a\ntransformation of the existing lead field matrix distorted by uncertainties and\npropose an iterative approach to estimate this transformation accurately. Major\nsources of the forward model uncertainties, including differences in geometry,\nconductivity, and source space resolution between the real and simulated head\nmodels, and misaligned electrode positions, are then simulated to test the\nproposed method. Results: ReLORETA and eLORETA are applied to simulated focal\nsources in different regions of the brain and the presence of various noise\nlevels as well as real data from a patient with focal epilepsy. The results\nshow that ReLORETA is considerably more robust and accurate than eLORETA in all\ncases. Conclusion: Having successfully dealt with the forward model\nuncertainties, ReLORETA proved to be a promising method for real-world clinical\napplications. Significance: eLORETA is one of the localization techniques that\ncould be used to study brain activity for medical applications such as\ndetermining the epileptogenic zone in patients with medically refractory\nepilepsy. However, the major limitation of eLORETA is sensitivity to the\nuncertainties in the forward model. Since this problem can substantially\nundermine its performance in real-world applications where the exact lead field\nmatrix is unknown, developing a more robust method capable of dealing with\nthese uncertainties is of significant interest.\n", "link": "http://arxiv.org/abs/2405.05790v1", "date": "2024-05-09", "relevancy": 2.0363, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5415}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5186}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robust%20eLORETA%20Technique%20for%20Localization%20of%20Brain%20Sources%20in%20the%0A%20%20Presence%20of%20Forward%20Model%20Uncertainties&body=Title%3A%20A%20Robust%20eLORETA%20Technique%20for%20Localization%20of%20Brain%20Sources%20in%20the%0A%20%20Presence%20of%20Forward%20Model%20Uncertainties%0AAuthor%3A%20A.%20Noroozi%20and%20M.%20Ravan%20and%20B.%20Razavi%20and%20R.%20S.%20Fisher%20and%20Y.%20Law%20and%20M.%20S.%20Hasan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20robust%20version%20of%20the%20well-known%20exact%0Alow-resolution%20electromagnetic%20tomography%20%28eLORETA%29%20technique%2C%20named%20ReLORETA%2C%0Ato%20localize%20brain%20sources%20in%20the%20presence%20of%20different%20forward%20model%0Auncertainties.%20Methods%3A%20We%20first%20assume%20that%20the%20true%20lead%20field%20matrix%20is%20a%0Atransformation%20of%20the%20existing%20lead%20field%20matrix%20distorted%20by%20uncertainties%20and%0Apropose%20an%20iterative%20approach%20to%20estimate%20this%20transformation%20accurately.%20Major%0Asources%20of%20the%20forward%20model%20uncertainties%2C%20including%20differences%20in%20geometry%2C%0Aconductivity%2C%20and%20source%20space%20resolution%20between%20the%20real%20and%20simulated%20head%0Amodels%2C%20and%20misaligned%20electrode%20positions%2C%20are%20then%20simulated%20to%20test%20the%0Aproposed%20method.%20Results%3A%20ReLORETA%20and%20eLORETA%20are%20applied%20to%20simulated%20focal%0Asources%20in%20different%20regions%20of%20the%20brain%20and%20the%20presence%20of%20various%20noise%0Alevels%20as%20well%20as%20real%20data%20from%20a%20patient%20with%20focal%20epilepsy.%20The%20results%0Ashow%20that%20ReLORETA%20is%20considerably%20more%20robust%20and%20accurate%20than%20eLORETA%20in%20all%0Acases.%20Conclusion%3A%20Having%20successfully%20dealt%20with%20the%20forward%20model%0Auncertainties%2C%20ReLORETA%20proved%20to%20be%20a%20promising%20method%20for%20real-world%20clinical%0Aapplications.%20Significance%3A%20eLORETA%20is%20one%20of%20the%20localization%20techniques%20that%0Acould%20be%20used%20to%20study%20brain%20activity%20for%20medical%20applications%20such%20as%0Adetermining%20the%20epileptogenic%20zone%20in%20patients%20with%20medically%20refractory%0Aepilepsy.%20However%2C%20the%20major%20limitation%20of%20eLORETA%20is%20sensitivity%20to%20the%0Auncertainties%20in%20the%20forward%20model.%20Since%20this%20problem%20can%20substantially%0Aundermine%20its%20performance%20in%20real-world%20applications%20where%20the%20exact%20lead%20field%0Amatrix%20is%20unknown%2C%20developing%20a%20more%20robust%20method%20capable%20of%20dealing%20with%0Athese%20uncertainties%20is%20of%20significant%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robust%2520eLORETA%2520Technique%2520for%2520Localization%2520of%2520Brain%2520Sources%2520in%2520the%250A%2520%2520Presence%2520of%2520Forward%2520Model%2520Uncertainties%26entry.906535625%3DA.%2520Noroozi%2520and%2520M.%2520Ravan%2520and%2520B.%2520Razavi%2520and%2520R.%2520S.%2520Fisher%2520and%2520Y.%2520Law%2520and%2520M.%2520S.%2520Hasan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520robust%2520version%2520of%2520the%2520well-known%2520exact%250Alow-resolution%2520electromagnetic%2520tomography%2520%2528eLORETA%2529%2520technique%252C%2520named%2520ReLORETA%252C%250Ato%2520localize%2520brain%2520sources%2520in%2520the%2520presence%2520of%2520different%2520forward%2520model%250Auncertainties.%2520Methods%253A%2520We%2520first%2520assume%2520that%2520the%2520true%2520lead%2520field%2520matrix%2520is%2520a%250Atransformation%2520of%2520the%2520existing%2520lead%2520field%2520matrix%2520distorted%2520by%2520uncertainties%2520and%250Apropose%2520an%2520iterative%2520approach%2520to%2520estimate%2520this%2520transformation%2520accurately.%2520Major%250Asources%2520of%2520the%2520forward%2520model%2520uncertainties%252C%2520including%2520differences%2520in%2520geometry%252C%250Aconductivity%252C%2520and%2520source%2520space%2520resolution%2520between%2520the%2520real%2520and%2520simulated%2520head%250Amodels%252C%2520and%2520misaligned%2520electrode%2520positions%252C%2520are%2520then%2520simulated%2520to%2520test%2520the%250Aproposed%2520method.%2520Results%253A%2520ReLORETA%2520and%2520eLORETA%2520are%2520applied%2520to%2520simulated%2520focal%250Asources%2520in%2520different%2520regions%2520of%2520the%2520brain%2520and%2520the%2520presence%2520of%2520various%2520noise%250Alevels%2520as%2520well%2520as%2520real%2520data%2520from%2520a%2520patient%2520with%2520focal%2520epilepsy.%2520The%2520results%250Ashow%2520that%2520ReLORETA%2520is%2520considerably%2520more%2520robust%2520and%2520accurate%2520than%2520eLORETA%2520in%2520all%250Acases.%2520Conclusion%253A%2520Having%2520successfully%2520dealt%2520with%2520the%2520forward%2520model%250Auncertainties%252C%2520ReLORETA%2520proved%2520to%2520be%2520a%2520promising%2520method%2520for%2520real-world%2520clinical%250Aapplications.%2520Significance%253A%2520eLORETA%2520is%2520one%2520of%2520the%2520localization%2520techniques%2520that%250Acould%2520be%2520used%2520to%2520study%2520brain%2520activity%2520for%2520medical%2520applications%2520such%2520as%250Adetermining%2520the%2520epileptogenic%2520zone%2520in%2520patients%2520with%2520medically%2520refractory%250Aepilepsy.%2520However%252C%2520the%2520major%2520limitation%2520of%2520eLORETA%2520is%2520sensitivity%2520to%2520the%250Auncertainties%2520in%2520the%2520forward%2520model.%2520Since%2520this%2520problem%2520can%2520substantially%250Aundermine%2520its%2520performance%2520in%2520real-world%2520applications%2520where%2520the%2520exact%2520lead%2520field%250Amatrix%2520is%2520unknown%252C%2520developing%2520a%2520more%2520robust%2520method%2520capable%2520of%2520dealing%2520with%250Athese%2520uncertainties%2520is%2520of%2520significant%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robust%20eLORETA%20Technique%20for%20Localization%20of%20Brain%20Sources%20in%20the%0A%20%20Presence%20of%20Forward%20Model%20Uncertainties&entry.906535625=A.%20Noroozi%20and%20M.%20Ravan%20and%20B.%20Razavi%20and%20R.%20S.%20Fisher%20and%20Y.%20Law%20and%20M.%20S.%20Hasan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20robust%20version%20of%20the%20well-known%20exact%0Alow-resolution%20electromagnetic%20tomography%20%28eLORETA%29%20technique%2C%20named%20ReLORETA%2C%0Ato%20localize%20brain%20sources%20in%20the%20presence%20of%20different%20forward%20model%0Auncertainties.%20Methods%3A%20We%20first%20assume%20that%20the%20true%20lead%20field%20matrix%20is%20a%0Atransformation%20of%20the%20existing%20lead%20field%20matrix%20distorted%20by%20uncertainties%20and%0Apropose%20an%20iterative%20approach%20to%20estimate%20this%20transformation%20accurately.%20Major%0Asources%20of%20the%20forward%20model%20uncertainties%2C%20including%20differences%20in%20geometry%2C%0Aconductivity%2C%20and%20source%20space%20resolution%20between%20the%20real%20and%20simulated%20head%0Amodels%2C%20and%20misaligned%20electrode%20positions%2C%20are%20then%20simulated%20to%20test%20the%0Aproposed%20method.%20Results%3A%20ReLORETA%20and%20eLORETA%20are%20applied%20to%20simulated%20focal%0Asources%20in%20different%20regions%20of%20the%20brain%20and%20the%20presence%20of%20various%20noise%0Alevels%20as%20well%20as%20real%20data%20from%20a%20patient%20with%20focal%20epilepsy.%20The%20results%0Ashow%20that%20ReLORETA%20is%20considerably%20more%20robust%20and%20accurate%20than%20eLORETA%20in%20all%0Acases.%20Conclusion%3A%20Having%20successfully%20dealt%20with%20the%20forward%20model%0Auncertainties%2C%20ReLORETA%20proved%20to%20be%20a%20promising%20method%20for%20real-world%20clinical%0Aapplications.%20Significance%3A%20eLORETA%20is%20one%20of%20the%20localization%20techniques%20that%0Acould%20be%20used%20to%20study%20brain%20activity%20for%20medical%20applications%20such%20as%0Adetermining%20the%20epileptogenic%20zone%20in%20patients%20with%20medically%20refractory%0Aepilepsy.%20However%2C%20the%20major%20limitation%20of%20eLORETA%20is%20sensitivity%20to%20the%0Auncertainties%20in%20the%20forward%20model.%20Since%20this%20problem%20can%20substantially%0Aundermine%20its%20performance%20in%20real-world%20applications%20where%20the%20exact%20lead%20field%0Amatrix%20is%20unknown%2C%20developing%20a%20more%20robust%20method%20capable%20of%20dealing%20with%0Athese%20uncertainties%20is%20of%20significant%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05790v1&entry.124074799=Read"},
{"title": "Transformer Architecture for NetsDB", "author": "Subodh Kamble and Kunal Sunil Kasodekar", "abstract": "  Transformers models have become the backbone of the current state-of-the-art\nmodels in language, vision, and multimodal domains. These models, at their\ncore, utilize multi-head self-attention to selectively aggregate context,\ngenerating dynamic contextual embeddings and modeling long-range dependencies\nfor a clear contextual understanding. Lixi et al. \\cite{zhou2022serving}\nproposed a method to use relational databases for deploying large-scale deep\nlearning models and created an open-source implementation called NetsDB for the\nsame. We build upon the previous work of these authors by creating an\nend-to-end implementation of the Encoder part of the transformer for model\nserving in NetsDB. Specifically, we construct a two-block encoder that includes\nMulti-Head Attention and its accompanying self-attention mechanism, Layer-Norm,\nDropout, FeedForward Layers, and the necessary residual connections. We load\nout weights from our model for distributed processing, deployment, and\nefficient inferencing. To prove the efficacy of our implementation, we conduct\na comprehensive performance analysis by comparing it with existing\nimplementations in PyTorch, Tensorflow, Flax, and MxNet across key metrics such\nas inference time and model size.\n", "link": "http://arxiv.org/abs/2405.04807v2", "date": "2024-05-09", "relevancy": 2.036, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.555}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20Architecture%20for%20NetsDB&body=Title%3A%20Transformer%20Architecture%20for%20NetsDB%0AAuthor%3A%20Subodh%20Kamble%20and%20Kunal%20Sunil%20Kasodekar%0AAbstract%3A%20%20%20Transformers%20models%20have%20become%20the%20backbone%20of%20the%20current%20state-of-the-art%0Amodels%20in%20language%2C%20vision%2C%20and%20multimodal%20domains.%20These%20models%2C%20at%20their%0Acore%2C%20utilize%20multi-head%20self-attention%20to%20selectively%20aggregate%20context%2C%0Agenerating%20dynamic%20contextual%20embeddings%20and%20modeling%20long-range%20dependencies%0Afor%20a%20clear%20contextual%20understanding.%20Lixi%20et%20al.%20%5Ccite%7Bzhou2022serving%7D%0Aproposed%20a%20method%20to%20use%20relational%20databases%20for%20deploying%20large-scale%20deep%0Alearning%20models%20and%20created%20an%20open-source%20implementation%20called%20NetsDB%20for%20the%0Asame.%20We%20build%20upon%20the%20previous%20work%20of%20these%20authors%20by%20creating%20an%0Aend-to-end%20implementation%20of%20the%20Encoder%20part%20of%20the%20transformer%20for%20model%0Aserving%20in%20NetsDB.%20Specifically%2C%20we%20construct%20a%20two-block%20encoder%20that%20includes%0AMulti-Head%20Attention%20and%20its%20accompanying%20self-attention%20mechanism%2C%20Layer-Norm%2C%0ADropout%2C%20FeedForward%20Layers%2C%20and%20the%20necessary%20residual%20connections.%20We%20load%0Aout%20weights%20from%20our%20model%20for%20distributed%20processing%2C%20deployment%2C%20and%0Aefficient%20inferencing.%20To%20prove%20the%20efficacy%20of%20our%20implementation%2C%20we%20conduct%0Aa%20comprehensive%20performance%20analysis%20by%20comparing%20it%20with%20existing%0Aimplementations%20in%20PyTorch%2C%20Tensorflow%2C%20Flax%2C%20and%20MxNet%20across%20key%20metrics%20such%0Aas%20inference%20time%20and%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520Architecture%2520for%2520NetsDB%26entry.906535625%3DSubodh%2520Kamble%2520and%2520Kunal%2520Sunil%2520Kasodekar%26entry.1292438233%3D%2520%2520Transformers%2520models%2520have%2520become%2520the%2520backbone%2520of%2520the%2520current%2520state-of-the-art%250Amodels%2520in%2520language%252C%2520vision%252C%2520and%2520multimodal%2520domains.%2520These%2520models%252C%2520at%2520their%250Acore%252C%2520utilize%2520multi-head%2520self-attention%2520to%2520selectively%2520aggregate%2520context%252C%250Agenerating%2520dynamic%2520contextual%2520embeddings%2520and%2520modeling%2520long-range%2520dependencies%250Afor%2520a%2520clear%2520contextual%2520understanding.%2520Lixi%2520et%2520al.%2520%255Ccite%257Bzhou2022serving%257D%250Aproposed%2520a%2520method%2520to%2520use%2520relational%2520databases%2520for%2520deploying%2520large-scale%2520deep%250Alearning%2520models%2520and%2520created%2520an%2520open-source%2520implementation%2520called%2520NetsDB%2520for%2520the%250Asame.%2520We%2520build%2520upon%2520the%2520previous%2520work%2520of%2520these%2520authors%2520by%2520creating%2520an%250Aend-to-end%2520implementation%2520of%2520the%2520Encoder%2520part%2520of%2520the%2520transformer%2520for%2520model%250Aserving%2520in%2520NetsDB.%2520Specifically%252C%2520we%2520construct%2520a%2520two-block%2520encoder%2520that%2520includes%250AMulti-Head%2520Attention%2520and%2520its%2520accompanying%2520self-attention%2520mechanism%252C%2520Layer-Norm%252C%250ADropout%252C%2520FeedForward%2520Layers%252C%2520and%2520the%2520necessary%2520residual%2520connections.%2520We%2520load%250Aout%2520weights%2520from%2520our%2520model%2520for%2520distributed%2520processing%252C%2520deployment%252C%2520and%250Aefficient%2520inferencing.%2520To%2520prove%2520the%2520efficacy%2520of%2520our%2520implementation%252C%2520we%2520conduct%250Aa%2520comprehensive%2520performance%2520analysis%2520by%2520comparing%2520it%2520with%2520existing%250Aimplementations%2520in%2520PyTorch%252C%2520Tensorflow%252C%2520Flax%252C%2520and%2520MxNet%2520across%2520key%2520metrics%2520such%250Aas%2520inference%2520time%2520and%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20Architecture%20for%20NetsDB&entry.906535625=Subodh%20Kamble%20and%20Kunal%20Sunil%20Kasodekar&entry.1292438233=%20%20Transformers%20models%20have%20become%20the%20backbone%20of%20the%20current%20state-of-the-art%0Amodels%20in%20language%2C%20vision%2C%20and%20multimodal%20domains.%20These%20models%2C%20at%20their%0Acore%2C%20utilize%20multi-head%20self-attention%20to%20selectively%20aggregate%20context%2C%0Agenerating%20dynamic%20contextual%20embeddings%20and%20modeling%20long-range%20dependencies%0Afor%20a%20clear%20contextual%20understanding.%20Lixi%20et%20al.%20%5Ccite%7Bzhou2022serving%7D%0Aproposed%20a%20method%20to%20use%20relational%20databases%20for%20deploying%20large-scale%20deep%0Alearning%20models%20and%20created%20an%20open-source%20implementation%20called%20NetsDB%20for%20the%0Asame.%20We%20build%20upon%20the%20previous%20work%20of%20these%20authors%20by%20creating%20an%0Aend-to-end%20implementation%20of%20the%20Encoder%20part%20of%20the%20transformer%20for%20model%0Aserving%20in%20NetsDB.%20Specifically%2C%20we%20construct%20a%20two-block%20encoder%20that%20includes%0AMulti-Head%20Attention%20and%20its%20accompanying%20self-attention%20mechanism%2C%20Layer-Norm%2C%0ADropout%2C%20FeedForward%20Layers%2C%20and%20the%20necessary%20residual%20connections.%20We%20load%0Aout%20weights%20from%20our%20model%20for%20distributed%20processing%2C%20deployment%2C%20and%0Aefficient%20inferencing.%20To%20prove%20the%20efficacy%20of%20our%20implementation%2C%20we%20conduct%0Aa%20comprehensive%20performance%20analysis%20by%20comparing%20it%20with%20existing%0Aimplementations%20in%20PyTorch%2C%20Tensorflow%2C%20Flax%2C%20and%20MxNet%20across%20key%20metrics%20such%0Aas%20inference%20time%20and%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04807v2&entry.124074799=Read"},
{"title": "FER-YOLO-Mamba: Facial Expression Detection and Classification Based on\n  Selective State Space", "author": "Hui Ma and Sen Lei and Turgay Celik and Heng-Chao Li", "abstract": "  Facial Expression Recognition (FER) plays a pivotal role in understanding\nhuman emotional cues. However, traditional FER methods based on visual\ninformation have some limitations, such as preprocessing, feature extraction,\nand multi-stage classification procedures. These not only increase\ncomputational complexity but also require a significant amount of computing\nresources. Considering Convolutional Neural Network (CNN)-based FER schemes\nfrequently prove inadequate in identifying the deep, long-distance dependencies\nembedded within facial expression images, and the Transformer's inherent\nquadratic computational complexity, this paper presents the FER-YOLO-Mamba\nmodel, which integrates the principles of Mamba and YOLO technologies to\nfacilitate efficient coordination in facial expression image recognition and\nlocalization. Within the FER-YOLO-Mamba model, we further devise a FER-YOLO-VSS\ndual-branch module, which combines the inherent strengths of convolutional\nlayers in local feature extraction with the exceptional capability of State\nSpace Models (SSMs) in revealing long-distance dependencies. To the best of our\nknowledge, this is the first Vision Mamba model designed for facial expression\ndetection and classification. To evaluate the performance of the proposed\nFER-YOLO-Mamba model, we conducted experiments on two benchmark datasets,\nRAF-DB and SFEW. The experimental results indicate that the FER-YOLO-Mamba\nmodel achieved better results compared to other models. The code is available\nfrom https://github.com/SwjtuMa/FER-YOLO-Mamba.\n", "link": "http://arxiv.org/abs/2405.01828v2", "date": "2024-05-09", "relevancy": 2.0218, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4965}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FER-YOLO-Mamba%3A%20Facial%20Expression%20Detection%20and%20Classification%20Based%20on%0A%20%20Selective%20State%20Space&body=Title%3A%20FER-YOLO-Mamba%3A%20Facial%20Expression%20Detection%20and%20Classification%20Based%20on%0A%20%20Selective%20State%20Space%0AAuthor%3A%20Hui%20Ma%20and%20Sen%20Lei%20and%20Turgay%20Celik%20and%20Heng-Chao%20Li%0AAbstract%3A%20%20%20Facial%20Expression%20Recognition%20%28FER%29%20plays%20a%20pivotal%20role%20in%20understanding%0Ahuman%20emotional%20cues.%20However%2C%20traditional%20FER%20methods%20based%20on%20visual%0Ainformation%20have%20some%20limitations%2C%20such%20as%20preprocessing%2C%20feature%20extraction%2C%0Aand%20multi-stage%20classification%20procedures.%20These%20not%20only%20increase%0Acomputational%20complexity%20but%20also%20require%20a%20significant%20amount%20of%20computing%0Aresources.%20Considering%20Convolutional%20Neural%20Network%20%28CNN%29-based%20FER%20schemes%0Afrequently%20prove%20inadequate%20in%20identifying%20the%20deep%2C%20long-distance%20dependencies%0Aembedded%20within%20facial%20expression%20images%2C%20and%20the%20Transformer%27s%20inherent%0Aquadratic%20computational%20complexity%2C%20this%20paper%20presents%20the%20FER-YOLO-Mamba%0Amodel%2C%20which%20integrates%20the%20principles%20of%20Mamba%20and%20YOLO%20technologies%20to%0Afacilitate%20efficient%20coordination%20in%20facial%20expression%20image%20recognition%20and%0Alocalization.%20Within%20the%20FER-YOLO-Mamba%20model%2C%20we%20further%20devise%20a%20FER-YOLO-VSS%0Adual-branch%20module%2C%20which%20combines%20the%20inherent%20strengths%20of%20convolutional%0Alayers%20in%20local%20feature%20extraction%20with%20the%20exceptional%20capability%20of%20State%0ASpace%20Models%20%28SSMs%29%20in%20revealing%20long-distance%20dependencies.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20Vision%20Mamba%20model%20designed%20for%20facial%20expression%0Adetection%20and%20classification.%20To%20evaluate%20the%20performance%20of%20the%20proposed%0AFER-YOLO-Mamba%20model%2C%20we%20conducted%20experiments%20on%20two%20benchmark%20datasets%2C%0ARAF-DB%20and%20SFEW.%20The%20experimental%20results%20indicate%20that%20the%20FER-YOLO-Mamba%0Amodel%20achieved%20better%20results%20compared%20to%20other%20models.%20The%20code%20is%20available%0Afrom%20https%3A//github.com/SwjtuMa/FER-YOLO-Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFER-YOLO-Mamba%253A%2520Facial%2520Expression%2520Detection%2520and%2520Classification%2520Based%2520on%250A%2520%2520Selective%2520State%2520Space%26entry.906535625%3DHui%2520Ma%2520and%2520Sen%2520Lei%2520and%2520Turgay%2520Celik%2520and%2520Heng-Chao%2520Li%26entry.1292438233%3D%2520%2520Facial%2520Expression%2520Recognition%2520%2528FER%2529%2520plays%2520a%2520pivotal%2520role%2520in%2520understanding%250Ahuman%2520emotional%2520cues.%2520However%252C%2520traditional%2520FER%2520methods%2520based%2520on%2520visual%250Ainformation%2520have%2520some%2520limitations%252C%2520such%2520as%2520preprocessing%252C%2520feature%2520extraction%252C%250Aand%2520multi-stage%2520classification%2520procedures.%2520These%2520not%2520only%2520increase%250Acomputational%2520complexity%2520but%2520also%2520require%2520a%2520significant%2520amount%2520of%2520computing%250Aresources.%2520Considering%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529-based%2520FER%2520schemes%250Afrequently%2520prove%2520inadequate%2520in%2520identifying%2520the%2520deep%252C%2520long-distance%2520dependencies%250Aembedded%2520within%2520facial%2520expression%2520images%252C%2520and%2520the%2520Transformer%2527s%2520inherent%250Aquadratic%2520computational%2520complexity%252C%2520this%2520paper%2520presents%2520the%2520FER-YOLO-Mamba%250Amodel%252C%2520which%2520integrates%2520the%2520principles%2520of%2520Mamba%2520and%2520YOLO%2520technologies%2520to%250Afacilitate%2520efficient%2520coordination%2520in%2520facial%2520expression%2520image%2520recognition%2520and%250Alocalization.%2520Within%2520the%2520FER-YOLO-Mamba%2520model%252C%2520we%2520further%2520devise%2520a%2520FER-YOLO-VSS%250Adual-branch%2520module%252C%2520which%2520combines%2520the%2520inherent%2520strengths%2520of%2520convolutional%250Alayers%2520in%2520local%2520feature%2520extraction%2520with%2520the%2520exceptional%2520capability%2520of%2520State%250ASpace%2520Models%2520%2528SSMs%2529%2520in%2520revealing%2520long-distance%2520dependencies.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520Vision%2520Mamba%2520model%2520designed%2520for%2520facial%2520expression%250Adetection%2520and%2520classification.%2520To%2520evaluate%2520the%2520performance%2520of%2520the%2520proposed%250AFER-YOLO-Mamba%2520model%252C%2520we%2520conducted%2520experiments%2520on%2520two%2520benchmark%2520datasets%252C%250ARAF-DB%2520and%2520SFEW.%2520The%2520experimental%2520results%2520indicate%2520that%2520the%2520FER-YOLO-Mamba%250Amodel%2520achieved%2520better%2520results%2520compared%2520to%2520other%2520models.%2520The%2520code%2520is%2520available%250Afrom%2520https%253A//github.com/SwjtuMa/FER-YOLO-Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FER-YOLO-Mamba%3A%20Facial%20Expression%20Detection%20and%20Classification%20Based%20on%0A%20%20Selective%20State%20Space&entry.906535625=Hui%20Ma%20and%20Sen%20Lei%20and%20Turgay%20Celik%20and%20Heng-Chao%20Li&entry.1292438233=%20%20Facial%20Expression%20Recognition%20%28FER%29%20plays%20a%20pivotal%20role%20in%20understanding%0Ahuman%20emotional%20cues.%20However%2C%20traditional%20FER%20methods%20based%20on%20visual%0Ainformation%20have%20some%20limitations%2C%20such%20as%20preprocessing%2C%20feature%20extraction%2C%0Aand%20multi-stage%20classification%20procedures.%20These%20not%20only%20increase%0Acomputational%20complexity%20but%20also%20require%20a%20significant%20amount%20of%20computing%0Aresources.%20Considering%20Convolutional%20Neural%20Network%20%28CNN%29-based%20FER%20schemes%0Afrequently%20prove%20inadequate%20in%20identifying%20the%20deep%2C%20long-distance%20dependencies%0Aembedded%20within%20facial%20expression%20images%2C%20and%20the%20Transformer%27s%20inherent%0Aquadratic%20computational%20complexity%2C%20this%20paper%20presents%20the%20FER-YOLO-Mamba%0Amodel%2C%20which%20integrates%20the%20principles%20of%20Mamba%20and%20YOLO%20technologies%20to%0Afacilitate%20efficient%20coordination%20in%20facial%20expression%20image%20recognition%20and%0Alocalization.%20Within%20the%20FER-YOLO-Mamba%20model%2C%20we%20further%20devise%20a%20FER-YOLO-VSS%0Adual-branch%20module%2C%20which%20combines%20the%20inherent%20strengths%20of%20convolutional%0Alayers%20in%20local%20feature%20extraction%20with%20the%20exceptional%20capability%20of%20State%0ASpace%20Models%20%28SSMs%29%20in%20revealing%20long-distance%20dependencies.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20Vision%20Mamba%20model%20designed%20for%20facial%20expression%0Adetection%20and%20classification.%20To%20evaluate%20the%20performance%20of%20the%20proposed%0AFER-YOLO-Mamba%20model%2C%20we%20conducted%20experiments%20on%20two%20benchmark%20datasets%2C%0ARAF-DB%20and%20SFEW.%20The%20experimental%20results%20indicate%20that%20the%20FER-YOLO-Mamba%0Amodel%20achieved%20better%20results%20compared%20to%20other%20models.%20The%20code%20is%20available%0Afrom%20https%3A//github.com/SwjtuMa/FER-YOLO-Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01828v2&entry.124074799=Read"},
{"title": "SlimPajama-DC: Understanding Data Combinations for LLM Training", "author": "Zhiqiang Shen and Tianhua Tao and Liqun Ma and Willie Neiswanger and Zhengzhong Liu and Hongyi Wang and Bowen Tan and Joel Hestness and Natalia Vassilieva and Daria Soboleva and Eric Xing", "abstract": "  This paper aims to understand the impacts of various data combinations (e.g.,\nweb text, Wikipedia, GitHub, books) on the pretraining of large language models\nusing SlimPajama. SlimPajama is a rigorously deduplicated, multi-source\ndataset, which has been refined and further deduplicated to 627B tokens from\nthe extensive 1.2T token RedPajama dataset contributed by Together. We have\ntermed our research as SlimPajama-DC, an empirical analysis designed to uncover\nfundamental characteristics and best practices associated with employing\nSlimPajama in the training of large language models. During our research with\nSlimPajama, two pivotal observations emerged: (1) Global deduplication vs.\nlocal deduplication. We analyze and discuss how global (across different\nsources of datasets) and local (within the single source of dataset)\ndeduplications affect the performance of trained models. (2) Proportions of\nhighly-deduplicated multi-source datasets in the combination. To study this, we\nconstruct six configurations on SlimPajama dataset and train individual ones\nusing 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration\noutperforms the 1.3B model trained on RedPajama using the same number of\ntraining tokens by a significant margin. All our 1.3B models are trained on\nCerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed\nprecision. We further extend our discoveries (such as increasing data diversity\nis crucial after global deduplication) on a 7B model with large batch-size\ntraining. Our SlimPajama-DC models are available at:\nhttps://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC\ndatasets are available at:\nhttps://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.\n", "link": "http://arxiv.org/abs/2309.10818v3", "date": "2024-05-09", "relevancy": 1.9938, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5208}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlimPajama-DC%3A%20Understanding%20Data%20Combinations%20for%20LLM%20Training&body=Title%3A%20SlimPajama-DC%3A%20Understanding%20Data%20Combinations%20for%20LLM%20Training%0AAuthor%3A%20Zhiqiang%20Shen%20and%20Tianhua%20Tao%20and%20Liqun%20Ma%20and%20Willie%20Neiswanger%20and%20Zhengzhong%20Liu%20and%20Hongyi%20Wang%20and%20Bowen%20Tan%20and%20Joel%20Hestness%20and%20Natalia%20Vassilieva%20and%20Daria%20Soboleva%20and%20Eric%20Xing%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20understand%20the%20impacts%20of%20various%20data%20combinations%20%28e.g.%2C%0Aweb%20text%2C%20Wikipedia%2C%20GitHub%2C%20books%29%20on%20the%20pretraining%20of%20large%20language%20models%0Ausing%20SlimPajama.%20SlimPajama%20is%20a%20rigorously%20deduplicated%2C%20multi-source%0Adataset%2C%20which%20has%20been%20refined%20and%20further%20deduplicated%20to%20627B%20tokens%20from%0Athe%20extensive%201.2T%20token%20RedPajama%20dataset%20contributed%20by%20Together.%20We%20have%0Atermed%20our%20research%20as%20SlimPajama-DC%2C%20an%20empirical%20analysis%20designed%20to%20uncover%0Afundamental%20characteristics%20and%20best%20practices%20associated%20with%20employing%0ASlimPajama%20in%20the%20training%20of%20large%20language%20models.%20During%20our%20research%20with%0ASlimPajama%2C%20two%20pivotal%20observations%20emerged%3A%20%281%29%20Global%20deduplication%20vs.%0Alocal%20deduplication.%20We%20analyze%20and%20discuss%20how%20global%20%28across%20different%0Asources%20of%20datasets%29%20and%20local%20%28within%20the%20single%20source%20of%20dataset%29%0Adeduplications%20affect%20the%20performance%20of%20trained%20models.%20%282%29%20Proportions%20of%0Ahighly-deduplicated%20multi-source%20datasets%20in%20the%20combination.%20To%20study%20this%2C%20we%0Aconstruct%20six%20configurations%20on%20SlimPajama%20dataset%20and%20train%20individual%20ones%0Ausing%201.3B%20Cerebras-GPT%20model%20with%20Alibi%20and%20SwiGLU.%20Our%20best%20configuration%0Aoutperforms%20the%201.3B%20model%20trained%20on%20RedPajama%20using%20the%20same%20number%20of%0Atraining%20tokens%20by%20a%20significant%20margin.%20All%20our%201.3B%20models%20are%20trained%20on%0ACerebras%2016%24%5Ctimes%24%20CS-2%20cluster%20with%20a%20total%20of%2080%20PFLOP/s%20in%20bf16%20mixed%0Aprecision.%20We%20further%20extend%20our%20discoveries%20%28such%20as%20increasing%20data%20diversity%0Ais%20crucial%20after%20global%20deduplication%29%20on%20a%207B%20model%20with%20large%20batch-size%0Atraining.%20Our%20SlimPajama-DC%20models%20are%20available%20at%3A%0Ahttps%3A//huggingface.co/MBZUAI-LLM/SlimPajama-DC%20and%20the%20separate%20SlimPajama-DC%0Adatasets%20are%20available%20at%3A%0Ahttps%3A//huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10818v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlimPajama-DC%253A%2520Understanding%2520Data%2520Combinations%2520for%2520LLM%2520Training%26entry.906535625%3DZhiqiang%2520Shen%2520and%2520Tianhua%2520Tao%2520and%2520Liqun%2520Ma%2520and%2520Willie%2520Neiswanger%2520and%2520Zhengzhong%2520Liu%2520and%2520Hongyi%2520Wang%2520and%2520Bowen%2520Tan%2520and%2520Joel%2520Hestness%2520and%2520Natalia%2520Vassilieva%2520and%2520Daria%2520Soboleva%2520and%2520Eric%2520Xing%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520understand%2520the%2520impacts%2520of%2520various%2520data%2520combinations%2520%2528e.g.%252C%250Aweb%2520text%252C%2520Wikipedia%252C%2520GitHub%252C%2520books%2529%2520on%2520the%2520pretraining%2520of%2520large%2520language%2520models%250Ausing%2520SlimPajama.%2520SlimPajama%2520is%2520a%2520rigorously%2520deduplicated%252C%2520multi-source%250Adataset%252C%2520which%2520has%2520been%2520refined%2520and%2520further%2520deduplicated%2520to%2520627B%2520tokens%2520from%250Athe%2520extensive%25201.2T%2520token%2520RedPajama%2520dataset%2520contributed%2520by%2520Together.%2520We%2520have%250Atermed%2520our%2520research%2520as%2520SlimPajama-DC%252C%2520an%2520empirical%2520analysis%2520designed%2520to%2520uncover%250Afundamental%2520characteristics%2520and%2520best%2520practices%2520associated%2520with%2520employing%250ASlimPajama%2520in%2520the%2520training%2520of%2520large%2520language%2520models.%2520During%2520our%2520research%2520with%250ASlimPajama%252C%2520two%2520pivotal%2520observations%2520emerged%253A%2520%25281%2529%2520Global%2520deduplication%2520vs.%250Alocal%2520deduplication.%2520We%2520analyze%2520and%2520discuss%2520how%2520global%2520%2528across%2520different%250Asources%2520of%2520datasets%2529%2520and%2520local%2520%2528within%2520the%2520single%2520source%2520of%2520dataset%2529%250Adeduplications%2520affect%2520the%2520performance%2520of%2520trained%2520models.%2520%25282%2529%2520Proportions%2520of%250Ahighly-deduplicated%2520multi-source%2520datasets%2520in%2520the%2520combination.%2520To%2520study%2520this%252C%2520we%250Aconstruct%2520six%2520configurations%2520on%2520SlimPajama%2520dataset%2520and%2520train%2520individual%2520ones%250Ausing%25201.3B%2520Cerebras-GPT%2520model%2520with%2520Alibi%2520and%2520SwiGLU.%2520Our%2520best%2520configuration%250Aoutperforms%2520the%25201.3B%2520model%2520trained%2520on%2520RedPajama%2520using%2520the%2520same%2520number%2520of%250Atraining%2520tokens%2520by%2520a%2520significant%2520margin.%2520All%2520our%25201.3B%2520models%2520are%2520trained%2520on%250ACerebras%252016%2524%255Ctimes%2524%2520CS-2%2520cluster%2520with%2520a%2520total%2520of%252080%2520PFLOP/s%2520in%2520bf16%2520mixed%250Aprecision.%2520We%2520further%2520extend%2520our%2520discoveries%2520%2528such%2520as%2520increasing%2520data%2520diversity%250Ais%2520crucial%2520after%2520global%2520deduplication%2529%2520on%2520a%25207B%2520model%2520with%2520large%2520batch-size%250Atraining.%2520Our%2520SlimPajama-DC%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//huggingface.co/MBZUAI-LLM/SlimPajama-DC%2520and%2520the%2520separate%2520SlimPajama-DC%250Adatasets%2520are%2520available%2520at%253A%250Ahttps%253A//huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10818v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlimPajama-DC%3A%20Understanding%20Data%20Combinations%20for%20LLM%20Training&entry.906535625=Zhiqiang%20Shen%20and%20Tianhua%20Tao%20and%20Liqun%20Ma%20and%20Willie%20Neiswanger%20and%20Zhengzhong%20Liu%20and%20Hongyi%20Wang%20and%20Bowen%20Tan%20and%20Joel%20Hestness%20and%20Natalia%20Vassilieva%20and%20Daria%20Soboleva%20and%20Eric%20Xing&entry.1292438233=%20%20This%20paper%20aims%20to%20understand%20the%20impacts%20of%20various%20data%20combinations%20%28e.g.%2C%0Aweb%20text%2C%20Wikipedia%2C%20GitHub%2C%20books%29%20on%20the%20pretraining%20of%20large%20language%20models%0Ausing%20SlimPajama.%20SlimPajama%20is%20a%20rigorously%20deduplicated%2C%20multi-source%0Adataset%2C%20which%20has%20been%20refined%20and%20further%20deduplicated%20to%20627B%20tokens%20from%0Athe%20extensive%201.2T%20token%20RedPajama%20dataset%20contributed%20by%20Together.%20We%20have%0Atermed%20our%20research%20as%20SlimPajama-DC%2C%20an%20empirical%20analysis%20designed%20to%20uncover%0Afundamental%20characteristics%20and%20best%20practices%20associated%20with%20employing%0ASlimPajama%20in%20the%20training%20of%20large%20language%20models.%20During%20our%20research%20with%0ASlimPajama%2C%20two%20pivotal%20observations%20emerged%3A%20%281%29%20Global%20deduplication%20vs.%0Alocal%20deduplication.%20We%20analyze%20and%20discuss%20how%20global%20%28across%20different%0Asources%20of%20datasets%29%20and%20local%20%28within%20the%20single%20source%20of%20dataset%29%0Adeduplications%20affect%20the%20performance%20of%20trained%20models.%20%282%29%20Proportions%20of%0Ahighly-deduplicated%20multi-source%20datasets%20in%20the%20combination.%20To%20study%20this%2C%20we%0Aconstruct%20six%20configurations%20on%20SlimPajama%20dataset%20and%20train%20individual%20ones%0Ausing%201.3B%20Cerebras-GPT%20model%20with%20Alibi%20and%20SwiGLU.%20Our%20best%20configuration%0Aoutperforms%20the%201.3B%20model%20trained%20on%20RedPajama%20using%20the%20same%20number%20of%0Atraining%20tokens%20by%20a%20significant%20margin.%20All%20our%201.3B%20models%20are%20trained%20on%0ACerebras%2016%24%5Ctimes%24%20CS-2%20cluster%20with%20a%20total%20of%2080%20PFLOP/s%20in%20bf16%20mixed%0Aprecision.%20We%20further%20extend%20our%20discoveries%20%28such%20as%20increasing%20data%20diversity%0Ais%20crucial%20after%20global%20deduplication%29%20on%20a%207B%20model%20with%20large%20batch-size%0Atraining.%20Our%20SlimPajama-DC%20models%20are%20available%20at%3A%0Ahttps%3A//huggingface.co/MBZUAI-LLM/SlimPajama-DC%20and%20the%20separate%20SlimPajama-DC%0Adatasets%20are%20available%20at%3A%0Ahttps%3A//huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10818v3&entry.124074799=Read"},
{"title": "Deep Diversity-Enhanced Feature Representation of Hyperspectral Images", "author": "Jinhui Hou and Zhiyu Zhu and Junhui Hou and Hui Liu and Huanqiang Zeng and Deyu Meng", "abstract": "  In this paper, we study the problem of efficiently and effectively embedding\nthe high-dimensional spatio-spectral information of hyperspectral (HS) images,\nguided by feature diversity. Specifically, based on the theoretical formulation\nthat feature diversity is correlated with the rank of the unfolded kernel\nmatrix, we rectify 3D convolution by modifying its topology to enhance the rank\nupper-bound. This modification yields a rank-enhanced spatial-spectral\nsymmetrical convolution set (ReS$^3$-ConvSet), which not only learns diverse\nand powerful feature representations but also saves network parameters.\nAdditionally, we also propose a novel diversity-aware regularization (DA-Reg)\nterm that directly acts on the feature maps to maximize independence among\nelements. To demonstrate the superiority of the proposed ReS$^3$-ConvSet and\nDA-Reg, we apply them to various HS image processing and analysis tasks,\nincluding denoising, spatial super-resolution, and classification. Extensive\nexperiments show that the proposed approaches outperform state-of-the-art\nmethods both quantitatively and qualitatively to a significant extent. The code\nis publicly available at https://github.com/jinnh/ReSSS-ConvSet.\n", "link": "http://arxiv.org/abs/2301.06132v3", "date": "2024-05-09", "relevancy": 1.9888, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5132}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5085}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Diversity-Enhanced%20Feature%20Representation%20of%20Hyperspectral%20Images&body=Title%3A%20Deep%20Diversity-Enhanced%20Feature%20Representation%20of%20Hyperspectral%20Images%0AAuthor%3A%20Jinhui%20Hou%20and%20Zhiyu%20Zhu%20and%20Junhui%20Hou%20and%20Hui%20Liu%20and%20Huanqiang%20Zeng%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20efficiently%20and%20effectively%20embedding%0Athe%20high-dimensional%20spatio-spectral%20information%20of%20hyperspectral%20%28HS%29%20images%2C%0Aguided%20by%20feature%20diversity.%20Specifically%2C%20based%20on%20the%20theoretical%20formulation%0Athat%20feature%20diversity%20is%20correlated%20with%20the%20rank%20of%20the%20unfolded%20kernel%0Amatrix%2C%20we%20rectify%203D%20convolution%20by%20modifying%20its%20topology%20to%20enhance%20the%20rank%0Aupper-bound.%20This%20modification%20yields%20a%20rank-enhanced%20spatial-spectral%0Asymmetrical%20convolution%20set%20%28ReS%24%5E3%24-ConvSet%29%2C%20which%20not%20only%20learns%20diverse%0Aand%20powerful%20feature%20representations%20but%20also%20saves%20network%20parameters.%0AAdditionally%2C%20we%20also%20propose%20a%20novel%20diversity-aware%20regularization%20%28DA-Reg%29%0Aterm%20that%20directly%20acts%20on%20the%20feature%20maps%20to%20maximize%20independence%20among%0Aelements.%20To%20demonstrate%20the%20superiority%20of%20the%20proposed%20ReS%24%5E3%24-ConvSet%20and%0ADA-Reg%2C%20we%20apply%20them%20to%20various%20HS%20image%20processing%20and%20analysis%20tasks%2C%0Aincluding%20denoising%2C%20spatial%20super-resolution%2C%20and%20classification.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20approaches%20outperform%20state-of-the-art%0Amethods%20both%20quantitatively%20and%20qualitatively%20to%20a%20significant%20extent.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/jinnh/ReSSS-ConvSet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.06132v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Diversity-Enhanced%2520Feature%2520Representation%2520of%2520Hyperspectral%2520Images%26entry.906535625%3DJinhui%2520Hou%2520and%2520Zhiyu%2520Zhu%2520and%2520Junhui%2520Hou%2520and%2520Hui%2520Liu%2520and%2520Huanqiang%2520Zeng%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520problem%2520of%2520efficiently%2520and%2520effectively%2520embedding%250Athe%2520high-dimensional%2520spatio-spectral%2520information%2520of%2520hyperspectral%2520%2528HS%2529%2520images%252C%250Aguided%2520by%2520feature%2520diversity.%2520Specifically%252C%2520based%2520on%2520the%2520theoretical%2520formulation%250Athat%2520feature%2520diversity%2520is%2520correlated%2520with%2520the%2520rank%2520of%2520the%2520unfolded%2520kernel%250Amatrix%252C%2520we%2520rectify%25203D%2520convolution%2520by%2520modifying%2520its%2520topology%2520to%2520enhance%2520the%2520rank%250Aupper-bound.%2520This%2520modification%2520yields%2520a%2520rank-enhanced%2520spatial-spectral%250Asymmetrical%2520convolution%2520set%2520%2528ReS%2524%255E3%2524-ConvSet%2529%252C%2520which%2520not%2520only%2520learns%2520diverse%250Aand%2520powerful%2520feature%2520representations%2520but%2520also%2520saves%2520network%2520parameters.%250AAdditionally%252C%2520we%2520also%2520propose%2520a%2520novel%2520diversity-aware%2520regularization%2520%2528DA-Reg%2529%250Aterm%2520that%2520directly%2520acts%2520on%2520the%2520feature%2520maps%2520to%2520maximize%2520independence%2520among%250Aelements.%2520To%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520ReS%2524%255E3%2524-ConvSet%2520and%250ADA-Reg%252C%2520we%2520apply%2520them%2520to%2520various%2520HS%2520image%2520processing%2520and%2520analysis%2520tasks%252C%250Aincluding%2520denoising%252C%2520spatial%2520super-resolution%252C%2520and%2520classification.%2520Extensive%250Aexperiments%2520show%2520that%2520the%2520proposed%2520approaches%2520outperform%2520state-of-the-art%250Amethods%2520both%2520quantitatively%2520and%2520qualitatively%2520to%2520a%2520significant%2520extent.%2520The%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/jinnh/ReSSS-ConvSet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.06132v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Diversity-Enhanced%20Feature%20Representation%20of%20Hyperspectral%20Images&entry.906535625=Jinhui%20Hou%20and%20Zhiyu%20Zhu%20and%20Junhui%20Hou%20and%20Hui%20Liu%20and%20Huanqiang%20Zeng%20and%20Deyu%20Meng&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20problem%20of%20efficiently%20and%20effectively%20embedding%0Athe%20high-dimensional%20spatio-spectral%20information%20of%20hyperspectral%20%28HS%29%20images%2C%0Aguided%20by%20feature%20diversity.%20Specifically%2C%20based%20on%20the%20theoretical%20formulation%0Athat%20feature%20diversity%20is%20correlated%20with%20the%20rank%20of%20the%20unfolded%20kernel%0Amatrix%2C%20we%20rectify%203D%20convolution%20by%20modifying%20its%20topology%20to%20enhance%20the%20rank%0Aupper-bound.%20This%20modification%20yields%20a%20rank-enhanced%20spatial-spectral%0Asymmetrical%20convolution%20set%20%28ReS%24%5E3%24-ConvSet%29%2C%20which%20not%20only%20learns%20diverse%0Aand%20powerful%20feature%20representations%20but%20also%20saves%20network%20parameters.%0AAdditionally%2C%20we%20also%20propose%20a%20novel%20diversity-aware%20regularization%20%28DA-Reg%29%0Aterm%20that%20directly%20acts%20on%20the%20feature%20maps%20to%20maximize%20independence%20among%0Aelements.%20To%20demonstrate%20the%20superiority%20of%20the%20proposed%20ReS%24%5E3%24-ConvSet%20and%0ADA-Reg%2C%20we%20apply%20them%20to%20various%20HS%20image%20processing%20and%20analysis%20tasks%2C%0Aincluding%20denoising%2C%20spatial%20super-resolution%2C%20and%20classification.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20approaches%20outperform%20state-of-the-art%0Amethods%20both%20quantitatively%20and%20qualitatively%20to%20a%20significant%20extent.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/jinnh/ReSSS-ConvSet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.06132v3&entry.124074799=Read"},
{"title": "Theoretical Guarantees of Data Augmented Last Layer Retraining Methods", "author": "Monica Welfert and Nathan Stromberg and Lalitha Sankar", "abstract": "  Ensuring fair predictions across many distinct subpopulations in the training\ndata can be prohibitive for large models. Recently, simple linear last layer\nretraining strategies, in combination with data augmentation methods such as\nupweighting, downsampling and mixup, have been shown to achieve\nstate-of-the-art performance for worst-group accuracy, which quantifies\naccuracy for the least prevalent subpopulation. For linear last layer\nretraining and the abovementioned augmentations, we present the optimal\nworst-group accuracy when modeling the distribution of the latent\nrepresentations (input to the last layer) as Gaussian for each subpopulation.\nWe evaluate and verify our results for both synthetic and large publicly\navailable datasets.\n", "link": "http://arxiv.org/abs/2405.05934v1", "date": "2024-05-09", "relevancy": 1.9878, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5096}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4947}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Guarantees%20of%20Data%20Augmented%20Last%20Layer%20Retraining%20Methods&body=Title%3A%20Theoretical%20Guarantees%20of%20Data%20Augmented%20Last%20Layer%20Retraining%20Methods%0AAuthor%3A%20Monica%20Welfert%20and%20Nathan%20Stromberg%20and%20Lalitha%20Sankar%0AAbstract%3A%20%20%20Ensuring%20fair%20predictions%20across%20many%20distinct%20subpopulations%20in%20the%20training%0Adata%20can%20be%20prohibitive%20for%20large%20models.%20Recently%2C%20simple%20linear%20last%20layer%0Aretraining%20strategies%2C%20in%20combination%20with%20data%20augmentation%20methods%20such%20as%0Aupweighting%2C%20downsampling%20and%20mixup%2C%20have%20been%20shown%20to%20achieve%0Astate-of-the-art%20performance%20for%20worst-group%20accuracy%2C%20which%20quantifies%0Aaccuracy%20for%20the%20least%20prevalent%20subpopulation.%20For%20linear%20last%20layer%0Aretraining%20and%20the%20abovementioned%20augmentations%2C%20we%20present%20the%20optimal%0Aworst-group%20accuracy%20when%20modeling%20the%20distribution%20of%20the%20latent%0Arepresentations%20%28input%20to%20the%20last%20layer%29%20as%20Gaussian%20for%20each%20subpopulation.%0AWe%20evaluate%20and%20verify%20our%20results%20for%20both%20synthetic%20and%20large%20publicly%0Aavailable%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Guarantees%2520of%2520Data%2520Augmented%2520Last%2520Layer%2520Retraining%2520Methods%26entry.906535625%3DMonica%2520Welfert%2520and%2520Nathan%2520Stromberg%2520and%2520Lalitha%2520Sankar%26entry.1292438233%3D%2520%2520Ensuring%2520fair%2520predictions%2520across%2520many%2520distinct%2520subpopulations%2520in%2520the%2520training%250Adata%2520can%2520be%2520prohibitive%2520for%2520large%2520models.%2520Recently%252C%2520simple%2520linear%2520last%2520layer%250Aretraining%2520strategies%252C%2520in%2520combination%2520with%2520data%2520augmentation%2520methods%2520such%2520as%250Aupweighting%252C%2520downsampling%2520and%2520mixup%252C%2520have%2520been%2520shown%2520to%2520achieve%250Astate-of-the-art%2520performance%2520for%2520worst-group%2520accuracy%252C%2520which%2520quantifies%250Aaccuracy%2520for%2520the%2520least%2520prevalent%2520subpopulation.%2520For%2520linear%2520last%2520layer%250Aretraining%2520and%2520the%2520abovementioned%2520augmentations%252C%2520we%2520present%2520the%2520optimal%250Aworst-group%2520accuracy%2520when%2520modeling%2520the%2520distribution%2520of%2520the%2520latent%250Arepresentations%2520%2528input%2520to%2520the%2520last%2520layer%2529%2520as%2520Gaussian%2520for%2520each%2520subpopulation.%250AWe%2520evaluate%2520and%2520verify%2520our%2520results%2520for%2520both%2520synthetic%2520and%2520large%2520publicly%250Aavailable%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Guarantees%20of%20Data%20Augmented%20Last%20Layer%20Retraining%20Methods&entry.906535625=Monica%20Welfert%20and%20Nathan%20Stromberg%20and%20Lalitha%20Sankar&entry.1292438233=%20%20Ensuring%20fair%20predictions%20across%20many%20distinct%20subpopulations%20in%20the%20training%0Adata%20can%20be%20prohibitive%20for%20large%20models.%20Recently%2C%20simple%20linear%20last%20layer%0Aretraining%20strategies%2C%20in%20combination%20with%20data%20augmentation%20methods%20such%20as%0Aupweighting%2C%20downsampling%20and%20mixup%2C%20have%20been%20shown%20to%20achieve%0Astate-of-the-art%20performance%20for%20worst-group%20accuracy%2C%20which%20quantifies%0Aaccuracy%20for%20the%20least%20prevalent%20subpopulation.%20For%20linear%20last%20layer%0Aretraining%20and%20the%20abovementioned%20augmentations%2C%20we%20present%20the%20optimal%0Aworst-group%20accuracy%20when%20modeling%20the%20distribution%20of%20the%20latent%0Arepresentations%20%28input%20to%20the%20last%20layer%29%20as%20Gaussian%20for%20each%20subpopulation.%0AWe%20evaluate%20and%20verify%20our%20results%20for%20both%20synthetic%20and%20large%20publicly%0Aavailable%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05934v1&entry.124074799=Read"},
{"title": "Autonomous Robotic Ultrasound System for Liver Follow-up Diagnosis:\n  Pilot Phantom Study", "author": "Tianpeng Zhang and Sekeun Kim and Jerome Charton and Haitong Ma and Kyungsang Kim and Na Li and Quanzheng Li", "abstract": "  The paper introduces a novel autonomous robot ultrasound (US) system\ntargeting liver follow-up scans for outpatients in local communities. Given a\ncomputed tomography (CT) image with specific target regions of interest, the\nproposed system carries out the autonomous follow-up scan in three steps: (i)\ninitial robot contact to surface, (ii) coordinate mapping between CT image and\nrobot, and (iii) target US scan. Utilizing 3D US-CT registration and deep\nlearning-based segmentation networks, we can achieve precise imaging of 3D\nhepatic veins, facilitating accurate coordinate mapping between CT and the\nrobot. This enables the automatic localization of follow-up targets within the\nCT image, allowing the robot to navigate precisely to the target's surface.\nEvaluation of the ultrasound phantom confirms the quality of the US-CT\nregistration and shows the robot reliably locates the targets in repeated\ntrials. The proposed framework holds the potential to significantly reduce time\nand costs for healthcare providers, clinicians, and follow-up patients, thereby\naddressing the increasing healthcare burden associated with chronic disease in\nlocal communities.\n", "link": "http://arxiv.org/abs/2405.05787v1", "date": "2024-05-09", "relevancy": 1.9812, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5064}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5028}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%20Robotic%20Ultrasound%20System%20for%20Liver%20Follow-up%20Diagnosis%3A%0A%20%20Pilot%20Phantom%20Study&body=Title%3A%20Autonomous%20Robotic%20Ultrasound%20System%20for%20Liver%20Follow-up%20Diagnosis%3A%0A%20%20Pilot%20Phantom%20Study%0AAuthor%3A%20Tianpeng%20Zhang%20and%20Sekeun%20Kim%20and%20Jerome%20Charton%20and%20Haitong%20Ma%20and%20Kyungsang%20Kim%20and%20Na%20Li%20and%20Quanzheng%20Li%0AAbstract%3A%20%20%20The%20paper%20introduces%20a%20novel%20autonomous%20robot%20ultrasound%20%28US%29%20system%0Atargeting%20liver%20follow-up%20scans%20for%20outpatients%20in%20local%20communities.%20Given%20a%0Acomputed%20tomography%20%28CT%29%20image%20with%20specific%20target%20regions%20of%20interest%2C%20the%0Aproposed%20system%20carries%20out%20the%20autonomous%20follow-up%20scan%20in%20three%20steps%3A%20%28i%29%0Ainitial%20robot%20contact%20to%20surface%2C%20%28ii%29%20coordinate%20mapping%20between%20CT%20image%20and%0Arobot%2C%20and%20%28iii%29%20target%20US%20scan.%20Utilizing%203D%20US-CT%20registration%20and%20deep%0Alearning-based%20segmentation%20networks%2C%20we%20can%20achieve%20precise%20imaging%20of%203D%0Ahepatic%20veins%2C%20facilitating%20accurate%20coordinate%20mapping%20between%20CT%20and%20the%0Arobot.%20This%20enables%20the%20automatic%20localization%20of%20follow-up%20targets%20within%20the%0ACT%20image%2C%20allowing%20the%20robot%20to%20navigate%20precisely%20to%20the%20target%27s%20surface.%0AEvaluation%20of%20the%20ultrasound%20phantom%20confirms%20the%20quality%20of%20the%20US-CT%0Aregistration%20and%20shows%20the%20robot%20reliably%20locates%20the%20targets%20in%20repeated%0Atrials.%20The%20proposed%20framework%20holds%20the%20potential%20to%20significantly%20reduce%20time%0Aand%20costs%20for%20healthcare%20providers%2C%20clinicians%2C%20and%20follow-up%20patients%2C%20thereby%0Aaddressing%20the%20increasing%20healthcare%20burden%20associated%20with%20chronic%20disease%20in%0Alocal%20communities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%2520Robotic%2520Ultrasound%2520System%2520for%2520Liver%2520Follow-up%2520Diagnosis%253A%250A%2520%2520Pilot%2520Phantom%2520Study%26entry.906535625%3DTianpeng%2520Zhang%2520and%2520Sekeun%2520Kim%2520and%2520Jerome%2520Charton%2520and%2520Haitong%2520Ma%2520and%2520Kyungsang%2520Kim%2520and%2520Na%2520Li%2520and%2520Quanzheng%2520Li%26entry.1292438233%3D%2520%2520The%2520paper%2520introduces%2520a%2520novel%2520autonomous%2520robot%2520ultrasound%2520%2528US%2529%2520system%250Atargeting%2520liver%2520follow-up%2520scans%2520for%2520outpatients%2520in%2520local%2520communities.%2520Given%2520a%250Acomputed%2520tomography%2520%2528CT%2529%2520image%2520with%2520specific%2520target%2520regions%2520of%2520interest%252C%2520the%250Aproposed%2520system%2520carries%2520out%2520the%2520autonomous%2520follow-up%2520scan%2520in%2520three%2520steps%253A%2520%2528i%2529%250Ainitial%2520robot%2520contact%2520to%2520surface%252C%2520%2528ii%2529%2520coordinate%2520mapping%2520between%2520CT%2520image%2520and%250Arobot%252C%2520and%2520%2528iii%2529%2520target%2520US%2520scan.%2520Utilizing%25203D%2520US-CT%2520registration%2520and%2520deep%250Alearning-based%2520segmentation%2520networks%252C%2520we%2520can%2520achieve%2520precise%2520imaging%2520of%25203D%250Ahepatic%2520veins%252C%2520facilitating%2520accurate%2520coordinate%2520mapping%2520between%2520CT%2520and%2520the%250Arobot.%2520This%2520enables%2520the%2520automatic%2520localization%2520of%2520follow-up%2520targets%2520within%2520the%250ACT%2520image%252C%2520allowing%2520the%2520robot%2520to%2520navigate%2520precisely%2520to%2520the%2520target%2527s%2520surface.%250AEvaluation%2520of%2520the%2520ultrasound%2520phantom%2520confirms%2520the%2520quality%2520of%2520the%2520US-CT%250Aregistration%2520and%2520shows%2520the%2520robot%2520reliably%2520locates%2520the%2520targets%2520in%2520repeated%250Atrials.%2520The%2520proposed%2520framework%2520holds%2520the%2520potential%2520to%2520significantly%2520reduce%2520time%250Aand%2520costs%2520for%2520healthcare%2520providers%252C%2520clinicians%252C%2520and%2520follow-up%2520patients%252C%2520thereby%250Aaddressing%2520the%2520increasing%2520healthcare%2520burden%2520associated%2520with%2520chronic%2520disease%2520in%250Alocal%2520communities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%20Robotic%20Ultrasound%20System%20for%20Liver%20Follow-up%20Diagnosis%3A%0A%20%20Pilot%20Phantom%20Study&entry.906535625=Tianpeng%20Zhang%20and%20Sekeun%20Kim%20and%20Jerome%20Charton%20and%20Haitong%20Ma%20and%20Kyungsang%20Kim%20and%20Na%20Li%20and%20Quanzheng%20Li&entry.1292438233=%20%20The%20paper%20introduces%20a%20novel%20autonomous%20robot%20ultrasound%20%28US%29%20system%0Atargeting%20liver%20follow-up%20scans%20for%20outpatients%20in%20local%20communities.%20Given%20a%0Acomputed%20tomography%20%28CT%29%20image%20with%20specific%20target%20regions%20of%20interest%2C%20the%0Aproposed%20system%20carries%20out%20the%20autonomous%20follow-up%20scan%20in%20three%20steps%3A%20%28i%29%0Ainitial%20robot%20contact%20to%20surface%2C%20%28ii%29%20coordinate%20mapping%20between%20CT%20image%20and%0Arobot%2C%20and%20%28iii%29%20target%20US%20scan.%20Utilizing%203D%20US-CT%20registration%20and%20deep%0Alearning-based%20segmentation%20networks%2C%20we%20can%20achieve%20precise%20imaging%20of%203D%0Ahepatic%20veins%2C%20facilitating%20accurate%20coordinate%20mapping%20between%20CT%20and%20the%0Arobot.%20This%20enables%20the%20automatic%20localization%20of%20follow-up%20targets%20within%20the%0ACT%20image%2C%20allowing%20the%20robot%20to%20navigate%20precisely%20to%20the%20target%27s%20surface.%0AEvaluation%20of%20the%20ultrasound%20phantom%20confirms%20the%20quality%20of%20the%20US-CT%0Aregistration%20and%20shows%20the%20robot%20reliably%20locates%20the%20targets%20in%20repeated%0Atrials.%20The%20proposed%20framework%20holds%20the%20potential%20to%20significantly%20reduce%20time%0Aand%20costs%20for%20healthcare%20providers%2C%20clinicians%2C%20and%20follow-up%20patients%2C%20thereby%0Aaddressing%20the%20increasing%20healthcare%20burden%20associated%20with%20chronic%20disease%20in%0Alocal%20communities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05787v1&entry.124074799=Read"},
{"title": "Gland Segmentation Via Dual Encoders and Boundary-Enhanced Attention", "author": "Huadeng Wang and Jiejiang Yu and Bingbing Li and Xipeng Pan and Zhenbing Liu and Rushi Lan and Xiaonan Luo", "abstract": "  Accurate and automated gland segmentation on pathological images can assist\npathologists in diagnosing the malignancy of colorectal adenocarcinoma.\nHowever, due to various gland shapes, severe deformation of malignant glands,\nand overlapping adhesions between glands. Gland segmentation has always been\nvery challenging. To address these problems, we propose a DEA model. This model\nconsists of two branches: the backbone encoding and decoding network and the\nlocal semantic extraction network. The backbone encoding and decoding network\nextracts advanced Semantic features, uses the proposed feature decoder to\nrestore feature space information, and then enhances the boundary features of\nthe gland through boundary enhancement attention. The local semantic extraction\nnetwork uses the pre-trained DeepLabv3+ as a Local semantic-guided encoder to\nrealize the extraction of edge features. Experimental results on two public\ndatasets, GlaS and CRAG, confirm that the performance of our method is better\nthan other gland segmentation methods.\n", "link": "http://arxiv.org/abs/2401.15990v2", "date": "2024-05-09", "relevancy": 1.9805, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5038}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4903}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gland%20Segmentation%20Via%20Dual%20Encoders%20and%20Boundary-Enhanced%20Attention&body=Title%3A%20Gland%20Segmentation%20Via%20Dual%20Encoders%20and%20Boundary-Enhanced%20Attention%0AAuthor%3A%20Huadeng%20Wang%20and%20Jiejiang%20Yu%20and%20Bingbing%20Li%20and%20Xipeng%20Pan%20and%20Zhenbing%20Liu%20and%20Rushi%20Lan%20and%20Xiaonan%20Luo%0AAbstract%3A%20%20%20Accurate%20and%20automated%20gland%20segmentation%20on%20pathological%20images%20can%20assist%0Apathologists%20in%20diagnosing%20the%20malignancy%20of%20colorectal%20adenocarcinoma.%0AHowever%2C%20due%20to%20various%20gland%20shapes%2C%20severe%20deformation%20of%20malignant%20glands%2C%0Aand%20overlapping%20adhesions%20between%20glands.%20Gland%20segmentation%20has%20always%20been%0Avery%20challenging.%20To%20address%20these%20problems%2C%20we%20propose%20a%20DEA%20model.%20This%20model%0Aconsists%20of%20two%20branches%3A%20the%20backbone%20encoding%20and%20decoding%20network%20and%20the%0Alocal%20semantic%20extraction%20network.%20The%20backbone%20encoding%20and%20decoding%20network%0Aextracts%20advanced%20Semantic%20features%2C%20uses%20the%20proposed%20feature%20decoder%20to%0Arestore%20feature%20space%20information%2C%20and%20then%20enhances%20the%20boundary%20features%20of%0Athe%20gland%20through%20boundary%20enhancement%20attention.%20The%20local%20semantic%20extraction%0Anetwork%20uses%20the%20pre-trained%20DeepLabv3%2B%20as%20a%20Local%20semantic-guided%20encoder%20to%0Arealize%20the%20extraction%20of%20edge%20features.%20Experimental%20results%20on%20two%20public%0Adatasets%2C%20GlaS%20and%20CRAG%2C%20confirm%20that%20the%20performance%20of%20our%20method%20is%20better%0Athan%20other%20gland%20segmentation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.15990v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGland%2520Segmentation%2520Via%2520Dual%2520Encoders%2520and%2520Boundary-Enhanced%2520Attention%26entry.906535625%3DHuadeng%2520Wang%2520and%2520Jiejiang%2520Yu%2520and%2520Bingbing%2520Li%2520and%2520Xipeng%2520Pan%2520and%2520Zhenbing%2520Liu%2520and%2520Rushi%2520Lan%2520and%2520Xiaonan%2520Luo%26entry.1292438233%3D%2520%2520Accurate%2520and%2520automated%2520gland%2520segmentation%2520on%2520pathological%2520images%2520can%2520assist%250Apathologists%2520in%2520diagnosing%2520the%2520malignancy%2520of%2520colorectal%2520adenocarcinoma.%250AHowever%252C%2520due%2520to%2520various%2520gland%2520shapes%252C%2520severe%2520deformation%2520of%2520malignant%2520glands%252C%250Aand%2520overlapping%2520adhesions%2520between%2520glands.%2520Gland%2520segmentation%2520has%2520always%2520been%250Avery%2520challenging.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%2520DEA%2520model.%2520This%2520model%250Aconsists%2520of%2520two%2520branches%253A%2520the%2520backbone%2520encoding%2520and%2520decoding%2520network%2520and%2520the%250Alocal%2520semantic%2520extraction%2520network.%2520The%2520backbone%2520encoding%2520and%2520decoding%2520network%250Aextracts%2520advanced%2520Semantic%2520features%252C%2520uses%2520the%2520proposed%2520feature%2520decoder%2520to%250Arestore%2520feature%2520space%2520information%252C%2520and%2520then%2520enhances%2520the%2520boundary%2520features%2520of%250Athe%2520gland%2520through%2520boundary%2520enhancement%2520attention.%2520The%2520local%2520semantic%2520extraction%250Anetwork%2520uses%2520the%2520pre-trained%2520DeepLabv3%252B%2520as%2520a%2520Local%2520semantic-guided%2520encoder%2520to%250Arealize%2520the%2520extraction%2520of%2520edge%2520features.%2520Experimental%2520results%2520on%2520two%2520public%250Adatasets%252C%2520GlaS%2520and%2520CRAG%252C%2520confirm%2520that%2520the%2520performance%2520of%2520our%2520method%2520is%2520better%250Athan%2520other%2520gland%2520segmentation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.15990v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gland%20Segmentation%20Via%20Dual%20Encoders%20and%20Boundary-Enhanced%20Attention&entry.906535625=Huadeng%20Wang%20and%20Jiejiang%20Yu%20and%20Bingbing%20Li%20and%20Xipeng%20Pan%20and%20Zhenbing%20Liu%20and%20Rushi%20Lan%20and%20Xiaonan%20Luo&entry.1292438233=%20%20Accurate%20and%20automated%20gland%20segmentation%20on%20pathological%20images%20can%20assist%0Apathologists%20in%20diagnosing%20the%20malignancy%20of%20colorectal%20adenocarcinoma.%0AHowever%2C%20due%20to%20various%20gland%20shapes%2C%20severe%20deformation%20of%20malignant%20glands%2C%0Aand%20overlapping%20adhesions%20between%20glands.%20Gland%20segmentation%20has%20always%20been%0Avery%20challenging.%20To%20address%20these%20problems%2C%20we%20propose%20a%20DEA%20model.%20This%20model%0Aconsists%20of%20two%20branches%3A%20the%20backbone%20encoding%20and%20decoding%20network%20and%20the%0Alocal%20semantic%20extraction%20network.%20The%20backbone%20encoding%20and%20decoding%20network%0Aextracts%20advanced%20Semantic%20features%2C%20uses%20the%20proposed%20feature%20decoder%20to%0Arestore%20feature%20space%20information%2C%20and%20then%20enhances%20the%20boundary%20features%20of%0Athe%20gland%20through%20boundary%20enhancement%20attention.%20The%20local%20semantic%20extraction%0Anetwork%20uses%20the%20pre-trained%20DeepLabv3%2B%20as%20a%20Local%20semantic-guided%20encoder%20to%0Arealize%20the%20extraction%20of%20edge%20features.%20Experimental%20results%20on%20two%20public%0Adatasets%2C%20GlaS%20and%20CRAG%2C%20confirm%20that%20the%20performance%20of%20our%20method%20is%20better%0Athan%20other%20gland%20segmentation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.15990v2&entry.124074799=Read"},
{"title": "Guess the Drift with LOP-UKF: LiDAR Odometry and Pacejka Model for\n  Real-Time Racecar Sideslip Estimation", "author": "Alessandro Toschi and Nicola Musiu and Francesco Gatti and Ayoub Raji and Francesco Amerotti and Micaela Verucchi and Marko Bertogna", "abstract": "  The sideslip angle, crucial for vehicle safety and stability, is determined\nusing both longitudinal and lateral velocities. However, measuring the lateral\ncomponent often necessitates costly sensors, leading to its common estimation,\na topic thoroughly explored in existing literature. This paper introduces\nLOP-UKF, a novel method for estimating vehicle lateral velocity by integrating\nLidar Odometry with the Pacejka tire model predictions, resulting in a robust\nestimation via an Unscendent Kalman Filter (UKF). This combination represents a\ndistinct alternative to more traditional methodologies, resulting in a reliable\nsolution also in edge cases. We present experimental results obtained using the\nDallara AV-21 across diverse circuits and track conditions, demonstrating the\neffectiveness of our method.\n", "link": "http://arxiv.org/abs/2405.05668v1", "date": "2024-05-09", "relevancy": 1.9775, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5138}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guess%20the%20Drift%20with%20LOP-UKF%3A%20LiDAR%20Odometry%20and%20Pacejka%20Model%20for%0A%20%20Real-Time%20Racecar%20Sideslip%20Estimation&body=Title%3A%20Guess%20the%20Drift%20with%20LOP-UKF%3A%20LiDAR%20Odometry%20and%20Pacejka%20Model%20for%0A%20%20Real-Time%20Racecar%20Sideslip%20Estimation%0AAuthor%3A%20Alessandro%20Toschi%20and%20Nicola%20Musiu%20and%20Francesco%20Gatti%20and%20Ayoub%20Raji%20and%20Francesco%20Amerotti%20and%20Micaela%20Verucchi%20and%20Marko%20Bertogna%0AAbstract%3A%20%20%20The%20sideslip%20angle%2C%20crucial%20for%20vehicle%20safety%20and%20stability%2C%20is%20determined%0Ausing%20both%20longitudinal%20and%20lateral%20velocities.%20However%2C%20measuring%20the%20lateral%0Acomponent%20often%20necessitates%20costly%20sensors%2C%20leading%20to%20its%20common%20estimation%2C%0Aa%20topic%20thoroughly%20explored%20in%20existing%20literature.%20This%20paper%20introduces%0ALOP-UKF%2C%20a%20novel%20method%20for%20estimating%20vehicle%20lateral%20velocity%20by%20integrating%0ALidar%20Odometry%20with%20the%20Pacejka%20tire%20model%20predictions%2C%20resulting%20in%20a%20robust%0Aestimation%20via%20an%20Unscendent%20Kalman%20Filter%20%28UKF%29.%20This%20combination%20represents%20a%0Adistinct%20alternative%20to%20more%20traditional%20methodologies%2C%20resulting%20in%20a%20reliable%0Asolution%20also%20in%20edge%20cases.%20We%20present%20experimental%20results%20obtained%20using%20the%0ADallara%20AV-21%20across%20diverse%20circuits%20and%20track%20conditions%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuess%2520the%2520Drift%2520with%2520LOP-UKF%253A%2520LiDAR%2520Odometry%2520and%2520Pacejka%2520Model%2520for%250A%2520%2520Real-Time%2520Racecar%2520Sideslip%2520Estimation%26entry.906535625%3DAlessandro%2520Toschi%2520and%2520Nicola%2520Musiu%2520and%2520Francesco%2520Gatti%2520and%2520Ayoub%2520Raji%2520and%2520Francesco%2520Amerotti%2520and%2520Micaela%2520Verucchi%2520and%2520Marko%2520Bertogna%26entry.1292438233%3D%2520%2520The%2520sideslip%2520angle%252C%2520crucial%2520for%2520vehicle%2520safety%2520and%2520stability%252C%2520is%2520determined%250Ausing%2520both%2520longitudinal%2520and%2520lateral%2520velocities.%2520However%252C%2520measuring%2520the%2520lateral%250Acomponent%2520often%2520necessitates%2520costly%2520sensors%252C%2520leading%2520to%2520its%2520common%2520estimation%252C%250Aa%2520topic%2520thoroughly%2520explored%2520in%2520existing%2520literature.%2520This%2520paper%2520introduces%250ALOP-UKF%252C%2520a%2520novel%2520method%2520for%2520estimating%2520vehicle%2520lateral%2520velocity%2520by%2520integrating%250ALidar%2520Odometry%2520with%2520the%2520Pacejka%2520tire%2520model%2520predictions%252C%2520resulting%2520in%2520a%2520robust%250Aestimation%2520via%2520an%2520Unscendent%2520Kalman%2520Filter%2520%2528UKF%2529.%2520This%2520combination%2520represents%2520a%250Adistinct%2520alternative%2520to%2520more%2520traditional%2520methodologies%252C%2520resulting%2520in%2520a%2520reliable%250Asolution%2520also%2520in%2520edge%2520cases.%2520We%2520present%2520experimental%2520results%2520obtained%2520using%2520the%250ADallara%2520AV-21%2520across%2520diverse%2520circuits%2520and%2520track%2520conditions%252C%2520demonstrating%2520the%250Aeffectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guess%20the%20Drift%20with%20LOP-UKF%3A%20LiDAR%20Odometry%20and%20Pacejka%20Model%20for%0A%20%20Real-Time%20Racecar%20Sideslip%20Estimation&entry.906535625=Alessandro%20Toschi%20and%20Nicola%20Musiu%20and%20Francesco%20Gatti%20and%20Ayoub%20Raji%20and%20Francesco%20Amerotti%20and%20Micaela%20Verucchi%20and%20Marko%20Bertogna&entry.1292438233=%20%20The%20sideslip%20angle%2C%20crucial%20for%20vehicle%20safety%20and%20stability%2C%20is%20determined%0Ausing%20both%20longitudinal%20and%20lateral%20velocities.%20However%2C%20measuring%20the%20lateral%0Acomponent%20often%20necessitates%20costly%20sensors%2C%20leading%20to%20its%20common%20estimation%2C%0Aa%20topic%20thoroughly%20explored%20in%20existing%20literature.%20This%20paper%20introduces%0ALOP-UKF%2C%20a%20novel%20method%20for%20estimating%20vehicle%20lateral%20velocity%20by%20integrating%0ALidar%20Odometry%20with%20the%20Pacejka%20tire%20model%20predictions%2C%20resulting%20in%20a%20robust%0Aestimation%20via%20an%20Unscendent%20Kalman%20Filter%20%28UKF%29.%20This%20combination%20represents%20a%0Adistinct%20alternative%20to%20more%20traditional%20methodologies%2C%20resulting%20in%20a%20reliable%0Asolution%20also%20in%20edge%20cases.%20We%20present%20experimental%20results%20obtained%20using%20the%0ADallara%20AV-21%20across%20diverse%20circuits%20and%20track%20conditions%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05668v1&entry.124074799=Read"},
{"title": "StructComp: Substituting Propagation with Structural Compression in\n  Training Graph Contrastive Learning", "author": "Shengzhong Zhang and Wenjie Yang and Xinyuan Cao and Hongwei Zhang and Zengfeng Huang", "abstract": "  Graph contrastive learning (GCL) has become a powerful tool for learning\ngraph data, but its scalability remains a significant challenge. In this work,\nwe propose a simple yet effective training framework called Structural\nCompression (StructComp) to address this issue. Inspired by a sparse low-rank\napproximation on the diffusion matrix, StructComp trains the encoder with the\ncompressed nodes. This allows the encoder not to perform any message passing\nduring the training stage, and significantly reduces the number of sample pairs\nin the contrastive loss. We theoretically prove that the original GCL loss can\nbe approximated with the contrastive loss computed by StructComp. Moreover,\nStructComp can be regarded as an additional regularization term for GCL models,\nresulting in a more robust encoder. Empirical studies on various datasets show\nthat StructComp greatly reduces the time and memory consumption while improving\nmodel performance compared to the vanilla GCL models and scalable training\nmethods.\n", "link": "http://arxiv.org/abs/2312.04865v4", "date": "2024-05-09", "relevancy": 1.9605, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5139}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructComp%3A%20Substituting%20Propagation%20with%20Structural%20Compression%20in%0A%20%20Training%20Graph%20Contrastive%20Learning&body=Title%3A%20StructComp%3A%20Substituting%20Propagation%20with%20Structural%20Compression%20in%0A%20%20Training%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Shengzhong%20Zhang%20and%20Wenjie%20Yang%20and%20Xinyuan%20Cao%20and%20Hongwei%20Zhang%20and%20Zengfeng%20Huang%0AAbstract%3A%20%20%20Graph%20contrastive%20learning%20%28GCL%29%20has%20become%20a%20powerful%20tool%20for%20learning%0Agraph%20data%2C%20but%20its%20scalability%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20effective%20training%20framework%20called%20Structural%0ACompression%20%28StructComp%29%20to%20address%20this%20issue.%20Inspired%20by%20a%20sparse%20low-rank%0Aapproximation%20on%20the%20diffusion%20matrix%2C%20StructComp%20trains%20the%20encoder%20with%20the%0Acompressed%20nodes.%20This%20allows%20the%20encoder%20not%20to%20perform%20any%20message%20passing%0Aduring%20the%20training%20stage%2C%20and%20significantly%20reduces%20the%20number%20of%20sample%20pairs%0Ain%20the%20contrastive%20loss.%20We%20theoretically%20prove%20that%20the%20original%20GCL%20loss%20can%0Abe%20approximated%20with%20the%20contrastive%20loss%20computed%20by%20StructComp.%20Moreover%2C%0AStructComp%20can%20be%20regarded%20as%20an%20additional%20regularization%20term%20for%20GCL%20models%2C%0Aresulting%20in%20a%20more%20robust%20encoder.%20Empirical%20studies%20on%20various%20datasets%20show%0Athat%20StructComp%20greatly%20reduces%20the%20time%20and%20memory%20consumption%20while%20improving%0Amodel%20performance%20compared%20to%20the%20vanilla%20GCL%20models%20and%20scalable%20training%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.04865v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructComp%253A%2520Substituting%2520Propagation%2520with%2520Structural%2520Compression%2520in%250A%2520%2520Training%2520Graph%2520Contrastive%2520Learning%26entry.906535625%3DShengzhong%2520Zhang%2520and%2520Wenjie%2520Yang%2520and%2520Xinyuan%2520Cao%2520and%2520Hongwei%2520Zhang%2520and%2520Zengfeng%2520Huang%26entry.1292438233%3D%2520%2520Graph%2520contrastive%2520learning%2520%2528GCL%2529%2520has%2520become%2520a%2520powerful%2520tool%2520for%2520learning%250Agraph%2520data%252C%2520but%2520its%2520scalability%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520simple%2520yet%2520effective%2520training%2520framework%2520called%2520Structural%250ACompression%2520%2528StructComp%2529%2520to%2520address%2520this%2520issue.%2520Inspired%2520by%2520a%2520sparse%2520low-rank%250Aapproximation%2520on%2520the%2520diffusion%2520matrix%252C%2520StructComp%2520trains%2520the%2520encoder%2520with%2520the%250Acompressed%2520nodes.%2520This%2520allows%2520the%2520encoder%2520not%2520to%2520perform%2520any%2520message%2520passing%250Aduring%2520the%2520training%2520stage%252C%2520and%2520significantly%2520reduces%2520the%2520number%2520of%2520sample%2520pairs%250Ain%2520the%2520contrastive%2520loss.%2520We%2520theoretically%2520prove%2520that%2520the%2520original%2520GCL%2520loss%2520can%250Abe%2520approximated%2520with%2520the%2520contrastive%2520loss%2520computed%2520by%2520StructComp.%2520Moreover%252C%250AStructComp%2520can%2520be%2520regarded%2520as%2520an%2520additional%2520regularization%2520term%2520for%2520GCL%2520models%252C%250Aresulting%2520in%2520a%2520more%2520robust%2520encoder.%2520Empirical%2520studies%2520on%2520various%2520datasets%2520show%250Athat%2520StructComp%2520greatly%2520reduces%2520the%2520time%2520and%2520memory%2520consumption%2520while%2520improving%250Amodel%2520performance%2520compared%2520to%2520the%2520vanilla%2520GCL%2520models%2520and%2520scalable%2520training%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.04865v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructComp%3A%20Substituting%20Propagation%20with%20Structural%20Compression%20in%0A%20%20Training%20Graph%20Contrastive%20Learning&entry.906535625=Shengzhong%20Zhang%20and%20Wenjie%20Yang%20and%20Xinyuan%20Cao%20and%20Hongwei%20Zhang%20and%20Zengfeng%20Huang&entry.1292438233=%20%20Graph%20contrastive%20learning%20%28GCL%29%20has%20become%20a%20powerful%20tool%20for%20learning%0Agraph%20data%2C%20but%20its%20scalability%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20effective%20training%20framework%20called%20Structural%0ACompression%20%28StructComp%29%20to%20address%20this%20issue.%20Inspired%20by%20a%20sparse%20low-rank%0Aapproximation%20on%20the%20diffusion%20matrix%2C%20StructComp%20trains%20the%20encoder%20with%20the%0Acompressed%20nodes.%20This%20allows%20the%20encoder%20not%20to%20perform%20any%20message%20passing%0Aduring%20the%20training%20stage%2C%20and%20significantly%20reduces%20the%20number%20of%20sample%20pairs%0Ain%20the%20contrastive%20loss.%20We%20theoretically%20prove%20that%20the%20original%20GCL%20loss%20can%0Abe%20approximated%20with%20the%20contrastive%20loss%20computed%20by%20StructComp.%20Moreover%2C%0AStructComp%20can%20be%20regarded%20as%20an%20additional%20regularization%20term%20for%20GCL%20models%2C%0Aresulting%20in%20a%20more%20robust%20encoder.%20Empirical%20studies%20on%20various%20datasets%20show%0Athat%20StructComp%20greatly%20reduces%20the%20time%20and%20memory%20consumption%20while%20improving%0Amodel%20performance%20compared%20to%20the%20vanilla%20GCL%20models%20and%20scalable%20training%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.04865v4&entry.124074799=Read"},
{"title": "Almost Global Asymptotic Trajectory Tracking for Fully-Actuated\n  Mechanical Systems on Homogeneous Riemannian Manifolds", "author": "Jake Welde and Vijay Kumar", "abstract": "  In this work, we address the design of tracking controllers that drive a\nmechanical system's state asymptotically towards a reference trajectory.\nMotivated by aerospace and robotics applications, we consider fully-actuated\nsystems evolving on the broad class of homogeneous spaces (encompassing all\nvector spaces, Lie groups, and spheres of any finite dimension). In this\nsetting, the transitive action of a Lie group on the configuration manifold\nenables an intrinsic description of the tracking error as an element of the\nstate space, even in the absence of a group structure on the configuration\nmanifold itself (e.g., for $\\mathbb{S}^2$). Such an error state facilitates the\ndesign of a generalized control policy depending smoothly on state and time,\nwhich drives the geometric tracking error to a designated origin from almost\nevery initial condition, thereby guaranteeing almost global convergence to the\nreference trajectory. Moreover, the proposed controller simplifies elegantly\nwhen specialized to a Lie group or the n-sphere. In summary, we propose a\nunified, intrinsic controller guaranteeing almost global asymptotic trajectory\ntracking for fully-actuated mechanical systems evolving on a broad class of\nmanifolds. We apply the method to an axisymmetric satellite and an\nomnidirectional aerial robot.\n", "link": "http://arxiv.org/abs/2403.04900v2", "date": "2024-05-09", "relevancy": 1.9593, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5061}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5009}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Almost%20Global%20Asymptotic%20Trajectory%20Tracking%20for%20Fully-Actuated%0A%20%20Mechanical%20Systems%20on%20Homogeneous%20Riemannian%20Manifolds&body=Title%3A%20Almost%20Global%20Asymptotic%20Trajectory%20Tracking%20for%20Fully-Actuated%0A%20%20Mechanical%20Systems%20on%20Homogeneous%20Riemannian%20Manifolds%0AAuthor%3A%20Jake%20Welde%20and%20Vijay%20Kumar%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20design%20of%20tracking%20controllers%20that%20drive%20a%0Amechanical%20system%27s%20state%20asymptotically%20towards%20a%20reference%20trajectory.%0AMotivated%20by%20aerospace%20and%20robotics%20applications%2C%20we%20consider%20fully-actuated%0Asystems%20evolving%20on%20the%20broad%20class%20of%20homogeneous%20spaces%20%28encompassing%20all%0Avector%20spaces%2C%20Lie%20groups%2C%20and%20spheres%20of%20any%20finite%20dimension%29.%20In%20this%0Asetting%2C%20the%20transitive%20action%20of%20a%20Lie%20group%20on%20the%20configuration%20manifold%0Aenables%20an%20intrinsic%20description%20of%20the%20tracking%20error%20as%20an%20element%20of%20the%0Astate%20space%2C%20even%20in%20the%20absence%20of%20a%20group%20structure%20on%20the%20configuration%0Amanifold%20itself%20%28e.g.%2C%20for%20%24%5Cmathbb%7BS%7D%5E2%24%29.%20Such%20an%20error%20state%20facilitates%20the%0Adesign%20of%20a%20generalized%20control%20policy%20depending%20smoothly%20on%20state%20and%20time%2C%0Awhich%20drives%20the%20geometric%20tracking%20error%20to%20a%20designated%20origin%20from%20almost%0Aevery%20initial%20condition%2C%20thereby%20guaranteeing%20almost%20global%20convergence%20to%20the%0Areference%20trajectory.%20Moreover%2C%20the%20proposed%20controller%20simplifies%20elegantly%0Awhen%20specialized%20to%20a%20Lie%20group%20or%20the%20n-sphere.%20In%20summary%2C%20we%20propose%20a%0Aunified%2C%20intrinsic%20controller%20guaranteeing%20almost%20global%20asymptotic%20trajectory%0Atracking%20for%20fully-actuated%20mechanical%20systems%20evolving%20on%20a%20broad%20class%20of%0Amanifolds.%20We%20apply%20the%20method%20to%20an%20axisymmetric%20satellite%20and%20an%0Aomnidirectional%20aerial%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04900v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlmost%2520Global%2520Asymptotic%2520Trajectory%2520Tracking%2520for%2520Fully-Actuated%250A%2520%2520Mechanical%2520Systems%2520on%2520Homogeneous%2520Riemannian%2520Manifolds%26entry.906535625%3DJake%2520Welde%2520and%2520Vijay%2520Kumar%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520design%2520of%2520tracking%2520controllers%2520that%2520drive%2520a%250Amechanical%2520system%2527s%2520state%2520asymptotically%2520towards%2520a%2520reference%2520trajectory.%250AMotivated%2520by%2520aerospace%2520and%2520robotics%2520applications%252C%2520we%2520consider%2520fully-actuated%250Asystems%2520evolving%2520on%2520the%2520broad%2520class%2520of%2520homogeneous%2520spaces%2520%2528encompassing%2520all%250Avector%2520spaces%252C%2520Lie%2520groups%252C%2520and%2520spheres%2520of%2520any%2520finite%2520dimension%2529.%2520In%2520this%250Asetting%252C%2520the%2520transitive%2520action%2520of%2520a%2520Lie%2520group%2520on%2520the%2520configuration%2520manifold%250Aenables%2520an%2520intrinsic%2520description%2520of%2520the%2520tracking%2520error%2520as%2520an%2520element%2520of%2520the%250Astate%2520space%252C%2520even%2520in%2520the%2520absence%2520of%2520a%2520group%2520structure%2520on%2520the%2520configuration%250Amanifold%2520itself%2520%2528e.g.%252C%2520for%2520%2524%255Cmathbb%257BS%257D%255E2%2524%2529.%2520Such%2520an%2520error%2520state%2520facilitates%2520the%250Adesign%2520of%2520a%2520generalized%2520control%2520policy%2520depending%2520smoothly%2520on%2520state%2520and%2520time%252C%250Awhich%2520drives%2520the%2520geometric%2520tracking%2520error%2520to%2520a%2520designated%2520origin%2520from%2520almost%250Aevery%2520initial%2520condition%252C%2520thereby%2520guaranteeing%2520almost%2520global%2520convergence%2520to%2520the%250Areference%2520trajectory.%2520Moreover%252C%2520the%2520proposed%2520controller%2520simplifies%2520elegantly%250Awhen%2520specialized%2520to%2520a%2520Lie%2520group%2520or%2520the%2520n-sphere.%2520In%2520summary%252C%2520we%2520propose%2520a%250Aunified%252C%2520intrinsic%2520controller%2520guaranteeing%2520almost%2520global%2520asymptotic%2520trajectory%250Atracking%2520for%2520fully-actuated%2520mechanical%2520systems%2520evolving%2520on%2520a%2520broad%2520class%2520of%250Amanifolds.%2520We%2520apply%2520the%2520method%2520to%2520an%2520axisymmetric%2520satellite%2520and%2520an%250Aomnidirectional%2520aerial%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04900v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Almost%20Global%20Asymptotic%20Trajectory%20Tracking%20for%20Fully-Actuated%0A%20%20Mechanical%20Systems%20on%20Homogeneous%20Riemannian%20Manifolds&entry.906535625=Jake%20Welde%20and%20Vijay%20Kumar&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20design%20of%20tracking%20controllers%20that%20drive%20a%0Amechanical%20system%27s%20state%20asymptotically%20towards%20a%20reference%20trajectory.%0AMotivated%20by%20aerospace%20and%20robotics%20applications%2C%20we%20consider%20fully-actuated%0Asystems%20evolving%20on%20the%20broad%20class%20of%20homogeneous%20spaces%20%28encompassing%20all%0Avector%20spaces%2C%20Lie%20groups%2C%20and%20spheres%20of%20any%20finite%20dimension%29.%20In%20this%0Asetting%2C%20the%20transitive%20action%20of%20a%20Lie%20group%20on%20the%20configuration%20manifold%0Aenables%20an%20intrinsic%20description%20of%20the%20tracking%20error%20as%20an%20element%20of%20the%0Astate%20space%2C%20even%20in%20the%20absence%20of%20a%20group%20structure%20on%20the%20configuration%0Amanifold%20itself%20%28e.g.%2C%20for%20%24%5Cmathbb%7BS%7D%5E2%24%29.%20Such%20an%20error%20state%20facilitates%20the%0Adesign%20of%20a%20generalized%20control%20policy%20depending%20smoothly%20on%20state%20and%20time%2C%0Awhich%20drives%20the%20geometric%20tracking%20error%20to%20a%20designated%20origin%20from%20almost%0Aevery%20initial%20condition%2C%20thereby%20guaranteeing%20almost%20global%20convergence%20to%20the%0Areference%20trajectory.%20Moreover%2C%20the%20proposed%20controller%20simplifies%20elegantly%0Awhen%20specialized%20to%20a%20Lie%20group%20or%20the%20n-sphere.%20In%20summary%2C%20we%20propose%20a%0Aunified%2C%20intrinsic%20controller%20guaranteeing%20almost%20global%20asymptotic%20trajectory%0Atracking%20for%20fully-actuated%20mechanical%20systems%20evolving%20on%20a%20broad%20class%20of%0Amanifolds.%20We%20apply%20the%20method%20to%20an%20axisymmetric%20satellite%20and%20an%0Aomnidirectional%20aerial%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04900v2&entry.124074799=Read"},
{"title": "Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference\n  Cost", "author": "Yuan Gao and Weizhong Zhang and Wenhan Luo and Lin Ma and Jin-Gang Yu and Gui-Song Xia and Jiayi Ma", "abstract": "  We aim at exploiting additional auxiliary labels from an independent\n(auxiliary) task to boost the primary task performance which we focus on, while\npreserving a single task inference cost of the primary task. While most\nexisting auxiliary learning methods are optimization-based relying on loss\nweights/gradients manipulation, our method is architecture-based with a\nflexible asymmetric structure for the primary and auxiliary tasks, which\nproduces different networks for training and inference. Specifically, starting\nfrom two single task networks/branches (each representing a task), we propose a\nnovel method with evolving networks where only primary-to-auxiliary links exist\nas the cross-task connections after convergence. These connections can be\nremoved during the primary task inference, resulting in a single-task inference\ncost. We achieve this by formulating a Neural Architecture Search (NAS)\nproblem, where we initialize bi-directional connections in the search space and\nguide the NAS optimization converging to an architecture with only the\nsingle-side primary-to-auxiliary connections. Moreover, our method can be\nincorporated with optimization-based auxiliary learning approaches. Extensive\nexperiments with six tasks on NYU v2, CityScapes, and Taskonomy datasets using\nVGG, ResNet, and ViT backbones validate the promising performance. The codes\nare available at https://github.com/ethanygao/Aux-NAS.\n", "link": "http://arxiv.org/abs/2405.05695v1", "date": "2024-05-09", "relevancy": 1.9554, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5097}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4748}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aux-NAS%3A%20Exploiting%20Auxiliary%20Labels%20with%20Negligibly%20Extra%20Inference%0A%20%20Cost&body=Title%3A%20Aux-NAS%3A%20Exploiting%20Auxiliary%20Labels%20with%20Negligibly%20Extra%20Inference%0A%20%20Cost%0AAuthor%3A%20Yuan%20Gao%20and%20Weizhong%20Zhang%20and%20Wenhan%20Luo%20and%20Lin%20Ma%20and%20Jin-Gang%20Yu%20and%20Gui-Song%20Xia%20and%20Jiayi%20Ma%0AAbstract%3A%20%20%20We%20aim%20at%20exploiting%20additional%20auxiliary%20labels%20from%20an%20independent%0A%28auxiliary%29%20task%20to%20boost%20the%20primary%20task%20performance%20which%20we%20focus%20on%2C%20while%0Apreserving%20a%20single%20task%20inference%20cost%20of%20the%20primary%20task.%20While%20most%0Aexisting%20auxiliary%20learning%20methods%20are%20optimization-based%20relying%20on%20loss%0Aweights/gradients%20manipulation%2C%20our%20method%20is%20architecture-based%20with%20a%0Aflexible%20asymmetric%20structure%20for%20the%20primary%20and%20auxiliary%20tasks%2C%20which%0Aproduces%20different%20networks%20for%20training%20and%20inference.%20Specifically%2C%20starting%0Afrom%20two%20single%20task%20networks/branches%20%28each%20representing%20a%20task%29%2C%20we%20propose%20a%0Anovel%20method%20with%20evolving%20networks%20where%20only%20primary-to-auxiliary%20links%20exist%0Aas%20the%20cross-task%20connections%20after%20convergence.%20These%20connections%20can%20be%0Aremoved%20during%20the%20primary%20task%20inference%2C%20resulting%20in%20a%20single-task%20inference%0Acost.%20We%20achieve%20this%20by%20formulating%20a%20Neural%20Architecture%20Search%20%28NAS%29%0Aproblem%2C%20where%20we%20initialize%20bi-directional%20connections%20in%20the%20search%20space%20and%0Aguide%20the%20NAS%20optimization%20converging%20to%20an%20architecture%20with%20only%20the%0Asingle-side%20primary-to-auxiliary%20connections.%20Moreover%2C%20our%20method%20can%20be%0Aincorporated%20with%20optimization-based%20auxiliary%20learning%20approaches.%20Extensive%0Aexperiments%20with%20six%20tasks%20on%20NYU%20v2%2C%20CityScapes%2C%20and%20Taskonomy%20datasets%20using%0AVGG%2C%20ResNet%2C%20and%20ViT%20backbones%20validate%20the%20promising%20performance.%20The%20codes%0Aare%20available%20at%20https%3A//github.com/ethanygao/Aux-NAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAux-NAS%253A%2520Exploiting%2520Auxiliary%2520Labels%2520with%2520Negligibly%2520Extra%2520Inference%250A%2520%2520Cost%26entry.906535625%3DYuan%2520Gao%2520and%2520Weizhong%2520Zhang%2520and%2520Wenhan%2520Luo%2520and%2520Lin%2520Ma%2520and%2520Jin-Gang%2520Yu%2520and%2520Gui-Song%2520Xia%2520and%2520Jiayi%2520Ma%26entry.1292438233%3D%2520%2520We%2520aim%2520at%2520exploiting%2520additional%2520auxiliary%2520labels%2520from%2520an%2520independent%250A%2528auxiliary%2529%2520task%2520to%2520boost%2520the%2520primary%2520task%2520performance%2520which%2520we%2520focus%2520on%252C%2520while%250Apreserving%2520a%2520single%2520task%2520inference%2520cost%2520of%2520the%2520primary%2520task.%2520While%2520most%250Aexisting%2520auxiliary%2520learning%2520methods%2520are%2520optimization-based%2520relying%2520on%2520loss%250Aweights/gradients%2520manipulation%252C%2520our%2520method%2520is%2520architecture-based%2520with%2520a%250Aflexible%2520asymmetric%2520structure%2520for%2520the%2520primary%2520and%2520auxiliary%2520tasks%252C%2520which%250Aproduces%2520different%2520networks%2520for%2520training%2520and%2520inference.%2520Specifically%252C%2520starting%250Afrom%2520two%2520single%2520task%2520networks/branches%2520%2528each%2520representing%2520a%2520task%2529%252C%2520we%2520propose%2520a%250Anovel%2520method%2520with%2520evolving%2520networks%2520where%2520only%2520primary-to-auxiliary%2520links%2520exist%250Aas%2520the%2520cross-task%2520connections%2520after%2520convergence.%2520These%2520connections%2520can%2520be%250Aremoved%2520during%2520the%2520primary%2520task%2520inference%252C%2520resulting%2520in%2520a%2520single-task%2520inference%250Acost.%2520We%2520achieve%2520this%2520by%2520formulating%2520a%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%250Aproblem%252C%2520where%2520we%2520initialize%2520bi-directional%2520connections%2520in%2520the%2520search%2520space%2520and%250Aguide%2520the%2520NAS%2520optimization%2520converging%2520to%2520an%2520architecture%2520with%2520only%2520the%250Asingle-side%2520primary-to-auxiliary%2520connections.%2520Moreover%252C%2520our%2520method%2520can%2520be%250Aincorporated%2520with%2520optimization-based%2520auxiliary%2520learning%2520approaches.%2520Extensive%250Aexperiments%2520with%2520six%2520tasks%2520on%2520NYU%2520v2%252C%2520CityScapes%252C%2520and%2520Taskonomy%2520datasets%2520using%250AVGG%252C%2520ResNet%252C%2520and%2520ViT%2520backbones%2520validate%2520the%2520promising%2520performance.%2520The%2520codes%250Aare%2520available%2520at%2520https%253A//github.com/ethanygao/Aux-NAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aux-NAS%3A%20Exploiting%20Auxiliary%20Labels%20with%20Negligibly%20Extra%20Inference%0A%20%20Cost&entry.906535625=Yuan%20Gao%20and%20Weizhong%20Zhang%20and%20Wenhan%20Luo%20and%20Lin%20Ma%20and%20Jin-Gang%20Yu%20and%20Gui-Song%20Xia%20and%20Jiayi%20Ma&entry.1292438233=%20%20We%20aim%20at%20exploiting%20additional%20auxiliary%20labels%20from%20an%20independent%0A%28auxiliary%29%20task%20to%20boost%20the%20primary%20task%20performance%20which%20we%20focus%20on%2C%20while%0Apreserving%20a%20single%20task%20inference%20cost%20of%20the%20primary%20task.%20While%20most%0Aexisting%20auxiliary%20learning%20methods%20are%20optimization-based%20relying%20on%20loss%0Aweights/gradients%20manipulation%2C%20our%20method%20is%20architecture-based%20with%20a%0Aflexible%20asymmetric%20structure%20for%20the%20primary%20and%20auxiliary%20tasks%2C%20which%0Aproduces%20different%20networks%20for%20training%20and%20inference.%20Specifically%2C%20starting%0Afrom%20two%20single%20task%20networks/branches%20%28each%20representing%20a%20task%29%2C%20we%20propose%20a%0Anovel%20method%20with%20evolving%20networks%20where%20only%20primary-to-auxiliary%20links%20exist%0Aas%20the%20cross-task%20connections%20after%20convergence.%20These%20connections%20can%20be%0Aremoved%20during%20the%20primary%20task%20inference%2C%20resulting%20in%20a%20single-task%20inference%0Acost.%20We%20achieve%20this%20by%20formulating%20a%20Neural%20Architecture%20Search%20%28NAS%29%0Aproblem%2C%20where%20we%20initialize%20bi-directional%20connections%20in%20the%20search%20space%20and%0Aguide%20the%20NAS%20optimization%20converging%20to%20an%20architecture%20with%20only%20the%0Asingle-side%20primary-to-auxiliary%20connections.%20Moreover%2C%20our%20method%20can%20be%0Aincorporated%20with%20optimization-based%20auxiliary%20learning%20approaches.%20Extensive%0Aexperiments%20with%20six%20tasks%20on%20NYU%20v2%2C%20CityScapes%2C%20and%20Taskonomy%20datasets%20using%0AVGG%2C%20ResNet%2C%20and%20ViT%20backbones%20validate%20the%20promising%20performance.%20The%20codes%0Aare%20available%20at%20https%3A//github.com/ethanygao/Aux-NAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05695v1&entry.124074799=Read"},
{"title": "MasterWeaver: Taming Editability and Identity for Personalized\n  Text-to-Image Generation", "author": "Yuxiang Wei and Zhilong Ji and Jinfeng Bai and Hongzhi Zhang and Lei Zhang and Wangmeng Zuo", "abstract": "  Text-to-image (T2I) diffusion models have shown significant success in\npersonalized text-to-image generation, which aims to generate novel images with\nhuman identities indicated by the reference images. Despite promising identity\nfidelity has been achieved by several tuning-free methods, they usually suffer\nfrom overfitting issues. The learned identity tends to entangle with irrelevant\ninformation, resulting in unsatisfied text controllability, especially on\nfaces. In this work, we present MasterWeaver, a test-time tuning-free method\ndesigned to generate personalized images with both faithful identity fidelity\nand flexible editability. Specifically, MasterWeaver adopts an encoder to\nextract identity features and steers the image generation through additional\nintroduced cross attention. To improve editability while maintaining identity\nfidelity, we propose an editing direction loss for training, which aligns the\nediting directions of our MasterWeaver with those of the original T2I model.\nAdditionally, a face-augmented dataset is constructed to facilitate\ndisentangled identity learning, and further improve the editability. Extensive\nexperiments demonstrate that our MasterWeaver can not only generate\npersonalized images with faithful identity, but also exhibit superiority in\ntext controllability. Our code will be publicly available at\nhttps://github.com/csyxwei/MasterWeaver.\n", "link": "http://arxiv.org/abs/2405.05806v1", "date": "2024-05-09", "relevancy": 1.9506, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6986}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6292}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MasterWeaver%3A%20Taming%20Editability%20and%20Identity%20for%20Personalized%0A%20%20Text-to-Image%20Generation&body=Title%3A%20MasterWeaver%3A%20Taming%20Editability%20and%20Identity%20for%20Personalized%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Yuxiang%20Wei%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Hongzhi%20Zhang%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20significant%20success%20in%0Apersonalized%20text-to-image%20generation%2C%20which%20aims%20to%20generate%20novel%20images%20with%0Ahuman%20identities%20indicated%20by%20the%20reference%20images.%20Despite%20promising%20identity%0Afidelity%20has%20been%20achieved%20by%20several%20tuning-free%20methods%2C%20they%20usually%20suffer%0Afrom%20overfitting%20issues.%20The%20learned%20identity%20tends%20to%20entangle%20with%20irrelevant%0Ainformation%2C%20resulting%20in%20unsatisfied%20text%20controllability%2C%20especially%20on%0Afaces.%20In%20this%20work%2C%20we%20present%20MasterWeaver%2C%20a%20test-time%20tuning-free%20method%0Adesigned%20to%20generate%20personalized%20images%20with%20both%20faithful%20identity%20fidelity%0Aand%20flexible%20editability.%20Specifically%2C%20MasterWeaver%20adopts%20an%20encoder%20to%0Aextract%20identity%20features%20and%20steers%20the%20image%20generation%20through%20additional%0Aintroduced%20cross%20attention.%20To%20improve%20editability%20while%20maintaining%20identity%0Afidelity%2C%20we%20propose%20an%20editing%20direction%20loss%20for%20training%2C%20which%20aligns%20the%0Aediting%20directions%20of%20our%20MasterWeaver%20with%20those%20of%20the%20original%20T2I%20model.%0AAdditionally%2C%20a%20face-augmented%20dataset%20is%20constructed%20to%20facilitate%0Adisentangled%20identity%20learning%2C%20and%20further%20improve%20the%20editability.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20MasterWeaver%20can%20not%20only%20generate%0Apersonalized%20images%20with%20faithful%20identity%2C%20but%20also%20exhibit%20superiority%20in%0Atext%20controllability.%20Our%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/csyxwei/MasterWeaver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasterWeaver%253A%2520Taming%2520Editability%2520and%2520Identity%2520for%2520Personalized%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Zhilong%2520Ji%2520and%2520Jinfeng%2520Bai%2520and%2520Hongzhi%2520Zhang%2520and%2520Lei%2520Zhang%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520shown%2520significant%2520success%2520in%250Apersonalized%2520text-to-image%2520generation%252C%2520which%2520aims%2520to%2520generate%2520novel%2520images%2520with%250Ahuman%2520identities%2520indicated%2520by%2520the%2520reference%2520images.%2520Despite%2520promising%2520identity%250Afidelity%2520has%2520been%2520achieved%2520by%2520several%2520tuning-free%2520methods%252C%2520they%2520usually%2520suffer%250Afrom%2520overfitting%2520issues.%2520The%2520learned%2520identity%2520tends%2520to%2520entangle%2520with%2520irrelevant%250Ainformation%252C%2520resulting%2520in%2520unsatisfied%2520text%2520controllability%252C%2520especially%2520on%250Afaces.%2520In%2520this%2520work%252C%2520we%2520present%2520MasterWeaver%252C%2520a%2520test-time%2520tuning-free%2520method%250Adesigned%2520to%2520generate%2520personalized%2520images%2520with%2520both%2520faithful%2520identity%2520fidelity%250Aand%2520flexible%2520editability.%2520Specifically%252C%2520MasterWeaver%2520adopts%2520an%2520encoder%2520to%250Aextract%2520identity%2520features%2520and%2520steers%2520the%2520image%2520generation%2520through%2520additional%250Aintroduced%2520cross%2520attention.%2520To%2520improve%2520editability%2520while%2520maintaining%2520identity%250Afidelity%252C%2520we%2520propose%2520an%2520editing%2520direction%2520loss%2520for%2520training%252C%2520which%2520aligns%2520the%250Aediting%2520directions%2520of%2520our%2520MasterWeaver%2520with%2520those%2520of%2520the%2520original%2520T2I%2520model.%250AAdditionally%252C%2520a%2520face-augmented%2520dataset%2520is%2520constructed%2520to%2520facilitate%250Adisentangled%2520identity%2520learning%252C%2520and%2520further%2520improve%2520the%2520editability.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520MasterWeaver%2520can%2520not%2520only%2520generate%250Apersonalized%2520images%2520with%2520faithful%2520identity%252C%2520but%2520also%2520exhibit%2520superiority%2520in%250Atext%2520controllability.%2520Our%2520code%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/csyxwei/MasterWeaver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MasterWeaver%3A%20Taming%20Editability%20and%20Identity%20for%20Personalized%0A%20%20Text-to-Image%20Generation&entry.906535625=Yuxiang%20Wei%20and%20Zhilong%20Ji%20and%20Jinfeng%20Bai%20and%20Hongzhi%20Zhang%20and%20Lei%20Zhang%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20have%20shown%20significant%20success%20in%0Apersonalized%20text-to-image%20generation%2C%20which%20aims%20to%20generate%20novel%20images%20with%0Ahuman%20identities%20indicated%20by%20the%20reference%20images.%20Despite%20promising%20identity%0Afidelity%20has%20been%20achieved%20by%20several%20tuning-free%20methods%2C%20they%20usually%20suffer%0Afrom%20overfitting%20issues.%20The%20learned%20identity%20tends%20to%20entangle%20with%20irrelevant%0Ainformation%2C%20resulting%20in%20unsatisfied%20text%20controllability%2C%20especially%20on%0Afaces.%20In%20this%20work%2C%20we%20present%20MasterWeaver%2C%20a%20test-time%20tuning-free%20method%0Adesigned%20to%20generate%20personalized%20images%20with%20both%20faithful%20identity%20fidelity%0Aand%20flexible%20editability.%20Specifically%2C%20MasterWeaver%20adopts%20an%20encoder%20to%0Aextract%20identity%20features%20and%20steers%20the%20image%20generation%20through%20additional%0Aintroduced%20cross%20attention.%20To%20improve%20editability%20while%20maintaining%20identity%0Afidelity%2C%20we%20propose%20an%20editing%20direction%20loss%20for%20training%2C%20which%20aligns%20the%0Aediting%20directions%20of%20our%20MasterWeaver%20with%20those%20of%20the%20original%20T2I%20model.%0AAdditionally%2C%20a%20face-augmented%20dataset%20is%20constructed%20to%20facilitate%0Adisentangled%20identity%20learning%2C%20and%20further%20improve%20the%20editability.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20MasterWeaver%20can%20not%20only%20generate%0Apersonalized%20images%20with%20faithful%20identity%2C%20but%20also%20exhibit%20superiority%20in%0Atext%20controllability.%20Our%20code%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/csyxwei/MasterWeaver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05806v1&entry.124074799=Read"},
{"title": "Deep Multi-Task Learning for Malware Image Classification", "author": "Ahmed Bensaoud and Jugal Kalita", "abstract": "  Malicious software is a pernicious global problem. A novel multi-task\nlearning framework is proposed in this paper for malware image classification\nfor accurate and fast malware detection. We generate bitmap (BMP) and (PNG)\nimages from malware features, which we feed to a deep learning classifier. Our\nstate-of-the-art multi-task learning approach has been tested on a new dataset,\nfor which we have collected approximately 100,000 benign and malicious PE, APK,\nMach-o, and ELF examples. Experiments with seven tasks tested with 4 activation\nfunctions, ReLU, LeakyReLU, PReLU, and ELU separately demonstrate that PReLU\ngives the highest accuracy of more than 99.87% on all tasks. Our model can\neffectively detect a variety of obfuscation methods like packing, encryption,\nand instruction overlapping, strengthing the beneficial claims of our model, in\naddition to achieving the state-of-art methods in terms of accuracy.\n", "link": "http://arxiv.org/abs/2405.05906v1", "date": "2024-05-09", "relevancy": 1.9501, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5016}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4939}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Multi-Task%20Learning%20for%20Malware%20Image%20Classification&body=Title%3A%20Deep%20Multi-Task%20Learning%20for%20Malware%20Image%20Classification%0AAuthor%3A%20Ahmed%20Bensaoud%20and%20Jugal%20Kalita%0AAbstract%3A%20%20%20Malicious%20software%20is%20a%20pernicious%20global%20problem.%20A%20novel%20multi-task%0Alearning%20framework%20is%20proposed%20in%20this%20paper%20for%20malware%20image%20classification%0Afor%20accurate%20and%20fast%20malware%20detection.%20We%20generate%20bitmap%20%28BMP%29%20and%20%28PNG%29%0Aimages%20from%20malware%20features%2C%20which%20we%20feed%20to%20a%20deep%20learning%20classifier.%20Our%0Astate-of-the-art%20multi-task%20learning%20approach%20has%20been%20tested%20on%20a%20new%20dataset%2C%0Afor%20which%20we%20have%20collected%20approximately%20100%2C000%20benign%20and%20malicious%20PE%2C%20APK%2C%0AMach-o%2C%20and%20ELF%20examples.%20Experiments%20with%20seven%20tasks%20tested%20with%204%20activation%0Afunctions%2C%20ReLU%2C%20LeakyReLU%2C%20PReLU%2C%20and%20ELU%20separately%20demonstrate%20that%20PReLU%0Agives%20the%20highest%20accuracy%20of%20more%20than%2099.87%25%20on%20all%20tasks.%20Our%20model%20can%0Aeffectively%20detect%20a%20variety%20of%20obfuscation%20methods%20like%20packing%2C%20encryption%2C%0Aand%20instruction%20overlapping%2C%20strengthing%20the%20beneficial%20claims%20of%20our%20model%2C%20in%0Aaddition%20to%20achieving%20the%20state-of-art%20methods%20in%20terms%20of%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Multi-Task%2520Learning%2520for%2520Malware%2520Image%2520Classification%26entry.906535625%3DAhmed%2520Bensaoud%2520and%2520Jugal%2520Kalita%26entry.1292438233%3D%2520%2520Malicious%2520software%2520is%2520a%2520pernicious%2520global%2520problem.%2520A%2520novel%2520multi-task%250Alearning%2520framework%2520is%2520proposed%2520in%2520this%2520paper%2520for%2520malware%2520image%2520classification%250Afor%2520accurate%2520and%2520fast%2520malware%2520detection.%2520We%2520generate%2520bitmap%2520%2528BMP%2529%2520and%2520%2528PNG%2529%250Aimages%2520from%2520malware%2520features%252C%2520which%2520we%2520feed%2520to%2520a%2520deep%2520learning%2520classifier.%2520Our%250Astate-of-the-art%2520multi-task%2520learning%2520approach%2520has%2520been%2520tested%2520on%2520a%2520new%2520dataset%252C%250Afor%2520which%2520we%2520have%2520collected%2520approximately%2520100%252C000%2520benign%2520and%2520malicious%2520PE%252C%2520APK%252C%250AMach-o%252C%2520and%2520ELF%2520examples.%2520Experiments%2520with%2520seven%2520tasks%2520tested%2520with%25204%2520activation%250Afunctions%252C%2520ReLU%252C%2520LeakyReLU%252C%2520PReLU%252C%2520and%2520ELU%2520separately%2520demonstrate%2520that%2520PReLU%250Agives%2520the%2520highest%2520accuracy%2520of%2520more%2520than%252099.87%2525%2520on%2520all%2520tasks.%2520Our%2520model%2520can%250Aeffectively%2520detect%2520a%2520variety%2520of%2520obfuscation%2520methods%2520like%2520packing%252C%2520encryption%252C%250Aand%2520instruction%2520overlapping%252C%2520strengthing%2520the%2520beneficial%2520claims%2520of%2520our%2520model%252C%2520in%250Aaddition%2520to%2520achieving%2520the%2520state-of-art%2520methods%2520in%2520terms%2520of%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Multi-Task%20Learning%20for%20Malware%20Image%20Classification&entry.906535625=Ahmed%20Bensaoud%20and%20Jugal%20Kalita&entry.1292438233=%20%20Malicious%20software%20is%20a%20pernicious%20global%20problem.%20A%20novel%20multi-task%0Alearning%20framework%20is%20proposed%20in%20this%20paper%20for%20malware%20image%20classification%0Afor%20accurate%20and%20fast%20malware%20detection.%20We%20generate%20bitmap%20%28BMP%29%20and%20%28PNG%29%0Aimages%20from%20malware%20features%2C%20which%20we%20feed%20to%20a%20deep%20learning%20classifier.%20Our%0Astate-of-the-art%20multi-task%20learning%20approach%20has%20been%20tested%20on%20a%20new%20dataset%2C%0Afor%20which%20we%20have%20collected%20approximately%20100%2C000%20benign%20and%20malicious%20PE%2C%20APK%2C%0AMach-o%2C%20and%20ELF%20examples.%20Experiments%20with%20seven%20tasks%20tested%20with%204%20activation%0Afunctions%2C%20ReLU%2C%20LeakyReLU%2C%20PReLU%2C%20and%20ELU%20separately%20demonstrate%20that%20PReLU%0Agives%20the%20highest%20accuracy%20of%20more%20than%2099.87%25%20on%20all%20tasks.%20Our%20model%20can%0Aeffectively%20detect%20a%20variety%20of%20obfuscation%20methods%20like%20packing%2C%20encryption%2C%0Aand%20instruction%20overlapping%2C%20strengthing%20the%20beneficial%20claims%20of%20our%20model%2C%20in%0Aaddition%20to%20achieving%20the%20state-of-art%20methods%20in%20terms%20of%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05906v1&entry.124074799=Read"},
{"title": "TransAnaNet: Transformer-based Anatomy Change Prediction Network for\n  Head and Neck Cancer Patient Radiotherapy", "author": "Meixu Chen and Kai Wang and Michael Dohopolski and Howard Morgan and Jing Wang", "abstract": "  Early identification of head and neck cancer (HNC) patients who would\nexperience significant anatomical change during radiotherapy (RT) is important\nto optimize patient clinical benefit and treatment resources. This study aims\nto assess the feasibility of using a vision-transformer (ViT) based neural\nnetwork to predict RT-induced anatomic change in HNC patients. We\nretrospectively included 121 HNC patients treated with definitive RT/CRT. We\ncollected the planning CT (pCT), planned dose, CBCTs acquired at the initial\ntreatment (CBCT01) and fraction 21 (CBCT21), and primary tumor volume (GTVp)\nand involved nodal volume (GTVn) delineated on both pCT and CBCTs for model\nconstruction and evaluation. A UNet-style ViT network was designed to learn\nspatial correspondence and contextual information from embedded CT, dose,\nCBCT01, GTVp, and GTVn image patches. The model estimated the deformation\nvector field between CBCT01 and CBCT21 as the prediction of anatomic change,\nand deformed CBCT01 was used as the prediction of CBCT21. We also generated\nbinary masks of GTVp, GTVn, and patient body for volumetric change evaluation.\nThe predicted image from the proposed method yielded the best similarity to the\nreal image (CBCT21) over pCT, CBCT01, and predicted CBCTs from other comparison\nmodels. The average MSE and SSIM between the normalized predicted CBCT to\nCBCT21 are 0.009 and 0.933, while the average dice coefficient between body\nmask, GTVp mask, and GTVn mask are 0.972, 0.792, and 0.821 respectively. The\nproposed method showed promising performance for predicting\nradiotherapy-induced anatomic change, which has the potential to assist in the\ndecision-making of HNC Adaptive RT.\n", "link": "http://arxiv.org/abs/2405.05674v1", "date": "2024-05-09", "relevancy": 1.9201, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TransAnaNet%3A%20Transformer-based%20Anatomy%20Change%20Prediction%20Network%20for%0A%20%20Head%20and%20Neck%20Cancer%20Patient%20Radiotherapy&body=Title%3A%20TransAnaNet%3A%20Transformer-based%20Anatomy%20Change%20Prediction%20Network%20for%0A%20%20Head%20and%20Neck%20Cancer%20Patient%20Radiotherapy%0AAuthor%3A%20Meixu%20Chen%20and%20Kai%20Wang%20and%20Michael%20Dohopolski%20and%20Howard%20Morgan%20and%20Jing%20Wang%0AAbstract%3A%20%20%20Early%20identification%20of%20head%20and%20neck%20cancer%20%28HNC%29%20patients%20who%20would%0Aexperience%20significant%20anatomical%20change%20during%20radiotherapy%20%28RT%29%20is%20important%0Ato%20optimize%20patient%20clinical%20benefit%20and%20treatment%20resources.%20This%20study%20aims%0Ato%20assess%20the%20feasibility%20of%20using%20a%20vision-transformer%20%28ViT%29%20based%20neural%0Anetwork%20to%20predict%20RT-induced%20anatomic%20change%20in%20HNC%20patients.%20We%0Aretrospectively%20included%20121%20HNC%20patients%20treated%20with%20definitive%20RT/CRT.%20We%0Acollected%20the%20planning%20CT%20%28pCT%29%2C%20planned%20dose%2C%20CBCTs%20acquired%20at%20the%20initial%0Atreatment%20%28CBCT01%29%20and%20fraction%2021%20%28CBCT21%29%2C%20and%20primary%20tumor%20volume%20%28GTVp%29%0Aand%20involved%20nodal%20volume%20%28GTVn%29%20delineated%20on%20both%20pCT%20and%20CBCTs%20for%20model%0Aconstruction%20and%20evaluation.%20A%20UNet-style%20ViT%20network%20was%20designed%20to%20learn%0Aspatial%20correspondence%20and%20contextual%20information%20from%20embedded%20CT%2C%20dose%2C%0ACBCT01%2C%20GTVp%2C%20and%20GTVn%20image%20patches.%20The%20model%20estimated%20the%20deformation%0Avector%20field%20between%20CBCT01%20and%20CBCT21%20as%20the%20prediction%20of%20anatomic%20change%2C%0Aand%20deformed%20CBCT01%20was%20used%20as%20the%20prediction%20of%20CBCT21.%20We%20also%20generated%0Abinary%20masks%20of%20GTVp%2C%20GTVn%2C%20and%20patient%20body%20for%20volumetric%20change%20evaluation.%0AThe%20predicted%20image%20from%20the%20proposed%20method%20yielded%20the%20best%20similarity%20to%20the%0Areal%20image%20%28CBCT21%29%20over%20pCT%2C%20CBCT01%2C%20and%20predicted%20CBCTs%20from%20other%20comparison%0Amodels.%20The%20average%20MSE%20and%20SSIM%20between%20the%20normalized%20predicted%20CBCT%20to%0ACBCT21%20are%200.009%20and%200.933%2C%20while%20the%20average%20dice%20coefficient%20between%20body%0Amask%2C%20GTVp%20mask%2C%20and%20GTVn%20mask%20are%200.972%2C%200.792%2C%20and%200.821%20respectively.%20The%0Aproposed%20method%20showed%20promising%20performance%20for%20predicting%0Aradiotherapy-induced%20anatomic%20change%2C%20which%20has%20the%20potential%20to%20assist%20in%20the%0Adecision-making%20of%20HNC%20Adaptive%20RT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransAnaNet%253A%2520Transformer-based%2520Anatomy%2520Change%2520Prediction%2520Network%2520for%250A%2520%2520Head%2520and%2520Neck%2520Cancer%2520Patient%2520Radiotherapy%26entry.906535625%3DMeixu%2520Chen%2520and%2520Kai%2520Wang%2520and%2520Michael%2520Dohopolski%2520and%2520Howard%2520Morgan%2520and%2520Jing%2520Wang%26entry.1292438233%3D%2520%2520Early%2520identification%2520of%2520head%2520and%2520neck%2520cancer%2520%2528HNC%2529%2520patients%2520who%2520would%250Aexperience%2520significant%2520anatomical%2520change%2520during%2520radiotherapy%2520%2528RT%2529%2520is%2520important%250Ato%2520optimize%2520patient%2520clinical%2520benefit%2520and%2520treatment%2520resources.%2520This%2520study%2520aims%250Ato%2520assess%2520the%2520feasibility%2520of%2520using%2520a%2520vision-transformer%2520%2528ViT%2529%2520based%2520neural%250Anetwork%2520to%2520predict%2520RT-induced%2520anatomic%2520change%2520in%2520HNC%2520patients.%2520We%250Aretrospectively%2520included%2520121%2520HNC%2520patients%2520treated%2520with%2520definitive%2520RT/CRT.%2520We%250Acollected%2520the%2520planning%2520CT%2520%2528pCT%2529%252C%2520planned%2520dose%252C%2520CBCTs%2520acquired%2520at%2520the%2520initial%250Atreatment%2520%2528CBCT01%2529%2520and%2520fraction%252021%2520%2528CBCT21%2529%252C%2520and%2520primary%2520tumor%2520volume%2520%2528GTVp%2529%250Aand%2520involved%2520nodal%2520volume%2520%2528GTVn%2529%2520delineated%2520on%2520both%2520pCT%2520and%2520CBCTs%2520for%2520model%250Aconstruction%2520and%2520evaluation.%2520A%2520UNet-style%2520ViT%2520network%2520was%2520designed%2520to%2520learn%250Aspatial%2520correspondence%2520and%2520contextual%2520information%2520from%2520embedded%2520CT%252C%2520dose%252C%250ACBCT01%252C%2520GTVp%252C%2520and%2520GTVn%2520image%2520patches.%2520The%2520model%2520estimated%2520the%2520deformation%250Avector%2520field%2520between%2520CBCT01%2520and%2520CBCT21%2520as%2520the%2520prediction%2520of%2520anatomic%2520change%252C%250Aand%2520deformed%2520CBCT01%2520was%2520used%2520as%2520the%2520prediction%2520of%2520CBCT21.%2520We%2520also%2520generated%250Abinary%2520masks%2520of%2520GTVp%252C%2520GTVn%252C%2520and%2520patient%2520body%2520for%2520volumetric%2520change%2520evaluation.%250AThe%2520predicted%2520image%2520from%2520the%2520proposed%2520method%2520yielded%2520the%2520best%2520similarity%2520to%2520the%250Areal%2520image%2520%2528CBCT21%2529%2520over%2520pCT%252C%2520CBCT01%252C%2520and%2520predicted%2520CBCTs%2520from%2520other%2520comparison%250Amodels.%2520The%2520average%2520MSE%2520and%2520SSIM%2520between%2520the%2520normalized%2520predicted%2520CBCT%2520to%250ACBCT21%2520are%25200.009%2520and%25200.933%252C%2520while%2520the%2520average%2520dice%2520coefficient%2520between%2520body%250Amask%252C%2520GTVp%2520mask%252C%2520and%2520GTVn%2520mask%2520are%25200.972%252C%25200.792%252C%2520and%25200.821%2520respectively.%2520The%250Aproposed%2520method%2520showed%2520promising%2520performance%2520for%2520predicting%250Aradiotherapy-induced%2520anatomic%2520change%252C%2520which%2520has%2520the%2520potential%2520to%2520assist%2520in%2520the%250Adecision-making%2520of%2520HNC%2520Adaptive%2520RT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TransAnaNet%3A%20Transformer-based%20Anatomy%20Change%20Prediction%20Network%20for%0A%20%20Head%20and%20Neck%20Cancer%20Patient%20Radiotherapy&entry.906535625=Meixu%20Chen%20and%20Kai%20Wang%20and%20Michael%20Dohopolski%20and%20Howard%20Morgan%20and%20Jing%20Wang&entry.1292438233=%20%20Early%20identification%20of%20head%20and%20neck%20cancer%20%28HNC%29%20patients%20who%20would%0Aexperience%20significant%20anatomical%20change%20during%20radiotherapy%20%28RT%29%20is%20important%0Ato%20optimize%20patient%20clinical%20benefit%20and%20treatment%20resources.%20This%20study%20aims%0Ato%20assess%20the%20feasibility%20of%20using%20a%20vision-transformer%20%28ViT%29%20based%20neural%0Anetwork%20to%20predict%20RT-induced%20anatomic%20change%20in%20HNC%20patients.%20We%0Aretrospectively%20included%20121%20HNC%20patients%20treated%20with%20definitive%20RT/CRT.%20We%0Acollected%20the%20planning%20CT%20%28pCT%29%2C%20planned%20dose%2C%20CBCTs%20acquired%20at%20the%20initial%0Atreatment%20%28CBCT01%29%20and%20fraction%2021%20%28CBCT21%29%2C%20and%20primary%20tumor%20volume%20%28GTVp%29%0Aand%20involved%20nodal%20volume%20%28GTVn%29%20delineated%20on%20both%20pCT%20and%20CBCTs%20for%20model%0Aconstruction%20and%20evaluation.%20A%20UNet-style%20ViT%20network%20was%20designed%20to%20learn%0Aspatial%20correspondence%20and%20contextual%20information%20from%20embedded%20CT%2C%20dose%2C%0ACBCT01%2C%20GTVp%2C%20and%20GTVn%20image%20patches.%20The%20model%20estimated%20the%20deformation%0Avector%20field%20between%20CBCT01%20and%20CBCT21%20as%20the%20prediction%20of%20anatomic%20change%2C%0Aand%20deformed%20CBCT01%20was%20used%20as%20the%20prediction%20of%20CBCT21.%20We%20also%20generated%0Abinary%20masks%20of%20GTVp%2C%20GTVn%2C%20and%20patient%20body%20for%20volumetric%20change%20evaluation.%0AThe%20predicted%20image%20from%20the%20proposed%20method%20yielded%20the%20best%20similarity%20to%20the%0Areal%20image%20%28CBCT21%29%20over%20pCT%2C%20CBCT01%2C%20and%20predicted%20CBCTs%20from%20other%20comparison%0Amodels.%20The%20average%20MSE%20and%20SSIM%20between%20the%20normalized%20predicted%20CBCT%20to%0ACBCT21%20are%200.009%20and%200.933%2C%20while%20the%20average%20dice%20coefficient%20between%20body%0Amask%2C%20GTVp%20mask%2C%20and%20GTVn%20mask%20are%200.972%2C%200.792%2C%20and%200.821%20respectively.%20The%0Aproposed%20method%20showed%20promising%20performance%20for%20predicting%0Aradiotherapy-induced%20anatomic%20change%2C%20which%20has%20the%20potential%20to%20assist%20in%20the%0Adecision-making%20of%20HNC%20Adaptive%20RT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05674v1&entry.124074799=Read"},
{"title": "LatentColorization: Latent Diffusion-Based Speaker Video Colorization", "author": "Rory Ward and Dan Bigioi and Shubhajit Basak and John G. Breslin and Peter Corcoran", "abstract": "  While current research predominantly focuses on image-based colorization, the\ndomain of video-based colorization remains relatively unexplored. Most existing\nvideo colorization techniques operate on a frame-by-frame basis, often\noverlooking the critical aspect of temporal coherence between successive\nframes. This approach can result in inconsistencies across frames, leading to\nundesirable effects like flickering or abrupt color transitions between frames.\nTo address these challenges, we harness the generative capabilities of a\nfine-tuned latent diffusion model designed specifically for video colorization,\nintroducing a novel solution for achieving temporal consistency in video\ncolorization, as well as demonstrating strong improvements on established image\nquality metrics compared to other existing methods. Furthermore, we perform a\nsubjective study, where users preferred our approach to the existing state of\nthe art. Our dataset encompasses a combination of conventional datasets and\nvideos from television/movies. In short, by leveraging the power of a\nfine-tuned latent diffusion-based colorization system with a temporal\nconsistency mechanism, we can improve the performance of automatic video\ncolorization by addressing the challenges of temporal inconsistency. A short\ndemonstration of our results can be seen in some example videos available at\nhttps://youtu.be/vDbzsZdFuxM.\n", "link": "http://arxiv.org/abs/2405.05707v1", "date": "2024-05-09", "relevancy": 1.9174, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6655}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6328}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LatentColorization%3A%20Latent%20Diffusion-Based%20Speaker%20Video%20Colorization&body=Title%3A%20LatentColorization%3A%20Latent%20Diffusion-Based%20Speaker%20Video%20Colorization%0AAuthor%3A%20Rory%20Ward%20and%20Dan%20Bigioi%20and%20Shubhajit%20Basak%20and%20John%20G.%20Breslin%20and%20Peter%20Corcoran%0AAbstract%3A%20%20%20While%20current%20research%20predominantly%20focuses%20on%20image-based%20colorization%2C%20the%0Adomain%20of%20video-based%20colorization%20remains%20relatively%20unexplored.%20Most%20existing%0Avideo%20colorization%20techniques%20operate%20on%20a%20frame-by-frame%20basis%2C%20often%0Aoverlooking%20the%20critical%20aspect%20of%20temporal%20coherence%20between%20successive%0Aframes.%20This%20approach%20can%20result%20in%20inconsistencies%20across%20frames%2C%20leading%20to%0Aundesirable%20effects%20like%20flickering%20or%20abrupt%20color%20transitions%20between%20frames.%0ATo%20address%20these%20challenges%2C%20we%20harness%20the%20generative%20capabilities%20of%20a%0Afine-tuned%20latent%20diffusion%20model%20designed%20specifically%20for%20video%20colorization%2C%0Aintroducing%20a%20novel%20solution%20for%20achieving%20temporal%20consistency%20in%20video%0Acolorization%2C%20as%20well%20as%20demonstrating%20strong%20improvements%20on%20established%20image%0Aquality%20metrics%20compared%20to%20other%20existing%20methods.%20Furthermore%2C%20we%20perform%20a%0Asubjective%20study%2C%20where%20users%20preferred%20our%20approach%20to%20the%20existing%20state%20of%0Athe%20art.%20Our%20dataset%20encompasses%20a%20combination%20of%20conventional%20datasets%20and%0Avideos%20from%20television/movies.%20In%20short%2C%20by%20leveraging%20the%20power%20of%20a%0Afine-tuned%20latent%20diffusion-based%20colorization%20system%20with%20a%20temporal%0Aconsistency%20mechanism%2C%20we%20can%20improve%20the%20performance%20of%20automatic%20video%0Acolorization%20by%20addressing%20the%20challenges%20of%20temporal%20inconsistency.%20A%20short%0Ademonstration%20of%20our%20results%20can%20be%20seen%20in%20some%20example%20videos%20available%20at%0Ahttps%3A//youtu.be/vDbzsZdFuxM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatentColorization%253A%2520Latent%2520Diffusion-Based%2520Speaker%2520Video%2520Colorization%26entry.906535625%3DRory%2520Ward%2520and%2520Dan%2520Bigioi%2520and%2520Shubhajit%2520Basak%2520and%2520John%2520G.%2520Breslin%2520and%2520Peter%2520Corcoran%26entry.1292438233%3D%2520%2520While%2520current%2520research%2520predominantly%2520focuses%2520on%2520image-based%2520colorization%252C%2520the%250Adomain%2520of%2520video-based%2520colorization%2520remains%2520relatively%2520unexplored.%2520Most%2520existing%250Avideo%2520colorization%2520techniques%2520operate%2520on%2520a%2520frame-by-frame%2520basis%252C%2520often%250Aoverlooking%2520the%2520critical%2520aspect%2520of%2520temporal%2520coherence%2520between%2520successive%250Aframes.%2520This%2520approach%2520can%2520result%2520in%2520inconsistencies%2520across%2520frames%252C%2520leading%2520to%250Aundesirable%2520effects%2520like%2520flickering%2520or%2520abrupt%2520color%2520transitions%2520between%2520frames.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520harness%2520the%2520generative%2520capabilities%2520of%2520a%250Afine-tuned%2520latent%2520diffusion%2520model%2520designed%2520specifically%2520for%2520video%2520colorization%252C%250Aintroducing%2520a%2520novel%2520solution%2520for%2520achieving%2520temporal%2520consistency%2520in%2520video%250Acolorization%252C%2520as%2520well%2520as%2520demonstrating%2520strong%2520improvements%2520on%2520established%2520image%250Aquality%2520metrics%2520compared%2520to%2520other%2520existing%2520methods.%2520Furthermore%252C%2520we%2520perform%2520a%250Asubjective%2520study%252C%2520where%2520users%2520preferred%2520our%2520approach%2520to%2520the%2520existing%2520state%2520of%250Athe%2520art.%2520Our%2520dataset%2520encompasses%2520a%2520combination%2520of%2520conventional%2520datasets%2520and%250Avideos%2520from%2520television/movies.%2520In%2520short%252C%2520by%2520leveraging%2520the%2520power%2520of%2520a%250Afine-tuned%2520latent%2520diffusion-based%2520colorization%2520system%2520with%2520a%2520temporal%250Aconsistency%2520mechanism%252C%2520we%2520can%2520improve%2520the%2520performance%2520of%2520automatic%2520video%250Acolorization%2520by%2520addressing%2520the%2520challenges%2520of%2520temporal%2520inconsistency.%2520A%2520short%250Ademonstration%2520of%2520our%2520results%2520can%2520be%2520seen%2520in%2520some%2520example%2520videos%2520available%2520at%250Ahttps%253A//youtu.be/vDbzsZdFuxM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatentColorization%3A%20Latent%20Diffusion-Based%20Speaker%20Video%20Colorization&entry.906535625=Rory%20Ward%20and%20Dan%20Bigioi%20and%20Shubhajit%20Basak%20and%20John%20G.%20Breslin%20and%20Peter%20Corcoran&entry.1292438233=%20%20While%20current%20research%20predominantly%20focuses%20on%20image-based%20colorization%2C%20the%0Adomain%20of%20video-based%20colorization%20remains%20relatively%20unexplored.%20Most%20existing%0Avideo%20colorization%20techniques%20operate%20on%20a%20frame-by-frame%20basis%2C%20often%0Aoverlooking%20the%20critical%20aspect%20of%20temporal%20coherence%20between%20successive%0Aframes.%20This%20approach%20can%20result%20in%20inconsistencies%20across%20frames%2C%20leading%20to%0Aundesirable%20effects%20like%20flickering%20or%20abrupt%20color%20transitions%20between%20frames.%0ATo%20address%20these%20challenges%2C%20we%20harness%20the%20generative%20capabilities%20of%20a%0Afine-tuned%20latent%20diffusion%20model%20designed%20specifically%20for%20video%20colorization%2C%0Aintroducing%20a%20novel%20solution%20for%20achieving%20temporal%20consistency%20in%20video%0Acolorization%2C%20as%20well%20as%20demonstrating%20strong%20improvements%20on%20established%20image%0Aquality%20metrics%20compared%20to%20other%20existing%20methods.%20Furthermore%2C%20we%20perform%20a%0Asubjective%20study%2C%20where%20users%20preferred%20our%20approach%20to%20the%20existing%20state%20of%0Athe%20art.%20Our%20dataset%20encompasses%20a%20combination%20of%20conventional%20datasets%20and%0Avideos%20from%20television/movies.%20In%20short%2C%20by%20leveraging%20the%20power%20of%20a%0Afine-tuned%20latent%20diffusion-based%20colorization%20system%20with%20a%20temporal%0Aconsistency%20mechanism%2C%20we%20can%20improve%20the%20performance%20of%20automatic%20video%0Acolorization%20by%20addressing%20the%20challenges%20of%20temporal%20inconsistency.%20A%20short%0Ademonstration%20of%20our%20results%20can%20be%20seen%20in%20some%20example%20videos%20available%20at%0Ahttps%3A//youtu.be/vDbzsZdFuxM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05707v1&entry.124074799=Read"},
{"title": "Estimating the Hessian Matrix of Ranking Objectives for Stochastic\n  Learning to Rank with Gradient Boosted Trees", "author": "Jingwei Kang and Maarten de Rijke and Harrie Oosterhuis", "abstract": "  Stochastic learning to rank (LTR) is a recent branch in the LTR field that\nconcerns the optimization of probabilistic ranking models. Their probabilistic\nbehavior enables certain ranking qualities that are impossible with\ndeterministic models. For example, they can increase the diversity of displayed\ndocuments, increase fairness of exposure over documents, and better balance\nexploitation and exploration through randomization. A core difficulty in LTR is\ngradient estimation, for this reason, existing stochastic LTR methods have been\nlimited to differentiable ranking models (e.g., neural networks). This is in\nstark contrast with the general field of LTR where Gradient Boosted Decision\nTrees (GBDTs) have long been considered the state-of-the-art. In this work, we\naddress this gap by introducing the first stochastic LTR method for GBDTs. Our\nmain contribution is a novel estimator for the second-order derivatives, i.e.,\nthe Hessian matrix, which is a requirement for effective GBDTs. To efficiently\ncompute both the first and second-order derivatives simultaneously, we\nincorporate our estimator into the existing PL-Rank framework, which was\noriginally designed for first-order derivatives only. Our experimental results\nindicate that stochastic LTR without the Hessian has extremely poor\nperformance, whilst the performance is competitive with the current\nstate-of-the-art with our estimated Hessian. Thus, through the contribution of\nour novel Hessian estimation method, we have successfully introduced GBDTs to\nstochastic LTR.\n", "link": "http://arxiv.org/abs/2404.12190v2", "date": "2024-05-09", "relevancy": 1.9082, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4779}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20the%20Hessian%20Matrix%20of%20Ranking%20Objectives%20for%20Stochastic%0A%20%20Learning%20to%20Rank%20with%20Gradient%20Boosted%20Trees&body=Title%3A%20Estimating%20the%20Hessian%20Matrix%20of%20Ranking%20Objectives%20for%20Stochastic%0A%20%20Learning%20to%20Rank%20with%20Gradient%20Boosted%20Trees%0AAuthor%3A%20Jingwei%20Kang%20and%20Maarten%20de%20Rijke%20and%20Harrie%20Oosterhuis%0AAbstract%3A%20%20%20Stochastic%20learning%20to%20rank%20%28LTR%29%20is%20a%20recent%20branch%20in%20the%20LTR%20field%20that%0Aconcerns%20the%20optimization%20of%20probabilistic%20ranking%20models.%20Their%20probabilistic%0Abehavior%20enables%20certain%20ranking%20qualities%20that%20are%20impossible%20with%0Adeterministic%20models.%20For%20example%2C%20they%20can%20increase%20the%20diversity%20of%20displayed%0Adocuments%2C%20increase%20fairness%20of%20exposure%20over%20documents%2C%20and%20better%20balance%0Aexploitation%20and%20exploration%20through%20randomization.%20A%20core%20difficulty%20in%20LTR%20is%0Agradient%20estimation%2C%20for%20this%20reason%2C%20existing%20stochastic%20LTR%20methods%20have%20been%0Alimited%20to%20differentiable%20ranking%20models%20%28e.g.%2C%20neural%20networks%29.%20This%20is%20in%0Astark%20contrast%20with%20the%20general%20field%20of%20LTR%20where%20Gradient%20Boosted%20Decision%0ATrees%20%28GBDTs%29%20have%20long%20been%20considered%20the%20state-of-the-art.%20In%20this%20work%2C%20we%0Aaddress%20this%20gap%20by%20introducing%20the%20first%20stochastic%20LTR%20method%20for%20GBDTs.%20Our%0Amain%20contribution%20is%20a%20novel%20estimator%20for%20the%20second-order%20derivatives%2C%20i.e.%2C%0Athe%20Hessian%20matrix%2C%20which%20is%20a%20requirement%20for%20effective%20GBDTs.%20To%20efficiently%0Acompute%20both%20the%20first%20and%20second-order%20derivatives%20simultaneously%2C%20we%0Aincorporate%20our%20estimator%20into%20the%20existing%20PL-Rank%20framework%2C%20which%20was%0Aoriginally%20designed%20for%20first-order%20derivatives%20only.%20Our%20experimental%20results%0Aindicate%20that%20stochastic%20LTR%20without%20the%20Hessian%20has%20extremely%20poor%0Aperformance%2C%20whilst%20the%20performance%20is%20competitive%20with%20the%20current%0Astate-of-the-art%20with%20our%20estimated%20Hessian.%20Thus%2C%20through%20the%20contribution%20of%0Aour%20novel%20Hessian%20estimation%20method%2C%20we%20have%20successfully%20introduced%20GBDTs%20to%0Astochastic%20LTR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12190v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520the%2520Hessian%2520Matrix%2520of%2520Ranking%2520Objectives%2520for%2520Stochastic%250A%2520%2520Learning%2520to%2520Rank%2520with%2520Gradient%2520Boosted%2520Trees%26entry.906535625%3DJingwei%2520Kang%2520and%2520Maarten%2520de%2520Rijke%2520and%2520Harrie%2520Oosterhuis%26entry.1292438233%3D%2520%2520Stochastic%2520learning%2520to%2520rank%2520%2528LTR%2529%2520is%2520a%2520recent%2520branch%2520in%2520the%2520LTR%2520field%2520that%250Aconcerns%2520the%2520optimization%2520of%2520probabilistic%2520ranking%2520models.%2520Their%2520probabilistic%250Abehavior%2520enables%2520certain%2520ranking%2520qualities%2520that%2520are%2520impossible%2520with%250Adeterministic%2520models.%2520For%2520example%252C%2520they%2520can%2520increase%2520the%2520diversity%2520of%2520displayed%250Adocuments%252C%2520increase%2520fairness%2520of%2520exposure%2520over%2520documents%252C%2520and%2520better%2520balance%250Aexploitation%2520and%2520exploration%2520through%2520randomization.%2520A%2520core%2520difficulty%2520in%2520LTR%2520is%250Agradient%2520estimation%252C%2520for%2520this%2520reason%252C%2520existing%2520stochastic%2520LTR%2520methods%2520have%2520been%250Alimited%2520to%2520differentiable%2520ranking%2520models%2520%2528e.g.%252C%2520neural%2520networks%2529.%2520This%2520is%2520in%250Astark%2520contrast%2520with%2520the%2520general%2520field%2520of%2520LTR%2520where%2520Gradient%2520Boosted%2520Decision%250ATrees%2520%2528GBDTs%2529%2520have%2520long%2520been%2520considered%2520the%2520state-of-the-art.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520this%2520gap%2520by%2520introducing%2520the%2520first%2520stochastic%2520LTR%2520method%2520for%2520GBDTs.%2520Our%250Amain%2520contribution%2520is%2520a%2520novel%2520estimator%2520for%2520the%2520second-order%2520derivatives%252C%2520i.e.%252C%250Athe%2520Hessian%2520matrix%252C%2520which%2520is%2520a%2520requirement%2520for%2520effective%2520GBDTs.%2520To%2520efficiently%250Acompute%2520both%2520the%2520first%2520and%2520second-order%2520derivatives%2520simultaneously%252C%2520we%250Aincorporate%2520our%2520estimator%2520into%2520the%2520existing%2520PL-Rank%2520framework%252C%2520which%2520was%250Aoriginally%2520designed%2520for%2520first-order%2520derivatives%2520only.%2520Our%2520experimental%2520results%250Aindicate%2520that%2520stochastic%2520LTR%2520without%2520the%2520Hessian%2520has%2520extremely%2520poor%250Aperformance%252C%2520whilst%2520the%2520performance%2520is%2520competitive%2520with%2520the%2520current%250Astate-of-the-art%2520with%2520our%2520estimated%2520Hessian.%2520Thus%252C%2520through%2520the%2520contribution%2520of%250Aour%2520novel%2520Hessian%2520estimation%2520method%252C%2520we%2520have%2520successfully%2520introduced%2520GBDTs%2520to%250Astochastic%2520LTR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12190v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20the%20Hessian%20Matrix%20of%20Ranking%20Objectives%20for%20Stochastic%0A%20%20Learning%20to%20Rank%20with%20Gradient%20Boosted%20Trees&entry.906535625=Jingwei%20Kang%20and%20Maarten%20de%20Rijke%20and%20Harrie%20Oosterhuis&entry.1292438233=%20%20Stochastic%20learning%20to%20rank%20%28LTR%29%20is%20a%20recent%20branch%20in%20the%20LTR%20field%20that%0Aconcerns%20the%20optimization%20of%20probabilistic%20ranking%20models.%20Their%20probabilistic%0Abehavior%20enables%20certain%20ranking%20qualities%20that%20are%20impossible%20with%0Adeterministic%20models.%20For%20example%2C%20they%20can%20increase%20the%20diversity%20of%20displayed%0Adocuments%2C%20increase%20fairness%20of%20exposure%20over%20documents%2C%20and%20better%20balance%0Aexploitation%20and%20exploration%20through%20randomization.%20A%20core%20difficulty%20in%20LTR%20is%0Agradient%20estimation%2C%20for%20this%20reason%2C%20existing%20stochastic%20LTR%20methods%20have%20been%0Alimited%20to%20differentiable%20ranking%20models%20%28e.g.%2C%20neural%20networks%29.%20This%20is%20in%0Astark%20contrast%20with%20the%20general%20field%20of%20LTR%20where%20Gradient%20Boosted%20Decision%0ATrees%20%28GBDTs%29%20have%20long%20been%20considered%20the%20state-of-the-art.%20In%20this%20work%2C%20we%0Aaddress%20this%20gap%20by%20introducing%20the%20first%20stochastic%20LTR%20method%20for%20GBDTs.%20Our%0Amain%20contribution%20is%20a%20novel%20estimator%20for%20the%20second-order%20derivatives%2C%20i.e.%2C%0Athe%20Hessian%20matrix%2C%20which%20is%20a%20requirement%20for%20effective%20GBDTs.%20To%20efficiently%0Acompute%20both%20the%20first%20and%20second-order%20derivatives%20simultaneously%2C%20we%0Aincorporate%20our%20estimator%20into%20the%20existing%20PL-Rank%20framework%2C%20which%20was%0Aoriginally%20designed%20for%20first-order%20derivatives%20only.%20Our%20experimental%20results%0Aindicate%20that%20stochastic%20LTR%20without%20the%20Hessian%20has%20extremely%20poor%0Aperformance%2C%20whilst%20the%20performance%20is%20competitive%20with%20the%20current%0Astate-of-the-art%20with%20our%20estimated%20Hessian.%20Thus%2C%20through%20the%20contribution%20of%0Aour%20novel%20Hessian%20estimation%20method%2C%20we%20have%20successfully%20introduced%20GBDTs%20to%0Astochastic%20LTR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12190v2&entry.124074799=Read"},
{"title": "Learned feature representations are biased by complexity, learning\n  order, position, and more", "author": "Andrew Kyle Lampinen and Stephanie C. Y. Chan and Katherine Hermann", "abstract": "  Representation learning, and interpreting learned representations, are key\nareas of focus in machine learning and neuroscience. Both fields generally use\nrepresentations as a means to understand or improve a system's computations. In\nthis work, however, we explore surprising dissociations between representation\nand computation that may pose challenges for such efforts. We create datasets\nin which we attempt to match the computational role that different features\nplay, while manipulating other properties of the features or the data. We train\nvarious deep learning architectures to compute these multiple abstract features\nabout their inputs. We find that their learned feature representations are\nsystematically biased towards representing some features more strongly than\nothers, depending upon extraneous properties such as feature complexity, the\norder in which features are learned, and the distribution of features over the\ninputs. For example, features that are simpler to compute or learned first tend\nto be represented more strongly and densely than features that are more complex\nor learned later, even if all features are learned equally well. We also\nexplore how these biases are affected by architectures, optimizers, and\ntraining regimes (e.g., in transformers, features decoded earlier in the output\nsequence also tend to be represented more strongly). Our results help to\ncharacterize the inductive biases of gradient-based representation learning.\nThese results also highlight a key challenge for interpretability $-$ or for\ncomparing the representations of models and brains $-$ disentangling extraneous\nbiases from the computationally important aspects of a system's internal\nrepresentations.\n", "link": "http://arxiv.org/abs/2405.05847v1", "date": "2024-05-09", "relevancy": 1.9041, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.483}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4798}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20feature%20representations%20are%20biased%20by%20complexity%2C%20learning%0A%20%20order%2C%20position%2C%20and%20more&body=Title%3A%20Learned%20feature%20representations%20are%20biased%20by%20complexity%2C%20learning%0A%20%20order%2C%20position%2C%20and%20more%0AAuthor%3A%20Andrew%20Kyle%20Lampinen%20and%20Stephanie%20C.%20Y.%20Chan%20and%20Katherine%20Hermann%0AAbstract%3A%20%20%20Representation%20learning%2C%20and%20interpreting%20learned%20representations%2C%20are%20key%0Aareas%20of%20focus%20in%20machine%20learning%20and%20neuroscience.%20Both%20fields%20generally%20use%0Arepresentations%20as%20a%20means%20to%20understand%20or%20improve%20a%20system%27s%20computations.%20In%0Athis%20work%2C%20however%2C%20we%20explore%20surprising%20dissociations%20between%20representation%0Aand%20computation%20that%20may%20pose%20challenges%20for%20such%20efforts.%20We%20create%20datasets%0Ain%20which%20we%20attempt%20to%20match%20the%20computational%20role%20that%20different%20features%0Aplay%2C%20while%20manipulating%20other%20properties%20of%20the%20features%20or%20the%20data.%20We%20train%0Avarious%20deep%20learning%20architectures%20to%20compute%20these%20multiple%20abstract%20features%0Aabout%20their%20inputs.%20We%20find%20that%20their%20learned%20feature%20representations%20are%0Asystematically%20biased%20towards%20representing%20some%20features%20more%20strongly%20than%0Aothers%2C%20depending%20upon%20extraneous%20properties%20such%20as%20feature%20complexity%2C%20the%0Aorder%20in%20which%20features%20are%20learned%2C%20and%20the%20distribution%20of%20features%20over%20the%0Ainputs.%20For%20example%2C%20features%20that%20are%20simpler%20to%20compute%20or%20learned%20first%20tend%0Ato%20be%20represented%20more%20strongly%20and%20densely%20than%20features%20that%20are%20more%20complex%0Aor%20learned%20later%2C%20even%20if%20all%20features%20are%20learned%20equally%20well.%20We%20also%0Aexplore%20how%20these%20biases%20are%20affected%20by%20architectures%2C%20optimizers%2C%20and%0Atraining%20regimes%20%28e.g.%2C%20in%20transformers%2C%20features%20decoded%20earlier%20in%20the%20output%0Asequence%20also%20tend%20to%20be%20represented%20more%20strongly%29.%20Our%20results%20help%20to%0Acharacterize%20the%20inductive%20biases%20of%20gradient-based%20representation%20learning.%0AThese%20results%20also%20highlight%20a%20key%20challenge%20for%20interpretability%20%24-%24%20or%20for%0Acomparing%20the%20representations%20of%20models%20and%20brains%20%24-%24%20disentangling%20extraneous%0Abiases%20from%20the%20computationally%20important%20aspects%20of%20a%20system%27s%20internal%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520feature%2520representations%2520are%2520biased%2520by%2520complexity%252C%2520learning%250A%2520%2520order%252C%2520position%252C%2520and%2520more%26entry.906535625%3DAndrew%2520Kyle%2520Lampinen%2520and%2520Stephanie%2520C.%2520Y.%2520Chan%2520and%2520Katherine%2520Hermann%26entry.1292438233%3D%2520%2520Representation%2520learning%252C%2520and%2520interpreting%2520learned%2520representations%252C%2520are%2520key%250Aareas%2520of%2520focus%2520in%2520machine%2520learning%2520and%2520neuroscience.%2520Both%2520fields%2520generally%2520use%250Arepresentations%2520as%2520a%2520means%2520to%2520understand%2520or%2520improve%2520a%2520system%2527s%2520computations.%2520In%250Athis%2520work%252C%2520however%252C%2520we%2520explore%2520surprising%2520dissociations%2520between%2520representation%250Aand%2520computation%2520that%2520may%2520pose%2520challenges%2520for%2520such%2520efforts.%2520We%2520create%2520datasets%250Ain%2520which%2520we%2520attempt%2520to%2520match%2520the%2520computational%2520role%2520that%2520different%2520features%250Aplay%252C%2520while%2520manipulating%2520other%2520properties%2520of%2520the%2520features%2520or%2520the%2520data.%2520We%2520train%250Avarious%2520deep%2520learning%2520architectures%2520to%2520compute%2520these%2520multiple%2520abstract%2520features%250Aabout%2520their%2520inputs.%2520We%2520find%2520that%2520their%2520learned%2520feature%2520representations%2520are%250Asystematically%2520biased%2520towards%2520representing%2520some%2520features%2520more%2520strongly%2520than%250Aothers%252C%2520depending%2520upon%2520extraneous%2520properties%2520such%2520as%2520feature%2520complexity%252C%2520the%250Aorder%2520in%2520which%2520features%2520are%2520learned%252C%2520and%2520the%2520distribution%2520of%2520features%2520over%2520the%250Ainputs.%2520For%2520example%252C%2520features%2520that%2520are%2520simpler%2520to%2520compute%2520or%2520learned%2520first%2520tend%250Ato%2520be%2520represented%2520more%2520strongly%2520and%2520densely%2520than%2520features%2520that%2520are%2520more%2520complex%250Aor%2520learned%2520later%252C%2520even%2520if%2520all%2520features%2520are%2520learned%2520equally%2520well.%2520We%2520also%250Aexplore%2520how%2520these%2520biases%2520are%2520affected%2520by%2520architectures%252C%2520optimizers%252C%2520and%250Atraining%2520regimes%2520%2528e.g.%252C%2520in%2520transformers%252C%2520features%2520decoded%2520earlier%2520in%2520the%2520output%250Asequence%2520also%2520tend%2520to%2520be%2520represented%2520more%2520strongly%2529.%2520Our%2520results%2520help%2520to%250Acharacterize%2520the%2520inductive%2520biases%2520of%2520gradient-based%2520representation%2520learning.%250AThese%2520results%2520also%2520highlight%2520a%2520key%2520challenge%2520for%2520interpretability%2520%2524-%2524%2520or%2520for%250Acomparing%2520the%2520representations%2520of%2520models%2520and%2520brains%2520%2524-%2524%2520disentangling%2520extraneous%250Abiases%2520from%2520the%2520computationally%2520important%2520aspects%2520of%2520a%2520system%2527s%2520internal%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20feature%20representations%20are%20biased%20by%20complexity%2C%20learning%0A%20%20order%2C%20position%2C%20and%20more&entry.906535625=Andrew%20Kyle%20Lampinen%20and%20Stephanie%20C.%20Y.%20Chan%20and%20Katherine%20Hermann&entry.1292438233=%20%20Representation%20learning%2C%20and%20interpreting%20learned%20representations%2C%20are%20key%0Aareas%20of%20focus%20in%20machine%20learning%20and%20neuroscience.%20Both%20fields%20generally%20use%0Arepresentations%20as%20a%20means%20to%20understand%20or%20improve%20a%20system%27s%20computations.%20In%0Athis%20work%2C%20however%2C%20we%20explore%20surprising%20dissociations%20between%20representation%0Aand%20computation%20that%20may%20pose%20challenges%20for%20such%20efforts.%20We%20create%20datasets%0Ain%20which%20we%20attempt%20to%20match%20the%20computational%20role%20that%20different%20features%0Aplay%2C%20while%20manipulating%20other%20properties%20of%20the%20features%20or%20the%20data.%20We%20train%0Avarious%20deep%20learning%20architectures%20to%20compute%20these%20multiple%20abstract%20features%0Aabout%20their%20inputs.%20We%20find%20that%20their%20learned%20feature%20representations%20are%0Asystematically%20biased%20towards%20representing%20some%20features%20more%20strongly%20than%0Aothers%2C%20depending%20upon%20extraneous%20properties%20such%20as%20feature%20complexity%2C%20the%0Aorder%20in%20which%20features%20are%20learned%2C%20and%20the%20distribution%20of%20features%20over%20the%0Ainputs.%20For%20example%2C%20features%20that%20are%20simpler%20to%20compute%20or%20learned%20first%20tend%0Ato%20be%20represented%20more%20strongly%20and%20densely%20than%20features%20that%20are%20more%20complex%0Aor%20learned%20later%2C%20even%20if%20all%20features%20are%20learned%20equally%20well.%20We%20also%0Aexplore%20how%20these%20biases%20are%20affected%20by%20architectures%2C%20optimizers%2C%20and%0Atraining%20regimes%20%28e.g.%2C%20in%20transformers%2C%20features%20decoded%20earlier%20in%20the%20output%0Asequence%20also%20tend%20to%20be%20represented%20more%20strongly%29.%20Our%20results%20help%20to%0Acharacterize%20the%20inductive%20biases%20of%20gradient-based%20representation%20learning.%0AThese%20results%20also%20highlight%20a%20key%20challenge%20for%20interpretability%20%24-%24%20or%20for%0Acomparing%20the%20representations%20of%20models%20and%20brains%20%24-%24%20disentangling%20extraneous%0Abiases%20from%20the%20computationally%20important%20aspects%20of%20a%20system%27s%20internal%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05847v1&entry.124074799=Read"},
{"title": "The Perspectivist Paradigm Shift: Assumptions and Challenges of\n  Capturing Human Labels", "author": "Eve Fleisig and Su Lin Blodgett and Dan Klein and Zeerak Talat", "abstract": "  Longstanding data labeling practices in machine learning involve collecting\nand aggregating labels from multiple annotators. But what should we do when\nannotators disagree? Though annotator disagreement has long been seen as a\nproblem to minimize, new perspectivist approaches challenge this assumption by\ntreating disagreement as a valuable source of information. In this position\npaper, we examine practices and assumptions surrounding the causes of\ndisagreement--some challenged by perspectivist approaches, and some that remain\nto be addressed--as well as practical and normative challenges for work\noperating under these assumptions. We conclude with recommendations for the\ndata labeling pipeline and avenues for future research engaging with\nsubjectivity and disagreement.\n", "link": "http://arxiv.org/abs/2405.05860v1", "date": "2024-05-09", "relevancy": 1.9014, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5353}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4674}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Perspectivist%20Paradigm%20Shift%3A%20Assumptions%20and%20Challenges%20of%0A%20%20Capturing%20Human%20Labels&body=Title%3A%20The%20Perspectivist%20Paradigm%20Shift%3A%20Assumptions%20and%20Challenges%20of%0A%20%20Capturing%20Human%20Labels%0AAuthor%3A%20Eve%20Fleisig%20and%20Su%20Lin%20Blodgett%20and%20Dan%20Klein%20and%20Zeerak%20Talat%0AAbstract%3A%20%20%20Longstanding%20data%20labeling%20practices%20in%20machine%20learning%20involve%20collecting%0Aand%20aggregating%20labels%20from%20multiple%20annotators.%20But%20what%20should%20we%20do%20when%0Aannotators%20disagree%3F%20Though%20annotator%20disagreement%20has%20long%20been%20seen%20as%20a%0Aproblem%20to%20minimize%2C%20new%20perspectivist%20approaches%20challenge%20this%20assumption%20by%0Atreating%20disagreement%20as%20a%20valuable%20source%20of%20information.%20In%20this%20position%0Apaper%2C%20we%20examine%20practices%20and%20assumptions%20surrounding%20the%20causes%20of%0Adisagreement--some%20challenged%20by%20perspectivist%20approaches%2C%20and%20some%20that%20remain%0Ato%20be%20addressed--as%20well%20as%20practical%20and%20normative%20challenges%20for%20work%0Aoperating%20under%20these%20assumptions.%20We%20conclude%20with%20recommendations%20for%20the%0Adata%20labeling%20pipeline%20and%20avenues%20for%20future%20research%20engaging%20with%0Asubjectivity%20and%20disagreement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Perspectivist%2520Paradigm%2520Shift%253A%2520Assumptions%2520and%2520Challenges%2520of%250A%2520%2520Capturing%2520Human%2520Labels%26entry.906535625%3DEve%2520Fleisig%2520and%2520Su%2520Lin%2520Blodgett%2520and%2520Dan%2520Klein%2520and%2520Zeerak%2520Talat%26entry.1292438233%3D%2520%2520Longstanding%2520data%2520labeling%2520practices%2520in%2520machine%2520learning%2520involve%2520collecting%250Aand%2520aggregating%2520labels%2520from%2520multiple%2520annotators.%2520But%2520what%2520should%2520we%2520do%2520when%250Aannotators%2520disagree%253F%2520Though%2520annotator%2520disagreement%2520has%2520long%2520been%2520seen%2520as%2520a%250Aproblem%2520to%2520minimize%252C%2520new%2520perspectivist%2520approaches%2520challenge%2520this%2520assumption%2520by%250Atreating%2520disagreement%2520as%2520a%2520valuable%2520source%2520of%2520information.%2520In%2520this%2520position%250Apaper%252C%2520we%2520examine%2520practices%2520and%2520assumptions%2520surrounding%2520the%2520causes%2520of%250Adisagreement--some%2520challenged%2520by%2520perspectivist%2520approaches%252C%2520and%2520some%2520that%2520remain%250Ato%2520be%2520addressed--as%2520well%2520as%2520practical%2520and%2520normative%2520challenges%2520for%2520work%250Aoperating%2520under%2520these%2520assumptions.%2520We%2520conclude%2520with%2520recommendations%2520for%2520the%250Adata%2520labeling%2520pipeline%2520and%2520avenues%2520for%2520future%2520research%2520engaging%2520with%250Asubjectivity%2520and%2520disagreement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Perspectivist%20Paradigm%20Shift%3A%20Assumptions%20and%20Challenges%20of%0A%20%20Capturing%20Human%20Labels&entry.906535625=Eve%20Fleisig%20and%20Su%20Lin%20Blodgett%20and%20Dan%20Klein%20and%20Zeerak%20Talat&entry.1292438233=%20%20Longstanding%20data%20labeling%20practices%20in%20machine%20learning%20involve%20collecting%0Aand%20aggregating%20labels%20from%20multiple%20annotators.%20But%20what%20should%20we%20do%20when%0Aannotators%20disagree%3F%20Though%20annotator%20disagreement%20has%20long%20been%20seen%20as%20a%0Aproblem%20to%20minimize%2C%20new%20perspectivist%20approaches%20challenge%20this%20assumption%20by%0Atreating%20disagreement%20as%20a%20valuable%20source%20of%20information.%20In%20this%20position%0Apaper%2C%20we%20examine%20practices%20and%20assumptions%20surrounding%20the%20causes%20of%0Adisagreement--some%20challenged%20by%20perspectivist%20approaches%2C%20and%20some%20that%20remain%0Ato%20be%20addressed--as%20well%20as%20practical%20and%20normative%20challenges%20for%20work%0Aoperating%20under%20these%20assumptions.%20We%20conclude%20with%20recommendations%20for%20the%0Adata%20labeling%20pipeline%20and%20avenues%20for%20future%20research%20engaging%20with%0Asubjectivity%20and%20disagreement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05860v1&entry.124074799=Read"},
{"title": "Distilling Diffusion Models into Conditional GANs", "author": "Minguk Kang and Richard Zhang and Connelly Barnes and Sylvain Paris and Suha Kwak and Jaesik Park and Eli Shechtman and Jun-Yan Zhu and Taesung Park", "abstract": "  We propose a method to distill a complex multistep diffusion model into a\nsingle-step conditional GAN student model, dramatically accelerating inference,\nwhile preserving image quality. Our approach interprets diffusion distillation\nas a paired image-to-image translation task, using noise-to-image pairs of the\ndiffusion model's ODE trajectory. For efficient regression loss computation, we\npropose E-LatentLPIPS, a perceptual loss operating directly in diffusion\nmodel's latent space, utilizing an ensemble of augmentations. Furthermore, we\nadapt a diffusion model to construct a multi-scale discriminator with a text\nalignment loss to build an effective conditional GAN-based formulation.\nE-LatentLPIPS converges more efficiently than many existing distillation\nmethods, even accounting for dataset construction costs. We demonstrate that\nour one-step generator outperforms cutting-edge one-step diffusion distillation\nmodels - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark.\n", "link": "http://arxiv.org/abs/2405.05967v1", "date": "2024-05-09", "relevancy": 1.8949, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7263}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6148}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Diffusion%20Models%20into%20Conditional%20GANs&body=Title%3A%20Distilling%20Diffusion%20Models%20into%20Conditional%20GANs%0AAuthor%3A%20Minguk%20Kang%20and%20Richard%20Zhang%20and%20Connelly%20Barnes%20and%20Sylvain%20Paris%20and%20Suha%20Kwak%20and%20Jaesik%20Park%20and%20Eli%20Shechtman%20and%20Jun-Yan%20Zhu%20and%20Taesung%20Park%0AAbstract%3A%20%20%20We%20propose%20a%20method%20to%20distill%20a%20complex%20multistep%20diffusion%20model%20into%20a%0Asingle-step%20conditional%20GAN%20student%20model%2C%20dramatically%20accelerating%20inference%2C%0Awhile%20preserving%20image%20quality.%20Our%20approach%20interprets%20diffusion%20distillation%0Aas%20a%20paired%20image-to-image%20translation%20task%2C%20using%20noise-to-image%20pairs%20of%20the%0Adiffusion%20model%27s%20ODE%20trajectory.%20For%20efficient%20regression%20loss%20computation%2C%20we%0Apropose%20E-LatentLPIPS%2C%20a%20perceptual%20loss%20operating%20directly%20in%20diffusion%0Amodel%27s%20latent%20space%2C%20utilizing%20an%20ensemble%20of%20augmentations.%20Furthermore%2C%20we%0Aadapt%20a%20diffusion%20model%20to%20construct%20a%20multi-scale%20discriminator%20with%20a%20text%0Aalignment%20loss%20to%20build%20an%20effective%20conditional%20GAN-based%20formulation.%0AE-LatentLPIPS%20converges%20more%20efficiently%20than%20many%20existing%20distillation%0Amethods%2C%20even%20accounting%20for%20dataset%20construction%20costs.%20We%20demonstrate%20that%0Aour%20one-step%20generator%20outperforms%20cutting-edge%20one-step%20diffusion%20distillation%0Amodels%20-%20DMD%2C%20SDXL-Turbo%2C%20and%20SDXL-Lightning%20-%20on%20the%20zero-shot%20COCO%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Diffusion%2520Models%2520into%2520Conditional%2520GANs%26entry.906535625%3DMinguk%2520Kang%2520and%2520Richard%2520Zhang%2520and%2520Connelly%2520Barnes%2520and%2520Sylvain%2520Paris%2520and%2520Suha%2520Kwak%2520and%2520Jaesik%2520Park%2520and%2520Eli%2520Shechtman%2520and%2520Jun-Yan%2520Zhu%2520and%2520Taesung%2520Park%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520to%2520distill%2520a%2520complex%2520multistep%2520diffusion%2520model%2520into%2520a%250Asingle-step%2520conditional%2520GAN%2520student%2520model%252C%2520dramatically%2520accelerating%2520inference%252C%250Awhile%2520preserving%2520image%2520quality.%2520Our%2520approach%2520interprets%2520diffusion%2520distillation%250Aas%2520a%2520paired%2520image-to-image%2520translation%2520task%252C%2520using%2520noise-to-image%2520pairs%2520of%2520the%250Adiffusion%2520model%2527s%2520ODE%2520trajectory.%2520For%2520efficient%2520regression%2520loss%2520computation%252C%2520we%250Apropose%2520E-LatentLPIPS%252C%2520a%2520perceptual%2520loss%2520operating%2520directly%2520in%2520diffusion%250Amodel%2527s%2520latent%2520space%252C%2520utilizing%2520an%2520ensemble%2520of%2520augmentations.%2520Furthermore%252C%2520we%250Aadapt%2520a%2520diffusion%2520model%2520to%2520construct%2520a%2520multi-scale%2520discriminator%2520with%2520a%2520text%250Aalignment%2520loss%2520to%2520build%2520an%2520effective%2520conditional%2520GAN-based%2520formulation.%250AE-LatentLPIPS%2520converges%2520more%2520efficiently%2520than%2520many%2520existing%2520distillation%250Amethods%252C%2520even%2520accounting%2520for%2520dataset%2520construction%2520costs.%2520We%2520demonstrate%2520that%250Aour%2520one-step%2520generator%2520outperforms%2520cutting-edge%2520one-step%2520diffusion%2520distillation%250Amodels%2520-%2520DMD%252C%2520SDXL-Turbo%252C%2520and%2520SDXL-Lightning%2520-%2520on%2520the%2520zero-shot%2520COCO%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Diffusion%20Models%20into%20Conditional%20GANs&entry.906535625=Minguk%20Kang%20and%20Richard%20Zhang%20and%20Connelly%20Barnes%20and%20Sylvain%20Paris%20and%20Suha%20Kwak%20and%20Jaesik%20Park%20and%20Eli%20Shechtman%20and%20Jun-Yan%20Zhu%20and%20Taesung%20Park&entry.1292438233=%20%20We%20propose%20a%20method%20to%20distill%20a%20complex%20multistep%20diffusion%20model%20into%20a%0Asingle-step%20conditional%20GAN%20student%20model%2C%20dramatically%20accelerating%20inference%2C%0Awhile%20preserving%20image%20quality.%20Our%20approach%20interprets%20diffusion%20distillation%0Aas%20a%20paired%20image-to-image%20translation%20task%2C%20using%20noise-to-image%20pairs%20of%20the%0Adiffusion%20model%27s%20ODE%20trajectory.%20For%20efficient%20regression%20loss%20computation%2C%20we%0Apropose%20E-LatentLPIPS%2C%20a%20perceptual%20loss%20operating%20directly%20in%20diffusion%0Amodel%27s%20latent%20space%2C%20utilizing%20an%20ensemble%20of%20augmentations.%20Furthermore%2C%20we%0Aadapt%20a%20diffusion%20model%20to%20construct%20a%20multi-scale%20discriminator%20with%20a%20text%0Aalignment%20loss%20to%20build%20an%20effective%20conditional%20GAN-based%20formulation.%0AE-LatentLPIPS%20converges%20more%20efficiently%20than%20many%20existing%20distillation%0Amethods%2C%20even%20accounting%20for%20dataset%20construction%20costs.%20We%20demonstrate%20that%0Aour%20one-step%20generator%20outperforms%20cutting-edge%20one-step%20diffusion%20distillation%0Amodels%20-%20DMD%2C%20SDXL-Turbo%2C%20and%20SDXL-Lightning%20-%20on%20the%20zero-shot%20COCO%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05967v1&entry.124074799=Read"},
{"title": "MRISegmentator-Abdomen: A Fully Automated Multi-Organ and Structure\n  Segmentation Tool for T1-weighted Abdominal MRI", "author": "Yan Zhuang and Tejas Sudharshan Mathai and Pritam Mukherjee and Brandon Khoury and Boah Kim and Benjamin Hou and Nusrat Rabbee and Ronald M. Summers", "abstract": "  Background: Segmentation of organs and structures in abdominal MRI is useful\nfor many clinical applications, such as disease diagnosis and radiotherapy.\nCurrent approaches have focused on delineating a limited set of abdominal\nstructures (13 types). To date, there is no publicly available abdominal MRI\ndataset with voxel-level annotations of multiple organs and structures.\nConsequently, a segmentation tool for multi-structure segmentation is also\nunavailable. Methods: We curated a T1-weighted abdominal MRI dataset consisting\nof 195 patients who underwent imaging at National Institutes of Health (NIH)\nClinical Center. The dataset comprises of axial pre-contrast T1, arterial,\nvenous, and delayed phases for each patient, thereby amounting to a total of\n780 series (69,248 2D slices). Each series contains voxel-level annotations of\n62 abdominal organs and structures. A 3D nnUNet model, dubbed as\nMRISegmentator-Abdomen (MRISegmentator in short), was trained on this dataset,\nand evaluation was conducted on an internal test set and two large external\ndatasets: AMOS22 and Duke Liver. The predicted segmentations were compared\nagainst the ground-truth using the Dice Similarity Coefficient (DSC) and\nNormalized Surface Distance (NSD). Findings: MRISegmentator achieved an average\nDSC of 0.861$\\pm$0.170 and a NSD of 0.924$\\pm$0.163 in the internal test set.\nOn the AMOS22 dataset, MRISegmentator attained an average DSC of\n0.829$\\pm$0.133 and a NSD of 0.908$\\pm$0.067. For the Duke Liver dataset, an\naverage DSC of 0.933$\\pm$0.015 and a NSD of 0.929$\\pm$0.021 was obtained.\nInterpretation: The proposed MRISegmentator provides automatic, accurate, and\nrobust segmentations of 62 organs and structures in T1-weighted abdominal MRI\nsequences. The tool has the potential to accelerate research on various\nclinical topics, such as abnormality detection, radiotherapy, disease\nclassification among others.\n", "link": "http://arxiv.org/abs/2405.05944v1", "date": "2024-05-09", "relevancy": 1.8906, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4841}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4768}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRISegmentator-Abdomen%3A%20A%20Fully%20Automated%20Multi-Organ%20and%20Structure%0A%20%20Segmentation%20Tool%20for%20T1-weighted%20Abdominal%20MRI&body=Title%3A%20MRISegmentator-Abdomen%3A%20A%20Fully%20Automated%20Multi-Organ%20and%20Structure%0A%20%20Segmentation%20Tool%20for%20T1-weighted%20Abdominal%20MRI%0AAuthor%3A%20Yan%20Zhuang%20and%20Tejas%20Sudharshan%20Mathai%20and%20Pritam%20Mukherjee%20and%20Brandon%20Khoury%20and%20Boah%20Kim%20and%20Benjamin%20Hou%20and%20Nusrat%20Rabbee%20and%20Ronald%20M.%20Summers%0AAbstract%3A%20%20%20Background%3A%20Segmentation%20of%20organs%20and%20structures%20in%20abdominal%20MRI%20is%20useful%0Afor%20many%20clinical%20applications%2C%20such%20as%20disease%20diagnosis%20and%20radiotherapy.%0ACurrent%20approaches%20have%20focused%20on%20delineating%20a%20limited%20set%20of%20abdominal%0Astructures%20%2813%20types%29.%20To%20date%2C%20there%20is%20no%20publicly%20available%20abdominal%20MRI%0Adataset%20with%20voxel-level%20annotations%20of%20multiple%20organs%20and%20structures.%0AConsequently%2C%20a%20segmentation%20tool%20for%20multi-structure%20segmentation%20is%20also%0Aunavailable.%20Methods%3A%20We%20curated%20a%20T1-weighted%20abdominal%20MRI%20dataset%20consisting%0Aof%20195%20patients%20who%20underwent%20imaging%20at%20National%20Institutes%20of%20Health%20%28NIH%29%0AClinical%20Center.%20The%20dataset%20comprises%20of%20axial%20pre-contrast%20T1%2C%20arterial%2C%0Avenous%2C%20and%20delayed%20phases%20for%20each%20patient%2C%20thereby%20amounting%20to%20a%20total%20of%0A780%20series%20%2869%2C248%202D%20slices%29.%20Each%20series%20contains%20voxel-level%20annotations%20of%0A62%20abdominal%20organs%20and%20structures.%20A%203D%20nnUNet%20model%2C%20dubbed%20as%0AMRISegmentator-Abdomen%20%28MRISegmentator%20in%20short%29%2C%20was%20trained%20on%20this%20dataset%2C%0Aand%20evaluation%20was%20conducted%20on%20an%20internal%20test%20set%20and%20two%20large%20external%0Adatasets%3A%20AMOS22%20and%20Duke%20Liver.%20The%20predicted%20segmentations%20were%20compared%0Aagainst%20the%20ground-truth%20using%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29%20and%0ANormalized%20Surface%20Distance%20%28NSD%29.%20Findings%3A%20MRISegmentator%20achieved%20an%20average%0ADSC%20of%200.861%24%5Cpm%240.170%20and%20a%20NSD%20of%200.924%24%5Cpm%240.163%20in%20the%20internal%20test%20set.%0AOn%20the%20AMOS22%20dataset%2C%20MRISegmentator%20attained%20an%20average%20DSC%20of%0A0.829%24%5Cpm%240.133%20and%20a%20NSD%20of%200.908%24%5Cpm%240.067.%20For%20the%20Duke%20Liver%20dataset%2C%20an%0Aaverage%20DSC%20of%200.933%24%5Cpm%240.015%20and%20a%20NSD%20of%200.929%24%5Cpm%240.021%20was%20obtained.%0AInterpretation%3A%20The%20proposed%20MRISegmentator%20provides%20automatic%2C%20accurate%2C%20and%0Arobust%20segmentations%20of%2062%20organs%20and%20structures%20in%20T1-weighted%20abdominal%20MRI%0Asequences.%20The%20tool%20has%20the%20potential%20to%20accelerate%20research%20on%20various%0Aclinical%20topics%2C%20such%20as%20abnormality%20detection%2C%20radiotherapy%2C%20disease%0Aclassification%20among%20others.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRISegmentator-Abdomen%253A%2520A%2520Fully%2520Automated%2520Multi-Organ%2520and%2520Structure%250A%2520%2520Segmentation%2520Tool%2520for%2520T1-weighted%2520Abdominal%2520MRI%26entry.906535625%3DYan%2520Zhuang%2520and%2520Tejas%2520Sudharshan%2520Mathai%2520and%2520Pritam%2520Mukherjee%2520and%2520Brandon%2520Khoury%2520and%2520Boah%2520Kim%2520and%2520Benjamin%2520Hou%2520and%2520Nusrat%2520Rabbee%2520and%2520Ronald%2520M.%2520Summers%26entry.1292438233%3D%2520%2520Background%253A%2520Segmentation%2520of%2520organs%2520and%2520structures%2520in%2520abdominal%2520MRI%2520is%2520useful%250Afor%2520many%2520clinical%2520applications%252C%2520such%2520as%2520disease%2520diagnosis%2520and%2520radiotherapy.%250ACurrent%2520approaches%2520have%2520focused%2520on%2520delineating%2520a%2520limited%2520set%2520of%2520abdominal%250Astructures%2520%252813%2520types%2529.%2520To%2520date%252C%2520there%2520is%2520no%2520publicly%2520available%2520abdominal%2520MRI%250Adataset%2520with%2520voxel-level%2520annotations%2520of%2520multiple%2520organs%2520and%2520structures.%250AConsequently%252C%2520a%2520segmentation%2520tool%2520for%2520multi-structure%2520segmentation%2520is%2520also%250Aunavailable.%2520Methods%253A%2520We%2520curated%2520a%2520T1-weighted%2520abdominal%2520MRI%2520dataset%2520consisting%250Aof%2520195%2520patients%2520who%2520underwent%2520imaging%2520at%2520National%2520Institutes%2520of%2520Health%2520%2528NIH%2529%250AClinical%2520Center.%2520The%2520dataset%2520comprises%2520of%2520axial%2520pre-contrast%2520T1%252C%2520arterial%252C%250Avenous%252C%2520and%2520delayed%2520phases%2520for%2520each%2520patient%252C%2520thereby%2520amounting%2520to%2520a%2520total%2520of%250A780%2520series%2520%252869%252C248%25202D%2520slices%2529.%2520Each%2520series%2520contains%2520voxel-level%2520annotations%2520of%250A62%2520abdominal%2520organs%2520and%2520structures.%2520A%25203D%2520nnUNet%2520model%252C%2520dubbed%2520as%250AMRISegmentator-Abdomen%2520%2528MRISegmentator%2520in%2520short%2529%252C%2520was%2520trained%2520on%2520this%2520dataset%252C%250Aand%2520evaluation%2520was%2520conducted%2520on%2520an%2520internal%2520test%2520set%2520and%2520two%2520large%2520external%250Adatasets%253A%2520AMOS22%2520and%2520Duke%2520Liver.%2520The%2520predicted%2520segmentations%2520were%2520compared%250Aagainst%2520the%2520ground-truth%2520using%2520the%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520and%250ANormalized%2520Surface%2520Distance%2520%2528NSD%2529.%2520Findings%253A%2520MRISegmentator%2520achieved%2520an%2520average%250ADSC%2520of%25200.861%2524%255Cpm%25240.170%2520and%2520a%2520NSD%2520of%25200.924%2524%255Cpm%25240.163%2520in%2520the%2520internal%2520test%2520set.%250AOn%2520the%2520AMOS22%2520dataset%252C%2520MRISegmentator%2520attained%2520an%2520average%2520DSC%2520of%250A0.829%2524%255Cpm%25240.133%2520and%2520a%2520NSD%2520of%25200.908%2524%255Cpm%25240.067.%2520For%2520the%2520Duke%2520Liver%2520dataset%252C%2520an%250Aaverage%2520DSC%2520of%25200.933%2524%255Cpm%25240.015%2520and%2520a%2520NSD%2520of%25200.929%2524%255Cpm%25240.021%2520was%2520obtained.%250AInterpretation%253A%2520The%2520proposed%2520MRISegmentator%2520provides%2520automatic%252C%2520accurate%252C%2520and%250Arobust%2520segmentations%2520of%252062%2520organs%2520and%2520structures%2520in%2520T1-weighted%2520abdominal%2520MRI%250Asequences.%2520The%2520tool%2520has%2520the%2520potential%2520to%2520accelerate%2520research%2520on%2520various%250Aclinical%2520topics%252C%2520such%2520as%2520abnormality%2520detection%252C%2520radiotherapy%252C%2520disease%250Aclassification%2520among%2520others.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRISegmentator-Abdomen%3A%20A%20Fully%20Automated%20Multi-Organ%20and%20Structure%0A%20%20Segmentation%20Tool%20for%20T1-weighted%20Abdominal%20MRI&entry.906535625=Yan%20Zhuang%20and%20Tejas%20Sudharshan%20Mathai%20and%20Pritam%20Mukherjee%20and%20Brandon%20Khoury%20and%20Boah%20Kim%20and%20Benjamin%20Hou%20and%20Nusrat%20Rabbee%20and%20Ronald%20M.%20Summers&entry.1292438233=%20%20Background%3A%20Segmentation%20of%20organs%20and%20structures%20in%20abdominal%20MRI%20is%20useful%0Afor%20many%20clinical%20applications%2C%20such%20as%20disease%20diagnosis%20and%20radiotherapy.%0ACurrent%20approaches%20have%20focused%20on%20delineating%20a%20limited%20set%20of%20abdominal%0Astructures%20%2813%20types%29.%20To%20date%2C%20there%20is%20no%20publicly%20available%20abdominal%20MRI%0Adataset%20with%20voxel-level%20annotations%20of%20multiple%20organs%20and%20structures.%0AConsequently%2C%20a%20segmentation%20tool%20for%20multi-structure%20segmentation%20is%20also%0Aunavailable.%20Methods%3A%20We%20curated%20a%20T1-weighted%20abdominal%20MRI%20dataset%20consisting%0Aof%20195%20patients%20who%20underwent%20imaging%20at%20National%20Institutes%20of%20Health%20%28NIH%29%0AClinical%20Center.%20The%20dataset%20comprises%20of%20axial%20pre-contrast%20T1%2C%20arterial%2C%0Avenous%2C%20and%20delayed%20phases%20for%20each%20patient%2C%20thereby%20amounting%20to%20a%20total%20of%0A780%20series%20%2869%2C248%202D%20slices%29.%20Each%20series%20contains%20voxel-level%20annotations%20of%0A62%20abdominal%20organs%20and%20structures.%20A%203D%20nnUNet%20model%2C%20dubbed%20as%0AMRISegmentator-Abdomen%20%28MRISegmentator%20in%20short%29%2C%20was%20trained%20on%20this%20dataset%2C%0Aand%20evaluation%20was%20conducted%20on%20an%20internal%20test%20set%20and%20two%20large%20external%0Adatasets%3A%20AMOS22%20and%20Duke%20Liver.%20The%20predicted%20segmentations%20were%20compared%0Aagainst%20the%20ground-truth%20using%20the%20Dice%20Similarity%20Coefficient%20%28DSC%29%20and%0ANormalized%20Surface%20Distance%20%28NSD%29.%20Findings%3A%20MRISegmentator%20achieved%20an%20average%0ADSC%20of%200.861%24%5Cpm%240.170%20and%20a%20NSD%20of%200.924%24%5Cpm%240.163%20in%20the%20internal%20test%20set.%0AOn%20the%20AMOS22%20dataset%2C%20MRISegmentator%20attained%20an%20average%20DSC%20of%0A0.829%24%5Cpm%240.133%20and%20a%20NSD%20of%200.908%24%5Cpm%240.067.%20For%20the%20Duke%20Liver%20dataset%2C%20an%0Aaverage%20DSC%20of%200.933%24%5Cpm%240.015%20and%20a%20NSD%20of%200.929%24%5Cpm%240.021%20was%20obtained.%0AInterpretation%3A%20The%20proposed%20MRISegmentator%20provides%20automatic%2C%20accurate%2C%20and%0Arobust%20segmentations%20of%2062%20organs%20and%20structures%20in%20T1-weighted%20abdominal%20MRI%0Asequences.%20The%20tool%20has%20the%20potential%20to%20accelerate%20research%20on%20various%0Aclinical%20topics%2C%20such%20as%20abnormality%20detection%2C%20radiotherapy%2C%20disease%0Aclassification%20among%20others.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05944v1&entry.124074799=Read"},
{"title": "Learning Extrinsic Dexterity with Parameterized Manipulation Primitives", "author": "Shih-Min Yang and Martin Magnusson and Johannes A. Stork and Todor Stoyanov", "abstract": "  Many practically relevant robot grasping problems feature a target object for\nwhich all grasps are occluded, e.g., by the environment. Single-shot grasp\nplanning invariably fails in such scenarios. Instead, it is necessary to first\nmanipulate the object into a configuration that affords a grasp. We solve this\nproblem by learning a sequence of actions that utilize the environment to\nchange the object's pose. Concretely, we employ hierarchical reinforcement\nlearning to combine a sequence of learned parameterized manipulation\nprimitives. By learning the low-level manipulation policies, our approach can\ncontrol the object's state through exploiting interactions between the object,\nthe gripper, and the environment. Designing such a complex behavior\nanalytically would be infeasible under uncontrolled conditions, as an analytic\napproach requires accurate physical modeling of the interaction and contact\ndynamics. In contrast, we learn a hierarchical policy model that operates\ndirectly on depth perception data, without the need for object detection, pose\nestimation, or manual design of controllers. We evaluate our approach on\npicking box-shaped objects of various weight, shape, and friction properties\nfrom a constrained table-top workspace. Our method transfers to a real robot\nand is able to successfully complete the object picking task in 98\\% of\nexperimental trials. Supplementary information and videos can be found at\nhttps://shihminyang.github.io/ED-PMP/.\n", "link": "http://arxiv.org/abs/2310.17785v3", "date": "2024-05-09", "relevancy": 1.8709, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6418}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6343}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Extrinsic%20Dexterity%20with%20Parameterized%20Manipulation%20Primitives&body=Title%3A%20Learning%20Extrinsic%20Dexterity%20with%20Parameterized%20Manipulation%20Primitives%0AAuthor%3A%20Shih-Min%20Yang%20and%20Martin%20Magnusson%20and%20Johannes%20A.%20Stork%20and%20Todor%20Stoyanov%0AAbstract%3A%20%20%20Many%20practically%20relevant%20robot%20grasping%20problems%20feature%20a%20target%20object%20for%0Awhich%20all%20grasps%20are%20occluded%2C%20e.g.%2C%20by%20the%20environment.%20Single-shot%20grasp%0Aplanning%20invariably%20fails%20in%20such%20scenarios.%20Instead%2C%20it%20is%20necessary%20to%20first%0Amanipulate%20the%20object%20into%20a%20configuration%20that%20affords%20a%20grasp.%20We%20solve%20this%0Aproblem%20by%20learning%20a%20sequence%20of%20actions%20that%20utilize%20the%20environment%20to%0Achange%20the%20object%27s%20pose.%20Concretely%2C%20we%20employ%20hierarchical%20reinforcement%0Alearning%20to%20combine%20a%20sequence%20of%20learned%20parameterized%20manipulation%0Aprimitives.%20By%20learning%20the%20low-level%20manipulation%20policies%2C%20our%20approach%20can%0Acontrol%20the%20object%27s%20state%20through%20exploiting%20interactions%20between%20the%20object%2C%0Athe%20gripper%2C%20and%20the%20environment.%20Designing%20such%20a%20complex%20behavior%0Aanalytically%20would%20be%20infeasible%20under%20uncontrolled%20conditions%2C%20as%20an%20analytic%0Aapproach%20requires%20accurate%20physical%20modeling%20of%20the%20interaction%20and%20contact%0Adynamics.%20In%20contrast%2C%20we%20learn%20a%20hierarchical%20policy%20model%20that%20operates%0Adirectly%20on%20depth%20perception%20data%2C%20without%20the%20need%20for%20object%20detection%2C%20pose%0Aestimation%2C%20or%20manual%20design%20of%20controllers.%20We%20evaluate%20our%20approach%20on%0Apicking%20box-shaped%20objects%20of%20various%20weight%2C%20shape%2C%20and%20friction%20properties%0Afrom%20a%20constrained%20table-top%20workspace.%20Our%20method%20transfers%20to%20a%20real%20robot%0Aand%20is%20able%20to%20successfully%20complete%20the%20object%20picking%20task%20in%2098%5C%25%20of%0Aexperimental%20trials.%20Supplementary%20information%20and%20videos%20can%20be%20found%20at%0Ahttps%3A//shihminyang.github.io/ED-PMP/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17785v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Extrinsic%2520Dexterity%2520with%2520Parameterized%2520Manipulation%2520Primitives%26entry.906535625%3DShih-Min%2520Yang%2520and%2520Martin%2520Magnusson%2520and%2520Johannes%2520A.%2520Stork%2520and%2520Todor%2520Stoyanov%26entry.1292438233%3D%2520%2520Many%2520practically%2520relevant%2520robot%2520grasping%2520problems%2520feature%2520a%2520target%2520object%2520for%250Awhich%2520all%2520grasps%2520are%2520occluded%252C%2520e.g.%252C%2520by%2520the%2520environment.%2520Single-shot%2520grasp%250Aplanning%2520invariably%2520fails%2520in%2520such%2520scenarios.%2520Instead%252C%2520it%2520is%2520necessary%2520to%2520first%250Amanipulate%2520the%2520object%2520into%2520a%2520configuration%2520that%2520affords%2520a%2520grasp.%2520We%2520solve%2520this%250Aproblem%2520by%2520learning%2520a%2520sequence%2520of%2520actions%2520that%2520utilize%2520the%2520environment%2520to%250Achange%2520the%2520object%2527s%2520pose.%2520Concretely%252C%2520we%2520employ%2520hierarchical%2520reinforcement%250Alearning%2520to%2520combine%2520a%2520sequence%2520of%2520learned%2520parameterized%2520manipulation%250Aprimitives.%2520By%2520learning%2520the%2520low-level%2520manipulation%2520policies%252C%2520our%2520approach%2520can%250Acontrol%2520the%2520object%2527s%2520state%2520through%2520exploiting%2520interactions%2520between%2520the%2520object%252C%250Athe%2520gripper%252C%2520and%2520the%2520environment.%2520Designing%2520such%2520a%2520complex%2520behavior%250Aanalytically%2520would%2520be%2520infeasible%2520under%2520uncontrolled%2520conditions%252C%2520as%2520an%2520analytic%250Aapproach%2520requires%2520accurate%2520physical%2520modeling%2520of%2520the%2520interaction%2520and%2520contact%250Adynamics.%2520In%2520contrast%252C%2520we%2520learn%2520a%2520hierarchical%2520policy%2520model%2520that%2520operates%250Adirectly%2520on%2520depth%2520perception%2520data%252C%2520without%2520the%2520need%2520for%2520object%2520detection%252C%2520pose%250Aestimation%252C%2520or%2520manual%2520design%2520of%2520controllers.%2520We%2520evaluate%2520our%2520approach%2520on%250Apicking%2520box-shaped%2520objects%2520of%2520various%2520weight%252C%2520shape%252C%2520and%2520friction%2520properties%250Afrom%2520a%2520constrained%2520table-top%2520workspace.%2520Our%2520method%2520transfers%2520to%2520a%2520real%2520robot%250Aand%2520is%2520able%2520to%2520successfully%2520complete%2520the%2520object%2520picking%2520task%2520in%252098%255C%2525%2520of%250Aexperimental%2520trials.%2520Supplementary%2520information%2520and%2520videos%2520can%2520be%2520found%2520at%250Ahttps%253A//shihminyang.github.io/ED-PMP/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17785v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Extrinsic%20Dexterity%20with%20Parameterized%20Manipulation%20Primitives&entry.906535625=Shih-Min%20Yang%20and%20Martin%20Magnusson%20and%20Johannes%20A.%20Stork%20and%20Todor%20Stoyanov&entry.1292438233=%20%20Many%20practically%20relevant%20robot%20grasping%20problems%20feature%20a%20target%20object%20for%0Awhich%20all%20grasps%20are%20occluded%2C%20e.g.%2C%20by%20the%20environment.%20Single-shot%20grasp%0Aplanning%20invariably%20fails%20in%20such%20scenarios.%20Instead%2C%20it%20is%20necessary%20to%20first%0Amanipulate%20the%20object%20into%20a%20configuration%20that%20affords%20a%20grasp.%20We%20solve%20this%0Aproblem%20by%20learning%20a%20sequence%20of%20actions%20that%20utilize%20the%20environment%20to%0Achange%20the%20object%27s%20pose.%20Concretely%2C%20we%20employ%20hierarchical%20reinforcement%0Alearning%20to%20combine%20a%20sequence%20of%20learned%20parameterized%20manipulation%0Aprimitives.%20By%20learning%20the%20low-level%20manipulation%20policies%2C%20our%20approach%20can%0Acontrol%20the%20object%27s%20state%20through%20exploiting%20interactions%20between%20the%20object%2C%0Athe%20gripper%2C%20and%20the%20environment.%20Designing%20such%20a%20complex%20behavior%0Aanalytically%20would%20be%20infeasible%20under%20uncontrolled%20conditions%2C%20as%20an%20analytic%0Aapproach%20requires%20accurate%20physical%20modeling%20of%20the%20interaction%20and%20contact%0Adynamics.%20In%20contrast%2C%20we%20learn%20a%20hierarchical%20policy%20model%20that%20operates%0Adirectly%20on%20depth%20perception%20data%2C%20without%20the%20need%20for%20object%20detection%2C%20pose%0Aestimation%2C%20or%20manual%20design%20of%20controllers.%20We%20evaluate%20our%20approach%20on%0Apicking%20box-shaped%20objects%20of%20various%20weight%2C%20shape%2C%20and%20friction%20properties%0Afrom%20a%20constrained%20table-top%20workspace.%20Our%20method%20transfers%20to%20a%20real%20robot%0Aand%20is%20able%20to%20successfully%20complete%20the%20object%20picking%20task%20in%2098%5C%25%20of%0Aexperimental%20trials.%20Supplementary%20information%20and%20videos%20can%20be%20found%20at%0Ahttps%3A//shihminyang.github.io/ED-PMP/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17785v3&entry.124074799=Read"},
{"title": "Lumina-T2X: Transforming Text into Any Modality, Resolution, and\n  Duration via Flow-based Large Diffusion Transformers", "author": "Peng Gao and Le Zhuo and Ziyi Lin and Chris Liu and Junsong Chen and Ruoyi Du and Enze Xie and Xu Luo and Longtian Qiu and Yuhang Zhang and Chen Lin and Rongjie Huang and Shijie Geng and Renrui Zhang and Junlin Xi and Wenqi Shao and Zhengkai Jiang and Tianshuo Yang and Weicai Ye and He Tong and Jingwen He and Yu Qiao and Hongsheng Li", "abstract": "  Sora unveils the potential of scaling Diffusion Transformer for generating\nphotorealistic images and videos at arbitrary resolutions, aspect ratios, and\ndurations, yet it still lacks sufficient implementation details. In this\ntechnical report, we introduce the Lumina-T2X family - a series of Flow-based\nLarge Diffusion Transformers (Flag-DiT) equipped with zero-initialized\nattention, as a unified framework designed to transform noise into images,\nvideos, multi-view 3D objects, and audio clips conditioned on text\ninstructions. By tokenizing the latent spatial-temporal space and incorporating\nlearnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X\nseamlessly unifies the representations of different modalities across various\nspatial-temporal resolutions. This unified approach enables training within a\nsingle framework for different modalities and allows for flexible generation of\nmultimodal data at any resolution, aspect ratio, and length during inference.\nAdvanced techniques like RoPE, RMSNorm, and flow matching enhance the\nstability, flexibility, and scalability of Flag-DiT, enabling models of\nLumina-T2X to scale up to 7 billion parameters and extend the context window to\n128K tokens. This is particularly beneficial for creating ultra-high-definition\nimages with our Lumina-T2I model and long 720p videos with our Lumina-T2V\nmodel. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT,\nrequires only 35% of the training computational costs of a\n600-million-parameter naive DiT. Our further comprehensive analysis underscores\nLumina-T2X's preliminary capability in resolution extrapolation,\nhigh-resolution editing, generating consistent 3D views, and synthesizing\nvideos with seamless transitions. We expect that the open-sourcing of\nLumina-T2X will further foster creativity, transparency, and diversity in the\ngenerative AI community.\n", "link": "http://arxiv.org/abs/2405.05945v1", "date": "2024-05-09", "relevancy": 1.8396, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6869}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6076}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumina-T2X%3A%20Transforming%20Text%20into%20Any%20Modality%2C%20Resolution%2C%20and%0A%20%20Duration%20via%20Flow-based%20Large%20Diffusion%20Transformers&body=Title%3A%20Lumina-T2X%3A%20Transforming%20Text%20into%20Any%20Modality%2C%20Resolution%2C%20and%0A%20%20Duration%20via%20Flow-based%20Large%20Diffusion%20Transformers%0AAuthor%3A%20Peng%20Gao%20and%20Le%20Zhuo%20and%20Ziyi%20Lin%20and%20Chris%20Liu%20and%20Junsong%20Chen%20and%20Ruoyi%20Du%20and%20Enze%20Xie%20and%20Xu%20Luo%20and%20Longtian%20Qiu%20and%20Yuhang%20Zhang%20and%20Chen%20Lin%20and%20Rongjie%20Huang%20and%20Shijie%20Geng%20and%20Renrui%20Zhang%20and%20Junlin%20Xi%20and%20Wenqi%20Shao%20and%20Zhengkai%20Jiang%20and%20Tianshuo%20Yang%20and%20Weicai%20Ye%20and%20He%20Tong%20and%20Jingwen%20He%20and%20Yu%20Qiao%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Sora%20unveils%20the%20potential%20of%20scaling%20Diffusion%20Transformer%20for%20generating%0Aphotorealistic%20images%20and%20videos%20at%20arbitrary%20resolutions%2C%20aspect%20ratios%2C%20and%0Adurations%2C%20yet%20it%20still%20lacks%20sufficient%20implementation%20details.%20In%20this%0Atechnical%20report%2C%20we%20introduce%20the%20Lumina-T2X%20family%20-%20a%20series%20of%20Flow-based%0ALarge%20Diffusion%20Transformers%20%28Flag-DiT%29%20equipped%20with%20zero-initialized%0Aattention%2C%20as%20a%20unified%20framework%20designed%20to%20transform%20noise%20into%20images%2C%0Avideos%2C%20multi-view%203D%20objects%2C%20and%20audio%20clips%20conditioned%20on%20text%0Ainstructions.%20By%20tokenizing%20the%20latent%20spatial-temporal%20space%20and%20incorporating%0Alearnable%20placeholders%20such%20as%20%5Bnextline%5D%20and%20%5Bnextframe%5D%20tokens%2C%20Lumina-T2X%0Aseamlessly%20unifies%20the%20representations%20of%20different%20modalities%20across%20various%0Aspatial-temporal%20resolutions.%20This%20unified%20approach%20enables%20training%20within%20a%0Asingle%20framework%20for%20different%20modalities%20and%20allows%20for%20flexible%20generation%20of%0Amultimodal%20data%20at%20any%20resolution%2C%20aspect%20ratio%2C%20and%20length%20during%20inference.%0AAdvanced%20techniques%20like%20RoPE%2C%20RMSNorm%2C%20and%20flow%20matching%20enhance%20the%0Astability%2C%20flexibility%2C%20and%20scalability%20of%20Flag-DiT%2C%20enabling%20models%20of%0ALumina-T2X%20to%20scale%20up%20to%207%20billion%20parameters%20and%20extend%20the%20context%20window%20to%0A128K%20tokens.%20This%20is%20particularly%20beneficial%20for%20creating%20ultra-high-definition%0Aimages%20with%20our%20Lumina-T2I%20model%20and%20long%20720p%20videos%20with%20our%20Lumina-T2V%0Amodel.%20Remarkably%2C%20Lumina-T2I%2C%20powered%20by%20a%205-billion-parameter%20Flag-DiT%2C%0Arequires%20only%2035%25%20of%20the%20training%20computational%20costs%20of%20a%0A600-million-parameter%20naive%20DiT.%20Our%20further%20comprehensive%20analysis%20underscores%0ALumina-T2X%27s%20preliminary%20capability%20in%20resolution%20extrapolation%2C%0Ahigh-resolution%20editing%2C%20generating%20consistent%203D%20views%2C%20and%20synthesizing%0Avideos%20with%20seamless%20transitions.%20We%20expect%20that%20the%20open-sourcing%20of%0ALumina-T2X%20will%20further%20foster%20creativity%2C%20transparency%2C%20and%20diversity%20in%20the%0Agenerative%20AI%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumina-T2X%253A%2520Transforming%2520Text%2520into%2520Any%2520Modality%252C%2520Resolution%252C%2520and%250A%2520%2520Duration%2520via%2520Flow-based%2520Large%2520Diffusion%2520Transformers%26entry.906535625%3DPeng%2520Gao%2520and%2520Le%2520Zhuo%2520and%2520Ziyi%2520Lin%2520and%2520Chris%2520Liu%2520and%2520Junsong%2520Chen%2520and%2520Ruoyi%2520Du%2520and%2520Enze%2520Xie%2520and%2520Xu%2520Luo%2520and%2520Longtian%2520Qiu%2520and%2520Yuhang%2520Zhang%2520and%2520Chen%2520Lin%2520and%2520Rongjie%2520Huang%2520and%2520Shijie%2520Geng%2520and%2520Renrui%2520Zhang%2520and%2520Junlin%2520Xi%2520and%2520Wenqi%2520Shao%2520and%2520Zhengkai%2520Jiang%2520and%2520Tianshuo%2520Yang%2520and%2520Weicai%2520Ye%2520and%2520He%2520Tong%2520and%2520Jingwen%2520He%2520and%2520Yu%2520Qiao%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Sora%2520unveils%2520the%2520potential%2520of%2520scaling%2520Diffusion%2520Transformer%2520for%2520generating%250Aphotorealistic%2520images%2520and%2520videos%2520at%2520arbitrary%2520resolutions%252C%2520aspect%2520ratios%252C%2520and%250Adurations%252C%2520yet%2520it%2520still%2520lacks%2520sufficient%2520implementation%2520details.%2520In%2520this%250Atechnical%2520report%252C%2520we%2520introduce%2520the%2520Lumina-T2X%2520family%2520-%2520a%2520series%2520of%2520Flow-based%250ALarge%2520Diffusion%2520Transformers%2520%2528Flag-DiT%2529%2520equipped%2520with%2520zero-initialized%250Aattention%252C%2520as%2520a%2520unified%2520framework%2520designed%2520to%2520transform%2520noise%2520into%2520images%252C%250Avideos%252C%2520multi-view%25203D%2520objects%252C%2520and%2520audio%2520clips%2520conditioned%2520on%2520text%250Ainstructions.%2520By%2520tokenizing%2520the%2520latent%2520spatial-temporal%2520space%2520and%2520incorporating%250Alearnable%2520placeholders%2520such%2520as%2520%255Bnextline%255D%2520and%2520%255Bnextframe%255D%2520tokens%252C%2520Lumina-T2X%250Aseamlessly%2520unifies%2520the%2520representations%2520of%2520different%2520modalities%2520across%2520various%250Aspatial-temporal%2520resolutions.%2520This%2520unified%2520approach%2520enables%2520training%2520within%2520a%250Asingle%2520framework%2520for%2520different%2520modalities%2520and%2520allows%2520for%2520flexible%2520generation%2520of%250Amultimodal%2520data%2520at%2520any%2520resolution%252C%2520aspect%2520ratio%252C%2520and%2520length%2520during%2520inference.%250AAdvanced%2520techniques%2520like%2520RoPE%252C%2520RMSNorm%252C%2520and%2520flow%2520matching%2520enhance%2520the%250Astability%252C%2520flexibility%252C%2520and%2520scalability%2520of%2520Flag-DiT%252C%2520enabling%2520models%2520of%250ALumina-T2X%2520to%2520scale%2520up%2520to%25207%2520billion%2520parameters%2520and%2520extend%2520the%2520context%2520window%2520to%250A128K%2520tokens.%2520This%2520is%2520particularly%2520beneficial%2520for%2520creating%2520ultra-high-definition%250Aimages%2520with%2520our%2520Lumina-T2I%2520model%2520and%2520long%2520720p%2520videos%2520with%2520our%2520Lumina-T2V%250Amodel.%2520Remarkably%252C%2520Lumina-T2I%252C%2520powered%2520by%2520a%25205-billion-parameter%2520Flag-DiT%252C%250Arequires%2520only%252035%2525%2520of%2520the%2520training%2520computational%2520costs%2520of%2520a%250A600-million-parameter%2520naive%2520DiT.%2520Our%2520further%2520comprehensive%2520analysis%2520underscores%250ALumina-T2X%2527s%2520preliminary%2520capability%2520in%2520resolution%2520extrapolation%252C%250Ahigh-resolution%2520editing%252C%2520generating%2520consistent%25203D%2520views%252C%2520and%2520synthesizing%250Avideos%2520with%2520seamless%2520transitions.%2520We%2520expect%2520that%2520the%2520open-sourcing%2520of%250ALumina-T2X%2520will%2520further%2520foster%2520creativity%252C%2520transparency%252C%2520and%2520diversity%2520in%2520the%250Agenerative%2520AI%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumina-T2X%3A%20Transforming%20Text%20into%20Any%20Modality%2C%20Resolution%2C%20and%0A%20%20Duration%20via%20Flow-based%20Large%20Diffusion%20Transformers&entry.906535625=Peng%20Gao%20and%20Le%20Zhuo%20and%20Ziyi%20Lin%20and%20Chris%20Liu%20and%20Junsong%20Chen%20and%20Ruoyi%20Du%20and%20Enze%20Xie%20and%20Xu%20Luo%20and%20Longtian%20Qiu%20and%20Yuhang%20Zhang%20and%20Chen%20Lin%20and%20Rongjie%20Huang%20and%20Shijie%20Geng%20and%20Renrui%20Zhang%20and%20Junlin%20Xi%20and%20Wenqi%20Shao%20and%20Zhengkai%20Jiang%20and%20Tianshuo%20Yang%20and%20Weicai%20Ye%20and%20He%20Tong%20and%20Jingwen%20He%20and%20Yu%20Qiao%20and%20Hongsheng%20Li&entry.1292438233=%20%20Sora%20unveils%20the%20potential%20of%20scaling%20Diffusion%20Transformer%20for%20generating%0Aphotorealistic%20images%20and%20videos%20at%20arbitrary%20resolutions%2C%20aspect%20ratios%2C%20and%0Adurations%2C%20yet%20it%20still%20lacks%20sufficient%20implementation%20details.%20In%20this%0Atechnical%20report%2C%20we%20introduce%20the%20Lumina-T2X%20family%20-%20a%20series%20of%20Flow-based%0ALarge%20Diffusion%20Transformers%20%28Flag-DiT%29%20equipped%20with%20zero-initialized%0Aattention%2C%20as%20a%20unified%20framework%20designed%20to%20transform%20noise%20into%20images%2C%0Avideos%2C%20multi-view%203D%20objects%2C%20and%20audio%20clips%20conditioned%20on%20text%0Ainstructions.%20By%20tokenizing%20the%20latent%20spatial-temporal%20space%20and%20incorporating%0Alearnable%20placeholders%20such%20as%20%5Bnextline%5D%20and%20%5Bnextframe%5D%20tokens%2C%20Lumina-T2X%0Aseamlessly%20unifies%20the%20representations%20of%20different%20modalities%20across%20various%0Aspatial-temporal%20resolutions.%20This%20unified%20approach%20enables%20training%20within%20a%0Asingle%20framework%20for%20different%20modalities%20and%20allows%20for%20flexible%20generation%20of%0Amultimodal%20data%20at%20any%20resolution%2C%20aspect%20ratio%2C%20and%20length%20during%20inference.%0AAdvanced%20techniques%20like%20RoPE%2C%20RMSNorm%2C%20and%20flow%20matching%20enhance%20the%0Astability%2C%20flexibility%2C%20and%20scalability%20of%20Flag-DiT%2C%20enabling%20models%20of%0ALumina-T2X%20to%20scale%20up%20to%207%20billion%20parameters%20and%20extend%20the%20context%20window%20to%0A128K%20tokens.%20This%20is%20particularly%20beneficial%20for%20creating%20ultra-high-definition%0Aimages%20with%20our%20Lumina-T2I%20model%20and%20long%20720p%20videos%20with%20our%20Lumina-T2V%0Amodel.%20Remarkably%2C%20Lumina-T2I%2C%20powered%20by%20a%205-billion-parameter%20Flag-DiT%2C%0Arequires%20only%2035%25%20of%20the%20training%20computational%20costs%20of%20a%0A600-million-parameter%20naive%20DiT.%20Our%20further%20comprehensive%20analysis%20underscores%0ALumina-T2X%27s%20preliminary%20capability%20in%20resolution%20extrapolation%2C%0Ahigh-resolution%20editing%2C%20generating%20consistent%203D%20views%2C%20and%20synthesizing%0Avideos%20with%20seamless%20transitions.%20We%20expect%20that%20the%20open-sourcing%20of%0ALumina-T2X%20will%20further%20foster%20creativity%2C%20transparency%2C%20and%20diversity%20in%20the%0Agenerative%20AI%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05945v1&entry.124074799=Read"},
{"title": "Federated Combinatorial Multi-Agent Multi-Armed Bandits", "author": "Fares Fourati and Mohamed-Slim Alouini and Vaneet Aggarwal", "abstract": "  This paper introduces a federated learning framework tailored for online\ncombinatorial optimization with bandit feedback. In this setting, agents select\nsubsets of arms, observe noisy rewards for these subsets without accessing\nindividual arm information, and can cooperate and share information at specific\nintervals. Our framework transforms any offline resilient single-agent\n$(\\alpha-\\epsilon)$-approximation algorithm, having a complexity of\n$\\tilde{\\mathcal{O}}(\\frac{\\psi}{\\epsilon^\\beta})$, where the logarithm is\nomitted, for some function $\\psi$ and constant $\\beta$, into an online\nmulti-agent algorithm with $m$ communicating agents and an $\\alpha$-regret of\nno more than $\\tilde{\\mathcal{O}}(m^{-\\frac{1}{3+\\beta}} \\psi^\\frac{1}{3+\\beta}\nT^\\frac{2+\\beta}{3+\\beta})$. This approach not only eliminates the $\\epsilon$\napproximation error but also ensures sublinear growth with respect to the time\nhorizon $T$ and demonstrates a linear speedup with an increasing number of\ncommunicating agents. Additionally, the algorithm is notably\ncommunication-efficient, requiring only a sublinear number of communication\nrounds, quantified as $\\tilde{\\mathcal{O}}\\left(\\psi\nT^\\frac{\\beta}{\\beta+1}\\right)$. Furthermore, the framework has been\nsuccessfully applied to online stochastic submodular maximization using various\noffline algorithms, yielding the first results for both single-agent and\nmulti-agent settings and recovering specialized single-agent theoretical\nguarantees. We empirically validate our approach to a stochastic data\nsummarization problem, illustrating the effectiveness of the proposed\nframework, even in single-agent scenarios.\n", "link": "http://arxiv.org/abs/2405.05950v1", "date": "2024-05-09", "relevancy": 1.8318, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4533}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Combinatorial%20Multi-Agent%20Multi-Armed%20Bandits&body=Title%3A%20Federated%20Combinatorial%20Multi-Agent%20Multi-Armed%20Bandits%0AAuthor%3A%20Fares%20Fourati%20and%20Mohamed-Slim%20Alouini%20and%20Vaneet%20Aggarwal%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20federated%20learning%20framework%20tailored%20for%20online%0Acombinatorial%20optimization%20with%20bandit%20feedback.%20In%20this%20setting%2C%20agents%20select%0Asubsets%20of%20arms%2C%20observe%20noisy%20rewards%20for%20these%20subsets%20without%20accessing%0Aindividual%20arm%20information%2C%20and%20can%20cooperate%20and%20share%20information%20at%20specific%0Aintervals.%20Our%20framework%20transforms%20any%20offline%20resilient%20single-agent%0A%24%28%5Calpha-%5Cepsilon%29%24-approximation%20algorithm%2C%20having%20a%20complexity%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cfrac%7B%5Cpsi%7D%7B%5Cepsilon%5E%5Cbeta%7D%29%24%2C%20where%20the%20logarithm%20is%0Aomitted%2C%20for%20some%20function%20%24%5Cpsi%24%20and%20constant%20%24%5Cbeta%24%2C%20into%20an%20online%0Amulti-agent%20algorithm%20with%20%24m%24%20communicating%20agents%20and%20an%20%24%5Calpha%24-regret%20of%0Ano%20more%20than%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28m%5E%7B-%5Cfrac%7B1%7D%7B3%2B%5Cbeta%7D%7D%20%5Cpsi%5E%5Cfrac%7B1%7D%7B3%2B%5Cbeta%7D%0AT%5E%5Cfrac%7B2%2B%5Cbeta%7D%7B3%2B%5Cbeta%7D%29%24.%20This%20approach%20not%20only%20eliminates%20the%20%24%5Cepsilon%24%0Aapproximation%20error%20but%20also%20ensures%20sublinear%20growth%20with%20respect%20to%20the%20time%0Ahorizon%20%24T%24%20and%20demonstrates%20a%20linear%20speedup%20with%20an%20increasing%20number%20of%0Acommunicating%20agents.%20Additionally%2C%20the%20algorithm%20is%20notably%0Acommunication-efficient%2C%20requiring%20only%20a%20sublinear%20number%20of%20communication%0Arounds%2C%20quantified%20as%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%5Cleft%28%5Cpsi%0AT%5E%5Cfrac%7B%5Cbeta%7D%7B%5Cbeta%2B1%7D%5Cright%29%24.%20Furthermore%2C%20the%20framework%20has%20been%0Asuccessfully%20applied%20to%20online%20stochastic%20submodular%20maximization%20using%20various%0Aoffline%20algorithms%2C%20yielding%20the%20first%20results%20for%20both%20single-agent%20and%0Amulti-agent%20settings%20and%20recovering%20specialized%20single-agent%20theoretical%0Aguarantees.%20We%20empirically%20validate%20our%20approach%20to%20a%20stochastic%20data%0Asummarization%20problem%2C%20illustrating%20the%20effectiveness%20of%20the%20proposed%0Aframework%2C%20even%20in%20single-agent%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Combinatorial%2520Multi-Agent%2520Multi-Armed%2520Bandits%26entry.906535625%3DFares%2520Fourati%2520and%2520Mohamed-Slim%2520Alouini%2520and%2520Vaneet%2520Aggarwal%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520federated%2520learning%2520framework%2520tailored%2520for%2520online%250Acombinatorial%2520optimization%2520with%2520bandit%2520feedback.%2520In%2520this%2520setting%252C%2520agents%2520select%250Asubsets%2520of%2520arms%252C%2520observe%2520noisy%2520rewards%2520for%2520these%2520subsets%2520without%2520accessing%250Aindividual%2520arm%2520information%252C%2520and%2520can%2520cooperate%2520and%2520share%2520information%2520at%2520specific%250Aintervals.%2520Our%2520framework%2520transforms%2520any%2520offline%2520resilient%2520single-agent%250A%2524%2528%255Calpha-%255Cepsilon%2529%2524-approximation%2520algorithm%252C%2520having%2520a%2520complexity%2520of%250A%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528%255Cfrac%257B%255Cpsi%257D%257B%255Cepsilon%255E%255Cbeta%257D%2529%2524%252C%2520where%2520the%2520logarithm%2520is%250Aomitted%252C%2520for%2520some%2520function%2520%2524%255Cpsi%2524%2520and%2520constant%2520%2524%255Cbeta%2524%252C%2520into%2520an%2520online%250Amulti-agent%2520algorithm%2520with%2520%2524m%2524%2520communicating%2520agents%2520and%2520an%2520%2524%255Calpha%2524-regret%2520of%250Ano%2520more%2520than%2520%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528m%255E%257B-%255Cfrac%257B1%257D%257B3%252B%255Cbeta%257D%257D%2520%255Cpsi%255E%255Cfrac%257B1%257D%257B3%252B%255Cbeta%257D%250AT%255E%255Cfrac%257B2%252B%255Cbeta%257D%257B3%252B%255Cbeta%257D%2529%2524.%2520This%2520approach%2520not%2520only%2520eliminates%2520the%2520%2524%255Cepsilon%2524%250Aapproximation%2520error%2520but%2520also%2520ensures%2520sublinear%2520growth%2520with%2520respect%2520to%2520the%2520time%250Ahorizon%2520%2524T%2524%2520and%2520demonstrates%2520a%2520linear%2520speedup%2520with%2520an%2520increasing%2520number%2520of%250Acommunicating%2520agents.%2520Additionally%252C%2520the%2520algorithm%2520is%2520notably%250Acommunication-efficient%252C%2520requiring%2520only%2520a%2520sublinear%2520number%2520of%2520communication%250Arounds%252C%2520quantified%2520as%2520%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%255Cleft%2528%255Cpsi%250AT%255E%255Cfrac%257B%255Cbeta%257D%257B%255Cbeta%252B1%257D%255Cright%2529%2524.%2520Furthermore%252C%2520the%2520framework%2520has%2520been%250Asuccessfully%2520applied%2520to%2520online%2520stochastic%2520submodular%2520maximization%2520using%2520various%250Aoffline%2520algorithms%252C%2520yielding%2520the%2520first%2520results%2520for%2520both%2520single-agent%2520and%250Amulti-agent%2520settings%2520and%2520recovering%2520specialized%2520single-agent%2520theoretical%250Aguarantees.%2520We%2520empirically%2520validate%2520our%2520approach%2520to%2520a%2520stochastic%2520data%250Asummarization%2520problem%252C%2520illustrating%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aframework%252C%2520even%2520in%2520single-agent%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Combinatorial%20Multi-Agent%20Multi-Armed%20Bandits&entry.906535625=Fares%20Fourati%20and%20Mohamed-Slim%20Alouini%20and%20Vaneet%20Aggarwal&entry.1292438233=%20%20This%20paper%20introduces%20a%20federated%20learning%20framework%20tailored%20for%20online%0Acombinatorial%20optimization%20with%20bandit%20feedback.%20In%20this%20setting%2C%20agents%20select%0Asubsets%20of%20arms%2C%20observe%20noisy%20rewards%20for%20these%20subsets%20without%20accessing%0Aindividual%20arm%20information%2C%20and%20can%20cooperate%20and%20share%20information%20at%20specific%0Aintervals.%20Our%20framework%20transforms%20any%20offline%20resilient%20single-agent%0A%24%28%5Calpha-%5Cepsilon%29%24-approximation%20algorithm%2C%20having%20a%20complexity%20of%0A%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28%5Cfrac%7B%5Cpsi%7D%7B%5Cepsilon%5E%5Cbeta%7D%29%24%2C%20where%20the%20logarithm%20is%0Aomitted%2C%20for%20some%20function%20%24%5Cpsi%24%20and%20constant%20%24%5Cbeta%24%2C%20into%20an%20online%0Amulti-agent%20algorithm%20with%20%24m%24%20communicating%20agents%20and%20an%20%24%5Calpha%24-regret%20of%0Ano%20more%20than%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28m%5E%7B-%5Cfrac%7B1%7D%7B3%2B%5Cbeta%7D%7D%20%5Cpsi%5E%5Cfrac%7B1%7D%7B3%2B%5Cbeta%7D%0AT%5E%5Cfrac%7B2%2B%5Cbeta%7D%7B3%2B%5Cbeta%7D%29%24.%20This%20approach%20not%20only%20eliminates%20the%20%24%5Cepsilon%24%0Aapproximation%20error%20but%20also%20ensures%20sublinear%20growth%20with%20respect%20to%20the%20time%0Ahorizon%20%24T%24%20and%20demonstrates%20a%20linear%20speedup%20with%20an%20increasing%20number%20of%0Acommunicating%20agents.%20Additionally%2C%20the%20algorithm%20is%20notably%0Acommunication-efficient%2C%20requiring%20only%20a%20sublinear%20number%20of%20communication%0Arounds%2C%20quantified%20as%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%5Cleft%28%5Cpsi%0AT%5E%5Cfrac%7B%5Cbeta%7D%7B%5Cbeta%2B1%7D%5Cright%29%24.%20Furthermore%2C%20the%20framework%20has%20been%0Asuccessfully%20applied%20to%20online%20stochastic%20submodular%20maximization%20using%20various%0Aoffline%20algorithms%2C%20yielding%20the%20first%20results%20for%20both%20single-agent%20and%0Amulti-agent%20settings%20and%20recovering%20specialized%20single-agent%20theoretical%0Aguarantees.%20We%20empirically%20validate%20our%20approach%20to%20a%20stochastic%20data%0Asummarization%20problem%2C%20illustrating%20the%20effectiveness%20of%20the%20proposed%0Aframework%2C%20even%20in%20single-agent%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05950v1&entry.124074799=Read"},
{"title": "A Multi-Level Superoptimizer for Tensor Programs", "author": "Mengdi Wu and Xinhao Cheng and Oded Padon and Zhihao Jia", "abstract": "  We introduce Mirage, the first multi-level superoptimizer for tensor\nprograms. A key idea in Mirage is $\\mu$Graphs, a uniform representation of\ntensor programs at the kernel, thread block, and thread levels of the GPU\ncompute hierarchy. $\\mu$Graphs enable Mirage to discover novel optimizations\nthat combine algebraic transformations, schedule transformations, and\ngeneration of new custom kernels. To navigate the large search space, Mirage\nintroduces a pruning technique based on abstraction that significantly reduces\nthe search space and provides a certain optimality guarantee. To ensure that\nthe optimized $\\mu$Graph is equivalent to the input program, Mirage introduces\na probabilistic equivalence verification procedure with strong theoretical\nguarantees. Our evaluation shows that Mirage outperforms existing approaches by\nup to 3.5$\\times$ even for DNNs that are widely used and heavily optimized.\nMirage is publicly available at https://github.com/mirage-project/mirage.\n", "link": "http://arxiv.org/abs/2405.05751v1", "date": "2024-05-09", "relevancy": 1.829, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4768}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4514}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Level%20Superoptimizer%20for%20Tensor%20Programs&body=Title%3A%20A%20Multi-Level%20Superoptimizer%20for%20Tensor%20Programs%0AAuthor%3A%20Mengdi%20Wu%20and%20Xinhao%20Cheng%20and%20Oded%20Padon%20and%20Zhihao%20Jia%0AAbstract%3A%20%20%20We%20introduce%20Mirage%2C%20the%20first%20multi-level%20superoptimizer%20for%20tensor%0Aprograms.%20A%20key%20idea%20in%20Mirage%20is%20%24%5Cmu%24Graphs%2C%20a%20uniform%20representation%20of%0Atensor%20programs%20at%20the%20kernel%2C%20thread%20block%2C%20and%20thread%20levels%20of%20the%20GPU%0Acompute%20hierarchy.%20%24%5Cmu%24Graphs%20enable%20Mirage%20to%20discover%20novel%20optimizations%0Athat%20combine%20algebraic%20transformations%2C%20schedule%20transformations%2C%20and%0Ageneration%20of%20new%20custom%20kernels.%20To%20navigate%20the%20large%20search%20space%2C%20Mirage%0Aintroduces%20a%20pruning%20technique%20based%20on%20abstraction%20that%20significantly%20reduces%0Athe%20search%20space%20and%20provides%20a%20certain%20optimality%20guarantee.%20To%20ensure%20that%0Athe%20optimized%20%24%5Cmu%24Graph%20is%20equivalent%20to%20the%20input%20program%2C%20Mirage%20introduces%0Aa%20probabilistic%20equivalence%20verification%20procedure%20with%20strong%20theoretical%0Aguarantees.%20Our%20evaluation%20shows%20that%20Mirage%20outperforms%20existing%20approaches%20by%0Aup%20to%203.5%24%5Ctimes%24%20even%20for%20DNNs%20that%20are%20widely%20used%20and%20heavily%20optimized.%0AMirage%20is%20publicly%20available%20at%20https%3A//github.com/mirage-project/mirage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Level%2520Superoptimizer%2520for%2520Tensor%2520Programs%26entry.906535625%3DMengdi%2520Wu%2520and%2520Xinhao%2520Cheng%2520and%2520Oded%2520Padon%2520and%2520Zhihao%2520Jia%26entry.1292438233%3D%2520%2520We%2520introduce%2520Mirage%252C%2520the%2520first%2520multi-level%2520superoptimizer%2520for%2520tensor%250Aprograms.%2520A%2520key%2520idea%2520in%2520Mirage%2520is%2520%2524%255Cmu%2524Graphs%252C%2520a%2520uniform%2520representation%2520of%250Atensor%2520programs%2520at%2520the%2520kernel%252C%2520thread%2520block%252C%2520and%2520thread%2520levels%2520of%2520the%2520GPU%250Acompute%2520hierarchy.%2520%2524%255Cmu%2524Graphs%2520enable%2520Mirage%2520to%2520discover%2520novel%2520optimizations%250Athat%2520combine%2520algebraic%2520transformations%252C%2520schedule%2520transformations%252C%2520and%250Ageneration%2520of%2520new%2520custom%2520kernels.%2520To%2520navigate%2520the%2520large%2520search%2520space%252C%2520Mirage%250Aintroduces%2520a%2520pruning%2520technique%2520based%2520on%2520abstraction%2520that%2520significantly%2520reduces%250Athe%2520search%2520space%2520and%2520provides%2520a%2520certain%2520optimality%2520guarantee.%2520To%2520ensure%2520that%250Athe%2520optimized%2520%2524%255Cmu%2524Graph%2520is%2520equivalent%2520to%2520the%2520input%2520program%252C%2520Mirage%2520introduces%250Aa%2520probabilistic%2520equivalence%2520verification%2520procedure%2520with%2520strong%2520theoretical%250Aguarantees.%2520Our%2520evaluation%2520shows%2520that%2520Mirage%2520outperforms%2520existing%2520approaches%2520by%250Aup%2520to%25203.5%2524%255Ctimes%2524%2520even%2520for%2520DNNs%2520that%2520are%2520widely%2520used%2520and%2520heavily%2520optimized.%250AMirage%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/mirage-project/mirage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Level%20Superoptimizer%20for%20Tensor%20Programs&entry.906535625=Mengdi%20Wu%20and%20Xinhao%20Cheng%20and%20Oded%20Padon%20and%20Zhihao%20Jia&entry.1292438233=%20%20We%20introduce%20Mirage%2C%20the%20first%20multi-level%20superoptimizer%20for%20tensor%0Aprograms.%20A%20key%20idea%20in%20Mirage%20is%20%24%5Cmu%24Graphs%2C%20a%20uniform%20representation%20of%0Atensor%20programs%20at%20the%20kernel%2C%20thread%20block%2C%20and%20thread%20levels%20of%20the%20GPU%0Acompute%20hierarchy.%20%24%5Cmu%24Graphs%20enable%20Mirage%20to%20discover%20novel%20optimizations%0Athat%20combine%20algebraic%20transformations%2C%20schedule%20transformations%2C%20and%0Ageneration%20of%20new%20custom%20kernels.%20To%20navigate%20the%20large%20search%20space%2C%20Mirage%0Aintroduces%20a%20pruning%20technique%20based%20on%20abstraction%20that%20significantly%20reduces%0Athe%20search%20space%20and%20provides%20a%20certain%20optimality%20guarantee.%20To%20ensure%20that%0Athe%20optimized%20%24%5Cmu%24Graph%20is%20equivalent%20to%20the%20input%20program%2C%20Mirage%20introduces%0Aa%20probabilistic%20equivalence%20verification%20procedure%20with%20strong%20theoretical%0Aguarantees.%20Our%20evaluation%20shows%20that%20Mirage%20outperforms%20existing%20approaches%20by%0Aup%20to%203.5%24%5Ctimes%24%20even%20for%20DNNs%20that%20are%20widely%20used%20and%20heavily%20optimized.%0AMirage%20is%20publicly%20available%20at%20https%3A//github.com/mirage-project/mirage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05751v1&entry.124074799=Read"},
{"title": "Optimal Baseline Corrections for Off-Policy Contextual Bandits", "author": "Shashank Gupta and Olivier Jeunen and Harrie Oosterhuis and Maarten de Rijke", "abstract": "  The off-policy learning paradigm allows for recommender systems and general\nranking applications to be framed as decision-making problems, where we aim to\nlearn decision policies that optimize an unbiased offline estimate of an online\nreward metric. With unbiasedness comes potentially high variance, and prevalent\nmethods exist to reduce estimation variance. These methods typically make use\nof control variates, either additive (i.e., baseline corrections or doubly\nrobust methods) or multiplicative (i.e., self-normalisation). Our work unifies\nthese approaches by proposing a single framework built on their equivalence in\nlearning scenarios. The foundation of our framework is the derivation of an\nequivalent baseline correction for all of the existing control variates.\nConsequently, our framework enables us to characterize the variance-optimal\nunbiased estimator and provide a closed-form solution for it. This optimal\nestimator brings significantly improved performance in both evaluation and\nlearning, and minimizes data requirements. Empirical observations corroborate\nour theoretical findings.\n", "link": "http://arxiv.org/abs/2405.05736v1", "date": "2024-05-09", "relevancy": 1.8246, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Baseline%20Corrections%20for%20Off-Policy%20Contextual%20Bandits&body=Title%3A%20Optimal%20Baseline%20Corrections%20for%20Off-Policy%20Contextual%20Bandits%0AAuthor%3A%20Shashank%20Gupta%20and%20Olivier%20Jeunen%20and%20Harrie%20Oosterhuis%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20The%20off-policy%20learning%20paradigm%20allows%20for%20recommender%20systems%20and%20general%0Aranking%20applications%20to%20be%20framed%20as%20decision-making%20problems%2C%20where%20we%20aim%20to%0Alearn%20decision%20policies%20that%20optimize%20an%20unbiased%20offline%20estimate%20of%20an%20online%0Areward%20metric.%20With%20unbiasedness%20comes%20potentially%20high%20variance%2C%20and%20prevalent%0Amethods%20exist%20to%20reduce%20estimation%20variance.%20These%20methods%20typically%20make%20use%0Aof%20control%20variates%2C%20either%20additive%20%28i.e.%2C%20baseline%20corrections%20or%20doubly%0Arobust%20methods%29%20or%20multiplicative%20%28i.e.%2C%20self-normalisation%29.%20Our%20work%20unifies%0Athese%20approaches%20by%20proposing%20a%20single%20framework%20built%20on%20their%20equivalence%20in%0Alearning%20scenarios.%20The%20foundation%20of%20our%20framework%20is%20the%20derivation%20of%20an%0Aequivalent%20baseline%20correction%20for%20all%20of%20the%20existing%20control%20variates.%0AConsequently%2C%20our%20framework%20enables%20us%20to%20characterize%20the%20variance-optimal%0Aunbiased%20estimator%20and%20provide%20a%20closed-form%20solution%20for%20it.%20This%20optimal%0Aestimator%20brings%20significantly%20improved%20performance%20in%20both%20evaluation%20and%0Alearning%2C%20and%20minimizes%20data%20requirements.%20Empirical%20observations%20corroborate%0Aour%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Baseline%2520Corrections%2520for%2520Off-Policy%2520Contextual%2520Bandits%26entry.906535625%3DShashank%2520Gupta%2520and%2520Olivier%2520Jeunen%2520and%2520Harrie%2520Oosterhuis%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3D%2520%2520The%2520off-policy%2520learning%2520paradigm%2520allows%2520for%2520recommender%2520systems%2520and%2520general%250Aranking%2520applications%2520to%2520be%2520framed%2520as%2520decision-making%2520problems%252C%2520where%2520we%2520aim%2520to%250Alearn%2520decision%2520policies%2520that%2520optimize%2520an%2520unbiased%2520offline%2520estimate%2520of%2520an%2520online%250Areward%2520metric.%2520With%2520unbiasedness%2520comes%2520potentially%2520high%2520variance%252C%2520and%2520prevalent%250Amethods%2520exist%2520to%2520reduce%2520estimation%2520variance.%2520These%2520methods%2520typically%2520make%2520use%250Aof%2520control%2520variates%252C%2520either%2520additive%2520%2528i.e.%252C%2520baseline%2520corrections%2520or%2520doubly%250Arobust%2520methods%2529%2520or%2520multiplicative%2520%2528i.e.%252C%2520self-normalisation%2529.%2520Our%2520work%2520unifies%250Athese%2520approaches%2520by%2520proposing%2520a%2520single%2520framework%2520built%2520on%2520their%2520equivalence%2520in%250Alearning%2520scenarios.%2520The%2520foundation%2520of%2520our%2520framework%2520is%2520the%2520derivation%2520of%2520an%250Aequivalent%2520baseline%2520correction%2520for%2520all%2520of%2520the%2520existing%2520control%2520variates.%250AConsequently%252C%2520our%2520framework%2520enables%2520us%2520to%2520characterize%2520the%2520variance-optimal%250Aunbiased%2520estimator%2520and%2520provide%2520a%2520closed-form%2520solution%2520for%2520it.%2520This%2520optimal%250Aestimator%2520brings%2520significantly%2520improved%2520performance%2520in%2520both%2520evaluation%2520and%250Alearning%252C%2520and%2520minimizes%2520data%2520requirements.%2520Empirical%2520observations%2520corroborate%250Aour%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Baseline%20Corrections%20for%20Off-Policy%20Contextual%20Bandits&entry.906535625=Shashank%20Gupta%20and%20Olivier%20Jeunen%20and%20Harrie%20Oosterhuis%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20The%20off-policy%20learning%20paradigm%20allows%20for%20recommender%20systems%20and%20general%0Aranking%20applications%20to%20be%20framed%20as%20decision-making%20problems%2C%20where%20we%20aim%20to%0Alearn%20decision%20policies%20that%20optimize%20an%20unbiased%20offline%20estimate%20of%20an%20online%0Areward%20metric.%20With%20unbiasedness%20comes%20potentially%20high%20variance%2C%20and%20prevalent%0Amethods%20exist%20to%20reduce%20estimation%20variance.%20These%20methods%20typically%20make%20use%0Aof%20control%20variates%2C%20either%20additive%20%28i.e.%2C%20baseline%20corrections%20or%20doubly%0Arobust%20methods%29%20or%20multiplicative%20%28i.e.%2C%20self-normalisation%29.%20Our%20work%20unifies%0Athese%20approaches%20by%20proposing%20a%20single%20framework%20built%20on%20their%20equivalence%20in%0Alearning%20scenarios.%20The%20foundation%20of%20our%20framework%20is%20the%20derivation%20of%20an%0Aequivalent%20baseline%20correction%20for%20all%20of%20the%20existing%20control%20variates.%0AConsequently%2C%20our%20framework%20enables%20us%20to%20characterize%20the%20variance-optimal%0Aunbiased%20estimator%20and%20provide%20a%20closed-form%20solution%20for%20it.%20This%20optimal%0Aestimator%20brings%20significantly%20improved%20performance%20in%20both%20evaluation%20and%0Alearning%2C%20and%20minimizes%20data%20requirements.%20Empirical%20observations%20corroborate%0Aour%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05736v1&entry.124074799=Read"},
{"title": "MSDiff: Multi-Scale Diffusion Model for Ultra-Sparse View CT\n  Reconstruction", "author": "Pinhuang Tan and Mengxiao Geng and Jingya Lu and Liu Shi and Bin Huang and Qiegen Liu", "abstract": "  Computed Tomography (CT) technology reduces radiation haz-ards to the human\nbody through sparse sampling, but fewer sampling angles pose challenges for\nimage reconstruction. Score-based generative models are widely used in\nsparse-view CT re-construction, performance diminishes significantly with a\nsharp reduction in projection angles. Therefore, we propose an ultra-sparse\nview CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff),\ndesigned to concentrate on the global distribution of information and\nfacilitate the reconstruction of sparse views with local image characteristics.\nSpecifically, the proposed model ingeniously integrates information from both\ncomprehensive sampling and selectively sparse sampling tech-niques. Through\nprecise adjustments in diffusion model, it is capable of extracting diverse\nnoise distribution, furthering the understanding of the overall structure of\nimages, and aiding the fully sampled model in recovering image information more\neffec-tively. By leveraging the inherent correlations within the projec-tion\ndata, we have designed an equidistant mask, enabling the model to focus its\nattention more effectively. Experimental re-sults demonstrated that the\nmulti-scale model approach signifi-cantly improved the quality of image\nreconstruction under ultra-sparse angles, with good generalization across\nvarious datasets.\n", "link": "http://arxiv.org/abs/2405.05814v1", "date": "2024-05-09", "relevancy": 1.7974, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6523}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5964}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSDiff%3A%20Multi-Scale%20Diffusion%20Model%20for%20Ultra-Sparse%20View%20CT%0A%20%20Reconstruction&body=Title%3A%20MSDiff%3A%20Multi-Scale%20Diffusion%20Model%20for%20Ultra-Sparse%20View%20CT%0A%20%20Reconstruction%0AAuthor%3A%20Pinhuang%20Tan%20and%20Mengxiao%20Geng%20and%20Jingya%20Lu%20and%20Liu%20Shi%20and%20Bin%20Huang%20and%20Qiegen%20Liu%0AAbstract%3A%20%20%20Computed%20Tomography%20%28CT%29%20technology%20reduces%20radiation%20haz-ards%20to%20the%20human%0Abody%20through%20sparse%20sampling%2C%20but%20fewer%20sampling%20angles%20pose%20challenges%20for%0Aimage%20reconstruction.%20Score-based%20generative%20models%20are%20widely%20used%20in%0Asparse-view%20CT%20re-construction%2C%20performance%20diminishes%20significantly%20with%20a%0Asharp%20reduction%20in%20projection%20angles.%20Therefore%2C%20we%20propose%20an%20ultra-sparse%0Aview%20CT%20reconstruction%20method%20utilizing%20multi-scale%20dif-fusion%20models%20%28MSDiff%29%2C%0Adesigned%20to%20concentrate%20on%20the%20global%20distribution%20of%20information%20and%0Afacilitate%20the%20reconstruction%20of%20sparse%20views%20with%20local%20image%20characteristics.%0ASpecifically%2C%20the%20proposed%20model%20ingeniously%20integrates%20information%20from%20both%0Acomprehensive%20sampling%20and%20selectively%20sparse%20sampling%20tech-niques.%20Through%0Aprecise%20adjustments%20in%20diffusion%20model%2C%20it%20is%20capable%20of%20extracting%20diverse%0Anoise%20distribution%2C%20furthering%20the%20understanding%20of%20the%20overall%20structure%20of%0Aimages%2C%20and%20aiding%20the%20fully%20sampled%20model%20in%20recovering%20image%20information%20more%0Aeffec-tively.%20By%20leveraging%20the%20inherent%20correlations%20within%20the%20projec-tion%0Adata%2C%20we%20have%20designed%20an%20equidistant%20mask%2C%20enabling%20the%20model%20to%20focus%20its%0Aattention%20more%20effectively.%20Experimental%20re-sults%20demonstrated%20that%20the%0Amulti-scale%20model%20approach%20signifi-cantly%20improved%20the%20quality%20of%20image%0Areconstruction%20under%20ultra-sparse%20angles%2C%20with%20good%20generalization%20across%0Avarious%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSDiff%253A%2520Multi-Scale%2520Diffusion%2520Model%2520for%2520Ultra-Sparse%2520View%2520CT%250A%2520%2520Reconstruction%26entry.906535625%3DPinhuang%2520Tan%2520and%2520Mengxiao%2520Geng%2520and%2520Jingya%2520Lu%2520and%2520Liu%2520Shi%2520and%2520Bin%2520Huang%2520and%2520Qiegen%2520Liu%26entry.1292438233%3D%2520%2520Computed%2520Tomography%2520%2528CT%2529%2520technology%2520reduces%2520radiation%2520haz-ards%2520to%2520the%2520human%250Abody%2520through%2520sparse%2520sampling%252C%2520but%2520fewer%2520sampling%2520angles%2520pose%2520challenges%2520for%250Aimage%2520reconstruction.%2520Score-based%2520generative%2520models%2520are%2520widely%2520used%2520in%250Asparse-view%2520CT%2520re-construction%252C%2520performance%2520diminishes%2520significantly%2520with%2520a%250Asharp%2520reduction%2520in%2520projection%2520angles.%2520Therefore%252C%2520we%2520propose%2520an%2520ultra-sparse%250Aview%2520CT%2520reconstruction%2520method%2520utilizing%2520multi-scale%2520dif-fusion%2520models%2520%2528MSDiff%2529%252C%250Adesigned%2520to%2520concentrate%2520on%2520the%2520global%2520distribution%2520of%2520information%2520and%250Afacilitate%2520the%2520reconstruction%2520of%2520sparse%2520views%2520with%2520local%2520image%2520characteristics.%250ASpecifically%252C%2520the%2520proposed%2520model%2520ingeniously%2520integrates%2520information%2520from%2520both%250Acomprehensive%2520sampling%2520and%2520selectively%2520sparse%2520sampling%2520tech-niques.%2520Through%250Aprecise%2520adjustments%2520in%2520diffusion%2520model%252C%2520it%2520is%2520capable%2520of%2520extracting%2520diverse%250Anoise%2520distribution%252C%2520furthering%2520the%2520understanding%2520of%2520the%2520overall%2520structure%2520of%250Aimages%252C%2520and%2520aiding%2520the%2520fully%2520sampled%2520model%2520in%2520recovering%2520image%2520information%2520more%250Aeffec-tively.%2520By%2520leveraging%2520the%2520inherent%2520correlations%2520within%2520the%2520projec-tion%250Adata%252C%2520we%2520have%2520designed%2520an%2520equidistant%2520mask%252C%2520enabling%2520the%2520model%2520to%2520focus%2520its%250Aattention%2520more%2520effectively.%2520Experimental%2520re-sults%2520demonstrated%2520that%2520the%250Amulti-scale%2520model%2520approach%2520signifi-cantly%2520improved%2520the%2520quality%2520of%2520image%250Areconstruction%2520under%2520ultra-sparse%2520angles%252C%2520with%2520good%2520generalization%2520across%250Avarious%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSDiff%3A%20Multi-Scale%20Diffusion%20Model%20for%20Ultra-Sparse%20View%20CT%0A%20%20Reconstruction&entry.906535625=Pinhuang%20Tan%20and%20Mengxiao%20Geng%20and%20Jingya%20Lu%20and%20Liu%20Shi%20and%20Bin%20Huang%20and%20Qiegen%20Liu&entry.1292438233=%20%20Computed%20Tomography%20%28CT%29%20technology%20reduces%20radiation%20haz-ards%20to%20the%20human%0Abody%20through%20sparse%20sampling%2C%20but%20fewer%20sampling%20angles%20pose%20challenges%20for%0Aimage%20reconstruction.%20Score-based%20generative%20models%20are%20widely%20used%20in%0Asparse-view%20CT%20re-construction%2C%20performance%20diminishes%20significantly%20with%20a%0Asharp%20reduction%20in%20projection%20angles.%20Therefore%2C%20we%20propose%20an%20ultra-sparse%0Aview%20CT%20reconstruction%20method%20utilizing%20multi-scale%20dif-fusion%20models%20%28MSDiff%29%2C%0Adesigned%20to%20concentrate%20on%20the%20global%20distribution%20of%20information%20and%0Afacilitate%20the%20reconstruction%20of%20sparse%20views%20with%20local%20image%20characteristics.%0ASpecifically%2C%20the%20proposed%20model%20ingeniously%20integrates%20information%20from%20both%0Acomprehensive%20sampling%20and%20selectively%20sparse%20sampling%20tech-niques.%20Through%0Aprecise%20adjustments%20in%20diffusion%20model%2C%20it%20is%20capable%20of%20extracting%20diverse%0Anoise%20distribution%2C%20furthering%20the%20understanding%20of%20the%20overall%20structure%20of%0Aimages%2C%20and%20aiding%20the%20fully%20sampled%20model%20in%20recovering%20image%20information%20more%0Aeffec-tively.%20By%20leveraging%20the%20inherent%20correlations%20within%20the%20projec-tion%0Adata%2C%20we%20have%20designed%20an%20equidistant%20mask%2C%20enabling%20the%20model%20to%20focus%20its%0Aattention%20more%20effectively.%20Experimental%20re-sults%20demonstrated%20that%20the%0Amulti-scale%20model%20approach%20signifi-cantly%20improved%20the%20quality%20of%20image%0Areconstruction%20under%20ultra-sparse%20angles%2C%20with%20good%20generalization%20across%0Avarious%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05814v1&entry.124074799=Read"},
{"title": "Could It Be Generated? Towards Practical Analysis of Memorization in\n  Text-To-Image Diffusion Models", "author": "Zhe Ma and Xuhong Zhang and Qingming Li and Tianyu Du and Wenzhi Chen and Zonghui Wang and Shouling Ji", "abstract": "  The past few years have witnessed substantial advancement in text-guided\nimage generation powered by diffusion models. However, it was shown that\ntext-to-image diffusion models are vulnerable to training image memorization,\nraising concerns on copyright infringement and privacy invasion. In this work,\nwe perform practical analysis of memorization in text-to-image diffusion\nmodels. Targeting a set of images to protect, we conduct quantitive analysis on\nthem without need to collect any prompts. Specifically, we first formally\ndefine the memorization of image and identify three necessary conditions of\nmemorization, respectively similarity, existence and probability. We then\nreveal the correlation between the model's prediction error and image\nreplication. Based on the correlation, we propose to utilize inversion\ntechniques to verify the safety of target images against memorization and\nmeasure the extent to which they are memorized. Model developers can utilize\nour analysis method to discover memorized images or reliably claim safety\nagainst memorization. Extensive experiments on the Stable Diffusion, a popular\nopen-source text-to-image diffusion model, demonstrate the effectiveness of our\nanalysis method.\n", "link": "http://arxiv.org/abs/2405.05846v1", "date": "2024-05-09", "relevancy": 1.7962, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6057}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5979}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Could%20It%20Be%20Generated%3F%20Towards%20Practical%20Analysis%20of%20Memorization%20in%0A%20%20Text-To-Image%20Diffusion%20Models&body=Title%3A%20Could%20It%20Be%20Generated%3F%20Towards%20Practical%20Analysis%20of%20Memorization%20in%0A%20%20Text-To-Image%20Diffusion%20Models%0AAuthor%3A%20Zhe%20Ma%20and%20Xuhong%20Zhang%20and%20Qingming%20Li%20and%20Tianyu%20Du%20and%20Wenzhi%20Chen%20and%20Zonghui%20Wang%20and%20Shouling%20Ji%0AAbstract%3A%20%20%20The%20past%20few%20years%20have%20witnessed%20substantial%20advancement%20in%20text-guided%0Aimage%20generation%20powered%20by%20diffusion%20models.%20However%2C%20it%20was%20shown%20that%0Atext-to-image%20diffusion%20models%20are%20vulnerable%20to%20training%20image%20memorization%2C%0Araising%20concerns%20on%20copyright%20infringement%20and%20privacy%20invasion.%20In%20this%20work%2C%0Awe%20perform%20practical%20analysis%20of%20memorization%20in%20text-to-image%20diffusion%0Amodels.%20Targeting%20a%20set%20of%20images%20to%20protect%2C%20we%20conduct%20quantitive%20analysis%20on%0Athem%20without%20need%20to%20collect%20any%20prompts.%20Specifically%2C%20we%20first%20formally%0Adefine%20the%20memorization%20of%20image%20and%20identify%20three%20necessary%20conditions%20of%0Amemorization%2C%20respectively%20similarity%2C%20existence%20and%20probability.%20We%20then%0Areveal%20the%20correlation%20between%20the%20model%27s%20prediction%20error%20and%20image%0Areplication.%20Based%20on%20the%20correlation%2C%20we%20propose%20to%20utilize%20inversion%0Atechniques%20to%20verify%20the%20safety%20of%20target%20images%20against%20memorization%20and%0Ameasure%20the%20extent%20to%20which%20they%20are%20memorized.%20Model%20developers%20can%20utilize%0Aour%20analysis%20method%20to%20discover%20memorized%20images%20or%20reliably%20claim%20safety%0Aagainst%20memorization.%20Extensive%20experiments%20on%20the%20Stable%20Diffusion%2C%20a%20popular%0Aopen-source%20text-to-image%20diffusion%20model%2C%20demonstrate%20the%20effectiveness%20of%20our%0Aanalysis%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCould%2520It%2520Be%2520Generated%253F%2520Towards%2520Practical%2520Analysis%2520of%2520Memorization%2520in%250A%2520%2520Text-To-Image%2520Diffusion%2520Models%26entry.906535625%3DZhe%2520Ma%2520and%2520Xuhong%2520Zhang%2520and%2520Qingming%2520Li%2520and%2520Tianyu%2520Du%2520and%2520Wenzhi%2520Chen%2520and%2520Zonghui%2520Wang%2520and%2520Shouling%2520Ji%26entry.1292438233%3D%2520%2520The%2520past%2520few%2520years%2520have%2520witnessed%2520substantial%2520advancement%2520in%2520text-guided%250Aimage%2520generation%2520powered%2520by%2520diffusion%2520models.%2520However%252C%2520it%2520was%2520shown%2520that%250Atext-to-image%2520diffusion%2520models%2520are%2520vulnerable%2520to%2520training%2520image%2520memorization%252C%250Araising%2520concerns%2520on%2520copyright%2520infringement%2520and%2520privacy%2520invasion.%2520In%2520this%2520work%252C%250Awe%2520perform%2520practical%2520analysis%2520of%2520memorization%2520in%2520text-to-image%2520diffusion%250Amodels.%2520Targeting%2520a%2520set%2520of%2520images%2520to%2520protect%252C%2520we%2520conduct%2520quantitive%2520analysis%2520on%250Athem%2520without%2520need%2520to%2520collect%2520any%2520prompts.%2520Specifically%252C%2520we%2520first%2520formally%250Adefine%2520the%2520memorization%2520of%2520image%2520and%2520identify%2520three%2520necessary%2520conditions%2520of%250Amemorization%252C%2520respectively%2520similarity%252C%2520existence%2520and%2520probability.%2520We%2520then%250Areveal%2520the%2520correlation%2520between%2520the%2520model%2527s%2520prediction%2520error%2520and%2520image%250Areplication.%2520Based%2520on%2520the%2520correlation%252C%2520we%2520propose%2520to%2520utilize%2520inversion%250Atechniques%2520to%2520verify%2520the%2520safety%2520of%2520target%2520images%2520against%2520memorization%2520and%250Ameasure%2520the%2520extent%2520to%2520which%2520they%2520are%2520memorized.%2520Model%2520developers%2520can%2520utilize%250Aour%2520analysis%2520method%2520to%2520discover%2520memorized%2520images%2520or%2520reliably%2520claim%2520safety%250Aagainst%2520memorization.%2520Extensive%2520experiments%2520on%2520the%2520Stable%2520Diffusion%252C%2520a%2520popular%250Aopen-source%2520text-to-image%2520diffusion%2520model%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aanalysis%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Could%20It%20Be%20Generated%3F%20Towards%20Practical%20Analysis%20of%20Memorization%20in%0A%20%20Text-To-Image%20Diffusion%20Models&entry.906535625=Zhe%20Ma%20and%20Xuhong%20Zhang%20and%20Qingming%20Li%20and%20Tianyu%20Du%20and%20Wenzhi%20Chen%20and%20Zonghui%20Wang%20and%20Shouling%20Ji&entry.1292438233=%20%20The%20past%20few%20years%20have%20witnessed%20substantial%20advancement%20in%20text-guided%0Aimage%20generation%20powered%20by%20diffusion%20models.%20However%2C%20it%20was%20shown%20that%0Atext-to-image%20diffusion%20models%20are%20vulnerable%20to%20training%20image%20memorization%2C%0Araising%20concerns%20on%20copyright%20infringement%20and%20privacy%20invasion.%20In%20this%20work%2C%0Awe%20perform%20practical%20analysis%20of%20memorization%20in%20text-to-image%20diffusion%0Amodels.%20Targeting%20a%20set%20of%20images%20to%20protect%2C%20we%20conduct%20quantitive%20analysis%20on%0Athem%20without%20need%20to%20collect%20any%20prompts.%20Specifically%2C%20we%20first%20formally%0Adefine%20the%20memorization%20of%20image%20and%20identify%20three%20necessary%20conditions%20of%0Amemorization%2C%20respectively%20similarity%2C%20existence%20and%20probability.%20We%20then%0Areveal%20the%20correlation%20between%20the%20model%27s%20prediction%20error%20and%20image%0Areplication.%20Based%20on%20the%20correlation%2C%20we%20propose%20to%20utilize%20inversion%0Atechniques%20to%20verify%20the%20safety%20of%20target%20images%20against%20memorization%20and%0Ameasure%20the%20extent%20to%20which%20they%20are%20memorized.%20Model%20developers%20can%20utilize%0Aour%20analysis%20method%20to%20discover%20memorized%20images%20or%20reliably%20claim%20safety%0Aagainst%20memorization.%20Extensive%20experiments%20on%20the%20Stable%20Diffusion%2C%20a%20popular%0Aopen-source%20text-to-image%20diffusion%20model%2C%20demonstrate%20the%20effectiveness%20of%20our%0Aanalysis%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05846v1&entry.124074799=Read"},
{"title": "Pre-trained Text-to-Image Diffusion Models Are Versatile Representation\n  Learners for Control", "author": "Gunshi Gupta and Karmesh Yadav and Yarin Gal and Dhruv Batra and Zsolt Kira and Cong Lu and Tim G. J. Rudner", "abstract": "  Embodied AI agents require a fine-grained understanding of the physical world\nmediated through visual and language inputs. Such capabilities are difficult to\nlearn solely from task-specific data. This has led to the emergence of\npre-trained vision-language models as a tool for transferring representations\nlearned from internet-scale data to downstream tasks and new domains. However,\ncommonly used contrastively trained representations such as in CLIP have been\nshown to fail at enabling embodied agents to gain a sufficiently fine-grained\nscene understanding -- a capability vital for control. To address this\nshortcoming, we consider representations from pre-trained text-to-image\ndiffusion models, which are explicitly optimized to generate images from text\nprompts and as such, contain text-conditioned representations that reflect\nhighly fine-grained visuo-spatial information. Using pre-trained text-to-image\ndiffusion models, we construct Stable Control Representations which allow\nlearning downstream control policies that generalize to complex, open-ended\nenvironments. We show that policies learned using Stable Control\nRepresentations are competitive with state-of-the-art representation learning\napproaches across a broad range of simulated control settings, encompassing\nchallenging manipulation and navigation tasks. Most notably, we show that\nStable Control Representations enable learning policies that exhibit\nstate-of-the-art performance on OVMM, a difficult open-vocabulary navigation\nbenchmark.\n", "link": "http://arxiv.org/abs/2405.05852v1", "date": "2024-05-09", "relevancy": 1.7917, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6004}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.594}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-trained%20Text-to-Image%20Diffusion%20Models%20Are%20Versatile%20Representation%0A%20%20Learners%20for%20Control&body=Title%3A%20Pre-trained%20Text-to-Image%20Diffusion%20Models%20Are%20Versatile%20Representation%0A%20%20Learners%20for%20Control%0AAuthor%3A%20Gunshi%20Gupta%20and%20Karmesh%20Yadav%20and%20Yarin%20Gal%20and%20Dhruv%20Batra%20and%20Zsolt%20Kira%20and%20Cong%20Lu%20and%20Tim%20G.%20J.%20Rudner%0AAbstract%3A%20%20%20Embodied%20AI%20agents%20require%20a%20fine-grained%20understanding%20of%20the%20physical%20world%0Amediated%20through%20visual%20and%20language%20inputs.%20Such%20capabilities%20are%20difficult%20to%0Alearn%20solely%20from%20task-specific%20data.%20This%20has%20led%20to%20the%20emergence%20of%0Apre-trained%20vision-language%20models%20as%20a%20tool%20for%20transferring%20representations%0Alearned%20from%20internet-scale%20data%20to%20downstream%20tasks%20and%20new%20domains.%20However%2C%0Acommonly%20used%20contrastively%20trained%20representations%20such%20as%20in%20CLIP%20have%20been%0Ashown%20to%20fail%20at%20enabling%20embodied%20agents%20to%20gain%20a%20sufficiently%20fine-grained%0Ascene%20understanding%20--%20a%20capability%20vital%20for%20control.%20To%20address%20this%0Ashortcoming%2C%20we%20consider%20representations%20from%20pre-trained%20text-to-image%0Adiffusion%20models%2C%20which%20are%20explicitly%20optimized%20to%20generate%20images%20from%20text%0Aprompts%20and%20as%20such%2C%20contain%20text-conditioned%20representations%20that%20reflect%0Ahighly%20fine-grained%20visuo-spatial%20information.%20Using%20pre-trained%20text-to-image%0Adiffusion%20models%2C%20we%20construct%20Stable%20Control%20Representations%20which%20allow%0Alearning%20downstream%20control%20policies%20that%20generalize%20to%20complex%2C%20open-ended%0Aenvironments.%20We%20show%20that%20policies%20learned%20using%20Stable%20Control%0ARepresentations%20are%20competitive%20with%20state-of-the-art%20representation%20learning%0Aapproaches%20across%20a%20broad%20range%20of%20simulated%20control%20settings%2C%20encompassing%0Achallenging%20manipulation%20and%20navigation%20tasks.%20Most%20notably%2C%20we%20show%20that%0AStable%20Control%20Representations%20enable%20learning%20policies%20that%20exhibit%0Astate-of-the-art%20performance%20on%20OVMM%2C%20a%20difficult%20open-vocabulary%20navigation%0Abenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-trained%2520Text-to-Image%2520Diffusion%2520Models%2520Are%2520Versatile%2520Representation%250A%2520%2520Learners%2520for%2520Control%26entry.906535625%3DGunshi%2520Gupta%2520and%2520Karmesh%2520Yadav%2520and%2520Yarin%2520Gal%2520and%2520Dhruv%2520Batra%2520and%2520Zsolt%2520Kira%2520and%2520Cong%2520Lu%2520and%2520Tim%2520G.%2520J.%2520Rudner%26entry.1292438233%3D%2520%2520Embodied%2520AI%2520agents%2520require%2520a%2520fine-grained%2520understanding%2520of%2520the%2520physical%2520world%250Amediated%2520through%2520visual%2520and%2520language%2520inputs.%2520Such%2520capabilities%2520are%2520difficult%2520to%250Alearn%2520solely%2520from%2520task-specific%2520data.%2520This%2520has%2520led%2520to%2520the%2520emergence%2520of%250Apre-trained%2520vision-language%2520models%2520as%2520a%2520tool%2520for%2520transferring%2520representations%250Alearned%2520from%2520internet-scale%2520data%2520to%2520downstream%2520tasks%2520and%2520new%2520domains.%2520However%252C%250Acommonly%2520used%2520contrastively%2520trained%2520representations%2520such%2520as%2520in%2520CLIP%2520have%2520been%250Ashown%2520to%2520fail%2520at%2520enabling%2520embodied%2520agents%2520to%2520gain%2520a%2520sufficiently%2520fine-grained%250Ascene%2520understanding%2520--%2520a%2520capability%2520vital%2520for%2520control.%2520To%2520address%2520this%250Ashortcoming%252C%2520we%2520consider%2520representations%2520from%2520pre-trained%2520text-to-image%250Adiffusion%2520models%252C%2520which%2520are%2520explicitly%2520optimized%2520to%2520generate%2520images%2520from%2520text%250Aprompts%2520and%2520as%2520such%252C%2520contain%2520text-conditioned%2520representations%2520that%2520reflect%250Ahighly%2520fine-grained%2520visuo-spatial%2520information.%2520Using%2520pre-trained%2520text-to-image%250Adiffusion%2520models%252C%2520we%2520construct%2520Stable%2520Control%2520Representations%2520which%2520allow%250Alearning%2520downstream%2520control%2520policies%2520that%2520generalize%2520to%2520complex%252C%2520open-ended%250Aenvironments.%2520We%2520show%2520that%2520policies%2520learned%2520using%2520Stable%2520Control%250ARepresentations%2520are%2520competitive%2520with%2520state-of-the-art%2520representation%2520learning%250Aapproaches%2520across%2520a%2520broad%2520range%2520of%2520simulated%2520control%2520settings%252C%2520encompassing%250Achallenging%2520manipulation%2520and%2520navigation%2520tasks.%2520Most%2520notably%252C%2520we%2520show%2520that%250AStable%2520Control%2520Representations%2520enable%2520learning%2520policies%2520that%2520exhibit%250Astate-of-the-art%2520performance%2520on%2520OVMM%252C%2520a%2520difficult%2520open-vocabulary%2520navigation%250Abenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-trained%20Text-to-Image%20Diffusion%20Models%20Are%20Versatile%20Representation%0A%20%20Learners%20for%20Control&entry.906535625=Gunshi%20Gupta%20and%20Karmesh%20Yadav%20and%20Yarin%20Gal%20and%20Dhruv%20Batra%20and%20Zsolt%20Kira%20and%20Cong%20Lu%20and%20Tim%20G.%20J.%20Rudner&entry.1292438233=%20%20Embodied%20AI%20agents%20require%20a%20fine-grained%20understanding%20of%20the%20physical%20world%0Amediated%20through%20visual%20and%20language%20inputs.%20Such%20capabilities%20are%20difficult%20to%0Alearn%20solely%20from%20task-specific%20data.%20This%20has%20led%20to%20the%20emergence%20of%0Apre-trained%20vision-language%20models%20as%20a%20tool%20for%20transferring%20representations%0Alearned%20from%20internet-scale%20data%20to%20downstream%20tasks%20and%20new%20domains.%20However%2C%0Acommonly%20used%20contrastively%20trained%20representations%20such%20as%20in%20CLIP%20have%20been%0Ashown%20to%20fail%20at%20enabling%20embodied%20agents%20to%20gain%20a%20sufficiently%20fine-grained%0Ascene%20understanding%20--%20a%20capability%20vital%20for%20control.%20To%20address%20this%0Ashortcoming%2C%20we%20consider%20representations%20from%20pre-trained%20text-to-image%0Adiffusion%20models%2C%20which%20are%20explicitly%20optimized%20to%20generate%20images%20from%20text%0Aprompts%20and%20as%20such%2C%20contain%20text-conditioned%20representations%20that%20reflect%0Ahighly%20fine-grained%20visuo-spatial%20information.%20Using%20pre-trained%20text-to-image%0Adiffusion%20models%2C%20we%20construct%20Stable%20Control%20Representations%20which%20allow%0Alearning%20downstream%20control%20policies%20that%20generalize%20to%20complex%2C%20open-ended%0Aenvironments.%20We%20show%20that%20policies%20learned%20using%20Stable%20Control%0ARepresentations%20are%20competitive%20with%20state-of-the-art%20representation%20learning%0Aapproaches%20across%20a%20broad%20range%20of%20simulated%20control%20settings%2C%20encompassing%0Achallenging%20manipulation%20and%20navigation%20tasks.%20Most%20notably%2C%20we%20show%20that%0AStable%20Control%20Representations%20enable%20learning%20policies%20that%20exhibit%0Astate-of-the-art%20performance%20on%20OVMM%2C%20a%20difficult%20open-vocabulary%20navigation%0Abenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05852v1&entry.124074799=Read"},
{"title": "RoboHop: Segment-based Topological Map Representation for Open-World\n  Visual Navigation", "author": "Sourav Garg and Krishan Rana and Mehdi Hosseinzadeh and Lachlan Mares and Niko S\u00fcnderhauf and Feras Dayoub and Ian Reid", "abstract": "  Mapping is crucial for spatial reasoning, planning and robot navigation.\nExisting approaches range from metric, which require precise geometry-based\noptimization, to purely topological, where image-as-node based graphs lack\nexplicit object-level reasoning and interconnectivity. In this paper, we\npropose a novel topological representation of an environment based on \"image\nsegments\", which are semantically meaningful and open-vocabulary queryable,\nconferring several advantages over previous works based on pixel-level\nfeatures. Unlike 3D scene graphs, we create a purely topological graph with\nsegments as nodes, where edges are formed by a) associating segment-level\ndescriptors between pairs of consecutive images and b) connecting neighboring\nsegments within an image using their pixel centroids. This unveils a\n\"continuous sense of a place\", defined by inter-image persistence of segments\nalong with their intra-image neighbours. It further enables us to represent and\nupdate segment-level descriptors through neighborhood aggregation using graph\nconvolution layers, which improves robot localization based on segment-level\nretrieval. Using real-world data, we show how our proposed map representation\ncan be used to i) generate navigation plans in the form of \"hops over segments\"\nand ii) search for target objects using natural language queries describing\nspatial relations of objects. Furthermore, we quantitatively analyze data\nassociation at the segment level, which underpins inter-image connectivity\nduring mapping and segment-level localization when revisiting the same place.\nFinally, we show preliminary trials on segment-level `hopping' based zero-shot\nreal-world navigation. Project page with supplementary details:\noravus.github.io/RoboHop/\n", "link": "http://arxiv.org/abs/2405.05792v1", "date": "2024-05-09", "relevancy": 1.785, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6164}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5891}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboHop%3A%20Segment-based%20Topological%20Map%20Representation%20for%20Open-World%0A%20%20Visual%20Navigation&body=Title%3A%20RoboHop%3A%20Segment-based%20Topological%20Map%20Representation%20for%20Open-World%0A%20%20Visual%20Navigation%0AAuthor%3A%20Sourav%20Garg%20and%20Krishan%20Rana%20and%20Mehdi%20Hosseinzadeh%20and%20Lachlan%20Mares%20and%20Niko%20S%C3%BCnderhauf%20and%20Feras%20Dayoub%20and%20Ian%20Reid%0AAbstract%3A%20%20%20Mapping%20is%20crucial%20for%20spatial%20reasoning%2C%20planning%20and%20robot%20navigation.%0AExisting%20approaches%20range%20from%20metric%2C%20which%20require%20precise%20geometry-based%0Aoptimization%2C%20to%20purely%20topological%2C%20where%20image-as-node%20based%20graphs%20lack%0Aexplicit%20object-level%20reasoning%20and%20interconnectivity.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20topological%20representation%20of%20an%20environment%20based%20on%20%22image%0Asegments%22%2C%20which%20are%20semantically%20meaningful%20and%20open-vocabulary%20queryable%2C%0Aconferring%20several%20advantages%20over%20previous%20works%20based%20on%20pixel-level%0Afeatures.%20Unlike%203D%20scene%20graphs%2C%20we%20create%20a%20purely%20topological%20graph%20with%0Asegments%20as%20nodes%2C%20where%20edges%20are%20formed%20by%20a%29%20associating%20segment-level%0Adescriptors%20between%20pairs%20of%20consecutive%20images%20and%20b%29%20connecting%20neighboring%0Asegments%20within%20an%20image%20using%20their%20pixel%20centroids.%20This%20unveils%20a%0A%22continuous%20sense%20of%20a%20place%22%2C%20defined%20by%20inter-image%20persistence%20of%20segments%0Aalong%20with%20their%20intra-image%20neighbours.%20It%20further%20enables%20us%20to%20represent%20and%0Aupdate%20segment-level%20descriptors%20through%20neighborhood%20aggregation%20using%20graph%0Aconvolution%20layers%2C%20which%20improves%20robot%20localization%20based%20on%20segment-level%0Aretrieval.%20Using%20real-world%20data%2C%20we%20show%20how%20our%20proposed%20map%20representation%0Acan%20be%20used%20to%20i%29%20generate%20navigation%20plans%20in%20the%20form%20of%20%22hops%20over%20segments%22%0Aand%20ii%29%20search%20for%20target%20objects%20using%20natural%20language%20queries%20describing%0Aspatial%20relations%20of%20objects.%20Furthermore%2C%20we%20quantitatively%20analyze%20data%0Aassociation%20at%20the%20segment%20level%2C%20which%20underpins%20inter-image%20connectivity%0Aduring%20mapping%20and%20segment-level%20localization%20when%20revisiting%20the%20same%20place.%0AFinally%2C%20we%20show%20preliminary%20trials%20on%20segment-level%20%60hopping%27%20based%20zero-shot%0Areal-world%20navigation.%20Project%20page%20with%20supplementary%20details%3A%0Aoravus.github.io/RoboHop/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboHop%253A%2520Segment-based%2520Topological%2520Map%2520Representation%2520for%2520Open-World%250A%2520%2520Visual%2520Navigation%26entry.906535625%3DSourav%2520Garg%2520and%2520Krishan%2520Rana%2520and%2520Mehdi%2520Hosseinzadeh%2520and%2520Lachlan%2520Mares%2520and%2520Niko%2520S%25C3%25BCnderhauf%2520and%2520Feras%2520Dayoub%2520and%2520Ian%2520Reid%26entry.1292438233%3D%2520%2520Mapping%2520is%2520crucial%2520for%2520spatial%2520reasoning%252C%2520planning%2520and%2520robot%2520navigation.%250AExisting%2520approaches%2520range%2520from%2520metric%252C%2520which%2520require%2520precise%2520geometry-based%250Aoptimization%252C%2520to%2520purely%2520topological%252C%2520where%2520image-as-node%2520based%2520graphs%2520lack%250Aexplicit%2520object-level%2520reasoning%2520and%2520interconnectivity.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520topological%2520representation%2520of%2520an%2520environment%2520based%2520on%2520%2522image%250Asegments%2522%252C%2520which%2520are%2520semantically%2520meaningful%2520and%2520open-vocabulary%2520queryable%252C%250Aconferring%2520several%2520advantages%2520over%2520previous%2520works%2520based%2520on%2520pixel-level%250Afeatures.%2520Unlike%25203D%2520scene%2520graphs%252C%2520we%2520create%2520a%2520purely%2520topological%2520graph%2520with%250Asegments%2520as%2520nodes%252C%2520where%2520edges%2520are%2520formed%2520by%2520a%2529%2520associating%2520segment-level%250Adescriptors%2520between%2520pairs%2520of%2520consecutive%2520images%2520and%2520b%2529%2520connecting%2520neighboring%250Asegments%2520within%2520an%2520image%2520using%2520their%2520pixel%2520centroids.%2520This%2520unveils%2520a%250A%2522continuous%2520sense%2520of%2520a%2520place%2522%252C%2520defined%2520by%2520inter-image%2520persistence%2520of%2520segments%250Aalong%2520with%2520their%2520intra-image%2520neighbours.%2520It%2520further%2520enables%2520us%2520to%2520represent%2520and%250Aupdate%2520segment-level%2520descriptors%2520through%2520neighborhood%2520aggregation%2520using%2520graph%250Aconvolution%2520layers%252C%2520which%2520improves%2520robot%2520localization%2520based%2520on%2520segment-level%250Aretrieval.%2520Using%2520real-world%2520data%252C%2520we%2520show%2520how%2520our%2520proposed%2520map%2520representation%250Acan%2520be%2520used%2520to%2520i%2529%2520generate%2520navigation%2520plans%2520in%2520the%2520form%2520of%2520%2522hops%2520over%2520segments%2522%250Aand%2520ii%2529%2520search%2520for%2520target%2520objects%2520using%2520natural%2520language%2520queries%2520describing%250Aspatial%2520relations%2520of%2520objects.%2520Furthermore%252C%2520we%2520quantitatively%2520analyze%2520data%250Aassociation%2520at%2520the%2520segment%2520level%252C%2520which%2520underpins%2520inter-image%2520connectivity%250Aduring%2520mapping%2520and%2520segment-level%2520localization%2520when%2520revisiting%2520the%2520same%2520place.%250AFinally%252C%2520we%2520show%2520preliminary%2520trials%2520on%2520segment-level%2520%2560hopping%2527%2520based%2520zero-shot%250Areal-world%2520navigation.%2520Project%2520page%2520with%2520supplementary%2520details%253A%250Aoravus.github.io/RoboHop/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboHop%3A%20Segment-based%20Topological%20Map%20Representation%20for%20Open-World%0A%20%20Visual%20Navigation&entry.906535625=Sourav%20Garg%20and%20Krishan%20Rana%20and%20Mehdi%20Hosseinzadeh%20and%20Lachlan%20Mares%20and%20Niko%20S%C3%BCnderhauf%20and%20Feras%20Dayoub%20and%20Ian%20Reid&entry.1292438233=%20%20Mapping%20is%20crucial%20for%20spatial%20reasoning%2C%20planning%20and%20robot%20navigation.%0AExisting%20approaches%20range%20from%20metric%2C%20which%20require%20precise%20geometry-based%0Aoptimization%2C%20to%20purely%20topological%2C%20where%20image-as-node%20based%20graphs%20lack%0Aexplicit%20object-level%20reasoning%20and%20interconnectivity.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20topological%20representation%20of%20an%20environment%20based%20on%20%22image%0Asegments%22%2C%20which%20are%20semantically%20meaningful%20and%20open-vocabulary%20queryable%2C%0Aconferring%20several%20advantages%20over%20previous%20works%20based%20on%20pixel-level%0Afeatures.%20Unlike%203D%20scene%20graphs%2C%20we%20create%20a%20purely%20topological%20graph%20with%0Asegments%20as%20nodes%2C%20where%20edges%20are%20formed%20by%20a%29%20associating%20segment-level%0Adescriptors%20between%20pairs%20of%20consecutive%20images%20and%20b%29%20connecting%20neighboring%0Asegments%20within%20an%20image%20using%20their%20pixel%20centroids.%20This%20unveils%20a%0A%22continuous%20sense%20of%20a%20place%22%2C%20defined%20by%20inter-image%20persistence%20of%20segments%0Aalong%20with%20their%20intra-image%20neighbours.%20It%20further%20enables%20us%20to%20represent%20and%0Aupdate%20segment-level%20descriptors%20through%20neighborhood%20aggregation%20using%20graph%0Aconvolution%20layers%2C%20which%20improves%20robot%20localization%20based%20on%20segment-level%0Aretrieval.%20Using%20real-world%20data%2C%20we%20show%20how%20our%20proposed%20map%20representation%0Acan%20be%20used%20to%20i%29%20generate%20navigation%20plans%20in%20the%20form%20of%20%22hops%20over%20segments%22%0Aand%20ii%29%20search%20for%20target%20objects%20using%20natural%20language%20queries%20describing%0Aspatial%20relations%20of%20objects.%20Furthermore%2C%20we%20quantitatively%20analyze%20data%0Aassociation%20at%20the%20segment%20level%2C%20which%20underpins%20inter-image%20connectivity%0Aduring%20mapping%20and%20segment-level%20localization%20when%20revisiting%20the%20same%20place.%0AFinally%2C%20we%20show%20preliminary%20trials%20on%20segment-level%20%60hopping%27%20based%20zero-shot%0Areal-world%20navigation.%20Project%20page%20with%20supplementary%20details%3A%0Aoravus.github.io/RoboHop/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05792v1&entry.124074799=Read"},
{"title": "Imprecise Multi-Armed Bandits", "author": "Vanessa Kosoy", "abstract": "  We introduce a novel multi-armed bandit framework, where each arm is\nassociated with a fixed unknown credal set over the space of outcomes (which\ncan be richer than just the reward). The arm-to-credal-set correspondence comes\nfrom a known class of hypotheses. We then define a notion of regret\ncorresponding to the lower prevision defined by these credal sets.\nEquivalently, the setting can be regarded as a two-player zero-sum game, where,\non each round, the agent chooses an arm and the adversary chooses the\ndistribution over outcomes from a set of options associated with this arm. The\nregret is defined with respect to the value of game. For certain natural\nhypothesis classes, loosely analgous to stochastic linear bandits (which are a\nspecial case of the resulting setting), we propose an algorithm and prove a\ncorresponding upper bound on regret. We also prove lower bounds on regret for\nparticular special cases.\n", "link": "http://arxiv.org/abs/2405.05673v1", "date": "2024-05-09", "relevancy": 1.7842, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.452}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imprecise%20Multi-Armed%20Bandits&body=Title%3A%20Imprecise%20Multi-Armed%20Bandits%0AAuthor%3A%20Vanessa%20Kosoy%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20multi-armed%20bandit%20framework%2C%20where%20each%20arm%20is%0Aassociated%20with%20a%20fixed%20unknown%20credal%20set%20over%20the%20space%20of%20outcomes%20%28which%0Acan%20be%20richer%20than%20just%20the%20reward%29.%20The%20arm-to-credal-set%20correspondence%20comes%0Afrom%20a%20known%20class%20of%20hypotheses.%20We%20then%20define%20a%20notion%20of%20regret%0Acorresponding%20to%20the%20lower%20prevision%20defined%20by%20these%20credal%20sets.%0AEquivalently%2C%20the%20setting%20can%20be%20regarded%20as%20a%20two-player%20zero-sum%20game%2C%20where%2C%0Aon%20each%20round%2C%20the%20agent%20chooses%20an%20arm%20and%20the%20adversary%20chooses%20the%0Adistribution%20over%20outcomes%20from%20a%20set%20of%20options%20associated%20with%20this%20arm.%20The%0Aregret%20is%20defined%20with%20respect%20to%20the%20value%20of%20game.%20For%20certain%20natural%0Ahypothesis%20classes%2C%20loosely%20analgous%20to%20stochastic%20linear%20bandits%20%28which%20are%20a%0Aspecial%20case%20of%20the%20resulting%20setting%29%2C%20we%20propose%20an%20algorithm%20and%20prove%20a%0Acorresponding%20upper%20bound%20on%20regret.%20We%20also%20prove%20lower%20bounds%20on%20regret%20for%0Aparticular%20special%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprecise%2520Multi-Armed%2520Bandits%26entry.906535625%3DVanessa%2520Kosoy%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520multi-armed%2520bandit%2520framework%252C%2520where%2520each%2520arm%2520is%250Aassociated%2520with%2520a%2520fixed%2520unknown%2520credal%2520set%2520over%2520the%2520space%2520of%2520outcomes%2520%2528which%250Acan%2520be%2520richer%2520than%2520just%2520the%2520reward%2529.%2520The%2520arm-to-credal-set%2520correspondence%2520comes%250Afrom%2520a%2520known%2520class%2520of%2520hypotheses.%2520We%2520then%2520define%2520a%2520notion%2520of%2520regret%250Acorresponding%2520to%2520the%2520lower%2520prevision%2520defined%2520by%2520these%2520credal%2520sets.%250AEquivalently%252C%2520the%2520setting%2520can%2520be%2520regarded%2520as%2520a%2520two-player%2520zero-sum%2520game%252C%2520where%252C%250Aon%2520each%2520round%252C%2520the%2520agent%2520chooses%2520an%2520arm%2520and%2520the%2520adversary%2520chooses%2520the%250Adistribution%2520over%2520outcomes%2520from%2520a%2520set%2520of%2520options%2520associated%2520with%2520this%2520arm.%2520The%250Aregret%2520is%2520defined%2520with%2520respect%2520to%2520the%2520value%2520of%2520game.%2520For%2520certain%2520natural%250Ahypothesis%2520classes%252C%2520loosely%2520analgous%2520to%2520stochastic%2520linear%2520bandits%2520%2528which%2520are%2520a%250Aspecial%2520case%2520of%2520the%2520resulting%2520setting%2529%252C%2520we%2520propose%2520an%2520algorithm%2520and%2520prove%2520a%250Acorresponding%2520upper%2520bound%2520on%2520regret.%2520We%2520also%2520prove%2520lower%2520bounds%2520on%2520regret%2520for%250Aparticular%2520special%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imprecise%20Multi-Armed%20Bandits&entry.906535625=Vanessa%20Kosoy&entry.1292438233=%20%20We%20introduce%20a%20novel%20multi-armed%20bandit%20framework%2C%20where%20each%20arm%20is%0Aassociated%20with%20a%20fixed%20unknown%20credal%20set%20over%20the%20space%20of%20outcomes%20%28which%0Acan%20be%20richer%20than%20just%20the%20reward%29.%20The%20arm-to-credal-set%20correspondence%20comes%0Afrom%20a%20known%20class%20of%20hypotheses.%20We%20then%20define%20a%20notion%20of%20regret%0Acorresponding%20to%20the%20lower%20prevision%20defined%20by%20these%20credal%20sets.%0AEquivalently%2C%20the%20setting%20can%20be%20regarded%20as%20a%20two-player%20zero-sum%20game%2C%20where%2C%0Aon%20each%20round%2C%20the%20agent%20chooses%20an%20arm%20and%20the%20adversary%20chooses%20the%0Adistribution%20over%20outcomes%20from%20a%20set%20of%20options%20associated%20with%20this%20arm.%20The%0Aregret%20is%20defined%20with%20respect%20to%20the%20value%20of%20game.%20For%20certain%20natural%0Ahypothesis%20classes%2C%20loosely%20analgous%20to%20stochastic%20linear%20bandits%20%28which%20are%20a%0Aspecial%20case%20of%20the%20resulting%20setting%29%2C%20we%20propose%20an%20algorithm%20and%20prove%20a%0Acorresponding%20upper%20bound%20on%20regret.%20We%20also%20prove%20lower%20bounds%20on%20regret%20for%0Aparticular%20special%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05673v1&entry.124074799=Read"},
{"title": "Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel\n  Fields", "author": "Yuhang Huang and SHilong Zou and Xinwang Liu and Kai Xu", "abstract": "  This paper presents a novel latent 3D diffusion model for the generation of\nneural voxel fields, aiming to achieve accurate part-aware structures. Compared\nto existing methods, there are two key designs to ensure high-quality and\naccurate part-aware generation. On one hand, we introduce a latent 3D diffusion\nprocess for neural voxel fields, enabling generation at significantly higher\nresolutions that can accurately capture rich textural and geometric details. On\nthe other hand, a part-aware shape decoder is introduced to integrate the part\ncodes into the neural voxel fields, guiding the accurate part decomposition and\nproducing high-quality rendering results. Through extensive experimentation and\ncomparisons with state-of-the-art methods, we evaluate our approach across four\ndifferent classes of data. The results demonstrate the superior generative\ncapabilities of our proposed method in part-aware shape generation,\noutperforming existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.00998v2", "date": "2024-05-09", "relevancy": 1.7786, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6366}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5892}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Part-aware%20Shape%20Generation%20with%20Latent%203D%20Diffusion%20of%20Neural%20Voxel%0A%20%20Fields&body=Title%3A%20Part-aware%20Shape%20Generation%20with%20Latent%203D%20Diffusion%20of%20Neural%20Voxel%0A%20%20Fields%0AAuthor%3A%20Yuhang%20Huang%20and%20SHilong%20Zou%20and%20Xinwang%20Liu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20latent%203D%20diffusion%20model%20for%20the%20generation%20of%0Aneural%20voxel%20fields%2C%20aiming%20to%20achieve%20accurate%20part-aware%20structures.%20Compared%0Ato%20existing%20methods%2C%20there%20are%20two%20key%20designs%20to%20ensure%20high-quality%20and%0Aaccurate%20part-aware%20generation.%20On%20one%20hand%2C%20we%20introduce%20a%20latent%203D%20diffusion%0Aprocess%20for%20neural%20voxel%20fields%2C%20enabling%20generation%20at%20significantly%20higher%0Aresolutions%20that%20can%20accurately%20capture%20rich%20textural%20and%20geometric%20details.%20On%0Athe%20other%20hand%2C%20a%20part-aware%20shape%20decoder%20is%20introduced%20to%20integrate%20the%20part%0Acodes%20into%20the%20neural%20voxel%20fields%2C%20guiding%20the%20accurate%20part%20decomposition%20and%0Aproducing%20high-quality%20rendering%20results.%20Through%20extensive%20experimentation%20and%0Acomparisons%20with%20state-of-the-art%20methods%2C%20we%20evaluate%20our%20approach%20across%20four%0Adifferent%20classes%20of%20data.%20The%20results%20demonstrate%20the%20superior%20generative%0Acapabilities%20of%20our%20proposed%20method%20in%20part-aware%20shape%20generation%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00998v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPart-aware%2520Shape%2520Generation%2520with%2520Latent%25203D%2520Diffusion%2520of%2520Neural%2520Voxel%250A%2520%2520Fields%26entry.906535625%3DYuhang%2520Huang%2520and%2520SHilong%2520Zou%2520and%2520Xinwang%2520Liu%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520latent%25203D%2520diffusion%2520model%2520for%2520the%2520generation%2520of%250Aneural%2520voxel%2520fields%252C%2520aiming%2520to%2520achieve%2520accurate%2520part-aware%2520structures.%2520Compared%250Ato%2520existing%2520methods%252C%2520there%2520are%2520two%2520key%2520designs%2520to%2520ensure%2520high-quality%2520and%250Aaccurate%2520part-aware%2520generation.%2520On%2520one%2520hand%252C%2520we%2520introduce%2520a%2520latent%25203D%2520diffusion%250Aprocess%2520for%2520neural%2520voxel%2520fields%252C%2520enabling%2520generation%2520at%2520significantly%2520higher%250Aresolutions%2520that%2520can%2520accurately%2520capture%2520rich%2520textural%2520and%2520geometric%2520details.%2520On%250Athe%2520other%2520hand%252C%2520a%2520part-aware%2520shape%2520decoder%2520is%2520introduced%2520to%2520integrate%2520the%2520part%250Acodes%2520into%2520the%2520neural%2520voxel%2520fields%252C%2520guiding%2520the%2520accurate%2520part%2520decomposition%2520and%250Aproducing%2520high-quality%2520rendering%2520results.%2520Through%2520extensive%2520experimentation%2520and%250Acomparisons%2520with%2520state-of-the-art%2520methods%252C%2520we%2520evaluate%2520our%2520approach%2520across%2520four%250Adifferent%2520classes%2520of%2520data.%2520The%2520results%2520demonstrate%2520the%2520superior%2520generative%250Acapabilities%2520of%2520our%2520proposed%2520method%2520in%2520part-aware%2520shape%2520generation%252C%250Aoutperforming%2520existing%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00998v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-aware%20Shape%20Generation%20with%20Latent%203D%20Diffusion%20of%20Neural%20Voxel%0A%20%20Fields&entry.906535625=Yuhang%20Huang%20and%20SHilong%20Zou%20and%20Xinwang%20Liu%20and%20Kai%20Xu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20latent%203D%20diffusion%20model%20for%20the%20generation%20of%0Aneural%20voxel%20fields%2C%20aiming%20to%20achieve%20accurate%20part-aware%20structures.%20Compared%0Ato%20existing%20methods%2C%20there%20are%20two%20key%20designs%20to%20ensure%20high-quality%20and%0Aaccurate%20part-aware%20generation.%20On%20one%20hand%2C%20we%20introduce%20a%20latent%203D%20diffusion%0Aprocess%20for%20neural%20voxel%20fields%2C%20enabling%20generation%20at%20significantly%20higher%0Aresolutions%20that%20can%20accurately%20capture%20rich%20textural%20and%20geometric%20details.%20On%0Athe%20other%20hand%2C%20a%20part-aware%20shape%20decoder%20is%20introduced%20to%20integrate%20the%20part%0Acodes%20into%20the%20neural%20voxel%20fields%2C%20guiding%20the%20accurate%20part%20decomposition%20and%0Aproducing%20high-quality%20rendering%20results.%20Through%20extensive%20experimentation%20and%0Acomparisons%20with%20state-of-the-art%20methods%2C%20we%20evaluate%20our%20approach%20across%20four%0Adifferent%20classes%20of%20data.%20The%20results%20demonstrate%20the%20superior%20generative%0Acapabilities%20of%20our%20proposed%20method%20in%20part-aware%20shape%20generation%2C%0Aoutperforming%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00998v2&entry.124074799=Read"},
{"title": "PiRD: Physics-informed Residual Diffusion for Flow Field Reconstruction", "author": "Siming Shan and Pengkai Wang and Song Chen and Jiaxu Liu and Chao Xu and Shengze Cai", "abstract": "  The use of machine learning in fluid dynamics is becoming more common to\nexpedite the computation when solving forward and inverse problems of partial\ndifferential equations. Yet, a notable challenge with existing convolutional\nneural network (CNN)-based methods for data fidelity enhancement is their\nreliance on specific low-fidelity data patterns and distributions during the\ntraining phase. In addition, the CNN-based method essentially treats the flow\nreconstruction task as a computer vision task that prioritizes the element-wise\nprecision which lacks a physical and mathematical explanation. This dependence\ncan dramatically affect the models' effectiveness in real-world scenarios,\nespecially when the low-fidelity input deviates from the training data or\ncontains noise not accounted for during training. The introduction of diffusion\nmodels in this context shows promise for improving performance and\ngeneralizability. Unlike direct mapping from a specific low-fidelity to a\nhigh-fidelity distribution, diffusion models learn to transition from any\nlow-fidelity distribution towards a high-fidelity one. Our proposed model -\nPhysics-informed Residual Diffusion, demonstrates the capability to elevate the\nquality of data from both standard low-fidelity inputs, to low-fidelity inputs\nwith injected Gaussian noise, and randomly collected samples. By integrating\nphysics-based insights into the objective function, it further refines the\naccuracy and the fidelity of the inferred high-quality data. Experimental\nresults have shown that our approach can effectively reconstruct high-quality\noutcomes for two-dimensional turbulent flows from a range of low-fidelity input\nconditions without requiring retraining.\n", "link": "http://arxiv.org/abs/2404.08412v2", "date": "2024-05-09", "relevancy": 1.774, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6648}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5799}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PiRD%3A%20Physics-informed%20Residual%20Diffusion%20for%20Flow%20Field%20Reconstruction&body=Title%3A%20PiRD%3A%20Physics-informed%20Residual%20Diffusion%20for%20Flow%20Field%20Reconstruction%0AAuthor%3A%20Siming%20Shan%20and%20Pengkai%20Wang%20and%20Song%20Chen%20and%20Jiaxu%20Liu%20and%20Chao%20Xu%20and%20Shengze%20Cai%0AAbstract%3A%20%20%20The%20use%20of%20machine%20learning%20in%20fluid%20dynamics%20is%20becoming%20more%20common%20to%0Aexpedite%20the%20computation%20when%20solving%20forward%20and%20inverse%20problems%20of%20partial%0Adifferential%20equations.%20Yet%2C%20a%20notable%20challenge%20with%20existing%20convolutional%0Aneural%20network%20%28CNN%29-based%20methods%20for%20data%20fidelity%20enhancement%20is%20their%0Areliance%20on%20specific%20low-fidelity%20data%20patterns%20and%20distributions%20during%20the%0Atraining%20phase.%20In%20addition%2C%20the%20CNN-based%20method%20essentially%20treats%20the%20flow%0Areconstruction%20task%20as%20a%20computer%20vision%20task%20that%20prioritizes%20the%20element-wise%0Aprecision%20which%20lacks%20a%20physical%20and%20mathematical%20explanation.%20This%20dependence%0Acan%20dramatically%20affect%20the%20models%27%20effectiveness%20in%20real-world%20scenarios%2C%0Aespecially%20when%20the%20low-fidelity%20input%20deviates%20from%20the%20training%20data%20or%0Acontains%20noise%20not%20accounted%20for%20during%20training.%20The%20introduction%20of%20diffusion%0Amodels%20in%20this%20context%20shows%20promise%20for%20improving%20performance%20and%0Ageneralizability.%20Unlike%20direct%20mapping%20from%20a%20specific%20low-fidelity%20to%20a%0Ahigh-fidelity%20distribution%2C%20diffusion%20models%20learn%20to%20transition%20from%20any%0Alow-fidelity%20distribution%20towards%20a%20high-fidelity%20one.%20Our%20proposed%20model%20-%0APhysics-informed%20Residual%20Diffusion%2C%20demonstrates%20the%20capability%20to%20elevate%20the%0Aquality%20of%20data%20from%20both%20standard%20low-fidelity%20inputs%2C%20to%20low-fidelity%20inputs%0Awith%20injected%20Gaussian%20noise%2C%20and%20randomly%20collected%20samples.%20By%20integrating%0Aphysics-based%20insights%20into%20the%20objective%20function%2C%20it%20further%20refines%20the%0Aaccuracy%20and%20the%20fidelity%20of%20the%20inferred%20high-quality%20data.%20Experimental%0Aresults%20have%20shown%20that%20our%20approach%20can%20effectively%20reconstruct%20high-quality%0Aoutcomes%20for%20two-dimensional%20turbulent%20flows%20from%20a%20range%20of%20low-fidelity%20input%0Aconditions%20without%20requiring%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPiRD%253A%2520Physics-informed%2520Residual%2520Diffusion%2520for%2520Flow%2520Field%2520Reconstruction%26entry.906535625%3DSiming%2520Shan%2520and%2520Pengkai%2520Wang%2520and%2520Song%2520Chen%2520and%2520Jiaxu%2520Liu%2520and%2520Chao%2520Xu%2520and%2520Shengze%2520Cai%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520machine%2520learning%2520in%2520fluid%2520dynamics%2520is%2520becoming%2520more%2520common%2520to%250Aexpedite%2520the%2520computation%2520when%2520solving%2520forward%2520and%2520inverse%2520problems%2520of%2520partial%250Adifferential%2520equations.%2520Yet%252C%2520a%2520notable%2520challenge%2520with%2520existing%2520convolutional%250Aneural%2520network%2520%2528CNN%2529-based%2520methods%2520for%2520data%2520fidelity%2520enhancement%2520is%2520their%250Areliance%2520on%2520specific%2520low-fidelity%2520data%2520patterns%2520and%2520distributions%2520during%2520the%250Atraining%2520phase.%2520In%2520addition%252C%2520the%2520CNN-based%2520method%2520essentially%2520treats%2520the%2520flow%250Areconstruction%2520task%2520as%2520a%2520computer%2520vision%2520task%2520that%2520prioritizes%2520the%2520element-wise%250Aprecision%2520which%2520lacks%2520a%2520physical%2520and%2520mathematical%2520explanation.%2520This%2520dependence%250Acan%2520dramatically%2520affect%2520the%2520models%2527%2520effectiveness%2520in%2520real-world%2520scenarios%252C%250Aespecially%2520when%2520the%2520low-fidelity%2520input%2520deviates%2520from%2520the%2520training%2520data%2520or%250Acontains%2520noise%2520not%2520accounted%2520for%2520during%2520training.%2520The%2520introduction%2520of%2520diffusion%250Amodels%2520in%2520this%2520context%2520shows%2520promise%2520for%2520improving%2520performance%2520and%250Ageneralizability.%2520Unlike%2520direct%2520mapping%2520from%2520a%2520specific%2520low-fidelity%2520to%2520a%250Ahigh-fidelity%2520distribution%252C%2520diffusion%2520models%2520learn%2520to%2520transition%2520from%2520any%250Alow-fidelity%2520distribution%2520towards%2520a%2520high-fidelity%2520one.%2520Our%2520proposed%2520model%2520-%250APhysics-informed%2520Residual%2520Diffusion%252C%2520demonstrates%2520the%2520capability%2520to%2520elevate%2520the%250Aquality%2520of%2520data%2520from%2520both%2520standard%2520low-fidelity%2520inputs%252C%2520to%2520low-fidelity%2520inputs%250Awith%2520injected%2520Gaussian%2520noise%252C%2520and%2520randomly%2520collected%2520samples.%2520By%2520integrating%250Aphysics-based%2520insights%2520into%2520the%2520objective%2520function%252C%2520it%2520further%2520refines%2520the%250Aaccuracy%2520and%2520the%2520fidelity%2520of%2520the%2520inferred%2520high-quality%2520data.%2520Experimental%250Aresults%2520have%2520shown%2520that%2520our%2520approach%2520can%2520effectively%2520reconstruct%2520high-quality%250Aoutcomes%2520for%2520two-dimensional%2520turbulent%2520flows%2520from%2520a%2520range%2520of%2520low-fidelity%2520input%250Aconditions%2520without%2520requiring%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PiRD%3A%20Physics-informed%20Residual%20Diffusion%20for%20Flow%20Field%20Reconstruction&entry.906535625=Siming%20Shan%20and%20Pengkai%20Wang%20and%20Song%20Chen%20and%20Jiaxu%20Liu%20and%20Chao%20Xu%20and%20Shengze%20Cai&entry.1292438233=%20%20The%20use%20of%20machine%20learning%20in%20fluid%20dynamics%20is%20becoming%20more%20common%20to%0Aexpedite%20the%20computation%20when%20solving%20forward%20and%20inverse%20problems%20of%20partial%0Adifferential%20equations.%20Yet%2C%20a%20notable%20challenge%20with%20existing%20convolutional%0Aneural%20network%20%28CNN%29-based%20methods%20for%20data%20fidelity%20enhancement%20is%20their%0Areliance%20on%20specific%20low-fidelity%20data%20patterns%20and%20distributions%20during%20the%0Atraining%20phase.%20In%20addition%2C%20the%20CNN-based%20method%20essentially%20treats%20the%20flow%0Areconstruction%20task%20as%20a%20computer%20vision%20task%20that%20prioritizes%20the%20element-wise%0Aprecision%20which%20lacks%20a%20physical%20and%20mathematical%20explanation.%20This%20dependence%0Acan%20dramatically%20affect%20the%20models%27%20effectiveness%20in%20real-world%20scenarios%2C%0Aespecially%20when%20the%20low-fidelity%20input%20deviates%20from%20the%20training%20data%20or%0Acontains%20noise%20not%20accounted%20for%20during%20training.%20The%20introduction%20of%20diffusion%0Amodels%20in%20this%20context%20shows%20promise%20for%20improving%20performance%20and%0Ageneralizability.%20Unlike%20direct%20mapping%20from%20a%20specific%20low-fidelity%20to%20a%0Ahigh-fidelity%20distribution%2C%20diffusion%20models%20learn%20to%20transition%20from%20any%0Alow-fidelity%20distribution%20towards%20a%20high-fidelity%20one.%20Our%20proposed%20model%20-%0APhysics-informed%20Residual%20Diffusion%2C%20demonstrates%20the%20capability%20to%20elevate%20the%0Aquality%20of%20data%20from%20both%20standard%20low-fidelity%20inputs%2C%20to%20low-fidelity%20inputs%0Awith%20injected%20Gaussian%20noise%2C%20and%20randomly%20collected%20samples.%20By%20integrating%0Aphysics-based%20insights%20into%20the%20objective%20function%2C%20it%20further%20refines%20the%0Aaccuracy%20and%20the%20fidelity%20of%20the%20inferred%20high-quality%20data.%20Experimental%0Aresults%20have%20shown%20that%20our%20approach%20can%20effectively%20reconstruct%20high-quality%0Aoutcomes%20for%20two-dimensional%20turbulent%20flows%20from%20a%20range%20of%20low-fidelity%20input%0Aconditions%20without%20requiring%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08412v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Masked Faces: Recognition, Detection, and\n  Unmasking", "author": "Mohamed Mahmoud and Mahmoud SalahEldin Kasem and Hyun-Soo Kang", "abstract": "  Masked face recognition (MFR) has emerged as a critical domain in biometric\nidentification, especially by the global COVID-19 pandemic, which introduced\nwidespread face masks. This survey paper presents a comprehensive analysis of\nthe challenges and advancements in recognising and detecting individuals with\nmasked faces, which has seen innovative shifts due to the necessity of adapting\nto new societal norms. Advanced through deep learning techniques, MFR, along\nwith Face Mask Recognition (FMR) and Face Unmasking (FU), represent significant\nareas of focus. These methods address unique challenges posed by obscured\nfacial features, from fully to partially covered faces. Our comprehensive\nreview delves into the various deep learning-based methodologies developed for\nMFR, FMR, and FU, highlighting their distinctive challenges and the solutions\nproposed to overcome them. Additionally, we explore benchmark datasets and\nevaluation metrics specifically tailored for assessing performance in MFR\nresearch. The survey also discusses the substantial obstacles still facing\nresearchers in this field and proposes future directions for the ongoing\ndevelopment of more robust and effective masked face recognition systems. This\npaper serves as an invaluable resource for researchers and practitioners,\noffering insights into the evolving landscape of face recognition technologies\nin the face of global health crises and beyond.\n", "link": "http://arxiv.org/abs/2405.05900v1", "date": "2024-05-09", "relevancy": 1.7666, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4448}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4433}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Masked%20Faces%3A%20Recognition%2C%20Detection%2C%20and%0A%20%20Unmasking&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Masked%20Faces%3A%20Recognition%2C%20Detection%2C%20and%0A%20%20Unmasking%0AAuthor%3A%20Mohamed%20Mahmoud%20and%20Mahmoud%20SalahEldin%20Kasem%20and%20Hyun-Soo%20Kang%0AAbstract%3A%20%20%20Masked%20face%20recognition%20%28MFR%29%20has%20emerged%20as%20a%20critical%20domain%20in%20biometric%0Aidentification%2C%20especially%20by%20the%20global%20COVID-19%20pandemic%2C%20which%20introduced%0Awidespread%20face%20masks.%20This%20survey%20paper%20presents%20a%20comprehensive%20analysis%20of%0Athe%20challenges%20and%20advancements%20in%20recognising%20and%20detecting%20individuals%20with%0Amasked%20faces%2C%20which%20has%20seen%20innovative%20shifts%20due%20to%20the%20necessity%20of%20adapting%0Ato%20new%20societal%20norms.%20Advanced%20through%20deep%20learning%20techniques%2C%20MFR%2C%20along%0Awith%20Face%20Mask%20Recognition%20%28FMR%29%20and%20Face%20Unmasking%20%28FU%29%2C%20represent%20significant%0Aareas%20of%20focus.%20These%20methods%20address%20unique%20challenges%20posed%20by%20obscured%0Afacial%20features%2C%20from%20fully%20to%20partially%20covered%20faces.%20Our%20comprehensive%0Areview%20delves%20into%20the%20various%20deep%20learning-based%20methodologies%20developed%20for%0AMFR%2C%20FMR%2C%20and%20FU%2C%20highlighting%20their%20distinctive%20challenges%20and%20the%20solutions%0Aproposed%20to%20overcome%20them.%20Additionally%2C%20we%20explore%20benchmark%20datasets%20and%0Aevaluation%20metrics%20specifically%20tailored%20for%20assessing%20performance%20in%20MFR%0Aresearch.%20The%20survey%20also%20discusses%20the%20substantial%20obstacles%20still%20facing%0Aresearchers%20in%20this%20field%20and%20proposes%20future%20directions%20for%20the%20ongoing%0Adevelopment%20of%20more%20robust%20and%20effective%20masked%20face%20recognition%20systems.%20This%0Apaper%20serves%20as%20an%20invaluable%20resource%20for%20researchers%20and%20practitioners%2C%0Aoffering%20insights%20into%20the%20evolving%20landscape%20of%20face%20recognition%20technologies%0Ain%20the%20face%20of%20global%20health%20crises%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520of%2520Masked%2520Faces%253A%2520Recognition%252C%2520Detection%252C%2520and%250A%2520%2520Unmasking%26entry.906535625%3DMohamed%2520Mahmoud%2520and%2520Mahmoud%2520SalahEldin%2520Kasem%2520and%2520Hyun-Soo%2520Kang%26entry.1292438233%3D%2520%2520Masked%2520face%2520recognition%2520%2528MFR%2529%2520has%2520emerged%2520as%2520a%2520critical%2520domain%2520in%2520biometric%250Aidentification%252C%2520especially%2520by%2520the%2520global%2520COVID-19%2520pandemic%252C%2520which%2520introduced%250Awidespread%2520face%2520masks.%2520This%2520survey%2520paper%2520presents%2520a%2520comprehensive%2520analysis%2520of%250Athe%2520challenges%2520and%2520advancements%2520in%2520recognising%2520and%2520detecting%2520individuals%2520with%250Amasked%2520faces%252C%2520which%2520has%2520seen%2520innovative%2520shifts%2520due%2520to%2520the%2520necessity%2520of%2520adapting%250Ato%2520new%2520societal%2520norms.%2520Advanced%2520through%2520deep%2520learning%2520techniques%252C%2520MFR%252C%2520along%250Awith%2520Face%2520Mask%2520Recognition%2520%2528FMR%2529%2520and%2520Face%2520Unmasking%2520%2528FU%2529%252C%2520represent%2520significant%250Aareas%2520of%2520focus.%2520These%2520methods%2520address%2520unique%2520challenges%2520posed%2520by%2520obscured%250Afacial%2520features%252C%2520from%2520fully%2520to%2520partially%2520covered%2520faces.%2520Our%2520comprehensive%250Areview%2520delves%2520into%2520the%2520various%2520deep%2520learning-based%2520methodologies%2520developed%2520for%250AMFR%252C%2520FMR%252C%2520and%2520FU%252C%2520highlighting%2520their%2520distinctive%2520challenges%2520and%2520the%2520solutions%250Aproposed%2520to%2520overcome%2520them.%2520Additionally%252C%2520we%2520explore%2520benchmark%2520datasets%2520and%250Aevaluation%2520metrics%2520specifically%2520tailored%2520for%2520assessing%2520performance%2520in%2520MFR%250Aresearch.%2520The%2520survey%2520also%2520discusses%2520the%2520substantial%2520obstacles%2520still%2520facing%250Aresearchers%2520in%2520this%2520field%2520and%2520proposes%2520future%2520directions%2520for%2520the%2520ongoing%250Adevelopment%2520of%2520more%2520robust%2520and%2520effective%2520masked%2520face%2520recognition%2520systems.%2520This%250Apaper%2520serves%2520as%2520an%2520invaluable%2520resource%2520for%2520researchers%2520and%2520practitioners%252C%250Aoffering%2520insights%2520into%2520the%2520evolving%2520landscape%2520of%2520face%2520recognition%2520technologies%250Ain%2520the%2520face%2520of%2520global%2520health%2520crises%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Masked%20Faces%3A%20Recognition%2C%20Detection%2C%20and%0A%20%20Unmasking&entry.906535625=Mohamed%20Mahmoud%20and%20Mahmoud%20SalahEldin%20Kasem%20and%20Hyun-Soo%20Kang&entry.1292438233=%20%20Masked%20face%20recognition%20%28MFR%29%20has%20emerged%20as%20a%20critical%20domain%20in%20biometric%0Aidentification%2C%20especially%20by%20the%20global%20COVID-19%20pandemic%2C%20which%20introduced%0Awidespread%20face%20masks.%20This%20survey%20paper%20presents%20a%20comprehensive%20analysis%20of%0Athe%20challenges%20and%20advancements%20in%20recognising%20and%20detecting%20individuals%20with%0Amasked%20faces%2C%20which%20has%20seen%20innovative%20shifts%20due%20to%20the%20necessity%20of%20adapting%0Ato%20new%20societal%20norms.%20Advanced%20through%20deep%20learning%20techniques%2C%20MFR%2C%20along%0Awith%20Face%20Mask%20Recognition%20%28FMR%29%20and%20Face%20Unmasking%20%28FU%29%2C%20represent%20significant%0Aareas%20of%20focus.%20These%20methods%20address%20unique%20challenges%20posed%20by%20obscured%0Afacial%20features%2C%20from%20fully%20to%20partially%20covered%20faces.%20Our%20comprehensive%0Areview%20delves%20into%20the%20various%20deep%20learning-based%20methodologies%20developed%20for%0AMFR%2C%20FMR%2C%20and%20FU%2C%20highlighting%20their%20distinctive%20challenges%20and%20the%20solutions%0Aproposed%20to%20overcome%20them.%20Additionally%2C%20we%20explore%20benchmark%20datasets%20and%0Aevaluation%20metrics%20specifically%20tailored%20for%20assessing%20performance%20in%20MFR%0Aresearch.%20The%20survey%20also%20discusses%20the%20substantial%20obstacles%20still%20facing%0Aresearchers%20in%20this%20field%20and%20proposes%20future%20directions%20for%20the%20ongoing%0Adevelopment%20of%20more%20robust%20and%20effective%20masked%20face%20recognition%20systems.%20This%0Apaper%20serves%20as%20an%20invaluable%20resource%20for%20researchers%20and%20practitioners%2C%0Aoffering%20insights%20into%20the%20evolving%20landscape%20of%20face%20recognition%20technologies%0Ain%20the%20face%20of%20global%20health%20crises%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05900v1&entry.124074799=Read"},
{"title": "Sequential Amodal Segmentation via Cumulative Occlusion Learning", "author": "Jiayang Ao and Qiuhong Ke and Krista A. Ehinger", "abstract": "  To fully understand the 3D context of a single image, a visual system must be\nable to segment both the visible and occluded regions of objects, while\ndiscerning their occlusion order. Ideally, the system should be able to handle\nany object and not be restricted to segmenting a limited set of object classes,\nespecially in robotic applications. Addressing this need, we introduce a\ndiffusion model with cumulative occlusion learning designed for sequential\namodal segmentation of objects with uncertain categories. This model\niteratively refines the prediction using the cumulative mask strategy during\ndiffusion, effectively capturing the uncertainty of invisible regions and\nadeptly reproducing the complex distribution of shapes and occlusion orders of\noccluded objects. It is akin to the human capability for amodal perception,\ni.e., to decipher the spatial ordering among objects and accurately predict\ncomplete contours for occluded objects in densely layered visual scenes.\nExperimental results across three amodal datasets show that our method\noutperforms established baselines.\n", "link": "http://arxiv.org/abs/2405.05791v1", "date": "2024-05-09", "relevancy": 1.7658, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5993}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5763}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sequential%20Amodal%20Segmentation%20via%20Cumulative%20Occlusion%20Learning&body=Title%3A%20Sequential%20Amodal%20Segmentation%20via%20Cumulative%20Occlusion%20Learning%0AAuthor%3A%20Jiayang%20Ao%20and%20Qiuhong%20Ke%20and%20Krista%20A.%20Ehinger%0AAbstract%3A%20%20%20To%20fully%20understand%20the%203D%20context%20of%20a%20single%20image%2C%20a%20visual%20system%20must%20be%0Aable%20to%20segment%20both%20the%20visible%20and%20occluded%20regions%20of%20objects%2C%20while%0Adiscerning%20their%20occlusion%20order.%20Ideally%2C%20the%20system%20should%20be%20able%20to%20handle%0Aany%20object%20and%20not%20be%20restricted%20to%20segmenting%20a%20limited%20set%20of%20object%20classes%2C%0Aespecially%20in%20robotic%20applications.%20Addressing%20this%20need%2C%20we%20introduce%20a%0Adiffusion%20model%20with%20cumulative%20occlusion%20learning%20designed%20for%20sequential%0Aamodal%20segmentation%20of%20objects%20with%20uncertain%20categories.%20This%20model%0Aiteratively%20refines%20the%20prediction%20using%20the%20cumulative%20mask%20strategy%20during%0Adiffusion%2C%20effectively%20capturing%20the%20uncertainty%20of%20invisible%20regions%20and%0Aadeptly%20reproducing%20the%20complex%20distribution%20of%20shapes%20and%20occlusion%20orders%20of%0Aoccluded%20objects.%20It%20is%20akin%20to%20the%20human%20capability%20for%20amodal%20perception%2C%0Ai.e.%2C%20to%20decipher%20the%20spatial%20ordering%20among%20objects%20and%20accurately%20predict%0Acomplete%20contours%20for%20occluded%20objects%20in%20densely%20layered%20visual%20scenes.%0AExperimental%20results%20across%20three%20amodal%20datasets%20show%20that%20our%20method%0Aoutperforms%20established%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSequential%2520Amodal%2520Segmentation%2520via%2520Cumulative%2520Occlusion%2520Learning%26entry.906535625%3DJiayang%2520Ao%2520and%2520Qiuhong%2520Ke%2520and%2520Krista%2520A.%2520Ehinger%26entry.1292438233%3D%2520%2520To%2520fully%2520understand%2520the%25203D%2520context%2520of%2520a%2520single%2520image%252C%2520a%2520visual%2520system%2520must%2520be%250Aable%2520to%2520segment%2520both%2520the%2520visible%2520and%2520occluded%2520regions%2520of%2520objects%252C%2520while%250Adiscerning%2520their%2520occlusion%2520order.%2520Ideally%252C%2520the%2520system%2520should%2520be%2520able%2520to%2520handle%250Aany%2520object%2520and%2520not%2520be%2520restricted%2520to%2520segmenting%2520a%2520limited%2520set%2520of%2520object%2520classes%252C%250Aespecially%2520in%2520robotic%2520applications.%2520Addressing%2520this%2520need%252C%2520we%2520introduce%2520a%250Adiffusion%2520model%2520with%2520cumulative%2520occlusion%2520learning%2520designed%2520for%2520sequential%250Aamodal%2520segmentation%2520of%2520objects%2520with%2520uncertain%2520categories.%2520This%2520model%250Aiteratively%2520refines%2520the%2520prediction%2520using%2520the%2520cumulative%2520mask%2520strategy%2520during%250Adiffusion%252C%2520effectively%2520capturing%2520the%2520uncertainty%2520of%2520invisible%2520regions%2520and%250Aadeptly%2520reproducing%2520the%2520complex%2520distribution%2520of%2520shapes%2520and%2520occlusion%2520orders%2520of%250Aoccluded%2520objects.%2520It%2520is%2520akin%2520to%2520the%2520human%2520capability%2520for%2520amodal%2520perception%252C%250Ai.e.%252C%2520to%2520decipher%2520the%2520spatial%2520ordering%2520among%2520objects%2520and%2520accurately%2520predict%250Acomplete%2520contours%2520for%2520occluded%2520objects%2520in%2520densely%2520layered%2520visual%2520scenes.%250AExperimental%2520results%2520across%2520three%2520amodal%2520datasets%2520show%2520that%2520our%2520method%250Aoutperforms%2520established%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sequential%20Amodal%20Segmentation%20via%20Cumulative%20Occlusion%20Learning&entry.906535625=Jiayang%20Ao%20and%20Qiuhong%20Ke%20and%20Krista%20A.%20Ehinger&entry.1292438233=%20%20To%20fully%20understand%20the%203D%20context%20of%20a%20single%20image%2C%20a%20visual%20system%20must%20be%0Aable%20to%20segment%20both%20the%20visible%20and%20occluded%20regions%20of%20objects%2C%20while%0Adiscerning%20their%20occlusion%20order.%20Ideally%2C%20the%20system%20should%20be%20able%20to%20handle%0Aany%20object%20and%20not%20be%20restricted%20to%20segmenting%20a%20limited%20set%20of%20object%20classes%2C%0Aespecially%20in%20robotic%20applications.%20Addressing%20this%20need%2C%20we%20introduce%20a%0Adiffusion%20model%20with%20cumulative%20occlusion%20learning%20designed%20for%20sequential%0Aamodal%20segmentation%20of%20objects%20with%20uncertain%20categories.%20This%20model%0Aiteratively%20refines%20the%20prediction%20using%20the%20cumulative%20mask%20strategy%20during%0Adiffusion%2C%20effectively%20capturing%20the%20uncertainty%20of%20invisible%20regions%20and%0Aadeptly%20reproducing%20the%20complex%20distribution%20of%20shapes%20and%20occlusion%20orders%20of%0Aoccluded%20objects.%20It%20is%20akin%20to%20the%20human%20capability%20for%20amodal%20perception%2C%0Ai.e.%2C%20to%20decipher%20the%20spatial%20ordering%20among%20objects%20and%20accurately%20predict%0Acomplete%20contours%20for%20occluded%20objects%20in%20densely%20layered%20visual%20scenes.%0AExperimental%20results%20across%20three%20amodal%20datasets%20show%20that%20our%20method%0Aoutperforms%20established%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05791v1&entry.124074799=Read"},
{"title": "Learning to Slice Wi-Fi Networks: A State-Augmented Primal-Dual Approach", "author": "Yi\u011fit Berkay Uslu and Roya Doostnejad and Alejandro Ribeiro and Navid NaderiAlizadeh", "abstract": "  Network slicing is a key feature in 5G/NG cellular networks that creates\ncustomized slices for different service types with various quality-of-service\n(QoS) requirements, which can achieve service differentiation and guarantee\nservice-level agreement (SLA) for each service type. In Wi-Fi networks, there\nis limited prior work on slicing, and a potential solution is based on a\nmulti-tenant architecture on a single access point (AP) that dedicates\ndifferent channels to different slices. In this paper, we define a flexible,\nconstrained learning framework to enable slicing in Wi-Fi networks subject to\nQoS requirements. We specifically propose an unsupervised learning-based\nnetwork slicing method that leverages a state-augmented primal-dual algorithm,\nwhere a neural network policy is trained offline to optimize a Lagrangian\nfunction and the dual variable dynamics are updated online in the execution\nphase. We show that state augmentation is crucial for generating slicing\ndecisions that meet the ergodic QoS requirements.\n", "link": "http://arxiv.org/abs/2405.05748v1", "date": "2024-05-09", "relevancy": 1.7586, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4536}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4422}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Slice%20Wi-Fi%20Networks%3A%20A%20State-Augmented%20Primal-Dual%20Approach&body=Title%3A%20Learning%20to%20Slice%20Wi-Fi%20Networks%3A%20A%20State-Augmented%20Primal-Dual%20Approach%0AAuthor%3A%20Yi%C4%9Fit%20Berkay%20Uslu%20and%20Roya%20Doostnejad%20and%20Alejandro%20Ribeiro%20and%20Navid%20NaderiAlizadeh%0AAbstract%3A%20%20%20Network%20slicing%20is%20a%20key%20feature%20in%205G/NG%20cellular%20networks%20that%20creates%0Acustomized%20slices%20for%20different%20service%20types%20with%20various%20quality-of-service%0A%28QoS%29%20requirements%2C%20which%20can%20achieve%20service%20differentiation%20and%20guarantee%0Aservice-level%20agreement%20%28SLA%29%20for%20each%20service%20type.%20In%20Wi-Fi%20networks%2C%20there%0Ais%20limited%20prior%20work%20on%20slicing%2C%20and%20a%20potential%20solution%20is%20based%20on%20a%0Amulti-tenant%20architecture%20on%20a%20single%20access%20point%20%28AP%29%20that%20dedicates%0Adifferent%20channels%20to%20different%20slices.%20In%20this%20paper%2C%20we%20define%20a%20flexible%2C%0Aconstrained%20learning%20framework%20to%20enable%20slicing%20in%20Wi-Fi%20networks%20subject%20to%0AQoS%20requirements.%20We%20specifically%20propose%20an%20unsupervised%20learning-based%0Anetwork%20slicing%20method%20that%20leverages%20a%20state-augmented%20primal-dual%20algorithm%2C%0Awhere%20a%20neural%20network%20policy%20is%20trained%20offline%20to%20optimize%20a%20Lagrangian%0Afunction%20and%20the%20dual%20variable%20dynamics%20are%20updated%20online%20in%20the%20execution%0Aphase.%20We%20show%20that%20state%20augmentation%20is%20crucial%20for%20generating%20slicing%0Adecisions%20that%20meet%20the%20ergodic%20QoS%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Slice%2520Wi-Fi%2520Networks%253A%2520A%2520State-Augmented%2520Primal-Dual%2520Approach%26entry.906535625%3DYi%25C4%259Fit%2520Berkay%2520Uslu%2520and%2520Roya%2520Doostnejad%2520and%2520Alejandro%2520Ribeiro%2520and%2520Navid%2520NaderiAlizadeh%26entry.1292438233%3D%2520%2520Network%2520slicing%2520is%2520a%2520key%2520feature%2520in%25205G/NG%2520cellular%2520networks%2520that%2520creates%250Acustomized%2520slices%2520for%2520different%2520service%2520types%2520with%2520various%2520quality-of-service%250A%2528QoS%2529%2520requirements%252C%2520which%2520can%2520achieve%2520service%2520differentiation%2520and%2520guarantee%250Aservice-level%2520agreement%2520%2528SLA%2529%2520for%2520each%2520service%2520type.%2520In%2520Wi-Fi%2520networks%252C%2520there%250Ais%2520limited%2520prior%2520work%2520on%2520slicing%252C%2520and%2520a%2520potential%2520solution%2520is%2520based%2520on%2520a%250Amulti-tenant%2520architecture%2520on%2520a%2520single%2520access%2520point%2520%2528AP%2529%2520that%2520dedicates%250Adifferent%2520channels%2520to%2520different%2520slices.%2520In%2520this%2520paper%252C%2520we%2520define%2520a%2520flexible%252C%250Aconstrained%2520learning%2520framework%2520to%2520enable%2520slicing%2520in%2520Wi-Fi%2520networks%2520subject%2520to%250AQoS%2520requirements.%2520We%2520specifically%2520propose%2520an%2520unsupervised%2520learning-based%250Anetwork%2520slicing%2520method%2520that%2520leverages%2520a%2520state-augmented%2520primal-dual%2520algorithm%252C%250Awhere%2520a%2520neural%2520network%2520policy%2520is%2520trained%2520offline%2520to%2520optimize%2520a%2520Lagrangian%250Afunction%2520and%2520the%2520dual%2520variable%2520dynamics%2520are%2520updated%2520online%2520in%2520the%2520execution%250Aphase.%2520We%2520show%2520that%2520state%2520augmentation%2520is%2520crucial%2520for%2520generating%2520slicing%250Adecisions%2520that%2520meet%2520the%2520ergodic%2520QoS%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Slice%20Wi-Fi%20Networks%3A%20A%20State-Augmented%20Primal-Dual%20Approach&entry.906535625=Yi%C4%9Fit%20Berkay%20Uslu%20and%20Roya%20Doostnejad%20and%20Alejandro%20Ribeiro%20and%20Navid%20NaderiAlizadeh&entry.1292438233=%20%20Network%20slicing%20is%20a%20key%20feature%20in%205G/NG%20cellular%20networks%20that%20creates%0Acustomized%20slices%20for%20different%20service%20types%20with%20various%20quality-of-service%0A%28QoS%29%20requirements%2C%20which%20can%20achieve%20service%20differentiation%20and%20guarantee%0Aservice-level%20agreement%20%28SLA%29%20for%20each%20service%20type.%20In%20Wi-Fi%20networks%2C%20there%0Ais%20limited%20prior%20work%20on%20slicing%2C%20and%20a%20potential%20solution%20is%20based%20on%20a%0Amulti-tenant%20architecture%20on%20a%20single%20access%20point%20%28AP%29%20that%20dedicates%0Adifferent%20channels%20to%20different%20slices.%20In%20this%20paper%2C%20we%20define%20a%20flexible%2C%0Aconstrained%20learning%20framework%20to%20enable%20slicing%20in%20Wi-Fi%20networks%20subject%20to%0AQoS%20requirements.%20We%20specifically%20propose%20an%20unsupervised%20learning-based%0Anetwork%20slicing%20method%20that%20leverages%20a%20state-augmented%20primal-dual%20algorithm%2C%0Awhere%20a%20neural%20network%20policy%20is%20trained%20offline%20to%20optimize%20a%20Lagrangian%0Afunction%20and%20the%20dual%20variable%20dynamics%20are%20updated%20online%20in%20the%20execution%0Aphase.%20We%20show%20that%20state%20augmentation%20is%20crucial%20for%20generating%20slicing%0Adecisions%20that%20meet%20the%20ergodic%20QoS%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05748v1&entry.124074799=Read"},
{"title": "A Linear Reconstruction Approach for Attribute Inference Attacks against\n  Synthetic Data", "author": "Meenatchi Sundaram Muthu Selva Annamalai and Andrea Gadotti and Luc Rocher", "abstract": "  Recent advances in synthetic data generation (SDG) have been hailed as a\nsolution to the difficult problem of sharing sensitive data while protecting\nprivacy. SDG aims to learn statistical properties of real data in order to\ngenerate \"artificial\" data that are structurally and statistically similar to\nsensitive data. However, prior research suggests that inference attacks on\nsynthetic data can undermine privacy, but only for specific outlier records. In\nthis work, we introduce a new attribute inference attack against synthetic\ndata. The attack is based on linear reconstruction methods for aggregate\nstatistics, which target all records in the dataset, not only outliers. We\nevaluate our attack on state-of-the-art SDG algorithms, including Probabilistic\nGraphical Models, Generative Adversarial Networks, and recent differentially\nprivate SDG mechanisms. By defining a formal privacy game, we show that our\nattack can be highly accurate even on arbitrary records, and that this is the\nresult of individual information leakage (as opposed to population-level\ninference). We then systematically evaluate the tradeoff between protecting\nprivacy and preserving statistical utility. Our findings suggest that current\nSDG methods cannot consistently provide sufficient privacy protection against\ninference attacks while retaining reasonable utility. The best method\nevaluated, a differentially private SDG mechanism, can provide both protection\nagainst inference attacks and reasonable utility, but only in very specific\nsettings. Lastly, we show that releasing a larger number of synthetic records\ncan improve utility but at the cost of making attacks far more effective.\n", "link": "http://arxiv.org/abs/2301.10053v3", "date": "2024-05-09", "relevancy": 1.7527, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4485}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4401}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Linear%20Reconstruction%20Approach%20for%20Attribute%20Inference%20Attacks%20against%0A%20%20Synthetic%20Data&body=Title%3A%20A%20Linear%20Reconstruction%20Approach%20for%20Attribute%20Inference%20Attacks%20against%0A%20%20Synthetic%20Data%0AAuthor%3A%20Meenatchi%20Sundaram%20Muthu%20Selva%20Annamalai%20and%20Andrea%20Gadotti%20and%20Luc%20Rocher%0AAbstract%3A%20%20%20Recent%20advances%20in%20synthetic%20data%20generation%20%28SDG%29%20have%20been%20hailed%20as%20a%0Asolution%20to%20the%20difficult%20problem%20of%20sharing%20sensitive%20data%20while%20protecting%0Aprivacy.%20SDG%20aims%20to%20learn%20statistical%20properties%20of%20real%20data%20in%20order%20to%0Agenerate%20%22artificial%22%20data%20that%20are%20structurally%20and%20statistically%20similar%20to%0Asensitive%20data.%20However%2C%20prior%20research%20suggests%20that%20inference%20attacks%20on%0Asynthetic%20data%20can%20undermine%20privacy%2C%20but%20only%20for%20specific%20outlier%20records.%20In%0Athis%20work%2C%20we%20introduce%20a%20new%20attribute%20inference%20attack%20against%20synthetic%0Adata.%20The%20attack%20is%20based%20on%20linear%20reconstruction%20methods%20for%20aggregate%0Astatistics%2C%20which%20target%20all%20records%20in%20the%20dataset%2C%20not%20only%20outliers.%20We%0Aevaluate%20our%20attack%20on%20state-of-the-art%20SDG%20algorithms%2C%20including%20Probabilistic%0AGraphical%20Models%2C%20Generative%20Adversarial%20Networks%2C%20and%20recent%20differentially%0Aprivate%20SDG%20mechanisms.%20By%20defining%20a%20formal%20privacy%20game%2C%20we%20show%20that%20our%0Aattack%20can%20be%20highly%20accurate%20even%20on%20arbitrary%20records%2C%20and%20that%20this%20is%20the%0Aresult%20of%20individual%20information%20leakage%20%28as%20opposed%20to%20population-level%0Ainference%29.%20We%20then%20systematically%20evaluate%20the%20tradeoff%20between%20protecting%0Aprivacy%20and%20preserving%20statistical%20utility.%20Our%20findings%20suggest%20that%20current%0ASDG%20methods%20cannot%20consistently%20provide%20sufficient%20privacy%20protection%20against%0Ainference%20attacks%20while%20retaining%20reasonable%20utility.%20The%20best%20method%0Aevaluated%2C%20a%20differentially%20private%20SDG%20mechanism%2C%20can%20provide%20both%20protection%0Aagainst%20inference%20attacks%20and%20reasonable%20utility%2C%20but%20only%20in%20very%20specific%0Asettings.%20Lastly%2C%20we%20show%20that%20releasing%20a%20larger%20number%20of%20synthetic%20records%0Acan%20improve%20utility%20but%20at%20the%20cost%20of%20making%20attacks%20far%20more%20effective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.10053v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Linear%2520Reconstruction%2520Approach%2520for%2520Attribute%2520Inference%2520Attacks%2520against%250A%2520%2520Synthetic%2520Data%26entry.906535625%3DMeenatchi%2520Sundaram%2520Muthu%2520Selva%2520Annamalai%2520and%2520Andrea%2520Gadotti%2520and%2520Luc%2520Rocher%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520synthetic%2520data%2520generation%2520%2528SDG%2529%2520have%2520been%2520hailed%2520as%2520a%250Asolution%2520to%2520the%2520difficult%2520problem%2520of%2520sharing%2520sensitive%2520data%2520while%2520protecting%250Aprivacy.%2520SDG%2520aims%2520to%2520learn%2520statistical%2520properties%2520of%2520real%2520data%2520in%2520order%2520to%250Agenerate%2520%2522artificial%2522%2520data%2520that%2520are%2520structurally%2520and%2520statistically%2520similar%2520to%250Asensitive%2520data.%2520However%252C%2520prior%2520research%2520suggests%2520that%2520inference%2520attacks%2520on%250Asynthetic%2520data%2520can%2520undermine%2520privacy%252C%2520but%2520only%2520for%2520specific%2520outlier%2520records.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520new%2520attribute%2520inference%2520attack%2520against%2520synthetic%250Adata.%2520The%2520attack%2520is%2520based%2520on%2520linear%2520reconstruction%2520methods%2520for%2520aggregate%250Astatistics%252C%2520which%2520target%2520all%2520records%2520in%2520the%2520dataset%252C%2520not%2520only%2520outliers.%2520We%250Aevaluate%2520our%2520attack%2520on%2520state-of-the-art%2520SDG%2520algorithms%252C%2520including%2520Probabilistic%250AGraphical%2520Models%252C%2520Generative%2520Adversarial%2520Networks%252C%2520and%2520recent%2520differentially%250Aprivate%2520SDG%2520mechanisms.%2520By%2520defining%2520a%2520formal%2520privacy%2520game%252C%2520we%2520show%2520that%2520our%250Aattack%2520can%2520be%2520highly%2520accurate%2520even%2520on%2520arbitrary%2520records%252C%2520and%2520that%2520this%2520is%2520the%250Aresult%2520of%2520individual%2520information%2520leakage%2520%2528as%2520opposed%2520to%2520population-level%250Ainference%2529.%2520We%2520then%2520systematically%2520evaluate%2520the%2520tradeoff%2520between%2520protecting%250Aprivacy%2520and%2520preserving%2520statistical%2520utility.%2520Our%2520findings%2520suggest%2520that%2520current%250ASDG%2520methods%2520cannot%2520consistently%2520provide%2520sufficient%2520privacy%2520protection%2520against%250Ainference%2520attacks%2520while%2520retaining%2520reasonable%2520utility.%2520The%2520best%2520method%250Aevaluated%252C%2520a%2520differentially%2520private%2520SDG%2520mechanism%252C%2520can%2520provide%2520both%2520protection%250Aagainst%2520inference%2520attacks%2520and%2520reasonable%2520utility%252C%2520but%2520only%2520in%2520very%2520specific%250Asettings.%2520Lastly%252C%2520we%2520show%2520that%2520releasing%2520a%2520larger%2520number%2520of%2520synthetic%2520records%250Acan%2520improve%2520utility%2520but%2520at%2520the%2520cost%2520of%2520making%2520attacks%2520far%2520more%2520effective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.10053v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Linear%20Reconstruction%20Approach%20for%20Attribute%20Inference%20Attacks%20against%0A%20%20Synthetic%20Data&entry.906535625=Meenatchi%20Sundaram%20Muthu%20Selva%20Annamalai%20and%20Andrea%20Gadotti%20and%20Luc%20Rocher&entry.1292438233=%20%20Recent%20advances%20in%20synthetic%20data%20generation%20%28SDG%29%20have%20been%20hailed%20as%20a%0Asolution%20to%20the%20difficult%20problem%20of%20sharing%20sensitive%20data%20while%20protecting%0Aprivacy.%20SDG%20aims%20to%20learn%20statistical%20properties%20of%20real%20data%20in%20order%20to%0Agenerate%20%22artificial%22%20data%20that%20are%20structurally%20and%20statistically%20similar%20to%0Asensitive%20data.%20However%2C%20prior%20research%20suggests%20that%20inference%20attacks%20on%0Asynthetic%20data%20can%20undermine%20privacy%2C%20but%20only%20for%20specific%20outlier%20records.%20In%0Athis%20work%2C%20we%20introduce%20a%20new%20attribute%20inference%20attack%20against%20synthetic%0Adata.%20The%20attack%20is%20based%20on%20linear%20reconstruction%20methods%20for%20aggregate%0Astatistics%2C%20which%20target%20all%20records%20in%20the%20dataset%2C%20not%20only%20outliers.%20We%0Aevaluate%20our%20attack%20on%20state-of-the-art%20SDG%20algorithms%2C%20including%20Probabilistic%0AGraphical%20Models%2C%20Generative%20Adversarial%20Networks%2C%20and%20recent%20differentially%0Aprivate%20SDG%20mechanisms.%20By%20defining%20a%20formal%20privacy%20game%2C%20we%20show%20that%20our%0Aattack%20can%20be%20highly%20accurate%20even%20on%20arbitrary%20records%2C%20and%20that%20this%20is%20the%0Aresult%20of%20individual%20information%20leakage%20%28as%20opposed%20to%20population-level%0Ainference%29.%20We%20then%20systematically%20evaluate%20the%20tradeoff%20between%20protecting%0Aprivacy%20and%20preserving%20statistical%20utility.%20Our%20findings%20suggest%20that%20current%0ASDG%20methods%20cannot%20consistently%20provide%20sufficient%20privacy%20protection%20against%0Ainference%20attacks%20while%20retaining%20reasonable%20utility.%20The%20best%20method%0Aevaluated%2C%20a%20differentially%20private%20SDG%20mechanism%2C%20can%20provide%20both%20protection%0Aagainst%20inference%20attacks%20and%20reasonable%20utility%2C%20but%20only%20in%20very%20specific%0Asettings.%20Lastly%2C%20we%20show%20that%20releasing%20a%20larger%20number%20of%20synthetic%20records%0Acan%20improve%20utility%20but%20at%20the%20cost%20of%20making%20attacks%20far%20more%20effective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.10053v3&entry.124074799=Read"},
{"title": "Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous\n  Driving via Semantic Masked World Model", "author": "Zeyu Gao and Yao Mu and Chen Chen and Jingliang Duan and Shengbo Eben Li and Ping Luo and Yanfeng Lu", "abstract": "  End-to-end autonomous driving provides a feasible way to automatically\nmaximize overall driving system performance by directly mapping the raw pixels\nfrom a front-facing camera to control signals. Recent advanced methods\nconstruct a latent world model to map the high dimensional observations into\ncompact latent space. However, the latent states embedded by the world model\nproposed in previous works may contain a large amount of task-irrelevant\ninformation, resulting in low sampling efficiency and poor robustness to input\nperturbations. Meanwhile, the training data distribution is usually unbalanced,\nand the learned policy is challenging to cope with the corner cases during the\ndriving process. To solve the above challenges, we present a SEMantic Masked\nrecurrent world model (SEM2), which introduces a semantic filter to extract key\ndriving-relevant features and make decisions via the filtered features, and is\ntrained with a multi-source data sampler, which aggregates common data and\nmultiple corner case data in a single batch, to balance the data distribution.\nExtensive experiments on CARLA show our method outperforms the state-of-the-art\napproaches in terms of sample efficiency and robustness to input permutations.\n", "link": "http://arxiv.org/abs/2210.04017v3", "date": "2024-05-09", "relevancy": 1.7523, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6076}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5782}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhance%20Sample%20Efficiency%20and%20Robustness%20of%20End-to-end%20Urban%20Autonomous%0A%20%20Driving%20via%20Semantic%20Masked%20World%20Model&body=Title%3A%20Enhance%20Sample%20Efficiency%20and%20Robustness%20of%20End-to-end%20Urban%20Autonomous%0A%20%20Driving%20via%20Semantic%20Masked%20World%20Model%0AAuthor%3A%20Zeyu%20Gao%20and%20Yao%20Mu%20and%20Chen%20Chen%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li%20and%20Ping%20Luo%20and%20Yanfeng%20Lu%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20provides%20a%20feasible%20way%20to%20automatically%0Amaximize%20overall%20driving%20system%20performance%20by%20directly%20mapping%20the%20raw%20pixels%0Afrom%20a%20front-facing%20camera%20to%20control%20signals.%20Recent%20advanced%20methods%0Aconstruct%20a%20latent%20world%20model%20to%20map%20the%20high%20dimensional%20observations%20into%0Acompact%20latent%20space.%20However%2C%20the%20latent%20states%20embedded%20by%20the%20world%20model%0Aproposed%20in%20previous%20works%20may%20contain%20a%20large%20amount%20of%20task-irrelevant%0Ainformation%2C%20resulting%20in%20low%20sampling%20efficiency%20and%20poor%20robustness%20to%20input%0Aperturbations.%20Meanwhile%2C%20the%20training%20data%20distribution%20is%20usually%20unbalanced%2C%0Aand%20the%20learned%20policy%20is%20challenging%20to%20cope%20with%20the%20corner%20cases%20during%20the%0Adriving%20process.%20To%20solve%20the%20above%20challenges%2C%20we%20present%20a%20SEMantic%20Masked%0Arecurrent%20world%20model%20%28SEM2%29%2C%20which%20introduces%20a%20semantic%20filter%20to%20extract%20key%0Adriving-relevant%20features%20and%20make%20decisions%20via%20the%20filtered%20features%2C%20and%20is%0Atrained%20with%20a%20multi-source%20data%20sampler%2C%20which%20aggregates%20common%20data%20and%0Amultiple%20corner%20case%20data%20in%20a%20single%20batch%2C%20to%20balance%20the%20data%20distribution.%0AExtensive%20experiments%20on%20CARLA%20show%20our%20method%20outperforms%20the%20state-of-the-art%0Aapproaches%20in%20terms%20of%20sample%20efficiency%20and%20robustness%20to%20input%20permutations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.04017v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhance%2520Sample%2520Efficiency%2520and%2520Robustness%2520of%2520End-to-end%2520Urban%2520Autonomous%250A%2520%2520Driving%2520via%2520Semantic%2520Masked%2520World%2520Model%26entry.906535625%3DZeyu%2520Gao%2520and%2520Yao%2520Mu%2520and%2520Chen%2520Chen%2520and%2520Jingliang%2520Duan%2520and%2520Shengbo%2520Eben%2520Li%2520and%2520Ping%2520Luo%2520and%2520Yanfeng%2520Lu%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520provides%2520a%2520feasible%2520way%2520to%2520automatically%250Amaximize%2520overall%2520driving%2520system%2520performance%2520by%2520directly%2520mapping%2520the%2520raw%2520pixels%250Afrom%2520a%2520front-facing%2520camera%2520to%2520control%2520signals.%2520Recent%2520advanced%2520methods%250Aconstruct%2520a%2520latent%2520world%2520model%2520to%2520map%2520the%2520high%2520dimensional%2520observations%2520into%250Acompact%2520latent%2520space.%2520However%252C%2520the%2520latent%2520states%2520embedded%2520by%2520the%2520world%2520model%250Aproposed%2520in%2520previous%2520works%2520may%2520contain%2520a%2520large%2520amount%2520of%2520task-irrelevant%250Ainformation%252C%2520resulting%2520in%2520low%2520sampling%2520efficiency%2520and%2520poor%2520robustness%2520to%2520input%250Aperturbations.%2520Meanwhile%252C%2520the%2520training%2520data%2520distribution%2520is%2520usually%2520unbalanced%252C%250Aand%2520the%2520learned%2520policy%2520is%2520challenging%2520to%2520cope%2520with%2520the%2520corner%2520cases%2520during%2520the%250Adriving%2520process.%2520To%2520solve%2520the%2520above%2520challenges%252C%2520we%2520present%2520a%2520SEMantic%2520Masked%250Arecurrent%2520world%2520model%2520%2528SEM2%2529%252C%2520which%2520introduces%2520a%2520semantic%2520filter%2520to%2520extract%2520key%250Adriving-relevant%2520features%2520and%2520make%2520decisions%2520via%2520the%2520filtered%2520features%252C%2520and%2520is%250Atrained%2520with%2520a%2520multi-source%2520data%2520sampler%252C%2520which%2520aggregates%2520common%2520data%2520and%250Amultiple%2520corner%2520case%2520data%2520in%2520a%2520single%2520batch%252C%2520to%2520balance%2520the%2520data%2520distribution.%250AExtensive%2520experiments%2520on%2520CARLA%2520show%2520our%2520method%2520outperforms%2520the%2520state-of-the-art%250Aapproaches%2520in%2520terms%2520of%2520sample%2520efficiency%2520and%2520robustness%2520to%2520input%2520permutations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.04017v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhance%20Sample%20Efficiency%20and%20Robustness%20of%20End-to-end%20Urban%20Autonomous%0A%20%20Driving%20via%20Semantic%20Masked%20World%20Model&entry.906535625=Zeyu%20Gao%20and%20Yao%20Mu%20and%20Chen%20Chen%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li%20and%20Ping%20Luo%20and%20Yanfeng%20Lu&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20provides%20a%20feasible%20way%20to%20automatically%0Amaximize%20overall%20driving%20system%20performance%20by%20directly%20mapping%20the%20raw%20pixels%0Afrom%20a%20front-facing%20camera%20to%20control%20signals.%20Recent%20advanced%20methods%0Aconstruct%20a%20latent%20world%20model%20to%20map%20the%20high%20dimensional%20observations%20into%0Acompact%20latent%20space.%20However%2C%20the%20latent%20states%20embedded%20by%20the%20world%20model%0Aproposed%20in%20previous%20works%20may%20contain%20a%20large%20amount%20of%20task-irrelevant%0Ainformation%2C%20resulting%20in%20low%20sampling%20efficiency%20and%20poor%20robustness%20to%20input%0Aperturbations.%20Meanwhile%2C%20the%20training%20data%20distribution%20is%20usually%20unbalanced%2C%0Aand%20the%20learned%20policy%20is%20challenging%20to%20cope%20with%20the%20corner%20cases%20during%20the%0Adriving%20process.%20To%20solve%20the%20above%20challenges%2C%20we%20present%20a%20SEMantic%20Masked%0Arecurrent%20world%20model%20%28SEM2%29%2C%20which%20introduces%20a%20semantic%20filter%20to%20extract%20key%0Adriving-relevant%20features%20and%20make%20decisions%20via%20the%20filtered%20features%2C%20and%20is%0Atrained%20with%20a%20multi-source%20data%20sampler%2C%20which%20aggregates%20common%20data%20and%0Amultiple%20corner%20case%20data%20in%20a%20single%20batch%2C%20to%20balance%20the%20data%20distribution.%0AExtensive%20experiments%20on%20CARLA%20show%20our%20method%20outperforms%20the%20state-of-the-art%0Aapproaches%20in%20terms%20of%20sample%20efficiency%20and%20robustness%20to%20input%20permutations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.04017v3&entry.124074799=Read"},
{"title": "ScatterUQ: Interactive Uncertainty Visualizations for Multiclass Deep\n  Learning Problems", "author": "Harry Li and Steven Jorgensen and John Holodnak and Allan Wollaber", "abstract": "  Recently, uncertainty-aware deep learning methods for multiclass labeling\nproblems have been developed that provide calibrated class prediction\nprobabilities and out-of-distribution (OOD) indicators, letting machine\nlearning (ML) consumers and engineers gauge a model's confidence in its\npredictions. However, this extra neural network prediction information is\nchallenging to scalably convey visually for arbitrary data sources under\nmultiple uncertainty contexts. To address these challenges, we present\nScatterUQ, an interactive system that provides targeted visualizations to allow\nusers to better understand model performance in context-driven uncertainty\nsettings. ScatterUQ leverages recent advances in distance-aware neural\nnetworks, together with dimensionality reduction techniques, to construct\nrobust, 2-D scatter plots explaining why a model predicts a test example to be\n(1) in-distribution and of a particular class, (2) in-distribution but unsure\nof the class, and (3) out-of-distribution. ML consumers and engineers can\nvisually compare the salient features of test samples with training examples\nthrough the use of a ``hover callback'' to understand model uncertainty\nperformance and decide follow up courses of action. We demonstrate the\neffectiveness of ScatterUQ to explain model uncertainty for a multiclass image\nclassification on a distance-aware neural network trained on Fashion-MNIST and\ntested on Fashion-MNIST (in distribution) and MNIST digits (out of\ndistribution), as well as a deep learning model for a cyber dataset. We\nquantitatively evaluate dimensionality reduction techniques to optimize our\ncontextually driven UQ visualizations. Our results indicate that the ScatterUQ\nsystem should scale to arbitrary, multiclass datasets. Our code is available at\nhttps://github.com/mit-ll-responsible-ai/equine-webapp\n", "link": "http://arxiv.org/abs/2308.04588v2", "date": "2024-05-09", "relevancy": 1.7513, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6249}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5825}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScatterUQ%3A%20Interactive%20Uncertainty%20Visualizations%20for%20Multiclass%20Deep%0A%20%20Learning%20Problems&body=Title%3A%20ScatterUQ%3A%20Interactive%20Uncertainty%20Visualizations%20for%20Multiclass%20Deep%0A%20%20Learning%20Problems%0AAuthor%3A%20Harry%20Li%20and%20Steven%20Jorgensen%20and%20John%20Holodnak%20and%20Allan%20Wollaber%0AAbstract%3A%20%20%20Recently%2C%20uncertainty-aware%20deep%20learning%20methods%20for%20multiclass%20labeling%0Aproblems%20have%20been%20developed%20that%20provide%20calibrated%20class%20prediction%0Aprobabilities%20and%20out-of-distribution%20%28OOD%29%20indicators%2C%20letting%20machine%0Alearning%20%28ML%29%20consumers%20and%20engineers%20gauge%20a%20model%27s%20confidence%20in%20its%0Apredictions.%20However%2C%20this%20extra%20neural%20network%20prediction%20information%20is%0Achallenging%20to%20scalably%20convey%20visually%20for%20arbitrary%20data%20sources%20under%0Amultiple%20uncertainty%20contexts.%20To%20address%20these%20challenges%2C%20we%20present%0AScatterUQ%2C%20an%20interactive%20system%20that%20provides%20targeted%20visualizations%20to%20allow%0Ausers%20to%20better%20understand%20model%20performance%20in%20context-driven%20uncertainty%0Asettings.%20ScatterUQ%20leverages%20recent%20advances%20in%20distance-aware%20neural%0Anetworks%2C%20together%20with%20dimensionality%20reduction%20techniques%2C%20to%20construct%0Arobust%2C%202-D%20scatter%20plots%20explaining%20why%20a%20model%20predicts%20a%20test%20example%20to%20be%0A%281%29%20in-distribution%20and%20of%20a%20particular%20class%2C%20%282%29%20in-distribution%20but%20unsure%0Aof%20the%20class%2C%20and%20%283%29%20out-of-distribution.%20ML%20consumers%20and%20engineers%20can%0Avisually%20compare%20the%20salient%20features%20of%20test%20samples%20with%20training%20examples%0Athrough%20the%20use%20of%20a%20%60%60hover%20callback%27%27%20to%20understand%20model%20uncertainty%0Aperformance%20and%20decide%20follow%20up%20courses%20of%20action.%20We%20demonstrate%20the%0Aeffectiveness%20of%20ScatterUQ%20to%20explain%20model%20uncertainty%20for%20a%20multiclass%20image%0Aclassification%20on%20a%20distance-aware%20neural%20network%20trained%20on%20Fashion-MNIST%20and%0Atested%20on%20Fashion-MNIST%20%28in%20distribution%29%20and%20MNIST%20digits%20%28out%20of%0Adistribution%29%2C%20as%20well%20as%20a%20deep%20learning%20model%20for%20a%20cyber%20dataset.%20We%0Aquantitatively%20evaluate%20dimensionality%20reduction%20techniques%20to%20optimize%20our%0Acontextually%20driven%20UQ%20visualizations.%20Our%20results%20indicate%20that%20the%20ScatterUQ%0Asystem%20should%20scale%20to%20arbitrary%2C%20multiclass%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mit-ll-responsible-ai/equine-webapp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04588v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScatterUQ%253A%2520Interactive%2520Uncertainty%2520Visualizations%2520for%2520Multiclass%2520Deep%250A%2520%2520Learning%2520Problems%26entry.906535625%3DHarry%2520Li%2520and%2520Steven%2520Jorgensen%2520and%2520John%2520Holodnak%2520and%2520Allan%2520Wollaber%26entry.1292438233%3D%2520%2520Recently%252C%2520uncertainty-aware%2520deep%2520learning%2520methods%2520for%2520multiclass%2520labeling%250Aproblems%2520have%2520been%2520developed%2520that%2520provide%2520calibrated%2520class%2520prediction%250Aprobabilities%2520and%2520out-of-distribution%2520%2528OOD%2529%2520indicators%252C%2520letting%2520machine%250Alearning%2520%2528ML%2529%2520consumers%2520and%2520engineers%2520gauge%2520a%2520model%2527s%2520confidence%2520in%2520its%250Apredictions.%2520However%252C%2520this%2520extra%2520neural%2520network%2520prediction%2520information%2520is%250Achallenging%2520to%2520scalably%2520convey%2520visually%2520for%2520arbitrary%2520data%2520sources%2520under%250Amultiple%2520uncertainty%2520contexts.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%250AScatterUQ%252C%2520an%2520interactive%2520system%2520that%2520provides%2520targeted%2520visualizations%2520to%2520allow%250Ausers%2520to%2520better%2520understand%2520model%2520performance%2520in%2520context-driven%2520uncertainty%250Asettings.%2520ScatterUQ%2520leverages%2520recent%2520advances%2520in%2520distance-aware%2520neural%250Anetworks%252C%2520together%2520with%2520dimensionality%2520reduction%2520techniques%252C%2520to%2520construct%250Arobust%252C%25202-D%2520scatter%2520plots%2520explaining%2520why%2520a%2520model%2520predicts%2520a%2520test%2520example%2520to%2520be%250A%25281%2529%2520in-distribution%2520and%2520of%2520a%2520particular%2520class%252C%2520%25282%2529%2520in-distribution%2520but%2520unsure%250Aof%2520the%2520class%252C%2520and%2520%25283%2529%2520out-of-distribution.%2520ML%2520consumers%2520and%2520engineers%2520can%250Avisually%2520compare%2520the%2520salient%2520features%2520of%2520test%2520samples%2520with%2520training%2520examples%250Athrough%2520the%2520use%2520of%2520a%2520%2560%2560hover%2520callback%2527%2527%2520to%2520understand%2520model%2520uncertainty%250Aperformance%2520and%2520decide%2520follow%2520up%2520courses%2520of%2520action.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520ScatterUQ%2520to%2520explain%2520model%2520uncertainty%2520for%2520a%2520multiclass%2520image%250Aclassification%2520on%2520a%2520distance-aware%2520neural%2520network%2520trained%2520on%2520Fashion-MNIST%2520and%250Atested%2520on%2520Fashion-MNIST%2520%2528in%2520distribution%2529%2520and%2520MNIST%2520digits%2520%2528out%2520of%250Adistribution%2529%252C%2520as%2520well%2520as%2520a%2520deep%2520learning%2520model%2520for%2520a%2520cyber%2520dataset.%2520We%250Aquantitatively%2520evaluate%2520dimensionality%2520reduction%2520techniques%2520to%2520optimize%2520our%250Acontextually%2520driven%2520UQ%2520visualizations.%2520Our%2520results%2520indicate%2520that%2520the%2520ScatterUQ%250Asystem%2520should%2520scale%2520to%2520arbitrary%252C%2520multiclass%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/mit-ll-responsible-ai/equine-webapp%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04588v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScatterUQ%3A%20Interactive%20Uncertainty%20Visualizations%20for%20Multiclass%20Deep%0A%20%20Learning%20Problems&entry.906535625=Harry%20Li%20and%20Steven%20Jorgensen%20and%20John%20Holodnak%20and%20Allan%20Wollaber&entry.1292438233=%20%20Recently%2C%20uncertainty-aware%20deep%20learning%20methods%20for%20multiclass%20labeling%0Aproblems%20have%20been%20developed%20that%20provide%20calibrated%20class%20prediction%0Aprobabilities%20and%20out-of-distribution%20%28OOD%29%20indicators%2C%20letting%20machine%0Alearning%20%28ML%29%20consumers%20and%20engineers%20gauge%20a%20model%27s%20confidence%20in%20its%0Apredictions.%20However%2C%20this%20extra%20neural%20network%20prediction%20information%20is%0Achallenging%20to%20scalably%20convey%20visually%20for%20arbitrary%20data%20sources%20under%0Amultiple%20uncertainty%20contexts.%20To%20address%20these%20challenges%2C%20we%20present%0AScatterUQ%2C%20an%20interactive%20system%20that%20provides%20targeted%20visualizations%20to%20allow%0Ausers%20to%20better%20understand%20model%20performance%20in%20context-driven%20uncertainty%0Asettings.%20ScatterUQ%20leverages%20recent%20advances%20in%20distance-aware%20neural%0Anetworks%2C%20together%20with%20dimensionality%20reduction%20techniques%2C%20to%20construct%0Arobust%2C%202-D%20scatter%20plots%20explaining%20why%20a%20model%20predicts%20a%20test%20example%20to%20be%0A%281%29%20in-distribution%20and%20of%20a%20particular%20class%2C%20%282%29%20in-distribution%20but%20unsure%0Aof%20the%20class%2C%20and%20%283%29%20out-of-distribution.%20ML%20consumers%20and%20engineers%20can%0Avisually%20compare%20the%20salient%20features%20of%20test%20samples%20with%20training%20examples%0Athrough%20the%20use%20of%20a%20%60%60hover%20callback%27%27%20to%20understand%20model%20uncertainty%0Aperformance%20and%20decide%20follow%20up%20courses%20of%20action.%20We%20demonstrate%20the%0Aeffectiveness%20of%20ScatterUQ%20to%20explain%20model%20uncertainty%20for%20a%20multiclass%20image%0Aclassification%20on%20a%20distance-aware%20neural%20network%20trained%20on%20Fashion-MNIST%20and%0Atested%20on%20Fashion-MNIST%20%28in%20distribution%29%20and%20MNIST%20digits%20%28out%20of%0Adistribution%29%2C%20as%20well%20as%20a%20deep%20learning%20model%20for%20a%20cyber%20dataset.%20We%0Aquantitatively%20evaluate%20dimensionality%20reduction%20techniques%20to%20optimize%20our%0Acontextually%20driven%20UQ%20visualizations.%20Our%20results%20indicate%20that%20the%20ScatterUQ%0Asystem%20should%20scale%20to%20arbitrary%2C%20multiclass%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mit-ll-responsible-ai/equine-webapp%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04588v2&entry.124074799=Read"},
{"title": "Faster Linear Systems and Matrix Norm Approximation via Multi-level\n  Sketched Preconditioning", "author": "Micha\u0142 Derezi\u0144ski and Christopher Musco and Jiaming Yang", "abstract": "  We present a new class of preconditioned iterative methods for solving linear\nsystems of the form $Ax = b$. Our methods are based on constructing a low-rank\nNystr\\\"om approximation to $A$ using sparse random sketching. This\napproximation is used to construct a preconditioner, which itself is inverted\nquickly using additional levels of random sketching and preconditioning. We\nprove that the convergence of our methods depends on a natural average\ncondition number of $A$, which improves as the rank of the Nystr\\\"om\napproximation increases. Concretely, this allows us to obtain faster runtimes\nfor a number of fundamental linear algebraic problems:\n  1. We show how to solve any $n\\times n$ linear system that is\nwell-conditioned except for $k$ outlying large singular values in\n$\\tilde{O}(n^{2.065} + k^\\omega)$ time, improving on a recent result of\n[Derezi\\'nski, Yang, STOC 2024] for all $k \\gtrsim n^{0.78}$.\n  2. We give the first $\\tilde{O}(n^2 + {d_\\lambda}^{\\omega}$) time algorithm\nfor solving a regularized linear system $(A + \\lambda I)x = b$, where $A$ is\npositive semidefinite with effective dimension $d_\\lambda$. This problem arises\nin applications like Gaussian process regression.\n  3. We give faster algorithms for approximating Schatten $p$-norms and other\nmatrix norms. For example, for the Schatten 1 (nuclear) norm, we give an\nalgorithm that runs in $\\tilde{O}(n^{2.11})$ time, improving on an\n$\\tilde{O}(n^{2.18})$ method of [Musco et al., ITCS 2018].\n  Interestingly, previous state-of-the-art algorithms for most of the problems\nabove relied on stochastic iterative methods, like stochastic coordinate and\ngradient descent. Our work takes a completely different approach, instead\nleveraging tools from matrix sketching.\n", "link": "http://arxiv.org/abs/2405.05865v1", "date": "2024-05-09", "relevancy": 1.7502, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4422}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4367}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Linear%20Systems%20and%20Matrix%20Norm%20Approximation%20via%20Multi-level%0A%20%20Sketched%20Preconditioning&body=Title%3A%20Faster%20Linear%20Systems%20and%20Matrix%20Norm%20Approximation%20via%20Multi-level%0A%20%20Sketched%20Preconditioning%0AAuthor%3A%20Micha%C5%82%20Derezi%C5%84ski%20and%20Christopher%20Musco%20and%20Jiaming%20Yang%0AAbstract%3A%20%20%20We%20present%20a%20new%20class%20of%20preconditioned%20iterative%20methods%20for%20solving%20linear%0Asystems%20of%20the%20form%20%24Ax%20%3D%20b%24.%20Our%20methods%20are%20based%20on%20constructing%20a%20low-rank%0ANystr%5C%22om%20approximation%20to%20%24A%24%20using%20sparse%20random%20sketching.%20This%0Aapproximation%20is%20used%20to%20construct%20a%20preconditioner%2C%20which%20itself%20is%20inverted%0Aquickly%20using%20additional%20levels%20of%20random%20sketching%20and%20preconditioning.%20We%0Aprove%20that%20the%20convergence%20of%20our%20methods%20depends%20on%20a%20natural%20average%0Acondition%20number%20of%20%24A%24%2C%20which%20improves%20as%20the%20rank%20of%20the%20Nystr%5C%22om%0Aapproximation%20increases.%20Concretely%2C%20this%20allows%20us%20to%20obtain%20faster%20runtimes%0Afor%20a%20number%20of%20fundamental%20linear%20algebraic%20problems%3A%0A%20%201.%20We%20show%20how%20to%20solve%20any%20%24n%5Ctimes%20n%24%20linear%20system%20that%20is%0Awell-conditioned%20except%20for%20%24k%24%20outlying%20large%20singular%20values%20in%0A%24%5Ctilde%7BO%7D%28n%5E%7B2.065%7D%20%2B%20k%5E%5Comega%29%24%20time%2C%20improving%20on%20a%20recent%20result%20of%0A%5BDerezi%5C%27nski%2C%20Yang%2C%20STOC%202024%5D%20for%20all%20%24k%20%5Cgtrsim%20n%5E%7B0.78%7D%24.%0A%20%202.%20We%20give%20the%20first%20%24%5Ctilde%7BO%7D%28n%5E2%20%2B%20%7Bd_%5Clambda%7D%5E%7B%5Comega%7D%24%29%20time%20algorithm%0Afor%20solving%20a%20regularized%20linear%20system%20%24%28A%20%2B%20%5Clambda%20I%29x%20%3D%20b%24%2C%20where%20%24A%24%20is%0Apositive%20semidefinite%20with%20effective%20dimension%20%24d_%5Clambda%24.%20This%20problem%20arises%0Ain%20applications%20like%20Gaussian%20process%20regression.%0A%20%203.%20We%20give%20faster%20algorithms%20for%20approximating%20Schatten%20%24p%24-norms%20and%20other%0Amatrix%20norms.%20For%20example%2C%20for%20the%20Schatten%201%20%28nuclear%29%20norm%2C%20we%20give%20an%0Aalgorithm%20that%20runs%20in%20%24%5Ctilde%7BO%7D%28n%5E%7B2.11%7D%29%24%20time%2C%20improving%20on%20an%0A%24%5Ctilde%7BO%7D%28n%5E%7B2.18%7D%29%24%20method%20of%20%5BMusco%20et%20al.%2C%20ITCS%202018%5D.%0A%20%20Interestingly%2C%20previous%20state-of-the-art%20algorithms%20for%20most%20of%20the%20problems%0Aabove%20relied%20on%20stochastic%20iterative%20methods%2C%20like%20stochastic%20coordinate%20and%0Agradient%20descent.%20Our%20work%20takes%20a%20completely%20different%20approach%2C%20instead%0Aleveraging%20tools%20from%20matrix%20sketching.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Linear%2520Systems%2520and%2520Matrix%2520Norm%2520Approximation%2520via%2520Multi-level%250A%2520%2520Sketched%2520Preconditioning%26entry.906535625%3DMicha%25C5%2582%2520Derezi%25C5%2584ski%2520and%2520Christopher%2520Musco%2520and%2520Jiaming%2520Yang%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520class%2520of%2520preconditioned%2520iterative%2520methods%2520for%2520solving%2520linear%250Asystems%2520of%2520the%2520form%2520%2524Ax%2520%253D%2520b%2524.%2520Our%2520methods%2520are%2520based%2520on%2520constructing%2520a%2520low-rank%250ANystr%255C%2522om%2520approximation%2520to%2520%2524A%2524%2520using%2520sparse%2520random%2520sketching.%2520This%250Aapproximation%2520is%2520used%2520to%2520construct%2520a%2520preconditioner%252C%2520which%2520itself%2520is%2520inverted%250Aquickly%2520using%2520additional%2520levels%2520of%2520random%2520sketching%2520and%2520preconditioning.%2520We%250Aprove%2520that%2520the%2520convergence%2520of%2520our%2520methods%2520depends%2520on%2520a%2520natural%2520average%250Acondition%2520number%2520of%2520%2524A%2524%252C%2520which%2520improves%2520as%2520the%2520rank%2520of%2520the%2520Nystr%255C%2522om%250Aapproximation%2520increases.%2520Concretely%252C%2520this%2520allows%2520us%2520to%2520obtain%2520faster%2520runtimes%250Afor%2520a%2520number%2520of%2520fundamental%2520linear%2520algebraic%2520problems%253A%250A%2520%25201.%2520We%2520show%2520how%2520to%2520solve%2520any%2520%2524n%255Ctimes%2520n%2524%2520linear%2520system%2520that%2520is%250Awell-conditioned%2520except%2520for%2520%2524k%2524%2520outlying%2520large%2520singular%2520values%2520in%250A%2524%255Ctilde%257BO%257D%2528n%255E%257B2.065%257D%2520%252B%2520k%255E%255Comega%2529%2524%2520time%252C%2520improving%2520on%2520a%2520recent%2520result%2520of%250A%255BDerezi%255C%2527nski%252C%2520Yang%252C%2520STOC%25202024%255D%2520for%2520all%2520%2524k%2520%255Cgtrsim%2520n%255E%257B0.78%257D%2524.%250A%2520%25202.%2520We%2520give%2520the%2520first%2520%2524%255Ctilde%257BO%257D%2528n%255E2%2520%252B%2520%257Bd_%255Clambda%257D%255E%257B%255Comega%257D%2524%2529%2520time%2520algorithm%250Afor%2520solving%2520a%2520regularized%2520linear%2520system%2520%2524%2528A%2520%252B%2520%255Clambda%2520I%2529x%2520%253D%2520b%2524%252C%2520where%2520%2524A%2524%2520is%250Apositive%2520semidefinite%2520with%2520effective%2520dimension%2520%2524d_%255Clambda%2524.%2520This%2520problem%2520arises%250Ain%2520applications%2520like%2520Gaussian%2520process%2520regression.%250A%2520%25203.%2520We%2520give%2520faster%2520algorithms%2520for%2520approximating%2520Schatten%2520%2524p%2524-norms%2520and%2520other%250Amatrix%2520norms.%2520For%2520example%252C%2520for%2520the%2520Schatten%25201%2520%2528nuclear%2529%2520norm%252C%2520we%2520give%2520an%250Aalgorithm%2520that%2520runs%2520in%2520%2524%255Ctilde%257BO%257D%2528n%255E%257B2.11%257D%2529%2524%2520time%252C%2520improving%2520on%2520an%250A%2524%255Ctilde%257BO%257D%2528n%255E%257B2.18%257D%2529%2524%2520method%2520of%2520%255BMusco%2520et%2520al.%252C%2520ITCS%25202018%255D.%250A%2520%2520Interestingly%252C%2520previous%2520state-of-the-art%2520algorithms%2520for%2520most%2520of%2520the%2520problems%250Aabove%2520relied%2520on%2520stochastic%2520iterative%2520methods%252C%2520like%2520stochastic%2520coordinate%2520and%250Agradient%2520descent.%2520Our%2520work%2520takes%2520a%2520completely%2520different%2520approach%252C%2520instead%250Aleveraging%2520tools%2520from%2520matrix%2520sketching.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Linear%20Systems%20and%20Matrix%20Norm%20Approximation%20via%20Multi-level%0A%20%20Sketched%20Preconditioning&entry.906535625=Micha%C5%82%20Derezi%C5%84ski%20and%20Christopher%20Musco%20and%20Jiaming%20Yang&entry.1292438233=%20%20We%20present%20a%20new%20class%20of%20preconditioned%20iterative%20methods%20for%20solving%20linear%0Asystems%20of%20the%20form%20%24Ax%20%3D%20b%24.%20Our%20methods%20are%20based%20on%20constructing%20a%20low-rank%0ANystr%5C%22om%20approximation%20to%20%24A%24%20using%20sparse%20random%20sketching.%20This%0Aapproximation%20is%20used%20to%20construct%20a%20preconditioner%2C%20which%20itself%20is%20inverted%0Aquickly%20using%20additional%20levels%20of%20random%20sketching%20and%20preconditioning.%20We%0Aprove%20that%20the%20convergence%20of%20our%20methods%20depends%20on%20a%20natural%20average%0Acondition%20number%20of%20%24A%24%2C%20which%20improves%20as%20the%20rank%20of%20the%20Nystr%5C%22om%0Aapproximation%20increases.%20Concretely%2C%20this%20allows%20us%20to%20obtain%20faster%20runtimes%0Afor%20a%20number%20of%20fundamental%20linear%20algebraic%20problems%3A%0A%20%201.%20We%20show%20how%20to%20solve%20any%20%24n%5Ctimes%20n%24%20linear%20system%20that%20is%0Awell-conditioned%20except%20for%20%24k%24%20outlying%20large%20singular%20values%20in%0A%24%5Ctilde%7BO%7D%28n%5E%7B2.065%7D%20%2B%20k%5E%5Comega%29%24%20time%2C%20improving%20on%20a%20recent%20result%20of%0A%5BDerezi%5C%27nski%2C%20Yang%2C%20STOC%202024%5D%20for%20all%20%24k%20%5Cgtrsim%20n%5E%7B0.78%7D%24.%0A%20%202.%20We%20give%20the%20first%20%24%5Ctilde%7BO%7D%28n%5E2%20%2B%20%7Bd_%5Clambda%7D%5E%7B%5Comega%7D%24%29%20time%20algorithm%0Afor%20solving%20a%20regularized%20linear%20system%20%24%28A%20%2B%20%5Clambda%20I%29x%20%3D%20b%24%2C%20where%20%24A%24%20is%0Apositive%20semidefinite%20with%20effective%20dimension%20%24d_%5Clambda%24.%20This%20problem%20arises%0Ain%20applications%20like%20Gaussian%20process%20regression.%0A%20%203.%20We%20give%20faster%20algorithms%20for%20approximating%20Schatten%20%24p%24-norms%20and%20other%0Amatrix%20norms.%20For%20example%2C%20for%20the%20Schatten%201%20%28nuclear%29%20norm%2C%20we%20give%20an%0Aalgorithm%20that%20runs%20in%20%24%5Ctilde%7BO%7D%28n%5E%7B2.11%7D%29%24%20time%2C%20improving%20on%20an%0A%24%5Ctilde%7BO%7D%28n%5E%7B2.18%7D%29%24%20method%20of%20%5BMusco%20et%20al.%2C%20ITCS%202018%5D.%0A%20%20Interestingly%2C%20previous%20state-of-the-art%20algorithms%20for%20most%20of%20the%20problems%0Aabove%20relied%20on%20stochastic%20iterative%20methods%2C%20like%20stochastic%20coordinate%20and%0Agradient%20descent.%20Our%20work%20takes%20a%20completely%20different%20approach%2C%20instead%0Aleveraging%20tools%20from%20matrix%20sketching.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05865v1&entry.124074799=Read"},
{"title": "Fine-grained Analysis and Faster Algorithms for Iteratively Solving\n  Linear Systems", "author": "Micha\u0142 Derezi\u0144ski and Daniel LeJeune and Deanna Needell and Elizaveta Rebrova", "abstract": "  While effective in practice, iterative methods for solving large systems of\nlinear equations can be significantly affected by problem-dependent condition\nnumber quantities. This makes characterizing their time complexity challenging,\nparticularly when we wish to make comparisons between deterministic and\nstochastic methods, that may or may not rely on preconditioning and/or fast\nmatrix multiplication. In this work, we consider a fine-grained notion of\ncomplexity for iterative linear solvers which we call the spectral tail\ncondition number, $\\kappa_\\ell$, defined as the ratio between the $\\ell$th\nlargest and the smallest singular value of the matrix representing the system.\n  Concretely, we prove the following main algorithmic result: Given an $n\\times\nn$ matrix $A$ and a vector $b$, we can find $\\tilde{x}$ such that\n$\\|A\\tilde{x}-b\\|\\leq\\epsilon\\|b\\|$ in time $\\tilde{O}(\\kappa_\\ell\\cdot n^2\\log\n1/\\epsilon)$ for any $\\ell = O(n^{\\frac1{\\omega-1}})=O(n^{0.729})$, where\n$\\omega \\approx 2.372$ is the current fast matrix multiplication exponent. This\nguarantee is achieved by Sketch-and-Project with Nesterov's acceleration. Some\nof the implications of our result, and of the use of $\\kappa_\\ell$, include\ndirect improvement over a fine-grained analysis of the Conjugate Gradient\nmethod, suggesting a stronger separation between deterministic and stochastic\niterative solvers; and relating the complexity of iterative solvers to the\nongoing algorithmic advances in fast matrix multiplication, since the bound on\n$\\ell$ improves with $\\omega$.\n  Our main technical contributions are new sharp characterizations for the\nfirst and second moments of the random projection matrix that commonly arises\nin sketching algorithms, building on a combination of techniques from\ncombinatorial sampling via determinantal point processes and Gaussian\nuniversality results from random matrix theory.\n", "link": "http://arxiv.org/abs/2405.05818v1", "date": "2024-05-09", "relevancy": 1.7388, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4424}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4394}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-grained%20Analysis%20and%20Faster%20Algorithms%20for%20Iteratively%20Solving%0A%20%20Linear%20Systems&body=Title%3A%20Fine-grained%20Analysis%20and%20Faster%20Algorithms%20for%20Iteratively%20Solving%0A%20%20Linear%20Systems%0AAuthor%3A%20Micha%C5%82%20Derezi%C5%84ski%20and%20Daniel%20LeJeune%20and%20Deanna%20Needell%20and%20Elizaveta%20Rebrova%0AAbstract%3A%20%20%20While%20effective%20in%20practice%2C%20iterative%20methods%20for%20solving%20large%20systems%20of%0Alinear%20equations%20can%20be%20significantly%20affected%20by%20problem-dependent%20condition%0Anumber%20quantities.%20This%20makes%20characterizing%20their%20time%20complexity%20challenging%2C%0Aparticularly%20when%20we%20wish%20to%20make%20comparisons%20between%20deterministic%20and%0Astochastic%20methods%2C%20that%20may%20or%20may%20not%20rely%20on%20preconditioning%20and/or%20fast%0Amatrix%20multiplication.%20In%20this%20work%2C%20we%20consider%20a%20fine-grained%20notion%20of%0Acomplexity%20for%20iterative%20linear%20solvers%20which%20we%20call%20the%20spectral%20tail%0Acondition%20number%2C%20%24%5Ckappa_%5Cell%24%2C%20defined%20as%20the%20ratio%20between%20the%20%24%5Cell%24th%0Alargest%20and%20the%20smallest%20singular%20value%20of%20the%20matrix%20representing%20the%20system.%0A%20%20Concretely%2C%20we%20prove%20the%20following%20main%20algorithmic%20result%3A%20Given%20an%20%24n%5Ctimes%0An%24%20matrix%20%24A%24%20and%20a%20vector%20%24b%24%2C%20we%20can%20find%20%24%5Ctilde%7Bx%7D%24%20such%20that%0A%24%5C%7CA%5Ctilde%7Bx%7D-b%5C%7C%5Cleq%5Cepsilon%5C%7Cb%5C%7C%24%20in%20time%20%24%5Ctilde%7BO%7D%28%5Ckappa_%5Cell%5Ccdot%20n%5E2%5Clog%0A1/%5Cepsilon%29%24%20for%20any%20%24%5Cell%20%3D%20O%28n%5E%7B%5Cfrac1%7B%5Comega-1%7D%7D%29%3DO%28n%5E%7B0.729%7D%29%24%2C%20where%0A%24%5Comega%20%5Capprox%202.372%24%20is%20the%20current%20fast%20matrix%20multiplication%20exponent.%20This%0Aguarantee%20is%20achieved%20by%20Sketch-and-Project%20with%20Nesterov%27s%20acceleration.%20Some%0Aof%20the%20implications%20of%20our%20result%2C%20and%20of%20the%20use%20of%20%24%5Ckappa_%5Cell%24%2C%20include%0Adirect%20improvement%20over%20a%20fine-grained%20analysis%20of%20the%20Conjugate%20Gradient%0Amethod%2C%20suggesting%20a%20stronger%20separation%20between%20deterministic%20and%20stochastic%0Aiterative%20solvers%3B%20and%20relating%20the%20complexity%20of%20iterative%20solvers%20to%20the%0Aongoing%20algorithmic%20advances%20in%20fast%20matrix%20multiplication%2C%20since%20the%20bound%20on%0A%24%5Cell%24%20improves%20with%20%24%5Comega%24.%0A%20%20Our%20main%20technical%20contributions%20are%20new%20sharp%20characterizations%20for%20the%0Afirst%20and%20second%20moments%20of%20the%20random%20projection%20matrix%20that%20commonly%20arises%0Ain%20sketching%20algorithms%2C%20building%20on%20a%20combination%20of%20techniques%20from%0Acombinatorial%20sampling%20via%20determinantal%20point%20processes%20and%20Gaussian%0Auniversality%20results%20from%20random%20matrix%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-grained%2520Analysis%2520and%2520Faster%2520Algorithms%2520for%2520Iteratively%2520Solving%250A%2520%2520Linear%2520Systems%26entry.906535625%3DMicha%25C5%2582%2520Derezi%25C5%2584ski%2520and%2520Daniel%2520LeJeune%2520and%2520Deanna%2520Needell%2520and%2520Elizaveta%2520Rebrova%26entry.1292438233%3D%2520%2520While%2520effective%2520in%2520practice%252C%2520iterative%2520methods%2520for%2520solving%2520large%2520systems%2520of%250Alinear%2520equations%2520can%2520be%2520significantly%2520affected%2520by%2520problem-dependent%2520condition%250Anumber%2520quantities.%2520This%2520makes%2520characterizing%2520their%2520time%2520complexity%2520challenging%252C%250Aparticularly%2520when%2520we%2520wish%2520to%2520make%2520comparisons%2520between%2520deterministic%2520and%250Astochastic%2520methods%252C%2520that%2520may%2520or%2520may%2520not%2520rely%2520on%2520preconditioning%2520and/or%2520fast%250Amatrix%2520multiplication.%2520In%2520this%2520work%252C%2520we%2520consider%2520a%2520fine-grained%2520notion%2520of%250Acomplexity%2520for%2520iterative%2520linear%2520solvers%2520which%2520we%2520call%2520the%2520spectral%2520tail%250Acondition%2520number%252C%2520%2524%255Ckappa_%255Cell%2524%252C%2520defined%2520as%2520the%2520ratio%2520between%2520the%2520%2524%255Cell%2524th%250Alargest%2520and%2520the%2520smallest%2520singular%2520value%2520of%2520the%2520matrix%2520representing%2520the%2520system.%250A%2520%2520Concretely%252C%2520we%2520prove%2520the%2520following%2520main%2520algorithmic%2520result%253A%2520Given%2520an%2520%2524n%255Ctimes%250An%2524%2520matrix%2520%2524A%2524%2520and%2520a%2520vector%2520%2524b%2524%252C%2520we%2520can%2520find%2520%2524%255Ctilde%257Bx%257D%2524%2520such%2520that%250A%2524%255C%257CA%255Ctilde%257Bx%257D-b%255C%257C%255Cleq%255Cepsilon%255C%257Cb%255C%257C%2524%2520in%2520time%2520%2524%255Ctilde%257BO%257D%2528%255Ckappa_%255Cell%255Ccdot%2520n%255E2%255Clog%250A1/%255Cepsilon%2529%2524%2520for%2520any%2520%2524%255Cell%2520%253D%2520O%2528n%255E%257B%255Cfrac1%257B%255Comega-1%257D%257D%2529%253DO%2528n%255E%257B0.729%257D%2529%2524%252C%2520where%250A%2524%255Comega%2520%255Capprox%25202.372%2524%2520is%2520the%2520current%2520fast%2520matrix%2520multiplication%2520exponent.%2520This%250Aguarantee%2520is%2520achieved%2520by%2520Sketch-and-Project%2520with%2520Nesterov%2527s%2520acceleration.%2520Some%250Aof%2520the%2520implications%2520of%2520our%2520result%252C%2520and%2520of%2520the%2520use%2520of%2520%2524%255Ckappa_%255Cell%2524%252C%2520include%250Adirect%2520improvement%2520over%2520a%2520fine-grained%2520analysis%2520of%2520the%2520Conjugate%2520Gradient%250Amethod%252C%2520suggesting%2520a%2520stronger%2520separation%2520between%2520deterministic%2520and%2520stochastic%250Aiterative%2520solvers%253B%2520and%2520relating%2520the%2520complexity%2520of%2520iterative%2520solvers%2520to%2520the%250Aongoing%2520algorithmic%2520advances%2520in%2520fast%2520matrix%2520multiplication%252C%2520since%2520the%2520bound%2520on%250A%2524%255Cell%2524%2520improves%2520with%2520%2524%255Comega%2524.%250A%2520%2520Our%2520main%2520technical%2520contributions%2520are%2520new%2520sharp%2520characterizations%2520for%2520the%250Afirst%2520and%2520second%2520moments%2520of%2520the%2520random%2520projection%2520matrix%2520that%2520commonly%2520arises%250Ain%2520sketching%2520algorithms%252C%2520building%2520on%2520a%2520combination%2520of%2520techniques%2520from%250Acombinatorial%2520sampling%2520via%2520determinantal%2520point%2520processes%2520and%2520Gaussian%250Auniversality%2520results%2520from%2520random%2520matrix%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-grained%20Analysis%20and%20Faster%20Algorithms%20for%20Iteratively%20Solving%0A%20%20Linear%20Systems&entry.906535625=Micha%C5%82%20Derezi%C5%84ski%20and%20Daniel%20LeJeune%20and%20Deanna%20Needell%20and%20Elizaveta%20Rebrova&entry.1292438233=%20%20While%20effective%20in%20practice%2C%20iterative%20methods%20for%20solving%20large%20systems%20of%0Alinear%20equations%20can%20be%20significantly%20affected%20by%20problem-dependent%20condition%0Anumber%20quantities.%20This%20makes%20characterizing%20their%20time%20complexity%20challenging%2C%0Aparticularly%20when%20we%20wish%20to%20make%20comparisons%20between%20deterministic%20and%0Astochastic%20methods%2C%20that%20may%20or%20may%20not%20rely%20on%20preconditioning%20and/or%20fast%0Amatrix%20multiplication.%20In%20this%20work%2C%20we%20consider%20a%20fine-grained%20notion%20of%0Acomplexity%20for%20iterative%20linear%20solvers%20which%20we%20call%20the%20spectral%20tail%0Acondition%20number%2C%20%24%5Ckappa_%5Cell%24%2C%20defined%20as%20the%20ratio%20between%20the%20%24%5Cell%24th%0Alargest%20and%20the%20smallest%20singular%20value%20of%20the%20matrix%20representing%20the%20system.%0A%20%20Concretely%2C%20we%20prove%20the%20following%20main%20algorithmic%20result%3A%20Given%20an%20%24n%5Ctimes%0An%24%20matrix%20%24A%24%20and%20a%20vector%20%24b%24%2C%20we%20can%20find%20%24%5Ctilde%7Bx%7D%24%20such%20that%0A%24%5C%7CA%5Ctilde%7Bx%7D-b%5C%7C%5Cleq%5Cepsilon%5C%7Cb%5C%7C%24%20in%20time%20%24%5Ctilde%7BO%7D%28%5Ckappa_%5Cell%5Ccdot%20n%5E2%5Clog%0A1/%5Cepsilon%29%24%20for%20any%20%24%5Cell%20%3D%20O%28n%5E%7B%5Cfrac1%7B%5Comega-1%7D%7D%29%3DO%28n%5E%7B0.729%7D%29%24%2C%20where%0A%24%5Comega%20%5Capprox%202.372%24%20is%20the%20current%20fast%20matrix%20multiplication%20exponent.%20This%0Aguarantee%20is%20achieved%20by%20Sketch-and-Project%20with%20Nesterov%27s%20acceleration.%20Some%0Aof%20the%20implications%20of%20our%20result%2C%20and%20of%20the%20use%20of%20%24%5Ckappa_%5Cell%24%2C%20include%0Adirect%20improvement%20over%20a%20fine-grained%20analysis%20of%20the%20Conjugate%20Gradient%0Amethod%2C%20suggesting%20a%20stronger%20separation%20between%20deterministic%20and%20stochastic%0Aiterative%20solvers%3B%20and%20relating%20the%20complexity%20of%20iterative%20solvers%20to%20the%0Aongoing%20algorithmic%20advances%20in%20fast%20matrix%20multiplication%2C%20since%20the%20bound%20on%0A%24%5Cell%24%20improves%20with%20%24%5Comega%24.%0A%20%20Our%20main%20technical%20contributions%20are%20new%20sharp%20characterizations%20for%20the%0Afirst%20and%20second%20moments%20of%20the%20random%20projection%20matrix%20that%20commonly%20arises%0Ain%20sketching%20algorithms%2C%20building%20on%20a%20combination%20of%20techniques%20from%0Acombinatorial%20sampling%20via%20determinantal%20point%20processes%20and%20Gaussian%0Auniversality%20results%20from%20random%20matrix%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05818v1&entry.124074799=Read"},
{"title": "VM-DDPM: Vision Mamba Diffusion for Medical Image Synthesis", "author": "Zhihan Ju and Wanting Zhou", "abstract": "  In the realm of smart healthcare, researchers enhance the scale and diversity\nof medical datasets through medical image synthesis. However, existing methods\nare limited by CNN local perception and Transformer quadratic complexity,\nmaking it difficult to balance structural texture consistency. To this end, we\npropose the Vision Mamba DDPM (VM-DDPM) based on State Space Model (SSM), fully\ncombining CNN local perception and SSM global modeling capabilities, while\nmaintaining linear computational complexity. Specifically, we designed a\nmulti-level feature extraction module called Multi-level State Space Block\n(MSSBlock), and a basic unit of encoder-decoder structure called State Space\nLayer (SSLayer) for medical pathological images. Besides, we designed a simple,\nPlug-and-Play, zero-parameter Sequence Regeneration strategy for the Cross-Scan\nModule (CSM), which enabled the S6 module to fully perceive the spatial\nfeatures of the 2D image and stimulate the generalization potential of the\nmodel. To our best knowledge, this is the first medical image synthesis model\nbased on the SSM-CNN hybrid architecture. Our experimental evaluation on three\ndatasets of different scales, i.e., ACDC, BraTS2018, and ChestXRay, as well as\nqualitative evaluation by radiologists, demonstrate that VM-DDPM achieves\nstate-of-the-art performance.\n", "link": "http://arxiv.org/abs/2405.05667v1", "date": "2024-05-09", "relevancy": 1.7052, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6196}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VM-DDPM%3A%20Vision%20Mamba%20Diffusion%20for%20Medical%20Image%20Synthesis&body=Title%3A%20VM-DDPM%3A%20Vision%20Mamba%20Diffusion%20for%20Medical%20Image%20Synthesis%0AAuthor%3A%20Zhihan%20Ju%20and%20Wanting%20Zhou%0AAbstract%3A%20%20%20In%20the%20realm%20of%20smart%20healthcare%2C%20researchers%20enhance%20the%20scale%20and%20diversity%0Aof%20medical%20datasets%20through%20medical%20image%20synthesis.%20However%2C%20existing%20methods%0Aare%20limited%20by%20CNN%20local%20perception%20and%20Transformer%20quadratic%20complexity%2C%0Amaking%20it%20difficult%20to%20balance%20structural%20texture%20consistency.%20To%20this%20end%2C%20we%0Apropose%20the%20Vision%20Mamba%20DDPM%20%28VM-DDPM%29%20based%20on%20State%20Space%20Model%20%28SSM%29%2C%20fully%0Acombining%20CNN%20local%20perception%20and%20SSM%20global%20modeling%20capabilities%2C%20while%0Amaintaining%20linear%20computational%20complexity.%20Specifically%2C%20we%20designed%20a%0Amulti-level%20feature%20extraction%20module%20called%20Multi-level%20State%20Space%20Block%0A%28MSSBlock%29%2C%20and%20a%20basic%20unit%20of%20encoder-decoder%20structure%20called%20State%20Space%0ALayer%20%28SSLayer%29%20for%20medical%20pathological%20images.%20Besides%2C%20we%20designed%20a%20simple%2C%0APlug-and-Play%2C%20zero-parameter%20Sequence%20Regeneration%20strategy%20for%20the%20Cross-Scan%0AModule%20%28CSM%29%2C%20which%20enabled%20the%20S6%20module%20to%20fully%20perceive%20the%20spatial%0Afeatures%20of%20the%202D%20image%20and%20stimulate%20the%20generalization%20potential%20of%20the%0Amodel.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20medical%20image%20synthesis%20model%0Abased%20on%20the%20SSM-CNN%20hybrid%20architecture.%20Our%20experimental%20evaluation%20on%20three%0Adatasets%20of%20different%20scales%2C%20i.e.%2C%20ACDC%2C%20BraTS2018%2C%20and%20ChestXRay%2C%20as%20well%20as%0Aqualitative%20evaluation%20by%20radiologists%2C%20demonstrate%20that%20VM-DDPM%20achieves%0Astate-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVM-DDPM%253A%2520Vision%2520Mamba%2520Diffusion%2520for%2520Medical%2520Image%2520Synthesis%26entry.906535625%3DZhihan%2520Ju%2520and%2520Wanting%2520Zhou%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520smart%2520healthcare%252C%2520researchers%2520enhance%2520the%2520scale%2520and%2520diversity%250Aof%2520medical%2520datasets%2520through%2520medical%2520image%2520synthesis.%2520However%252C%2520existing%2520methods%250Aare%2520limited%2520by%2520CNN%2520local%2520perception%2520and%2520Transformer%2520quadratic%2520complexity%252C%250Amaking%2520it%2520difficult%2520to%2520balance%2520structural%2520texture%2520consistency.%2520To%2520this%2520end%252C%2520we%250Apropose%2520the%2520Vision%2520Mamba%2520DDPM%2520%2528VM-DDPM%2529%2520based%2520on%2520State%2520Space%2520Model%2520%2528SSM%2529%252C%2520fully%250Acombining%2520CNN%2520local%2520perception%2520and%2520SSM%2520global%2520modeling%2520capabilities%252C%2520while%250Amaintaining%2520linear%2520computational%2520complexity.%2520Specifically%252C%2520we%2520designed%2520a%250Amulti-level%2520feature%2520extraction%2520module%2520called%2520Multi-level%2520State%2520Space%2520Block%250A%2528MSSBlock%2529%252C%2520and%2520a%2520basic%2520unit%2520of%2520encoder-decoder%2520structure%2520called%2520State%2520Space%250ALayer%2520%2528SSLayer%2529%2520for%2520medical%2520pathological%2520images.%2520Besides%252C%2520we%2520designed%2520a%2520simple%252C%250APlug-and-Play%252C%2520zero-parameter%2520Sequence%2520Regeneration%2520strategy%2520for%2520the%2520Cross-Scan%250AModule%2520%2528CSM%2529%252C%2520which%2520enabled%2520the%2520S6%2520module%2520to%2520fully%2520perceive%2520the%2520spatial%250Afeatures%2520of%2520the%25202D%2520image%2520and%2520stimulate%2520the%2520generalization%2520potential%2520of%2520the%250Amodel.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520is%2520the%2520first%2520medical%2520image%2520synthesis%2520model%250Abased%2520on%2520the%2520SSM-CNN%2520hybrid%2520architecture.%2520Our%2520experimental%2520evaluation%2520on%2520three%250Adatasets%2520of%2520different%2520scales%252C%2520i.e.%252C%2520ACDC%252C%2520BraTS2018%252C%2520and%2520ChestXRay%252C%2520as%2520well%2520as%250Aqualitative%2520evaluation%2520by%2520radiologists%252C%2520demonstrate%2520that%2520VM-DDPM%2520achieves%250Astate-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VM-DDPM%3A%20Vision%20Mamba%20Diffusion%20for%20Medical%20Image%20Synthesis&entry.906535625=Zhihan%20Ju%20and%20Wanting%20Zhou&entry.1292438233=%20%20In%20the%20realm%20of%20smart%20healthcare%2C%20researchers%20enhance%20the%20scale%20and%20diversity%0Aof%20medical%20datasets%20through%20medical%20image%20synthesis.%20However%2C%20existing%20methods%0Aare%20limited%20by%20CNN%20local%20perception%20and%20Transformer%20quadratic%20complexity%2C%0Amaking%20it%20difficult%20to%20balance%20structural%20texture%20consistency.%20To%20this%20end%2C%20we%0Apropose%20the%20Vision%20Mamba%20DDPM%20%28VM-DDPM%29%20based%20on%20State%20Space%20Model%20%28SSM%29%2C%20fully%0Acombining%20CNN%20local%20perception%20and%20SSM%20global%20modeling%20capabilities%2C%20while%0Amaintaining%20linear%20computational%20complexity.%20Specifically%2C%20we%20designed%20a%0Amulti-level%20feature%20extraction%20module%20called%20Multi-level%20State%20Space%20Block%0A%28MSSBlock%29%2C%20and%20a%20basic%20unit%20of%20encoder-decoder%20structure%20called%20State%20Space%0ALayer%20%28SSLayer%29%20for%20medical%20pathological%20images.%20Besides%2C%20we%20designed%20a%20simple%2C%0APlug-and-Play%2C%20zero-parameter%20Sequence%20Regeneration%20strategy%20for%20the%20Cross-Scan%0AModule%20%28CSM%29%2C%20which%20enabled%20the%20S6%20module%20to%20fully%20perceive%20the%20spatial%0Afeatures%20of%20the%202D%20image%20and%20stimulate%20the%20generalization%20potential%20of%20the%0Amodel.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20medical%20image%20synthesis%20model%0Abased%20on%20the%20SSM-CNN%20hybrid%20architecture.%20Our%20experimental%20evaluation%20on%20three%0Adatasets%20of%20different%20scales%2C%20i.e.%2C%20ACDC%2C%20BraTS2018%2C%20and%20ChestXRay%2C%20as%20well%20as%0Aqualitative%20evaluation%20by%20radiologists%2C%20demonstrate%20that%20VM-DDPM%20achieves%0Astate-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05667v1&entry.124074799=Read"},
{"title": "DP-MDM: Detail-Preserving MR Reconstruction via Multiple Diffusion\n  Models", "author": "Mengxiao Geng and Jiahao Zhu and Xiaolin Zhu and Qiqing Liu and Dong Liang and Qiegen Liu", "abstract": "  Detail features of magnetic resonance images play a cru-cial role in accurate\nmedical diagnosis and treatment, as they capture subtle changes that pose\nchallenges for doc-tors when performing precise judgments. However, the widely\nutilized naive diffusion model has limitations, as it fails to accurately\ncapture more intricate details. To en-hance the quality of MRI reconstruction,\nwe propose a comprehensive detail-preserving reconstruction method using\nmultiple diffusion models to extract structure and detail features in k-space\ndomain instead of image do-main. Moreover, virtual binary modal masks are\nutilized to refine the range of values in k-space data through highly adaptive\ncenter windows, which allows the model to focus its attention more efficiently.\nLast but not least, an inverted pyramid structure is employed, where the\ntop-down image information gradually decreases, ena-bling a cascade\nrepresentation. The framework effective-ly represents multi-scale sampled data,\ntaking into ac-count the sparsity of the inverted pyramid architecture, and\nutilizes cascade training data distribution to repre-sent multi-scale data.\nThrough a step-by-step refinement approach, the method refines the\napproximation of de-tails. Finally, the proposed method was evaluated by\ncon-ducting experiments on clinical and public datasets. The results\ndemonstrate that the proposed method outper-forms other methods.\n", "link": "http://arxiv.org/abs/2405.05763v1", "date": "2024-05-09", "relevancy": 1.6491, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6086}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5342}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-MDM%3A%20Detail-Preserving%20MR%20Reconstruction%20via%20Multiple%20Diffusion%0A%20%20Models&body=Title%3A%20DP-MDM%3A%20Detail-Preserving%20MR%20Reconstruction%20via%20Multiple%20Diffusion%0A%20%20Models%0AAuthor%3A%20Mengxiao%20Geng%20and%20Jiahao%20Zhu%20and%20Xiaolin%20Zhu%20and%20Qiqing%20Liu%20and%20Dong%20Liang%20and%20Qiegen%20Liu%0AAbstract%3A%20%20%20Detail%20features%20of%20magnetic%20resonance%20images%20play%20a%20cru-cial%20role%20in%20accurate%0Amedical%20diagnosis%20and%20treatment%2C%20as%20they%20capture%20subtle%20changes%20that%20pose%0Achallenges%20for%20doc-tors%20when%20performing%20precise%20judgments.%20However%2C%20the%20widely%0Autilized%20naive%20diffusion%20model%20has%20limitations%2C%20as%20it%20fails%20to%20accurately%0Acapture%20more%20intricate%20details.%20To%20en-hance%20the%20quality%20of%20MRI%20reconstruction%2C%0Awe%20propose%20a%20comprehensive%20detail-preserving%20reconstruction%20method%20using%0Amultiple%20diffusion%20models%20to%20extract%20structure%20and%20detail%20features%20in%20k-space%0Adomain%20instead%20of%20image%20do-main.%20Moreover%2C%20virtual%20binary%20modal%20masks%20are%0Autilized%20to%20refine%20the%20range%20of%20values%20in%20k-space%20data%20through%20highly%20adaptive%0Acenter%20windows%2C%20which%20allows%20the%20model%20to%20focus%20its%20attention%20more%20efficiently.%0ALast%20but%20not%20least%2C%20an%20inverted%20pyramid%20structure%20is%20employed%2C%20where%20the%0Atop-down%20image%20information%20gradually%20decreases%2C%20ena-bling%20a%20cascade%0Arepresentation.%20The%20framework%20effective-ly%20represents%20multi-scale%20sampled%20data%2C%0Ataking%20into%20ac-count%20the%20sparsity%20of%20the%20inverted%20pyramid%20architecture%2C%20and%0Autilizes%20cascade%20training%20data%20distribution%20to%20repre-sent%20multi-scale%20data.%0AThrough%20a%20step-by-step%20refinement%20approach%2C%20the%20method%20refines%20the%0Aapproximation%20of%20de-tails.%20Finally%2C%20the%20proposed%20method%20was%20evaluated%20by%0Acon-ducting%20experiments%20on%20clinical%20and%20public%20datasets.%20The%20results%0Ademonstrate%20that%20the%20proposed%20method%20outper-forms%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-MDM%253A%2520Detail-Preserving%2520MR%2520Reconstruction%2520via%2520Multiple%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DMengxiao%2520Geng%2520and%2520Jiahao%2520Zhu%2520and%2520Xiaolin%2520Zhu%2520and%2520Qiqing%2520Liu%2520and%2520Dong%2520Liang%2520and%2520Qiegen%2520Liu%26entry.1292438233%3D%2520%2520Detail%2520features%2520of%2520magnetic%2520resonance%2520images%2520play%2520a%2520cru-cial%2520role%2520in%2520accurate%250Amedical%2520diagnosis%2520and%2520treatment%252C%2520as%2520they%2520capture%2520subtle%2520changes%2520that%2520pose%250Achallenges%2520for%2520doc-tors%2520when%2520performing%2520precise%2520judgments.%2520However%252C%2520the%2520widely%250Autilized%2520naive%2520diffusion%2520model%2520has%2520limitations%252C%2520as%2520it%2520fails%2520to%2520accurately%250Acapture%2520more%2520intricate%2520details.%2520To%2520en-hance%2520the%2520quality%2520of%2520MRI%2520reconstruction%252C%250Awe%2520propose%2520a%2520comprehensive%2520detail-preserving%2520reconstruction%2520method%2520using%250Amultiple%2520diffusion%2520models%2520to%2520extract%2520structure%2520and%2520detail%2520features%2520in%2520k-space%250Adomain%2520instead%2520of%2520image%2520do-main.%2520Moreover%252C%2520virtual%2520binary%2520modal%2520masks%2520are%250Autilized%2520to%2520refine%2520the%2520range%2520of%2520values%2520in%2520k-space%2520data%2520through%2520highly%2520adaptive%250Acenter%2520windows%252C%2520which%2520allows%2520the%2520model%2520to%2520focus%2520its%2520attention%2520more%2520efficiently.%250ALast%2520but%2520not%2520least%252C%2520an%2520inverted%2520pyramid%2520structure%2520is%2520employed%252C%2520where%2520the%250Atop-down%2520image%2520information%2520gradually%2520decreases%252C%2520ena-bling%2520a%2520cascade%250Arepresentation.%2520The%2520framework%2520effective-ly%2520represents%2520multi-scale%2520sampled%2520data%252C%250Ataking%2520into%2520ac-count%2520the%2520sparsity%2520of%2520the%2520inverted%2520pyramid%2520architecture%252C%2520and%250Autilizes%2520cascade%2520training%2520data%2520distribution%2520to%2520repre-sent%2520multi-scale%2520data.%250AThrough%2520a%2520step-by-step%2520refinement%2520approach%252C%2520the%2520method%2520refines%2520the%250Aapproximation%2520of%2520de-tails.%2520Finally%252C%2520the%2520proposed%2520method%2520was%2520evaluated%2520by%250Acon-ducting%2520experiments%2520on%2520clinical%2520and%2520public%2520datasets.%2520The%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520outper-forms%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-MDM%3A%20Detail-Preserving%20MR%20Reconstruction%20via%20Multiple%20Diffusion%0A%20%20Models&entry.906535625=Mengxiao%20Geng%20and%20Jiahao%20Zhu%20and%20Xiaolin%20Zhu%20and%20Qiqing%20Liu%20and%20Dong%20Liang%20and%20Qiegen%20Liu&entry.1292438233=%20%20Detail%20features%20of%20magnetic%20resonance%20images%20play%20a%20cru-cial%20role%20in%20accurate%0Amedical%20diagnosis%20and%20treatment%2C%20as%20they%20capture%20subtle%20changes%20that%20pose%0Achallenges%20for%20doc-tors%20when%20performing%20precise%20judgments.%20However%2C%20the%20widely%0Autilized%20naive%20diffusion%20model%20has%20limitations%2C%20as%20it%20fails%20to%20accurately%0Acapture%20more%20intricate%20details.%20To%20en-hance%20the%20quality%20of%20MRI%20reconstruction%2C%0Awe%20propose%20a%20comprehensive%20detail-preserving%20reconstruction%20method%20using%0Amultiple%20diffusion%20models%20to%20extract%20structure%20and%20detail%20features%20in%20k-space%0Adomain%20instead%20of%20image%20do-main.%20Moreover%2C%20virtual%20binary%20modal%20masks%20are%0Autilized%20to%20refine%20the%20range%20of%20values%20in%20k-space%20data%20through%20highly%20adaptive%0Acenter%20windows%2C%20which%20allows%20the%20model%20to%20focus%20its%20attention%20more%20efficiently.%0ALast%20but%20not%20least%2C%20an%20inverted%20pyramid%20structure%20is%20employed%2C%20where%20the%0Atop-down%20image%20information%20gradually%20decreases%2C%20ena-bling%20a%20cascade%0Arepresentation.%20The%20framework%20effective-ly%20represents%20multi-scale%20sampled%20data%2C%0Ataking%20into%20ac-count%20the%20sparsity%20of%20the%20inverted%20pyramid%20architecture%2C%20and%0Autilizes%20cascade%20training%20data%20distribution%20to%20repre-sent%20multi-scale%20data.%0AThrough%20a%20step-by-step%20refinement%20approach%2C%20the%20method%20refines%20the%0Aapproximation%20of%20de-tails.%20Finally%2C%20the%20proposed%20method%20was%20evaluated%20by%0Acon-ducting%20experiments%20on%20clinical%20and%20public%20datasets.%20The%20results%0Ademonstrate%20that%20the%20proposed%20method%20outper-forms%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05763v1&entry.124074799=Read"},
{"title": "Position: Leverage Foundational Models for Black-Box Optimization", "author": "Xingyou Song and Yingtao Tian and Robert Tjarko Lange and Chansoo Lee and Yujin Tang and Yutian Chen", "abstract": "  Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave\nof innovation in the machine learning research domain, resulting in substantial\nimpact across diverse fields such as reinforcement learning, robotics, and\ncomputer vision. Their incorporation has been rapid and transformative, marking\na significant paradigm shift in the field of machine learning research.\nHowever, the field of experimental design, grounded on black-box optimization,\nhas been much less affected by such a paradigm shift, even though integrating\nLLMs with optimization presents a unique landscape ripe for exploration. In\nthis position paper, we frame the field of black-box optimization around\nsequence-based foundation models and organize their relationship with previous\nliterature. We discuss the most promising ways foundational language models can\nrevolutionize optimization, which include harnessing the vast wealth of\ninformation encapsulated in free-form text to enrich task comprehension,\nutilizing highly flexible sequence models such as Transformers to engineer\nsuperior optimization strategies, and enhancing performance prediction over\npreviously unseen search spaces.\n", "link": "http://arxiv.org/abs/2405.03547v2", "date": "2024-05-09", "relevancy": 1.4504, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4916}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4798}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Leverage%20Foundational%20Models%20for%20Black-Box%20Optimization&body=Title%3A%20Position%3A%20Leverage%20Foundational%20Models%20for%20Black-Box%20Optimization%0AAuthor%3A%20Xingyou%20Song%20and%20Yingtao%20Tian%20and%20Robert%20Tjarko%20Lange%20and%20Chansoo%20Lee%20and%20Yujin%20Tang%20and%20Yutian%20Chen%0AAbstract%3A%20%20%20Undeniably%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20stirred%20an%20extraordinary%20wave%0Aof%20innovation%20in%20the%20machine%20learning%20research%20domain%2C%20resulting%20in%20substantial%0Aimpact%20across%20diverse%20fields%20such%20as%20reinforcement%20learning%2C%20robotics%2C%20and%0Acomputer%20vision.%20Their%20incorporation%20has%20been%20rapid%20and%20transformative%2C%20marking%0Aa%20significant%20paradigm%20shift%20in%20the%20field%20of%20machine%20learning%20research.%0AHowever%2C%20the%20field%20of%20experimental%20design%2C%20grounded%20on%20black-box%20optimization%2C%0Ahas%20been%20much%20less%20affected%20by%20such%20a%20paradigm%20shift%2C%20even%20though%20integrating%0ALLMs%20with%20optimization%20presents%20a%20unique%20landscape%20ripe%20for%20exploration.%20In%0Athis%20position%20paper%2C%20we%20frame%20the%20field%20of%20black-box%20optimization%20around%0Asequence-based%20foundation%20models%20and%20organize%20their%20relationship%20with%20previous%0Aliterature.%20We%20discuss%20the%20most%20promising%20ways%20foundational%20language%20models%20can%0Arevolutionize%20optimization%2C%20which%20include%20harnessing%20the%20vast%20wealth%20of%0Ainformation%20encapsulated%20in%20free-form%20text%20to%20enrich%20task%20comprehension%2C%0Autilizing%20highly%20flexible%20sequence%20models%20such%20as%20Transformers%20to%20engineer%0Asuperior%20optimization%20strategies%2C%20and%20enhancing%20performance%20prediction%20over%0Apreviously%20unseen%20search%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Leverage%2520Foundational%2520Models%2520for%2520Black-Box%2520Optimization%26entry.906535625%3DXingyou%2520Song%2520and%2520Yingtao%2520Tian%2520and%2520Robert%2520Tjarko%2520Lange%2520and%2520Chansoo%2520Lee%2520and%2520Yujin%2520Tang%2520and%2520Yutian%2520Chen%26entry.1292438233%3D%2520%2520Undeniably%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520stirred%2520an%2520extraordinary%2520wave%250Aof%2520innovation%2520in%2520the%2520machine%2520learning%2520research%2520domain%252C%2520resulting%2520in%2520substantial%250Aimpact%2520across%2520diverse%2520fields%2520such%2520as%2520reinforcement%2520learning%252C%2520robotics%252C%2520and%250Acomputer%2520vision.%2520Their%2520incorporation%2520has%2520been%2520rapid%2520and%2520transformative%252C%2520marking%250Aa%2520significant%2520paradigm%2520shift%2520in%2520the%2520field%2520of%2520machine%2520learning%2520research.%250AHowever%252C%2520the%2520field%2520of%2520experimental%2520design%252C%2520grounded%2520on%2520black-box%2520optimization%252C%250Ahas%2520been%2520much%2520less%2520affected%2520by%2520such%2520a%2520paradigm%2520shift%252C%2520even%2520though%2520integrating%250ALLMs%2520with%2520optimization%2520presents%2520a%2520unique%2520landscape%2520ripe%2520for%2520exploration.%2520In%250Athis%2520position%2520paper%252C%2520we%2520frame%2520the%2520field%2520of%2520black-box%2520optimization%2520around%250Asequence-based%2520foundation%2520models%2520and%2520organize%2520their%2520relationship%2520with%2520previous%250Aliterature.%2520We%2520discuss%2520the%2520most%2520promising%2520ways%2520foundational%2520language%2520models%2520can%250Arevolutionize%2520optimization%252C%2520which%2520include%2520harnessing%2520the%2520vast%2520wealth%2520of%250Ainformation%2520encapsulated%2520in%2520free-form%2520text%2520to%2520enrich%2520task%2520comprehension%252C%250Autilizing%2520highly%2520flexible%2520sequence%2520models%2520such%2520as%2520Transformers%2520to%2520engineer%250Asuperior%2520optimization%2520strategies%252C%2520and%2520enhancing%2520performance%2520prediction%2520over%250Apreviously%2520unseen%2520search%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Leverage%20Foundational%20Models%20for%20Black-Box%20Optimization&entry.906535625=Xingyou%20Song%20and%20Yingtao%20Tian%20and%20Robert%20Tjarko%20Lange%20and%20Chansoo%20Lee%20and%20Yujin%20Tang%20and%20Yutian%20Chen&entry.1292438233=%20%20Undeniably%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20stirred%20an%20extraordinary%20wave%0Aof%20innovation%20in%20the%20machine%20learning%20research%20domain%2C%20resulting%20in%20substantial%0Aimpact%20across%20diverse%20fields%20such%20as%20reinforcement%20learning%2C%20robotics%2C%20and%0Acomputer%20vision.%20Their%20incorporation%20has%20been%20rapid%20and%20transformative%2C%20marking%0Aa%20significant%20paradigm%20shift%20in%20the%20field%20of%20machine%20learning%20research.%0AHowever%2C%20the%20field%20of%20experimental%20design%2C%20grounded%20on%20black-box%20optimization%2C%0Ahas%20been%20much%20less%20affected%20by%20such%20a%20paradigm%20shift%2C%20even%20though%20integrating%0ALLMs%20with%20optimization%20presents%20a%20unique%20landscape%20ripe%20for%20exploration.%20In%0Athis%20position%20paper%2C%20we%20frame%20the%20field%20of%20black-box%20optimization%20around%0Asequence-based%20foundation%20models%20and%20organize%20their%20relationship%20with%20previous%0Aliterature.%20We%20discuss%20the%20most%20promising%20ways%20foundational%20language%20models%20can%0Arevolutionize%20optimization%2C%20which%20include%20harnessing%20the%20vast%20wealth%20of%0Ainformation%20encapsulated%20in%20free-form%20text%20to%20enrich%20task%20comprehension%2C%0Autilizing%20highly%20flexible%20sequence%20models%20such%20as%20Transformers%20to%20engineer%0Asuperior%20optimization%20strategies%2C%20and%20enhancing%20performance%20prediction%20over%0Apreviously%20unseen%20search%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03547v2&entry.124074799=Read"},
{"title": "CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts", "author": "Jiachen Li and Xinyao Wang and Sijie Zhu and Chia-Wen Kuo and Lu Xu and Fan Chen and Jitesh Jain and Humphrey Shi and Longyin Wen", "abstract": "  Recent advancements in Multimodal Large Language Models (LLMs) have focused\nprimarily on scaling by increasing text-image pair data and enhancing LLMs to\nimprove performance on multimodal tasks. However, these scaling approaches are\ncomputationally expensive and overlook the significance of improving model\ncapabilities from the vision side. Inspired by the successful applications of\nMixture-of-Experts (MoE) in LLMs, which improves model scalability during\ntraining while keeping inference costs similar to those of smaller models, we\npropose CuMo. CuMo incorporates Co-upcycled Top-K sparsely-gated\nMixture-of-experts blocks into both the vision encoder and the MLP connector,\nthereby enhancing the multimodal LLMs with minimal additional activated\nparameters during inference. CuMo first pre-trains the MLP blocks and then\ninitializes each expert in the MoE block from the pre-trained MLP block during\nthe visual instruction tuning stage. Auxiliary losses are used to ensure a\nbalanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMs\nacross various VQA and visual-instruction-following benchmarks using models\nwithin each model size group, all while training exclusively on open-sourced\ndatasets. The code and model weights for CuMo are open-sourced at\nhttps://github.com/SHI-Labs/CuMo.\n", "link": "http://arxiv.org/abs/2405.05949v1", "date": "2024-05-09", "relevancy": 1.5817, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5439}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5174}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CuMo%3A%20Scaling%20Multimodal%20LLM%20with%20Co-Upcycled%20Mixture-of-Experts&body=Title%3A%20CuMo%3A%20Scaling%20Multimodal%20LLM%20with%20Co-Upcycled%20Mixture-of-Experts%0AAuthor%3A%20Jiachen%20Li%20and%20Xinyao%20Wang%20and%20Sijie%20Zhu%20and%20Chia-Wen%20Kuo%20and%20Lu%20Xu%20and%20Fan%20Chen%20and%20Jitesh%20Jain%20and%20Humphrey%20Shi%20and%20Longyin%20Wen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28LLMs%29%20have%20focused%0Aprimarily%20on%20scaling%20by%20increasing%20text-image%20pair%20data%20and%20enhancing%20LLMs%20to%0Aimprove%20performance%20on%20multimodal%20tasks.%20However%2C%20these%20scaling%20approaches%20are%0Acomputationally%20expensive%20and%20overlook%20the%20significance%20of%20improving%20model%0Acapabilities%20from%20the%20vision%20side.%20Inspired%20by%20the%20successful%20applications%20of%0AMixture-of-Experts%20%28MoE%29%20in%20LLMs%2C%20which%20improves%20model%20scalability%20during%0Atraining%20while%20keeping%20inference%20costs%20similar%20to%20those%20of%20smaller%20models%2C%20we%0Apropose%20CuMo.%20CuMo%20incorporates%20Co-upcycled%20Top-K%20sparsely-gated%0AMixture-of-experts%20blocks%20into%20both%20the%20vision%20encoder%20and%20the%20MLP%20connector%2C%0Athereby%20enhancing%20the%20multimodal%20LLMs%20with%20minimal%20additional%20activated%0Aparameters%20during%20inference.%20CuMo%20first%20pre-trains%20the%20MLP%20blocks%20and%20then%0Ainitializes%20each%20expert%20in%20the%20MoE%20block%20from%20the%20pre-trained%20MLP%20block%20during%0Athe%20visual%20instruction%20tuning%20stage.%20Auxiliary%20losses%20are%20used%20to%20ensure%20a%0Abalanced%20loading%20of%20experts.%20CuMo%20outperforms%20state-of-the-art%20multimodal%20LLMs%0Aacross%20various%20VQA%20and%20visual-instruction-following%20benchmarks%20using%20models%0Awithin%20each%20model%20size%20group%2C%20all%20while%20training%20exclusively%20on%20open-sourced%0Adatasets.%20The%20code%20and%20model%20weights%20for%20CuMo%20are%20open-sourced%20at%0Ahttps%3A//github.com/SHI-Labs/CuMo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCuMo%253A%2520Scaling%2520Multimodal%2520LLM%2520with%2520Co-Upcycled%2520Mixture-of-Experts%26entry.906535625%3DJiachen%2520Li%2520and%2520Xinyao%2520Wang%2520and%2520Sijie%2520Zhu%2520and%2520Chia-Wen%2520Kuo%2520and%2520Lu%2520Xu%2520and%2520Fan%2520Chen%2520and%2520Jitesh%2520Jain%2520and%2520Humphrey%2520Shi%2520and%2520Longyin%2520Wen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520focused%250Aprimarily%2520on%2520scaling%2520by%2520increasing%2520text-image%2520pair%2520data%2520and%2520enhancing%2520LLMs%2520to%250Aimprove%2520performance%2520on%2520multimodal%2520tasks.%2520However%252C%2520these%2520scaling%2520approaches%2520are%250Acomputationally%2520expensive%2520and%2520overlook%2520the%2520significance%2520of%2520improving%2520model%250Acapabilities%2520from%2520the%2520vision%2520side.%2520Inspired%2520by%2520the%2520successful%2520applications%2520of%250AMixture-of-Experts%2520%2528MoE%2529%2520in%2520LLMs%252C%2520which%2520improves%2520model%2520scalability%2520during%250Atraining%2520while%2520keeping%2520inference%2520costs%2520similar%2520to%2520those%2520of%2520smaller%2520models%252C%2520we%250Apropose%2520CuMo.%2520CuMo%2520incorporates%2520Co-upcycled%2520Top-K%2520sparsely-gated%250AMixture-of-experts%2520blocks%2520into%2520both%2520the%2520vision%2520encoder%2520and%2520the%2520MLP%2520connector%252C%250Athereby%2520enhancing%2520the%2520multimodal%2520LLMs%2520with%2520minimal%2520additional%2520activated%250Aparameters%2520during%2520inference.%2520CuMo%2520first%2520pre-trains%2520the%2520MLP%2520blocks%2520and%2520then%250Ainitializes%2520each%2520expert%2520in%2520the%2520MoE%2520block%2520from%2520the%2520pre-trained%2520MLP%2520block%2520during%250Athe%2520visual%2520instruction%2520tuning%2520stage.%2520Auxiliary%2520losses%2520are%2520used%2520to%2520ensure%2520a%250Abalanced%2520loading%2520of%2520experts.%2520CuMo%2520outperforms%2520state-of-the-art%2520multimodal%2520LLMs%250Aacross%2520various%2520VQA%2520and%2520visual-instruction-following%2520benchmarks%2520using%2520models%250Awithin%2520each%2520model%2520size%2520group%252C%2520all%2520while%2520training%2520exclusively%2520on%2520open-sourced%250Adatasets.%2520The%2520code%2520and%2520model%2520weights%2520for%2520CuMo%2520are%2520open-sourced%2520at%250Ahttps%253A//github.com/SHI-Labs/CuMo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CuMo%3A%20Scaling%20Multimodal%20LLM%20with%20Co-Upcycled%20Mixture-of-Experts&entry.906535625=Jiachen%20Li%20and%20Xinyao%20Wang%20and%20Sijie%20Zhu%20and%20Chia-Wen%20Kuo%20and%20Lu%20Xu%20and%20Fan%20Chen%20and%20Jitesh%20Jain%20and%20Humphrey%20Shi%20and%20Longyin%20Wen&entry.1292438233=%20%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28LLMs%29%20have%20focused%0Aprimarily%20on%20scaling%20by%20increasing%20text-image%20pair%20data%20and%20enhancing%20LLMs%20to%0Aimprove%20performance%20on%20multimodal%20tasks.%20However%2C%20these%20scaling%20approaches%20are%0Acomputationally%20expensive%20and%20overlook%20the%20significance%20of%20improving%20model%0Acapabilities%20from%20the%20vision%20side.%20Inspired%20by%20the%20successful%20applications%20of%0AMixture-of-Experts%20%28MoE%29%20in%20LLMs%2C%20which%20improves%20model%20scalability%20during%0Atraining%20while%20keeping%20inference%20costs%20similar%20to%20those%20of%20smaller%20models%2C%20we%0Apropose%20CuMo.%20CuMo%20incorporates%20Co-upcycled%20Top-K%20sparsely-gated%0AMixture-of-experts%20blocks%20into%20both%20the%20vision%20encoder%20and%20the%20MLP%20connector%2C%0Athereby%20enhancing%20the%20multimodal%20LLMs%20with%20minimal%20additional%20activated%0Aparameters%20during%20inference.%20CuMo%20first%20pre-trains%20the%20MLP%20blocks%20and%20then%0Ainitializes%20each%20expert%20in%20the%20MoE%20block%20from%20the%20pre-trained%20MLP%20block%20during%0Athe%20visual%20instruction%20tuning%20stage.%20Auxiliary%20losses%20are%20used%20to%20ensure%20a%0Abalanced%20loading%20of%20experts.%20CuMo%20outperforms%20state-of-the-art%20multimodal%20LLMs%0Aacross%20various%20VQA%20and%20visual-instruction-following%20benchmarks%20using%20models%0Awithin%20each%20model%20size%20group%2C%20all%20while%20training%20exclusively%20on%20open-sourced%0Adatasets.%20The%20code%20and%20model%20weights%20for%20CuMo%20are%20open-sourced%20at%0Ahttps%3A//github.com/SHI-Labs/CuMo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05949v1&entry.124074799=Read"},
{"title": "Large Human Language Models: A Need and the Challenges", "author": "Nikita Soni and H. Andrew Schwartz and Jo\u00e3o Sedoc and Niranjan Balasubramanian", "abstract": "  As research in human-centered NLP advances, there is a growing recognition of\nthe importance of incorporating human and social factors into NLP models. At\nthe same time, our NLP systems have become heavily reliant on LLMs, most of\nwhich do not model authors. To build NLP systems that can truly understand\nhuman language, we must better integrate human contexts into LLMs. This brings\nto the fore a range of design considerations and challenges in terms of what\nhuman aspects to capture, how to represent them, and what modeling strategies\nto pursue. To address these, we advocate for three positions toward creating\nlarge human language models (LHLMs) using concepts from psychological and\nbehavioral sciences: First, LM training should include the human context.\nSecond, LHLMs should recognize that people are more than their group(s). Third,\nLHLMs should be able to account for the dynamic and temporally-dependent nature\nof the human context. We refer to relevant advances and present open challenges\nthat need to be addressed and their possible solutions in realizing these\ngoals.\n", "link": "http://arxiv.org/abs/2312.07751v3", "date": "2024-05-09", "relevancy": 1.4038, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4769}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4604}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Human%20Language%20Models%3A%20A%20Need%20and%20the%20Challenges&body=Title%3A%20Large%20Human%20Language%20Models%3A%20A%20Need%20and%20the%20Challenges%0AAuthor%3A%20Nikita%20Soni%20and%20H.%20Andrew%20Schwartz%20and%20Jo%C3%A3o%20Sedoc%20and%20Niranjan%20Balasubramanian%0AAbstract%3A%20%20%20As%20research%20in%20human-centered%20NLP%20advances%2C%20there%20is%20a%20growing%20recognition%20of%0Athe%20importance%20of%20incorporating%20human%20and%20social%20factors%20into%20NLP%20models.%20At%0Athe%20same%20time%2C%20our%20NLP%20systems%20have%20become%20heavily%20reliant%20on%20LLMs%2C%20most%20of%0Awhich%20do%20not%20model%20authors.%20To%20build%20NLP%20systems%20that%20can%20truly%20understand%0Ahuman%20language%2C%20we%20must%20better%20integrate%20human%20contexts%20into%20LLMs.%20This%20brings%0Ato%20the%20fore%20a%20range%20of%20design%20considerations%20and%20challenges%20in%20terms%20of%20what%0Ahuman%20aspects%20to%20capture%2C%20how%20to%20represent%20them%2C%20and%20what%20modeling%20strategies%0Ato%20pursue.%20To%20address%20these%2C%20we%20advocate%20for%20three%20positions%20toward%20creating%0Alarge%20human%20language%20models%20%28LHLMs%29%20using%20concepts%20from%20psychological%20and%0Abehavioral%20sciences%3A%20First%2C%20LM%20training%20should%20include%20the%20human%20context.%0ASecond%2C%20LHLMs%20should%20recognize%20that%20people%20are%20more%20than%20their%20group%28s%29.%20Third%2C%0ALHLMs%20should%20be%20able%20to%20account%20for%20the%20dynamic%20and%20temporally-dependent%20nature%0Aof%20the%20human%20context.%20We%20refer%20to%20relevant%20advances%20and%20present%20open%20challenges%0Athat%20need%20to%20be%20addressed%20and%20their%20possible%20solutions%20in%20realizing%20these%0Agoals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07751v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Human%2520Language%2520Models%253A%2520A%2520Need%2520and%2520the%2520Challenges%26entry.906535625%3DNikita%2520Soni%2520and%2520H.%2520Andrew%2520Schwartz%2520and%2520Jo%25C3%25A3o%2520Sedoc%2520and%2520Niranjan%2520Balasubramanian%26entry.1292438233%3D%2520%2520As%2520research%2520in%2520human-centered%2520NLP%2520advances%252C%2520there%2520is%2520a%2520growing%2520recognition%2520of%250Athe%2520importance%2520of%2520incorporating%2520human%2520and%2520social%2520factors%2520into%2520NLP%2520models.%2520At%250Athe%2520same%2520time%252C%2520our%2520NLP%2520systems%2520have%2520become%2520heavily%2520reliant%2520on%2520LLMs%252C%2520most%2520of%250Awhich%2520do%2520not%2520model%2520authors.%2520To%2520build%2520NLP%2520systems%2520that%2520can%2520truly%2520understand%250Ahuman%2520language%252C%2520we%2520must%2520better%2520integrate%2520human%2520contexts%2520into%2520LLMs.%2520This%2520brings%250Ato%2520the%2520fore%2520a%2520range%2520of%2520design%2520considerations%2520and%2520challenges%2520in%2520terms%2520of%2520what%250Ahuman%2520aspects%2520to%2520capture%252C%2520how%2520to%2520represent%2520them%252C%2520and%2520what%2520modeling%2520strategies%250Ato%2520pursue.%2520To%2520address%2520these%252C%2520we%2520advocate%2520for%2520three%2520positions%2520toward%2520creating%250Alarge%2520human%2520language%2520models%2520%2528LHLMs%2529%2520using%2520concepts%2520from%2520psychological%2520and%250Abehavioral%2520sciences%253A%2520First%252C%2520LM%2520training%2520should%2520include%2520the%2520human%2520context.%250ASecond%252C%2520LHLMs%2520should%2520recognize%2520that%2520people%2520are%2520more%2520than%2520their%2520group%2528s%2529.%2520Third%252C%250ALHLMs%2520should%2520be%2520able%2520to%2520account%2520for%2520the%2520dynamic%2520and%2520temporally-dependent%2520nature%250Aof%2520the%2520human%2520context.%2520We%2520refer%2520to%2520relevant%2520advances%2520and%2520present%2520open%2520challenges%250Athat%2520need%2520to%2520be%2520addressed%2520and%2520their%2520possible%2520solutions%2520in%2520realizing%2520these%250Agoals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07751v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Human%20Language%20Models%3A%20A%20Need%20and%20the%20Challenges&entry.906535625=Nikita%20Soni%20and%20H.%20Andrew%20Schwartz%20and%20Jo%C3%A3o%20Sedoc%20and%20Niranjan%20Balasubramanian&entry.1292438233=%20%20As%20research%20in%20human-centered%20NLP%20advances%2C%20there%20is%20a%20growing%20recognition%20of%0Athe%20importance%20of%20incorporating%20human%20and%20social%20factors%20into%20NLP%20models.%20At%0Athe%20same%20time%2C%20our%20NLP%20systems%20have%20become%20heavily%20reliant%20on%20LLMs%2C%20most%20of%0Awhich%20do%20not%20model%20authors.%20To%20build%20NLP%20systems%20that%20can%20truly%20understand%0Ahuman%20language%2C%20we%20must%20better%20integrate%20human%20contexts%20into%20LLMs.%20This%20brings%0Ato%20the%20fore%20a%20range%20of%20design%20considerations%20and%20challenges%20in%20terms%20of%20what%0Ahuman%20aspects%20to%20capture%2C%20how%20to%20represent%20them%2C%20and%20what%20modeling%20strategies%0Ato%20pursue.%20To%20address%20these%2C%20we%20advocate%20for%20three%20positions%20toward%20creating%0Alarge%20human%20language%20models%20%28LHLMs%29%20using%20concepts%20from%20psychological%20and%0Abehavioral%20sciences%3A%20First%2C%20LM%20training%20should%20include%20the%20human%20context.%0ASecond%2C%20LHLMs%20should%20recognize%20that%20people%20are%20more%20than%20their%20group%28s%29.%20Third%2C%0ALHLMs%20should%20be%20able%20to%20account%20for%20the%20dynamic%20and%20temporally-dependent%20nature%0Aof%20the%20human%20context.%20We%20refer%20to%20relevant%20advances%20and%20present%20open%20challenges%0Athat%20need%20to%20be%20addressed%20and%20their%20possible%20solutions%20in%20realizing%20these%0Agoals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07751v3&entry.124074799=Read"},
{"title": "A Flow-Based Model for Conditional and Probabilistic Electricity\n  Consumption Profile Generation and Prediction", "author": "Weijie Xia and Chenguang Wang and Peter Palensky and Pedro P. Vergara", "abstract": "  Residential Load Profile (RLP) generation and prediction are critical for the\noperation and planning of distribution networks, especially as diverse\nlow-carbon technologies (e.g., photovoltaic and electric vehicles) are\nincreasingly adopted. This paper introduces a novel flow-based generative\nmodel, termed Full Convolutional Profile Flow (FCPFlow), which is uniquely\ndesigned for both conditional and unconditional RLP generation, and for\nprobabilistic load forecasting. By introducing two new layers--the invertible\nlinear layer and the invertible normalization layer--the proposed FCPFlow\narchitecture shows three main advantages compared to traditional statistical\nand contemporary deep generative models: 1) it is well-suited for RLP\ngeneration under continuous conditions, such as varying weather and annual\nelectricity consumption, 2) it demonstrates superior scalability in different\ndatasets compared to traditional statistical models, and 3) it also\ndemonstrates better modeling capabilities in capturing the complex correlation\nof RLPs compared with deep generative models.\n", "link": "http://arxiv.org/abs/2405.02180v3", "date": "2024-05-09", "relevancy": 0.9708, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5601}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4587}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Flow-Based%20Model%20for%20Conditional%20and%20Probabilistic%20Electricity%0A%20%20Consumption%20Profile%20Generation%20and%20Prediction&body=Title%3A%20A%20Flow-Based%20Model%20for%20Conditional%20and%20Probabilistic%20Electricity%0A%20%20Consumption%20Profile%20Generation%20and%20Prediction%0AAuthor%3A%20Weijie%20Xia%20and%20Chenguang%20Wang%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara%0AAbstract%3A%20%20%20Residential%20Load%20Profile%20%28RLP%29%20generation%20and%20prediction%20are%20critical%20for%20the%0Aoperation%20and%20planning%20of%20distribution%20networks%2C%20especially%20as%20diverse%0Alow-carbon%20technologies%20%28e.g.%2C%20photovoltaic%20and%20electric%20vehicles%29%20are%0Aincreasingly%20adopted.%20This%20paper%20introduces%20a%20novel%20flow-based%20generative%0Amodel%2C%20termed%20Full%20Convolutional%20Profile%20Flow%20%28FCPFlow%29%2C%20which%20is%20uniquely%0Adesigned%20for%20both%20conditional%20and%20unconditional%20RLP%20generation%2C%20and%20for%0Aprobabilistic%20load%20forecasting.%20By%20introducing%20two%20new%20layers--the%20invertible%0Alinear%20layer%20and%20the%20invertible%20normalization%20layer--the%20proposed%20FCPFlow%0Aarchitecture%20shows%20three%20main%20advantages%20compared%20to%20traditional%20statistical%0Aand%20contemporary%20deep%20generative%20models%3A%201%29%20it%20is%20well-suited%20for%20RLP%0Ageneration%20under%20continuous%20conditions%2C%20such%20as%20varying%20weather%20and%20annual%0Aelectricity%20consumption%2C%202%29%20it%20demonstrates%20superior%20scalability%20in%20different%0Adatasets%20compared%20to%20traditional%20statistical%20models%2C%20and%203%29%20it%20also%0Ademonstrates%20better%20modeling%20capabilities%20in%20capturing%20the%20complex%20correlation%0Aof%20RLPs%20compared%20with%20deep%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02180v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Flow-Based%2520Model%2520for%2520Conditional%2520and%2520Probabilistic%2520Electricity%250A%2520%2520Consumption%2520Profile%2520Generation%2520and%2520Prediction%26entry.906535625%3DWeijie%2520Xia%2520and%2520Chenguang%2520Wang%2520and%2520Peter%2520Palensky%2520and%2520Pedro%2520P.%2520Vergara%26entry.1292438233%3D%2520%2520Residential%2520Load%2520Profile%2520%2528RLP%2529%2520generation%2520and%2520prediction%2520are%2520critical%2520for%2520the%250Aoperation%2520and%2520planning%2520of%2520distribution%2520networks%252C%2520especially%2520as%2520diverse%250Alow-carbon%2520technologies%2520%2528e.g.%252C%2520photovoltaic%2520and%2520electric%2520vehicles%2529%2520are%250Aincreasingly%2520adopted.%2520This%2520paper%2520introduces%2520a%2520novel%2520flow-based%2520generative%250Amodel%252C%2520termed%2520Full%2520Convolutional%2520Profile%2520Flow%2520%2528FCPFlow%2529%252C%2520which%2520is%2520uniquely%250Adesigned%2520for%2520both%2520conditional%2520and%2520unconditional%2520RLP%2520generation%252C%2520and%2520for%250Aprobabilistic%2520load%2520forecasting.%2520By%2520introducing%2520two%2520new%2520layers--the%2520invertible%250Alinear%2520layer%2520and%2520the%2520invertible%2520normalization%2520layer--the%2520proposed%2520FCPFlow%250Aarchitecture%2520shows%2520three%2520main%2520advantages%2520compared%2520to%2520traditional%2520statistical%250Aand%2520contemporary%2520deep%2520generative%2520models%253A%25201%2529%2520it%2520is%2520well-suited%2520for%2520RLP%250Ageneration%2520under%2520continuous%2520conditions%252C%2520such%2520as%2520varying%2520weather%2520and%2520annual%250Aelectricity%2520consumption%252C%25202%2529%2520it%2520demonstrates%2520superior%2520scalability%2520in%2520different%250Adatasets%2520compared%2520to%2520traditional%2520statistical%2520models%252C%2520and%25203%2529%2520it%2520also%250Ademonstrates%2520better%2520modeling%2520capabilities%2520in%2520capturing%2520the%2520complex%2520correlation%250Aof%2520RLPs%2520compared%2520with%2520deep%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02180v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Flow-Based%20Model%20for%20Conditional%20and%20Probabilistic%20Electricity%0A%20%20Consumption%20Profile%20Generation%20and%20Prediction&entry.906535625=Weijie%20Xia%20and%20Chenguang%20Wang%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara&entry.1292438233=%20%20Residential%20Load%20Profile%20%28RLP%29%20generation%20and%20prediction%20are%20critical%20for%20the%0Aoperation%20and%20planning%20of%20distribution%20networks%2C%20especially%20as%20diverse%0Alow-carbon%20technologies%20%28e.g.%2C%20photovoltaic%20and%20electric%20vehicles%29%20are%0Aincreasingly%20adopted.%20This%20paper%20introduces%20a%20novel%20flow-based%20generative%0Amodel%2C%20termed%20Full%20Convolutional%20Profile%20Flow%20%28FCPFlow%29%2C%20which%20is%20uniquely%0Adesigned%20for%20both%20conditional%20and%20unconditional%20RLP%20generation%2C%20and%20for%0Aprobabilistic%20load%20forecasting.%20By%20introducing%20two%20new%20layers--the%20invertible%0Alinear%20layer%20and%20the%20invertible%20normalization%20layer--the%20proposed%20FCPFlow%0Aarchitecture%20shows%20three%20main%20advantages%20compared%20to%20traditional%20statistical%0Aand%20contemporary%20deep%20generative%20models%3A%201%29%20it%20is%20well-suited%20for%20RLP%0Ageneration%20under%20continuous%20conditions%2C%20such%20as%20varying%20weather%20and%20annual%0Aelectricity%20consumption%2C%202%29%20it%20demonstrates%20superior%20scalability%20in%20different%0Adatasets%20compared%20to%20traditional%20statistical%20models%2C%20and%203%29%20it%20also%0Ademonstrates%20better%20modeling%20capabilities%20in%20capturing%20the%20complex%20correlation%0Aof%20RLPs%20compared%20with%20deep%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02180v3&entry.124074799=Read"},
{"title": "Age Aware Scheduling for Differentially-Private Federated Learning", "author": "Kuan-Yu Lin and Hsuan-Yin Lin and Yu-Pin Hsu and Yu-Chih Huang", "abstract": "  This paper explores differentially-private federated learning (FL) across\ntime-varying databases, delving into a nuanced three-way tradeoff involving\nage, accuracy, and differential privacy (DP). Emphasizing the potential\nadvantages of scheduling, we propose an optimization problem aimed at meeting\nDP requirements while minimizing the loss difference between the aggregated\nmodel and the model obtained without DP constraints. To harness the benefits of\nscheduling, we introduce an age-dependent upper bound on the loss, leading to\nthe development of an age-aware scheduling design. Simulation results\nunderscore the superior performance of our proposed scheme compared to FL with\nclassic DP, which does not consider scheduling as a design factor. This\nresearch contributes insights into the interplay of age, accuracy, and DP in\nfederated learning, with practical implications for scheduling strategies.\n", "link": "http://arxiv.org/abs/2405.05962v1", "date": "2024-05-09", "relevancy": 1.624, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4083}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4068}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Age%20Aware%20Scheduling%20for%20Differentially-Private%20Federated%20Learning&body=Title%3A%20Age%20Aware%20Scheduling%20for%20Differentially-Private%20Federated%20Learning%0AAuthor%3A%20Kuan-Yu%20Lin%20and%20Hsuan-Yin%20Lin%20and%20Yu-Pin%20Hsu%20and%20Yu-Chih%20Huang%0AAbstract%3A%20%20%20This%20paper%20explores%20differentially-private%20federated%20learning%20%28FL%29%20across%0Atime-varying%20databases%2C%20delving%20into%20a%20nuanced%20three-way%20tradeoff%20involving%0Aage%2C%20accuracy%2C%20and%20differential%20privacy%20%28DP%29.%20Emphasizing%20the%20potential%0Aadvantages%20of%20scheduling%2C%20we%20propose%20an%20optimization%20problem%20aimed%20at%20meeting%0ADP%20requirements%20while%20minimizing%20the%20loss%20difference%20between%20the%20aggregated%0Amodel%20and%20the%20model%20obtained%20without%20DP%20constraints.%20To%20harness%20the%20benefits%20of%0Ascheduling%2C%20we%20introduce%20an%20age-dependent%20upper%20bound%20on%20the%20loss%2C%20leading%20to%0Athe%20development%20of%20an%20age-aware%20scheduling%20design.%20Simulation%20results%0Aunderscore%20the%20superior%20performance%20of%20our%20proposed%20scheme%20compared%20to%20FL%20with%0Aclassic%20DP%2C%20which%20does%20not%20consider%20scheduling%20as%20a%20design%20factor.%20This%0Aresearch%20contributes%20insights%20into%20the%20interplay%20of%20age%2C%20accuracy%2C%20and%20DP%20in%0Afederated%20learning%2C%20with%20practical%20implications%20for%20scheduling%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAge%2520Aware%2520Scheduling%2520for%2520Differentially-Private%2520Federated%2520Learning%26entry.906535625%3DKuan-Yu%2520Lin%2520and%2520Hsuan-Yin%2520Lin%2520and%2520Yu-Pin%2520Hsu%2520and%2520Yu-Chih%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520differentially-private%2520federated%2520learning%2520%2528FL%2529%2520across%250Atime-varying%2520databases%252C%2520delving%2520into%2520a%2520nuanced%2520three-way%2520tradeoff%2520involving%250Aage%252C%2520accuracy%252C%2520and%2520differential%2520privacy%2520%2528DP%2529.%2520Emphasizing%2520the%2520potential%250Aadvantages%2520of%2520scheduling%252C%2520we%2520propose%2520an%2520optimization%2520problem%2520aimed%2520at%2520meeting%250ADP%2520requirements%2520while%2520minimizing%2520the%2520loss%2520difference%2520between%2520the%2520aggregated%250Amodel%2520and%2520the%2520model%2520obtained%2520without%2520DP%2520constraints.%2520To%2520harness%2520the%2520benefits%2520of%250Ascheduling%252C%2520we%2520introduce%2520an%2520age-dependent%2520upper%2520bound%2520on%2520the%2520loss%252C%2520leading%2520to%250Athe%2520development%2520of%2520an%2520age-aware%2520scheduling%2520design.%2520Simulation%2520results%250Aunderscore%2520the%2520superior%2520performance%2520of%2520our%2520proposed%2520scheme%2520compared%2520to%2520FL%2520with%250Aclassic%2520DP%252C%2520which%2520does%2520not%2520consider%2520scheduling%2520as%2520a%2520design%2520factor.%2520This%250Aresearch%2520contributes%2520insights%2520into%2520the%2520interplay%2520of%2520age%252C%2520accuracy%252C%2520and%2520DP%2520in%250Afederated%2520learning%252C%2520with%2520practical%2520implications%2520for%2520scheduling%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Age%20Aware%20Scheduling%20for%20Differentially-Private%20Federated%20Learning&entry.906535625=Kuan-Yu%20Lin%20and%20Hsuan-Yin%20Lin%20and%20Yu-Pin%20Hsu%20and%20Yu-Chih%20Huang&entry.1292438233=%20%20This%20paper%20explores%20differentially-private%20federated%20learning%20%28FL%29%20across%0Atime-varying%20databases%2C%20delving%20into%20a%20nuanced%20three-way%20tradeoff%20involving%0Aage%2C%20accuracy%2C%20and%20differential%20privacy%20%28DP%29.%20Emphasizing%20the%20potential%0Aadvantages%20of%20scheduling%2C%20we%20propose%20an%20optimization%20problem%20aimed%20at%20meeting%0ADP%20requirements%20while%20minimizing%20the%20loss%20difference%20between%20the%20aggregated%0Amodel%20and%20the%20model%20obtained%20without%20DP%20constraints.%20To%20harness%20the%20benefits%20of%0Ascheduling%2C%20we%20introduce%20an%20age-dependent%20upper%20bound%20on%20the%20loss%2C%20leading%20to%0Athe%20development%20of%20an%20age-aware%20scheduling%20design.%20Simulation%20results%0Aunderscore%20the%20superior%20performance%20of%20our%20proposed%20scheme%20compared%20to%20FL%20with%0Aclassic%20DP%2C%20which%20does%20not%20consider%20scheduling%20as%20a%20design%20factor.%20This%0Aresearch%20contributes%20insights%20into%20the%20interplay%20of%20age%2C%20accuracy%2C%20and%20DP%20in%0Afederated%20learning%2C%20with%20practical%20implications%20for%20scheduling%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05962v1&entry.124074799=Read"},
{"title": "Self-Supervised Learning of Time Series Representation via Diffusion\n  Process and Imputation-Interpolation-Forecasting Mask", "author": "Zineb Senane and Lele Cao and Valentin Leonhard Buchner and Yusuke Tashiro and Lei You and Pawel Herman and Mats Nordahl and Ruibo Tu and Vilhelm von Ehrenheim", "abstract": "  Time Series Representation Learning (TSRL) focuses on generating informative\nrepresentations for various Time Series (TS) modeling tasks. Traditional\nSelf-Supervised Learning (SSL) methods in TSRL fall into four main categories:\nreconstructive, adversarial, contrastive, and predictive, each with a common\nchallenge of sensitivity to noise and intricate data nuances. Recently,\ndiffusion-based methods have shown advanced generative capabilities. However,\nthey primarily target specific application scenarios like imputation and\nforecasting, leaving a gap in leveraging diffusion models for generic TSRL. Our\nwork, Time Series Diffusion Embedding (TSDE), bridges this gap as the first\ndiffusion-based SSL TSRL approach. TSDE segments TS data into observed and\nmasked parts using an Imputation-Interpolation-Forecasting (IIF) mask. It\napplies a trainable embedding function, featuring dual-orthogonal Transformer\nencoders with a crossover mechanism, to the observed part. We train a reverse\ndiffusion process conditioned on the embeddings, designed to predict noise\nadded to the masked part. Extensive experiments demonstrate TSDE's superiority\nin imputation, interpolation, forecasting, anomaly detection, classification,\nand clustering. We also conduct an ablation study, present embedding\nvisualizations, and compare inference speed, further substantiating TSDE's\nefficiency and validity in learning representations of TS data.\n", "link": "http://arxiv.org/abs/2405.05959v1", "date": "2024-05-09", "relevancy": 1.5936, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5736}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5471}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20of%20Time%20Series%20Representation%20via%20Diffusion%0A%20%20Process%20and%20Imputation-Interpolation-Forecasting%20Mask&body=Title%3A%20Self-Supervised%20Learning%20of%20Time%20Series%20Representation%20via%20Diffusion%0A%20%20Process%20and%20Imputation-Interpolation-Forecasting%20Mask%0AAuthor%3A%20Zineb%20Senane%20and%20Lele%20Cao%20and%20Valentin%20Leonhard%20Buchner%20and%20Yusuke%20Tashiro%20and%20Lei%20You%20and%20Pawel%20Herman%20and%20Mats%20Nordahl%20and%20Ruibo%20Tu%20and%20Vilhelm%20von%20Ehrenheim%0AAbstract%3A%20%20%20Time%20Series%20Representation%20Learning%20%28TSRL%29%20focuses%20on%20generating%20informative%0Arepresentations%20for%20various%20Time%20Series%20%28TS%29%20modeling%20tasks.%20Traditional%0ASelf-Supervised%20Learning%20%28SSL%29%20methods%20in%20TSRL%20fall%20into%20four%20main%20categories%3A%0Areconstructive%2C%20adversarial%2C%20contrastive%2C%20and%20predictive%2C%20each%20with%20a%20common%0Achallenge%20of%20sensitivity%20to%20noise%20and%20intricate%20data%20nuances.%20Recently%2C%0Adiffusion-based%20methods%20have%20shown%20advanced%20generative%20capabilities.%20However%2C%0Athey%20primarily%20target%20specific%20application%20scenarios%20like%20imputation%20and%0Aforecasting%2C%20leaving%20a%20gap%20in%20leveraging%20diffusion%20models%20for%20generic%20TSRL.%20Our%0Awork%2C%20Time%20Series%20Diffusion%20Embedding%20%28TSDE%29%2C%20bridges%20this%20gap%20as%20the%20first%0Adiffusion-based%20SSL%20TSRL%20approach.%20TSDE%20segments%20TS%20data%20into%20observed%20and%0Amasked%20parts%20using%20an%20Imputation-Interpolation-Forecasting%20%28IIF%29%20mask.%20It%0Aapplies%20a%20trainable%20embedding%20function%2C%20featuring%20dual-orthogonal%20Transformer%0Aencoders%20with%20a%20crossover%20mechanism%2C%20to%20the%20observed%20part.%20We%20train%20a%20reverse%0Adiffusion%20process%20conditioned%20on%20the%20embeddings%2C%20designed%20to%20predict%20noise%0Aadded%20to%20the%20masked%20part.%20Extensive%20experiments%20demonstrate%20TSDE%27s%20superiority%0Ain%20imputation%2C%20interpolation%2C%20forecasting%2C%20anomaly%20detection%2C%20classification%2C%0Aand%20clustering.%20We%20also%20conduct%20an%20ablation%20study%2C%20present%20embedding%0Avisualizations%2C%20and%20compare%20inference%20speed%2C%20further%20substantiating%20TSDE%27s%0Aefficiency%20and%20validity%20in%20learning%20representations%20of%20TS%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520of%2520Time%2520Series%2520Representation%2520via%2520Diffusion%250A%2520%2520Process%2520and%2520Imputation-Interpolation-Forecasting%2520Mask%26entry.906535625%3DZineb%2520Senane%2520and%2520Lele%2520Cao%2520and%2520Valentin%2520Leonhard%2520Buchner%2520and%2520Yusuke%2520Tashiro%2520and%2520Lei%2520You%2520and%2520Pawel%2520Herman%2520and%2520Mats%2520Nordahl%2520and%2520Ruibo%2520Tu%2520and%2520Vilhelm%2520von%2520Ehrenheim%26entry.1292438233%3D%2520%2520Time%2520Series%2520Representation%2520Learning%2520%2528TSRL%2529%2520focuses%2520on%2520generating%2520informative%250Arepresentations%2520for%2520various%2520Time%2520Series%2520%2528TS%2529%2520modeling%2520tasks.%2520Traditional%250ASelf-Supervised%2520Learning%2520%2528SSL%2529%2520methods%2520in%2520TSRL%2520fall%2520into%2520four%2520main%2520categories%253A%250Areconstructive%252C%2520adversarial%252C%2520contrastive%252C%2520and%2520predictive%252C%2520each%2520with%2520a%2520common%250Achallenge%2520of%2520sensitivity%2520to%2520noise%2520and%2520intricate%2520data%2520nuances.%2520Recently%252C%250Adiffusion-based%2520methods%2520have%2520shown%2520advanced%2520generative%2520capabilities.%2520However%252C%250Athey%2520primarily%2520target%2520specific%2520application%2520scenarios%2520like%2520imputation%2520and%250Aforecasting%252C%2520leaving%2520a%2520gap%2520in%2520leveraging%2520diffusion%2520models%2520for%2520generic%2520TSRL.%2520Our%250Awork%252C%2520Time%2520Series%2520Diffusion%2520Embedding%2520%2528TSDE%2529%252C%2520bridges%2520this%2520gap%2520as%2520the%2520first%250Adiffusion-based%2520SSL%2520TSRL%2520approach.%2520TSDE%2520segments%2520TS%2520data%2520into%2520observed%2520and%250Amasked%2520parts%2520using%2520an%2520Imputation-Interpolation-Forecasting%2520%2528IIF%2529%2520mask.%2520It%250Aapplies%2520a%2520trainable%2520embedding%2520function%252C%2520featuring%2520dual-orthogonal%2520Transformer%250Aencoders%2520with%2520a%2520crossover%2520mechanism%252C%2520to%2520the%2520observed%2520part.%2520We%2520train%2520a%2520reverse%250Adiffusion%2520process%2520conditioned%2520on%2520the%2520embeddings%252C%2520designed%2520to%2520predict%2520noise%250Aadded%2520to%2520the%2520masked%2520part.%2520Extensive%2520experiments%2520demonstrate%2520TSDE%2527s%2520superiority%250Ain%2520imputation%252C%2520interpolation%252C%2520forecasting%252C%2520anomaly%2520detection%252C%2520classification%252C%250Aand%2520clustering.%2520We%2520also%2520conduct%2520an%2520ablation%2520study%252C%2520present%2520embedding%250Avisualizations%252C%2520and%2520compare%2520inference%2520speed%252C%2520further%2520substantiating%2520TSDE%2527s%250Aefficiency%2520and%2520validity%2520in%2520learning%2520representations%2520of%2520TS%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20of%20Time%20Series%20Representation%20via%20Diffusion%0A%20%20Process%20and%20Imputation-Interpolation-Forecasting%20Mask&entry.906535625=Zineb%20Senane%20and%20Lele%20Cao%20and%20Valentin%20Leonhard%20Buchner%20and%20Yusuke%20Tashiro%20and%20Lei%20You%20and%20Pawel%20Herman%20and%20Mats%20Nordahl%20and%20Ruibo%20Tu%20and%20Vilhelm%20von%20Ehrenheim&entry.1292438233=%20%20Time%20Series%20Representation%20Learning%20%28TSRL%29%20focuses%20on%20generating%20informative%0Arepresentations%20for%20various%20Time%20Series%20%28TS%29%20modeling%20tasks.%20Traditional%0ASelf-Supervised%20Learning%20%28SSL%29%20methods%20in%20TSRL%20fall%20into%20four%20main%20categories%3A%0Areconstructive%2C%20adversarial%2C%20contrastive%2C%20and%20predictive%2C%20each%20with%20a%20common%0Achallenge%20of%20sensitivity%20to%20noise%20and%20intricate%20data%20nuances.%20Recently%2C%0Adiffusion-based%20methods%20have%20shown%20advanced%20generative%20capabilities.%20However%2C%0Athey%20primarily%20target%20specific%20application%20scenarios%20like%20imputation%20and%0Aforecasting%2C%20leaving%20a%20gap%20in%20leveraging%20diffusion%20models%20for%20generic%20TSRL.%20Our%0Awork%2C%20Time%20Series%20Diffusion%20Embedding%20%28TSDE%29%2C%20bridges%20this%20gap%20as%20the%20first%0Adiffusion-based%20SSL%20TSRL%20approach.%20TSDE%20segments%20TS%20data%20into%20observed%20and%0Amasked%20parts%20using%20an%20Imputation-Interpolation-Forecasting%20%28IIF%29%20mask.%20It%0Aapplies%20a%20trainable%20embedding%20function%2C%20featuring%20dual-orthogonal%20Transformer%0Aencoders%20with%20a%20crossover%20mechanism%2C%20to%20the%20observed%20part.%20We%20train%20a%20reverse%0Adiffusion%20process%20conditioned%20on%20the%20embeddings%2C%20designed%20to%20predict%20noise%0Aadded%20to%20the%20masked%20part.%20Extensive%20experiments%20demonstrate%20TSDE%27s%20superiority%0Ain%20imputation%2C%20interpolation%2C%20forecasting%2C%20anomaly%20detection%2C%20classification%2C%0Aand%20clustering.%20We%20also%20conduct%20an%20ablation%20study%2C%20present%20embedding%0Avisualizations%2C%20and%20compare%20inference%20speed%2C%20further%20substantiating%20TSDE%27s%0Aefficiency%20and%20validity%20in%20learning%20representations%20of%20TS%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05959v1&entry.124074799=Read"},
{"title": "ExtremeCast: Boosting Extreme Value Prediction for Global Weather\n  Forecast", "author": "Wanghan Xu and Kang Chen and Tao Han and Hao Chen and Wanli Ouyang and Lei Bai", "abstract": "  Data-driven weather forecast based on machine learning (ML) has experienced\nrapid development and demonstrated superior performance in the global\nmedium-range forecast compared to traditional physics-based dynamical models.\nHowever, most of these ML models struggle with accurately predicting extreme\nweather, which is closely related to the extreme value prediction. Through\nmathematical analysis, we prove that the use of symmetric losses, such as the\nMean Squared Error (MSE), leads to biased predictions and underestimation of\nextreme values. To address this issue, we introduce Exloss, a novel loss\nfunction that performs asymmetric optimization and highlights extreme values to\nobtain accurate extreme weather forecast. Furthermore, we introduce a\ntraining-free extreme value enhancement strategy named ExEnsemble, which\nincreases the variance of pixel values and improves the forecast robustness.\nCombined with an advanced global weather forecast model, extensive experiments\nshow that our solution can achieve state-of-the-art performance in extreme\nweather prediction, while maintaining the overall forecast accuracy comparable\nto the top medium-range forecast models.\n", "link": "http://arxiv.org/abs/2402.01295v3", "date": "2024-05-09", "relevancy": 1.4721, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5171}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4706}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExtremeCast%3A%20Boosting%20Extreme%20Value%20Prediction%20for%20Global%20Weather%0A%20%20Forecast&body=Title%3A%20ExtremeCast%3A%20Boosting%20Extreme%20Value%20Prediction%20for%20Global%20Weather%0A%20%20Forecast%0AAuthor%3A%20Wanghan%20Xu%20and%20Kang%20Chen%20and%20Tao%20Han%20and%20Hao%20Chen%20and%20Wanli%20Ouyang%20and%20Lei%20Bai%0AAbstract%3A%20%20%20Data-driven%20weather%20forecast%20based%20on%20machine%20learning%20%28ML%29%20has%20experienced%0Arapid%20development%20and%20demonstrated%20superior%20performance%20in%20the%20global%0Amedium-range%20forecast%20compared%20to%20traditional%20physics-based%20dynamical%20models.%0AHowever%2C%20most%20of%20these%20ML%20models%20struggle%20with%20accurately%20predicting%20extreme%0Aweather%2C%20which%20is%20closely%20related%20to%20the%20extreme%20value%20prediction.%20Through%0Amathematical%20analysis%2C%20we%20prove%20that%20the%20use%20of%20symmetric%20losses%2C%20such%20as%20the%0AMean%20Squared%20Error%20%28MSE%29%2C%20leads%20to%20biased%20predictions%20and%20underestimation%20of%0Aextreme%20values.%20To%20address%20this%20issue%2C%20we%20introduce%20Exloss%2C%20a%20novel%20loss%0Afunction%20that%20performs%20asymmetric%20optimization%20and%20highlights%20extreme%20values%20to%0Aobtain%20accurate%20extreme%20weather%20forecast.%20Furthermore%2C%20we%20introduce%20a%0Atraining-free%20extreme%20value%20enhancement%20strategy%20named%20ExEnsemble%2C%20which%0Aincreases%20the%20variance%20of%20pixel%20values%20and%20improves%20the%20forecast%20robustness.%0ACombined%20with%20an%20advanced%20global%20weather%20forecast%20model%2C%20extensive%20experiments%0Ashow%20that%20our%20solution%20can%20achieve%20state-of-the-art%20performance%20in%20extreme%0Aweather%20prediction%2C%20while%20maintaining%20the%20overall%20forecast%20accuracy%20comparable%0Ato%20the%20top%20medium-range%20forecast%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01295v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtremeCast%253A%2520Boosting%2520Extreme%2520Value%2520Prediction%2520for%2520Global%2520Weather%250A%2520%2520Forecast%26entry.906535625%3DWanghan%2520Xu%2520and%2520Kang%2520Chen%2520and%2520Tao%2520Han%2520and%2520Hao%2520Chen%2520and%2520Wanli%2520Ouyang%2520and%2520Lei%2520Bai%26entry.1292438233%3D%2520%2520Data-driven%2520weather%2520forecast%2520based%2520on%2520machine%2520learning%2520%2528ML%2529%2520has%2520experienced%250Arapid%2520development%2520and%2520demonstrated%2520superior%2520performance%2520in%2520the%2520global%250Amedium-range%2520forecast%2520compared%2520to%2520traditional%2520physics-based%2520dynamical%2520models.%250AHowever%252C%2520most%2520of%2520these%2520ML%2520models%2520struggle%2520with%2520accurately%2520predicting%2520extreme%250Aweather%252C%2520which%2520is%2520closely%2520related%2520to%2520the%2520extreme%2520value%2520prediction.%2520Through%250Amathematical%2520analysis%252C%2520we%2520prove%2520that%2520the%2520use%2520of%2520symmetric%2520losses%252C%2520such%2520as%2520the%250AMean%2520Squared%2520Error%2520%2528MSE%2529%252C%2520leads%2520to%2520biased%2520predictions%2520and%2520underestimation%2520of%250Aextreme%2520values.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Exloss%252C%2520a%2520novel%2520loss%250Afunction%2520that%2520performs%2520asymmetric%2520optimization%2520and%2520highlights%2520extreme%2520values%2520to%250Aobtain%2520accurate%2520extreme%2520weather%2520forecast.%2520Furthermore%252C%2520we%2520introduce%2520a%250Atraining-free%2520extreme%2520value%2520enhancement%2520strategy%2520named%2520ExEnsemble%252C%2520which%250Aincreases%2520the%2520variance%2520of%2520pixel%2520values%2520and%2520improves%2520the%2520forecast%2520robustness.%250ACombined%2520with%2520an%2520advanced%2520global%2520weather%2520forecast%2520model%252C%2520extensive%2520experiments%250Ashow%2520that%2520our%2520solution%2520can%2520achieve%2520state-of-the-art%2520performance%2520in%2520extreme%250Aweather%2520prediction%252C%2520while%2520maintaining%2520the%2520overall%2520forecast%2520accuracy%2520comparable%250Ato%2520the%2520top%2520medium-range%2520forecast%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01295v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExtremeCast%3A%20Boosting%20Extreme%20Value%20Prediction%20for%20Global%20Weather%0A%20%20Forecast&entry.906535625=Wanghan%20Xu%20and%20Kang%20Chen%20and%20Tao%20Han%20and%20Hao%20Chen%20and%20Wanli%20Ouyang%20and%20Lei%20Bai&entry.1292438233=%20%20Data-driven%20weather%20forecast%20based%20on%20machine%20learning%20%28ML%29%20has%20experienced%0Arapid%20development%20and%20demonstrated%20superior%20performance%20in%20the%20global%0Amedium-range%20forecast%20compared%20to%20traditional%20physics-based%20dynamical%20models.%0AHowever%2C%20most%20of%20these%20ML%20models%20struggle%20with%20accurately%20predicting%20extreme%0Aweather%2C%20which%20is%20closely%20related%20to%20the%20extreme%20value%20prediction.%20Through%0Amathematical%20analysis%2C%20we%20prove%20that%20the%20use%20of%20symmetric%20losses%2C%20such%20as%20the%0AMean%20Squared%20Error%20%28MSE%29%2C%20leads%20to%20biased%20predictions%20and%20underestimation%20of%0Aextreme%20values.%20To%20address%20this%20issue%2C%20we%20introduce%20Exloss%2C%20a%20novel%20loss%0Afunction%20that%20performs%20asymmetric%20optimization%20and%20highlights%20extreme%20values%20to%0Aobtain%20accurate%20extreme%20weather%20forecast.%20Furthermore%2C%20we%20introduce%20a%0Atraining-free%20extreme%20value%20enhancement%20strategy%20named%20ExEnsemble%2C%20which%0Aincreases%20the%20variance%20of%20pixel%20values%20and%20improves%20the%20forecast%20robustness.%0ACombined%20with%20an%20advanced%20global%20weather%20forecast%20model%2C%20extensive%20experiments%0Ashow%20that%20our%20solution%20can%20achieve%20state-of-the-art%20performance%20in%20extreme%0Aweather%20prediction%2C%20while%20maintaining%20the%20overall%20forecast%20accuracy%20comparable%0Ato%20the%20top%20medium-range%20forecast%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01295v3&entry.124074799=Read"},
{"title": "Predicting PDEs Fast and Efficiently with Equivariant Extreme Learning\n  Machines", "author": "Hans Harder and Sebastian Peitz", "abstract": "  We utilize extreme learning machines for the prediction of partial\ndifferential equations (PDEs). Our method splits the state space into multiple\nwindows that are predicted individually using a single model. Despite requiring\nonly few data points (in some cases, our method can learn from a single\nfull-state snapshot), it still achieves high accuracy and can predict the flow\nof PDEs over long time horizons. Moreover, we show how additional symmetries\ncan be exploited to increase sample efficiency and to enforce equivariance.\n", "link": "http://arxiv.org/abs/2404.18530v3", "date": "2024-05-09", "relevancy": 1.459, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4887}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4857}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20PDEs%20Fast%20and%20Efficiently%20with%20Equivariant%20Extreme%20Learning%0A%20%20Machines&body=Title%3A%20Predicting%20PDEs%20Fast%20and%20Efficiently%20with%20Equivariant%20Extreme%20Learning%0A%20%20Machines%0AAuthor%3A%20Hans%20Harder%20and%20Sebastian%20Peitz%0AAbstract%3A%20%20%20We%20utilize%20extreme%20learning%20machines%20for%20the%20prediction%20of%20partial%0Adifferential%20equations%20%28PDEs%29.%20Our%20method%20splits%20the%20state%20space%20into%20multiple%0Awindows%20that%20are%20predicted%20individually%20using%20a%20single%20model.%20Despite%20requiring%0Aonly%20few%20data%20points%20%28in%20some%20cases%2C%20our%20method%20can%20learn%20from%20a%20single%0Afull-state%20snapshot%29%2C%20it%20still%20achieves%20high%20accuracy%20and%20can%20predict%20the%20flow%0Aof%20PDEs%20over%20long%20time%20horizons.%20Moreover%2C%20we%20show%20how%20additional%20symmetries%0Acan%20be%20exploited%20to%20increase%20sample%20efficiency%20and%20to%20enforce%20equivariance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18530v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520PDEs%2520Fast%2520and%2520Efficiently%2520with%2520Equivariant%2520Extreme%2520Learning%250A%2520%2520Machines%26entry.906535625%3DHans%2520Harder%2520and%2520Sebastian%2520Peitz%26entry.1292438233%3D%2520%2520We%2520utilize%2520extreme%2520learning%2520machines%2520for%2520the%2520prediction%2520of%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529.%2520Our%2520method%2520splits%2520the%2520state%2520space%2520into%2520multiple%250Awindows%2520that%2520are%2520predicted%2520individually%2520using%2520a%2520single%2520model.%2520Despite%2520requiring%250Aonly%2520few%2520data%2520points%2520%2528in%2520some%2520cases%252C%2520our%2520method%2520can%2520learn%2520from%2520a%2520single%250Afull-state%2520snapshot%2529%252C%2520it%2520still%2520achieves%2520high%2520accuracy%2520and%2520can%2520predict%2520the%2520flow%250Aof%2520PDEs%2520over%2520long%2520time%2520horizons.%2520Moreover%252C%2520we%2520show%2520how%2520additional%2520symmetries%250Acan%2520be%2520exploited%2520to%2520increase%2520sample%2520efficiency%2520and%2520to%2520enforce%2520equivariance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18530v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20PDEs%20Fast%20and%20Efficiently%20with%20Equivariant%20Extreme%20Learning%0A%20%20Machines&entry.906535625=Hans%20Harder%20and%20Sebastian%20Peitz&entry.1292438233=%20%20We%20utilize%20extreme%20learning%20machines%20for%20the%20prediction%20of%20partial%0Adifferential%20equations%20%28PDEs%29.%20Our%20method%20splits%20the%20state%20space%20into%20multiple%0Awindows%20that%20are%20predicted%20individually%20using%20a%20single%20model.%20Despite%20requiring%0Aonly%20few%20data%20points%20%28in%20some%20cases%2C%20our%20method%20can%20learn%20from%20a%20single%0Afull-state%20snapshot%29%2C%20it%20still%20achieves%20high%20accuracy%20and%20can%20predict%20the%20flow%0Aof%20PDEs%20over%20long%20time%20horizons.%20Moreover%2C%20we%20show%20how%20additional%20symmetries%0Acan%20be%20exploited%20to%20increase%20sample%20efficiency%20and%20to%20enforce%20equivariance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18530v3&entry.124074799=Read"},
{"title": "Can large language models understand uncommon meanings of common words?", "author": "Jinyang Wu and Feihu Che and Xinxin Zheng and Shuai Zhang and Ruihan Jin and Shuai Nie and Pengpeng Shao and Jianhua Tao", "abstract": "  Large language models (LLMs) like ChatGPT have shown significant advancements\nacross diverse natural language understanding (NLU) tasks, including\nintelligent dialogue and autonomous agents. Yet, lacking widely acknowledged\ntesting mechanisms, answering `whether LLMs are stochastic parrots or genuinely\ncomprehend the world' remains unclear, fostering numerous studies and sparking\nheated debates. Prevailing research mainly focuses on surface-level NLU,\nneglecting fine-grained explorations. However, such explorations are crucial\nfor understanding their unique comprehension mechanisms, aligning with human\ncognition, and finally enhancing LLMs' general NLU capacities. To address this\ngap, our study delves into LLMs' nuanced semantic comprehension capabilities,\nparticularly regarding common words with uncommon meanings. The idea stems from\nfoundational principles of human communication within psychology, which\nunderscore accurate shared understandings of word semantics. Specifically, this\npaper presents the innovative construction of a Lexical Semantic Comprehension\n(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing\nboth fine-grained and cross-lingual dimensions. Introducing models of both\nopen-source and closed-source, varied scales and architectures, our extensive\nempirical experiments demonstrate the inferior performance of existing models\nin this basic lexical-meaning understanding task. Notably, even the\nstate-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%\nand 22.3%, respectively. Additionally, multiple advanced prompting techniques\nand retrieval-augmented generation are also introduced to help alleviate this\ntrouble, yet limitations persist. By highlighting the above critical\nshortcomings, this research motivates further investigation and offers novel\ninsights for developing more intelligent LLMs.\n", "link": "http://arxiv.org/abs/2405.05741v1", "date": "2024-05-09", "relevancy": 1.4766, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.501}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4872}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20large%20language%20models%20understand%20uncommon%20meanings%20of%20common%20words%3F&body=Title%3A%20Can%20large%20language%20models%20understand%20uncommon%20meanings%20of%20common%20words%3F%0AAuthor%3A%20Jinyang%20Wu%20and%20Feihu%20Che%20and%20Xinxin%20Zheng%20and%20Shuai%20Zhang%20and%20Ruihan%20Jin%20and%20Shuai%20Nie%20and%20Pengpeng%20Shao%20and%20Jianhua%20Tao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20have%20shown%20significant%20advancements%0Aacross%20diverse%20natural%20language%20understanding%20%28NLU%29%20tasks%2C%20including%0Aintelligent%20dialogue%20and%20autonomous%20agents.%20Yet%2C%20lacking%20widely%20acknowledged%0Atesting%20mechanisms%2C%20answering%20%60whether%20LLMs%20are%20stochastic%20parrots%20or%20genuinely%0Acomprehend%20the%20world%27%20remains%20unclear%2C%20fostering%20numerous%20studies%20and%20sparking%0Aheated%20debates.%20Prevailing%20research%20mainly%20focuses%20on%20surface-level%20NLU%2C%0Aneglecting%20fine-grained%20explorations.%20However%2C%20such%20explorations%20are%20crucial%0Afor%20understanding%20their%20unique%20comprehension%20mechanisms%2C%20aligning%20with%20human%0Acognition%2C%20and%20finally%20enhancing%20LLMs%27%20general%20NLU%20capacities.%20To%20address%20this%0Agap%2C%20our%20study%20delves%20into%20LLMs%27%20nuanced%20semantic%20comprehension%20capabilities%2C%0Aparticularly%20regarding%20common%20words%20with%20uncommon%20meanings.%20The%20idea%20stems%20from%0Afoundational%20principles%20of%20human%20communication%20within%20psychology%2C%20which%0Aunderscore%20accurate%20shared%20understandings%20of%20word%20semantics.%20Specifically%2C%20this%0Apaper%20presents%20the%20innovative%20construction%20of%20a%20Lexical%20Semantic%20Comprehension%0A%28LeSC%29%20dataset%20with%20novel%20evaluation%20metrics%2C%20the%20first%20benchmark%20encompassing%0Aboth%20fine-grained%20and%20cross-lingual%20dimensions.%20Introducing%20models%20of%20both%0Aopen-source%20and%20closed-source%2C%20varied%20scales%20and%20architectures%2C%20our%20extensive%0Aempirical%20experiments%20demonstrate%20the%20inferior%20performance%20of%20existing%20models%0Ain%20this%20basic%20lexical-meaning%20understanding%20task.%20Notably%2C%20even%20the%0Astate-of-the-art%20LLMs%20GPT-4%20and%20GPT-3.5%20lag%20behind%2016-year-old%20humans%20by%203.9%25%0Aand%2022.3%25%2C%20respectively.%20Additionally%2C%20multiple%20advanced%20prompting%20techniques%0Aand%20retrieval-augmented%20generation%20are%20also%20introduced%20to%20help%20alleviate%20this%0Atrouble%2C%20yet%20limitations%20persist.%20By%20highlighting%20the%20above%20critical%0Ashortcomings%2C%20this%20research%20motivates%20further%20investigation%20and%20offers%20novel%0Ainsights%20for%20developing%20more%20intelligent%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520large%2520language%2520models%2520understand%2520uncommon%2520meanings%2520of%2520common%2520words%253F%26entry.906535625%3DJinyang%2520Wu%2520and%2520Feihu%2520Che%2520and%2520Xinxin%2520Zheng%2520and%2520Shuai%2520Zhang%2520and%2520Ruihan%2520Jin%2520and%2520Shuai%2520Nie%2520and%2520Pengpeng%2520Shao%2520and%2520Jianhua%2520Tao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520like%2520ChatGPT%2520have%2520shown%2520significant%2520advancements%250Aacross%2520diverse%2520natural%2520language%2520understanding%2520%2528NLU%2529%2520tasks%252C%2520including%250Aintelligent%2520dialogue%2520and%2520autonomous%2520agents.%2520Yet%252C%2520lacking%2520widely%2520acknowledged%250Atesting%2520mechanisms%252C%2520answering%2520%2560whether%2520LLMs%2520are%2520stochastic%2520parrots%2520or%2520genuinely%250Acomprehend%2520the%2520world%2527%2520remains%2520unclear%252C%2520fostering%2520numerous%2520studies%2520and%2520sparking%250Aheated%2520debates.%2520Prevailing%2520research%2520mainly%2520focuses%2520on%2520surface-level%2520NLU%252C%250Aneglecting%2520fine-grained%2520explorations.%2520However%252C%2520such%2520explorations%2520are%2520crucial%250Afor%2520understanding%2520their%2520unique%2520comprehension%2520mechanisms%252C%2520aligning%2520with%2520human%250Acognition%252C%2520and%2520finally%2520enhancing%2520LLMs%2527%2520general%2520NLU%2520capacities.%2520To%2520address%2520this%250Agap%252C%2520our%2520study%2520delves%2520into%2520LLMs%2527%2520nuanced%2520semantic%2520comprehension%2520capabilities%252C%250Aparticularly%2520regarding%2520common%2520words%2520with%2520uncommon%2520meanings.%2520The%2520idea%2520stems%2520from%250Afoundational%2520principles%2520of%2520human%2520communication%2520within%2520psychology%252C%2520which%250Aunderscore%2520accurate%2520shared%2520understandings%2520of%2520word%2520semantics.%2520Specifically%252C%2520this%250Apaper%2520presents%2520the%2520innovative%2520construction%2520of%2520a%2520Lexical%2520Semantic%2520Comprehension%250A%2528LeSC%2529%2520dataset%2520with%2520novel%2520evaluation%2520metrics%252C%2520the%2520first%2520benchmark%2520encompassing%250Aboth%2520fine-grained%2520and%2520cross-lingual%2520dimensions.%2520Introducing%2520models%2520of%2520both%250Aopen-source%2520and%2520closed-source%252C%2520varied%2520scales%2520and%2520architectures%252C%2520our%2520extensive%250Aempirical%2520experiments%2520demonstrate%2520the%2520inferior%2520performance%2520of%2520existing%2520models%250Ain%2520this%2520basic%2520lexical-meaning%2520understanding%2520task.%2520Notably%252C%2520even%2520the%250Astate-of-the-art%2520LLMs%2520GPT-4%2520and%2520GPT-3.5%2520lag%2520behind%252016-year-old%2520humans%2520by%25203.9%2525%250Aand%252022.3%2525%252C%2520respectively.%2520Additionally%252C%2520multiple%2520advanced%2520prompting%2520techniques%250Aand%2520retrieval-augmented%2520generation%2520are%2520also%2520introduced%2520to%2520help%2520alleviate%2520this%250Atrouble%252C%2520yet%2520limitations%2520persist.%2520By%2520highlighting%2520the%2520above%2520critical%250Ashortcomings%252C%2520this%2520research%2520motivates%2520further%2520investigation%2520and%2520offers%2520novel%250Ainsights%2520for%2520developing%2520more%2520intelligent%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20large%20language%20models%20understand%20uncommon%20meanings%20of%20common%20words%3F&entry.906535625=Jinyang%20Wu%20and%20Feihu%20Che%20and%20Xinxin%20Zheng%20and%20Shuai%20Zhang%20and%20Ruihan%20Jin%20and%20Shuai%20Nie%20and%20Pengpeng%20Shao%20and%20Jianhua%20Tao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20have%20shown%20significant%20advancements%0Aacross%20diverse%20natural%20language%20understanding%20%28NLU%29%20tasks%2C%20including%0Aintelligent%20dialogue%20and%20autonomous%20agents.%20Yet%2C%20lacking%20widely%20acknowledged%0Atesting%20mechanisms%2C%20answering%20%60whether%20LLMs%20are%20stochastic%20parrots%20or%20genuinely%0Acomprehend%20the%20world%27%20remains%20unclear%2C%20fostering%20numerous%20studies%20and%20sparking%0Aheated%20debates.%20Prevailing%20research%20mainly%20focuses%20on%20surface-level%20NLU%2C%0Aneglecting%20fine-grained%20explorations.%20However%2C%20such%20explorations%20are%20crucial%0Afor%20understanding%20their%20unique%20comprehension%20mechanisms%2C%20aligning%20with%20human%0Acognition%2C%20and%20finally%20enhancing%20LLMs%27%20general%20NLU%20capacities.%20To%20address%20this%0Agap%2C%20our%20study%20delves%20into%20LLMs%27%20nuanced%20semantic%20comprehension%20capabilities%2C%0Aparticularly%20regarding%20common%20words%20with%20uncommon%20meanings.%20The%20idea%20stems%20from%0Afoundational%20principles%20of%20human%20communication%20within%20psychology%2C%20which%0Aunderscore%20accurate%20shared%20understandings%20of%20word%20semantics.%20Specifically%2C%20this%0Apaper%20presents%20the%20innovative%20construction%20of%20a%20Lexical%20Semantic%20Comprehension%0A%28LeSC%29%20dataset%20with%20novel%20evaluation%20metrics%2C%20the%20first%20benchmark%20encompassing%0Aboth%20fine-grained%20and%20cross-lingual%20dimensions.%20Introducing%20models%20of%20both%0Aopen-source%20and%20closed-source%2C%20varied%20scales%20and%20architectures%2C%20our%20extensive%0Aempirical%20experiments%20demonstrate%20the%20inferior%20performance%20of%20existing%20models%0Ain%20this%20basic%20lexical-meaning%20understanding%20task.%20Notably%2C%20even%20the%0Astate-of-the-art%20LLMs%20GPT-4%20and%20GPT-3.5%20lag%20behind%2016-year-old%20humans%20by%203.9%25%0Aand%2022.3%25%2C%20respectively.%20Additionally%2C%20multiple%20advanced%20prompting%20techniques%0Aand%20retrieval-augmented%20generation%20are%20also%20introduced%20to%20help%20alleviate%20this%0Atrouble%2C%20yet%20limitations%20persist.%20By%20highlighting%20the%20above%20critical%0Ashortcomings%2C%20this%20research%20motivates%20further%20investigation%20and%20offers%20novel%0Ainsights%20for%20developing%20more%20intelligent%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05741v1&entry.124074799=Read"},
{"title": "SubGDiff: A Subgraph Diffusion Model to Improve Molecular Representation\n  Learning", "author": "Jiying Zhang and Zijing Liu and Yu Wang and Yu Li", "abstract": "  Molecular representation learning has shown great success in advancing\nAI-based drug discovery. The core of many recent works is based on the fact\nthat the 3D geometric structure of molecules provides essential information\nabout their physical and chemical characteristics. Recently, denoising\ndiffusion probabilistic models have achieved impressive performance in 3D\nmolecular representation learning. However, most existing molecular diffusion\nmodels treat each atom as an independent entity, overlooking the dependency\namong atoms within the molecular substructures. This paper introduces a novel\napproach that enhances molecular representation learning by incorporating\nsubstructural information within the diffusion process. We propose a novel\ndiffusion model termed SubGDiff for involving the molecular subgraph\ninformation in diffusion. Specifically, SubGDiff adopts three vital techniques:\ni) subgraph prediction, ii) expectation state, and iii) k-step same subgraph\ndiffusion, to enhance the perception of molecular substructure in the denoising\nnetwork. Experimentally, extensive downstream tasks demonstrate the superior\nperformance of our approach. The code is available at\nhttps://github.com/youjibiying/SubGDiff.\n", "link": "http://arxiv.org/abs/2405.05665v1", "date": "2024-05-09", "relevancy": 1.6367, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5803}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5416}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SubGDiff%3A%20A%20Subgraph%20Diffusion%20Model%20to%20Improve%20Molecular%20Representation%0A%20%20Learning&body=Title%3A%20SubGDiff%3A%20A%20Subgraph%20Diffusion%20Model%20to%20Improve%20Molecular%20Representation%0A%20%20Learning%0AAuthor%3A%20Jiying%20Zhang%20and%20Zijing%20Liu%20and%20Yu%20Wang%20and%20Yu%20Li%0AAbstract%3A%20%20%20Molecular%20representation%20learning%20has%20shown%20great%20success%20in%20advancing%0AAI-based%20drug%20discovery.%20The%20core%20of%20many%20recent%20works%20is%20based%20on%20the%20fact%0Athat%20the%203D%20geometric%20structure%20of%20molecules%20provides%20essential%20information%0Aabout%20their%20physical%20and%20chemical%20characteristics.%20Recently%2C%20denoising%0Adiffusion%20probabilistic%20models%20have%20achieved%20impressive%20performance%20in%203D%0Amolecular%20representation%20learning.%20However%2C%20most%20existing%20molecular%20diffusion%0Amodels%20treat%20each%20atom%20as%20an%20independent%20entity%2C%20overlooking%20the%20dependency%0Aamong%20atoms%20within%20the%20molecular%20substructures.%20This%20paper%20introduces%20a%20novel%0Aapproach%20that%20enhances%20molecular%20representation%20learning%20by%20incorporating%0Asubstructural%20information%20within%20the%20diffusion%20process.%20We%20propose%20a%20novel%0Adiffusion%20model%20termed%20SubGDiff%20for%20involving%20the%20molecular%20subgraph%0Ainformation%20in%20diffusion.%20Specifically%2C%20SubGDiff%20adopts%20three%20vital%20techniques%3A%0Ai%29%20subgraph%20prediction%2C%20ii%29%20expectation%20state%2C%20and%20iii%29%20k-step%20same%20subgraph%0Adiffusion%2C%20to%20enhance%20the%20perception%20of%20molecular%20substructure%20in%20the%20denoising%0Anetwork.%20Experimentally%2C%20extensive%20downstream%20tasks%20demonstrate%20the%20superior%0Aperformance%20of%20our%20approach.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/youjibiying/SubGDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubGDiff%253A%2520A%2520Subgraph%2520Diffusion%2520Model%2520to%2520Improve%2520Molecular%2520Representation%250A%2520%2520Learning%26entry.906535625%3DJiying%2520Zhang%2520and%2520Zijing%2520Liu%2520and%2520Yu%2520Wang%2520and%2520Yu%2520Li%26entry.1292438233%3D%2520%2520Molecular%2520representation%2520learning%2520has%2520shown%2520great%2520success%2520in%2520advancing%250AAI-based%2520drug%2520discovery.%2520The%2520core%2520of%2520many%2520recent%2520works%2520is%2520based%2520on%2520the%2520fact%250Athat%2520the%25203D%2520geometric%2520structure%2520of%2520molecules%2520provides%2520essential%2520information%250Aabout%2520their%2520physical%2520and%2520chemical%2520characteristics.%2520Recently%252C%2520denoising%250Adiffusion%2520probabilistic%2520models%2520have%2520achieved%2520impressive%2520performance%2520in%25203D%250Amolecular%2520representation%2520learning.%2520However%252C%2520most%2520existing%2520molecular%2520diffusion%250Amodels%2520treat%2520each%2520atom%2520as%2520an%2520independent%2520entity%252C%2520overlooking%2520the%2520dependency%250Aamong%2520atoms%2520within%2520the%2520molecular%2520substructures.%2520This%2520paper%2520introduces%2520a%2520novel%250Aapproach%2520that%2520enhances%2520molecular%2520representation%2520learning%2520by%2520incorporating%250Asubstructural%2520information%2520within%2520the%2520diffusion%2520process.%2520We%2520propose%2520a%2520novel%250Adiffusion%2520model%2520termed%2520SubGDiff%2520for%2520involving%2520the%2520molecular%2520subgraph%250Ainformation%2520in%2520diffusion.%2520Specifically%252C%2520SubGDiff%2520adopts%2520three%2520vital%2520techniques%253A%250Ai%2529%2520subgraph%2520prediction%252C%2520ii%2529%2520expectation%2520state%252C%2520and%2520iii%2529%2520k-step%2520same%2520subgraph%250Adiffusion%252C%2520to%2520enhance%2520the%2520perception%2520of%2520molecular%2520substructure%2520in%2520the%2520denoising%250Anetwork.%2520Experimentally%252C%2520extensive%2520downstream%2520tasks%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520approach.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/youjibiying/SubGDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SubGDiff%3A%20A%20Subgraph%20Diffusion%20Model%20to%20Improve%20Molecular%20Representation%0A%20%20Learning&entry.906535625=Jiying%20Zhang%20and%20Zijing%20Liu%20and%20Yu%20Wang%20and%20Yu%20Li&entry.1292438233=%20%20Molecular%20representation%20learning%20has%20shown%20great%20success%20in%20advancing%0AAI-based%20drug%20discovery.%20The%20core%20of%20many%20recent%20works%20is%20based%20on%20the%20fact%0Athat%20the%203D%20geometric%20structure%20of%20molecules%20provides%20essential%20information%0Aabout%20their%20physical%20and%20chemical%20characteristics.%20Recently%2C%20denoising%0Adiffusion%20probabilistic%20models%20have%20achieved%20impressive%20performance%20in%203D%0Amolecular%20representation%20learning.%20However%2C%20most%20existing%20molecular%20diffusion%0Amodels%20treat%20each%20atom%20as%20an%20independent%20entity%2C%20overlooking%20the%20dependency%0Aamong%20atoms%20within%20the%20molecular%20substructures.%20This%20paper%20introduces%20a%20novel%0Aapproach%20that%20enhances%20molecular%20representation%20learning%20by%20incorporating%0Asubstructural%20information%20within%20the%20diffusion%20process.%20We%20propose%20a%20novel%0Adiffusion%20model%20termed%20SubGDiff%20for%20involving%20the%20molecular%20subgraph%0Ainformation%20in%20diffusion.%20Specifically%2C%20SubGDiff%20adopts%20three%20vital%20techniques%3A%0Ai%29%20subgraph%20prediction%2C%20ii%29%20expectation%20state%2C%20and%20iii%29%20k-step%20same%20subgraph%0Adiffusion%2C%20to%20enhance%20the%20perception%20of%20molecular%20substructure%20in%20the%20denoising%0Anetwork.%20Experimentally%2C%20extensive%20downstream%20tasks%20demonstrate%20the%20superior%0Aperformance%20of%20our%20approach.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/youjibiying/SubGDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05665v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


