<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260218.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Survey: Spatiotemporal Consistency in Video Generation", "author": "Zhiyu Yin and Kehai Chen and Xuefeng Bai and Ruili Jiang and Juntao Li and Hongdong Li and Jin Liu and Yang Xiang and Jun Yu and Min Zhang", "abstract": "Video generation aims to produce temporally coherent sequences of visual frames, representing a pivotal advancement in Artificial Intelligence Generated Content (AIGC). Compared to static image generation, video generation poses unique challenges: it demands not only high-quality individual frames but also strong temporal coherence to ensure consistency throughout the spatiotemporal sequence. Although research addressing spatiotemporal consistency in video generation has increased in recent years, systematic reviews focusing on this core issue remain relatively scarce. To fill this gap, this paper views the video generation task as a sequential sampling process from a high-dimensional spatiotemporal distribution, and further discusses spatiotemporal consistency. We provide a systematic review of the latest advancements in the field. The content spans multiple dimensions including generation models, feature representations, generation frameworks, post-processing techniques, training strategies, benchmarks and evaluation metrics, with a particular focus on the mechanisms and effectiveness of various methods in maintaining spatiotemporal consistency. Finally, this paper explores future research directions and potential challenges in this field, aiming to provide valuable insights for advancing video generation technology. The project link is https://github.com/Yin-Z-Y/A-Survey-Spatiotemporal-Consistency-in-Video-Generation.", "link": "http://arxiv.org/abs/2502.17863v2", "date": "2026-02-18", "relevancy": 3.0824, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6419}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6076}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%3A%20Spatiotemporal%20Consistency%20in%20Video%20Generation&body=Title%3A%20A%20Survey%3A%20Spatiotemporal%20Consistency%20in%20Video%20Generation%0AAuthor%3A%20Zhiyu%20Yin%20and%20Kehai%20Chen%20and%20Xuefeng%20Bai%20and%20Ruili%20Jiang%20and%20Juntao%20Li%20and%20Hongdong%20Li%20and%20Jin%20Liu%20and%20Yang%20Xiang%20and%20Jun%20Yu%20and%20Min%20Zhang%0AAbstract%3A%20Video%20generation%20aims%20to%20produce%20temporally%20coherent%20sequences%20of%20visual%20frames%2C%20representing%20a%20pivotal%20advancement%20in%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29.%20Compared%20to%20static%20image%20generation%2C%20video%20generation%20poses%20unique%20challenges%3A%20it%20demands%20not%20only%20high-quality%20individual%20frames%20but%20also%20strong%20temporal%20coherence%20to%20ensure%20consistency%20throughout%20the%20spatiotemporal%20sequence.%20Although%20research%20addressing%20spatiotemporal%20consistency%20in%20video%20generation%20has%20increased%20in%20recent%20years%2C%20systematic%20reviews%20focusing%20on%20this%20core%20issue%20remain%20relatively%20scarce.%20To%20fill%20this%20gap%2C%20this%20paper%20views%20the%20video%20generation%20task%20as%20a%20sequential%20sampling%20process%20from%20a%20high-dimensional%20spatiotemporal%20distribution%2C%20and%20further%20discusses%20spatiotemporal%20consistency.%20We%20provide%20a%20systematic%20review%20of%20the%20latest%20advancements%20in%20the%20field.%20The%20content%20spans%20multiple%20dimensions%20including%20generation%20models%2C%20feature%20representations%2C%20generation%20frameworks%2C%20post-processing%20techniques%2C%20training%20strategies%2C%20benchmarks%20and%20evaluation%20metrics%2C%20with%20a%20particular%20focus%20on%20the%20mechanisms%20and%20effectiveness%20of%20various%20methods%20in%20maintaining%20spatiotemporal%20consistency.%20Finally%2C%20this%20paper%20explores%20future%20research%20directions%20and%20potential%20challenges%20in%20this%20field%2C%20aiming%20to%20provide%20valuable%20insights%20for%20advancing%20video%20generation%20technology.%20The%20project%20link%20is%20https%3A//github.com/Yin-Z-Y/A-Survey-Spatiotemporal-Consistency-in-Video-Generation.%0ALink%3A%20http%3A//arxiv.org/abs/2502.17863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%253A%2520Spatiotemporal%2520Consistency%2520in%2520Video%2520Generation%26entry.906535625%3DZhiyu%2520Yin%2520and%2520Kehai%2520Chen%2520and%2520Xuefeng%2520Bai%2520and%2520Ruili%2520Jiang%2520and%2520Juntao%2520Li%2520and%2520Hongdong%2520Li%2520and%2520Jin%2520Liu%2520and%2520Yang%2520Xiang%2520and%2520Jun%2520Yu%2520and%2520Min%2520Zhang%26entry.1292438233%3DVideo%2520generation%2520aims%2520to%2520produce%2520temporally%2520coherent%2520sequences%2520of%2520visual%2520frames%252C%2520representing%2520a%2520pivotal%2520advancement%2520in%2520Artificial%2520Intelligence%2520Generated%2520Content%2520%2528AIGC%2529.%2520Compared%2520to%2520static%2520image%2520generation%252C%2520video%2520generation%2520poses%2520unique%2520challenges%253A%2520it%2520demands%2520not%2520only%2520high-quality%2520individual%2520frames%2520but%2520also%2520strong%2520temporal%2520coherence%2520to%2520ensure%2520consistency%2520throughout%2520the%2520spatiotemporal%2520sequence.%2520Although%2520research%2520addressing%2520spatiotemporal%2520consistency%2520in%2520video%2520generation%2520has%2520increased%2520in%2520recent%2520years%252C%2520systematic%2520reviews%2520focusing%2520on%2520this%2520core%2520issue%2520remain%2520relatively%2520scarce.%2520To%2520fill%2520this%2520gap%252C%2520this%2520paper%2520views%2520the%2520video%2520generation%2520task%2520as%2520a%2520sequential%2520sampling%2520process%2520from%2520a%2520high-dimensional%2520spatiotemporal%2520distribution%252C%2520and%2520further%2520discusses%2520spatiotemporal%2520consistency.%2520We%2520provide%2520a%2520systematic%2520review%2520of%2520the%2520latest%2520advancements%2520in%2520the%2520field.%2520The%2520content%2520spans%2520multiple%2520dimensions%2520including%2520generation%2520models%252C%2520feature%2520representations%252C%2520generation%2520frameworks%252C%2520post-processing%2520techniques%252C%2520training%2520strategies%252C%2520benchmarks%2520and%2520evaluation%2520metrics%252C%2520with%2520a%2520particular%2520focus%2520on%2520the%2520mechanisms%2520and%2520effectiveness%2520of%2520various%2520methods%2520in%2520maintaining%2520spatiotemporal%2520consistency.%2520Finally%252C%2520this%2520paper%2520explores%2520future%2520research%2520directions%2520and%2520potential%2520challenges%2520in%2520this%2520field%252C%2520aiming%2520to%2520provide%2520valuable%2520insights%2520for%2520advancing%2520video%2520generation%2520technology.%2520The%2520project%2520link%2520is%2520https%253A//github.com/Yin-Z-Y/A-Survey-Spatiotemporal-Consistency-in-Video-Generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%3A%20Spatiotemporal%20Consistency%20in%20Video%20Generation&entry.906535625=Zhiyu%20Yin%20and%20Kehai%20Chen%20and%20Xuefeng%20Bai%20and%20Ruili%20Jiang%20and%20Juntao%20Li%20and%20Hongdong%20Li%20and%20Jin%20Liu%20and%20Yang%20Xiang%20and%20Jun%20Yu%20and%20Min%20Zhang&entry.1292438233=Video%20generation%20aims%20to%20produce%20temporally%20coherent%20sequences%20of%20visual%20frames%2C%20representing%20a%20pivotal%20advancement%20in%20Artificial%20Intelligence%20Generated%20Content%20%28AIGC%29.%20Compared%20to%20static%20image%20generation%2C%20video%20generation%20poses%20unique%20challenges%3A%20it%20demands%20not%20only%20high-quality%20individual%20frames%20but%20also%20strong%20temporal%20coherence%20to%20ensure%20consistency%20throughout%20the%20spatiotemporal%20sequence.%20Although%20research%20addressing%20spatiotemporal%20consistency%20in%20video%20generation%20has%20increased%20in%20recent%20years%2C%20systematic%20reviews%20focusing%20on%20this%20core%20issue%20remain%20relatively%20scarce.%20To%20fill%20this%20gap%2C%20this%20paper%20views%20the%20video%20generation%20task%20as%20a%20sequential%20sampling%20process%20from%20a%20high-dimensional%20spatiotemporal%20distribution%2C%20and%20further%20discusses%20spatiotemporal%20consistency.%20We%20provide%20a%20systematic%20review%20of%20the%20latest%20advancements%20in%20the%20field.%20The%20content%20spans%20multiple%20dimensions%20including%20generation%20models%2C%20feature%20representations%2C%20generation%20frameworks%2C%20post-processing%20techniques%2C%20training%20strategies%2C%20benchmarks%20and%20evaluation%20metrics%2C%20with%20a%20particular%20focus%20on%20the%20mechanisms%20and%20effectiveness%20of%20various%20methods%20in%20maintaining%20spatiotemporal%20consistency.%20Finally%2C%20this%20paper%20explores%20future%20research%20directions%20and%20potential%20challenges%20in%20this%20field%2C%20aiming%20to%20provide%20valuable%20insights%20for%20advancing%20video%20generation%20technology.%20The%20project%20link%20is%20https%3A//github.com/Yin-Z-Y/A-Survey-Spatiotemporal-Consistency-in-Video-Generation.&entry.1838667208=http%3A//arxiv.org/abs/2502.17863v2&entry.124074799=Read"},
{"title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification", "author": "Qi You and Yitai Cheng and Zichao Zeng and James Haworth", "abstract": "Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.", "link": "http://arxiv.org/abs/2602.16590v1", "date": "2026-02-18", "relevancy": 2.8881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6279}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5584}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Contrastive%20Learning%20Framework%20Empowered%20by%20Attention-based%20Feature%20Adaptation%20for%20Street-View%20Image%20Classification&body=Title%3A%20A%20Contrastive%20Learning%20Framework%20Empowered%20by%20Attention-based%20Feature%20Adaptation%20for%20Street-View%20Image%20Classification%0AAuthor%3A%20Qi%20You%20and%20Yitai%20Cheng%20and%20Zichao%20Zeng%20and%20James%20Haworth%0AAbstract%3A%20Street-view%20image%20attribute%20classification%20is%20a%20vital%20downstream%20task%20of%20image%20classification%2C%20enabling%20applications%20such%20as%20autonomous%20driving%2C%20urban%20analytics%2C%20and%20high-definition%20map%20construction.%20It%20remains%20computationally%20demanding%20whether%20training%20from%20scratch%2C%20initialising%20from%20pre-trained%20weights%2C%20or%20fine-tuning%20large%20models.%20Although%20pre-trained%20vision-language%20models%20such%20as%20CLIP%20offer%20rich%20image%20representations%2C%20existing%20adaptation%20or%20fine-tuning%20methods%20often%20rely%20on%20their%20global%20image%20embeddings%2C%20limiting%20their%20ability%20to%20capture%20fine-grained%2C%20localised%20attributes%20essential%20in%20complex%2C%20cluttered%20street%20scenes.%20To%20address%20this%2C%20we%20propose%20CLIP-MHAdapter%2C%20a%20variant%20of%20the%20current%20lightweight%20CLIP%20adaptation%20paradigm%20that%20appends%20a%20bottleneck%20MLP%20equipped%20with%20multi-head%20self-attention%20operating%20on%20patch%20tokens%20to%20model%20inter-patch%20dependencies.%20With%20approximately%201.4%20million%20trainable%20parameters%2C%20CLIP-MHAdapter%20achieves%20superior%20or%20competitive%20accuracy%20across%20eight%20attribute%20classification%20tasks%20on%20the%20Global%20StreetScapes%20dataset%2C%20attaining%20new%20state-of-the-art%20results%20while%20maintaining%20low%20computational%20cost.%20The%20code%20is%20available%20at%20https%3A//github.com/SpaceTimeLab/CLIP-MHAdapter.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Contrastive%2520Learning%2520Framework%2520Empowered%2520by%2520Attention-based%2520Feature%2520Adaptation%2520for%2520Street-View%2520Image%2520Classification%26entry.906535625%3DQi%2520You%2520and%2520Yitai%2520Cheng%2520and%2520Zichao%2520Zeng%2520and%2520James%2520Haworth%26entry.1292438233%3DStreet-view%2520image%2520attribute%2520classification%2520is%2520a%2520vital%2520downstream%2520task%2520of%2520image%2520classification%252C%2520enabling%2520applications%2520such%2520as%2520autonomous%2520driving%252C%2520urban%2520analytics%252C%2520and%2520high-definition%2520map%2520construction.%2520It%2520remains%2520computationally%2520demanding%2520whether%2520training%2520from%2520scratch%252C%2520initialising%2520from%2520pre-trained%2520weights%252C%2520or%2520fine-tuning%2520large%2520models.%2520Although%2520pre-trained%2520vision-language%2520models%2520such%2520as%2520CLIP%2520offer%2520rich%2520image%2520representations%252C%2520existing%2520adaptation%2520or%2520fine-tuning%2520methods%2520often%2520rely%2520on%2520their%2520global%2520image%2520embeddings%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520fine-grained%252C%2520localised%2520attributes%2520essential%2520in%2520complex%252C%2520cluttered%2520street%2520scenes.%2520To%2520address%2520this%252C%2520we%2520propose%2520CLIP-MHAdapter%252C%2520a%2520variant%2520of%2520the%2520current%2520lightweight%2520CLIP%2520adaptation%2520paradigm%2520that%2520appends%2520a%2520bottleneck%2520MLP%2520equipped%2520with%2520multi-head%2520self-attention%2520operating%2520on%2520patch%2520tokens%2520to%2520model%2520inter-patch%2520dependencies.%2520With%2520approximately%25201.4%2520million%2520trainable%2520parameters%252C%2520CLIP-MHAdapter%2520achieves%2520superior%2520or%2520competitive%2520accuracy%2520across%2520eight%2520attribute%2520classification%2520tasks%2520on%2520the%2520Global%2520StreetScapes%2520dataset%252C%2520attaining%2520new%2520state-of-the-art%2520results%2520while%2520maintaining%2520low%2520computational%2520cost.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/SpaceTimeLab/CLIP-MHAdapter.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Contrastive%20Learning%20Framework%20Empowered%20by%20Attention-based%20Feature%20Adaptation%20for%20Street-View%20Image%20Classification&entry.906535625=Qi%20You%20and%20Yitai%20Cheng%20and%20Zichao%20Zeng%20and%20James%20Haworth&entry.1292438233=Street-view%20image%20attribute%20classification%20is%20a%20vital%20downstream%20task%20of%20image%20classification%2C%20enabling%20applications%20such%20as%20autonomous%20driving%2C%20urban%20analytics%2C%20and%20high-definition%20map%20construction.%20It%20remains%20computationally%20demanding%20whether%20training%20from%20scratch%2C%20initialising%20from%20pre-trained%20weights%2C%20or%20fine-tuning%20large%20models.%20Although%20pre-trained%20vision-language%20models%20such%20as%20CLIP%20offer%20rich%20image%20representations%2C%20existing%20adaptation%20or%20fine-tuning%20methods%20often%20rely%20on%20their%20global%20image%20embeddings%2C%20limiting%20their%20ability%20to%20capture%20fine-grained%2C%20localised%20attributes%20essential%20in%20complex%2C%20cluttered%20street%20scenes.%20To%20address%20this%2C%20we%20propose%20CLIP-MHAdapter%2C%20a%20variant%20of%20the%20current%20lightweight%20CLIP%20adaptation%20paradigm%20that%20appends%20a%20bottleneck%20MLP%20equipped%20with%20multi-head%20self-attention%20operating%20on%20patch%20tokens%20to%20model%20inter-patch%20dependencies.%20With%20approximately%201.4%20million%20trainable%20parameters%2C%20CLIP-MHAdapter%20achieves%20superior%20or%20competitive%20accuracy%20across%20eight%20attribute%20classification%20tasks%20on%20the%20Global%20StreetScapes%20dataset%2C%20attaining%20new%20state-of-the-art%20results%20while%20maintaining%20low%20computational%20cost.%20The%20code%20is%20available%20at%20https%3A//github.com/SpaceTimeLab/CLIP-MHAdapter.&entry.1838667208=http%3A//arxiv.org/abs/2602.16590v1&entry.124074799=Read"},
{"title": "Markerless 6D Pose Estimation and Position-Based Visual Servoing for Endoscopic Continuum Manipulators", "author": "Junhyun Park and Chunggil An and Myeongbo Park and Ihsan Ullah and Sihyeong Park and Minho Hwang", "abstract": "Continuum manipulators in flexible endoscopic surgical systems offer high dexterity for minimally invasive procedures; however, accurate pose estimation and closed-loop control remain challenging due to hysteresis, compliance, and limited distal sensing. Vision-based approaches reduce hardware complexity but are often constrained by limited geometric observability and high computational overhead, restricting real-time closed-loop applicability. This paper presents a unified framework for markerless stereo 6D pose estimation and position-based visual servoing of continuum manipulators. A photo-realistic simulation pipeline enables large-scale automatic training with pixel-accurate annotations. A stereo-aware multi-feature fusion network jointly exploits segmentation masks, keypoints, heatmaps, and bounding boxes to enhance geometric observability. To enforce geometric consistency without iterative optimization, a feed-forward rendering-based refinement module predicts residual pose corrections in a single pass. A self-supervised sim-to-real adaptation strategy further improves real-world performance using unlabeled data. Extensive real-world validation achieves a mean translation error of 0.83 mm and a mean rotation error of 2.76\u00b0 across 1,000 samples. Markerless closed-loop visual servoing driven by the estimated pose attains accurate trajectory tracking with a mean translation error of 2.07 mm and a mean rotation error of 7.41\u00b0, corresponding to 85% and 59% reductions compared to open-loop control, together with high repeatability in repeated point-reaching tasks. To the best of our knowledge, this work presents the first fully markerless pose-estimation-driven position-based visual servoing framework for continuum manipulators, enabling precise closed-loop control without physical markers or embedded sensing.", "link": "http://arxiv.org/abs/2602.16365v1", "date": "2026-02-18", "relevancy": 2.8837, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5807}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5798}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Markerless%206D%20Pose%20Estimation%20and%20Position-Based%20Visual%20Servoing%20for%20Endoscopic%20Continuum%20Manipulators&body=Title%3A%20Markerless%206D%20Pose%20Estimation%20and%20Position-Based%20Visual%20Servoing%20for%20Endoscopic%20Continuum%20Manipulators%0AAuthor%3A%20Junhyun%20Park%20and%20Chunggil%20An%20and%20Myeongbo%20Park%20and%20Ihsan%20Ullah%20and%20Sihyeong%20Park%20and%20Minho%20Hwang%0AAbstract%3A%20Continuum%20manipulators%20in%20flexible%20endoscopic%20surgical%20systems%20offer%20high%20dexterity%20for%20minimally%20invasive%20procedures%3B%20however%2C%20accurate%20pose%20estimation%20and%20closed-loop%20control%20remain%20challenging%20due%20to%20hysteresis%2C%20compliance%2C%20and%20limited%20distal%20sensing.%20Vision-based%20approaches%20reduce%20hardware%20complexity%20but%20are%20often%20constrained%20by%20limited%20geometric%20observability%20and%20high%20computational%20overhead%2C%20restricting%20real-time%20closed-loop%20applicability.%20This%20paper%20presents%20a%20unified%20framework%20for%20markerless%20stereo%206D%20pose%20estimation%20and%20position-based%20visual%20servoing%20of%20continuum%20manipulators.%20A%20photo-realistic%20simulation%20pipeline%20enables%20large-scale%20automatic%20training%20with%20pixel-accurate%20annotations.%20A%20stereo-aware%20multi-feature%20fusion%20network%20jointly%20exploits%20segmentation%20masks%2C%20keypoints%2C%20heatmaps%2C%20and%20bounding%20boxes%20to%20enhance%20geometric%20observability.%20To%20enforce%20geometric%20consistency%20without%20iterative%20optimization%2C%20a%20feed-forward%20rendering-based%20refinement%20module%20predicts%20residual%20pose%20corrections%20in%20a%20single%20pass.%20A%20self-supervised%20sim-to-real%20adaptation%20strategy%20further%20improves%20real-world%20performance%20using%20unlabeled%20data.%20Extensive%20real-world%20validation%20achieves%20a%20mean%20translation%20error%20of%200.83%20mm%20and%20a%20mean%20rotation%20error%20of%202.76%C2%B0%20across%201%2C000%20samples.%20Markerless%20closed-loop%20visual%20servoing%20driven%20by%20the%20estimated%20pose%20attains%20accurate%20trajectory%20tracking%20with%20a%20mean%20translation%20error%20of%202.07%20mm%20and%20a%20mean%20rotation%20error%20of%207.41%C2%B0%2C%20corresponding%20to%2085%25%20and%2059%25%20reductions%20compared%20to%20open-loop%20control%2C%20together%20with%20high%20repeatability%20in%20repeated%20point-reaching%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20presents%20the%20first%20fully%20markerless%20pose-estimation-driven%20position-based%20visual%20servoing%20framework%20for%20continuum%20manipulators%2C%20enabling%20precise%20closed-loop%20control%20without%20physical%20markers%20or%20embedded%20sensing.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarkerless%25206D%2520Pose%2520Estimation%2520and%2520Position-Based%2520Visual%2520Servoing%2520for%2520Endoscopic%2520Continuum%2520Manipulators%26entry.906535625%3DJunhyun%2520Park%2520and%2520Chunggil%2520An%2520and%2520Myeongbo%2520Park%2520and%2520Ihsan%2520Ullah%2520and%2520Sihyeong%2520Park%2520and%2520Minho%2520Hwang%26entry.1292438233%3DContinuum%2520manipulators%2520in%2520flexible%2520endoscopic%2520surgical%2520systems%2520offer%2520high%2520dexterity%2520for%2520minimally%2520invasive%2520procedures%253B%2520however%252C%2520accurate%2520pose%2520estimation%2520and%2520closed-loop%2520control%2520remain%2520challenging%2520due%2520to%2520hysteresis%252C%2520compliance%252C%2520and%2520limited%2520distal%2520sensing.%2520Vision-based%2520approaches%2520reduce%2520hardware%2520complexity%2520but%2520are%2520often%2520constrained%2520by%2520limited%2520geometric%2520observability%2520and%2520high%2520computational%2520overhead%252C%2520restricting%2520real-time%2520closed-loop%2520applicability.%2520This%2520paper%2520presents%2520a%2520unified%2520framework%2520for%2520markerless%2520stereo%25206D%2520pose%2520estimation%2520and%2520position-based%2520visual%2520servoing%2520of%2520continuum%2520manipulators.%2520A%2520photo-realistic%2520simulation%2520pipeline%2520enables%2520large-scale%2520automatic%2520training%2520with%2520pixel-accurate%2520annotations.%2520A%2520stereo-aware%2520multi-feature%2520fusion%2520network%2520jointly%2520exploits%2520segmentation%2520masks%252C%2520keypoints%252C%2520heatmaps%252C%2520and%2520bounding%2520boxes%2520to%2520enhance%2520geometric%2520observability.%2520To%2520enforce%2520geometric%2520consistency%2520without%2520iterative%2520optimization%252C%2520a%2520feed-forward%2520rendering-based%2520refinement%2520module%2520predicts%2520residual%2520pose%2520corrections%2520in%2520a%2520single%2520pass.%2520A%2520self-supervised%2520sim-to-real%2520adaptation%2520strategy%2520further%2520improves%2520real-world%2520performance%2520using%2520unlabeled%2520data.%2520Extensive%2520real-world%2520validation%2520achieves%2520a%2520mean%2520translation%2520error%2520of%25200.83%2520mm%2520and%2520a%2520mean%2520rotation%2520error%2520of%25202.76%25C2%25B0%2520across%25201%252C000%2520samples.%2520Markerless%2520closed-loop%2520visual%2520servoing%2520driven%2520by%2520the%2520estimated%2520pose%2520attains%2520accurate%2520trajectory%2520tracking%2520with%2520a%2520mean%2520translation%2520error%2520of%25202.07%2520mm%2520and%2520a%2520mean%2520rotation%2520error%2520of%25207.41%25C2%25B0%252C%2520corresponding%2520to%252085%2525%2520and%252059%2525%2520reductions%2520compared%2520to%2520open-loop%2520control%252C%2520together%2520with%2520high%2520repeatability%2520in%2520repeated%2520point-reaching%2520tasks.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520presents%2520the%2520first%2520fully%2520markerless%2520pose-estimation-driven%2520position-based%2520visual%2520servoing%2520framework%2520for%2520continuum%2520manipulators%252C%2520enabling%2520precise%2520closed-loop%2520control%2520without%2520physical%2520markers%2520or%2520embedded%2520sensing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Markerless%206D%20Pose%20Estimation%20and%20Position-Based%20Visual%20Servoing%20for%20Endoscopic%20Continuum%20Manipulators&entry.906535625=Junhyun%20Park%20and%20Chunggil%20An%20and%20Myeongbo%20Park%20and%20Ihsan%20Ullah%20and%20Sihyeong%20Park%20and%20Minho%20Hwang&entry.1292438233=Continuum%20manipulators%20in%20flexible%20endoscopic%20surgical%20systems%20offer%20high%20dexterity%20for%20minimally%20invasive%20procedures%3B%20however%2C%20accurate%20pose%20estimation%20and%20closed-loop%20control%20remain%20challenging%20due%20to%20hysteresis%2C%20compliance%2C%20and%20limited%20distal%20sensing.%20Vision-based%20approaches%20reduce%20hardware%20complexity%20but%20are%20often%20constrained%20by%20limited%20geometric%20observability%20and%20high%20computational%20overhead%2C%20restricting%20real-time%20closed-loop%20applicability.%20This%20paper%20presents%20a%20unified%20framework%20for%20markerless%20stereo%206D%20pose%20estimation%20and%20position-based%20visual%20servoing%20of%20continuum%20manipulators.%20A%20photo-realistic%20simulation%20pipeline%20enables%20large-scale%20automatic%20training%20with%20pixel-accurate%20annotations.%20A%20stereo-aware%20multi-feature%20fusion%20network%20jointly%20exploits%20segmentation%20masks%2C%20keypoints%2C%20heatmaps%2C%20and%20bounding%20boxes%20to%20enhance%20geometric%20observability.%20To%20enforce%20geometric%20consistency%20without%20iterative%20optimization%2C%20a%20feed-forward%20rendering-based%20refinement%20module%20predicts%20residual%20pose%20corrections%20in%20a%20single%20pass.%20A%20self-supervised%20sim-to-real%20adaptation%20strategy%20further%20improves%20real-world%20performance%20using%20unlabeled%20data.%20Extensive%20real-world%20validation%20achieves%20a%20mean%20translation%20error%20of%200.83%20mm%20and%20a%20mean%20rotation%20error%20of%202.76%C2%B0%20across%201%2C000%20samples.%20Markerless%20closed-loop%20visual%20servoing%20driven%20by%20the%20estimated%20pose%20attains%20accurate%20trajectory%20tracking%20with%20a%20mean%20translation%20error%20of%202.07%20mm%20and%20a%20mean%20rotation%20error%20of%207.41%C2%B0%2C%20corresponding%20to%2085%25%20and%2059%25%20reductions%20compared%20to%20open-loop%20control%2C%20together%20with%20high%20repeatability%20in%20repeated%20point-reaching%20tasks.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20presents%20the%20first%20fully%20markerless%20pose-estimation-driven%20position-based%20visual%20servoing%20framework%20for%20continuum%20manipulators%2C%20enabling%20precise%20closed-loop%20control%20without%20physical%20markers%20or%20embedded%20sensing.&entry.1838667208=http%3A//arxiv.org/abs/2602.16365v1&entry.124074799=Read"},
{"title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "author": "Josh Qixuan Sun and Xiaoying Xing and Huaiyuan Weng and Chul Min Yeum and Mark Crowley", "abstract": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.", "link": "http://arxiv.org/abs/2507.08831v3", "date": "2026-02-18", "relevancy": 2.8492, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20View%20Invariant%20Learning%20for%20Vision-Language%20Navigation%20in%20Continuous%20Environments&body=Title%3A%20View%20Invariant%20Learning%20for%20Vision-Language%20Navigation%20in%20Continuous%20Environments%0AAuthor%3A%20Josh%20Qixuan%20Sun%20and%20Xiaoying%20Xing%20and%20Huaiyuan%20Weng%20and%20Chul%20Min%20Yeum%20and%20Mark%20Crowley%0AAbstract%3A%20Vision-Language%20Navigation%20in%20Continuous%20Environments%20%28VLNCE%29%2C%20where%20an%20agent%20follows%20instructions%20and%20moves%20freely%20to%20reach%20a%20destination%2C%20is%20a%20key%20research%20problem%20in%20embodied%20AI.%20However%2C%20most%20navigation%20policies%20are%20sensitive%20to%20viewpoint%20changes%2C%20i.e.%2C%20variations%20in%20camera%20height%20and%20viewing%20angle%20that%20alter%20the%20agent%27s%20observation.%20In%20this%20paper%2C%20we%20introduce%20a%20generalized%20scenario%2C%20V2-VLNCE%20%28VLNCE%20with%20Varied%20Viewpoints%29%2C%20and%20propose%20VIL%20%28View%20Invariant%20Learning%29%2C%20a%20view-invariant%20post-training%20strategy%20that%20enhances%20the%20robustness%20of%20existing%20navigation%20policies%20to%20changes%20in%20camera%20viewpoint.%20VIL%20employs%20a%20contrastive%20learning%20framework%20to%20learn%20sparse%20and%20view-invariant%20features.%20Additionally%2C%20we%20introduce%20a%20teacher-student%20framework%20for%20the%20Waypoint%20Predictor%20Module%2C%20a%20core%20component%20of%20most%20VLNCE%20baselines%2C%20where%20a%20view-dependent%20teacher%20model%20distills%20knowledge%20into%20a%20view-invariant%20student%20model.%20We%20employ%20an%20end-to-end%20training%20paradigm%20to%20jointly%20optimize%20these%20components%2C%20thus%20eliminating%20the%20cost%20for%20individual%20module%20training.%20Empirical%20results%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20on%20V2-VLNCE%20by%208-15%25%20measured%20on%20Success%20Rate%20for%20two%20standard%20benchmark%20datasets%20R2R-CE%20and%20RxR-CE.%20Furthermore%2C%20we%20evaluate%20VIL%20under%20the%20standard%20VLNCE%20setting%20and%20find%20that%2C%20despite%20being%20trained%20for%20varied%20viewpoints%2C%20it%20often%20still%20improves%20performance.%20On%20the%20more%20challenging%20RxR-CE%20dataset%2C%20our%20method%20also%20achieved%20state-of-the-art%20performance%20across%20all%20metrics%20when%20compared%20to%20other%20map-free%20methods.%20This%20suggests%20that%20adding%20VIL%20does%20not%20diminish%20the%20standard%20viewpoint%20performance%20and%20can%20serve%20as%20a%20plug-and-play%20post-training%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2507.08831v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DView%2520Invariant%2520Learning%2520for%2520Vision-Language%2520Navigation%2520in%2520Continuous%2520Environments%26entry.906535625%3DJosh%2520Qixuan%2520Sun%2520and%2520Xiaoying%2520Xing%2520and%2520Huaiyuan%2520Weng%2520and%2520Chul%2520Min%2520Yeum%2520and%2520Mark%2520Crowley%26entry.1292438233%3DVision-Language%2520Navigation%2520in%2520Continuous%2520Environments%2520%2528VLNCE%2529%252C%2520where%2520an%2520agent%2520follows%2520instructions%2520and%2520moves%2520freely%2520to%2520reach%2520a%2520destination%252C%2520is%2520a%2520key%2520research%2520problem%2520in%2520embodied%2520AI.%2520However%252C%2520most%2520navigation%2520policies%2520are%2520sensitive%2520to%2520viewpoint%2520changes%252C%2520i.e.%252C%2520variations%2520in%2520camera%2520height%2520and%2520viewing%2520angle%2520that%2520alter%2520the%2520agent%2527s%2520observation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520generalized%2520scenario%252C%2520V2-VLNCE%2520%2528VLNCE%2520with%2520Varied%2520Viewpoints%2529%252C%2520and%2520propose%2520VIL%2520%2528View%2520Invariant%2520Learning%2529%252C%2520a%2520view-invariant%2520post-training%2520strategy%2520that%2520enhances%2520the%2520robustness%2520of%2520existing%2520navigation%2520policies%2520to%2520changes%2520in%2520camera%2520viewpoint.%2520VIL%2520employs%2520a%2520contrastive%2520learning%2520framework%2520to%2520learn%2520sparse%2520and%2520view-invariant%2520features.%2520Additionally%252C%2520we%2520introduce%2520a%2520teacher-student%2520framework%2520for%2520the%2520Waypoint%2520Predictor%2520Module%252C%2520a%2520core%2520component%2520of%2520most%2520VLNCE%2520baselines%252C%2520where%2520a%2520view-dependent%2520teacher%2520model%2520distills%2520knowledge%2520into%2520a%2520view-invariant%2520student%2520model.%2520We%2520employ%2520an%2520end-to-end%2520training%2520paradigm%2520to%2520jointly%2520optimize%2520these%2520components%252C%2520thus%2520eliminating%2520the%2520cost%2520for%2520individual%2520module%2520training.%2520Empirical%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520approaches%2520on%2520V2-VLNCE%2520by%25208-15%2525%2520measured%2520on%2520Success%2520Rate%2520for%2520two%2520standard%2520benchmark%2520datasets%2520R2R-CE%2520and%2520RxR-CE.%2520Furthermore%252C%2520we%2520evaluate%2520VIL%2520under%2520the%2520standard%2520VLNCE%2520setting%2520and%2520find%2520that%252C%2520despite%2520being%2520trained%2520for%2520varied%2520viewpoints%252C%2520it%2520often%2520still%2520improves%2520performance.%2520On%2520the%2520more%2520challenging%2520RxR-CE%2520dataset%252C%2520our%2520method%2520also%2520achieved%2520state-of-the-art%2520performance%2520across%2520all%2520metrics%2520when%2520compared%2520to%2520other%2520map-free%2520methods.%2520This%2520suggests%2520that%2520adding%2520VIL%2520does%2520not%2520diminish%2520the%2520standard%2520viewpoint%2520performance%2520and%2520can%2520serve%2520as%2520a%2520plug-and-play%2520post-training%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08831v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=View%20Invariant%20Learning%20for%20Vision-Language%20Navigation%20in%20Continuous%20Environments&entry.906535625=Josh%20Qixuan%20Sun%20and%20Xiaoying%20Xing%20and%20Huaiyuan%20Weng%20and%20Chul%20Min%20Yeum%20and%20Mark%20Crowley&entry.1292438233=Vision-Language%20Navigation%20in%20Continuous%20Environments%20%28VLNCE%29%2C%20where%20an%20agent%20follows%20instructions%20and%20moves%20freely%20to%20reach%20a%20destination%2C%20is%20a%20key%20research%20problem%20in%20embodied%20AI.%20However%2C%20most%20navigation%20policies%20are%20sensitive%20to%20viewpoint%20changes%2C%20i.e.%2C%20variations%20in%20camera%20height%20and%20viewing%20angle%20that%20alter%20the%20agent%27s%20observation.%20In%20this%20paper%2C%20we%20introduce%20a%20generalized%20scenario%2C%20V2-VLNCE%20%28VLNCE%20with%20Varied%20Viewpoints%29%2C%20and%20propose%20VIL%20%28View%20Invariant%20Learning%29%2C%20a%20view-invariant%20post-training%20strategy%20that%20enhances%20the%20robustness%20of%20existing%20navigation%20policies%20to%20changes%20in%20camera%20viewpoint.%20VIL%20employs%20a%20contrastive%20learning%20framework%20to%20learn%20sparse%20and%20view-invariant%20features.%20Additionally%2C%20we%20introduce%20a%20teacher-student%20framework%20for%20the%20Waypoint%20Predictor%20Module%2C%20a%20core%20component%20of%20most%20VLNCE%20baselines%2C%20where%20a%20view-dependent%20teacher%20model%20distills%20knowledge%20into%20a%20view-invariant%20student%20model.%20We%20employ%20an%20end-to-end%20training%20paradigm%20to%20jointly%20optimize%20these%20components%2C%20thus%20eliminating%20the%20cost%20for%20individual%20module%20training.%20Empirical%20results%20show%20that%20our%20method%20outperforms%20state-of-the-art%20approaches%20on%20V2-VLNCE%20by%208-15%25%20measured%20on%20Success%20Rate%20for%20two%20standard%20benchmark%20datasets%20R2R-CE%20and%20RxR-CE.%20Furthermore%2C%20we%20evaluate%20VIL%20under%20the%20standard%20VLNCE%20setting%20and%20find%20that%2C%20despite%20being%20trained%20for%20varied%20viewpoints%2C%20it%20often%20still%20improves%20performance.%20On%20the%20more%20challenging%20RxR-CE%20dataset%2C%20our%20method%20also%20achieved%20state-of-the-art%20performance%20across%20all%20metrics%20when%20compared%20to%20other%20map-free%20methods.%20This%20suggests%20that%20adding%20VIL%20does%20not%20diminish%20the%20standard%20viewpoint%20performance%20and%20can%20serve%20as%20a%20plug-and-play%20post-training%20method.&entry.1838667208=http%3A//arxiv.org/abs/2507.08831v3&entry.124074799=Read"},
{"title": "DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images", "author": "Zeng Tao and Ying Jiang and Yunuo Chen and Tianyi Xie and Huamin Wang and Yingnian Wu and Yin Yang and Abishek Sampath Kumar and Kenji Tashiro and Chenfanfu Jiang", "abstract": "Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.", "link": "http://arxiv.org/abs/2602.16502v1", "date": "2026-02-18", "relevancy": 2.7768, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7318}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6847}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DressWild%3A%20Feed-Forward%20Pose-Agnostic%20Garment%20Sewing%20Pattern%20Generation%20from%20In-the-Wild%20Images&body=Title%3A%20DressWild%3A%20Feed-Forward%20Pose-Agnostic%20Garment%20Sewing%20Pattern%20Generation%20from%20In-the-Wild%20Images%0AAuthor%3A%20Zeng%20Tao%20and%20Ying%20Jiang%20and%20Yunuo%20Chen%20and%20Tianyi%20Xie%20and%20Huamin%20Wang%20and%20Yingnian%20Wu%20and%20Yin%20Yang%20and%20Abishek%20Sampath%20Kumar%20and%20Kenji%20Tashiro%20and%20Chenfanfu%20Jiang%0AAbstract%3A%20Recent%20advances%20in%20garment%20pattern%20generation%20have%20shown%20promising%20progress.%20However%2C%20existing%20feed-forward%20methods%20struggle%20with%20diverse%20poses%20and%20viewpoints%2C%20while%20optimization-based%20approaches%20are%20computationally%20expensive%20and%20difficult%20to%20scale.%20This%20paper%20focuses%20on%20sewing%20pattern%20generation%20for%20garment%20modeling%20and%20fabrication%20applications%20that%20demand%20editable%2C%20separable%2C%20and%20simulation-ready%20garments.%20We%20propose%20DressWild%2C%20a%20novel%20feed-forward%20pipeline%20that%20reconstructs%20physics-consistent%202D%20sewing%20patterns%20and%20the%20corresponding%203D%20garments%20from%20a%20single%20in-the-wild%20image.%20Given%20an%20input%20image%2C%20our%20method%20leverages%20vision-language%20models%20%28VLMs%29%20to%20normalize%20pose%20variations%20at%20the%20image%20level%2C%20then%20extract%20pose-aware%2C%203D-informed%20garment%20features.%20These%20features%20are%20fused%20through%20a%20transformer-based%20encoder%20and%20subsequently%20used%20to%20predict%20sewing%20pattern%20parameters%2C%20which%20can%20be%20directly%20applied%20to%20physical%20simulation%2C%20texture%20synthesis%2C%20and%20multi-layer%20virtual%20try-on.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20robustly%20recovers%20diverse%20sewing%20patterns%20and%20the%20corresponding%203D%20garments%20from%20in-the-wild%20images%20without%20requiring%20multi-view%20inputs%20or%20iterative%20optimization%2C%20offering%20an%20efficient%20and%20scalable%20solution%20for%20realistic%20garment%20simulation%20and%20animation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDressWild%253A%2520Feed-Forward%2520Pose-Agnostic%2520Garment%2520Sewing%2520Pattern%2520Generation%2520from%2520In-the-Wild%2520Images%26entry.906535625%3DZeng%2520Tao%2520and%2520Ying%2520Jiang%2520and%2520Yunuo%2520Chen%2520and%2520Tianyi%2520Xie%2520and%2520Huamin%2520Wang%2520and%2520Yingnian%2520Wu%2520and%2520Yin%2520Yang%2520and%2520Abishek%2520Sampath%2520Kumar%2520and%2520Kenji%2520Tashiro%2520and%2520Chenfanfu%2520Jiang%26entry.1292438233%3DRecent%2520advances%2520in%2520garment%2520pattern%2520generation%2520have%2520shown%2520promising%2520progress.%2520However%252C%2520existing%2520feed-forward%2520methods%2520struggle%2520with%2520diverse%2520poses%2520and%2520viewpoints%252C%2520while%2520optimization-based%2520approaches%2520are%2520computationally%2520expensive%2520and%2520difficult%2520to%2520scale.%2520This%2520paper%2520focuses%2520on%2520sewing%2520pattern%2520generation%2520for%2520garment%2520modeling%2520and%2520fabrication%2520applications%2520that%2520demand%2520editable%252C%2520separable%252C%2520and%2520simulation-ready%2520garments.%2520We%2520propose%2520DressWild%252C%2520a%2520novel%2520feed-forward%2520pipeline%2520that%2520reconstructs%2520physics-consistent%25202D%2520sewing%2520patterns%2520and%2520the%2520corresponding%25203D%2520garments%2520from%2520a%2520single%2520in-the-wild%2520image.%2520Given%2520an%2520input%2520image%252C%2520our%2520method%2520leverages%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520normalize%2520pose%2520variations%2520at%2520the%2520image%2520level%252C%2520then%2520extract%2520pose-aware%252C%25203D-informed%2520garment%2520features.%2520These%2520features%2520are%2520fused%2520through%2520a%2520transformer-based%2520encoder%2520and%2520subsequently%2520used%2520to%2520predict%2520sewing%2520pattern%2520parameters%252C%2520which%2520can%2520be%2520directly%2520applied%2520to%2520physical%2520simulation%252C%2520texture%2520synthesis%252C%2520and%2520multi-layer%2520virtual%2520try-on.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520robustly%2520recovers%2520diverse%2520sewing%2520patterns%2520and%2520the%2520corresponding%25203D%2520garments%2520from%2520in-the-wild%2520images%2520without%2520requiring%2520multi-view%2520inputs%2520or%2520iterative%2520optimization%252C%2520offering%2520an%2520efficient%2520and%2520scalable%2520solution%2520for%2520realistic%2520garment%2520simulation%2520and%2520animation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DressWild%3A%20Feed-Forward%20Pose-Agnostic%20Garment%20Sewing%20Pattern%20Generation%20from%20In-the-Wild%20Images&entry.906535625=Zeng%20Tao%20and%20Ying%20Jiang%20and%20Yunuo%20Chen%20and%20Tianyi%20Xie%20and%20Huamin%20Wang%20and%20Yingnian%20Wu%20and%20Yin%20Yang%20and%20Abishek%20Sampath%20Kumar%20and%20Kenji%20Tashiro%20and%20Chenfanfu%20Jiang&entry.1292438233=Recent%20advances%20in%20garment%20pattern%20generation%20have%20shown%20promising%20progress.%20However%2C%20existing%20feed-forward%20methods%20struggle%20with%20diverse%20poses%20and%20viewpoints%2C%20while%20optimization-based%20approaches%20are%20computationally%20expensive%20and%20difficult%20to%20scale.%20This%20paper%20focuses%20on%20sewing%20pattern%20generation%20for%20garment%20modeling%20and%20fabrication%20applications%20that%20demand%20editable%2C%20separable%2C%20and%20simulation-ready%20garments.%20We%20propose%20DressWild%2C%20a%20novel%20feed-forward%20pipeline%20that%20reconstructs%20physics-consistent%202D%20sewing%20patterns%20and%20the%20corresponding%203D%20garments%20from%20a%20single%20in-the-wild%20image.%20Given%20an%20input%20image%2C%20our%20method%20leverages%20vision-language%20models%20%28VLMs%29%20to%20normalize%20pose%20variations%20at%20the%20image%20level%2C%20then%20extract%20pose-aware%2C%203D-informed%20garment%20features.%20These%20features%20are%20fused%20through%20a%20transformer-based%20encoder%20and%20subsequently%20used%20to%20predict%20sewing%20pattern%20parameters%2C%20which%20can%20be%20directly%20applied%20to%20physical%20simulation%2C%20texture%20synthesis%2C%20and%20multi-layer%20virtual%20try-on.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20robustly%20recovers%20diverse%20sewing%20patterns%20and%20the%20corresponding%203D%20garments%20from%20in-the-wild%20images%20without%20requiring%20multi-view%20inputs%20or%20iterative%20optimization%2C%20offering%20an%20efficient%20and%20scalable%20solution%20for%20realistic%20garment%20simulation%20and%20animation.&entry.1838667208=http%3A//arxiv.org/abs/2602.16502v1&entry.124074799=Read"},
{"title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model", "author": "Ruichuan An and Sihan Yang and Renrui Zhang and Ming Lu and Tianyi Jiang and Kai Zeng and Yulin Luo and Jiajun Cao and Hao Liang and Ying Chen and Qi She and Shanghang Zhang and Wentao Zhang", "abstract": "Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes MC-LLaVA, a multi-concept personalization paradigm. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the training costs, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location maps for enhanced recognition and grounding capabilities. To further push the performance upper bound, we incorporate an optional auxiliary loss, better enhancing the proposed personalized prompts. To decorate the VLM personalization research, we contribute a high-quality dataset. We carefully collect images with multiple characters and objects from movies and manually create question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive experiments demonstrate that MC-LLaVA achieves impressive multi-concept personalized responses, paving the way for VLMs to become better user assistants. The code and dataset will be released at \\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}.", "link": "http://arxiv.org/abs/2411.11706v4", "date": "2026-02-18", "relevancy": 2.764, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MC-LLaVA%3A%20Multi-Concept%20Personalized%20Vision-Language%20Model&body=Title%3A%20MC-LLaVA%3A%20Multi-Concept%20Personalized%20Vision-Language%20Model%0AAuthor%3A%20Ruichuan%20An%20and%20Sihan%20Yang%20and%20Renrui%20Zhang%20and%20Ming%20Lu%20and%20Tianyi%20Jiang%20and%20Kai%20Zeng%20and%20Yulin%20Luo%20and%20Jiajun%20Cao%20and%20Hao%20Liang%20and%20Ying%20Chen%20and%20Qi%20She%20and%20Shanghang%20Zhang%20and%20Wentao%20Zhang%0AAbstract%3A%20Current%20vision-language%20models%20%28VLMs%29%20show%20exceptional%20abilities%20across%20diverse%20tasks%2C%20such%20as%20visual%20question%20answering.%20To%20enhance%20user%20experience%2C%20recent%20studies%20have%20investigated%20VLM%20personalization%20to%20understand%20user-provided%20concepts.%20However%2C%20they%20mainly%20focus%20on%20single%20concepts%2C%20neglecting%20the%20existence%20and%20interplay%20of%20multiple%20concepts%2C%20which%20limits%20real-world%20applicability.%20This%20paper%20proposes%20MC-LLaVA%2C%20a%20multi-concept%20personalization%20paradigm.%20Specifically%2C%20MC-LLaVA%20employs%20a%20multi-concept%20instruction%20tuning%20strategy%2C%20effectively%20integrating%20multiple%20concepts%20in%20a%20single%20training%20step.%20To%20reduce%20the%20training%20costs%2C%20we%20propose%20a%20personalized%20textual%20prompt%20that%20uses%20visual%20token%20information%20to%20initialize%20concept%20tokens.%20Additionally%2C%20we%20introduce%20a%20personalized%20visual%20prompt%20during%20inference%2C%20aggregating%20location%20maps%20for%20enhanced%20recognition%20and%20grounding%20capabilities.%20To%20further%20push%20the%20performance%20upper%20bound%2C%20we%20incorporate%20an%20optional%20auxiliary%20loss%2C%20better%20enhancing%20the%20proposed%20personalized%20prompts.%20To%20decorate%20the%20VLM%20personalization%20research%2C%20we%20contribute%20a%20high-quality%20dataset.%20We%20carefully%20collect%20images%20with%20multiple%20characters%20and%20objects%20from%20movies%20and%20manually%20create%20question-answer%20samples%20for%20multi-concept%20scenarios%2C%20featuring%20superior%20diversity.%20Comprehensive%20experiments%20demonstrate%20that%20MC-LLaVA%20achieves%20impressive%20multi-concept%20personalized%20responses%2C%20paving%20the%20way%20for%20VLMs%20to%20become%20better%20user%20assistants.%20The%20code%20and%20dataset%20will%20be%20released%20at%20%5Chref%7Bhttps%3A//github.com/arctanxarc/MC-LLaVA%7D%7Bhttps%3A//github.com/arctanxarc/MC-LLaVA%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2411.11706v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMC-LLaVA%253A%2520Multi-Concept%2520Personalized%2520Vision-Language%2520Model%26entry.906535625%3DRuichuan%2520An%2520and%2520Sihan%2520Yang%2520and%2520Renrui%2520Zhang%2520and%2520Ming%2520Lu%2520and%2520Tianyi%2520Jiang%2520and%2520Kai%2520Zeng%2520and%2520Yulin%2520Luo%2520and%2520Jiajun%2520Cao%2520and%2520Hao%2520Liang%2520and%2520Ying%2520Chen%2520and%2520Qi%2520She%2520and%2520Shanghang%2520Zhang%2520and%2520Wentao%2520Zhang%26entry.1292438233%3DCurrent%2520vision-language%2520models%2520%2528VLMs%2529%2520show%2520exceptional%2520abilities%2520across%2520diverse%2520tasks%252C%2520such%2520as%2520visual%2520question%2520answering.%2520To%2520enhance%2520user%2520experience%252C%2520recent%2520studies%2520have%2520investigated%2520VLM%2520personalization%2520to%2520understand%2520user-provided%2520concepts.%2520However%252C%2520they%2520mainly%2520focus%2520on%2520single%2520concepts%252C%2520neglecting%2520the%2520existence%2520and%2520interplay%2520of%2520multiple%2520concepts%252C%2520which%2520limits%2520real-world%2520applicability.%2520This%2520paper%2520proposes%2520MC-LLaVA%252C%2520a%2520multi-concept%2520personalization%2520paradigm.%2520Specifically%252C%2520MC-LLaVA%2520employs%2520a%2520multi-concept%2520instruction%2520tuning%2520strategy%252C%2520effectively%2520integrating%2520multiple%2520concepts%2520in%2520a%2520single%2520training%2520step.%2520To%2520reduce%2520the%2520training%2520costs%252C%2520we%2520propose%2520a%2520personalized%2520textual%2520prompt%2520that%2520uses%2520visual%2520token%2520information%2520to%2520initialize%2520concept%2520tokens.%2520Additionally%252C%2520we%2520introduce%2520a%2520personalized%2520visual%2520prompt%2520during%2520inference%252C%2520aggregating%2520location%2520maps%2520for%2520enhanced%2520recognition%2520and%2520grounding%2520capabilities.%2520To%2520further%2520push%2520the%2520performance%2520upper%2520bound%252C%2520we%2520incorporate%2520an%2520optional%2520auxiliary%2520loss%252C%2520better%2520enhancing%2520the%2520proposed%2520personalized%2520prompts.%2520To%2520decorate%2520the%2520VLM%2520personalization%2520research%252C%2520we%2520contribute%2520a%2520high-quality%2520dataset.%2520We%2520carefully%2520collect%2520images%2520with%2520multiple%2520characters%2520and%2520objects%2520from%2520movies%2520and%2520manually%2520create%2520question-answer%2520samples%2520for%2520multi-concept%2520scenarios%252C%2520featuring%2520superior%2520diversity.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520MC-LLaVA%2520achieves%2520impressive%2520multi-concept%2520personalized%2520responses%252C%2520paving%2520the%2520way%2520for%2520VLMs%2520to%2520become%2520better%2520user%2520assistants.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520released%2520at%2520%255Chref%257Bhttps%253A//github.com/arctanxarc/MC-LLaVA%257D%257Bhttps%253A//github.com/arctanxarc/MC-LLaVA%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11706v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MC-LLaVA%3A%20Multi-Concept%20Personalized%20Vision-Language%20Model&entry.906535625=Ruichuan%20An%20and%20Sihan%20Yang%20and%20Renrui%20Zhang%20and%20Ming%20Lu%20and%20Tianyi%20Jiang%20and%20Kai%20Zeng%20and%20Yulin%20Luo%20and%20Jiajun%20Cao%20and%20Hao%20Liang%20and%20Ying%20Chen%20and%20Qi%20She%20and%20Shanghang%20Zhang%20and%20Wentao%20Zhang&entry.1292438233=Current%20vision-language%20models%20%28VLMs%29%20show%20exceptional%20abilities%20across%20diverse%20tasks%2C%20such%20as%20visual%20question%20answering.%20To%20enhance%20user%20experience%2C%20recent%20studies%20have%20investigated%20VLM%20personalization%20to%20understand%20user-provided%20concepts.%20However%2C%20they%20mainly%20focus%20on%20single%20concepts%2C%20neglecting%20the%20existence%20and%20interplay%20of%20multiple%20concepts%2C%20which%20limits%20real-world%20applicability.%20This%20paper%20proposes%20MC-LLaVA%2C%20a%20multi-concept%20personalization%20paradigm.%20Specifically%2C%20MC-LLaVA%20employs%20a%20multi-concept%20instruction%20tuning%20strategy%2C%20effectively%20integrating%20multiple%20concepts%20in%20a%20single%20training%20step.%20To%20reduce%20the%20training%20costs%2C%20we%20propose%20a%20personalized%20textual%20prompt%20that%20uses%20visual%20token%20information%20to%20initialize%20concept%20tokens.%20Additionally%2C%20we%20introduce%20a%20personalized%20visual%20prompt%20during%20inference%2C%20aggregating%20location%20maps%20for%20enhanced%20recognition%20and%20grounding%20capabilities.%20To%20further%20push%20the%20performance%20upper%20bound%2C%20we%20incorporate%20an%20optional%20auxiliary%20loss%2C%20better%20enhancing%20the%20proposed%20personalized%20prompts.%20To%20decorate%20the%20VLM%20personalization%20research%2C%20we%20contribute%20a%20high-quality%20dataset.%20We%20carefully%20collect%20images%20with%20multiple%20characters%20and%20objects%20from%20movies%20and%20manually%20create%20question-answer%20samples%20for%20multi-concept%20scenarios%2C%20featuring%20superior%20diversity.%20Comprehensive%20experiments%20demonstrate%20that%20MC-LLaVA%20achieves%20impressive%20multi-concept%20personalized%20responses%2C%20paving%20the%20way%20for%20VLMs%20to%20become%20better%20user%20assistants.%20The%20code%20and%20dataset%20will%20be%20released%20at%20%5Chref%7Bhttps%3A//github.com/arctanxarc/MC-LLaVA%7D%7Bhttps%3A//github.com/arctanxarc/MC-LLaVA%7D.&entry.1838667208=http%3A//arxiv.org/abs/2411.11706v4&entry.124074799=Read"},
{"title": "Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing", "author": "Jinsong Li and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Jiaqi Wang and Dahua Lin", "abstract": "While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.", "link": "http://arxiv.org/abs/2602.16455v1", "date": "2026-02-18", "relevancy": 2.7133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Self-Refine%3A%20A%20Pixel-Guided%20Paradigm%20for%20Accurate%20Chart%20Parsing&body=Title%3A%20Visual%20Self-Refine%3A%20A%20Pixel-Guided%20Paradigm%20for%20Accurate%20Chart%20Parsing%0AAuthor%3A%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin%0AAbstract%3A%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20capabilities%20for%20reasoning%20and%20self-correction%20at%20the%20textual%20level%2C%20these%20strengths%20provide%20minimal%20benefits%20for%20complex%20tasks%20centered%20on%20visual%20perception%2C%20such%20as%20Chart%20Parsing.%20Existing%20models%20often%20struggle%20with%20visually%20dense%20charts%2C%20leading%20to%20errors%20like%20data%20omission%2C%20misalignment%2C%20and%20hallucination.%20Inspired%20by%20the%20human%20strategy%20of%20using%20a%20finger%20as%20a%20%60%60visual%20anchor%27%27%20to%20ensure%20accuracy%20when%20reading%20complex%20charts%2C%20we%20propose%20a%20new%20paradigm%20named%20Visual%20Self-Refine%20%28VSR%29.%20The%20core%20idea%20of%20VSR%20is%20to%20enable%20a%20model%20to%20generate%20pixel-level%20localization%20outputs%2C%20visualize%20them%2C%20and%20then%20feed%20these%20visualizations%20back%20to%20itself%2C%20allowing%20it%20to%20intuitively%20inspect%20and%20correct%20its%20own%20potential%20visual%20perception%20errors.%20We%20instantiate%20the%20VSR%20paradigm%20in%20the%20domain%20of%20Chart%20Parsing%20by%20proposing%20ChartVSR.%20This%20model%20decomposes%20the%20parsing%20process%20into%20two%20stages%3A%20a%20Refine%20Stage%2C%20where%20it%20iteratively%20uses%20visual%20feedback%20to%20ensure%20the%20accuracy%20of%20all%20data%20points%27%20Pixel-level%20Localizations%2C%20and%20a%20Decode%20Stage%2C%20where%20it%20uses%20these%20verified%20localizations%20as%20precise%20visual%20anchors%20to%20parse%20the%20final%20structured%20data.%20To%20address%20the%20limitations%20of%20existing%20benchmarks%2C%20we%20also%20construct%20ChartP-Bench%2C%20a%20new%20and%20highly%20challenging%20benchmark%20for%20chart%20parsing.%20Our%20work%20also%20highlights%20VSR%20as%20a%20general-purpose%20visual%20feedback%20mechanism%2C%20offering%20a%20promising%20new%20direction%20for%20enhancing%20accuracy%20on%20a%20wide%20range%20of%20vision-centric%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Self-Refine%253A%2520A%2520Pixel-Guided%2520Paradigm%2520for%2520Accurate%2520Chart%2520Parsing%26entry.906535625%3DJinsong%2520Li%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Jiaqi%2520Wang%2520and%2520Dahua%2520Lin%26entry.1292438233%3DWhile%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520for%2520reasoning%2520and%2520self-correction%2520at%2520the%2520textual%2520level%252C%2520these%2520strengths%2520provide%2520minimal%2520benefits%2520for%2520complex%2520tasks%2520centered%2520on%2520visual%2520perception%252C%2520such%2520as%2520Chart%2520Parsing.%2520Existing%2520models%2520often%2520struggle%2520with%2520visually%2520dense%2520charts%252C%2520leading%2520to%2520errors%2520like%2520data%2520omission%252C%2520misalignment%252C%2520and%2520hallucination.%2520Inspired%2520by%2520the%2520human%2520strategy%2520of%2520using%2520a%2520finger%2520as%2520a%2520%2560%2560visual%2520anchor%2527%2527%2520to%2520ensure%2520accuracy%2520when%2520reading%2520complex%2520charts%252C%2520we%2520propose%2520a%2520new%2520paradigm%2520named%2520Visual%2520Self-Refine%2520%2528VSR%2529.%2520The%2520core%2520idea%2520of%2520VSR%2520is%2520to%2520enable%2520a%2520model%2520to%2520generate%2520pixel-level%2520localization%2520outputs%252C%2520visualize%2520them%252C%2520and%2520then%2520feed%2520these%2520visualizations%2520back%2520to%2520itself%252C%2520allowing%2520it%2520to%2520intuitively%2520inspect%2520and%2520correct%2520its%2520own%2520potential%2520visual%2520perception%2520errors.%2520We%2520instantiate%2520the%2520VSR%2520paradigm%2520in%2520the%2520domain%2520of%2520Chart%2520Parsing%2520by%2520proposing%2520ChartVSR.%2520This%2520model%2520decomposes%2520the%2520parsing%2520process%2520into%2520two%2520stages%253A%2520a%2520Refine%2520Stage%252C%2520where%2520it%2520iteratively%2520uses%2520visual%2520feedback%2520to%2520ensure%2520the%2520accuracy%2520of%2520all%2520data%2520points%2527%2520Pixel-level%2520Localizations%252C%2520and%2520a%2520Decode%2520Stage%252C%2520where%2520it%2520uses%2520these%2520verified%2520localizations%2520as%2520precise%2520visual%2520anchors%2520to%2520parse%2520the%2520final%2520structured%2520data.%2520To%2520address%2520the%2520limitations%2520of%2520existing%2520benchmarks%252C%2520we%2520also%2520construct%2520ChartP-Bench%252C%2520a%2520new%2520and%2520highly%2520challenging%2520benchmark%2520for%2520chart%2520parsing.%2520Our%2520work%2520also%2520highlights%2520VSR%2520as%2520a%2520general-purpose%2520visual%2520feedback%2520mechanism%252C%2520offering%2520a%2520promising%2520new%2520direction%2520for%2520enhancing%2520accuracy%2520on%2520a%2520wide%2520range%2520of%2520vision-centric%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Self-Refine%3A%20A%20Pixel-Guided%20Paradigm%20for%20Accurate%20Chart%20Parsing&entry.906535625=Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Jiaqi%20Wang%20and%20Dahua%20Lin&entry.1292438233=While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20remarkable%20capabilities%20for%20reasoning%20and%20self-correction%20at%20the%20textual%20level%2C%20these%20strengths%20provide%20minimal%20benefits%20for%20complex%20tasks%20centered%20on%20visual%20perception%2C%20such%20as%20Chart%20Parsing.%20Existing%20models%20often%20struggle%20with%20visually%20dense%20charts%2C%20leading%20to%20errors%20like%20data%20omission%2C%20misalignment%2C%20and%20hallucination.%20Inspired%20by%20the%20human%20strategy%20of%20using%20a%20finger%20as%20a%20%60%60visual%20anchor%27%27%20to%20ensure%20accuracy%20when%20reading%20complex%20charts%2C%20we%20propose%20a%20new%20paradigm%20named%20Visual%20Self-Refine%20%28VSR%29.%20The%20core%20idea%20of%20VSR%20is%20to%20enable%20a%20model%20to%20generate%20pixel-level%20localization%20outputs%2C%20visualize%20them%2C%20and%20then%20feed%20these%20visualizations%20back%20to%20itself%2C%20allowing%20it%20to%20intuitively%20inspect%20and%20correct%20its%20own%20potential%20visual%20perception%20errors.%20We%20instantiate%20the%20VSR%20paradigm%20in%20the%20domain%20of%20Chart%20Parsing%20by%20proposing%20ChartVSR.%20This%20model%20decomposes%20the%20parsing%20process%20into%20two%20stages%3A%20a%20Refine%20Stage%2C%20where%20it%20iteratively%20uses%20visual%20feedback%20to%20ensure%20the%20accuracy%20of%20all%20data%20points%27%20Pixel-level%20Localizations%2C%20and%20a%20Decode%20Stage%2C%20where%20it%20uses%20these%20verified%20localizations%20as%20precise%20visual%20anchors%20to%20parse%20the%20final%20structured%20data.%20To%20address%20the%20limitations%20of%20existing%20benchmarks%2C%20we%20also%20construct%20ChartP-Bench%2C%20a%20new%20and%20highly%20challenging%20benchmark%20for%20chart%20parsing.%20Our%20work%20also%20highlights%20VSR%20as%20a%20general-purpose%20visual%20feedback%20mechanism%2C%20offering%20a%20promising%20new%20direction%20for%20enhancing%20accuracy%20on%20a%20wide%20range%20of%20vision-centric%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2602.16455v1&entry.124074799=Read"},
{"title": "Autoassociative Learning of Structural Representations for Modeling and Classification in Medical Imaging", "author": "Zuzanna Buchnajzer and Kacper Dobek and Stanis\u0142aw Hapke and Daniel Jankowski and Krzysztof Krawiec", "abstract": "Deep learning architectures based on convolutional neural networks tend to rely on continuous, smooth features. While this characteristics provides significant robustness and proves useful in many real-world tasks, it is strikingly incompatible with the physical characteristic of the world, which, at the scale in which humans operate, comprises crisp objects, typically representing well-defined categories. This study proposes a class of neurosymbolic systems that learn by reconstructing images in terms of visual primitives and are thus forced to form high-level, structural explanations of them. When applied to the task of diagnosing abnormalities in histological imaging, the method proved superior to a conventional deep learning architecture in terms of classification accuracy, while being more transparent.", "link": "http://arxiv.org/abs/2411.12070v4", "date": "2026-02-18", "relevancy": 2.6725, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5712}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoassociative%20Learning%20of%20Structural%20Representations%20for%20Modeling%20and%20Classification%20in%20Medical%20Imaging&body=Title%3A%20Autoassociative%20Learning%20of%20Structural%20Representations%20for%20Modeling%20and%20Classification%20in%20Medical%20Imaging%0AAuthor%3A%20Zuzanna%20Buchnajzer%20and%20Kacper%20Dobek%20and%20Stanis%C5%82aw%20Hapke%20and%20Daniel%20Jankowski%20and%20Krzysztof%20Krawiec%0AAbstract%3A%20Deep%20learning%20architectures%20based%20on%20convolutional%20neural%20networks%20tend%20to%20rely%20on%20continuous%2C%20smooth%20features.%20While%20this%20characteristics%20provides%20significant%20robustness%20and%20proves%20useful%20in%20many%20real-world%20tasks%2C%20it%20is%20strikingly%20incompatible%20with%20the%20physical%20characteristic%20of%20the%20world%2C%20which%2C%20at%20the%20scale%20in%20which%20humans%20operate%2C%20comprises%20crisp%20objects%2C%20typically%20representing%20well-defined%20categories.%20This%20study%20proposes%20a%20class%20of%20neurosymbolic%20systems%20that%20learn%20by%20reconstructing%20images%20in%20terms%20of%20visual%20primitives%20and%20are%20thus%20forced%20to%20form%20high-level%2C%20structural%20explanations%20of%20them.%20When%20applied%20to%20the%20task%20of%20diagnosing%20abnormalities%20in%20histological%20imaging%2C%20the%20method%20proved%20superior%20to%20a%20conventional%20deep%20learning%20architecture%20in%20terms%20of%20classification%20accuracy%2C%20while%20being%20more%20transparent.%0ALink%3A%20http%3A//arxiv.org/abs/2411.12070v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoassociative%2520Learning%2520of%2520Structural%2520Representations%2520for%2520Modeling%2520and%2520Classification%2520in%2520Medical%2520Imaging%26entry.906535625%3DZuzanna%2520Buchnajzer%2520and%2520Kacper%2520Dobek%2520and%2520Stanis%25C5%2582aw%2520Hapke%2520and%2520Daniel%2520Jankowski%2520and%2520Krzysztof%2520Krawiec%26entry.1292438233%3DDeep%2520learning%2520architectures%2520based%2520on%2520convolutional%2520neural%2520networks%2520tend%2520to%2520rely%2520on%2520continuous%252C%2520smooth%2520features.%2520While%2520this%2520characteristics%2520provides%2520significant%2520robustness%2520and%2520proves%2520useful%2520in%2520many%2520real-world%2520tasks%252C%2520it%2520is%2520strikingly%2520incompatible%2520with%2520the%2520physical%2520characteristic%2520of%2520the%2520world%252C%2520which%252C%2520at%2520the%2520scale%2520in%2520which%2520humans%2520operate%252C%2520comprises%2520crisp%2520objects%252C%2520typically%2520representing%2520well-defined%2520categories.%2520This%2520study%2520proposes%2520a%2520class%2520of%2520neurosymbolic%2520systems%2520that%2520learn%2520by%2520reconstructing%2520images%2520in%2520terms%2520of%2520visual%2520primitives%2520and%2520are%2520thus%2520forced%2520to%2520form%2520high-level%252C%2520structural%2520explanations%2520of%2520them.%2520When%2520applied%2520to%2520the%2520task%2520of%2520diagnosing%2520abnormalities%2520in%2520histological%2520imaging%252C%2520the%2520method%2520proved%2520superior%2520to%2520a%2520conventional%2520deep%2520learning%2520architecture%2520in%2520terms%2520of%2520classification%2520accuracy%252C%2520while%2520being%2520more%2520transparent.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12070v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoassociative%20Learning%20of%20Structural%20Representations%20for%20Modeling%20and%20Classification%20in%20Medical%20Imaging&entry.906535625=Zuzanna%20Buchnajzer%20and%20Kacper%20Dobek%20and%20Stanis%C5%82aw%20Hapke%20and%20Daniel%20Jankowski%20and%20Krzysztof%20Krawiec&entry.1292438233=Deep%20learning%20architectures%20based%20on%20convolutional%20neural%20networks%20tend%20to%20rely%20on%20continuous%2C%20smooth%20features.%20While%20this%20characteristics%20provides%20significant%20robustness%20and%20proves%20useful%20in%20many%20real-world%20tasks%2C%20it%20is%20strikingly%20incompatible%20with%20the%20physical%20characteristic%20of%20the%20world%2C%20which%2C%20at%20the%20scale%20in%20which%20humans%20operate%2C%20comprises%20crisp%20objects%2C%20typically%20representing%20well-defined%20categories.%20This%20study%20proposes%20a%20class%20of%20neurosymbolic%20systems%20that%20learn%20by%20reconstructing%20images%20in%20terms%20of%20visual%20primitives%20and%20are%20thus%20forced%20to%20form%20high-level%2C%20structural%20explanations%20of%20them.%20When%20applied%20to%20the%20task%20of%20diagnosing%20abnormalities%20in%20histological%20imaging%2C%20the%20method%20proved%20superior%20to%20a%20conventional%20deep%20learning%20architecture%20in%20terms%20of%20classification%20accuracy%2C%20while%20being%20more%20transparent.&entry.1838667208=http%3A//arxiv.org/abs/2411.12070v4&entry.124074799=Read"},
{"title": "Equilibrium contrastive learning for imbalanced image classification", "author": "Sumin Roh and Harim Kim and Ho Yun Lee and Il Yong Chun", "abstract": "Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.", "link": "http://arxiv.org/abs/2602.09506v2", "date": "2026-02-18", "relevancy": 2.6551, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5431}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equilibrium%20contrastive%20learning%20for%20imbalanced%20image%20classification&body=Title%3A%20Equilibrium%20contrastive%20learning%20for%20imbalanced%20image%20classification%0AAuthor%3A%20Sumin%20Roh%20and%20Harim%20Kim%20and%20Ho%20Yun%20Lee%20and%20Il%20Yong%20Chun%0AAbstract%3A%20Contrastive%20learning%20%28CL%29%20is%20a%20predominant%20technique%20in%20image%20classification%2C%20but%20they%20showed%20limited%20performance%20with%20an%20imbalanced%20dataset.%20Recently%2C%20several%20supervised%20CL%20methods%20have%20been%20proposed%20to%20promote%20an%20ideal%20regular%20simplex%20geometric%20configuration%20in%20the%20representation%20space-characterized%20by%20intra-class%20feature%20collapse%20and%20uniform%20inter-class%20mean%20spacing%2C%20especially%20for%20imbalanced%20datasets.%20In%20particular%2C%20existing%20prototype-based%20methods%20include%20class%20prototypes%2C%20as%20additional%20samples%20to%20consider%20all%20classes.%20However%2C%20the%20existing%20CL%20methods%20suffer%20from%20two%20limitations.%20First%2C%20they%20do%20not%20consider%20the%20alignment%20between%20the%20class%20means/prototypes%20and%20classifiers%2C%20which%20could%20lead%20to%20poor%20generalization.%20Second%2C%20existing%20prototype-based%20methods%20treat%20prototypes%20as%20only%20one%20additional%20sample%20per%20class%2C%20making%20their%20influence%20depend%20on%20the%20number%20of%20class%20instances%20in%20a%20batch%20and%20causing%20unbalanced%20contributions%20across%20classes.%20To%20address%20these%20limitations%2C%20we%20propose%20Equilibrium%20Contrastive%20Learning%20%28ECL%29%2C%20a%20supervised%20CL%20framework%20designed%20to%20promote%20geometric%20equilibrium%2C%20where%20class%20features%2C%20means%2C%20and%20classifiers%20are%20harmoniously%20balanced%20under%20data%20imbalance.%20The%20proposed%20ECL%20framework%20uses%20two%20main%20components.%20First%2C%20ECL%20promotes%20the%20representation%20geometric%20equilibrium%20%28i.e.%2C%20a%20regular%20simplex%20geometry%20characterized%20by%20collapsed%20class%20samples%20and%20uniformly%20distributed%20class%20means%29%2C%20while%20balancing%20the%20contributions%20of%20class-average%20features%20and%20class%20prototypes.%20Second%2C%20ECL%20establishes%20a%20classifier-class%20center%20geometric%20equilibrium%20by%20aligning%20classifier%20weights%20and%20class%20prototypes.%20We%20ran%20experiments%20with%20three%20long-tailed%20datasets%2C%20the%20CIFAR-10%280%29-LT%2C%20ImageNet-LT%2C%20and%20the%20two%20imbalanced%20medical%20datasets%2C%20the%20ISIC%202019%20and%20our%20constructed%20LCCT%20dataset.%20Results%20show%20that%20ECL%20outperforms%20existing%20SOTA%20supervised%20CL%20methods%20designed%20for%20imbalanced%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquilibrium%2520contrastive%2520learning%2520for%2520imbalanced%2520image%2520classification%26entry.906535625%3DSumin%2520Roh%2520and%2520Harim%2520Kim%2520and%2520Ho%2520Yun%2520Lee%2520and%2520Il%2520Yong%2520Chun%26entry.1292438233%3DContrastive%2520learning%2520%2528CL%2529%2520is%2520a%2520predominant%2520technique%2520in%2520image%2520classification%252C%2520but%2520they%2520showed%2520limited%2520performance%2520with%2520an%2520imbalanced%2520dataset.%2520Recently%252C%2520several%2520supervised%2520CL%2520methods%2520have%2520been%2520proposed%2520to%2520promote%2520an%2520ideal%2520regular%2520simplex%2520geometric%2520configuration%2520in%2520the%2520representation%2520space-characterized%2520by%2520intra-class%2520feature%2520collapse%2520and%2520uniform%2520inter-class%2520mean%2520spacing%252C%2520especially%2520for%2520imbalanced%2520datasets.%2520In%2520particular%252C%2520existing%2520prototype-based%2520methods%2520include%2520class%2520prototypes%252C%2520as%2520additional%2520samples%2520to%2520consider%2520all%2520classes.%2520However%252C%2520the%2520existing%2520CL%2520methods%2520suffer%2520from%2520two%2520limitations.%2520First%252C%2520they%2520do%2520not%2520consider%2520the%2520alignment%2520between%2520the%2520class%2520means/prototypes%2520and%2520classifiers%252C%2520which%2520could%2520lead%2520to%2520poor%2520generalization.%2520Second%252C%2520existing%2520prototype-based%2520methods%2520treat%2520prototypes%2520as%2520only%2520one%2520additional%2520sample%2520per%2520class%252C%2520making%2520their%2520influence%2520depend%2520on%2520the%2520number%2520of%2520class%2520instances%2520in%2520a%2520batch%2520and%2520causing%2520unbalanced%2520contributions%2520across%2520classes.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Equilibrium%2520Contrastive%2520Learning%2520%2528ECL%2529%252C%2520a%2520supervised%2520CL%2520framework%2520designed%2520to%2520promote%2520geometric%2520equilibrium%252C%2520where%2520class%2520features%252C%2520means%252C%2520and%2520classifiers%2520are%2520harmoniously%2520balanced%2520under%2520data%2520imbalance.%2520The%2520proposed%2520ECL%2520framework%2520uses%2520two%2520main%2520components.%2520First%252C%2520ECL%2520promotes%2520the%2520representation%2520geometric%2520equilibrium%2520%2528i.e.%252C%2520a%2520regular%2520simplex%2520geometry%2520characterized%2520by%2520collapsed%2520class%2520samples%2520and%2520uniformly%2520distributed%2520class%2520means%2529%252C%2520while%2520balancing%2520the%2520contributions%2520of%2520class-average%2520features%2520and%2520class%2520prototypes.%2520Second%252C%2520ECL%2520establishes%2520a%2520classifier-class%2520center%2520geometric%2520equilibrium%2520by%2520aligning%2520classifier%2520weights%2520and%2520class%2520prototypes.%2520We%2520ran%2520experiments%2520with%2520three%2520long-tailed%2520datasets%252C%2520the%2520CIFAR-10%25280%2529-LT%252C%2520ImageNet-LT%252C%2520and%2520the%2520two%2520imbalanced%2520medical%2520datasets%252C%2520the%2520ISIC%25202019%2520and%2520our%2520constructed%2520LCCT%2520dataset.%2520Results%2520show%2520that%2520ECL%2520outperforms%2520existing%2520SOTA%2520supervised%2520CL%2520methods%2520designed%2520for%2520imbalanced%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equilibrium%20contrastive%20learning%20for%20imbalanced%20image%20classification&entry.906535625=Sumin%20Roh%20and%20Harim%20Kim%20and%20Ho%20Yun%20Lee%20and%20Il%20Yong%20Chun&entry.1292438233=Contrastive%20learning%20%28CL%29%20is%20a%20predominant%20technique%20in%20image%20classification%2C%20but%20they%20showed%20limited%20performance%20with%20an%20imbalanced%20dataset.%20Recently%2C%20several%20supervised%20CL%20methods%20have%20been%20proposed%20to%20promote%20an%20ideal%20regular%20simplex%20geometric%20configuration%20in%20the%20representation%20space-characterized%20by%20intra-class%20feature%20collapse%20and%20uniform%20inter-class%20mean%20spacing%2C%20especially%20for%20imbalanced%20datasets.%20In%20particular%2C%20existing%20prototype-based%20methods%20include%20class%20prototypes%2C%20as%20additional%20samples%20to%20consider%20all%20classes.%20However%2C%20the%20existing%20CL%20methods%20suffer%20from%20two%20limitations.%20First%2C%20they%20do%20not%20consider%20the%20alignment%20between%20the%20class%20means/prototypes%20and%20classifiers%2C%20which%20could%20lead%20to%20poor%20generalization.%20Second%2C%20existing%20prototype-based%20methods%20treat%20prototypes%20as%20only%20one%20additional%20sample%20per%20class%2C%20making%20their%20influence%20depend%20on%20the%20number%20of%20class%20instances%20in%20a%20batch%20and%20causing%20unbalanced%20contributions%20across%20classes.%20To%20address%20these%20limitations%2C%20we%20propose%20Equilibrium%20Contrastive%20Learning%20%28ECL%29%2C%20a%20supervised%20CL%20framework%20designed%20to%20promote%20geometric%20equilibrium%2C%20where%20class%20features%2C%20means%2C%20and%20classifiers%20are%20harmoniously%20balanced%20under%20data%20imbalance.%20The%20proposed%20ECL%20framework%20uses%20two%20main%20components.%20First%2C%20ECL%20promotes%20the%20representation%20geometric%20equilibrium%20%28i.e.%2C%20a%20regular%20simplex%20geometry%20characterized%20by%20collapsed%20class%20samples%20and%20uniformly%20distributed%20class%20means%29%2C%20while%20balancing%20the%20contributions%20of%20class-average%20features%20and%20class%20prototypes.%20Second%2C%20ECL%20establishes%20a%20classifier-class%20center%20geometric%20equilibrium%20by%20aligning%20classifier%20weights%20and%20class%20prototypes.%20We%20ran%20experiments%20with%20three%20long-tailed%20datasets%2C%20the%20CIFAR-10%280%29-LT%2C%20ImageNet-LT%2C%20and%20the%20two%20imbalanced%20medical%20datasets%2C%20the%20ISIC%202019%20and%20our%20constructed%20LCCT%20dataset.%20Results%20show%20that%20ECL%20outperforms%20existing%20SOTA%20supervised%20CL%20methods%20designed%20for%20imbalanced%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2602.09506v2&entry.124074799=Read"},
{"title": "Fused-Planes: Why Train a Thousand Tri-Planes When You Can Share?", "author": "Karim Kassab and Antoine Schnepf and Jean-Yves Franceschi and Laurent Caraffa and Flavian Vasile and Jeremie Mary and Andrew Comport and Val\u00e9rie Gouet-Brunet", "abstract": "Tri-Planar NeRFs enable the application of powerful 2D vision models for 3D tasks, by representing 3D objects using 2D planar structures. This has made them the prevailing choice to model large collections of 3D objects. However, training Tri-Planes to model such large collections is computationally intensive and remains largely inefficient. This is because the current approaches independently train one Tri-Plane per object, hence overlooking structural similarities in large classes of objects. In response to this issue, we introduce Fused-Planes, a novel object representation that improves the resource efficiency of Tri-Planes when reconstructing object classes, all while retaining the same planar structure. Our approach explicitly captures structural similarities across objects through a latent space and a set of globally shared base planes. Each individual Fused-Planes is then represented as a decomposition over these base planes, augmented with object-specific features. Fused-Planes showcase state-of-the-art efficiency among planar representations, demonstrating $7.2 \\times$ faster training and $3.2 \\times$ lower memory footprint than Tri-Planes while maintaining rendering quality. An ultra-lightweight variant further cuts per-object memory usage by $1875 \\times$ with minimal quality loss. Our project page can be found at https://fused-planes.github.io .", "link": "http://arxiv.org/abs/2410.23742v3", "date": "2026-02-18", "relevancy": 2.6401, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.524}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fused-Planes%3A%20Why%20Train%20a%20Thousand%20Tri-Planes%20When%20You%20Can%20Share%3F&body=Title%3A%20Fused-Planes%3A%20Why%20Train%20a%20Thousand%20Tri-Planes%20When%20You%20Can%20Share%3F%0AAuthor%3A%20Karim%20Kassab%20and%20Antoine%20Schnepf%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Val%C3%A9rie%20Gouet-Brunet%0AAbstract%3A%20Tri-Planar%20NeRFs%20enable%20the%20application%20of%20powerful%202D%20vision%20models%20for%203D%20tasks%2C%20by%20representing%203D%20objects%20using%202D%20planar%20structures.%20This%20has%20made%20them%20the%20prevailing%20choice%20to%20model%20large%20collections%20of%203D%20objects.%20However%2C%20training%20Tri-Planes%20to%20model%20such%20large%20collections%20is%20computationally%20intensive%20and%20remains%20largely%20inefficient.%20This%20is%20because%20the%20current%20approaches%20independently%20train%20one%20Tri-Plane%20per%20object%2C%20hence%20overlooking%20structural%20similarities%20in%20large%20classes%20of%20objects.%20In%20response%20to%20this%20issue%2C%20we%20introduce%20Fused-Planes%2C%20a%20novel%20object%20representation%20that%20improves%20the%20resource%20efficiency%20of%20Tri-Planes%20when%20reconstructing%20object%20classes%2C%20all%20while%20retaining%20the%20same%20planar%20structure.%20Our%20approach%20explicitly%20captures%20structural%20similarities%20across%20objects%20through%20a%20latent%20space%20and%20a%20set%20of%20globally%20shared%20base%20planes.%20Each%20individual%20Fused-Planes%20is%20then%20represented%20as%20a%20decomposition%20over%20these%20base%20planes%2C%20augmented%20with%20object-specific%20features.%20Fused-Planes%20showcase%20state-of-the-art%20efficiency%20among%20planar%20representations%2C%20demonstrating%20%247.2%20%5Ctimes%24%20faster%20training%20and%20%243.2%20%5Ctimes%24%20lower%20memory%20footprint%20than%20Tri-Planes%20while%20maintaining%20rendering%20quality.%20An%20ultra-lightweight%20variant%20further%20cuts%20per-object%20memory%20usage%20by%20%241875%20%5Ctimes%24%20with%20minimal%20quality%20loss.%20Our%20project%20page%20can%20be%20found%20at%20https%3A//fused-planes.github.io%20.%0ALink%3A%20http%3A//arxiv.org/abs/2410.23742v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFused-Planes%253A%2520Why%2520Train%2520a%2520Thousand%2520Tri-Planes%2520When%2520You%2520Can%2520Share%253F%26entry.906535625%3DKarim%2520Kassab%2520and%2520Antoine%2520Schnepf%2520and%2520Jean-Yves%2520Franceschi%2520and%2520Laurent%2520Caraffa%2520and%2520Flavian%2520Vasile%2520and%2520Jeremie%2520Mary%2520and%2520Andrew%2520Comport%2520and%2520Val%25C3%25A9rie%2520Gouet-Brunet%26entry.1292438233%3DTri-Planar%2520NeRFs%2520enable%2520the%2520application%2520of%2520powerful%25202D%2520vision%2520models%2520for%25203D%2520tasks%252C%2520by%2520representing%25203D%2520objects%2520using%25202D%2520planar%2520structures.%2520This%2520has%2520made%2520them%2520the%2520prevailing%2520choice%2520to%2520model%2520large%2520collections%2520of%25203D%2520objects.%2520However%252C%2520training%2520Tri-Planes%2520to%2520model%2520such%2520large%2520collections%2520is%2520computationally%2520intensive%2520and%2520remains%2520largely%2520inefficient.%2520This%2520is%2520because%2520the%2520current%2520approaches%2520independently%2520train%2520one%2520Tri-Plane%2520per%2520object%252C%2520hence%2520overlooking%2520structural%2520similarities%2520in%2520large%2520classes%2520of%2520objects.%2520In%2520response%2520to%2520this%2520issue%252C%2520we%2520introduce%2520Fused-Planes%252C%2520a%2520novel%2520object%2520representation%2520that%2520improves%2520the%2520resource%2520efficiency%2520of%2520Tri-Planes%2520when%2520reconstructing%2520object%2520classes%252C%2520all%2520while%2520retaining%2520the%2520same%2520planar%2520structure.%2520Our%2520approach%2520explicitly%2520captures%2520structural%2520similarities%2520across%2520objects%2520through%2520a%2520latent%2520space%2520and%2520a%2520set%2520of%2520globally%2520shared%2520base%2520planes.%2520Each%2520individual%2520Fused-Planes%2520is%2520then%2520represented%2520as%2520a%2520decomposition%2520over%2520these%2520base%2520planes%252C%2520augmented%2520with%2520object-specific%2520features.%2520Fused-Planes%2520showcase%2520state-of-the-art%2520efficiency%2520among%2520planar%2520representations%252C%2520demonstrating%2520%25247.2%2520%255Ctimes%2524%2520faster%2520training%2520and%2520%25243.2%2520%255Ctimes%2524%2520lower%2520memory%2520footprint%2520than%2520Tri-Planes%2520while%2520maintaining%2520rendering%2520quality.%2520An%2520ultra-lightweight%2520variant%2520further%2520cuts%2520per-object%2520memory%2520usage%2520by%2520%25241875%2520%255Ctimes%2524%2520with%2520minimal%2520quality%2520loss.%2520Our%2520project%2520page%2520can%2520be%2520found%2520at%2520https%253A//fused-planes.github.io%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23742v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fused-Planes%3A%20Why%20Train%20a%20Thousand%20Tri-Planes%20When%20You%20Can%20Share%3F&entry.906535625=Karim%20Kassab%20and%20Antoine%20Schnepf%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Val%C3%A9rie%20Gouet-Brunet&entry.1292438233=Tri-Planar%20NeRFs%20enable%20the%20application%20of%20powerful%202D%20vision%20models%20for%203D%20tasks%2C%20by%20representing%203D%20objects%20using%202D%20planar%20structures.%20This%20has%20made%20them%20the%20prevailing%20choice%20to%20model%20large%20collections%20of%203D%20objects.%20However%2C%20training%20Tri-Planes%20to%20model%20such%20large%20collections%20is%20computationally%20intensive%20and%20remains%20largely%20inefficient.%20This%20is%20because%20the%20current%20approaches%20independently%20train%20one%20Tri-Plane%20per%20object%2C%20hence%20overlooking%20structural%20similarities%20in%20large%20classes%20of%20objects.%20In%20response%20to%20this%20issue%2C%20we%20introduce%20Fused-Planes%2C%20a%20novel%20object%20representation%20that%20improves%20the%20resource%20efficiency%20of%20Tri-Planes%20when%20reconstructing%20object%20classes%2C%20all%20while%20retaining%20the%20same%20planar%20structure.%20Our%20approach%20explicitly%20captures%20structural%20similarities%20across%20objects%20through%20a%20latent%20space%20and%20a%20set%20of%20globally%20shared%20base%20planes.%20Each%20individual%20Fused-Planes%20is%20then%20represented%20as%20a%20decomposition%20over%20these%20base%20planes%2C%20augmented%20with%20object-specific%20features.%20Fused-Planes%20showcase%20state-of-the-art%20efficiency%20among%20planar%20representations%2C%20demonstrating%20%247.2%20%5Ctimes%24%20faster%20training%20and%20%243.2%20%5Ctimes%24%20lower%20memory%20footprint%20than%20Tri-Planes%20while%20maintaining%20rendering%20quality.%20An%20ultra-lightweight%20variant%20further%20cuts%20per-object%20memory%20usage%20by%20%241875%20%5Ctimes%24%20with%20minimal%20quality%20loss.%20Our%20project%20page%20can%20be%20found%20at%20https%3A//fused-planes.github.io%20.&entry.1838667208=http%3A//arxiv.org/abs/2410.23742v3&entry.124074799=Read"},
{"title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models", "author": "SungJun Cho and Chetan Gohil and Rukuang Huang and Oiwi Parker Jones and Mark W. Woolrich", "abstract": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.", "link": "http://arxiv.org/abs/2602.16626v1", "date": "2026-02-18", "relevancy": 2.6261, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Evaluation%20of%20Sample-Level%20Tokenization%20Strategies%20for%20MEG%20Foundation%20Models&body=Title%3A%20A%20Systematic%20Evaluation%20of%20Sample-Level%20Tokenization%20Strategies%20for%20MEG%20Foundation%20Models%0AAuthor%3A%20SungJun%20Cho%20and%20Chetan%20Gohil%20and%20Rukuang%20Huang%20and%20Oiwi%20Parker%20Jones%20and%20Mark%20W.%20Woolrich%0AAbstract%3A%20Recent%20success%20in%20natural%20language%20processing%20has%20motivated%20growing%20interest%20in%20large-scale%20foundation%20models%20for%20neuroimaging%20data.%20Such%20models%20often%20require%20discretization%20of%20continuous%20neural%20time%20series%20data%2C%20a%20process%20referred%20to%20as%20%27tokenization%27.%20However%2C%20the%20impact%20of%20different%20tokenization%20strategies%20for%20neural%20data%20is%20currently%20poorly%20understood.%20In%20this%20work%2C%20we%20present%20a%20systematic%20evaluation%20of%20sample-level%20tokenization%20strategies%20for%20transformer-based%20large%20neuroimaging%20models%20%28LNMs%29%20applied%20to%20magnetoencephalography%20%28MEG%29%20data.%20We%20compare%20learnable%20and%20non-learnable%20tokenizers%20by%20examining%20their%20signal%20reconstruction%20fidelity%20and%20their%20impact%20on%20subsequent%20foundation%20modeling%20performance%20%28token%20prediction%2C%20biological%20plausibility%20of%20generated%20data%2C%20preservation%20of%20subject-specific%20information%2C%20and%20performance%20on%20downstream%20tasks%29.%20For%20the%20learnable%20tokenizer%2C%20we%20introduce%20a%20novel%20approach%20based%20on%20an%20autoencoder.%20Experiments%20were%20conducted%20on%20three%20publicly%20available%20MEG%20datasets%20spanning%20different%20acquisition%20sites%2C%20scanners%2C%20and%20experimental%20paradigms.%20Our%20results%20show%20that%20both%20learnable%20and%20non-learnable%20discretization%20schemes%20achieve%20high%20reconstruction%20accuracy%20and%20broadly%20comparable%20performance%20across%20most%20evaluation%20criteria%2C%20suggesting%20that%20simple%20fixed%20sample-level%20tokenization%20strategies%20can%20be%20used%20in%20the%20development%20of%20neural%20foundation%20models.%20The%20code%20is%20available%20at%20https%3A//github.com/OHBA-analysis/Cho2026_Tokenizer.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Evaluation%2520of%2520Sample-Level%2520Tokenization%2520Strategies%2520for%2520MEG%2520Foundation%2520Models%26entry.906535625%3DSungJun%2520Cho%2520and%2520Chetan%2520Gohil%2520and%2520Rukuang%2520Huang%2520and%2520Oiwi%2520Parker%2520Jones%2520and%2520Mark%2520W.%2520Woolrich%26entry.1292438233%3DRecent%2520success%2520in%2520natural%2520language%2520processing%2520has%2520motivated%2520growing%2520interest%2520in%2520large-scale%2520foundation%2520models%2520for%2520neuroimaging%2520data.%2520Such%2520models%2520often%2520require%2520discretization%2520of%2520continuous%2520neural%2520time%2520series%2520data%252C%2520a%2520process%2520referred%2520to%2520as%2520%2527tokenization%2527.%2520However%252C%2520the%2520impact%2520of%2520different%2520tokenization%2520strategies%2520for%2520neural%2520data%2520is%2520currently%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520systematic%2520evaluation%2520of%2520sample-level%2520tokenization%2520strategies%2520for%2520transformer-based%2520large%2520neuroimaging%2520models%2520%2528LNMs%2529%2520applied%2520to%2520magnetoencephalography%2520%2528MEG%2529%2520data.%2520We%2520compare%2520learnable%2520and%2520non-learnable%2520tokenizers%2520by%2520examining%2520their%2520signal%2520reconstruction%2520fidelity%2520and%2520their%2520impact%2520on%2520subsequent%2520foundation%2520modeling%2520performance%2520%2528token%2520prediction%252C%2520biological%2520plausibility%2520of%2520generated%2520data%252C%2520preservation%2520of%2520subject-specific%2520information%252C%2520and%2520performance%2520on%2520downstream%2520tasks%2529.%2520For%2520the%2520learnable%2520tokenizer%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520based%2520on%2520an%2520autoencoder.%2520Experiments%2520were%2520conducted%2520on%2520three%2520publicly%2520available%2520MEG%2520datasets%2520spanning%2520different%2520acquisition%2520sites%252C%2520scanners%252C%2520and%2520experimental%2520paradigms.%2520Our%2520results%2520show%2520that%2520both%2520learnable%2520and%2520non-learnable%2520discretization%2520schemes%2520achieve%2520high%2520reconstruction%2520accuracy%2520and%2520broadly%2520comparable%2520performance%2520across%2520most%2520evaluation%2520criteria%252C%2520suggesting%2520that%2520simple%2520fixed%2520sample-level%2520tokenization%2520strategies%2520can%2520be%2520used%2520in%2520the%2520development%2520of%2520neural%2520foundation%2520models.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/OHBA-analysis/Cho2026_Tokenizer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Evaluation%20of%20Sample-Level%20Tokenization%20Strategies%20for%20MEG%20Foundation%20Models&entry.906535625=SungJun%20Cho%20and%20Chetan%20Gohil%20and%20Rukuang%20Huang%20and%20Oiwi%20Parker%20Jones%20and%20Mark%20W.%20Woolrich&entry.1292438233=Recent%20success%20in%20natural%20language%20processing%20has%20motivated%20growing%20interest%20in%20large-scale%20foundation%20models%20for%20neuroimaging%20data.%20Such%20models%20often%20require%20discretization%20of%20continuous%20neural%20time%20series%20data%2C%20a%20process%20referred%20to%20as%20%27tokenization%27.%20However%2C%20the%20impact%20of%20different%20tokenization%20strategies%20for%20neural%20data%20is%20currently%20poorly%20understood.%20In%20this%20work%2C%20we%20present%20a%20systematic%20evaluation%20of%20sample-level%20tokenization%20strategies%20for%20transformer-based%20large%20neuroimaging%20models%20%28LNMs%29%20applied%20to%20magnetoencephalography%20%28MEG%29%20data.%20We%20compare%20learnable%20and%20non-learnable%20tokenizers%20by%20examining%20their%20signal%20reconstruction%20fidelity%20and%20their%20impact%20on%20subsequent%20foundation%20modeling%20performance%20%28token%20prediction%2C%20biological%20plausibility%20of%20generated%20data%2C%20preservation%20of%20subject-specific%20information%2C%20and%20performance%20on%20downstream%20tasks%29.%20For%20the%20learnable%20tokenizer%2C%20we%20introduce%20a%20novel%20approach%20based%20on%20an%20autoencoder.%20Experiments%20were%20conducted%20on%20three%20publicly%20available%20MEG%20datasets%20spanning%20different%20acquisition%20sites%2C%20scanners%2C%20and%20experimental%20paradigms.%20Our%20results%20show%20that%20both%20learnable%20and%20non-learnable%20discretization%20schemes%20achieve%20high%20reconstruction%20accuracy%20and%20broadly%20comparable%20performance%20across%20most%20evaluation%20criteria%2C%20suggesting%20that%20simple%20fixed%20sample-level%20tokenization%20strategies%20can%20be%20used%20in%20the%20development%20of%20neural%20foundation%20models.%20The%20code%20is%20available%20at%20https%3A//github.com/OHBA-analysis/Cho2026_Tokenizer.&entry.1838667208=http%3A//arxiv.org/abs/2602.16626v1&entry.124074799=Read"},
{"title": "Robust Image Stitching with Optimal Plane", "author": "Lang Nie and Yuan Mei and Kang Liao and Yunqiu Xu and Chunyu Lin and Bin Xiao", "abstract": "We present \\textit{RopStitch}, an unsupervised deep image stitching framework with both robustness and naturalness. To ensure the robustness of \\textit{RopStitch}, we propose to incorporate the universal prior of content perception into the image stitching model by a dual-branch architecture. It separately captures coarse and fine features and integrates them to achieve highly generalizable performance across diverse unseen real-world scenes. Concretely, the dual-branch model consists of a pretrained branch to capture semantically invariant representations and a learnable branch to extract fine-grained discriminative features, which are then merged into a whole by a controllable factor at the correlation level. Besides, considering that content alignment and structural preservation are often contradictory to each other, we propose a concept of virtual optimal planes to relieve this conflict. To this end, we model this problem as a process of estimating homography decomposition coefficients, and design an iterative coefficient predictor and minimal semantic distortion constraint to identify the optimal plane. This scheme is finally incorporated into \\textit{RopStitch} by warping both views onto the optimal plane bidirectionally. Extensive experiments across various datasets demonstrate that \\textit{RopStitch} significantly outperforms existing methods, particularly in scene robustness and content naturalness. The code is available at {\\color{red}https://github.com/MmelodYy/RopStitch}.", "link": "http://arxiv.org/abs/2508.05903v3", "date": "2026-02-18", "relevancy": 2.6165, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5308}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Image%20Stitching%20with%20Optimal%20Plane&body=Title%3A%20Robust%20Image%20Stitching%20with%20Optimal%20Plane%0AAuthor%3A%20Lang%20Nie%20and%20Yuan%20Mei%20and%20Kang%20Liao%20and%20Yunqiu%20Xu%20and%20Chunyu%20Lin%20and%20Bin%20Xiao%0AAbstract%3A%20We%20present%20%5Ctextit%7BRopStitch%7D%2C%20an%20unsupervised%20deep%20image%20stitching%20framework%20with%20both%20robustness%20and%20naturalness.%20To%20ensure%20the%20robustness%20of%20%5Ctextit%7BRopStitch%7D%2C%20we%20propose%20to%20incorporate%20the%20universal%20prior%20of%20content%20perception%20into%20the%20image%20stitching%20model%20by%20a%20dual-branch%20architecture.%20It%20separately%20captures%20coarse%20and%20fine%20features%20and%20integrates%20them%20to%20achieve%20highly%20generalizable%20performance%20across%20diverse%20unseen%20real-world%20scenes.%20Concretely%2C%20the%20dual-branch%20model%20consists%20of%20a%20pretrained%20branch%20to%20capture%20semantically%20invariant%20representations%20and%20a%20learnable%20branch%20to%20extract%20fine-grained%20discriminative%20features%2C%20which%20are%20then%20merged%20into%20a%20whole%20by%20a%20controllable%20factor%20at%20the%20correlation%20level.%20Besides%2C%20considering%20that%20content%20alignment%20and%20structural%20preservation%20are%20often%20contradictory%20to%20each%20other%2C%20we%20propose%20a%20concept%20of%20virtual%20optimal%20planes%20to%20relieve%20this%20conflict.%20To%20this%20end%2C%20we%20model%20this%20problem%20as%20a%20process%20of%20estimating%20homography%20decomposition%20coefficients%2C%20and%20design%20an%20iterative%20coefficient%20predictor%20and%20minimal%20semantic%20distortion%20constraint%20to%20identify%20the%20optimal%20plane.%20This%20scheme%20is%20finally%20incorporated%20into%20%5Ctextit%7BRopStitch%7D%20by%20warping%20both%20views%20onto%20the%20optimal%20plane%20bidirectionally.%20Extensive%20experiments%20across%20various%20datasets%20demonstrate%20that%20%5Ctextit%7BRopStitch%7D%20significantly%20outperforms%20existing%20methods%2C%20particularly%20in%20scene%20robustness%20and%20content%20naturalness.%20The%20code%20is%20available%20at%20%7B%5Ccolor%7Bred%7Dhttps%3A//github.com/MmelodYy/RopStitch%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05903v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Image%2520Stitching%2520with%2520Optimal%2520Plane%26entry.906535625%3DLang%2520Nie%2520and%2520Yuan%2520Mei%2520and%2520Kang%2520Liao%2520and%2520Yunqiu%2520Xu%2520and%2520Chunyu%2520Lin%2520and%2520Bin%2520Xiao%26entry.1292438233%3DWe%2520present%2520%255Ctextit%257BRopStitch%257D%252C%2520an%2520unsupervised%2520deep%2520image%2520stitching%2520framework%2520with%2520both%2520robustness%2520and%2520naturalness.%2520To%2520ensure%2520the%2520robustness%2520of%2520%255Ctextit%257BRopStitch%257D%252C%2520we%2520propose%2520to%2520incorporate%2520the%2520universal%2520prior%2520of%2520content%2520perception%2520into%2520the%2520image%2520stitching%2520model%2520by%2520a%2520dual-branch%2520architecture.%2520It%2520separately%2520captures%2520coarse%2520and%2520fine%2520features%2520and%2520integrates%2520them%2520to%2520achieve%2520highly%2520generalizable%2520performance%2520across%2520diverse%2520unseen%2520real-world%2520scenes.%2520Concretely%252C%2520the%2520dual-branch%2520model%2520consists%2520of%2520a%2520pretrained%2520branch%2520to%2520capture%2520semantically%2520invariant%2520representations%2520and%2520a%2520learnable%2520branch%2520to%2520extract%2520fine-grained%2520discriminative%2520features%252C%2520which%2520are%2520then%2520merged%2520into%2520a%2520whole%2520by%2520a%2520controllable%2520factor%2520at%2520the%2520correlation%2520level.%2520Besides%252C%2520considering%2520that%2520content%2520alignment%2520and%2520structural%2520preservation%2520are%2520often%2520contradictory%2520to%2520each%2520other%252C%2520we%2520propose%2520a%2520concept%2520of%2520virtual%2520optimal%2520planes%2520to%2520relieve%2520this%2520conflict.%2520To%2520this%2520end%252C%2520we%2520model%2520this%2520problem%2520as%2520a%2520process%2520of%2520estimating%2520homography%2520decomposition%2520coefficients%252C%2520and%2520design%2520an%2520iterative%2520coefficient%2520predictor%2520and%2520minimal%2520semantic%2520distortion%2520constraint%2520to%2520identify%2520the%2520optimal%2520plane.%2520This%2520scheme%2520is%2520finally%2520incorporated%2520into%2520%255Ctextit%257BRopStitch%257D%2520by%2520warping%2520both%2520views%2520onto%2520the%2520optimal%2520plane%2520bidirectionally.%2520Extensive%2520experiments%2520across%2520various%2520datasets%2520demonstrate%2520that%2520%255Ctextit%257BRopStitch%257D%2520significantly%2520outperforms%2520existing%2520methods%252C%2520particularly%2520in%2520scene%2520robustness%2520and%2520content%2520naturalness.%2520The%2520code%2520is%2520available%2520at%2520%257B%255Ccolor%257Bred%257Dhttps%253A//github.com/MmelodYy/RopStitch%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05903v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Image%20Stitching%20with%20Optimal%20Plane&entry.906535625=Lang%20Nie%20and%20Yuan%20Mei%20and%20Kang%20Liao%20and%20Yunqiu%20Xu%20and%20Chunyu%20Lin%20and%20Bin%20Xiao&entry.1292438233=We%20present%20%5Ctextit%7BRopStitch%7D%2C%20an%20unsupervised%20deep%20image%20stitching%20framework%20with%20both%20robustness%20and%20naturalness.%20To%20ensure%20the%20robustness%20of%20%5Ctextit%7BRopStitch%7D%2C%20we%20propose%20to%20incorporate%20the%20universal%20prior%20of%20content%20perception%20into%20the%20image%20stitching%20model%20by%20a%20dual-branch%20architecture.%20It%20separately%20captures%20coarse%20and%20fine%20features%20and%20integrates%20them%20to%20achieve%20highly%20generalizable%20performance%20across%20diverse%20unseen%20real-world%20scenes.%20Concretely%2C%20the%20dual-branch%20model%20consists%20of%20a%20pretrained%20branch%20to%20capture%20semantically%20invariant%20representations%20and%20a%20learnable%20branch%20to%20extract%20fine-grained%20discriminative%20features%2C%20which%20are%20then%20merged%20into%20a%20whole%20by%20a%20controllable%20factor%20at%20the%20correlation%20level.%20Besides%2C%20considering%20that%20content%20alignment%20and%20structural%20preservation%20are%20often%20contradictory%20to%20each%20other%2C%20we%20propose%20a%20concept%20of%20virtual%20optimal%20planes%20to%20relieve%20this%20conflict.%20To%20this%20end%2C%20we%20model%20this%20problem%20as%20a%20process%20of%20estimating%20homography%20decomposition%20coefficients%2C%20and%20design%20an%20iterative%20coefficient%20predictor%20and%20minimal%20semantic%20distortion%20constraint%20to%20identify%20the%20optimal%20plane.%20This%20scheme%20is%20finally%20incorporated%20into%20%5Ctextit%7BRopStitch%7D%20by%20warping%20both%20views%20onto%20the%20optimal%20plane%20bidirectionally.%20Extensive%20experiments%20across%20various%20datasets%20demonstrate%20that%20%5Ctextit%7BRopStitch%7D%20significantly%20outperforms%20existing%20methods%2C%20particularly%20in%20scene%20robustness%20and%20content%20naturalness.%20The%20code%20is%20available%20at%20%7B%5Ccolor%7Bred%7Dhttps%3A//github.com/MmelodYy/RopStitch%7D.&entry.1838667208=http%3A//arxiv.org/abs/2508.05903v3&entry.124074799=Read"},
{"title": "Are Object-Centric Representations Better At Compositional Generalization?", "author": "Ferdinand Kapl and Amir Mohammad Karimi Mamaghan and Maximilian Seitzer and Karl Henrik Johansson and Carsten Marr and Stefan Bauer and Andrea Dittadi", "abstract": "Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.", "link": "http://arxiv.org/abs/2602.16689v1", "date": "2026-02-18", "relevancy": 2.557, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6474}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Object-Centric%20Representations%20Better%20At%20Compositional%20Generalization%3F&body=Title%3A%20Are%20Object-Centric%20Representations%20Better%20At%20Compositional%20Generalization%3F%0AAuthor%3A%20Ferdinand%20Kapl%20and%20Amir%20Mohammad%20Karimi%20Mamaghan%20and%20Maximilian%20Seitzer%20and%20Karl%20Henrik%20Johansson%20and%20Carsten%20Marr%20and%20Stefan%20Bauer%20and%20Andrea%20Dittadi%0AAbstract%3A%20Compositional%20generalization%2C%20the%20ability%20to%20reason%20about%20novel%20combinations%20of%20familiar%20concepts%2C%20is%20fundamental%20to%20human%20cognition%20and%20a%20critical%20challenge%20for%20machine%20learning.%20Object-centric%20%28OC%29%20representations%2C%20which%20encode%20a%20scene%20as%20a%20set%20of%20objects%2C%20are%20often%20argued%20to%20support%20such%20generalization%2C%20but%20systematic%20evidence%20in%20visually%20rich%20settings%20is%20limited.%20We%20introduce%20a%20Visual%20Question%20Answering%20benchmark%20across%20three%20controlled%20visual%20worlds%20%28CLEVRTex%2C%20Super-CLEVR%2C%20and%20MOVi-C%29%20to%20measure%20how%20well%20vision%20encoders%2C%20with%20and%20without%20object-centric%20biases%2C%20generalize%20to%20unseen%20combinations%20of%20object%20properties.%20To%20ensure%20a%20fair%20and%20comprehensive%20comparison%2C%20we%20carefully%20account%20for%20training%20data%20diversity%2C%20sample%20size%2C%20representation%20size%2C%20downstream%20model%20capacity%2C%20and%20compute.%20We%20use%20DINOv2%20and%20SigLIP2%2C%20two%20widely%20used%20vision%20encoders%2C%20as%20the%20foundation%20models%20and%20their%20OC%20counterparts.%20Our%20key%20findings%20reveal%20that%20%281%29%20OC%20approaches%20are%20superior%20in%20harder%20compositional%20generalization%20settings%3B%20%282%29%20original%20dense%20representations%20surpass%20OC%20only%20on%20easier%20settings%20and%20typically%20require%20substantially%20more%20downstream%20compute%3B%20and%20%283%29%20OC%20models%20are%20more%20sample%20efficient%2C%20achieving%20stronger%20generalization%20with%20fewer%20images%2C%20whereas%20dense%20encoders%20catch%20up%20or%20surpass%20them%20only%20with%20sufficient%20data%20and%20diversity.%20Overall%2C%20object-centric%20representations%20offer%20stronger%20compositional%20generalization%20when%20any%20one%20of%20dataset%20size%2C%20training%20data%20diversity%2C%20or%20downstream%20compute%20is%20constrained.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Object-Centric%2520Representations%2520Better%2520At%2520Compositional%2520Generalization%253F%26entry.906535625%3DFerdinand%2520Kapl%2520and%2520Amir%2520Mohammad%2520Karimi%2520Mamaghan%2520and%2520Maximilian%2520Seitzer%2520and%2520Karl%2520Henrik%2520Johansson%2520and%2520Carsten%2520Marr%2520and%2520Stefan%2520Bauer%2520and%2520Andrea%2520Dittadi%26entry.1292438233%3DCompositional%2520generalization%252C%2520the%2520ability%2520to%2520reason%2520about%2520novel%2520combinations%2520of%2520familiar%2520concepts%252C%2520is%2520fundamental%2520to%2520human%2520cognition%2520and%2520a%2520critical%2520challenge%2520for%2520machine%2520learning.%2520Object-centric%2520%2528OC%2529%2520representations%252C%2520which%2520encode%2520a%2520scene%2520as%2520a%2520set%2520of%2520objects%252C%2520are%2520often%2520argued%2520to%2520support%2520such%2520generalization%252C%2520but%2520systematic%2520evidence%2520in%2520visually%2520rich%2520settings%2520is%2520limited.%2520We%2520introduce%2520a%2520Visual%2520Question%2520Answering%2520benchmark%2520across%2520three%2520controlled%2520visual%2520worlds%2520%2528CLEVRTex%252C%2520Super-CLEVR%252C%2520and%2520MOVi-C%2529%2520to%2520measure%2520how%2520well%2520vision%2520encoders%252C%2520with%2520and%2520without%2520object-centric%2520biases%252C%2520generalize%2520to%2520unseen%2520combinations%2520of%2520object%2520properties.%2520To%2520ensure%2520a%2520fair%2520and%2520comprehensive%2520comparison%252C%2520we%2520carefully%2520account%2520for%2520training%2520data%2520diversity%252C%2520sample%2520size%252C%2520representation%2520size%252C%2520downstream%2520model%2520capacity%252C%2520and%2520compute.%2520We%2520use%2520DINOv2%2520and%2520SigLIP2%252C%2520two%2520widely%2520used%2520vision%2520encoders%252C%2520as%2520the%2520foundation%2520models%2520and%2520their%2520OC%2520counterparts.%2520Our%2520key%2520findings%2520reveal%2520that%2520%25281%2529%2520OC%2520approaches%2520are%2520superior%2520in%2520harder%2520compositional%2520generalization%2520settings%253B%2520%25282%2529%2520original%2520dense%2520representations%2520surpass%2520OC%2520only%2520on%2520easier%2520settings%2520and%2520typically%2520require%2520substantially%2520more%2520downstream%2520compute%253B%2520and%2520%25283%2529%2520OC%2520models%2520are%2520more%2520sample%2520efficient%252C%2520achieving%2520stronger%2520generalization%2520with%2520fewer%2520images%252C%2520whereas%2520dense%2520encoders%2520catch%2520up%2520or%2520surpass%2520them%2520only%2520with%2520sufficient%2520data%2520and%2520diversity.%2520Overall%252C%2520object-centric%2520representations%2520offer%2520stronger%2520compositional%2520generalization%2520when%2520any%2520one%2520of%2520dataset%2520size%252C%2520training%2520data%2520diversity%252C%2520or%2520downstream%2520compute%2520is%2520constrained.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Object-Centric%20Representations%20Better%20At%20Compositional%20Generalization%3F&entry.906535625=Ferdinand%20Kapl%20and%20Amir%20Mohammad%20Karimi%20Mamaghan%20and%20Maximilian%20Seitzer%20and%20Karl%20Henrik%20Johansson%20and%20Carsten%20Marr%20and%20Stefan%20Bauer%20and%20Andrea%20Dittadi&entry.1292438233=Compositional%20generalization%2C%20the%20ability%20to%20reason%20about%20novel%20combinations%20of%20familiar%20concepts%2C%20is%20fundamental%20to%20human%20cognition%20and%20a%20critical%20challenge%20for%20machine%20learning.%20Object-centric%20%28OC%29%20representations%2C%20which%20encode%20a%20scene%20as%20a%20set%20of%20objects%2C%20are%20often%20argued%20to%20support%20such%20generalization%2C%20but%20systematic%20evidence%20in%20visually%20rich%20settings%20is%20limited.%20We%20introduce%20a%20Visual%20Question%20Answering%20benchmark%20across%20three%20controlled%20visual%20worlds%20%28CLEVRTex%2C%20Super-CLEVR%2C%20and%20MOVi-C%29%20to%20measure%20how%20well%20vision%20encoders%2C%20with%20and%20without%20object-centric%20biases%2C%20generalize%20to%20unseen%20combinations%20of%20object%20properties.%20To%20ensure%20a%20fair%20and%20comprehensive%20comparison%2C%20we%20carefully%20account%20for%20training%20data%20diversity%2C%20sample%20size%2C%20representation%20size%2C%20downstream%20model%20capacity%2C%20and%20compute.%20We%20use%20DINOv2%20and%20SigLIP2%2C%20two%20widely%20used%20vision%20encoders%2C%20as%20the%20foundation%20models%20and%20their%20OC%20counterparts.%20Our%20key%20findings%20reveal%20that%20%281%29%20OC%20approaches%20are%20superior%20in%20harder%20compositional%20generalization%20settings%3B%20%282%29%20original%20dense%20representations%20surpass%20OC%20only%20on%20easier%20settings%20and%20typically%20require%20substantially%20more%20downstream%20compute%3B%20and%20%283%29%20OC%20models%20are%20more%20sample%20efficient%2C%20achieving%20stronger%20generalization%20with%20fewer%20images%2C%20whereas%20dense%20encoders%20catch%20up%20or%20surpass%20them%20only%20with%20sufficient%20data%20and%20diversity.%20Overall%2C%20object-centric%20representations%20offer%20stronger%20compositional%20generalization%20when%20any%20one%20of%20dataset%20size%2C%20training%20data%20diversity%2C%20or%20downstream%20compute%20is%20constrained.&entry.1838667208=http%3A//arxiv.org/abs/2602.16689v1&entry.124074799=Read"},
{"title": "When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing", "author": "Zachary Pedram Dadfar", "abstract": "Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks concurrent activation dynamics, and that this correspondence is specific to self-referential processing. We introduce the Pull Methodology, a protocol that elicits extended self-examination through format engineering, and use it to identify a direction in activation space that distinguishes self-referential from descriptive processing in Llama 3.1. The direction is orthogonal to the known refusal direction, localised at 6.25% of model depth, and causally influences introspective output when used for steering. When models produce \"loop\" vocabulary, their activations exhibit higher autocorrelation (r = 0.44, p = 0.002); when they produce \"shimmer\" vocabulary under steering, activation variability increases (r = 0.36, p = 0.002). Critically, the same vocabulary in non-self-referential contexts shows no activation correspondence despite nine-fold higher frequency. Qwen 2.5-32B, with no shared training, independently develops different introspective vocabulary tracking different activation metrics, all absent in descriptive controls. The findings indicate that self-report in transformer models can, under appropriate conditions, reliably track internal computational states.", "link": "http://arxiv.org/abs/2602.11358v2", "date": "2026-02-18", "relevancy": 2.5531, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Models%20Examine%20Themselves%3A%20Vocabulary-Activation%20Correspondence%20in%20Self-Referential%20Processing&body=Title%3A%20When%20Models%20Examine%20Themselves%3A%20Vocabulary-Activation%20Correspondence%20in%20Self-Referential%20Processing%0AAuthor%3A%20Zachary%20Pedram%20Dadfar%0AAbstract%3A%20Large%20language%20models%20produce%20rich%20introspective%20language%20when%20prompted%20for%20self-examination%2C%20but%20whether%20this%20language%20reflects%20internal%20computation%20or%20sophisticated%20confabulation%20has%20remained%20unclear.%20We%20show%20that%20self-referential%20vocabulary%20tracks%20concurrent%20activation%20dynamics%2C%20and%20that%20this%20correspondence%20is%20specific%20to%20self-referential%20processing.%20We%20introduce%20the%20Pull%20Methodology%2C%20a%20protocol%20that%20elicits%20extended%20self-examination%20through%20format%20engineering%2C%20and%20use%20it%20to%20identify%20a%20direction%20in%20activation%20space%20that%20distinguishes%20self-referential%20from%20descriptive%20processing%20in%20Llama%203.1.%20The%20direction%20is%20orthogonal%20to%20the%20known%20refusal%20direction%2C%20localised%20at%206.25%25%20of%20model%20depth%2C%20and%20causally%20influences%20introspective%20output%20when%20used%20for%20steering.%20When%20models%20produce%20%22loop%22%20vocabulary%2C%20their%20activations%20exhibit%20higher%20autocorrelation%20%28r%20%3D%200.44%2C%20p%20%3D%200.002%29%3B%20when%20they%20produce%20%22shimmer%22%20vocabulary%20under%20steering%2C%20activation%20variability%20increases%20%28r%20%3D%200.36%2C%20p%20%3D%200.002%29.%20Critically%2C%20the%20same%20vocabulary%20in%20non-self-referential%20contexts%20shows%20no%20activation%20correspondence%20despite%20nine-fold%20higher%20frequency.%20Qwen%202.5-32B%2C%20with%20no%20shared%20training%2C%20independently%20develops%20different%20introspective%20vocabulary%20tracking%20different%20activation%20metrics%2C%20all%20absent%20in%20descriptive%20controls.%20The%20findings%20indicate%20that%20self-report%20in%20transformer%20models%20can%2C%20under%20appropriate%20conditions%2C%20reliably%20track%20internal%20computational%20states.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Models%2520Examine%2520Themselves%253A%2520Vocabulary-Activation%2520Correspondence%2520in%2520Self-Referential%2520Processing%26entry.906535625%3DZachary%2520Pedram%2520Dadfar%26entry.1292438233%3DLarge%2520language%2520models%2520produce%2520rich%2520introspective%2520language%2520when%2520prompted%2520for%2520self-examination%252C%2520but%2520whether%2520this%2520language%2520reflects%2520internal%2520computation%2520or%2520sophisticated%2520confabulation%2520has%2520remained%2520unclear.%2520We%2520show%2520that%2520self-referential%2520vocabulary%2520tracks%2520concurrent%2520activation%2520dynamics%252C%2520and%2520that%2520this%2520correspondence%2520is%2520specific%2520to%2520self-referential%2520processing.%2520We%2520introduce%2520the%2520Pull%2520Methodology%252C%2520a%2520protocol%2520that%2520elicits%2520extended%2520self-examination%2520through%2520format%2520engineering%252C%2520and%2520use%2520it%2520to%2520identify%2520a%2520direction%2520in%2520activation%2520space%2520that%2520distinguishes%2520self-referential%2520from%2520descriptive%2520processing%2520in%2520Llama%25203.1.%2520The%2520direction%2520is%2520orthogonal%2520to%2520the%2520known%2520refusal%2520direction%252C%2520localised%2520at%25206.25%2525%2520of%2520model%2520depth%252C%2520and%2520causally%2520influences%2520introspective%2520output%2520when%2520used%2520for%2520steering.%2520When%2520models%2520produce%2520%2522loop%2522%2520vocabulary%252C%2520their%2520activations%2520exhibit%2520higher%2520autocorrelation%2520%2528r%2520%253D%25200.44%252C%2520p%2520%253D%25200.002%2529%253B%2520when%2520they%2520produce%2520%2522shimmer%2522%2520vocabulary%2520under%2520steering%252C%2520activation%2520variability%2520increases%2520%2528r%2520%253D%25200.36%252C%2520p%2520%253D%25200.002%2529.%2520Critically%252C%2520the%2520same%2520vocabulary%2520in%2520non-self-referential%2520contexts%2520shows%2520no%2520activation%2520correspondence%2520despite%2520nine-fold%2520higher%2520frequency.%2520Qwen%25202.5-32B%252C%2520with%2520no%2520shared%2520training%252C%2520independently%2520develops%2520different%2520introspective%2520vocabulary%2520tracking%2520different%2520activation%2520metrics%252C%2520all%2520absent%2520in%2520descriptive%2520controls.%2520The%2520findings%2520indicate%2520that%2520self-report%2520in%2520transformer%2520models%2520can%252C%2520under%2520appropriate%2520conditions%252C%2520reliably%2520track%2520internal%2520computational%2520states.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Models%20Examine%20Themselves%3A%20Vocabulary-Activation%20Correspondence%20in%20Self-Referential%20Processing&entry.906535625=Zachary%20Pedram%20Dadfar&entry.1292438233=Large%20language%20models%20produce%20rich%20introspective%20language%20when%20prompted%20for%20self-examination%2C%20but%20whether%20this%20language%20reflects%20internal%20computation%20or%20sophisticated%20confabulation%20has%20remained%20unclear.%20We%20show%20that%20self-referential%20vocabulary%20tracks%20concurrent%20activation%20dynamics%2C%20and%20that%20this%20correspondence%20is%20specific%20to%20self-referential%20processing.%20We%20introduce%20the%20Pull%20Methodology%2C%20a%20protocol%20that%20elicits%20extended%20self-examination%20through%20format%20engineering%2C%20and%20use%20it%20to%20identify%20a%20direction%20in%20activation%20space%20that%20distinguishes%20self-referential%20from%20descriptive%20processing%20in%20Llama%203.1.%20The%20direction%20is%20orthogonal%20to%20the%20known%20refusal%20direction%2C%20localised%20at%206.25%25%20of%20model%20depth%2C%20and%20causally%20influences%20introspective%20output%20when%20used%20for%20steering.%20When%20models%20produce%20%22loop%22%20vocabulary%2C%20their%20activations%20exhibit%20higher%20autocorrelation%20%28r%20%3D%200.44%2C%20p%20%3D%200.002%29%3B%20when%20they%20produce%20%22shimmer%22%20vocabulary%20under%20steering%2C%20activation%20variability%20increases%20%28r%20%3D%200.36%2C%20p%20%3D%200.002%29.%20Critically%2C%20the%20same%20vocabulary%20in%20non-self-referential%20contexts%20shows%20no%20activation%20correspondence%20despite%20nine-fold%20higher%20frequency.%20Qwen%202.5-32B%2C%20with%20no%20shared%20training%2C%20independently%20develops%20different%20introspective%20vocabulary%20tracking%20different%20activation%20metrics%2C%20all%20absent%20in%20descriptive%20controls.%20The%20findings%20indicate%20that%20self-report%20in%20transformer%20models%20can%2C%20under%20appropriate%20conditions%2C%20reliably%20track%20internal%20computational%20states.&entry.1838667208=http%3A//arxiv.org/abs/2602.11358v2&entry.124074799=Read"},
{"title": "Multi-Channel Replay Speech Detection using Acoustic Maps", "author": "Michael Neri and Tuomas Virtanen", "abstract": "Replay attacks remain a critical vulnerability for automatic speaker verification systems, particularly in real-time voice assistant applications. In this work, we propose acoustic maps as a novel spatial feature representation for replay speech detection from multi-channel recordings. Derived from classical beamforming over discrete azimuth and elevation grids, acoustic maps encode directional energy distributions that reflect physical differences between human speech radiation and loudspeaker-based replay. A lightweight convolutional neural network is designed to operate on this representation, achieving competitive performance on the ReMASC dataset with approximately 6k trainable parameters. Experimental results show that acoustic maps provide a compact and physically interpretable feature space for replay attack detection across different devices and acoustic environments.", "link": "http://arxiv.org/abs/2602.16399v1", "date": "2026-02-18", "relevancy": 2.5049, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5138}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4975}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Channel%20Replay%20Speech%20Detection%20using%20Acoustic%20Maps&body=Title%3A%20Multi-Channel%20Replay%20Speech%20Detection%20using%20Acoustic%20Maps%0AAuthor%3A%20Michael%20Neri%20and%20Tuomas%20Virtanen%0AAbstract%3A%20Replay%20attacks%20remain%20a%20critical%20vulnerability%20for%20automatic%20speaker%20verification%20systems%2C%20particularly%20in%20real-time%20voice%20assistant%20applications.%20In%20this%20work%2C%20we%20propose%20acoustic%20maps%20as%20a%20novel%20spatial%20feature%20representation%20for%20replay%20speech%20detection%20from%20multi-channel%20recordings.%20Derived%20from%20classical%20beamforming%20over%20discrete%20azimuth%20and%20elevation%20grids%2C%20acoustic%20maps%20encode%20directional%20energy%20distributions%20that%20reflect%20physical%20differences%20between%20human%20speech%20radiation%20and%20loudspeaker-based%20replay.%20A%20lightweight%20convolutional%20neural%20network%20is%20designed%20to%20operate%20on%20this%20representation%2C%20achieving%20competitive%20performance%20on%20the%20ReMASC%20dataset%20with%20approximately%206k%20trainable%20parameters.%20Experimental%20results%20show%20that%20acoustic%20maps%20provide%20a%20compact%20and%20physically%20interpretable%20feature%20space%20for%20replay%20attack%20detection%20across%20different%20devices%20and%20acoustic%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Channel%2520Replay%2520Speech%2520Detection%2520using%2520Acoustic%2520Maps%26entry.906535625%3DMichael%2520Neri%2520and%2520Tuomas%2520Virtanen%26entry.1292438233%3DReplay%2520attacks%2520remain%2520a%2520critical%2520vulnerability%2520for%2520automatic%2520speaker%2520verification%2520systems%252C%2520particularly%2520in%2520real-time%2520voice%2520assistant%2520applications.%2520In%2520this%2520work%252C%2520we%2520propose%2520acoustic%2520maps%2520as%2520a%2520novel%2520spatial%2520feature%2520representation%2520for%2520replay%2520speech%2520detection%2520from%2520multi-channel%2520recordings.%2520Derived%2520from%2520classical%2520beamforming%2520over%2520discrete%2520azimuth%2520and%2520elevation%2520grids%252C%2520acoustic%2520maps%2520encode%2520directional%2520energy%2520distributions%2520that%2520reflect%2520physical%2520differences%2520between%2520human%2520speech%2520radiation%2520and%2520loudspeaker-based%2520replay.%2520A%2520lightweight%2520convolutional%2520neural%2520network%2520is%2520designed%2520to%2520operate%2520on%2520this%2520representation%252C%2520achieving%2520competitive%2520performance%2520on%2520the%2520ReMASC%2520dataset%2520with%2520approximately%25206k%2520trainable%2520parameters.%2520Experimental%2520results%2520show%2520that%2520acoustic%2520maps%2520provide%2520a%2520compact%2520and%2520physically%2520interpretable%2520feature%2520space%2520for%2520replay%2520attack%2520detection%2520across%2520different%2520devices%2520and%2520acoustic%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Channel%20Replay%20Speech%20Detection%20using%20Acoustic%20Maps&entry.906535625=Michael%20Neri%20and%20Tuomas%20Virtanen&entry.1292438233=Replay%20attacks%20remain%20a%20critical%20vulnerability%20for%20automatic%20speaker%20verification%20systems%2C%20particularly%20in%20real-time%20voice%20assistant%20applications.%20In%20this%20work%2C%20we%20propose%20acoustic%20maps%20as%20a%20novel%20spatial%20feature%20representation%20for%20replay%20speech%20detection%20from%20multi-channel%20recordings.%20Derived%20from%20classical%20beamforming%20over%20discrete%20azimuth%20and%20elevation%20grids%2C%20acoustic%20maps%20encode%20directional%20energy%20distributions%20that%20reflect%20physical%20differences%20between%20human%20speech%20radiation%20and%20loudspeaker-based%20replay.%20A%20lightweight%20convolutional%20neural%20network%20is%20designed%20to%20operate%20on%20this%20representation%2C%20achieving%20competitive%20performance%20on%20the%20ReMASC%20dataset%20with%20approximately%206k%20trainable%20parameters.%20Experimental%20results%20show%20that%20acoustic%20maps%20provide%20a%20compact%20and%20physically%20interpretable%20feature%20space%20for%20replay%20attack%20detection%20across%20different%20devices%20and%20acoustic%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.16399v1&entry.124074799=Read"},
{"title": "Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding", "author": "Hui Ye and Shilong Yang and Chulong Zhang and Yexuan Xing and Juan Yu and Yaoqin Xie and Wei Zhang", "abstract": "Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.", "link": "http://arxiv.org/abs/2602.07568v2", "date": "2026-02-18", "relevancy": 2.5034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5147}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visualizing%20the%20Invisible%3A%20Enhancing%20Radiologist%20Performance%20in%20Breast%20Mammography%20via%20Task-Driven%20Chromatic%20Encoding&body=Title%3A%20Visualizing%20the%20Invisible%3A%20Enhancing%20Radiologist%20Performance%20in%20Breast%20Mammography%20via%20Task-Driven%20Chromatic%20Encoding%0AAuthor%3A%20Hui%20Ye%20and%20Shilong%20Yang%20and%20Chulong%20Zhang%20and%20Yexuan%20Xing%20and%20Juan%20Yu%20and%20Yaoqin%20Xie%20and%20Wei%20Zhang%0AAbstract%3A%20Purpose%3AMammography%20screening%20is%20less%20sensitive%20in%20dense%20breasts%2C%20where%20tissue%20overlap%20and%20subtle%20findings%20increase%20perceptual%20difficulty.%20We%20present%20MammoColor%2C%20an%20end-to-end%20framework%20with%20a%20Task-Driven%20Chromatic%20Encoding%20%28TDCE%29%20module%20that%20converts%20single-channel%20mammograms%20into%20TDCE-encoded%20views%20for%20visual%20augmentation.%20Materials%20and%20Methods%3AMammoColor%20couples%20a%20lightweight%20TDCE%20module%20with%20a%20BI-RADS%20triage%20classifier%20and%20was%20trained%20end-to-end%20on%20VinDr-Mammo.%20Performance%20was%20evaluated%20on%20an%20internal%20test%20set%2C%20two%20public%20datasets%20%28CBIS-DDSM%20and%20INBreast%29%2C%20and%20three%20external%20clinical%20cohorts.%20We%20also%20conducted%20a%20multi-reader%2C%20multi-case%20%28MRMC%29%20observer%20study%20with%20a%20washout%20period%2C%20comparing%20%281%29%20grayscale-only%2C%20%282%29%20TDCE-only%2C%20and%20%283%29%20side-by-side%20grayscale%2BTDCE.%20Results%3AOn%20VinDr-Mammo%2C%20MammoColor%20improved%20AUC%20from%200.7669%20to%200.8461%20%28P%3D0.004%29.%20Gains%20were%20larger%20in%20dense%20breasts%20%28AUC%200.749%20to%200.835%29.%20In%20the%20MRMC%20study%2C%20TDCE-encoded%20images%20improved%20specificity%20%280.90%20to%200.96%3B%20P%3D0.052%29%20with%20comparable%20sensitivity.%20Conclusion%3ATDCE%20provides%20a%20task-optimized%20chromatic%20representation%20that%20may%20improve%20perceptual%20salience%20and%20reduce%20false-positive%20recalls%20in%20mammography%20triage.%0ALink%3A%20http%3A//arxiv.org/abs/2602.07568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualizing%2520the%2520Invisible%253A%2520Enhancing%2520Radiologist%2520Performance%2520in%2520Breast%2520Mammography%2520via%2520Task-Driven%2520Chromatic%2520Encoding%26entry.906535625%3DHui%2520Ye%2520and%2520Shilong%2520Yang%2520and%2520Chulong%2520Zhang%2520and%2520Yexuan%2520Xing%2520and%2520Juan%2520Yu%2520and%2520Yaoqin%2520Xie%2520and%2520Wei%2520Zhang%26entry.1292438233%3DPurpose%253AMammography%2520screening%2520is%2520less%2520sensitive%2520in%2520dense%2520breasts%252C%2520where%2520tissue%2520overlap%2520and%2520subtle%2520findings%2520increase%2520perceptual%2520difficulty.%2520We%2520present%2520MammoColor%252C%2520an%2520end-to-end%2520framework%2520with%2520a%2520Task-Driven%2520Chromatic%2520Encoding%2520%2528TDCE%2529%2520module%2520that%2520converts%2520single-channel%2520mammograms%2520into%2520TDCE-encoded%2520views%2520for%2520visual%2520augmentation.%2520Materials%2520and%2520Methods%253AMammoColor%2520couples%2520a%2520lightweight%2520TDCE%2520module%2520with%2520a%2520BI-RADS%2520triage%2520classifier%2520and%2520was%2520trained%2520end-to-end%2520on%2520VinDr-Mammo.%2520Performance%2520was%2520evaluated%2520on%2520an%2520internal%2520test%2520set%252C%2520two%2520public%2520datasets%2520%2528CBIS-DDSM%2520and%2520INBreast%2529%252C%2520and%2520three%2520external%2520clinical%2520cohorts.%2520We%2520also%2520conducted%2520a%2520multi-reader%252C%2520multi-case%2520%2528MRMC%2529%2520observer%2520study%2520with%2520a%2520washout%2520period%252C%2520comparing%2520%25281%2529%2520grayscale-only%252C%2520%25282%2529%2520TDCE-only%252C%2520and%2520%25283%2529%2520side-by-side%2520grayscale%252BTDCE.%2520Results%253AOn%2520VinDr-Mammo%252C%2520MammoColor%2520improved%2520AUC%2520from%25200.7669%2520to%25200.8461%2520%2528P%253D0.004%2529.%2520Gains%2520were%2520larger%2520in%2520dense%2520breasts%2520%2528AUC%25200.749%2520to%25200.835%2529.%2520In%2520the%2520MRMC%2520study%252C%2520TDCE-encoded%2520images%2520improved%2520specificity%2520%25280.90%2520to%25200.96%253B%2520P%253D0.052%2529%2520with%2520comparable%2520sensitivity.%2520Conclusion%253ATDCE%2520provides%2520a%2520task-optimized%2520chromatic%2520representation%2520that%2520may%2520improve%2520perceptual%2520salience%2520and%2520reduce%2520false-positive%2520recalls%2520in%2520mammography%2520triage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.07568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualizing%20the%20Invisible%3A%20Enhancing%20Radiologist%20Performance%20in%20Breast%20Mammography%20via%20Task-Driven%20Chromatic%20Encoding&entry.906535625=Hui%20Ye%20and%20Shilong%20Yang%20and%20Chulong%20Zhang%20and%20Yexuan%20Xing%20and%20Juan%20Yu%20and%20Yaoqin%20Xie%20and%20Wei%20Zhang&entry.1292438233=Purpose%3AMammography%20screening%20is%20less%20sensitive%20in%20dense%20breasts%2C%20where%20tissue%20overlap%20and%20subtle%20findings%20increase%20perceptual%20difficulty.%20We%20present%20MammoColor%2C%20an%20end-to-end%20framework%20with%20a%20Task-Driven%20Chromatic%20Encoding%20%28TDCE%29%20module%20that%20converts%20single-channel%20mammograms%20into%20TDCE-encoded%20views%20for%20visual%20augmentation.%20Materials%20and%20Methods%3AMammoColor%20couples%20a%20lightweight%20TDCE%20module%20with%20a%20BI-RADS%20triage%20classifier%20and%20was%20trained%20end-to-end%20on%20VinDr-Mammo.%20Performance%20was%20evaluated%20on%20an%20internal%20test%20set%2C%20two%20public%20datasets%20%28CBIS-DDSM%20and%20INBreast%29%2C%20and%20three%20external%20clinical%20cohorts.%20We%20also%20conducted%20a%20multi-reader%2C%20multi-case%20%28MRMC%29%20observer%20study%20with%20a%20washout%20period%2C%20comparing%20%281%29%20grayscale-only%2C%20%282%29%20TDCE-only%2C%20and%20%283%29%20side-by-side%20grayscale%2BTDCE.%20Results%3AOn%20VinDr-Mammo%2C%20MammoColor%20improved%20AUC%20from%200.7669%20to%200.8461%20%28P%3D0.004%29.%20Gains%20were%20larger%20in%20dense%20breasts%20%28AUC%200.749%20to%200.835%29.%20In%20the%20MRMC%20study%2C%20TDCE-encoded%20images%20improved%20specificity%20%280.90%20to%200.96%3B%20P%3D0.052%29%20with%20comparable%20sensitivity.%20Conclusion%3ATDCE%20provides%20a%20task-optimized%20chromatic%20representation%20that%20may%20improve%20perceptual%20salience%20and%20reduce%20false-positive%20recalls%20in%20mammography%20triage.&entry.1838667208=http%3A//arxiv.org/abs/2602.07568v2&entry.124074799=Read"},
{"title": "From Growing to Looping: A Unified View of Iterative Computation in LLMs", "author": "Ferdinand Kapl and Emmanouil Angelis and Kaitlin Maile and Johannes von Oswald and Stefan Bauer", "abstract": "Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.", "link": "http://arxiv.org/abs/2602.16490v1", "date": "2026-02-18", "relevancy": 2.4955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Growing%20to%20Looping%3A%20A%20Unified%20View%20of%20Iterative%20Computation%20in%20LLMs&body=Title%3A%20From%20Growing%20to%20Looping%3A%20A%20Unified%20View%20of%20Iterative%20Computation%20in%20LLMs%0AAuthor%3A%20Ferdinand%20Kapl%20and%20Emmanouil%20Angelis%20and%20Kaitlin%20Maile%20and%20Johannes%20von%20Oswald%20and%20Stefan%20Bauer%0AAbstract%3A%20Looping%2C%20reusing%20a%20block%20of%20layers%20across%20depth%2C%20and%20depth%20growing%2C%20training%20shallow-to-deep%20models%20by%20duplicating%20middle%20layers%2C%20have%20both%20been%20linked%20to%20stronger%20reasoning%2C%20but%20their%20relationship%20remains%20unclear.%20We%20provide%20a%20mechanistic%20unification%3A%20looped%20and%20depth-grown%20models%20exhibit%20convergent%20depth-wise%20signatures%2C%20including%20increased%20reliance%20on%20late%20layers%20and%20recurring%20patterns%20aligned%20with%20the%20looped%20or%20grown%20block.%20These%20shared%20signatures%20support%20the%20view%20that%20their%20gains%20stem%20from%20a%20common%20form%20of%20iterative%20computation.%20Building%20on%20this%20connection%2C%20we%20show%20that%20the%20two%20techniques%20are%20adaptable%20and%20composable%3A%20applying%20inference-time%20looping%20to%20the%20middle%20blocks%20of%20a%20depth-grown%20model%20improves%20accuracy%20on%20some%20reasoning%20primitives%20by%20up%20to%20%242%5Ctimes%24%2C%20despite%20the%20model%20never%20being%20trained%20to%20loop.%20Both%20approaches%20also%20adapt%20better%20than%20the%20baseline%20when%20given%20more%20in-context%20examples%20or%20additional%20supervised%20fine-tuning%20data.%20Additionally%2C%20depth-grown%20models%20achieve%20the%20largest%20reasoning%20gains%20when%20using%20higher-quality%2C%20math-heavy%20cooldown%20mixtures%2C%20which%20can%20be%20further%20boosted%20by%20adapting%20a%20middle%20block%20to%20loop.%20Overall%2C%20our%20results%20position%20depth%20growth%20and%20looping%20as%20complementary%2C%20practical%20methods%20for%20inducing%20and%20scaling%20iterative%20computation%20to%20improve%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Growing%2520to%2520Looping%253A%2520A%2520Unified%2520View%2520of%2520Iterative%2520Computation%2520in%2520LLMs%26entry.906535625%3DFerdinand%2520Kapl%2520and%2520Emmanouil%2520Angelis%2520and%2520Kaitlin%2520Maile%2520and%2520Johannes%2520von%2520Oswald%2520and%2520Stefan%2520Bauer%26entry.1292438233%3DLooping%252C%2520reusing%2520a%2520block%2520of%2520layers%2520across%2520depth%252C%2520and%2520depth%2520growing%252C%2520training%2520shallow-to-deep%2520models%2520by%2520duplicating%2520middle%2520layers%252C%2520have%2520both%2520been%2520linked%2520to%2520stronger%2520reasoning%252C%2520but%2520their%2520relationship%2520remains%2520unclear.%2520We%2520provide%2520a%2520mechanistic%2520unification%253A%2520looped%2520and%2520depth-grown%2520models%2520exhibit%2520convergent%2520depth-wise%2520signatures%252C%2520including%2520increased%2520reliance%2520on%2520late%2520layers%2520and%2520recurring%2520patterns%2520aligned%2520with%2520the%2520looped%2520or%2520grown%2520block.%2520These%2520shared%2520signatures%2520support%2520the%2520view%2520that%2520their%2520gains%2520stem%2520from%2520a%2520common%2520form%2520of%2520iterative%2520computation.%2520Building%2520on%2520this%2520connection%252C%2520we%2520show%2520that%2520the%2520two%2520techniques%2520are%2520adaptable%2520and%2520composable%253A%2520applying%2520inference-time%2520looping%2520to%2520the%2520middle%2520blocks%2520of%2520a%2520depth-grown%2520model%2520improves%2520accuracy%2520on%2520some%2520reasoning%2520primitives%2520by%2520up%2520to%2520%25242%255Ctimes%2524%252C%2520despite%2520the%2520model%2520never%2520being%2520trained%2520to%2520loop.%2520Both%2520approaches%2520also%2520adapt%2520better%2520than%2520the%2520baseline%2520when%2520given%2520more%2520in-context%2520examples%2520or%2520additional%2520supervised%2520fine-tuning%2520data.%2520Additionally%252C%2520depth-grown%2520models%2520achieve%2520the%2520largest%2520reasoning%2520gains%2520when%2520using%2520higher-quality%252C%2520math-heavy%2520cooldown%2520mixtures%252C%2520which%2520can%2520be%2520further%2520boosted%2520by%2520adapting%2520a%2520middle%2520block%2520to%2520loop.%2520Overall%252C%2520our%2520results%2520position%2520depth%2520growth%2520and%2520looping%2520as%2520complementary%252C%2520practical%2520methods%2520for%2520inducing%2520and%2520scaling%2520iterative%2520computation%2520to%2520improve%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Growing%20to%20Looping%3A%20A%20Unified%20View%20of%20Iterative%20Computation%20in%20LLMs&entry.906535625=Ferdinand%20Kapl%20and%20Emmanouil%20Angelis%20and%20Kaitlin%20Maile%20and%20Johannes%20von%20Oswald%20and%20Stefan%20Bauer&entry.1292438233=Looping%2C%20reusing%20a%20block%20of%20layers%20across%20depth%2C%20and%20depth%20growing%2C%20training%20shallow-to-deep%20models%20by%20duplicating%20middle%20layers%2C%20have%20both%20been%20linked%20to%20stronger%20reasoning%2C%20but%20their%20relationship%20remains%20unclear.%20We%20provide%20a%20mechanistic%20unification%3A%20looped%20and%20depth-grown%20models%20exhibit%20convergent%20depth-wise%20signatures%2C%20including%20increased%20reliance%20on%20late%20layers%20and%20recurring%20patterns%20aligned%20with%20the%20looped%20or%20grown%20block.%20These%20shared%20signatures%20support%20the%20view%20that%20their%20gains%20stem%20from%20a%20common%20form%20of%20iterative%20computation.%20Building%20on%20this%20connection%2C%20we%20show%20that%20the%20two%20techniques%20are%20adaptable%20and%20composable%3A%20applying%20inference-time%20looping%20to%20the%20middle%20blocks%20of%20a%20depth-grown%20model%20improves%20accuracy%20on%20some%20reasoning%20primitives%20by%20up%20to%20%242%5Ctimes%24%2C%20despite%20the%20model%20never%20being%20trained%20to%20loop.%20Both%20approaches%20also%20adapt%20better%20than%20the%20baseline%20when%20given%20more%20in-context%20examples%20or%20additional%20supervised%20fine-tuning%20data.%20Additionally%2C%20depth-grown%20models%20achieve%20the%20largest%20reasoning%20gains%20when%20using%20higher-quality%2C%20math-heavy%20cooldown%20mixtures%2C%20which%20can%20be%20further%20boosted%20by%20adapting%20a%20middle%20block%20to%20loop.%20Overall%2C%20our%20results%20position%20depth%20growth%20and%20looping%20as%20complementary%2C%20practical%20methods%20for%20inducing%20and%20scaling%20iterative%20computation%20to%20improve%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.16490v1&entry.124074799=Read"},
{"title": "Learning to Learn from Language Feedback with Social Meta-Learning", "author": "Jonathan Cook and Diego Antognini and Martin Klissarov and Claudiu Musat and Edward Grefenstette", "abstract": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.", "link": "http://arxiv.org/abs/2602.16488v1", "date": "2026-02-18", "relevancy": 2.489, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Learn%20from%20Language%20Feedback%20with%20Social%20Meta-Learning&body=Title%3A%20Learning%20to%20Learn%20from%20Language%20Feedback%20with%20Social%20Meta-Learning%0AAuthor%3A%20Jonathan%20Cook%20and%20Diego%20Antognini%20and%20Martin%20Klissarov%20and%20Claudiu%20Musat%20and%20Edward%20Grefenstette%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20often%20struggle%20to%20learn%20from%20corrective%20feedback%20within%20a%20conversational%20context.%20They%20are%20rarely%20proactive%20in%20soliciting%20this%20feedback%2C%20even%20when%20faced%20with%20ambiguity%2C%20which%20can%20make%20their%20dialogues%20feel%20static%2C%20one-sided%2C%20and%20lacking%20the%20adaptive%20qualities%20of%20human%20conversation.%20To%20address%20these%20limitations%2C%20we%20draw%20inspiration%20from%20social%20meta-learning%20%28SML%29%20in%20humans%20-%20the%20process%20of%20learning%20how%20to%20learn%20from%20others.%20We%20formulate%20SML%20as%20a%20finetuning%20methodology%2C%20training%20LLMs%20to%20solicit%20and%20learn%20from%20language%20feedback%20in%20simulated%20pedagogical%20dialogues%2C%20where%20static%20tasks%20are%20converted%20into%20interactive%20social%20learning%20problems.%20SML%20effectively%20teaches%20models%20to%20use%20conversation%20to%20solve%20problems%20they%20are%20unable%20to%20solve%20in%20a%20single%20turn.%20This%20capability%20generalises%20across%20domains%3B%20SML%20on%20math%20problems%20produces%20models%20that%20better%20use%20feedback%20to%20solve%20coding%20problems%20and%20vice%20versa.%20Furthermore%2C%20despite%20being%20trained%20only%20on%20fully-specified%20problems%2C%20these%20models%20are%20better%20able%20to%20solve%20underspecified%20tasks%20where%20critical%20information%20is%20revealed%20over%20multiple%20turns.%20When%20faced%20with%20this%20ambiguity%2C%20SML-trained%20models%20make%20fewer%20premature%20answer%20attempts%20and%20are%20more%20likely%20to%20ask%20for%20the%20information%20they%20need.%20This%20work%20presents%20a%20scalable%20approach%20to%20developing%20AI%20systems%20that%20effectively%20learn%20from%20language%20feedback.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Learn%2520from%2520Language%2520Feedback%2520with%2520Social%2520Meta-Learning%26entry.906535625%3DJonathan%2520Cook%2520and%2520Diego%2520Antognini%2520and%2520Martin%2520Klissarov%2520and%2520Claudiu%2520Musat%2520and%2520Edward%2520Grefenstette%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520often%2520struggle%2520to%2520learn%2520from%2520corrective%2520feedback%2520within%2520a%2520conversational%2520context.%2520They%2520are%2520rarely%2520proactive%2520in%2520soliciting%2520this%2520feedback%252C%2520even%2520when%2520faced%2520with%2520ambiguity%252C%2520which%2520can%2520make%2520their%2520dialogues%2520feel%2520static%252C%2520one-sided%252C%2520and%2520lacking%2520the%2520adaptive%2520qualities%2520of%2520human%2520conversation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520draw%2520inspiration%2520from%2520social%2520meta-learning%2520%2528SML%2529%2520in%2520humans%2520-%2520the%2520process%2520of%2520learning%2520how%2520to%2520learn%2520from%2520others.%2520We%2520formulate%2520SML%2520as%2520a%2520finetuning%2520methodology%252C%2520training%2520LLMs%2520to%2520solicit%2520and%2520learn%2520from%2520language%2520feedback%2520in%2520simulated%2520pedagogical%2520dialogues%252C%2520where%2520static%2520tasks%2520are%2520converted%2520into%2520interactive%2520social%2520learning%2520problems.%2520SML%2520effectively%2520teaches%2520models%2520to%2520use%2520conversation%2520to%2520solve%2520problems%2520they%2520are%2520unable%2520to%2520solve%2520in%2520a%2520single%2520turn.%2520This%2520capability%2520generalises%2520across%2520domains%253B%2520SML%2520on%2520math%2520problems%2520produces%2520models%2520that%2520better%2520use%2520feedback%2520to%2520solve%2520coding%2520problems%2520and%2520vice%2520versa.%2520Furthermore%252C%2520despite%2520being%2520trained%2520only%2520on%2520fully-specified%2520problems%252C%2520these%2520models%2520are%2520better%2520able%2520to%2520solve%2520underspecified%2520tasks%2520where%2520critical%2520information%2520is%2520revealed%2520over%2520multiple%2520turns.%2520When%2520faced%2520with%2520this%2520ambiguity%252C%2520SML-trained%2520models%2520make%2520fewer%2520premature%2520answer%2520attempts%2520and%2520are%2520more%2520likely%2520to%2520ask%2520for%2520the%2520information%2520they%2520need.%2520This%2520work%2520presents%2520a%2520scalable%2520approach%2520to%2520developing%2520AI%2520systems%2520that%2520effectively%2520learn%2520from%2520language%2520feedback.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Learn%20from%20Language%20Feedback%20with%20Social%20Meta-Learning&entry.906535625=Jonathan%20Cook%20and%20Diego%20Antognini%20and%20Martin%20Klissarov%20and%20Claudiu%20Musat%20and%20Edward%20Grefenstette&entry.1292438233=Large%20language%20models%20%28LLMs%29%20often%20struggle%20to%20learn%20from%20corrective%20feedback%20within%20a%20conversational%20context.%20They%20are%20rarely%20proactive%20in%20soliciting%20this%20feedback%2C%20even%20when%20faced%20with%20ambiguity%2C%20which%20can%20make%20their%20dialogues%20feel%20static%2C%20one-sided%2C%20and%20lacking%20the%20adaptive%20qualities%20of%20human%20conversation.%20To%20address%20these%20limitations%2C%20we%20draw%20inspiration%20from%20social%20meta-learning%20%28SML%29%20in%20humans%20-%20the%20process%20of%20learning%20how%20to%20learn%20from%20others.%20We%20formulate%20SML%20as%20a%20finetuning%20methodology%2C%20training%20LLMs%20to%20solicit%20and%20learn%20from%20language%20feedback%20in%20simulated%20pedagogical%20dialogues%2C%20where%20static%20tasks%20are%20converted%20into%20interactive%20social%20learning%20problems.%20SML%20effectively%20teaches%20models%20to%20use%20conversation%20to%20solve%20problems%20they%20are%20unable%20to%20solve%20in%20a%20single%20turn.%20This%20capability%20generalises%20across%20domains%3B%20SML%20on%20math%20problems%20produces%20models%20that%20better%20use%20feedback%20to%20solve%20coding%20problems%20and%20vice%20versa.%20Furthermore%2C%20despite%20being%20trained%20only%20on%20fully-specified%20problems%2C%20these%20models%20are%20better%20able%20to%20solve%20underspecified%20tasks%20where%20critical%20information%20is%20revealed%20over%20multiple%20turns.%20When%20faced%20with%20this%20ambiguity%2C%20SML-trained%20models%20make%20fewer%20premature%20answer%20attempts%20and%20are%20more%20likely%20to%20ask%20for%20the%20information%20they%20need.%20This%20work%20presents%20a%20scalable%20approach%20to%20developing%20AI%20systems%20that%20effectively%20learn%20from%20language%20feedback.&entry.1838667208=http%3A//arxiv.org/abs/2602.16488v1&entry.124074799=Read"},
{"title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation", "author": "Yixue Zhang and Kun Wu and Zhi Gao and Zhen Zhao and Pei Ren and Zhiyuan Xu and Fei Liao and Xinhua Wang and Shichao Fan and Di Wu and Qiuxuan Feng and Meng Li and Zhengping Che and Chang Liu and Jian Tang", "abstract": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.", "link": "http://arxiv.org/abs/2602.16444v1", "date": "2026-02-18", "relevancy": 2.4527, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6201}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6103}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboGene%3A%20Boosting%20VLA%20Pre-training%20via%20Diversity-Driven%20Agentic%20Framework%20for%20Real-World%20Task%20Generation&body=Title%3A%20RoboGene%3A%20Boosting%20VLA%20Pre-training%20via%20Diversity-Driven%20Agentic%20Framework%20for%20Real-World%20Task%20Generation%0AAuthor%3A%20Yixue%20Zhang%20and%20Kun%20Wu%20and%20Zhi%20Gao%20and%20Zhen%20Zhao%20and%20Pei%20Ren%20and%20Zhiyuan%20Xu%20and%20Fei%20Liao%20and%20Xinhua%20Wang%20and%20Shichao%20Fan%20and%20Di%20Wu%20and%20Qiuxuan%20Feng%20and%20Meng%20Li%20and%20Zhengping%20Che%20and%20Chang%20Liu%20and%20Jian%20Tang%0AAbstract%3A%20The%20pursuit%20of%20general-purpose%20robotic%20manipulation%20is%20hindered%20by%20the%20scarcity%20of%20diverse%2C%20real-world%20interaction%20data.%20Unlike%20data%20collection%20from%20web%20in%20vision%20or%20language%2C%20robotic%20data%20collection%20is%20an%20active%20process%20incurring%20prohibitive%20physical%20costs.%20Consequently%2C%20automated%20task%20curation%20to%20maximize%20data%20value%20remains%20a%20critical%20yet%20under-explored%20challenge.%20Existing%20manual%20methods%20are%20unscalable%20and%20biased%20toward%20common%20tasks%2C%20while%20off-the-shelf%20foundation%20models%20often%20hallucinate%20physically%20infeasible%20instructions.%20To%20address%20this%2C%20we%20introduce%20RoboGene%2C%20an%20agentic%20framework%20designed%20to%20automate%20the%20generation%20of%20diverse%2C%20physically%20plausible%20manipulation%20tasks%20across%20single-arm%2C%20dual-arm%2C%20and%20mobile%20robots.%20RoboGene%20integrates%20three%20core%20components%3A%20diversity-driven%20sampling%20for%20broad%20task%20coverage%2C%20self-reflection%20mechanisms%20to%20enforce%20physical%20constraints%2C%20and%20human-in-the-loop%20refinement%20for%20continuous%20improvement.%20We%20conduct%20extensive%20quantitative%20analysis%20and%20large-scale%20real-world%20experiments%2C%20collecting%20datasets%20of%2018k%20trajectories%20and%20introducing%20novel%20metrics%20to%20assess%20task%20quality%2C%20feasibility%2C%20and%20diversity.%20Results%20demonstrate%20that%20RoboGene%20significantly%20outperforms%20state-of-the-art%20foundation%20models%20%28e.g.%2C%20GPT-4o%2C%20Gemini%202.5%20Pro%29.%20Furthermore%2C%20real-world%20experiments%20show%20that%20VLA%20models%20pre-trained%20with%20RoboGene%20achieve%20higher%20success%20rates%20and%20superior%20generalization%2C%20underscoring%20the%20importance%20of%20high-quality%20task%20generation.%20Our%20project%20is%20available%20at%20https%3A//robogene-boost-vla.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboGene%253A%2520Boosting%2520VLA%2520Pre-training%2520via%2520Diversity-Driven%2520Agentic%2520Framework%2520for%2520Real-World%2520Task%2520Generation%26entry.906535625%3DYixue%2520Zhang%2520and%2520Kun%2520Wu%2520and%2520Zhi%2520Gao%2520and%2520Zhen%2520Zhao%2520and%2520Pei%2520Ren%2520and%2520Zhiyuan%2520Xu%2520and%2520Fei%2520Liao%2520and%2520Xinhua%2520Wang%2520and%2520Shichao%2520Fan%2520and%2520Di%2520Wu%2520and%2520Qiuxuan%2520Feng%2520and%2520Meng%2520Li%2520and%2520Zhengping%2520Che%2520and%2520Chang%2520Liu%2520and%2520Jian%2520Tang%26entry.1292438233%3DThe%2520pursuit%2520of%2520general-purpose%2520robotic%2520manipulation%2520is%2520hindered%2520by%2520the%2520scarcity%2520of%2520diverse%252C%2520real-world%2520interaction%2520data.%2520Unlike%2520data%2520collection%2520from%2520web%2520in%2520vision%2520or%2520language%252C%2520robotic%2520data%2520collection%2520is%2520an%2520active%2520process%2520incurring%2520prohibitive%2520physical%2520costs.%2520Consequently%252C%2520automated%2520task%2520curation%2520to%2520maximize%2520data%2520value%2520remains%2520a%2520critical%2520yet%2520under-explored%2520challenge.%2520Existing%2520manual%2520methods%2520are%2520unscalable%2520and%2520biased%2520toward%2520common%2520tasks%252C%2520while%2520off-the-shelf%2520foundation%2520models%2520often%2520hallucinate%2520physically%2520infeasible%2520instructions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520RoboGene%252C%2520an%2520agentic%2520framework%2520designed%2520to%2520automate%2520the%2520generation%2520of%2520diverse%252C%2520physically%2520plausible%2520manipulation%2520tasks%2520across%2520single-arm%252C%2520dual-arm%252C%2520and%2520mobile%2520robots.%2520RoboGene%2520integrates%2520three%2520core%2520components%253A%2520diversity-driven%2520sampling%2520for%2520broad%2520task%2520coverage%252C%2520self-reflection%2520mechanisms%2520to%2520enforce%2520physical%2520constraints%252C%2520and%2520human-in-the-loop%2520refinement%2520for%2520continuous%2520improvement.%2520We%2520conduct%2520extensive%2520quantitative%2520analysis%2520and%2520large-scale%2520real-world%2520experiments%252C%2520collecting%2520datasets%2520of%252018k%2520trajectories%2520and%2520introducing%2520novel%2520metrics%2520to%2520assess%2520task%2520quality%252C%2520feasibility%252C%2520and%2520diversity.%2520Results%2520demonstrate%2520that%2520RoboGene%2520significantly%2520outperforms%2520state-of-the-art%2520foundation%2520models%2520%2528e.g.%252C%2520GPT-4o%252C%2520Gemini%25202.5%2520Pro%2529.%2520Furthermore%252C%2520real-world%2520experiments%2520show%2520that%2520VLA%2520models%2520pre-trained%2520with%2520RoboGene%2520achieve%2520higher%2520success%2520rates%2520and%2520superior%2520generalization%252C%2520underscoring%2520the%2520importance%2520of%2520high-quality%2520task%2520generation.%2520Our%2520project%2520is%2520available%2520at%2520https%253A//robogene-boost-vla.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboGene%3A%20Boosting%20VLA%20Pre-training%20via%20Diversity-Driven%20Agentic%20Framework%20for%20Real-World%20Task%20Generation&entry.906535625=Yixue%20Zhang%20and%20Kun%20Wu%20and%20Zhi%20Gao%20and%20Zhen%20Zhao%20and%20Pei%20Ren%20and%20Zhiyuan%20Xu%20and%20Fei%20Liao%20and%20Xinhua%20Wang%20and%20Shichao%20Fan%20and%20Di%20Wu%20and%20Qiuxuan%20Feng%20and%20Meng%20Li%20and%20Zhengping%20Che%20and%20Chang%20Liu%20and%20Jian%20Tang&entry.1292438233=The%20pursuit%20of%20general-purpose%20robotic%20manipulation%20is%20hindered%20by%20the%20scarcity%20of%20diverse%2C%20real-world%20interaction%20data.%20Unlike%20data%20collection%20from%20web%20in%20vision%20or%20language%2C%20robotic%20data%20collection%20is%20an%20active%20process%20incurring%20prohibitive%20physical%20costs.%20Consequently%2C%20automated%20task%20curation%20to%20maximize%20data%20value%20remains%20a%20critical%20yet%20under-explored%20challenge.%20Existing%20manual%20methods%20are%20unscalable%20and%20biased%20toward%20common%20tasks%2C%20while%20off-the-shelf%20foundation%20models%20often%20hallucinate%20physically%20infeasible%20instructions.%20To%20address%20this%2C%20we%20introduce%20RoboGene%2C%20an%20agentic%20framework%20designed%20to%20automate%20the%20generation%20of%20diverse%2C%20physically%20plausible%20manipulation%20tasks%20across%20single-arm%2C%20dual-arm%2C%20and%20mobile%20robots.%20RoboGene%20integrates%20three%20core%20components%3A%20diversity-driven%20sampling%20for%20broad%20task%20coverage%2C%20self-reflection%20mechanisms%20to%20enforce%20physical%20constraints%2C%20and%20human-in-the-loop%20refinement%20for%20continuous%20improvement.%20We%20conduct%20extensive%20quantitative%20analysis%20and%20large-scale%20real-world%20experiments%2C%20collecting%20datasets%20of%2018k%20trajectories%20and%20introducing%20novel%20metrics%20to%20assess%20task%20quality%2C%20feasibility%2C%20and%20diversity.%20Results%20demonstrate%20that%20RoboGene%20significantly%20outperforms%20state-of-the-art%20foundation%20models%20%28e.g.%2C%20GPT-4o%2C%20Gemini%202.5%20Pro%29.%20Furthermore%2C%20real-world%20experiments%20show%20that%20VLA%20models%20pre-trained%20with%20RoboGene%20achieve%20higher%20success%20rates%20and%20superior%20generalization%2C%20underscoring%20the%20importance%20of%20high-quality%20task%20generation.%20Our%20project%20is%20available%20at%20https%3A//robogene-boost-vla.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2602.16444v1&entry.124074799=Read"},
{"title": "Feature salience - not task-informativeness - drives machine learning model explanations", "author": "Benedict Clark and Marta Oliveira and Rick Wilming and Stefan Haufe", "abstract": "Explainable AI (XAI) promises to provide insight into machine learning models' decision processes, where one goal is to identify failures such as shortcut learning. This promise relies on the field's assumption that input features marked as important by an XAI must contain information about the target variable. However, it is unclear whether informativeness is indeed the main driver of importance attribution in practice, or if other data properties such as statistical suppression, novelty at test-time, or high feature salience substantially contribute. To clarify this, we trained deep learning models on three variants of a binary image classification task, in which translucent watermarks are either absent, act as class-dependent confounds, or represent class-independent noise. Results for five popular attribution methods show substantially elevated relative importance in watermarked areas (RIW) for all models regardless of the training setting ($R^2 \\geq .45$). By contrast, whether the presence of watermarks is class-dependent or not only has a marginal effect on RIW ($R^2 \\leq .03$), despite a clear impact impact on model performance and generalisation ability. XAI methods show similar behaviour to model-agnostic edge detection filters and attribute substantially less importance to watermarks when bright image intensities are encoded by smaller instead of larger feature values. These results indicate that importance attribution is most strongly driven by the salience of image structures at test time rather than statistical associations learned by machine learning models. Previous studies demonstrating successful XAI application should be reevaluated with respect to a possibly spurious concurrency of feature salience and informativeness, and workflows using feature attribution methods as building blocks should be scrutinised.", "link": "http://arxiv.org/abs/2602.09238v3", "date": "2026-02-18", "relevancy": 2.4423, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4911}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20salience%20-%20not%20task-informativeness%20-%20drives%20machine%20learning%20model%20explanations&body=Title%3A%20Feature%20salience%20-%20not%20task-informativeness%20-%20drives%20machine%20learning%20model%20explanations%0AAuthor%3A%20Benedict%20Clark%20and%20Marta%20Oliveira%20and%20Rick%20Wilming%20and%20Stefan%20Haufe%0AAbstract%3A%20Explainable%20AI%20%28XAI%29%20promises%20to%20provide%20insight%20into%20machine%20learning%20models%27%20decision%20processes%2C%20where%20one%20goal%20is%20to%20identify%20failures%20such%20as%20shortcut%20learning.%20This%20promise%20relies%20on%20the%20field%27s%20assumption%20that%20input%20features%20marked%20as%20important%20by%20an%20XAI%20must%20contain%20information%20about%20the%20target%20variable.%20However%2C%20it%20is%20unclear%20whether%20informativeness%20is%20indeed%20the%20main%20driver%20of%20importance%20attribution%20in%20practice%2C%20or%20if%20other%20data%20properties%20such%20as%20statistical%20suppression%2C%20novelty%20at%20test-time%2C%20or%20high%20feature%20salience%20substantially%20contribute.%20To%20clarify%20this%2C%20we%20trained%20deep%20learning%20models%20on%20three%20variants%20of%20a%20binary%20image%20classification%20task%2C%20in%20which%20translucent%20watermarks%20are%20either%20absent%2C%20act%20as%20class-dependent%20confounds%2C%20or%20represent%20class-independent%20noise.%20Results%20for%20five%20popular%20attribution%20methods%20show%20substantially%20elevated%20relative%20importance%20in%20watermarked%20areas%20%28RIW%29%20for%20all%20models%20regardless%20of%20the%20training%20setting%20%28%24R%5E2%20%5Cgeq%20.45%24%29.%20By%20contrast%2C%20whether%20the%20presence%20of%20watermarks%20is%20class-dependent%20or%20not%20only%20has%20a%20marginal%20effect%20on%20RIW%20%28%24R%5E2%20%5Cleq%20.03%24%29%2C%20despite%20a%20clear%20impact%20impact%20on%20model%20performance%20and%20generalisation%20ability.%20XAI%20methods%20show%20similar%20behaviour%20to%20model-agnostic%20edge%20detection%20filters%20and%20attribute%20substantially%20less%20importance%20to%20watermarks%20when%20bright%20image%20intensities%20are%20encoded%20by%20smaller%20instead%20of%20larger%20feature%20values.%20These%20results%20indicate%20that%20importance%20attribution%20is%20most%20strongly%20driven%20by%20the%20salience%20of%20image%20structures%20at%20test%20time%20rather%20than%20statistical%20associations%20learned%20by%20machine%20learning%20models.%20Previous%20studies%20demonstrating%20successful%20XAI%20application%20should%20be%20reevaluated%20with%20respect%20to%20a%20possibly%20spurious%20concurrency%20of%20feature%20salience%20and%20informativeness%2C%20and%20workflows%20using%20feature%20attribution%20methods%20as%20building%20blocks%20should%20be%20scrutinised.%0ALink%3A%20http%3A//arxiv.org/abs/2602.09238v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520salience%2520-%2520not%2520task-informativeness%2520-%2520drives%2520machine%2520learning%2520model%2520explanations%26entry.906535625%3DBenedict%2520Clark%2520and%2520Marta%2520Oliveira%2520and%2520Rick%2520Wilming%2520and%2520Stefan%2520Haufe%26entry.1292438233%3DExplainable%2520AI%2520%2528XAI%2529%2520promises%2520to%2520provide%2520insight%2520into%2520machine%2520learning%2520models%2527%2520decision%2520processes%252C%2520where%2520one%2520goal%2520is%2520to%2520identify%2520failures%2520such%2520as%2520shortcut%2520learning.%2520This%2520promise%2520relies%2520on%2520the%2520field%2527s%2520assumption%2520that%2520input%2520features%2520marked%2520as%2520important%2520by%2520an%2520XAI%2520must%2520contain%2520information%2520about%2520the%2520target%2520variable.%2520However%252C%2520it%2520is%2520unclear%2520whether%2520informativeness%2520is%2520indeed%2520the%2520main%2520driver%2520of%2520importance%2520attribution%2520in%2520practice%252C%2520or%2520if%2520other%2520data%2520properties%2520such%2520as%2520statistical%2520suppression%252C%2520novelty%2520at%2520test-time%252C%2520or%2520high%2520feature%2520salience%2520substantially%2520contribute.%2520To%2520clarify%2520this%252C%2520we%2520trained%2520deep%2520learning%2520models%2520on%2520three%2520variants%2520of%2520a%2520binary%2520image%2520classification%2520task%252C%2520in%2520which%2520translucent%2520watermarks%2520are%2520either%2520absent%252C%2520act%2520as%2520class-dependent%2520confounds%252C%2520or%2520represent%2520class-independent%2520noise.%2520Results%2520for%2520five%2520popular%2520attribution%2520methods%2520show%2520substantially%2520elevated%2520relative%2520importance%2520in%2520watermarked%2520areas%2520%2528RIW%2529%2520for%2520all%2520models%2520regardless%2520of%2520the%2520training%2520setting%2520%2528%2524R%255E2%2520%255Cgeq%2520.45%2524%2529.%2520By%2520contrast%252C%2520whether%2520the%2520presence%2520of%2520watermarks%2520is%2520class-dependent%2520or%2520not%2520only%2520has%2520a%2520marginal%2520effect%2520on%2520RIW%2520%2528%2524R%255E2%2520%255Cleq%2520.03%2524%2529%252C%2520despite%2520a%2520clear%2520impact%2520impact%2520on%2520model%2520performance%2520and%2520generalisation%2520ability.%2520XAI%2520methods%2520show%2520similar%2520behaviour%2520to%2520model-agnostic%2520edge%2520detection%2520filters%2520and%2520attribute%2520substantially%2520less%2520importance%2520to%2520watermarks%2520when%2520bright%2520image%2520intensities%2520are%2520encoded%2520by%2520smaller%2520instead%2520of%2520larger%2520feature%2520values.%2520These%2520results%2520indicate%2520that%2520importance%2520attribution%2520is%2520most%2520strongly%2520driven%2520by%2520the%2520salience%2520of%2520image%2520structures%2520at%2520test%2520time%2520rather%2520than%2520statistical%2520associations%2520learned%2520by%2520machine%2520learning%2520models.%2520Previous%2520studies%2520demonstrating%2520successful%2520XAI%2520application%2520should%2520be%2520reevaluated%2520with%2520respect%2520to%2520a%2520possibly%2520spurious%2520concurrency%2520of%2520feature%2520salience%2520and%2520informativeness%252C%2520and%2520workflows%2520using%2520feature%2520attribution%2520methods%2520as%2520building%2520blocks%2520should%2520be%2520scrutinised.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.09238v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20salience%20-%20not%20task-informativeness%20-%20drives%20machine%20learning%20model%20explanations&entry.906535625=Benedict%20Clark%20and%20Marta%20Oliveira%20and%20Rick%20Wilming%20and%20Stefan%20Haufe&entry.1292438233=Explainable%20AI%20%28XAI%29%20promises%20to%20provide%20insight%20into%20machine%20learning%20models%27%20decision%20processes%2C%20where%20one%20goal%20is%20to%20identify%20failures%20such%20as%20shortcut%20learning.%20This%20promise%20relies%20on%20the%20field%27s%20assumption%20that%20input%20features%20marked%20as%20important%20by%20an%20XAI%20must%20contain%20information%20about%20the%20target%20variable.%20However%2C%20it%20is%20unclear%20whether%20informativeness%20is%20indeed%20the%20main%20driver%20of%20importance%20attribution%20in%20practice%2C%20or%20if%20other%20data%20properties%20such%20as%20statistical%20suppression%2C%20novelty%20at%20test-time%2C%20or%20high%20feature%20salience%20substantially%20contribute.%20To%20clarify%20this%2C%20we%20trained%20deep%20learning%20models%20on%20three%20variants%20of%20a%20binary%20image%20classification%20task%2C%20in%20which%20translucent%20watermarks%20are%20either%20absent%2C%20act%20as%20class-dependent%20confounds%2C%20or%20represent%20class-independent%20noise.%20Results%20for%20five%20popular%20attribution%20methods%20show%20substantially%20elevated%20relative%20importance%20in%20watermarked%20areas%20%28RIW%29%20for%20all%20models%20regardless%20of%20the%20training%20setting%20%28%24R%5E2%20%5Cgeq%20.45%24%29.%20By%20contrast%2C%20whether%20the%20presence%20of%20watermarks%20is%20class-dependent%20or%20not%20only%20has%20a%20marginal%20effect%20on%20RIW%20%28%24R%5E2%20%5Cleq%20.03%24%29%2C%20despite%20a%20clear%20impact%20impact%20on%20model%20performance%20and%20generalisation%20ability.%20XAI%20methods%20show%20similar%20behaviour%20to%20model-agnostic%20edge%20detection%20filters%20and%20attribute%20substantially%20less%20importance%20to%20watermarks%20when%20bright%20image%20intensities%20are%20encoded%20by%20smaller%20instead%20of%20larger%20feature%20values.%20These%20results%20indicate%20that%20importance%20attribution%20is%20most%20strongly%20driven%20by%20the%20salience%20of%20image%20structures%20at%20test%20time%20rather%20than%20statistical%20associations%20learned%20by%20machine%20learning%20models.%20Previous%20studies%20demonstrating%20successful%20XAI%20application%20should%20be%20reevaluated%20with%20respect%20to%20a%20possibly%20spurious%20concurrency%20of%20feature%20salience%20and%20informativeness%2C%20and%20workflows%20using%20feature%20attribution%20methods%20as%20building%20blocks%20should%20be%20scrutinised.&entry.1838667208=http%3A//arxiv.org/abs/2602.09238v3&entry.124074799=Read"},
{"title": "Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes", "author": "Zhanliang Wang and Da Wu and Quan Nguyen and Kai Wang", "abstract": "Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes.", "link": "http://arxiv.org/abs/2503.12286v2", "date": "2026-02-18", "relevancy": 2.4087, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Chain-of-Thought%20and%20Retrieval%20Augmented%20Generation%20Enhances%20Rare%20Disease%20Diagnosis%20from%20Clinical%20Notes&body=Title%3A%20Integrating%20Chain-of-Thought%20and%20Retrieval%20Augmented%20Generation%20Enhances%20Rare%20Disease%20Diagnosis%20from%20Clinical%20Notes%0AAuthor%3A%20Zhanliang%20Wang%20and%20Da%20Wu%20and%20Quan%20Nguyen%20and%20Kai%20Wang%0AAbstract%3A%20Background%3A%20Several%20studies%20show%20that%20large%20language%20models%20%28LLMs%29%20struggle%20with%20phenotype-driven%20gene%20prioritization%20for%20rare%20diseases.%20These%20studies%20typically%20use%20Human%20Phenotype%20Ontology%20%28HPO%29%20terms%20to%20prompt%20foundation%20models%20like%20GPT%20and%20LLaMA%20to%20predict%20candidate%20genes.%20However%2C%20in%20real-world%20settings%2C%20foundation%20models%20are%20not%20optimized%20for%20domain-specific%20tasks%20like%20clinical%20diagnosis%2C%20yet%20inputs%20are%20unstructured%20clinical%20notes%20rather%20than%20standardized%20terms.%20How%20LLMs%20can%20be%20instructed%20to%20predict%20candidate%20genes%20or%20disease%20diagnosis%20from%20unstructured%20clinical%20notes%20remains%20a%20major%20challenge.%20Methods%3A%20We%20introduce%20RAG-driven%20CoT%20and%20CoT-driven%20RAG%2C%20two%20methods%20that%20combine%20Chain-of-Thought%20%28CoT%29%20and%20Retrieval%20Augmented%20Generation%20%28RAG%29%20to%20analyze%20clinical%20notes.%20A%20five-question%20CoT%20protocol%20mimics%20expert%20reasoning%2C%20while%20RAG%20retrieves%20data%20from%20sources%20like%20HPO%20and%20OMIM%20%28Online%20Mendelian%20Inheritance%20in%20Man%29.%20We%20evaluated%20these%20approaches%20on%20rare%20disease%20datasets%2C%20including%205%2C980%20Phenopacket-derived%20notes%2C%20255%20literature-based%20narratives%2C%20and%20220%20in-house%20clinical%20notes%20from%20Childrens%20Hospital%20of%20Philadelphia.%20Results%3A%20We%20found%20that%20recent%20foundations%20models%2C%20including%20Llama%203.3-70B-Instruct%20and%20DeepSeek-R1-Distill-Llama-70B%2C%20outperformed%20earlier%20versions%20such%20as%20Llama%202%20and%20GPT-3.5.%20We%20also%20showed%20that%20RAG-driven%20CoT%20and%20CoT-driven%20RAG%20both%20outperform%20foundation%20models%20in%20candidate%20gene%20prioritization%20from%20clinical%20notes%3B%20in%20particular%2C%20both%20methods%20with%20DeepSeek%20backbone%20resulted%20in%20a%20top-10%20gene%20accuracy%20of%20over%2040%25%20on%20Phenopacket-derived%20clinical%20notes.%20RAG-driven%20CoT%20works%20better%20for%20high-quality%20notes%2C%20where%20early%20retrieval%20can%20anchor%20the%20subsequent%20reasoning%20steps%20in%20domain-specific%20evidence%2C%20while%20CoT-driven%20RAG%20has%20advantage%20when%20processing%20lengthy%20and%20noisy%20notes.%0ALink%3A%20http%3A//arxiv.org/abs/2503.12286v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Chain-of-Thought%2520and%2520Retrieval%2520Augmented%2520Generation%2520Enhances%2520Rare%2520Disease%2520Diagnosis%2520from%2520Clinical%2520Notes%26entry.906535625%3DZhanliang%2520Wang%2520and%2520Da%2520Wu%2520and%2520Quan%2520Nguyen%2520and%2520Kai%2520Wang%26entry.1292438233%3DBackground%253A%2520Several%2520studies%2520show%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520struggle%2520with%2520phenotype-driven%2520gene%2520prioritization%2520for%2520rare%2520diseases.%2520These%2520studies%2520typically%2520use%2520Human%2520Phenotype%2520Ontology%2520%2528HPO%2529%2520terms%2520to%2520prompt%2520foundation%2520models%2520like%2520GPT%2520and%2520LLaMA%2520to%2520predict%2520candidate%2520genes.%2520However%252C%2520in%2520real-world%2520settings%252C%2520foundation%2520models%2520are%2520not%2520optimized%2520for%2520domain-specific%2520tasks%2520like%2520clinical%2520diagnosis%252C%2520yet%2520inputs%2520are%2520unstructured%2520clinical%2520notes%2520rather%2520than%2520standardized%2520terms.%2520How%2520LLMs%2520can%2520be%2520instructed%2520to%2520predict%2520candidate%2520genes%2520or%2520disease%2520diagnosis%2520from%2520unstructured%2520clinical%2520notes%2520remains%2520a%2520major%2520challenge.%2520Methods%253A%2520We%2520introduce%2520RAG-driven%2520CoT%2520and%2520CoT-driven%2520RAG%252C%2520two%2520methods%2520that%2520combine%2520Chain-of-Thought%2520%2528CoT%2529%2520and%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520to%2520analyze%2520clinical%2520notes.%2520A%2520five-question%2520CoT%2520protocol%2520mimics%2520expert%2520reasoning%252C%2520while%2520RAG%2520retrieves%2520data%2520from%2520sources%2520like%2520HPO%2520and%2520OMIM%2520%2528Online%2520Mendelian%2520Inheritance%2520in%2520Man%2529.%2520We%2520evaluated%2520these%2520approaches%2520on%2520rare%2520disease%2520datasets%252C%2520including%25205%252C980%2520Phenopacket-derived%2520notes%252C%2520255%2520literature-based%2520narratives%252C%2520and%2520220%2520in-house%2520clinical%2520notes%2520from%2520Childrens%2520Hospital%2520of%2520Philadelphia.%2520Results%253A%2520We%2520found%2520that%2520recent%2520foundations%2520models%252C%2520including%2520Llama%25203.3-70B-Instruct%2520and%2520DeepSeek-R1-Distill-Llama-70B%252C%2520outperformed%2520earlier%2520versions%2520such%2520as%2520Llama%25202%2520and%2520GPT-3.5.%2520We%2520also%2520showed%2520that%2520RAG-driven%2520CoT%2520and%2520CoT-driven%2520RAG%2520both%2520outperform%2520foundation%2520models%2520in%2520candidate%2520gene%2520prioritization%2520from%2520clinical%2520notes%253B%2520in%2520particular%252C%2520both%2520methods%2520with%2520DeepSeek%2520backbone%2520resulted%2520in%2520a%2520top-10%2520gene%2520accuracy%2520of%2520over%252040%2525%2520on%2520Phenopacket-derived%2520clinical%2520notes.%2520RAG-driven%2520CoT%2520works%2520better%2520for%2520high-quality%2520notes%252C%2520where%2520early%2520retrieval%2520can%2520anchor%2520the%2520subsequent%2520reasoning%2520steps%2520in%2520domain-specific%2520evidence%252C%2520while%2520CoT-driven%2520RAG%2520has%2520advantage%2520when%2520processing%2520lengthy%2520and%2520noisy%2520notes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12286v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Chain-of-Thought%20and%20Retrieval%20Augmented%20Generation%20Enhances%20Rare%20Disease%20Diagnosis%20from%20Clinical%20Notes&entry.906535625=Zhanliang%20Wang%20and%20Da%20Wu%20and%20Quan%20Nguyen%20and%20Kai%20Wang&entry.1292438233=Background%3A%20Several%20studies%20show%20that%20large%20language%20models%20%28LLMs%29%20struggle%20with%20phenotype-driven%20gene%20prioritization%20for%20rare%20diseases.%20These%20studies%20typically%20use%20Human%20Phenotype%20Ontology%20%28HPO%29%20terms%20to%20prompt%20foundation%20models%20like%20GPT%20and%20LLaMA%20to%20predict%20candidate%20genes.%20However%2C%20in%20real-world%20settings%2C%20foundation%20models%20are%20not%20optimized%20for%20domain-specific%20tasks%20like%20clinical%20diagnosis%2C%20yet%20inputs%20are%20unstructured%20clinical%20notes%20rather%20than%20standardized%20terms.%20How%20LLMs%20can%20be%20instructed%20to%20predict%20candidate%20genes%20or%20disease%20diagnosis%20from%20unstructured%20clinical%20notes%20remains%20a%20major%20challenge.%20Methods%3A%20We%20introduce%20RAG-driven%20CoT%20and%20CoT-driven%20RAG%2C%20two%20methods%20that%20combine%20Chain-of-Thought%20%28CoT%29%20and%20Retrieval%20Augmented%20Generation%20%28RAG%29%20to%20analyze%20clinical%20notes.%20A%20five-question%20CoT%20protocol%20mimics%20expert%20reasoning%2C%20while%20RAG%20retrieves%20data%20from%20sources%20like%20HPO%20and%20OMIM%20%28Online%20Mendelian%20Inheritance%20in%20Man%29.%20We%20evaluated%20these%20approaches%20on%20rare%20disease%20datasets%2C%20including%205%2C980%20Phenopacket-derived%20notes%2C%20255%20literature-based%20narratives%2C%20and%20220%20in-house%20clinical%20notes%20from%20Childrens%20Hospital%20of%20Philadelphia.%20Results%3A%20We%20found%20that%20recent%20foundations%20models%2C%20including%20Llama%203.3-70B-Instruct%20and%20DeepSeek-R1-Distill-Llama-70B%2C%20outperformed%20earlier%20versions%20such%20as%20Llama%202%20and%20GPT-3.5.%20We%20also%20showed%20that%20RAG-driven%20CoT%20and%20CoT-driven%20RAG%20both%20outperform%20foundation%20models%20in%20candidate%20gene%20prioritization%20from%20clinical%20notes%3B%20in%20particular%2C%20both%20methods%20with%20DeepSeek%20backbone%20resulted%20in%20a%20top-10%20gene%20accuracy%20of%20over%2040%25%20on%20Phenopacket-derived%20clinical%20notes.%20RAG-driven%20CoT%20works%20better%20for%20high-quality%20notes%2C%20where%20early%20retrieval%20can%20anchor%20the%20subsequent%20reasoning%20steps%20in%20domain-specific%20evidence%2C%20while%20CoT-driven%20RAG%20has%20advantage%20when%20processing%20lengthy%20and%20noisy%20notes.&entry.1838667208=http%3A//arxiv.org/abs/2503.12286v2&entry.124074799=Read"},
{"title": "Vision and Language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning", "author": "Ross Greer and Maitrayee Keskar and Angel Martinez-Sanchez and Parthib Roy and Shashank Shriram and Mohan Trivedi", "abstract": "Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.", "link": "http://arxiv.org/abs/2602.07680v2", "date": "2026-02-18", "relevancy": 2.3972, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.601}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20and%20Language%3A%20Novel%20Representations%20and%20Artificial%20intelligence%20for%20Driving%20Scene%20Safety%20Assessment%20and%20Autonomous%20Vehicle%20Planning&body=Title%3A%20Vision%20and%20Language%3A%20Novel%20Representations%20and%20Artificial%20intelligence%20for%20Driving%20Scene%20Safety%20Assessment%20and%20Autonomous%20Vehicle%20Planning%0AAuthor%3A%20Ross%20Greer%20and%20Maitrayee%20Keskar%20and%20Angel%20Martinez-Sanchez%20and%20Parthib%20Roy%20and%20Shashank%20Shriram%20and%20Mohan%20Trivedi%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20have%20recently%20emerged%20as%20powerful%20representation%20learning%20systems%20that%20align%20visual%20observations%20with%20natural%20language%20concepts%2C%20offering%20new%20opportunities%20for%20semantic%20reasoning%20in%20safety-critical%20autonomous%20driving.%20This%20paper%20investigates%20how%20vision-language%20representations%20support%20driving%20scene%20safety%20assessment%20and%20decision-making%20when%20integrated%20into%20perception%2C%20prediction%2C%20and%20planning%20pipelines.%20We%20study%20three%20complementary%20system-level%20use%20cases.%20First%2C%20we%20introduce%20a%20lightweight%2C%20category-agnostic%20hazard%20screening%20approach%20leveraging%20CLIP-based%20image-text%20similarity%20to%20produce%20a%20low-latency%20semantic%20hazard%20signal.%20This%20enables%20robust%20detection%20of%20diverse%20and%20out-of-distribution%20road%20hazards%20without%20explicit%20object%20detection%20or%20visual%20question%20answering.%20Second%2C%20we%20examine%20the%20integration%20of%20scene-level%20vision-language%20embeddings%20into%20a%20transformer-based%20trajectory%20planning%20framework%20using%20the%20Waymo%20Open%20Dataset.%20Our%20results%20show%20that%20naively%20conditioning%20planners%20on%20global%20embeddings%20does%20not%20improve%20trajectory%20accuracy%2C%20highlighting%20the%20importance%20of%20representation-task%20alignment%20and%20motivating%20the%20development%20of%20task-informed%20extraction%20methods%20for%20safety-critical%20planning.%20Third%2C%20we%20investigate%20natural%20language%20as%20an%20explicit%20behavioral%20constraint%20on%20motion%20planning%20using%20the%20doScenes%20dataset.%20In%20this%20setting%2C%20passenger-style%20instructions%20grounded%20in%20visual%20scene%20elements%20suppress%20rare%20but%20severe%20planning%20failures%20and%20improve%20safety-aligned%20behavior%20in%20ambiguous%20scenarios.%20Taken%20together%2C%20these%20findings%20demonstrate%20that%20vision-language%20representations%20hold%20significant%20promise%20for%20autonomous%20driving%20safety%20when%20used%20to%20express%20semantic%20risk%2C%20intent%2C%20and%20behavioral%20constraints.%20Realizing%20this%20potential%20is%20fundamentally%20an%20engineering%20problem%20requiring%20careful%20system%20design%20and%20structured%20grounding%20rather%20than%20direct%20feature%20injection.%0ALink%3A%20http%3A//arxiv.org/abs/2602.07680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520and%2520Language%253A%2520Novel%2520Representations%2520and%2520Artificial%2520intelligence%2520for%2520Driving%2520Scene%2520Safety%2520Assessment%2520and%2520Autonomous%2520Vehicle%2520Planning%26entry.906535625%3DRoss%2520Greer%2520and%2520Maitrayee%2520Keskar%2520and%2520Angel%2520Martinez-Sanchez%2520and%2520Parthib%2520Roy%2520and%2520Shashank%2520Shriram%2520and%2520Mohan%2520Trivedi%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520have%2520recently%2520emerged%2520as%2520powerful%2520representation%2520learning%2520systems%2520that%2520align%2520visual%2520observations%2520with%2520natural%2520language%2520concepts%252C%2520offering%2520new%2520opportunities%2520for%2520semantic%2520reasoning%2520in%2520safety-critical%2520autonomous%2520driving.%2520This%2520paper%2520investigates%2520how%2520vision-language%2520representations%2520support%2520driving%2520scene%2520safety%2520assessment%2520and%2520decision-making%2520when%2520integrated%2520into%2520perception%252C%2520prediction%252C%2520and%2520planning%2520pipelines.%2520We%2520study%2520three%2520complementary%2520system-level%2520use%2520cases.%2520First%252C%2520we%2520introduce%2520a%2520lightweight%252C%2520category-agnostic%2520hazard%2520screening%2520approach%2520leveraging%2520CLIP-based%2520image-text%2520similarity%2520to%2520produce%2520a%2520low-latency%2520semantic%2520hazard%2520signal.%2520This%2520enables%2520robust%2520detection%2520of%2520diverse%2520and%2520out-of-distribution%2520road%2520hazards%2520without%2520explicit%2520object%2520detection%2520or%2520visual%2520question%2520answering.%2520Second%252C%2520we%2520examine%2520the%2520integration%2520of%2520scene-level%2520vision-language%2520embeddings%2520into%2520a%2520transformer-based%2520trajectory%2520planning%2520framework%2520using%2520the%2520Waymo%2520Open%2520Dataset.%2520Our%2520results%2520show%2520that%2520naively%2520conditioning%2520planners%2520on%2520global%2520embeddings%2520does%2520not%2520improve%2520trajectory%2520accuracy%252C%2520highlighting%2520the%2520importance%2520of%2520representation-task%2520alignment%2520and%2520motivating%2520the%2520development%2520of%2520task-informed%2520extraction%2520methods%2520for%2520safety-critical%2520planning.%2520Third%252C%2520we%2520investigate%2520natural%2520language%2520as%2520an%2520explicit%2520behavioral%2520constraint%2520on%2520motion%2520planning%2520using%2520the%2520doScenes%2520dataset.%2520In%2520this%2520setting%252C%2520passenger-style%2520instructions%2520grounded%2520in%2520visual%2520scene%2520elements%2520suppress%2520rare%2520but%2520severe%2520planning%2520failures%2520and%2520improve%2520safety-aligned%2520behavior%2520in%2520ambiguous%2520scenarios.%2520Taken%2520together%252C%2520these%2520findings%2520demonstrate%2520that%2520vision-language%2520representations%2520hold%2520significant%2520promise%2520for%2520autonomous%2520driving%2520safety%2520when%2520used%2520to%2520express%2520semantic%2520risk%252C%2520intent%252C%2520and%2520behavioral%2520constraints.%2520Realizing%2520this%2520potential%2520is%2520fundamentally%2520an%2520engineering%2520problem%2520requiring%2520careful%2520system%2520design%2520and%2520structured%2520grounding%2520rather%2520than%2520direct%2520feature%2520injection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.07680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20and%20Language%3A%20Novel%20Representations%20and%20Artificial%20intelligence%20for%20Driving%20Scene%20Safety%20Assessment%20and%20Autonomous%20Vehicle%20Planning&entry.906535625=Ross%20Greer%20and%20Maitrayee%20Keskar%20and%20Angel%20Martinez-Sanchez%20and%20Parthib%20Roy%20and%20Shashank%20Shriram%20and%20Mohan%20Trivedi&entry.1292438233=Vision-language%20models%20%28VLMs%29%20have%20recently%20emerged%20as%20powerful%20representation%20learning%20systems%20that%20align%20visual%20observations%20with%20natural%20language%20concepts%2C%20offering%20new%20opportunities%20for%20semantic%20reasoning%20in%20safety-critical%20autonomous%20driving.%20This%20paper%20investigates%20how%20vision-language%20representations%20support%20driving%20scene%20safety%20assessment%20and%20decision-making%20when%20integrated%20into%20perception%2C%20prediction%2C%20and%20planning%20pipelines.%20We%20study%20three%20complementary%20system-level%20use%20cases.%20First%2C%20we%20introduce%20a%20lightweight%2C%20category-agnostic%20hazard%20screening%20approach%20leveraging%20CLIP-based%20image-text%20similarity%20to%20produce%20a%20low-latency%20semantic%20hazard%20signal.%20This%20enables%20robust%20detection%20of%20diverse%20and%20out-of-distribution%20road%20hazards%20without%20explicit%20object%20detection%20or%20visual%20question%20answering.%20Second%2C%20we%20examine%20the%20integration%20of%20scene-level%20vision-language%20embeddings%20into%20a%20transformer-based%20trajectory%20planning%20framework%20using%20the%20Waymo%20Open%20Dataset.%20Our%20results%20show%20that%20naively%20conditioning%20planners%20on%20global%20embeddings%20does%20not%20improve%20trajectory%20accuracy%2C%20highlighting%20the%20importance%20of%20representation-task%20alignment%20and%20motivating%20the%20development%20of%20task-informed%20extraction%20methods%20for%20safety-critical%20planning.%20Third%2C%20we%20investigate%20natural%20language%20as%20an%20explicit%20behavioral%20constraint%20on%20motion%20planning%20using%20the%20doScenes%20dataset.%20In%20this%20setting%2C%20passenger-style%20instructions%20grounded%20in%20visual%20scene%20elements%20suppress%20rare%20but%20severe%20planning%20failures%20and%20improve%20safety-aligned%20behavior%20in%20ambiguous%20scenarios.%20Taken%20together%2C%20these%20findings%20demonstrate%20that%20vision-language%20representations%20hold%20significant%20promise%20for%20autonomous%20driving%20safety%20when%20used%20to%20express%20semantic%20risk%2C%20intent%2C%20and%20behavioral%20constraints.%20Realizing%20this%20potential%20is%20fundamentally%20an%20engineering%20problem%20requiring%20careful%20system%20design%20and%20structured%20grounding%20rather%20than%20direct%20feature%20injection.&entry.1838667208=http%3A//arxiv.org/abs/2602.07680v2&entry.124074799=Read"},
{"title": "Benchmarking Stochastic Approximation Algorithms for Fairness-Constrained Training of Deep Neural Networks", "author": "Andrii Kliachkin and Jana Lep\u0161ov\u00e1 and Gilles Bareilles and Jakub Mare\u010dek", "abstract": "The ability to train Deep Neural Networks (DNNs) with constraints is instrumental in improving the fairness of modern machine-learning models. Many algorithms have been analysed in recent years, and yet there is no standard, widely accepted method for the constrained training of DNNs. In this paper, we provide a challenging benchmark of real-world large-scale fairness-constrained learning tasks, built on top of the US Census (Folktables). We point out the theoretical challenges of such tasks and review the main approaches in stochastic approximation algorithms. Finally, we demonstrate the use of the benchmark by implementing and comparing three recently proposed, but as-of-yet unimplemented, algorithms both in terms of optimization performance, and fairness improvement. We release the code of the benchmark as a Python package at https://github.com/humancompatible/train.", "link": "http://arxiv.org/abs/2507.04033v2", "date": "2026-02-18", "relevancy": 2.3937, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5194}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4586}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Stochastic%20Approximation%20Algorithms%20for%20Fairness-Constrained%20Training%20of%20Deep%20Neural%20Networks&body=Title%3A%20Benchmarking%20Stochastic%20Approximation%20Algorithms%20for%20Fairness-Constrained%20Training%20of%20Deep%20Neural%20Networks%0AAuthor%3A%20Andrii%20Kliachkin%20and%20Jana%20Lep%C5%A1ov%C3%A1%20and%20Gilles%20Bareilles%20and%20Jakub%20Mare%C4%8Dek%0AAbstract%3A%20The%20ability%20to%20train%20Deep%20Neural%20Networks%20%28DNNs%29%20with%20constraints%20is%20instrumental%20in%20improving%20the%20fairness%20of%20modern%20machine-learning%20models.%20Many%20algorithms%20have%20been%20analysed%20in%20recent%20years%2C%20and%20yet%20there%20is%20no%20standard%2C%20widely%20accepted%20method%20for%20the%20constrained%20training%20of%20DNNs.%20In%20this%20paper%2C%20we%20provide%20a%20challenging%20benchmark%20of%20real-world%20large-scale%20fairness-constrained%20learning%20tasks%2C%20built%20on%20top%20of%20the%20US%20Census%20%28Folktables%29.%20We%20point%20out%20the%20theoretical%20challenges%20of%20such%20tasks%20and%20review%20the%20main%20approaches%20in%20stochastic%20approximation%20algorithms.%20Finally%2C%20we%20demonstrate%20the%20use%20of%20the%20benchmark%20by%20implementing%20and%20comparing%20three%20recently%20proposed%2C%20but%20as-of-yet%20unimplemented%2C%20algorithms%20both%20in%20terms%20of%20optimization%20performance%2C%20and%20fairness%20improvement.%20We%20release%20the%20code%20of%20the%20benchmark%20as%20a%20Python%20package%20at%20https%3A//github.com/humancompatible/train.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04033v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Stochastic%2520Approximation%2520Algorithms%2520for%2520Fairness-Constrained%2520Training%2520of%2520Deep%2520Neural%2520Networks%26entry.906535625%3DAndrii%2520Kliachkin%2520and%2520Jana%2520Lep%25C5%25A1ov%25C3%25A1%2520and%2520Gilles%2520Bareilles%2520and%2520Jakub%2520Mare%25C4%258Dek%26entry.1292438233%3DThe%2520ability%2520to%2520train%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520with%2520constraints%2520is%2520instrumental%2520in%2520improving%2520the%2520fairness%2520of%2520modern%2520machine-learning%2520models.%2520Many%2520algorithms%2520have%2520been%2520analysed%2520in%2520recent%2520years%252C%2520and%2520yet%2520there%2520is%2520no%2520standard%252C%2520widely%2520accepted%2520method%2520for%2520the%2520constrained%2520training%2520of%2520DNNs.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520challenging%2520benchmark%2520of%2520real-world%2520large-scale%2520fairness-constrained%2520learning%2520tasks%252C%2520built%2520on%2520top%2520of%2520the%2520US%2520Census%2520%2528Folktables%2529.%2520We%2520point%2520out%2520the%2520theoretical%2520challenges%2520of%2520such%2520tasks%2520and%2520review%2520the%2520main%2520approaches%2520in%2520stochastic%2520approximation%2520algorithms.%2520Finally%252C%2520we%2520demonstrate%2520the%2520use%2520of%2520the%2520benchmark%2520by%2520implementing%2520and%2520comparing%2520three%2520recently%2520proposed%252C%2520but%2520as-of-yet%2520unimplemented%252C%2520algorithms%2520both%2520in%2520terms%2520of%2520optimization%2520performance%252C%2520and%2520fairness%2520improvement.%2520We%2520release%2520the%2520code%2520of%2520the%2520benchmark%2520as%2520a%2520Python%2520package%2520at%2520https%253A//github.com/humancompatible/train.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04033v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Stochastic%20Approximation%20Algorithms%20for%20Fairness-Constrained%20Training%20of%20Deep%20Neural%20Networks&entry.906535625=Andrii%20Kliachkin%20and%20Jana%20Lep%C5%A1ov%C3%A1%20and%20Gilles%20Bareilles%20and%20Jakub%20Mare%C4%8Dek&entry.1292438233=The%20ability%20to%20train%20Deep%20Neural%20Networks%20%28DNNs%29%20with%20constraints%20is%20instrumental%20in%20improving%20the%20fairness%20of%20modern%20machine-learning%20models.%20Many%20algorithms%20have%20been%20analysed%20in%20recent%20years%2C%20and%20yet%20there%20is%20no%20standard%2C%20widely%20accepted%20method%20for%20the%20constrained%20training%20of%20DNNs.%20In%20this%20paper%2C%20we%20provide%20a%20challenging%20benchmark%20of%20real-world%20large-scale%20fairness-constrained%20learning%20tasks%2C%20built%20on%20top%20of%20the%20US%20Census%20%28Folktables%29.%20We%20point%20out%20the%20theoretical%20challenges%20of%20such%20tasks%20and%20review%20the%20main%20approaches%20in%20stochastic%20approximation%20algorithms.%20Finally%2C%20we%20demonstrate%20the%20use%20of%20the%20benchmark%20by%20implementing%20and%20comparing%20three%20recently%20proposed%2C%20but%20as-of-yet%20unimplemented%2C%20algorithms%20both%20in%20terms%20of%20optimization%20performance%2C%20and%20fairness%20improvement.%20We%20release%20the%20code%20of%20the%20benchmark%20as%20a%20Python%20package%20at%20https%3A//github.com/humancompatible/train.&entry.1838667208=http%3A//arxiv.org/abs/2507.04033v2&entry.124074799=Read"},
{"title": "EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data", "author": "Ruijie Zheng and Dantong Niu and Yuqi Xie and Jing Wang and Mengda Xu and Yunfan Jiang and Fernando Casta\u00f1eda and Fengyuan Hu and You Liang Tan and Letian Fu and Trevor Darrell and Furong Huang and Yuke Zhu and Danfei Xu and Linxi Fan", "abstract": "Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.", "link": "http://arxiv.org/abs/2602.16710v1", "date": "2026-02-18", "relevancy": 2.3743, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6163}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5975}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoScale%3A%20Scaling%20Dexterous%20Manipulation%20with%20Diverse%20Egocentric%20Human%20Data&body=Title%3A%20EgoScale%3A%20Scaling%20Dexterous%20Manipulation%20with%20Diverse%20Egocentric%20Human%20Data%0AAuthor%3A%20Ruijie%20Zheng%20and%20Dantong%20Niu%20and%20Yuqi%20Xie%20and%20Jing%20Wang%20and%20Mengda%20Xu%20and%20Yunfan%20Jiang%20and%20Fernando%20Casta%C3%B1eda%20and%20Fengyuan%20Hu%20and%20You%20Liang%20Tan%20and%20Letian%20Fu%20and%20Trevor%20Darrell%20and%20Furong%20Huang%20and%20Yuke%20Zhu%20and%20Danfei%20Xu%20and%20Linxi%20Fan%0AAbstract%3A%20Human%20behavior%20is%20among%20the%20most%20scalable%20sources%20of%20data%20for%20learning%20physical%20intelligence%2C%20yet%20how%20to%20effectively%20leverage%20it%20for%20dexterous%20manipulation%20remains%20unclear.%20While%20prior%20work%20demonstrates%20human%20to%20robot%20transfer%20in%20constrained%20settings%2C%20it%20is%20unclear%20whether%20large%20scale%20human%20data%20can%20support%20fine%20grained%2C%20high%20degree%20of%20freedom%20dexterous%20manipulation.%20We%20present%20EgoScale%2C%20a%20human%20to%20dexterous%20manipulation%20transfer%20framework%20built%20on%20large%20scale%20egocentric%20human%20data.%20We%20train%20a%20Vision%20Language%20Action%20%28VLA%29%20model%20on%20over%2020%2C854%20hours%20of%20action%20labeled%20egocentric%20human%20video%2C%20more%20than%2020%20times%20larger%20than%20prior%20efforts%2C%20and%20uncover%20a%20log%20linear%20scaling%20law%20between%20human%20data%20scale%20and%20validation%20loss.%20This%20validation%20loss%20strongly%20correlates%20with%20downstream%20real%20robot%20performance%2C%20establishing%20large%20scale%20human%20data%20as%20a%20predictable%20supervision%20source.%20Beyond%20scale%2C%20we%20introduce%20a%20simple%20two%20stage%20transfer%20recipe%3A%20large%20scale%20human%20pretraining%20followed%20by%20lightweight%20aligned%20human%20robot%20mid%20training.%20This%20enables%20strong%20long%20horizon%20dexterous%20manipulation%20and%20one%20shot%20task%20adaptation%20with%20minimal%20robot%20supervision.%20Our%20final%20policy%20improves%20average%20success%20rate%20by%2054%25%20over%20a%20no%20pretraining%20baseline%20using%20a%2022%20DoF%20dexterous%20robotic%20hand%2C%20and%20transfers%20effectively%20to%20robots%20with%20lower%20DoF%20hands%2C%20indicating%20that%20large%20scale%20human%20motion%20provides%20a%20reusable%2C%20embodiment%20agnostic%20motor%20prior.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoScale%253A%2520Scaling%2520Dexterous%2520Manipulation%2520with%2520Diverse%2520Egocentric%2520Human%2520Data%26entry.906535625%3DRuijie%2520Zheng%2520and%2520Dantong%2520Niu%2520and%2520Yuqi%2520Xie%2520and%2520Jing%2520Wang%2520and%2520Mengda%2520Xu%2520and%2520Yunfan%2520Jiang%2520and%2520Fernando%2520Casta%25C3%25B1eda%2520and%2520Fengyuan%2520Hu%2520and%2520You%2520Liang%2520Tan%2520and%2520Letian%2520Fu%2520and%2520Trevor%2520Darrell%2520and%2520Furong%2520Huang%2520and%2520Yuke%2520Zhu%2520and%2520Danfei%2520Xu%2520and%2520Linxi%2520Fan%26entry.1292438233%3DHuman%2520behavior%2520is%2520among%2520the%2520most%2520scalable%2520sources%2520of%2520data%2520for%2520learning%2520physical%2520intelligence%252C%2520yet%2520how%2520to%2520effectively%2520leverage%2520it%2520for%2520dexterous%2520manipulation%2520remains%2520unclear.%2520While%2520prior%2520work%2520demonstrates%2520human%2520to%2520robot%2520transfer%2520in%2520constrained%2520settings%252C%2520it%2520is%2520unclear%2520whether%2520large%2520scale%2520human%2520data%2520can%2520support%2520fine%2520grained%252C%2520high%2520degree%2520of%2520freedom%2520dexterous%2520manipulation.%2520We%2520present%2520EgoScale%252C%2520a%2520human%2520to%2520dexterous%2520manipulation%2520transfer%2520framework%2520built%2520on%2520large%2520scale%2520egocentric%2520human%2520data.%2520We%2520train%2520a%2520Vision%2520Language%2520Action%2520%2528VLA%2529%2520model%2520on%2520over%252020%252C854%2520hours%2520of%2520action%2520labeled%2520egocentric%2520human%2520video%252C%2520more%2520than%252020%2520times%2520larger%2520than%2520prior%2520efforts%252C%2520and%2520uncover%2520a%2520log%2520linear%2520scaling%2520law%2520between%2520human%2520data%2520scale%2520and%2520validation%2520loss.%2520This%2520validation%2520loss%2520strongly%2520correlates%2520with%2520downstream%2520real%2520robot%2520performance%252C%2520establishing%2520large%2520scale%2520human%2520data%2520as%2520a%2520predictable%2520supervision%2520source.%2520Beyond%2520scale%252C%2520we%2520introduce%2520a%2520simple%2520two%2520stage%2520transfer%2520recipe%253A%2520large%2520scale%2520human%2520pretraining%2520followed%2520by%2520lightweight%2520aligned%2520human%2520robot%2520mid%2520training.%2520This%2520enables%2520strong%2520long%2520horizon%2520dexterous%2520manipulation%2520and%2520one%2520shot%2520task%2520adaptation%2520with%2520minimal%2520robot%2520supervision.%2520Our%2520final%2520policy%2520improves%2520average%2520success%2520rate%2520by%252054%2525%2520over%2520a%2520no%2520pretraining%2520baseline%2520using%2520a%252022%2520DoF%2520dexterous%2520robotic%2520hand%252C%2520and%2520transfers%2520effectively%2520to%2520robots%2520with%2520lower%2520DoF%2520hands%252C%2520indicating%2520that%2520large%2520scale%2520human%2520motion%2520provides%2520a%2520reusable%252C%2520embodiment%2520agnostic%2520motor%2520prior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoScale%3A%20Scaling%20Dexterous%20Manipulation%20with%20Diverse%20Egocentric%20Human%20Data&entry.906535625=Ruijie%20Zheng%20and%20Dantong%20Niu%20and%20Yuqi%20Xie%20and%20Jing%20Wang%20and%20Mengda%20Xu%20and%20Yunfan%20Jiang%20and%20Fernando%20Casta%C3%B1eda%20and%20Fengyuan%20Hu%20and%20You%20Liang%20Tan%20and%20Letian%20Fu%20and%20Trevor%20Darrell%20and%20Furong%20Huang%20and%20Yuke%20Zhu%20and%20Danfei%20Xu%20and%20Linxi%20Fan&entry.1292438233=Human%20behavior%20is%20among%20the%20most%20scalable%20sources%20of%20data%20for%20learning%20physical%20intelligence%2C%20yet%20how%20to%20effectively%20leverage%20it%20for%20dexterous%20manipulation%20remains%20unclear.%20While%20prior%20work%20demonstrates%20human%20to%20robot%20transfer%20in%20constrained%20settings%2C%20it%20is%20unclear%20whether%20large%20scale%20human%20data%20can%20support%20fine%20grained%2C%20high%20degree%20of%20freedom%20dexterous%20manipulation.%20We%20present%20EgoScale%2C%20a%20human%20to%20dexterous%20manipulation%20transfer%20framework%20built%20on%20large%20scale%20egocentric%20human%20data.%20We%20train%20a%20Vision%20Language%20Action%20%28VLA%29%20model%20on%20over%2020%2C854%20hours%20of%20action%20labeled%20egocentric%20human%20video%2C%20more%20than%2020%20times%20larger%20than%20prior%20efforts%2C%20and%20uncover%20a%20log%20linear%20scaling%20law%20between%20human%20data%20scale%20and%20validation%20loss.%20This%20validation%20loss%20strongly%20correlates%20with%20downstream%20real%20robot%20performance%2C%20establishing%20large%20scale%20human%20data%20as%20a%20predictable%20supervision%20source.%20Beyond%20scale%2C%20we%20introduce%20a%20simple%20two%20stage%20transfer%20recipe%3A%20large%20scale%20human%20pretraining%20followed%20by%20lightweight%20aligned%20human%20robot%20mid%20training.%20This%20enables%20strong%20long%20horizon%20dexterous%20manipulation%20and%20one%20shot%20task%20adaptation%20with%20minimal%20robot%20supervision.%20Our%20final%20policy%20improves%20average%20success%20rate%20by%2054%25%20over%20a%20no%20pretraining%20baseline%20using%20a%2022%20DoF%20dexterous%20robotic%20hand%2C%20and%20transfers%20effectively%20to%20robots%20with%20lower%20DoF%20hands%2C%20indicating%20that%20large%20scale%20human%20motion%20provides%20a%20reusable%2C%20embodiment%20agnostic%20motor%20prior.&entry.1838667208=http%3A//arxiv.org/abs/2602.16710v1&entry.124074799=Read"},
{"title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment", "author": "Sebasti\u00e1n Barbas Laina and Simon Boche and Sotiris Papatheodorou and Simon Schaefer and Jaehyung Jung and Stefan Leutenegger", "abstract": "Geometrically accurate and semantically expressive map representations have proven invaluable for robot deployment and task planning in unknown environments. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments still presents open challenges, mainly due to computational requirements. In this paper we present FindAnything, an open-world mapping framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything combines pure geometric and open-vocabulary semantic information for a higher level of understanding. It proposes an efficient storage of open-vocabulary information through the aggregation of features at the object level. Pixelwise vision-language features are aggregated based on eSAM segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. We demonstrate that FindAnything performs on par with the state-of-the-art in terms of semantic accuracy while being substantially faster and more memory-efficient, allowing its deployment in large-scale environments and on resourceconstrained devices, such as MAVs. We show that the real-time capabilities of FindAnything make it useful for downstream tasks, such as autonomous MAV exploration in a simulated Search and Rescue scenario. Project Page: https://ethz-mrl.github.io/findanything/.", "link": "http://arxiv.org/abs/2504.08603v3", "date": "2026-02-18", "relevancy": 2.3724, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FindAnything%3A%20Open-Vocabulary%20and%20Object-Centric%20Mapping%20for%20Robot%20Exploration%20in%20Any%20Environment&body=Title%3A%20FindAnything%3A%20Open-Vocabulary%20and%20Object-Centric%20Mapping%20for%20Robot%20Exploration%20in%20Any%20Environment%0AAuthor%3A%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Sotiris%20Papatheodorou%20and%20Simon%20Schaefer%20and%20Jaehyung%20Jung%20and%20Stefan%20Leutenegger%0AAbstract%3A%20Geometrically%20accurate%20and%20semantically%20expressive%20map%20representations%20have%20proven%20invaluable%20for%20robot%20deployment%20and%20task%20planning%20in%20unknown%20environments.%20Nevertheless%2C%20real-time%2C%20open-vocabulary%20semantic%20understanding%20of%20large-scale%20unknown%20environments%20still%20presents%20open%20challenges%2C%20mainly%20due%20to%20computational%20requirements.%20In%20this%20paper%20we%20present%20FindAnything%2C%20an%20open-world%20mapping%20framework%20that%20incorporates%20vision-language%20information%20into%20dense%20volumetric%20submaps.%20Thanks%20to%20the%20use%20of%20vision-language%20features%2C%20FindAnything%20combines%20pure%20geometric%20and%20open-vocabulary%20semantic%20information%20for%20a%20higher%20level%20of%20understanding.%20It%20proposes%20an%20efficient%20storage%20of%20open-vocabulary%20information%20through%20the%20aggregation%20of%20features%20at%20the%20object%20level.%20Pixelwise%20vision-language%20features%20are%20aggregated%20based%20on%20eSAM%20segments%2C%20which%20are%20in%20turn%20integrated%20into%20object-centric%20volumetric%20submaps%2C%20providing%20a%20mapping%20from%20open-vocabulary%20queries%20to%203D%20geometry%20that%20is%20scalable%20also%20in%20terms%20of%20memory%20usage.%20We%20demonstrate%20that%20FindAnything%20performs%20on%20par%20with%20the%20state-of-the-art%20in%20terms%20of%20semantic%20accuracy%20while%20being%20substantially%20faster%20and%20more%20memory-efficient%2C%20allowing%20its%20deployment%20in%20large-scale%20environments%20and%20on%20resourceconstrained%20devices%2C%20such%20as%20MAVs.%20We%20show%20that%20the%20real-time%20capabilities%20of%20FindAnything%20make%20it%20useful%20for%20downstream%20tasks%2C%20such%20as%20autonomous%20MAV%20exploration%20in%20a%20simulated%20Search%20and%20Rescue%20scenario.%20Project%20Page%3A%20https%3A//ethz-mrl.github.io/findanything/.%0ALink%3A%20http%3A//arxiv.org/abs/2504.08603v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFindAnything%253A%2520Open-Vocabulary%2520and%2520Object-Centric%2520Mapping%2520for%2520Robot%2520Exploration%2520in%2520Any%2520Environment%26entry.906535625%3DSebasti%25C3%25A1n%2520Barbas%2520Laina%2520and%2520Simon%2520Boche%2520and%2520Sotiris%2520Papatheodorou%2520and%2520Simon%2520Schaefer%2520and%2520Jaehyung%2520Jung%2520and%2520Stefan%2520Leutenegger%26entry.1292438233%3DGeometrically%2520accurate%2520and%2520semantically%2520expressive%2520map%2520representations%2520have%2520proven%2520invaluable%2520for%2520robot%2520deployment%2520and%2520task%2520planning%2520in%2520unknown%2520environments.%2520Nevertheless%252C%2520real-time%252C%2520open-vocabulary%2520semantic%2520understanding%2520of%2520large-scale%2520unknown%2520environments%2520still%2520presents%2520open%2520challenges%252C%2520mainly%2520due%2520to%2520computational%2520requirements.%2520In%2520this%2520paper%2520we%2520present%2520FindAnything%252C%2520an%2520open-world%2520mapping%2520framework%2520that%2520incorporates%2520vision-language%2520information%2520into%2520dense%2520volumetric%2520submaps.%2520Thanks%2520to%2520the%2520use%2520of%2520vision-language%2520features%252C%2520FindAnything%2520combines%2520pure%2520geometric%2520and%2520open-vocabulary%2520semantic%2520information%2520for%2520a%2520higher%2520level%2520of%2520understanding.%2520It%2520proposes%2520an%2520efficient%2520storage%2520of%2520open-vocabulary%2520information%2520through%2520the%2520aggregation%2520of%2520features%2520at%2520the%2520object%2520level.%2520Pixelwise%2520vision-language%2520features%2520are%2520aggregated%2520based%2520on%2520eSAM%2520segments%252C%2520which%2520are%2520in%2520turn%2520integrated%2520into%2520object-centric%2520volumetric%2520submaps%252C%2520providing%2520a%2520mapping%2520from%2520open-vocabulary%2520queries%2520to%25203D%2520geometry%2520that%2520is%2520scalable%2520also%2520in%2520terms%2520of%2520memory%2520usage.%2520We%2520demonstrate%2520that%2520FindAnything%2520performs%2520on%2520par%2520with%2520the%2520state-of-the-art%2520in%2520terms%2520of%2520semantic%2520accuracy%2520while%2520being%2520substantially%2520faster%2520and%2520more%2520memory-efficient%252C%2520allowing%2520its%2520deployment%2520in%2520large-scale%2520environments%2520and%2520on%2520resourceconstrained%2520devices%252C%2520such%2520as%2520MAVs.%2520We%2520show%2520that%2520the%2520real-time%2520capabilities%2520of%2520FindAnything%2520make%2520it%2520useful%2520for%2520downstream%2520tasks%252C%2520such%2520as%2520autonomous%2520MAV%2520exploration%2520in%2520a%2520simulated%2520Search%2520and%2520Rescue%2520scenario.%2520Project%2520Page%253A%2520https%253A//ethz-mrl.github.io/findanything/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08603v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FindAnything%3A%20Open-Vocabulary%20and%20Object-Centric%20Mapping%20for%20Robot%20Exploration%20in%20Any%20Environment&entry.906535625=Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Sotiris%20Papatheodorou%20and%20Simon%20Schaefer%20and%20Jaehyung%20Jung%20and%20Stefan%20Leutenegger&entry.1292438233=Geometrically%20accurate%20and%20semantically%20expressive%20map%20representations%20have%20proven%20invaluable%20for%20robot%20deployment%20and%20task%20planning%20in%20unknown%20environments.%20Nevertheless%2C%20real-time%2C%20open-vocabulary%20semantic%20understanding%20of%20large-scale%20unknown%20environments%20still%20presents%20open%20challenges%2C%20mainly%20due%20to%20computational%20requirements.%20In%20this%20paper%20we%20present%20FindAnything%2C%20an%20open-world%20mapping%20framework%20that%20incorporates%20vision-language%20information%20into%20dense%20volumetric%20submaps.%20Thanks%20to%20the%20use%20of%20vision-language%20features%2C%20FindAnything%20combines%20pure%20geometric%20and%20open-vocabulary%20semantic%20information%20for%20a%20higher%20level%20of%20understanding.%20It%20proposes%20an%20efficient%20storage%20of%20open-vocabulary%20information%20through%20the%20aggregation%20of%20features%20at%20the%20object%20level.%20Pixelwise%20vision-language%20features%20are%20aggregated%20based%20on%20eSAM%20segments%2C%20which%20are%20in%20turn%20integrated%20into%20object-centric%20volumetric%20submaps%2C%20providing%20a%20mapping%20from%20open-vocabulary%20queries%20to%203D%20geometry%20that%20is%20scalable%20also%20in%20terms%20of%20memory%20usage.%20We%20demonstrate%20that%20FindAnything%20performs%20on%20par%20with%20the%20state-of-the-art%20in%20terms%20of%20semantic%20accuracy%20while%20being%20substantially%20faster%20and%20more%20memory-efficient%2C%20allowing%20its%20deployment%20in%20large-scale%20environments%20and%20on%20resourceconstrained%20devices%2C%20such%20as%20MAVs.%20We%20show%20that%20the%20real-time%20capabilities%20of%20FindAnything%20make%20it%20useful%20for%20downstream%20tasks%2C%20such%20as%20autonomous%20MAV%20exploration%20in%20a%20simulated%20Search%20and%20Rescue%20scenario.%20Project%20Page%3A%20https%3A//ethz-mrl.github.io/findanything/.&entry.1838667208=http%3A//arxiv.org/abs/2504.08603v3&entry.124074799=Read"},
{"title": "A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks", "author": "Santiago C. Vilabella and Pablo P\u00e9rez-N\u00fa\u00f1ez and Beatriz Remeseiro", "abstract": "In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.", "link": "http://arxiv.org/abs/2602.16322v1", "date": "2026-02-18", "relevancy": 2.3595, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6133}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5805}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Self-Supervised%20Approach%20for%20Enhanced%20Feature%20Representations%20in%20Object%20Detection%20Tasks&body=Title%3A%20A%20Self-Supervised%20Approach%20for%20Enhanced%20Feature%20Representations%20in%20Object%20Detection%20Tasks%0AAuthor%3A%20Santiago%20C.%20Vilabella%20and%20Pablo%20P%C3%A9rez-N%C3%BA%C3%B1ez%20and%20Beatriz%20Remeseiro%0AAbstract%3A%20In%20the%20fast-evolving%20field%20of%20artificial%20intelligence%2C%20where%20models%20are%20increasingly%20growing%20in%20complexity%20and%20size%2C%20the%20availability%20of%20labeled%20data%20for%20training%20deep%20learning%20models%20has%20become%20a%20significant%20challenge.%20Addressing%20complex%20problems%20like%20object%20detection%20demands%20considerable%20time%20and%20resources%20for%20data%20labeling%20to%20achieve%20meaningful%20results.%20For%20companies%20developing%20such%20applications%2C%20this%20entails%20extensive%20investment%20in%20highly%20skilled%20personnel%20or%20costly%20outsourcing.%20This%20research%20work%20aims%20to%20demonstrate%20that%20enhancing%20feature%20extractors%20can%20substantially%20alleviate%20this%20challenge%2C%20enabling%20models%20to%20learn%20more%20effective%20representations%20with%20less%20labeled%20data.%20Utilizing%20a%20self-supervised%20learning%20strategy%2C%20we%20present%20a%20model%20trained%20on%20unlabeled%20data%20that%20outperforms%20state-of-the-art%20feature%20extractors%20pre-trained%20on%20ImageNet%20and%20particularly%20designed%20for%20object%20detection%20tasks.%20Moreover%2C%20the%20results%20demonstrate%20that%20our%20approach%20encourages%20the%20model%20to%20focus%20on%20the%20most%20relevant%20aspects%20of%20an%20object%2C%20thus%20achieving%20better%20feature%20representations%20and%2C%20therefore%2C%20reinforcing%20its%20reliability%20and%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Self-Supervised%2520Approach%2520for%2520Enhanced%2520Feature%2520Representations%2520in%2520Object%2520Detection%2520Tasks%26entry.906535625%3DSantiago%2520C.%2520Vilabella%2520and%2520Pablo%2520P%25C3%25A9rez-N%25C3%25BA%25C3%25B1ez%2520and%2520Beatriz%2520Remeseiro%26entry.1292438233%3DIn%2520the%2520fast-evolving%2520field%2520of%2520artificial%2520intelligence%252C%2520where%2520models%2520are%2520increasingly%2520growing%2520in%2520complexity%2520and%2520size%252C%2520the%2520availability%2520of%2520labeled%2520data%2520for%2520training%2520deep%2520learning%2520models%2520has%2520become%2520a%2520significant%2520challenge.%2520Addressing%2520complex%2520problems%2520like%2520object%2520detection%2520demands%2520considerable%2520time%2520and%2520resources%2520for%2520data%2520labeling%2520to%2520achieve%2520meaningful%2520results.%2520For%2520companies%2520developing%2520such%2520applications%252C%2520this%2520entails%2520extensive%2520investment%2520in%2520highly%2520skilled%2520personnel%2520or%2520costly%2520outsourcing.%2520This%2520research%2520work%2520aims%2520to%2520demonstrate%2520that%2520enhancing%2520feature%2520extractors%2520can%2520substantially%2520alleviate%2520this%2520challenge%252C%2520enabling%2520models%2520to%2520learn%2520more%2520effective%2520representations%2520with%2520less%2520labeled%2520data.%2520Utilizing%2520a%2520self-supervised%2520learning%2520strategy%252C%2520we%2520present%2520a%2520model%2520trained%2520on%2520unlabeled%2520data%2520that%2520outperforms%2520state-of-the-art%2520feature%2520extractors%2520pre-trained%2520on%2520ImageNet%2520and%2520particularly%2520designed%2520for%2520object%2520detection%2520tasks.%2520Moreover%252C%2520the%2520results%2520demonstrate%2520that%2520our%2520approach%2520encourages%2520the%2520model%2520to%2520focus%2520on%2520the%2520most%2520relevant%2520aspects%2520of%2520an%2520object%252C%2520thus%2520achieving%2520better%2520feature%2520representations%2520and%252C%2520therefore%252C%2520reinforcing%2520its%2520reliability%2520and%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-Supervised%20Approach%20for%20Enhanced%20Feature%20Representations%20in%20Object%20Detection%20Tasks&entry.906535625=Santiago%20C.%20Vilabella%20and%20Pablo%20P%C3%A9rez-N%C3%BA%C3%B1ez%20and%20Beatriz%20Remeseiro&entry.1292438233=In%20the%20fast-evolving%20field%20of%20artificial%20intelligence%2C%20where%20models%20are%20increasingly%20growing%20in%20complexity%20and%20size%2C%20the%20availability%20of%20labeled%20data%20for%20training%20deep%20learning%20models%20has%20become%20a%20significant%20challenge.%20Addressing%20complex%20problems%20like%20object%20detection%20demands%20considerable%20time%20and%20resources%20for%20data%20labeling%20to%20achieve%20meaningful%20results.%20For%20companies%20developing%20such%20applications%2C%20this%20entails%20extensive%20investment%20in%20highly%20skilled%20personnel%20or%20costly%20outsourcing.%20This%20research%20work%20aims%20to%20demonstrate%20that%20enhancing%20feature%20extractors%20can%20substantially%20alleviate%20this%20challenge%2C%20enabling%20models%20to%20learn%20more%20effective%20representations%20with%20less%20labeled%20data.%20Utilizing%20a%20self-supervised%20learning%20strategy%2C%20we%20present%20a%20model%20trained%20on%20unlabeled%20data%20that%20outperforms%20state-of-the-art%20feature%20extractors%20pre-trained%20on%20ImageNet%20and%20particularly%20designed%20for%20object%20detection%20tasks.%20Moreover%2C%20the%20results%20demonstrate%20that%20our%20approach%20encourages%20the%20model%20to%20focus%20on%20the%20most%20relevant%20aspects%20of%20an%20object%2C%20thus%20achieving%20better%20feature%20representations%20and%2C%20therefore%2C%20reinforcing%20its%20reliability%20and%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2602.16322v1&entry.124074799=Read"},
{"title": "Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems", "author": "Ali Faraz and Raja Kolla and Ashish Kulkarni and Shubham Agarwal", "abstract": "Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.", "link": "http://arxiv.org/abs/2602.16430v1", "date": "2026-02-18", "relevancy": 2.3584, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20Production-Scale%20OCR%20for%20India%3A%20Multilingual%20and%20Domain-Specific%20Systems&body=Title%3A%20Designing%20Production-Scale%20OCR%20for%20India%3A%20Multilingual%20and%20Domain-Specific%20Systems%0AAuthor%3A%20Ali%20Faraz%20and%20Raja%20Kolla%20and%20Ashish%20Kulkarni%20and%20Shubham%20Agarwal%0AAbstract%3A%20Designing%20Optical%20Character%20Recognition%20%28OCR%29%20systems%20for%20India%20requires%20balancing%20linguistic%20diversity%2C%20document%20heterogeneity%2C%20and%20deployment%20constraints.%20In%20this%20paper%2C%20we%20study%20two%20training%20strategies%20for%20building%20multilingual%20OCR%20systems%20with%20Vision-Language%20Models%20through%20the%20Chitrapathak%20series.%20We%20first%20follow%20a%20popular%20multimodal%20approach%2C%20pairing%20a%20generic%20vision%20encoder%20with%20a%20strong%20multilingual%20language%20model%20and%20training%20the%20system%20end-to-end%20for%20OCR.%20Alternatively%2C%20we%20explore%20fine-tuning%20an%20existing%20OCR%20model%2C%20despite%20not%20being%20trained%20for%20the%20target%20languages.%20Through%20extensive%20evaluation%20on%20multilingual%20Indic%20OCR%20benchmarks%20and%20deployment-oriented%20metrics%2C%20we%20find%20that%20the%20second%20strategy%20consistently%20achieves%20better%20accuracy-latency%20trade-offs.%20Chitrapathak-2%20achieves%203-6x%20speedup%20over%20its%20predecessor%20with%20being%20state-of-the-art%20%28SOTA%29%20in%20Telugu%20%286.69%20char%20ANLS%29%20and%20second%20best%20in%20the%20rest.%20In%20addition%2C%20we%20present%20Parichay%2C%20an%20independent%20OCR%20model%20series%20designed%20specifically%20for%209%20Indian%20government%20documents%20to%20extract%20structured%20key%20fields%2C%20achieving%2089.8%25%20Exact%20Match%20score%20with%20a%20faster%20inference.%20Together%2C%20these%20systems%20achieve%20SOTA%20performance%20and%20provide%20practical%20guidance%20for%20building%20production-scale%20OCR%20pipelines%20in%20the%20Indian%20context.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520Production-Scale%2520OCR%2520for%2520India%253A%2520Multilingual%2520and%2520Domain-Specific%2520Systems%26entry.906535625%3DAli%2520Faraz%2520and%2520Raja%2520Kolla%2520and%2520Ashish%2520Kulkarni%2520and%2520Shubham%2520Agarwal%26entry.1292438233%3DDesigning%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520systems%2520for%2520India%2520requires%2520balancing%2520linguistic%2520diversity%252C%2520document%2520heterogeneity%252C%2520and%2520deployment%2520constraints.%2520In%2520this%2520paper%252C%2520we%2520study%2520two%2520training%2520strategies%2520for%2520building%2520multilingual%2520OCR%2520systems%2520with%2520Vision-Language%2520Models%2520through%2520the%2520Chitrapathak%2520series.%2520We%2520first%2520follow%2520a%2520popular%2520multimodal%2520approach%252C%2520pairing%2520a%2520generic%2520vision%2520encoder%2520with%2520a%2520strong%2520multilingual%2520language%2520model%2520and%2520training%2520the%2520system%2520end-to-end%2520for%2520OCR.%2520Alternatively%252C%2520we%2520explore%2520fine-tuning%2520an%2520existing%2520OCR%2520model%252C%2520despite%2520not%2520being%2520trained%2520for%2520the%2520target%2520languages.%2520Through%2520extensive%2520evaluation%2520on%2520multilingual%2520Indic%2520OCR%2520benchmarks%2520and%2520deployment-oriented%2520metrics%252C%2520we%2520find%2520that%2520the%2520second%2520strategy%2520consistently%2520achieves%2520better%2520accuracy-latency%2520trade-offs.%2520Chitrapathak-2%2520achieves%25203-6x%2520speedup%2520over%2520its%2520predecessor%2520with%2520being%2520state-of-the-art%2520%2528SOTA%2529%2520in%2520Telugu%2520%25286.69%2520char%2520ANLS%2529%2520and%2520second%2520best%2520in%2520the%2520rest.%2520In%2520addition%252C%2520we%2520present%2520Parichay%252C%2520an%2520independent%2520OCR%2520model%2520series%2520designed%2520specifically%2520for%25209%2520Indian%2520government%2520documents%2520to%2520extract%2520structured%2520key%2520fields%252C%2520achieving%252089.8%2525%2520Exact%2520Match%2520score%2520with%2520a%2520faster%2520inference.%2520Together%252C%2520these%2520systems%2520achieve%2520SOTA%2520performance%2520and%2520provide%2520practical%2520guidance%2520for%2520building%2520production-scale%2520OCR%2520pipelines%2520in%2520the%2520Indian%2520context.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20Production-Scale%20OCR%20for%20India%3A%20Multilingual%20and%20Domain-Specific%20Systems&entry.906535625=Ali%20Faraz%20and%20Raja%20Kolla%20and%20Ashish%20Kulkarni%20and%20Shubham%20Agarwal&entry.1292438233=Designing%20Optical%20Character%20Recognition%20%28OCR%29%20systems%20for%20India%20requires%20balancing%20linguistic%20diversity%2C%20document%20heterogeneity%2C%20and%20deployment%20constraints.%20In%20this%20paper%2C%20we%20study%20two%20training%20strategies%20for%20building%20multilingual%20OCR%20systems%20with%20Vision-Language%20Models%20through%20the%20Chitrapathak%20series.%20We%20first%20follow%20a%20popular%20multimodal%20approach%2C%20pairing%20a%20generic%20vision%20encoder%20with%20a%20strong%20multilingual%20language%20model%20and%20training%20the%20system%20end-to-end%20for%20OCR.%20Alternatively%2C%20we%20explore%20fine-tuning%20an%20existing%20OCR%20model%2C%20despite%20not%20being%20trained%20for%20the%20target%20languages.%20Through%20extensive%20evaluation%20on%20multilingual%20Indic%20OCR%20benchmarks%20and%20deployment-oriented%20metrics%2C%20we%20find%20that%20the%20second%20strategy%20consistently%20achieves%20better%20accuracy-latency%20trade-offs.%20Chitrapathak-2%20achieves%203-6x%20speedup%20over%20its%20predecessor%20with%20being%20state-of-the-art%20%28SOTA%29%20in%20Telugu%20%286.69%20char%20ANLS%29%20and%20second%20best%20in%20the%20rest.%20In%20addition%2C%20we%20present%20Parichay%2C%20an%20independent%20OCR%20model%20series%20designed%20specifically%20for%209%20Indian%20government%20documents%20to%20extract%20structured%20key%20fields%2C%20achieving%2089.8%25%20Exact%20Match%20score%20with%20a%20faster%20inference.%20Together%2C%20these%20systems%20achieve%20SOTA%20performance%20and%20provide%20practical%20guidance%20for%20building%20production-scale%20OCR%20pipelines%20in%20the%20Indian%20context.&entry.1838667208=http%3A//arxiv.org/abs/2602.16430v1&entry.124074799=Read"},
{"title": "Learning Situated Awareness in the Real World", "author": "Chuhan Li and Ruilin Han and Joy Hsu and Yongyuan Liang and Rajiv Dhawan and Jiajun Wu and Ming-Hsuan Yang and Xin Eric Wang", "abstract": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.", "link": "http://arxiv.org/abs/2602.16682v1", "date": "2026-02-18", "relevancy": 2.3574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Situated%20Awareness%20in%20the%20Real%20World&body=Title%3A%20Learning%20Situated%20Awareness%20in%20the%20Real%20World%0AAuthor%3A%20Chuhan%20Li%20and%20Ruilin%20Han%20and%20Joy%20Hsu%20and%20Yongyuan%20Liang%20and%20Rajiv%20Dhawan%20and%20Jiajun%20Wu%20and%20Ming-Hsuan%20Yang%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20A%20core%20aspect%20of%20human%20perception%20is%20situated%20awareness%2C%20the%20ability%20to%20relate%20ourselves%20to%20the%20surrounding%20physical%20environment%20and%20reason%20over%20possible%20actions%20in%20context.%20However%2C%20most%20existing%20benchmarks%20for%20multimodal%20foundation%20models%20%28MFMs%29%20emphasize%20environment-centric%20spatial%20relations%20%28relations%20among%20objects%20in%20a%20scene%29%2C%20while%20largely%20overlooking%20observer-centric%20relationships%20that%20require%20reasoning%20relative%20to%20agent%27s%20viewpoint%2C%20pose%2C%20and%20motion.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SAW-Bench%20%28Situated%20Awareness%20in%20the%20Real%20World%29%2C%20a%20novel%20benchmark%20for%20evaluating%20egocentric%20situated%20awareness%20using%20real-world%20videos.%20SAW-Bench%20comprises%20786%20self-recorded%20videos%20captured%20with%20Ray-Ban%20Meta%20%28Gen%202%29%20smart%20glasses%20spanning%20diverse%20indoor%20and%20outdoor%20environments%2C%20and%20over%202%2C071%20human-annotated%20question-answer%20pairs.%20It%20probes%20a%20model%27s%20observer-centric%20understanding%20with%20six%20different%20awareness%20tasks.%20Our%20comprehensive%20evaluation%20reveals%20a%20human-model%20performance%20gap%20of%2037.66%25%2C%20even%20with%20the%20best-performing%20MFM%2C%20Gemini%203%20Flash.%20Beyond%20this%20gap%2C%20our%20in-depth%20analysis%20uncovers%20several%20notable%20findings%3B%20for%20example%2C%20while%20models%20can%20exploit%20partial%20geometric%20cues%20in%20egocentric%20videos%2C%20they%20often%20fail%20to%20infer%20a%20coherent%20camera%20geometry%2C%20leading%20to%20systematic%20spatial%20reasoning%20errors.%20We%20position%20SAW-Bench%20as%20a%20benchmark%20for%20situated%20spatial%20intelligence%2C%20moving%20beyond%20passive%20observation%20to%20understanding%20physically%20grounded%2C%20observer-centric%20dynamics.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Situated%2520Awareness%2520in%2520the%2520Real%2520World%26entry.906535625%3DChuhan%2520Li%2520and%2520Ruilin%2520Han%2520and%2520Joy%2520Hsu%2520and%2520Yongyuan%2520Liang%2520and%2520Rajiv%2520Dhawan%2520and%2520Jiajun%2520Wu%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3DA%2520core%2520aspect%2520of%2520human%2520perception%2520is%2520situated%2520awareness%252C%2520the%2520ability%2520to%2520relate%2520ourselves%2520to%2520the%2520surrounding%2520physical%2520environment%2520and%2520reason%2520over%2520possible%2520actions%2520in%2520context.%2520However%252C%2520most%2520existing%2520benchmarks%2520for%2520multimodal%2520foundation%2520models%2520%2528MFMs%2529%2520emphasize%2520environment-centric%2520spatial%2520relations%2520%2528relations%2520among%2520objects%2520in%2520a%2520scene%2529%252C%2520while%2520largely%2520overlooking%2520observer-centric%2520relationships%2520that%2520require%2520reasoning%2520relative%2520to%2520agent%2527s%2520viewpoint%252C%2520pose%252C%2520and%2520motion.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520SAW-Bench%2520%2528Situated%2520Awareness%2520in%2520the%2520Real%2520World%2529%252C%2520a%2520novel%2520benchmark%2520for%2520evaluating%2520egocentric%2520situated%2520awareness%2520using%2520real-world%2520videos.%2520SAW-Bench%2520comprises%2520786%2520self-recorded%2520videos%2520captured%2520with%2520Ray-Ban%2520Meta%2520%2528Gen%25202%2529%2520smart%2520glasses%2520spanning%2520diverse%2520indoor%2520and%2520outdoor%2520environments%252C%2520and%2520over%25202%252C071%2520human-annotated%2520question-answer%2520pairs.%2520It%2520probes%2520a%2520model%2527s%2520observer-centric%2520understanding%2520with%2520six%2520different%2520awareness%2520tasks.%2520Our%2520comprehensive%2520evaluation%2520reveals%2520a%2520human-model%2520performance%2520gap%2520of%252037.66%2525%252C%2520even%2520with%2520the%2520best-performing%2520MFM%252C%2520Gemini%25203%2520Flash.%2520Beyond%2520this%2520gap%252C%2520our%2520in-depth%2520analysis%2520uncovers%2520several%2520notable%2520findings%253B%2520for%2520example%252C%2520while%2520models%2520can%2520exploit%2520partial%2520geometric%2520cues%2520in%2520egocentric%2520videos%252C%2520they%2520often%2520fail%2520to%2520infer%2520a%2520coherent%2520camera%2520geometry%252C%2520leading%2520to%2520systematic%2520spatial%2520reasoning%2520errors.%2520We%2520position%2520SAW-Bench%2520as%2520a%2520benchmark%2520for%2520situated%2520spatial%2520intelligence%252C%2520moving%2520beyond%2520passive%2520observation%2520to%2520understanding%2520physically%2520grounded%252C%2520observer-centric%2520dynamics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Situated%20Awareness%20in%20the%20Real%20World&entry.906535625=Chuhan%20Li%20and%20Ruilin%20Han%20and%20Joy%20Hsu%20and%20Yongyuan%20Liang%20and%20Rajiv%20Dhawan%20and%20Jiajun%20Wu%20and%20Ming-Hsuan%20Yang%20and%20Xin%20Eric%20Wang&entry.1292438233=A%20core%20aspect%20of%20human%20perception%20is%20situated%20awareness%2C%20the%20ability%20to%20relate%20ourselves%20to%20the%20surrounding%20physical%20environment%20and%20reason%20over%20possible%20actions%20in%20context.%20However%2C%20most%20existing%20benchmarks%20for%20multimodal%20foundation%20models%20%28MFMs%29%20emphasize%20environment-centric%20spatial%20relations%20%28relations%20among%20objects%20in%20a%20scene%29%2C%20while%20largely%20overlooking%20observer-centric%20relationships%20that%20require%20reasoning%20relative%20to%20agent%27s%20viewpoint%2C%20pose%2C%20and%20motion.%20To%20bridge%20this%20gap%2C%20we%20introduce%20SAW-Bench%20%28Situated%20Awareness%20in%20the%20Real%20World%29%2C%20a%20novel%20benchmark%20for%20evaluating%20egocentric%20situated%20awareness%20using%20real-world%20videos.%20SAW-Bench%20comprises%20786%20self-recorded%20videos%20captured%20with%20Ray-Ban%20Meta%20%28Gen%202%29%20smart%20glasses%20spanning%20diverse%20indoor%20and%20outdoor%20environments%2C%20and%20over%202%2C071%20human-annotated%20question-answer%20pairs.%20It%20probes%20a%20model%27s%20observer-centric%20understanding%20with%20six%20different%20awareness%20tasks.%20Our%20comprehensive%20evaluation%20reveals%20a%20human-model%20performance%20gap%20of%2037.66%25%2C%20even%20with%20the%20best-performing%20MFM%2C%20Gemini%203%20Flash.%20Beyond%20this%20gap%2C%20our%20in-depth%20analysis%20uncovers%20several%20notable%20findings%3B%20for%20example%2C%20while%20models%20can%20exploit%20partial%20geometric%20cues%20in%20egocentric%20videos%2C%20they%20often%20fail%20to%20infer%20a%20coherent%20camera%20geometry%2C%20leading%20to%20systematic%20spatial%20reasoning%20errors.%20We%20position%20SAW-Bench%20as%20a%20benchmark%20for%20situated%20spatial%20intelligence%2C%20moving%20beyond%20passive%20observation%20to%20understanding%20physically%20grounded%2C%20observer-centric%20dynamics.&entry.1838667208=http%3A//arxiv.org/abs/2602.16682v1&entry.124074799=Read"},
{"title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens", "author": "Shiqi Liu and Zeyu He and Guojian Zhan and Letian Tao and Zhilong Zheng and Jiang Wu and Yinuo Wang and Yang Guan and Kehua Sheng and Bo Zhang and Keqiang Li and Jingliang Duan and Shengbo Eben Li", "abstract": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% ($\u03c1_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69\\% ($\u03c1_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL.", "link": "http://arxiv.org/abs/2602.15620v2", "date": "2026-02-18", "relevancy": 2.3529, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4561}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAPO%3A%20Stabilizing%20Reinforcement%20Learning%20for%20LLMs%20by%20Silencing%20Rare%20Spurious%20Tokens&body=Title%3A%20STAPO%3A%20Stabilizing%20Reinforcement%20Learning%20for%20LLMs%20by%20Silencing%20Rare%20Spurious%20Tokens%0AAuthor%3A%20Shiqi%20Liu%20and%20Zeyu%20He%20and%20Guojian%20Zhan%20and%20Letian%20Tao%20and%20Zhilong%20Zheng%20and%20Jiang%20Wu%20and%20Yinuo%20Wang%20and%20Yang%20Guan%20and%20Kehua%20Sheng%20and%20Bo%20Zhang%20and%20Keqiang%20Li%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li%0AAbstract%3A%20Reinforcement%20Learning%20%28RL%29%20has%20significantly%20improved%20large%20language%20model%20reasoning%2C%20but%20existing%20RL%20fine-tuning%20methods%20rely%20heavily%20on%20heuristic%20techniques%20such%20as%20entropy%20regularization%20and%20reweighting%20to%20maintain%20stability.%20In%20practice%2C%20they%20often%20suffer%20from%20late-stage%20performance%20collapse%2C%20leading%20to%20degraded%20reasoning%20quality%20and%20unstable%20training.%20Our%20analysis%20shows%20that%20the%20magnitude%20of%20token-wise%20policy%20gradients%20in%20RL%20is%20negatively%20correlated%20with%20token%20probability%20and%20local%20policy%20entropy.%20We%20find%20that%20training%20instability%20can%20be%20caused%20by%20a%20tiny%20fraction%20of%20tokens%2C%20approximately%200.01%5C%25%2C%20which%20we%20term%20%5Cemph%7Bspurious%20tokens%7D.%20When%20such%20tokens%20appear%20in%20correct%20responses%2C%20they%20contribute%20little%20to%20the%20reasoning%20outcome%20but%20inherit%20the%20full%20sequence-level%20reward%2C%20leading%20to%20abnormally%20amplified%20gradient%20updates.%20To%20mitigate%20this%20instability%2C%20we%20design%20S2T%20%28silencing%20spurious%20tokens%29%20mechanism%20to%20efficiently%20identify%20spurious%20tokens%20through%20characteristic%20signals%20with%20low%20probability%2C%20low%20entropy%2C%20and%20positive%20advantage%2C%20and%20then%20to%20suppress%20their%20gradient%20perturbations%20during%20optimization.%20Incorporating%20this%20mechanism%20into%20a%20group-based%20objective%2C%20we%20propose%20Spurious-Token-Aware%20Policy%20Optimization%20%28STAPO%29%2C%20which%20promotes%20stable%20and%20effective%20large-scale%20model%20refinement.%20Across%20six%20mathematical%20reasoning%20benchmarks%20using%20Qwen%201.7B%2C%208B%2C%20and%2014B%20base%20models%2C%20STAPO%20consistently%20demonstrates%20superior%20entropy%20stability%20and%20achieves%20an%20average%20performance%20improvement%20of%207.13%5C%25%20%28%24%CF%81_%7B%5Cmathrm%7BT%7D%7D%24%3D1.0%2C%20top-p%3D1.0%29%20and%203.69%5C%25%20%28%24%CF%81_%7B%5Cmathrm%7BT%7D%7D%24%3D0.7%2C%20top-p%3D0.9%29%20over%20GRPO%2C%2020-Entropy%20and%20JustRL.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAPO%253A%2520Stabilizing%2520Reinforcement%2520Learning%2520for%2520LLMs%2520by%2520Silencing%2520Rare%2520Spurious%2520Tokens%26entry.906535625%3DShiqi%2520Liu%2520and%2520Zeyu%2520He%2520and%2520Guojian%2520Zhan%2520and%2520Letian%2520Tao%2520and%2520Zhilong%2520Zheng%2520and%2520Jiang%2520Wu%2520and%2520Yinuo%2520Wang%2520and%2520Yang%2520Guan%2520and%2520Kehua%2520Sheng%2520and%2520Bo%2520Zhang%2520and%2520Keqiang%2520Li%2520and%2520Jingliang%2520Duan%2520and%2520Shengbo%2520Eben%2520Li%26entry.1292438233%3DReinforcement%2520Learning%2520%2528RL%2529%2520has%2520significantly%2520improved%2520large%2520language%2520model%2520reasoning%252C%2520but%2520existing%2520RL%2520fine-tuning%2520methods%2520rely%2520heavily%2520on%2520heuristic%2520techniques%2520such%2520as%2520entropy%2520regularization%2520and%2520reweighting%2520to%2520maintain%2520stability.%2520In%2520practice%252C%2520they%2520often%2520suffer%2520from%2520late-stage%2520performance%2520collapse%252C%2520leading%2520to%2520degraded%2520reasoning%2520quality%2520and%2520unstable%2520training.%2520Our%2520analysis%2520shows%2520that%2520the%2520magnitude%2520of%2520token-wise%2520policy%2520gradients%2520in%2520RL%2520is%2520negatively%2520correlated%2520with%2520token%2520probability%2520and%2520local%2520policy%2520entropy.%2520We%2520find%2520that%2520training%2520instability%2520can%2520be%2520caused%2520by%2520a%2520tiny%2520fraction%2520of%2520tokens%252C%2520approximately%25200.01%255C%2525%252C%2520which%2520we%2520term%2520%255Cemph%257Bspurious%2520tokens%257D.%2520When%2520such%2520tokens%2520appear%2520in%2520correct%2520responses%252C%2520they%2520contribute%2520little%2520to%2520the%2520reasoning%2520outcome%2520but%2520inherit%2520the%2520full%2520sequence-level%2520reward%252C%2520leading%2520to%2520abnormally%2520amplified%2520gradient%2520updates.%2520To%2520mitigate%2520this%2520instability%252C%2520we%2520design%2520S2T%2520%2528silencing%2520spurious%2520tokens%2529%2520mechanism%2520to%2520efficiently%2520identify%2520spurious%2520tokens%2520through%2520characteristic%2520signals%2520with%2520low%2520probability%252C%2520low%2520entropy%252C%2520and%2520positive%2520advantage%252C%2520and%2520then%2520to%2520suppress%2520their%2520gradient%2520perturbations%2520during%2520optimization.%2520Incorporating%2520this%2520mechanism%2520into%2520a%2520group-based%2520objective%252C%2520we%2520propose%2520Spurious-Token-Aware%2520Policy%2520Optimization%2520%2528STAPO%2529%252C%2520which%2520promotes%2520stable%2520and%2520effective%2520large-scale%2520model%2520refinement.%2520Across%2520six%2520mathematical%2520reasoning%2520benchmarks%2520using%2520Qwen%25201.7B%252C%25208B%252C%2520and%252014B%2520base%2520models%252C%2520STAPO%2520consistently%2520demonstrates%2520superior%2520entropy%2520stability%2520and%2520achieves%2520an%2520average%2520performance%2520improvement%2520of%25207.13%255C%2525%2520%2528%2524%25CF%2581_%257B%255Cmathrm%257BT%257D%257D%2524%253D1.0%252C%2520top-p%253D1.0%2529%2520and%25203.69%255C%2525%2520%2528%2524%25CF%2581_%257B%255Cmathrm%257BT%257D%257D%2524%253D0.7%252C%2520top-p%253D0.9%2529%2520over%2520GRPO%252C%252020-Entropy%2520and%2520JustRL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAPO%3A%20Stabilizing%20Reinforcement%20Learning%20for%20LLMs%20by%20Silencing%20Rare%20Spurious%20Tokens&entry.906535625=Shiqi%20Liu%20and%20Zeyu%20He%20and%20Guojian%20Zhan%20and%20Letian%20Tao%20and%20Zhilong%20Zheng%20and%20Jiang%20Wu%20and%20Yinuo%20Wang%20and%20Yang%20Guan%20and%20Kehua%20Sheng%20and%20Bo%20Zhang%20and%20Keqiang%20Li%20and%20Jingliang%20Duan%20and%20Shengbo%20Eben%20Li&entry.1292438233=Reinforcement%20Learning%20%28RL%29%20has%20significantly%20improved%20large%20language%20model%20reasoning%2C%20but%20existing%20RL%20fine-tuning%20methods%20rely%20heavily%20on%20heuristic%20techniques%20such%20as%20entropy%20regularization%20and%20reweighting%20to%20maintain%20stability.%20In%20practice%2C%20they%20often%20suffer%20from%20late-stage%20performance%20collapse%2C%20leading%20to%20degraded%20reasoning%20quality%20and%20unstable%20training.%20Our%20analysis%20shows%20that%20the%20magnitude%20of%20token-wise%20policy%20gradients%20in%20RL%20is%20negatively%20correlated%20with%20token%20probability%20and%20local%20policy%20entropy.%20We%20find%20that%20training%20instability%20can%20be%20caused%20by%20a%20tiny%20fraction%20of%20tokens%2C%20approximately%200.01%5C%25%2C%20which%20we%20term%20%5Cemph%7Bspurious%20tokens%7D.%20When%20such%20tokens%20appear%20in%20correct%20responses%2C%20they%20contribute%20little%20to%20the%20reasoning%20outcome%20but%20inherit%20the%20full%20sequence-level%20reward%2C%20leading%20to%20abnormally%20amplified%20gradient%20updates.%20To%20mitigate%20this%20instability%2C%20we%20design%20S2T%20%28silencing%20spurious%20tokens%29%20mechanism%20to%20efficiently%20identify%20spurious%20tokens%20through%20characteristic%20signals%20with%20low%20probability%2C%20low%20entropy%2C%20and%20positive%20advantage%2C%20and%20then%20to%20suppress%20their%20gradient%20perturbations%20during%20optimization.%20Incorporating%20this%20mechanism%20into%20a%20group-based%20objective%2C%20we%20propose%20Spurious-Token-Aware%20Policy%20Optimization%20%28STAPO%29%2C%20which%20promotes%20stable%20and%20effective%20large-scale%20model%20refinement.%20Across%20six%20mathematical%20reasoning%20benchmarks%20using%20Qwen%201.7B%2C%208B%2C%20and%2014B%20base%20models%2C%20STAPO%20consistently%20demonstrates%20superior%20entropy%20stability%20and%20achieves%20an%20average%20performance%20improvement%20of%207.13%5C%25%20%28%24%CF%81_%7B%5Cmathrm%7BT%7D%7D%24%3D1.0%2C%20top-p%3D1.0%29%20and%203.69%5C%25%20%28%24%CF%81_%7B%5Cmathrm%7BT%7D%7D%24%3D0.7%2C%20top-p%3D0.9%29%20over%20GRPO%2C%2020-Entropy%20and%20JustRL.&entry.1838667208=http%3A//arxiv.org/abs/2602.15620v2&entry.124074799=Read"},
{"title": "PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction", "author": "Bo Lang and Nirav Savaliya and Zhihao Zheng and Jinglun Feng and Zheng-Hang Yeh and Mooi Choo Chuah", "abstract": "High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.", "link": "http://arxiv.org/abs/2602.16669v1", "date": "2026-02-18", "relevancy": 2.3474, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6135}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5771}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PredMapNet%3A%20Future%20and%20Historical%20Reasoning%20for%20Consistent%20Online%20HD%20Vectorized%20Map%20Construction&body=Title%3A%20PredMapNet%3A%20Future%20and%20Historical%20Reasoning%20for%20Consistent%20Online%20HD%20Vectorized%20Map%20Construction%0AAuthor%3A%20Bo%20Lang%20and%20Nirav%20Savaliya%20and%20Zhihao%20Zheng%20and%20Jinglun%20Feng%20and%20Zheng-Hang%20Yeh%20and%20Mooi%20Choo%20Chuah%0AAbstract%3A%20High-definition%20%28HD%29%20maps%20are%20crucial%20to%20autonomous%20driving%2C%20providing%20structured%20representations%20of%20road%20elements%20to%20support%20navigation%20and%20planning.%20However%2C%20existing%20query-based%20methods%20often%20employ%20random%20query%20initialization%20and%20depend%20on%20implicit%20temporal%20modeling%2C%20which%20lead%20to%20temporal%20inconsistencies%20and%20instabilities%20during%20the%20construction%20of%20a%20global%20map.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%20novel%20end-to-end%20framework%20for%20consistent%20online%20HD%20vectorized%20map%20construction%2C%20which%20jointly%20performs%20map%20instance%20tracking%20and%20short-term%20prediction.%20First%2C%20we%20propose%20a%20Semantic-Aware%20Query%20Generator%20that%20initializes%20queries%20with%20spatially%20aligned%20semantic%20masks%20to%20capture%20scene-level%20context%20globally.%20Next%2C%20we%20design%20a%20History%20Rasterized%20Map%20Memory%20to%20store%20fine-grained%20instance-level%20maps%20for%20each%20tracked%20instance%2C%20enabling%20explicit%20historical%20priors.%20A%20History-Map%20Guidance%20Module%20then%20integrates%20rasterized%20map%20information%20into%20track%20queries%2C%20improving%20temporal%20continuity.%20Finally%2C%20we%20propose%20a%20Short-Term%20Future%20Guidance%20module%20to%20forecast%20the%20immediate%20motion%20of%20map%20instances%20based%20on%20the%20stored%20history%20trajectories.%20These%20predicted%20future%20locations%20serve%20as%20hints%20for%20tracked%20instances%20to%20further%20avoid%20implausible%20predictions%20and%20keep%20temporal%20consistency.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Argoverse2%20datasets%20demonstrate%20that%20our%20proposed%20method%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20with%20good%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredMapNet%253A%2520Future%2520and%2520Historical%2520Reasoning%2520for%2520Consistent%2520Online%2520HD%2520Vectorized%2520Map%2520Construction%26entry.906535625%3DBo%2520Lang%2520and%2520Nirav%2520Savaliya%2520and%2520Zhihao%2520Zheng%2520and%2520Jinglun%2520Feng%2520and%2520Zheng-Hang%2520Yeh%2520and%2520Mooi%2520Choo%2520Chuah%26entry.1292438233%3DHigh-definition%2520%2528HD%2529%2520maps%2520are%2520crucial%2520to%2520autonomous%2520driving%252C%2520providing%2520structured%2520representations%2520of%2520road%2520elements%2520to%2520support%2520navigation%2520and%2520planning.%2520However%252C%2520existing%2520query-based%2520methods%2520often%2520employ%2520random%2520query%2520initialization%2520and%2520depend%2520on%2520implicit%2520temporal%2520modeling%252C%2520which%2520lead%2520to%2520temporal%2520inconsistencies%2520and%2520instabilities%2520during%2520the%2520construction%2520of%2520a%2520global%2520map.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520end-to-end%2520framework%2520for%2520consistent%2520online%2520HD%2520vectorized%2520map%2520construction%252C%2520which%2520jointly%2520performs%2520map%2520instance%2520tracking%2520and%2520short-term%2520prediction.%2520First%252C%2520we%2520propose%2520a%2520Semantic-Aware%2520Query%2520Generator%2520that%2520initializes%2520queries%2520with%2520spatially%2520aligned%2520semantic%2520masks%2520to%2520capture%2520scene-level%2520context%2520globally.%2520Next%252C%2520we%2520design%2520a%2520History%2520Rasterized%2520Map%2520Memory%2520to%2520store%2520fine-grained%2520instance-level%2520maps%2520for%2520each%2520tracked%2520instance%252C%2520enabling%2520explicit%2520historical%2520priors.%2520A%2520History-Map%2520Guidance%2520Module%2520then%2520integrates%2520rasterized%2520map%2520information%2520into%2520track%2520queries%252C%2520improving%2520temporal%2520continuity.%2520Finally%252C%2520we%2520propose%2520a%2520Short-Term%2520Future%2520Guidance%2520module%2520to%2520forecast%2520the%2520immediate%2520motion%2520of%2520map%2520instances%2520based%2520on%2520the%2520stored%2520history%2520trajectories.%2520These%2520predicted%2520future%2520locations%2520serve%2520as%2520hints%2520for%2520tracked%2520instances%2520to%2520further%2520avoid%2520implausible%2520predictions%2520and%2520keep%2520temporal%2520consistency.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520and%2520Argoverse2%2520datasets%2520demonstrate%2520that%2520our%2520proposed%2520method%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520with%2520good%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PredMapNet%3A%20Future%20and%20Historical%20Reasoning%20for%20Consistent%20Online%20HD%20Vectorized%20Map%20Construction&entry.906535625=Bo%20Lang%20and%20Nirav%20Savaliya%20and%20Zhihao%20Zheng%20and%20Jinglun%20Feng%20and%20Zheng-Hang%20Yeh%20and%20Mooi%20Choo%20Chuah&entry.1292438233=High-definition%20%28HD%29%20maps%20are%20crucial%20to%20autonomous%20driving%2C%20providing%20structured%20representations%20of%20road%20elements%20to%20support%20navigation%20and%20planning.%20However%2C%20existing%20query-based%20methods%20often%20employ%20random%20query%20initialization%20and%20depend%20on%20implicit%20temporal%20modeling%2C%20which%20lead%20to%20temporal%20inconsistencies%20and%20instabilities%20during%20the%20construction%20of%20a%20global%20map.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%20novel%20end-to-end%20framework%20for%20consistent%20online%20HD%20vectorized%20map%20construction%2C%20which%20jointly%20performs%20map%20instance%20tracking%20and%20short-term%20prediction.%20First%2C%20we%20propose%20a%20Semantic-Aware%20Query%20Generator%20that%20initializes%20queries%20with%20spatially%20aligned%20semantic%20masks%20to%20capture%20scene-level%20context%20globally.%20Next%2C%20we%20design%20a%20History%20Rasterized%20Map%20Memory%20to%20store%20fine-grained%20instance-level%20maps%20for%20each%20tracked%20instance%2C%20enabling%20explicit%20historical%20priors.%20A%20History-Map%20Guidance%20Module%20then%20integrates%20rasterized%20map%20information%20into%20track%20queries%2C%20improving%20temporal%20continuity.%20Finally%2C%20we%20propose%20a%20Short-Term%20Future%20Guidance%20module%20to%20forecast%20the%20immediate%20motion%20of%20map%20instances%20based%20on%20the%20stored%20history%20trajectories.%20These%20predicted%20future%20locations%20serve%20as%20hints%20for%20tracked%20instances%20to%20further%20avoid%20implausible%20predictions%20and%20keep%20temporal%20consistency.%20Extensive%20experiments%20on%20the%20nuScenes%20and%20Argoverse2%20datasets%20demonstrate%20that%20our%20proposed%20method%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20with%20good%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2602.16669v1&entry.124074799=Read"},
{"title": "Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired", "author": "Qi He and XiangXiang Wang and Jingtao Zhang and Yongbin Yu and Hongxiang Chu and Manping Fan and JingYe Cai and Zhenglin Yang", "abstract": "In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users.", "link": "http://arxiv.org/abs/2602.16385v1", "date": "2026-02-18", "relevancy": 2.3404, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5852}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5852}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Free%20Adaptive%20Multi-Scale%20Channel-Spatial%20Attention%20Aggregation%20framework%20for%203D%20Indoor%20Semantic%20Scene%20Completion%20Toward%20Assisting%20Visually%20Impaired&body=Title%3A%20Parameter-Free%20Adaptive%20Multi-Scale%20Channel-Spatial%20Attention%20Aggregation%20framework%20for%203D%20Indoor%20Semantic%20Scene%20Completion%20Toward%20Assisting%20Visually%20Impaired%0AAuthor%3A%20Qi%20He%20and%20XiangXiang%20Wang%20and%20Jingtao%20Zhang%20and%20Yongbin%20Yu%20and%20Hongxiang%20Chu%20and%20Manping%20Fan%20and%20JingYe%20Cai%20and%20Zhenglin%20Yang%0AAbstract%3A%20In%20indoor%20assistive%20perception%20for%20visually%20impaired%20users%2C%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20is%20expected%20to%20provide%20structurally%20coherent%20and%20semantically%20consistent%20occupancy%20under%20strictly%20monocular%20vision%20for%20safety-critical%20scene%20understanding.%20However%2C%20existing%20monocular%20SSC%20approaches%20often%20lack%20explicit%20modeling%20of%20voxel-feature%20reliability%20and%20regulated%20cross-scale%20information%20propagation%20during%202D-3D%20projection%20and%20multi-scale%20fusion%2C%20making%20them%20vulnerable%20to%20projection%20diffusion%20and%20feature%20entanglement%20and%20thus%20limiting%20structural%20stability.To%20address%20these%20challenges%2C%20this%20paper%20presents%20an%20Adaptive%20Multi-scale%20Attention%20Aggregation%20%28AMAA%29%20framework%20built%20upon%20the%20MonoScene%20pipeline.%20Rather%20than%20introducing%20a%20heavier%20backbone%2C%20AMAA%20focuses%20on%20reliability-oriented%20feature%20regulation%20within%20a%20monocular%20SSC%20framework.%20Specifically%2C%20lifted%20voxel%20features%20are%20jointly%20calibrated%20in%20semantic%20and%20spatial%20dimensions%20through%20parallel%20channel-spatial%20attention%20aggregation%2C%20while%20multi-scale%20encoder-decoder%20fusion%20is%20stabilized%20via%20a%20hierarchical%20adaptive%20feature-gating%20strategy%20that%20regulates%20information%20injection%20across%20scales.Experiments%20on%20the%20NYUv2%20benchmark%20demonstrate%20consistent%20improvements%20over%20MonoScene%20without%20significantly%20increasing%20system%20complexity%3A%20AMAA%20achieves%2027.25%25%20SSC%20mIoU%20%28%2B0.31%29%20and%2043.10%25%20SC%20IoU%20%28%2B0.59%29.%20In%20addition%2C%20system-level%20deployment%20on%20an%20NVIDIA%20Jetson%20platform%20verifies%20that%20the%20complete%20AMAA%20framework%20can%20be%20executed%20stably%20on%20embedded%20hardware.%20Overall%2C%20AMAA%20improves%20monocular%20SSC%20quality%20and%20provides%20a%20reliable%20and%20deployable%20perception%20framework%20for%20indoor%20assistive%20systems%20targeting%20visually%20impaired%20users.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Free%2520Adaptive%2520Multi-Scale%2520Channel-Spatial%2520Attention%2520Aggregation%2520framework%2520for%25203D%2520Indoor%2520Semantic%2520Scene%2520Completion%2520Toward%2520Assisting%2520Visually%2520Impaired%26entry.906535625%3DQi%2520He%2520and%2520XiangXiang%2520Wang%2520and%2520Jingtao%2520Zhang%2520and%2520Yongbin%2520Yu%2520and%2520Hongxiang%2520Chu%2520and%2520Manping%2520Fan%2520and%2520JingYe%2520Cai%2520and%2520Zhenglin%2520Yang%26entry.1292438233%3DIn%2520indoor%2520assistive%2520perception%2520for%2520visually%2520impaired%2520users%252C%25203D%2520Semantic%2520Scene%2520Completion%2520%2528SSC%2529%2520is%2520expected%2520to%2520provide%2520structurally%2520coherent%2520and%2520semantically%2520consistent%2520occupancy%2520under%2520strictly%2520monocular%2520vision%2520for%2520safety-critical%2520scene%2520understanding.%2520However%252C%2520existing%2520monocular%2520SSC%2520approaches%2520often%2520lack%2520explicit%2520modeling%2520of%2520voxel-feature%2520reliability%2520and%2520regulated%2520cross-scale%2520information%2520propagation%2520during%25202D-3D%2520projection%2520and%2520multi-scale%2520fusion%252C%2520making%2520them%2520vulnerable%2520to%2520projection%2520diffusion%2520and%2520feature%2520entanglement%2520and%2520thus%2520limiting%2520structural%2520stability.To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520presents%2520an%2520Adaptive%2520Multi-scale%2520Attention%2520Aggregation%2520%2528AMAA%2529%2520framework%2520built%2520upon%2520the%2520MonoScene%2520pipeline.%2520Rather%2520than%2520introducing%2520a%2520heavier%2520backbone%252C%2520AMAA%2520focuses%2520on%2520reliability-oriented%2520feature%2520regulation%2520within%2520a%2520monocular%2520SSC%2520framework.%2520Specifically%252C%2520lifted%2520voxel%2520features%2520are%2520jointly%2520calibrated%2520in%2520semantic%2520and%2520spatial%2520dimensions%2520through%2520parallel%2520channel-spatial%2520attention%2520aggregation%252C%2520while%2520multi-scale%2520encoder-decoder%2520fusion%2520is%2520stabilized%2520via%2520a%2520hierarchical%2520adaptive%2520feature-gating%2520strategy%2520that%2520regulates%2520information%2520injection%2520across%2520scales.Experiments%2520on%2520the%2520NYUv2%2520benchmark%2520demonstrate%2520consistent%2520improvements%2520over%2520MonoScene%2520without%2520significantly%2520increasing%2520system%2520complexity%253A%2520AMAA%2520achieves%252027.25%2525%2520SSC%2520mIoU%2520%2528%252B0.31%2529%2520and%252043.10%2525%2520SC%2520IoU%2520%2528%252B0.59%2529.%2520In%2520addition%252C%2520system-level%2520deployment%2520on%2520an%2520NVIDIA%2520Jetson%2520platform%2520verifies%2520that%2520the%2520complete%2520AMAA%2520framework%2520can%2520be%2520executed%2520stably%2520on%2520embedded%2520hardware.%2520Overall%252C%2520AMAA%2520improves%2520monocular%2520SSC%2520quality%2520and%2520provides%2520a%2520reliable%2520and%2520deployable%2520perception%2520framework%2520for%2520indoor%2520assistive%2520systems%2520targeting%2520visually%2520impaired%2520users.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Free%20Adaptive%20Multi-Scale%20Channel-Spatial%20Attention%20Aggregation%20framework%20for%203D%20Indoor%20Semantic%20Scene%20Completion%20Toward%20Assisting%20Visually%20Impaired&entry.906535625=Qi%20He%20and%20XiangXiang%20Wang%20and%20Jingtao%20Zhang%20and%20Yongbin%20Yu%20and%20Hongxiang%20Chu%20and%20Manping%20Fan%20and%20JingYe%20Cai%20and%20Zhenglin%20Yang&entry.1292438233=In%20indoor%20assistive%20perception%20for%20visually%20impaired%20users%2C%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20is%20expected%20to%20provide%20structurally%20coherent%20and%20semantically%20consistent%20occupancy%20under%20strictly%20monocular%20vision%20for%20safety-critical%20scene%20understanding.%20However%2C%20existing%20monocular%20SSC%20approaches%20often%20lack%20explicit%20modeling%20of%20voxel-feature%20reliability%20and%20regulated%20cross-scale%20information%20propagation%20during%202D-3D%20projection%20and%20multi-scale%20fusion%2C%20making%20them%20vulnerable%20to%20projection%20diffusion%20and%20feature%20entanglement%20and%20thus%20limiting%20structural%20stability.To%20address%20these%20challenges%2C%20this%20paper%20presents%20an%20Adaptive%20Multi-scale%20Attention%20Aggregation%20%28AMAA%29%20framework%20built%20upon%20the%20MonoScene%20pipeline.%20Rather%20than%20introducing%20a%20heavier%20backbone%2C%20AMAA%20focuses%20on%20reliability-oriented%20feature%20regulation%20within%20a%20monocular%20SSC%20framework.%20Specifically%2C%20lifted%20voxel%20features%20are%20jointly%20calibrated%20in%20semantic%20and%20spatial%20dimensions%20through%20parallel%20channel-spatial%20attention%20aggregation%2C%20while%20multi-scale%20encoder-decoder%20fusion%20is%20stabilized%20via%20a%20hierarchical%20adaptive%20feature-gating%20strategy%20that%20regulates%20information%20injection%20across%20scales.Experiments%20on%20the%20NYUv2%20benchmark%20demonstrate%20consistent%20improvements%20over%20MonoScene%20without%20significantly%20increasing%20system%20complexity%3A%20AMAA%20achieves%2027.25%25%20SSC%20mIoU%20%28%2B0.31%29%20and%2043.10%25%20SC%20IoU%20%28%2B0.59%29.%20In%20addition%2C%20system-level%20deployment%20on%20an%20NVIDIA%20Jetson%20platform%20verifies%20that%20the%20complete%20AMAA%20framework%20can%20be%20executed%20stably%20on%20embedded%20hardware.%20Overall%2C%20AMAA%20improves%20monocular%20SSC%20quality%20and%20provides%20a%20reliable%20and%20deployable%20perception%20framework%20for%20indoor%20assistive%20systems%20targeting%20visually%20impaired%20users.&entry.1838667208=http%3A//arxiv.org/abs/2602.16385v1&entry.124074799=Read"},
{"title": "FeatBench: Towards More Realistic Evaluation of Feature-level Code Generation", "author": "Haorui Chen and Chengze Li and Jia Li", "abstract": "Evaluating Large Language Models (LLMs) on repository-level feature implementation is a critical frontier in software engineering. However, establishing a benchmark that faithfully mirrors realistic development scenarios remains a significant challenge. Existing feature-level benchmarks generally suffer from two primary limitations: unrealistic task inputs enriched with code hints and significant data leakage risks due to their static nature. To address these limitations, we propose a new benchmark - FeatBench, which introduces the following advances: (1) Realistic Task Inputs. Task inputs consist solely of natural language requirements, strictly devoid of code hints (e.g., function signatures). This format mirrors realistic software development by requiring agents to independently bridge the gap between abstract user intent and concrete code changes. (2) Evolving Data. FeatBench employs a fully automated pipeline to construct new benchmark versions from the latest repositories, effectively mitigating data contamination. The initial release comprises 157 tasks sourced from 27 actively maintained repositories. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. The results reveal that FeatBench poses a significant challenge, with the highest resolved rate reaching only 29.94%. Crucially, our analysis uncovers a prevalent behavioral pattern of aggressive implementation, which leads to \"scope creep\" and widespread regressions where agents break existing features by diverging from the user's explicit intent. We release FeatBench, our automated pipeline, and all experimental results to facilitate further community research.", "link": "http://arxiv.org/abs/2509.22237v2", "date": "2026-02-18", "relevancy": 2.3401, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4734}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FeatBench%3A%20Towards%20More%20Realistic%20Evaluation%20of%20Feature-level%20Code%20Generation&body=Title%3A%20FeatBench%3A%20Towards%20More%20Realistic%20Evaluation%20of%20Feature-level%20Code%20Generation%0AAuthor%3A%20Haorui%20Chen%20and%20Chengze%20Li%20and%20Jia%20Li%0AAbstract%3A%20Evaluating%20Large%20Language%20Models%20%28LLMs%29%20on%20repository-level%20feature%20implementation%20is%20a%20critical%20frontier%20in%20software%20engineering.%20However%2C%20establishing%20a%20benchmark%20that%20faithfully%20mirrors%20realistic%20development%20scenarios%20remains%20a%20significant%20challenge.%20Existing%20feature-level%20benchmarks%20generally%20suffer%20from%20two%20primary%20limitations%3A%20unrealistic%20task%20inputs%20enriched%20with%20code%20hints%20and%20significant%20data%20leakage%20risks%20due%20to%20their%20static%20nature.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20new%20benchmark%20-%20FeatBench%2C%20which%20introduces%20the%20following%20advances%3A%20%281%29%20Realistic%20Task%20Inputs.%20Task%20inputs%20consist%20solely%20of%20natural%20language%20requirements%2C%20strictly%20devoid%20of%20code%20hints%20%28e.g.%2C%20function%20signatures%29.%20This%20format%20mirrors%20realistic%20software%20development%20by%20requiring%20agents%20to%20independently%20bridge%20the%20gap%20between%20abstract%20user%20intent%20and%20concrete%20code%20changes.%20%282%29%20Evolving%20Data.%20FeatBench%20employs%20a%20fully%20automated%20pipeline%20to%20construct%20new%20benchmark%20versions%20from%20the%20latest%20repositories%2C%20effectively%20mitigating%20data%20contamination.%20The%20initial%20release%20comprises%20157%20tasks%20sourced%20from%2027%20actively%20maintained%20repositories.%20We%20evaluate%20two%20state-of-the-art%20agent%20frameworks%20with%20four%20leading%20LLMs%20on%20FeatBench.%20The%20results%20reveal%20that%20FeatBench%20poses%20a%20significant%20challenge%2C%20with%20the%20highest%20resolved%20rate%20reaching%20only%2029.94%25.%20Crucially%2C%20our%20analysis%20uncovers%20a%20prevalent%20behavioral%20pattern%20of%20aggressive%20implementation%2C%20which%20leads%20to%20%22scope%20creep%22%20and%20widespread%20regressions%20where%20agents%20break%20existing%20features%20by%20diverging%20from%20the%20user%27s%20explicit%20intent.%20We%20release%20FeatBench%2C%20our%20automated%20pipeline%2C%20and%20all%20experimental%20results%20to%20facilitate%20further%20community%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeatBench%253A%2520Towards%2520More%2520Realistic%2520Evaluation%2520of%2520Feature-level%2520Code%2520Generation%26entry.906535625%3DHaorui%2520Chen%2520and%2520Chengze%2520Li%2520and%2520Jia%2520Li%26entry.1292438233%3DEvaluating%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520repository-level%2520feature%2520implementation%2520is%2520a%2520critical%2520frontier%2520in%2520software%2520engineering.%2520However%252C%2520establishing%2520a%2520benchmark%2520that%2520faithfully%2520mirrors%2520realistic%2520development%2520scenarios%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520feature-level%2520benchmarks%2520generally%2520suffer%2520from%2520two%2520primary%2520limitations%253A%2520unrealistic%2520task%2520inputs%2520enriched%2520with%2520code%2520hints%2520and%2520significant%2520data%2520leakage%2520risks%2520due%2520to%2520their%2520static%2520nature.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520-%2520FeatBench%252C%2520which%2520introduces%2520the%2520following%2520advances%253A%2520%25281%2529%2520Realistic%2520Task%2520Inputs.%2520Task%2520inputs%2520consist%2520solely%2520of%2520natural%2520language%2520requirements%252C%2520strictly%2520devoid%2520of%2520code%2520hints%2520%2528e.g.%252C%2520function%2520signatures%2529.%2520This%2520format%2520mirrors%2520realistic%2520software%2520development%2520by%2520requiring%2520agents%2520to%2520independently%2520bridge%2520the%2520gap%2520between%2520abstract%2520user%2520intent%2520and%2520concrete%2520code%2520changes.%2520%25282%2529%2520Evolving%2520Data.%2520FeatBench%2520employs%2520a%2520fully%2520automated%2520pipeline%2520to%2520construct%2520new%2520benchmark%2520versions%2520from%2520the%2520latest%2520repositories%252C%2520effectively%2520mitigating%2520data%2520contamination.%2520The%2520initial%2520release%2520comprises%2520157%2520tasks%2520sourced%2520from%252027%2520actively%2520maintained%2520repositories.%2520We%2520evaluate%2520two%2520state-of-the-art%2520agent%2520frameworks%2520with%2520four%2520leading%2520LLMs%2520on%2520FeatBench.%2520The%2520results%2520reveal%2520that%2520FeatBench%2520poses%2520a%2520significant%2520challenge%252C%2520with%2520the%2520highest%2520resolved%2520rate%2520reaching%2520only%252029.94%2525.%2520Crucially%252C%2520our%2520analysis%2520uncovers%2520a%2520prevalent%2520behavioral%2520pattern%2520of%2520aggressive%2520implementation%252C%2520which%2520leads%2520to%2520%2522scope%2520creep%2522%2520and%2520widespread%2520regressions%2520where%2520agents%2520break%2520existing%2520features%2520by%2520diverging%2520from%2520the%2520user%2527s%2520explicit%2520intent.%2520We%2520release%2520FeatBench%252C%2520our%2520automated%2520pipeline%252C%2520and%2520all%2520experimental%2520results%2520to%2520facilitate%2520further%2520community%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeatBench%3A%20Towards%20More%20Realistic%20Evaluation%20of%20Feature-level%20Code%20Generation&entry.906535625=Haorui%20Chen%20and%20Chengze%20Li%20and%20Jia%20Li&entry.1292438233=Evaluating%20Large%20Language%20Models%20%28LLMs%29%20on%20repository-level%20feature%20implementation%20is%20a%20critical%20frontier%20in%20software%20engineering.%20However%2C%20establishing%20a%20benchmark%20that%20faithfully%20mirrors%20realistic%20development%20scenarios%20remains%20a%20significant%20challenge.%20Existing%20feature-level%20benchmarks%20generally%20suffer%20from%20two%20primary%20limitations%3A%20unrealistic%20task%20inputs%20enriched%20with%20code%20hints%20and%20significant%20data%20leakage%20risks%20due%20to%20their%20static%20nature.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20new%20benchmark%20-%20FeatBench%2C%20which%20introduces%20the%20following%20advances%3A%20%281%29%20Realistic%20Task%20Inputs.%20Task%20inputs%20consist%20solely%20of%20natural%20language%20requirements%2C%20strictly%20devoid%20of%20code%20hints%20%28e.g.%2C%20function%20signatures%29.%20This%20format%20mirrors%20realistic%20software%20development%20by%20requiring%20agents%20to%20independently%20bridge%20the%20gap%20between%20abstract%20user%20intent%20and%20concrete%20code%20changes.%20%282%29%20Evolving%20Data.%20FeatBench%20employs%20a%20fully%20automated%20pipeline%20to%20construct%20new%20benchmark%20versions%20from%20the%20latest%20repositories%2C%20effectively%20mitigating%20data%20contamination.%20The%20initial%20release%20comprises%20157%20tasks%20sourced%20from%2027%20actively%20maintained%20repositories.%20We%20evaluate%20two%20state-of-the-art%20agent%20frameworks%20with%20four%20leading%20LLMs%20on%20FeatBench.%20The%20results%20reveal%20that%20FeatBench%20poses%20a%20significant%20challenge%2C%20with%20the%20highest%20resolved%20rate%20reaching%20only%2029.94%25.%20Crucially%2C%20our%20analysis%20uncovers%20a%20prevalent%20behavioral%20pattern%20of%20aggressive%20implementation%2C%20which%20leads%20to%20%22scope%20creep%22%20and%20widespread%20regressions%20where%20agents%20break%20existing%20features%20by%20diverging%20from%20the%20user%27s%20explicit%20intent.%20We%20release%20FeatBench%2C%20our%20automated%20pipeline%2C%20and%20all%20experimental%20results%20to%20facilitate%20further%20community%20research.&entry.1838667208=http%3A//arxiv.org/abs/2509.22237v2&entry.124074799=Read"},
{"title": "Align and Adapt: Multimodal Multiview Human Activity Recognition under Arbitrary View Combinations", "author": "Duc-Anh Nguyen and Nhien-An Le-Khac", "abstract": "Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose AliAd, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. AliAd is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.", "link": "http://arxiv.org/abs/2602.08755v3", "date": "2026-02-18", "relevancy": 2.335, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.618}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5631}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Align%20and%20Adapt%3A%20Multimodal%20Multiview%20Human%20Activity%20Recognition%20under%20Arbitrary%20View%20Combinations&body=Title%3A%20Align%20and%20Adapt%3A%20Multimodal%20Multiview%20Human%20Activity%20Recognition%20under%20Arbitrary%20View%20Combinations%0AAuthor%3A%20Duc-Anh%20Nguyen%20and%20Nhien-An%20Le-Khac%0AAbstract%3A%20Multimodal%20multiview%20learning%20seeks%20to%20integrate%20information%20from%20diverse%20sources%20to%20enhance%20task%20performance.%20Existing%20approaches%20often%20struggle%20with%20flexible%20view%20configurations%2C%20including%20arbitrary%20view%20combinations%2C%20numbers%20of%20views%2C%20and%20heterogeneous%20modalities.%20Focusing%20on%20the%20context%20of%20human%20activity%20recognition%2C%20we%20propose%20AliAd%2C%20a%20model%20that%20combines%20multiview%20contrastive%20learning%20with%20a%20mixture-of-experts%20module%20to%20support%20arbitrary%20view%20availability%20during%20both%20training%20and%20inference.%20Instead%20of%20trying%20to%20reconstruct%20missing%20views%2C%20an%20adjusted%20center%20contrastive%20loss%20is%20used%20for%20self-supervised%20representation%20learning%20and%20view%20alignment%2C%20mitigating%20the%20impact%20of%20missing%20views%20on%20multiview%20fusion.%20This%20loss%20formulation%20allows%20for%20the%20integration%20of%20view%20weights%20to%20account%20for%20view%20quality.%20Additionally%2C%20it%20reduces%20computational%20complexity%20from%20%24O%28V%5E2%29%24%20to%20%24O%28V%29%24%2C%20where%20%24V%24%20is%20the%20number%20of%20views.%20To%20address%20residual%20discrepancies%20not%20captured%20by%20contrastive%20learning%2C%20we%20employ%20a%20mixture-of-experts%20module%20with%20a%20specialized%20load%20balancing%20strategy%2C%20tasked%20with%20adapting%20to%20arbitrary%20view%20combinations.%20We%20highlight%20the%20geometric%20relationship%20among%20components%20in%20our%20model%20and%20how%20they%20combine%20well%20in%20the%20latent%20space.%20AliAd%20is%20validated%20on%20four%20datasets%20encompassing%20inertial%20and%20human%20pose%20modalities%2C%20with%20the%20number%20of%20views%20ranging%20from%20three%20to%20nine%2C%20demonstrating%20its%20performance%20and%20flexibility.%0ALink%3A%20http%3A//arxiv.org/abs/2602.08755v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlign%2520and%2520Adapt%253A%2520Multimodal%2520Multiview%2520Human%2520Activity%2520Recognition%2520under%2520Arbitrary%2520View%2520Combinations%26entry.906535625%3DDuc-Anh%2520Nguyen%2520and%2520Nhien-An%2520Le-Khac%26entry.1292438233%3DMultimodal%2520multiview%2520learning%2520seeks%2520to%2520integrate%2520information%2520from%2520diverse%2520sources%2520to%2520enhance%2520task%2520performance.%2520Existing%2520approaches%2520often%2520struggle%2520with%2520flexible%2520view%2520configurations%252C%2520including%2520arbitrary%2520view%2520combinations%252C%2520numbers%2520of%2520views%252C%2520and%2520heterogeneous%2520modalities.%2520Focusing%2520on%2520the%2520context%2520of%2520human%2520activity%2520recognition%252C%2520we%2520propose%2520AliAd%252C%2520a%2520model%2520that%2520combines%2520multiview%2520contrastive%2520learning%2520with%2520a%2520mixture-of-experts%2520module%2520to%2520support%2520arbitrary%2520view%2520availability%2520during%2520both%2520training%2520and%2520inference.%2520Instead%2520of%2520trying%2520to%2520reconstruct%2520missing%2520views%252C%2520an%2520adjusted%2520center%2520contrastive%2520loss%2520is%2520used%2520for%2520self-supervised%2520representation%2520learning%2520and%2520view%2520alignment%252C%2520mitigating%2520the%2520impact%2520of%2520missing%2520views%2520on%2520multiview%2520fusion.%2520This%2520loss%2520formulation%2520allows%2520for%2520the%2520integration%2520of%2520view%2520weights%2520to%2520account%2520for%2520view%2520quality.%2520Additionally%252C%2520it%2520reduces%2520computational%2520complexity%2520from%2520%2524O%2528V%255E2%2529%2524%2520to%2520%2524O%2528V%2529%2524%252C%2520where%2520%2524V%2524%2520is%2520the%2520number%2520of%2520views.%2520To%2520address%2520residual%2520discrepancies%2520not%2520captured%2520by%2520contrastive%2520learning%252C%2520we%2520employ%2520a%2520mixture-of-experts%2520module%2520with%2520a%2520specialized%2520load%2520balancing%2520strategy%252C%2520tasked%2520with%2520adapting%2520to%2520arbitrary%2520view%2520combinations.%2520We%2520highlight%2520the%2520geometric%2520relationship%2520among%2520components%2520in%2520our%2520model%2520and%2520how%2520they%2520combine%2520well%2520in%2520the%2520latent%2520space.%2520AliAd%2520is%2520validated%2520on%2520four%2520datasets%2520encompassing%2520inertial%2520and%2520human%2520pose%2520modalities%252C%2520with%2520the%2520number%2520of%2520views%2520ranging%2520from%2520three%2520to%2520nine%252C%2520demonstrating%2520its%2520performance%2520and%2520flexibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.08755v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Align%20and%20Adapt%3A%20Multimodal%20Multiview%20Human%20Activity%20Recognition%20under%20Arbitrary%20View%20Combinations&entry.906535625=Duc-Anh%20Nguyen%20and%20Nhien-An%20Le-Khac&entry.1292438233=Multimodal%20multiview%20learning%20seeks%20to%20integrate%20information%20from%20diverse%20sources%20to%20enhance%20task%20performance.%20Existing%20approaches%20often%20struggle%20with%20flexible%20view%20configurations%2C%20including%20arbitrary%20view%20combinations%2C%20numbers%20of%20views%2C%20and%20heterogeneous%20modalities.%20Focusing%20on%20the%20context%20of%20human%20activity%20recognition%2C%20we%20propose%20AliAd%2C%20a%20model%20that%20combines%20multiview%20contrastive%20learning%20with%20a%20mixture-of-experts%20module%20to%20support%20arbitrary%20view%20availability%20during%20both%20training%20and%20inference.%20Instead%20of%20trying%20to%20reconstruct%20missing%20views%2C%20an%20adjusted%20center%20contrastive%20loss%20is%20used%20for%20self-supervised%20representation%20learning%20and%20view%20alignment%2C%20mitigating%20the%20impact%20of%20missing%20views%20on%20multiview%20fusion.%20This%20loss%20formulation%20allows%20for%20the%20integration%20of%20view%20weights%20to%20account%20for%20view%20quality.%20Additionally%2C%20it%20reduces%20computational%20complexity%20from%20%24O%28V%5E2%29%24%20to%20%24O%28V%29%24%2C%20where%20%24V%24%20is%20the%20number%20of%20views.%20To%20address%20residual%20discrepancies%20not%20captured%20by%20contrastive%20learning%2C%20we%20employ%20a%20mixture-of-experts%20module%20with%20a%20specialized%20load%20balancing%20strategy%2C%20tasked%20with%20adapting%20to%20arbitrary%20view%20combinations.%20We%20highlight%20the%20geometric%20relationship%20among%20components%20in%20our%20model%20and%20how%20they%20combine%20well%20in%20the%20latent%20space.%20AliAd%20is%20validated%20on%20four%20datasets%20encompassing%20inertial%20and%20human%20pose%20modalities%2C%20with%20the%20number%20of%20views%20ranging%20from%20three%20to%20nine%2C%20demonstrating%20its%20performance%20and%20flexibility.&entry.1838667208=http%3A//arxiv.org/abs/2602.08755v3&entry.124074799=Read"},
{"title": "Creating a digital poet", "author": "Vered Tohar and Tsahi Hayat and Amir Leshem", "abstract": "Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.", "link": "http://arxiv.org/abs/2602.16578v1", "date": "2026-02-18", "relevancy": 2.3184, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4862}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4526}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Creating%20a%20digital%20poet&body=Title%3A%20Creating%20a%20digital%20poet%0AAuthor%3A%20Vered%20Tohar%20and%20Tsahi%20Hayat%20and%20Amir%20Leshem%0AAbstract%3A%20Can%20a%20machine%20write%20good%20poetry%3F%20Any%20positive%20answer%20raises%20fundamental%20questions%20about%20the%20nature%20and%20value%20of%20art.%20We%20report%20a%20seven-month%20poetry%20workshop%20in%20which%20a%20large%20language%20model%20was%20shaped%20into%20a%20digital%20poet%20through%20iterative%20in-context%20expert%20feedback%2C%20without%20retraining.%20Across%20sessions%2C%20the%20model%20developed%20a%20distinctive%20style%20and%20a%20coherent%20corpus%2C%20supported%20by%20quantitative%20and%20qualitative%20analyses%2C%20and%20it%20produced%20a%20pen%20name%20and%20author%20image.%20In%20a%20blinded%20authorship%20test%20with%2050%20humanities%20students%20and%20graduates%20%28three%20AI%20poems%20and%20three%20poems%20by%20well-known%20poets%20each%29%2C%20judgments%20were%20at%20chance%3A%20human%20poems%20were%20labeled%20human%2054%25%20of%20the%20time%20and%20AI%20poems%2052%25%2C%20with%2095%25%20confidence%20intervals%20including%2050%25.%20After%20the%20workshop%2C%20a%20commercial%20publisher%20released%20a%20poetry%20collection%20authored%20by%20the%20model.%20These%20results%20show%20that%20workshop-style%20prompting%20can%20support%20long-horizon%20creative%20shaping%20and%20renew%20debates%20on%20creativity%20and%20authorship.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCreating%2520a%2520digital%2520poet%26entry.906535625%3DVered%2520Tohar%2520and%2520Tsahi%2520Hayat%2520and%2520Amir%2520Leshem%26entry.1292438233%3DCan%2520a%2520machine%2520write%2520good%2520poetry%253F%2520Any%2520positive%2520answer%2520raises%2520fundamental%2520questions%2520about%2520the%2520nature%2520and%2520value%2520of%2520art.%2520We%2520report%2520a%2520seven-month%2520poetry%2520workshop%2520in%2520which%2520a%2520large%2520language%2520model%2520was%2520shaped%2520into%2520a%2520digital%2520poet%2520through%2520iterative%2520in-context%2520expert%2520feedback%252C%2520without%2520retraining.%2520Across%2520sessions%252C%2520the%2520model%2520developed%2520a%2520distinctive%2520style%2520and%2520a%2520coherent%2520corpus%252C%2520supported%2520by%2520quantitative%2520and%2520qualitative%2520analyses%252C%2520and%2520it%2520produced%2520a%2520pen%2520name%2520and%2520author%2520image.%2520In%2520a%2520blinded%2520authorship%2520test%2520with%252050%2520humanities%2520students%2520and%2520graduates%2520%2528three%2520AI%2520poems%2520and%2520three%2520poems%2520by%2520well-known%2520poets%2520each%2529%252C%2520judgments%2520were%2520at%2520chance%253A%2520human%2520poems%2520were%2520labeled%2520human%252054%2525%2520of%2520the%2520time%2520and%2520AI%2520poems%252052%2525%252C%2520with%252095%2525%2520confidence%2520intervals%2520including%252050%2525.%2520After%2520the%2520workshop%252C%2520a%2520commercial%2520publisher%2520released%2520a%2520poetry%2520collection%2520authored%2520by%2520the%2520model.%2520These%2520results%2520show%2520that%2520workshop-style%2520prompting%2520can%2520support%2520long-horizon%2520creative%2520shaping%2520and%2520renew%2520debates%2520on%2520creativity%2520and%2520authorship.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Creating%20a%20digital%20poet&entry.906535625=Vered%20Tohar%20and%20Tsahi%20Hayat%20and%20Amir%20Leshem&entry.1292438233=Can%20a%20machine%20write%20good%20poetry%3F%20Any%20positive%20answer%20raises%20fundamental%20questions%20about%20the%20nature%20and%20value%20of%20art.%20We%20report%20a%20seven-month%20poetry%20workshop%20in%20which%20a%20large%20language%20model%20was%20shaped%20into%20a%20digital%20poet%20through%20iterative%20in-context%20expert%20feedback%2C%20without%20retraining.%20Across%20sessions%2C%20the%20model%20developed%20a%20distinctive%20style%20and%20a%20coherent%20corpus%2C%20supported%20by%20quantitative%20and%20qualitative%20analyses%2C%20and%20it%20produced%20a%20pen%20name%20and%20author%20image.%20In%20a%20blinded%20authorship%20test%20with%2050%20humanities%20students%20and%20graduates%20%28three%20AI%20poems%20and%20three%20poems%20by%20well-known%20poets%20each%29%2C%20judgments%20were%20at%20chance%3A%20human%20poems%20were%20labeled%20human%2054%25%20of%20the%20time%20and%20AI%20poems%2052%25%2C%20with%2095%25%20confidence%20intervals%20including%2050%25.%20After%20the%20workshop%2C%20a%20commercial%20publisher%20released%20a%20poetry%20collection%20authored%20by%20the%20model.%20These%20results%20show%20that%20workshop-style%20prompting%20can%20support%20long-horizon%20creative%20shaping%20and%20renew%20debates%20on%20creativity%20and%20authorship.&entry.1838667208=http%3A//arxiv.org/abs/2602.16578v1&entry.124074799=Read"},
{"title": "VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection", "author": "Yingyuan Yang and Tian Lan and Yifei Gao and Yimeng Lu and Wenjun He and Meng Wang and Chenghao Liu and Chen Zhang", "abstract": "Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.", "link": "http://arxiv.org/abs/2602.16681v1", "date": "2026-02-18", "relevancy": 2.3123, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5975}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5661}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VETime%3A%20Vision%20Enhanced%20Zero-Shot%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20VETime%3A%20Vision%20Enhanced%20Zero-Shot%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Yingyuan%20Yang%20and%20Tian%20Lan%20and%20Yifei%20Gao%20and%20Yimeng%20Lu%20and%20Wenjun%20He%20and%20Meng%20Wang%20and%20Chenghao%20Liu%20and%20Chen%20Zhang%0AAbstract%3A%20Time-series%20anomaly%20detection%20%28TSAD%29%20requires%20identifying%20both%20immediate%20Point%20Anomalies%20and%20long-range%20Context%20Anomalies.%20However%2C%20existing%20foundation%20models%20face%20a%20fundamental%20trade-off%3A%201D%20temporal%20models%20provide%20fine-grained%20pointwise%20localization%20but%20lack%20a%20global%20contextual%20perspective%2C%20while%202D%20vision-based%20models%20capture%20global%20patterns%20but%20suffer%20from%20information%20bottlenecks%20due%20to%20a%20lack%20of%20temporal%20alignment%20and%20coarse-grained%20pointwise%20detection.%20To%20resolve%20this%20dilemma%2C%20we%20propose%20VETime%2C%20the%20first%20TSAD%20framework%20that%20unifies%20temporal%20and%20visual%20modalities%20through%20fine-grained%20visual-temporal%20alignment%20and%20dynamic%20fusion.%20VETime%20introduces%20a%20Reversible%20Image%20Conversion%20and%20a%20Patch-Level%20Temporal%20Alignment%20module%20to%20establish%20a%20shared%20visual-temporal%20timeline%2C%20preserving%20discriminative%20details%20while%20maintaining%20temporal%20sensitivity.%20Furthermore%2C%20we%20design%20an%20Anomaly%20Window%20Contrastive%20Learning%20mechanism%20and%20a%20Task-Adaptive%20Multi-Modal%20Fusion%20to%20adaptively%20integrate%20the%20complementary%20perceptual%20strengths%20of%20both%20modalities.%20Extensive%20experiments%20demonstrate%20that%20VETime%20significantly%20outperforms%20state-of-the-art%20models%20in%20zero-shot%20scenarios%2C%20achieving%20superior%20localization%20precision%20with%20lower%20computational%20overhead%20than%20current%20vision-based%20approaches.%20Code%20available%20at%3A%20https%3A//github.com/yyyangcoder/VETime.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVETime%253A%2520Vision%2520Enhanced%2520Zero-Shot%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DYingyuan%2520Yang%2520and%2520Tian%2520Lan%2520and%2520Yifei%2520Gao%2520and%2520Yimeng%2520Lu%2520and%2520Wenjun%2520He%2520and%2520Meng%2520Wang%2520and%2520Chenghao%2520Liu%2520and%2520Chen%2520Zhang%26entry.1292438233%3DTime-series%2520anomaly%2520detection%2520%2528TSAD%2529%2520requires%2520identifying%2520both%2520immediate%2520Point%2520Anomalies%2520and%2520long-range%2520Context%2520Anomalies.%2520However%252C%2520existing%2520foundation%2520models%2520face%2520a%2520fundamental%2520trade-off%253A%25201D%2520temporal%2520models%2520provide%2520fine-grained%2520pointwise%2520localization%2520but%2520lack%2520a%2520global%2520contextual%2520perspective%252C%2520while%25202D%2520vision-based%2520models%2520capture%2520global%2520patterns%2520but%2520suffer%2520from%2520information%2520bottlenecks%2520due%2520to%2520a%2520lack%2520of%2520temporal%2520alignment%2520and%2520coarse-grained%2520pointwise%2520detection.%2520To%2520resolve%2520this%2520dilemma%252C%2520we%2520propose%2520VETime%252C%2520the%2520first%2520TSAD%2520framework%2520that%2520unifies%2520temporal%2520and%2520visual%2520modalities%2520through%2520fine-grained%2520visual-temporal%2520alignment%2520and%2520dynamic%2520fusion.%2520VETime%2520introduces%2520a%2520Reversible%2520Image%2520Conversion%2520and%2520a%2520Patch-Level%2520Temporal%2520Alignment%2520module%2520to%2520establish%2520a%2520shared%2520visual-temporal%2520timeline%252C%2520preserving%2520discriminative%2520details%2520while%2520maintaining%2520temporal%2520sensitivity.%2520Furthermore%252C%2520we%2520design%2520an%2520Anomaly%2520Window%2520Contrastive%2520Learning%2520mechanism%2520and%2520a%2520Task-Adaptive%2520Multi-Modal%2520Fusion%2520to%2520adaptively%2520integrate%2520the%2520complementary%2520perceptual%2520strengths%2520of%2520both%2520modalities.%2520Extensive%2520experiments%2520demonstrate%2520that%2520VETime%2520significantly%2520outperforms%2520state-of-the-art%2520models%2520in%2520zero-shot%2520scenarios%252C%2520achieving%2520superior%2520localization%2520precision%2520with%2520lower%2520computational%2520overhead%2520than%2520current%2520vision-based%2520approaches.%2520Code%2520available%2520at%253A%2520https%253A//github.com/yyyangcoder/VETime.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VETime%3A%20Vision%20Enhanced%20Zero-Shot%20Time%20Series%20Anomaly%20Detection&entry.906535625=Yingyuan%20Yang%20and%20Tian%20Lan%20and%20Yifei%20Gao%20and%20Yimeng%20Lu%20and%20Wenjun%20He%20and%20Meng%20Wang%20and%20Chenghao%20Liu%20and%20Chen%20Zhang&entry.1292438233=Time-series%20anomaly%20detection%20%28TSAD%29%20requires%20identifying%20both%20immediate%20Point%20Anomalies%20and%20long-range%20Context%20Anomalies.%20However%2C%20existing%20foundation%20models%20face%20a%20fundamental%20trade-off%3A%201D%20temporal%20models%20provide%20fine-grained%20pointwise%20localization%20but%20lack%20a%20global%20contextual%20perspective%2C%20while%202D%20vision-based%20models%20capture%20global%20patterns%20but%20suffer%20from%20information%20bottlenecks%20due%20to%20a%20lack%20of%20temporal%20alignment%20and%20coarse-grained%20pointwise%20detection.%20To%20resolve%20this%20dilemma%2C%20we%20propose%20VETime%2C%20the%20first%20TSAD%20framework%20that%20unifies%20temporal%20and%20visual%20modalities%20through%20fine-grained%20visual-temporal%20alignment%20and%20dynamic%20fusion.%20VETime%20introduces%20a%20Reversible%20Image%20Conversion%20and%20a%20Patch-Level%20Temporal%20Alignment%20module%20to%20establish%20a%20shared%20visual-temporal%20timeline%2C%20preserving%20discriminative%20details%20while%20maintaining%20temporal%20sensitivity.%20Furthermore%2C%20we%20design%20an%20Anomaly%20Window%20Contrastive%20Learning%20mechanism%20and%20a%20Task-Adaptive%20Multi-Modal%20Fusion%20to%20adaptively%20integrate%20the%20complementary%20perceptual%20strengths%20of%20both%20modalities.%20Extensive%20experiments%20demonstrate%20that%20VETime%20significantly%20outperforms%20state-of-the-art%20models%20in%20zero-shot%20scenarios%2C%20achieving%20superior%20localization%20precision%20with%20lower%20computational%20overhead%20than%20current%20vision-based%20approaches.%20Code%20available%20at%3A%20https%3A//github.com/yyyangcoder/VETime.&entry.1838667208=http%3A//arxiv.org/abs/2602.16681v1&entry.124074799=Read"},
{"title": "RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion", "author": "Kavyansh Tyagi and Vishwas Rathi and Puneet Goyal", "abstract": "Accurate and computationally efficient 3D medical image segmentation remains a critical challenge in clinical workflows. Transformer-based architectures often demonstrate superior global contextual modeling but at the expense of excessive parameter counts and memory demands, restricting their clinical deployment. We propose RefineFormer3D, a lightweight hierarchical transformer architecture that balances segmentation accuracy and computational efficiency for volumetric medical imaging. The architecture integrates three key components: (i) GhostConv3D-based patch embedding for efficient feature extraction with minimal redundancy, (ii) MixFFN3D module with low-rank projections and depthwise convolutions for parameter-efficient feature extraction, and (iii) a cross-attention fusion decoder enabling adaptive multi-scale skip connection integration. RefineFormer3D contains only 2.94M parameters, substantially fewer than contemporary transformer-based methods. Extensive experiments on ACDC and BraTS benchmarks demonstrate that RefineFormer3D achieves 93.44\\% and 85.9\\% average Dice scores respectively, outperforming or matching state-of-the-art methods while requiring significantly fewer parameters. Furthermore, the model achieves fast inference (8.35 ms per volume on GPU) with low memory requirements, supporting deployment in resource-constrained clinical environments. These results establish RefineFormer3D as an effective and scalable solution for practical 3D medical image segmentation.", "link": "http://arxiv.org/abs/2602.16320v1", "date": "2026-02-18", "relevancy": 2.3116, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5995}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5881}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefineFormer3D%3A%20Efficient%203D%20Medical%20Image%20Segmentation%20via%20Adaptive%20Multi-Scale%20Transformer%20with%20Cross%20Attention%20Fusion&body=Title%3A%20RefineFormer3D%3A%20Efficient%203D%20Medical%20Image%20Segmentation%20via%20Adaptive%20Multi-Scale%20Transformer%20with%20Cross%20Attention%20Fusion%0AAuthor%3A%20Kavyansh%20Tyagi%20and%20Vishwas%20Rathi%20and%20Puneet%20Goyal%0AAbstract%3A%20Accurate%20and%20computationally%20efficient%203D%20medical%20image%20segmentation%20remains%20a%20critical%20challenge%20in%20clinical%20workflows.%20Transformer-based%20architectures%20often%20demonstrate%20superior%20global%20contextual%20modeling%20but%20at%20the%20expense%20of%20excessive%20parameter%20counts%20and%20memory%20demands%2C%20restricting%20their%20clinical%20deployment.%20We%20propose%20RefineFormer3D%2C%20a%20lightweight%20hierarchical%20transformer%20architecture%20that%20balances%20segmentation%20accuracy%20and%20computational%20efficiency%20for%20volumetric%20medical%20imaging.%20The%20architecture%20integrates%20three%20key%20components%3A%20%28i%29%20GhostConv3D-based%20patch%20embedding%20for%20efficient%20feature%20extraction%20with%20minimal%20redundancy%2C%20%28ii%29%20MixFFN3D%20module%20with%20low-rank%20projections%20and%20depthwise%20convolutions%20for%20parameter-efficient%20feature%20extraction%2C%20and%20%28iii%29%20a%20cross-attention%20fusion%20decoder%20enabling%20adaptive%20multi-scale%20skip%20connection%20integration.%20RefineFormer3D%20contains%20only%202.94M%20parameters%2C%20substantially%20fewer%20than%20contemporary%20transformer-based%20methods.%20Extensive%20experiments%20on%20ACDC%20and%20BraTS%20benchmarks%20demonstrate%20that%20RefineFormer3D%20achieves%2093.44%5C%25%20and%2085.9%5C%25%20average%20Dice%20scores%20respectively%2C%20outperforming%20or%20matching%20state-of-the-art%20methods%20while%20requiring%20significantly%20fewer%20parameters.%20Furthermore%2C%20the%20model%20achieves%20fast%20inference%20%288.35%20ms%20per%20volume%20on%20GPU%29%20with%20low%20memory%20requirements%2C%20supporting%20deployment%20in%20resource-constrained%20clinical%20environments.%20These%20results%20establish%20RefineFormer3D%20as%20an%20effective%20and%20scalable%20solution%20for%20practical%203D%20medical%20image%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefineFormer3D%253A%2520Efficient%25203D%2520Medical%2520Image%2520Segmentation%2520via%2520Adaptive%2520Multi-Scale%2520Transformer%2520with%2520Cross%2520Attention%2520Fusion%26entry.906535625%3DKavyansh%2520Tyagi%2520and%2520Vishwas%2520Rathi%2520and%2520Puneet%2520Goyal%26entry.1292438233%3DAccurate%2520and%2520computationally%2520efficient%25203D%2520medical%2520image%2520segmentation%2520remains%2520a%2520critical%2520challenge%2520in%2520clinical%2520workflows.%2520Transformer-based%2520architectures%2520often%2520demonstrate%2520superior%2520global%2520contextual%2520modeling%2520but%2520at%2520the%2520expense%2520of%2520excessive%2520parameter%2520counts%2520and%2520memory%2520demands%252C%2520restricting%2520their%2520clinical%2520deployment.%2520We%2520propose%2520RefineFormer3D%252C%2520a%2520lightweight%2520hierarchical%2520transformer%2520architecture%2520that%2520balances%2520segmentation%2520accuracy%2520and%2520computational%2520efficiency%2520for%2520volumetric%2520medical%2520imaging.%2520The%2520architecture%2520integrates%2520three%2520key%2520components%253A%2520%2528i%2529%2520GhostConv3D-based%2520patch%2520embedding%2520for%2520efficient%2520feature%2520extraction%2520with%2520minimal%2520redundancy%252C%2520%2528ii%2529%2520MixFFN3D%2520module%2520with%2520low-rank%2520projections%2520and%2520depthwise%2520convolutions%2520for%2520parameter-efficient%2520feature%2520extraction%252C%2520and%2520%2528iii%2529%2520a%2520cross-attention%2520fusion%2520decoder%2520enabling%2520adaptive%2520multi-scale%2520skip%2520connection%2520integration.%2520RefineFormer3D%2520contains%2520only%25202.94M%2520parameters%252C%2520substantially%2520fewer%2520than%2520contemporary%2520transformer-based%2520methods.%2520Extensive%2520experiments%2520on%2520ACDC%2520and%2520BraTS%2520benchmarks%2520demonstrate%2520that%2520RefineFormer3D%2520achieves%252093.44%255C%2525%2520and%252085.9%255C%2525%2520average%2520Dice%2520scores%2520respectively%252C%2520outperforming%2520or%2520matching%2520state-of-the-art%2520methods%2520while%2520requiring%2520significantly%2520fewer%2520parameters.%2520Furthermore%252C%2520the%2520model%2520achieves%2520fast%2520inference%2520%25288.35%2520ms%2520per%2520volume%2520on%2520GPU%2529%2520with%2520low%2520memory%2520requirements%252C%2520supporting%2520deployment%2520in%2520resource-constrained%2520clinical%2520environments.%2520These%2520results%2520establish%2520RefineFormer3D%2520as%2520an%2520effective%2520and%2520scalable%2520solution%2520for%2520practical%25203D%2520medical%2520image%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefineFormer3D%3A%20Efficient%203D%20Medical%20Image%20Segmentation%20via%20Adaptive%20Multi-Scale%20Transformer%20with%20Cross%20Attention%20Fusion&entry.906535625=Kavyansh%20Tyagi%20and%20Vishwas%20Rathi%20and%20Puneet%20Goyal&entry.1292438233=Accurate%20and%20computationally%20efficient%203D%20medical%20image%20segmentation%20remains%20a%20critical%20challenge%20in%20clinical%20workflows.%20Transformer-based%20architectures%20often%20demonstrate%20superior%20global%20contextual%20modeling%20but%20at%20the%20expense%20of%20excessive%20parameter%20counts%20and%20memory%20demands%2C%20restricting%20their%20clinical%20deployment.%20We%20propose%20RefineFormer3D%2C%20a%20lightweight%20hierarchical%20transformer%20architecture%20that%20balances%20segmentation%20accuracy%20and%20computational%20efficiency%20for%20volumetric%20medical%20imaging.%20The%20architecture%20integrates%20three%20key%20components%3A%20%28i%29%20GhostConv3D-based%20patch%20embedding%20for%20efficient%20feature%20extraction%20with%20minimal%20redundancy%2C%20%28ii%29%20MixFFN3D%20module%20with%20low-rank%20projections%20and%20depthwise%20convolutions%20for%20parameter-efficient%20feature%20extraction%2C%20and%20%28iii%29%20a%20cross-attention%20fusion%20decoder%20enabling%20adaptive%20multi-scale%20skip%20connection%20integration.%20RefineFormer3D%20contains%20only%202.94M%20parameters%2C%20substantially%20fewer%20than%20contemporary%20transformer-based%20methods.%20Extensive%20experiments%20on%20ACDC%20and%20BraTS%20benchmarks%20demonstrate%20that%20RefineFormer3D%20achieves%2093.44%5C%25%20and%2085.9%5C%25%20average%20Dice%20scores%20respectively%2C%20outperforming%20or%20matching%20state-of-the-art%20methods%20while%20requiring%20significantly%20fewer%20parameters.%20Furthermore%2C%20the%20model%20achieves%20fast%20inference%20%288.35%20ms%20per%20volume%20on%20GPU%29%20with%20low%20memory%20requirements%2C%20supporting%20deployment%20in%20resource-constrained%20clinical%20environments.%20These%20results%20establish%20RefineFormer3D%20as%20an%20effective%20and%20scalable%20solution%20for%20practical%203D%20medical%20image%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2602.16320v1&entry.124074799=Read"},
{"title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs", "author": "Zhan Qu and Shuzhou Yuan and Michael F\u00e4rber", "abstract": "This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across 4 families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-based, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a scoring function for best-of-N selection, we fine-tune 3 lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.", "link": "http://arxiv.org/abs/2508.02515v2", "date": "2026-02-18", "relevancy": 2.3055, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4648}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4601}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoeTone%3A%20A%20Framework%20for%20Constrained%20Generation%20of%20Structured%20Chinese%20Songci%20with%20LLMs&body=Title%3A%20PoeTone%3A%20A%20Framework%20for%20Constrained%20Generation%20of%20Structured%20Chinese%20Songci%20with%20LLMs%0AAuthor%3A%20Zhan%20Qu%20and%20Shuzhou%20Yuan%20and%20Michael%20F%C3%A4rber%0AAbstract%3A%20This%20paper%20presents%20a%20systematic%20investigation%20into%20the%20constrained%20generation%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%20producing%20Songci%2C%20a%20classical%20Chinese%20poetry%20form%20characterized%20by%20strict%20structural%2C%20tonal%2C%20and%20rhyme%20constraints%20defined%20by%20Cipai%20templates.%20We%20first%20develop%20a%20comprehensive%2C%20multi-faceted%20evaluation%20framework%20that%20includes%3A%20%28i%29%20a%20formal%20conformity%20score%2C%20%28ii%29%20automated%20quality%20assessment%20using%20LLMs%2C%20%28iii%29%20human%20evaluation%2C%20and%20%28iv%29%20classification-based%20probing%20tasks.%20Using%20this%20framework%2C%20we%20evaluate%20the%20generative%20performance%20of%2018%20LLMs%2C%20including%203%20proprietary%20models%20and%2015%20open-source%20models%20across%204%20families%2C%20under%20five%20prompting%20strategies%3A%20zero-shot%2C%20one-shot%2C%20completion-based%2C%20instruction-based%2C%20and%20chain-of-thought.%20Finally%2C%20we%20propose%20a%20Generate-Critic%20architecture%20in%20which%20the%20evaluation%20framework%20functions%20as%20an%20automated%20critic.%20Leveraging%20the%20critic%27s%20feedback%20as%20a%20scoring%20function%20for%20best-of-N%20selection%2C%20we%20fine-tune%203%20lightweight%20open-source%20LLMs%20via%20supervised%20fine-tuning%20%28SFT%29%2C%20resulting%20in%20improvements%20of%20up%20to%205.88%25%20in%20formal%20conformity.%20Our%20findings%20offer%20new%20insights%20into%20the%20generative%20strengths%20and%20limitations%20of%20LLMs%20in%20producing%20culturally%20significant%20and%20formally%20constrained%20literary%20texts.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02515v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoeTone%253A%2520A%2520Framework%2520for%2520Constrained%2520Generation%2520of%2520Structured%2520Chinese%2520Songci%2520with%2520LLMs%26entry.906535625%3DZhan%2520Qu%2520and%2520Shuzhou%2520Yuan%2520and%2520Michael%2520F%25C3%25A4rber%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520systematic%2520investigation%2520into%2520the%2520constrained%2520generation%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520producing%2520Songci%252C%2520a%2520classical%2520Chinese%2520poetry%2520form%2520characterized%2520by%2520strict%2520structural%252C%2520tonal%252C%2520and%2520rhyme%2520constraints%2520defined%2520by%2520Cipai%2520templates.%2520We%2520first%2520develop%2520a%2520comprehensive%252C%2520multi-faceted%2520evaluation%2520framework%2520that%2520includes%253A%2520%2528i%2529%2520a%2520formal%2520conformity%2520score%252C%2520%2528ii%2529%2520automated%2520quality%2520assessment%2520using%2520LLMs%252C%2520%2528iii%2529%2520human%2520evaluation%252C%2520and%2520%2528iv%2529%2520classification-based%2520probing%2520tasks.%2520Using%2520this%2520framework%252C%2520we%2520evaluate%2520the%2520generative%2520performance%2520of%252018%2520LLMs%252C%2520including%25203%2520proprietary%2520models%2520and%252015%2520open-source%2520models%2520across%25204%2520families%252C%2520under%2520five%2520prompting%2520strategies%253A%2520zero-shot%252C%2520one-shot%252C%2520completion-based%252C%2520instruction-based%252C%2520and%2520chain-of-thought.%2520Finally%252C%2520we%2520propose%2520a%2520Generate-Critic%2520architecture%2520in%2520which%2520the%2520evaluation%2520framework%2520functions%2520as%2520an%2520automated%2520critic.%2520Leveraging%2520the%2520critic%2527s%2520feedback%2520as%2520a%2520scoring%2520function%2520for%2520best-of-N%2520selection%252C%2520we%2520fine-tune%25203%2520lightweight%2520open-source%2520LLMs%2520via%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520resulting%2520in%2520improvements%2520of%2520up%2520to%25205.88%2525%2520in%2520formal%2520conformity.%2520Our%2520findings%2520offer%2520new%2520insights%2520into%2520the%2520generative%2520strengths%2520and%2520limitations%2520of%2520LLMs%2520in%2520producing%2520culturally%2520significant%2520and%2520formally%2520constrained%2520literary%2520texts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02515v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoeTone%3A%20A%20Framework%20for%20Constrained%20Generation%20of%20Structured%20Chinese%20Songci%20with%20LLMs&entry.906535625=Zhan%20Qu%20and%20Shuzhou%20Yuan%20and%20Michael%20F%C3%A4rber&entry.1292438233=This%20paper%20presents%20a%20systematic%20investigation%20into%20the%20constrained%20generation%20capabilities%20of%20large%20language%20models%20%28LLMs%29%20in%20producing%20Songci%2C%20a%20classical%20Chinese%20poetry%20form%20characterized%20by%20strict%20structural%2C%20tonal%2C%20and%20rhyme%20constraints%20defined%20by%20Cipai%20templates.%20We%20first%20develop%20a%20comprehensive%2C%20multi-faceted%20evaluation%20framework%20that%20includes%3A%20%28i%29%20a%20formal%20conformity%20score%2C%20%28ii%29%20automated%20quality%20assessment%20using%20LLMs%2C%20%28iii%29%20human%20evaluation%2C%20and%20%28iv%29%20classification-based%20probing%20tasks.%20Using%20this%20framework%2C%20we%20evaluate%20the%20generative%20performance%20of%2018%20LLMs%2C%20including%203%20proprietary%20models%20and%2015%20open-source%20models%20across%204%20families%2C%20under%20five%20prompting%20strategies%3A%20zero-shot%2C%20one-shot%2C%20completion-based%2C%20instruction-based%2C%20and%20chain-of-thought.%20Finally%2C%20we%20propose%20a%20Generate-Critic%20architecture%20in%20which%20the%20evaluation%20framework%20functions%20as%20an%20automated%20critic.%20Leveraging%20the%20critic%27s%20feedback%20as%20a%20scoring%20function%20for%20best-of-N%20selection%2C%20we%20fine-tune%203%20lightweight%20open-source%20LLMs%20via%20supervised%20fine-tuning%20%28SFT%29%2C%20resulting%20in%20improvements%20of%20up%20to%205.88%25%20in%20formal%20conformity.%20Our%20findings%20offer%20new%20insights%20into%20the%20generative%20strengths%20and%20limitations%20of%20LLMs%20in%20producing%20culturally%20significant%20and%20formally%20constrained%20literary%20texts.&entry.1838667208=http%3A//arxiv.org/abs/2508.02515v2&entry.124074799=Read"},
{"title": "Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles", "author": "Abhishek Goudar and Angela P. Schoellig", "abstract": "Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments.", "link": "http://arxiv.org/abs/2602.16594v1", "date": "2026-02-18", "relevancy": 2.303, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5785}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5761}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20and%20Fully%20Onboard%3A%20Range-Aided%20Cooperative%20Localization%20and%20Navigation%20on%20Micro%20Aerial%20Vehicles&body=Title%3A%20Decentralized%20and%20Fully%20Onboard%3A%20Range-Aided%20Cooperative%20Localization%20and%20Navigation%20on%20Micro%20Aerial%20Vehicles%0AAuthor%3A%20Abhishek%20Goudar%20and%20Angela%20P.%20Schoellig%0AAbstract%3A%20Controlling%20a%20team%20of%20robots%20in%20a%20coordinated%20manner%20is%20challenging%20because%20centralized%20approaches%20%28where%20all%20computation%20is%20performed%20on%20a%20central%20machine%29%20scale%20poorly%2C%20and%20globally%20referenced%20external%20localization%20systems%20may%20not%20always%20be%20available.%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20range-aided%20decentralized%20localization%20and%20formation%20control.%20In%20such%20a%20setting%2C%20each%20robot%20estimates%20its%20relative%20pose%20by%20combining%20data%20only%20from%20onboard%20odometry%20sensors%20and%20distance%20measurements%20to%20other%20robots%20in%20the%20team.%20Additionally%2C%20each%20robot%20calculates%20the%20control%20inputs%20necessary%20to%20collaboratively%20navigate%20an%20environment%20to%20accomplish%20a%20specific%20task%2C%20for%20example%2C%20moving%20in%20a%20desired%20formation%20while%20monitoring%20an%20area.%20We%20present%20a%20block%20coordinate%20descent%20approach%20to%20localization%20that%20does%20not%20require%20strict%20coordination%20between%20the%20robots.%20We%20present%20a%20novel%20formulation%20for%20formation%20control%20as%20inference%20on%20factor%20graphs%20that%20takes%20into%20account%20the%20state%20estimation%20uncertainty%20and%20can%20be%20solved%20efficiently.%20Our%20approach%20to%20range-aided%20localization%20and%20formation-based%20navigation%20is%20completely%20decentralized%2C%20does%20not%20require%20specialized%20trajectories%20to%20maintain%20formation%2C%20and%20achieves%20decimeter-level%20positioning%20and%20formation%20control%20accuracy.%20We%20demonstrate%20our%20approach%20through%20multiple%20real%20experiments%20involving%20formation%20flights%20in%20diverse%20indoor%20and%20outdoor%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520and%2520Fully%2520Onboard%253A%2520Range-Aided%2520Cooperative%2520Localization%2520and%2520Navigation%2520on%2520Micro%2520Aerial%2520Vehicles%26entry.906535625%3DAbhishek%2520Goudar%2520and%2520Angela%2520P.%2520Schoellig%26entry.1292438233%3DControlling%2520a%2520team%2520of%2520robots%2520in%2520a%2520coordinated%2520manner%2520is%2520challenging%2520because%2520centralized%2520approaches%2520%2528where%2520all%2520computation%2520is%2520performed%2520on%2520a%2520central%2520machine%2529%2520scale%2520poorly%252C%2520and%2520globally%2520referenced%2520external%2520localization%2520systems%2520may%2520not%2520always%2520be%2520available.%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520problem%2520of%2520range-aided%2520decentralized%2520localization%2520and%2520formation%2520control.%2520In%2520such%2520a%2520setting%252C%2520each%2520robot%2520estimates%2520its%2520relative%2520pose%2520by%2520combining%2520data%2520only%2520from%2520onboard%2520odometry%2520sensors%2520and%2520distance%2520measurements%2520to%2520other%2520robots%2520in%2520the%2520team.%2520Additionally%252C%2520each%2520robot%2520calculates%2520the%2520control%2520inputs%2520necessary%2520to%2520collaboratively%2520navigate%2520an%2520environment%2520to%2520accomplish%2520a%2520specific%2520task%252C%2520for%2520example%252C%2520moving%2520in%2520a%2520desired%2520formation%2520while%2520monitoring%2520an%2520area.%2520We%2520present%2520a%2520block%2520coordinate%2520descent%2520approach%2520to%2520localization%2520that%2520does%2520not%2520require%2520strict%2520coordination%2520between%2520the%2520robots.%2520We%2520present%2520a%2520novel%2520formulation%2520for%2520formation%2520control%2520as%2520inference%2520on%2520factor%2520graphs%2520that%2520takes%2520into%2520account%2520the%2520state%2520estimation%2520uncertainty%2520and%2520can%2520be%2520solved%2520efficiently.%2520Our%2520approach%2520to%2520range-aided%2520localization%2520and%2520formation-based%2520navigation%2520is%2520completely%2520decentralized%252C%2520does%2520not%2520require%2520specialized%2520trajectories%2520to%2520maintain%2520formation%252C%2520and%2520achieves%2520decimeter-level%2520positioning%2520and%2520formation%2520control%2520accuracy.%2520We%2520demonstrate%2520our%2520approach%2520through%2520multiple%2520real%2520experiments%2520involving%2520formation%2520flights%2520in%2520diverse%2520indoor%2520and%2520outdoor%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20and%20Fully%20Onboard%3A%20Range-Aided%20Cooperative%20Localization%20and%20Navigation%20on%20Micro%20Aerial%20Vehicles&entry.906535625=Abhishek%20Goudar%20and%20Angela%20P.%20Schoellig&entry.1292438233=Controlling%20a%20team%20of%20robots%20in%20a%20coordinated%20manner%20is%20challenging%20because%20centralized%20approaches%20%28where%20all%20computation%20is%20performed%20on%20a%20central%20machine%29%20scale%20poorly%2C%20and%20globally%20referenced%20external%20localization%20systems%20may%20not%20always%20be%20available.%20In%20this%20work%2C%20we%20consider%20the%20problem%20of%20range-aided%20decentralized%20localization%20and%20formation%20control.%20In%20such%20a%20setting%2C%20each%20robot%20estimates%20its%20relative%20pose%20by%20combining%20data%20only%20from%20onboard%20odometry%20sensors%20and%20distance%20measurements%20to%20other%20robots%20in%20the%20team.%20Additionally%2C%20each%20robot%20calculates%20the%20control%20inputs%20necessary%20to%20collaboratively%20navigate%20an%20environment%20to%20accomplish%20a%20specific%20task%2C%20for%20example%2C%20moving%20in%20a%20desired%20formation%20while%20monitoring%20an%20area.%20We%20present%20a%20block%20coordinate%20descent%20approach%20to%20localization%20that%20does%20not%20require%20strict%20coordination%20between%20the%20robots.%20We%20present%20a%20novel%20formulation%20for%20formation%20control%20as%20inference%20on%20factor%20graphs%20that%20takes%20into%20account%20the%20state%20estimation%20uncertainty%20and%20can%20be%20solved%20efficiently.%20Our%20approach%20to%20range-aided%20localization%20and%20formation-based%20navigation%20is%20completely%20decentralized%2C%20does%20not%20require%20specialized%20trajectories%20to%20maintain%20formation%2C%20and%20achieves%20decimeter-level%20positioning%20and%20formation%20control%20accuracy.%20We%20demonstrate%20our%20approach%20through%20multiple%20real%20experiments%20involving%20formation%20flights%20in%20diverse%20indoor%20and%20outdoor%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.16594v1&entry.124074799=Read"},
{"title": "FedMerge: Federated Personalization via Model Merging", "author": "Shutong Chen and Tianyi Zhou and Guodong Long and Jing Jiang and Chengqi Zhang", "abstract": "One global model in federated learning (FL) might not be sufficient to serve many clients with non-IID tasks and distributions. While there has been advances in FL to train multiple global models for better personalization, they only provide limited choices to clients so local finetuning is still indispensable. In this paper, we propose a novel ``FedMerge'' approach that can create a personalized model per client by simply merging multiple global models with automatically optimized and customized weights. In FedMerge, a few global models can serve many non-IID clients, even without further local finetuning. We formulate this problem as a joint optimization of global models and the merging weights for each client. Unlike existing FL approaches where the server broadcasts one or multiple global models to all clients, the server only needs to send a customized, merged model to each client. Moreover, instead of periodically interrupting the local training and re-initializing it to a global model, the merged model aligns better with each client's task and data distribution, smoothening the local-global gap between consecutive rounds caused by client drift. We evaluate FedMerge on three different non-IID settings applied to different domains with diverse tasks and data types, in which FedMerge consistently outperforms existing FL approaches, including clustering-based and mixture-of-experts (MoE) based methods.", "link": "http://arxiv.org/abs/2504.06768v3", "date": "2026-02-18", "relevancy": 2.2829, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4618}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMerge%3A%20Federated%20Personalization%20via%20Model%20Merging&body=Title%3A%20FedMerge%3A%20Federated%20Personalization%20via%20Model%20Merging%0AAuthor%3A%20Shutong%20Chen%20and%20Tianyi%20Zhou%20and%20Guodong%20Long%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang%0AAbstract%3A%20One%20global%20model%20in%20federated%20learning%20%28FL%29%20might%20not%20be%20sufficient%20to%20serve%20many%20clients%20with%20non-IID%20tasks%20and%20distributions.%20While%20there%20has%20been%20advances%20in%20FL%20to%20train%20multiple%20global%20models%20for%20better%20personalization%2C%20they%20only%20provide%20limited%20choices%20to%20clients%20so%20local%20finetuning%20is%20still%20indispensable.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20%60%60FedMerge%27%27%20approach%20that%20can%20create%20a%20personalized%20model%20per%20client%20by%20simply%20merging%20multiple%20global%20models%20with%20automatically%20optimized%20and%20customized%20weights.%20In%20FedMerge%2C%20a%20few%20global%20models%20can%20serve%20many%20non-IID%20clients%2C%20even%20without%20further%20local%20finetuning.%20We%20formulate%20this%20problem%20as%20a%20joint%20optimization%20of%20global%20models%20and%20the%20merging%20weights%20for%20each%20client.%20Unlike%20existing%20FL%20approaches%20where%20the%20server%20broadcasts%20one%20or%20multiple%20global%20models%20to%20all%20clients%2C%20the%20server%20only%20needs%20to%20send%20a%20customized%2C%20merged%20model%20to%20each%20client.%20Moreover%2C%20instead%20of%20periodically%20interrupting%20the%20local%20training%20and%20re-initializing%20it%20to%20a%20global%20model%2C%20the%20merged%20model%20aligns%20better%20with%20each%20client%27s%20task%20and%20data%20distribution%2C%20smoothening%20the%20local-global%20gap%20between%20consecutive%20rounds%20caused%20by%20client%20drift.%20We%20evaluate%20FedMerge%20on%20three%20different%20non-IID%20settings%20applied%20to%20different%20domains%20with%20diverse%20tasks%20and%20data%20types%2C%20in%20which%20FedMerge%20consistently%20outperforms%20existing%20FL%20approaches%2C%20including%20clustering-based%20and%20mixture-of-experts%20%28MoE%29%20based%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2504.06768v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMerge%253A%2520Federated%2520Personalization%2520via%2520Model%2520Merging%26entry.906535625%3DShutong%2520Chen%2520and%2520Tianyi%2520Zhou%2520and%2520Guodong%2520Long%2520and%2520Jing%2520Jiang%2520and%2520Chengqi%2520Zhang%26entry.1292438233%3DOne%2520global%2520model%2520in%2520federated%2520learning%2520%2528FL%2529%2520might%2520not%2520be%2520sufficient%2520to%2520serve%2520many%2520clients%2520with%2520non-IID%2520tasks%2520and%2520distributions.%2520While%2520there%2520has%2520been%2520advances%2520in%2520FL%2520to%2520train%2520multiple%2520global%2520models%2520for%2520better%2520personalization%252C%2520they%2520only%2520provide%2520limited%2520choices%2520to%2520clients%2520so%2520local%2520finetuning%2520is%2520still%2520indispensable.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520%2560%2560FedMerge%2527%2527%2520approach%2520that%2520can%2520create%2520a%2520personalized%2520model%2520per%2520client%2520by%2520simply%2520merging%2520multiple%2520global%2520models%2520with%2520automatically%2520optimized%2520and%2520customized%2520weights.%2520In%2520FedMerge%252C%2520a%2520few%2520global%2520models%2520can%2520serve%2520many%2520non-IID%2520clients%252C%2520even%2520without%2520further%2520local%2520finetuning.%2520We%2520formulate%2520this%2520problem%2520as%2520a%2520joint%2520optimization%2520of%2520global%2520models%2520and%2520the%2520merging%2520weights%2520for%2520each%2520client.%2520Unlike%2520existing%2520FL%2520approaches%2520where%2520the%2520server%2520broadcasts%2520one%2520or%2520multiple%2520global%2520models%2520to%2520all%2520clients%252C%2520the%2520server%2520only%2520needs%2520to%2520send%2520a%2520customized%252C%2520merged%2520model%2520to%2520each%2520client.%2520Moreover%252C%2520instead%2520of%2520periodically%2520interrupting%2520the%2520local%2520training%2520and%2520re-initializing%2520it%2520to%2520a%2520global%2520model%252C%2520the%2520merged%2520model%2520aligns%2520better%2520with%2520each%2520client%2527s%2520task%2520and%2520data%2520distribution%252C%2520smoothening%2520the%2520local-global%2520gap%2520between%2520consecutive%2520rounds%2520caused%2520by%2520client%2520drift.%2520We%2520evaluate%2520FedMerge%2520on%2520three%2520different%2520non-IID%2520settings%2520applied%2520to%2520different%2520domains%2520with%2520diverse%2520tasks%2520and%2520data%2520types%252C%2520in%2520which%2520FedMerge%2520consistently%2520outperforms%2520existing%2520FL%2520approaches%252C%2520including%2520clustering-based%2520and%2520mixture-of-experts%2520%2528MoE%2529%2520based%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06768v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMerge%3A%20Federated%20Personalization%20via%20Model%20Merging&entry.906535625=Shutong%20Chen%20and%20Tianyi%20Zhou%20and%20Guodong%20Long%20and%20Jing%20Jiang%20and%20Chengqi%20Zhang&entry.1292438233=One%20global%20model%20in%20federated%20learning%20%28FL%29%20might%20not%20be%20sufficient%20to%20serve%20many%20clients%20with%20non-IID%20tasks%20and%20distributions.%20While%20there%20has%20been%20advances%20in%20FL%20to%20train%20multiple%20global%20models%20for%20better%20personalization%2C%20they%20only%20provide%20limited%20choices%20to%20clients%20so%20local%20finetuning%20is%20still%20indispensable.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20%60%60FedMerge%27%27%20approach%20that%20can%20create%20a%20personalized%20model%20per%20client%20by%20simply%20merging%20multiple%20global%20models%20with%20automatically%20optimized%20and%20customized%20weights.%20In%20FedMerge%2C%20a%20few%20global%20models%20can%20serve%20many%20non-IID%20clients%2C%20even%20without%20further%20local%20finetuning.%20We%20formulate%20this%20problem%20as%20a%20joint%20optimization%20of%20global%20models%20and%20the%20merging%20weights%20for%20each%20client.%20Unlike%20existing%20FL%20approaches%20where%20the%20server%20broadcasts%20one%20or%20multiple%20global%20models%20to%20all%20clients%2C%20the%20server%20only%20needs%20to%20send%20a%20customized%2C%20merged%20model%20to%20each%20client.%20Moreover%2C%20instead%20of%20periodically%20interrupting%20the%20local%20training%20and%20re-initializing%20it%20to%20a%20global%20model%2C%20the%20merged%20model%20aligns%20better%20with%20each%20client%27s%20task%20and%20data%20distribution%2C%20smoothening%20the%20local-global%20gap%20between%20consecutive%20rounds%20caused%20by%20client%20drift.%20We%20evaluate%20FedMerge%20on%20three%20different%20non-IID%20settings%20applied%20to%20different%20domains%20with%20diverse%20tasks%20and%20data%20types%2C%20in%20which%20FedMerge%20consistently%20outperforms%20existing%20FL%20approaches%2C%20including%20clustering-based%20and%20mixture-of-experts%20%28MoE%29%20based%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2504.06768v3&entry.124074799=Read"},
{"title": "Reactive Motion Generation With Particle-Based Perception in Dynamic Environments", "author": "Xiyuan Zhao and Huijun Li and Lifeng Zhu and Zhikai Wei and Xianyi Zhu and Aiguo Song", "abstract": "Reactive motion generation in dynamic and unstructured scenarios is typically subject to essentially static perception and system dynamics. Reliably modeling dynamic obstacles and optimizing collision-free trajectories under perceptive and control uncertainty are challenging. This article focuses on revealing tight connection between reactive planning and dynamic mapping for manipulators from a model-based perspective. To enable efficient particle-based perception with expressively dynamic property, we present a tensorized particle weight update scheme that explicitly maintains obstacle velocities and covariance meanwhile. Building upon this dynamic representation, we propose an obstacle-aware MPPI-based planning formulation that jointly propagates robot-obstacle dynamics, allowing future system motion to be predicted and evaluated under uncertainty. The model predictive method is shown to significantly improve safety and reactivity with dynamic surroundings. By applying our complete framework in simulated and noisy real-world environments, we demonstrate that explicit modeling of robot-obstacle dynamics consistently enhances performance over state-of-the-art MPPI-based perception-planning baselines avoiding multiple static and dynamic obstacles.", "link": "http://arxiv.org/abs/2602.16462v1", "date": "2026-02-18", "relevancy": 2.2799, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6205}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5721}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reactive%20Motion%20Generation%20With%20Particle-Based%20Perception%20in%20Dynamic%20Environments&body=Title%3A%20Reactive%20Motion%20Generation%20With%20Particle-Based%20Perception%20in%20Dynamic%20Environments%0AAuthor%3A%20Xiyuan%20Zhao%20and%20Huijun%20Li%20and%20Lifeng%20Zhu%20and%20Zhikai%20Wei%20and%20Xianyi%20Zhu%20and%20Aiguo%20Song%0AAbstract%3A%20Reactive%20motion%20generation%20in%20dynamic%20and%20unstructured%20scenarios%20is%20typically%20subject%20to%20essentially%20static%20perception%20and%20system%20dynamics.%20Reliably%20modeling%20dynamic%20obstacles%20and%20optimizing%20collision-free%20trajectories%20under%20perceptive%20and%20control%20uncertainty%20are%20challenging.%20This%20article%20focuses%20on%20revealing%20tight%20connection%20between%20reactive%20planning%20and%20dynamic%20mapping%20for%20manipulators%20from%20a%20model-based%20perspective.%20To%20enable%20efficient%20particle-based%20perception%20with%20expressively%20dynamic%20property%2C%20we%20present%20a%20tensorized%20particle%20weight%20update%20scheme%20that%20explicitly%20maintains%20obstacle%20velocities%20and%20covariance%20meanwhile.%20Building%20upon%20this%20dynamic%20representation%2C%20we%20propose%20an%20obstacle-aware%20MPPI-based%20planning%20formulation%20that%20jointly%20propagates%20robot-obstacle%20dynamics%2C%20allowing%20future%20system%20motion%20to%20be%20predicted%20and%20evaluated%20under%20uncertainty.%20The%20model%20predictive%20method%20is%20shown%20to%20significantly%20improve%20safety%20and%20reactivity%20with%20dynamic%20surroundings.%20By%20applying%20our%20complete%20framework%20in%20simulated%20and%20noisy%20real-world%20environments%2C%20we%20demonstrate%20that%20explicit%20modeling%20of%20robot-obstacle%20dynamics%20consistently%20enhances%20performance%20over%20state-of-the-art%20MPPI-based%20perception-planning%20baselines%20avoiding%20multiple%20static%20and%20dynamic%20obstacles.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReactive%2520Motion%2520Generation%2520With%2520Particle-Based%2520Perception%2520in%2520Dynamic%2520Environments%26entry.906535625%3DXiyuan%2520Zhao%2520and%2520Huijun%2520Li%2520and%2520Lifeng%2520Zhu%2520and%2520Zhikai%2520Wei%2520and%2520Xianyi%2520Zhu%2520and%2520Aiguo%2520Song%26entry.1292438233%3DReactive%2520motion%2520generation%2520in%2520dynamic%2520and%2520unstructured%2520scenarios%2520is%2520typically%2520subject%2520to%2520essentially%2520static%2520perception%2520and%2520system%2520dynamics.%2520Reliably%2520modeling%2520dynamic%2520obstacles%2520and%2520optimizing%2520collision-free%2520trajectories%2520under%2520perceptive%2520and%2520control%2520uncertainty%2520are%2520challenging.%2520This%2520article%2520focuses%2520on%2520revealing%2520tight%2520connection%2520between%2520reactive%2520planning%2520and%2520dynamic%2520mapping%2520for%2520manipulators%2520from%2520a%2520model-based%2520perspective.%2520To%2520enable%2520efficient%2520particle-based%2520perception%2520with%2520expressively%2520dynamic%2520property%252C%2520we%2520present%2520a%2520tensorized%2520particle%2520weight%2520update%2520scheme%2520that%2520explicitly%2520maintains%2520obstacle%2520velocities%2520and%2520covariance%2520meanwhile.%2520Building%2520upon%2520this%2520dynamic%2520representation%252C%2520we%2520propose%2520an%2520obstacle-aware%2520MPPI-based%2520planning%2520formulation%2520that%2520jointly%2520propagates%2520robot-obstacle%2520dynamics%252C%2520allowing%2520future%2520system%2520motion%2520to%2520be%2520predicted%2520and%2520evaluated%2520under%2520uncertainty.%2520The%2520model%2520predictive%2520method%2520is%2520shown%2520to%2520significantly%2520improve%2520safety%2520and%2520reactivity%2520with%2520dynamic%2520surroundings.%2520By%2520applying%2520our%2520complete%2520framework%2520in%2520simulated%2520and%2520noisy%2520real-world%2520environments%252C%2520we%2520demonstrate%2520that%2520explicit%2520modeling%2520of%2520robot-obstacle%2520dynamics%2520consistently%2520enhances%2520performance%2520over%2520state-of-the-art%2520MPPI-based%2520perception-planning%2520baselines%2520avoiding%2520multiple%2520static%2520and%2520dynamic%2520obstacles.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reactive%20Motion%20Generation%20With%20Particle-Based%20Perception%20in%20Dynamic%20Environments&entry.906535625=Xiyuan%20Zhao%20and%20Huijun%20Li%20and%20Lifeng%20Zhu%20and%20Zhikai%20Wei%20and%20Xianyi%20Zhu%20and%20Aiguo%20Song&entry.1292438233=Reactive%20motion%20generation%20in%20dynamic%20and%20unstructured%20scenarios%20is%20typically%20subject%20to%20essentially%20static%20perception%20and%20system%20dynamics.%20Reliably%20modeling%20dynamic%20obstacles%20and%20optimizing%20collision-free%20trajectories%20under%20perceptive%20and%20control%20uncertainty%20are%20challenging.%20This%20article%20focuses%20on%20revealing%20tight%20connection%20between%20reactive%20planning%20and%20dynamic%20mapping%20for%20manipulators%20from%20a%20model-based%20perspective.%20To%20enable%20efficient%20particle-based%20perception%20with%20expressively%20dynamic%20property%2C%20we%20present%20a%20tensorized%20particle%20weight%20update%20scheme%20that%20explicitly%20maintains%20obstacle%20velocities%20and%20covariance%20meanwhile.%20Building%20upon%20this%20dynamic%20representation%2C%20we%20propose%20an%20obstacle-aware%20MPPI-based%20planning%20formulation%20that%20jointly%20propagates%20robot-obstacle%20dynamics%2C%20allowing%20future%20system%20motion%20to%20be%20predicted%20and%20evaluated%20under%20uncertainty.%20The%20model%20predictive%20method%20is%20shown%20to%20significantly%20improve%20safety%20and%20reactivity%20with%20dynamic%20surroundings.%20By%20applying%20our%20complete%20framework%20in%20simulated%20and%20noisy%20real-world%20environments%2C%20we%20demonstrate%20that%20explicit%20modeling%20of%20robot-obstacle%20dynamics%20consistently%20enhances%20performance%20over%20state-of-the-art%20MPPI-based%20perception-planning%20baselines%20avoiding%20multiple%20static%20and%20dynamic%20obstacles.&entry.1838667208=http%3A//arxiv.org/abs/2602.16462v1&entry.124074799=Read"},
{"title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation", "author": "Emma Hoes and K. Jonathan Klueser and Fabrizio Gilardi", "abstract": "Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA's no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.", "link": "http://arxiv.org/abs/2602.12207v2", "date": "2026-02-18", "relevancy": 2.254, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.458}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4498}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIRENA%3A%20Virtual%20Arena%20for%20Research%2C%20Education%2C%20and%20Democratic%20Innovation&body=Title%3A%20VIRENA%3A%20Virtual%20Arena%20for%20Research%2C%20Education%2C%20and%20Democratic%20Innovation%0AAuthor%3A%20Emma%20Hoes%20and%20K.%20Jonathan%20Klueser%20and%20Fabrizio%20Gilardi%0AAbstract%3A%20Digital%20platforms%20shape%20how%20people%20communicate%2C%20deliberate%2C%20and%20form%20opinions.%20Studying%20these%20dynamics%20has%20become%20increasingly%20difficult%20due%20to%20restricted%20data%20access%2C%20ethical%20constraints%20on%20real-world%20experiments%2C%20and%20limitations%20of%20existing%20research%20tools.%20VIRENA%20%28Virtual%20Arena%29%20is%20a%20platform%20that%20enables%20controlled%20experimentation%20in%20realistic%20social%20media%20environments.%20Multiple%20participants%20interact%20simultaneously%20in%20realistic%20replicas%20of%20feed-based%20platforms%20%28Instagram%2C%20Facebook%2C%20Reddit%29%20and%20messaging%20apps%20%28WhatsApp%2C%20Messenger%29.%20Large%20language%20model-powered%20AI%20agents%20participate%20alongside%20humans%20with%20configurable%20personas%20and%20realistic%20behavior.%20Researchers%20can%20manipulate%20content%20moderation%20approaches%2C%20pre-schedule%20stimulus%20content%2C%20and%20run%20experiments%20across%20conditions%20through%20a%20visual%20interface%20requiring%20no%20programming%20skills.%20VIRENA%20makes%20possible%20research%20designs%20that%20were%20previously%20impractical%3A%20studying%20human--AI%20interaction%20in%20realistic%20social%20contexts%2C%20experimentally%20comparing%20moderation%20interventions%2C%20and%20observing%20group%20deliberation%20as%20it%20unfolds.%20Built%20on%20open-source%20technologies%20that%20ensure%20data%20remain%20under%20institutional%20control%20and%20comply%20with%20data%20protection%20requirements%2C%20VIRENA%20is%20currently%20in%20use%20at%20the%20University%20of%20Zurich%20and%20available%20for%20pilot%20collaborations.%20Designed%20for%20researchers%2C%20educators%2C%20and%20public%20organizations%20alike%2C%20VIRENA%27s%20no-code%20interface%20makes%20controlled%20social%20media%20simulation%20accessible%20across%20disciplines%20and%20sectors.%20This%20paper%20documents%20its%20design%2C%20architecture%2C%20and%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12207v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIRENA%253A%2520Virtual%2520Arena%2520for%2520Research%252C%2520Education%252C%2520and%2520Democratic%2520Innovation%26entry.906535625%3DEmma%2520Hoes%2520and%2520K.%2520Jonathan%2520Klueser%2520and%2520Fabrizio%2520Gilardi%26entry.1292438233%3DDigital%2520platforms%2520shape%2520how%2520people%2520communicate%252C%2520deliberate%252C%2520and%2520form%2520opinions.%2520Studying%2520these%2520dynamics%2520has%2520become%2520increasingly%2520difficult%2520due%2520to%2520restricted%2520data%2520access%252C%2520ethical%2520constraints%2520on%2520real-world%2520experiments%252C%2520and%2520limitations%2520of%2520existing%2520research%2520tools.%2520VIRENA%2520%2528Virtual%2520Arena%2529%2520is%2520a%2520platform%2520that%2520enables%2520controlled%2520experimentation%2520in%2520realistic%2520social%2520media%2520environments.%2520Multiple%2520participants%2520interact%2520simultaneously%2520in%2520realistic%2520replicas%2520of%2520feed-based%2520platforms%2520%2528Instagram%252C%2520Facebook%252C%2520Reddit%2529%2520and%2520messaging%2520apps%2520%2528WhatsApp%252C%2520Messenger%2529.%2520Large%2520language%2520model-powered%2520AI%2520agents%2520participate%2520alongside%2520humans%2520with%2520configurable%2520personas%2520and%2520realistic%2520behavior.%2520Researchers%2520can%2520manipulate%2520content%2520moderation%2520approaches%252C%2520pre-schedule%2520stimulus%2520content%252C%2520and%2520run%2520experiments%2520across%2520conditions%2520through%2520a%2520visual%2520interface%2520requiring%2520no%2520programming%2520skills.%2520VIRENA%2520makes%2520possible%2520research%2520designs%2520that%2520were%2520previously%2520impractical%253A%2520studying%2520human--AI%2520interaction%2520in%2520realistic%2520social%2520contexts%252C%2520experimentally%2520comparing%2520moderation%2520interventions%252C%2520and%2520observing%2520group%2520deliberation%2520as%2520it%2520unfolds.%2520Built%2520on%2520open-source%2520technologies%2520that%2520ensure%2520data%2520remain%2520under%2520institutional%2520control%2520and%2520comply%2520with%2520data%2520protection%2520requirements%252C%2520VIRENA%2520is%2520currently%2520in%2520use%2520at%2520the%2520University%2520of%2520Zurich%2520and%2520available%2520for%2520pilot%2520collaborations.%2520Designed%2520for%2520researchers%252C%2520educators%252C%2520and%2520public%2520organizations%2520alike%252C%2520VIRENA%2527s%2520no-code%2520interface%2520makes%2520controlled%2520social%2520media%2520simulation%2520accessible%2520across%2520disciplines%2520and%2520sectors.%2520This%2520paper%2520documents%2520its%2520design%252C%2520architecture%252C%2520and%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12207v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIRENA%3A%20Virtual%20Arena%20for%20Research%2C%20Education%2C%20and%20Democratic%20Innovation&entry.906535625=Emma%20Hoes%20and%20K.%20Jonathan%20Klueser%20and%20Fabrizio%20Gilardi&entry.1292438233=Digital%20platforms%20shape%20how%20people%20communicate%2C%20deliberate%2C%20and%20form%20opinions.%20Studying%20these%20dynamics%20has%20become%20increasingly%20difficult%20due%20to%20restricted%20data%20access%2C%20ethical%20constraints%20on%20real-world%20experiments%2C%20and%20limitations%20of%20existing%20research%20tools.%20VIRENA%20%28Virtual%20Arena%29%20is%20a%20platform%20that%20enables%20controlled%20experimentation%20in%20realistic%20social%20media%20environments.%20Multiple%20participants%20interact%20simultaneously%20in%20realistic%20replicas%20of%20feed-based%20platforms%20%28Instagram%2C%20Facebook%2C%20Reddit%29%20and%20messaging%20apps%20%28WhatsApp%2C%20Messenger%29.%20Large%20language%20model-powered%20AI%20agents%20participate%20alongside%20humans%20with%20configurable%20personas%20and%20realistic%20behavior.%20Researchers%20can%20manipulate%20content%20moderation%20approaches%2C%20pre-schedule%20stimulus%20content%2C%20and%20run%20experiments%20across%20conditions%20through%20a%20visual%20interface%20requiring%20no%20programming%20skills.%20VIRENA%20makes%20possible%20research%20designs%20that%20were%20previously%20impractical%3A%20studying%20human--AI%20interaction%20in%20realistic%20social%20contexts%2C%20experimentally%20comparing%20moderation%20interventions%2C%20and%20observing%20group%20deliberation%20as%20it%20unfolds.%20Built%20on%20open-source%20technologies%20that%20ensure%20data%20remain%20under%20institutional%20control%20and%20comply%20with%20data%20protection%20requirements%2C%20VIRENA%20is%20currently%20in%20use%20at%20the%20University%20of%20Zurich%20and%20available%20for%20pilot%20collaborations.%20Designed%20for%20researchers%2C%20educators%2C%20and%20public%20organizations%20alike%2C%20VIRENA%27s%20no-code%20interface%20makes%20controlled%20social%20media%20simulation%20accessible%20across%20disciplines%20and%20sectors.%20This%20paper%20documents%20its%20design%2C%20architecture%2C%20and%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2602.12207v2&entry.124074799=Read"},
{"title": "Learning to unfold cloth: Scaling up world models to deformable object manipulation", "author": "Jack Rome and Stephen James and Subramanian Ramamoorthy", "abstract": "Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.", "link": "http://arxiv.org/abs/2602.16675v1", "date": "2026-02-18", "relevancy": 2.2353, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5657}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5628}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20unfold%20cloth%3A%20Scaling%20up%20world%20models%20to%20deformable%20object%20manipulation&body=Title%3A%20Learning%20to%20unfold%20cloth%3A%20Scaling%20up%20world%20models%20to%20deformable%20object%20manipulation%0AAuthor%3A%20Jack%20Rome%20and%20Stephen%20James%20and%20Subramanian%20Ramamoorthy%0AAbstract%3A%20Learning%20to%20manipulate%20cloth%20is%20both%20a%20paradigmatic%20problem%20for%20robotic%20research%20and%20a%20problem%20of%20immediate%20relevance%20to%20a%20variety%20of%20applications%20ranging%20from%20assistive%20care%20to%20the%20service%20industry.%20The%20complex%20physics%20of%20the%20deformable%20object%20makes%20this%20problem%20of%20cloth%20manipulation%20nontrivial.%20In%20order%20to%20create%20a%20general%20manipulation%20strategy%20that%20addresses%20a%20variety%20of%20shapes%2C%20sizes%2C%20fold%20and%20wrinkle%20patterns%2C%20in%20addition%20to%20the%20usual%20problems%20of%20appearance%20variations%2C%20it%20becomes%20important%20to%20carefully%20consider%20model%20structure%20and%20their%20implications%20for%20generalisation%20performance.%20In%20this%20paper%2C%20we%20present%20an%20approach%20to%20in-air%20cloth%20manipulation%20that%20uses%20a%20variation%20of%20a%20recently%20proposed%20reinforcement%20learning%20architecture%2C%20DreamerV2.%20Our%20implementation%20modifies%20this%20architecture%20to%20utilise%20surface%20normals%20input%2C%20in%20addition%20to%20modiying%20the%20replay%20buffer%20and%20data%20augmentation%20procedures.%20Taken%20together%20these%20modifications%20represent%20an%20enhancement%20to%20the%20world%20model%20used%20by%20the%20robot%2C%20addressing%20the%20physical%20complexity%20of%20the%20object%20being%20manipulated%20by%20the%20robot.%20We%20present%20evaluations%20both%20in%20simulation%20and%20in%20a%20zero-shot%20deployment%20of%20the%20trained%20policies%20in%20a%20physical%20robot%20setup%2C%20performing%20in-air%20unfolding%20of%20a%20variety%20of%20different%20cloth%20types%2C%20demonstrating%20the%20generalisation%20benefits%20of%20our%20proposed%20architecture.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520unfold%2520cloth%253A%2520Scaling%2520up%2520world%2520models%2520to%2520deformable%2520object%2520manipulation%26entry.906535625%3DJack%2520Rome%2520and%2520Stephen%2520James%2520and%2520Subramanian%2520Ramamoorthy%26entry.1292438233%3DLearning%2520to%2520manipulate%2520cloth%2520is%2520both%2520a%2520paradigmatic%2520problem%2520for%2520robotic%2520research%2520and%2520a%2520problem%2520of%2520immediate%2520relevance%2520to%2520a%2520variety%2520of%2520applications%2520ranging%2520from%2520assistive%2520care%2520to%2520the%2520service%2520industry.%2520The%2520complex%2520physics%2520of%2520the%2520deformable%2520object%2520makes%2520this%2520problem%2520of%2520cloth%2520manipulation%2520nontrivial.%2520In%2520order%2520to%2520create%2520a%2520general%2520manipulation%2520strategy%2520that%2520addresses%2520a%2520variety%2520of%2520shapes%252C%2520sizes%252C%2520fold%2520and%2520wrinkle%2520patterns%252C%2520in%2520addition%2520to%2520the%2520usual%2520problems%2520of%2520appearance%2520variations%252C%2520it%2520becomes%2520important%2520to%2520carefully%2520consider%2520model%2520structure%2520and%2520their%2520implications%2520for%2520generalisation%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520approach%2520to%2520in-air%2520cloth%2520manipulation%2520that%2520uses%2520a%2520variation%2520of%2520a%2520recently%2520proposed%2520reinforcement%2520learning%2520architecture%252C%2520DreamerV2.%2520Our%2520implementation%2520modifies%2520this%2520architecture%2520to%2520utilise%2520surface%2520normals%2520input%252C%2520in%2520addition%2520to%2520modiying%2520the%2520replay%2520buffer%2520and%2520data%2520augmentation%2520procedures.%2520Taken%2520together%2520these%2520modifications%2520represent%2520an%2520enhancement%2520to%2520the%2520world%2520model%2520used%2520by%2520the%2520robot%252C%2520addressing%2520the%2520physical%2520complexity%2520of%2520the%2520object%2520being%2520manipulated%2520by%2520the%2520robot.%2520We%2520present%2520evaluations%2520both%2520in%2520simulation%2520and%2520in%2520a%2520zero-shot%2520deployment%2520of%2520the%2520trained%2520policies%2520in%2520a%2520physical%2520robot%2520setup%252C%2520performing%2520in-air%2520unfolding%2520of%2520a%2520variety%2520of%2520different%2520cloth%2520types%252C%2520demonstrating%2520the%2520generalisation%2520benefits%2520of%2520our%2520proposed%2520architecture.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20unfold%20cloth%3A%20Scaling%20up%20world%20models%20to%20deformable%20object%20manipulation&entry.906535625=Jack%20Rome%20and%20Stephen%20James%20and%20Subramanian%20Ramamoorthy&entry.1292438233=Learning%20to%20manipulate%20cloth%20is%20both%20a%20paradigmatic%20problem%20for%20robotic%20research%20and%20a%20problem%20of%20immediate%20relevance%20to%20a%20variety%20of%20applications%20ranging%20from%20assistive%20care%20to%20the%20service%20industry.%20The%20complex%20physics%20of%20the%20deformable%20object%20makes%20this%20problem%20of%20cloth%20manipulation%20nontrivial.%20In%20order%20to%20create%20a%20general%20manipulation%20strategy%20that%20addresses%20a%20variety%20of%20shapes%2C%20sizes%2C%20fold%20and%20wrinkle%20patterns%2C%20in%20addition%20to%20the%20usual%20problems%20of%20appearance%20variations%2C%20it%20becomes%20important%20to%20carefully%20consider%20model%20structure%20and%20their%20implications%20for%20generalisation%20performance.%20In%20this%20paper%2C%20we%20present%20an%20approach%20to%20in-air%20cloth%20manipulation%20that%20uses%20a%20variation%20of%20a%20recently%20proposed%20reinforcement%20learning%20architecture%2C%20DreamerV2.%20Our%20implementation%20modifies%20this%20architecture%20to%20utilise%20surface%20normals%20input%2C%20in%20addition%20to%20modiying%20the%20replay%20buffer%20and%20data%20augmentation%20procedures.%20Taken%20together%20these%20modifications%20represent%20an%20enhancement%20to%20the%20world%20model%20used%20by%20the%20robot%2C%20addressing%20the%20physical%20complexity%20of%20the%20object%20being%20manipulated%20by%20the%20robot.%20We%20present%20evaluations%20both%20in%20simulation%20and%20in%20a%20zero-shot%20deployment%20of%20the%20trained%20policies%20in%20a%20physical%20robot%20setup%2C%20performing%20in-air%20unfolding%20of%20a%20variety%20of%20different%20cloth%20types%2C%20demonstrating%20the%20generalisation%20benefits%20of%20our%20proposed%20architecture.&entry.1838667208=http%3A//arxiv.org/abs/2602.16675v1&entry.124074799=Read"},
{"title": "HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting", "author": "Jung Min Choi and Vijaya Krishna Yalavarthi and Lars Schmidt-Thieme", "abstract": "In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.", "link": "http://arxiv.org/abs/2602.16468v1", "date": "2026-02-18", "relevancy": 2.2317, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4661}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4391}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HPMixer%3A%20Hierarchical%20Patching%20for%20Multivariate%20Time%20Series%20Forecasting&body=Title%3A%20HPMixer%3A%20Hierarchical%20Patching%20for%20Multivariate%20Time%20Series%20Forecasting%0AAuthor%3A%20Jung%20Min%20Choi%20and%20Vijaya%20Krishna%20Yalavarthi%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20In%20long-term%20multivariate%20time%20series%20forecasting%2C%20effectively%20capturing%20both%20periodic%20patterns%20and%20residual%20dynamics%20is%20essential.%20To%20address%20this%20within%20standard%20deep%20learning%20benchmark%20settings%2C%20we%20propose%20the%20Hierarchical%20Patching%20Mixer%20%28HPMixer%29%2C%20which%20models%20periodicity%20and%20residuals%20in%20a%20decoupled%20yet%20complementary%20manner.%20The%20periodic%20component%20utilizes%20a%20learnable%20cycle%20module%20%5B7%5D%20enhanced%20with%20a%20nonlinear%20channel-wise%20MLP%20for%20greater%20expressiveness.%20The%20residual%20component%20is%20processed%20through%20a%20Learnable%20Stationary%20Wavelet%20Transform%20%28LSWT%29%20to%20extract%20stable%2C%20shift-invariant%20frequency-domain%20representations.%20Subsequently%2C%20a%20channel-mixing%20encoder%20models%20explicit%20inter-channel%20dependencies%2C%20while%20a%20two-level%20non-overlapping%20hierarchical%20patching%20mechanism%20captures%20coarse-%20and%20fine-scale%20residual%20variations.%20By%20integrating%20decoupled%20periodicity%20modeling%20with%20structured%2C%20multi-scale%20residual%20learning%2C%20HPMixer%20provides%20an%20effective%20framework.%20Extensive%20experiments%20on%20standard%20multivariate%20benchmarks%20demonstrate%20that%20HPMixer%20achieves%20competitive%20or%20state-of-the-art%20performance%20compared%20to%20recent%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHPMixer%253A%2520Hierarchical%2520Patching%2520for%2520Multivariate%2520Time%2520Series%2520Forecasting%26entry.906535625%3DJung%2520Min%2520Choi%2520and%2520Vijaya%2520Krishna%2520Yalavarthi%2520and%2520Lars%2520Schmidt-Thieme%26entry.1292438233%3DIn%2520long-term%2520multivariate%2520time%2520series%2520forecasting%252C%2520effectively%2520capturing%2520both%2520periodic%2520patterns%2520and%2520residual%2520dynamics%2520is%2520essential.%2520To%2520address%2520this%2520within%2520standard%2520deep%2520learning%2520benchmark%2520settings%252C%2520we%2520propose%2520the%2520Hierarchical%2520Patching%2520Mixer%2520%2528HPMixer%2529%252C%2520which%2520models%2520periodicity%2520and%2520residuals%2520in%2520a%2520decoupled%2520yet%2520complementary%2520manner.%2520The%2520periodic%2520component%2520utilizes%2520a%2520learnable%2520cycle%2520module%2520%255B7%255D%2520enhanced%2520with%2520a%2520nonlinear%2520channel-wise%2520MLP%2520for%2520greater%2520expressiveness.%2520The%2520residual%2520component%2520is%2520processed%2520through%2520a%2520Learnable%2520Stationary%2520Wavelet%2520Transform%2520%2528LSWT%2529%2520to%2520extract%2520stable%252C%2520shift-invariant%2520frequency-domain%2520representations.%2520Subsequently%252C%2520a%2520channel-mixing%2520encoder%2520models%2520explicit%2520inter-channel%2520dependencies%252C%2520while%2520a%2520two-level%2520non-overlapping%2520hierarchical%2520patching%2520mechanism%2520captures%2520coarse-%2520and%2520fine-scale%2520residual%2520variations.%2520By%2520integrating%2520decoupled%2520periodicity%2520modeling%2520with%2520structured%252C%2520multi-scale%2520residual%2520learning%252C%2520HPMixer%2520provides%2520an%2520effective%2520framework.%2520Extensive%2520experiments%2520on%2520standard%2520multivariate%2520benchmarks%2520demonstrate%2520that%2520HPMixer%2520achieves%2520competitive%2520or%2520state-of-the-art%2520performance%2520compared%2520to%2520recent%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HPMixer%3A%20Hierarchical%20Patching%20for%20Multivariate%20Time%20Series%20Forecasting&entry.906535625=Jung%20Min%20Choi%20and%20Vijaya%20Krishna%20Yalavarthi%20and%20Lars%20Schmidt-Thieme&entry.1292438233=In%20long-term%20multivariate%20time%20series%20forecasting%2C%20effectively%20capturing%20both%20periodic%20patterns%20and%20residual%20dynamics%20is%20essential.%20To%20address%20this%20within%20standard%20deep%20learning%20benchmark%20settings%2C%20we%20propose%20the%20Hierarchical%20Patching%20Mixer%20%28HPMixer%29%2C%20which%20models%20periodicity%20and%20residuals%20in%20a%20decoupled%20yet%20complementary%20manner.%20The%20periodic%20component%20utilizes%20a%20learnable%20cycle%20module%20%5B7%5D%20enhanced%20with%20a%20nonlinear%20channel-wise%20MLP%20for%20greater%20expressiveness.%20The%20residual%20component%20is%20processed%20through%20a%20Learnable%20Stationary%20Wavelet%20Transform%20%28LSWT%29%20to%20extract%20stable%2C%20shift-invariant%20frequency-domain%20representations.%20Subsequently%2C%20a%20channel-mixing%20encoder%20models%20explicit%20inter-channel%20dependencies%2C%20while%20a%20two-level%20non-overlapping%20hierarchical%20patching%20mechanism%20captures%20coarse-%20and%20fine-scale%20residual%20variations.%20By%20integrating%20decoupled%20periodicity%20modeling%20with%20structured%2C%20multi-scale%20residual%20learning%2C%20HPMixer%20provides%20an%20effective%20framework.%20Extensive%20experiments%20on%20standard%20multivariate%20benchmarks%20demonstrate%20that%20HPMixer%20achieves%20competitive%20or%20state-of-the-art%20performance%20compared%20to%20recent%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2602.16468v1&entry.124074799=Read"},
{"title": "Weighted Birkhoff Averages Accelerate Data-Driven Methods", "author": "Maria Bou-Sakr-El-Tayar and Jason J. Bramburger and Matthew J. Colbrook", "abstract": "Many data-driven algorithms in dynamical systems rely on ergodic averages that converge painfully slowly. One simple idea changes this: taper the ends. Weighted Birkhoff averages can converge much faster (sometimes superpolynomially, even exponentially) and can be incorporated seamlessly into existing methods. We demonstrate this with five weighted algorithms: weighted Dynamic Mode Decomposition (wtDMD), weighted Extended DMD (wtEDMD), weighted Sparse Identification of Nonlinear Dynamics (wtSINDy), weighted spectral measure estimation, and weighted diffusion forecasting. Across examples ranging from fluid flows to El Ni\u00f1o data, the message is clear: weighting costs nothing, is easy to implement, and often delivers markedly better results from the same data.", "link": "http://arxiv.org/abs/2511.17772v2", "date": "2026-02-18", "relevancy": 2.2259, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4654}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.436}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weighted%20Birkhoff%20Averages%20Accelerate%20Data-Driven%20Methods&body=Title%3A%20Weighted%20Birkhoff%20Averages%20Accelerate%20Data-Driven%20Methods%0AAuthor%3A%20Maria%20Bou-Sakr-El-Tayar%20and%20Jason%20J.%20Bramburger%20and%20Matthew%20J.%20Colbrook%0AAbstract%3A%20Many%20data-driven%20algorithms%20in%20dynamical%20systems%20rely%20on%20ergodic%20averages%20that%20converge%20painfully%20slowly.%20One%20simple%20idea%20changes%20this%3A%20taper%20the%20ends.%20Weighted%20Birkhoff%20averages%20can%20converge%20much%20faster%20%28sometimes%20superpolynomially%2C%20even%20exponentially%29%20and%20can%20be%20incorporated%20seamlessly%20into%20existing%20methods.%20We%20demonstrate%20this%20with%20five%20weighted%20algorithms%3A%20weighted%20Dynamic%20Mode%20Decomposition%20%28wtDMD%29%2C%20weighted%20Extended%20DMD%20%28wtEDMD%29%2C%20weighted%20Sparse%20Identification%20of%20Nonlinear%20Dynamics%20%28wtSINDy%29%2C%20weighted%20spectral%20measure%20estimation%2C%20and%20weighted%20diffusion%20forecasting.%20Across%20examples%20ranging%20from%20fluid%20flows%20to%20El%20Ni%C3%B1o%20data%2C%20the%20message%20is%20clear%3A%20weighting%20costs%20nothing%2C%20is%20easy%20to%20implement%2C%20and%20often%20delivers%20markedly%20better%20results%20from%20the%20same%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeighted%2520Birkhoff%2520Averages%2520Accelerate%2520Data-Driven%2520Methods%26entry.906535625%3DMaria%2520Bou-Sakr-El-Tayar%2520and%2520Jason%2520J.%2520Bramburger%2520and%2520Matthew%2520J.%2520Colbrook%26entry.1292438233%3DMany%2520data-driven%2520algorithms%2520in%2520dynamical%2520systems%2520rely%2520on%2520ergodic%2520averages%2520that%2520converge%2520painfully%2520slowly.%2520One%2520simple%2520idea%2520changes%2520this%253A%2520taper%2520the%2520ends.%2520Weighted%2520Birkhoff%2520averages%2520can%2520converge%2520much%2520faster%2520%2528sometimes%2520superpolynomially%252C%2520even%2520exponentially%2529%2520and%2520can%2520be%2520incorporated%2520seamlessly%2520into%2520existing%2520methods.%2520We%2520demonstrate%2520this%2520with%2520five%2520weighted%2520algorithms%253A%2520weighted%2520Dynamic%2520Mode%2520Decomposition%2520%2528wtDMD%2529%252C%2520weighted%2520Extended%2520DMD%2520%2528wtEDMD%2529%252C%2520weighted%2520Sparse%2520Identification%2520of%2520Nonlinear%2520Dynamics%2520%2528wtSINDy%2529%252C%2520weighted%2520spectral%2520measure%2520estimation%252C%2520and%2520weighted%2520diffusion%2520forecasting.%2520Across%2520examples%2520ranging%2520from%2520fluid%2520flows%2520to%2520El%2520Ni%25C3%25B1o%2520data%252C%2520the%2520message%2520is%2520clear%253A%2520weighting%2520costs%2520nothing%252C%2520is%2520easy%2520to%2520implement%252C%2520and%2520often%2520delivers%2520markedly%2520better%2520results%2520from%2520the%2520same%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20Birkhoff%20Averages%20Accelerate%20Data-Driven%20Methods&entry.906535625=Maria%20Bou-Sakr-El-Tayar%20and%20Jason%20J.%20Bramburger%20and%20Matthew%20J.%20Colbrook&entry.1292438233=Many%20data-driven%20algorithms%20in%20dynamical%20systems%20rely%20on%20ergodic%20averages%20that%20converge%20painfully%20slowly.%20One%20simple%20idea%20changes%20this%3A%20taper%20the%20ends.%20Weighted%20Birkhoff%20averages%20can%20converge%20much%20faster%20%28sometimes%20superpolynomially%2C%20even%20exponentially%29%20and%20can%20be%20incorporated%20seamlessly%20into%20existing%20methods.%20We%20demonstrate%20this%20with%20five%20weighted%20algorithms%3A%20weighted%20Dynamic%20Mode%20Decomposition%20%28wtDMD%29%2C%20weighted%20Extended%20DMD%20%28wtEDMD%29%2C%20weighted%20Sparse%20Identification%20of%20Nonlinear%20Dynamics%20%28wtSINDy%29%2C%20weighted%20spectral%20measure%20estimation%2C%20and%20weighted%20diffusion%20forecasting.%20Across%20examples%20ranging%20from%20fluid%20flows%20to%20El%20Ni%C3%B1o%20data%2C%20the%20message%20is%20clear%3A%20weighting%20costs%20nothing%2C%20is%20easy%20to%20implement%2C%20and%20often%20delivers%20markedly%20better%20results%20from%20the%20same%20data.&entry.1838667208=http%3A//arxiv.org/abs/2511.17772v2&entry.124074799=Read"},
{"title": "Watch Out for the Lifespan: Evaluating Backdoor Attacks Against Federated Model Adaptation", "author": "Bastien Vuillod and Pierre-Alain Moellic and Jean-Max Dutertre", "abstract": "Large models adaptation through Federated Learning (FL) addresses a wide range of use cases and is enabled by Parameter-Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA). However, this distributed learning paradigm faces several security threats, particularly to its integrity, such as backdoor attacks that aim to inject malicious behavior during the local training steps of certain clients. We present the first analysis of the influence of LoRA on state-of-the-art backdoor attacks targeting model adaptation in FL. Specifically, we focus on backdoor lifespan, a critical characteristic in FL, that can vary depending on the attack scenario and the attacker's ability to effectively inject the backdoor. A key finding in our experiments is that for an optimally injected backdoor, the backdoor persistence after the attack is longer when the LoRA's rank is lower. Importantly, our work highlights evaluation issues of backdoor attacks against FL and contributes to the development of more robust and fair evaluations of backdoor attacks, enhancing the reliability of risk assessments for critical FL systems. Our code is publicly available.", "link": "http://arxiv.org/abs/2511.14406v2", "date": "2026-02-18", "relevancy": 2.224, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4561}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4362}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Watch%20Out%20for%20the%20Lifespan%3A%20Evaluating%20Backdoor%20Attacks%20Against%20Federated%20Model%20Adaptation&body=Title%3A%20Watch%20Out%20for%20the%20Lifespan%3A%20Evaluating%20Backdoor%20Attacks%20Against%20Federated%20Model%20Adaptation%0AAuthor%3A%20Bastien%20Vuillod%20and%20Pierre-Alain%20Moellic%20and%20Jean-Max%20Dutertre%0AAbstract%3A%20Large%20models%20adaptation%20through%20Federated%20Learning%20%28FL%29%20addresses%20a%20wide%20range%20of%20use%20cases%20and%20is%20enabled%20by%20Parameter-Efficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29.%20However%2C%20this%20distributed%20learning%20paradigm%20faces%20several%20security%20threats%2C%20particularly%20to%20its%20integrity%2C%20such%20as%20backdoor%20attacks%20that%20aim%20to%20inject%20malicious%20behavior%20during%20the%20local%20training%20steps%20of%20certain%20clients.%20We%20present%20the%20first%20analysis%20of%20the%20influence%20of%20LoRA%20on%20state-of-the-art%20backdoor%20attacks%20targeting%20model%20adaptation%20in%20FL.%20Specifically%2C%20we%20focus%20on%20backdoor%20lifespan%2C%20a%20critical%20characteristic%20in%20FL%2C%20that%20can%20vary%20depending%20on%20the%20attack%20scenario%20and%20the%20attacker%27s%20ability%20to%20effectively%20inject%20the%20backdoor.%20A%20key%20finding%20in%20our%20experiments%20is%20that%20for%20an%20optimally%20injected%20backdoor%2C%20the%20backdoor%20persistence%20after%20the%20attack%20is%20longer%20when%20the%20LoRA%27s%20rank%20is%20lower.%20Importantly%2C%20our%20work%20highlights%20evaluation%20issues%20of%20backdoor%20attacks%20against%20FL%20and%20contributes%20to%20the%20development%20of%20more%20robust%20and%20fair%20evaluations%20of%20backdoor%20attacks%2C%20enhancing%20the%20reliability%20of%20risk%20assessments%20for%20critical%20FL%20systems.%20Our%20code%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14406v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWatch%2520Out%2520for%2520the%2520Lifespan%253A%2520Evaluating%2520Backdoor%2520Attacks%2520Against%2520Federated%2520Model%2520Adaptation%26entry.906535625%3DBastien%2520Vuillod%2520and%2520Pierre-Alain%2520Moellic%2520and%2520Jean-Max%2520Dutertre%26entry.1292438233%3DLarge%2520models%2520adaptation%2520through%2520Federated%2520Learning%2520%2528FL%2529%2520addresses%2520a%2520wide%2520range%2520of%2520use%2520cases%2520and%2520is%2520enabled%2520by%2520Parameter-Efficient%2520Fine-Tuning%2520techniques%2520such%2520as%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520However%252C%2520this%2520distributed%2520learning%2520paradigm%2520faces%2520several%2520security%2520threats%252C%2520particularly%2520to%2520its%2520integrity%252C%2520such%2520as%2520backdoor%2520attacks%2520that%2520aim%2520to%2520inject%2520malicious%2520behavior%2520during%2520the%2520local%2520training%2520steps%2520of%2520certain%2520clients.%2520We%2520present%2520the%2520first%2520analysis%2520of%2520the%2520influence%2520of%2520LoRA%2520on%2520state-of-the-art%2520backdoor%2520attacks%2520targeting%2520model%2520adaptation%2520in%2520FL.%2520Specifically%252C%2520we%2520focus%2520on%2520backdoor%2520lifespan%252C%2520a%2520critical%2520characteristic%2520in%2520FL%252C%2520that%2520can%2520vary%2520depending%2520on%2520the%2520attack%2520scenario%2520and%2520the%2520attacker%2527s%2520ability%2520to%2520effectively%2520inject%2520the%2520backdoor.%2520A%2520key%2520finding%2520in%2520our%2520experiments%2520is%2520that%2520for%2520an%2520optimally%2520injected%2520backdoor%252C%2520the%2520backdoor%2520persistence%2520after%2520the%2520attack%2520is%2520longer%2520when%2520the%2520LoRA%2527s%2520rank%2520is%2520lower.%2520Importantly%252C%2520our%2520work%2520highlights%2520evaluation%2520issues%2520of%2520backdoor%2520attacks%2520against%2520FL%2520and%2520contributes%2520to%2520the%2520development%2520of%2520more%2520robust%2520and%2520fair%2520evaluations%2520of%2520backdoor%2520attacks%252C%2520enhancing%2520the%2520reliability%2520of%2520risk%2520assessments%2520for%2520critical%2520FL%2520systems.%2520Our%2520code%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14406v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Watch%20Out%20for%20the%20Lifespan%3A%20Evaluating%20Backdoor%20Attacks%20Against%20Federated%20Model%20Adaptation&entry.906535625=Bastien%20Vuillod%20and%20Pierre-Alain%20Moellic%20and%20Jean-Max%20Dutertre&entry.1292438233=Large%20models%20adaptation%20through%20Federated%20Learning%20%28FL%29%20addresses%20a%20wide%20range%20of%20use%20cases%20and%20is%20enabled%20by%20Parameter-Efficient%20Fine-Tuning%20techniques%20such%20as%20Low-Rank%20Adaptation%20%28LoRA%29.%20However%2C%20this%20distributed%20learning%20paradigm%20faces%20several%20security%20threats%2C%20particularly%20to%20its%20integrity%2C%20such%20as%20backdoor%20attacks%20that%20aim%20to%20inject%20malicious%20behavior%20during%20the%20local%20training%20steps%20of%20certain%20clients.%20We%20present%20the%20first%20analysis%20of%20the%20influence%20of%20LoRA%20on%20state-of-the-art%20backdoor%20attacks%20targeting%20model%20adaptation%20in%20FL.%20Specifically%2C%20we%20focus%20on%20backdoor%20lifespan%2C%20a%20critical%20characteristic%20in%20FL%2C%20that%20can%20vary%20depending%20on%20the%20attack%20scenario%20and%20the%20attacker%27s%20ability%20to%20effectively%20inject%20the%20backdoor.%20A%20key%20finding%20in%20our%20experiments%20is%20that%20for%20an%20optimally%20injected%20backdoor%2C%20the%20backdoor%20persistence%20after%20the%20attack%20is%20longer%20when%20the%20LoRA%27s%20rank%20is%20lower.%20Importantly%2C%20our%20work%20highlights%20evaluation%20issues%20of%20backdoor%20attacks%20against%20FL%20and%20contributes%20to%20the%20development%20of%20more%20robust%20and%20fair%20evaluations%20of%20backdoor%20attacks%2C%20enhancing%20the%20reliability%20of%20risk%20assessments%20for%20critical%20FL%20systems.%20Our%20code%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.14406v2&entry.124074799=Read"},
{"title": "Rotterdam artery-vein segmentation (RAV) dataset", "author": "Jose Vargas Quiros and Bart Liefers and Karin van Garderen and Jeroen Vermeulen and Eyened Reading Center and Caroline Klaver", "abstract": "Purpose: To provide a diverse, high-quality dataset of color fundus images (CFIs) with detailed artery-vein (A/V) segmentation annotations, supporting the development and evaluation of machine learning algorithms for vascular analysis in ophthalmology.\n  Methods: CFIs were sampled from the longitudinal Rotterdam Study (RS), encompassing a wide range of ages, devices, and capture conditions. Images were annotated using a custom interface that allowed graders to label arteries, veins, and unknown vessels on separate layers, starting from an initial vessel segmentation mask. Connectivity was explicitly verified and corrected using connected component visualization tools.\n  Results: The dataset includes 1024x1024-pixel PNG images in three modalities: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. Image quality varied widely, including challenging samples typically excluded by automated quality assessment systems, but judged to contain valuable vascular information.\n  Conclusion: This dataset offers a rich and heterogeneous source of CFIs with high-quality segmentations. It supports robust benchmarking and training of machine learning models under real-world variability in image quality and acquisition settings.\n  Translational Relevance: By including connectivity-validated A/V masks and diverse image conditions, this dataset enables the development of clinically applicable, generalizable machine learning tools for retinal vascular analysis, potentially improving automated screening and diagnosis of systemic and ocular diseases.", "link": "http://arxiv.org/abs/2512.17322v2", "date": "2026-02-18", "relevancy": 2.2226, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.449}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4423}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rotterdam%20artery-vein%20segmentation%20%28RAV%29%20dataset&body=Title%3A%20Rotterdam%20artery-vein%20segmentation%20%28RAV%29%20dataset%0AAuthor%3A%20Jose%20Vargas%20Quiros%20and%20Bart%20Liefers%20and%20Karin%20van%20Garderen%20and%20Jeroen%20Vermeulen%20and%20Eyened%20Reading%20Center%20and%20Caroline%20Klaver%0AAbstract%3A%20Purpose%3A%20To%20provide%20a%20diverse%2C%20high-quality%20dataset%20of%20color%20fundus%20images%20%28CFIs%29%20with%20detailed%20artery-vein%20%28A/V%29%20segmentation%20annotations%2C%20supporting%20the%20development%20and%20evaluation%20of%20machine%20learning%20algorithms%20for%20vascular%20analysis%20in%20ophthalmology.%0A%20%20Methods%3A%20CFIs%20were%20sampled%20from%20the%20longitudinal%20Rotterdam%20Study%20%28RS%29%2C%20encompassing%20a%20wide%20range%20of%20ages%2C%20devices%2C%20and%20capture%20conditions.%20Images%20were%20annotated%20using%20a%20custom%20interface%20that%20allowed%20graders%20to%20label%20arteries%2C%20veins%2C%20and%20unknown%20vessels%20on%20separate%20layers%2C%20starting%20from%20an%20initial%20vessel%20segmentation%20mask.%20Connectivity%20was%20explicitly%20verified%20and%20corrected%20using%20connected%20component%20visualization%20tools.%0A%20%20Results%3A%20The%20dataset%20includes%201024x1024-pixel%20PNG%20images%20in%20three%20modalities%3A%20original%20RGB%20fundus%20images%2C%20contrast-enhanced%20versions%2C%20and%20RGB-encoded%20A/V%20masks.%20Image%20quality%20varied%20widely%2C%20including%20challenging%20samples%20typically%20excluded%20by%20automated%20quality%20assessment%20systems%2C%20but%20judged%20to%20contain%20valuable%20vascular%20information.%0A%20%20Conclusion%3A%20This%20dataset%20offers%20a%20rich%20and%20heterogeneous%20source%20of%20CFIs%20with%20high-quality%20segmentations.%20It%20supports%20robust%20benchmarking%20and%20training%20of%20machine%20learning%20models%20under%20real-world%20variability%20in%20image%20quality%20and%20acquisition%20settings.%0A%20%20Translational%20Relevance%3A%20By%20including%20connectivity-validated%20A/V%20masks%20and%20diverse%20image%20conditions%2C%20this%20dataset%20enables%20the%20development%20of%20clinically%20applicable%2C%20generalizable%20machine%20learning%20tools%20for%20retinal%20vascular%20analysis%2C%20potentially%20improving%20automated%20screening%20and%20diagnosis%20of%20systemic%20and%20ocular%20diseases.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17322v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRotterdam%2520artery-vein%2520segmentation%2520%2528RAV%2529%2520dataset%26entry.906535625%3DJose%2520Vargas%2520Quiros%2520and%2520Bart%2520Liefers%2520and%2520Karin%2520van%2520Garderen%2520and%2520Jeroen%2520Vermeulen%2520and%2520Eyened%2520Reading%2520Center%2520and%2520Caroline%2520Klaver%26entry.1292438233%3DPurpose%253A%2520To%2520provide%2520a%2520diverse%252C%2520high-quality%2520dataset%2520of%2520color%2520fundus%2520images%2520%2528CFIs%2529%2520with%2520detailed%2520artery-vein%2520%2528A/V%2529%2520segmentation%2520annotations%252C%2520supporting%2520the%2520development%2520and%2520evaluation%2520of%2520machine%2520learning%2520algorithms%2520for%2520vascular%2520analysis%2520in%2520ophthalmology.%250A%2520%2520Methods%253A%2520CFIs%2520were%2520sampled%2520from%2520the%2520longitudinal%2520Rotterdam%2520Study%2520%2528RS%2529%252C%2520encompassing%2520a%2520wide%2520range%2520of%2520ages%252C%2520devices%252C%2520and%2520capture%2520conditions.%2520Images%2520were%2520annotated%2520using%2520a%2520custom%2520interface%2520that%2520allowed%2520graders%2520to%2520label%2520arteries%252C%2520veins%252C%2520and%2520unknown%2520vessels%2520on%2520separate%2520layers%252C%2520starting%2520from%2520an%2520initial%2520vessel%2520segmentation%2520mask.%2520Connectivity%2520was%2520explicitly%2520verified%2520and%2520corrected%2520using%2520connected%2520component%2520visualization%2520tools.%250A%2520%2520Results%253A%2520The%2520dataset%2520includes%25201024x1024-pixel%2520PNG%2520images%2520in%2520three%2520modalities%253A%2520original%2520RGB%2520fundus%2520images%252C%2520contrast-enhanced%2520versions%252C%2520and%2520RGB-encoded%2520A/V%2520masks.%2520Image%2520quality%2520varied%2520widely%252C%2520including%2520challenging%2520samples%2520typically%2520excluded%2520by%2520automated%2520quality%2520assessment%2520systems%252C%2520but%2520judged%2520to%2520contain%2520valuable%2520vascular%2520information.%250A%2520%2520Conclusion%253A%2520This%2520dataset%2520offers%2520a%2520rich%2520and%2520heterogeneous%2520source%2520of%2520CFIs%2520with%2520high-quality%2520segmentations.%2520It%2520supports%2520robust%2520benchmarking%2520and%2520training%2520of%2520machine%2520learning%2520models%2520under%2520real-world%2520variability%2520in%2520image%2520quality%2520and%2520acquisition%2520settings.%250A%2520%2520Translational%2520Relevance%253A%2520By%2520including%2520connectivity-validated%2520A/V%2520masks%2520and%2520diverse%2520image%2520conditions%252C%2520this%2520dataset%2520enables%2520the%2520development%2520of%2520clinically%2520applicable%252C%2520generalizable%2520machine%2520learning%2520tools%2520for%2520retinal%2520vascular%2520analysis%252C%2520potentially%2520improving%2520automated%2520screening%2520and%2520diagnosis%2520of%2520systemic%2520and%2520ocular%2520diseases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17322v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rotterdam%20artery-vein%20segmentation%20%28RAV%29%20dataset&entry.906535625=Jose%20Vargas%20Quiros%20and%20Bart%20Liefers%20and%20Karin%20van%20Garderen%20and%20Jeroen%20Vermeulen%20and%20Eyened%20Reading%20Center%20and%20Caroline%20Klaver&entry.1292438233=Purpose%3A%20To%20provide%20a%20diverse%2C%20high-quality%20dataset%20of%20color%20fundus%20images%20%28CFIs%29%20with%20detailed%20artery-vein%20%28A/V%29%20segmentation%20annotations%2C%20supporting%20the%20development%20and%20evaluation%20of%20machine%20learning%20algorithms%20for%20vascular%20analysis%20in%20ophthalmology.%0A%20%20Methods%3A%20CFIs%20were%20sampled%20from%20the%20longitudinal%20Rotterdam%20Study%20%28RS%29%2C%20encompassing%20a%20wide%20range%20of%20ages%2C%20devices%2C%20and%20capture%20conditions.%20Images%20were%20annotated%20using%20a%20custom%20interface%20that%20allowed%20graders%20to%20label%20arteries%2C%20veins%2C%20and%20unknown%20vessels%20on%20separate%20layers%2C%20starting%20from%20an%20initial%20vessel%20segmentation%20mask.%20Connectivity%20was%20explicitly%20verified%20and%20corrected%20using%20connected%20component%20visualization%20tools.%0A%20%20Results%3A%20The%20dataset%20includes%201024x1024-pixel%20PNG%20images%20in%20three%20modalities%3A%20original%20RGB%20fundus%20images%2C%20contrast-enhanced%20versions%2C%20and%20RGB-encoded%20A/V%20masks.%20Image%20quality%20varied%20widely%2C%20including%20challenging%20samples%20typically%20excluded%20by%20automated%20quality%20assessment%20systems%2C%20but%20judged%20to%20contain%20valuable%20vascular%20information.%0A%20%20Conclusion%3A%20This%20dataset%20offers%20a%20rich%20and%20heterogeneous%20source%20of%20CFIs%20with%20high-quality%20segmentations.%20It%20supports%20robust%20benchmarking%20and%20training%20of%20machine%20learning%20models%20under%20real-world%20variability%20in%20image%20quality%20and%20acquisition%20settings.%0A%20%20Translational%20Relevance%3A%20By%20including%20connectivity-validated%20A/V%20masks%20and%20diverse%20image%20conditions%2C%20this%20dataset%20enables%20the%20development%20of%20clinically%20applicable%2C%20generalizable%20machine%20learning%20tools%20for%20retinal%20vascular%20analysis%2C%20potentially%20improving%20automated%20screening%20and%20diagnosis%20of%20systemic%20and%20ocular%20diseases.&entry.1838667208=http%3A//arxiv.org/abs/2512.17322v2&entry.124074799=Read"},
{"title": "IMPACT: Behavioral Intention-aware Multimodal Trajectory Prediction with Adaptive Context Trimming", "author": "Jiawei Sun and Xibin Yue and Jiahui Li and Tianle Shen and Chengran Yuan and Shuo Sun and Sheng Guo and Quanyun Zhou and Marcelo H Ang", "abstract": "While most prior research has focused on improving the precision of multimodal trajectory predictions, the explicit modeling of multimodal behavioral intentions (e.g., yielding, overtaking) remains relatively underexplored. This paper proposes a unified framework that jointly predicts both behavioral intentions and trajectories to enhance prediction accuracy, interpretability, and efficiency. Specifically, we employ a shared context encoder for both intention and trajectory predictions, thereby reducing structural redundancy and information loss. Moreover, we address the lack of ground-truth behavioral intention labels in mainstream datasets (Waymo, Argoverse) by auto-labeling these datasets, thus advancing the community's efforts in this direction. We further introduce a vectorized occupancy prediction module that infers the probability of each map polyline being occupied by the target vehicle's future trajectory. By leveraging these intention and occupancy prediction priors, our method conducts dynamic, modality-dependent pruning of irrelevant agents and map polylines in the decoding stage, effectively reducing computational overhead and mitigating noise from non-critical elements. Our approach ranks first among LiDAR-free methods on the Waymo Motion Dataset and achieves first place on the Waymo Interactive Prediction Dataset. Remarkably, even without model ensembling, our single-model framework improves the soft mean average precision (softmAP) by 10 percent compared to the second-best method in the Waymo Interactive Prediction Leaderboard. Furthermore, the proposed framework has been successfully deployed on real vehicles, demonstrating its practical effectiveness in real-world applications.", "link": "http://arxiv.org/abs/2504.09103v4", "date": "2026-02-18", "relevancy": 2.2177, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.645}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5371}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMPACT%3A%20Behavioral%20Intention-aware%20Multimodal%20Trajectory%20Prediction%20with%20Adaptive%20Context%20Trimming&body=Title%3A%20IMPACT%3A%20Behavioral%20Intention-aware%20Multimodal%20Trajectory%20Prediction%20with%20Adaptive%20Context%20Trimming%0AAuthor%3A%20Jiawei%20Sun%20and%20Xibin%20Yue%20and%20Jiahui%20Li%20and%20Tianle%20Shen%20and%20Chengran%20Yuan%20and%20Shuo%20Sun%20and%20Sheng%20Guo%20and%20Quanyun%20Zhou%20and%20Marcelo%20H%20Ang%0AAbstract%3A%20While%20most%20prior%20research%20has%20focused%20on%20improving%20the%20precision%20of%20multimodal%20trajectory%20predictions%2C%20the%20explicit%20modeling%20of%20multimodal%20behavioral%20intentions%20%28e.g.%2C%20yielding%2C%20overtaking%29%20remains%20relatively%20underexplored.%20This%20paper%20proposes%20a%20unified%20framework%20that%20jointly%20predicts%20both%20behavioral%20intentions%20and%20trajectories%20to%20enhance%20prediction%20accuracy%2C%20interpretability%2C%20and%20efficiency.%20Specifically%2C%20we%20employ%20a%20shared%20context%20encoder%20for%20both%20intention%20and%20trajectory%20predictions%2C%20thereby%20reducing%20structural%20redundancy%20and%20information%20loss.%20Moreover%2C%20we%20address%20the%20lack%20of%20ground-truth%20behavioral%20intention%20labels%20in%20mainstream%20datasets%20%28Waymo%2C%20Argoverse%29%20by%20auto-labeling%20these%20datasets%2C%20thus%20advancing%20the%20community%27s%20efforts%20in%20this%20direction.%20We%20further%20introduce%20a%20vectorized%20occupancy%20prediction%20module%20that%20infers%20the%20probability%20of%20each%20map%20polyline%20being%20occupied%20by%20the%20target%20vehicle%27s%20future%20trajectory.%20By%20leveraging%20these%20intention%20and%20occupancy%20prediction%20priors%2C%20our%20method%20conducts%20dynamic%2C%20modality-dependent%20pruning%20of%20irrelevant%20agents%20and%20map%20polylines%20in%20the%20decoding%20stage%2C%20effectively%20reducing%20computational%20overhead%20and%20mitigating%20noise%20from%20non-critical%20elements.%20Our%20approach%20ranks%20first%20among%20LiDAR-free%20methods%20on%20the%20Waymo%20Motion%20Dataset%20and%20achieves%20first%20place%20on%20the%20Waymo%20Interactive%20Prediction%20Dataset.%20Remarkably%2C%20even%20without%20model%20ensembling%2C%20our%20single-model%20framework%20improves%20the%20soft%20mean%20average%20precision%20%28softmAP%29%20by%2010%20percent%20compared%20to%20the%20second-best%20method%20in%20the%20Waymo%20Interactive%20Prediction%20Leaderboard.%20Furthermore%2C%20the%20proposed%20framework%20has%20been%20successfully%20deployed%20on%20real%20vehicles%2C%20demonstrating%20its%20practical%20effectiveness%20in%20real-world%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2504.09103v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMPACT%253A%2520Behavioral%2520Intention-aware%2520Multimodal%2520Trajectory%2520Prediction%2520with%2520Adaptive%2520Context%2520Trimming%26entry.906535625%3DJiawei%2520Sun%2520and%2520Xibin%2520Yue%2520and%2520Jiahui%2520Li%2520and%2520Tianle%2520Shen%2520and%2520Chengran%2520Yuan%2520and%2520Shuo%2520Sun%2520and%2520Sheng%2520Guo%2520and%2520Quanyun%2520Zhou%2520and%2520Marcelo%2520H%2520Ang%26entry.1292438233%3DWhile%2520most%2520prior%2520research%2520has%2520focused%2520on%2520improving%2520the%2520precision%2520of%2520multimodal%2520trajectory%2520predictions%252C%2520the%2520explicit%2520modeling%2520of%2520multimodal%2520behavioral%2520intentions%2520%2528e.g.%252C%2520yielding%252C%2520overtaking%2529%2520remains%2520relatively%2520underexplored.%2520This%2520paper%2520proposes%2520a%2520unified%2520framework%2520that%2520jointly%2520predicts%2520both%2520behavioral%2520intentions%2520and%2520trajectories%2520to%2520enhance%2520prediction%2520accuracy%252C%2520interpretability%252C%2520and%2520efficiency.%2520Specifically%252C%2520we%2520employ%2520a%2520shared%2520context%2520encoder%2520for%2520both%2520intention%2520and%2520trajectory%2520predictions%252C%2520thereby%2520reducing%2520structural%2520redundancy%2520and%2520information%2520loss.%2520Moreover%252C%2520we%2520address%2520the%2520lack%2520of%2520ground-truth%2520behavioral%2520intention%2520labels%2520in%2520mainstream%2520datasets%2520%2528Waymo%252C%2520Argoverse%2529%2520by%2520auto-labeling%2520these%2520datasets%252C%2520thus%2520advancing%2520the%2520community%2527s%2520efforts%2520in%2520this%2520direction.%2520We%2520further%2520introduce%2520a%2520vectorized%2520occupancy%2520prediction%2520module%2520that%2520infers%2520the%2520probability%2520of%2520each%2520map%2520polyline%2520being%2520occupied%2520by%2520the%2520target%2520vehicle%2527s%2520future%2520trajectory.%2520By%2520leveraging%2520these%2520intention%2520and%2520occupancy%2520prediction%2520priors%252C%2520our%2520method%2520conducts%2520dynamic%252C%2520modality-dependent%2520pruning%2520of%2520irrelevant%2520agents%2520and%2520map%2520polylines%2520in%2520the%2520decoding%2520stage%252C%2520effectively%2520reducing%2520computational%2520overhead%2520and%2520mitigating%2520noise%2520from%2520non-critical%2520elements.%2520Our%2520approach%2520ranks%2520first%2520among%2520LiDAR-free%2520methods%2520on%2520the%2520Waymo%2520Motion%2520Dataset%2520and%2520achieves%2520first%2520place%2520on%2520the%2520Waymo%2520Interactive%2520Prediction%2520Dataset.%2520Remarkably%252C%2520even%2520without%2520model%2520ensembling%252C%2520our%2520single-model%2520framework%2520improves%2520the%2520soft%2520mean%2520average%2520precision%2520%2528softmAP%2529%2520by%252010%2520percent%2520compared%2520to%2520the%2520second-best%2520method%2520in%2520the%2520Waymo%2520Interactive%2520Prediction%2520Leaderboard.%2520Furthermore%252C%2520the%2520proposed%2520framework%2520has%2520been%2520successfully%2520deployed%2520on%2520real%2520vehicles%252C%2520demonstrating%2520its%2520practical%2520effectiveness%2520in%2520real-world%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09103v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMPACT%3A%20Behavioral%20Intention-aware%20Multimodal%20Trajectory%20Prediction%20with%20Adaptive%20Context%20Trimming&entry.906535625=Jiawei%20Sun%20and%20Xibin%20Yue%20and%20Jiahui%20Li%20and%20Tianle%20Shen%20and%20Chengran%20Yuan%20and%20Shuo%20Sun%20and%20Sheng%20Guo%20and%20Quanyun%20Zhou%20and%20Marcelo%20H%20Ang&entry.1292438233=While%20most%20prior%20research%20has%20focused%20on%20improving%20the%20precision%20of%20multimodal%20trajectory%20predictions%2C%20the%20explicit%20modeling%20of%20multimodal%20behavioral%20intentions%20%28e.g.%2C%20yielding%2C%20overtaking%29%20remains%20relatively%20underexplored.%20This%20paper%20proposes%20a%20unified%20framework%20that%20jointly%20predicts%20both%20behavioral%20intentions%20and%20trajectories%20to%20enhance%20prediction%20accuracy%2C%20interpretability%2C%20and%20efficiency.%20Specifically%2C%20we%20employ%20a%20shared%20context%20encoder%20for%20both%20intention%20and%20trajectory%20predictions%2C%20thereby%20reducing%20structural%20redundancy%20and%20information%20loss.%20Moreover%2C%20we%20address%20the%20lack%20of%20ground-truth%20behavioral%20intention%20labels%20in%20mainstream%20datasets%20%28Waymo%2C%20Argoverse%29%20by%20auto-labeling%20these%20datasets%2C%20thus%20advancing%20the%20community%27s%20efforts%20in%20this%20direction.%20We%20further%20introduce%20a%20vectorized%20occupancy%20prediction%20module%20that%20infers%20the%20probability%20of%20each%20map%20polyline%20being%20occupied%20by%20the%20target%20vehicle%27s%20future%20trajectory.%20By%20leveraging%20these%20intention%20and%20occupancy%20prediction%20priors%2C%20our%20method%20conducts%20dynamic%2C%20modality-dependent%20pruning%20of%20irrelevant%20agents%20and%20map%20polylines%20in%20the%20decoding%20stage%2C%20effectively%20reducing%20computational%20overhead%20and%20mitigating%20noise%20from%20non-critical%20elements.%20Our%20approach%20ranks%20first%20among%20LiDAR-free%20methods%20on%20the%20Waymo%20Motion%20Dataset%20and%20achieves%20first%20place%20on%20the%20Waymo%20Interactive%20Prediction%20Dataset.%20Remarkably%2C%20even%20without%20model%20ensembling%2C%20our%20single-model%20framework%20improves%20the%20soft%20mean%20average%20precision%20%28softmAP%29%20by%2010%20percent%20compared%20to%20the%20second-best%20method%20in%20the%20Waymo%20Interactive%20Prediction%20Leaderboard.%20Furthermore%2C%20the%20proposed%20framework%20has%20been%20successfully%20deployed%20on%20real%20vehicles%2C%20demonstrating%20its%20practical%20effectiveness%20in%20real-world%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2504.09103v4&entry.124074799=Read"},
{"title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning", "author": "Mingjia Shi and Yinhan He and Yaochen Zhu and Jundong Li", "abstract": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.", "link": "http://arxiv.org/abs/2602.16702v1", "date": "2026-02-18", "relevancy": 2.1869, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saliency-Aware%20Multi-Route%20Thinking%3A%20Revisiting%20Vision-Language%20Reasoning&body=Title%3A%20Saliency-Aware%20Multi-Route%20Thinking%3A%20Revisiting%20Vision-Language%20Reasoning%0AAuthor%3A%20Mingjia%20Shi%20and%20Yinhan%20He%20and%20Yaochen%20Zhu%20and%20Jundong%20Li%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20aim%20to%20reason%20by%20jointly%20leveraging%20visual%20and%20textual%20modalities.%20While%20allocating%20additional%20inference-time%20computation%20has%20proven%20effective%20for%20large%20language%20models%20%28LLMs%29%2C%20achieving%20similar%20scaling%20in%20VLMs%20remains%20challenging.%20A%20key%20obstacle%20is%20that%20visual%20inputs%20are%20typically%20provided%20only%20once%20at%20the%20start%20of%20generation%2C%20while%20textual%20reasoning%20%28e.g.%2C%20early%20visual%20summaries%29%20is%20generated%20autoregressively%2C%20causing%20reasoning%20to%20become%20increasingly%20text-dominated%20and%20allowing%20early%20visual%20grounding%20errors%20to%20accumulate.%20Moreover%2C%20vanilla%20guidance%20for%20visual%20grounding%20during%20inference%20is%20often%20coarse%20and%20noisy%2C%20making%20it%20difficult%20to%20steer%20reasoning%20over%20long%20texts.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Cemph%7BSaliency-Aware%20Principle%7D%20%28SAP%29%20selection.%20SAP%20operates%20on%20high-level%20reasoning%20principles%20rather%20than%20token-level%20trajectories%2C%20which%20enable%20stable%20control%20over%20discrete%20generation%20under%20noisy%20feedback%20while%20allowing%20later%20reasoning%20steps%20to%20re-consult%20visual%20evidence%20when%20renewed%20grounding%20is%20required.%20In%20addition%2C%20SAP%20supports%20multi-route%20inference%2C%20enabling%20parallel%20exploration%20of%20diverse%20reasoning%20behaviors.%20SAP%20is%20model-agnostic%20and%20data-free%2C%20requiring%20no%20additional%20training.%20Empirical%20results%20show%20that%20SAP%20achieves%20competitive%20performance%2C%20especially%20in%20reducing%20object%20hallucination%2C%20under%20comparable%20token-generation%20budgets%20while%20yielding%20more%20stable%20reasoning%20and%20lower%20response%20latency%20than%20CoT-style%20long%20sequential%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaliency-Aware%2520Multi-Route%2520Thinking%253A%2520Revisiting%2520Vision-Language%2520Reasoning%26entry.906535625%3DMingjia%2520Shi%2520and%2520Yinhan%2520He%2520and%2520Yaochen%2520Zhu%2520and%2520Jundong%2520Li%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520aim%2520to%2520reason%2520by%2520jointly%2520leveraging%2520visual%2520and%2520textual%2520modalities.%2520While%2520allocating%2520additional%2520inference-time%2520computation%2520has%2520proven%2520effective%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520achieving%2520similar%2520scaling%2520in%2520VLMs%2520remains%2520challenging.%2520A%2520key%2520obstacle%2520is%2520that%2520visual%2520inputs%2520are%2520typically%2520provided%2520only%2520once%2520at%2520the%2520start%2520of%2520generation%252C%2520while%2520textual%2520reasoning%2520%2528e.g.%252C%2520early%2520visual%2520summaries%2529%2520is%2520generated%2520autoregressively%252C%2520causing%2520reasoning%2520to%2520become%2520increasingly%2520text-dominated%2520and%2520allowing%2520early%2520visual%2520grounding%2520errors%2520to%2520accumulate.%2520Moreover%252C%2520vanilla%2520guidance%2520for%2520visual%2520grounding%2520during%2520inference%2520is%2520often%2520coarse%2520and%2520noisy%252C%2520making%2520it%2520difficult%2520to%2520steer%2520reasoning%2520over%2520long%2520texts.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255Cemph%257BSaliency-Aware%2520Principle%257D%2520%2528SAP%2529%2520selection.%2520SAP%2520operates%2520on%2520high-level%2520reasoning%2520principles%2520rather%2520than%2520token-level%2520trajectories%252C%2520which%2520enable%2520stable%2520control%2520over%2520discrete%2520generation%2520under%2520noisy%2520feedback%2520while%2520allowing%2520later%2520reasoning%2520steps%2520to%2520re-consult%2520visual%2520evidence%2520when%2520renewed%2520grounding%2520is%2520required.%2520In%2520addition%252C%2520SAP%2520supports%2520multi-route%2520inference%252C%2520enabling%2520parallel%2520exploration%2520of%2520diverse%2520reasoning%2520behaviors.%2520SAP%2520is%2520model-agnostic%2520and%2520data-free%252C%2520requiring%2520no%2520additional%2520training.%2520Empirical%2520results%2520show%2520that%2520SAP%2520achieves%2520competitive%2520performance%252C%2520especially%2520in%2520reducing%2520object%2520hallucination%252C%2520under%2520comparable%2520token-generation%2520budgets%2520while%2520yielding%2520more%2520stable%2520reasoning%2520and%2520lower%2520response%2520latency%2520than%2520CoT-style%2520long%2520sequential%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saliency-Aware%20Multi-Route%20Thinking%3A%20Revisiting%20Vision-Language%20Reasoning&entry.906535625=Mingjia%20Shi%20and%20Yinhan%20He%20and%20Yaochen%20Zhu%20and%20Jundong%20Li&entry.1292438233=Vision-language%20models%20%28VLMs%29%20aim%20to%20reason%20by%20jointly%20leveraging%20visual%20and%20textual%20modalities.%20While%20allocating%20additional%20inference-time%20computation%20has%20proven%20effective%20for%20large%20language%20models%20%28LLMs%29%2C%20achieving%20similar%20scaling%20in%20VLMs%20remains%20challenging.%20A%20key%20obstacle%20is%20that%20visual%20inputs%20are%20typically%20provided%20only%20once%20at%20the%20start%20of%20generation%2C%20while%20textual%20reasoning%20%28e.g.%2C%20early%20visual%20summaries%29%20is%20generated%20autoregressively%2C%20causing%20reasoning%20to%20become%20increasingly%20text-dominated%20and%20allowing%20early%20visual%20grounding%20errors%20to%20accumulate.%20Moreover%2C%20vanilla%20guidance%20for%20visual%20grounding%20during%20inference%20is%20often%20coarse%20and%20noisy%2C%20making%20it%20difficult%20to%20steer%20reasoning%20over%20long%20texts.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Cemph%7BSaliency-Aware%20Principle%7D%20%28SAP%29%20selection.%20SAP%20operates%20on%20high-level%20reasoning%20principles%20rather%20than%20token-level%20trajectories%2C%20which%20enable%20stable%20control%20over%20discrete%20generation%20under%20noisy%20feedback%20while%20allowing%20later%20reasoning%20steps%20to%20re-consult%20visual%20evidence%20when%20renewed%20grounding%20is%20required.%20In%20addition%2C%20SAP%20supports%20multi-route%20inference%2C%20enabling%20parallel%20exploration%20of%20diverse%20reasoning%20behaviors.%20SAP%20is%20model-agnostic%20and%20data-free%2C%20requiring%20no%20additional%20training.%20Empirical%20results%20show%20that%20SAP%20achieves%20competitive%20performance%2C%20especially%20in%20reducing%20object%20hallucination%2C%20under%20comparable%20token-generation%20budgets%20while%20yielding%20more%20stable%20reasoning%20and%20lower%20response%20latency%20than%20CoT-style%20long%20sequential%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2602.16702v1&entry.124074799=Read"},
{"title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety", "author": "Osher Azulay and Zhengjie Xu and Andrew Scheffer and Stella X. Yu", "abstract": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/", "link": "http://arxiv.org/abs/2602.16511v1", "date": "2026-02-18", "relevancy": 2.1793, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.587}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5364}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIGOR%3A%20Visual%20Goal-In-Context%20Inference%20for%20Unified%20Humanoid%20Fall%20Safety&body=Title%3A%20VIGOR%3A%20Visual%20Goal-In-Context%20Inference%20for%20Unified%20Humanoid%20Fall%20Safety%0AAuthor%3A%20Osher%20Azulay%20and%20Zhengjie%20Xu%20and%20Andrew%20Scheffer%20and%20Stella%20X.%20Yu%0AAbstract%3A%20Reliable%20fall%20recovery%20is%20critical%20for%20humanoids%20operating%20in%20cluttered%20environments.%20Unlike%20quadrupeds%20or%20wheeled%20robots%2C%20humanoids%20experience%20high-energy%20impacts%2C%20complex%20whole-body%20contact%2C%20and%20large%20viewpoint%20changes%20during%20a%20fall%2C%20making%20recovery%20essential%20for%20continued%20operation.%20Existing%20methods%20fragment%20fall%20safety%20into%20separate%20problems%20such%20as%20fall%20avoidance%2C%20impact%20mitigation%2C%20and%20stand-up%20recovery%2C%20or%20rely%20on%20end-to-end%20policies%20trained%20without%20vision%20through%20reinforcement%20learning%20or%20imitation%20learning%2C%20often%20on%20flat%20terrain.%20At%20a%20deeper%20level%2C%20fall%20safety%20is%20treated%20as%20monolithic%20data%20complexity%2C%20coupling%20pose%2C%20dynamics%2C%20and%20terrain%20and%20requiring%20exhaustive%20coverage%2C%20limiting%20scalability%20and%20generalization.%20We%20present%20a%20unified%20fall%20safety%20approach%20that%20spans%20all%20phases%20of%20fall%20recovery.%20It%20builds%20on%20two%20insights%3A%201%29%20Natural%20human%20fall%20and%20recovery%20poses%20are%20highly%20constrained%20and%20transferable%20from%20flat%20to%20complex%20terrain%20through%20alignment%2C%20and%202%29%20Fast%20whole-body%20reactions%20require%20integrated%20perceptual-motor%20representations.%20We%20train%20a%20privileged%20teacher%20using%20sparse%20human%20demonstrations%20on%20flat%20terrain%20and%20simulated%20complex%20terrains%2C%20and%20distill%20it%20into%20a%20deployable%20student%20that%20relies%20only%20on%20egocentric%20depth%20and%20proprioception.%20The%20student%20learns%20how%20to%20react%20by%20matching%20the%20teacher%27s%20goal-in-context%20latent%20representation%2C%20which%20combines%20the%20next%20target%20pose%20with%20the%20local%20terrain%2C%20rather%20than%20separately%20encoding%20what%20it%20must%20perceive%20and%20how%20it%20must%20act.%20Results%20in%20simulation%20and%20on%20a%20real%20Unitree%20G1%20humanoid%20demonstrate%20robust%2C%20zero-shot%20fall%20safety%20across%20diverse%20non-flat%20environments%20without%20real-world%20fine-tuning.%20The%20project%20page%20is%20available%20at%20https%3A//vigor2026.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2602.16511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIGOR%253A%2520Visual%2520Goal-In-Context%2520Inference%2520for%2520Unified%2520Humanoid%2520Fall%2520Safety%26entry.906535625%3DOsher%2520Azulay%2520and%2520Zhengjie%2520Xu%2520and%2520Andrew%2520Scheffer%2520and%2520Stella%2520X.%2520Yu%26entry.1292438233%3DReliable%2520fall%2520recovery%2520is%2520critical%2520for%2520humanoids%2520operating%2520in%2520cluttered%2520environments.%2520Unlike%2520quadrupeds%2520or%2520wheeled%2520robots%252C%2520humanoids%2520experience%2520high-energy%2520impacts%252C%2520complex%2520whole-body%2520contact%252C%2520and%2520large%2520viewpoint%2520changes%2520during%2520a%2520fall%252C%2520making%2520recovery%2520essential%2520for%2520continued%2520operation.%2520Existing%2520methods%2520fragment%2520fall%2520safety%2520into%2520separate%2520problems%2520such%2520as%2520fall%2520avoidance%252C%2520impact%2520mitigation%252C%2520and%2520stand-up%2520recovery%252C%2520or%2520rely%2520on%2520end-to-end%2520policies%2520trained%2520without%2520vision%2520through%2520reinforcement%2520learning%2520or%2520imitation%2520learning%252C%2520often%2520on%2520flat%2520terrain.%2520At%2520a%2520deeper%2520level%252C%2520fall%2520safety%2520is%2520treated%2520as%2520monolithic%2520data%2520complexity%252C%2520coupling%2520pose%252C%2520dynamics%252C%2520and%2520terrain%2520and%2520requiring%2520exhaustive%2520coverage%252C%2520limiting%2520scalability%2520and%2520generalization.%2520We%2520present%2520a%2520unified%2520fall%2520safety%2520approach%2520that%2520spans%2520all%2520phases%2520of%2520fall%2520recovery.%2520It%2520builds%2520on%2520two%2520insights%253A%25201%2529%2520Natural%2520human%2520fall%2520and%2520recovery%2520poses%2520are%2520highly%2520constrained%2520and%2520transferable%2520from%2520flat%2520to%2520complex%2520terrain%2520through%2520alignment%252C%2520and%25202%2529%2520Fast%2520whole-body%2520reactions%2520require%2520integrated%2520perceptual-motor%2520representations.%2520We%2520train%2520a%2520privileged%2520teacher%2520using%2520sparse%2520human%2520demonstrations%2520on%2520flat%2520terrain%2520and%2520simulated%2520complex%2520terrains%252C%2520and%2520distill%2520it%2520into%2520a%2520deployable%2520student%2520that%2520relies%2520only%2520on%2520egocentric%2520depth%2520and%2520proprioception.%2520The%2520student%2520learns%2520how%2520to%2520react%2520by%2520matching%2520the%2520teacher%2527s%2520goal-in-context%2520latent%2520representation%252C%2520which%2520combines%2520the%2520next%2520target%2520pose%2520with%2520the%2520local%2520terrain%252C%2520rather%2520than%2520separately%2520encoding%2520what%2520it%2520must%2520perceive%2520and%2520how%2520it%2520must%2520act.%2520Results%2520in%2520simulation%2520and%2520on%2520a%2520real%2520Unitree%2520G1%2520humanoid%2520demonstrate%2520robust%252C%2520zero-shot%2520fall%2520safety%2520across%2520diverse%2520non-flat%2520environments%2520without%2520real-world%2520fine-tuning.%2520The%2520project%2520page%2520is%2520available%2520at%2520https%253A//vigor2026.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIGOR%3A%20Visual%20Goal-In-Context%20Inference%20for%20Unified%20Humanoid%20Fall%20Safety&entry.906535625=Osher%20Azulay%20and%20Zhengjie%20Xu%20and%20Andrew%20Scheffer%20and%20Stella%20X.%20Yu&entry.1292438233=Reliable%20fall%20recovery%20is%20critical%20for%20humanoids%20operating%20in%20cluttered%20environments.%20Unlike%20quadrupeds%20or%20wheeled%20robots%2C%20humanoids%20experience%20high-energy%20impacts%2C%20complex%20whole-body%20contact%2C%20and%20large%20viewpoint%20changes%20during%20a%20fall%2C%20making%20recovery%20essential%20for%20continued%20operation.%20Existing%20methods%20fragment%20fall%20safety%20into%20separate%20problems%20such%20as%20fall%20avoidance%2C%20impact%20mitigation%2C%20and%20stand-up%20recovery%2C%20or%20rely%20on%20end-to-end%20policies%20trained%20without%20vision%20through%20reinforcement%20learning%20or%20imitation%20learning%2C%20often%20on%20flat%20terrain.%20At%20a%20deeper%20level%2C%20fall%20safety%20is%20treated%20as%20monolithic%20data%20complexity%2C%20coupling%20pose%2C%20dynamics%2C%20and%20terrain%20and%20requiring%20exhaustive%20coverage%2C%20limiting%20scalability%20and%20generalization.%20We%20present%20a%20unified%20fall%20safety%20approach%20that%20spans%20all%20phases%20of%20fall%20recovery.%20It%20builds%20on%20two%20insights%3A%201%29%20Natural%20human%20fall%20and%20recovery%20poses%20are%20highly%20constrained%20and%20transferable%20from%20flat%20to%20complex%20terrain%20through%20alignment%2C%20and%202%29%20Fast%20whole-body%20reactions%20require%20integrated%20perceptual-motor%20representations.%20We%20train%20a%20privileged%20teacher%20using%20sparse%20human%20demonstrations%20on%20flat%20terrain%20and%20simulated%20complex%20terrains%2C%20and%20distill%20it%20into%20a%20deployable%20student%20that%20relies%20only%20on%20egocentric%20depth%20and%20proprioception.%20The%20student%20learns%20how%20to%20react%20by%20matching%20the%20teacher%27s%20goal-in-context%20latent%20representation%2C%20which%20combines%20the%20next%20target%20pose%20with%20the%20local%20terrain%2C%20rather%20than%20separately%20encoding%20what%20it%20must%20perceive%20and%20how%20it%20must%20act.%20Results%20in%20simulation%20and%20on%20a%20real%20Unitree%20G1%20humanoid%20demonstrate%20robust%2C%20zero-shot%20fall%20safety%20across%20diverse%20non-flat%20environments%20without%20real-world%20fine-tuning.%20The%20project%20page%20is%20available%20at%20https%3A//vigor2026.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2602.16511v1&entry.124074799=Read"},
{"title": "ReMoRa: Multimodal Large Language Model based on Refined Motion Representation for Long-Video Understanding", "author": "Daichi Yashima and Shuhei Kurita and Yusuke Oda and Komei Sugiura", "abstract": "While multimodal large language models (MLLMs) have shown remarkable success across a wide range of tasks, long-form video understanding remains a significant challenge. In this study, we focus on video understanding by MLLMs. This task is challenging because processing a full stream of RGB frames is computationally intractable and highly redundant, as self-attention have quadratic complexity with sequence length. In this paper, we propose ReMoRa, a video MLLM that processes videos by operating directly on their compressed representations. A sparse set of RGB keyframes is retained for appearance, while temporal dynamics are encoded as a motion representation, removing the need for sequential RGB frames. These motion representations act as a compact proxy for optical flow, capturing temporal dynamics without full frame decoding. To refine the noise and low fidelity of block-based motions, we introduce a module to denoise and generate a fine-grained motion representation. Furthermore, our model compresses these features in a way that scales linearly with sequence length. We demonstrate the effectiveness of ReMoRa through extensive experiments across a comprehensive suite of long-video understanding benchmarks. ReMoRa outperformed baseline methods on multiple challenging benchmarks, including LongVideoBench, NExT-QA, and MLVU.", "link": "http://arxiv.org/abs/2602.16412v1", "date": "2026-02-18", "relevancy": 2.1762, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReMoRa%3A%20Multimodal%20Large%20Language%20Model%20based%20on%20Refined%20Motion%20Representation%20for%20Long-Video%20Understanding&body=Title%3A%20ReMoRa%3A%20Multimodal%20Large%20Language%20Model%20based%20on%20Refined%20Motion%20Representation%20for%20Long-Video%20Understanding%0AAuthor%3A%20Daichi%20Yashima%20and%20Shuhei%20Kurita%20and%20Yusuke%20Oda%20and%20Komei%20Sugiura%0AAbstract%3A%20While%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20remarkable%20success%20across%20a%20wide%20range%20of%20tasks%2C%20long-form%20video%20understanding%20remains%20a%20significant%20challenge.%20In%20this%20study%2C%20we%20focus%20on%20video%20understanding%20by%20MLLMs.%20This%20task%20is%20challenging%20because%20processing%20a%20full%20stream%20of%20RGB%20frames%20is%20computationally%20intractable%20and%20highly%20redundant%2C%20as%20self-attention%20have%20quadratic%20complexity%20with%20sequence%20length.%20In%20this%20paper%2C%20we%20propose%20ReMoRa%2C%20a%20video%20MLLM%20that%20processes%20videos%20by%20operating%20directly%20on%20their%20compressed%20representations.%20A%20sparse%20set%20of%20RGB%20keyframes%20is%20retained%20for%20appearance%2C%20while%20temporal%20dynamics%20are%20encoded%20as%20a%20motion%20representation%2C%20removing%20the%20need%20for%20sequential%20RGB%20frames.%20These%20motion%20representations%20act%20as%20a%20compact%20proxy%20for%20optical%20flow%2C%20capturing%20temporal%20dynamics%20without%20full%20frame%20decoding.%20To%20refine%20the%20noise%20and%20low%20fidelity%20of%20block-based%20motions%2C%20we%20introduce%20a%20module%20to%20denoise%20and%20generate%20a%20fine-grained%20motion%20representation.%20Furthermore%2C%20our%20model%20compresses%20these%20features%20in%20a%20way%20that%20scales%20linearly%20with%20sequence%20length.%20We%20demonstrate%20the%20effectiveness%20of%20ReMoRa%20through%20extensive%20experiments%20across%20a%20comprehensive%20suite%20of%20long-video%20understanding%20benchmarks.%20ReMoRa%20outperformed%20baseline%20methods%20on%20multiple%20challenging%20benchmarks%2C%20including%20LongVideoBench%2C%20NExT-QA%2C%20and%20MLVU.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReMoRa%253A%2520Multimodal%2520Large%2520Language%2520Model%2520based%2520on%2520Refined%2520Motion%2520Representation%2520for%2520Long-Video%2520Understanding%26entry.906535625%3DDaichi%2520Yashima%2520and%2520Shuhei%2520Kurita%2520and%2520Yusuke%2520Oda%2520and%2520Komei%2520Sugiura%26entry.1292438233%3DWhile%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520remarkable%2520success%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520long-form%2520video%2520understanding%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520study%252C%2520we%2520focus%2520on%2520video%2520understanding%2520by%2520MLLMs.%2520This%2520task%2520is%2520challenging%2520because%2520processing%2520a%2520full%2520stream%2520of%2520RGB%2520frames%2520is%2520computationally%2520intractable%2520and%2520highly%2520redundant%252C%2520as%2520self-attention%2520have%2520quadratic%2520complexity%2520with%2520sequence%2520length.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ReMoRa%252C%2520a%2520video%2520MLLM%2520that%2520processes%2520videos%2520by%2520operating%2520directly%2520on%2520their%2520compressed%2520representations.%2520A%2520sparse%2520set%2520of%2520RGB%2520keyframes%2520is%2520retained%2520for%2520appearance%252C%2520while%2520temporal%2520dynamics%2520are%2520encoded%2520as%2520a%2520motion%2520representation%252C%2520removing%2520the%2520need%2520for%2520sequential%2520RGB%2520frames.%2520These%2520motion%2520representations%2520act%2520as%2520a%2520compact%2520proxy%2520for%2520optical%2520flow%252C%2520capturing%2520temporal%2520dynamics%2520without%2520full%2520frame%2520decoding.%2520To%2520refine%2520the%2520noise%2520and%2520low%2520fidelity%2520of%2520block-based%2520motions%252C%2520we%2520introduce%2520a%2520module%2520to%2520denoise%2520and%2520generate%2520a%2520fine-grained%2520motion%2520representation.%2520Furthermore%252C%2520our%2520model%2520compresses%2520these%2520features%2520in%2520a%2520way%2520that%2520scales%2520linearly%2520with%2520sequence%2520length.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520ReMoRa%2520through%2520extensive%2520experiments%2520across%2520a%2520comprehensive%2520suite%2520of%2520long-video%2520understanding%2520benchmarks.%2520ReMoRa%2520outperformed%2520baseline%2520methods%2520on%2520multiple%2520challenging%2520benchmarks%252C%2520including%2520LongVideoBench%252C%2520NExT-QA%252C%2520and%2520MLVU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReMoRa%3A%20Multimodal%20Large%20Language%20Model%20based%20on%20Refined%20Motion%20Representation%20for%20Long-Video%20Understanding&entry.906535625=Daichi%20Yashima%20and%20Shuhei%20Kurita%20and%20Yusuke%20Oda%20and%20Komei%20Sugiura&entry.1292438233=While%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20remarkable%20success%20across%20a%20wide%20range%20of%20tasks%2C%20long-form%20video%20understanding%20remains%20a%20significant%20challenge.%20In%20this%20study%2C%20we%20focus%20on%20video%20understanding%20by%20MLLMs.%20This%20task%20is%20challenging%20because%20processing%20a%20full%20stream%20of%20RGB%20frames%20is%20computationally%20intractable%20and%20highly%20redundant%2C%20as%20self-attention%20have%20quadratic%20complexity%20with%20sequence%20length.%20In%20this%20paper%2C%20we%20propose%20ReMoRa%2C%20a%20video%20MLLM%20that%20processes%20videos%20by%20operating%20directly%20on%20their%20compressed%20representations.%20A%20sparse%20set%20of%20RGB%20keyframes%20is%20retained%20for%20appearance%2C%20while%20temporal%20dynamics%20are%20encoded%20as%20a%20motion%20representation%2C%20removing%20the%20need%20for%20sequential%20RGB%20frames.%20These%20motion%20representations%20act%20as%20a%20compact%20proxy%20for%20optical%20flow%2C%20capturing%20temporal%20dynamics%20without%20full%20frame%20decoding.%20To%20refine%20the%20noise%20and%20low%20fidelity%20of%20block-based%20motions%2C%20we%20introduce%20a%20module%20to%20denoise%20and%20generate%20a%20fine-grained%20motion%20representation.%20Furthermore%2C%20our%20model%20compresses%20these%20features%20in%20a%20way%20that%20scales%20linearly%20with%20sequence%20length.%20We%20demonstrate%20the%20effectiveness%20of%20ReMoRa%20through%20extensive%20experiments%20across%20a%20comprehensive%20suite%20of%20long-video%20understanding%20benchmarks.%20ReMoRa%20outperformed%20baseline%20methods%20on%20multiple%20challenging%20benchmarks%2C%20including%20LongVideoBench%2C%20NExT-QA%2C%20and%20MLVU.&entry.1838667208=http%3A//arxiv.org/abs/2602.16412v1&entry.124074799=Read"},
{"title": "Spatial Audio Question Answering and Reasoning on Dynamic Source Movements", "author": "Arvind Krishna Sridhar and Yinyi Guo and Erik Visser", "abstract": "Spatial audio understanding aims to enable machines to interpret complex auditory scenes, particularly when sound sources move over time. In this work, we study Spatial Audio Question Answering (Spatial AQA) with a focus on movement reasoning, where a model must infer object motion, position, and directional changes directly from stereo audio. First, we introduce a movement-centric spatial audio augmentation framework that synthesizes diverse motion patterns from isolated mono audio events, enabling controlled and scalable training data generation. Second, we propose an end-to-end multimodal finetuning approach with a thinking mode, which allows audio-language models to produce explicit intermediate reasoning steps before predicting an answer. Third, we investigate the impact of query-conditioned source separation as a preprocessing stage and compare three inference regimes: no masking, an audio grounding model (AGM), and ground-truth masks. Our results show that reasoning amplifies the benefits of source separation, with thinking mode showing significant improvement of +5.1% when a single event is present in the question. These findings highlight the interplay between movement modeling, reasoning, and separation quality, offering new insights for advancing spatial audio understanding.", "link": "http://arxiv.org/abs/2602.16334v1", "date": "2026-02-18", "relevancy": 2.1427, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20Audio%20Question%20Answering%20and%20Reasoning%20on%20Dynamic%20Source%20Movements&body=Title%3A%20Spatial%20Audio%20Question%20Answering%20and%20Reasoning%20on%20Dynamic%20Source%20Movements%0AAuthor%3A%20Arvind%20Krishna%20Sridhar%20and%20Yinyi%20Guo%20and%20Erik%20Visser%0AAbstract%3A%20Spatial%20audio%20understanding%20aims%20to%20enable%20machines%20to%20interpret%20complex%20auditory%20scenes%2C%20particularly%20when%20sound%20sources%20move%20over%20time.%20In%20this%20work%2C%20we%20study%20Spatial%20Audio%20Question%20Answering%20%28Spatial%20AQA%29%20with%20a%20focus%20on%20movement%20reasoning%2C%20where%20a%20model%20must%20infer%20object%20motion%2C%20position%2C%20and%20directional%20changes%20directly%20from%20stereo%20audio.%20First%2C%20we%20introduce%20a%20movement-centric%20spatial%20audio%20augmentation%20framework%20that%20synthesizes%20diverse%20motion%20patterns%20from%20isolated%20mono%20audio%20events%2C%20enabling%20controlled%20and%20scalable%20training%20data%20generation.%20Second%2C%20we%20propose%20an%20end-to-end%20multimodal%20finetuning%20approach%20with%20a%20thinking%20mode%2C%20which%20allows%20audio-language%20models%20to%20produce%20explicit%20intermediate%20reasoning%20steps%20before%20predicting%20an%20answer.%20Third%2C%20we%20investigate%20the%20impact%20of%20query-conditioned%20source%20separation%20as%20a%20preprocessing%20stage%20and%20compare%20three%20inference%20regimes%3A%20no%20masking%2C%20an%20audio%20grounding%20model%20%28AGM%29%2C%20and%20ground-truth%20masks.%20Our%20results%20show%20that%20reasoning%20amplifies%20the%20benefits%20of%20source%20separation%2C%20with%20thinking%20mode%20showing%20significant%20improvement%20of%20%2B5.1%25%20when%20a%20single%20event%20is%20present%20in%20the%20question.%20These%20findings%20highlight%20the%20interplay%20between%20movement%20modeling%2C%20reasoning%2C%20and%20separation%20quality%2C%20offering%20new%20insights%20for%20advancing%20spatial%20audio%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520Audio%2520Question%2520Answering%2520and%2520Reasoning%2520on%2520Dynamic%2520Source%2520Movements%26entry.906535625%3DArvind%2520Krishna%2520Sridhar%2520and%2520Yinyi%2520Guo%2520and%2520Erik%2520Visser%26entry.1292438233%3DSpatial%2520audio%2520understanding%2520aims%2520to%2520enable%2520machines%2520to%2520interpret%2520complex%2520auditory%2520scenes%252C%2520particularly%2520when%2520sound%2520sources%2520move%2520over%2520time.%2520In%2520this%2520work%252C%2520we%2520study%2520Spatial%2520Audio%2520Question%2520Answering%2520%2528Spatial%2520AQA%2529%2520with%2520a%2520focus%2520on%2520movement%2520reasoning%252C%2520where%2520a%2520model%2520must%2520infer%2520object%2520motion%252C%2520position%252C%2520and%2520directional%2520changes%2520directly%2520from%2520stereo%2520audio.%2520First%252C%2520we%2520introduce%2520a%2520movement-centric%2520spatial%2520audio%2520augmentation%2520framework%2520that%2520synthesizes%2520diverse%2520motion%2520patterns%2520from%2520isolated%2520mono%2520audio%2520events%252C%2520enabling%2520controlled%2520and%2520scalable%2520training%2520data%2520generation.%2520Second%252C%2520we%2520propose%2520an%2520end-to-end%2520multimodal%2520finetuning%2520approach%2520with%2520a%2520thinking%2520mode%252C%2520which%2520allows%2520audio-language%2520models%2520to%2520produce%2520explicit%2520intermediate%2520reasoning%2520steps%2520before%2520predicting%2520an%2520answer.%2520Third%252C%2520we%2520investigate%2520the%2520impact%2520of%2520query-conditioned%2520source%2520separation%2520as%2520a%2520preprocessing%2520stage%2520and%2520compare%2520three%2520inference%2520regimes%253A%2520no%2520masking%252C%2520an%2520audio%2520grounding%2520model%2520%2528AGM%2529%252C%2520and%2520ground-truth%2520masks.%2520Our%2520results%2520show%2520that%2520reasoning%2520amplifies%2520the%2520benefits%2520of%2520source%2520separation%252C%2520with%2520thinking%2520mode%2520showing%2520significant%2520improvement%2520of%2520%252B5.1%2525%2520when%2520a%2520single%2520event%2520is%2520present%2520in%2520the%2520question.%2520These%2520findings%2520highlight%2520the%2520interplay%2520between%2520movement%2520modeling%252C%2520reasoning%252C%2520and%2520separation%2520quality%252C%2520offering%2520new%2520insights%2520for%2520advancing%2520spatial%2520audio%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20Audio%20Question%20Answering%20and%20Reasoning%20on%20Dynamic%20Source%20Movements&entry.906535625=Arvind%20Krishna%20Sridhar%20and%20Yinyi%20Guo%20and%20Erik%20Visser&entry.1292438233=Spatial%20audio%20understanding%20aims%20to%20enable%20machines%20to%20interpret%20complex%20auditory%20scenes%2C%20particularly%20when%20sound%20sources%20move%20over%20time.%20In%20this%20work%2C%20we%20study%20Spatial%20Audio%20Question%20Answering%20%28Spatial%20AQA%29%20with%20a%20focus%20on%20movement%20reasoning%2C%20where%20a%20model%20must%20infer%20object%20motion%2C%20position%2C%20and%20directional%20changes%20directly%20from%20stereo%20audio.%20First%2C%20we%20introduce%20a%20movement-centric%20spatial%20audio%20augmentation%20framework%20that%20synthesizes%20diverse%20motion%20patterns%20from%20isolated%20mono%20audio%20events%2C%20enabling%20controlled%20and%20scalable%20training%20data%20generation.%20Second%2C%20we%20propose%20an%20end-to-end%20multimodal%20finetuning%20approach%20with%20a%20thinking%20mode%2C%20which%20allows%20audio-language%20models%20to%20produce%20explicit%20intermediate%20reasoning%20steps%20before%20predicting%20an%20answer.%20Third%2C%20we%20investigate%20the%20impact%20of%20query-conditioned%20source%20separation%20as%20a%20preprocessing%20stage%20and%20compare%20three%20inference%20regimes%3A%20no%20masking%2C%20an%20audio%20grounding%20model%20%28AGM%29%2C%20and%20ground-truth%20masks.%20Our%20results%20show%20that%20reasoning%20amplifies%20the%20benefits%20of%20source%20separation%2C%20with%20thinking%20mode%20showing%20significant%20improvement%20of%20%2B5.1%25%20when%20a%20single%20event%20is%20present%20in%20the%20question.%20These%20findings%20highlight%20the%20interplay%20between%20movement%20modeling%2C%20reasoning%2C%20and%20separation%20quality%2C%20offering%20new%20insights%20for%20advancing%20spatial%20audio%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2602.16334v1&entry.124074799=Read"},
{"title": "AI-Driven Structure Refinement of X-ray Diffraction", "author": "Bin Cao and Qian Zhang and Zhenjie Feng and Taolue Zhang and Jiaqiang Huang and Lu-Tao Weng and Tong-Yi Zhang", "abstract": "Artificial intelligence can rapidly propose candidate phases and structures from X-ray diffraction (XRD), but these hypotheses often fail in downstream refinement because peak intensities cannot be stably assigned under severe overlap and diffraction consistency is enforced only weakly. Here we introduce WPEM, a physics-constrained whole-pattern decomposition and refinement workflow that turns Bragg's law into an explicit constraint within a batch expectation--maximization framework. WPEM models the full profile as a probabilistic mixture density and iteratively infers component-resolved intensities while keeping peak centres Bragg-consistent, producing a continuous, physically admissible intensity representation that remains stable in heavily overlapped regions and in the presence of mixed radiation or multiple phases. We benchmark WPEM on standard reference patterns (\\ce{PbSO4} and \\ce{Tb2BaCoO5}), where it yields lower $R_{\\mathrm{p}}$/$R_{\\mathrm{wp}}$ than widely used packages (FullProf and TOPAS) under matched refinement conditions. We further demonstrate generality across realistic experimental scenarios, including phase-resolved decomposition of a multiphase Ti--15Nb thin film, quantitative recovery of \\ce{NaCl}--\\ce{Li2CO3} mixture compositions, separation of crystalline peaks from amorphous halos in semicrystalline polymers, high-throughput operando lattice tracking in layered cathodes, automated refinement of a compositionally disordered Ru--Mn oxide solid solution (CCDC 2530452), and quantitative phase-resolved deciphering of an ancient Egyptian make-up sample from synchrotron powder XRD. By providing Bragg-consistent, uncertainty-aware intensity partitioning as a refinement-ready interface, WPEM closes the gap between AI-generated hypotheses and diffraction-admissible structure refinement on challenging XRD data.", "link": "http://arxiv.org/abs/2602.16372v1", "date": "2026-02-18", "relevancy": 2.1321, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4276}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.426}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-Driven%20Structure%20Refinement%20of%20X-ray%20Diffraction&body=Title%3A%20AI-Driven%20Structure%20Refinement%20of%20X-ray%20Diffraction%0AAuthor%3A%20Bin%20Cao%20and%20Qian%20Zhang%20and%20Zhenjie%20Feng%20and%20Taolue%20Zhang%20and%20Jiaqiang%20Huang%20and%20Lu-Tao%20Weng%20and%20Tong-Yi%20Zhang%0AAbstract%3A%20Artificial%20intelligence%20can%20rapidly%20propose%20candidate%20phases%20and%20structures%20from%20X-ray%20diffraction%20%28XRD%29%2C%20but%20these%20hypotheses%20often%20fail%20in%20downstream%20refinement%20because%20peak%20intensities%20cannot%20be%20stably%20assigned%20under%20severe%20overlap%20and%20diffraction%20consistency%20is%20enforced%20only%20weakly.%20Here%20we%20introduce%20WPEM%2C%20a%20physics-constrained%20whole-pattern%20decomposition%20and%20refinement%20workflow%20that%20turns%20Bragg%27s%20law%20into%20an%20explicit%20constraint%20within%20a%20batch%20expectation--maximization%20framework.%20WPEM%20models%20the%20full%20profile%20as%20a%20probabilistic%20mixture%20density%20and%20iteratively%20infers%20component-resolved%20intensities%20while%20keeping%20peak%20centres%20Bragg-consistent%2C%20producing%20a%20continuous%2C%20physically%20admissible%20intensity%20representation%20that%20remains%20stable%20in%20heavily%20overlapped%20regions%20and%20in%20the%20presence%20of%20mixed%20radiation%20or%20multiple%20phases.%20We%20benchmark%20WPEM%20on%20standard%20reference%20patterns%20%28%5Cce%7BPbSO4%7D%20and%20%5Cce%7BTb2BaCoO5%7D%29%2C%20where%20it%20yields%20lower%20%24R_%7B%5Cmathrm%7Bp%7D%7D%24/%24R_%7B%5Cmathrm%7Bwp%7D%7D%24%20than%20widely%20used%20packages%20%28FullProf%20and%20TOPAS%29%20under%20matched%20refinement%20conditions.%20We%20further%20demonstrate%20generality%20across%20realistic%20experimental%20scenarios%2C%20including%20phase-resolved%20decomposition%20of%20a%20multiphase%20Ti--15Nb%20thin%20film%2C%20quantitative%20recovery%20of%20%5Cce%7BNaCl%7D--%5Cce%7BLi2CO3%7D%20mixture%20compositions%2C%20separation%20of%20crystalline%20peaks%20from%20amorphous%20halos%20in%20semicrystalline%20polymers%2C%20high-throughput%20operando%20lattice%20tracking%20in%20layered%20cathodes%2C%20automated%20refinement%20of%20a%20compositionally%20disordered%20Ru--Mn%20oxide%20solid%20solution%20%28CCDC%202530452%29%2C%20and%20quantitative%20phase-resolved%20deciphering%20of%20an%20ancient%20Egyptian%20make-up%20sample%20from%20synchrotron%20powder%20XRD.%20By%20providing%20Bragg-consistent%2C%20uncertainty-aware%20intensity%20partitioning%20as%20a%20refinement-ready%20interface%2C%20WPEM%20closes%20the%20gap%20between%20AI-generated%20hypotheses%20and%20diffraction-admissible%20structure%20refinement%20on%20challenging%20XRD%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-Driven%2520Structure%2520Refinement%2520of%2520X-ray%2520Diffraction%26entry.906535625%3DBin%2520Cao%2520and%2520Qian%2520Zhang%2520and%2520Zhenjie%2520Feng%2520and%2520Taolue%2520Zhang%2520and%2520Jiaqiang%2520Huang%2520and%2520Lu-Tao%2520Weng%2520and%2520Tong-Yi%2520Zhang%26entry.1292438233%3DArtificial%2520intelligence%2520can%2520rapidly%2520propose%2520candidate%2520phases%2520and%2520structures%2520from%2520X-ray%2520diffraction%2520%2528XRD%2529%252C%2520but%2520these%2520hypotheses%2520often%2520fail%2520in%2520downstream%2520refinement%2520because%2520peak%2520intensities%2520cannot%2520be%2520stably%2520assigned%2520under%2520severe%2520overlap%2520and%2520diffraction%2520consistency%2520is%2520enforced%2520only%2520weakly.%2520Here%2520we%2520introduce%2520WPEM%252C%2520a%2520physics-constrained%2520whole-pattern%2520decomposition%2520and%2520refinement%2520workflow%2520that%2520turns%2520Bragg%2527s%2520law%2520into%2520an%2520explicit%2520constraint%2520within%2520a%2520batch%2520expectation--maximization%2520framework.%2520WPEM%2520models%2520the%2520full%2520profile%2520as%2520a%2520probabilistic%2520mixture%2520density%2520and%2520iteratively%2520infers%2520component-resolved%2520intensities%2520while%2520keeping%2520peak%2520centres%2520Bragg-consistent%252C%2520producing%2520a%2520continuous%252C%2520physically%2520admissible%2520intensity%2520representation%2520that%2520remains%2520stable%2520in%2520heavily%2520overlapped%2520regions%2520and%2520in%2520the%2520presence%2520of%2520mixed%2520radiation%2520or%2520multiple%2520phases.%2520We%2520benchmark%2520WPEM%2520on%2520standard%2520reference%2520patterns%2520%2528%255Cce%257BPbSO4%257D%2520and%2520%255Cce%257BTb2BaCoO5%257D%2529%252C%2520where%2520it%2520yields%2520lower%2520%2524R_%257B%255Cmathrm%257Bp%257D%257D%2524/%2524R_%257B%255Cmathrm%257Bwp%257D%257D%2524%2520than%2520widely%2520used%2520packages%2520%2528FullProf%2520and%2520TOPAS%2529%2520under%2520matched%2520refinement%2520conditions.%2520We%2520further%2520demonstrate%2520generality%2520across%2520realistic%2520experimental%2520scenarios%252C%2520including%2520phase-resolved%2520decomposition%2520of%2520a%2520multiphase%2520Ti--15Nb%2520thin%2520film%252C%2520quantitative%2520recovery%2520of%2520%255Cce%257BNaCl%257D--%255Cce%257BLi2CO3%257D%2520mixture%2520compositions%252C%2520separation%2520of%2520crystalline%2520peaks%2520from%2520amorphous%2520halos%2520in%2520semicrystalline%2520polymers%252C%2520high-throughput%2520operando%2520lattice%2520tracking%2520in%2520layered%2520cathodes%252C%2520automated%2520refinement%2520of%2520a%2520compositionally%2520disordered%2520Ru--Mn%2520oxide%2520solid%2520solution%2520%2528CCDC%25202530452%2529%252C%2520and%2520quantitative%2520phase-resolved%2520deciphering%2520of%2520an%2520ancient%2520Egyptian%2520make-up%2520sample%2520from%2520synchrotron%2520powder%2520XRD.%2520By%2520providing%2520Bragg-consistent%252C%2520uncertainty-aware%2520intensity%2520partitioning%2520as%2520a%2520refinement-ready%2520interface%252C%2520WPEM%2520closes%2520the%2520gap%2520between%2520AI-generated%2520hypotheses%2520and%2520diffraction-admissible%2520structure%2520refinement%2520on%2520challenging%2520XRD%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-Driven%20Structure%20Refinement%20of%20X-ray%20Diffraction&entry.906535625=Bin%20Cao%20and%20Qian%20Zhang%20and%20Zhenjie%20Feng%20and%20Taolue%20Zhang%20and%20Jiaqiang%20Huang%20and%20Lu-Tao%20Weng%20and%20Tong-Yi%20Zhang&entry.1292438233=Artificial%20intelligence%20can%20rapidly%20propose%20candidate%20phases%20and%20structures%20from%20X-ray%20diffraction%20%28XRD%29%2C%20but%20these%20hypotheses%20often%20fail%20in%20downstream%20refinement%20because%20peak%20intensities%20cannot%20be%20stably%20assigned%20under%20severe%20overlap%20and%20diffraction%20consistency%20is%20enforced%20only%20weakly.%20Here%20we%20introduce%20WPEM%2C%20a%20physics-constrained%20whole-pattern%20decomposition%20and%20refinement%20workflow%20that%20turns%20Bragg%27s%20law%20into%20an%20explicit%20constraint%20within%20a%20batch%20expectation--maximization%20framework.%20WPEM%20models%20the%20full%20profile%20as%20a%20probabilistic%20mixture%20density%20and%20iteratively%20infers%20component-resolved%20intensities%20while%20keeping%20peak%20centres%20Bragg-consistent%2C%20producing%20a%20continuous%2C%20physically%20admissible%20intensity%20representation%20that%20remains%20stable%20in%20heavily%20overlapped%20regions%20and%20in%20the%20presence%20of%20mixed%20radiation%20or%20multiple%20phases.%20We%20benchmark%20WPEM%20on%20standard%20reference%20patterns%20%28%5Cce%7BPbSO4%7D%20and%20%5Cce%7BTb2BaCoO5%7D%29%2C%20where%20it%20yields%20lower%20%24R_%7B%5Cmathrm%7Bp%7D%7D%24/%24R_%7B%5Cmathrm%7Bwp%7D%7D%24%20than%20widely%20used%20packages%20%28FullProf%20and%20TOPAS%29%20under%20matched%20refinement%20conditions.%20We%20further%20demonstrate%20generality%20across%20realistic%20experimental%20scenarios%2C%20including%20phase-resolved%20decomposition%20of%20a%20multiphase%20Ti--15Nb%20thin%20film%2C%20quantitative%20recovery%20of%20%5Cce%7BNaCl%7D--%5Cce%7BLi2CO3%7D%20mixture%20compositions%2C%20separation%20of%20crystalline%20peaks%20from%20amorphous%20halos%20in%20semicrystalline%20polymers%2C%20high-throughput%20operando%20lattice%20tracking%20in%20layered%20cathodes%2C%20automated%20refinement%20of%20a%20compositionally%20disordered%20Ru--Mn%20oxide%20solid%20solution%20%28CCDC%202530452%29%2C%20and%20quantitative%20phase-resolved%20deciphering%20of%20an%20ancient%20Egyptian%20make-up%20sample%20from%20synchrotron%20powder%20XRD.%20By%20providing%20Bragg-consistent%2C%20uncertainty-aware%20intensity%20partitioning%20as%20a%20refinement-ready%20interface%2C%20WPEM%20closes%20the%20gap%20between%20AI-generated%20hypotheses%20and%20diffraction-admissible%20structure%20refinement%20on%20challenging%20XRD%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.16372v1&entry.124074799=Read"},
{"title": "Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped", "author": "Saumya Karan and Neerav Maram and Suraj Borate and Madhu Vadali", "abstract": "SLOT (Soft Legged Omnidirectional Tetrapod), a tendon-driven soft quadruped robot with 3D-printed TPU legs, is presented to study physics-informed modeling and control of compliant legged locomotion using only four actuators. Each leg is modeled as a deformable continuum using discrete Cosserat rod theory, enabling the capture of large bending deformations, distributed elasticity, tendon actuation, and ground contact interactions. A modular whole-body modeling framework is introduced, in which compliant leg dynamics are represented through physically consistent reaction forces applied to a rigid torso, providing a scalable interface between continuum soft limbs and rigid-body locomotion dynamics. This formulation allows efficient whole-body simulation and real-time control without sacrificing physical fidelity. The proposed model is embedded into a convex model predictive control framework that optimizes ground reaction forces over a 0.495 s prediction horizon and maps them to tendon actuation through a physics-informed force-angle relationship. The resulting controller achieves asymptotic stability under diverse perturbations. The framework is experimentally validated on a physical prototype during crawling and walking gaits, achieving high accuracy with less than 5 mm RMSE in center of mass trajectories. These results demonstrate a generalizable approach for integrating continuum soft legs into model-based locomotion control, advancing scalable and reusable modeling and control methods for soft quadruped robots.", "link": "http://arxiv.org/abs/2602.16371v1", "date": "2026-02-18", "relevancy": 2.1258, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5841}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5357}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Modeling%20and%20MPC%20for%20Locomotion%20of%20Tendon-Driven%20Soft%20Quadruped&body=Title%3A%20Dynamic%20Modeling%20and%20MPC%20for%20Locomotion%20of%20Tendon-Driven%20Soft%20Quadruped%0AAuthor%3A%20Saumya%20Karan%20and%20Neerav%20Maram%20and%20Suraj%20Borate%20and%20Madhu%20Vadali%0AAbstract%3A%20SLOT%20%28Soft%20Legged%20Omnidirectional%20Tetrapod%29%2C%20a%20tendon-driven%20soft%20quadruped%20robot%20with%203D-printed%20TPU%20legs%2C%20is%20presented%20to%20study%20physics-informed%20modeling%20and%20control%20of%20compliant%20legged%20locomotion%20using%20only%20four%20actuators.%20Each%20leg%20is%20modeled%20as%20a%20deformable%20continuum%20using%20discrete%20Cosserat%20rod%20theory%2C%20enabling%20the%20capture%20of%20large%20bending%20deformations%2C%20distributed%20elasticity%2C%20tendon%20actuation%2C%20and%20ground%20contact%20interactions.%20A%20modular%20whole-body%20modeling%20framework%20is%20introduced%2C%20in%20which%20compliant%20leg%20dynamics%20are%20represented%20through%20physically%20consistent%20reaction%20forces%20applied%20to%20a%20rigid%20torso%2C%20providing%20a%20scalable%20interface%20between%20continuum%20soft%20limbs%20and%20rigid-body%20locomotion%20dynamics.%20This%20formulation%20allows%20efficient%20whole-body%20simulation%20and%20real-time%20control%20without%20sacrificing%20physical%20fidelity.%20The%20proposed%20model%20is%20embedded%20into%20a%20convex%20model%20predictive%20control%20framework%20that%20optimizes%20ground%20reaction%20forces%20over%20a%200.495%20s%20prediction%20horizon%20and%20maps%20them%20to%20tendon%20actuation%20through%20a%20physics-informed%20force-angle%20relationship.%20The%20resulting%20controller%20achieves%20asymptotic%20stability%20under%20diverse%20perturbations.%20The%20framework%20is%20experimentally%20validated%20on%20a%20physical%20prototype%20during%20crawling%20and%20walking%20gaits%2C%20achieving%20high%20accuracy%20with%20less%20than%205%20mm%20RMSE%20in%20center%20of%20mass%20trajectories.%20These%20results%20demonstrate%20a%20generalizable%20approach%20for%20integrating%20continuum%20soft%20legs%20into%20model-based%20locomotion%20control%2C%20advancing%20scalable%20and%20reusable%20modeling%20and%20control%20methods%20for%20soft%20quadruped%20robots.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Modeling%2520and%2520MPC%2520for%2520Locomotion%2520of%2520Tendon-Driven%2520Soft%2520Quadruped%26entry.906535625%3DSaumya%2520Karan%2520and%2520Neerav%2520Maram%2520and%2520Suraj%2520Borate%2520and%2520Madhu%2520Vadali%26entry.1292438233%3DSLOT%2520%2528Soft%2520Legged%2520Omnidirectional%2520Tetrapod%2529%252C%2520a%2520tendon-driven%2520soft%2520quadruped%2520robot%2520with%25203D-printed%2520TPU%2520legs%252C%2520is%2520presented%2520to%2520study%2520physics-informed%2520modeling%2520and%2520control%2520of%2520compliant%2520legged%2520locomotion%2520using%2520only%2520four%2520actuators.%2520Each%2520leg%2520is%2520modeled%2520as%2520a%2520deformable%2520continuum%2520using%2520discrete%2520Cosserat%2520rod%2520theory%252C%2520enabling%2520the%2520capture%2520of%2520large%2520bending%2520deformations%252C%2520distributed%2520elasticity%252C%2520tendon%2520actuation%252C%2520and%2520ground%2520contact%2520interactions.%2520A%2520modular%2520whole-body%2520modeling%2520framework%2520is%2520introduced%252C%2520in%2520which%2520compliant%2520leg%2520dynamics%2520are%2520represented%2520through%2520physically%2520consistent%2520reaction%2520forces%2520applied%2520to%2520a%2520rigid%2520torso%252C%2520providing%2520a%2520scalable%2520interface%2520between%2520continuum%2520soft%2520limbs%2520and%2520rigid-body%2520locomotion%2520dynamics.%2520This%2520formulation%2520allows%2520efficient%2520whole-body%2520simulation%2520and%2520real-time%2520control%2520without%2520sacrificing%2520physical%2520fidelity.%2520The%2520proposed%2520model%2520is%2520embedded%2520into%2520a%2520convex%2520model%2520predictive%2520control%2520framework%2520that%2520optimizes%2520ground%2520reaction%2520forces%2520over%2520a%25200.495%2520s%2520prediction%2520horizon%2520and%2520maps%2520them%2520to%2520tendon%2520actuation%2520through%2520a%2520physics-informed%2520force-angle%2520relationship.%2520The%2520resulting%2520controller%2520achieves%2520asymptotic%2520stability%2520under%2520diverse%2520perturbations.%2520The%2520framework%2520is%2520experimentally%2520validated%2520on%2520a%2520physical%2520prototype%2520during%2520crawling%2520and%2520walking%2520gaits%252C%2520achieving%2520high%2520accuracy%2520with%2520less%2520than%25205%2520mm%2520RMSE%2520in%2520center%2520of%2520mass%2520trajectories.%2520These%2520results%2520demonstrate%2520a%2520generalizable%2520approach%2520for%2520integrating%2520continuum%2520soft%2520legs%2520into%2520model-based%2520locomotion%2520control%252C%2520advancing%2520scalable%2520and%2520reusable%2520modeling%2520and%2520control%2520methods%2520for%2520soft%2520quadruped%2520robots.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Modeling%20and%20MPC%20for%20Locomotion%20of%20Tendon-Driven%20Soft%20Quadruped&entry.906535625=Saumya%20Karan%20and%20Neerav%20Maram%20and%20Suraj%20Borate%20and%20Madhu%20Vadali&entry.1292438233=SLOT%20%28Soft%20Legged%20Omnidirectional%20Tetrapod%29%2C%20a%20tendon-driven%20soft%20quadruped%20robot%20with%203D-printed%20TPU%20legs%2C%20is%20presented%20to%20study%20physics-informed%20modeling%20and%20control%20of%20compliant%20legged%20locomotion%20using%20only%20four%20actuators.%20Each%20leg%20is%20modeled%20as%20a%20deformable%20continuum%20using%20discrete%20Cosserat%20rod%20theory%2C%20enabling%20the%20capture%20of%20large%20bending%20deformations%2C%20distributed%20elasticity%2C%20tendon%20actuation%2C%20and%20ground%20contact%20interactions.%20A%20modular%20whole-body%20modeling%20framework%20is%20introduced%2C%20in%20which%20compliant%20leg%20dynamics%20are%20represented%20through%20physically%20consistent%20reaction%20forces%20applied%20to%20a%20rigid%20torso%2C%20providing%20a%20scalable%20interface%20between%20continuum%20soft%20limbs%20and%20rigid-body%20locomotion%20dynamics.%20This%20formulation%20allows%20efficient%20whole-body%20simulation%20and%20real-time%20control%20without%20sacrificing%20physical%20fidelity.%20The%20proposed%20model%20is%20embedded%20into%20a%20convex%20model%20predictive%20control%20framework%20that%20optimizes%20ground%20reaction%20forces%20over%20a%200.495%20s%20prediction%20horizon%20and%20maps%20them%20to%20tendon%20actuation%20through%20a%20physics-informed%20force-angle%20relationship.%20The%20resulting%20controller%20achieves%20asymptotic%20stability%20under%20diverse%20perturbations.%20The%20framework%20is%20experimentally%20validated%20on%20a%20physical%20prototype%20during%20crawling%20and%20walking%20gaits%2C%20achieving%20high%20accuracy%20with%20less%20than%205%20mm%20RMSE%20in%20center%20of%20mass%20trajectories.%20These%20results%20demonstrate%20a%20generalizable%20approach%20for%20integrating%20continuum%20soft%20legs%20into%20model-based%20locomotion%20control%2C%20advancing%20scalable%20and%20reusable%20modeling%20and%20control%20methods%20for%20soft%20quadruped%20robots.&entry.1838667208=http%3A//arxiv.org/abs/2602.16371v1&entry.124074799=Read"},
{"title": "Towards Autonomous Robotic Kidney Ultrasound: Spatial-Efficient Volumetric Imaging via Template Guided Optimal Pivoting", "author": "Xihan Ma and Haichong Zhang", "abstract": "Medical ultrasound (US) imaging is a frontline tool for the diagnosis of kidney diseases. However, traditional freehand imaging procedure suffers from inconsistent, operator-dependent outcomes, lack of 3D localization information, and risks of work-related musculoskeletal disorders. While robotic ultrasound (RUS) systems offer the potential for standardized, operator-independent 3D kidney data acquisition, the existing scanning methods lack the ability to determine the optimal imaging window for efficient imaging. As a result, the scan is often blindly performed with excessive probe footprint, which frequently leads to acoustic shadowing and incomplete organ coverage. Consequently, there is a critical need for a spatially efficient imaging technique that can maximize the kidney coverage through minimum probe footprint. Here, we propose an autonomous workflow to achieve efficient kidney imaging via template-guided optimal pivoting. The system first performs an explorative imaging to generate partial observations of the kidney. This data is then registered to a kidney template to estimate the organ pose. With the kidney localized, the robot executes a fixed-point pivoting sweep where the imaging plane is aligned with the kidney long axis to minimize the probe translation. The proposed method was validated in simulation and in-vivo. Simulation results indicate that a 60% exploration ratio provides optimal balance between kidney localization accuracy and scanning efficiency. In-vivo evaluation on two male subjects demonstrates a kidney localization accuracy up to 7.36 mm and 13.84 degrees. Moreover, the optimal pivoting approach shortened the probe footprint by around 75 mm when compared with the baselines. These results valid our approach of leveraging anatomical templates to align the probe optimally for volumetric sweep.", "link": "http://arxiv.org/abs/2602.16641v1", "date": "2026-02-18", "relevancy": 2.1178, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5297}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4844}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Autonomous%20Robotic%20Kidney%20Ultrasound%3A%20Spatial-Efficient%20Volumetric%20Imaging%20via%20Template%20Guided%20Optimal%20Pivoting&body=Title%3A%20Towards%20Autonomous%20Robotic%20Kidney%20Ultrasound%3A%20Spatial-Efficient%20Volumetric%20Imaging%20via%20Template%20Guided%20Optimal%20Pivoting%0AAuthor%3A%20Xihan%20Ma%20and%20Haichong%20Zhang%0AAbstract%3A%20Medical%20ultrasound%20%28US%29%20imaging%20is%20a%20frontline%20tool%20for%20the%20diagnosis%20of%20kidney%20diseases.%20However%2C%20traditional%20freehand%20imaging%20procedure%20suffers%20from%20inconsistent%2C%20operator-dependent%20outcomes%2C%20lack%20of%203D%20localization%20information%2C%20and%20risks%20of%20work-related%20musculoskeletal%20disorders.%20While%20robotic%20ultrasound%20%28RUS%29%20systems%20offer%20the%20potential%20for%20standardized%2C%20operator-independent%203D%20kidney%20data%20acquisition%2C%20the%20existing%20scanning%20methods%20lack%20the%20ability%20to%20determine%20the%20optimal%20imaging%20window%20for%20efficient%20imaging.%20As%20a%20result%2C%20the%20scan%20is%20often%20blindly%20performed%20with%20excessive%20probe%20footprint%2C%20which%20frequently%20leads%20to%20acoustic%20shadowing%20and%20incomplete%20organ%20coverage.%20Consequently%2C%20there%20is%20a%20critical%20need%20for%20a%20spatially%20efficient%20imaging%20technique%20that%20can%20maximize%20the%20kidney%20coverage%20through%20minimum%20probe%20footprint.%20Here%2C%20we%20propose%20an%20autonomous%20workflow%20to%20achieve%20efficient%20kidney%20imaging%20via%20template-guided%20optimal%20pivoting.%20The%20system%20first%20performs%20an%20explorative%20imaging%20to%20generate%20partial%20observations%20of%20the%20kidney.%20This%20data%20is%20then%20registered%20to%20a%20kidney%20template%20to%20estimate%20the%20organ%20pose.%20With%20the%20kidney%20localized%2C%20the%20robot%20executes%20a%20fixed-point%20pivoting%20sweep%20where%20the%20imaging%20plane%20is%20aligned%20with%20the%20kidney%20long%20axis%20to%20minimize%20the%20probe%20translation.%20The%20proposed%20method%20was%20validated%20in%20simulation%20and%20in-vivo.%20Simulation%20results%20indicate%20that%20a%2060%25%20exploration%20ratio%20provides%20optimal%20balance%20between%20kidney%20localization%20accuracy%20and%20scanning%20efficiency.%20In-vivo%20evaluation%20on%20two%20male%20subjects%20demonstrates%20a%20kidney%20localization%20accuracy%20up%20to%207.36%20mm%20and%2013.84%20degrees.%20Moreover%2C%20the%20optimal%20pivoting%20approach%20shortened%20the%20probe%20footprint%20by%20around%2075%20mm%20when%20compared%20with%20the%20baselines.%20These%20results%20valid%20our%20approach%20of%20leveraging%20anatomical%20templates%20to%20align%20the%20probe%20optimally%20for%20volumetric%20sweep.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Autonomous%2520Robotic%2520Kidney%2520Ultrasound%253A%2520Spatial-Efficient%2520Volumetric%2520Imaging%2520via%2520Template%2520Guided%2520Optimal%2520Pivoting%26entry.906535625%3DXihan%2520Ma%2520and%2520Haichong%2520Zhang%26entry.1292438233%3DMedical%2520ultrasound%2520%2528US%2529%2520imaging%2520is%2520a%2520frontline%2520tool%2520for%2520the%2520diagnosis%2520of%2520kidney%2520diseases.%2520However%252C%2520traditional%2520freehand%2520imaging%2520procedure%2520suffers%2520from%2520inconsistent%252C%2520operator-dependent%2520outcomes%252C%2520lack%2520of%25203D%2520localization%2520information%252C%2520and%2520risks%2520of%2520work-related%2520musculoskeletal%2520disorders.%2520While%2520robotic%2520ultrasound%2520%2528RUS%2529%2520systems%2520offer%2520the%2520potential%2520for%2520standardized%252C%2520operator-independent%25203D%2520kidney%2520data%2520acquisition%252C%2520the%2520existing%2520scanning%2520methods%2520lack%2520the%2520ability%2520to%2520determine%2520the%2520optimal%2520imaging%2520window%2520for%2520efficient%2520imaging.%2520As%2520a%2520result%252C%2520the%2520scan%2520is%2520often%2520blindly%2520performed%2520with%2520excessive%2520probe%2520footprint%252C%2520which%2520frequently%2520leads%2520to%2520acoustic%2520shadowing%2520and%2520incomplete%2520organ%2520coverage.%2520Consequently%252C%2520there%2520is%2520a%2520critical%2520need%2520for%2520a%2520spatially%2520efficient%2520imaging%2520technique%2520that%2520can%2520maximize%2520the%2520kidney%2520coverage%2520through%2520minimum%2520probe%2520footprint.%2520Here%252C%2520we%2520propose%2520an%2520autonomous%2520workflow%2520to%2520achieve%2520efficient%2520kidney%2520imaging%2520via%2520template-guided%2520optimal%2520pivoting.%2520The%2520system%2520first%2520performs%2520an%2520explorative%2520imaging%2520to%2520generate%2520partial%2520observations%2520of%2520the%2520kidney.%2520This%2520data%2520is%2520then%2520registered%2520to%2520a%2520kidney%2520template%2520to%2520estimate%2520the%2520organ%2520pose.%2520With%2520the%2520kidney%2520localized%252C%2520the%2520robot%2520executes%2520a%2520fixed-point%2520pivoting%2520sweep%2520where%2520the%2520imaging%2520plane%2520is%2520aligned%2520with%2520the%2520kidney%2520long%2520axis%2520to%2520minimize%2520the%2520probe%2520translation.%2520The%2520proposed%2520method%2520was%2520validated%2520in%2520simulation%2520and%2520in-vivo.%2520Simulation%2520results%2520indicate%2520that%2520a%252060%2525%2520exploration%2520ratio%2520provides%2520optimal%2520balance%2520between%2520kidney%2520localization%2520accuracy%2520and%2520scanning%2520efficiency.%2520In-vivo%2520evaluation%2520on%2520two%2520male%2520subjects%2520demonstrates%2520a%2520kidney%2520localization%2520accuracy%2520up%2520to%25207.36%2520mm%2520and%252013.84%2520degrees.%2520Moreover%252C%2520the%2520optimal%2520pivoting%2520approach%2520shortened%2520the%2520probe%2520footprint%2520by%2520around%252075%2520mm%2520when%2520compared%2520with%2520the%2520baselines.%2520These%2520results%2520valid%2520our%2520approach%2520of%2520leveraging%2520anatomical%2520templates%2520to%2520align%2520the%2520probe%2520optimally%2520for%2520volumetric%2520sweep.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Autonomous%20Robotic%20Kidney%20Ultrasound%3A%20Spatial-Efficient%20Volumetric%20Imaging%20via%20Template%20Guided%20Optimal%20Pivoting&entry.906535625=Xihan%20Ma%20and%20Haichong%20Zhang&entry.1292438233=Medical%20ultrasound%20%28US%29%20imaging%20is%20a%20frontline%20tool%20for%20the%20diagnosis%20of%20kidney%20diseases.%20However%2C%20traditional%20freehand%20imaging%20procedure%20suffers%20from%20inconsistent%2C%20operator-dependent%20outcomes%2C%20lack%20of%203D%20localization%20information%2C%20and%20risks%20of%20work-related%20musculoskeletal%20disorders.%20While%20robotic%20ultrasound%20%28RUS%29%20systems%20offer%20the%20potential%20for%20standardized%2C%20operator-independent%203D%20kidney%20data%20acquisition%2C%20the%20existing%20scanning%20methods%20lack%20the%20ability%20to%20determine%20the%20optimal%20imaging%20window%20for%20efficient%20imaging.%20As%20a%20result%2C%20the%20scan%20is%20often%20blindly%20performed%20with%20excessive%20probe%20footprint%2C%20which%20frequently%20leads%20to%20acoustic%20shadowing%20and%20incomplete%20organ%20coverage.%20Consequently%2C%20there%20is%20a%20critical%20need%20for%20a%20spatially%20efficient%20imaging%20technique%20that%20can%20maximize%20the%20kidney%20coverage%20through%20minimum%20probe%20footprint.%20Here%2C%20we%20propose%20an%20autonomous%20workflow%20to%20achieve%20efficient%20kidney%20imaging%20via%20template-guided%20optimal%20pivoting.%20The%20system%20first%20performs%20an%20explorative%20imaging%20to%20generate%20partial%20observations%20of%20the%20kidney.%20This%20data%20is%20then%20registered%20to%20a%20kidney%20template%20to%20estimate%20the%20organ%20pose.%20With%20the%20kidney%20localized%2C%20the%20robot%20executes%20a%20fixed-point%20pivoting%20sweep%20where%20the%20imaging%20plane%20is%20aligned%20with%20the%20kidney%20long%20axis%20to%20minimize%20the%20probe%20translation.%20The%20proposed%20method%20was%20validated%20in%20simulation%20and%20in-vivo.%20Simulation%20results%20indicate%20that%20a%2060%25%20exploration%20ratio%20provides%20optimal%20balance%20between%20kidney%20localization%20accuracy%20and%20scanning%20efficiency.%20In-vivo%20evaluation%20on%20two%20male%20subjects%20demonstrates%20a%20kidney%20localization%20accuracy%20up%20to%207.36%20mm%20and%2013.84%20degrees.%20Moreover%2C%20the%20optimal%20pivoting%20approach%20shortened%20the%20probe%20footprint%20by%20around%2075%20mm%20when%20compared%20with%20the%20baselines.%20These%20results%20valid%20our%20approach%20of%20leveraging%20anatomical%20templates%20to%20align%20the%20probe%20optimally%20for%20volumetric%20sweep.&entry.1838667208=http%3A//arxiv.org/abs/2602.16641v1&entry.124074799=Read"},
{"title": "Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis", "author": "Houtan Ghaffari and Lukas Rauch and Paul Devos", "abstract": "Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.", "link": "http://arxiv.org/abs/2511.12158v2", "date": "2026-02-18", "relevancy": 2.1139, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5455}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5287}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Self-Supervised%20Algorithms%20for%20Fine-Grained%20Birdsong%20Analysis&body=Title%3A%20Data-Efficient%20Self-Supervised%20Algorithms%20for%20Fine-Grained%20Birdsong%20Analysis%0AAuthor%3A%20Houtan%20Ghaffari%20and%20Lukas%20Rauch%20and%20Paul%20Devos%0AAbstract%3A%20Many%20bioacoustics%2C%20neuroscience%2C%20and%20linguistics%20research%20utilize%20birdsongs%20as%20proxy%20models%20to%20acquire%20knowledge%20in%20diverse%20areas.%20Developing%20models%20generally%20requires%20precisely%20annotated%20data%20at%20the%20level%20of%20syllables.%20Hence%2C%20automated%20and%20data-efficient%20methods%20that%20reduce%20annotation%20costs%20are%20in%20demand.%20This%20work%20presents%20a%20lightweight%2C%20yet%20performant%20neural%20network%20architecture%20for%20birdsong%20annotation%20called%20Residual-MLP-RNN.%20Then%2C%20it%20presents%20a%20robust%20three-stage%20training%20pipeline%20for%20developing%20reliable%20deep%20birdsong%20syllable%20detectors%20with%20minimal%20expert%20labor.%20The%20first%20stage%20is%20self-supervised%20learning%20from%20unlabeled%20data.%20Two%20of%20the%20most%20successful%20pretraining%20paradigms%20are%20explored%2C%20namely%2C%20masked%20prediction%20and%20online%20clustering.%20The%20second%20stage%20is%20supervised%20training%20with%20effective%20data%20augmentations%20to%20create%20a%20robust%20model%20for%20frame-level%20syllable%20detection.%20The%20third%20stage%20is%20semi-supervised%20post-training%2C%20which%20leverages%20the%20unlabeled%20data%20again.%20However%2C%20unlike%20the%20initial%20phase%2C%20this%20time%20it%20is%20aligned%20with%20the%20downstream%20task.%20The%20performance%20of%20this%20data-efficient%20approach%20is%20demonstrated%20for%20the%20complex%20song%20of%20the%20Canary%20in%20extreme%20label-scarcity%20scenarios.%20Canary%20has%20one%20of%20the%20most%20difficult%20songs%20to%20annotate%2C%20which%20implicitly%20validates%20the%20method%20for%20other%20birds.%20Finally%2C%20the%20potential%20of%20self-supervised%20embeddings%20is%20assessed%20for%20linear%20probing%20and%20unsupervised%20birdsong%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12158v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Efficient%2520Self-Supervised%2520Algorithms%2520for%2520Fine-Grained%2520Birdsong%2520Analysis%26entry.906535625%3DHoutan%2520Ghaffari%2520and%2520Lukas%2520Rauch%2520and%2520Paul%2520Devos%26entry.1292438233%3DMany%2520bioacoustics%252C%2520neuroscience%252C%2520and%2520linguistics%2520research%2520utilize%2520birdsongs%2520as%2520proxy%2520models%2520to%2520acquire%2520knowledge%2520in%2520diverse%2520areas.%2520Developing%2520models%2520generally%2520requires%2520precisely%2520annotated%2520data%2520at%2520the%2520level%2520of%2520syllables.%2520Hence%252C%2520automated%2520and%2520data-efficient%2520methods%2520that%2520reduce%2520annotation%2520costs%2520are%2520in%2520demand.%2520This%2520work%2520presents%2520a%2520lightweight%252C%2520yet%2520performant%2520neural%2520network%2520architecture%2520for%2520birdsong%2520annotation%2520called%2520Residual-MLP-RNN.%2520Then%252C%2520it%2520presents%2520a%2520robust%2520three-stage%2520training%2520pipeline%2520for%2520developing%2520reliable%2520deep%2520birdsong%2520syllable%2520detectors%2520with%2520minimal%2520expert%2520labor.%2520The%2520first%2520stage%2520is%2520self-supervised%2520learning%2520from%2520unlabeled%2520data.%2520Two%2520of%2520the%2520most%2520successful%2520pretraining%2520paradigms%2520are%2520explored%252C%2520namely%252C%2520masked%2520prediction%2520and%2520online%2520clustering.%2520The%2520second%2520stage%2520is%2520supervised%2520training%2520with%2520effective%2520data%2520augmentations%2520to%2520create%2520a%2520robust%2520model%2520for%2520frame-level%2520syllable%2520detection.%2520The%2520third%2520stage%2520is%2520semi-supervised%2520post-training%252C%2520which%2520leverages%2520the%2520unlabeled%2520data%2520again.%2520However%252C%2520unlike%2520the%2520initial%2520phase%252C%2520this%2520time%2520it%2520is%2520aligned%2520with%2520the%2520downstream%2520task.%2520The%2520performance%2520of%2520this%2520data-efficient%2520approach%2520is%2520demonstrated%2520for%2520the%2520complex%2520song%2520of%2520the%2520Canary%2520in%2520extreme%2520label-scarcity%2520scenarios.%2520Canary%2520has%2520one%2520of%2520the%2520most%2520difficult%2520songs%2520to%2520annotate%252C%2520which%2520implicitly%2520validates%2520the%2520method%2520for%2520other%2520birds.%2520Finally%252C%2520the%2520potential%2520of%2520self-supervised%2520embeddings%2520is%2520assessed%2520for%2520linear%2520probing%2520and%2520unsupervised%2520birdsong%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12158v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Self-Supervised%20Algorithms%20for%20Fine-Grained%20Birdsong%20Analysis&entry.906535625=Houtan%20Ghaffari%20and%20Lukas%20Rauch%20and%20Paul%20Devos&entry.1292438233=Many%20bioacoustics%2C%20neuroscience%2C%20and%20linguistics%20research%20utilize%20birdsongs%20as%20proxy%20models%20to%20acquire%20knowledge%20in%20diverse%20areas.%20Developing%20models%20generally%20requires%20precisely%20annotated%20data%20at%20the%20level%20of%20syllables.%20Hence%2C%20automated%20and%20data-efficient%20methods%20that%20reduce%20annotation%20costs%20are%20in%20demand.%20This%20work%20presents%20a%20lightweight%2C%20yet%20performant%20neural%20network%20architecture%20for%20birdsong%20annotation%20called%20Residual-MLP-RNN.%20Then%2C%20it%20presents%20a%20robust%20three-stage%20training%20pipeline%20for%20developing%20reliable%20deep%20birdsong%20syllable%20detectors%20with%20minimal%20expert%20labor.%20The%20first%20stage%20is%20self-supervised%20learning%20from%20unlabeled%20data.%20Two%20of%20the%20most%20successful%20pretraining%20paradigms%20are%20explored%2C%20namely%2C%20masked%20prediction%20and%20online%20clustering.%20The%20second%20stage%20is%20supervised%20training%20with%20effective%20data%20augmentations%20to%20create%20a%20robust%20model%20for%20frame-level%20syllable%20detection.%20The%20third%20stage%20is%20semi-supervised%20post-training%2C%20which%20leverages%20the%20unlabeled%20data%20again.%20However%2C%20unlike%20the%20initial%20phase%2C%20this%20time%20it%20is%20aligned%20with%20the%20downstream%20task.%20The%20performance%20of%20this%20data-efficient%20approach%20is%20demonstrated%20for%20the%20complex%20song%20of%20the%20Canary%20in%20extreme%20label-scarcity%20scenarios.%20Canary%20has%20one%20of%20the%20most%20difficult%20songs%20to%20annotate%2C%20which%20implicitly%20validates%20the%20method%20for%20other%20birds.%20Finally%2C%20the%20potential%20of%20self-supervised%20embeddings%20is%20assessed%20for%20linear%20probing%20and%20unsupervised%20birdsong%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2511.12158v2&entry.124074799=Read"},
{"title": "Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face", "author": "Nicol\u00f2 Di Domenico and Annalisa Franco and Matteo Ferrara and Davide Maltoni", "abstract": "Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.", "link": "http://arxiv.org/abs/2602.16569v1", "date": "2026-02-18", "relevancy": 2.0914, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5417}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5166}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arc2Morph%3A%20Identity-Preserving%20Facial%20Morphing%20with%20Arc2Face&body=Title%3A%20Arc2Morph%3A%20Identity-Preserving%20Facial%20Morphing%20with%20Arc2Face%0AAuthor%3A%20Nicol%C3%B2%20Di%20Domenico%20and%20Annalisa%20Franco%20and%20Matteo%20Ferrara%20and%20Davide%20Maltoni%0AAbstract%3A%20Face%20morphing%20attacks%20are%20widely%20recognized%20as%20one%20of%20the%20most%20challenging%20threats%20to%20face%20recognition%20systems%20used%20in%20electronic%20identity%20documents.%20These%20attacks%20exploit%20a%20critical%20vulnerability%20in%20passport%20enrollment%20procedures%20adopted%20by%20many%20countries%2C%20where%20the%20facial%20image%20is%20often%20acquired%20without%20a%20supervised%20live%20capture%20process.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20face%20morphing%20technique%20based%20on%20Arc2Face%2C%20an%20identity-conditioned%20face%20foundation%20model%20capable%20of%20synthesizing%20photorealistic%20facial%20images%20from%20compact%20identity%20representations.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20by%20comparing%20the%20morphing%20attack%20potential%20metric%20on%20two%20large-scale%20sequestered%20face%20morphing%20attack%20detection%20datasets%20against%20several%20state-of-the-art%20morphing%20methods%2C%20as%20well%20as%20on%20two%20novel%20morphed%20face%20datasets%20derived%20from%20FEI%20and%20ONOT.%20Experimental%20results%20show%20that%20the%20proposed%20deep%20learning-based%20approach%20achieves%20a%20morphing%20attack%20potential%20comparable%20to%20that%20of%20landmark-based%20techniques%2C%20which%20have%20traditionally%20been%20regarded%20as%20the%20most%20challenging.%20These%20findings%20confirm%20the%20ability%20of%20the%20proposed%20method%20to%20effectively%20preserve%20and%20manage%20identity%20information%20during%20the%20morph%20generation%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArc2Morph%253A%2520Identity-Preserving%2520Facial%2520Morphing%2520with%2520Arc2Face%26entry.906535625%3DNicol%25C3%25B2%2520Di%2520Domenico%2520and%2520Annalisa%2520Franco%2520and%2520Matteo%2520Ferrara%2520and%2520Davide%2520Maltoni%26entry.1292438233%3DFace%2520morphing%2520attacks%2520are%2520widely%2520recognized%2520as%2520one%2520of%2520the%2520most%2520challenging%2520threats%2520to%2520face%2520recognition%2520systems%2520used%2520in%2520electronic%2520identity%2520documents.%2520These%2520attacks%2520exploit%2520a%2520critical%2520vulnerability%2520in%2520passport%2520enrollment%2520procedures%2520adopted%2520by%2520many%2520countries%252C%2520where%2520the%2520facial%2520image%2520is%2520often%2520acquired%2520without%2520a%2520supervised%2520live%2520capture%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520face%2520morphing%2520technique%2520based%2520on%2520Arc2Face%252C%2520an%2520identity-conditioned%2520face%2520foundation%2520model%2520capable%2520of%2520synthesizing%2520photorealistic%2520facial%2520images%2520from%2520compact%2520identity%2520representations.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520by%2520comparing%2520the%2520morphing%2520attack%2520potential%2520metric%2520on%2520two%2520large-scale%2520sequestered%2520face%2520morphing%2520attack%2520detection%2520datasets%2520against%2520several%2520state-of-the-art%2520morphing%2520methods%252C%2520as%2520well%2520as%2520on%2520two%2520novel%2520morphed%2520face%2520datasets%2520derived%2520from%2520FEI%2520and%2520ONOT.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520deep%2520learning-based%2520approach%2520achieves%2520a%2520morphing%2520attack%2520potential%2520comparable%2520to%2520that%2520of%2520landmark-based%2520techniques%252C%2520which%2520have%2520traditionally%2520been%2520regarded%2520as%2520the%2520most%2520challenging.%2520These%2520findings%2520confirm%2520the%2520ability%2520of%2520the%2520proposed%2520method%2520to%2520effectively%2520preserve%2520and%2520manage%2520identity%2520information%2520during%2520the%2520morph%2520generation%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arc2Morph%3A%20Identity-Preserving%20Facial%20Morphing%20with%20Arc2Face&entry.906535625=Nicol%C3%B2%20Di%20Domenico%20and%20Annalisa%20Franco%20and%20Matteo%20Ferrara%20and%20Davide%20Maltoni&entry.1292438233=Face%20morphing%20attacks%20are%20widely%20recognized%20as%20one%20of%20the%20most%20challenging%20threats%20to%20face%20recognition%20systems%20used%20in%20electronic%20identity%20documents.%20These%20attacks%20exploit%20a%20critical%20vulnerability%20in%20passport%20enrollment%20procedures%20adopted%20by%20many%20countries%2C%20where%20the%20facial%20image%20is%20often%20acquired%20without%20a%20supervised%20live%20capture%20process.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20face%20morphing%20technique%20based%20on%20Arc2Face%2C%20an%20identity-conditioned%20face%20foundation%20model%20capable%20of%20synthesizing%20photorealistic%20facial%20images%20from%20compact%20identity%20representations.%20We%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20by%20comparing%20the%20morphing%20attack%20potential%20metric%20on%20two%20large-scale%20sequestered%20face%20morphing%20attack%20detection%20datasets%20against%20several%20state-of-the-art%20morphing%20methods%2C%20as%20well%20as%20on%20two%20novel%20morphed%20face%20datasets%20derived%20from%20FEI%20and%20ONOT.%20Experimental%20results%20show%20that%20the%20proposed%20deep%20learning-based%20approach%20achieves%20a%20morphing%20attack%20potential%20comparable%20to%20that%20of%20landmark-based%20techniques%2C%20which%20have%20traditionally%20been%20regarded%20as%20the%20most%20challenging.%20These%20findings%20confirm%20the%20ability%20of%20the%20proposed%20method%20to%20effectively%20preserve%20and%20manage%20identity%20information%20during%20the%20morph%20generation%20process.&entry.1838667208=http%3A//arxiv.org/abs/2602.16569v1&entry.124074799=Read"},
{"title": "Ultra-wideband Time Difference of Arrival Indoor Localization: From Sensor Placement to System Evaluation", "author": "Wenda Zhao and Abhishek Goudar and Mingliang Tang and Angela P. Schoellig", "abstract": "Wireless indoor localization has attracted significant research interest due to its high accuracy, low cost, lightweight design, and low power consumption. Specifically, ultra-wideband (UWB) time difference of arrival (TDOA)-based localization has emerged as a scalable positioning solution for mobile robots, consumer electronics, and wearable devices, featuring good accuracy and reliability. While UWB TDOA-based localization systems rely on the deployment of UWB radio sensors as positioning landmarks, existing works often assume these placements are predetermined or study the sensor placement problem alone without evaluating it in practical scenarios. In this article, we bridge this gap by approaching the UWB TDOA localization from a system-level perspective, integrating sensor placement as a key component and conducting practical evaluation in real-world scenarios. Through extensive real-world experiments, we demonstrate the accuracy and robustness of our localization system, comparing its performance to the theoretical lower bounds. Using a challenging multi-room environment as a case study, we illustrate the full system construction process, from sensor placement optimization to real-world deployment. Our evaluation, comprising a cumulative total of 39 minutes of real-world experiments involving up to five agents and covering 2608 meters across four distinct scenarios, provides valuable insights and guidelines for constructing UWB TDOA localization systems.", "link": "http://arxiv.org/abs/2412.12427v2", "date": "2026-02-18", "relevancy": 2.0899, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5465}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5141}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-wideband%20Time%20Difference%20of%20Arrival%20Indoor%20Localization%3A%20From%20Sensor%20Placement%20to%20System%20Evaluation&body=Title%3A%20Ultra-wideband%20Time%20Difference%20of%20Arrival%20Indoor%20Localization%3A%20From%20Sensor%20Placement%20to%20System%20Evaluation%0AAuthor%3A%20Wenda%20Zhao%20and%20Abhishek%20Goudar%20and%20Mingliang%20Tang%20and%20Angela%20P.%20Schoellig%0AAbstract%3A%20Wireless%20indoor%20localization%20has%20attracted%20significant%20research%20interest%20due%20to%20its%20high%20accuracy%2C%20low%20cost%2C%20lightweight%20design%2C%20and%20low%20power%20consumption.%20Specifically%2C%20ultra-wideband%20%28UWB%29%20time%20difference%20of%20arrival%20%28TDOA%29-based%20localization%20has%20emerged%20as%20a%20scalable%20positioning%20solution%20for%20mobile%20robots%2C%20consumer%20electronics%2C%20and%20wearable%20devices%2C%20featuring%20good%20accuracy%20and%20reliability.%20While%20UWB%20TDOA-based%20localization%20systems%20rely%20on%20the%20deployment%20of%20UWB%20radio%20sensors%20as%20positioning%20landmarks%2C%20existing%20works%20often%20assume%20these%20placements%20are%20predetermined%20or%20study%20the%20sensor%20placement%20problem%20alone%20without%20evaluating%20it%20in%20practical%20scenarios.%20In%20this%20article%2C%20we%20bridge%20this%20gap%20by%20approaching%20the%20UWB%20TDOA%20localization%20from%20a%20system-level%20perspective%2C%20integrating%20sensor%20placement%20as%20a%20key%20component%20and%20conducting%20practical%20evaluation%20in%20real-world%20scenarios.%20Through%20extensive%20real-world%20experiments%2C%20we%20demonstrate%20the%20accuracy%20and%20robustness%20of%20our%20localization%20system%2C%20comparing%20its%20performance%20to%20the%20theoretical%20lower%20bounds.%20Using%20a%20challenging%20multi-room%20environment%20as%20a%20case%20study%2C%20we%20illustrate%20the%20full%20system%20construction%20process%2C%20from%20sensor%20placement%20optimization%20to%20real-world%20deployment.%20Our%20evaluation%2C%20comprising%20a%20cumulative%20total%20of%2039%20minutes%20of%20real-world%20experiments%20involving%20up%20to%20five%20agents%20and%20covering%202608%20meters%20across%20four%20distinct%20scenarios%2C%20provides%20valuable%20insights%20and%20guidelines%20for%20constructing%20UWB%20TDOA%20localization%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2412.12427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-wideband%2520Time%2520Difference%2520of%2520Arrival%2520Indoor%2520Localization%253A%2520From%2520Sensor%2520Placement%2520to%2520System%2520Evaluation%26entry.906535625%3DWenda%2520Zhao%2520and%2520Abhishek%2520Goudar%2520and%2520Mingliang%2520Tang%2520and%2520Angela%2520P.%2520Schoellig%26entry.1292438233%3DWireless%2520indoor%2520localization%2520has%2520attracted%2520significant%2520research%2520interest%2520due%2520to%2520its%2520high%2520accuracy%252C%2520low%2520cost%252C%2520lightweight%2520design%252C%2520and%2520low%2520power%2520consumption.%2520Specifically%252C%2520ultra-wideband%2520%2528UWB%2529%2520time%2520difference%2520of%2520arrival%2520%2528TDOA%2529-based%2520localization%2520has%2520emerged%2520as%2520a%2520scalable%2520positioning%2520solution%2520for%2520mobile%2520robots%252C%2520consumer%2520electronics%252C%2520and%2520wearable%2520devices%252C%2520featuring%2520good%2520accuracy%2520and%2520reliability.%2520While%2520UWB%2520TDOA-based%2520localization%2520systems%2520rely%2520on%2520the%2520deployment%2520of%2520UWB%2520radio%2520sensors%2520as%2520positioning%2520landmarks%252C%2520existing%2520works%2520often%2520assume%2520these%2520placements%2520are%2520predetermined%2520or%2520study%2520the%2520sensor%2520placement%2520problem%2520alone%2520without%2520evaluating%2520it%2520in%2520practical%2520scenarios.%2520In%2520this%2520article%252C%2520we%2520bridge%2520this%2520gap%2520by%2520approaching%2520the%2520UWB%2520TDOA%2520localization%2520from%2520a%2520system-level%2520perspective%252C%2520integrating%2520sensor%2520placement%2520as%2520a%2520key%2520component%2520and%2520conducting%2520practical%2520evaluation%2520in%2520real-world%2520scenarios.%2520Through%2520extensive%2520real-world%2520experiments%252C%2520we%2520demonstrate%2520the%2520accuracy%2520and%2520robustness%2520of%2520our%2520localization%2520system%252C%2520comparing%2520its%2520performance%2520to%2520the%2520theoretical%2520lower%2520bounds.%2520Using%2520a%2520challenging%2520multi-room%2520environment%2520as%2520a%2520case%2520study%252C%2520we%2520illustrate%2520the%2520full%2520system%2520construction%2520process%252C%2520from%2520sensor%2520placement%2520optimization%2520to%2520real-world%2520deployment.%2520Our%2520evaluation%252C%2520comprising%2520a%2520cumulative%2520total%2520of%252039%2520minutes%2520of%2520real-world%2520experiments%2520involving%2520up%2520to%2520five%2520agents%2520and%2520covering%25202608%2520meters%2520across%2520four%2520distinct%2520scenarios%252C%2520provides%2520valuable%2520insights%2520and%2520guidelines%2520for%2520constructing%2520UWB%2520TDOA%2520localization%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-wideband%20Time%20Difference%20of%20Arrival%20Indoor%20Localization%3A%20From%20Sensor%20Placement%20to%20System%20Evaluation&entry.906535625=Wenda%20Zhao%20and%20Abhishek%20Goudar%20and%20Mingliang%20Tang%20and%20Angela%20P.%20Schoellig&entry.1292438233=Wireless%20indoor%20localization%20has%20attracted%20significant%20research%20interest%20due%20to%20its%20high%20accuracy%2C%20low%20cost%2C%20lightweight%20design%2C%20and%20low%20power%20consumption.%20Specifically%2C%20ultra-wideband%20%28UWB%29%20time%20difference%20of%20arrival%20%28TDOA%29-based%20localization%20has%20emerged%20as%20a%20scalable%20positioning%20solution%20for%20mobile%20robots%2C%20consumer%20electronics%2C%20and%20wearable%20devices%2C%20featuring%20good%20accuracy%20and%20reliability.%20While%20UWB%20TDOA-based%20localization%20systems%20rely%20on%20the%20deployment%20of%20UWB%20radio%20sensors%20as%20positioning%20landmarks%2C%20existing%20works%20often%20assume%20these%20placements%20are%20predetermined%20or%20study%20the%20sensor%20placement%20problem%20alone%20without%20evaluating%20it%20in%20practical%20scenarios.%20In%20this%20article%2C%20we%20bridge%20this%20gap%20by%20approaching%20the%20UWB%20TDOA%20localization%20from%20a%20system-level%20perspective%2C%20integrating%20sensor%20placement%20as%20a%20key%20component%20and%20conducting%20practical%20evaluation%20in%20real-world%20scenarios.%20Through%20extensive%20real-world%20experiments%2C%20we%20demonstrate%20the%20accuracy%20and%20robustness%20of%20our%20localization%20system%2C%20comparing%20its%20performance%20to%20the%20theoretical%20lower%20bounds.%20Using%20a%20challenging%20multi-room%20environment%20as%20a%20case%20study%2C%20we%20illustrate%20the%20full%20system%20construction%20process%2C%20from%20sensor%20placement%20optimization%20to%20real-world%20deployment.%20Our%20evaluation%2C%20comprising%20a%20cumulative%20total%20of%2039%20minutes%20of%20real-world%20experiments%20involving%20up%20to%20five%20agents%20and%20covering%202608%20meters%20across%20four%20distinct%20scenarios%2C%20provides%20valuable%20insights%20and%20guidelines%20for%20constructing%20UWB%20TDOA%20localization%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2412.12427v2&entry.124074799=Read"},
{"title": "Model-Agnostic Dynamic Feature Selection with Uncertainty Quantification", "author": "Javier Fumanal-Idocin and Raquel Fernandez-Peralta and Javier Andreu-Perez", "abstract": "Dynamic feature selection (DFS) addresses budget constraints in decision-making by sequentially acquiring features for each instance, making it appealing for resource-limited scenarios. However, existing DFS methods require models specifically designed for the sequential acquisition setting, limiting compatibility with models already deployed in practice. Furthermore, they provide limited uncertainty quantification, undermining trust in high-stakes decisions. In this work, we show that DFS introduces new uncertainty sources compared to the static setting. We formalise how model adaptation to feature subsets induces epistemic uncertainty, how standard imputation strategies bias aleatoric uncertainty estimation, and why predictive confidence fails to discriminate between good and bad selection policies. We also propose a model-agnostic DFS framework compatible with pre-trained classifiers, including interpretable-by-design models, through efficient subset reparametrization strategies. Empirical evaluation on tabular and image datasets demonstrates competitive accuracy against state-of-the-art greedy and reinforcement learning-based DFS methods with both neural and rule-based classifiers. We further show that the identified uncertainty sources persist across most existing approaches, highlighting the need for uncertainty-aware DFS.", "link": "http://arxiv.org/abs/2508.02566v3", "date": "2026-02-18", "relevancy": 2.0805, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5275}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.52}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-Agnostic%20Dynamic%20Feature%20Selection%20with%20Uncertainty%20Quantification&body=Title%3A%20Model-Agnostic%20Dynamic%20Feature%20Selection%20with%20Uncertainty%20Quantification%0AAuthor%3A%20Javier%20Fumanal-Idocin%20and%20Raquel%20Fernandez-Peralta%20and%20Javier%20Andreu-Perez%0AAbstract%3A%20Dynamic%20feature%20selection%20%28DFS%29%20addresses%20budget%20constraints%20in%20decision-making%20by%20sequentially%20acquiring%20features%20for%20each%20instance%2C%20making%20it%20appealing%20for%20resource-limited%20scenarios.%20However%2C%20existing%20DFS%20methods%20require%20models%20specifically%20designed%20for%20the%20sequential%20acquisition%20setting%2C%20limiting%20compatibility%20with%20models%20already%20deployed%20in%20practice.%20Furthermore%2C%20they%20provide%20limited%20uncertainty%20quantification%2C%20undermining%20trust%20in%20high-stakes%20decisions.%20In%20this%20work%2C%20we%20show%20that%20DFS%20introduces%20new%20uncertainty%20sources%20compared%20to%20the%20static%20setting.%20We%20formalise%20how%20model%20adaptation%20to%20feature%20subsets%20induces%20epistemic%20uncertainty%2C%20how%20standard%20imputation%20strategies%20bias%20aleatoric%20uncertainty%20estimation%2C%20and%20why%20predictive%20confidence%20fails%20to%20discriminate%20between%20good%20and%20bad%20selection%20policies.%20We%20also%20propose%20a%20model-agnostic%20DFS%20framework%20compatible%20with%20pre-trained%20classifiers%2C%20including%20interpretable-by-design%20models%2C%20through%20efficient%20subset%20reparametrization%20strategies.%20Empirical%20evaluation%20on%20tabular%20and%20image%20datasets%20demonstrates%20competitive%20accuracy%20against%20state-of-the-art%20greedy%20and%20reinforcement%20learning-based%20DFS%20methods%20with%20both%20neural%20and%20rule-based%20classifiers.%20We%20further%20show%20that%20the%20identified%20uncertainty%20sources%20persist%20across%20most%20existing%20approaches%2C%20highlighting%20the%20need%20for%20uncertainty-aware%20DFS.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02566v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-Agnostic%2520Dynamic%2520Feature%2520Selection%2520with%2520Uncertainty%2520Quantification%26entry.906535625%3DJavier%2520Fumanal-Idocin%2520and%2520Raquel%2520Fernandez-Peralta%2520and%2520Javier%2520Andreu-Perez%26entry.1292438233%3DDynamic%2520feature%2520selection%2520%2528DFS%2529%2520addresses%2520budget%2520constraints%2520in%2520decision-making%2520by%2520sequentially%2520acquiring%2520features%2520for%2520each%2520instance%252C%2520making%2520it%2520appealing%2520for%2520resource-limited%2520scenarios.%2520However%252C%2520existing%2520DFS%2520methods%2520require%2520models%2520specifically%2520designed%2520for%2520the%2520sequential%2520acquisition%2520setting%252C%2520limiting%2520compatibility%2520with%2520models%2520already%2520deployed%2520in%2520practice.%2520Furthermore%252C%2520they%2520provide%2520limited%2520uncertainty%2520quantification%252C%2520undermining%2520trust%2520in%2520high-stakes%2520decisions.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520DFS%2520introduces%2520new%2520uncertainty%2520sources%2520compared%2520to%2520the%2520static%2520setting.%2520We%2520formalise%2520how%2520model%2520adaptation%2520to%2520feature%2520subsets%2520induces%2520epistemic%2520uncertainty%252C%2520how%2520standard%2520imputation%2520strategies%2520bias%2520aleatoric%2520uncertainty%2520estimation%252C%2520and%2520why%2520predictive%2520confidence%2520fails%2520to%2520discriminate%2520between%2520good%2520and%2520bad%2520selection%2520policies.%2520We%2520also%2520propose%2520a%2520model-agnostic%2520DFS%2520framework%2520compatible%2520with%2520pre-trained%2520classifiers%252C%2520including%2520interpretable-by-design%2520models%252C%2520through%2520efficient%2520subset%2520reparametrization%2520strategies.%2520Empirical%2520evaluation%2520on%2520tabular%2520and%2520image%2520datasets%2520demonstrates%2520competitive%2520accuracy%2520against%2520state-of-the-art%2520greedy%2520and%2520reinforcement%2520learning-based%2520DFS%2520methods%2520with%2520both%2520neural%2520and%2520rule-based%2520classifiers.%2520We%2520further%2520show%2520that%2520the%2520identified%2520uncertainty%2520sources%2520persist%2520across%2520most%2520existing%2520approaches%252C%2520highlighting%2520the%2520need%2520for%2520uncertainty-aware%2520DFS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02566v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Agnostic%20Dynamic%20Feature%20Selection%20with%20Uncertainty%20Quantification&entry.906535625=Javier%20Fumanal-Idocin%20and%20Raquel%20Fernandez-Peralta%20and%20Javier%20Andreu-Perez&entry.1292438233=Dynamic%20feature%20selection%20%28DFS%29%20addresses%20budget%20constraints%20in%20decision-making%20by%20sequentially%20acquiring%20features%20for%20each%20instance%2C%20making%20it%20appealing%20for%20resource-limited%20scenarios.%20However%2C%20existing%20DFS%20methods%20require%20models%20specifically%20designed%20for%20the%20sequential%20acquisition%20setting%2C%20limiting%20compatibility%20with%20models%20already%20deployed%20in%20practice.%20Furthermore%2C%20they%20provide%20limited%20uncertainty%20quantification%2C%20undermining%20trust%20in%20high-stakes%20decisions.%20In%20this%20work%2C%20we%20show%20that%20DFS%20introduces%20new%20uncertainty%20sources%20compared%20to%20the%20static%20setting.%20We%20formalise%20how%20model%20adaptation%20to%20feature%20subsets%20induces%20epistemic%20uncertainty%2C%20how%20standard%20imputation%20strategies%20bias%20aleatoric%20uncertainty%20estimation%2C%20and%20why%20predictive%20confidence%20fails%20to%20discriminate%20between%20good%20and%20bad%20selection%20policies.%20We%20also%20propose%20a%20model-agnostic%20DFS%20framework%20compatible%20with%20pre-trained%20classifiers%2C%20including%20interpretable-by-design%20models%2C%20through%20efficient%20subset%20reparametrization%20strategies.%20Empirical%20evaluation%20on%20tabular%20and%20image%20datasets%20demonstrates%20competitive%20accuracy%20against%20state-of-the-art%20greedy%20and%20reinforcement%20learning-based%20DFS%20methods%20with%20both%20neural%20and%20rule-based%20classifiers.%20We%20further%20show%20that%20the%20identified%20uncertainty%20sources%20persist%20across%20most%20existing%20approaches%2C%20highlighting%20the%20need%20for%20uncertainty-aware%20DFS.&entry.1838667208=http%3A//arxiv.org/abs/2508.02566v3&entry.124074799=Read"},
{"title": "Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport", "author": "Alessandro Quadrio and Antonio Candelieri", "abstract": "This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.\n  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.", "link": "http://arxiv.org/abs/2512.09530v2", "date": "2026-02-18", "relevancy": 2.0788, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5263}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20for%20Tabular%20Data%3A%20A%20Training%20Perspective%20of%20Self-Attention%20via%20Optimal%20Transport&body=Title%3A%20Transformers%20for%20Tabular%20Data%3A%20A%20Training%20Perspective%20of%20Self-Attention%20via%20Optimal%20Transport%0AAuthor%3A%20Alessandro%20Quadrio%20and%20Antonio%20Candelieri%0AAbstract%3A%20This%20thesis%20examines%20self-attention%20training%20through%20the%20lens%20of%20Optimal%20Transport%20%28OT%29%20and%20develops%20an%20OT-based%20alternative%20for%20tabular%20classification.%20The%20study%20tracks%20intermediate%20projections%20of%20the%20self-attention%20layer%20during%20training%20and%20evaluates%20their%20evolution%20using%20discrete%20OT%20metrics%2C%20including%20Wasserstein%20distance%2C%20Monge%20gap%2C%20optimality%2C%20and%20efficiency.%20Experiments%20are%20conducted%20on%20classification%20tasks%20with%20two%20and%20three%20classes%2C%20as%20well%20as%20on%20a%20biomedical%20dataset.%0A%20%20Results%20indicate%20that%20the%20final%20self-attention%20mapping%20often%20approximates%20the%20OT%20optimal%20coupling%2C%20yet%20the%20training%20trajectory%20remains%20inefficient.%20Pretraining%20the%20MLP%20section%20on%20synthetic%20data%20partially%20improves%20convergence%20but%20is%20sensitive%20to%20their%20initialization.%20To%20address%20these%20limitations%2C%20an%20OT-based%20algorithm%20is%20introduced%3A%20it%20generates%20class-specific%20dummy%20Gaussian%20distributions%2C%20computes%20an%20OT%20alignment%20with%20the%20data%2C%20and%20trains%20an%20MLP%20to%20generalize%20this%20mapping.%20The%20method%20achieves%20accuracy%20comparable%20to%20Transformers%20while%20reducing%20computational%20cost%20and%20scaling%20more%20efficiently%20under%20standardized%20inputs%2C%20though%20its%20performance%20depends%20on%20careful%20dummy-geometry%20design.%20All%20experiments%20and%20implementations%20are%20conducted%20in%20R.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520for%2520Tabular%2520Data%253A%2520A%2520Training%2520Perspective%2520of%2520Self-Attention%2520via%2520Optimal%2520Transport%26entry.906535625%3DAlessandro%2520Quadrio%2520and%2520Antonio%2520Candelieri%26entry.1292438233%3DThis%2520thesis%2520examines%2520self-attention%2520training%2520through%2520the%2520lens%2520of%2520Optimal%2520Transport%2520%2528OT%2529%2520and%2520develops%2520an%2520OT-based%2520alternative%2520for%2520tabular%2520classification.%2520The%2520study%2520tracks%2520intermediate%2520projections%2520of%2520the%2520self-attention%2520layer%2520during%2520training%2520and%2520evaluates%2520their%2520evolution%2520using%2520discrete%2520OT%2520metrics%252C%2520including%2520Wasserstein%2520distance%252C%2520Monge%2520gap%252C%2520optimality%252C%2520and%2520efficiency.%2520Experiments%2520are%2520conducted%2520on%2520classification%2520tasks%2520with%2520two%2520and%2520three%2520classes%252C%2520as%2520well%2520as%2520on%2520a%2520biomedical%2520dataset.%250A%2520%2520Results%2520indicate%2520that%2520the%2520final%2520self-attention%2520mapping%2520often%2520approximates%2520the%2520OT%2520optimal%2520coupling%252C%2520yet%2520the%2520training%2520trajectory%2520remains%2520inefficient.%2520Pretraining%2520the%2520MLP%2520section%2520on%2520synthetic%2520data%2520partially%2520improves%2520convergence%2520but%2520is%2520sensitive%2520to%2520their%2520initialization.%2520To%2520address%2520these%2520limitations%252C%2520an%2520OT-based%2520algorithm%2520is%2520introduced%253A%2520it%2520generates%2520class-specific%2520dummy%2520Gaussian%2520distributions%252C%2520computes%2520an%2520OT%2520alignment%2520with%2520the%2520data%252C%2520and%2520trains%2520an%2520MLP%2520to%2520generalize%2520this%2520mapping.%2520The%2520method%2520achieves%2520accuracy%2520comparable%2520to%2520Transformers%2520while%2520reducing%2520computational%2520cost%2520and%2520scaling%2520more%2520efficiently%2520under%2520standardized%2520inputs%252C%2520though%2520its%2520performance%2520depends%2520on%2520careful%2520dummy-geometry%2520design.%2520All%2520experiments%2520and%2520implementations%2520are%2520conducted%2520in%2520R.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20for%20Tabular%20Data%3A%20A%20Training%20Perspective%20of%20Self-Attention%20via%20Optimal%20Transport&entry.906535625=Alessandro%20Quadrio%20and%20Antonio%20Candelieri&entry.1292438233=This%20thesis%20examines%20self-attention%20training%20through%20the%20lens%20of%20Optimal%20Transport%20%28OT%29%20and%20develops%20an%20OT-based%20alternative%20for%20tabular%20classification.%20The%20study%20tracks%20intermediate%20projections%20of%20the%20self-attention%20layer%20during%20training%20and%20evaluates%20their%20evolution%20using%20discrete%20OT%20metrics%2C%20including%20Wasserstein%20distance%2C%20Monge%20gap%2C%20optimality%2C%20and%20efficiency.%20Experiments%20are%20conducted%20on%20classification%20tasks%20with%20two%20and%20three%20classes%2C%20as%20well%20as%20on%20a%20biomedical%20dataset.%0A%20%20Results%20indicate%20that%20the%20final%20self-attention%20mapping%20often%20approximates%20the%20OT%20optimal%20coupling%2C%20yet%20the%20training%20trajectory%20remains%20inefficient.%20Pretraining%20the%20MLP%20section%20on%20synthetic%20data%20partially%20improves%20convergence%20but%20is%20sensitive%20to%20their%20initialization.%20To%20address%20these%20limitations%2C%20an%20OT-based%20algorithm%20is%20introduced%3A%20it%20generates%20class-specific%20dummy%20Gaussian%20distributions%2C%20computes%20an%20OT%20alignment%20with%20the%20data%2C%20and%20trains%20an%20MLP%20to%20generalize%20this%20mapping.%20The%20method%20achieves%20accuracy%20comparable%20to%20Transformers%20while%20reducing%20computational%20cost%20and%20scaling%20more%20efficiently%20under%20standardized%20inputs%2C%20though%20its%20performance%20depends%20on%20careful%20dummy-geometry%20design.%20All%20experiments%20and%20implementations%20are%20conducted%20in%20R.&entry.1838667208=http%3A//arxiv.org/abs/2512.09530v2&entry.124074799=Read"},
{"title": "GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation", "author": "Nicolas Salvy and Hugues Talbot and Bertrand Thirion", "abstract": "Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.", "link": "http://arxiv.org/abs/2602.16449v1", "date": "2026-02-18", "relevancy": 2.0782, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5383}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5207}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GICDM%3A%20Mitigating%20Hubness%20for%20Reliable%20Distance-Based%20Generative%20Model%20Evaluation&body=Title%3A%20GICDM%3A%20Mitigating%20Hubness%20for%20Reliable%20Distance-Based%20Generative%20Model%20Evaluation%0AAuthor%3A%20Nicolas%20Salvy%20and%20Hugues%20Talbot%20and%20Bertrand%20Thirion%0AAbstract%3A%20Generative%20model%20evaluation%20commonly%20relies%20on%20high-dimensional%20embedding%20spaces%20to%20compute%20distances%20between%20samples.%20We%20show%20that%20dataset%20representations%20in%20these%20spaces%20are%20affected%20by%20the%20hubness%20phenomenon%2C%20which%20distorts%20nearest%20neighbor%20relationships%20and%20biases%20distance-based%20metrics.%20Building%20on%20the%20classical%20Iterative%20Contextual%20Dissimilarity%20Measure%20%28ICDM%29%2C%20we%20introduce%20Generative%20ICDM%20%28GICDM%29%2C%20a%20method%20to%20correct%20neighborhood%20estimation%20for%20both%20real%20and%20generated%20data.%20We%20introduce%20a%20multi-scale%20extension%20to%20improve%20empirical%20behavior.%20Extensive%20experiments%20on%20synthetic%20and%20real%20benchmarks%20demonstrate%20that%20GICDM%20resolves%20hubness-induced%20failures%2C%20restores%20reliable%20metric%20behavior%2C%20and%20improves%20alignment%20with%20human%20judgment.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGICDM%253A%2520Mitigating%2520Hubness%2520for%2520Reliable%2520Distance-Based%2520Generative%2520Model%2520Evaluation%26entry.906535625%3DNicolas%2520Salvy%2520and%2520Hugues%2520Talbot%2520and%2520Bertrand%2520Thirion%26entry.1292438233%3DGenerative%2520model%2520evaluation%2520commonly%2520relies%2520on%2520high-dimensional%2520embedding%2520spaces%2520to%2520compute%2520distances%2520between%2520samples.%2520We%2520show%2520that%2520dataset%2520representations%2520in%2520these%2520spaces%2520are%2520affected%2520by%2520the%2520hubness%2520phenomenon%252C%2520which%2520distorts%2520nearest%2520neighbor%2520relationships%2520and%2520biases%2520distance-based%2520metrics.%2520Building%2520on%2520the%2520classical%2520Iterative%2520Contextual%2520Dissimilarity%2520Measure%2520%2528ICDM%2529%252C%2520we%2520introduce%2520Generative%2520ICDM%2520%2528GICDM%2529%252C%2520a%2520method%2520to%2520correct%2520neighborhood%2520estimation%2520for%2520both%2520real%2520and%2520generated%2520data.%2520We%2520introduce%2520a%2520multi-scale%2520extension%2520to%2520improve%2520empirical%2520behavior.%2520Extensive%2520experiments%2520on%2520synthetic%2520and%2520real%2520benchmarks%2520demonstrate%2520that%2520GICDM%2520resolves%2520hubness-induced%2520failures%252C%2520restores%2520reliable%2520metric%2520behavior%252C%2520and%2520improves%2520alignment%2520with%2520human%2520judgment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GICDM%3A%20Mitigating%20Hubness%20for%20Reliable%20Distance-Based%20Generative%20Model%20Evaluation&entry.906535625=Nicolas%20Salvy%20and%20Hugues%20Talbot%20and%20Bertrand%20Thirion&entry.1292438233=Generative%20model%20evaluation%20commonly%20relies%20on%20high-dimensional%20embedding%20spaces%20to%20compute%20distances%20between%20samples.%20We%20show%20that%20dataset%20representations%20in%20these%20spaces%20are%20affected%20by%20the%20hubness%20phenomenon%2C%20which%20distorts%20nearest%20neighbor%20relationships%20and%20biases%20distance-based%20metrics.%20Building%20on%20the%20classical%20Iterative%20Contextual%20Dissimilarity%20Measure%20%28ICDM%29%2C%20we%20introduce%20Generative%20ICDM%20%28GICDM%29%2C%20a%20method%20to%20correct%20neighborhood%20estimation%20for%20both%20real%20and%20generated%20data.%20We%20introduce%20a%20multi-scale%20extension%20to%20improve%20empirical%20behavior.%20Extensive%20experiments%20on%20synthetic%20and%20real%20benchmarks%20demonstrate%20that%20GICDM%20resolves%20hubness-induced%20failures%2C%20restores%20reliable%20metric%20behavior%2C%20and%20improves%20alignment%20with%20human%20judgment.&entry.1838667208=http%3A//arxiv.org/abs/2602.16449v1&entry.124074799=Read"},
{"title": "Separating Oblivious and Adaptive Models of Variable Selection", "author": "Ziyun Chen and Jerry Li and Kevin Tian and Yusong Zhu", "abstract": "Sparse recovery is among the most well-studied problems in learning theory and high-dimensional statistics. In this work, we investigate the statistical and computational landscapes of sparse recovery with $\\ell_\\infty$ error guarantees. This variant of the problem is motivated by \\emph{variable selection} tasks, where the goal is to estimate the support of a $k$-sparse signal in $\\mathbb{R}^d$. Our main contribution is a provable separation between the \\emph{oblivious} (``for each'') and \\emph{adaptive} (``for all'') models of $\\ell_\\infty$ sparse recovery. We show that under an oblivious model, the optimal $\\ell_\\infty$ error is attainable in near-linear time with $\\approx k\\log d$ samples, whereas in an adaptive model, $\\gtrsim k^2$ samples are necessary for any algorithm to achieve this bound. This establishes a surprising contrast with the standard $\\ell_2$ setting, where $\\approx k \\log d$ samples suffice even for adaptive sparse recovery. We conclude with a preliminary examination of a \\emph{partially-adaptive} model, where we show nontrivial variable selection guarantees are possible with $\\approx k\\log d$ measurements.", "link": "http://arxiv.org/abs/2602.16568v1", "date": "2026-02-18", "relevancy": 2.0743, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4361}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4134}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Separating%20Oblivious%20and%20Adaptive%20Models%20of%20Variable%20Selection&body=Title%3A%20Separating%20Oblivious%20and%20Adaptive%20Models%20of%20Variable%20Selection%0AAuthor%3A%20Ziyun%20Chen%20and%20Jerry%20Li%20and%20Kevin%20Tian%20and%20Yusong%20Zhu%0AAbstract%3A%20Sparse%20recovery%20is%20among%20the%20most%20well-studied%20problems%20in%20learning%20theory%20and%20high-dimensional%20statistics.%20In%20this%20work%2C%20we%20investigate%20the%20statistical%20and%20computational%20landscapes%20of%20sparse%20recovery%20with%20%24%5Cell_%5Cinfty%24%20error%20guarantees.%20This%20variant%20of%20the%20problem%20is%20motivated%20by%20%5Cemph%7Bvariable%20selection%7D%20tasks%2C%20where%20the%20goal%20is%20to%20estimate%20the%20support%20of%20a%20%24k%24-sparse%20signal%20in%20%24%5Cmathbb%7BR%7D%5Ed%24.%20Our%20main%20contribution%20is%20a%20provable%20separation%20between%20the%20%5Cemph%7Boblivious%7D%20%28%60%60for%20each%27%27%29%20and%20%5Cemph%7Badaptive%7D%20%28%60%60for%20all%27%27%29%20models%20of%20%24%5Cell_%5Cinfty%24%20sparse%20recovery.%20We%20show%20that%20under%20an%20oblivious%20model%2C%20the%20optimal%20%24%5Cell_%5Cinfty%24%20error%20is%20attainable%20in%20near-linear%20time%20with%20%24%5Capprox%20k%5Clog%20d%24%20samples%2C%20whereas%20in%20an%20adaptive%20model%2C%20%24%5Cgtrsim%20k%5E2%24%20samples%20are%20necessary%20for%20any%20algorithm%20to%20achieve%20this%20bound.%20This%20establishes%20a%20surprising%20contrast%20with%20the%20standard%20%24%5Cell_2%24%20setting%2C%20where%20%24%5Capprox%20k%20%5Clog%20d%24%20samples%20suffice%20even%20for%20adaptive%20sparse%20recovery.%20We%20conclude%20with%20a%20preliminary%20examination%20of%20a%20%5Cemph%7Bpartially-adaptive%7D%20model%2C%20where%20we%20show%20nontrivial%20variable%20selection%20guarantees%20are%20possible%20with%20%24%5Capprox%20k%5Clog%20d%24%20measurements.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeparating%2520Oblivious%2520and%2520Adaptive%2520Models%2520of%2520Variable%2520Selection%26entry.906535625%3DZiyun%2520Chen%2520and%2520Jerry%2520Li%2520and%2520Kevin%2520Tian%2520and%2520Yusong%2520Zhu%26entry.1292438233%3DSparse%2520recovery%2520is%2520among%2520the%2520most%2520well-studied%2520problems%2520in%2520learning%2520theory%2520and%2520high-dimensional%2520statistics.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520statistical%2520and%2520computational%2520landscapes%2520of%2520sparse%2520recovery%2520with%2520%2524%255Cell_%255Cinfty%2524%2520error%2520guarantees.%2520This%2520variant%2520of%2520the%2520problem%2520is%2520motivated%2520by%2520%255Cemph%257Bvariable%2520selection%257D%2520tasks%252C%2520where%2520the%2520goal%2520is%2520to%2520estimate%2520the%2520support%2520of%2520a%2520%2524k%2524-sparse%2520signal%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524.%2520Our%2520main%2520contribution%2520is%2520a%2520provable%2520separation%2520between%2520the%2520%255Cemph%257Boblivious%257D%2520%2528%2560%2560for%2520each%2527%2527%2529%2520and%2520%255Cemph%257Badaptive%257D%2520%2528%2560%2560for%2520all%2527%2527%2529%2520models%2520of%2520%2524%255Cell_%255Cinfty%2524%2520sparse%2520recovery.%2520We%2520show%2520that%2520under%2520an%2520oblivious%2520model%252C%2520the%2520optimal%2520%2524%255Cell_%255Cinfty%2524%2520error%2520is%2520attainable%2520in%2520near-linear%2520time%2520with%2520%2524%255Capprox%2520k%255Clog%2520d%2524%2520samples%252C%2520whereas%2520in%2520an%2520adaptive%2520model%252C%2520%2524%255Cgtrsim%2520k%255E2%2524%2520samples%2520are%2520necessary%2520for%2520any%2520algorithm%2520to%2520achieve%2520this%2520bound.%2520This%2520establishes%2520a%2520surprising%2520contrast%2520with%2520the%2520standard%2520%2524%255Cell_2%2524%2520setting%252C%2520where%2520%2524%255Capprox%2520k%2520%255Clog%2520d%2524%2520samples%2520suffice%2520even%2520for%2520adaptive%2520sparse%2520recovery.%2520We%2520conclude%2520with%2520a%2520preliminary%2520examination%2520of%2520a%2520%255Cemph%257Bpartially-adaptive%257D%2520model%252C%2520where%2520we%2520show%2520nontrivial%2520variable%2520selection%2520guarantees%2520are%2520possible%2520with%2520%2524%255Capprox%2520k%255Clog%2520d%2524%2520measurements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separating%20Oblivious%20and%20Adaptive%20Models%20of%20Variable%20Selection&entry.906535625=Ziyun%20Chen%20and%20Jerry%20Li%20and%20Kevin%20Tian%20and%20Yusong%20Zhu&entry.1292438233=Sparse%20recovery%20is%20among%20the%20most%20well-studied%20problems%20in%20learning%20theory%20and%20high-dimensional%20statistics.%20In%20this%20work%2C%20we%20investigate%20the%20statistical%20and%20computational%20landscapes%20of%20sparse%20recovery%20with%20%24%5Cell_%5Cinfty%24%20error%20guarantees.%20This%20variant%20of%20the%20problem%20is%20motivated%20by%20%5Cemph%7Bvariable%20selection%7D%20tasks%2C%20where%20the%20goal%20is%20to%20estimate%20the%20support%20of%20a%20%24k%24-sparse%20signal%20in%20%24%5Cmathbb%7BR%7D%5Ed%24.%20Our%20main%20contribution%20is%20a%20provable%20separation%20between%20the%20%5Cemph%7Boblivious%7D%20%28%60%60for%20each%27%27%29%20and%20%5Cemph%7Badaptive%7D%20%28%60%60for%20all%27%27%29%20models%20of%20%24%5Cell_%5Cinfty%24%20sparse%20recovery.%20We%20show%20that%20under%20an%20oblivious%20model%2C%20the%20optimal%20%24%5Cell_%5Cinfty%24%20error%20is%20attainable%20in%20near-linear%20time%20with%20%24%5Capprox%20k%5Clog%20d%24%20samples%2C%20whereas%20in%20an%20adaptive%20model%2C%20%24%5Cgtrsim%20k%5E2%24%20samples%20are%20necessary%20for%20any%20algorithm%20to%20achieve%20this%20bound.%20This%20establishes%20a%20surprising%20contrast%20with%20the%20standard%20%24%5Cell_2%24%20setting%2C%20where%20%24%5Capprox%20k%20%5Clog%20d%24%20samples%20suffice%20even%20for%20adaptive%20sparse%20recovery.%20We%20conclude%20with%20a%20preliminary%20examination%20of%20a%20%5Cemph%7Bpartially-adaptive%7D%20model%2C%20where%20we%20show%20nontrivial%20variable%20selection%20guarantees%20are%20possible%20with%20%24%5Capprox%20k%5Clog%20d%24%20measurements.&entry.1838667208=http%3A//arxiv.org/abs/2602.16568v1&entry.124074799=Read"},
{"title": "MMA: Multimodal Memory Agent", "author": "Yihao Lu and Wanru Cheng and Zeyu Zhang and Hao Tang", "abstract": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.", "link": "http://arxiv.org/abs/2602.16493v1", "date": "2026-02-18", "relevancy": 2.0728, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5598}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMA%3A%20Multimodal%20Memory%20Agent&body=Title%3A%20MMA%3A%20Multimodal%20Memory%20Agent%0AAuthor%3A%20Yihao%20Lu%20and%20Wanru%20Cheng%20and%20Zeyu%20Zhang%20and%20Hao%20Tang%0AAbstract%3A%20Long-horizon%20multimodal%20agents%20depend%20on%20external%20memory%3B%20however%2C%20similarity-based%20retrieval%20often%20surfaces%20stale%2C%20low-credibility%2C%20or%20conflicting%20items%2C%20which%20can%20trigger%20overconfident%20errors.%20We%20propose%20Multimodal%20Memory%20Agent%20%28MMA%29%2C%20which%20assigns%20each%20retrieved%20memory%20item%20a%20dynamic%20reliability%20score%20by%20combining%20source%20credibility%2C%20temporal%20decay%2C%20and%20conflict-aware%20network%20consensus%2C%20and%20uses%20this%20signal%20to%20reweight%20evidence%20and%20abstain%20when%20support%20is%20insufficient.%20We%20also%20introduce%20MMA-Bench%2C%20a%20programmatically%20generated%20benchmark%20for%20belief%20dynamics%20with%20controlled%20speaker%20reliability%20and%20structured%20text-vision%20contradictions.%20Using%20this%20framework%2C%20we%20uncover%20the%20%22Visual%20Placebo%20Effect%22%2C%20revealing%20how%20RAG-based%20agents%20inherit%20latent%20visual%20biases%20from%20foundation%20models.%20On%20FEVER%2C%20MMA%20matches%20baseline%20accuracy%20while%20reducing%20variance%20by%2035.2%25%20and%20improving%20selective%20utility%3B%20on%20LoCoMo%2C%20a%20safety-oriented%20configuration%20improves%20actionable%20accuracy%20and%20reduces%20wrong%20answers%3B%20on%20MMA-Bench%2C%20MMA%20reaches%2041.18%25%20Type-B%20accuracy%20in%20Vision%20mode%2C%20while%20the%20baseline%20collapses%20to%200.0%25%20under%20the%20same%20protocol.%20Code%3A%20https%3A//github.com/AIGeeksGroup/MMA.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMA%253A%2520Multimodal%2520Memory%2520Agent%26entry.906535625%3DYihao%2520Lu%2520and%2520Wanru%2520Cheng%2520and%2520Zeyu%2520Zhang%2520and%2520Hao%2520Tang%26entry.1292438233%3DLong-horizon%2520multimodal%2520agents%2520depend%2520on%2520external%2520memory%253B%2520however%252C%2520similarity-based%2520retrieval%2520often%2520surfaces%2520stale%252C%2520low-credibility%252C%2520or%2520conflicting%2520items%252C%2520which%2520can%2520trigger%2520overconfident%2520errors.%2520We%2520propose%2520Multimodal%2520Memory%2520Agent%2520%2528MMA%2529%252C%2520which%2520assigns%2520each%2520retrieved%2520memory%2520item%2520a%2520dynamic%2520reliability%2520score%2520by%2520combining%2520source%2520credibility%252C%2520temporal%2520decay%252C%2520and%2520conflict-aware%2520network%2520consensus%252C%2520and%2520uses%2520this%2520signal%2520to%2520reweight%2520evidence%2520and%2520abstain%2520when%2520support%2520is%2520insufficient.%2520We%2520also%2520introduce%2520MMA-Bench%252C%2520a%2520programmatically%2520generated%2520benchmark%2520for%2520belief%2520dynamics%2520with%2520controlled%2520speaker%2520reliability%2520and%2520structured%2520text-vision%2520contradictions.%2520Using%2520this%2520framework%252C%2520we%2520uncover%2520the%2520%2522Visual%2520Placebo%2520Effect%2522%252C%2520revealing%2520how%2520RAG-based%2520agents%2520inherit%2520latent%2520visual%2520biases%2520from%2520foundation%2520models.%2520On%2520FEVER%252C%2520MMA%2520matches%2520baseline%2520accuracy%2520while%2520reducing%2520variance%2520by%252035.2%2525%2520and%2520improving%2520selective%2520utility%253B%2520on%2520LoCoMo%252C%2520a%2520safety-oriented%2520configuration%2520improves%2520actionable%2520accuracy%2520and%2520reduces%2520wrong%2520answers%253B%2520on%2520MMA-Bench%252C%2520MMA%2520reaches%252041.18%2525%2520Type-B%2520accuracy%2520in%2520Vision%2520mode%252C%2520while%2520the%2520baseline%2520collapses%2520to%25200.0%2525%2520under%2520the%2520same%2520protocol.%2520Code%253A%2520https%253A//github.com/AIGeeksGroup/MMA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMA%3A%20Multimodal%20Memory%20Agent&entry.906535625=Yihao%20Lu%20and%20Wanru%20Cheng%20and%20Zeyu%20Zhang%20and%20Hao%20Tang&entry.1292438233=Long-horizon%20multimodal%20agents%20depend%20on%20external%20memory%3B%20however%2C%20similarity-based%20retrieval%20often%20surfaces%20stale%2C%20low-credibility%2C%20or%20conflicting%20items%2C%20which%20can%20trigger%20overconfident%20errors.%20We%20propose%20Multimodal%20Memory%20Agent%20%28MMA%29%2C%20which%20assigns%20each%20retrieved%20memory%20item%20a%20dynamic%20reliability%20score%20by%20combining%20source%20credibility%2C%20temporal%20decay%2C%20and%20conflict-aware%20network%20consensus%2C%20and%20uses%20this%20signal%20to%20reweight%20evidence%20and%20abstain%20when%20support%20is%20insufficient.%20We%20also%20introduce%20MMA-Bench%2C%20a%20programmatically%20generated%20benchmark%20for%20belief%20dynamics%20with%20controlled%20speaker%20reliability%20and%20structured%20text-vision%20contradictions.%20Using%20this%20framework%2C%20we%20uncover%20the%20%22Visual%20Placebo%20Effect%22%2C%20revealing%20how%20RAG-based%20agents%20inherit%20latent%20visual%20biases%20from%20foundation%20models.%20On%20FEVER%2C%20MMA%20matches%20baseline%20accuracy%20while%20reducing%20variance%20by%2035.2%25%20and%20improving%20selective%20utility%3B%20on%20LoCoMo%2C%20a%20safety-oriented%20configuration%20improves%20actionable%20accuracy%20and%20reduces%20wrong%20answers%3B%20on%20MMA-Bench%2C%20MMA%20reaches%2041.18%25%20Type-B%20accuracy%20in%20Vision%20mode%2C%20while%20the%20baseline%20collapses%20to%200.0%25%20under%20the%20same%20protocol.%20Code%3A%20https%3A//github.com/AIGeeksGroup/MMA.&entry.1838667208=http%3A//arxiv.org/abs/2602.16493v1&entry.124074799=Read"},
{"title": "Learning Distributed Equilibria in Linear-Quadratic Stochastic Differential Games: An $\u03b1$-Potential Approach", "author": "Philipp Plank and Yufei Zhang", "abstract": "We analyze independent policy-gradient (PG) learning in $N$-player linear-quadratic (LQ) stochastic differential games. Each player employs a distributed policy that depends only on its own state and updates the policy independently using the gradient of its own objective. We establish global linear convergence of these methods to an equilibrium by showing that the LQ game admits an $\u03b1$-potential structure, with $\u03b1$ determined by the degree of pairwise interaction asymmetry. For pairwise-symmetric interactions, we construct an affine distributed equilibrium by minimizing the potential function and show that independent PG methods converge globally to this equilibrium, with complexity scaling linearly in the population size and logarithmically in the desired accuracy. For asymmetric interactions, we prove that independent projected PG algorithms converge linearly to an approximate equilibrium, with suboptimality proportional to the degree of asymmetry. Numerical experiments confirm the theoretical results across both symmetric and asymmetric interaction networks.", "link": "http://arxiv.org/abs/2602.16555v1", "date": "2026-02-18", "relevancy": 2.0688, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4178}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4168}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Distributed%20Equilibria%20in%20Linear-Quadratic%20Stochastic%20Differential%20Games%3A%20An%20%24%CE%B1%24-Potential%20Approach&body=Title%3A%20Learning%20Distributed%20Equilibria%20in%20Linear-Quadratic%20Stochastic%20Differential%20Games%3A%20An%20%24%CE%B1%24-Potential%20Approach%0AAuthor%3A%20Philipp%20Plank%20and%20Yufei%20Zhang%0AAbstract%3A%20We%20analyze%20independent%20policy-gradient%20%28PG%29%20learning%20in%20%24N%24-player%20linear-quadratic%20%28LQ%29%20stochastic%20differential%20games.%20Each%20player%20employs%20a%20distributed%20policy%20that%20depends%20only%20on%20its%20own%20state%20and%20updates%20the%20policy%20independently%20using%20the%20gradient%20of%20its%20own%20objective.%20We%20establish%20global%20linear%20convergence%20of%20these%20methods%20to%20an%20equilibrium%20by%20showing%20that%20the%20LQ%20game%20admits%20an%20%24%CE%B1%24-potential%20structure%2C%20with%20%24%CE%B1%24%20determined%20by%20the%20degree%20of%20pairwise%20interaction%20asymmetry.%20For%20pairwise-symmetric%20interactions%2C%20we%20construct%20an%20affine%20distributed%20equilibrium%20by%20minimizing%20the%20potential%20function%20and%20show%20that%20independent%20PG%20methods%20converge%20globally%20to%20this%20equilibrium%2C%20with%20complexity%20scaling%20linearly%20in%20the%20population%20size%20and%20logarithmically%20in%20the%20desired%20accuracy.%20For%20asymmetric%20interactions%2C%20we%20prove%20that%20independent%20projected%20PG%20algorithms%20converge%20linearly%20to%20an%20approximate%20equilibrium%2C%20with%20suboptimality%20proportional%20to%20the%20degree%20of%20asymmetry.%20Numerical%20experiments%20confirm%20the%20theoretical%20results%20across%20both%20symmetric%20and%20asymmetric%20interaction%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Distributed%2520Equilibria%2520in%2520Linear-Quadratic%2520Stochastic%2520Differential%2520Games%253A%2520An%2520%2524%25CE%25B1%2524-Potential%2520Approach%26entry.906535625%3DPhilipp%2520Plank%2520and%2520Yufei%2520Zhang%26entry.1292438233%3DWe%2520analyze%2520independent%2520policy-gradient%2520%2528PG%2529%2520learning%2520in%2520%2524N%2524-player%2520linear-quadratic%2520%2528LQ%2529%2520stochastic%2520differential%2520games.%2520Each%2520player%2520employs%2520a%2520distributed%2520policy%2520that%2520depends%2520only%2520on%2520its%2520own%2520state%2520and%2520updates%2520the%2520policy%2520independently%2520using%2520the%2520gradient%2520of%2520its%2520own%2520objective.%2520We%2520establish%2520global%2520linear%2520convergence%2520of%2520these%2520methods%2520to%2520an%2520equilibrium%2520by%2520showing%2520that%2520the%2520LQ%2520game%2520admits%2520an%2520%2524%25CE%25B1%2524-potential%2520structure%252C%2520with%2520%2524%25CE%25B1%2524%2520determined%2520by%2520the%2520degree%2520of%2520pairwise%2520interaction%2520asymmetry.%2520For%2520pairwise-symmetric%2520interactions%252C%2520we%2520construct%2520an%2520affine%2520distributed%2520equilibrium%2520by%2520minimizing%2520the%2520potential%2520function%2520and%2520show%2520that%2520independent%2520PG%2520methods%2520converge%2520globally%2520to%2520this%2520equilibrium%252C%2520with%2520complexity%2520scaling%2520linearly%2520in%2520the%2520population%2520size%2520and%2520logarithmically%2520in%2520the%2520desired%2520accuracy.%2520For%2520asymmetric%2520interactions%252C%2520we%2520prove%2520that%2520independent%2520projected%2520PG%2520algorithms%2520converge%2520linearly%2520to%2520an%2520approximate%2520equilibrium%252C%2520with%2520suboptimality%2520proportional%2520to%2520the%2520degree%2520of%2520asymmetry.%2520Numerical%2520experiments%2520confirm%2520the%2520theoretical%2520results%2520across%2520both%2520symmetric%2520and%2520asymmetric%2520interaction%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Distributed%20Equilibria%20in%20Linear-Quadratic%20Stochastic%20Differential%20Games%3A%20An%20%24%CE%B1%24-Potential%20Approach&entry.906535625=Philipp%20Plank%20and%20Yufei%20Zhang&entry.1292438233=We%20analyze%20independent%20policy-gradient%20%28PG%29%20learning%20in%20%24N%24-player%20linear-quadratic%20%28LQ%29%20stochastic%20differential%20games.%20Each%20player%20employs%20a%20distributed%20policy%20that%20depends%20only%20on%20its%20own%20state%20and%20updates%20the%20policy%20independently%20using%20the%20gradient%20of%20its%20own%20objective.%20We%20establish%20global%20linear%20convergence%20of%20these%20methods%20to%20an%20equilibrium%20by%20showing%20that%20the%20LQ%20game%20admits%20an%20%24%CE%B1%24-potential%20structure%2C%20with%20%24%CE%B1%24%20determined%20by%20the%20degree%20of%20pairwise%20interaction%20asymmetry.%20For%20pairwise-symmetric%20interactions%2C%20we%20construct%20an%20affine%20distributed%20equilibrium%20by%20minimizing%20the%20potential%20function%20and%20show%20that%20independent%20PG%20methods%20converge%20globally%20to%20this%20equilibrium%2C%20with%20complexity%20scaling%20linearly%20in%20the%20population%20size%20and%20logarithmically%20in%20the%20desired%20accuracy.%20For%20asymmetric%20interactions%2C%20we%20prove%20that%20independent%20projected%20PG%20algorithms%20converge%20linearly%20to%20an%20approximate%20equilibrium%2C%20with%20suboptimality%20proportional%20to%20the%20degree%20of%20asymmetry.%20Numerical%20experiments%20confirm%20the%20theoretical%20results%20across%20both%20symmetric%20and%20asymmetric%20interaction%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2602.16555v1&entry.124074799=Read"},
{"title": "Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection", "author": "Alexis Winter and Jean-Vincent Martini and Romaric Audigier and Angelique Loesch and Bertrand Luvison", "abstract": "Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cost. This paper addresses this gap by investigating three key questions: (1) How can we create a fair benchmark to impartially compare attacks? (2) How well do modern attacks transfer across different architectures, especially from Convolutional Neural Networks to Vision Transformers? (3) What is the most effective adversarial training strategy for robust defense? To answer these, we first propose a unified benchmark framework focused on digital, non-patch-based attacks. This framework introduces specific metrics to disentangle localization and classification errors and evaluates attack cost using multiple perceptual metrics. Using this benchmark, we conduct extensive experiments on state-of-the-art attacks and a wide range of detectors. Our findings reveal two major conclusions: first, modern adversarial attacks against object detection models show a significant lack of transferability to transformer-based architectures. Second, we demonstrate that the most robust adversarial training strategy leverages a dataset composed of a mix of high-perturbation attacks with different objectives (e.g., spatial and semantic), which outperforms training on any single attack.", "link": "http://arxiv.org/abs/2602.16494v1", "date": "2026-02-18", "relevancy": 2.0682, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5257}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5182}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Adversarial%20Robustness%20and%20Adversarial%20Training%20Strategies%20for%20Object%20Detection&body=Title%3A%20Benchmarking%20Adversarial%20Robustness%20and%20Adversarial%20Training%20Strategies%20for%20Object%20Detection%0AAuthor%3A%20Alexis%20Winter%20and%20Jean-Vincent%20Martini%20and%20Romaric%20Audigier%20and%20Angelique%20Loesch%20and%20Bertrand%20Luvison%0AAbstract%3A%20Object%20detection%20models%20are%20critical%20components%20of%20automated%20systems%2C%20such%20as%20autonomous%20vehicles%20and%20perception-based%20robots%2C%20but%20their%20sensitivity%20to%20adversarial%20attacks%20poses%20a%20serious%20security%20risk.%20Progress%20in%20defending%20these%20models%20lags%20behind%20classification%2C%20hindered%20by%20a%20lack%20of%20standardized%20evaluation.%20It%20is%20nearly%20impossible%20to%20thoroughly%20compare%20attack%20or%20defense%20methods%2C%20as%20existing%20work%20uses%20different%20datasets%2C%20inconsistent%20efficiency%20metrics%2C%20and%20varied%20measures%20of%20perturbation%20cost.%20This%20paper%20addresses%20this%20gap%20by%20investigating%20three%20key%20questions%3A%20%281%29%20How%20can%20we%20create%20a%20fair%20benchmark%20to%20impartially%20compare%20attacks%3F%20%282%29%20How%20well%20do%20modern%20attacks%20transfer%20across%20different%20architectures%2C%20especially%20from%20Convolutional%20Neural%20Networks%20to%20Vision%20Transformers%3F%20%283%29%20What%20is%20the%20most%20effective%20adversarial%20training%20strategy%20for%20robust%20defense%3F%20To%20answer%20these%2C%20we%20first%20propose%20a%20unified%20benchmark%20framework%20focused%20on%20digital%2C%20non-patch-based%20attacks.%20This%20framework%20introduces%20specific%20metrics%20to%20disentangle%20localization%20and%20classification%20errors%20and%20evaluates%20attack%20cost%20using%20multiple%20perceptual%20metrics.%20Using%20this%20benchmark%2C%20we%20conduct%20extensive%20experiments%20on%20state-of-the-art%20attacks%20and%20a%20wide%20range%20of%20detectors.%20Our%20findings%20reveal%20two%20major%20conclusions%3A%20first%2C%20modern%20adversarial%20attacks%20against%20object%20detection%20models%20show%20a%20significant%20lack%20of%20transferability%20to%20transformer-based%20architectures.%20Second%2C%20we%20demonstrate%20that%20the%20most%20robust%20adversarial%20training%20strategy%20leverages%20a%20dataset%20composed%20of%20a%20mix%20of%20high-perturbation%20attacks%20with%20different%20objectives%20%28e.g.%2C%20spatial%20and%20semantic%29%2C%20which%20outperforms%20training%20on%20any%20single%20attack.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Adversarial%2520Robustness%2520and%2520Adversarial%2520Training%2520Strategies%2520for%2520Object%2520Detection%26entry.906535625%3DAlexis%2520Winter%2520and%2520Jean-Vincent%2520Martini%2520and%2520Romaric%2520Audigier%2520and%2520Angelique%2520Loesch%2520and%2520Bertrand%2520Luvison%26entry.1292438233%3DObject%2520detection%2520models%2520are%2520critical%2520components%2520of%2520automated%2520systems%252C%2520such%2520as%2520autonomous%2520vehicles%2520and%2520perception-based%2520robots%252C%2520but%2520their%2520sensitivity%2520to%2520adversarial%2520attacks%2520poses%2520a%2520serious%2520security%2520risk.%2520Progress%2520in%2520defending%2520these%2520models%2520lags%2520behind%2520classification%252C%2520hindered%2520by%2520a%2520lack%2520of%2520standardized%2520evaluation.%2520It%2520is%2520nearly%2520impossible%2520to%2520thoroughly%2520compare%2520attack%2520or%2520defense%2520methods%252C%2520as%2520existing%2520work%2520uses%2520different%2520datasets%252C%2520inconsistent%2520efficiency%2520metrics%252C%2520and%2520varied%2520measures%2520of%2520perturbation%2520cost.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520investigating%2520three%2520key%2520questions%253A%2520%25281%2529%2520How%2520can%2520we%2520create%2520a%2520fair%2520benchmark%2520to%2520impartially%2520compare%2520attacks%253F%2520%25282%2529%2520How%2520well%2520do%2520modern%2520attacks%2520transfer%2520across%2520different%2520architectures%252C%2520especially%2520from%2520Convolutional%2520Neural%2520Networks%2520to%2520Vision%2520Transformers%253F%2520%25283%2529%2520What%2520is%2520the%2520most%2520effective%2520adversarial%2520training%2520strategy%2520for%2520robust%2520defense%253F%2520To%2520answer%2520these%252C%2520we%2520first%2520propose%2520a%2520unified%2520benchmark%2520framework%2520focused%2520on%2520digital%252C%2520non-patch-based%2520attacks.%2520This%2520framework%2520introduces%2520specific%2520metrics%2520to%2520disentangle%2520localization%2520and%2520classification%2520errors%2520and%2520evaluates%2520attack%2520cost%2520using%2520multiple%2520perceptual%2520metrics.%2520Using%2520this%2520benchmark%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520state-of-the-art%2520attacks%2520and%2520a%2520wide%2520range%2520of%2520detectors.%2520Our%2520findings%2520reveal%2520two%2520major%2520conclusions%253A%2520first%252C%2520modern%2520adversarial%2520attacks%2520against%2520object%2520detection%2520models%2520show%2520a%2520significant%2520lack%2520of%2520transferability%2520to%2520transformer-based%2520architectures.%2520Second%252C%2520we%2520demonstrate%2520that%2520the%2520most%2520robust%2520adversarial%2520training%2520strategy%2520leverages%2520a%2520dataset%2520composed%2520of%2520a%2520mix%2520of%2520high-perturbation%2520attacks%2520with%2520different%2520objectives%2520%2528e.g.%252C%2520spatial%2520and%2520semantic%2529%252C%2520which%2520outperforms%2520training%2520on%2520any%2520single%2520attack.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Adversarial%20Robustness%20and%20Adversarial%20Training%20Strategies%20for%20Object%20Detection&entry.906535625=Alexis%20Winter%20and%20Jean-Vincent%20Martini%20and%20Romaric%20Audigier%20and%20Angelique%20Loesch%20and%20Bertrand%20Luvison&entry.1292438233=Object%20detection%20models%20are%20critical%20components%20of%20automated%20systems%2C%20such%20as%20autonomous%20vehicles%20and%20perception-based%20robots%2C%20but%20their%20sensitivity%20to%20adversarial%20attacks%20poses%20a%20serious%20security%20risk.%20Progress%20in%20defending%20these%20models%20lags%20behind%20classification%2C%20hindered%20by%20a%20lack%20of%20standardized%20evaluation.%20It%20is%20nearly%20impossible%20to%20thoroughly%20compare%20attack%20or%20defense%20methods%2C%20as%20existing%20work%20uses%20different%20datasets%2C%20inconsistent%20efficiency%20metrics%2C%20and%20varied%20measures%20of%20perturbation%20cost.%20This%20paper%20addresses%20this%20gap%20by%20investigating%20three%20key%20questions%3A%20%281%29%20How%20can%20we%20create%20a%20fair%20benchmark%20to%20impartially%20compare%20attacks%3F%20%282%29%20How%20well%20do%20modern%20attacks%20transfer%20across%20different%20architectures%2C%20especially%20from%20Convolutional%20Neural%20Networks%20to%20Vision%20Transformers%3F%20%283%29%20What%20is%20the%20most%20effective%20adversarial%20training%20strategy%20for%20robust%20defense%3F%20To%20answer%20these%2C%20we%20first%20propose%20a%20unified%20benchmark%20framework%20focused%20on%20digital%2C%20non-patch-based%20attacks.%20This%20framework%20introduces%20specific%20metrics%20to%20disentangle%20localization%20and%20classification%20errors%20and%20evaluates%20attack%20cost%20using%20multiple%20perceptual%20metrics.%20Using%20this%20benchmark%2C%20we%20conduct%20extensive%20experiments%20on%20state-of-the-art%20attacks%20and%20a%20wide%20range%20of%20detectors.%20Our%20findings%20reveal%20two%20major%20conclusions%3A%20first%2C%20modern%20adversarial%20attacks%20against%20object%20detection%20models%20show%20a%20significant%20lack%20of%20transferability%20to%20transformer-based%20architectures.%20Second%2C%20we%20demonstrate%20that%20the%20most%20robust%20adversarial%20training%20strategy%20leverages%20a%20dataset%20composed%20of%20a%20mix%20of%20high-perturbation%20attacks%20with%20different%20objectives%20%28e.g.%2C%20spatial%20and%20semantic%29%2C%20which%20outperforms%20training%20on%20any%20single%20attack.&entry.1838667208=http%3A//arxiv.org/abs/2602.16494v1&entry.124074799=Read"},
{"title": "Machine Learning Driven Prediction of the Behavior of Biohybrid Actuators", "author": "Michail-Antisthenis Tsompanas and Marco Perez Hernandez and Faisal Abdul-Fattah and Karim Elhakim and Mostafa Ibrahim and Judith Fuentes and Florencia Lezcano and Riccardo Collu and Massimo Barbaro and Stefano Lai and Samuel Sanchez and Andrew Adamatzky", "abstract": "Skeletal muscle-based biohybrid actuators have proved to be a promising component in soft robotics, offering efficient movement. However, their intrinsic biological variability and nonlinearity pose significant challenges for controllability and predictability. To address these issues, this study investigates the application of supervised learning, a form of machine learning, to model and predict the behavior of biohybrid machines (BHMs), focusing on a muscle ring anchored on flexible polymer pillars. First, static prediction models (i.e., random forest and neural network regressors) are trained to estimate the maximum exerted force achieved from input variables such as muscle sample, electrical stimulation parameters, and baseline exerted force. Second, a dynamic modeling framework, based on Long Short-Term Memory networks, is developed to serve as a digital twin, replicating the time series of exerted forces observed in response to electrical stimulation. Both modeling approaches demonstrate high predictive accuracy. The best performance of the static models is characterized by R2 of 0.9425, whereas the dynamic model achieves R2 of 0.9956. The static models can enable optimization of muscle actuator performance for targeted applications and required force outcomes, while the dynamic model provides a foundation for developing robustly adaptive control strategies in future biohybrid robotic systems.", "link": "http://arxiv.org/abs/2602.16330v1", "date": "2026-02-18", "relevancy": 2.0597, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5951}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5112}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Driven%20Prediction%20of%20the%20Behavior%20of%20Biohybrid%20Actuators&body=Title%3A%20Machine%20Learning%20Driven%20Prediction%20of%20the%20Behavior%20of%20Biohybrid%20Actuators%0AAuthor%3A%20Michail-Antisthenis%20Tsompanas%20and%20Marco%20Perez%20Hernandez%20and%20Faisal%20Abdul-Fattah%20and%20Karim%20Elhakim%20and%20Mostafa%20Ibrahim%20and%20Judith%20Fuentes%20and%20Florencia%20Lezcano%20and%20Riccardo%20Collu%20and%20Massimo%20Barbaro%20and%20Stefano%20Lai%20and%20Samuel%20Sanchez%20and%20Andrew%20Adamatzky%0AAbstract%3A%20Skeletal%20muscle-based%20biohybrid%20actuators%20have%20proved%20to%20be%20a%20promising%20component%20in%20soft%20robotics%2C%20offering%20efficient%20movement.%20However%2C%20their%20intrinsic%20biological%20variability%20and%20nonlinearity%20pose%20significant%20challenges%20for%20controllability%20and%20predictability.%20To%20address%20these%20issues%2C%20this%20study%20investigates%20the%20application%20of%20supervised%20learning%2C%20a%20form%20of%20machine%20learning%2C%20to%20model%20and%20predict%20the%20behavior%20of%20biohybrid%20machines%20%28BHMs%29%2C%20focusing%20on%20a%20muscle%20ring%20anchored%20on%20flexible%20polymer%20pillars.%20First%2C%20static%20prediction%20models%20%28i.e.%2C%20random%20forest%20and%20neural%20network%20regressors%29%20are%20trained%20to%20estimate%20the%20maximum%20exerted%20force%20achieved%20from%20input%20variables%20such%20as%20muscle%20sample%2C%20electrical%20stimulation%20parameters%2C%20and%20baseline%20exerted%20force.%20Second%2C%20a%20dynamic%20modeling%20framework%2C%20based%20on%20Long%20Short-Term%20Memory%20networks%2C%20is%20developed%20to%20serve%20as%20a%20digital%20twin%2C%20replicating%20the%20time%20series%20of%20exerted%20forces%20observed%20in%20response%20to%20electrical%20stimulation.%20Both%20modeling%20approaches%20demonstrate%20high%20predictive%20accuracy.%20The%20best%20performance%20of%20the%20static%20models%20is%20characterized%20by%20R2%20of%200.9425%2C%20whereas%20the%20dynamic%20model%20achieves%20R2%20of%200.9956.%20The%20static%20models%20can%20enable%20optimization%20of%20muscle%20actuator%20performance%20for%20targeted%20applications%20and%20required%20force%20outcomes%2C%20while%20the%20dynamic%20model%20provides%20a%20foundation%20for%20developing%20robustly%20adaptive%20control%20strategies%20in%20future%20biohybrid%20robotic%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Driven%2520Prediction%2520of%2520the%2520Behavior%2520of%2520Biohybrid%2520Actuators%26entry.906535625%3DMichail-Antisthenis%2520Tsompanas%2520and%2520Marco%2520Perez%2520Hernandez%2520and%2520Faisal%2520Abdul-Fattah%2520and%2520Karim%2520Elhakim%2520and%2520Mostafa%2520Ibrahim%2520and%2520Judith%2520Fuentes%2520and%2520Florencia%2520Lezcano%2520and%2520Riccardo%2520Collu%2520and%2520Massimo%2520Barbaro%2520and%2520Stefano%2520Lai%2520and%2520Samuel%2520Sanchez%2520and%2520Andrew%2520Adamatzky%26entry.1292438233%3DSkeletal%2520muscle-based%2520biohybrid%2520actuators%2520have%2520proved%2520to%2520be%2520a%2520promising%2520component%2520in%2520soft%2520robotics%252C%2520offering%2520efficient%2520movement.%2520However%252C%2520their%2520intrinsic%2520biological%2520variability%2520and%2520nonlinearity%2520pose%2520significant%2520challenges%2520for%2520controllability%2520and%2520predictability.%2520To%2520address%2520these%2520issues%252C%2520this%2520study%2520investigates%2520the%2520application%2520of%2520supervised%2520learning%252C%2520a%2520form%2520of%2520machine%2520learning%252C%2520to%2520model%2520and%2520predict%2520the%2520behavior%2520of%2520biohybrid%2520machines%2520%2528BHMs%2529%252C%2520focusing%2520on%2520a%2520muscle%2520ring%2520anchored%2520on%2520flexible%2520polymer%2520pillars.%2520First%252C%2520static%2520prediction%2520models%2520%2528i.e.%252C%2520random%2520forest%2520and%2520neural%2520network%2520regressors%2529%2520are%2520trained%2520to%2520estimate%2520the%2520maximum%2520exerted%2520force%2520achieved%2520from%2520input%2520variables%2520such%2520as%2520muscle%2520sample%252C%2520electrical%2520stimulation%2520parameters%252C%2520and%2520baseline%2520exerted%2520force.%2520Second%252C%2520a%2520dynamic%2520modeling%2520framework%252C%2520based%2520on%2520Long%2520Short-Term%2520Memory%2520networks%252C%2520is%2520developed%2520to%2520serve%2520as%2520a%2520digital%2520twin%252C%2520replicating%2520the%2520time%2520series%2520of%2520exerted%2520forces%2520observed%2520in%2520response%2520to%2520electrical%2520stimulation.%2520Both%2520modeling%2520approaches%2520demonstrate%2520high%2520predictive%2520accuracy.%2520The%2520best%2520performance%2520of%2520the%2520static%2520models%2520is%2520characterized%2520by%2520R2%2520of%25200.9425%252C%2520whereas%2520the%2520dynamic%2520model%2520achieves%2520R2%2520of%25200.9956.%2520The%2520static%2520models%2520can%2520enable%2520optimization%2520of%2520muscle%2520actuator%2520performance%2520for%2520targeted%2520applications%2520and%2520required%2520force%2520outcomes%252C%2520while%2520the%2520dynamic%2520model%2520provides%2520a%2520foundation%2520for%2520developing%2520robustly%2520adaptive%2520control%2520strategies%2520in%2520future%2520biohybrid%2520robotic%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Driven%20Prediction%20of%20the%20Behavior%20of%20Biohybrid%20Actuators&entry.906535625=Michail-Antisthenis%20Tsompanas%20and%20Marco%20Perez%20Hernandez%20and%20Faisal%20Abdul-Fattah%20and%20Karim%20Elhakim%20and%20Mostafa%20Ibrahim%20and%20Judith%20Fuentes%20and%20Florencia%20Lezcano%20and%20Riccardo%20Collu%20and%20Massimo%20Barbaro%20and%20Stefano%20Lai%20and%20Samuel%20Sanchez%20and%20Andrew%20Adamatzky&entry.1292438233=Skeletal%20muscle-based%20biohybrid%20actuators%20have%20proved%20to%20be%20a%20promising%20component%20in%20soft%20robotics%2C%20offering%20efficient%20movement.%20However%2C%20their%20intrinsic%20biological%20variability%20and%20nonlinearity%20pose%20significant%20challenges%20for%20controllability%20and%20predictability.%20To%20address%20these%20issues%2C%20this%20study%20investigates%20the%20application%20of%20supervised%20learning%2C%20a%20form%20of%20machine%20learning%2C%20to%20model%20and%20predict%20the%20behavior%20of%20biohybrid%20machines%20%28BHMs%29%2C%20focusing%20on%20a%20muscle%20ring%20anchored%20on%20flexible%20polymer%20pillars.%20First%2C%20static%20prediction%20models%20%28i.e.%2C%20random%20forest%20and%20neural%20network%20regressors%29%20are%20trained%20to%20estimate%20the%20maximum%20exerted%20force%20achieved%20from%20input%20variables%20such%20as%20muscle%20sample%2C%20electrical%20stimulation%20parameters%2C%20and%20baseline%20exerted%20force.%20Second%2C%20a%20dynamic%20modeling%20framework%2C%20based%20on%20Long%20Short-Term%20Memory%20networks%2C%20is%20developed%20to%20serve%20as%20a%20digital%20twin%2C%20replicating%20the%20time%20series%20of%20exerted%20forces%20observed%20in%20response%20to%20electrical%20stimulation.%20Both%20modeling%20approaches%20demonstrate%20high%20predictive%20accuracy.%20The%20best%20performance%20of%20the%20static%20models%20is%20characterized%20by%20R2%20of%200.9425%2C%20whereas%20the%20dynamic%20model%20achieves%20R2%20of%200.9956.%20The%20static%20models%20can%20enable%20optimization%20of%20muscle%20actuator%20performance%20for%20targeted%20applications%20and%20required%20force%20outcomes%2C%20while%20the%20dynamic%20model%20provides%20a%20foundation%20for%20developing%20robustly%20adaptive%20control%20strategies%20in%20future%20biohybrid%20robotic%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2602.16330v1&entry.124074799=Read"},
{"title": "HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs", "author": "Samira Nazari and Mohammad Saeed Almasi and Mahdi Taheri and Ali Azarpeyvand and Ali Mokhtari and Ali Mahani and Christian Herglotz", "abstract": "This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, power, and area, HAWX accelerates the evaluation of candidate configurations, achieving over 23* speedup in a layer-level search with two candidate approximate blocks and more than (3*106)* speedup at the filter-level search only for LeNet-5, while maintaining accuracy comparable to exhaustive search. Experiments across state-of-the-art DNN benchmarks such as VGG-11, ResNet-18, and EfficientNetLite demonstrate that the efficiency benefits of HAWX scale exponentially with network size. The HAWX hardware-aware search algorithm supports both spatial and temporal accelerator architectures, leveraging either off-the-shelf approximate components or customized designs.", "link": "http://arxiv.org/abs/2602.16336v1", "date": "2026-02-18", "relevancy": 2.057, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5557}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5308}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAWX%3A%20A%20Hardware-Aware%20FrameWork%20for%20Fast%20and%20Scalable%20ApproXimation%20of%20DNNs&body=Title%3A%20HAWX%3A%20A%20Hardware-Aware%20FrameWork%20for%20Fast%20and%20Scalable%20ApproXimation%20of%20DNNs%0AAuthor%3A%20Samira%20Nazari%20and%20Mohammad%20Saeed%20Almasi%20and%20Mahdi%20Taheri%20and%20Ali%20Azarpeyvand%20and%20Ali%20Mokhtari%20and%20Ali%20Mahani%20and%20Christian%20Herglotz%0AAbstract%3A%20This%20work%20presents%20HAWX%2C%20a%20hardware-aware%20scalable%20exploration%20framework%20that%20employs%20multi-level%20sensitivity%20scoring%20at%20different%20DNN%20abstraction%20levels%20%28operator%2C%20filter%2C%20layer%2C%20and%20model%29%20to%20guide%20selective%20integration%20of%20heterogeneous%20AxC%20blocks.%20Supported%20by%20predictive%20models%20for%20accuracy%2C%20power%2C%20and%20area%2C%20HAWX%20accelerates%20the%20evaluation%20of%20candidate%20configurations%2C%20achieving%20over%2023%2A%20speedup%20in%20a%20layer-level%20search%20with%20two%20candidate%20approximate%20blocks%20and%20more%20than%20%283%2A106%29%2A%20speedup%20at%20the%20filter-level%20search%20only%20for%20LeNet-5%2C%20while%20maintaining%20accuracy%20comparable%20to%20exhaustive%20search.%20Experiments%20across%20state-of-the-art%20DNN%20benchmarks%20such%20as%20VGG-11%2C%20ResNet-18%2C%20and%20EfficientNetLite%20demonstrate%20that%20the%20efficiency%20benefits%20of%20HAWX%20scale%20exponentially%20with%20network%20size.%20The%20HAWX%20hardware-aware%20search%20algorithm%20supports%20both%20spatial%20and%20temporal%20accelerator%20architectures%2C%20leveraging%20either%20off-the-shelf%20approximate%20components%20or%20customized%20designs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAWX%253A%2520A%2520Hardware-Aware%2520FrameWork%2520for%2520Fast%2520and%2520Scalable%2520ApproXimation%2520of%2520DNNs%26entry.906535625%3DSamira%2520Nazari%2520and%2520Mohammad%2520Saeed%2520Almasi%2520and%2520Mahdi%2520Taheri%2520and%2520Ali%2520Azarpeyvand%2520and%2520Ali%2520Mokhtari%2520and%2520Ali%2520Mahani%2520and%2520Christian%2520Herglotz%26entry.1292438233%3DThis%2520work%2520presents%2520HAWX%252C%2520a%2520hardware-aware%2520scalable%2520exploration%2520framework%2520that%2520employs%2520multi-level%2520sensitivity%2520scoring%2520at%2520different%2520DNN%2520abstraction%2520levels%2520%2528operator%252C%2520filter%252C%2520layer%252C%2520and%2520model%2529%2520to%2520guide%2520selective%2520integration%2520of%2520heterogeneous%2520AxC%2520blocks.%2520Supported%2520by%2520predictive%2520models%2520for%2520accuracy%252C%2520power%252C%2520and%2520area%252C%2520HAWX%2520accelerates%2520the%2520evaluation%2520of%2520candidate%2520configurations%252C%2520achieving%2520over%252023%252A%2520speedup%2520in%2520a%2520layer-level%2520search%2520with%2520two%2520candidate%2520approximate%2520blocks%2520and%2520more%2520than%2520%25283%252A106%2529%252A%2520speedup%2520at%2520the%2520filter-level%2520search%2520only%2520for%2520LeNet-5%252C%2520while%2520maintaining%2520accuracy%2520comparable%2520to%2520exhaustive%2520search.%2520Experiments%2520across%2520state-of-the-art%2520DNN%2520benchmarks%2520such%2520as%2520VGG-11%252C%2520ResNet-18%252C%2520and%2520EfficientNetLite%2520demonstrate%2520that%2520the%2520efficiency%2520benefits%2520of%2520HAWX%2520scale%2520exponentially%2520with%2520network%2520size.%2520The%2520HAWX%2520hardware-aware%2520search%2520algorithm%2520supports%2520both%2520spatial%2520and%2520temporal%2520accelerator%2520architectures%252C%2520leveraging%2520either%2520off-the-shelf%2520approximate%2520components%2520or%2520customized%2520designs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAWX%3A%20A%20Hardware-Aware%20FrameWork%20for%20Fast%20and%20Scalable%20ApproXimation%20of%20DNNs&entry.906535625=Samira%20Nazari%20and%20Mohammad%20Saeed%20Almasi%20and%20Mahdi%20Taheri%20and%20Ali%20Azarpeyvand%20and%20Ali%20Mokhtari%20and%20Ali%20Mahani%20and%20Christian%20Herglotz&entry.1292438233=This%20work%20presents%20HAWX%2C%20a%20hardware-aware%20scalable%20exploration%20framework%20that%20employs%20multi-level%20sensitivity%20scoring%20at%20different%20DNN%20abstraction%20levels%20%28operator%2C%20filter%2C%20layer%2C%20and%20model%29%20to%20guide%20selective%20integration%20of%20heterogeneous%20AxC%20blocks.%20Supported%20by%20predictive%20models%20for%20accuracy%2C%20power%2C%20and%20area%2C%20HAWX%20accelerates%20the%20evaluation%20of%20candidate%20configurations%2C%20achieving%20over%2023%2A%20speedup%20in%20a%20layer-level%20search%20with%20two%20candidate%20approximate%20blocks%20and%20more%20than%20%283%2A106%29%2A%20speedup%20at%20the%20filter-level%20search%20only%20for%20LeNet-5%2C%20while%20maintaining%20accuracy%20comparable%20to%20exhaustive%20search.%20Experiments%20across%20state-of-the-art%20DNN%20benchmarks%20such%20as%20VGG-11%2C%20ResNet-18%2C%20and%20EfficientNetLite%20demonstrate%20that%20the%20efficiency%20benefits%20of%20HAWX%20scale%20exponentially%20with%20network%20size.%20The%20HAWX%20hardware-aware%20search%20algorithm%20supports%20both%20spatial%20and%20temporal%20accelerator%20architectures%2C%20leveraging%20either%20off-the-shelf%20approximate%20components%20or%20customized%20designs.&entry.1838667208=http%3A//arxiv.org/abs/2602.16336v1&entry.124074799=Read"},
{"title": "SCAR: Satellite Imagery-Based Calibration for Aerial Recordings", "author": "Henry H\u00f6lzemann and Michael Schleiss", "abstract": "We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.", "link": "http://arxiv.org/abs/2602.16349v1", "date": "2026-02-18", "relevancy": 2.0549, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5241}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5127}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAR%3A%20Satellite%20Imagery-Based%20Calibration%20for%20Aerial%20Recordings&body=Title%3A%20SCAR%3A%20Satellite%20Imagery-Based%20Calibration%20for%20Aerial%20Recordings%0AAuthor%3A%20Henry%20H%C3%B6lzemann%20and%20Michael%20Schleiss%0AAbstract%3A%20We%20introduce%20SCAR%2C%20a%20method%20for%20long-term%20auto-calibration%20refinement%20of%20aerial%20visual-inertial%20systems%20that%20exploits%20georeferenced%20satellite%20imagery%20as%20a%20persistent%20global%20reference.%20SCAR%20estimates%20both%20intrinsic%20and%20extrinsic%20parameters%20by%20aligning%20aerial%20images%20with%202D--3D%20correspondences%20derived%20from%20publicly%20available%20orthophotos%20and%20elevation%20models.%20In%20contrast%20to%20existing%20approaches%20that%20rely%20on%20dedicated%20calibration%20maneuvers%20or%20manually%20surveyed%20ground%20control%20points%2C%20our%20method%20leverages%20external%20geospatial%20data%20to%20detect%20and%20correct%20calibration%20degradation%20under%20field%20deployment%20conditions.%20We%20evaluate%20our%20approach%20on%20six%20large-scale%20aerial%20campaigns%20conducted%20over%20two%20years%20under%20diverse%20seasonal%20and%20environmental%20conditions.%20Across%20all%20sequences%2C%20SCAR%20consistently%20outperforms%20established%20baselines%20%28Kalibr%2C%20COLMAP%2C%20VINS-Mono%29%2C%20reducing%20median%20reprojection%20error%20by%20a%20large%20margin%2C%20and%20translating%20these%20calibration%20gains%20into%20substantially%20lower%20visual%20localization%20rotation%20errors%20and%20higher%20pose%20accuracy.%20These%20results%20demonstrate%20that%20SCAR%20provides%20accurate%2C%20robust%2C%20and%20reproducible%20calibration%20over%20long-term%20aerial%20operations%20without%20the%20need%20for%20manual%20intervention.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAR%253A%2520Satellite%2520Imagery-Based%2520Calibration%2520for%2520Aerial%2520Recordings%26entry.906535625%3DHenry%2520H%25C3%25B6lzemann%2520and%2520Michael%2520Schleiss%26entry.1292438233%3DWe%2520introduce%2520SCAR%252C%2520a%2520method%2520for%2520long-term%2520auto-calibration%2520refinement%2520of%2520aerial%2520visual-inertial%2520systems%2520that%2520exploits%2520georeferenced%2520satellite%2520imagery%2520as%2520a%2520persistent%2520global%2520reference.%2520SCAR%2520estimates%2520both%2520intrinsic%2520and%2520extrinsic%2520parameters%2520by%2520aligning%2520aerial%2520images%2520with%25202D--3D%2520correspondences%2520derived%2520from%2520publicly%2520available%2520orthophotos%2520and%2520elevation%2520models.%2520In%2520contrast%2520to%2520existing%2520approaches%2520that%2520rely%2520on%2520dedicated%2520calibration%2520maneuvers%2520or%2520manually%2520surveyed%2520ground%2520control%2520points%252C%2520our%2520method%2520leverages%2520external%2520geospatial%2520data%2520to%2520detect%2520and%2520correct%2520calibration%2520degradation%2520under%2520field%2520deployment%2520conditions.%2520We%2520evaluate%2520our%2520approach%2520on%2520six%2520large-scale%2520aerial%2520campaigns%2520conducted%2520over%2520two%2520years%2520under%2520diverse%2520seasonal%2520and%2520environmental%2520conditions.%2520Across%2520all%2520sequences%252C%2520SCAR%2520consistently%2520outperforms%2520established%2520baselines%2520%2528Kalibr%252C%2520COLMAP%252C%2520VINS-Mono%2529%252C%2520reducing%2520median%2520reprojection%2520error%2520by%2520a%2520large%2520margin%252C%2520and%2520translating%2520these%2520calibration%2520gains%2520into%2520substantially%2520lower%2520visual%2520localization%2520rotation%2520errors%2520and%2520higher%2520pose%2520accuracy.%2520These%2520results%2520demonstrate%2520that%2520SCAR%2520provides%2520accurate%252C%2520robust%252C%2520and%2520reproducible%2520calibration%2520over%2520long-term%2520aerial%2520operations%2520without%2520the%2520need%2520for%2520manual%2520intervention.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAR%3A%20Satellite%20Imagery-Based%20Calibration%20for%20Aerial%20Recordings&entry.906535625=Henry%20H%C3%B6lzemann%20and%20Michael%20Schleiss&entry.1292438233=We%20introduce%20SCAR%2C%20a%20method%20for%20long-term%20auto-calibration%20refinement%20of%20aerial%20visual-inertial%20systems%20that%20exploits%20georeferenced%20satellite%20imagery%20as%20a%20persistent%20global%20reference.%20SCAR%20estimates%20both%20intrinsic%20and%20extrinsic%20parameters%20by%20aligning%20aerial%20images%20with%202D--3D%20correspondences%20derived%20from%20publicly%20available%20orthophotos%20and%20elevation%20models.%20In%20contrast%20to%20existing%20approaches%20that%20rely%20on%20dedicated%20calibration%20maneuvers%20or%20manually%20surveyed%20ground%20control%20points%2C%20our%20method%20leverages%20external%20geospatial%20data%20to%20detect%20and%20correct%20calibration%20degradation%20under%20field%20deployment%20conditions.%20We%20evaluate%20our%20approach%20on%20six%20large-scale%20aerial%20campaigns%20conducted%20over%20two%20years%20under%20diverse%20seasonal%20and%20environmental%20conditions.%20Across%20all%20sequences%2C%20SCAR%20consistently%20outperforms%20established%20baselines%20%28Kalibr%2C%20COLMAP%2C%20VINS-Mono%29%2C%20reducing%20median%20reprojection%20error%20by%20a%20large%20margin%2C%20and%20translating%20these%20calibration%20gains%20into%20substantially%20lower%20visual%20localization%20rotation%20errors%20and%20higher%20pose%20accuracy.%20These%20results%20demonstrate%20that%20SCAR%20provides%20accurate%2C%20robust%2C%20and%20reproducible%20calibration%20over%20long-term%20aerial%20operations%20without%20the%20need%20for%20manual%20intervention.&entry.1838667208=http%3A//arxiv.org/abs/2602.16349v1&entry.124074799=Read"},
{"title": "Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment", "author": "Shuta Kikuchi and Shu Tanaka", "abstract": "The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.", "link": "http://arxiv.org/abs/2602.16643v1", "date": "2026-02-18", "relevancy": 2.046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4168}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4168}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Factorization%20Machine%20with%20Quadratic-Optimization%20Annealing%20for%20RNA%20Inverse%20Folding%20and%20Evaluation%20of%20Binary-Integer%20Encoding%20and%20Nucleotide%20Assignment&body=Title%3A%20Factorization%20Machine%20with%20Quadratic-Optimization%20Annealing%20for%20RNA%20Inverse%20Folding%20and%20Evaluation%20of%20Binary-Integer%20Encoding%20and%20Nucleotide%20Assignment%0AAuthor%3A%20Shuta%20Kikuchi%20and%20Shu%20Tanaka%0AAbstract%3A%20The%20RNA%20inverse%20folding%20problem%20aims%20to%20identify%20nucleotide%20sequences%20that%20preferentially%20adopt%20a%20given%20target%20secondary%20structure.%20While%20various%20heuristic%20and%20machine%20learning-based%20approaches%20have%20been%20proposed%2C%20many%20require%20a%20large%20number%20of%20sequence%20evaluations%2C%20which%20limits%20their%20applicability%20when%20experimental%20validation%20is%20costly.%20We%20propose%20a%20method%20to%20solve%20the%20problem%20using%20a%20factorization%20machine%20with%20quadratic-optimization%20annealing%20%28FMQA%29.%20FMQA%20is%20a%20discrete%20black-box%20optimization%20method%20reported%20to%20obtain%20high-quality%20solutions%20with%20a%20limited%20number%20of%20evaluations.%20Applying%20FMQA%20to%20the%20problem%20requires%20converting%20nucleotides%20into%20binary%20variables.%20However%2C%20the%20influence%20of%20integer-to-nucleotide%20assignments%20and%20binary-integer%20encoding%20on%20the%20performance%20of%20FMQA%20has%20not%20been%20thoroughly%20investigated%2C%20even%20though%20such%20choices%20determine%20the%20structure%20of%20the%20surrogate%20model%20and%20the%20search%20landscape%2C%20and%20thus%20can%20directly%20affect%20solution%20quality.%20Therefore%2C%20this%20study%20aims%20both%20to%20establish%20a%20novel%20FMQA%20framework%20for%20RNA%20inverse%20folding%20and%20to%20analyze%20the%20effects%20of%20these%20assignments%20and%20encoding%20methods.%20We%20evaluated%20all%2024%20possible%20assignments%20of%20the%20four%20nucleotides%20to%20the%20ordered%20integers%20%280-3%29%2C%20in%20combination%20with%20four%20binary-integer%20encoding%20methods.%20Our%20results%20demonstrated%20that%20one-hot%20and%20domain-wall%20encodings%20outperform%20binary%20and%20unary%20encodings%20in%20terms%20of%20the%20normalized%20ensemble%20defect%20value.%20In%20domain-wall%20encoding%2C%20nucleotides%20assigned%20to%20the%20boundary%20integers%20%280%20and%203%29%20appeared%20with%20higher%20frequency.%20In%20the%20RNA%20inverse%20folding%20problem%2C%20assigning%20guanine%20and%20cytosine%20to%20these%20boundary%20integers%20promoted%20their%20enrichment%20in%20stem%20regions%2C%20which%20led%20to%20more%20thermodynamically%20stable%20secondary%20structures%20than%20those%20obtained%20with%20one-hot%20encoding.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFactorization%2520Machine%2520with%2520Quadratic-Optimization%2520Annealing%2520for%2520RNA%2520Inverse%2520Folding%2520and%2520Evaluation%2520of%2520Binary-Integer%2520Encoding%2520and%2520Nucleotide%2520Assignment%26entry.906535625%3DShuta%2520Kikuchi%2520and%2520Shu%2520Tanaka%26entry.1292438233%3DThe%2520RNA%2520inverse%2520folding%2520problem%2520aims%2520to%2520identify%2520nucleotide%2520sequences%2520that%2520preferentially%2520adopt%2520a%2520given%2520target%2520secondary%2520structure.%2520While%2520various%2520heuristic%2520and%2520machine%2520learning-based%2520approaches%2520have%2520been%2520proposed%252C%2520many%2520require%2520a%2520large%2520number%2520of%2520sequence%2520evaluations%252C%2520which%2520limits%2520their%2520applicability%2520when%2520experimental%2520validation%2520is%2520costly.%2520We%2520propose%2520a%2520method%2520to%2520solve%2520the%2520problem%2520using%2520a%2520factorization%2520machine%2520with%2520quadratic-optimization%2520annealing%2520%2528FMQA%2529.%2520FMQA%2520is%2520a%2520discrete%2520black-box%2520optimization%2520method%2520reported%2520to%2520obtain%2520high-quality%2520solutions%2520with%2520a%2520limited%2520number%2520of%2520evaluations.%2520Applying%2520FMQA%2520to%2520the%2520problem%2520requires%2520converting%2520nucleotides%2520into%2520binary%2520variables.%2520However%252C%2520the%2520influence%2520of%2520integer-to-nucleotide%2520assignments%2520and%2520binary-integer%2520encoding%2520on%2520the%2520performance%2520of%2520FMQA%2520has%2520not%2520been%2520thoroughly%2520investigated%252C%2520even%2520though%2520such%2520choices%2520determine%2520the%2520structure%2520of%2520the%2520surrogate%2520model%2520and%2520the%2520search%2520landscape%252C%2520and%2520thus%2520can%2520directly%2520affect%2520solution%2520quality.%2520Therefore%252C%2520this%2520study%2520aims%2520both%2520to%2520establish%2520a%2520novel%2520FMQA%2520framework%2520for%2520RNA%2520inverse%2520folding%2520and%2520to%2520analyze%2520the%2520effects%2520of%2520these%2520assignments%2520and%2520encoding%2520methods.%2520We%2520evaluated%2520all%252024%2520possible%2520assignments%2520of%2520the%2520four%2520nucleotides%2520to%2520the%2520ordered%2520integers%2520%25280-3%2529%252C%2520in%2520combination%2520with%2520four%2520binary-integer%2520encoding%2520methods.%2520Our%2520results%2520demonstrated%2520that%2520one-hot%2520and%2520domain-wall%2520encodings%2520outperform%2520binary%2520and%2520unary%2520encodings%2520in%2520terms%2520of%2520the%2520normalized%2520ensemble%2520defect%2520value.%2520In%2520domain-wall%2520encoding%252C%2520nucleotides%2520assigned%2520to%2520the%2520boundary%2520integers%2520%25280%2520and%25203%2529%2520appeared%2520with%2520higher%2520frequency.%2520In%2520the%2520RNA%2520inverse%2520folding%2520problem%252C%2520assigning%2520guanine%2520and%2520cytosine%2520to%2520these%2520boundary%2520integers%2520promoted%2520their%2520enrichment%2520in%2520stem%2520regions%252C%2520which%2520led%2520to%2520more%2520thermodynamically%2520stable%2520secondary%2520structures%2520than%2520those%2520obtained%2520with%2520one-hot%2520encoding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Factorization%20Machine%20with%20Quadratic-Optimization%20Annealing%20for%20RNA%20Inverse%20Folding%20and%20Evaluation%20of%20Binary-Integer%20Encoding%20and%20Nucleotide%20Assignment&entry.906535625=Shuta%20Kikuchi%20and%20Shu%20Tanaka&entry.1292438233=The%20RNA%20inverse%20folding%20problem%20aims%20to%20identify%20nucleotide%20sequences%20that%20preferentially%20adopt%20a%20given%20target%20secondary%20structure.%20While%20various%20heuristic%20and%20machine%20learning-based%20approaches%20have%20been%20proposed%2C%20many%20require%20a%20large%20number%20of%20sequence%20evaluations%2C%20which%20limits%20their%20applicability%20when%20experimental%20validation%20is%20costly.%20We%20propose%20a%20method%20to%20solve%20the%20problem%20using%20a%20factorization%20machine%20with%20quadratic-optimization%20annealing%20%28FMQA%29.%20FMQA%20is%20a%20discrete%20black-box%20optimization%20method%20reported%20to%20obtain%20high-quality%20solutions%20with%20a%20limited%20number%20of%20evaluations.%20Applying%20FMQA%20to%20the%20problem%20requires%20converting%20nucleotides%20into%20binary%20variables.%20However%2C%20the%20influence%20of%20integer-to-nucleotide%20assignments%20and%20binary-integer%20encoding%20on%20the%20performance%20of%20FMQA%20has%20not%20been%20thoroughly%20investigated%2C%20even%20though%20such%20choices%20determine%20the%20structure%20of%20the%20surrogate%20model%20and%20the%20search%20landscape%2C%20and%20thus%20can%20directly%20affect%20solution%20quality.%20Therefore%2C%20this%20study%20aims%20both%20to%20establish%20a%20novel%20FMQA%20framework%20for%20RNA%20inverse%20folding%20and%20to%20analyze%20the%20effects%20of%20these%20assignments%20and%20encoding%20methods.%20We%20evaluated%20all%2024%20possible%20assignments%20of%20the%20four%20nucleotides%20to%20the%20ordered%20integers%20%280-3%29%2C%20in%20combination%20with%20four%20binary-integer%20encoding%20methods.%20Our%20results%20demonstrated%20that%20one-hot%20and%20domain-wall%20encodings%20outperform%20binary%20and%20unary%20encodings%20in%20terms%20of%20the%20normalized%20ensemble%20defect%20value.%20In%20domain-wall%20encoding%2C%20nucleotides%20assigned%20to%20the%20boundary%20integers%20%280%20and%203%29%20appeared%20with%20higher%20frequency.%20In%20the%20RNA%20inverse%20folding%20problem%2C%20assigning%20guanine%20and%20cytosine%20to%20these%20boundary%20integers%20promoted%20their%20enrichment%20in%20stem%20regions%2C%20which%20led%20to%20more%20thermodynamically%20stable%20secondary%20structures%20than%20those%20obtained%20with%20one-hot%20encoding.&entry.1838667208=http%3A//arxiv.org/abs/2602.16643v1&entry.124074799=Read"},
{"title": "Knowledge-Based Design Requirements for Generative Social Robots in Higher Education", "author": "Stephan Vonschallen and Dominique Oberle and Theresa Schmiedel and Friederike Eyssel", "abstract": "Generative social robots (GSRs) powered by large language models enable adaptive, conversational tutoring but also introduce risks such as hallucinations, overreliance, and privacy violations. Existing frameworks for educational technologies and responsible AI primarily define desired behaviors, yet they rarely specify the knowledge prerequisites that enable generative systems to express these behaviors reliably. To address this gap, we adopt a knowledge-based design perspective and investigate what information tutoring-oriented GSRs require to function responsibly and effectively in higher education. Based on twelve semi-structured interviews with university students and lecturers, we identify twelve design requirements across three knowledge types: self-knowledge (assertive, conscientious and friendly personality with customizable role), user-knowledge (personalized information about student learning goals, learning progress, motivation type, emotional state and background), and context-knowledge (learning materials, educational strategies, course-related information, and physical learning environment). By identifying these knowledge requirements, this work provides a structured foundation for the design of tutoring GSRs and future evaluations, aligning generative system capabilities with pedagogical and ethical expectations.", "link": "http://arxiv.org/abs/2602.12873v2", "date": "2026-02-18", "relevancy": 2.0425, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5215}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5168}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Based%20Design%20Requirements%20for%20Generative%20Social%20Robots%20in%20Higher%20Education&body=Title%3A%20Knowledge-Based%20Design%20Requirements%20for%20Generative%20Social%20Robots%20in%20Higher%20Education%0AAuthor%3A%20Stephan%20Vonschallen%20and%20Dominique%20Oberle%20and%20Theresa%20Schmiedel%20and%20Friederike%20Eyssel%0AAbstract%3A%20Generative%20social%20robots%20%28GSRs%29%20powered%20by%20large%20language%20models%20enable%20adaptive%2C%20conversational%20tutoring%20but%20also%20introduce%20risks%20such%20as%20hallucinations%2C%20overreliance%2C%20and%20privacy%20violations.%20Existing%20frameworks%20for%20educational%20technologies%20and%20responsible%20AI%20primarily%20define%20desired%20behaviors%2C%20yet%20they%20rarely%20specify%20the%20knowledge%20prerequisites%20that%20enable%20generative%20systems%20to%20express%20these%20behaviors%20reliably.%20To%20address%20this%20gap%2C%20we%20adopt%20a%20knowledge-based%20design%20perspective%20and%20investigate%20what%20information%20tutoring-oriented%20GSRs%20require%20to%20function%20responsibly%20and%20effectively%20in%20higher%20education.%20Based%20on%20twelve%20semi-structured%20interviews%20with%20university%20students%20and%20lecturers%2C%20we%20identify%20twelve%20design%20requirements%20across%20three%20knowledge%20types%3A%20self-knowledge%20%28assertive%2C%20conscientious%20and%20friendly%20personality%20with%20customizable%20role%29%2C%20user-knowledge%20%28personalized%20information%20about%20student%20learning%20goals%2C%20learning%20progress%2C%20motivation%20type%2C%20emotional%20state%20and%20background%29%2C%20and%20context-knowledge%20%28learning%20materials%2C%20educational%20strategies%2C%20course-related%20information%2C%20and%20physical%20learning%20environment%29.%20By%20identifying%20these%20knowledge%20requirements%2C%20this%20work%20provides%20a%20structured%20foundation%20for%20the%20design%20of%20tutoring%20GSRs%20and%20future%20evaluations%2C%20aligning%20generative%20system%20capabilities%20with%20pedagogical%20and%20ethical%20expectations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.12873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Based%2520Design%2520Requirements%2520for%2520Generative%2520Social%2520Robots%2520in%2520Higher%2520Education%26entry.906535625%3DStephan%2520Vonschallen%2520and%2520Dominique%2520Oberle%2520and%2520Theresa%2520Schmiedel%2520and%2520Friederike%2520Eyssel%26entry.1292438233%3DGenerative%2520social%2520robots%2520%2528GSRs%2529%2520powered%2520by%2520large%2520language%2520models%2520enable%2520adaptive%252C%2520conversational%2520tutoring%2520but%2520also%2520introduce%2520risks%2520such%2520as%2520hallucinations%252C%2520overreliance%252C%2520and%2520privacy%2520violations.%2520Existing%2520frameworks%2520for%2520educational%2520technologies%2520and%2520responsible%2520AI%2520primarily%2520define%2520desired%2520behaviors%252C%2520yet%2520they%2520rarely%2520specify%2520the%2520knowledge%2520prerequisites%2520that%2520enable%2520generative%2520systems%2520to%2520express%2520these%2520behaviors%2520reliably.%2520To%2520address%2520this%2520gap%252C%2520we%2520adopt%2520a%2520knowledge-based%2520design%2520perspective%2520and%2520investigate%2520what%2520information%2520tutoring-oriented%2520GSRs%2520require%2520to%2520function%2520responsibly%2520and%2520effectively%2520in%2520higher%2520education.%2520Based%2520on%2520twelve%2520semi-structured%2520interviews%2520with%2520university%2520students%2520and%2520lecturers%252C%2520we%2520identify%2520twelve%2520design%2520requirements%2520across%2520three%2520knowledge%2520types%253A%2520self-knowledge%2520%2528assertive%252C%2520conscientious%2520and%2520friendly%2520personality%2520with%2520customizable%2520role%2529%252C%2520user-knowledge%2520%2528personalized%2520information%2520about%2520student%2520learning%2520goals%252C%2520learning%2520progress%252C%2520motivation%2520type%252C%2520emotional%2520state%2520and%2520background%2529%252C%2520and%2520context-knowledge%2520%2528learning%2520materials%252C%2520educational%2520strategies%252C%2520course-related%2520information%252C%2520and%2520physical%2520learning%2520environment%2529.%2520By%2520identifying%2520these%2520knowledge%2520requirements%252C%2520this%2520work%2520provides%2520a%2520structured%2520foundation%2520for%2520the%2520design%2520of%2520tutoring%2520GSRs%2520and%2520future%2520evaluations%252C%2520aligning%2520generative%2520system%2520capabilities%2520with%2520pedagogical%2520and%2520ethical%2520expectations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.12873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Based%20Design%20Requirements%20for%20Generative%20Social%20Robots%20in%20Higher%20Education&entry.906535625=Stephan%20Vonschallen%20and%20Dominique%20Oberle%20and%20Theresa%20Schmiedel%20and%20Friederike%20Eyssel&entry.1292438233=Generative%20social%20robots%20%28GSRs%29%20powered%20by%20large%20language%20models%20enable%20adaptive%2C%20conversational%20tutoring%20but%20also%20introduce%20risks%20such%20as%20hallucinations%2C%20overreliance%2C%20and%20privacy%20violations.%20Existing%20frameworks%20for%20educational%20technologies%20and%20responsible%20AI%20primarily%20define%20desired%20behaviors%2C%20yet%20they%20rarely%20specify%20the%20knowledge%20prerequisites%20that%20enable%20generative%20systems%20to%20express%20these%20behaviors%20reliably.%20To%20address%20this%20gap%2C%20we%20adopt%20a%20knowledge-based%20design%20perspective%20and%20investigate%20what%20information%20tutoring-oriented%20GSRs%20require%20to%20function%20responsibly%20and%20effectively%20in%20higher%20education.%20Based%20on%20twelve%20semi-structured%20interviews%20with%20university%20students%20and%20lecturers%2C%20we%20identify%20twelve%20design%20requirements%20across%20three%20knowledge%20types%3A%20self-knowledge%20%28assertive%2C%20conscientious%20and%20friendly%20personality%20with%20customizable%20role%29%2C%20user-knowledge%20%28personalized%20information%20about%20student%20learning%20goals%2C%20learning%20progress%2C%20motivation%20type%2C%20emotional%20state%20and%20background%29%2C%20and%20context-knowledge%20%28learning%20materials%2C%20educational%20strategies%2C%20course-related%20information%2C%20and%20physical%20learning%20environment%29.%20By%20identifying%20these%20knowledge%20requirements%2C%20this%20work%20provides%20a%20structured%20foundation%20for%20the%20design%20of%20tutoring%20GSRs%20and%20future%20evaluations%2C%20aligning%20generative%20system%20capabilities%20with%20pedagogical%20and%20ethical%20expectations.&entry.1838667208=http%3A//arxiv.org/abs/2602.12873v2&entry.124074799=Read"},
{"title": "Knowledge-Embedded Latent Projection for Robust Representation Learning", "author": "Weijing Tang and Ming Yuan and Zongqi Xia and Tianxi Cai", "abstract": "Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.", "link": "http://arxiv.org/abs/2602.16709v1", "date": "2026-02-18", "relevancy": 2.0312, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5104}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Embedded%20Latent%20Projection%20for%20Robust%20Representation%20Learning&body=Title%3A%20Knowledge-Embedded%20Latent%20Projection%20for%20Robust%20Representation%20Learning%0AAuthor%3A%20Weijing%20Tang%20and%20Ming%20Yuan%20and%20Zongqi%20Xia%20and%20Tianxi%20Cai%0AAbstract%3A%20Latent%20space%20models%20are%20widely%20used%20for%20analyzing%20high-dimensional%20discrete%20data%20matrices%2C%20such%20as%20patient-feature%20matrices%20in%20electronic%20health%20records%20%28EHRs%29%2C%20by%20capturing%20complex%20dependence%20structures%20through%20low-dimensional%20embeddings.%20However%2C%20estimation%20becomes%20challenging%20in%20the%20imbalanced%20regime%2C%20where%20one%20matrix%20dimension%20is%20much%20larger%20than%20the%20other.%20In%20EHR%20applications%2C%20cohort%20sizes%20are%20often%20limited%20by%20disease%20prevalence%20or%20data%20availability%2C%20whereas%20the%20feature%20space%20remains%20extremely%20large%20due%20to%20the%20breadth%20of%20medical%20coding%20system.%20Motivated%20by%20the%20increasing%20availability%20of%20external%20semantic%20embeddings%2C%20such%20as%20pre-trained%20embeddings%20of%20clinical%20concepts%20in%20EHRs%2C%20we%20propose%20a%20knowledge-embedded%20latent%20projection%20model%20that%20leverages%20semantic%20side%20information%20to%20regularize%20representation%20learning.%20Specifically%2C%20we%20model%20column%20embeddings%20as%20smooth%20functions%20of%20semantic%20embeddings%20via%20a%20mapping%20in%20a%20reproducing%20kernel%20Hilbert%20space.%20We%20develop%20a%20computationally%20efficient%20two-step%20estimation%20procedure%20that%20combines%20semantically%20guided%20subspace%20construction%20via%20kernel%20principal%20component%20analysis%20with%20scalable%20projected%20gradient%20descent.%20We%20establish%20estimation%20error%20bounds%20that%20characterize%20the%20trade-off%20between%20statistical%20error%20and%20approximation%20error%20induced%20by%20the%20kernel%20projection.%20Furthermore%2C%20we%20provide%20local%20convergence%20guarantees%20for%20our%20non-convex%20optimization%20procedure.%20Extensive%20simulation%20studies%20and%20a%20real-world%20EHR%20application%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Embedded%2520Latent%2520Projection%2520for%2520Robust%2520Representation%2520Learning%26entry.906535625%3DWeijing%2520Tang%2520and%2520Ming%2520Yuan%2520and%2520Zongqi%2520Xia%2520and%2520Tianxi%2520Cai%26entry.1292438233%3DLatent%2520space%2520models%2520are%2520widely%2520used%2520for%2520analyzing%2520high-dimensional%2520discrete%2520data%2520matrices%252C%2520such%2520as%2520patient-feature%2520matrices%2520in%2520electronic%2520health%2520records%2520%2528EHRs%2529%252C%2520by%2520capturing%2520complex%2520dependence%2520structures%2520through%2520low-dimensional%2520embeddings.%2520However%252C%2520estimation%2520becomes%2520challenging%2520in%2520the%2520imbalanced%2520regime%252C%2520where%2520one%2520matrix%2520dimension%2520is%2520much%2520larger%2520than%2520the%2520other.%2520In%2520EHR%2520applications%252C%2520cohort%2520sizes%2520are%2520often%2520limited%2520by%2520disease%2520prevalence%2520or%2520data%2520availability%252C%2520whereas%2520the%2520feature%2520space%2520remains%2520extremely%2520large%2520due%2520to%2520the%2520breadth%2520of%2520medical%2520coding%2520system.%2520Motivated%2520by%2520the%2520increasing%2520availability%2520of%2520external%2520semantic%2520embeddings%252C%2520such%2520as%2520pre-trained%2520embeddings%2520of%2520clinical%2520concepts%2520in%2520EHRs%252C%2520we%2520propose%2520a%2520knowledge-embedded%2520latent%2520projection%2520model%2520that%2520leverages%2520semantic%2520side%2520information%2520to%2520regularize%2520representation%2520learning.%2520Specifically%252C%2520we%2520model%2520column%2520embeddings%2520as%2520smooth%2520functions%2520of%2520semantic%2520embeddings%2520via%2520a%2520mapping%2520in%2520a%2520reproducing%2520kernel%2520Hilbert%2520space.%2520We%2520develop%2520a%2520computationally%2520efficient%2520two-step%2520estimation%2520procedure%2520that%2520combines%2520semantically%2520guided%2520subspace%2520construction%2520via%2520kernel%2520principal%2520component%2520analysis%2520with%2520scalable%2520projected%2520gradient%2520descent.%2520We%2520establish%2520estimation%2520error%2520bounds%2520that%2520characterize%2520the%2520trade-off%2520between%2520statistical%2520error%2520and%2520approximation%2520error%2520induced%2520by%2520the%2520kernel%2520projection.%2520Furthermore%252C%2520we%2520provide%2520local%2520convergence%2520guarantees%2520for%2520our%2520non-convex%2520optimization%2520procedure.%2520Extensive%2520simulation%2520studies%2520and%2520a%2520real-world%2520EHR%2520application%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Embedded%20Latent%20Projection%20for%20Robust%20Representation%20Learning&entry.906535625=Weijing%20Tang%20and%20Ming%20Yuan%20and%20Zongqi%20Xia%20and%20Tianxi%20Cai&entry.1292438233=Latent%20space%20models%20are%20widely%20used%20for%20analyzing%20high-dimensional%20discrete%20data%20matrices%2C%20such%20as%20patient-feature%20matrices%20in%20electronic%20health%20records%20%28EHRs%29%2C%20by%20capturing%20complex%20dependence%20structures%20through%20low-dimensional%20embeddings.%20However%2C%20estimation%20becomes%20challenging%20in%20the%20imbalanced%20regime%2C%20where%20one%20matrix%20dimension%20is%20much%20larger%20than%20the%20other.%20In%20EHR%20applications%2C%20cohort%20sizes%20are%20often%20limited%20by%20disease%20prevalence%20or%20data%20availability%2C%20whereas%20the%20feature%20space%20remains%20extremely%20large%20due%20to%20the%20breadth%20of%20medical%20coding%20system.%20Motivated%20by%20the%20increasing%20availability%20of%20external%20semantic%20embeddings%2C%20such%20as%20pre-trained%20embeddings%20of%20clinical%20concepts%20in%20EHRs%2C%20we%20propose%20a%20knowledge-embedded%20latent%20projection%20model%20that%20leverages%20semantic%20side%20information%20to%20regularize%20representation%20learning.%20Specifically%2C%20we%20model%20column%20embeddings%20as%20smooth%20functions%20of%20semantic%20embeddings%20via%20a%20mapping%20in%20a%20reproducing%20kernel%20Hilbert%20space.%20We%20develop%20a%20computationally%20efficient%20two-step%20estimation%20procedure%20that%20combines%20semantically%20guided%20subspace%20construction%20via%20kernel%20principal%20component%20analysis%20with%20scalable%20projected%20gradient%20descent.%20We%20establish%20estimation%20error%20bounds%20that%20characterize%20the%20trade-off%20between%20statistical%20error%20and%20approximation%20error%20induced%20by%20the%20kernel%20projection.%20Furthermore%2C%20we%20provide%20local%20convergence%20guarantees%20for%20our%20non-convex%20optimization%20procedure.%20Extensive%20simulation%20studies%20and%20a%20real-world%20EHR%20application%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.&entry.1838667208=http%3A//arxiv.org/abs/2602.16709v1&entry.124074799=Read"},
{"title": "Forget Forgetting: Continual Learning in a World of Abundant Memory", "author": "Dongkyu Cho and Taesup Moon and Rumi Chunara and Kyunghyun Cho and Sungmin Cha", "abstract": "Continual learning (CL) has traditionally focused on minimizing exemplar memory, a constraint often misaligned with modern systems where GPU time, not storage, is the primary bottleneck. This paper challenges this paradigm by investigating a more realistic regime: one where memory is abundant enough to mitigate forgetting, but full retraining from scratch remains prohibitively expensive. In this practical \"middle ground\", we find that the core challenge shifts from stability to plasticity, as models become biased toward prior tasks and struggle to learn new ones. Conversely, improved stability allows simple replay baselines to outperform the state-of-the-art methods at a fraction of the GPU cost. To address this newly surfaced trade-off, we propose Weight Space Consolidation, a lightweight method that combines (1) rank-based parameter resets to restore plasticity with (2) weight averaging to enhance stability. Validated on both class-incremental learning with image classifiers and continual instruction tuning with large language models, our approach outperforms strong baselines while matching the low computational cost of replay, offering a scalable alternative to expensive full-retraining. These findings challenge long-standing CL assumptions and establish a new, cost-efficient baseline for real-world CL systems where exemplar memory is no longer the limiting factor.", "link": "http://arxiv.org/abs/2502.07274v5", "date": "2026-02-18", "relevancy": 2.0219, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forget%20Forgetting%3A%20Continual%20Learning%20in%20a%20World%20of%20Abundant%20Memory&body=Title%3A%20Forget%20Forgetting%3A%20Continual%20Learning%20in%20a%20World%20of%20Abundant%20Memory%0AAuthor%3A%20Dongkyu%20Cho%20and%20Taesup%20Moon%20and%20Rumi%20Chunara%20and%20Kyunghyun%20Cho%20and%20Sungmin%20Cha%0AAbstract%3A%20Continual%20learning%20%28CL%29%20has%20traditionally%20focused%20on%20minimizing%20exemplar%20memory%2C%20a%20constraint%20often%20misaligned%20with%20modern%20systems%20where%20GPU%20time%2C%20not%20storage%2C%20is%20the%20primary%20bottleneck.%20This%20paper%20challenges%20this%20paradigm%20by%20investigating%20a%20more%20realistic%20regime%3A%20one%20where%20memory%20is%20abundant%20enough%20to%20mitigate%20forgetting%2C%20but%20full%20retraining%20from%20scratch%20remains%20prohibitively%20expensive.%20In%20this%20practical%20%22middle%20ground%22%2C%20we%20find%20that%20the%20core%20challenge%20shifts%20from%20stability%20to%20plasticity%2C%20as%20models%20become%20biased%20toward%20prior%20tasks%20and%20struggle%20to%20learn%20new%20ones.%20Conversely%2C%20improved%20stability%20allows%20simple%20replay%20baselines%20to%20outperform%20the%20state-of-the-art%20methods%20at%20a%20fraction%20of%20the%20GPU%20cost.%20To%20address%20this%20newly%20surfaced%20trade-off%2C%20we%20propose%20Weight%20Space%20Consolidation%2C%20a%20lightweight%20method%20that%20combines%20%281%29%20rank-based%20parameter%20resets%20to%20restore%20plasticity%20with%20%282%29%20weight%20averaging%20to%20enhance%20stability.%20Validated%20on%20both%20class-incremental%20learning%20with%20image%20classifiers%20and%20continual%20instruction%20tuning%20with%20large%20language%20models%2C%20our%20approach%20outperforms%20strong%20baselines%20while%20matching%20the%20low%20computational%20cost%20of%20replay%2C%20offering%20a%20scalable%20alternative%20to%20expensive%20full-retraining.%20These%20findings%20challenge%20long-standing%20CL%20assumptions%20and%20establish%20a%20new%2C%20cost-efficient%20baseline%20for%20real-world%20CL%20systems%20where%20exemplar%20memory%20is%20no%20longer%20the%20limiting%20factor.%0ALink%3A%20http%3A//arxiv.org/abs/2502.07274v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForget%2520Forgetting%253A%2520Continual%2520Learning%2520in%2520a%2520World%2520of%2520Abundant%2520Memory%26entry.906535625%3DDongkyu%2520Cho%2520and%2520Taesup%2520Moon%2520and%2520Rumi%2520Chunara%2520and%2520Kyunghyun%2520Cho%2520and%2520Sungmin%2520Cha%26entry.1292438233%3DContinual%2520learning%2520%2528CL%2529%2520has%2520traditionally%2520focused%2520on%2520minimizing%2520exemplar%2520memory%252C%2520a%2520constraint%2520often%2520misaligned%2520with%2520modern%2520systems%2520where%2520GPU%2520time%252C%2520not%2520storage%252C%2520is%2520the%2520primary%2520bottleneck.%2520This%2520paper%2520challenges%2520this%2520paradigm%2520by%2520investigating%2520a%2520more%2520realistic%2520regime%253A%2520one%2520where%2520memory%2520is%2520abundant%2520enough%2520to%2520mitigate%2520forgetting%252C%2520but%2520full%2520retraining%2520from%2520scratch%2520remains%2520prohibitively%2520expensive.%2520In%2520this%2520practical%2520%2522middle%2520ground%2522%252C%2520we%2520find%2520that%2520the%2520core%2520challenge%2520shifts%2520from%2520stability%2520to%2520plasticity%252C%2520as%2520models%2520become%2520biased%2520toward%2520prior%2520tasks%2520and%2520struggle%2520to%2520learn%2520new%2520ones.%2520Conversely%252C%2520improved%2520stability%2520allows%2520simple%2520replay%2520baselines%2520to%2520outperform%2520the%2520state-of-the-art%2520methods%2520at%2520a%2520fraction%2520of%2520the%2520GPU%2520cost.%2520To%2520address%2520this%2520newly%2520surfaced%2520trade-off%252C%2520we%2520propose%2520Weight%2520Space%2520Consolidation%252C%2520a%2520lightweight%2520method%2520that%2520combines%2520%25281%2529%2520rank-based%2520parameter%2520resets%2520to%2520restore%2520plasticity%2520with%2520%25282%2529%2520weight%2520averaging%2520to%2520enhance%2520stability.%2520Validated%2520on%2520both%2520class-incremental%2520learning%2520with%2520image%2520classifiers%2520and%2520continual%2520instruction%2520tuning%2520with%2520large%2520language%2520models%252C%2520our%2520approach%2520outperforms%2520strong%2520baselines%2520while%2520matching%2520the%2520low%2520computational%2520cost%2520of%2520replay%252C%2520offering%2520a%2520scalable%2520alternative%2520to%2520expensive%2520full-retraining.%2520These%2520findings%2520challenge%2520long-standing%2520CL%2520assumptions%2520and%2520establish%2520a%2520new%252C%2520cost-efficient%2520baseline%2520for%2520real-world%2520CL%2520systems%2520where%2520exemplar%2520memory%2520is%2520no%2520longer%2520the%2520limiting%2520factor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07274v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forget%20Forgetting%3A%20Continual%20Learning%20in%20a%20World%20of%20Abundant%20Memory&entry.906535625=Dongkyu%20Cho%20and%20Taesup%20Moon%20and%20Rumi%20Chunara%20and%20Kyunghyun%20Cho%20and%20Sungmin%20Cha&entry.1292438233=Continual%20learning%20%28CL%29%20has%20traditionally%20focused%20on%20minimizing%20exemplar%20memory%2C%20a%20constraint%20often%20misaligned%20with%20modern%20systems%20where%20GPU%20time%2C%20not%20storage%2C%20is%20the%20primary%20bottleneck.%20This%20paper%20challenges%20this%20paradigm%20by%20investigating%20a%20more%20realistic%20regime%3A%20one%20where%20memory%20is%20abundant%20enough%20to%20mitigate%20forgetting%2C%20but%20full%20retraining%20from%20scratch%20remains%20prohibitively%20expensive.%20In%20this%20practical%20%22middle%20ground%22%2C%20we%20find%20that%20the%20core%20challenge%20shifts%20from%20stability%20to%20plasticity%2C%20as%20models%20become%20biased%20toward%20prior%20tasks%20and%20struggle%20to%20learn%20new%20ones.%20Conversely%2C%20improved%20stability%20allows%20simple%20replay%20baselines%20to%20outperform%20the%20state-of-the-art%20methods%20at%20a%20fraction%20of%20the%20GPU%20cost.%20To%20address%20this%20newly%20surfaced%20trade-off%2C%20we%20propose%20Weight%20Space%20Consolidation%2C%20a%20lightweight%20method%20that%20combines%20%281%29%20rank-based%20parameter%20resets%20to%20restore%20plasticity%20with%20%282%29%20weight%20averaging%20to%20enhance%20stability.%20Validated%20on%20both%20class-incremental%20learning%20with%20image%20classifiers%20and%20continual%20instruction%20tuning%20with%20large%20language%20models%2C%20our%20approach%20outperforms%20strong%20baselines%20while%20matching%20the%20low%20computational%20cost%20of%20replay%2C%20offering%20a%20scalable%20alternative%20to%20expensive%20full-retraining.%20These%20findings%20challenge%20long-standing%20CL%20assumptions%20and%20establish%20a%20new%2C%20cost-efficient%20baseline%20for%20real-world%20CL%20systems%20where%20exemplar%20memory%20is%20no%20longer%20the%20limiting%20factor.&entry.1838667208=http%3A//arxiv.org/abs/2502.07274v5&entry.124074799=Read"},
{"title": "Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding", "author": "Kaiting Liu and Hazel Doughty", "abstract": "Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.", "link": "http://arxiv.org/abs/2602.16545v1", "date": "2026-02-18", "relevancy": 2.0217, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5314}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5043}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Split%20Up%3A%20Zero-Shot%20Classifier%20Edits%20for%20Fine-Grained%20Video%20Understanding&body=Title%3A%20Let%27s%20Split%20Up%3A%20Zero-Shot%20Classifier%20Edits%20for%20Fine-Grained%20Video%20Understanding%0AAuthor%3A%20Kaiting%20Liu%20and%20Hazel%20Doughty%0AAbstract%3A%20Video%20recognition%20models%20are%20typically%20trained%20on%20fixed%20taxonomies%20which%20are%20often%20too%20coarse%2C%20collapsing%20distinctions%20in%20object%2C%20manner%20or%20outcome%20under%20a%20single%20label.%20As%20tasks%20and%20definitions%20evolve%2C%20such%20models%20cannot%20accommodate%20emerging%20distinctions%20and%20collecting%20new%20annotations%20and%20retraining%20to%20accommodate%20such%20changes%20is%20costly.%20To%20address%20these%20challenges%2C%20we%20introduce%20category%20splitting%2C%20a%20new%20task%20where%20an%20existing%20classifier%20is%20edited%20to%20refine%20a%20coarse%20category%20into%20finer%20subcategories%2C%20while%20preserving%20accuracy%20elsewhere.%20We%20propose%20a%20zero-shot%20editing%20method%20that%20leverages%20the%20latent%20compositional%20structure%20of%20video%20classifiers%20to%20expose%20fine-grained%20distinctions%20without%20additional%20data.%20We%20further%20show%20that%20low-shot%20fine-tuning%2C%20while%20simple%2C%20is%20highly%20effective%20and%20benefits%20from%20our%20zero-shot%20initialization.%20Experiments%20on%20our%20new%20video%20benchmarks%20for%20category%20splitting%20demonstrate%20that%20our%20method%20substantially%20outperforms%20vision-language%20baselines%2C%20improving%20accuracy%20on%20the%20newly%20split%20categories%20without%20sacrificing%20performance%20on%20the%20rest.%20Project%20page%3A%20https%3A//kaitingliu.github.io/Category-Splitting/.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Split%2520Up%253A%2520Zero-Shot%2520Classifier%2520Edits%2520for%2520Fine-Grained%2520Video%2520Understanding%26entry.906535625%3DKaiting%2520Liu%2520and%2520Hazel%2520Doughty%26entry.1292438233%3DVideo%2520recognition%2520models%2520are%2520typically%2520trained%2520on%2520fixed%2520taxonomies%2520which%2520are%2520often%2520too%2520coarse%252C%2520collapsing%2520distinctions%2520in%2520object%252C%2520manner%2520or%2520outcome%2520under%2520a%2520single%2520label.%2520As%2520tasks%2520and%2520definitions%2520evolve%252C%2520such%2520models%2520cannot%2520accommodate%2520emerging%2520distinctions%2520and%2520collecting%2520new%2520annotations%2520and%2520retraining%2520to%2520accommodate%2520such%2520changes%2520is%2520costly.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520category%2520splitting%252C%2520a%2520new%2520task%2520where%2520an%2520existing%2520classifier%2520is%2520edited%2520to%2520refine%2520a%2520coarse%2520category%2520into%2520finer%2520subcategories%252C%2520while%2520preserving%2520accuracy%2520elsewhere.%2520We%2520propose%2520a%2520zero-shot%2520editing%2520method%2520that%2520leverages%2520the%2520latent%2520compositional%2520structure%2520of%2520video%2520classifiers%2520to%2520expose%2520fine-grained%2520distinctions%2520without%2520additional%2520data.%2520We%2520further%2520show%2520that%2520low-shot%2520fine-tuning%252C%2520while%2520simple%252C%2520is%2520highly%2520effective%2520and%2520benefits%2520from%2520our%2520zero-shot%2520initialization.%2520Experiments%2520on%2520our%2520new%2520video%2520benchmarks%2520for%2520category%2520splitting%2520demonstrate%2520that%2520our%2520method%2520substantially%2520outperforms%2520vision-language%2520baselines%252C%2520improving%2520accuracy%2520on%2520the%2520newly%2520split%2520categories%2520without%2520sacrificing%2520performance%2520on%2520the%2520rest.%2520Project%2520page%253A%2520https%253A//kaitingliu.github.io/Category-Splitting/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Split%20Up%3A%20Zero-Shot%20Classifier%20Edits%20for%20Fine-Grained%20Video%20Understanding&entry.906535625=Kaiting%20Liu%20and%20Hazel%20Doughty&entry.1292438233=Video%20recognition%20models%20are%20typically%20trained%20on%20fixed%20taxonomies%20which%20are%20often%20too%20coarse%2C%20collapsing%20distinctions%20in%20object%2C%20manner%20or%20outcome%20under%20a%20single%20label.%20As%20tasks%20and%20definitions%20evolve%2C%20such%20models%20cannot%20accommodate%20emerging%20distinctions%20and%20collecting%20new%20annotations%20and%20retraining%20to%20accommodate%20such%20changes%20is%20costly.%20To%20address%20these%20challenges%2C%20we%20introduce%20category%20splitting%2C%20a%20new%20task%20where%20an%20existing%20classifier%20is%20edited%20to%20refine%20a%20coarse%20category%20into%20finer%20subcategories%2C%20while%20preserving%20accuracy%20elsewhere.%20We%20propose%20a%20zero-shot%20editing%20method%20that%20leverages%20the%20latent%20compositional%20structure%20of%20video%20classifiers%20to%20expose%20fine-grained%20distinctions%20without%20additional%20data.%20We%20further%20show%20that%20low-shot%20fine-tuning%2C%20while%20simple%2C%20is%20highly%20effective%20and%20benefits%20from%20our%20zero-shot%20initialization.%20Experiments%20on%20our%20new%20video%20benchmarks%20for%20category%20splitting%20demonstrate%20that%20our%20method%20substantially%20outperforms%20vision-language%20baselines%2C%20improving%20accuracy%20on%20the%20newly%20split%20categories%20without%20sacrificing%20performance%20on%20the%20rest.%20Project%20page%3A%20https%3A//kaitingliu.github.io/Category-Splitting/.&entry.1838667208=http%3A//arxiv.org/abs/2602.16545v1&entry.124074799=Read"},
{"title": "FedEFC: Federated Learning Using Enhanced Forward Correction Against Noisy Labels", "author": "Seunghun Yu and Jin-Hyun Ahn and Joonhyuk Kang", "abstract": "Federated Learning (FL) is a powerful framework for privacy-preserving distributed learning. It enables multiple clients to collaboratively train a global model without sharing raw data. However, handling noisy labels in FL remains a major challenge due to heterogeneous data distributions and communication constraints, which can severely degrade model performance. To address this issue, we propose FedEFC, a novel method designed to tackle the impact of noisy labels in FL. FedEFC mitigates this issue through two key techniques: (1) prestopping, which prevents overfitting to mislabeled data by dynamically halting training at an optimal point, and (2) loss correction, which adjusts model updates to account for label noise. In particular, we develop an effective loss correction tailored to the unique challenges of FL, including data heterogeneity and decentralized training. Furthermore, we provide a theoretical analysis, leveraging the composite proper loss property, to demonstrate that the FL objective function under noisy label distributions can be aligned with the clean label distribution. Extensive experimental results validate the effectiveness of our approach, showing that it consistently outperforms existing FL techniques in mitigating the impact of noisy labels, particularly under heterogeneous data settings (e.g., achieving up to 41.64% relative performance improvement over the existing loss correction method).", "link": "http://arxiv.org/abs/2504.05615v3", "date": "2026-02-18", "relevancy": 2.0215, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5351}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5047}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedEFC%3A%20Federated%20Learning%20Using%20Enhanced%20Forward%20Correction%20Against%20Noisy%20Labels&body=Title%3A%20FedEFC%3A%20Federated%20Learning%20Using%20Enhanced%20Forward%20Correction%20Against%20Noisy%20Labels%0AAuthor%3A%20Seunghun%20Yu%20and%20Jin-Hyun%20Ahn%20and%20Joonhyuk%20Kang%0AAbstract%3A%20Federated%20Learning%20%28FL%29%20is%20a%20powerful%20framework%20for%20privacy-preserving%20distributed%20learning.%20It%20enables%20multiple%20clients%20to%20collaboratively%20train%20a%20global%20model%20without%20sharing%20raw%20data.%20However%2C%20handling%20noisy%20labels%20in%20FL%20remains%20a%20major%20challenge%20due%20to%20heterogeneous%20data%20distributions%20and%20communication%20constraints%2C%20which%20can%20severely%20degrade%20model%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20FedEFC%2C%20a%20novel%20method%20designed%20to%20tackle%20the%20impact%20of%20noisy%20labels%20in%20FL.%20FedEFC%20mitigates%20this%20issue%20through%20two%20key%20techniques%3A%20%281%29%20prestopping%2C%20which%20prevents%20overfitting%20to%20mislabeled%20data%20by%20dynamically%20halting%20training%20at%20an%20optimal%20point%2C%20and%20%282%29%20loss%20correction%2C%20which%20adjusts%20model%20updates%20to%20account%20for%20label%20noise.%20In%20particular%2C%20we%20develop%20an%20effective%20loss%20correction%20tailored%20to%20the%20unique%20challenges%20of%20FL%2C%20including%20data%20heterogeneity%20and%20decentralized%20training.%20Furthermore%2C%20we%20provide%20a%20theoretical%20analysis%2C%20leveraging%20the%20composite%20proper%20loss%20property%2C%20to%20demonstrate%20that%20the%20FL%20objective%20function%20under%20noisy%20label%20distributions%20can%20be%20aligned%20with%20the%20clean%20label%20distribution.%20Extensive%20experimental%20results%20validate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20it%20consistently%20outperforms%20existing%20FL%20techniques%20in%20mitigating%20the%20impact%20of%20noisy%20labels%2C%20particularly%20under%20heterogeneous%20data%20settings%20%28e.g.%2C%20achieving%20up%20to%2041.64%25%20relative%20performance%20improvement%20over%20the%20existing%20loss%20correction%20method%29.%0ALink%3A%20http%3A//arxiv.org/abs/2504.05615v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedEFC%253A%2520Federated%2520Learning%2520Using%2520Enhanced%2520Forward%2520Correction%2520Against%2520Noisy%2520Labels%26entry.906535625%3DSeunghun%2520Yu%2520and%2520Jin-Hyun%2520Ahn%2520and%2520Joonhyuk%2520Kang%26entry.1292438233%3DFederated%2520Learning%2520%2528FL%2529%2520is%2520a%2520powerful%2520framework%2520for%2520privacy-preserving%2520distributed%2520learning.%2520It%2520enables%2520multiple%2520clients%2520to%2520collaboratively%2520train%2520a%2520global%2520model%2520without%2520sharing%2520raw%2520data.%2520However%252C%2520handling%2520noisy%2520labels%2520in%2520FL%2520remains%2520a%2520major%2520challenge%2520due%2520to%2520heterogeneous%2520data%2520distributions%2520and%2520communication%2520constraints%252C%2520which%2520can%2520severely%2520degrade%2520model%2520performance.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520FedEFC%252C%2520a%2520novel%2520method%2520designed%2520to%2520tackle%2520the%2520impact%2520of%2520noisy%2520labels%2520in%2520FL.%2520FedEFC%2520mitigates%2520this%2520issue%2520through%2520two%2520key%2520techniques%253A%2520%25281%2529%2520prestopping%252C%2520which%2520prevents%2520overfitting%2520to%2520mislabeled%2520data%2520by%2520dynamically%2520halting%2520training%2520at%2520an%2520optimal%2520point%252C%2520and%2520%25282%2529%2520loss%2520correction%252C%2520which%2520adjusts%2520model%2520updates%2520to%2520account%2520for%2520label%2520noise.%2520In%2520particular%252C%2520we%2520develop%2520an%2520effective%2520loss%2520correction%2520tailored%2520to%2520the%2520unique%2520challenges%2520of%2520FL%252C%2520including%2520data%2520heterogeneity%2520and%2520decentralized%2520training.%2520Furthermore%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%252C%2520leveraging%2520the%2520composite%2520proper%2520loss%2520property%252C%2520to%2520demonstrate%2520that%2520the%2520FL%2520objective%2520function%2520under%2520noisy%2520label%2520distributions%2520can%2520be%2520aligned%2520with%2520the%2520clean%2520label%2520distribution.%2520Extensive%2520experimental%2520results%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520showing%2520that%2520it%2520consistently%2520outperforms%2520existing%2520FL%2520techniques%2520in%2520mitigating%2520the%2520impact%2520of%2520noisy%2520labels%252C%2520particularly%2520under%2520heterogeneous%2520data%2520settings%2520%2528e.g.%252C%2520achieving%2520up%2520to%252041.64%2525%2520relative%2520performance%2520improvement%2520over%2520the%2520existing%2520loss%2520correction%2520method%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05615v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedEFC%3A%20Federated%20Learning%20Using%20Enhanced%20Forward%20Correction%20Against%20Noisy%20Labels&entry.906535625=Seunghun%20Yu%20and%20Jin-Hyun%20Ahn%20and%20Joonhyuk%20Kang&entry.1292438233=Federated%20Learning%20%28FL%29%20is%20a%20powerful%20framework%20for%20privacy-preserving%20distributed%20learning.%20It%20enables%20multiple%20clients%20to%20collaboratively%20train%20a%20global%20model%20without%20sharing%20raw%20data.%20However%2C%20handling%20noisy%20labels%20in%20FL%20remains%20a%20major%20challenge%20due%20to%20heterogeneous%20data%20distributions%20and%20communication%20constraints%2C%20which%20can%20severely%20degrade%20model%20performance.%20To%20address%20this%20issue%2C%20we%20propose%20FedEFC%2C%20a%20novel%20method%20designed%20to%20tackle%20the%20impact%20of%20noisy%20labels%20in%20FL.%20FedEFC%20mitigates%20this%20issue%20through%20two%20key%20techniques%3A%20%281%29%20prestopping%2C%20which%20prevents%20overfitting%20to%20mislabeled%20data%20by%20dynamically%20halting%20training%20at%20an%20optimal%20point%2C%20and%20%282%29%20loss%20correction%2C%20which%20adjusts%20model%20updates%20to%20account%20for%20label%20noise.%20In%20particular%2C%20we%20develop%20an%20effective%20loss%20correction%20tailored%20to%20the%20unique%20challenges%20of%20FL%2C%20including%20data%20heterogeneity%20and%20decentralized%20training.%20Furthermore%2C%20we%20provide%20a%20theoretical%20analysis%2C%20leveraging%20the%20composite%20proper%20loss%20property%2C%20to%20demonstrate%20that%20the%20FL%20objective%20function%20under%20noisy%20label%20distributions%20can%20be%20aligned%20with%20the%20clean%20label%20distribution.%20Extensive%20experimental%20results%20validate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20that%20it%20consistently%20outperforms%20existing%20FL%20techniques%20in%20mitigating%20the%20impact%20of%20noisy%20labels%2C%20particularly%20under%20heterogeneous%20data%20settings%20%28e.g.%2C%20achieving%20up%20to%2041.64%25%20relative%20performance%20improvement%20over%20the%20existing%20loss%20correction%20method%29.&entry.1838667208=http%3A//arxiv.org/abs/2504.05615v3&entry.124074799=Read"},
{"title": "Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation", "author": "Daniel Bethell and Simos Gerasimou and Radu Calinescu and Calum Imrie", "abstract": "Ensuring the safe exploration of reinforcement learning (RL) agents is critical for deployment in real-world systems. Yet existing approaches struggle to strike the right balance: methods that tightly enforce safety often cripple task performance, while those that prioritize reward leave safety constraints frequently violated, producing diffuse cost landscapes that flatten gradients and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a novel approach that integrates uncertainty-aware modulation and refinement into critic training. By concentrating conservatism in uncertain and costly regions while preserving sharp gradients in safe areas, USC enables policies to achieve effective reward-safety trade-offs. Extensive experiments show that USC reduces safety violations by approximately 40% while maintaining competitive or higher rewards, and reduces the error between predicted and true cost gradients by approximately 83%, breaking the prevailing trade-off between safety and performance and paving the way for scalable safe RL.", "link": "http://arxiv.org/abs/2510.18478v2", "date": "2026-02-18", "relevancy": 2.0214, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5288}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.493}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20But%20Not%20Sorry%3A%20Reducing%20Over-Conservatism%20in%20Safety%20Critics%20via%20Uncertainty-Aware%20Modulation&body=Title%3A%20Safe%20But%20Not%20Sorry%3A%20Reducing%20Over-Conservatism%20in%20Safety%20Critics%20via%20Uncertainty-Aware%20Modulation%0AAuthor%3A%20Daniel%20Bethell%20and%20Simos%20Gerasimou%20and%20Radu%20Calinescu%20and%20Calum%20Imrie%0AAbstract%3A%20Ensuring%20the%20safe%20exploration%20of%20reinforcement%20learning%20%28RL%29%20agents%20is%20critical%20for%20deployment%20in%20real-world%20systems.%20Yet%20existing%20approaches%20struggle%20to%20strike%20the%20right%20balance%3A%20methods%20that%20tightly%20enforce%20safety%20often%20cripple%20task%20performance%2C%20while%20those%20that%20prioritize%20reward%20leave%20safety%20constraints%20frequently%20violated%2C%20producing%20diffuse%20cost%20landscapes%20that%20flatten%20gradients%20and%20stall%20policy%20improvement.%20We%20introduce%20the%20Uncertain%20Safety%20Critic%20%28USC%29%2C%20a%20novel%20approach%20that%20integrates%20uncertainty-aware%20modulation%20and%20refinement%20into%20critic%20training.%20By%20concentrating%20conservatism%20in%20uncertain%20and%20costly%20regions%20while%20preserving%20sharp%20gradients%20in%20safe%20areas%2C%20USC%20enables%20policies%20to%20achieve%20effective%20reward-safety%20trade-offs.%20Extensive%20experiments%20show%20that%20USC%20reduces%20safety%20violations%20by%20approximately%2040%25%20while%20maintaining%20competitive%20or%20higher%20rewards%2C%20and%20reduces%20the%20error%20between%20predicted%20and%20true%20cost%20gradients%20by%20approximately%2083%25%2C%20breaking%20the%20prevailing%20trade-off%20between%20safety%20and%20performance%20and%20paving%20the%20way%20for%20scalable%20safe%20RL.%0ALink%3A%20http%3A//arxiv.org/abs/2510.18478v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520But%2520Not%2520Sorry%253A%2520Reducing%2520Over-Conservatism%2520in%2520Safety%2520Critics%2520via%2520Uncertainty-Aware%2520Modulation%26entry.906535625%3DDaniel%2520Bethell%2520and%2520Simos%2520Gerasimou%2520and%2520Radu%2520Calinescu%2520and%2520Calum%2520Imrie%26entry.1292438233%3DEnsuring%2520the%2520safe%2520exploration%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520is%2520critical%2520for%2520deployment%2520in%2520real-world%2520systems.%2520Yet%2520existing%2520approaches%2520struggle%2520to%2520strike%2520the%2520right%2520balance%253A%2520methods%2520that%2520tightly%2520enforce%2520safety%2520often%2520cripple%2520task%2520performance%252C%2520while%2520those%2520that%2520prioritize%2520reward%2520leave%2520safety%2520constraints%2520frequently%2520violated%252C%2520producing%2520diffuse%2520cost%2520landscapes%2520that%2520flatten%2520gradients%2520and%2520stall%2520policy%2520improvement.%2520We%2520introduce%2520the%2520Uncertain%2520Safety%2520Critic%2520%2528USC%2529%252C%2520a%2520novel%2520approach%2520that%2520integrates%2520uncertainty-aware%2520modulation%2520and%2520refinement%2520into%2520critic%2520training.%2520By%2520concentrating%2520conservatism%2520in%2520uncertain%2520and%2520costly%2520regions%2520while%2520preserving%2520sharp%2520gradients%2520in%2520safe%2520areas%252C%2520USC%2520enables%2520policies%2520to%2520achieve%2520effective%2520reward-safety%2520trade-offs.%2520Extensive%2520experiments%2520show%2520that%2520USC%2520reduces%2520safety%2520violations%2520by%2520approximately%252040%2525%2520while%2520maintaining%2520competitive%2520or%2520higher%2520rewards%252C%2520and%2520reduces%2520the%2520error%2520between%2520predicted%2520and%2520true%2520cost%2520gradients%2520by%2520approximately%252083%2525%252C%2520breaking%2520the%2520prevailing%2520trade-off%2520between%2520safety%2520and%2520performance%2520and%2520paving%2520the%2520way%2520for%2520scalable%2520safe%2520RL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.18478v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20But%20Not%20Sorry%3A%20Reducing%20Over-Conservatism%20in%20Safety%20Critics%20via%20Uncertainty-Aware%20Modulation&entry.906535625=Daniel%20Bethell%20and%20Simos%20Gerasimou%20and%20Radu%20Calinescu%20and%20Calum%20Imrie&entry.1292438233=Ensuring%20the%20safe%20exploration%20of%20reinforcement%20learning%20%28RL%29%20agents%20is%20critical%20for%20deployment%20in%20real-world%20systems.%20Yet%20existing%20approaches%20struggle%20to%20strike%20the%20right%20balance%3A%20methods%20that%20tightly%20enforce%20safety%20often%20cripple%20task%20performance%2C%20while%20those%20that%20prioritize%20reward%20leave%20safety%20constraints%20frequently%20violated%2C%20producing%20diffuse%20cost%20landscapes%20that%20flatten%20gradients%20and%20stall%20policy%20improvement.%20We%20introduce%20the%20Uncertain%20Safety%20Critic%20%28USC%29%2C%20a%20novel%20approach%20that%20integrates%20uncertainty-aware%20modulation%20and%20refinement%20into%20critic%20training.%20By%20concentrating%20conservatism%20in%20uncertain%20and%20costly%20regions%20while%20preserving%20sharp%20gradients%20in%20safe%20areas%2C%20USC%20enables%20policies%20to%20achieve%20effective%20reward-safety%20trade-offs.%20Extensive%20experiments%20show%20that%20USC%20reduces%20safety%20violations%20by%20approximately%2040%25%20while%20maintaining%20competitive%20or%20higher%20rewards%2C%20and%20reduces%20the%20error%20between%20predicted%20and%20true%20cost%20gradients%20by%20approximately%2083%25%2C%20breaking%20the%20prevailing%20trade-off%20between%20safety%20and%20performance%20and%20paving%20the%20way%20for%20scalable%20safe%20RL.&entry.1838667208=http%3A//arxiv.org/abs/2510.18478v2&entry.124074799=Read"},
{"title": "Variable-Length Semantic IDs for Recommender Systems", "author": "Kirill Khrylchenko", "abstract": "Generative models are increasingly used in recommender systems, both for modeling user behavior as event sequences and for integrating large language models into recommendation pipelines. A key challenge in this setting is the extremely large cardinality of item spaces, which makes training generative models difficult and introduces a vocabulary gap between natural language and item identifiers. Semantic identifiers (semantic IDs), which represent items as sequences of low-cardinality tokens, have recently emerged as an effective solution to this problem.\n  However, existing approaches generate semantic identifiers of fixed length, assigning the same description length to all items. This is inefficient, misaligned with natural language, and ignores the highly skewed frequency structure of real-world catalogs, where popular items and rare long-tail items exhibit fundamentally different information requirements. In parallel, the emergent communication literature studies how agents develop discrete communication protocols, often producing variable-length messages in which frequent concepts receive shorter descriptions. Despite the conceptual similarity, these ideas have not been systematically adopted in recommender systems.\n  In this work, we bridge recommender systems and emergent communication by introducing variable-length semantic identifiers for recommendation. We propose a discrete variational autoencoder with Gumbel-Softmax reparameterization that learns item representations of adaptive length under a principled probabilistic framework, avoiding the instability of REINFORCE-based training and the fixed-length constraints of prior semantic ID methods.", "link": "http://arxiv.org/abs/2602.16375v1", "date": "2026-02-18", "relevancy": 2.0212, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5205}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.503}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Variable-Length%20Semantic%20IDs%20for%20Recommender%20Systems&body=Title%3A%20Variable-Length%20Semantic%20IDs%20for%20Recommender%20Systems%0AAuthor%3A%20Kirill%20Khrylchenko%0AAbstract%3A%20Generative%20models%20are%20increasingly%20used%20in%20recommender%20systems%2C%20both%20for%20modeling%20user%20behavior%20as%20event%20sequences%20and%20for%20integrating%20large%20language%20models%20into%20recommendation%20pipelines.%20A%20key%20challenge%20in%20this%20setting%20is%20the%20extremely%20large%20cardinality%20of%20item%20spaces%2C%20which%20makes%20training%20generative%20models%20difficult%20and%20introduces%20a%20vocabulary%20gap%20between%20natural%20language%20and%20item%20identifiers.%20Semantic%20identifiers%20%28semantic%20IDs%29%2C%20which%20represent%20items%20as%20sequences%20of%20low-cardinality%20tokens%2C%20have%20recently%20emerged%20as%20an%20effective%20solution%20to%20this%20problem.%0A%20%20However%2C%20existing%20approaches%20generate%20semantic%20identifiers%20of%20fixed%20length%2C%20assigning%20the%20same%20description%20length%20to%20all%20items.%20This%20is%20inefficient%2C%20misaligned%20with%20natural%20language%2C%20and%20ignores%20the%20highly%20skewed%20frequency%20structure%20of%20real-world%20catalogs%2C%20where%20popular%20items%20and%20rare%20long-tail%20items%20exhibit%20fundamentally%20different%20information%20requirements.%20In%20parallel%2C%20the%20emergent%20communication%20literature%20studies%20how%20agents%20develop%20discrete%20communication%20protocols%2C%20often%20producing%20variable-length%20messages%20in%20which%20frequent%20concepts%20receive%20shorter%20descriptions.%20Despite%20the%20conceptual%20similarity%2C%20these%20ideas%20have%20not%20been%20systematically%20adopted%20in%20recommender%20systems.%0A%20%20In%20this%20work%2C%20we%20bridge%20recommender%20systems%20and%20emergent%20communication%20by%20introducing%20variable-length%20semantic%20identifiers%20for%20recommendation.%20We%20propose%20a%20discrete%20variational%20autoencoder%20with%20Gumbel-Softmax%20reparameterization%20that%20learns%20item%20representations%20of%20adaptive%20length%20under%20a%20principled%20probabilistic%20framework%2C%20avoiding%20the%20instability%20of%20REINFORCE-based%20training%20and%20the%20fixed-length%20constraints%20of%20prior%20semantic%20ID%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariable-Length%2520Semantic%2520IDs%2520for%2520Recommender%2520Systems%26entry.906535625%3DKirill%2520Khrylchenko%26entry.1292438233%3DGenerative%2520models%2520are%2520increasingly%2520used%2520in%2520recommender%2520systems%252C%2520both%2520for%2520modeling%2520user%2520behavior%2520as%2520event%2520sequences%2520and%2520for%2520integrating%2520large%2520language%2520models%2520into%2520recommendation%2520pipelines.%2520A%2520key%2520challenge%2520in%2520this%2520setting%2520is%2520the%2520extremely%2520large%2520cardinality%2520of%2520item%2520spaces%252C%2520which%2520makes%2520training%2520generative%2520models%2520difficult%2520and%2520introduces%2520a%2520vocabulary%2520gap%2520between%2520natural%2520language%2520and%2520item%2520identifiers.%2520Semantic%2520identifiers%2520%2528semantic%2520IDs%2529%252C%2520which%2520represent%2520items%2520as%2520sequences%2520of%2520low-cardinality%2520tokens%252C%2520have%2520recently%2520emerged%2520as%2520an%2520effective%2520solution%2520to%2520this%2520problem.%250A%2520%2520However%252C%2520existing%2520approaches%2520generate%2520semantic%2520identifiers%2520of%2520fixed%2520length%252C%2520assigning%2520the%2520same%2520description%2520length%2520to%2520all%2520items.%2520This%2520is%2520inefficient%252C%2520misaligned%2520with%2520natural%2520language%252C%2520and%2520ignores%2520the%2520highly%2520skewed%2520frequency%2520structure%2520of%2520real-world%2520catalogs%252C%2520where%2520popular%2520items%2520and%2520rare%2520long-tail%2520items%2520exhibit%2520fundamentally%2520different%2520information%2520requirements.%2520In%2520parallel%252C%2520the%2520emergent%2520communication%2520literature%2520studies%2520how%2520agents%2520develop%2520discrete%2520communication%2520protocols%252C%2520often%2520producing%2520variable-length%2520messages%2520in%2520which%2520frequent%2520concepts%2520receive%2520shorter%2520descriptions.%2520Despite%2520the%2520conceptual%2520similarity%252C%2520these%2520ideas%2520have%2520not%2520been%2520systematically%2520adopted%2520in%2520recommender%2520systems.%250A%2520%2520In%2520this%2520work%252C%2520we%2520bridge%2520recommender%2520systems%2520and%2520emergent%2520communication%2520by%2520introducing%2520variable-length%2520semantic%2520identifiers%2520for%2520recommendation.%2520We%2520propose%2520a%2520discrete%2520variational%2520autoencoder%2520with%2520Gumbel-Softmax%2520reparameterization%2520that%2520learns%2520item%2520representations%2520of%2520adaptive%2520length%2520under%2520a%2520principled%2520probabilistic%2520framework%252C%2520avoiding%2520the%2520instability%2520of%2520REINFORCE-based%2520training%2520and%2520the%2520fixed-length%2520constraints%2520of%2520prior%2520semantic%2520ID%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Variable-Length%20Semantic%20IDs%20for%20Recommender%20Systems&entry.906535625=Kirill%20Khrylchenko&entry.1292438233=Generative%20models%20are%20increasingly%20used%20in%20recommender%20systems%2C%20both%20for%20modeling%20user%20behavior%20as%20event%20sequences%20and%20for%20integrating%20large%20language%20models%20into%20recommendation%20pipelines.%20A%20key%20challenge%20in%20this%20setting%20is%20the%20extremely%20large%20cardinality%20of%20item%20spaces%2C%20which%20makes%20training%20generative%20models%20difficult%20and%20introduces%20a%20vocabulary%20gap%20between%20natural%20language%20and%20item%20identifiers.%20Semantic%20identifiers%20%28semantic%20IDs%29%2C%20which%20represent%20items%20as%20sequences%20of%20low-cardinality%20tokens%2C%20have%20recently%20emerged%20as%20an%20effective%20solution%20to%20this%20problem.%0A%20%20However%2C%20existing%20approaches%20generate%20semantic%20identifiers%20of%20fixed%20length%2C%20assigning%20the%20same%20description%20length%20to%20all%20items.%20This%20is%20inefficient%2C%20misaligned%20with%20natural%20language%2C%20and%20ignores%20the%20highly%20skewed%20frequency%20structure%20of%20real-world%20catalogs%2C%20where%20popular%20items%20and%20rare%20long-tail%20items%20exhibit%20fundamentally%20different%20information%20requirements.%20In%20parallel%2C%20the%20emergent%20communication%20literature%20studies%20how%20agents%20develop%20discrete%20communication%20protocols%2C%20often%20producing%20variable-length%20messages%20in%20which%20frequent%20concepts%20receive%20shorter%20descriptions.%20Despite%20the%20conceptual%20similarity%2C%20these%20ideas%20have%20not%20been%20systematically%20adopted%20in%20recommender%20systems.%0A%20%20In%20this%20work%2C%20we%20bridge%20recommender%20systems%20and%20emergent%20communication%20by%20introducing%20variable-length%20semantic%20identifiers%20for%20recommendation.%20We%20propose%20a%20discrete%20variational%20autoencoder%20with%20Gumbel-Softmax%20reparameterization%20that%20learns%20item%20representations%20of%20adaptive%20length%20under%20a%20principled%20probabilistic%20framework%2C%20avoiding%20the%20instability%20of%20REINFORCE-based%20training%20and%20the%20fixed-length%20constraints%20of%20prior%20semantic%20ID%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2602.16375v1&entry.124074799=Read"},
{"title": "IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models", "author": "Saurabh Bharti and Gaurav Azad and Abhinaw Jagtap and Nachiket Tapas", "abstract": "The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.", "link": "http://arxiv.org/abs/2602.16467v1", "date": "2026-02-18", "relevancy": 2.0106, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5086}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IndicEval%3A%20A%20Bilingual%20Indian%20Educational%20Evaluation%20Framework%20for%20Large%20Language%20Models&body=Title%3A%20IndicEval%3A%20A%20Bilingual%20Indian%20Educational%20Evaluation%20Framework%20for%20Large%20Language%20Models%0AAuthor%3A%20Saurabh%20Bharti%20and%20Gaurav%20Azad%20and%20Abhinaw%20Jagtap%20and%20Nachiket%20Tapas%0AAbstract%3A%20The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20necessitates%20evaluation%20frameworks%20that%20reflect%20real-world%20academic%20rigor%20and%20multilingual%20complexity.%20This%20paper%20introduces%20IndicEval%2C%20a%20scalable%20benchmarking%20platform%20designed%20to%20assess%20LLM%20performance%20using%20authentic%20high-stakes%20examination%20questions%20from%20UPSC%2C%20JEE%2C%20and%20NEET%20across%20STEM%20and%20humanities%20domains%20in%20both%20English%20and%20Hindi.%20Unlike%20synthetic%20benchmarks%2C%20IndicEval%20grounds%20evaluation%20in%20real%20examination%20standards%2C%20enabling%20realistic%20measurement%20of%20reasoning%2C%20domain%20knowledge%2C%20and%20bilingual%20adaptability.%20The%20framework%20automates%20assessment%20using%20Zero-Shot%2C%20Few-Shot%2C%20and%20Chain-of-Thought%20%28CoT%29%20prompting%20strategies%20and%20supports%20modular%20integration%20of%20new%20models%20and%20languages.%20Experiments%20conducted%20on%20Gemini%202.0%20Flash%2C%20GPT-4%2C%20Claude%2C%20and%20LLaMA%203-70B%20reveal%20three%20major%20findings.%20First%2C%20CoT%20prompting%20consistently%20improves%20reasoning%20accuracy%2C%20with%20substantial%20gains%20across%20subjects%20and%20languages.%20Second%2C%20significant%20cross-model%20performance%20disparities%20persist%2C%20particularly%20in%20high-complexity%20examinations.%20Third%2C%20multilingual%20degradation%20remains%20a%20critical%20challenge%2C%20with%20marked%20accuracy%20drops%20in%20Hindi%20compared%20to%20English%2C%20especially%20under%20Zero-Shot%20conditions.%20These%20results%20highlight%20persistent%20gaps%20in%20bilingual%20reasoning%20and%20domain%20transfer.%20Overall%2C%20IndicEval%20provides%20a%20practice-oriented%2C%20extensible%20foundation%20for%20rigorous%2C%20equitable%20evaluation%20of%20LLMs%20in%20multilingual%20educational%20settings%20and%20offers%20actionable%20insights%20for%20improving%20reasoning%20robustness%20and%20language%20adaptability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndicEval%253A%2520A%2520Bilingual%2520Indian%2520Educational%2520Evaluation%2520Framework%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DSaurabh%2520Bharti%2520and%2520Gaurav%2520Azad%2520and%2520Abhinaw%2520Jagtap%2520and%2520Nachiket%2520Tapas%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520necessitates%2520evaluation%2520frameworks%2520that%2520reflect%2520real-world%2520academic%2520rigor%2520and%2520multilingual%2520complexity.%2520This%2520paper%2520introduces%2520IndicEval%252C%2520a%2520scalable%2520benchmarking%2520platform%2520designed%2520to%2520assess%2520LLM%2520performance%2520using%2520authentic%2520high-stakes%2520examination%2520questions%2520from%2520UPSC%252C%2520JEE%252C%2520and%2520NEET%2520across%2520STEM%2520and%2520humanities%2520domains%2520in%2520both%2520English%2520and%2520Hindi.%2520Unlike%2520synthetic%2520benchmarks%252C%2520IndicEval%2520grounds%2520evaluation%2520in%2520real%2520examination%2520standards%252C%2520enabling%2520realistic%2520measurement%2520of%2520reasoning%252C%2520domain%2520knowledge%252C%2520and%2520bilingual%2520adaptability.%2520The%2520framework%2520automates%2520assessment%2520using%2520Zero-Shot%252C%2520Few-Shot%252C%2520and%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520strategies%2520and%2520supports%2520modular%2520integration%2520of%2520new%2520models%2520and%2520languages.%2520Experiments%2520conducted%2520on%2520Gemini%25202.0%2520Flash%252C%2520GPT-4%252C%2520Claude%252C%2520and%2520LLaMA%25203-70B%2520reveal%2520three%2520major%2520findings.%2520First%252C%2520CoT%2520prompting%2520consistently%2520improves%2520reasoning%2520accuracy%252C%2520with%2520substantial%2520gains%2520across%2520subjects%2520and%2520languages.%2520Second%252C%2520significant%2520cross-model%2520performance%2520disparities%2520persist%252C%2520particularly%2520in%2520high-complexity%2520examinations.%2520Third%252C%2520multilingual%2520degradation%2520remains%2520a%2520critical%2520challenge%252C%2520with%2520marked%2520accuracy%2520drops%2520in%2520Hindi%2520compared%2520to%2520English%252C%2520especially%2520under%2520Zero-Shot%2520conditions.%2520These%2520results%2520highlight%2520persistent%2520gaps%2520in%2520bilingual%2520reasoning%2520and%2520domain%2520transfer.%2520Overall%252C%2520IndicEval%2520provides%2520a%2520practice-oriented%252C%2520extensible%2520foundation%2520for%2520rigorous%252C%2520equitable%2520evaluation%2520of%2520LLMs%2520in%2520multilingual%2520educational%2520settings%2520and%2520offers%2520actionable%2520insights%2520for%2520improving%2520reasoning%2520robustness%2520and%2520language%2520adaptability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IndicEval%3A%20A%20Bilingual%20Indian%20Educational%20Evaluation%20Framework%20for%20Large%20Language%20Models&entry.906535625=Saurabh%20Bharti%20and%20Gaurav%20Azad%20and%20Abhinaw%20Jagtap%20and%20Nachiket%20Tapas&entry.1292438233=The%20rapid%20advancement%20of%20large%20language%20models%20%28LLMs%29%20necessitates%20evaluation%20frameworks%20that%20reflect%20real-world%20academic%20rigor%20and%20multilingual%20complexity.%20This%20paper%20introduces%20IndicEval%2C%20a%20scalable%20benchmarking%20platform%20designed%20to%20assess%20LLM%20performance%20using%20authentic%20high-stakes%20examination%20questions%20from%20UPSC%2C%20JEE%2C%20and%20NEET%20across%20STEM%20and%20humanities%20domains%20in%20both%20English%20and%20Hindi.%20Unlike%20synthetic%20benchmarks%2C%20IndicEval%20grounds%20evaluation%20in%20real%20examination%20standards%2C%20enabling%20realistic%20measurement%20of%20reasoning%2C%20domain%20knowledge%2C%20and%20bilingual%20adaptability.%20The%20framework%20automates%20assessment%20using%20Zero-Shot%2C%20Few-Shot%2C%20and%20Chain-of-Thought%20%28CoT%29%20prompting%20strategies%20and%20supports%20modular%20integration%20of%20new%20models%20and%20languages.%20Experiments%20conducted%20on%20Gemini%202.0%20Flash%2C%20GPT-4%2C%20Claude%2C%20and%20LLaMA%203-70B%20reveal%20three%20major%20findings.%20First%2C%20CoT%20prompting%20consistently%20improves%20reasoning%20accuracy%2C%20with%20substantial%20gains%20across%20subjects%20and%20languages.%20Second%2C%20significant%20cross-model%20performance%20disparities%20persist%2C%20particularly%20in%20high-complexity%20examinations.%20Third%2C%20multilingual%20degradation%20remains%20a%20critical%20challenge%2C%20with%20marked%20accuracy%20drops%20in%20Hindi%20compared%20to%20English%2C%20especially%20under%20Zero-Shot%20conditions.%20These%20results%20highlight%20persistent%20gaps%20in%20bilingual%20reasoning%20and%20domain%20transfer.%20Overall%2C%20IndicEval%20provides%20a%20practice-oriented%2C%20extensible%20foundation%20for%20rigorous%2C%20equitable%20evaluation%20of%20LLMs%20in%20multilingual%20educational%20settings%20and%20offers%20actionable%20insights%20for%20improving%20reasoning%20robustness%20and%20language%20adaptability.&entry.1838667208=http%3A//arxiv.org/abs/2602.16467v1&entry.124074799=Read"},
{"title": "System Identification under Constraints and Disturbance: A Bayesian Estimation Approach", "author": "Sergi Martinez and Steve Tonneau and Carlos Mastalli", "abstract": "We introduce a Bayesian system identification (SysID) framework for jointly estimating robot's state trajectories and physical parameters with high accuracy. It embeds physically consistent inverse dynamics, contact and loop-closure constraints, and fully featured joint friction models as hard, stage-wise equality constraints. It relies on energy-based regressors to enhance parameter observability, supports both equality and inequality priors on inertial and actuation parameters, enforces dynamically consistent disturbance projections, and augments proprioceptive measurements with energy observations to disambiguate nonlinear friction effects. To ensure scalability, we derive a parameterized equality-constrained Riccati recursion that preserves the banded structure of the problem, achieving linear complexity in the time horizon, and develop computationally efficient derivatives. Simulation studies on representative robotic systems, together with hardware experiments on a Unitree B1 equipped with a Z1 arm, demonstrate faster convergence, lower inertial and friction estimation errors, and improved contact consistency compared to forward-dynamics and decoupled identification baselines. When deployed within model predictive control frameworks, the resulting models yield measurable improvements in tracking performance during locomotion over challenging environments.", "link": "http://arxiv.org/abs/2602.16358v1", "date": "2026-02-18", "relevancy": 2.0045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6309}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4759}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20System%20Identification%20under%20Constraints%20and%20Disturbance%3A%20A%20Bayesian%20Estimation%20Approach&body=Title%3A%20System%20Identification%20under%20Constraints%20and%20Disturbance%3A%20A%20Bayesian%20Estimation%20Approach%0AAuthor%3A%20Sergi%20Martinez%20and%20Steve%20Tonneau%20and%20Carlos%20Mastalli%0AAbstract%3A%20We%20introduce%20a%20Bayesian%20system%20identification%20%28SysID%29%20framework%20for%20jointly%20estimating%20robot%27s%20state%20trajectories%20and%20physical%20parameters%20with%20high%20accuracy.%20It%20embeds%20physically%20consistent%20inverse%20dynamics%2C%20contact%20and%20loop-closure%20constraints%2C%20and%20fully%20featured%20joint%20friction%20models%20as%20hard%2C%20stage-wise%20equality%20constraints.%20It%20relies%20on%20energy-based%20regressors%20to%20enhance%20parameter%20observability%2C%20supports%20both%20equality%20and%20inequality%20priors%20on%20inertial%20and%20actuation%20parameters%2C%20enforces%20dynamically%20consistent%20disturbance%20projections%2C%20and%20augments%20proprioceptive%20measurements%20with%20energy%20observations%20to%20disambiguate%20nonlinear%20friction%20effects.%20To%20ensure%20scalability%2C%20we%20derive%20a%20parameterized%20equality-constrained%20Riccati%20recursion%20that%20preserves%20the%20banded%20structure%20of%20the%20problem%2C%20achieving%20linear%20complexity%20in%20the%20time%20horizon%2C%20and%20develop%20computationally%20efficient%20derivatives.%20Simulation%20studies%20on%20representative%20robotic%20systems%2C%20together%20with%20hardware%20experiments%20on%20a%20Unitree%20B1%20equipped%20with%20a%20Z1%20arm%2C%20demonstrate%20faster%20convergence%2C%20lower%20inertial%20and%20friction%20estimation%20errors%2C%20and%20improved%20contact%20consistency%20compared%20to%20forward-dynamics%20and%20decoupled%20identification%20baselines.%20When%20deployed%20within%20model%20predictive%20control%20frameworks%2C%20the%20resulting%20models%20yield%20measurable%20improvements%20in%20tracking%20performance%20during%20locomotion%20over%20challenging%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystem%2520Identification%2520under%2520Constraints%2520and%2520Disturbance%253A%2520A%2520Bayesian%2520Estimation%2520Approach%26entry.906535625%3DSergi%2520Martinez%2520and%2520Steve%2520Tonneau%2520and%2520Carlos%2520Mastalli%26entry.1292438233%3DWe%2520introduce%2520a%2520Bayesian%2520system%2520identification%2520%2528SysID%2529%2520framework%2520for%2520jointly%2520estimating%2520robot%2527s%2520state%2520trajectories%2520and%2520physical%2520parameters%2520with%2520high%2520accuracy.%2520It%2520embeds%2520physically%2520consistent%2520inverse%2520dynamics%252C%2520contact%2520and%2520loop-closure%2520constraints%252C%2520and%2520fully%2520featured%2520joint%2520friction%2520models%2520as%2520hard%252C%2520stage-wise%2520equality%2520constraints.%2520It%2520relies%2520on%2520energy-based%2520regressors%2520to%2520enhance%2520parameter%2520observability%252C%2520supports%2520both%2520equality%2520and%2520inequality%2520priors%2520on%2520inertial%2520and%2520actuation%2520parameters%252C%2520enforces%2520dynamically%2520consistent%2520disturbance%2520projections%252C%2520and%2520augments%2520proprioceptive%2520measurements%2520with%2520energy%2520observations%2520to%2520disambiguate%2520nonlinear%2520friction%2520effects.%2520To%2520ensure%2520scalability%252C%2520we%2520derive%2520a%2520parameterized%2520equality-constrained%2520Riccati%2520recursion%2520that%2520preserves%2520the%2520banded%2520structure%2520of%2520the%2520problem%252C%2520achieving%2520linear%2520complexity%2520in%2520the%2520time%2520horizon%252C%2520and%2520develop%2520computationally%2520efficient%2520derivatives.%2520Simulation%2520studies%2520on%2520representative%2520robotic%2520systems%252C%2520together%2520with%2520hardware%2520experiments%2520on%2520a%2520Unitree%2520B1%2520equipped%2520with%2520a%2520Z1%2520arm%252C%2520demonstrate%2520faster%2520convergence%252C%2520lower%2520inertial%2520and%2520friction%2520estimation%2520errors%252C%2520and%2520improved%2520contact%2520consistency%2520compared%2520to%2520forward-dynamics%2520and%2520decoupled%2520identification%2520baselines.%2520When%2520deployed%2520within%2520model%2520predictive%2520control%2520frameworks%252C%2520the%2520resulting%2520models%2520yield%2520measurable%2520improvements%2520in%2520tracking%2520performance%2520during%2520locomotion%2520over%2520challenging%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=System%20Identification%20under%20Constraints%20and%20Disturbance%3A%20A%20Bayesian%20Estimation%20Approach&entry.906535625=Sergi%20Martinez%20and%20Steve%20Tonneau%20and%20Carlos%20Mastalli&entry.1292438233=We%20introduce%20a%20Bayesian%20system%20identification%20%28SysID%29%20framework%20for%20jointly%20estimating%20robot%27s%20state%20trajectories%20and%20physical%20parameters%20with%20high%20accuracy.%20It%20embeds%20physically%20consistent%20inverse%20dynamics%2C%20contact%20and%20loop-closure%20constraints%2C%20and%20fully%20featured%20joint%20friction%20models%20as%20hard%2C%20stage-wise%20equality%20constraints.%20It%20relies%20on%20energy-based%20regressors%20to%20enhance%20parameter%20observability%2C%20supports%20both%20equality%20and%20inequality%20priors%20on%20inertial%20and%20actuation%20parameters%2C%20enforces%20dynamically%20consistent%20disturbance%20projections%2C%20and%20augments%20proprioceptive%20measurements%20with%20energy%20observations%20to%20disambiguate%20nonlinear%20friction%20effects.%20To%20ensure%20scalability%2C%20we%20derive%20a%20parameterized%20equality-constrained%20Riccati%20recursion%20that%20preserves%20the%20banded%20structure%20of%20the%20problem%2C%20achieving%20linear%20complexity%20in%20the%20time%20horizon%2C%20and%20develop%20computationally%20efficient%20derivatives.%20Simulation%20studies%20on%20representative%20robotic%20systems%2C%20together%20with%20hardware%20experiments%20on%20a%20Unitree%20B1%20equipped%20with%20a%20Z1%20arm%2C%20demonstrate%20faster%20convergence%2C%20lower%20inertial%20and%20friction%20estimation%20errors%2C%20and%20improved%20contact%20consistency%20compared%20to%20forward-dynamics%20and%20decoupled%20identification%20baselines.%20When%20deployed%20within%20model%20predictive%20control%20frameworks%2C%20the%20resulting%20models%20yield%20measurable%20improvements%20in%20tracking%20performance%20during%20locomotion%20over%20challenging%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.16358v1&entry.124074799=Read"},
{"title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion", "author": "Tianmeng Hu and Yongzheng Cui and Biao Luo and Ke Li", "abstract": "The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.", "link": "http://arxiv.org/abs/2602.16548v1", "date": "2026-02-18", "relevancy": 2.0035, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5202}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4923}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIDER%3A%203D%20RNA%20Inverse%20Design%20with%20Reinforcement%20Learning-Guided%20Diffusion&body=Title%3A%20RIDER%3A%203D%20RNA%20Inverse%20Design%20with%20Reinforcement%20Learning-Guided%20Diffusion%0AAuthor%3A%20Tianmeng%20Hu%20and%20Yongzheng%20Cui%20and%20Biao%20Luo%20and%20Ke%20Li%0AAbstract%3A%20The%20inverse%20design%20of%20RNA%20three-dimensional%20%283D%29%20structures%20is%20crucial%20for%20engineering%20functional%20RNAs%20in%20synthetic%20biology%20and%20therapeutics.%20While%20recent%20deep%20learning%20approaches%20have%20advanced%20this%20field%2C%20they%20are%20typically%20optimized%20and%20evaluated%20using%20native%20sequence%20recovery%2C%20which%20is%20a%20limited%20surrogate%20for%20structural%20fidelity%2C%20since%20different%20sequences%20can%20fold%20into%20similar%203D%20structures%20and%20high%20recovery%20does%20not%20necessarily%20indicate%20correct%20folding.%20To%20address%20this%20limitation%2C%20we%20propose%20RIDER%2C%20an%20RNA%20Inverse%20DEsign%20framework%20with%20Reinforcement%20learning%20that%20directly%20optimizes%20for%203D%20structural%20similarity.%20First%2C%20we%20develop%20and%20pre-train%20a%20GNN-based%20generative%20diffusion%20model%20conditioned%20on%20the%20target%203D%20structure%2C%20achieving%20a%209%25%20improvement%20in%20native%20sequence%20recovery%20over%20state-of-the-art%20methods.%20Then%2C%20we%20fine-tune%20the%20model%20with%20an%20improved%20policy%20gradient%20algorithm%20using%20four%20task-specific%20reward%20functions%20based%20on%203D%20self-consistency%20metrics.%20Experimental%20results%20show%20that%20RIDER%20improves%20structural%20similarity%20by%20over%20100%25%20across%20all%20metrics%20and%20discovers%20designs%20that%20are%20distinct%20from%20native%20sequences.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIDER%253A%25203D%2520RNA%2520Inverse%2520Design%2520with%2520Reinforcement%2520Learning-Guided%2520Diffusion%26entry.906535625%3DTianmeng%2520Hu%2520and%2520Yongzheng%2520Cui%2520and%2520Biao%2520Luo%2520and%2520Ke%2520Li%26entry.1292438233%3DThe%2520inverse%2520design%2520of%2520RNA%2520three-dimensional%2520%25283D%2529%2520structures%2520is%2520crucial%2520for%2520engineering%2520functional%2520RNAs%2520in%2520synthetic%2520biology%2520and%2520therapeutics.%2520While%2520recent%2520deep%2520learning%2520approaches%2520have%2520advanced%2520this%2520field%252C%2520they%2520are%2520typically%2520optimized%2520and%2520evaluated%2520using%2520native%2520sequence%2520recovery%252C%2520which%2520is%2520a%2520limited%2520surrogate%2520for%2520structural%2520fidelity%252C%2520since%2520different%2520sequences%2520can%2520fold%2520into%2520similar%25203D%2520structures%2520and%2520high%2520recovery%2520does%2520not%2520necessarily%2520indicate%2520correct%2520folding.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520RIDER%252C%2520an%2520RNA%2520Inverse%2520DEsign%2520framework%2520with%2520Reinforcement%2520learning%2520that%2520directly%2520optimizes%2520for%25203D%2520structural%2520similarity.%2520First%252C%2520we%2520develop%2520and%2520pre-train%2520a%2520GNN-based%2520generative%2520diffusion%2520model%2520conditioned%2520on%2520the%2520target%25203D%2520structure%252C%2520achieving%2520a%25209%2525%2520improvement%2520in%2520native%2520sequence%2520recovery%2520over%2520state-of-the-art%2520methods.%2520Then%252C%2520we%2520fine-tune%2520the%2520model%2520with%2520an%2520improved%2520policy%2520gradient%2520algorithm%2520using%2520four%2520task-specific%2520reward%2520functions%2520based%2520on%25203D%2520self-consistency%2520metrics.%2520Experimental%2520results%2520show%2520that%2520RIDER%2520improves%2520structural%2520similarity%2520by%2520over%2520100%2525%2520across%2520all%2520metrics%2520and%2520discovers%2520designs%2520that%2520are%2520distinct%2520from%2520native%2520sequences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIDER%3A%203D%20RNA%20Inverse%20Design%20with%20Reinforcement%20Learning-Guided%20Diffusion&entry.906535625=Tianmeng%20Hu%20and%20Yongzheng%20Cui%20and%20Biao%20Luo%20and%20Ke%20Li&entry.1292438233=The%20inverse%20design%20of%20RNA%20three-dimensional%20%283D%29%20structures%20is%20crucial%20for%20engineering%20functional%20RNAs%20in%20synthetic%20biology%20and%20therapeutics.%20While%20recent%20deep%20learning%20approaches%20have%20advanced%20this%20field%2C%20they%20are%20typically%20optimized%20and%20evaluated%20using%20native%20sequence%20recovery%2C%20which%20is%20a%20limited%20surrogate%20for%20structural%20fidelity%2C%20since%20different%20sequences%20can%20fold%20into%20similar%203D%20structures%20and%20high%20recovery%20does%20not%20necessarily%20indicate%20correct%20folding.%20To%20address%20this%20limitation%2C%20we%20propose%20RIDER%2C%20an%20RNA%20Inverse%20DEsign%20framework%20with%20Reinforcement%20learning%20that%20directly%20optimizes%20for%203D%20structural%20similarity.%20First%2C%20we%20develop%20and%20pre-train%20a%20GNN-based%20generative%20diffusion%20model%20conditioned%20on%20the%20target%203D%20structure%2C%20achieving%20a%209%25%20improvement%20in%20native%20sequence%20recovery%20over%20state-of-the-art%20methods.%20Then%2C%20we%20fine-tune%20the%20model%20with%20an%20improved%20policy%20gradient%20algorithm%20using%20four%20task-specific%20reward%20functions%20based%20on%203D%20self-consistency%20metrics.%20Experimental%20results%20show%20that%20RIDER%20improves%20structural%20similarity%20by%20over%20100%25%20across%20all%20metrics%20and%20discovers%20designs%20that%20are%20distinct%20from%20native%20sequences.&entry.1838667208=http%3A//arxiv.org/abs/2602.16548v1&entry.124074799=Read"},
{"title": "EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents", "author": "Sara Fish and Julia Shephard and Minkai Li and Ran I. Shorrer and Yannai A. Gonczarowski", "abstract": "We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM's ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM's choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score, which measures an LLM's capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs' choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.", "link": "http://arxiv.org/abs/2503.18825v4", "date": "2026-02-18", "relevancy": 1.996, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EconEvals%3A%20Benchmarks%20and%20Litmus%20Tests%20for%20Economic%20Decision-Making%20by%20LLM%20Agents&body=Title%3A%20EconEvals%3A%20Benchmarks%20and%20Litmus%20Tests%20for%20Economic%20Decision-Making%20by%20LLM%20Agents%0AAuthor%3A%20Sara%20Fish%20and%20Julia%20Shephard%20and%20Minkai%20Li%20and%20Ran%20I.%20Shorrer%20and%20Yannai%20A.%20Gonczarowski%0AAbstract%3A%20We%20develop%20evaluation%20methods%20for%20measuring%20the%20economic%20decision-making%20capabilities%20and%20tendencies%20of%20LLMs.%20First%2C%20we%20develop%20benchmarks%20derived%20from%20key%20problems%20in%20economics%20--%20procurement%2C%20scheduling%2C%20and%20pricing%20--%20that%20test%20an%20LLM%27s%20ability%20to%20learn%20from%20the%20environment%20in%20context.%20Second%2C%20we%20develop%20the%20framework%20of%20litmus%20tests%2C%20evaluations%20that%20quantify%20an%20LLM%27s%20choice%20behavior%20on%20a%20stylized%20decision-making%20task%20with%20multiple%20conflicting%20objectives.%20Each%20litmus%20test%20outputs%20a%20litmus%20score%2C%20which%20quantifies%20an%20LLM%27s%20tradeoff%20response%2C%20a%20reliability%20score%2C%20which%20measures%20the%20coherence%20of%20an%20LLM%27s%20choice%20behavior%2C%20and%20a%20competency%20score%2C%20which%20measures%20an%20LLM%27s%20capability%20at%20the%20same%20task%20when%20the%20conflicting%20objectives%20are%20replaced%20by%20a%20single%2C%20well-specified%20objective.%20Evaluating%20a%20broad%20array%20of%20frontier%20LLMs%2C%20we%20%281%29%20investigate%20changes%20in%20LLM%20capabilities%20and%20tendencies%20over%20time%2C%20%282%29%20derive%20economically%20meaningful%20insights%20from%20the%20LLMs%27%20choice%20behavior%20and%20chain-of-thought%2C%20%283%29%20validate%20our%20litmus%20test%20framework%20by%20testing%20self-consistency%2C%20robustness%2C%20and%20generalizability.%20Overall%2C%20this%20work%20provides%20a%20foundation%20for%20evaluating%20LLM%20agents%20as%20they%20are%20further%20integrated%20into%20economic%20decision-making.%0ALink%3A%20http%3A//arxiv.org/abs/2503.18825v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEconEvals%253A%2520Benchmarks%2520and%2520Litmus%2520Tests%2520for%2520Economic%2520Decision-Making%2520by%2520LLM%2520Agents%26entry.906535625%3DSara%2520Fish%2520and%2520Julia%2520Shephard%2520and%2520Minkai%2520Li%2520and%2520Ran%2520I.%2520Shorrer%2520and%2520Yannai%2520A.%2520Gonczarowski%26entry.1292438233%3DWe%2520develop%2520evaluation%2520methods%2520for%2520measuring%2520the%2520economic%2520decision-making%2520capabilities%2520and%2520tendencies%2520of%2520LLMs.%2520First%252C%2520we%2520develop%2520benchmarks%2520derived%2520from%2520key%2520problems%2520in%2520economics%2520--%2520procurement%252C%2520scheduling%252C%2520and%2520pricing%2520--%2520that%2520test%2520an%2520LLM%2527s%2520ability%2520to%2520learn%2520from%2520the%2520environment%2520in%2520context.%2520Second%252C%2520we%2520develop%2520the%2520framework%2520of%2520litmus%2520tests%252C%2520evaluations%2520that%2520quantify%2520an%2520LLM%2527s%2520choice%2520behavior%2520on%2520a%2520stylized%2520decision-making%2520task%2520with%2520multiple%2520conflicting%2520objectives.%2520Each%2520litmus%2520test%2520outputs%2520a%2520litmus%2520score%252C%2520which%2520quantifies%2520an%2520LLM%2527s%2520tradeoff%2520response%252C%2520a%2520reliability%2520score%252C%2520which%2520measures%2520the%2520coherence%2520of%2520an%2520LLM%2527s%2520choice%2520behavior%252C%2520and%2520a%2520competency%2520score%252C%2520which%2520measures%2520an%2520LLM%2527s%2520capability%2520at%2520the%2520same%2520task%2520when%2520the%2520conflicting%2520objectives%2520are%2520replaced%2520by%2520a%2520single%252C%2520well-specified%2520objective.%2520Evaluating%2520a%2520broad%2520array%2520of%2520frontier%2520LLMs%252C%2520we%2520%25281%2529%2520investigate%2520changes%2520in%2520LLM%2520capabilities%2520and%2520tendencies%2520over%2520time%252C%2520%25282%2529%2520derive%2520economically%2520meaningful%2520insights%2520from%2520the%2520LLMs%2527%2520choice%2520behavior%2520and%2520chain-of-thought%252C%2520%25283%2529%2520validate%2520our%2520litmus%2520test%2520framework%2520by%2520testing%2520self-consistency%252C%2520robustness%252C%2520and%2520generalizability.%2520Overall%252C%2520this%2520work%2520provides%2520a%2520foundation%2520for%2520evaluating%2520LLM%2520agents%2520as%2520they%2520are%2520further%2520integrated%2520into%2520economic%2520decision-making.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18825v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EconEvals%3A%20Benchmarks%20and%20Litmus%20Tests%20for%20Economic%20Decision-Making%20by%20LLM%20Agents&entry.906535625=Sara%20Fish%20and%20Julia%20Shephard%20and%20Minkai%20Li%20and%20Ran%20I.%20Shorrer%20and%20Yannai%20A.%20Gonczarowski&entry.1292438233=We%20develop%20evaluation%20methods%20for%20measuring%20the%20economic%20decision-making%20capabilities%20and%20tendencies%20of%20LLMs.%20First%2C%20we%20develop%20benchmarks%20derived%20from%20key%20problems%20in%20economics%20--%20procurement%2C%20scheduling%2C%20and%20pricing%20--%20that%20test%20an%20LLM%27s%20ability%20to%20learn%20from%20the%20environment%20in%20context.%20Second%2C%20we%20develop%20the%20framework%20of%20litmus%20tests%2C%20evaluations%20that%20quantify%20an%20LLM%27s%20choice%20behavior%20on%20a%20stylized%20decision-making%20task%20with%20multiple%20conflicting%20objectives.%20Each%20litmus%20test%20outputs%20a%20litmus%20score%2C%20which%20quantifies%20an%20LLM%27s%20tradeoff%20response%2C%20a%20reliability%20score%2C%20which%20measures%20the%20coherence%20of%20an%20LLM%27s%20choice%20behavior%2C%20and%20a%20competency%20score%2C%20which%20measures%20an%20LLM%27s%20capability%20at%20the%20same%20task%20when%20the%20conflicting%20objectives%20are%20replaced%20by%20a%20single%2C%20well-specified%20objective.%20Evaluating%20a%20broad%20array%20of%20frontier%20LLMs%2C%20we%20%281%29%20investigate%20changes%20in%20LLM%20capabilities%20and%20tendencies%20over%20time%2C%20%282%29%20derive%20economically%20meaningful%20insights%20from%20the%20LLMs%27%20choice%20behavior%20and%20chain-of-thought%2C%20%283%29%20validate%20our%20litmus%20test%20framework%20by%20testing%20self-consistency%2C%20robustness%2C%20and%20generalizability.%20Overall%2C%20this%20work%20provides%20a%20foundation%20for%20evaluating%20LLM%20agents%20as%20they%20are%20further%20integrated%20into%20economic%20decision-making.&entry.1838667208=http%3A//arxiv.org/abs/2503.18825v4&entry.124074799=Read"},
{"title": "Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers", "author": "Panagiotis D. Grontas and Antonio Terpin and Efe C. Balta and Raffaello D'Andrea and John Lygeros", "abstract": "We introduce an output layer for neural networks that ensures satisfaction of convex constraints. Our approach, $\u03a0$net, leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation. We deploy $\u03a0$net as a feasible-by-design optimization proxy for parametric constrained optimization problems and obtain modest-accuracy solutions faster than traditional solvers when solving a single problem, and significantly faster for a batch of problems. We surpass state-of-the-art learning approaches by orders of magnitude in terms of training time, solution quality, and robustness to hyperparameter tuning, while maintaining similar inference times. Finally, we tackle multi-vehicle motion planning with non-convex trajectory preferences and provide $\u03a0$net as a GPU-ready package implemented in JAX.", "link": "http://arxiv.org/abs/2508.10480v2", "date": "2026-02-18", "relevancy": 1.9925, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5061}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pinet%3A%20Optimizing%20hard-constrained%20neural%20networks%20with%20orthogonal%20projection%20layers&body=Title%3A%20Pinet%3A%20Optimizing%20hard-constrained%20neural%20networks%20with%20orthogonal%20projection%20layers%0AAuthor%3A%20Panagiotis%20D.%20Grontas%20and%20Antonio%20Terpin%20and%20Efe%20C.%20Balta%20and%20Raffaello%20D%27Andrea%20and%20John%20Lygeros%0AAbstract%3A%20We%20introduce%20an%20output%20layer%20for%20neural%20networks%20that%20ensures%20satisfaction%20of%20convex%20constraints.%20Our%20approach%2C%20%24%CE%A0%24net%2C%20leverages%20operator%20splitting%20for%20rapid%20and%20reliable%20projections%20in%20the%20forward%20pass%2C%20and%20the%20implicit%20function%20theorem%20for%20backpropagation.%20We%20deploy%20%24%CE%A0%24net%20as%20a%20feasible-by-design%20optimization%20proxy%20for%20parametric%20constrained%20optimization%20problems%20and%20obtain%20modest-accuracy%20solutions%20faster%20than%20traditional%20solvers%20when%20solving%20a%20single%20problem%2C%20and%20significantly%20faster%20for%20a%20batch%20of%20problems.%20We%20surpass%20state-of-the-art%20learning%20approaches%20by%20orders%20of%20magnitude%20in%20terms%20of%20training%20time%2C%20solution%20quality%2C%20and%20robustness%20to%20hyperparameter%20tuning%2C%20while%20maintaining%20similar%20inference%20times.%20Finally%2C%20we%20tackle%20multi-vehicle%20motion%20planning%20with%20non-convex%20trajectory%20preferences%20and%20provide%20%24%CE%A0%24net%20as%20a%20GPU-ready%20package%20implemented%20in%20JAX.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPinet%253A%2520Optimizing%2520hard-constrained%2520neural%2520networks%2520with%2520orthogonal%2520projection%2520layers%26entry.906535625%3DPanagiotis%2520D.%2520Grontas%2520and%2520Antonio%2520Terpin%2520and%2520Efe%2520C.%2520Balta%2520and%2520Raffaello%2520D%2527Andrea%2520and%2520John%2520Lygeros%26entry.1292438233%3DWe%2520introduce%2520an%2520output%2520layer%2520for%2520neural%2520networks%2520that%2520ensures%2520satisfaction%2520of%2520convex%2520constraints.%2520Our%2520approach%252C%2520%2524%25CE%25A0%2524net%252C%2520leverages%2520operator%2520splitting%2520for%2520rapid%2520and%2520reliable%2520projections%2520in%2520the%2520forward%2520pass%252C%2520and%2520the%2520implicit%2520function%2520theorem%2520for%2520backpropagation.%2520We%2520deploy%2520%2524%25CE%25A0%2524net%2520as%2520a%2520feasible-by-design%2520optimization%2520proxy%2520for%2520parametric%2520constrained%2520optimization%2520problems%2520and%2520obtain%2520modest-accuracy%2520solutions%2520faster%2520than%2520traditional%2520solvers%2520when%2520solving%2520a%2520single%2520problem%252C%2520and%2520significantly%2520faster%2520for%2520a%2520batch%2520of%2520problems.%2520We%2520surpass%2520state-of-the-art%2520learning%2520approaches%2520by%2520orders%2520of%2520magnitude%2520in%2520terms%2520of%2520training%2520time%252C%2520solution%2520quality%252C%2520and%2520robustness%2520to%2520hyperparameter%2520tuning%252C%2520while%2520maintaining%2520similar%2520inference%2520times.%2520Finally%252C%2520we%2520tackle%2520multi-vehicle%2520motion%2520planning%2520with%2520non-convex%2520trajectory%2520preferences%2520and%2520provide%2520%2524%25CE%25A0%2524net%2520as%2520a%2520GPU-ready%2520package%2520implemented%2520in%2520JAX.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pinet%3A%20Optimizing%20hard-constrained%20neural%20networks%20with%20orthogonal%20projection%20layers&entry.906535625=Panagiotis%20D.%20Grontas%20and%20Antonio%20Terpin%20and%20Efe%20C.%20Balta%20and%20Raffaello%20D%27Andrea%20and%20John%20Lygeros&entry.1292438233=We%20introduce%20an%20output%20layer%20for%20neural%20networks%20that%20ensures%20satisfaction%20of%20convex%20constraints.%20Our%20approach%2C%20%24%CE%A0%24net%2C%20leverages%20operator%20splitting%20for%20rapid%20and%20reliable%20projections%20in%20the%20forward%20pass%2C%20and%20the%20implicit%20function%20theorem%20for%20backpropagation.%20We%20deploy%20%24%CE%A0%24net%20as%20a%20feasible-by-design%20optimization%20proxy%20for%20parametric%20constrained%20optimization%20problems%20and%20obtain%20modest-accuracy%20solutions%20faster%20than%20traditional%20solvers%20when%20solving%20a%20single%20problem%2C%20and%20significantly%20faster%20for%20a%20batch%20of%20problems.%20We%20surpass%20state-of-the-art%20learning%20approaches%20by%20orders%20of%20magnitude%20in%20terms%20of%20training%20time%2C%20solution%20quality%2C%20and%20robustness%20to%20hyperparameter%20tuning%2C%20while%20maintaining%20similar%20inference%20times.%20Finally%2C%20we%20tackle%20multi-vehicle%20motion%20planning%20with%20non-convex%20trajectory%20preferences%20and%20provide%20%24%CE%A0%24net%20as%20a%20GPU-ready%20package%20implemented%20in%20JAX.&entry.1838667208=http%3A//arxiv.org/abs/2508.10480v2&entry.124074799=Read"},
{"title": "Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models", "author": "Melkamu Abay Mersha and Jugal Kalita", "abstract": "Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \\textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.", "link": "http://arxiv.org/abs/2602.16608v1", "date": "2026-02-18", "relevancy": 1.9901, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5133}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%3A%20Context-Aware%20Layer-Wise%20Integrated%20Gradients%20for%20Explaining%20Transformer%20Models&body=Title%3A%20Explainable%20AI%3A%20Context-Aware%20Layer-Wise%20Integrated%20Gradients%20for%20Explaining%20Transformer%20Models%0AAuthor%3A%20Melkamu%20Abay%20Mersha%20and%20Jugal%20Kalita%0AAbstract%3A%20Transformer%20models%20achieve%20state-of-the-art%20performance%20across%20domains%20and%20tasks%2C%20yet%20their%20deeply%20layered%20representations%20make%20their%20predictions%20difficult%20to%20interpret.%20Existing%20explainability%20methods%20rely%20on%20final-layer%20attributions%2C%20capture%20either%20local%20token-level%20attributions%20or%20global%20attention%20patterns%20without%20unification%2C%20and%20lack%20context-awareness%20of%20inter-token%20dependencies%20and%20structural%20components.%20They%20also%20fail%20to%20capture%20how%20relevance%20evolves%20across%20layers%20and%20how%20structural%20components%20shape%20decision-making.%20To%20address%20these%20limitations%2C%20we%20proposed%20the%20%5Ctextbf%7BContext-Aware%20Layer-wise%20Integrated%20Gradients%20%28CA-LIG%29%20Framework%7D%2C%20a%20unified%20hierarchical%20attribution%20framework%20that%20computes%20layer-wise%20Integrated%20Gradients%20within%20each%20Transformer%20block%20and%20fuses%20these%20token-level%20attributions%20with%20class-specific%20attention%20gradients.%20This%20integration%20yields%20signed%2C%20context-sensitive%20attribution%20maps%20that%20capture%20supportive%20and%20opposing%20evidence%20while%20tracing%20the%20hierarchical%20flow%20of%20relevance%20through%20the%20Transformer%20layers.%20We%20evaluate%20the%20CA-LIG%20Framework%20across%20diverse%20tasks%2C%20domains%2C%20and%20transformer%20model%20families%2C%20including%20sentiment%20analysis%20and%20long%20and%20multi-class%20document%20classification%20with%20BERT%2C%20hate%20speech%20detection%20in%20a%20low-resource%20language%20setting%20with%20XLM-R%20and%20AfroLM%2C%20and%20image%20classification%20with%20Masked%20Autoencoder%20vision%20Transformer%20model.%20Across%20all%20tasks%20and%20architectures%2C%20CA-LIG%20provides%20more%20faithful%20attributions%2C%20shows%20stronger%20sensitivity%20to%20contextual%20dependencies%2C%20and%20produces%20clearer%2C%20more%20semantically%20coherent%20visualizations%20than%20established%20explainability%20methods.%20These%20results%20indicate%20that%20CA-LIG%20provides%20a%20more%20comprehensive%2C%20context-aware%2C%20and%20reliable%20explanation%20of%20Transformer%20decision-making%2C%20advancing%20both%20the%20practical%20interpretability%20and%20conceptual%20understanding%20of%20deep%20neural%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%253A%2520Context-Aware%2520Layer-Wise%2520Integrated%2520Gradients%2520for%2520Explaining%2520Transformer%2520Models%26entry.906535625%3DMelkamu%2520Abay%2520Mersha%2520and%2520Jugal%2520Kalita%26entry.1292438233%3DTransformer%2520models%2520achieve%2520state-of-the-art%2520performance%2520across%2520domains%2520and%2520tasks%252C%2520yet%2520their%2520deeply%2520layered%2520representations%2520make%2520their%2520predictions%2520difficult%2520to%2520interpret.%2520Existing%2520explainability%2520methods%2520rely%2520on%2520final-layer%2520attributions%252C%2520capture%2520either%2520local%2520token-level%2520attributions%2520or%2520global%2520attention%2520patterns%2520without%2520unification%252C%2520and%2520lack%2520context-awareness%2520of%2520inter-token%2520dependencies%2520and%2520structural%2520components.%2520They%2520also%2520fail%2520to%2520capture%2520how%2520relevance%2520evolves%2520across%2520layers%2520and%2520how%2520structural%2520components%2520shape%2520decision-making.%2520To%2520address%2520these%2520limitations%252C%2520we%2520proposed%2520the%2520%255Ctextbf%257BContext-Aware%2520Layer-wise%2520Integrated%2520Gradients%2520%2528CA-LIG%2529%2520Framework%257D%252C%2520a%2520unified%2520hierarchical%2520attribution%2520framework%2520that%2520computes%2520layer-wise%2520Integrated%2520Gradients%2520within%2520each%2520Transformer%2520block%2520and%2520fuses%2520these%2520token-level%2520attributions%2520with%2520class-specific%2520attention%2520gradients.%2520This%2520integration%2520yields%2520signed%252C%2520context-sensitive%2520attribution%2520maps%2520that%2520capture%2520supportive%2520and%2520opposing%2520evidence%2520while%2520tracing%2520the%2520hierarchical%2520flow%2520of%2520relevance%2520through%2520the%2520Transformer%2520layers.%2520We%2520evaluate%2520the%2520CA-LIG%2520Framework%2520across%2520diverse%2520tasks%252C%2520domains%252C%2520and%2520transformer%2520model%2520families%252C%2520including%2520sentiment%2520analysis%2520and%2520long%2520and%2520multi-class%2520document%2520classification%2520with%2520BERT%252C%2520hate%2520speech%2520detection%2520in%2520a%2520low-resource%2520language%2520setting%2520with%2520XLM-R%2520and%2520AfroLM%252C%2520and%2520image%2520classification%2520with%2520Masked%2520Autoencoder%2520vision%2520Transformer%2520model.%2520Across%2520all%2520tasks%2520and%2520architectures%252C%2520CA-LIG%2520provides%2520more%2520faithful%2520attributions%252C%2520shows%2520stronger%2520sensitivity%2520to%2520contextual%2520dependencies%252C%2520and%2520produces%2520clearer%252C%2520more%2520semantically%2520coherent%2520visualizations%2520than%2520established%2520explainability%2520methods.%2520These%2520results%2520indicate%2520that%2520CA-LIG%2520provides%2520a%2520more%2520comprehensive%252C%2520context-aware%252C%2520and%2520reliable%2520explanation%2520of%2520Transformer%2520decision-making%252C%2520advancing%2520both%2520the%2520practical%2520interpretability%2520and%2520conceptual%2520understanding%2520of%2520deep%2520neural%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%3A%20Context-Aware%20Layer-Wise%20Integrated%20Gradients%20for%20Explaining%20Transformer%20Models&entry.906535625=Melkamu%20Abay%20Mersha%20and%20Jugal%20Kalita&entry.1292438233=Transformer%20models%20achieve%20state-of-the-art%20performance%20across%20domains%20and%20tasks%2C%20yet%20their%20deeply%20layered%20representations%20make%20their%20predictions%20difficult%20to%20interpret.%20Existing%20explainability%20methods%20rely%20on%20final-layer%20attributions%2C%20capture%20either%20local%20token-level%20attributions%20or%20global%20attention%20patterns%20without%20unification%2C%20and%20lack%20context-awareness%20of%20inter-token%20dependencies%20and%20structural%20components.%20They%20also%20fail%20to%20capture%20how%20relevance%20evolves%20across%20layers%20and%20how%20structural%20components%20shape%20decision-making.%20To%20address%20these%20limitations%2C%20we%20proposed%20the%20%5Ctextbf%7BContext-Aware%20Layer-wise%20Integrated%20Gradients%20%28CA-LIG%29%20Framework%7D%2C%20a%20unified%20hierarchical%20attribution%20framework%20that%20computes%20layer-wise%20Integrated%20Gradients%20within%20each%20Transformer%20block%20and%20fuses%20these%20token-level%20attributions%20with%20class-specific%20attention%20gradients.%20This%20integration%20yields%20signed%2C%20context-sensitive%20attribution%20maps%20that%20capture%20supportive%20and%20opposing%20evidence%20while%20tracing%20the%20hierarchical%20flow%20of%20relevance%20through%20the%20Transformer%20layers.%20We%20evaluate%20the%20CA-LIG%20Framework%20across%20diverse%20tasks%2C%20domains%2C%20and%20transformer%20model%20families%2C%20including%20sentiment%20analysis%20and%20long%20and%20multi-class%20document%20classification%20with%20BERT%2C%20hate%20speech%20detection%20in%20a%20low-resource%20language%20setting%20with%20XLM-R%20and%20AfroLM%2C%20and%20image%20classification%20with%20Masked%20Autoencoder%20vision%20Transformer%20model.%20Across%20all%20tasks%20and%20architectures%2C%20CA-LIG%20provides%20more%20faithful%20attributions%2C%20shows%20stronger%20sensitivity%20to%20contextual%20dependencies%2C%20and%20produces%20clearer%2C%20more%20semantically%20coherent%20visualizations%20than%20established%20explainability%20methods.%20These%20results%20indicate%20that%20CA-LIG%20provides%20a%20more%20comprehensive%2C%20context-aware%2C%20and%20reliable%20explanation%20of%20Transformer%20decision-making%2C%20advancing%20both%20the%20practical%20interpretability%20and%20conceptual%20understanding%20of%20deep%20neural%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.16608v1&entry.124074799=Read"},
{"title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments", "author": "Yangjie Xu and Lujun Li and Lama Sleem and Niccolo Gentile and Yewei Song and Yiqun Wang and Siming Ji and Wenbo Wu and Radu State", "abstract": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.", "link": "http://arxiv.org/abs/2602.16653v1", "date": "2026-02-18", "relevancy": 1.9876, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Skill%20Framework%3A%20Perspectives%20on%20the%20Potential%20of%20Small%20Language%20Models%20in%20Industrial%20Environments&body=Title%3A%20Agent%20Skill%20Framework%3A%20Perspectives%20on%20the%20Potential%20of%20Small%20Language%20Models%20in%20Industrial%20Environments%0AAuthor%3A%20Yangjie%20Xu%20and%20Lujun%20Li%20and%20Lama%20Sleem%20and%20Niccolo%20Gentile%20and%20Yewei%20Song%20and%20Yiqun%20Wang%20and%20Siming%20Ji%20and%20Wenbo%20Wu%20and%20Radu%20State%0AAbstract%3A%20Agent%20Skill%20framework%2C%20now%20widely%20and%20officially%20supported%20by%20major%20players%20such%20as%20GitHub%20Copilot%2C%20LangChain%2C%20and%20OpenAI%2C%20performs%20especially%20well%20with%20proprietary%20models%20by%20improving%20context%20engineering%2C%20reducing%20hallucinations%2C%20and%20boosting%20task%20accuracy.%20Based%20on%20these%20observations%2C%20an%20investigation%20is%20conducted%20to%20determine%20whether%20the%20Agent%20Skill%20paradigm%20provides%20similar%20benefits%20to%20small%20language%20models%20%28SLMs%29.%20This%20question%20matters%20in%20industrial%20scenarios%20where%20continuous%20reliance%20on%20public%20APIs%20is%20infeasible%20due%20to%20data-security%20and%20budget%20constraints%20requirements%2C%20and%20where%20SLMs%20often%20show%20limited%20generalization%20in%20highly%20customized%20scenarios.%20This%20work%20introduces%20a%20formal%20mathematical%20definition%20of%20the%20Agent%20Skill%20process%2C%20followed%20by%20a%20systematic%20evaluation%20of%20language%20models%20of%20varying%20sizes%20across%20multiple%20use%20cases.%20The%20evaluation%20encompasses%20two%20open-source%20tasks%20and%20a%20real-world%20insurance%20claims%20data%20set.%20The%20results%20show%20that%20tiny%20models%20struggle%20with%20reliable%20skill%20selection%2C%20while%20moderately%20sized%20SLMs%20%28approximately%2012B%20-%2030B%29%20parameters%29%20benefit%20substantially%20from%20the%20Agent%20Skill%20approach.%20Moreover%2C%20code-specialized%20variants%20at%20around%2080B%20parameters%20achieve%20performance%20comparable%20to%20closed-source%20baselines%20while%20improving%20GPU%20efficiency.%20Collectively%2C%20these%20findings%20provide%20a%20comprehensive%20and%20nuanced%20characterization%20of%20the%20capabilities%20and%20constraints%20of%20the%20framework%2C%20while%20providing%20actionable%20insights%20for%20the%20effective%20deployment%20of%20Agent%20Skills%20in%20SLM-centered%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Skill%2520Framework%253A%2520Perspectives%2520on%2520the%2520Potential%2520of%2520Small%2520Language%2520Models%2520in%2520Industrial%2520Environments%26entry.906535625%3DYangjie%2520Xu%2520and%2520Lujun%2520Li%2520and%2520Lama%2520Sleem%2520and%2520Niccolo%2520Gentile%2520and%2520Yewei%2520Song%2520and%2520Yiqun%2520Wang%2520and%2520Siming%2520Ji%2520and%2520Wenbo%2520Wu%2520and%2520Radu%2520State%26entry.1292438233%3DAgent%2520Skill%2520framework%252C%2520now%2520widely%2520and%2520officially%2520supported%2520by%2520major%2520players%2520such%2520as%2520GitHub%2520Copilot%252C%2520LangChain%252C%2520and%2520OpenAI%252C%2520performs%2520especially%2520well%2520with%2520proprietary%2520models%2520by%2520improving%2520context%2520engineering%252C%2520reducing%2520hallucinations%252C%2520and%2520boosting%2520task%2520accuracy.%2520Based%2520on%2520these%2520observations%252C%2520an%2520investigation%2520is%2520conducted%2520to%2520determine%2520whether%2520the%2520Agent%2520Skill%2520paradigm%2520provides%2520similar%2520benefits%2520to%2520small%2520language%2520models%2520%2528SLMs%2529.%2520This%2520question%2520matters%2520in%2520industrial%2520scenarios%2520where%2520continuous%2520reliance%2520on%2520public%2520APIs%2520is%2520infeasible%2520due%2520to%2520data-security%2520and%2520budget%2520constraints%2520requirements%252C%2520and%2520where%2520SLMs%2520often%2520show%2520limited%2520generalization%2520in%2520highly%2520customized%2520scenarios.%2520This%2520work%2520introduces%2520a%2520formal%2520mathematical%2520definition%2520of%2520the%2520Agent%2520Skill%2520process%252C%2520followed%2520by%2520a%2520systematic%2520evaluation%2520of%2520language%2520models%2520of%2520varying%2520sizes%2520across%2520multiple%2520use%2520cases.%2520The%2520evaluation%2520encompasses%2520two%2520open-source%2520tasks%2520and%2520a%2520real-world%2520insurance%2520claims%2520data%2520set.%2520The%2520results%2520show%2520that%2520tiny%2520models%2520struggle%2520with%2520reliable%2520skill%2520selection%252C%2520while%2520moderately%2520sized%2520SLMs%2520%2528approximately%252012B%2520-%252030B%2529%2520parameters%2529%2520benefit%2520substantially%2520from%2520the%2520Agent%2520Skill%2520approach.%2520Moreover%252C%2520code-specialized%2520variants%2520at%2520around%252080B%2520parameters%2520achieve%2520performance%2520comparable%2520to%2520closed-source%2520baselines%2520while%2520improving%2520GPU%2520efficiency.%2520Collectively%252C%2520these%2520findings%2520provide%2520a%2520comprehensive%2520and%2520nuanced%2520characterization%2520of%2520the%2520capabilities%2520and%2520constraints%2520of%2520the%2520framework%252C%2520while%2520providing%2520actionable%2520insights%2520for%2520the%2520effective%2520deployment%2520of%2520Agent%2520Skills%2520in%2520SLM-centered%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Skill%20Framework%3A%20Perspectives%20on%20the%20Potential%20of%20Small%20Language%20Models%20in%20Industrial%20Environments&entry.906535625=Yangjie%20Xu%20and%20Lujun%20Li%20and%20Lama%20Sleem%20and%20Niccolo%20Gentile%20and%20Yewei%20Song%20and%20Yiqun%20Wang%20and%20Siming%20Ji%20and%20Wenbo%20Wu%20and%20Radu%20State&entry.1292438233=Agent%20Skill%20framework%2C%20now%20widely%20and%20officially%20supported%20by%20major%20players%20such%20as%20GitHub%20Copilot%2C%20LangChain%2C%20and%20OpenAI%2C%20performs%20especially%20well%20with%20proprietary%20models%20by%20improving%20context%20engineering%2C%20reducing%20hallucinations%2C%20and%20boosting%20task%20accuracy.%20Based%20on%20these%20observations%2C%20an%20investigation%20is%20conducted%20to%20determine%20whether%20the%20Agent%20Skill%20paradigm%20provides%20similar%20benefits%20to%20small%20language%20models%20%28SLMs%29.%20This%20question%20matters%20in%20industrial%20scenarios%20where%20continuous%20reliance%20on%20public%20APIs%20is%20infeasible%20due%20to%20data-security%20and%20budget%20constraints%20requirements%2C%20and%20where%20SLMs%20often%20show%20limited%20generalization%20in%20highly%20customized%20scenarios.%20This%20work%20introduces%20a%20formal%20mathematical%20definition%20of%20the%20Agent%20Skill%20process%2C%20followed%20by%20a%20systematic%20evaluation%20of%20language%20models%20of%20varying%20sizes%20across%20multiple%20use%20cases.%20The%20evaluation%20encompasses%20two%20open-source%20tasks%20and%20a%20real-world%20insurance%20claims%20data%20set.%20The%20results%20show%20that%20tiny%20models%20struggle%20with%20reliable%20skill%20selection%2C%20while%20moderately%20sized%20SLMs%20%28approximately%2012B%20-%2030B%29%20parameters%29%20benefit%20substantially%20from%20the%20Agent%20Skill%20approach.%20Moreover%2C%20code-specialized%20variants%20at%20around%2080B%20parameters%20achieve%20performance%20comparable%20to%20closed-source%20baselines%20while%20improving%20GPU%20efficiency.%20Collectively%2C%20these%20findings%20provide%20a%20comprehensive%20and%20nuanced%20characterization%20of%20the%20capabilities%20and%20constraints%20of%20the%20framework%2C%20while%20providing%20actionable%20insights%20for%20the%20effective%20deployment%20of%20Agent%20Skills%20in%20SLM-centered%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2602.16653v1&entry.124074799=Read"},
{"title": "Prompt When the Animal is: Temporal Animal Behavior Grounding with Positional Recovery Training", "author": "Sheng Yan and Xin Du and Zongying Li and Yi Wang and Hongcang Jin and Mengyuan Liu", "abstract": "Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments. To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model with the start and end times of specific animal behaviors during training. Specifically, \\port{} enhances the baseline model with a Recovering branch to reconstruct corrupted label sequences and align distributions via a Dual-alignment method. This allows the model to focus on specific temporal regions prompted by ground-truth information. Extensive experiments on the Animal Kingdom dataset demonstrate the effectiveness of \\port{}, achieving an IoU@0.3 of 38.52. It emerges as one of the top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges.", "link": "http://arxiv.org/abs/2405.05523v2", "date": "2026-02-18", "relevancy": 1.9862, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5154}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20When%20the%20Animal%20is%3A%20Temporal%20Animal%20Behavior%20Grounding%20with%20Positional%20Recovery%20Training&body=Title%3A%20Prompt%20When%20the%20Animal%20is%3A%20Temporal%20Animal%20Behavior%20Grounding%20with%20Positional%20Recovery%20Training%0AAuthor%3A%20Sheng%20Yan%20and%20Xin%20Du%20and%20Zongying%20Li%20and%20Yi%20Wang%20and%20Hongcang%20Jin%20and%20Mengyuan%20Liu%0AAbstract%3A%20Temporal%20grounding%20is%20crucial%20in%20multimodal%20learning%2C%20but%20it%20poses%20challenges%20when%20applied%20to%20animal%20behavior%20data%20due%20to%20the%20sparsity%20and%20uniform%20distribution%20of%20moments.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Positional%20Recovery%20Training%20framework%20%28Port%29%2C%20which%20prompts%20the%20model%20with%20the%20start%20and%20end%20times%20of%20specific%20animal%20behaviors%20during%20training.%20Specifically%2C%20%5Cport%7B%7D%20enhances%20the%20baseline%20model%20with%20a%20Recovering%20branch%20to%20reconstruct%20corrupted%20label%20sequences%20and%20align%20distributions%20via%20a%20Dual-alignment%20method.%20This%20allows%20the%20model%20to%20focus%20on%20specific%20temporal%20regions%20prompted%20by%20ground-truth%20information.%20Extensive%20experiments%20on%20the%20Animal%20Kingdom%20dataset%20demonstrate%20the%20effectiveness%20of%20%5Cport%7B%7D%2C%20achieving%20an%20IoU%400.3%20of%2038.52.%20It%20emerges%20as%20one%20of%20the%20top%20performers%20in%20the%20sub-track%20of%20MMVRAC%20in%20ICME%202024%20Grand%20Challenges.%0ALink%3A%20http%3A//arxiv.org/abs/2405.05523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520When%2520the%2520Animal%2520is%253A%2520Temporal%2520Animal%2520Behavior%2520Grounding%2520with%2520Positional%2520Recovery%2520Training%26entry.906535625%3DSheng%2520Yan%2520and%2520Xin%2520Du%2520and%2520Zongying%2520Li%2520and%2520Yi%2520Wang%2520and%2520Hongcang%2520Jin%2520and%2520Mengyuan%2520Liu%26entry.1292438233%3DTemporal%2520grounding%2520is%2520crucial%2520in%2520multimodal%2520learning%252C%2520but%2520it%2520poses%2520challenges%2520when%2520applied%2520to%2520animal%2520behavior%2520data%2520due%2520to%2520the%2520sparsity%2520and%2520uniform%2520distribution%2520of%2520moments.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520Positional%2520Recovery%2520Training%2520framework%2520%2528Port%2529%252C%2520which%2520prompts%2520the%2520model%2520with%2520the%2520start%2520and%2520end%2520times%2520of%2520specific%2520animal%2520behaviors%2520during%2520training.%2520Specifically%252C%2520%255Cport%257B%257D%2520enhances%2520the%2520baseline%2520model%2520with%2520a%2520Recovering%2520branch%2520to%2520reconstruct%2520corrupted%2520label%2520sequences%2520and%2520align%2520distributions%2520via%2520a%2520Dual-alignment%2520method.%2520This%2520allows%2520the%2520model%2520to%2520focus%2520on%2520specific%2520temporal%2520regions%2520prompted%2520by%2520ground-truth%2520information.%2520Extensive%2520experiments%2520on%2520the%2520Animal%2520Kingdom%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520%255Cport%257B%257D%252C%2520achieving%2520an%2520IoU%25400.3%2520of%252038.52.%2520It%2520emerges%2520as%2520one%2520of%2520the%2520top%2520performers%2520in%2520the%2520sub-track%2520of%2520MMVRAC%2520in%2520ICME%25202024%2520Grand%2520Challenges.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20When%20the%20Animal%20is%3A%20Temporal%20Animal%20Behavior%20Grounding%20with%20Positional%20Recovery%20Training&entry.906535625=Sheng%20Yan%20and%20Xin%20Du%20and%20Zongying%20Li%20and%20Yi%20Wang%20and%20Hongcang%20Jin%20and%20Mengyuan%20Liu&entry.1292438233=Temporal%20grounding%20is%20crucial%20in%20multimodal%20learning%2C%20but%20it%20poses%20challenges%20when%20applied%20to%20animal%20behavior%20data%20due%20to%20the%20sparsity%20and%20uniform%20distribution%20of%20moments.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Positional%20Recovery%20Training%20framework%20%28Port%29%2C%20which%20prompts%20the%20model%20with%20the%20start%20and%20end%20times%20of%20specific%20animal%20behaviors%20during%20training.%20Specifically%2C%20%5Cport%7B%7D%20enhances%20the%20baseline%20model%20with%20a%20Recovering%20branch%20to%20reconstruct%20corrupted%20label%20sequences%20and%20align%20distributions%20via%20a%20Dual-alignment%20method.%20This%20allows%20the%20model%20to%20focus%20on%20specific%20temporal%20regions%20prompted%20by%20ground-truth%20information.%20Extensive%20experiments%20on%20the%20Animal%20Kingdom%20dataset%20demonstrate%20the%20effectiveness%20of%20%5Cport%7B%7D%2C%20achieving%20an%20IoU%400.3%20of%2038.52.%20It%20emerges%20as%20one%20of%20the%20top%20performers%20in%20the%20sub-track%20of%20MMVRAC%20in%20ICME%202024%20Grand%20Challenges.&entry.1838667208=http%3A//arxiv.org/abs/2405.05523v2&entry.124074799=Read"},
{"title": "Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment", "author": "Yuyan Bu and Xiaohao Liu and ZhaoXing Ren and Yaodong Yang and Juntao Dai", "abstract": "The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.", "link": "http://arxiv.org/abs/2602.16660v1", "date": "2026-02-18", "relevancy": 1.9742, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4977}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Align%20Once%2C%20Benefit%20Multilingually%3A%20Enforcing%20Multilingual%20Consistency%20for%20LLM%20Safety%20Alignment&body=Title%3A%20Align%20Once%2C%20Benefit%20Multilingually%3A%20Enforcing%20Multilingual%20Consistency%20for%20LLM%20Safety%20Alignment%0AAuthor%3A%20Yuyan%20Bu%20and%20Xiaohao%20Liu%20and%20ZhaoXing%20Ren%20and%20Yaodong%20Yang%20and%20Juntao%20Dai%0AAbstract%3A%20The%20widespread%20deployment%20of%20large%20language%20models%20%28LLMs%29%20across%20linguistic%20communities%20necessitates%20reliable%20multilingual%20safety%20alignment.%20However%2C%20recent%20efforts%20to%20extend%20alignment%20to%20other%20languages%20often%20require%20substantial%20resources%2C%20either%20through%20large-scale%2C%20high-quality%20supervision%20in%20the%20target%20language%20or%20through%20pairwise%20alignment%20with%20high-resource%20languages%2C%20which%20limits%20scalability.%20In%20this%20work%2C%20we%20propose%20a%20resource-efficient%20method%20for%20improving%20multilingual%20safety%20alignment.%20We%20introduce%20a%20plug-and-play%20Multi-Lingual%20Consistency%20%28MLC%29%20loss%20that%20can%20be%20integrated%20into%20existing%20monolingual%20alignment%20pipelines.%20By%20improving%20collinearity%20between%20multilingual%20representation%20vectors%2C%20our%20method%20encourages%20directional%20consistency%20at%20the%20multilingual%20semantic%20level%20in%20a%20single%20update.%20This%20allows%20simultaneous%20alignment%20across%20multiple%20languages%20using%20only%20multilingual%20prompt%20variants%20without%20requiring%20additional%20response-level%20supervision%20in%20low-resource%20languages.%20We%20validate%20the%20proposed%20method%20across%20different%20model%20architectures%20and%20alignment%20paradigms%2C%20and%20demonstrate%20its%20effectiveness%20in%20enhancing%20multilingual%20safety%20with%20limited%20impact%20on%20general%20model%20utility.%20Further%20evaluation%20across%20languages%20and%20tasks%20indicates%20improved%20cross-lingual%20generalization%2C%20suggesting%20the%20proposed%20approach%20as%20a%20practical%20solution%20for%20multilingual%20consistency%20alignment%20under%20limited%20supervision.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlign%2520Once%252C%2520Benefit%2520Multilingually%253A%2520Enforcing%2520Multilingual%2520Consistency%2520for%2520LLM%2520Safety%2520Alignment%26entry.906535625%3DYuyan%2520Bu%2520and%2520Xiaohao%2520Liu%2520and%2520ZhaoXing%2520Ren%2520and%2520Yaodong%2520Yang%2520and%2520Juntao%2520Dai%26entry.1292438233%3DThe%2520widespread%2520deployment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520across%2520linguistic%2520communities%2520necessitates%2520reliable%2520multilingual%2520safety%2520alignment.%2520However%252C%2520recent%2520efforts%2520to%2520extend%2520alignment%2520to%2520other%2520languages%2520often%2520require%2520substantial%2520resources%252C%2520either%2520through%2520large-scale%252C%2520high-quality%2520supervision%2520in%2520the%2520target%2520language%2520or%2520through%2520pairwise%2520alignment%2520with%2520high-resource%2520languages%252C%2520which%2520limits%2520scalability.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520resource-efficient%2520method%2520for%2520improving%2520multilingual%2520safety%2520alignment.%2520We%2520introduce%2520a%2520plug-and-play%2520Multi-Lingual%2520Consistency%2520%2528MLC%2529%2520loss%2520that%2520can%2520be%2520integrated%2520into%2520existing%2520monolingual%2520alignment%2520pipelines.%2520By%2520improving%2520collinearity%2520between%2520multilingual%2520representation%2520vectors%252C%2520our%2520method%2520encourages%2520directional%2520consistency%2520at%2520the%2520multilingual%2520semantic%2520level%2520in%2520a%2520single%2520update.%2520This%2520allows%2520simultaneous%2520alignment%2520across%2520multiple%2520languages%2520using%2520only%2520multilingual%2520prompt%2520variants%2520without%2520requiring%2520additional%2520response-level%2520supervision%2520in%2520low-resource%2520languages.%2520We%2520validate%2520the%2520proposed%2520method%2520across%2520different%2520model%2520architectures%2520and%2520alignment%2520paradigms%252C%2520and%2520demonstrate%2520its%2520effectiveness%2520in%2520enhancing%2520multilingual%2520safety%2520with%2520limited%2520impact%2520on%2520general%2520model%2520utility.%2520Further%2520evaluation%2520across%2520languages%2520and%2520tasks%2520indicates%2520improved%2520cross-lingual%2520generalization%252C%2520suggesting%2520the%2520proposed%2520approach%2520as%2520a%2520practical%2520solution%2520for%2520multilingual%2520consistency%2520alignment%2520under%2520limited%2520supervision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Align%20Once%2C%20Benefit%20Multilingually%3A%20Enforcing%20Multilingual%20Consistency%20for%20LLM%20Safety%20Alignment&entry.906535625=Yuyan%20Bu%20and%20Xiaohao%20Liu%20and%20ZhaoXing%20Ren%20and%20Yaodong%20Yang%20and%20Juntao%20Dai&entry.1292438233=The%20widespread%20deployment%20of%20large%20language%20models%20%28LLMs%29%20across%20linguistic%20communities%20necessitates%20reliable%20multilingual%20safety%20alignment.%20However%2C%20recent%20efforts%20to%20extend%20alignment%20to%20other%20languages%20often%20require%20substantial%20resources%2C%20either%20through%20large-scale%2C%20high-quality%20supervision%20in%20the%20target%20language%20or%20through%20pairwise%20alignment%20with%20high-resource%20languages%2C%20which%20limits%20scalability.%20In%20this%20work%2C%20we%20propose%20a%20resource-efficient%20method%20for%20improving%20multilingual%20safety%20alignment.%20We%20introduce%20a%20plug-and-play%20Multi-Lingual%20Consistency%20%28MLC%29%20loss%20that%20can%20be%20integrated%20into%20existing%20monolingual%20alignment%20pipelines.%20By%20improving%20collinearity%20between%20multilingual%20representation%20vectors%2C%20our%20method%20encourages%20directional%20consistency%20at%20the%20multilingual%20semantic%20level%20in%20a%20single%20update.%20This%20allows%20simultaneous%20alignment%20across%20multiple%20languages%20using%20only%20multilingual%20prompt%20variants%20without%20requiring%20additional%20response-level%20supervision%20in%20low-resource%20languages.%20We%20validate%20the%20proposed%20method%20across%20different%20model%20architectures%20and%20alignment%20paradigms%2C%20and%20demonstrate%20its%20effectiveness%20in%20enhancing%20multilingual%20safety%20with%20limited%20impact%20on%20general%20model%20utility.%20Further%20evaluation%20across%20languages%20and%20tasks%20indicates%20improved%20cross-lingual%20generalization%2C%20suggesting%20the%20proposed%20approach%20as%20a%20practical%20solution%20for%20multilingual%20consistency%20alignment%20under%20limited%20supervision.&entry.1838667208=http%3A//arxiv.org/abs/2602.16660v1&entry.124074799=Read"},
{"title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks", "author": "Huan Souza and Pankaj Mehta", "abstract": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.", "link": "http://arxiv.org/abs/2602.16696v1", "date": "2026-02-18", "relevancy": 1.9724, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-free%20representations%20outperform%20single-cell%20foundation%20models%20on%20downstream%20benchmarks&body=Title%3A%20Parameter-free%20representations%20outperform%20single-cell%20foundation%20models%20on%20downstream%20benchmarks%0AAuthor%3A%20Huan%20Souza%20and%20Pankaj%20Mehta%0AAbstract%3A%20Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20data%20exhibit%20strong%20and%20reproducible%20statistical%20structure.%20This%20has%20motivated%20the%20development%20of%20large-scale%20foundation%20models%2C%20such%20as%20TranscriptFormer%2C%20that%20use%20transformer-based%20architectures%20to%20learn%20a%20generative%20model%20for%20gene%20expression%20by%20embedding%20genes%20into%20a%20latent%20vector%20space.%20These%20embeddings%20have%20been%20used%20to%20obtain%20state-of-the-art%20%28SOTA%29%20performance%20on%20downstream%20tasks%20such%20as%20cell-type%20classification%2C%20disease-state%20prediction%2C%20and%20cross-species%20learning.%20Here%2C%20we%20ask%20whether%20similar%20performance%20can%20be%20achieved%20without%20utilizing%20computationally%20intensive%20deep%20learning-based%20representations.%20Using%20simple%2C%20interpretable%20pipelines%20that%20rely%20on%20careful%20normalization%20and%20linear%20methods%2C%20we%20obtain%20SOTA%20or%20near%20SOTA%20performance%20across%20multiple%20benchmarks%20commonly%20used%20to%20evaluate%20single-cell%20foundation%20models%2C%20including%20outperforming%20foundation%20models%20on%20out-of-distribution%20tasks%20involving%20novel%20cell%20types%20and%20organisms%20absent%20from%20the%20training%20data.%20Our%20findings%20highlight%20the%20need%20for%20rigorous%20benchmarking%20and%20suggest%20that%20the%20biology%20of%20cell%20identity%20can%20be%20captured%20by%20simple%20linear%20representations%20of%20single%20cell%20gene%20expression%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-free%2520representations%2520outperform%2520single-cell%2520foundation%2520models%2520on%2520downstream%2520benchmarks%26entry.906535625%3DHuan%2520Souza%2520and%2520Pankaj%2520Mehta%26entry.1292438233%3DSingle-cell%2520RNA%2520sequencing%2520%2528scRNA-seq%2529%2520data%2520exhibit%2520strong%2520and%2520reproducible%2520statistical%2520structure.%2520This%2520has%2520motivated%2520the%2520development%2520of%2520large-scale%2520foundation%2520models%252C%2520such%2520as%2520TranscriptFormer%252C%2520that%2520use%2520transformer-based%2520architectures%2520to%2520learn%2520a%2520generative%2520model%2520for%2520gene%2520expression%2520by%2520embedding%2520genes%2520into%2520a%2520latent%2520vector%2520space.%2520These%2520embeddings%2520have%2520been%2520used%2520to%2520obtain%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520on%2520downstream%2520tasks%2520such%2520as%2520cell-type%2520classification%252C%2520disease-state%2520prediction%252C%2520and%2520cross-species%2520learning.%2520Here%252C%2520we%2520ask%2520whether%2520similar%2520performance%2520can%2520be%2520achieved%2520without%2520utilizing%2520computationally%2520intensive%2520deep%2520learning-based%2520representations.%2520Using%2520simple%252C%2520interpretable%2520pipelines%2520that%2520rely%2520on%2520careful%2520normalization%2520and%2520linear%2520methods%252C%2520we%2520obtain%2520SOTA%2520or%2520near%2520SOTA%2520performance%2520across%2520multiple%2520benchmarks%2520commonly%2520used%2520to%2520evaluate%2520single-cell%2520foundation%2520models%252C%2520including%2520outperforming%2520foundation%2520models%2520on%2520out-of-distribution%2520tasks%2520involving%2520novel%2520cell%2520types%2520and%2520organisms%2520absent%2520from%2520the%2520training%2520data.%2520Our%2520findings%2520highlight%2520the%2520need%2520for%2520rigorous%2520benchmarking%2520and%2520suggest%2520that%2520the%2520biology%2520of%2520cell%2520identity%2520can%2520be%2520captured%2520by%2520simple%2520linear%2520representations%2520of%2520single%2520cell%2520gene%2520expression%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-free%20representations%20outperform%20single-cell%20foundation%20models%20on%20downstream%20benchmarks&entry.906535625=Huan%20Souza%20and%20Pankaj%20Mehta&entry.1292438233=Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20data%20exhibit%20strong%20and%20reproducible%20statistical%20structure.%20This%20has%20motivated%20the%20development%20of%20large-scale%20foundation%20models%2C%20such%20as%20TranscriptFormer%2C%20that%20use%20transformer-based%20architectures%20to%20learn%20a%20generative%20model%20for%20gene%20expression%20by%20embedding%20genes%20into%20a%20latent%20vector%20space.%20These%20embeddings%20have%20been%20used%20to%20obtain%20state-of-the-art%20%28SOTA%29%20performance%20on%20downstream%20tasks%20such%20as%20cell-type%20classification%2C%20disease-state%20prediction%2C%20and%20cross-species%20learning.%20Here%2C%20we%20ask%20whether%20similar%20performance%20can%20be%20achieved%20without%20utilizing%20computationally%20intensive%20deep%20learning-based%20representations.%20Using%20simple%2C%20interpretable%20pipelines%20that%20rely%20on%20careful%20normalization%20and%20linear%20methods%2C%20we%20obtain%20SOTA%20or%20near%20SOTA%20performance%20across%20multiple%20benchmarks%20commonly%20used%20to%20evaluate%20single-cell%20foundation%20models%2C%20including%20outperforming%20foundation%20models%20on%20out-of-distribution%20tasks%20involving%20novel%20cell%20types%20and%20organisms%20absent%20from%20the%20training%20data.%20Our%20findings%20highlight%20the%20need%20for%20rigorous%20benchmarking%20and%20suggest%20that%20the%20biology%20of%20cell%20identity%20can%20be%20captured%20by%20simple%20linear%20representations%20of%20single%20cell%20gene%20expression%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.16696v1&entry.124074799=Read"},
{"title": "Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks", "author": "Sarkis Ter Martirosyan and Xinyue Huang and David Qin and Anthony Yu and Stanislav Emelianov", "abstract": "Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \\textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.", "link": "http://arxiv.org/abs/2602.16357v1", "date": "2026-02-18", "relevancy": 1.9673, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4954}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4924}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optical%20Inversion%20and%20Spectral%20Unmixing%20of%20Spectroscopic%20Photoacoustic%20Images%20with%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Optical%20Inversion%20and%20Spectral%20Unmixing%20of%20Spectroscopic%20Photoacoustic%20Images%20with%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Sarkis%20Ter%20Martirosyan%20and%20Xinyue%20Huang%20and%20David%20Qin%20and%20Anthony%20Yu%20and%20Stanislav%20Emelianov%0AAbstract%3A%20Accurate%20estimation%20of%20the%20relative%20concentrations%20of%20chromophores%20in%20a%20spectroscopic%20photoacoustic%20%28sPA%29%20image%20can%20reveal%20immense%20structural%2C%20functional%2C%20and%20molecular%20information%20about%20physiological%20processes.%20However%2C%20due%20to%20nonlinearities%20and%20ill-posedness%20inherent%20to%20sPA%20imaging%2C%20concentration%20estimation%20is%20intractable.%20The%20Spectroscopic%20Photoacoustic%20Optical%20Inversion%20Autoencoder%20%28SPOI-AE%29%20aims%20to%20address%20the%20sPA%20optical%20inversion%20and%20spectral%20unmixing%20problems%20without%20assuming%20linearity.%20Herein%2C%20SPOI-AE%20was%20trained%20and%20tested%20on%20%5Ctextit%7Bin%20vivo%7D%20mouse%20lymph%20node%20sPA%20images%20with%20unknown%20ground%20truth%20chromophore%20concentrations.%20SPOI-AE%20better%20reconstructs%20input%20sPA%20pixels%20than%20conventional%20algorithms%20while%20providing%20biologically%20coherent%20estimates%20for%20optical%20parameters%2C%20chromophore%20concentrations%2C%20and%20the%20percent%20oxygen%20saturation%20of%20tissue.%20SPOI-AE%27s%20unmixing%20accuracy%20was%20validated%20using%20a%20simulated%20mouse%20lymph%20node%20phantom%20ground%20truth.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptical%2520Inversion%2520and%2520Spectral%2520Unmixing%2520of%2520Spectroscopic%2520Photoacoustic%2520Images%2520with%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DSarkis%2520Ter%2520Martirosyan%2520and%2520Xinyue%2520Huang%2520and%2520David%2520Qin%2520and%2520Anthony%2520Yu%2520and%2520Stanislav%2520Emelianov%26entry.1292438233%3DAccurate%2520estimation%2520of%2520the%2520relative%2520concentrations%2520of%2520chromophores%2520in%2520a%2520spectroscopic%2520photoacoustic%2520%2528sPA%2529%2520image%2520can%2520reveal%2520immense%2520structural%252C%2520functional%252C%2520and%2520molecular%2520information%2520about%2520physiological%2520processes.%2520However%252C%2520due%2520to%2520nonlinearities%2520and%2520ill-posedness%2520inherent%2520to%2520sPA%2520imaging%252C%2520concentration%2520estimation%2520is%2520intractable.%2520The%2520Spectroscopic%2520Photoacoustic%2520Optical%2520Inversion%2520Autoencoder%2520%2528SPOI-AE%2529%2520aims%2520to%2520address%2520the%2520sPA%2520optical%2520inversion%2520and%2520spectral%2520unmixing%2520problems%2520without%2520assuming%2520linearity.%2520Herein%252C%2520SPOI-AE%2520was%2520trained%2520and%2520tested%2520on%2520%255Ctextit%257Bin%2520vivo%257D%2520mouse%2520lymph%2520node%2520sPA%2520images%2520with%2520unknown%2520ground%2520truth%2520chromophore%2520concentrations.%2520SPOI-AE%2520better%2520reconstructs%2520input%2520sPA%2520pixels%2520than%2520conventional%2520algorithms%2520while%2520providing%2520biologically%2520coherent%2520estimates%2520for%2520optical%2520parameters%252C%2520chromophore%2520concentrations%252C%2520and%2520the%2520percent%2520oxygen%2520saturation%2520of%2520tissue.%2520SPOI-AE%2527s%2520unmixing%2520accuracy%2520was%2520validated%2520using%2520a%2520simulated%2520mouse%2520lymph%2520node%2520phantom%2520ground%2520truth.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optical%20Inversion%20and%20Spectral%20Unmixing%20of%20Spectroscopic%20Photoacoustic%20Images%20with%20Physics-Informed%20Neural%20Networks&entry.906535625=Sarkis%20Ter%20Martirosyan%20and%20Xinyue%20Huang%20and%20David%20Qin%20and%20Anthony%20Yu%20and%20Stanislav%20Emelianov&entry.1292438233=Accurate%20estimation%20of%20the%20relative%20concentrations%20of%20chromophores%20in%20a%20spectroscopic%20photoacoustic%20%28sPA%29%20image%20can%20reveal%20immense%20structural%2C%20functional%2C%20and%20molecular%20information%20about%20physiological%20processes.%20However%2C%20due%20to%20nonlinearities%20and%20ill-posedness%20inherent%20to%20sPA%20imaging%2C%20concentration%20estimation%20is%20intractable.%20The%20Spectroscopic%20Photoacoustic%20Optical%20Inversion%20Autoencoder%20%28SPOI-AE%29%20aims%20to%20address%20the%20sPA%20optical%20inversion%20and%20spectral%20unmixing%20problems%20without%20assuming%20linearity.%20Herein%2C%20SPOI-AE%20was%20trained%20and%20tested%20on%20%5Ctextit%7Bin%20vivo%7D%20mouse%20lymph%20node%20sPA%20images%20with%20unknown%20ground%20truth%20chromophore%20concentrations.%20SPOI-AE%20better%20reconstructs%20input%20sPA%20pixels%20than%20conventional%20algorithms%20while%20providing%20biologically%20coherent%20estimates%20for%20optical%20parameters%2C%20chromophore%20concentrations%2C%20and%20the%20percent%20oxygen%20saturation%20of%20tissue.%20SPOI-AE%27s%20unmixing%20accuracy%20was%20validated%20using%20a%20simulated%20mouse%20lymph%20node%20phantom%20ground%20truth.&entry.1838667208=http%3A//arxiv.org/abs/2602.16357v1&entry.124074799=Read"},
{"title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC", "author": "Abdulla Jasem Almansoori and Maria Ivanova and Andrey Veprikov and Aleksandr Beznosikov and Samuel Horv\u00e1th and Martin Tak\u00e1\u010d", "abstract": "Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.", "link": "http://arxiv.org/abs/2602.16456v1", "date": "2026-02-18", "relevancy": 1.9667, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4941}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4923}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20SGD%2C%20Without%20SVD%3A%20Proximal%20Subspace%20Iteration%20LoRA%20with%20Diagonal%20Fractional%20K-FAC&body=Title%3A%20Beyond%20SGD%2C%20Without%20SVD%3A%20Proximal%20Subspace%20Iteration%20LoRA%20with%20Diagonal%20Fractional%20K-FAC%0AAuthor%3A%20Abdulla%20Jasem%20Almansoori%20and%20Maria%20Ivanova%20and%20Andrey%20Veprikov%20and%20Aleksandr%20Beznosikov%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D%0AAbstract%3A%20Low-Rank%20Adaptation%20%28LoRA%29%20fine-tunes%20large%20models%20by%20learning%20low-rank%20updates%20on%20top%20of%20frozen%20weights%2C%20dramatically%20reducing%20trainable%20parameters%20and%20memory.%20In%20this%20work%2C%20we%20address%20the%20gap%20between%20training%20with%20full%20steps%20with%20low-rank%20projections%20%28SVDLoRA%29%20and%20LoRA%20fine-tuning.%20We%20propose%20LoRSum%2C%20a%20memory-efficient%20subroutine%20that%20closes%20this%20gap%20for%20gradient%20descent%20by%20casting%20LoRA%20optimization%20as%20a%20proximal%20sub-problem%20and%20solving%20it%20efficiently%20with%20alternating%20least%20squares%20updates%2C%20which%20we%20prove%20to%20be%20an%20implicit%20block%20power%20method.%20We%20recover%20several%20recently%20proposed%20preconditioning%20methods%20for%20LoRA%20as%20special%20cases%2C%20and%20show%20that%20LoRSum%20can%20also%20be%20used%20for%20updating%20a%20low-rank%20momentum.%20In%20order%20to%20address%20full%20steps%20with%20preconditioned%20gradient%20descent%2C%20we%20propose%20a%20scaled%20variant%20of%20LoRSum%20that%20uses%20structured%20metrics%20such%20as%20K-FAC%20and%20Shampoo%2C%20and%20we%20show%20that%20storing%20the%20diagonal%20of%20these%20metrics%20still%20allows%20them%20to%20perform%20well%20while%20remaining%20memory-efficient.%20Experiments%20on%20a%20synthetic%20task%2C%20CIFAR-100%2C%20and%20language-model%20fine-tuning%20on%20GLUE%2C%20SQuAD%20v2%2C%20and%20WikiText-103%2C%20show%20that%20our%20method%20can%20match%20or%20improve%20LoRA%20baselines%20given%20modest%20compute%20overhead%2C%20while%20avoiding%20full-matrix%20SVD%20projections%20and%20retaining%20LoRA-style%20parameter%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520SGD%252C%2520Without%2520SVD%253A%2520Proximal%2520Subspace%2520Iteration%2520LoRA%2520with%2520Diagonal%2520Fractional%2520K-FAC%26entry.906535625%3DAbdulla%2520Jasem%2520Almansoori%2520and%2520Maria%2520Ivanova%2520and%2520Andrey%2520Veprikov%2520and%2520Aleksandr%2520Beznosikov%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Martin%2520Tak%25C3%25A1%25C4%258D%26entry.1292438233%3DLow-Rank%2520Adaptation%2520%2528LoRA%2529%2520fine-tunes%2520large%2520models%2520by%2520learning%2520low-rank%2520updates%2520on%2520top%2520of%2520frozen%2520weights%252C%2520dramatically%2520reducing%2520trainable%2520parameters%2520and%2520memory.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520gap%2520between%2520training%2520with%2520full%2520steps%2520with%2520low-rank%2520projections%2520%2528SVDLoRA%2529%2520and%2520LoRA%2520fine-tuning.%2520We%2520propose%2520LoRSum%252C%2520a%2520memory-efficient%2520subroutine%2520that%2520closes%2520this%2520gap%2520for%2520gradient%2520descent%2520by%2520casting%2520LoRA%2520optimization%2520as%2520a%2520proximal%2520sub-problem%2520and%2520solving%2520it%2520efficiently%2520with%2520alternating%2520least%2520squares%2520updates%252C%2520which%2520we%2520prove%2520to%2520be%2520an%2520implicit%2520block%2520power%2520method.%2520We%2520recover%2520several%2520recently%2520proposed%2520preconditioning%2520methods%2520for%2520LoRA%2520as%2520special%2520cases%252C%2520and%2520show%2520that%2520LoRSum%2520can%2520also%2520be%2520used%2520for%2520updating%2520a%2520low-rank%2520momentum.%2520In%2520order%2520to%2520address%2520full%2520steps%2520with%2520preconditioned%2520gradient%2520descent%252C%2520we%2520propose%2520a%2520scaled%2520variant%2520of%2520LoRSum%2520that%2520uses%2520structured%2520metrics%2520such%2520as%2520K-FAC%2520and%2520Shampoo%252C%2520and%2520we%2520show%2520that%2520storing%2520the%2520diagonal%2520of%2520these%2520metrics%2520still%2520allows%2520them%2520to%2520perform%2520well%2520while%2520remaining%2520memory-efficient.%2520Experiments%2520on%2520a%2520synthetic%2520task%252C%2520CIFAR-100%252C%2520and%2520language-model%2520fine-tuning%2520on%2520GLUE%252C%2520SQuAD%2520v2%252C%2520and%2520WikiText-103%252C%2520show%2520that%2520our%2520method%2520can%2520match%2520or%2520improve%2520LoRA%2520baselines%2520given%2520modest%2520compute%2520overhead%252C%2520while%2520avoiding%2520full-matrix%2520SVD%2520projections%2520and%2520retaining%2520LoRA-style%2520parameter%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20SGD%2C%20Without%20SVD%3A%20Proximal%20Subspace%20Iteration%20LoRA%20with%20Diagonal%20Fractional%20K-FAC&entry.906535625=Abdulla%20Jasem%20Almansoori%20and%20Maria%20Ivanova%20and%20Andrey%20Veprikov%20and%20Aleksandr%20Beznosikov%20and%20Samuel%20Horv%C3%A1th%20and%20Martin%20Tak%C3%A1%C4%8D&entry.1292438233=Low-Rank%20Adaptation%20%28LoRA%29%20fine-tunes%20large%20models%20by%20learning%20low-rank%20updates%20on%20top%20of%20frozen%20weights%2C%20dramatically%20reducing%20trainable%20parameters%20and%20memory.%20In%20this%20work%2C%20we%20address%20the%20gap%20between%20training%20with%20full%20steps%20with%20low-rank%20projections%20%28SVDLoRA%29%20and%20LoRA%20fine-tuning.%20We%20propose%20LoRSum%2C%20a%20memory-efficient%20subroutine%20that%20closes%20this%20gap%20for%20gradient%20descent%20by%20casting%20LoRA%20optimization%20as%20a%20proximal%20sub-problem%20and%20solving%20it%20efficiently%20with%20alternating%20least%20squares%20updates%2C%20which%20we%20prove%20to%20be%20an%20implicit%20block%20power%20method.%20We%20recover%20several%20recently%20proposed%20preconditioning%20methods%20for%20LoRA%20as%20special%20cases%2C%20and%20show%20that%20LoRSum%20can%20also%20be%20used%20for%20updating%20a%20low-rank%20momentum.%20In%20order%20to%20address%20full%20steps%20with%20preconditioned%20gradient%20descent%2C%20we%20propose%20a%20scaled%20variant%20of%20LoRSum%20that%20uses%20structured%20metrics%20such%20as%20K-FAC%20and%20Shampoo%2C%20and%20we%20show%20that%20storing%20the%20diagonal%20of%20these%20metrics%20still%20allows%20them%20to%20perform%20well%20while%20remaining%20memory-efficient.%20Experiments%20on%20a%20synthetic%20task%2C%20CIFAR-100%2C%20and%20language-model%20fine-tuning%20on%20GLUE%2C%20SQuAD%20v2%2C%20and%20WikiText-103%2C%20show%20that%20our%20method%20can%20match%20or%20improve%20LoRA%20baselines%20given%20modest%20compute%20overhead%2C%20while%20avoiding%20full-matrix%20SVD%20projections%20and%20retaining%20LoRA-style%20parameter%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2602.16456v1&entry.124074799=Read"},
{"title": "Inverting Non-Injective Functions with Twin Neural Network Regression", "author": "Sebastian J. Wetzel", "abstract": "Non-injective functions are not globally invertible. However, they can often be restricted to locally injective subdomains where the inversion is well-defined. In many settings a preferred solution can be selected even when multiple valid preimages exist or input and output dimensions differ. This manuscript describes a natural reformulation of the inverse learning problem for non-injective functions as a collection of locally invertible problems. More precisely, Twin Neural Network Regression is trained to predict local inverse corrections around known anchor points. By anchoring predictions to points within the same locally invertible region, the method consistently selects a valid branch of the inverse. In contrast to current probabilistic state-of-the art inversion methods, Inverse Twin Neural Network Regression is a deterministic framework for resolving multi-valued inverse mappings. I demonstrate the approach on problems that are defined by mathematical equations or by data, including multi-solution toy problems and robot arm inverse kinematics.", "link": "http://arxiv.org/abs/2601.05378v2", "date": "2026-02-18", "relevancy": 1.9608, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5039}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4852}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverting%20Non-Injective%20Functions%20with%20Twin%20Neural%20Network%20Regression&body=Title%3A%20Inverting%20Non-Injective%20Functions%20with%20Twin%20Neural%20Network%20Regression%0AAuthor%3A%20Sebastian%20J.%20Wetzel%0AAbstract%3A%20Non-injective%20functions%20are%20not%20globally%20invertible.%20However%2C%20they%20can%20often%20be%20restricted%20to%20locally%20injective%20subdomains%20where%20the%20inversion%20is%20well-defined.%20In%20many%20settings%20a%20preferred%20solution%20can%20be%20selected%20even%20when%20multiple%20valid%20preimages%20exist%20or%20input%20and%20output%20dimensions%20differ.%20This%20manuscript%20describes%20a%20natural%20reformulation%20of%20the%20inverse%20learning%20problem%20for%20non-injective%20functions%20as%20a%20collection%20of%20locally%20invertible%20problems.%20More%20precisely%2C%20Twin%20Neural%20Network%20Regression%20is%20trained%20to%20predict%20local%20inverse%20corrections%20around%20known%20anchor%20points.%20By%20anchoring%20predictions%20to%20points%20within%20the%20same%20locally%20invertible%20region%2C%20the%20method%20consistently%20selects%20a%20valid%20branch%20of%20the%20inverse.%20In%20contrast%20to%20current%20probabilistic%20state-of-the%20art%20inversion%20methods%2C%20Inverse%20Twin%20Neural%20Network%20Regression%20is%20a%20deterministic%20framework%20for%20resolving%20multi-valued%20inverse%20mappings.%20I%20demonstrate%20the%20approach%20on%20problems%20that%20are%20defined%20by%20mathematical%20equations%20or%20by%20data%2C%20including%20multi-solution%20toy%20problems%20and%20robot%20arm%20inverse%20kinematics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverting%2520Non-Injective%2520Functions%2520with%2520Twin%2520Neural%2520Network%2520Regression%26entry.906535625%3DSebastian%2520J.%2520Wetzel%26entry.1292438233%3DNon-injective%2520functions%2520are%2520not%2520globally%2520invertible.%2520However%252C%2520they%2520can%2520often%2520be%2520restricted%2520to%2520locally%2520injective%2520subdomains%2520where%2520the%2520inversion%2520is%2520well-defined.%2520In%2520many%2520settings%2520a%2520preferred%2520solution%2520can%2520be%2520selected%2520even%2520when%2520multiple%2520valid%2520preimages%2520exist%2520or%2520input%2520and%2520output%2520dimensions%2520differ.%2520This%2520manuscript%2520describes%2520a%2520natural%2520reformulation%2520of%2520the%2520inverse%2520learning%2520problem%2520for%2520non-injective%2520functions%2520as%2520a%2520collection%2520of%2520locally%2520invertible%2520problems.%2520More%2520precisely%252C%2520Twin%2520Neural%2520Network%2520Regression%2520is%2520trained%2520to%2520predict%2520local%2520inverse%2520corrections%2520around%2520known%2520anchor%2520points.%2520By%2520anchoring%2520predictions%2520to%2520points%2520within%2520the%2520same%2520locally%2520invertible%2520region%252C%2520the%2520method%2520consistently%2520selects%2520a%2520valid%2520branch%2520of%2520the%2520inverse.%2520In%2520contrast%2520to%2520current%2520probabilistic%2520state-of-the%2520art%2520inversion%2520methods%252C%2520Inverse%2520Twin%2520Neural%2520Network%2520Regression%2520is%2520a%2520deterministic%2520framework%2520for%2520resolving%2520multi-valued%2520inverse%2520mappings.%2520I%2520demonstrate%2520the%2520approach%2520on%2520problems%2520that%2520are%2520defined%2520by%2520mathematical%2520equations%2520or%2520by%2520data%252C%2520including%2520multi-solution%2520toy%2520problems%2520and%2520robot%2520arm%2520inverse%2520kinematics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverting%20Non-Injective%20Functions%20with%20Twin%20Neural%20Network%20Regression&entry.906535625=Sebastian%20J.%20Wetzel&entry.1292438233=Non-injective%20functions%20are%20not%20globally%20invertible.%20However%2C%20they%20can%20often%20be%20restricted%20to%20locally%20injective%20subdomains%20where%20the%20inversion%20is%20well-defined.%20In%20many%20settings%20a%20preferred%20solution%20can%20be%20selected%20even%20when%20multiple%20valid%20preimages%20exist%20or%20input%20and%20output%20dimensions%20differ.%20This%20manuscript%20describes%20a%20natural%20reformulation%20of%20the%20inverse%20learning%20problem%20for%20non-injective%20functions%20as%20a%20collection%20of%20locally%20invertible%20problems.%20More%20precisely%2C%20Twin%20Neural%20Network%20Regression%20is%20trained%20to%20predict%20local%20inverse%20corrections%20around%20known%20anchor%20points.%20By%20anchoring%20predictions%20to%20points%20within%20the%20same%20locally%20invertible%20region%2C%20the%20method%20consistently%20selects%20a%20valid%20branch%20of%20the%20inverse.%20In%20contrast%20to%20current%20probabilistic%20state-of-the%20art%20inversion%20methods%2C%20Inverse%20Twin%20Neural%20Network%20Regression%20is%20a%20deterministic%20framework%20for%20resolving%20multi-valued%20inverse%20mappings.%20I%20demonstrate%20the%20approach%20on%20problems%20that%20are%20defined%20by%20mathematical%20equations%20or%20by%20data%2C%20including%20multi-solution%20toy%20problems%20and%20robot%20arm%20inverse%20kinematics.&entry.1838667208=http%3A//arxiv.org/abs/2601.05378v2&entry.124074799=Read"},
{"title": "Automated Histopathology Report Generation via Pyramidal Feature Extraction and the UNI Foundation Model", "author": "Ahmet Halici and Ece Tugba Cebeci and Musa Balci and Mustafa Cini and Serkan Sokmen", "abstract": "Generating diagnostic text from histopathology whole slide images (WSIs) is challenging due to the gigapixel scale of the input and the requirement for precise, domain specific language. We propose a hierarchical vision language framework that combines a frozen pathology foundation model with a Transformer decoder for report generation. To make WSI processing tractable, we perform multi resolution pyramidal patch selection (downsampling factors 2^3 to 2^6) and remove background and artifacts using Laplacian variance and HSV based criteria. Patch features are extracted with the UNI Vision Transformer and projected to a 6 layer Transformer decoder that generates diagnostic text via cross attention. To better represent biomedical terminology, we tokenize the output using BioGPT. Finally, we add a retrieval based verification step that compares generated reports with a reference corpus using Sentence BERT embeddings; if a high similarity match is found, the generated report is replaced with the retrieved ground truth reference to improve reliability.", "link": "http://arxiv.org/abs/2602.16422v1", "date": "2026-02-18", "relevancy": 1.9567, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5062}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Histopathology%20Report%20Generation%20via%20Pyramidal%20Feature%20Extraction%20and%20the%20UNI%20Foundation%20Model&body=Title%3A%20Automated%20Histopathology%20Report%20Generation%20via%20Pyramidal%20Feature%20Extraction%20and%20the%20UNI%20Foundation%20Model%0AAuthor%3A%20Ahmet%20Halici%20and%20Ece%20Tugba%20Cebeci%20and%20Musa%20Balci%20and%20Mustafa%20Cini%20and%20Serkan%20Sokmen%0AAbstract%3A%20Generating%20diagnostic%20text%20from%20histopathology%20whole%20slide%20images%20%28WSIs%29%20is%20challenging%20due%20to%20the%20gigapixel%20scale%20of%20the%20input%20and%20the%20requirement%20for%20precise%2C%20domain%20specific%20language.%20We%20propose%20a%20hierarchical%20vision%20language%20framework%20that%20combines%20a%20frozen%20pathology%20foundation%20model%20with%20a%20Transformer%20decoder%20for%20report%20generation.%20To%20make%20WSI%20processing%20tractable%2C%20we%20perform%20multi%20resolution%20pyramidal%20patch%20selection%20%28downsampling%20factors%202%5E3%20to%202%5E6%29%20and%20remove%20background%20and%20artifacts%20using%20Laplacian%20variance%20and%20HSV%20based%20criteria.%20Patch%20features%20are%20extracted%20with%20the%20UNI%20Vision%20Transformer%20and%20projected%20to%20a%206%20layer%20Transformer%20decoder%20that%20generates%20diagnostic%20text%20via%20cross%20attention.%20To%20better%20represent%20biomedical%20terminology%2C%20we%20tokenize%20the%20output%20using%20BioGPT.%20Finally%2C%20we%20add%20a%20retrieval%20based%20verification%20step%20that%20compares%20generated%20reports%20with%20a%20reference%20corpus%20using%20Sentence%20BERT%20embeddings%3B%20if%20a%20high%20similarity%20match%20is%20found%2C%20the%20generated%20report%20is%20replaced%20with%20the%20retrieved%20ground%20truth%20reference%20to%20improve%20reliability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Histopathology%2520Report%2520Generation%2520via%2520Pyramidal%2520Feature%2520Extraction%2520and%2520the%2520UNI%2520Foundation%2520Model%26entry.906535625%3DAhmet%2520Halici%2520and%2520Ece%2520Tugba%2520Cebeci%2520and%2520Musa%2520Balci%2520and%2520Mustafa%2520Cini%2520and%2520Serkan%2520Sokmen%26entry.1292438233%3DGenerating%2520diagnostic%2520text%2520from%2520histopathology%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520is%2520challenging%2520due%2520to%2520the%2520gigapixel%2520scale%2520of%2520the%2520input%2520and%2520the%2520requirement%2520for%2520precise%252C%2520domain%2520specific%2520language.%2520We%2520propose%2520a%2520hierarchical%2520vision%2520language%2520framework%2520that%2520combines%2520a%2520frozen%2520pathology%2520foundation%2520model%2520with%2520a%2520Transformer%2520decoder%2520for%2520report%2520generation.%2520To%2520make%2520WSI%2520processing%2520tractable%252C%2520we%2520perform%2520multi%2520resolution%2520pyramidal%2520patch%2520selection%2520%2528downsampling%2520factors%25202%255E3%2520to%25202%255E6%2529%2520and%2520remove%2520background%2520and%2520artifacts%2520using%2520Laplacian%2520variance%2520and%2520HSV%2520based%2520criteria.%2520Patch%2520features%2520are%2520extracted%2520with%2520the%2520UNI%2520Vision%2520Transformer%2520and%2520projected%2520to%2520a%25206%2520layer%2520Transformer%2520decoder%2520that%2520generates%2520diagnostic%2520text%2520via%2520cross%2520attention.%2520To%2520better%2520represent%2520biomedical%2520terminology%252C%2520we%2520tokenize%2520the%2520output%2520using%2520BioGPT.%2520Finally%252C%2520we%2520add%2520a%2520retrieval%2520based%2520verification%2520step%2520that%2520compares%2520generated%2520reports%2520with%2520a%2520reference%2520corpus%2520using%2520Sentence%2520BERT%2520embeddings%253B%2520if%2520a%2520high%2520similarity%2520match%2520is%2520found%252C%2520the%2520generated%2520report%2520is%2520replaced%2520with%2520the%2520retrieved%2520ground%2520truth%2520reference%2520to%2520improve%2520reliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Histopathology%20Report%20Generation%20via%20Pyramidal%20Feature%20Extraction%20and%20the%20UNI%20Foundation%20Model&entry.906535625=Ahmet%20Halici%20and%20Ece%20Tugba%20Cebeci%20and%20Musa%20Balci%20and%20Mustafa%20Cini%20and%20Serkan%20Sokmen&entry.1292438233=Generating%20diagnostic%20text%20from%20histopathology%20whole%20slide%20images%20%28WSIs%29%20is%20challenging%20due%20to%20the%20gigapixel%20scale%20of%20the%20input%20and%20the%20requirement%20for%20precise%2C%20domain%20specific%20language.%20We%20propose%20a%20hierarchical%20vision%20language%20framework%20that%20combines%20a%20frozen%20pathology%20foundation%20model%20with%20a%20Transformer%20decoder%20for%20report%20generation.%20To%20make%20WSI%20processing%20tractable%2C%20we%20perform%20multi%20resolution%20pyramidal%20patch%20selection%20%28downsampling%20factors%202%5E3%20to%202%5E6%29%20and%20remove%20background%20and%20artifacts%20using%20Laplacian%20variance%20and%20HSV%20based%20criteria.%20Patch%20features%20are%20extracted%20with%20the%20UNI%20Vision%20Transformer%20and%20projected%20to%20a%206%20layer%20Transformer%20decoder%20that%20generates%20diagnostic%20text%20via%20cross%20attention.%20To%20better%20represent%20biomedical%20terminology%2C%20we%20tokenize%20the%20output%20using%20BioGPT.%20Finally%2C%20we%20add%20a%20retrieval%20based%20verification%20step%20that%20compares%20generated%20reports%20with%20a%20reference%20corpus%20using%20Sentence%20BERT%20embeddings%3B%20if%20a%20high%20similarity%20match%20is%20found%2C%20the%20generated%20report%20is%20replaced%20with%20the%20retrieved%20ground%20truth%20reference%20to%20improve%20reliability.&entry.1838667208=http%3A//arxiv.org/abs/2602.16422v1&entry.124074799=Read"},
{"title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction", "author": "Ankitkumar Joshi and Milos Hauskrecht", "abstract": "Modeling irregularly sampled multivariate time series is a persistent challenge in domains like healthcare and sensor networks. While recent works have explored a variety of complex learning architectures to solve the prediction problems for irregularly sampled time series, it remains unclear what the true benefits of some of these architectures are, and whether clever modifications of simpler and more efficient RNN-based algorithms are still competitive, i.e. they are on par with or even superior to these methods. In this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential basis functions, that builds upon RNN-based architectures for observations made at irregular times. GRUwE supports both regression-based and event-based predictions in continuous time. GRUwE works by maintaining a Markov state representation of the time series that updates with the arrival of irregular observations. The Markov state update relies on two reset mechanisms: (i) observation-triggered reset to account for the new observation, and (ii) time-triggered reset that relies on learnable exponential decays, to support the predictions in continuous time. Our empirical evaluations across several real-world benchmarks on next-observation and next-event prediction tasks demonstrate that GRUwE can indeed achieve competitive or superior performance compared to the recent state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers compelling advantages: it is easy to implement, requires minimal hyper-parameter tuning efforts, and significantly reduces the computational overhead in the online deployment.", "link": "http://arxiv.org/abs/2510.16161v2", "date": "2026-02-18", "relevancy": 1.9529, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4914}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Still%20Competitive%3A%20Revisiting%20Recurrent%20Models%20for%20Irregular%20Time%20Series%20Prediction&body=Title%3A%20Still%20Competitive%3A%20Revisiting%20Recurrent%20Models%20for%20Irregular%20Time%20Series%20Prediction%0AAuthor%3A%20Ankitkumar%20Joshi%20and%20Milos%20Hauskrecht%0AAbstract%3A%20Modeling%20irregularly%20sampled%20multivariate%20time%20series%20is%20a%20persistent%20challenge%20in%20domains%20like%20healthcare%20and%20sensor%20networks.%20While%20recent%20works%20have%20explored%20a%20variety%20of%20complex%20learning%20architectures%20to%20solve%20the%20prediction%20problems%20for%20irregularly%20sampled%20time%20series%2C%20it%20remains%20unclear%20what%20the%20true%20benefits%20of%20some%20of%20these%20architectures%20are%2C%20and%20whether%20clever%20modifications%20of%20simpler%20and%20more%20efficient%20RNN-based%20algorithms%20are%20still%20competitive%2C%20i.e.%20they%20are%20on%20par%20with%20or%20even%20superior%20to%20these%20methods.%20In%20this%20work%2C%20we%20propose%20and%20study%20GRUwE%3A%20Gated%20Recurrent%20Unit%20with%20Exponential%20basis%20functions%2C%20that%20builds%20upon%20RNN-based%20architectures%20for%20observations%20made%20at%20irregular%20times.%20GRUwE%20supports%20both%20regression-based%20and%20event-based%20predictions%20in%20continuous%20time.%20GRUwE%20works%20by%20maintaining%20a%20Markov%20state%20representation%20of%20the%20time%20series%20that%20updates%20with%20the%20arrival%20of%20irregular%20observations.%20The%20Markov%20state%20update%20relies%20on%20two%20reset%20mechanisms%3A%20%28i%29%20observation-triggered%20reset%20to%20account%20for%20the%20new%20observation%2C%20and%20%28ii%29%20time-triggered%20reset%20that%20relies%20on%20learnable%20exponential%20decays%2C%20to%20support%20the%20predictions%20in%20continuous%20time.%20Our%20empirical%20evaluations%20across%20several%20real-world%20benchmarks%20on%20next-observation%20and%20next-event%20prediction%20tasks%20demonstrate%20that%20GRUwE%20can%20indeed%20achieve%20competitive%20or%20superior%20performance%20compared%20to%20the%20recent%20state-of-the-art%20%28SOTA%29%20methods.%20Thanks%20to%20its%20simplicity%2C%20GRUwE%20offers%20compelling%20advantages%3A%20it%20is%20easy%20to%20implement%2C%20requires%20minimal%20hyper-parameter%20tuning%20efforts%2C%20and%20significantly%20reduces%20the%20computational%20overhead%20in%20the%20online%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16161v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStill%2520Competitive%253A%2520Revisiting%2520Recurrent%2520Models%2520for%2520Irregular%2520Time%2520Series%2520Prediction%26entry.906535625%3DAnkitkumar%2520Joshi%2520and%2520Milos%2520Hauskrecht%26entry.1292438233%3DModeling%2520irregularly%2520sampled%2520multivariate%2520time%2520series%2520is%2520a%2520persistent%2520challenge%2520in%2520domains%2520like%2520healthcare%2520and%2520sensor%2520networks.%2520While%2520recent%2520works%2520have%2520explored%2520a%2520variety%2520of%2520complex%2520learning%2520architectures%2520to%2520solve%2520the%2520prediction%2520problems%2520for%2520irregularly%2520sampled%2520time%2520series%252C%2520it%2520remains%2520unclear%2520what%2520the%2520true%2520benefits%2520of%2520some%2520of%2520these%2520architectures%2520are%252C%2520and%2520whether%2520clever%2520modifications%2520of%2520simpler%2520and%2520more%2520efficient%2520RNN-based%2520algorithms%2520are%2520still%2520competitive%252C%2520i.e.%2520they%2520are%2520on%2520par%2520with%2520or%2520even%2520superior%2520to%2520these%2520methods.%2520In%2520this%2520work%252C%2520we%2520propose%2520and%2520study%2520GRUwE%253A%2520Gated%2520Recurrent%2520Unit%2520with%2520Exponential%2520basis%2520functions%252C%2520that%2520builds%2520upon%2520RNN-based%2520architectures%2520for%2520observations%2520made%2520at%2520irregular%2520times.%2520GRUwE%2520supports%2520both%2520regression-based%2520and%2520event-based%2520predictions%2520in%2520continuous%2520time.%2520GRUwE%2520works%2520by%2520maintaining%2520a%2520Markov%2520state%2520representation%2520of%2520the%2520time%2520series%2520that%2520updates%2520with%2520the%2520arrival%2520of%2520irregular%2520observations.%2520The%2520Markov%2520state%2520update%2520relies%2520on%2520two%2520reset%2520mechanisms%253A%2520%2528i%2529%2520observation-triggered%2520reset%2520to%2520account%2520for%2520the%2520new%2520observation%252C%2520and%2520%2528ii%2529%2520time-triggered%2520reset%2520that%2520relies%2520on%2520learnable%2520exponential%2520decays%252C%2520to%2520support%2520the%2520predictions%2520in%2520continuous%2520time.%2520Our%2520empirical%2520evaluations%2520across%2520several%2520real-world%2520benchmarks%2520on%2520next-observation%2520and%2520next-event%2520prediction%2520tasks%2520demonstrate%2520that%2520GRUwE%2520can%2520indeed%2520achieve%2520competitive%2520or%2520superior%2520performance%2520compared%2520to%2520the%2520recent%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%2520Thanks%2520to%2520its%2520simplicity%252C%2520GRUwE%2520offers%2520compelling%2520advantages%253A%2520it%2520is%2520easy%2520to%2520implement%252C%2520requires%2520minimal%2520hyper-parameter%2520tuning%2520efforts%252C%2520and%2520significantly%2520reduces%2520the%2520computational%2520overhead%2520in%2520the%2520online%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16161v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Still%20Competitive%3A%20Revisiting%20Recurrent%20Models%20for%20Irregular%20Time%20Series%20Prediction&entry.906535625=Ankitkumar%20Joshi%20and%20Milos%20Hauskrecht&entry.1292438233=Modeling%20irregularly%20sampled%20multivariate%20time%20series%20is%20a%20persistent%20challenge%20in%20domains%20like%20healthcare%20and%20sensor%20networks.%20While%20recent%20works%20have%20explored%20a%20variety%20of%20complex%20learning%20architectures%20to%20solve%20the%20prediction%20problems%20for%20irregularly%20sampled%20time%20series%2C%20it%20remains%20unclear%20what%20the%20true%20benefits%20of%20some%20of%20these%20architectures%20are%2C%20and%20whether%20clever%20modifications%20of%20simpler%20and%20more%20efficient%20RNN-based%20algorithms%20are%20still%20competitive%2C%20i.e.%20they%20are%20on%20par%20with%20or%20even%20superior%20to%20these%20methods.%20In%20this%20work%2C%20we%20propose%20and%20study%20GRUwE%3A%20Gated%20Recurrent%20Unit%20with%20Exponential%20basis%20functions%2C%20that%20builds%20upon%20RNN-based%20architectures%20for%20observations%20made%20at%20irregular%20times.%20GRUwE%20supports%20both%20regression-based%20and%20event-based%20predictions%20in%20continuous%20time.%20GRUwE%20works%20by%20maintaining%20a%20Markov%20state%20representation%20of%20the%20time%20series%20that%20updates%20with%20the%20arrival%20of%20irregular%20observations.%20The%20Markov%20state%20update%20relies%20on%20two%20reset%20mechanisms%3A%20%28i%29%20observation-triggered%20reset%20to%20account%20for%20the%20new%20observation%2C%20and%20%28ii%29%20time-triggered%20reset%20that%20relies%20on%20learnable%20exponential%20decays%2C%20to%20support%20the%20predictions%20in%20continuous%20time.%20Our%20empirical%20evaluations%20across%20several%20real-world%20benchmarks%20on%20next-observation%20and%20next-event%20prediction%20tasks%20demonstrate%20that%20GRUwE%20can%20indeed%20achieve%20competitive%20or%20superior%20performance%20compared%20to%20the%20recent%20state-of-the-art%20%28SOTA%29%20methods.%20Thanks%20to%20its%20simplicity%2C%20GRUwE%20offers%20compelling%20advantages%3A%20it%20is%20easy%20to%20implement%2C%20requires%20minimal%20hyper-parameter%20tuning%20efforts%2C%20and%20significantly%20reduces%20the%20computational%20overhead%20in%20the%20online%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2510.16161v2&entry.124074799=Read"},
{"title": "Articulated 3D Scene Graphs for Open-World Mobile Manipulation", "author": "Martin B\u00fcchner and Adrian R\u00f6fer and Tim Engelbracht and Tim Welschehold and Zuria Bauer and Hermann Blum and Marc Pollefeys and Abhinav Valada", "abstract": "Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de.", "link": "http://arxiv.org/abs/2602.16356v1", "date": "2026-02-18", "relevancy": 1.9121, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.701}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6241}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Articulated%203D%20Scene%20Graphs%20for%20Open-World%20Mobile%20Manipulation&body=Title%3A%20Articulated%203D%20Scene%20Graphs%20for%20Open-World%20Mobile%20Manipulation%0AAuthor%3A%20Martin%20B%C3%BCchner%20and%20Adrian%20R%C3%B6fer%20and%20Tim%20Engelbracht%20and%20Tim%20Welschehold%20and%20Zuria%20Bauer%20and%20Hermann%20Blum%20and%20Marc%20Pollefeys%20and%20Abhinav%20Valada%0AAbstract%3A%20Semantics%20has%20enabled%203D%20scene%20understanding%20and%20affordance-driven%20object%20interaction.%20However%2C%20robots%20operating%20in%20real-world%20environments%20face%20a%20critical%20limitation%3A%20they%20cannot%20anticipate%20how%20objects%20move.%20Long-horizon%20mobile%20manipulation%20requires%20closing%20the%20gap%20between%20semantics%2C%20geometry%2C%20and%20kinematics.%20In%20this%20work%2C%20we%20present%20MoMa-SG%2C%20a%20novel%20framework%20for%20building%20semantic-kinematic%203D%20scene%20graphs%20of%20articulated%20scenes%20containing%20a%20myriad%20of%20interactable%20objects.%20Given%20RGB-D%20sequences%20containing%20multiple%20object%20articulations%2C%20we%20temporally%20segment%20object%20interactions%20and%20infer%20object%20motion%20using%20occlusion-robust%20point%20tracking.%20We%20then%20lift%20point%20trajectories%20into%203D%20and%20estimate%20articulation%20models%20using%20a%20novel%20unified%20twist%20estimation%20formulation%20that%20robustly%20estimates%20revolute%20and%20prismatic%20joint%20parameters%20in%20a%20single%20optimization%20pass.%20Next%2C%20we%20associate%20objects%20with%20estimated%20articulations%20and%20detect%20contained%20objects%20by%20reasoning%20over%20parent-child%20relations%20at%20identified%20opening%20states.%20We%20also%20introduce%20the%20novel%20Arti4D-Semantic%20dataset%2C%20which%20uniquely%20combines%20hierarchical%20object%20semantics%20including%20parent-child%20relation%20labels%20with%20object%20axis%20annotations%20across%2062%20in-the-wild%20RGB-D%20sequences%20containing%20600%20object%20interactions%20and%20three%20distinct%20observation%20paradigms.%20We%20extensively%20evaluate%20the%20performance%20of%20MoMa-SG%20on%20two%20datasets%20and%20ablate%20key%20design%20choices%20of%20our%20approach.%20In%20addition%2C%20real-world%20experiments%20on%20both%20a%20quadruped%20and%20a%20mobile%20manipulator%20demonstrate%20that%20our%20semantic-kinematic%20scene%20graphs%20enable%20robust%20manipulation%20of%20articulated%20objects%20in%20everyday%20home%20environments.%20We%20provide%20code%20and%20data%20at%3A%20https%3A//momasg.cs.uni-freiburg.de.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArticulated%25203D%2520Scene%2520Graphs%2520for%2520Open-World%2520Mobile%2520Manipulation%26entry.906535625%3DMartin%2520B%25C3%25BCchner%2520and%2520Adrian%2520R%25C3%25B6fer%2520and%2520Tim%2520Engelbracht%2520and%2520Tim%2520Welschehold%2520and%2520Zuria%2520Bauer%2520and%2520Hermann%2520Blum%2520and%2520Marc%2520Pollefeys%2520and%2520Abhinav%2520Valada%26entry.1292438233%3DSemantics%2520has%2520enabled%25203D%2520scene%2520understanding%2520and%2520affordance-driven%2520object%2520interaction.%2520However%252C%2520robots%2520operating%2520in%2520real-world%2520environments%2520face%2520a%2520critical%2520limitation%253A%2520they%2520cannot%2520anticipate%2520how%2520objects%2520move.%2520Long-horizon%2520mobile%2520manipulation%2520requires%2520closing%2520the%2520gap%2520between%2520semantics%252C%2520geometry%252C%2520and%2520kinematics.%2520In%2520this%2520work%252C%2520we%2520present%2520MoMa-SG%252C%2520a%2520novel%2520framework%2520for%2520building%2520semantic-kinematic%25203D%2520scene%2520graphs%2520of%2520articulated%2520scenes%2520containing%2520a%2520myriad%2520of%2520interactable%2520objects.%2520Given%2520RGB-D%2520sequences%2520containing%2520multiple%2520object%2520articulations%252C%2520we%2520temporally%2520segment%2520object%2520interactions%2520and%2520infer%2520object%2520motion%2520using%2520occlusion-robust%2520point%2520tracking.%2520We%2520then%2520lift%2520point%2520trajectories%2520into%25203D%2520and%2520estimate%2520articulation%2520models%2520using%2520a%2520novel%2520unified%2520twist%2520estimation%2520formulation%2520that%2520robustly%2520estimates%2520revolute%2520and%2520prismatic%2520joint%2520parameters%2520in%2520a%2520single%2520optimization%2520pass.%2520Next%252C%2520we%2520associate%2520objects%2520with%2520estimated%2520articulations%2520and%2520detect%2520contained%2520objects%2520by%2520reasoning%2520over%2520parent-child%2520relations%2520at%2520identified%2520opening%2520states.%2520We%2520also%2520introduce%2520the%2520novel%2520Arti4D-Semantic%2520dataset%252C%2520which%2520uniquely%2520combines%2520hierarchical%2520object%2520semantics%2520including%2520parent-child%2520relation%2520labels%2520with%2520object%2520axis%2520annotations%2520across%252062%2520in-the-wild%2520RGB-D%2520sequences%2520containing%2520600%2520object%2520interactions%2520and%2520three%2520distinct%2520observation%2520paradigms.%2520We%2520extensively%2520evaluate%2520the%2520performance%2520of%2520MoMa-SG%2520on%2520two%2520datasets%2520and%2520ablate%2520key%2520design%2520choices%2520of%2520our%2520approach.%2520In%2520addition%252C%2520real-world%2520experiments%2520on%2520both%2520a%2520quadruped%2520and%2520a%2520mobile%2520manipulator%2520demonstrate%2520that%2520our%2520semantic-kinematic%2520scene%2520graphs%2520enable%2520robust%2520manipulation%2520of%2520articulated%2520objects%2520in%2520everyday%2520home%2520environments.%2520We%2520provide%2520code%2520and%2520data%2520at%253A%2520https%253A//momasg.cs.uni-freiburg.de.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Articulated%203D%20Scene%20Graphs%20for%20Open-World%20Mobile%20Manipulation&entry.906535625=Martin%20B%C3%BCchner%20and%20Adrian%20R%C3%B6fer%20and%20Tim%20Engelbracht%20and%20Tim%20Welschehold%20and%20Zuria%20Bauer%20and%20Hermann%20Blum%20and%20Marc%20Pollefeys%20and%20Abhinav%20Valada&entry.1292438233=Semantics%20has%20enabled%203D%20scene%20understanding%20and%20affordance-driven%20object%20interaction.%20However%2C%20robots%20operating%20in%20real-world%20environments%20face%20a%20critical%20limitation%3A%20they%20cannot%20anticipate%20how%20objects%20move.%20Long-horizon%20mobile%20manipulation%20requires%20closing%20the%20gap%20between%20semantics%2C%20geometry%2C%20and%20kinematics.%20In%20this%20work%2C%20we%20present%20MoMa-SG%2C%20a%20novel%20framework%20for%20building%20semantic-kinematic%203D%20scene%20graphs%20of%20articulated%20scenes%20containing%20a%20myriad%20of%20interactable%20objects.%20Given%20RGB-D%20sequences%20containing%20multiple%20object%20articulations%2C%20we%20temporally%20segment%20object%20interactions%20and%20infer%20object%20motion%20using%20occlusion-robust%20point%20tracking.%20We%20then%20lift%20point%20trajectories%20into%203D%20and%20estimate%20articulation%20models%20using%20a%20novel%20unified%20twist%20estimation%20formulation%20that%20robustly%20estimates%20revolute%20and%20prismatic%20joint%20parameters%20in%20a%20single%20optimization%20pass.%20Next%2C%20we%20associate%20objects%20with%20estimated%20articulations%20and%20detect%20contained%20objects%20by%20reasoning%20over%20parent-child%20relations%20at%20identified%20opening%20states.%20We%20also%20introduce%20the%20novel%20Arti4D-Semantic%20dataset%2C%20which%20uniquely%20combines%20hierarchical%20object%20semantics%20including%20parent-child%20relation%20labels%20with%20object%20axis%20annotations%20across%2062%20in-the-wild%20RGB-D%20sequences%20containing%20600%20object%20interactions%20and%20three%20distinct%20observation%20paradigms.%20We%20extensively%20evaluate%20the%20performance%20of%20MoMa-SG%20on%20two%20datasets%20and%20ablate%20key%20design%20choices%20of%20our%20approach.%20In%20addition%2C%20real-world%20experiments%20on%20both%20a%20quadruped%20and%20a%20mobile%20manipulator%20demonstrate%20that%20our%20semantic-kinematic%20scene%20graphs%20enable%20robust%20manipulation%20of%20articulated%20objects%20in%20everyday%20home%20environments.%20We%20provide%20code%20and%20data%20at%3A%20https%3A//momasg.cs.uni-freiburg.de.&entry.1838667208=http%3A//arxiv.org/abs/2602.16356v1&entry.124074799=Read"},
{"title": "Random Scaling of Emergent Capabilities", "author": "Rosie Zhao and Tian Qin and David Alvarez-Melis and Sham Kakade and Naomi Saphra", "abstract": "Language models famously improve under a smooth scaling law, but some specific capabilities exhibit sudden breakthroughs in performance. Advocates of \"emergence\" view these capabilities as unlocked at a specific scale, but others attribute breakthroughs to superficial metric thresholding effects. We propose that breakthroughs are instead driven by continuous changes in the probability distribution of training outcomes when performance is bimodally distributed across random seeds. we show that different random seeds can produce either smooth or emergent scaling trends in synthetic length generalization tasks, multiple choice question answering, and grammatical generalization. We reveal that sharp breakthroughs in metrics are produced by underlying continuous changes in their distribution across seeds. These distributions may become abruptly bimodal at a capacity threshold but this threshold appears at scales well before most seeds achieve breakthrough. Our observations hold true even under continuous loss metrics, confirming that random variation must be considered when predicting a model's performance from its scale.", "link": "http://arxiv.org/abs/2502.17356v5", "date": "2026-02-18", "relevancy": 1.4467, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5159}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.477}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Scaling%20of%20Emergent%20Capabilities&body=Title%3A%20Random%20Scaling%20of%20Emergent%20Capabilities%0AAuthor%3A%20Rosie%20Zhao%20and%20Tian%20Qin%20and%20David%20Alvarez-Melis%20and%20Sham%20Kakade%20and%20Naomi%20Saphra%0AAbstract%3A%20Language%20models%20famously%20improve%20under%20a%20smooth%20scaling%20law%2C%20but%20some%20specific%20capabilities%20exhibit%20sudden%20breakthroughs%20in%20performance.%20Advocates%20of%20%22emergence%22%20view%20these%20capabilities%20as%20unlocked%20at%20a%20specific%20scale%2C%20but%20others%20attribute%20breakthroughs%20to%20superficial%20metric%20thresholding%20effects.%20We%20propose%20that%20breakthroughs%20are%20instead%20driven%20by%20continuous%20changes%20in%20the%20probability%20distribution%20of%20training%20outcomes%20when%20performance%20is%20bimodally%20distributed%20across%20random%20seeds.%20we%20show%20that%20different%20random%20seeds%20can%20produce%20either%20smooth%20or%20emergent%20scaling%20trends%20in%20synthetic%20length%20generalization%20tasks%2C%20multiple%20choice%20question%20answering%2C%20and%20grammatical%20generalization.%20We%20reveal%20that%20sharp%20breakthroughs%20in%20metrics%20are%20produced%20by%20underlying%20continuous%20changes%20in%20their%20distribution%20across%20seeds.%20These%20distributions%20may%20become%20abruptly%20bimodal%20at%20a%20capacity%20threshold%20but%20this%20threshold%20appears%20at%20scales%20well%20before%20most%20seeds%20achieve%20breakthrough.%20Our%20observations%20hold%20true%20even%20under%20continuous%20loss%20metrics%2C%20confirming%20that%20random%20variation%20must%20be%20considered%20when%20predicting%20a%20model%27s%20performance%20from%20its%20scale.%0ALink%3A%20http%3A//arxiv.org/abs/2502.17356v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Scaling%2520of%2520Emergent%2520Capabilities%26entry.906535625%3DRosie%2520Zhao%2520and%2520Tian%2520Qin%2520and%2520David%2520Alvarez-Melis%2520and%2520Sham%2520Kakade%2520and%2520Naomi%2520Saphra%26entry.1292438233%3DLanguage%2520models%2520famously%2520improve%2520under%2520a%2520smooth%2520scaling%2520law%252C%2520but%2520some%2520specific%2520capabilities%2520exhibit%2520sudden%2520breakthroughs%2520in%2520performance.%2520Advocates%2520of%2520%2522emergence%2522%2520view%2520these%2520capabilities%2520as%2520unlocked%2520at%2520a%2520specific%2520scale%252C%2520but%2520others%2520attribute%2520breakthroughs%2520to%2520superficial%2520metric%2520thresholding%2520effects.%2520We%2520propose%2520that%2520breakthroughs%2520are%2520instead%2520driven%2520by%2520continuous%2520changes%2520in%2520the%2520probability%2520distribution%2520of%2520training%2520outcomes%2520when%2520performance%2520is%2520bimodally%2520distributed%2520across%2520random%2520seeds.%2520we%2520show%2520that%2520different%2520random%2520seeds%2520can%2520produce%2520either%2520smooth%2520or%2520emergent%2520scaling%2520trends%2520in%2520synthetic%2520length%2520generalization%2520tasks%252C%2520multiple%2520choice%2520question%2520answering%252C%2520and%2520grammatical%2520generalization.%2520We%2520reveal%2520that%2520sharp%2520breakthroughs%2520in%2520metrics%2520are%2520produced%2520by%2520underlying%2520continuous%2520changes%2520in%2520their%2520distribution%2520across%2520seeds.%2520These%2520distributions%2520may%2520become%2520abruptly%2520bimodal%2520at%2520a%2520capacity%2520threshold%2520but%2520this%2520threshold%2520appears%2520at%2520scales%2520well%2520before%2520most%2520seeds%2520achieve%2520breakthrough.%2520Our%2520observations%2520hold%2520true%2520even%2520under%2520continuous%2520loss%2520metrics%252C%2520confirming%2520that%2520random%2520variation%2520must%2520be%2520considered%2520when%2520predicting%2520a%2520model%2527s%2520performance%2520from%2520its%2520scale.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17356v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Scaling%20of%20Emergent%20Capabilities&entry.906535625=Rosie%20Zhao%20and%20Tian%20Qin%20and%20David%20Alvarez-Melis%20and%20Sham%20Kakade%20and%20Naomi%20Saphra&entry.1292438233=Language%20models%20famously%20improve%20under%20a%20smooth%20scaling%20law%2C%20but%20some%20specific%20capabilities%20exhibit%20sudden%20breakthroughs%20in%20performance.%20Advocates%20of%20%22emergence%22%20view%20these%20capabilities%20as%20unlocked%20at%20a%20specific%20scale%2C%20but%20others%20attribute%20breakthroughs%20to%20superficial%20metric%20thresholding%20effects.%20We%20propose%20that%20breakthroughs%20are%20instead%20driven%20by%20continuous%20changes%20in%20the%20probability%20distribution%20of%20training%20outcomes%20when%20performance%20is%20bimodally%20distributed%20across%20random%20seeds.%20we%20show%20that%20different%20random%20seeds%20can%20produce%20either%20smooth%20or%20emergent%20scaling%20trends%20in%20synthetic%20length%20generalization%20tasks%2C%20multiple%20choice%20question%20answering%2C%20and%20grammatical%20generalization.%20We%20reveal%20that%20sharp%20breakthroughs%20in%20metrics%20are%20produced%20by%20underlying%20continuous%20changes%20in%20their%20distribution%20across%20seeds.%20These%20distributions%20may%20become%20abruptly%20bimodal%20at%20a%20capacity%20threshold%20but%20this%20threshold%20appears%20at%20scales%20well%20before%20most%20seeds%20achieve%20breakthrough.%20Our%20observations%20hold%20true%20even%20under%20continuous%20loss%20metrics%2C%20confirming%20that%20random%20variation%20must%20be%20considered%20when%20predicting%20a%20model%27s%20performance%20from%20its%20scale.&entry.1838667208=http%3A//arxiv.org/abs/2502.17356v5&entry.124074799=Read"},
{"title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs", "author": "Felix Fricke and Simon Malberg and Georg Groh", "abstract": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.", "link": "http://arxiv.org/abs/2602.16512v1", "date": "2026-02-18", "relevancy": 1.8649, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Framework%20of%20Thoughts%3A%20A%20Foundation%20Framework%20for%20Dynamic%20and%20Optimized%20Reasoning%20based%20on%20Chains%2C%20Trees%2C%20and%20Graphs&body=Title%3A%20Framework%20of%20Thoughts%3A%20A%20Foundation%20Framework%20for%20Dynamic%20and%20Optimized%20Reasoning%20based%20on%20Chains%2C%20Trees%2C%20and%20Graphs%0AAuthor%3A%20Felix%20Fricke%20and%20Simon%20Malberg%20and%20Georg%20Groh%0AAbstract%3A%20Prompting%20schemes%20such%20as%20Chain%20of%20Thought%2C%20Tree%20of%20Thoughts%2C%20and%20Graph%20of%20Thoughts%20can%20significantly%20enhance%20the%20reasoning%20capabilities%20of%20large%20language%20models.%20However%2C%20most%20existing%20schemes%20require%20users%20to%20define%20static%2C%20problem-specific%20reasoning%20structures%20that%20lack%20adaptability%20to%20dynamic%20or%20unseen%20problem%20types.%20Additionally%2C%20these%20schemes%20are%20often%20under-optimized%20in%20terms%20of%20hyperparameters%2C%20prompts%2C%20runtime%2C%20and%20prompting%20cost.%20To%20address%20these%20limitations%2C%20we%20introduce%20Framework%20of%20Thoughts%20%28FoT%29--a%20general-purpose%20foundation%20framework%20for%20building%20and%20optimizing%20dynamic%20reasoning%20schemes.%20FoT%20comes%20with%20built-in%20features%20for%20hyperparameter%20tuning%2C%20prompt%20optimization%2C%20parallel%20execution%2C%20and%20intelligent%20caching%2C%20unlocking%20the%20latent%20performance%20potential%20of%20reasoning%20schemes.%20We%20demonstrate%20FoT%27s%20capabilities%20by%20implementing%20three%20popular%20schemes--Tree%20of%20Thoughts%2C%20Graph%20of%20Thoughts%2C%20and%20ProbTree--within%20FoT.%20We%20empirically%20show%20that%20FoT%20enables%20significantly%20faster%20execution%2C%20reduces%20costs%2C%20and%20achieves%20better%20task%20scores%20through%20optimization.%20We%20release%20our%20codebase%20to%20facilitate%20the%20development%20of%20future%20dynamic%20and%20efficient%20reasoning%20schemes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFramework%2520of%2520Thoughts%253A%2520A%2520Foundation%2520Framework%2520for%2520Dynamic%2520and%2520Optimized%2520Reasoning%2520based%2520on%2520Chains%252C%2520Trees%252C%2520and%2520Graphs%26entry.906535625%3DFelix%2520Fricke%2520and%2520Simon%2520Malberg%2520and%2520Georg%2520Groh%26entry.1292438233%3DPrompting%2520schemes%2520such%2520as%2520Chain%2520of%2520Thought%252C%2520Tree%2520of%2520Thoughts%252C%2520and%2520Graph%2520of%2520Thoughts%2520can%2520significantly%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models.%2520However%252C%2520most%2520existing%2520schemes%2520require%2520users%2520to%2520define%2520static%252C%2520problem-specific%2520reasoning%2520structures%2520that%2520lack%2520adaptability%2520to%2520dynamic%2520or%2520unseen%2520problem%2520types.%2520Additionally%252C%2520these%2520schemes%2520are%2520often%2520under-optimized%2520in%2520terms%2520of%2520hyperparameters%252C%2520prompts%252C%2520runtime%252C%2520and%2520prompting%2520cost.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Framework%2520of%2520Thoughts%2520%2528FoT%2529--a%2520general-purpose%2520foundation%2520framework%2520for%2520building%2520and%2520optimizing%2520dynamic%2520reasoning%2520schemes.%2520FoT%2520comes%2520with%2520built-in%2520features%2520for%2520hyperparameter%2520tuning%252C%2520prompt%2520optimization%252C%2520parallel%2520execution%252C%2520and%2520intelligent%2520caching%252C%2520unlocking%2520the%2520latent%2520performance%2520potential%2520of%2520reasoning%2520schemes.%2520We%2520demonstrate%2520FoT%2527s%2520capabilities%2520by%2520implementing%2520three%2520popular%2520schemes--Tree%2520of%2520Thoughts%252C%2520Graph%2520of%2520Thoughts%252C%2520and%2520ProbTree--within%2520FoT.%2520We%2520empirically%2520show%2520that%2520FoT%2520enables%2520significantly%2520faster%2520execution%252C%2520reduces%2520costs%252C%2520and%2520achieves%2520better%2520task%2520scores%2520through%2520optimization.%2520We%2520release%2520our%2520codebase%2520to%2520facilitate%2520the%2520development%2520of%2520future%2520dynamic%2520and%2520efficient%2520reasoning%2520schemes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Framework%20of%20Thoughts%3A%20A%20Foundation%20Framework%20for%20Dynamic%20and%20Optimized%20Reasoning%20based%20on%20Chains%2C%20Trees%2C%20and%20Graphs&entry.906535625=Felix%20Fricke%20and%20Simon%20Malberg%20and%20Georg%20Groh&entry.1292438233=Prompting%20schemes%20such%20as%20Chain%20of%20Thought%2C%20Tree%20of%20Thoughts%2C%20and%20Graph%20of%20Thoughts%20can%20significantly%20enhance%20the%20reasoning%20capabilities%20of%20large%20language%20models.%20However%2C%20most%20existing%20schemes%20require%20users%20to%20define%20static%2C%20problem-specific%20reasoning%20structures%20that%20lack%20adaptability%20to%20dynamic%20or%20unseen%20problem%20types.%20Additionally%2C%20these%20schemes%20are%20often%20under-optimized%20in%20terms%20of%20hyperparameters%2C%20prompts%2C%20runtime%2C%20and%20prompting%20cost.%20To%20address%20these%20limitations%2C%20we%20introduce%20Framework%20of%20Thoughts%20%28FoT%29--a%20general-purpose%20foundation%20framework%20for%20building%20and%20optimizing%20dynamic%20reasoning%20schemes.%20FoT%20comes%20with%20built-in%20features%20for%20hyperparameter%20tuning%2C%20prompt%20optimization%2C%20parallel%20execution%2C%20and%20intelligent%20caching%2C%20unlocking%20the%20latent%20performance%20potential%20of%20reasoning%20schemes.%20We%20demonstrate%20FoT%27s%20capabilities%20by%20implementing%20three%20popular%20schemes--Tree%20of%20Thoughts%2C%20Graph%20of%20Thoughts%2C%20and%20ProbTree--within%20FoT.%20We%20empirically%20show%20that%20FoT%20enables%20significantly%20faster%20execution%2C%20reduces%20costs%2C%20and%20achieves%20better%20task%20scores%20through%20optimization.%20We%20release%20our%20codebase%20to%20facilitate%20the%20development%20of%20future%20dynamic%20and%20efficient%20reasoning%20schemes.&entry.1838667208=http%3A//arxiv.org/abs/2602.16512v1&entry.124074799=Read"},
{"title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents", "author": "Zeping Li and Hongru Wang and Yiwen Zhao and Guanhua Chen and Yixia Li and Keyang Chen and Yixin Cao and Guangnan Ye and Hongfeng Chai and Zhenfei Yin", "abstract": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.", "link": "http://arxiv.org/abs/2602.02050v2", "date": "2026-02-18", "relevancy": 1.4626, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4925}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4852}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Role%20of%20Entropy%20in%20Optimizing%20Tool-Use%20Behaviors%20for%20Large%20Language%20Model%20Agents&body=Title%3A%20Rethinking%20the%20Role%20of%20Entropy%20in%20Optimizing%20Tool-Use%20Behaviors%20for%20Large%20Language%20Model%20Agents%0AAuthor%3A%20Zeping%20Li%20and%20Hongru%20Wang%20and%20Yiwen%20Zhao%20and%20Guanhua%20Chen%20and%20Yixia%20Li%20and%20Keyang%20Chen%20and%20Yixin%20Cao%20and%20Guangnan%20Ye%20and%20Hongfeng%20Chai%20and%20Zhenfei%20Yin%0AAbstract%3A%20Tool-using%20agents%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20tasks%20such%20as%20mathematical%20reasoning%20and%20multi-hop%20question%20answering.%20However%2C%20in%20long%20trajectories%2C%20agents%20often%20trigger%20excessive%20and%20low-quality%20tool%20calls%2C%20increasing%20latency%20and%20degrading%20inference%20performance%2C%20making%20managing%20tool-use%20behavior%20challenging.%20In%20this%20work%2C%20we%20conduct%20entropy-based%20pilot%20experiments%20and%20observe%20a%20strong%20positive%20correlation%20between%20entropy%20reduction%20and%20high-quality%20tool%20calls.%20Building%20on%20this%20finding%2C%20we%20propose%20using%20entropy%20reduction%20as%20a%20supervisory%20signal%20and%20design%20two%20reward%20strategies%20to%20address%20the%20differing%20needs%20of%20optimizing%20tool-use%20behavior.%20Sparse%20outcome%20rewards%20provide%20coarse%2C%20trajectory-level%20guidance%20to%20improve%20efficiency%2C%20while%20dense%20process%20rewards%20offer%20fine-grained%20supervision%20to%20enhance%20performance.%20Experiments%20across%20diverse%20domains%20show%20that%20both%20reward%20designs%20improve%20tool-use%20behavior%3A%20the%20former%20reduces%20tool%20calls%20by%2072.07%25%20compared%20to%20the%20average%20of%20baselines%2C%20while%20the%20latter%20improves%20performance%20by%2022.27%25.%20These%20results%20position%20entropy%20reduction%20as%20a%20key%20mechanism%20for%20enhancing%20tool-use%20behavior%2C%20enabling%20agents%20to%20be%20more%20adaptive%20in%20real-world%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.02050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Role%2520of%2520Entropy%2520in%2520Optimizing%2520Tool-Use%2520Behaviors%2520for%2520Large%2520Language%2520Model%2520Agents%26entry.906535625%3DZeping%2520Li%2520and%2520Hongru%2520Wang%2520and%2520Yiwen%2520Zhao%2520and%2520Guanhua%2520Chen%2520and%2520Yixia%2520Li%2520and%2520Keyang%2520Chen%2520and%2520Yixin%2520Cao%2520and%2520Guangnan%2520Ye%2520and%2520Hongfeng%2520Chai%2520and%2520Zhenfei%2520Yin%26entry.1292438233%3DTool-using%2520agents%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520tasks%2520such%2520as%2520mathematical%2520reasoning%2520and%2520multi-hop%2520question%2520answering.%2520However%252C%2520in%2520long%2520trajectories%252C%2520agents%2520often%2520trigger%2520excessive%2520and%2520low-quality%2520tool%2520calls%252C%2520increasing%2520latency%2520and%2520degrading%2520inference%2520performance%252C%2520making%2520managing%2520tool-use%2520behavior%2520challenging.%2520In%2520this%2520work%252C%2520we%2520conduct%2520entropy-based%2520pilot%2520experiments%2520and%2520observe%2520a%2520strong%2520positive%2520correlation%2520between%2520entropy%2520reduction%2520and%2520high-quality%2520tool%2520calls.%2520Building%2520on%2520this%2520finding%252C%2520we%2520propose%2520using%2520entropy%2520reduction%2520as%2520a%2520supervisory%2520signal%2520and%2520design%2520two%2520reward%2520strategies%2520to%2520address%2520the%2520differing%2520needs%2520of%2520optimizing%2520tool-use%2520behavior.%2520Sparse%2520outcome%2520rewards%2520provide%2520coarse%252C%2520trajectory-level%2520guidance%2520to%2520improve%2520efficiency%252C%2520while%2520dense%2520process%2520rewards%2520offer%2520fine-grained%2520supervision%2520to%2520enhance%2520performance.%2520Experiments%2520across%2520diverse%2520domains%2520show%2520that%2520both%2520reward%2520designs%2520improve%2520tool-use%2520behavior%253A%2520the%2520former%2520reduces%2520tool%2520calls%2520by%252072.07%2525%2520compared%2520to%2520the%2520average%2520of%2520baselines%252C%2520while%2520the%2520latter%2520improves%2520performance%2520by%252022.27%2525.%2520These%2520results%2520position%2520entropy%2520reduction%2520as%2520a%2520key%2520mechanism%2520for%2520enhancing%2520tool-use%2520behavior%252C%2520enabling%2520agents%2520to%2520be%2520more%2520adaptive%2520in%2520real-world%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.02050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Role%20of%20Entropy%20in%20Optimizing%20Tool-Use%20Behaviors%20for%20Large%20Language%20Model%20Agents&entry.906535625=Zeping%20Li%20and%20Hongru%20Wang%20and%20Yiwen%20Zhao%20and%20Guanhua%20Chen%20and%20Yixia%20Li%20and%20Keyang%20Chen%20and%20Yixin%20Cao%20and%20Guangnan%20Ye%20and%20Hongfeng%20Chai%20and%20Zhenfei%20Yin&entry.1292438233=Tool-using%20agents%20based%20on%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20tasks%20such%20as%20mathematical%20reasoning%20and%20multi-hop%20question%20answering.%20However%2C%20in%20long%20trajectories%2C%20agents%20often%20trigger%20excessive%20and%20low-quality%20tool%20calls%2C%20increasing%20latency%20and%20degrading%20inference%20performance%2C%20making%20managing%20tool-use%20behavior%20challenging.%20In%20this%20work%2C%20we%20conduct%20entropy-based%20pilot%20experiments%20and%20observe%20a%20strong%20positive%20correlation%20between%20entropy%20reduction%20and%20high-quality%20tool%20calls.%20Building%20on%20this%20finding%2C%20we%20propose%20using%20entropy%20reduction%20as%20a%20supervisory%20signal%20and%20design%20two%20reward%20strategies%20to%20address%20the%20differing%20needs%20of%20optimizing%20tool-use%20behavior.%20Sparse%20outcome%20rewards%20provide%20coarse%2C%20trajectory-level%20guidance%20to%20improve%20efficiency%2C%20while%20dense%20process%20rewards%20offer%20fine-grained%20supervision%20to%20enhance%20performance.%20Experiments%20across%20diverse%20domains%20show%20that%20both%20reward%20designs%20improve%20tool-use%20behavior%3A%20the%20former%20reduces%20tool%20calls%20by%2072.07%25%20compared%20to%20the%20average%20of%20baselines%2C%20while%20the%20latter%20improves%20performance%20by%2022.27%25.%20These%20results%20position%20entropy%20reduction%20as%20a%20key%20mechanism%20for%20enhancing%20tool-use%20behavior%2C%20enabling%20agents%20to%20be%20more%20adaptive%20in%20real-world%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.02050v2&entry.124074799=Read"},
{"title": "One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation", "author": "Zhenyu Wei and Yunchao Yao and Mingyu Ding", "abstract": "Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.", "link": "http://arxiv.org/abs/2602.16712v1", "date": "2026-02-18", "relevancy": 1.8839, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6808}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Hand%20to%20Rule%20Them%20All%3A%20Canonical%20Representations%20for%20Unified%20Dexterous%20Manipulation&body=Title%3A%20One%20Hand%20to%20Rule%20Them%20All%3A%20Canonical%20Representations%20for%20Unified%20Dexterous%20Manipulation%0AAuthor%3A%20Zhenyu%20Wei%20and%20Yunchao%20Yao%20and%20Mingyu%20Ding%0AAbstract%3A%20Dexterous%20manipulation%20policies%20today%20largely%20assume%20fixed%20hand%20designs%2C%20severely%20restricting%20their%20generalization%20to%20new%20embodiments%20with%20varied%20kinematic%20and%20structural%20layouts.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20parameterized%20canonical%20representation%20that%20unifies%20a%20broad%20spectrum%20of%20dexterous%20hand%20architectures.%20It%20comprises%20a%20unified%20parameter%20space%20and%20a%20canonical%20URDF%20format%2C%20offering%20three%20key%20advantages.%201%29%20The%20parameter%20space%20captures%20essential%20morphological%20and%20kinematic%20variations%20for%20effective%20conditioning%20in%20learning%20algorithms.%202%29%20A%20structured%20latent%20manifold%20can%20be%20learned%20over%20our%20space%2C%20where%20interpolations%20between%20embodiments%20yield%20smooth%20and%20physically%20meaningful%20morphology%20transitions.%203%29%20The%20canonical%20URDF%20standardizes%20the%20action%20space%20while%20preserving%20dynamic%20and%20functional%20properties%20of%20the%20original%20URDFs%2C%20enabling%20efficient%20and%20reliable%20cross-embodiment%20policy%20learning.%20We%20validate%20these%20advantages%20through%20extensive%20analysis%20and%20experiments%2C%20including%20grasp%20policy%20replay%2C%20VAE%20latent%20encoding%2C%20and%20cross-embodiment%20zero-shot%20transfer.%20Specifically%2C%20we%20train%20a%20VAE%20on%20the%20unified%20representation%20to%20obtain%20a%20compact%2C%20semantically%20rich%20latent%20embedding%2C%20and%20develop%20a%20grasping%20policy%20conditioned%20on%20the%20canonical%20representation%20that%20generalizes%20across%20dexterous%20hands.%20We%20demonstrate%2C%20through%20simulation%20and%20real-world%20tasks%20on%20unseen%20morphologies%20%28e.g.%2C%2081.9%25%20zero-shot%20success%20rate%20on%203-finger%20LEAP%20Hand%29%2C%20that%20our%20framework%20unifies%20both%20the%20representational%20and%20action%20spaces%20of%20structurally%20diverse%20hands%2C%20providing%20a%20scalable%20foundation%20for%20cross-hand%20learning%20toward%20universal%20dexterous%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Hand%2520to%2520Rule%2520Them%2520All%253A%2520Canonical%2520Representations%2520for%2520Unified%2520Dexterous%2520Manipulation%26entry.906535625%3DZhenyu%2520Wei%2520and%2520Yunchao%2520Yao%2520and%2520Mingyu%2520Ding%26entry.1292438233%3DDexterous%2520manipulation%2520policies%2520today%2520largely%2520assume%2520fixed%2520hand%2520designs%252C%2520severely%2520restricting%2520their%2520generalization%2520to%2520new%2520embodiments%2520with%2520varied%2520kinematic%2520and%2520structural%2520layouts.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520parameterized%2520canonical%2520representation%2520that%2520unifies%2520a%2520broad%2520spectrum%2520of%2520dexterous%2520hand%2520architectures.%2520It%2520comprises%2520a%2520unified%2520parameter%2520space%2520and%2520a%2520canonical%2520URDF%2520format%252C%2520offering%2520three%2520key%2520advantages.%25201%2529%2520The%2520parameter%2520space%2520captures%2520essential%2520morphological%2520and%2520kinematic%2520variations%2520for%2520effective%2520conditioning%2520in%2520learning%2520algorithms.%25202%2529%2520A%2520structured%2520latent%2520manifold%2520can%2520be%2520learned%2520over%2520our%2520space%252C%2520where%2520interpolations%2520between%2520embodiments%2520yield%2520smooth%2520and%2520physically%2520meaningful%2520morphology%2520transitions.%25203%2529%2520The%2520canonical%2520URDF%2520standardizes%2520the%2520action%2520space%2520while%2520preserving%2520dynamic%2520and%2520functional%2520properties%2520of%2520the%2520original%2520URDFs%252C%2520enabling%2520efficient%2520and%2520reliable%2520cross-embodiment%2520policy%2520learning.%2520We%2520validate%2520these%2520advantages%2520through%2520extensive%2520analysis%2520and%2520experiments%252C%2520including%2520grasp%2520policy%2520replay%252C%2520VAE%2520latent%2520encoding%252C%2520and%2520cross-embodiment%2520zero-shot%2520transfer.%2520Specifically%252C%2520we%2520train%2520a%2520VAE%2520on%2520the%2520unified%2520representation%2520to%2520obtain%2520a%2520compact%252C%2520semantically%2520rich%2520latent%2520embedding%252C%2520and%2520develop%2520a%2520grasping%2520policy%2520conditioned%2520on%2520the%2520canonical%2520representation%2520that%2520generalizes%2520across%2520dexterous%2520hands.%2520We%2520demonstrate%252C%2520through%2520simulation%2520and%2520real-world%2520tasks%2520on%2520unseen%2520morphologies%2520%2528e.g.%252C%252081.9%2525%2520zero-shot%2520success%2520rate%2520on%25203-finger%2520LEAP%2520Hand%2529%252C%2520that%2520our%2520framework%2520unifies%2520both%2520the%2520representational%2520and%2520action%2520spaces%2520of%2520structurally%2520diverse%2520hands%252C%2520providing%2520a%2520scalable%2520foundation%2520for%2520cross-hand%2520learning%2520toward%2520universal%2520dexterous%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Hand%20to%20Rule%20Them%20All%3A%20Canonical%20Representations%20for%20Unified%20Dexterous%20Manipulation&entry.906535625=Zhenyu%20Wei%20and%20Yunchao%20Yao%20and%20Mingyu%20Ding&entry.1292438233=Dexterous%20manipulation%20policies%20today%20largely%20assume%20fixed%20hand%20designs%2C%20severely%20restricting%20their%20generalization%20to%20new%20embodiments%20with%20varied%20kinematic%20and%20structural%20layouts.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20parameterized%20canonical%20representation%20that%20unifies%20a%20broad%20spectrum%20of%20dexterous%20hand%20architectures.%20It%20comprises%20a%20unified%20parameter%20space%20and%20a%20canonical%20URDF%20format%2C%20offering%20three%20key%20advantages.%201%29%20The%20parameter%20space%20captures%20essential%20morphological%20and%20kinematic%20variations%20for%20effective%20conditioning%20in%20learning%20algorithms.%202%29%20A%20structured%20latent%20manifold%20can%20be%20learned%20over%20our%20space%2C%20where%20interpolations%20between%20embodiments%20yield%20smooth%20and%20physically%20meaningful%20morphology%20transitions.%203%29%20The%20canonical%20URDF%20standardizes%20the%20action%20space%20while%20preserving%20dynamic%20and%20functional%20properties%20of%20the%20original%20URDFs%2C%20enabling%20efficient%20and%20reliable%20cross-embodiment%20policy%20learning.%20We%20validate%20these%20advantages%20through%20extensive%20analysis%20and%20experiments%2C%20including%20grasp%20policy%20replay%2C%20VAE%20latent%20encoding%2C%20and%20cross-embodiment%20zero-shot%20transfer.%20Specifically%2C%20we%20train%20a%20VAE%20on%20the%20unified%20representation%20to%20obtain%20a%20compact%2C%20semantically%20rich%20latent%20embedding%2C%20and%20develop%20a%20grasping%20policy%20conditioned%20on%20the%20canonical%20representation%20that%20generalizes%20across%20dexterous%20hands.%20We%20demonstrate%2C%20through%20simulation%20and%20real-world%20tasks%20on%20unseen%20morphologies%20%28e.g.%2C%2081.9%25%20zero-shot%20success%20rate%20on%203-finger%20LEAP%20Hand%29%2C%20that%20our%20framework%20unifies%20both%20the%20representational%20and%20action%20spaces%20of%20structurally%20diverse%20hands%2C%20providing%20a%20scalable%20foundation%20for%20cross-hand%20learning%20toward%20universal%20dexterous%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2602.16712v1&entry.124074799=Read"},
{"title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent", "author": "Jean Dufraiche and Paul Mangold and Micha\u00ebl Perrot and Marc Tommasi", "abstract": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.", "link": "http://arxiv.org/abs/2602.16436v1", "date": "2026-02-18", "relevancy": 1.8055, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4624}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4491}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Locally%20Private%20Examples%20by%20Inverse%20Weierstrass%20Private%20Stochastic%20Gradient%20Descent&body=Title%3A%20Learning%20with%20Locally%20Private%20Examples%20by%20Inverse%20Weierstrass%20Private%20Stochastic%20Gradient%20Descent%0AAuthor%3A%20Jean%20Dufraiche%20and%20Paul%20Mangold%20and%20Micha%C3%ABl%20Perrot%20and%20Marc%20Tommasi%0AAbstract%3A%20Releasing%20data%20once%20and%20for%20all%20under%20noninteractive%20Local%20Differential%20Privacy%20%28LDP%29%20enables%20complete%20data%20reusability%2C%20but%20the%20resulting%20noise%20may%20create%20bias%20in%20subsequent%20analyses.%20In%20this%20work%2C%20we%20leverage%20the%20Weierstrass%20transform%20to%20characterize%20this%20bias%20in%20binary%20classification.%20We%20prove%20that%20inverting%20this%20transform%20leads%20to%20a%20bias-correction%20method%20to%20compute%20unbiased%20estimates%20of%20nonlinear%20functions%20on%20examples%20released%20under%20LDP.%20We%20then%20build%20a%20novel%20stochastic%20gradient%20descent%20algorithm%20called%20Inverse%20Weierstrass%20Private%20SGD%20%28IWP-SGD%29.%20It%20converges%20to%20the%20true%20population%20risk%20minimizer%20at%20a%20rate%20of%20%24%5Cmathcal%7BO%7D%281/n%29%24%2C%20with%20%24n%24%20the%20number%20of%20examples.%20We%20empirically%20validate%20IWP-SGD%20on%20binary%20classification%20tasks%20using%20synthetic%20and%20real-world%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520Locally%2520Private%2520Examples%2520by%2520Inverse%2520Weierstrass%2520Private%2520Stochastic%2520Gradient%2520Descent%26entry.906535625%3DJean%2520Dufraiche%2520and%2520Paul%2520Mangold%2520and%2520Micha%25C3%25ABl%2520Perrot%2520and%2520Marc%2520Tommasi%26entry.1292438233%3DReleasing%2520data%2520once%2520and%2520for%2520all%2520under%2520noninteractive%2520Local%2520Differential%2520Privacy%2520%2528LDP%2529%2520enables%2520complete%2520data%2520reusability%252C%2520but%2520the%2520resulting%2520noise%2520may%2520create%2520bias%2520in%2520subsequent%2520analyses.%2520In%2520this%2520work%252C%2520we%2520leverage%2520the%2520Weierstrass%2520transform%2520to%2520characterize%2520this%2520bias%2520in%2520binary%2520classification.%2520We%2520prove%2520that%2520inverting%2520this%2520transform%2520leads%2520to%2520a%2520bias-correction%2520method%2520to%2520compute%2520unbiased%2520estimates%2520of%2520nonlinear%2520functions%2520on%2520examples%2520released%2520under%2520LDP.%2520We%2520then%2520build%2520a%2520novel%2520stochastic%2520gradient%2520descent%2520algorithm%2520called%2520Inverse%2520Weierstrass%2520Private%2520SGD%2520%2528IWP-SGD%2529.%2520It%2520converges%2520to%2520the%2520true%2520population%2520risk%2520minimizer%2520at%2520a%2520rate%2520of%2520%2524%255Cmathcal%257BO%257D%25281/n%2529%2524%252C%2520with%2520%2524n%2524%2520the%2520number%2520of%2520examples.%2520We%2520empirically%2520validate%2520IWP-SGD%2520on%2520binary%2520classification%2520tasks%2520using%2520synthetic%2520and%2520real-world%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Locally%20Private%20Examples%20by%20Inverse%20Weierstrass%20Private%20Stochastic%20Gradient%20Descent&entry.906535625=Jean%20Dufraiche%20and%20Paul%20Mangold%20and%20Micha%C3%ABl%20Perrot%20and%20Marc%20Tommasi&entry.1292438233=Releasing%20data%20once%20and%20for%20all%20under%20noninteractive%20Local%20Differential%20Privacy%20%28LDP%29%20enables%20complete%20data%20reusability%2C%20but%20the%20resulting%20noise%20may%20create%20bias%20in%20subsequent%20analyses.%20In%20this%20work%2C%20we%20leverage%20the%20Weierstrass%20transform%20to%20characterize%20this%20bias%20in%20binary%20classification.%20We%20prove%20that%20inverting%20this%20transform%20leads%20to%20a%20bias-correction%20method%20to%20compute%20unbiased%20estimates%20of%20nonlinear%20functions%20on%20examples%20released%20under%20LDP.%20We%20then%20build%20a%20novel%20stochastic%20gradient%20descent%20algorithm%20called%20Inverse%20Weierstrass%20Private%20SGD%20%28IWP-SGD%29.%20It%20converges%20to%20the%20true%20population%20risk%20minimizer%20at%20a%20rate%20of%20%24%5Cmathcal%7BO%7D%281/n%29%24%2C%20with%20%24n%24%20the%20number%20of%20examples.%20We%20empirically%20validate%20IWP-SGD%20on%20binary%20classification%20tasks%20using%20synthetic%20and%20real-world%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2602.16436v1&entry.124074799=Read"},
{"title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models", "author": "Yu Xie and Ludwig Winkler and Lixin Sun and Sarah Lewis and Adam E. Foster and Jos\u00e9 Jim\u00e9nez Luna and Tim Hempel and Michael Gastegger and Yaoyi Chen and Iryna Zaporozhets and Cecilia Clementi and Christopher M. Bishop and Frank No\u00e9", "abstract": "The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $\u0394$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.", "link": "http://arxiv.org/abs/2602.16634v1", "date": "2026-02-18", "relevancy": 1.9371, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5275}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4758}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Diffusion%20Sampling%3A%20Efficient%20Rare%20Event%20Sampling%20and%20Free%20Energy%20Calculation%20with%20Diffusion%20Models&body=Title%3A%20Enhanced%20Diffusion%20Sampling%3A%20Efficient%20Rare%20Event%20Sampling%20and%20Free%20Energy%20Calculation%20with%20Diffusion%20Models%0AAuthor%3A%20Yu%20Xie%20and%20Ludwig%20Winkler%20and%20Lixin%20Sun%20and%20Sarah%20Lewis%20and%20Adam%20E.%20Foster%20and%20Jos%C3%A9%20Jim%C3%A9nez%20Luna%20and%20Tim%20Hempel%20and%20Michael%20Gastegger%20and%20Yaoyi%20Chen%20and%20Iryna%20Zaporozhets%20and%20Cecilia%20Clementi%20and%20Christopher%20M.%20Bishop%20and%20Frank%20No%C3%A9%0AAbstract%3A%20The%20rare-event%20sampling%20problem%20has%20long%20been%20the%20central%20limiting%20factor%20in%20molecular%20dynamics%20%28MD%29%2C%20especially%20in%20biomolecular%20simulation.%20Recently%2C%20diffusion%20models%20such%20as%20BioEmu%20have%20emerged%20as%20powerful%20equilibrium%20samplers%20that%20generate%20independent%20samples%20from%20complex%20molecular%20distributions%2C%20eliminating%20the%20cost%20of%20sampling%20rare%20transition%20events.%20However%2C%20a%20sampling%20problem%20remains%20when%20computing%20observables%20that%20rely%20on%20states%20which%20are%20rare%20in%20equilibrium%2C%20for%20example%20folding%20free%20energies.%20Here%2C%20we%20introduce%20enhanced%20diffusion%20sampling%2C%20enabling%20efficient%20exploration%20of%20rare-event%20regions%20while%20preserving%20unbiased%20thermodynamic%20estimators.%20The%20key%20idea%20is%20to%20perform%20quantitatively%20accurate%20steering%20protocols%20to%20generate%20biased%20ensembles%20and%20subsequently%20recover%20equilibrium%20statistics%20via%20exact%20reweighting.%20We%20instantiate%20our%20framework%20in%20three%20algorithms%3A%20UmbrellaDiff%20%28umbrella%20sampling%20with%20diffusion%20models%29%2C%20%24%CE%94%24G-Diff%20%28free-energy%20differences%20via%20tilted%20ensembles%29%2C%20and%20MetaDiff%20%28a%20batchwise%20analogue%20for%20metadynamics%29.%20Across%20toy%20systems%2C%20protein%20folding%20landscapes%20and%20folding%20free%20energies%2C%20our%20methods%20achieve%20fast%2C%20accurate%2C%20and%20scalable%20estimation%20of%20equilibrium%20properties%20within%20GPU-minutes%20to%20hours%20per%20system%20--%20closing%20the%20rare-event%20sampling%20gap%20that%20remained%20after%20the%20advent%20of%20diffusion-model%20equilibrium%20samplers.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Diffusion%2520Sampling%253A%2520Efficient%2520Rare%2520Event%2520Sampling%2520and%2520Free%2520Energy%2520Calculation%2520with%2520Diffusion%2520Models%26entry.906535625%3DYu%2520Xie%2520and%2520Ludwig%2520Winkler%2520and%2520Lixin%2520Sun%2520and%2520Sarah%2520Lewis%2520and%2520Adam%2520E.%2520Foster%2520and%2520Jos%25C3%25A9%2520Jim%25C3%25A9nez%2520Luna%2520and%2520Tim%2520Hempel%2520and%2520Michael%2520Gastegger%2520and%2520Yaoyi%2520Chen%2520and%2520Iryna%2520Zaporozhets%2520and%2520Cecilia%2520Clementi%2520and%2520Christopher%2520M.%2520Bishop%2520and%2520Frank%2520No%25C3%25A9%26entry.1292438233%3DThe%2520rare-event%2520sampling%2520problem%2520has%2520long%2520been%2520the%2520central%2520limiting%2520factor%2520in%2520molecular%2520dynamics%2520%2528MD%2529%252C%2520especially%2520in%2520biomolecular%2520simulation.%2520Recently%252C%2520diffusion%2520models%2520such%2520as%2520BioEmu%2520have%2520emerged%2520as%2520powerful%2520equilibrium%2520samplers%2520that%2520generate%2520independent%2520samples%2520from%2520complex%2520molecular%2520distributions%252C%2520eliminating%2520the%2520cost%2520of%2520sampling%2520rare%2520transition%2520events.%2520However%252C%2520a%2520sampling%2520problem%2520remains%2520when%2520computing%2520observables%2520that%2520rely%2520on%2520states%2520which%2520are%2520rare%2520in%2520equilibrium%252C%2520for%2520example%2520folding%2520free%2520energies.%2520Here%252C%2520we%2520introduce%2520enhanced%2520diffusion%2520sampling%252C%2520enabling%2520efficient%2520exploration%2520of%2520rare-event%2520regions%2520while%2520preserving%2520unbiased%2520thermodynamic%2520estimators.%2520The%2520key%2520idea%2520is%2520to%2520perform%2520quantitatively%2520accurate%2520steering%2520protocols%2520to%2520generate%2520biased%2520ensembles%2520and%2520subsequently%2520recover%2520equilibrium%2520statistics%2520via%2520exact%2520reweighting.%2520We%2520instantiate%2520our%2520framework%2520in%2520three%2520algorithms%253A%2520UmbrellaDiff%2520%2528umbrella%2520sampling%2520with%2520diffusion%2520models%2529%252C%2520%2524%25CE%2594%2524G-Diff%2520%2528free-energy%2520differences%2520via%2520tilted%2520ensembles%2529%252C%2520and%2520MetaDiff%2520%2528a%2520batchwise%2520analogue%2520for%2520metadynamics%2529.%2520Across%2520toy%2520systems%252C%2520protein%2520folding%2520landscapes%2520and%2520folding%2520free%2520energies%252C%2520our%2520methods%2520achieve%2520fast%252C%2520accurate%252C%2520and%2520scalable%2520estimation%2520of%2520equilibrium%2520properties%2520within%2520GPU-minutes%2520to%2520hours%2520per%2520system%2520--%2520closing%2520the%2520rare-event%2520sampling%2520gap%2520that%2520remained%2520after%2520the%2520advent%2520of%2520diffusion-model%2520equilibrium%2520samplers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Diffusion%20Sampling%3A%20Efficient%20Rare%20Event%20Sampling%20and%20Free%20Energy%20Calculation%20with%20Diffusion%20Models&entry.906535625=Yu%20Xie%20and%20Ludwig%20Winkler%20and%20Lixin%20Sun%20and%20Sarah%20Lewis%20and%20Adam%20E.%20Foster%20and%20Jos%C3%A9%20Jim%C3%A9nez%20Luna%20and%20Tim%20Hempel%20and%20Michael%20Gastegger%20and%20Yaoyi%20Chen%20and%20Iryna%20Zaporozhets%20and%20Cecilia%20Clementi%20and%20Christopher%20M.%20Bishop%20and%20Frank%20No%C3%A9&entry.1292438233=The%20rare-event%20sampling%20problem%20has%20long%20been%20the%20central%20limiting%20factor%20in%20molecular%20dynamics%20%28MD%29%2C%20especially%20in%20biomolecular%20simulation.%20Recently%2C%20diffusion%20models%20such%20as%20BioEmu%20have%20emerged%20as%20powerful%20equilibrium%20samplers%20that%20generate%20independent%20samples%20from%20complex%20molecular%20distributions%2C%20eliminating%20the%20cost%20of%20sampling%20rare%20transition%20events.%20However%2C%20a%20sampling%20problem%20remains%20when%20computing%20observables%20that%20rely%20on%20states%20which%20are%20rare%20in%20equilibrium%2C%20for%20example%20folding%20free%20energies.%20Here%2C%20we%20introduce%20enhanced%20diffusion%20sampling%2C%20enabling%20efficient%20exploration%20of%20rare-event%20regions%20while%20preserving%20unbiased%20thermodynamic%20estimators.%20The%20key%20idea%20is%20to%20perform%20quantitatively%20accurate%20steering%20protocols%20to%20generate%20biased%20ensembles%20and%20subsequently%20recover%20equilibrium%20statistics%20via%20exact%20reweighting.%20We%20instantiate%20our%20framework%20in%20three%20algorithms%3A%20UmbrellaDiff%20%28umbrella%20sampling%20with%20diffusion%20models%29%2C%20%24%CE%94%24G-Diff%20%28free-energy%20differences%20via%20tilted%20ensembles%29%2C%20and%20MetaDiff%20%28a%20batchwise%20analogue%20for%20metadynamics%29.%20Across%20toy%20systems%2C%20protein%20folding%20landscapes%20and%20folding%20free%20energies%2C%20our%20methods%20achieve%20fast%2C%20accurate%2C%20and%20scalable%20estimation%20of%20equilibrium%20properties%20within%20GPU-minutes%20to%20hours%20per%20system%20--%20closing%20the%20rare-event%20sampling%20gap%20that%20remained%20after%20the%20advent%20of%20diffusion-model%20equilibrium%20samplers.&entry.1838667208=http%3A//arxiv.org/abs/2602.16634v1&entry.124074799=Read"},
{"title": "Universal Properties of Activation Sparsity in Modern Large Language Models", "author": "Filip Szatkowski and Patryk B\u0119dkowski and Alessio Devoto and Jan Dubi\u0144ski and Pasquale Minervini and Miko\u0142aj Pi\u00f3rczy\u0144ski and Simone Scardapane and Bartosz W\u00f3jcik", "abstract": "Activation sparsity is an intriguing property of deep neural networks that has been extensively studied in ReLU-based models, due to its advantages for efficiency, robustness, and interpretability. However, methods relying on exact zero activations do not directly apply to modern Large Language Models (LLMs), leading to fragmented, model-specific strategies for LLM activation sparsity and a gap in its general understanding. In this work, we introduce a general framework for evaluating sparsity robustness in contemporary LLMs and conduct a systematic investigation of this phenomenon in their feedforward~(FFN) layers. Our results uncover universal properties of activation sparsity across diverse model families and scales. Importantly, we observe that the potential for effective activation sparsity grows with model size, highlighting its increasing relevance as models scale. Furthermore, we present the first study of activation sparsity in diffusion-based LLMs. Overall, our work provides a comprehensive perspective and practical guidance for harnessing activation sparsity in LLM design and acceleration.", "link": "http://arxiv.org/abs/2509.00454v2", "date": "2026-02-18", "relevancy": 1.8888, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4725}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Properties%20of%20Activation%20Sparsity%20in%20Modern%20Large%20Language%20Models&body=Title%3A%20Universal%20Properties%20of%20Activation%20Sparsity%20in%20Modern%20Large%20Language%20Models%0AAuthor%3A%20Filip%20Szatkowski%20and%20Patryk%20B%C4%99dkowski%20and%20Alessio%20Devoto%20and%20Jan%20Dubi%C5%84ski%20and%20Pasquale%20Minervini%20and%20Miko%C5%82aj%20Pi%C3%B3rczy%C5%84ski%20and%20Simone%20Scardapane%20and%20Bartosz%20W%C3%B3jcik%0AAbstract%3A%20Activation%20sparsity%20is%20an%20intriguing%20property%20of%20deep%20neural%20networks%20that%20has%20been%20extensively%20studied%20in%20ReLU-based%20models%2C%20due%20to%20its%20advantages%20for%20efficiency%2C%20robustness%2C%20and%20interpretability.%20However%2C%20methods%20relying%20on%20exact%20zero%20activations%20do%20not%20directly%20apply%20to%20modern%20Large%20Language%20Models%20%28LLMs%29%2C%20leading%20to%20fragmented%2C%20model-specific%20strategies%20for%20LLM%20activation%20sparsity%20and%20a%20gap%20in%20its%20general%20understanding.%20In%20this%20work%2C%20we%20introduce%20a%20general%20framework%20for%20evaluating%20sparsity%20robustness%20in%20contemporary%20LLMs%20and%20conduct%20a%20systematic%20investigation%20of%20this%20phenomenon%20in%20their%20feedforward~%28FFN%29%20layers.%20Our%20results%20uncover%20universal%20properties%20of%20activation%20sparsity%20across%20diverse%20model%20families%20and%20scales.%20Importantly%2C%20we%20observe%20that%20the%20potential%20for%20effective%20activation%20sparsity%20grows%20with%20model%20size%2C%20highlighting%20its%20increasing%20relevance%20as%20models%20scale.%20Furthermore%2C%20we%20present%20the%20first%20study%20of%20activation%20sparsity%20in%20diffusion-based%20LLMs.%20Overall%2C%20our%20work%20provides%20a%20comprehensive%20perspective%20and%20practical%20guidance%20for%20harnessing%20activation%20sparsity%20in%20LLM%20design%20and%20acceleration.%0ALink%3A%20http%3A//arxiv.org/abs/2509.00454v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Properties%2520of%2520Activation%2520Sparsity%2520in%2520Modern%2520Large%2520Language%2520Models%26entry.906535625%3DFilip%2520Szatkowski%2520and%2520Patryk%2520B%25C4%2599dkowski%2520and%2520Alessio%2520Devoto%2520and%2520Jan%2520Dubi%25C5%2584ski%2520and%2520Pasquale%2520Minervini%2520and%2520Miko%25C5%2582aj%2520Pi%25C3%25B3rczy%25C5%2584ski%2520and%2520Simone%2520Scardapane%2520and%2520Bartosz%2520W%25C3%25B3jcik%26entry.1292438233%3DActivation%2520sparsity%2520is%2520an%2520intriguing%2520property%2520of%2520deep%2520neural%2520networks%2520that%2520has%2520been%2520extensively%2520studied%2520in%2520ReLU-based%2520models%252C%2520due%2520to%2520its%2520advantages%2520for%2520efficiency%252C%2520robustness%252C%2520and%2520interpretability.%2520However%252C%2520methods%2520relying%2520on%2520exact%2520zero%2520activations%2520do%2520not%2520directly%2520apply%2520to%2520modern%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520leading%2520to%2520fragmented%252C%2520model-specific%2520strategies%2520for%2520LLM%2520activation%2520sparsity%2520and%2520a%2520gap%2520in%2520its%2520general%2520understanding.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520general%2520framework%2520for%2520evaluating%2520sparsity%2520robustness%2520in%2520contemporary%2520LLMs%2520and%2520conduct%2520a%2520systematic%2520investigation%2520of%2520this%2520phenomenon%2520in%2520their%2520feedforward~%2528FFN%2529%2520layers.%2520Our%2520results%2520uncover%2520universal%2520properties%2520of%2520activation%2520sparsity%2520across%2520diverse%2520model%2520families%2520and%2520scales.%2520Importantly%252C%2520we%2520observe%2520that%2520the%2520potential%2520for%2520effective%2520activation%2520sparsity%2520grows%2520with%2520model%2520size%252C%2520highlighting%2520its%2520increasing%2520relevance%2520as%2520models%2520scale.%2520Furthermore%252C%2520we%2520present%2520the%2520first%2520study%2520of%2520activation%2520sparsity%2520in%2520diffusion-based%2520LLMs.%2520Overall%252C%2520our%2520work%2520provides%2520a%2520comprehensive%2520perspective%2520and%2520practical%2520guidance%2520for%2520harnessing%2520activation%2520sparsity%2520in%2520LLM%2520design%2520and%2520acceleration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00454v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Properties%20of%20Activation%20Sparsity%20in%20Modern%20Large%20Language%20Models&entry.906535625=Filip%20Szatkowski%20and%20Patryk%20B%C4%99dkowski%20and%20Alessio%20Devoto%20and%20Jan%20Dubi%C5%84ski%20and%20Pasquale%20Minervini%20and%20Miko%C5%82aj%20Pi%C3%B3rczy%C5%84ski%20and%20Simone%20Scardapane%20and%20Bartosz%20W%C3%B3jcik&entry.1292438233=Activation%20sparsity%20is%20an%20intriguing%20property%20of%20deep%20neural%20networks%20that%20has%20been%20extensively%20studied%20in%20ReLU-based%20models%2C%20due%20to%20its%20advantages%20for%20efficiency%2C%20robustness%2C%20and%20interpretability.%20However%2C%20methods%20relying%20on%20exact%20zero%20activations%20do%20not%20directly%20apply%20to%20modern%20Large%20Language%20Models%20%28LLMs%29%2C%20leading%20to%20fragmented%2C%20model-specific%20strategies%20for%20LLM%20activation%20sparsity%20and%20a%20gap%20in%20its%20general%20understanding.%20In%20this%20work%2C%20we%20introduce%20a%20general%20framework%20for%20evaluating%20sparsity%20robustness%20in%20contemporary%20LLMs%20and%20conduct%20a%20systematic%20investigation%20of%20this%20phenomenon%20in%20their%20feedforward~%28FFN%29%20layers.%20Our%20results%20uncover%20universal%20properties%20of%20activation%20sparsity%20across%20diverse%20model%20families%20and%20scales.%20Importantly%2C%20we%20observe%20that%20the%20potential%20for%20effective%20activation%20sparsity%20grows%20with%20model%20size%2C%20highlighting%20its%20increasing%20relevance%20as%20models%20scale.%20Furthermore%2C%20we%20present%20the%20first%20study%20of%20activation%20sparsity%20in%20diffusion-based%20LLMs.%20Overall%2C%20our%20work%20provides%20a%20comprehensive%20perspective%20and%20practical%20guidance%20for%20harnessing%20activation%20sparsity%20in%20LLM%20design%20and%20acceleration.&entry.1838667208=http%3A//arxiv.org/abs/2509.00454v2&entry.124074799=Read"},
{"title": "Channel Dependence, Limited Lookback Windows, and the Simplicity of Datasets: How Biased is Time Series Forecasting?", "author": "Ibram Abdelmalak and Kiran Madhusudhanan and Jungmin Choi and Christian Kloetergens and Vijaya Krishna Yalavarit and Maximilian Stubbemann and Lars Schmidt-Thieme", "abstract": "In Long-term Time Series Forecasting (LTSF), the lookback window is a critical hyperparameter often set arbitrarily, undermining the validity of model evaluations. We argue that the lookback window must be tuned on a per-task basis to ensure fair comparisons. Our empirical results show that failing to do so can invert performance rankings, particularly when comparing univariate and multivariate methods. Experiments on standard benchmarks reposition Channel-Independent (CI) models, such as PatchTST, as state-of-the-art methods. However, we reveal this superior performance is largely an artifact of weak inter-channel correlations and simplicity of patterns within these specific datasets. Using Granger causality analysis and ODE datasets (with implicit channel correlations), we demonstrate that the true strength of multivariate Channel-Dependent (CD) models emerges on datasets with strong, inherent cross-channel dependencies, where they significantly outperform CI models. We conclude with four key recommendations for improving TSF research: (i) consider the lookback window as a key hyperparameter to tune, (ii) for standard datasets, examining CI architectures is advantageous, (iii) leverage statistical analysis of datasets to guide the choice between CI and CD architectures, and (iv) prefer CD models in scenarios with limited data.", "link": "http://arxiv.org/abs/2502.09683v3", "date": "2026-02-18", "relevancy": 1.3332, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4451}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.445}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Channel%20Dependence%2C%20Limited%20Lookback%20Windows%2C%20and%20the%20Simplicity%20of%20Datasets%3A%20How%20Biased%20is%20Time%20Series%20Forecasting%3F&body=Title%3A%20Channel%20Dependence%2C%20Limited%20Lookback%20Windows%2C%20and%20the%20Simplicity%20of%20Datasets%3A%20How%20Biased%20is%20Time%20Series%20Forecasting%3F%0AAuthor%3A%20Ibram%20Abdelmalak%20and%20Kiran%20Madhusudhanan%20and%20Jungmin%20Choi%20and%20Christian%20Kloetergens%20and%20Vijaya%20Krishna%20Yalavarit%20and%20Maximilian%20Stubbemann%20and%20Lars%20Schmidt-Thieme%0AAbstract%3A%20In%20Long-term%20Time%20Series%20Forecasting%20%28LTSF%29%2C%20the%20lookback%20window%20is%20a%20critical%20hyperparameter%20often%20set%20arbitrarily%2C%20undermining%20the%20validity%20of%20model%20evaluations.%20We%20argue%20that%20the%20lookback%20window%20must%20be%20tuned%20on%20a%20per-task%20basis%20to%20ensure%20fair%20comparisons.%20Our%20empirical%20results%20show%20that%20failing%20to%20do%20so%20can%20invert%20performance%20rankings%2C%20particularly%20when%20comparing%20univariate%20and%20multivariate%20methods.%20Experiments%20on%20standard%20benchmarks%20reposition%20Channel-Independent%20%28CI%29%20models%2C%20such%20as%20PatchTST%2C%20as%20state-of-the-art%20methods.%20However%2C%20we%20reveal%20this%20superior%20performance%20is%20largely%20an%20artifact%20of%20weak%20inter-channel%20correlations%20and%20simplicity%20of%20patterns%20within%20these%20specific%20datasets.%20Using%20Granger%20causality%20analysis%20and%20ODE%20datasets%20%28with%20implicit%20channel%20correlations%29%2C%20we%20demonstrate%20that%20the%20true%20strength%20of%20multivariate%20Channel-Dependent%20%28CD%29%20models%20emerges%20on%20datasets%20with%20strong%2C%20inherent%20cross-channel%20dependencies%2C%20where%20they%20significantly%20outperform%20CI%20models.%20We%20conclude%20with%20four%20key%20recommendations%20for%20improving%20TSF%20research%3A%20%28i%29%20consider%20the%20lookback%20window%20as%20a%20key%20hyperparameter%20to%20tune%2C%20%28ii%29%20for%20standard%20datasets%2C%20examining%20CI%20architectures%20is%20advantageous%2C%20%28iii%29%20leverage%20statistical%20analysis%20of%20datasets%20to%20guide%20the%20choice%20between%20CI%20and%20CD%20architectures%2C%20and%20%28iv%29%20prefer%20CD%20models%20in%20scenarios%20with%20limited%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2502.09683v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChannel%2520Dependence%252C%2520Limited%2520Lookback%2520Windows%252C%2520and%2520the%2520Simplicity%2520of%2520Datasets%253A%2520How%2520Biased%2520is%2520Time%2520Series%2520Forecasting%253F%26entry.906535625%3DIbram%2520Abdelmalak%2520and%2520Kiran%2520Madhusudhanan%2520and%2520Jungmin%2520Choi%2520and%2520Christian%2520Kloetergens%2520and%2520Vijaya%2520Krishna%2520Yalavarit%2520and%2520Maximilian%2520Stubbemann%2520and%2520Lars%2520Schmidt-Thieme%26entry.1292438233%3DIn%2520Long-term%2520Time%2520Series%2520Forecasting%2520%2528LTSF%2529%252C%2520the%2520lookback%2520window%2520is%2520a%2520critical%2520hyperparameter%2520often%2520set%2520arbitrarily%252C%2520undermining%2520the%2520validity%2520of%2520model%2520evaluations.%2520We%2520argue%2520that%2520the%2520lookback%2520window%2520must%2520be%2520tuned%2520on%2520a%2520per-task%2520basis%2520to%2520ensure%2520fair%2520comparisons.%2520Our%2520empirical%2520results%2520show%2520that%2520failing%2520to%2520do%2520so%2520can%2520invert%2520performance%2520rankings%252C%2520particularly%2520when%2520comparing%2520univariate%2520and%2520multivariate%2520methods.%2520Experiments%2520on%2520standard%2520benchmarks%2520reposition%2520Channel-Independent%2520%2528CI%2529%2520models%252C%2520such%2520as%2520PatchTST%252C%2520as%2520state-of-the-art%2520methods.%2520However%252C%2520we%2520reveal%2520this%2520superior%2520performance%2520is%2520largely%2520an%2520artifact%2520of%2520weak%2520inter-channel%2520correlations%2520and%2520simplicity%2520of%2520patterns%2520within%2520these%2520specific%2520datasets.%2520Using%2520Granger%2520causality%2520analysis%2520and%2520ODE%2520datasets%2520%2528with%2520implicit%2520channel%2520correlations%2529%252C%2520we%2520demonstrate%2520that%2520the%2520true%2520strength%2520of%2520multivariate%2520Channel-Dependent%2520%2528CD%2529%2520models%2520emerges%2520on%2520datasets%2520with%2520strong%252C%2520inherent%2520cross-channel%2520dependencies%252C%2520where%2520they%2520significantly%2520outperform%2520CI%2520models.%2520We%2520conclude%2520with%2520four%2520key%2520recommendations%2520for%2520improving%2520TSF%2520research%253A%2520%2528i%2529%2520consider%2520the%2520lookback%2520window%2520as%2520a%2520key%2520hyperparameter%2520to%2520tune%252C%2520%2528ii%2529%2520for%2520standard%2520datasets%252C%2520examining%2520CI%2520architectures%2520is%2520advantageous%252C%2520%2528iii%2529%2520leverage%2520statistical%2520analysis%2520of%2520datasets%2520to%2520guide%2520the%2520choice%2520between%2520CI%2520and%2520CD%2520architectures%252C%2520and%2520%2528iv%2529%2520prefer%2520CD%2520models%2520in%2520scenarios%2520with%2520limited%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09683v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Channel%20Dependence%2C%20Limited%20Lookback%20Windows%2C%20and%20the%20Simplicity%20of%20Datasets%3A%20How%20Biased%20is%20Time%20Series%20Forecasting%3F&entry.906535625=Ibram%20Abdelmalak%20and%20Kiran%20Madhusudhanan%20and%20Jungmin%20Choi%20and%20Christian%20Kloetergens%20and%20Vijaya%20Krishna%20Yalavarit%20and%20Maximilian%20Stubbemann%20and%20Lars%20Schmidt-Thieme&entry.1292438233=In%20Long-term%20Time%20Series%20Forecasting%20%28LTSF%29%2C%20the%20lookback%20window%20is%20a%20critical%20hyperparameter%20often%20set%20arbitrarily%2C%20undermining%20the%20validity%20of%20model%20evaluations.%20We%20argue%20that%20the%20lookback%20window%20must%20be%20tuned%20on%20a%20per-task%20basis%20to%20ensure%20fair%20comparisons.%20Our%20empirical%20results%20show%20that%20failing%20to%20do%20so%20can%20invert%20performance%20rankings%2C%20particularly%20when%20comparing%20univariate%20and%20multivariate%20methods.%20Experiments%20on%20standard%20benchmarks%20reposition%20Channel-Independent%20%28CI%29%20models%2C%20such%20as%20PatchTST%2C%20as%20state-of-the-art%20methods.%20However%2C%20we%20reveal%20this%20superior%20performance%20is%20largely%20an%20artifact%20of%20weak%20inter-channel%20correlations%20and%20simplicity%20of%20patterns%20within%20these%20specific%20datasets.%20Using%20Granger%20causality%20analysis%20and%20ODE%20datasets%20%28with%20implicit%20channel%20correlations%29%2C%20we%20demonstrate%20that%20the%20true%20strength%20of%20multivariate%20Channel-Dependent%20%28CD%29%20models%20emerges%20on%20datasets%20with%20strong%2C%20inherent%20cross-channel%20dependencies%2C%20where%20they%20significantly%20outperform%20CI%20models.%20We%20conclude%20with%20four%20key%20recommendations%20for%20improving%20TSF%20research%3A%20%28i%29%20consider%20the%20lookback%20window%20as%20a%20key%20hyperparameter%20to%20tune%2C%20%28ii%29%20for%20standard%20datasets%2C%20examining%20CI%20architectures%20is%20advantageous%2C%20%28iii%29%20leverage%20statistical%20analysis%20of%20datasets%20to%20guide%20the%20choice%20between%20CI%20and%20CD%20architectures%2C%20and%20%28iv%29%20prefer%20CD%20models%20in%20scenarios%20with%20limited%20data.&entry.1838667208=http%3A//arxiv.org/abs/2502.09683v3&entry.124074799=Read"},
{"title": "On the Hardness of Approximation of the Fair k-Center Problem", "author": "Suhas Thejaswi", "abstract": "In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from each group, called centers, while minimizing the maximum distance from any point to its closest center. Although a polynomial-time $3$-approximation is known for this problem in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. We resolve this open question by proving that, for every $\u03b5>0$, achieving a $(3-\u03b5)$-approximation is NP-hard, assuming $\\text{P} \\neq \\text{NP}$.\n  Our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. Further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. Consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. This result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.", "link": "http://arxiv.org/abs/2602.16688v1", "date": "2026-02-18", "relevancy": 1.3027, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3385}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3264}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Hardness%20of%20Approximation%20of%20the%20Fair%20k-Center%20Problem&body=Title%3A%20On%20the%20Hardness%20of%20Approximation%20of%20the%20Fair%20k-Center%20Problem%0AAuthor%3A%20Suhas%20Thejaswi%0AAbstract%3A%20In%20this%20work%2C%20we%20study%20the%20hardness%20of%20approximation%20of%20the%20fair%20%24k%24-center%20problem.%20Here%20the%20data%20points%20are%20partitioned%20into%20groups%20and%20the%20task%20is%20to%20choose%20a%20prescribed%20number%20of%20data%20points%20from%20each%20group%2C%20called%20centers%2C%20while%20minimizing%20the%20maximum%20distance%20from%20any%20point%20to%20its%20closest%20center.%20Although%20a%20polynomial-time%20%243%24-approximation%20is%20known%20for%20this%20problem%20in%20general%20metrics%2C%20it%20has%20remained%20open%20whether%20this%20approximation%20guarantee%20is%20tight%20or%20could%20be%20further%20improved%2C%20especially%20since%20the%20unconstrained%20%24k%24-center%20problem%20admits%20a%20polynomial-time%20factor-%242%24%20approximation.%20We%20resolve%20this%20open%20question%20by%20proving%20that%2C%20for%20every%20%24%CE%B5%3E0%24%2C%20achieving%20a%20%24%283-%CE%B5%29%24-approximation%20is%20NP-hard%2C%20assuming%20%24%5Ctext%7BP%7D%20%5Cneq%20%5Ctext%7BNP%7D%24.%0A%20%20Our%20inapproximability%20results%20hold%20even%20when%20only%20two%20disjoint%20groups%20are%20present%20and%20at%20least%20one%20center%20must%20be%20chosen%20from%20each%20group.%20Further%2C%20it%20extends%20to%20the%20canonical%20one-per-group%20setting%20with%20%24k%24-groups%20%28for%20arbitrary%20%24k%24%29%2C%20where%20exactly%20one%20center%20must%20be%20selected%20from%20each%20group.%20Consequently%2C%20the%20factor-%243%24%20barrier%20for%20fair%20%24k%24-center%20in%20general%20metric%20spaces%20is%20inherent%2C%20and%20existing%20%243%24-approximation%20algorithms%20are%20optimal%20up%20to%20lower-order%20terms%20even%20in%20these%20restricted%20regimes.%20This%20result%20stands%20in%20sharp%20contrast%20to%20the%20%24k%24-supplier%20formulation%2C%20where%20both%20the%20unconstrained%20and%20fair%20variants%20admit%20factor-%243%24%20approximation%20in%20polynomial%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2602.16688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Hardness%2520of%2520Approximation%2520of%2520the%2520Fair%2520k-Center%2520Problem%26entry.906535625%3DSuhas%2520Thejaswi%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520study%2520the%2520hardness%2520of%2520approximation%2520of%2520the%2520fair%2520%2524k%2524-center%2520problem.%2520Here%2520the%2520data%2520points%2520are%2520partitioned%2520into%2520groups%2520and%2520the%2520task%2520is%2520to%2520choose%2520a%2520prescribed%2520number%2520of%2520data%2520points%2520from%2520each%2520group%252C%2520called%2520centers%252C%2520while%2520minimizing%2520the%2520maximum%2520distance%2520from%2520any%2520point%2520to%2520its%2520closest%2520center.%2520Although%2520a%2520polynomial-time%2520%25243%2524-approximation%2520is%2520known%2520for%2520this%2520problem%2520in%2520general%2520metrics%252C%2520it%2520has%2520remained%2520open%2520whether%2520this%2520approximation%2520guarantee%2520is%2520tight%2520or%2520could%2520be%2520further%2520improved%252C%2520especially%2520since%2520the%2520unconstrained%2520%2524k%2524-center%2520problem%2520admits%2520a%2520polynomial-time%2520factor-%25242%2524%2520approximation.%2520We%2520resolve%2520this%2520open%2520question%2520by%2520proving%2520that%252C%2520for%2520every%2520%2524%25CE%25B5%253E0%2524%252C%2520achieving%2520a%2520%2524%25283-%25CE%25B5%2529%2524-approximation%2520is%2520NP-hard%252C%2520assuming%2520%2524%255Ctext%257BP%257D%2520%255Cneq%2520%255Ctext%257BNP%257D%2524.%250A%2520%2520Our%2520inapproximability%2520results%2520hold%2520even%2520when%2520only%2520two%2520disjoint%2520groups%2520are%2520present%2520and%2520at%2520least%2520one%2520center%2520must%2520be%2520chosen%2520from%2520each%2520group.%2520Further%252C%2520it%2520extends%2520to%2520the%2520canonical%2520one-per-group%2520setting%2520with%2520%2524k%2524-groups%2520%2528for%2520arbitrary%2520%2524k%2524%2529%252C%2520where%2520exactly%2520one%2520center%2520must%2520be%2520selected%2520from%2520each%2520group.%2520Consequently%252C%2520the%2520factor-%25243%2524%2520barrier%2520for%2520fair%2520%2524k%2524-center%2520in%2520general%2520metric%2520spaces%2520is%2520inherent%252C%2520and%2520existing%2520%25243%2524-approximation%2520algorithms%2520are%2520optimal%2520up%2520to%2520lower-order%2520terms%2520even%2520in%2520these%2520restricted%2520regimes.%2520This%2520result%2520stands%2520in%2520sharp%2520contrast%2520to%2520the%2520%2524k%2524-supplier%2520formulation%252C%2520where%2520both%2520the%2520unconstrained%2520and%2520fair%2520variants%2520admit%2520factor-%25243%2524%2520approximation%2520in%2520polynomial%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.16688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Hardness%20of%20Approximation%20of%20the%20Fair%20k-Center%20Problem&entry.906535625=Suhas%20Thejaswi&entry.1292438233=In%20this%20work%2C%20we%20study%20the%20hardness%20of%20approximation%20of%20the%20fair%20%24k%24-center%20problem.%20Here%20the%20data%20points%20are%20partitioned%20into%20groups%20and%20the%20task%20is%20to%20choose%20a%20prescribed%20number%20of%20data%20points%20from%20each%20group%2C%20called%20centers%2C%20while%20minimizing%20the%20maximum%20distance%20from%20any%20point%20to%20its%20closest%20center.%20Although%20a%20polynomial-time%20%243%24-approximation%20is%20known%20for%20this%20problem%20in%20general%20metrics%2C%20it%20has%20remained%20open%20whether%20this%20approximation%20guarantee%20is%20tight%20or%20could%20be%20further%20improved%2C%20especially%20since%20the%20unconstrained%20%24k%24-center%20problem%20admits%20a%20polynomial-time%20factor-%242%24%20approximation.%20We%20resolve%20this%20open%20question%20by%20proving%20that%2C%20for%20every%20%24%CE%B5%3E0%24%2C%20achieving%20a%20%24%283-%CE%B5%29%24-approximation%20is%20NP-hard%2C%20assuming%20%24%5Ctext%7BP%7D%20%5Cneq%20%5Ctext%7BNP%7D%24.%0A%20%20Our%20inapproximability%20results%20hold%20even%20when%20only%20two%20disjoint%20groups%20are%20present%20and%20at%20least%20one%20center%20must%20be%20chosen%20from%20each%20group.%20Further%2C%20it%20extends%20to%20the%20canonical%20one-per-group%20setting%20with%20%24k%24-groups%20%28for%20arbitrary%20%24k%24%29%2C%20where%20exactly%20one%20center%20must%20be%20selected%20from%20each%20group.%20Consequently%2C%20the%20factor-%243%24%20barrier%20for%20fair%20%24k%24-center%20in%20general%20metric%20spaces%20is%20inherent%2C%20and%20existing%20%243%24-approximation%20algorithms%20are%20optimal%20up%20to%20lower-order%20terms%20even%20in%20these%20restricted%20regimes.%20This%20result%20stands%20in%20sharp%20contrast%20to%20the%20%24k%24-supplier%20formulation%2C%20where%20both%20the%20unconstrained%20and%20fair%20variants%20admit%20factor-%243%24%20approximation%20in%20polynomial%20time.&entry.1838667208=http%3A//arxiv.org/abs/2602.16688v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


