<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241024.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D\n  Generation", "author": "Hansheng Chen and Bokui Shen and Yulin Liu and Ruoxi Shi and Linqi Zhou and Connor Z. Lin and Jiayuan Gu and Hao Su and Gordon Wetzstein and Leonidas Guibas", "abstract": "  Multi-view image diffusion models have significantly advanced open-domain 3D\nobject generation. However, most existing models rely on 2D network\narchitectures that lack inherent 3D biases, resulting in compromised geometric\nconsistency. To address this challenge, we introduce 3D-Adapter, a plug-in\nmodule designed to infuse 3D geometry awareness into pretrained image diffusion\nmodels. Central to our approach is the idea of 3D feedback augmentation: for\neach denoising step in the sampling loop, 3D-Adapter decodes intermediate\nmulti-view features into a coherent 3D representation, then re-encodes the\nrendered RGBD views to augment the pretrained base model through feature\naddition. We study two variants of 3D-Adapter: a fast feed-forward version\nbased on Gaussian splatting and a versatile training-free version utilizing\nneural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter\nnot only greatly enhances the geometry quality of text-to-multi-view models\nsuch as Instant3D and Zero123++, but also enables high-quality 3D generation\nusing the plain text-to-image Stable Diffusion. Furthermore, we showcase the\nbroad application potential of 3D-Adapter by presenting high quality results in\ntext-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.\n", "link": "http://arxiv.org/abs/2410.18974v1", "date": "2024-10-24", "relevancy": 3.574, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7367}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7367}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-Adapter%3A%20Geometry-Consistent%20Multi-View%20Diffusion%20for%20High-Quality%203D%0A%20%20Generation&body=Title%3A%203D-Adapter%3A%20Geometry-Consistent%20Multi-View%20Diffusion%20for%20High-Quality%203D%0A%20%20Generation%0AAuthor%3A%20Hansheng%20Chen%20and%20Bokui%20Shen%20and%20Yulin%20Liu%20and%20Ruoxi%20Shi%20and%20Linqi%20Zhou%20and%20Connor%20Z.%20Lin%20and%20Jiayuan%20Gu%20and%20Hao%20Su%20and%20Gordon%20Wetzstein%20and%20Leonidas%20Guibas%0AAbstract%3A%20%20%20Multi-view%20image%20diffusion%20models%20have%20significantly%20advanced%20open-domain%203D%0Aobject%20generation.%20However%2C%20most%20existing%20models%20rely%20on%202D%20network%0Aarchitectures%20that%20lack%20inherent%203D%20biases%2C%20resulting%20in%20compromised%20geometric%0Aconsistency.%20To%20address%20this%20challenge%2C%20we%20introduce%203D-Adapter%2C%20a%20plug-in%0Amodule%20designed%20to%20infuse%203D%20geometry%20awareness%20into%20pretrained%20image%20diffusion%0Amodels.%20Central%20to%20our%20approach%20is%20the%20idea%20of%203D%20feedback%20augmentation%3A%20for%0Aeach%20denoising%20step%20in%20the%20sampling%20loop%2C%203D-Adapter%20decodes%20intermediate%0Amulti-view%20features%20into%20a%20coherent%203D%20representation%2C%20then%20re-encodes%20the%0Arendered%20RGBD%20views%20to%20augment%20the%20pretrained%20base%20model%20through%20feature%0Aaddition.%20We%20study%20two%20variants%20of%203D-Adapter%3A%20a%20fast%20feed-forward%20version%0Abased%20on%20Gaussian%20splatting%20and%20a%20versatile%20training-free%20version%20utilizing%0Aneural%20fields%20and%20meshes.%20Our%20extensive%20experiments%20demonstrate%20that%203D-Adapter%0Anot%20only%20greatly%20enhances%20the%20geometry%20quality%20of%20text-to-multi-view%20models%0Asuch%20as%20Instant3D%20and%20Zero123%2B%2B%2C%20but%20also%20enables%20high-quality%203D%20generation%0Ausing%20the%20plain%20text-to-image%20Stable%20Diffusion.%20Furthermore%2C%20we%20showcase%20the%0Abroad%20application%20potential%20of%203D-Adapter%20by%20presenting%20high%20quality%20results%20in%0Atext-to-3D%2C%20image-to-3D%2C%20text-to-texture%2C%20and%20text-to-avatar%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-Adapter%253A%2520Geometry-Consistent%2520Multi-View%2520Diffusion%2520for%2520High-Quality%25203D%250A%2520%2520Generation%26entry.906535625%3DHansheng%2520Chen%2520and%2520Bokui%2520Shen%2520and%2520Yulin%2520Liu%2520and%2520Ruoxi%2520Shi%2520and%2520Linqi%2520Zhou%2520and%2520Connor%2520Z.%2520Lin%2520and%2520Jiayuan%2520Gu%2520and%2520Hao%2520Su%2520and%2520Gordon%2520Wetzstein%2520and%2520Leonidas%2520Guibas%26entry.1292438233%3D%2520%2520Multi-view%2520image%2520diffusion%2520models%2520have%2520significantly%2520advanced%2520open-domain%25203D%250Aobject%2520generation.%2520However%252C%2520most%2520existing%2520models%2520rely%2520on%25202D%2520network%250Aarchitectures%2520that%2520lack%2520inherent%25203D%2520biases%252C%2520resulting%2520in%2520compromised%2520geometric%250Aconsistency.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%25203D-Adapter%252C%2520a%2520plug-in%250Amodule%2520designed%2520to%2520infuse%25203D%2520geometry%2520awareness%2520into%2520pretrained%2520image%2520diffusion%250Amodels.%2520Central%2520to%2520our%2520approach%2520is%2520the%2520idea%2520of%25203D%2520feedback%2520augmentation%253A%2520for%250Aeach%2520denoising%2520step%2520in%2520the%2520sampling%2520loop%252C%25203D-Adapter%2520decodes%2520intermediate%250Amulti-view%2520features%2520into%2520a%2520coherent%25203D%2520representation%252C%2520then%2520re-encodes%2520the%250Arendered%2520RGBD%2520views%2520to%2520augment%2520the%2520pretrained%2520base%2520model%2520through%2520feature%250Aaddition.%2520We%2520study%2520two%2520variants%2520of%25203D-Adapter%253A%2520a%2520fast%2520feed-forward%2520version%250Abased%2520on%2520Gaussian%2520splatting%2520and%2520a%2520versatile%2520training-free%2520version%2520utilizing%250Aneural%2520fields%2520and%2520meshes.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%25203D-Adapter%250Anot%2520only%2520greatly%2520enhances%2520the%2520geometry%2520quality%2520of%2520text-to-multi-view%2520models%250Asuch%2520as%2520Instant3D%2520and%2520Zero123%252B%252B%252C%2520but%2520also%2520enables%2520high-quality%25203D%2520generation%250Ausing%2520the%2520plain%2520text-to-image%2520Stable%2520Diffusion.%2520Furthermore%252C%2520we%2520showcase%2520the%250Abroad%2520application%2520potential%2520of%25203D-Adapter%2520by%2520presenting%2520high%2520quality%2520results%2520in%250Atext-to-3D%252C%2520image-to-3D%252C%2520text-to-texture%252C%2520and%2520text-to-avatar%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-Adapter%3A%20Geometry-Consistent%20Multi-View%20Diffusion%20for%20High-Quality%203D%0A%20%20Generation&entry.906535625=Hansheng%20Chen%20and%20Bokui%20Shen%20and%20Yulin%20Liu%20and%20Ruoxi%20Shi%20and%20Linqi%20Zhou%20and%20Connor%20Z.%20Lin%20and%20Jiayuan%20Gu%20and%20Hao%20Su%20and%20Gordon%20Wetzstein%20and%20Leonidas%20Guibas&entry.1292438233=%20%20Multi-view%20image%20diffusion%20models%20have%20significantly%20advanced%20open-domain%203D%0Aobject%20generation.%20However%2C%20most%20existing%20models%20rely%20on%202D%20network%0Aarchitectures%20that%20lack%20inherent%203D%20biases%2C%20resulting%20in%20compromised%20geometric%0Aconsistency.%20To%20address%20this%20challenge%2C%20we%20introduce%203D-Adapter%2C%20a%20plug-in%0Amodule%20designed%20to%20infuse%203D%20geometry%20awareness%20into%20pretrained%20image%20diffusion%0Amodels.%20Central%20to%20our%20approach%20is%20the%20idea%20of%203D%20feedback%20augmentation%3A%20for%0Aeach%20denoising%20step%20in%20the%20sampling%20loop%2C%203D-Adapter%20decodes%20intermediate%0Amulti-view%20features%20into%20a%20coherent%203D%20representation%2C%20then%20re-encodes%20the%0Arendered%20RGBD%20views%20to%20augment%20the%20pretrained%20base%20model%20through%20feature%0Aaddition.%20We%20study%20two%20variants%20of%203D-Adapter%3A%20a%20fast%20feed-forward%20version%0Abased%20on%20Gaussian%20splatting%20and%20a%20versatile%20training-free%20version%20utilizing%0Aneural%20fields%20and%20meshes.%20Our%20extensive%20experiments%20demonstrate%20that%203D-Adapter%0Anot%20only%20greatly%20enhances%20the%20geometry%20quality%20of%20text-to-multi-view%20models%0Asuch%20as%20Instant3D%20and%20Zero123%2B%2B%2C%20but%20also%20enables%20high-quality%203D%20generation%0Ausing%20the%20plain%20text-to-image%20Stable%20Diffusion.%20Furthermore%2C%20we%20showcase%20the%0Abroad%20application%20potential%20of%203D-Adapter%20by%20presenting%20high%20quality%20results%20in%0Atext-to-3D%2C%20image-to-3D%2C%20text-to-texture%2C%20and%20text-to-avatar%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18974v1&entry.124074799=Read"},
{"title": "PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary\n  Views", "author": "Xin Fei and Wenzhao Zheng and Yueqi Duan and Wei Zhan and Masayoshi Tomizuka and Kurt Keutzer and Jiwen Lu", "abstract": "  We propose PixelGaussian, an efficient feed-forward framework for learning\ngeneralizable 3D Gaussian reconstruction from arbitrary views. Most existing\nmethods rely on uniform pixel-wise Gaussian representations, which learn a\nfixed number of 3D Gaussians for each view and cannot generalize well to more\ninput views. Differently, our PixelGaussian dynamically adapts both the\nGaussian distribution and quantity based on geometric complexity, leading to\nmore efficient representations and significant improvements in reconstruction\nquality. Specifically, we introduce a Cascade Gaussian Adapter to adjust\nGaussian distribution according to local geometry complexity identified by a\nkeypoint scorer. CGA leverages deformable attention in context-aware\nhypernetworks to guide Gaussian pruning and splitting, ensuring accurate\nrepresentation in complex regions while reducing redundancy. Furthermore, we\ndesign a transformer-based Iterative Gaussian Refiner module that refines\nGaussian representations through direct image-Gaussian interactions. Our\nPixelGaussian can effectively reduce Gaussian redundancy as input views\nincrease. We conduct extensive experiments on the large-scale ACID and\nRealEstate10K datasets, where our method achieves state-of-the-art performance\nwith good generalization to various numbers of views. Code:\nhttps://github.com/Barrybarry-Smith/PixelGaussian.\n", "link": "http://arxiv.org/abs/2410.18979v1", "date": "2024-10-24", "relevancy": 3.3161, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6756}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6634}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixelGaussian%3A%20Generalizable%203D%20Gaussian%20Reconstruction%20from%20Arbitrary%0A%20%20Views&body=Title%3A%20PixelGaussian%3A%20Generalizable%203D%20Gaussian%20Reconstruction%20from%20Arbitrary%0A%20%20Views%0AAuthor%3A%20Xin%20Fei%20and%20Wenzhao%20Zheng%20and%20Yueqi%20Duan%20and%20Wei%20Zhan%20and%20Masayoshi%20Tomizuka%20and%20Kurt%20Keutzer%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20We%20propose%20PixelGaussian%2C%20an%20efficient%20feed-forward%20framework%20for%20learning%0Ageneralizable%203D%20Gaussian%20reconstruction%20from%20arbitrary%20views.%20Most%20existing%0Amethods%20rely%20on%20uniform%20pixel-wise%20Gaussian%20representations%2C%20which%20learn%20a%0Afixed%20number%20of%203D%20Gaussians%20for%20each%20view%20and%20cannot%20generalize%20well%20to%20more%0Ainput%20views.%20Differently%2C%20our%20PixelGaussian%20dynamically%20adapts%20both%20the%0AGaussian%20distribution%20and%20quantity%20based%20on%20geometric%20complexity%2C%20leading%20to%0Amore%20efficient%20representations%20and%20significant%20improvements%20in%20reconstruction%0Aquality.%20Specifically%2C%20we%20introduce%20a%20Cascade%20Gaussian%20Adapter%20to%20adjust%0AGaussian%20distribution%20according%20to%20local%20geometry%20complexity%20identified%20by%20a%0Akeypoint%20scorer.%20CGA%20leverages%20deformable%20attention%20in%20context-aware%0Ahypernetworks%20to%20guide%20Gaussian%20pruning%20and%20splitting%2C%20ensuring%20accurate%0Arepresentation%20in%20complex%20regions%20while%20reducing%20redundancy.%20Furthermore%2C%20we%0Adesign%20a%20transformer-based%20Iterative%20Gaussian%20Refiner%20module%20that%20refines%0AGaussian%20representations%20through%20direct%20image-Gaussian%20interactions.%20Our%0APixelGaussian%20can%20effectively%20reduce%20Gaussian%20redundancy%20as%20input%20views%0Aincrease.%20We%20conduct%20extensive%20experiments%20on%20the%20large-scale%20ACID%20and%0ARealEstate10K%20datasets%2C%20where%20our%20method%20achieves%20state-of-the-art%20performance%0Awith%20good%20generalization%20to%20various%20numbers%20of%20views.%20Code%3A%0Ahttps%3A//github.com/Barrybarry-Smith/PixelGaussian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixelGaussian%253A%2520Generalizable%25203D%2520Gaussian%2520Reconstruction%2520from%2520Arbitrary%250A%2520%2520Views%26entry.906535625%3DXin%2520Fei%2520and%2520Wenzhao%2520Zheng%2520and%2520Yueqi%2520Duan%2520and%2520Wei%2520Zhan%2520and%2520Masayoshi%2520Tomizuka%2520and%2520Kurt%2520Keutzer%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520We%2520propose%2520PixelGaussian%252C%2520an%2520efficient%2520feed-forward%2520framework%2520for%2520learning%250Ageneralizable%25203D%2520Gaussian%2520reconstruction%2520from%2520arbitrary%2520views.%2520Most%2520existing%250Amethods%2520rely%2520on%2520uniform%2520pixel-wise%2520Gaussian%2520representations%252C%2520which%2520learn%2520a%250Afixed%2520number%2520of%25203D%2520Gaussians%2520for%2520each%2520view%2520and%2520cannot%2520generalize%2520well%2520to%2520more%250Ainput%2520views.%2520Differently%252C%2520our%2520PixelGaussian%2520dynamically%2520adapts%2520both%2520the%250AGaussian%2520distribution%2520and%2520quantity%2520based%2520on%2520geometric%2520complexity%252C%2520leading%2520to%250Amore%2520efficient%2520representations%2520and%2520significant%2520improvements%2520in%2520reconstruction%250Aquality.%2520Specifically%252C%2520we%2520introduce%2520a%2520Cascade%2520Gaussian%2520Adapter%2520to%2520adjust%250AGaussian%2520distribution%2520according%2520to%2520local%2520geometry%2520complexity%2520identified%2520by%2520a%250Akeypoint%2520scorer.%2520CGA%2520leverages%2520deformable%2520attention%2520in%2520context-aware%250Ahypernetworks%2520to%2520guide%2520Gaussian%2520pruning%2520and%2520splitting%252C%2520ensuring%2520accurate%250Arepresentation%2520in%2520complex%2520regions%2520while%2520reducing%2520redundancy.%2520Furthermore%252C%2520we%250Adesign%2520a%2520transformer-based%2520Iterative%2520Gaussian%2520Refiner%2520module%2520that%2520refines%250AGaussian%2520representations%2520through%2520direct%2520image-Gaussian%2520interactions.%2520Our%250APixelGaussian%2520can%2520effectively%2520reduce%2520Gaussian%2520redundancy%2520as%2520input%2520views%250Aincrease.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%2520large-scale%2520ACID%2520and%250ARealEstate10K%2520datasets%252C%2520where%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%250Awith%2520good%2520generalization%2520to%2520various%2520numbers%2520of%2520views.%2520Code%253A%250Ahttps%253A//github.com/Barrybarry-Smith/PixelGaussian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixelGaussian%3A%20Generalizable%203D%20Gaussian%20Reconstruction%20from%20Arbitrary%0A%20%20Views&entry.906535625=Xin%20Fei%20and%20Wenzhao%20Zheng%20and%20Yueqi%20Duan%20and%20Wei%20Zhan%20and%20Masayoshi%20Tomizuka%20and%20Kurt%20Keutzer%20and%20Jiwen%20Lu&entry.1292438233=%20%20We%20propose%20PixelGaussian%2C%20an%20efficient%20feed-forward%20framework%20for%20learning%0Ageneralizable%203D%20Gaussian%20reconstruction%20from%20arbitrary%20views.%20Most%20existing%0Amethods%20rely%20on%20uniform%20pixel-wise%20Gaussian%20representations%2C%20which%20learn%20a%0Afixed%20number%20of%203D%20Gaussians%20for%20each%20view%20and%20cannot%20generalize%20well%20to%20more%0Ainput%20views.%20Differently%2C%20our%20PixelGaussian%20dynamically%20adapts%20both%20the%0AGaussian%20distribution%20and%20quantity%20based%20on%20geometric%20complexity%2C%20leading%20to%0Amore%20efficient%20representations%20and%20significant%20improvements%20in%20reconstruction%0Aquality.%20Specifically%2C%20we%20introduce%20a%20Cascade%20Gaussian%20Adapter%20to%20adjust%0AGaussian%20distribution%20according%20to%20local%20geometry%20complexity%20identified%20by%20a%0Akeypoint%20scorer.%20CGA%20leverages%20deformable%20attention%20in%20context-aware%0Ahypernetworks%20to%20guide%20Gaussian%20pruning%20and%20splitting%2C%20ensuring%20accurate%0Arepresentation%20in%20complex%20regions%20while%20reducing%20redundancy.%20Furthermore%2C%20we%0Adesign%20a%20transformer-based%20Iterative%20Gaussian%20Refiner%20module%20that%20refines%0AGaussian%20representations%20through%20direct%20image-Gaussian%20interactions.%20Our%0APixelGaussian%20can%20effectively%20reduce%20Gaussian%20redundancy%20as%20input%20views%0Aincrease.%20We%20conduct%20extensive%20experiments%20on%20the%20large-scale%20ACID%20and%0ARealEstate10K%20datasets%2C%20where%20our%20method%20achieves%20state-of-the-art%20performance%0Awith%20good%20generalization%20to%20various%20numbers%20of%20views.%20Code%3A%0Ahttps%3A//github.com/Barrybarry-Smith/PixelGaussian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18979v1&entry.124074799=Read"},
{"title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling", "author": "Mingtong Zhang and Kaifeng Zhang and Yunzhu Li", "abstract": "  Videos of robots interacting with objects encode rich information about the\nobjects' dynamics. However, existing video prediction approaches typically do\nnot explicitly account for the 3D information from videos, such as robot\nactions and objects' 3D states, limiting their use in real-world robotic\napplications. In this work, we introduce a framework to learn object dynamics\ndirectly from multi-view RGB videos by explicitly considering the robot's\naction trajectories and their effects on scene dynamics. We utilize the 3D\nGaussian representation of 3D Gaussian Splatting (3DGS) to train a\nparticle-based dynamics model using Graph Neural Networks. This model operates\non sparse control particles downsampled from the densely tracked 3D Gaussian\nreconstructions. By learning the neural dynamics model on offline robot\ninteraction data, our method can predict object motions under varying initial\nconfigurations and unseen robot actions. The 3D transformations of Gaussians\ncan be interpolated from the motions of control particles, enabling the\nrendering of predicted future object states and achieving action-conditioned\nvideo prediction. The dynamics model can also be applied to model-based\nplanning frameworks for object manipulation tasks. We conduct experiments on\nvarious kinds of deformable materials, including ropes, clothes, and stuffed\nanimals, demonstrating our framework's ability to model complex shapes and\ndynamics. Our project page is available at https://gs-dynamics.github.io.\n", "link": "http://arxiv.org/abs/2410.18912v1", "date": "2024-10-24", "relevancy": 3.2682, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7124}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6357}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%203D%20Gaussian%20Tracking%20for%20Graph-Based%20Neural%20Dynamics%20Modeling&body=Title%3A%20Dynamic%203D%20Gaussian%20Tracking%20for%20Graph-Based%20Neural%20Dynamics%20Modeling%0AAuthor%3A%20Mingtong%20Zhang%20and%20Kaifeng%20Zhang%20and%20Yunzhu%20Li%0AAbstract%3A%20%20%20Videos%20of%20robots%20interacting%20with%20objects%20encode%20rich%20information%20about%20the%0Aobjects%27%20dynamics.%20However%2C%20existing%20video%20prediction%20approaches%20typically%20do%0Anot%20explicitly%20account%20for%20the%203D%20information%20from%20videos%2C%20such%20as%20robot%0Aactions%20and%20objects%27%203D%20states%2C%20limiting%20their%20use%20in%20real-world%20robotic%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20a%20framework%20to%20learn%20object%20dynamics%0Adirectly%20from%20multi-view%20RGB%20videos%20by%20explicitly%20considering%20the%20robot%27s%0Aaction%20trajectories%20and%20their%20effects%20on%20scene%20dynamics.%20We%20utilize%20the%203D%0AGaussian%20representation%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20train%20a%0Aparticle-based%20dynamics%20model%20using%20Graph%20Neural%20Networks.%20This%20model%20operates%0Aon%20sparse%20control%20particles%20downsampled%20from%20the%20densely%20tracked%203D%20Gaussian%0Areconstructions.%20By%20learning%20the%20neural%20dynamics%20model%20on%20offline%20robot%0Ainteraction%20data%2C%20our%20method%20can%20predict%20object%20motions%20under%20varying%20initial%0Aconfigurations%20and%20unseen%20robot%20actions.%20The%203D%20transformations%20of%20Gaussians%0Acan%20be%20interpolated%20from%20the%20motions%20of%20control%20particles%2C%20enabling%20the%0Arendering%20of%20predicted%20future%20object%20states%20and%20achieving%20action-conditioned%0Avideo%20prediction.%20The%20dynamics%20model%20can%20also%20be%20applied%20to%20model-based%0Aplanning%20frameworks%20for%20object%20manipulation%20tasks.%20We%20conduct%20experiments%20on%0Avarious%20kinds%20of%20deformable%20materials%2C%20including%20ropes%2C%20clothes%2C%20and%20stuffed%0Aanimals%2C%20demonstrating%20our%20framework%27s%20ability%20to%20model%20complex%20shapes%20and%0Adynamics.%20Our%20project%20page%20is%20available%20at%20https%3A//gs-dynamics.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%25203D%2520Gaussian%2520Tracking%2520for%2520Graph-Based%2520Neural%2520Dynamics%2520Modeling%26entry.906535625%3DMingtong%2520Zhang%2520and%2520Kaifeng%2520Zhang%2520and%2520Yunzhu%2520Li%26entry.1292438233%3D%2520%2520Videos%2520of%2520robots%2520interacting%2520with%2520objects%2520encode%2520rich%2520information%2520about%2520the%250Aobjects%2527%2520dynamics.%2520However%252C%2520existing%2520video%2520prediction%2520approaches%2520typically%2520do%250Anot%2520explicitly%2520account%2520for%2520the%25203D%2520information%2520from%2520videos%252C%2520such%2520as%2520robot%250Aactions%2520and%2520objects%2527%25203D%2520states%252C%2520limiting%2520their%2520use%2520in%2520real-world%2520robotic%250Aapplications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520framework%2520to%2520learn%2520object%2520dynamics%250Adirectly%2520from%2520multi-view%2520RGB%2520videos%2520by%2520explicitly%2520considering%2520the%2520robot%2527s%250Aaction%2520trajectories%2520and%2520their%2520effects%2520on%2520scene%2520dynamics.%2520We%2520utilize%2520the%25203D%250AGaussian%2520representation%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520to%2520train%2520a%250Aparticle-based%2520dynamics%2520model%2520using%2520Graph%2520Neural%2520Networks.%2520This%2520model%2520operates%250Aon%2520sparse%2520control%2520particles%2520downsampled%2520from%2520the%2520densely%2520tracked%25203D%2520Gaussian%250Areconstructions.%2520By%2520learning%2520the%2520neural%2520dynamics%2520model%2520on%2520offline%2520robot%250Ainteraction%2520data%252C%2520our%2520method%2520can%2520predict%2520object%2520motions%2520under%2520varying%2520initial%250Aconfigurations%2520and%2520unseen%2520robot%2520actions.%2520The%25203D%2520transformations%2520of%2520Gaussians%250Acan%2520be%2520interpolated%2520from%2520the%2520motions%2520of%2520control%2520particles%252C%2520enabling%2520the%250Arendering%2520of%2520predicted%2520future%2520object%2520states%2520and%2520achieving%2520action-conditioned%250Avideo%2520prediction.%2520The%2520dynamics%2520model%2520can%2520also%2520be%2520applied%2520to%2520model-based%250Aplanning%2520frameworks%2520for%2520object%2520manipulation%2520tasks.%2520We%2520conduct%2520experiments%2520on%250Avarious%2520kinds%2520of%2520deformable%2520materials%252C%2520including%2520ropes%252C%2520clothes%252C%2520and%2520stuffed%250Aanimals%252C%2520demonstrating%2520our%2520framework%2527s%2520ability%2520to%2520model%2520complex%2520shapes%2520and%250Adynamics.%2520Our%2520project%2520page%2520is%2520available%2520at%2520https%253A//gs-dynamics.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%203D%20Gaussian%20Tracking%20for%20Graph-Based%20Neural%20Dynamics%20Modeling&entry.906535625=Mingtong%20Zhang%20and%20Kaifeng%20Zhang%20and%20Yunzhu%20Li&entry.1292438233=%20%20Videos%20of%20robots%20interacting%20with%20objects%20encode%20rich%20information%20about%20the%0Aobjects%27%20dynamics.%20However%2C%20existing%20video%20prediction%20approaches%20typically%20do%0Anot%20explicitly%20account%20for%20the%203D%20information%20from%20videos%2C%20such%20as%20robot%0Aactions%20and%20objects%27%203D%20states%2C%20limiting%20their%20use%20in%20real-world%20robotic%0Aapplications.%20In%20this%20work%2C%20we%20introduce%20a%20framework%20to%20learn%20object%20dynamics%0Adirectly%20from%20multi-view%20RGB%20videos%20by%20explicitly%20considering%20the%20robot%27s%0Aaction%20trajectories%20and%20their%20effects%20on%20scene%20dynamics.%20We%20utilize%20the%203D%0AGaussian%20representation%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20to%20train%20a%0Aparticle-based%20dynamics%20model%20using%20Graph%20Neural%20Networks.%20This%20model%20operates%0Aon%20sparse%20control%20particles%20downsampled%20from%20the%20densely%20tracked%203D%20Gaussian%0Areconstructions.%20By%20learning%20the%20neural%20dynamics%20model%20on%20offline%20robot%0Ainteraction%20data%2C%20our%20method%20can%20predict%20object%20motions%20under%20varying%20initial%0Aconfigurations%20and%20unseen%20robot%20actions.%20The%203D%20transformations%20of%20Gaussians%0Acan%20be%20interpolated%20from%20the%20motions%20of%20control%20particles%2C%20enabling%20the%0Arendering%20of%20predicted%20future%20object%20states%20and%20achieving%20action-conditioned%0Avideo%20prediction.%20The%20dynamics%20model%20can%20also%20be%20applied%20to%20model-based%0Aplanning%20frameworks%20for%20object%20manipulation%20tasks.%20We%20conduct%20experiments%20on%0Avarious%20kinds%20of%20deformable%20materials%2C%20including%20ropes%2C%20clothes%2C%20and%20stuffed%0Aanimals%2C%20demonstrating%20our%20framework%27s%20ability%20to%20model%20complex%20shapes%20and%0Adynamics.%20Our%20project%20page%20is%20available%20at%20https%3A//gs-dynamics.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18912v1&entry.124074799=Read"},
{"title": "Large Spatial Model: End-to-end Unposed Images to Semantic 3D", "author": "Zhiwen Fan and Jian Zhang and Wenyan Cong and Peihao Wang and Renjie Li and Kairun Wen and Shijie Zhou and Achuta Kadambi and Zhangyang Wang and Danfei Xu and Boris Ivanovic and Marco Pavone and Yue Wang", "abstract": "  Reconstructing and understanding 3D structures from a limited number of\nimages is a well-established problem in computer vision. Traditional methods\nusually break this task into multiple subtasks, each requiring complex\ntransformations between different data representations. For instance, dense\nreconstruction through Structure-from-Motion (SfM) involves converting images\ninto key points, optimizing camera parameters, and estimating structures.\nAfterward, accurate sparse reconstructions are required for further dense\nmodeling, which is subsequently fed into task-specific neural networks. This\nmulti-step process results in considerable processing time and increased\nengineering complexity.\n  In this work, we present the Large Spatial Model (LSM), which processes\nunposed RGB images directly into semantic radiance fields. LSM simultaneously\nestimates geometry, appearance, and semantics in a single feed-forward\noperation, and it can generate versatile label maps by interacting with\nlanguage at novel viewpoints. Leveraging a Transformer-based architecture, LSM\nintegrates global geometry through pixel-aligned point maps. To enhance spatial\nattribute regression, we incorporate local context aggregation with multi-scale\nfusion, improving the accuracy of fine local details. To tackle the scarcity of\nlabeled 3D semantic data and enable natural language-driven scene manipulation,\nwe incorporate a pre-trained 2D language-based segmentation model into a\n3D-consistent semantic feature field. An efficient decoder then parameterizes a\nset of semantic anisotropic Gaussians, facilitating supervised end-to-end\nlearning. Extensive experiments across various tasks show that LSM unifies\nmultiple 3D vision tasks directly from unposed images, achieving real-time\nsemantic 3D reconstruction for the first time.\n", "link": "http://arxiv.org/abs/2410.18956v1", "date": "2024-10-24", "relevancy": 3.2298, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6469}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Spatial%20Model%3A%20End-to-end%20Unposed%20Images%20to%20Semantic%203D&body=Title%3A%20Large%20Spatial%20Model%3A%20End-to-end%20Unposed%20Images%20to%20Semantic%203D%0AAuthor%3A%20Zhiwen%20Fan%20and%20Jian%20Zhang%20and%20Wenyan%20Cong%20and%20Peihao%20Wang%20and%20Renjie%20Li%20and%20Kairun%20Wen%20and%20Shijie%20Zhou%20and%20Achuta%20Kadambi%20and%20Zhangyang%20Wang%20and%20Danfei%20Xu%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Yue%20Wang%0AAbstract%3A%20%20%20Reconstructing%20and%20understanding%203D%20structures%20from%20a%20limited%20number%20of%0Aimages%20is%20a%20well-established%20problem%20in%20computer%20vision.%20Traditional%20methods%0Ausually%20break%20this%20task%20into%20multiple%20subtasks%2C%20each%20requiring%20complex%0Atransformations%20between%20different%20data%20representations.%20For%20instance%2C%20dense%0Areconstruction%20through%20Structure-from-Motion%20%28SfM%29%20involves%20converting%20images%0Ainto%20key%20points%2C%20optimizing%20camera%20parameters%2C%20and%20estimating%20structures.%0AAfterward%2C%20accurate%20sparse%20reconstructions%20are%20required%20for%20further%20dense%0Amodeling%2C%20which%20is%20subsequently%20fed%20into%20task-specific%20neural%20networks.%20This%0Amulti-step%20process%20results%20in%20considerable%20processing%20time%20and%20increased%0Aengineering%20complexity.%0A%20%20In%20this%20work%2C%20we%20present%20the%20Large%20Spatial%20Model%20%28LSM%29%2C%20which%20processes%0Aunposed%20RGB%20images%20directly%20into%20semantic%20radiance%20fields.%20LSM%20simultaneously%0Aestimates%20geometry%2C%20appearance%2C%20and%20semantics%20in%20a%20single%20feed-forward%0Aoperation%2C%20and%20it%20can%20generate%20versatile%20label%20maps%20by%20interacting%20with%0Alanguage%20at%20novel%20viewpoints.%20Leveraging%20a%20Transformer-based%20architecture%2C%20LSM%0Aintegrates%20global%20geometry%20through%20pixel-aligned%20point%20maps.%20To%20enhance%20spatial%0Aattribute%20regression%2C%20we%20incorporate%20local%20context%20aggregation%20with%20multi-scale%0Afusion%2C%20improving%20the%20accuracy%20of%20fine%20local%20details.%20To%20tackle%20the%20scarcity%20of%0Alabeled%203D%20semantic%20data%20and%20enable%20natural%20language-driven%20scene%20manipulation%2C%0Awe%20incorporate%20a%20pre-trained%202D%20language-based%20segmentation%20model%20into%20a%0A3D-consistent%20semantic%20feature%20field.%20An%20efficient%20decoder%20then%20parameterizes%20a%0Aset%20of%20semantic%20anisotropic%20Gaussians%2C%20facilitating%20supervised%20end-to-end%0Alearning.%20Extensive%20experiments%20across%20various%20tasks%20show%20that%20LSM%20unifies%0Amultiple%203D%20vision%20tasks%20directly%20from%20unposed%20images%2C%20achieving%20real-time%0Asemantic%203D%20reconstruction%20for%20the%20first%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Spatial%2520Model%253A%2520End-to-end%2520Unposed%2520Images%2520to%2520Semantic%25203D%26entry.906535625%3DZhiwen%2520Fan%2520and%2520Jian%2520Zhang%2520and%2520Wenyan%2520Cong%2520and%2520Peihao%2520Wang%2520and%2520Renjie%2520Li%2520and%2520Kairun%2520Wen%2520and%2520Shijie%2520Zhou%2520and%2520Achuta%2520Kadambi%2520and%2520Zhangyang%2520Wang%2520and%2520Danfei%2520Xu%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520Reconstructing%2520and%2520understanding%25203D%2520structures%2520from%2520a%2520limited%2520number%2520of%250Aimages%2520is%2520a%2520well-established%2520problem%2520in%2520computer%2520vision.%2520Traditional%2520methods%250Ausually%2520break%2520this%2520task%2520into%2520multiple%2520subtasks%252C%2520each%2520requiring%2520complex%250Atransformations%2520between%2520different%2520data%2520representations.%2520For%2520instance%252C%2520dense%250Areconstruction%2520through%2520Structure-from-Motion%2520%2528SfM%2529%2520involves%2520converting%2520images%250Ainto%2520key%2520points%252C%2520optimizing%2520camera%2520parameters%252C%2520and%2520estimating%2520structures.%250AAfterward%252C%2520accurate%2520sparse%2520reconstructions%2520are%2520required%2520for%2520further%2520dense%250Amodeling%252C%2520which%2520is%2520subsequently%2520fed%2520into%2520task-specific%2520neural%2520networks.%2520This%250Amulti-step%2520process%2520results%2520in%2520considerable%2520processing%2520time%2520and%2520increased%250Aengineering%2520complexity.%250A%2520%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520Large%2520Spatial%2520Model%2520%2528LSM%2529%252C%2520which%2520processes%250Aunposed%2520RGB%2520images%2520directly%2520into%2520semantic%2520radiance%2520fields.%2520LSM%2520simultaneously%250Aestimates%2520geometry%252C%2520appearance%252C%2520and%2520semantics%2520in%2520a%2520single%2520feed-forward%250Aoperation%252C%2520and%2520it%2520can%2520generate%2520versatile%2520label%2520maps%2520by%2520interacting%2520with%250Alanguage%2520at%2520novel%2520viewpoints.%2520Leveraging%2520a%2520Transformer-based%2520architecture%252C%2520LSM%250Aintegrates%2520global%2520geometry%2520through%2520pixel-aligned%2520point%2520maps.%2520To%2520enhance%2520spatial%250Aattribute%2520regression%252C%2520we%2520incorporate%2520local%2520context%2520aggregation%2520with%2520multi-scale%250Afusion%252C%2520improving%2520the%2520accuracy%2520of%2520fine%2520local%2520details.%2520To%2520tackle%2520the%2520scarcity%2520of%250Alabeled%25203D%2520semantic%2520data%2520and%2520enable%2520natural%2520language-driven%2520scene%2520manipulation%252C%250Awe%2520incorporate%2520a%2520pre-trained%25202D%2520language-based%2520segmentation%2520model%2520into%2520a%250A3D-consistent%2520semantic%2520feature%2520field.%2520An%2520efficient%2520decoder%2520then%2520parameterizes%2520a%250Aset%2520of%2520semantic%2520anisotropic%2520Gaussians%252C%2520facilitating%2520supervised%2520end-to-end%250Alearning.%2520Extensive%2520experiments%2520across%2520various%2520tasks%2520show%2520that%2520LSM%2520unifies%250Amultiple%25203D%2520vision%2520tasks%2520directly%2520from%2520unposed%2520images%252C%2520achieving%2520real-time%250Asemantic%25203D%2520reconstruction%2520for%2520the%2520first%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Spatial%20Model%3A%20End-to-end%20Unposed%20Images%20to%20Semantic%203D&entry.906535625=Zhiwen%20Fan%20and%20Jian%20Zhang%20and%20Wenyan%20Cong%20and%20Peihao%20Wang%20and%20Renjie%20Li%20and%20Kairun%20Wen%20and%20Shijie%20Zhou%20and%20Achuta%20Kadambi%20and%20Zhangyang%20Wang%20and%20Danfei%20Xu%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Yue%20Wang&entry.1292438233=%20%20Reconstructing%20and%20understanding%203D%20structures%20from%20a%20limited%20number%20of%0Aimages%20is%20a%20well-established%20problem%20in%20computer%20vision.%20Traditional%20methods%0Ausually%20break%20this%20task%20into%20multiple%20subtasks%2C%20each%20requiring%20complex%0Atransformations%20between%20different%20data%20representations.%20For%20instance%2C%20dense%0Areconstruction%20through%20Structure-from-Motion%20%28SfM%29%20involves%20converting%20images%0Ainto%20key%20points%2C%20optimizing%20camera%20parameters%2C%20and%20estimating%20structures.%0AAfterward%2C%20accurate%20sparse%20reconstructions%20are%20required%20for%20further%20dense%0Amodeling%2C%20which%20is%20subsequently%20fed%20into%20task-specific%20neural%20networks.%20This%0Amulti-step%20process%20results%20in%20considerable%20processing%20time%20and%20increased%0Aengineering%20complexity.%0A%20%20In%20this%20work%2C%20we%20present%20the%20Large%20Spatial%20Model%20%28LSM%29%2C%20which%20processes%0Aunposed%20RGB%20images%20directly%20into%20semantic%20radiance%20fields.%20LSM%20simultaneously%0Aestimates%20geometry%2C%20appearance%2C%20and%20semantics%20in%20a%20single%20feed-forward%0Aoperation%2C%20and%20it%20can%20generate%20versatile%20label%20maps%20by%20interacting%20with%0Alanguage%20at%20novel%20viewpoints.%20Leveraging%20a%20Transformer-based%20architecture%2C%20LSM%0Aintegrates%20global%20geometry%20through%20pixel-aligned%20point%20maps.%20To%20enhance%20spatial%0Aattribute%20regression%2C%20we%20incorporate%20local%20context%20aggregation%20with%20multi-scale%0Afusion%2C%20improving%20the%20accuracy%20of%20fine%20local%20details.%20To%20tackle%20the%20scarcity%20of%0Alabeled%203D%20semantic%20data%20and%20enable%20natural%20language-driven%20scene%20manipulation%2C%0Awe%20incorporate%20a%20pre-trained%202D%20language-based%20segmentation%20model%20into%20a%0A3D-consistent%20semantic%20feature%20field.%20An%20efficient%20decoder%20then%20parameterizes%20a%0Aset%20of%20semantic%20anisotropic%20Gaussians%2C%20facilitating%20supervised%20end-to-end%0Alearning.%20Extensive%20experiments%20across%20various%20tasks%20show%20that%20LSM%20unifies%0Amultiple%203D%20vision%20tasks%20directly%20from%20unposed%20images%2C%20achieving%20real-time%0Asemantic%203D%20reconstruction%20for%20the%20first%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18956v1&entry.124074799=Read"},
{"title": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse\n  View Synthesis", "author": "Liang Han and Junsheng Zhou and Yu-Shen Liu and Zhizhong Han", "abstract": "  Novel view synthesis from sparse inputs is a vital yet challenging task in 3D\ncomputer vision. Previous methods explore 3D Gaussian Splatting with neural\npriors (e.g. depth priors) as an additional supervision, demonstrating\npromising quality and efficiency compared to the NeRF based methods. However,\nthe neural priors from 2D pretrained models are often noisy and blurry, which\nstruggle to precisely guide the learning of radiance fields. In this paper, We\npropose a novel method for synthesizing novel views from sparse views with\nGaussian Splatting that does not require external prior as supervision. Our key\nidea lies in exploring the self-supervisions inherent in the binocular stereo\nconsistency between each pair of binocular images constructed with\ndisparity-guided image warping. To this end, we additionally introduce a\nGaussian opacity constraint which regularizes the Gaussian locations and avoids\nGaussian redundancy for improving the robustness and efficiency of inferring 3D\nGaussians from sparse views. Extensive experiments on the LLFF, DTU, and\nBlender datasets demonstrate that our method significantly outperforms the\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2410.18822v1", "date": "2024-10-24", "relevancy": 3.205, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6635}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6404}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Binocular-Guided%203D%20Gaussian%20Splatting%20with%20View%20Consistency%20for%20Sparse%0A%20%20View%20Synthesis&body=Title%3A%20Binocular-Guided%203D%20Gaussian%20Splatting%20with%20View%20Consistency%20for%20Sparse%0A%20%20View%20Synthesis%0AAuthor%3A%20Liang%20Han%20and%20Junsheng%20Zhou%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20from%20sparse%20inputs%20is%20a%20vital%20yet%20challenging%20task%20in%203D%0Acomputer%20vision.%20Previous%20methods%20explore%203D%20Gaussian%20Splatting%20with%20neural%0Apriors%20%28e.g.%20depth%20priors%29%20as%20an%20additional%20supervision%2C%20demonstrating%0Apromising%20quality%20and%20efficiency%20compared%20to%20the%20NeRF%20based%20methods.%20However%2C%0Athe%20neural%20priors%20from%202D%20pretrained%20models%20are%20often%20noisy%20and%20blurry%2C%20which%0Astruggle%20to%20precisely%20guide%20the%20learning%20of%20radiance%20fields.%20In%20this%20paper%2C%20We%0Apropose%20a%20novel%20method%20for%20synthesizing%20novel%20views%20from%20sparse%20views%20with%0AGaussian%20Splatting%20that%20does%20not%20require%20external%20prior%20as%20supervision.%20Our%20key%0Aidea%20lies%20in%20exploring%20the%20self-supervisions%20inherent%20in%20the%20binocular%20stereo%0Aconsistency%20between%20each%20pair%20of%20binocular%20images%20constructed%20with%0Adisparity-guided%20image%20warping.%20To%20this%20end%2C%20we%20additionally%20introduce%20a%0AGaussian%20opacity%20constraint%20which%20regularizes%20the%20Gaussian%20locations%20and%20avoids%0AGaussian%20redundancy%20for%20improving%20the%20robustness%20and%20efficiency%20of%20inferring%203D%0AGaussians%20from%20sparse%20views.%20Extensive%20experiments%20on%20the%20LLFF%2C%20DTU%2C%20and%0ABlender%20datasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%20the%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBinocular-Guided%25203D%2520Gaussian%2520Splatting%2520with%2520View%2520Consistency%2520for%2520Sparse%250A%2520%2520View%2520Synthesis%26entry.906535625%3DLiang%2520Han%2520and%2520Junsheng%2520Zhou%2520and%2520Yu-Shen%2520Liu%2520and%2520Zhizhong%2520Han%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520from%2520sparse%2520inputs%2520is%2520a%2520vital%2520yet%2520challenging%2520task%2520in%25203D%250Acomputer%2520vision.%2520Previous%2520methods%2520explore%25203D%2520Gaussian%2520Splatting%2520with%2520neural%250Apriors%2520%2528e.g.%2520depth%2520priors%2529%2520as%2520an%2520additional%2520supervision%252C%2520demonstrating%250Apromising%2520quality%2520and%2520efficiency%2520compared%2520to%2520the%2520NeRF%2520based%2520methods.%2520However%252C%250Athe%2520neural%2520priors%2520from%25202D%2520pretrained%2520models%2520are%2520often%2520noisy%2520and%2520blurry%252C%2520which%250Astruggle%2520to%2520precisely%2520guide%2520the%2520learning%2520of%2520radiance%2520fields.%2520In%2520this%2520paper%252C%2520We%250Apropose%2520a%2520novel%2520method%2520for%2520synthesizing%2520novel%2520views%2520from%2520sparse%2520views%2520with%250AGaussian%2520Splatting%2520that%2520does%2520not%2520require%2520external%2520prior%2520as%2520supervision.%2520Our%2520key%250Aidea%2520lies%2520in%2520exploring%2520the%2520self-supervisions%2520inherent%2520in%2520the%2520binocular%2520stereo%250Aconsistency%2520between%2520each%2520pair%2520of%2520binocular%2520images%2520constructed%2520with%250Adisparity-guided%2520image%2520warping.%2520To%2520this%2520end%252C%2520we%2520additionally%2520introduce%2520a%250AGaussian%2520opacity%2520constraint%2520which%2520regularizes%2520the%2520Gaussian%2520locations%2520and%2520avoids%250AGaussian%2520redundancy%2520for%2520improving%2520the%2520robustness%2520and%2520efficiency%2520of%2520inferring%25203D%250AGaussians%2520from%2520sparse%2520views.%2520Extensive%2520experiments%2520on%2520the%2520LLFF%252C%2520DTU%252C%2520and%250ABlender%2520datasets%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520the%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Binocular-Guided%203D%20Gaussian%20Splatting%20with%20View%20Consistency%20for%20Sparse%0A%20%20View%20Synthesis&entry.906535625=Liang%20Han%20and%20Junsheng%20Zhou%20and%20Yu-Shen%20Liu%20and%20Zhizhong%20Han&entry.1292438233=%20%20Novel%20view%20synthesis%20from%20sparse%20inputs%20is%20a%20vital%20yet%20challenging%20task%20in%203D%0Acomputer%20vision.%20Previous%20methods%20explore%203D%20Gaussian%20Splatting%20with%20neural%0Apriors%20%28e.g.%20depth%20priors%29%20as%20an%20additional%20supervision%2C%20demonstrating%0Apromising%20quality%20and%20efficiency%20compared%20to%20the%20NeRF%20based%20methods.%20However%2C%0Athe%20neural%20priors%20from%202D%20pretrained%20models%20are%20often%20noisy%20and%20blurry%2C%20which%0Astruggle%20to%20precisely%20guide%20the%20learning%20of%20radiance%20fields.%20In%20this%20paper%2C%20We%0Apropose%20a%20novel%20method%20for%20synthesizing%20novel%20views%20from%20sparse%20views%20with%0AGaussian%20Splatting%20that%20does%20not%20require%20external%20prior%20as%20supervision.%20Our%20key%0Aidea%20lies%20in%20exploring%20the%20self-supervisions%20inherent%20in%20the%20binocular%20stereo%0Aconsistency%20between%20each%20pair%20of%20binocular%20images%20constructed%20with%0Adisparity-guided%20image%20warping.%20To%20this%20end%2C%20we%20additionally%20introduce%20a%0AGaussian%20opacity%20constraint%20which%20regularizes%20the%20Gaussian%20locations%20and%20avoids%0AGaussian%20redundancy%20for%20improving%20the%20robustness%20and%20efficiency%20of%20inferring%203D%0AGaussians%20from%20sparse%20views.%20Extensive%20experiments%20on%20the%20LLFF%2C%20DTU%2C%20and%0ABlender%20datasets%20demonstrate%20that%20our%20method%20significantly%20outperforms%20the%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18822v1&entry.124074799=Read"},
{"title": "Sort-free Gaussian Splatting via Weighted Sum Rendering", "author": "Qiqi Hou and Randall Rauwendaal and Zifeng Li and Hoang Le and Farzad Farhadzadeh and Fatih Porikli and Alexei Bourd and Amir Said", "abstract": "  Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant\nadvancement in 3D scene reconstruction, attracting considerable attention due\nto its ability to recover high-fidelity details while maintaining low\ncomplexity. Despite the promising results achieved by 3DGS, its rendering\nperformance is constrained by its dependence on costly non-commutative\nalpha-blending operations. These operations mandate complex view dependent\nsorting operations that introduce computational overhead, especially on the\nresource-constrained platforms such as mobile phones. In this paper, we propose\nWeighted Sum Rendering, which approximates alpha blending with weighted sums,\nthereby removing the need for sorting. This simplifies implementation, delivers\nsuperior performance, and eliminates the \"popping\" artifacts caused by sorting.\nExperimental results show that optimizing a generalized Gaussian splatting\nformulation to the new differentiable rendering yields competitive image\nquality. The method was implemented and tested in a mobile device GPU,\nachieving on average $1.23\\times$ faster rendering.\n", "link": "http://arxiv.org/abs/2410.18931v1", "date": "2024-10-24", "relevancy": 3.1092, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6651}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6132}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sort-free%20Gaussian%20Splatting%20via%20Weighted%20Sum%20Rendering&body=Title%3A%20Sort-free%20Gaussian%20Splatting%20via%20Weighted%20Sum%20Rendering%0AAuthor%3A%20Qiqi%20Hou%20and%20Randall%20Rauwendaal%20and%20Zifeng%20Li%20and%20Hoang%20Le%20and%20Farzad%20Farhadzadeh%20and%20Fatih%20Porikli%20and%20Alexei%20Bourd%20and%20Amir%20Said%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20significant%0Aadvancement%20in%203D%20scene%20reconstruction%2C%20attracting%20considerable%20attention%20due%0Ato%20its%20ability%20to%20recover%20high-fidelity%20details%20while%20maintaining%20low%0Acomplexity.%20Despite%20the%20promising%20results%20achieved%20by%203DGS%2C%20its%20rendering%0Aperformance%20is%20constrained%20by%20its%20dependence%20on%20costly%20non-commutative%0Aalpha-blending%20operations.%20These%20operations%20mandate%20complex%20view%20dependent%0Asorting%20operations%20that%20introduce%20computational%20overhead%2C%20especially%20on%20the%0Aresource-constrained%20platforms%20such%20as%20mobile%20phones.%20In%20this%20paper%2C%20we%20propose%0AWeighted%20Sum%20Rendering%2C%20which%20approximates%20alpha%20blending%20with%20weighted%20sums%2C%0Athereby%20removing%20the%20need%20for%20sorting.%20This%20simplifies%20implementation%2C%20delivers%0Asuperior%20performance%2C%20and%20eliminates%20the%20%22popping%22%20artifacts%20caused%20by%20sorting.%0AExperimental%20results%20show%20that%20optimizing%20a%20generalized%20Gaussian%20splatting%0Aformulation%20to%20the%20new%20differentiable%20rendering%20yields%20competitive%20image%0Aquality.%20The%20method%20was%20implemented%20and%20tested%20in%20a%20mobile%20device%20GPU%2C%0Aachieving%20on%20average%20%241.23%5Ctimes%24%20faster%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSort-free%2520Gaussian%2520Splatting%2520via%2520Weighted%2520Sum%2520Rendering%26entry.906535625%3DQiqi%2520Hou%2520and%2520Randall%2520Rauwendaal%2520and%2520Zifeng%2520Li%2520and%2520Hoang%2520Le%2520and%2520Farzad%2520Farhadzadeh%2520and%2520Fatih%2520Porikli%2520and%2520Alexei%2520Bourd%2520and%2520Amir%2520Said%26entry.1292438233%3D%2520%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520significant%250Aadvancement%2520in%25203D%2520scene%2520reconstruction%252C%2520attracting%2520considerable%2520attention%2520due%250Ato%2520its%2520ability%2520to%2520recover%2520high-fidelity%2520details%2520while%2520maintaining%2520low%250Acomplexity.%2520Despite%2520the%2520promising%2520results%2520achieved%2520by%25203DGS%252C%2520its%2520rendering%250Aperformance%2520is%2520constrained%2520by%2520its%2520dependence%2520on%2520costly%2520non-commutative%250Aalpha-blending%2520operations.%2520These%2520operations%2520mandate%2520complex%2520view%2520dependent%250Asorting%2520operations%2520that%2520introduce%2520computational%2520overhead%252C%2520especially%2520on%2520the%250Aresource-constrained%2520platforms%2520such%2520as%2520mobile%2520phones.%2520In%2520this%2520paper%252C%2520we%2520propose%250AWeighted%2520Sum%2520Rendering%252C%2520which%2520approximates%2520alpha%2520blending%2520with%2520weighted%2520sums%252C%250Athereby%2520removing%2520the%2520need%2520for%2520sorting.%2520This%2520simplifies%2520implementation%252C%2520delivers%250Asuperior%2520performance%252C%2520and%2520eliminates%2520the%2520%2522popping%2522%2520artifacts%2520caused%2520by%2520sorting.%250AExperimental%2520results%2520show%2520that%2520optimizing%2520a%2520generalized%2520Gaussian%2520splatting%250Aformulation%2520to%2520the%2520new%2520differentiable%2520rendering%2520yields%2520competitive%2520image%250Aquality.%2520The%2520method%2520was%2520implemented%2520and%2520tested%2520in%2520a%2520mobile%2520device%2520GPU%252C%250Aachieving%2520on%2520average%2520%25241.23%255Ctimes%2524%2520faster%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sort-free%20Gaussian%20Splatting%20via%20Weighted%20Sum%20Rendering&entry.906535625=Qiqi%20Hou%20and%20Randall%20Rauwendaal%20and%20Zifeng%20Li%20and%20Hoang%20Le%20and%20Farzad%20Farhadzadeh%20and%20Fatih%20Porikli%20and%20Alexei%20Bourd%20and%20Amir%20Said&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20significant%0Aadvancement%20in%203D%20scene%20reconstruction%2C%20attracting%20considerable%20attention%20due%0Ato%20its%20ability%20to%20recover%20high-fidelity%20details%20while%20maintaining%20low%0Acomplexity.%20Despite%20the%20promising%20results%20achieved%20by%203DGS%2C%20its%20rendering%0Aperformance%20is%20constrained%20by%20its%20dependence%20on%20costly%20non-commutative%0Aalpha-blending%20operations.%20These%20operations%20mandate%20complex%20view%20dependent%0Asorting%20operations%20that%20introduce%20computational%20overhead%2C%20especially%20on%20the%0Aresource-constrained%20platforms%20such%20as%20mobile%20phones.%20In%20this%20paper%2C%20we%20propose%0AWeighted%20Sum%20Rendering%2C%20which%20approximates%20alpha%20blending%20with%20weighted%20sums%2C%0Athereby%20removing%20the%20need%20for%20sorting.%20This%20simplifies%20implementation%2C%20delivers%0Asuperior%20performance%2C%20and%20eliminates%20the%20%22popping%22%20artifacts%20caused%20by%20sorting.%0AExperimental%20results%20show%20that%20optimizing%20a%20generalized%20Gaussian%20splatting%0Aformulation%20to%20the%20new%20differentiable%20rendering%20yields%20competitive%20image%0Aquality.%20The%20method%20was%20implemented%20and%20tested%20in%20a%20mobile%20device%20GPU%2C%0Aachieving%20on%20average%20%241.23%5Ctimes%24%20faster%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18931v1&entry.124074799=Read"},
{"title": "Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map\n  Filtering", "author": "Ido Sobol and Chenfeng Xu and Or Litany", "abstract": "  Generating realistic images from arbitrary views based on a single source\nimage remains a significant challenge in computer vision, with broad\napplications ranging from e-commerce to immersive virtual experiences. Recent\nadvancements in diffusion models, particularly the Zero-1-to-3 model, have been\nwidely adopted for generating plausible views, videos, and 3D models. However,\nthese models still struggle with inconsistencies and implausibility in new\nviews generation, especially for challenging changes in viewpoint. In this\nwork, we propose Zero-to-Hero, a novel test-time approach that enhances view\nsynthesis by manipulating attention maps during the denoising process of\nZero-1-to-3. By drawing an analogy between the denoising process and stochastic\ngradient descent (SGD), we implement a filtering mechanism that aggregates\nattention maps, enhancing generation reliability and authenticity. This process\nimproves geometric consistency without requiring retraining or significant\ncomputational resources. Additionally, we modify the self-attention mechanism\nto integrate information from the source view, reducing shape distortions.\nThese processes are further supported by a specialized sampling schedule.\nExperimental results demonstrate substantial improvements in fidelity and\nconsistency, validated on a diverse set of out-of-distribution objects.\nAdditionally, we demonstrate the general applicability and effectiveness of\nZero-to-Hero in multi-view, and image generation conditioned on semantic maps\nand pose.\n", "link": "http://arxiv.org/abs/2405.18677v2", "date": "2024-10-24", "relevancy": 3.0361, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6293}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5962}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-to-Hero%3A%20Enhancing%20Zero-Shot%20Novel%20View%20Synthesis%20via%20Attention%20Map%0A%20%20Filtering&body=Title%3A%20Zero-to-Hero%3A%20Enhancing%20Zero-Shot%20Novel%20View%20Synthesis%20via%20Attention%20Map%0A%20%20Filtering%0AAuthor%3A%20Ido%20Sobol%20and%20Chenfeng%20Xu%20and%20Or%20Litany%0AAbstract%3A%20%20%20Generating%20realistic%20images%20from%20arbitrary%20views%20based%20on%20a%20single%20source%0Aimage%20remains%20a%20significant%20challenge%20in%20computer%20vision%2C%20with%20broad%0Aapplications%20ranging%20from%20e-commerce%20to%20immersive%20virtual%20experiences.%20Recent%0Aadvancements%20in%20diffusion%20models%2C%20particularly%20the%20Zero-1-to-3%20model%2C%20have%20been%0Awidely%20adopted%20for%20generating%20plausible%20views%2C%20videos%2C%20and%203D%20models.%20However%2C%0Athese%20models%20still%20struggle%20with%20inconsistencies%20and%20implausibility%20in%20new%0Aviews%20generation%2C%20especially%20for%20challenging%20changes%20in%20viewpoint.%20In%20this%0Awork%2C%20we%20propose%20Zero-to-Hero%2C%20a%20novel%20test-time%20approach%20that%20enhances%20view%0Asynthesis%20by%20manipulating%20attention%20maps%20during%20the%20denoising%20process%20of%0AZero-1-to-3.%20By%20drawing%20an%20analogy%20between%20the%20denoising%20process%20and%20stochastic%0Agradient%20descent%20%28SGD%29%2C%20we%20implement%20a%20filtering%20mechanism%20that%20aggregates%0Aattention%20maps%2C%20enhancing%20generation%20reliability%20and%20authenticity.%20This%20process%0Aimproves%20geometric%20consistency%20without%20requiring%20retraining%20or%20significant%0Acomputational%20resources.%20Additionally%2C%20we%20modify%20the%20self-attention%20mechanism%0Ato%20integrate%20information%20from%20the%20source%20view%2C%20reducing%20shape%20distortions.%0AThese%20processes%20are%20further%20supported%20by%20a%20specialized%20sampling%20schedule.%0AExperimental%20results%20demonstrate%20substantial%20improvements%20in%20fidelity%20and%0Aconsistency%2C%20validated%20on%20a%20diverse%20set%20of%20out-of-distribution%20objects.%0AAdditionally%2C%20we%20demonstrate%20the%20general%20applicability%20and%20effectiveness%20of%0AZero-to-Hero%20in%20multi-view%2C%20and%20image%20generation%20conditioned%20on%20semantic%20maps%0Aand%20pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-to-Hero%253A%2520Enhancing%2520Zero-Shot%2520Novel%2520View%2520Synthesis%2520via%2520Attention%2520Map%250A%2520%2520Filtering%26entry.906535625%3DIdo%2520Sobol%2520and%2520Chenfeng%2520Xu%2520and%2520Or%2520Litany%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520images%2520from%2520arbitrary%2520views%2520based%2520on%2520a%2520single%2520source%250Aimage%2520remains%2520a%2520significant%2520challenge%2520in%2520computer%2520vision%252C%2520with%2520broad%250Aapplications%2520ranging%2520from%2520e-commerce%2520to%2520immersive%2520virtual%2520experiences.%2520Recent%250Aadvancements%2520in%2520diffusion%2520models%252C%2520particularly%2520the%2520Zero-1-to-3%2520model%252C%2520have%2520been%250Awidely%2520adopted%2520for%2520generating%2520plausible%2520views%252C%2520videos%252C%2520and%25203D%2520models.%2520However%252C%250Athese%2520models%2520still%2520struggle%2520with%2520inconsistencies%2520and%2520implausibility%2520in%2520new%250Aviews%2520generation%252C%2520especially%2520for%2520challenging%2520changes%2520in%2520viewpoint.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Zero-to-Hero%252C%2520a%2520novel%2520test-time%2520approach%2520that%2520enhances%2520view%250Asynthesis%2520by%2520manipulating%2520attention%2520maps%2520during%2520the%2520denoising%2520process%2520of%250AZero-1-to-3.%2520By%2520drawing%2520an%2520analogy%2520between%2520the%2520denoising%2520process%2520and%2520stochastic%250Agradient%2520descent%2520%2528SGD%2529%252C%2520we%2520implement%2520a%2520filtering%2520mechanism%2520that%2520aggregates%250Aattention%2520maps%252C%2520enhancing%2520generation%2520reliability%2520and%2520authenticity.%2520This%2520process%250Aimproves%2520geometric%2520consistency%2520without%2520requiring%2520retraining%2520or%2520significant%250Acomputational%2520resources.%2520Additionally%252C%2520we%2520modify%2520the%2520self-attention%2520mechanism%250Ato%2520integrate%2520information%2520from%2520the%2520source%2520view%252C%2520reducing%2520shape%2520distortions.%250AThese%2520processes%2520are%2520further%2520supported%2520by%2520a%2520specialized%2520sampling%2520schedule.%250AExperimental%2520results%2520demonstrate%2520substantial%2520improvements%2520in%2520fidelity%2520and%250Aconsistency%252C%2520validated%2520on%2520a%2520diverse%2520set%2520of%2520out-of-distribution%2520objects.%250AAdditionally%252C%2520we%2520demonstrate%2520the%2520general%2520applicability%2520and%2520effectiveness%2520of%250AZero-to-Hero%2520in%2520multi-view%252C%2520and%2520image%2520generation%2520conditioned%2520on%2520semantic%2520maps%250Aand%2520pose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-to-Hero%3A%20Enhancing%20Zero-Shot%20Novel%20View%20Synthesis%20via%20Attention%20Map%0A%20%20Filtering&entry.906535625=Ido%20Sobol%20and%20Chenfeng%20Xu%20and%20Or%20Litany&entry.1292438233=%20%20Generating%20realistic%20images%20from%20arbitrary%20views%20based%20on%20a%20single%20source%0Aimage%20remains%20a%20significant%20challenge%20in%20computer%20vision%2C%20with%20broad%0Aapplications%20ranging%20from%20e-commerce%20to%20immersive%20virtual%20experiences.%20Recent%0Aadvancements%20in%20diffusion%20models%2C%20particularly%20the%20Zero-1-to-3%20model%2C%20have%20been%0Awidely%20adopted%20for%20generating%20plausible%20views%2C%20videos%2C%20and%203D%20models.%20However%2C%0Athese%20models%20still%20struggle%20with%20inconsistencies%20and%20implausibility%20in%20new%0Aviews%20generation%2C%20especially%20for%20challenging%20changes%20in%20viewpoint.%20In%20this%0Awork%2C%20we%20propose%20Zero-to-Hero%2C%20a%20novel%20test-time%20approach%20that%20enhances%20view%0Asynthesis%20by%20manipulating%20attention%20maps%20during%20the%20denoising%20process%20of%0AZero-1-to-3.%20By%20drawing%20an%20analogy%20between%20the%20denoising%20process%20and%20stochastic%0Agradient%20descent%20%28SGD%29%2C%20we%20implement%20a%20filtering%20mechanism%20that%20aggregates%0Aattention%20maps%2C%20enhancing%20generation%20reliability%20and%20authenticity.%20This%20process%0Aimproves%20geometric%20consistency%20without%20requiring%20retraining%20or%20significant%0Acomputational%20resources.%20Additionally%2C%20we%20modify%20the%20self-attention%20mechanism%0Ato%20integrate%20information%20from%20the%20source%20view%2C%20reducing%20shape%20distortions.%0AThese%20processes%20are%20further%20supported%20by%20a%20specialized%20sampling%20schedule.%0AExperimental%20results%20demonstrate%20substantial%20improvements%20in%20fidelity%20and%0Aconsistency%2C%20validated%20on%20a%20diverse%20set%20of%20out-of-distribution%20objects.%0AAdditionally%2C%20we%20demonstrate%20the%20general%20applicability%20and%20effectiveness%20of%0AZero-to-Hero%20in%20multi-view%2C%20and%20image%20generation%20conditioned%20on%20semantic%20maps%0Aand%20pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18677v2&entry.124074799=Read"},
{"title": "Where Am I and What Will I See: An Auto-Regressive Model for Spatial\n  Localization and View Prediction", "author": "Junyi Chen and Di Huang and Weicai Ye and Wanli Ouyang and Tong He", "abstract": "  Spatial intelligence is the ability of a machine to perceive, reason, and act\nin three dimensions within space and time. Recent advancements in large-scale\nauto-regressive models have demonstrated remarkable capabilities across various\nreasoning tasks. However, these models often struggle with fundamental aspects\nof spatial reasoning, particularly in answering questions like \"Where am I?\"\nand \"What will I see?\". While some attempts have been done, existing approaches\ntypically treat them as separate tasks, failing to capture their interconnected\nnature. In this paper, we present Generative Spatial Transformer (GST), a novel\nauto-regressive framework that jointly addresses spatial localization and view\nprediction. Our model simultaneously estimates the camera pose from a single\nimage and predicts the view from a new camera pose, effectively bridging the\ngap between spatial awareness and visual prediction. The proposed innovative\ncamera tokenization method enables the model to learn the joint distribution of\n2D projections and their corresponding spatial perspectives in an\nauto-regressive manner. This unified training paradigm demonstrates that joint\noptimization of pose estimation and novel view synthesis leads to improved\nperformance in both tasks, for the first time, highlighting the inherent\nrelationship between spatial awareness and visual prediction.\n", "link": "http://arxiv.org/abs/2410.18962v1", "date": "2024-10-24", "relevancy": 3.0332, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.619}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.605}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20Am%20I%20and%20What%20Will%20I%20See%3A%20An%20Auto-Regressive%20Model%20for%20Spatial%0A%20%20Localization%20and%20View%20Prediction&body=Title%3A%20Where%20Am%20I%20and%20What%20Will%20I%20See%3A%20An%20Auto-Regressive%20Model%20for%20Spatial%0A%20%20Localization%20and%20View%20Prediction%0AAuthor%3A%20Junyi%20Chen%20and%20Di%20Huang%20and%20Weicai%20Ye%20and%20Wanli%20Ouyang%20and%20Tong%20He%0AAbstract%3A%20%20%20Spatial%20intelligence%20is%20the%20ability%20of%20a%20machine%20to%20perceive%2C%20reason%2C%20and%20act%0Ain%20three%20dimensions%20within%20space%20and%20time.%20Recent%20advancements%20in%20large-scale%0Aauto-regressive%20models%20have%20demonstrated%20remarkable%20capabilities%20across%20various%0Areasoning%20tasks.%20However%2C%20these%20models%20often%20struggle%20with%20fundamental%20aspects%0Aof%20spatial%20reasoning%2C%20particularly%20in%20answering%20questions%20like%20%22Where%20am%20I%3F%22%0Aand%20%22What%20will%20I%20see%3F%22.%20While%20some%20attempts%20have%20been%20done%2C%20existing%20approaches%0Atypically%20treat%20them%20as%20separate%20tasks%2C%20failing%20to%20capture%20their%20interconnected%0Anature.%20In%20this%20paper%2C%20we%20present%20Generative%20Spatial%20Transformer%20%28GST%29%2C%20a%20novel%0Aauto-regressive%20framework%20that%20jointly%20addresses%20spatial%20localization%20and%20view%0Aprediction.%20Our%20model%20simultaneously%20estimates%20the%20camera%20pose%20from%20a%20single%0Aimage%20and%20predicts%20the%20view%20from%20a%20new%20camera%20pose%2C%20effectively%20bridging%20the%0Agap%20between%20spatial%20awareness%20and%20visual%20prediction.%20The%20proposed%20innovative%0Acamera%20tokenization%20method%20enables%20the%20model%20to%20learn%20the%20joint%20distribution%20of%0A2D%20projections%20and%20their%20corresponding%20spatial%20perspectives%20in%20an%0Aauto-regressive%20manner.%20This%20unified%20training%20paradigm%20demonstrates%20that%20joint%0Aoptimization%20of%20pose%20estimation%20and%20novel%20view%20synthesis%20leads%20to%20improved%0Aperformance%20in%20both%20tasks%2C%20for%20the%20first%20time%2C%20highlighting%20the%20inherent%0Arelationship%20between%20spatial%20awareness%20and%20visual%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520Am%2520I%2520and%2520What%2520Will%2520I%2520See%253A%2520An%2520Auto-Regressive%2520Model%2520for%2520Spatial%250A%2520%2520Localization%2520and%2520View%2520Prediction%26entry.906535625%3DJunyi%2520Chen%2520and%2520Di%2520Huang%2520and%2520Weicai%2520Ye%2520and%2520Wanli%2520Ouyang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520Spatial%2520intelligence%2520is%2520the%2520ability%2520of%2520a%2520machine%2520to%2520perceive%252C%2520reason%252C%2520and%2520act%250Ain%2520three%2520dimensions%2520within%2520space%2520and%2520time.%2520Recent%2520advancements%2520in%2520large-scale%250Aauto-regressive%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%2520various%250Areasoning%2520tasks.%2520However%252C%2520these%2520models%2520often%2520struggle%2520with%2520fundamental%2520aspects%250Aof%2520spatial%2520reasoning%252C%2520particularly%2520in%2520answering%2520questions%2520like%2520%2522Where%2520am%2520I%253F%2522%250Aand%2520%2522What%2520will%2520I%2520see%253F%2522.%2520While%2520some%2520attempts%2520have%2520been%2520done%252C%2520existing%2520approaches%250Atypically%2520treat%2520them%2520as%2520separate%2520tasks%252C%2520failing%2520to%2520capture%2520their%2520interconnected%250Anature.%2520In%2520this%2520paper%252C%2520we%2520present%2520Generative%2520Spatial%2520Transformer%2520%2528GST%2529%252C%2520a%2520novel%250Aauto-regressive%2520framework%2520that%2520jointly%2520addresses%2520spatial%2520localization%2520and%2520view%250Aprediction.%2520Our%2520model%2520simultaneously%2520estimates%2520the%2520camera%2520pose%2520from%2520a%2520single%250Aimage%2520and%2520predicts%2520the%2520view%2520from%2520a%2520new%2520camera%2520pose%252C%2520effectively%2520bridging%2520the%250Agap%2520between%2520spatial%2520awareness%2520and%2520visual%2520prediction.%2520The%2520proposed%2520innovative%250Acamera%2520tokenization%2520method%2520enables%2520the%2520model%2520to%2520learn%2520the%2520joint%2520distribution%2520of%250A2D%2520projections%2520and%2520their%2520corresponding%2520spatial%2520perspectives%2520in%2520an%250Aauto-regressive%2520manner.%2520This%2520unified%2520training%2520paradigm%2520demonstrates%2520that%2520joint%250Aoptimization%2520of%2520pose%2520estimation%2520and%2520novel%2520view%2520synthesis%2520leads%2520to%2520improved%250Aperformance%2520in%2520both%2520tasks%252C%2520for%2520the%2520first%2520time%252C%2520highlighting%2520the%2520inherent%250Arelationship%2520between%2520spatial%2520awareness%2520and%2520visual%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20Am%20I%20and%20What%20Will%20I%20See%3A%20An%20Auto-Regressive%20Model%20for%20Spatial%0A%20%20Localization%20and%20View%20Prediction&entry.906535625=Junyi%20Chen%20and%20Di%20Huang%20and%20Weicai%20Ye%20and%20Wanli%20Ouyang%20and%20Tong%20He&entry.1292438233=%20%20Spatial%20intelligence%20is%20the%20ability%20of%20a%20machine%20to%20perceive%2C%20reason%2C%20and%20act%0Ain%20three%20dimensions%20within%20space%20and%20time.%20Recent%20advancements%20in%20large-scale%0Aauto-regressive%20models%20have%20demonstrated%20remarkable%20capabilities%20across%20various%0Areasoning%20tasks.%20However%2C%20these%20models%20often%20struggle%20with%20fundamental%20aspects%0Aof%20spatial%20reasoning%2C%20particularly%20in%20answering%20questions%20like%20%22Where%20am%20I%3F%22%0Aand%20%22What%20will%20I%20see%3F%22.%20While%20some%20attempts%20have%20been%20done%2C%20existing%20approaches%0Atypically%20treat%20them%20as%20separate%20tasks%2C%20failing%20to%20capture%20their%20interconnected%0Anature.%20In%20this%20paper%2C%20we%20present%20Generative%20Spatial%20Transformer%20%28GST%29%2C%20a%20novel%0Aauto-regressive%20framework%20that%20jointly%20addresses%20spatial%20localization%20and%20view%0Aprediction.%20Our%20model%20simultaneously%20estimates%20the%20camera%20pose%20from%20a%20single%0Aimage%20and%20predicts%20the%20view%20from%20a%20new%20camera%20pose%2C%20effectively%20bridging%20the%0Agap%20between%20spatial%20awareness%20and%20visual%20prediction.%20The%20proposed%20innovative%0Acamera%20tokenization%20method%20enables%20the%20model%20to%20learn%20the%20joint%20distribution%20of%0A2D%20projections%20and%20their%20corresponding%20spatial%20perspectives%20in%20an%0Aauto-regressive%20manner.%20This%20unified%20training%20paradigm%20demonstrates%20that%20joint%0Aoptimization%20of%20pose%20estimation%20and%20novel%20view%20synthesis%20leads%20to%20improved%0Aperformance%20in%20both%20tasks%2C%20for%20the%20first%20time%2C%20highlighting%20the%20inherent%0Arelationship%20between%20spatial%20awareness%20and%20visual%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18962v1&entry.124074799=Read"},
{"title": "Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video\n  MLLMs", "author": "Zijia Zhao and Haoyu Lu and Yuqi Huo and Yifan Du and Tongtian Yue and Longteng Guo and Bingning Wang and Weipeng Chen and Jing Liu", "abstract": "  Video understanding is a crucial next step for multimodal large language\nmodels (MLLMs). Various benchmarks are introduced for better evaluating the\nMLLMs. Nevertheless, current video benchmarks are still inefficient for\nevaluating video models during iterative development due to the high cost of\nconstructing datasets and the difficulty in isolating specific skills. In this\npaper, we propose VideoNIAH (Video Needle In A Haystack), a benchmark\nconstruction framework through synthetic video generation. VideoNIAH decouples\nvideo content from their query-responses by inserting unrelated visual\n'needles' into original videos. The framework automates the generation of\nquery-response pairs using predefined rules, minimizing manual labor. The\nqueries focus on specific aspects of video understanding, enabling more\nskill-specific evaluations. The separation between video content and the\nqueries also allow for increased video variety and evaluations across different\nlengths. Utilizing VideoNIAH, we compile a video benchmark VNBench, which\nincludes tasks such as retrieval, ordering, and counting to evaluate three key\naspects of video understanding: temporal perception, chronological ordering,\nand spatio-temporal coherence. We conduct a comprehensive evaluation of both\nproprietary and open-source models, uncovering significant differences in their\nvideo understanding capabilities across various tasks. Additionally, we perform\nan in-depth analysis of the test results and model configurations. Based on\nthese findings, we provide some advice for improving video MLLM training,\noffering valuable insights to guide future research and model development. The\ncode and data are available at https://github.com/joez17/VideoNIAH.\n", "link": "http://arxiv.org/abs/2406.09367v2", "date": "2024-10-24", "relevancy": 2.9119, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Needle%20In%20A%20Video%20Haystack%3A%20A%20Scalable%20Synthetic%20Evaluator%20for%20Video%0A%20%20MLLMs&body=Title%3A%20Needle%20In%20A%20Video%20Haystack%3A%20A%20Scalable%20Synthetic%20Evaluator%20for%20Video%0A%20%20MLLMs%0AAuthor%3A%20Zijia%20Zhao%20and%20Haoyu%20Lu%20and%20Yuqi%20Huo%20and%20Yifan%20Du%20and%20Tongtian%20Yue%20and%20Longteng%20Guo%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Video%20understanding%20is%20a%20crucial%20next%20step%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%20Various%20benchmarks%20are%20introduced%20for%20better%20evaluating%20the%0AMLLMs.%20Nevertheless%2C%20current%20video%20benchmarks%20are%20still%20inefficient%20for%0Aevaluating%20video%20models%20during%20iterative%20development%20due%20to%20the%20high%20cost%20of%0Aconstructing%20datasets%20and%20the%20difficulty%20in%20isolating%20specific%20skills.%20In%20this%0Apaper%2C%20we%20propose%20VideoNIAH%20%28Video%20Needle%20In%20A%20Haystack%29%2C%20a%20benchmark%0Aconstruction%20framework%20through%20synthetic%20video%20generation.%20VideoNIAH%20decouples%0Avideo%20content%20from%20their%20query-responses%20by%20inserting%20unrelated%20visual%0A%27needles%27%20into%20original%20videos.%20The%20framework%20automates%20the%20generation%20of%0Aquery-response%20pairs%20using%20predefined%20rules%2C%20minimizing%20manual%20labor.%20The%0Aqueries%20focus%20on%20specific%20aspects%20of%20video%20understanding%2C%20enabling%20more%0Askill-specific%20evaluations.%20The%20separation%20between%20video%20content%20and%20the%0Aqueries%20also%20allow%20for%20increased%20video%20variety%20and%20evaluations%20across%20different%0Alengths.%20Utilizing%20VideoNIAH%2C%20we%20compile%20a%20video%20benchmark%20VNBench%2C%20which%0Aincludes%20tasks%20such%20as%20retrieval%2C%20ordering%2C%20and%20counting%20to%20evaluate%20three%20key%0Aaspects%20of%20video%20understanding%3A%20temporal%20perception%2C%20chronological%20ordering%2C%0Aand%20spatio-temporal%20coherence.%20We%20conduct%20a%20comprehensive%20evaluation%20of%20both%0Aproprietary%20and%20open-source%20models%2C%20uncovering%20significant%20differences%20in%20their%0Avideo%20understanding%20capabilities%20across%20various%20tasks.%20Additionally%2C%20we%20perform%0Aan%20in-depth%20analysis%20of%20the%20test%20results%20and%20model%20configurations.%20Based%20on%0Athese%20findings%2C%20we%20provide%20some%20advice%20for%20improving%20video%20MLLM%20training%2C%0Aoffering%20valuable%20insights%20to%20guide%20future%20research%20and%20model%20development.%20The%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/joez17/VideoNIAH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeedle%2520In%2520A%2520Video%2520Haystack%253A%2520A%2520Scalable%2520Synthetic%2520Evaluator%2520for%2520Video%250A%2520%2520MLLMs%26entry.906535625%3DZijia%2520Zhao%2520and%2520Haoyu%2520Lu%2520and%2520Yuqi%2520Huo%2520and%2520Yifan%2520Du%2520and%2520Tongtian%2520Yue%2520and%2520Longteng%2520Guo%2520and%2520Bingning%2520Wang%2520and%2520Weipeng%2520Chen%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520Video%2520understanding%2520is%2520a%2520crucial%2520next%2520step%2520for%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529.%2520Various%2520benchmarks%2520are%2520introduced%2520for%2520better%2520evaluating%2520the%250AMLLMs.%2520Nevertheless%252C%2520current%2520video%2520benchmarks%2520are%2520still%2520inefficient%2520for%250Aevaluating%2520video%2520models%2520during%2520iterative%2520development%2520due%2520to%2520the%2520high%2520cost%2520of%250Aconstructing%2520datasets%2520and%2520the%2520difficulty%2520in%2520isolating%2520specific%2520skills.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520VideoNIAH%2520%2528Video%2520Needle%2520In%2520A%2520Haystack%2529%252C%2520a%2520benchmark%250Aconstruction%2520framework%2520through%2520synthetic%2520video%2520generation.%2520VideoNIAH%2520decouples%250Avideo%2520content%2520from%2520their%2520query-responses%2520by%2520inserting%2520unrelated%2520visual%250A%2527needles%2527%2520into%2520original%2520videos.%2520The%2520framework%2520automates%2520the%2520generation%2520of%250Aquery-response%2520pairs%2520using%2520predefined%2520rules%252C%2520minimizing%2520manual%2520labor.%2520The%250Aqueries%2520focus%2520on%2520specific%2520aspects%2520of%2520video%2520understanding%252C%2520enabling%2520more%250Askill-specific%2520evaluations.%2520The%2520separation%2520between%2520video%2520content%2520and%2520the%250Aqueries%2520also%2520allow%2520for%2520increased%2520video%2520variety%2520and%2520evaluations%2520across%2520different%250Alengths.%2520Utilizing%2520VideoNIAH%252C%2520we%2520compile%2520a%2520video%2520benchmark%2520VNBench%252C%2520which%250Aincludes%2520tasks%2520such%2520as%2520retrieval%252C%2520ordering%252C%2520and%2520counting%2520to%2520evaluate%2520three%2520key%250Aaspects%2520of%2520video%2520understanding%253A%2520temporal%2520perception%252C%2520chronological%2520ordering%252C%250Aand%2520spatio-temporal%2520coherence.%2520We%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520both%250Aproprietary%2520and%2520open-source%2520models%252C%2520uncovering%2520significant%2520differences%2520in%2520their%250Avideo%2520understanding%2520capabilities%2520across%2520various%2520tasks.%2520Additionally%252C%2520we%2520perform%250Aan%2520in-depth%2520analysis%2520of%2520the%2520test%2520results%2520and%2520model%2520configurations.%2520Based%2520on%250Athese%2520findings%252C%2520we%2520provide%2520some%2520advice%2520for%2520improving%2520video%2520MLLM%2520training%252C%250Aoffering%2520valuable%2520insights%2520to%2520guide%2520future%2520research%2520and%2520model%2520development.%2520The%250Acode%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/joez17/VideoNIAH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Needle%20In%20A%20Video%20Haystack%3A%20A%20Scalable%20Synthetic%20Evaluator%20for%20Video%0A%20%20MLLMs&entry.906535625=Zijia%20Zhao%20and%20Haoyu%20Lu%20and%20Yuqi%20Huo%20and%20Yifan%20Du%20and%20Tongtian%20Yue%20and%20Longteng%20Guo%20and%20Bingning%20Wang%20and%20Weipeng%20Chen%20and%20Jing%20Liu&entry.1292438233=%20%20Video%20understanding%20is%20a%20crucial%20next%20step%20for%20multimodal%20large%20language%0Amodels%20%28MLLMs%29.%20Various%20benchmarks%20are%20introduced%20for%20better%20evaluating%20the%0AMLLMs.%20Nevertheless%2C%20current%20video%20benchmarks%20are%20still%20inefficient%20for%0Aevaluating%20video%20models%20during%20iterative%20development%20due%20to%20the%20high%20cost%20of%0Aconstructing%20datasets%20and%20the%20difficulty%20in%20isolating%20specific%20skills.%20In%20this%0Apaper%2C%20we%20propose%20VideoNIAH%20%28Video%20Needle%20In%20A%20Haystack%29%2C%20a%20benchmark%0Aconstruction%20framework%20through%20synthetic%20video%20generation.%20VideoNIAH%20decouples%0Avideo%20content%20from%20their%20query-responses%20by%20inserting%20unrelated%20visual%0A%27needles%27%20into%20original%20videos.%20The%20framework%20automates%20the%20generation%20of%0Aquery-response%20pairs%20using%20predefined%20rules%2C%20minimizing%20manual%20labor.%20The%0Aqueries%20focus%20on%20specific%20aspects%20of%20video%20understanding%2C%20enabling%20more%0Askill-specific%20evaluations.%20The%20separation%20between%20video%20content%20and%20the%0Aqueries%20also%20allow%20for%20increased%20video%20variety%20and%20evaluations%20across%20different%0Alengths.%20Utilizing%20VideoNIAH%2C%20we%20compile%20a%20video%20benchmark%20VNBench%2C%20which%0Aincludes%20tasks%20such%20as%20retrieval%2C%20ordering%2C%20and%20counting%20to%20evaluate%20three%20key%0Aaspects%20of%20video%20understanding%3A%20temporal%20perception%2C%20chronological%20ordering%2C%0Aand%20spatio-temporal%20coherence.%20We%20conduct%20a%20comprehensive%20evaluation%20of%20both%0Aproprietary%20and%20open-source%20models%2C%20uncovering%20significant%20differences%20in%20their%0Avideo%20understanding%20capabilities%20across%20various%20tasks.%20Additionally%2C%20we%20perform%0Aan%20in-depth%20analysis%20of%20the%20test%20results%20and%20model%20configurations.%20Based%20on%0Athese%20findings%2C%20we%20provide%20some%20advice%20for%20improving%20video%20MLLM%20training%2C%0Aoffering%20valuable%20insights%20to%20guide%20future%20research%20and%20model%20development.%20The%0Acode%20and%20data%20are%20available%20at%20https%3A//github.com/joez17/VideoNIAH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09367v2&entry.124074799=Read"},
{"title": "PCP-MAE: Learning to Predict Centers for Point Masked Autoencoders", "author": "Xiangdong Zhang and Shaofeng Zhang and Junchi Yan", "abstract": "  Masked autoencoder has been widely explored in point cloud self-supervised\nlearning, whereby the point cloud is generally divided into visible and masked\nparts. These methods typically include an encoder accepting visible patches\n(normalized) and corresponding patch centers (position) as input, with the\ndecoder accepting the output of the encoder and the centers (position) of the\nmasked parts to reconstruct each point in the masked patches. Then, the\npre-trained encoders are used for downstream tasks. In this paper, we show a\nmotivating empirical result that when directly feeding the centers of masked\npatches to the decoder without information from the encoder, it still\nreconstructs well. In other words, the centers of patches are important and the\nreconstruction objective does not necessarily rely on representations of the\nencoder, thus preventing the encoder from learning semantic representations.\nBased on this key observation, we propose a simple yet effective method, i.e.,\nlearning to Predict Centers for Point Masked AutoEncoders (PCP-MAE) which\nguides the model to learn to predict the significant centers and use the\npredicted centers to replace the directly provided centers. Specifically, we\npropose a Predicting Center Module (PCM) that shares parameters with the\noriginal encoder with extra cross-attention to predict centers. Our method is\nof high pre-training efficiency compared to other alternatives and achieves\ngreat improvement over Point-MAE, particularly surpassing it by 5.50% on\nOBJ-BG, 6.03% on OBJ-ONLY, and 5.17% on PB-T50-RS for 3D object classification\non the ScanObjectNN dataset. The code is available at\nhttps://github.com/aHapBean/PCP-MAE.\n", "link": "http://arxiv.org/abs/2408.08753v2", "date": "2024-10-24", "relevancy": 2.8739, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6545}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5364}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCP-MAE%3A%20Learning%20to%20Predict%20Centers%20for%20Point%20Masked%20Autoencoders&body=Title%3A%20PCP-MAE%3A%20Learning%20to%20Predict%20Centers%20for%20Point%20Masked%20Autoencoders%0AAuthor%3A%20Xiangdong%20Zhang%20and%20Shaofeng%20Zhang%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Masked%20autoencoder%20has%20been%20widely%20explored%20in%20point%20cloud%20self-supervised%0Alearning%2C%20whereby%20the%20point%20cloud%20is%20generally%20divided%20into%20visible%20and%20masked%0Aparts.%20These%20methods%20typically%20include%20an%20encoder%20accepting%20visible%20patches%0A%28normalized%29%20and%20corresponding%20patch%20centers%20%28position%29%20as%20input%2C%20with%20the%0Adecoder%20accepting%20the%20output%20of%20the%20encoder%20and%20the%20centers%20%28position%29%20of%20the%0Amasked%20parts%20to%20reconstruct%20each%20point%20in%20the%20masked%20patches.%20Then%2C%20the%0Apre-trained%20encoders%20are%20used%20for%20downstream%20tasks.%20In%20this%20paper%2C%20we%20show%20a%0Amotivating%20empirical%20result%20that%20when%20directly%20feeding%20the%20centers%20of%20masked%0Apatches%20to%20the%20decoder%20without%20information%20from%20the%20encoder%2C%20it%20still%0Areconstructs%20well.%20In%20other%20words%2C%20the%20centers%20of%20patches%20are%20important%20and%20the%0Areconstruction%20objective%20does%20not%20necessarily%20rely%20on%20representations%20of%20the%0Aencoder%2C%20thus%20preventing%20the%20encoder%20from%20learning%20semantic%20representations.%0ABased%20on%20this%20key%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20i.e.%2C%0Alearning%20to%20Predict%20Centers%20for%20Point%20Masked%20AutoEncoders%20%28PCP-MAE%29%20which%0Aguides%20the%20model%20to%20learn%20to%20predict%20the%20significant%20centers%20and%20use%20the%0Apredicted%20centers%20to%20replace%20the%20directly%20provided%20centers.%20Specifically%2C%20we%0Apropose%20a%20Predicting%20Center%20Module%20%28PCM%29%20that%20shares%20parameters%20with%20the%0Aoriginal%20encoder%20with%20extra%20cross-attention%20to%20predict%20centers.%20Our%20method%20is%0Aof%20high%20pre-training%20efficiency%20compared%20to%20other%20alternatives%20and%20achieves%0Agreat%20improvement%20over%20Point-MAE%2C%20particularly%20surpassing%20it%20by%205.50%25%20on%0AOBJ-BG%2C%206.03%25%20on%20OBJ-ONLY%2C%20and%205.17%25%20on%20PB-T50-RS%20for%203D%20object%20classification%0Aon%20the%20ScanObjectNN%20dataset.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/aHapBean/PCP-MAE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCP-MAE%253A%2520Learning%2520to%2520Predict%2520Centers%2520for%2520Point%2520Masked%2520Autoencoders%26entry.906535625%3DXiangdong%2520Zhang%2520and%2520Shaofeng%2520Zhang%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Masked%2520autoencoder%2520has%2520been%2520widely%2520explored%2520in%2520point%2520cloud%2520self-supervised%250Alearning%252C%2520whereby%2520the%2520point%2520cloud%2520is%2520generally%2520divided%2520into%2520visible%2520and%2520masked%250Aparts.%2520These%2520methods%2520typically%2520include%2520an%2520encoder%2520accepting%2520visible%2520patches%250A%2528normalized%2529%2520and%2520corresponding%2520patch%2520centers%2520%2528position%2529%2520as%2520input%252C%2520with%2520the%250Adecoder%2520accepting%2520the%2520output%2520of%2520the%2520encoder%2520and%2520the%2520centers%2520%2528position%2529%2520of%2520the%250Amasked%2520parts%2520to%2520reconstruct%2520each%2520point%2520in%2520the%2520masked%2520patches.%2520Then%252C%2520the%250Apre-trained%2520encoders%2520are%2520used%2520for%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520show%2520a%250Amotivating%2520empirical%2520result%2520that%2520when%2520directly%2520feeding%2520the%2520centers%2520of%2520masked%250Apatches%2520to%2520the%2520decoder%2520without%2520information%2520from%2520the%2520encoder%252C%2520it%2520still%250Areconstructs%2520well.%2520In%2520other%2520words%252C%2520the%2520centers%2520of%2520patches%2520are%2520important%2520and%2520the%250Areconstruction%2520objective%2520does%2520not%2520necessarily%2520rely%2520on%2520representations%2520of%2520the%250Aencoder%252C%2520thus%2520preventing%2520the%2520encoder%2520from%2520learning%2520semantic%2520representations.%250ABased%2520on%2520this%2520key%2520observation%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520method%252C%2520i.e.%252C%250Alearning%2520to%2520Predict%2520Centers%2520for%2520Point%2520Masked%2520AutoEncoders%2520%2528PCP-MAE%2529%2520which%250Aguides%2520the%2520model%2520to%2520learn%2520to%2520predict%2520the%2520significant%2520centers%2520and%2520use%2520the%250Apredicted%2520centers%2520to%2520replace%2520the%2520directly%2520provided%2520centers.%2520Specifically%252C%2520we%250Apropose%2520a%2520Predicting%2520Center%2520Module%2520%2528PCM%2529%2520that%2520shares%2520parameters%2520with%2520the%250Aoriginal%2520encoder%2520with%2520extra%2520cross-attention%2520to%2520predict%2520centers.%2520Our%2520method%2520is%250Aof%2520high%2520pre-training%2520efficiency%2520compared%2520to%2520other%2520alternatives%2520and%2520achieves%250Agreat%2520improvement%2520over%2520Point-MAE%252C%2520particularly%2520surpassing%2520it%2520by%25205.50%2525%2520on%250AOBJ-BG%252C%25206.03%2525%2520on%2520OBJ-ONLY%252C%2520and%25205.17%2525%2520on%2520PB-T50-RS%2520for%25203D%2520object%2520classification%250Aon%2520the%2520ScanObjectNN%2520dataset.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/aHapBean/PCP-MAE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCP-MAE%3A%20Learning%20to%20Predict%20Centers%20for%20Point%20Masked%20Autoencoders&entry.906535625=Xiangdong%20Zhang%20and%20Shaofeng%20Zhang%20and%20Junchi%20Yan&entry.1292438233=%20%20Masked%20autoencoder%20has%20been%20widely%20explored%20in%20point%20cloud%20self-supervised%0Alearning%2C%20whereby%20the%20point%20cloud%20is%20generally%20divided%20into%20visible%20and%20masked%0Aparts.%20These%20methods%20typically%20include%20an%20encoder%20accepting%20visible%20patches%0A%28normalized%29%20and%20corresponding%20patch%20centers%20%28position%29%20as%20input%2C%20with%20the%0Adecoder%20accepting%20the%20output%20of%20the%20encoder%20and%20the%20centers%20%28position%29%20of%20the%0Amasked%20parts%20to%20reconstruct%20each%20point%20in%20the%20masked%20patches.%20Then%2C%20the%0Apre-trained%20encoders%20are%20used%20for%20downstream%20tasks.%20In%20this%20paper%2C%20we%20show%20a%0Amotivating%20empirical%20result%20that%20when%20directly%20feeding%20the%20centers%20of%20masked%0Apatches%20to%20the%20decoder%20without%20information%20from%20the%20encoder%2C%20it%20still%0Areconstructs%20well.%20In%20other%20words%2C%20the%20centers%20of%20patches%20are%20important%20and%20the%0Areconstruction%20objective%20does%20not%20necessarily%20rely%20on%20representations%20of%20the%0Aencoder%2C%20thus%20preventing%20the%20encoder%20from%20learning%20semantic%20representations.%0ABased%20on%20this%20key%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20method%2C%20i.e.%2C%0Alearning%20to%20Predict%20Centers%20for%20Point%20Masked%20AutoEncoders%20%28PCP-MAE%29%20which%0Aguides%20the%20model%20to%20learn%20to%20predict%20the%20significant%20centers%20and%20use%20the%0Apredicted%20centers%20to%20replace%20the%20directly%20provided%20centers.%20Specifically%2C%20we%0Apropose%20a%20Predicting%20Center%20Module%20%28PCM%29%20that%20shares%20parameters%20with%20the%0Aoriginal%20encoder%20with%20extra%20cross-attention%20to%20predict%20centers.%20Our%20method%20is%0Aof%20high%20pre-training%20efficiency%20compared%20to%20other%20alternatives%20and%20achieves%0Agreat%20improvement%20over%20Point-MAE%2C%20particularly%20surpassing%20it%20by%205.50%25%20on%0AOBJ-BG%2C%206.03%25%20on%20OBJ-ONLY%2C%20and%205.17%25%20on%20PB-T50-RS%20for%203D%20object%20classification%0Aon%20the%20ScanObjectNN%20dataset.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/aHapBean/PCP-MAE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08753v2&entry.124074799=Read"},
{"title": "Learning Geodesics of Geometric Shape Deformations From Images", "author": "Nian Wu and Miaomiao Zhang", "abstract": "  This paper presents a novel method, named geodesic deformable networks (GDN),\nthat for the first time enables the learning of geodesic flows of deformation\nfields derived from images. In particular, the capability of our proposed GDN\nbeing able to predict geodesics is important for quantifying and comparing\ndeformable shape presented in images. The geodesic deformations, also known as\noptimal transformations that align pairwise images, are often parameterized by\na time sequence of smooth vector fields governed by nonlinear differential\nequations. A bountiful literature has been focusing on learning the initial\nconditions (e.g., initial velocity fields) based on registration networks.\nHowever, the definition of geodesics central to deformation-based shape\nanalysis is blind to the networks. To address this problem, we carefully\ndevelop an efficient neural operator to treat the geodesics as unknown mapping\nfunctions learned from the latent deformation spaces. A composition of integral\noperators and smooth activation functions is then formulated to effectively\napproximate such mappings. In contrast to previous works, our GDN jointly\noptimizes a newly defined geodesic loss, which adds additional benefits to\npromote the network regularizability and generalizability. We demonstrate the\neffectiveness of GDN on both 2D synthetic data and 3D real brain magnetic\nresonance imaging (MRI).\n", "link": "http://arxiv.org/abs/2410.18797v1", "date": "2024-10-24", "relevancy": 2.8522, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5852}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5633}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Geodesics%20of%20Geometric%20Shape%20Deformations%20From%20Images&body=Title%3A%20Learning%20Geodesics%20of%20Geometric%20Shape%20Deformations%20From%20Images%0AAuthor%3A%20Nian%20Wu%20and%20Miaomiao%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20method%2C%20named%20geodesic%20deformable%20networks%20%28GDN%29%2C%0Athat%20for%20the%20first%20time%20enables%20the%20learning%20of%20geodesic%20flows%20of%20deformation%0Afields%20derived%20from%20images.%20In%20particular%2C%20the%20capability%20of%20our%20proposed%20GDN%0Abeing%20able%20to%20predict%20geodesics%20is%20important%20for%20quantifying%20and%20comparing%0Adeformable%20shape%20presented%20in%20images.%20The%20geodesic%20deformations%2C%20also%20known%20as%0Aoptimal%20transformations%20that%20align%20pairwise%20images%2C%20are%20often%20parameterized%20by%0Aa%20time%20sequence%20of%20smooth%20vector%20fields%20governed%20by%20nonlinear%20differential%0Aequations.%20A%20bountiful%20literature%20has%20been%20focusing%20on%20learning%20the%20initial%0Aconditions%20%28e.g.%2C%20initial%20velocity%20fields%29%20based%20on%20registration%20networks.%0AHowever%2C%20the%20definition%20of%20geodesics%20central%20to%20deformation-based%20shape%0Aanalysis%20is%20blind%20to%20the%20networks.%20To%20address%20this%20problem%2C%20we%20carefully%0Adevelop%20an%20efficient%20neural%20operator%20to%20treat%20the%20geodesics%20as%20unknown%20mapping%0Afunctions%20learned%20from%20the%20latent%20deformation%20spaces.%20A%20composition%20of%20integral%0Aoperators%20and%20smooth%20activation%20functions%20is%20then%20formulated%20to%20effectively%0Aapproximate%20such%20mappings.%20In%20contrast%20to%20previous%20works%2C%20our%20GDN%20jointly%0Aoptimizes%20a%20newly%20defined%20geodesic%20loss%2C%20which%20adds%20additional%20benefits%20to%0Apromote%20the%20network%20regularizability%20and%20generalizability.%20We%20demonstrate%20the%0Aeffectiveness%20of%20GDN%20on%20both%202D%20synthetic%20data%20and%203D%20real%20brain%20magnetic%0Aresonance%20imaging%20%28MRI%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Geodesics%2520of%2520Geometric%2520Shape%2520Deformations%2520From%2520Images%26entry.906535625%3DNian%2520Wu%2520and%2520Miaomiao%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520method%252C%2520named%2520geodesic%2520deformable%2520networks%2520%2528GDN%2529%252C%250Athat%2520for%2520the%2520first%2520time%2520enables%2520the%2520learning%2520of%2520geodesic%2520flows%2520of%2520deformation%250Afields%2520derived%2520from%2520images.%2520In%2520particular%252C%2520the%2520capability%2520of%2520our%2520proposed%2520GDN%250Abeing%2520able%2520to%2520predict%2520geodesics%2520is%2520important%2520for%2520quantifying%2520and%2520comparing%250Adeformable%2520shape%2520presented%2520in%2520images.%2520The%2520geodesic%2520deformations%252C%2520also%2520known%2520as%250Aoptimal%2520transformations%2520that%2520align%2520pairwise%2520images%252C%2520are%2520often%2520parameterized%2520by%250Aa%2520time%2520sequence%2520of%2520smooth%2520vector%2520fields%2520governed%2520by%2520nonlinear%2520differential%250Aequations.%2520A%2520bountiful%2520literature%2520has%2520been%2520focusing%2520on%2520learning%2520the%2520initial%250Aconditions%2520%2528e.g.%252C%2520initial%2520velocity%2520fields%2529%2520based%2520on%2520registration%2520networks.%250AHowever%252C%2520the%2520definition%2520of%2520geodesics%2520central%2520to%2520deformation-based%2520shape%250Aanalysis%2520is%2520blind%2520to%2520the%2520networks.%2520To%2520address%2520this%2520problem%252C%2520we%2520carefully%250Adevelop%2520an%2520efficient%2520neural%2520operator%2520to%2520treat%2520the%2520geodesics%2520as%2520unknown%2520mapping%250Afunctions%2520learned%2520from%2520the%2520latent%2520deformation%2520spaces.%2520A%2520composition%2520of%2520integral%250Aoperators%2520and%2520smooth%2520activation%2520functions%2520is%2520then%2520formulated%2520to%2520effectively%250Aapproximate%2520such%2520mappings.%2520In%2520contrast%2520to%2520previous%2520works%252C%2520our%2520GDN%2520jointly%250Aoptimizes%2520a%2520newly%2520defined%2520geodesic%2520loss%252C%2520which%2520adds%2520additional%2520benefits%2520to%250Apromote%2520the%2520network%2520regularizability%2520and%2520generalizability.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520GDN%2520on%2520both%25202D%2520synthetic%2520data%2520and%25203D%2520real%2520brain%2520magnetic%250Aresonance%2520imaging%2520%2528MRI%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Geodesics%20of%20Geometric%20Shape%20Deformations%20From%20Images&entry.906535625=Nian%20Wu%20and%20Miaomiao%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20method%2C%20named%20geodesic%20deformable%20networks%20%28GDN%29%2C%0Athat%20for%20the%20first%20time%20enables%20the%20learning%20of%20geodesic%20flows%20of%20deformation%0Afields%20derived%20from%20images.%20In%20particular%2C%20the%20capability%20of%20our%20proposed%20GDN%0Abeing%20able%20to%20predict%20geodesics%20is%20important%20for%20quantifying%20and%20comparing%0Adeformable%20shape%20presented%20in%20images.%20The%20geodesic%20deformations%2C%20also%20known%20as%0Aoptimal%20transformations%20that%20align%20pairwise%20images%2C%20are%20often%20parameterized%20by%0Aa%20time%20sequence%20of%20smooth%20vector%20fields%20governed%20by%20nonlinear%20differential%0Aequations.%20A%20bountiful%20literature%20has%20been%20focusing%20on%20learning%20the%20initial%0Aconditions%20%28e.g.%2C%20initial%20velocity%20fields%29%20based%20on%20registration%20networks.%0AHowever%2C%20the%20definition%20of%20geodesics%20central%20to%20deformation-based%20shape%0Aanalysis%20is%20blind%20to%20the%20networks.%20To%20address%20this%20problem%2C%20we%20carefully%0Adevelop%20an%20efficient%20neural%20operator%20to%20treat%20the%20geodesics%20as%20unknown%20mapping%0Afunctions%20learned%20from%20the%20latent%20deformation%20spaces.%20A%20composition%20of%20integral%0Aoperators%20and%20smooth%20activation%20functions%20is%20then%20formulated%20to%20effectively%0Aapproximate%20such%20mappings.%20In%20contrast%20to%20previous%20works%2C%20our%20GDN%20jointly%0Aoptimizes%20a%20newly%20defined%20geodesic%20loss%2C%20which%20adds%20additional%20benefits%20to%0Apromote%20the%20network%20regularizability%20and%20generalizability.%20We%20demonstrate%20the%0Aeffectiveness%20of%20GDN%20on%20both%202D%20synthetic%20data%20and%203D%20real%20brain%20magnetic%0Aresonance%20imaging%20%28MRI%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18797v1&entry.124074799=Read"},
{"title": "FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation", "author": "Christopher T. H Teo and Milad Abdollahzadeh and Xinda Ma and Ngai-man Cheung", "abstract": "  Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair\ntext-to-image (T2I) generation. Specifically, this approach leverages readily\navailable reference images to learn inclusive prompts for each target Sensitive\nAttribute (tSA), allowing for fair image generation. In this work, we first\nreveal that this prompt learning-based approach results in degraded sample\nquality. Our analysis shows that the approach's training objective -- which\naims to align the embedding differences of learned prompts and reference images\n-- could be sub-optimal, resulting in distortion of the learned prompts and\ndegraded generated images. To further substantiate this claim, as our major\ncontribution, we deep dive into the denoising subnetwork of the T2I model to\ntrack down the effect of these learned prompts by analyzing the cross-attention\nmaps. In our analysis, we propose a novel prompt switching analysis: I2H and\nH2I. Furthermore, we propose new quantitative characterization of\ncross-attention maps. Our analysis reveals abnormalities in the early denoising\nsteps, perpetuating improper global structure that results in degradation in\nthe generated samples. Building on insights from our analysis, we propose two\nideas: (i) Prompt Queuing and (ii) Attention Amplification to address the\nquality issue. Extensive experimental results on a wide range of tSAs show that\nour proposed method outperforms SOTA approach's image generation quality, while\nachieving competitive fairness. More resources at FairQueue Project site:\nhttps://sutd-visual-computing-group.github.io/FairQueue\n", "link": "http://arxiv.org/abs/2410.18615v1", "date": "2024-10-24", "relevancy": 2.8358, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5718}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5714}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairQueue%3A%20Rethinking%20Prompt%20Learning%20for%20Fair%20Text-to-Image%20Generation&body=Title%3A%20FairQueue%3A%20Rethinking%20Prompt%20Learning%20for%20Fair%20Text-to-Image%20Generation%0AAuthor%3A%20Christopher%20T.%20H%20Teo%20and%20Milad%20Abdollahzadeh%20and%20Xinda%20Ma%20and%20Ngai-man%20Cheung%0AAbstract%3A%20%20%20Recently%2C%20prompt%20learning%20has%20emerged%20as%20the%20state-of-the-art%20%28SOTA%29%20for%20fair%0Atext-to-image%20%28T2I%29%20generation.%20Specifically%2C%20this%20approach%20leverages%20readily%0Aavailable%20reference%20images%20to%20learn%20inclusive%20prompts%20for%20each%20target%20Sensitive%0AAttribute%20%28tSA%29%2C%20allowing%20for%20fair%20image%20generation.%20In%20this%20work%2C%20we%20first%0Areveal%20that%20this%20prompt%20learning-based%20approach%20results%20in%20degraded%20sample%0Aquality.%20Our%20analysis%20shows%20that%20the%20approach%27s%20training%20objective%20--%20which%0Aaims%20to%20align%20the%20embedding%20differences%20of%20learned%20prompts%20and%20reference%20images%0A--%20could%20be%20sub-optimal%2C%20resulting%20in%20distortion%20of%20the%20learned%20prompts%20and%0Adegraded%20generated%20images.%20To%20further%20substantiate%20this%20claim%2C%20as%20our%20major%0Acontribution%2C%20we%20deep%20dive%20into%20the%20denoising%20subnetwork%20of%20the%20T2I%20model%20to%0Atrack%20down%20the%20effect%20of%20these%20learned%20prompts%20by%20analyzing%20the%20cross-attention%0Amaps.%20In%20our%20analysis%2C%20we%20propose%20a%20novel%20prompt%20switching%20analysis%3A%20I2H%20and%0AH2I.%20Furthermore%2C%20we%20propose%20new%20quantitative%20characterization%20of%0Across-attention%20maps.%20Our%20analysis%20reveals%20abnormalities%20in%20the%20early%20denoising%0Asteps%2C%20perpetuating%20improper%20global%20structure%20that%20results%20in%20degradation%20in%0Athe%20generated%20samples.%20Building%20on%20insights%20from%20our%20analysis%2C%20we%20propose%20two%0Aideas%3A%20%28i%29%20Prompt%20Queuing%20and%20%28ii%29%20Attention%20Amplification%20to%20address%20the%0Aquality%20issue.%20Extensive%20experimental%20results%20on%20a%20wide%20range%20of%20tSAs%20show%20that%0Aour%20proposed%20method%20outperforms%20SOTA%20approach%27s%20image%20generation%20quality%2C%20while%0Aachieving%20competitive%20fairness.%20More%20resources%20at%20FairQueue%20Project%20site%3A%0Ahttps%3A//sutd-visual-computing-group.github.io/FairQueue%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairQueue%253A%2520Rethinking%2520Prompt%2520Learning%2520for%2520Fair%2520Text-to-Image%2520Generation%26entry.906535625%3DChristopher%2520T.%2520H%2520Teo%2520and%2520Milad%2520Abdollahzadeh%2520and%2520Xinda%2520Ma%2520and%2520Ngai-man%2520Cheung%26entry.1292438233%3D%2520%2520Recently%252C%2520prompt%2520learning%2520has%2520emerged%2520as%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520for%2520fair%250Atext-to-image%2520%2528T2I%2529%2520generation.%2520Specifically%252C%2520this%2520approach%2520leverages%2520readily%250Aavailable%2520reference%2520images%2520to%2520learn%2520inclusive%2520prompts%2520for%2520each%2520target%2520Sensitive%250AAttribute%2520%2528tSA%2529%252C%2520allowing%2520for%2520fair%2520image%2520generation.%2520In%2520this%2520work%252C%2520we%2520first%250Areveal%2520that%2520this%2520prompt%2520learning-based%2520approach%2520results%2520in%2520degraded%2520sample%250Aquality.%2520Our%2520analysis%2520shows%2520that%2520the%2520approach%2527s%2520training%2520objective%2520--%2520which%250Aaims%2520to%2520align%2520the%2520embedding%2520differences%2520of%2520learned%2520prompts%2520and%2520reference%2520images%250A--%2520could%2520be%2520sub-optimal%252C%2520resulting%2520in%2520distortion%2520of%2520the%2520learned%2520prompts%2520and%250Adegraded%2520generated%2520images.%2520To%2520further%2520substantiate%2520this%2520claim%252C%2520as%2520our%2520major%250Acontribution%252C%2520we%2520deep%2520dive%2520into%2520the%2520denoising%2520subnetwork%2520of%2520the%2520T2I%2520model%2520to%250Atrack%2520down%2520the%2520effect%2520of%2520these%2520learned%2520prompts%2520by%2520analyzing%2520the%2520cross-attention%250Amaps.%2520In%2520our%2520analysis%252C%2520we%2520propose%2520a%2520novel%2520prompt%2520switching%2520analysis%253A%2520I2H%2520and%250AH2I.%2520Furthermore%252C%2520we%2520propose%2520new%2520quantitative%2520characterization%2520of%250Across-attention%2520maps.%2520Our%2520analysis%2520reveals%2520abnormalities%2520in%2520the%2520early%2520denoising%250Asteps%252C%2520perpetuating%2520improper%2520global%2520structure%2520that%2520results%2520in%2520degradation%2520in%250Athe%2520generated%2520samples.%2520Building%2520on%2520insights%2520from%2520our%2520analysis%252C%2520we%2520propose%2520two%250Aideas%253A%2520%2528i%2529%2520Prompt%2520Queuing%2520and%2520%2528ii%2529%2520Attention%2520Amplification%2520to%2520address%2520the%250Aquality%2520issue.%2520Extensive%2520experimental%2520results%2520on%2520a%2520wide%2520range%2520of%2520tSAs%2520show%2520that%250Aour%2520proposed%2520method%2520outperforms%2520SOTA%2520approach%2527s%2520image%2520generation%2520quality%252C%2520while%250Aachieving%2520competitive%2520fairness.%2520More%2520resources%2520at%2520FairQueue%2520Project%2520site%253A%250Ahttps%253A//sutd-visual-computing-group.github.io/FairQueue%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairQueue%3A%20Rethinking%20Prompt%20Learning%20for%20Fair%20Text-to-Image%20Generation&entry.906535625=Christopher%20T.%20H%20Teo%20and%20Milad%20Abdollahzadeh%20and%20Xinda%20Ma%20and%20Ngai-man%20Cheung&entry.1292438233=%20%20Recently%2C%20prompt%20learning%20has%20emerged%20as%20the%20state-of-the-art%20%28SOTA%29%20for%20fair%0Atext-to-image%20%28T2I%29%20generation.%20Specifically%2C%20this%20approach%20leverages%20readily%0Aavailable%20reference%20images%20to%20learn%20inclusive%20prompts%20for%20each%20target%20Sensitive%0AAttribute%20%28tSA%29%2C%20allowing%20for%20fair%20image%20generation.%20In%20this%20work%2C%20we%20first%0Areveal%20that%20this%20prompt%20learning-based%20approach%20results%20in%20degraded%20sample%0Aquality.%20Our%20analysis%20shows%20that%20the%20approach%27s%20training%20objective%20--%20which%0Aaims%20to%20align%20the%20embedding%20differences%20of%20learned%20prompts%20and%20reference%20images%0A--%20could%20be%20sub-optimal%2C%20resulting%20in%20distortion%20of%20the%20learned%20prompts%20and%0Adegraded%20generated%20images.%20To%20further%20substantiate%20this%20claim%2C%20as%20our%20major%0Acontribution%2C%20we%20deep%20dive%20into%20the%20denoising%20subnetwork%20of%20the%20T2I%20model%20to%0Atrack%20down%20the%20effect%20of%20these%20learned%20prompts%20by%20analyzing%20the%20cross-attention%0Amaps.%20In%20our%20analysis%2C%20we%20propose%20a%20novel%20prompt%20switching%20analysis%3A%20I2H%20and%0AH2I.%20Furthermore%2C%20we%20propose%20new%20quantitative%20characterization%20of%0Across-attention%20maps.%20Our%20analysis%20reveals%20abnormalities%20in%20the%20early%20denoising%0Asteps%2C%20perpetuating%20improper%20global%20structure%20that%20results%20in%20degradation%20in%0Athe%20generated%20samples.%20Building%20on%20insights%20from%20our%20analysis%2C%20we%20propose%20two%0Aideas%3A%20%28i%29%20Prompt%20Queuing%20and%20%28ii%29%20Attention%20Amplification%20to%20address%20the%0Aquality%20issue.%20Extensive%20experimental%20results%20on%20a%20wide%20range%20of%20tSAs%20show%20that%0Aour%20proposed%20method%20outperforms%20SOTA%20approach%27s%20image%20generation%20quality%2C%20while%0Aachieving%20competitive%20fairness.%20More%20resources%20at%20FairQueue%20Project%20site%3A%0Ahttps%3A//sutd-visual-computing-group.github.io/FairQueue%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18615v1&entry.124074799=Read"},
{"title": "PointPatchRL -- Masked Reconstruction Improves Reinforcement Learning on\n  Point Clouds", "author": "Bal\u00e1zs Gyenes and Nikolai Franke and Philipp Becker and Gerhard Neumann", "abstract": "  Perceiving the environment via cameras is crucial for Reinforcement Learning\n(RL) in robotics. While images are a convenient form of representation, they\noften complicate extracting important geometric details, especially with\nvarying geometries or deformable objects. In contrast, point clouds naturally\nrepresent this geometry and easily integrate color and positional data from\nmultiple camera views. However, while deep learning on point clouds has seen\nmany recent successes, RL on point clouds is under-researched, with only the\nsimplest encoder architecture considered in the literature. We introduce\nPointPatchRL (PPRL), a method for RL on point clouds that builds on the common\nparadigm of dividing point clouds into overlapping patches, tokenizing them,\nand processing the tokens with transformers. PPRL provides significant\nimprovements compared with other point-cloud processing architectures\npreviously used for RL. We then complement PPRL with masked reconstruction for\nrepresentation learning and show that our method outperforms strong model-free\nand model-based baselines on image observations in complex manipulation tasks\ncontaining deformable objects and variations in target object geometry. Videos\nand code are available at https://alrhub.github.io/pprl-website\n", "link": "http://arxiv.org/abs/2410.18800v1", "date": "2024-10-24", "relevancy": 2.791, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5644}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5554}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointPatchRL%20--%20Masked%20Reconstruction%20Improves%20Reinforcement%20Learning%20on%0A%20%20Point%20Clouds&body=Title%3A%20PointPatchRL%20--%20Masked%20Reconstruction%20Improves%20Reinforcement%20Learning%20on%0A%20%20Point%20Clouds%0AAuthor%3A%20Bal%C3%A1zs%20Gyenes%20and%20Nikolai%20Franke%20and%20Philipp%20Becker%20and%20Gerhard%20Neumann%0AAbstract%3A%20%20%20Perceiving%20the%20environment%20via%20cameras%20is%20crucial%20for%20Reinforcement%20Learning%0A%28RL%29%20in%20robotics.%20While%20images%20are%20a%20convenient%20form%20of%20representation%2C%20they%0Aoften%20complicate%20extracting%20important%20geometric%20details%2C%20especially%20with%0Avarying%20geometries%20or%20deformable%20objects.%20In%20contrast%2C%20point%20clouds%20naturally%0Arepresent%20this%20geometry%20and%20easily%20integrate%20color%20and%20positional%20data%20from%0Amultiple%20camera%20views.%20However%2C%20while%20deep%20learning%20on%20point%20clouds%20has%20seen%0Amany%20recent%20successes%2C%20RL%20on%20point%20clouds%20is%20under-researched%2C%20with%20only%20the%0Asimplest%20encoder%20architecture%20considered%20in%20the%20literature.%20We%20introduce%0APointPatchRL%20%28PPRL%29%2C%20a%20method%20for%20RL%20on%20point%20clouds%20that%20builds%20on%20the%20common%0Aparadigm%20of%20dividing%20point%20clouds%20into%20overlapping%20patches%2C%20tokenizing%20them%2C%0Aand%20processing%20the%20tokens%20with%20transformers.%20PPRL%20provides%20significant%0Aimprovements%20compared%20with%20other%20point-cloud%20processing%20architectures%0Apreviously%20used%20for%20RL.%20We%20then%20complement%20PPRL%20with%20masked%20reconstruction%20for%0Arepresentation%20learning%20and%20show%20that%20our%20method%20outperforms%20strong%20model-free%0Aand%20model-based%20baselines%20on%20image%20observations%20in%20complex%20manipulation%20tasks%0Acontaining%20deformable%20objects%20and%20variations%20in%20target%20object%20geometry.%20Videos%0Aand%20code%20are%20available%20at%20https%3A//alrhub.github.io/pprl-website%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointPatchRL%2520--%2520Masked%2520Reconstruction%2520Improves%2520Reinforcement%2520Learning%2520on%250A%2520%2520Point%2520Clouds%26entry.906535625%3DBal%25C3%25A1zs%2520Gyenes%2520and%2520Nikolai%2520Franke%2520and%2520Philipp%2520Becker%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3D%2520%2520Perceiving%2520the%2520environment%2520via%2520cameras%2520is%2520crucial%2520for%2520Reinforcement%2520Learning%250A%2528RL%2529%2520in%2520robotics.%2520While%2520images%2520are%2520a%2520convenient%2520form%2520of%2520representation%252C%2520they%250Aoften%2520complicate%2520extracting%2520important%2520geometric%2520details%252C%2520especially%2520with%250Avarying%2520geometries%2520or%2520deformable%2520objects.%2520In%2520contrast%252C%2520point%2520clouds%2520naturally%250Arepresent%2520this%2520geometry%2520and%2520easily%2520integrate%2520color%2520and%2520positional%2520data%2520from%250Amultiple%2520camera%2520views.%2520However%252C%2520while%2520deep%2520learning%2520on%2520point%2520clouds%2520has%2520seen%250Amany%2520recent%2520successes%252C%2520RL%2520on%2520point%2520clouds%2520is%2520under-researched%252C%2520with%2520only%2520the%250Asimplest%2520encoder%2520architecture%2520considered%2520in%2520the%2520literature.%2520We%2520introduce%250APointPatchRL%2520%2528PPRL%2529%252C%2520a%2520method%2520for%2520RL%2520on%2520point%2520clouds%2520that%2520builds%2520on%2520the%2520common%250Aparadigm%2520of%2520dividing%2520point%2520clouds%2520into%2520overlapping%2520patches%252C%2520tokenizing%2520them%252C%250Aand%2520processing%2520the%2520tokens%2520with%2520transformers.%2520PPRL%2520provides%2520significant%250Aimprovements%2520compared%2520with%2520other%2520point-cloud%2520processing%2520architectures%250Apreviously%2520used%2520for%2520RL.%2520We%2520then%2520complement%2520PPRL%2520with%2520masked%2520reconstruction%2520for%250Arepresentation%2520learning%2520and%2520show%2520that%2520our%2520method%2520outperforms%2520strong%2520model-free%250Aand%2520model-based%2520baselines%2520on%2520image%2520observations%2520in%2520complex%2520manipulation%2520tasks%250Acontaining%2520deformable%2520objects%2520and%2520variations%2520in%2520target%2520object%2520geometry.%2520Videos%250Aand%2520code%2520are%2520available%2520at%2520https%253A//alrhub.github.io/pprl-website%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointPatchRL%20--%20Masked%20Reconstruction%20Improves%20Reinforcement%20Learning%20on%0A%20%20Point%20Clouds&entry.906535625=Bal%C3%A1zs%20Gyenes%20and%20Nikolai%20Franke%20and%20Philipp%20Becker%20and%20Gerhard%20Neumann&entry.1292438233=%20%20Perceiving%20the%20environment%20via%20cameras%20is%20crucial%20for%20Reinforcement%20Learning%0A%28RL%29%20in%20robotics.%20While%20images%20are%20a%20convenient%20form%20of%20representation%2C%20they%0Aoften%20complicate%20extracting%20important%20geometric%20details%2C%20especially%20with%0Avarying%20geometries%20or%20deformable%20objects.%20In%20contrast%2C%20point%20clouds%20naturally%0Arepresent%20this%20geometry%20and%20easily%20integrate%20color%20and%20positional%20data%20from%0Amultiple%20camera%20views.%20However%2C%20while%20deep%20learning%20on%20point%20clouds%20has%20seen%0Amany%20recent%20successes%2C%20RL%20on%20point%20clouds%20is%20under-researched%2C%20with%20only%20the%0Asimplest%20encoder%20architecture%20considered%20in%20the%20literature.%20We%20introduce%0APointPatchRL%20%28PPRL%29%2C%20a%20method%20for%20RL%20on%20point%20clouds%20that%20builds%20on%20the%20common%0Aparadigm%20of%20dividing%20point%20clouds%20into%20overlapping%20patches%2C%20tokenizing%20them%2C%0Aand%20processing%20the%20tokens%20with%20transformers.%20PPRL%20provides%20significant%0Aimprovements%20compared%20with%20other%20point-cloud%20processing%20architectures%0Apreviously%20used%20for%20RL.%20We%20then%20complement%20PPRL%20with%20masked%20reconstruction%20for%0Arepresentation%20learning%20and%20show%20that%20our%20method%20outperforms%20strong%20model-free%0Aand%20model-based%20baselines%20on%20image%20observations%20in%20complex%20manipulation%20tasks%0Acontaining%20deformable%20objects%20and%20variations%20in%20target%20object%20geometry.%20Videos%0Aand%20code%20are%20available%20at%20https%3A//alrhub.github.io/pprl-website%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18800v1&entry.124074799=Read"},
{"title": "Highly efficient non-rigid registration in k-space with application to\n  cardiac Magnetic Resonance Imaging", "author": "Aya Ghoul and Kerstin Hammernik and Andreas Lingg and Patrick Krumm and Daniel Rueckert and Sergios Gatidis and Thomas K\u00fcstner", "abstract": "  In Magnetic Resonance Imaging (MRI), high temporal-resolved motion can be\nuseful for image acquisition and reconstruction, MR-guided radiotherapy,\ndynamic contrast-enhancement, flow and perfusion imaging, and functional\nassessment of motion patterns in cardiovascular, abdominal, peristaltic, fetal,\nor musculoskeletal imaging. Conventionally, these motion estimates are derived\nthrough image-based registration, a particularly challenging task for complex\nmotion patterns and high dynamic resolution. The accelerated scans in such\napplications result in imaging artifacts that compromise the motion estimation.\nIn this work, we propose a novel self-supervised deep learning-based framework,\ndubbed the Local-All Pass Attention Network (LAPANet), for non-rigid motion\nestimation directly from the acquired accelerated Fourier space, i.e. k-space.\nThe proposed approach models non-rigid motion as the cumulative sum of local\ntranslational displacements, following the Local All-Pass (LAP) registration\ntechnique. LAPANet was evaluated on cardiac motion estimation across various\nsampling trajectories and acceleration rates. Our results demonstrate superior\naccuracy compared to prior conventional and deep learning-based registration\nmethods, accommodating as few as 2 lines/frame in a Cartesian trajectory and 3\nspokes/frame in a non-Cartesian trajectory. The achieved high temporal\nresolution (less than 5 ms) for non-rigid motion opens new avenues for motion\ndetection, tracking and correction in dynamic and real-time MRI applications.\n", "link": "http://arxiv.org/abs/2410.18834v1", "date": "2024-10-24", "relevancy": 2.7694, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5759}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5703}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Highly%20efficient%20non-rigid%20registration%20in%20k-space%20with%20application%20to%0A%20%20cardiac%20Magnetic%20Resonance%20Imaging&body=Title%3A%20Highly%20efficient%20non-rigid%20registration%20in%20k-space%20with%20application%20to%0A%20%20cardiac%20Magnetic%20Resonance%20Imaging%0AAuthor%3A%20Aya%20Ghoul%20and%20Kerstin%20Hammernik%20and%20Andreas%20Lingg%20and%20Patrick%20Krumm%20and%20Daniel%20Rueckert%20and%20Sergios%20Gatidis%20and%20Thomas%20K%C3%BCstner%0AAbstract%3A%20%20%20In%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%20high%20temporal-resolved%20motion%20can%20be%0Auseful%20for%20image%20acquisition%20and%20reconstruction%2C%20MR-guided%20radiotherapy%2C%0Adynamic%20contrast-enhancement%2C%20flow%20and%20perfusion%20imaging%2C%20and%20functional%0Aassessment%20of%20motion%20patterns%20in%20cardiovascular%2C%20abdominal%2C%20peristaltic%2C%20fetal%2C%0Aor%20musculoskeletal%20imaging.%20Conventionally%2C%20these%20motion%20estimates%20are%20derived%0Athrough%20image-based%20registration%2C%20a%20particularly%20challenging%20task%20for%20complex%0Amotion%20patterns%20and%20high%20dynamic%20resolution.%20The%20accelerated%20scans%20in%20such%0Aapplications%20result%20in%20imaging%20artifacts%20that%20compromise%20the%20motion%20estimation.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20self-supervised%20deep%20learning-based%20framework%2C%0Adubbed%20the%20Local-All%20Pass%20Attention%20Network%20%28LAPANet%29%2C%20for%20non-rigid%20motion%0Aestimation%20directly%20from%20the%20acquired%20accelerated%20Fourier%20space%2C%20i.e.%20k-space.%0AThe%20proposed%20approach%20models%20non-rigid%20motion%20as%20the%20cumulative%20sum%20of%20local%0Atranslational%20displacements%2C%20following%20the%20Local%20All-Pass%20%28LAP%29%20registration%0Atechnique.%20LAPANet%20was%20evaluated%20on%20cardiac%20motion%20estimation%20across%20various%0Asampling%20trajectories%20and%20acceleration%20rates.%20Our%20results%20demonstrate%20superior%0Aaccuracy%20compared%20to%20prior%20conventional%20and%20deep%20learning-based%20registration%0Amethods%2C%20accommodating%20as%20few%20as%202%20lines/frame%20in%20a%20Cartesian%20trajectory%20and%203%0Aspokes/frame%20in%20a%20non-Cartesian%20trajectory.%20The%20achieved%20high%20temporal%0Aresolution%20%28less%20than%205%20ms%29%20for%20non-rigid%20motion%20opens%20new%20avenues%20for%20motion%0Adetection%2C%20tracking%20and%20correction%20in%20dynamic%20and%20real-time%20MRI%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHighly%2520efficient%2520non-rigid%2520registration%2520in%2520k-space%2520with%2520application%2520to%250A%2520%2520cardiac%2520Magnetic%2520Resonance%2520Imaging%26entry.906535625%3DAya%2520Ghoul%2520and%2520Kerstin%2520Hammernik%2520and%2520Andreas%2520Lingg%2520and%2520Patrick%2520Krumm%2520and%2520Daniel%2520Rueckert%2520and%2520Sergios%2520Gatidis%2520and%2520Thomas%2520K%25C3%25BCstner%26entry.1292438233%3D%2520%2520In%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%252C%2520high%2520temporal-resolved%2520motion%2520can%2520be%250Auseful%2520for%2520image%2520acquisition%2520and%2520reconstruction%252C%2520MR-guided%2520radiotherapy%252C%250Adynamic%2520contrast-enhancement%252C%2520flow%2520and%2520perfusion%2520imaging%252C%2520and%2520functional%250Aassessment%2520of%2520motion%2520patterns%2520in%2520cardiovascular%252C%2520abdominal%252C%2520peristaltic%252C%2520fetal%252C%250Aor%2520musculoskeletal%2520imaging.%2520Conventionally%252C%2520these%2520motion%2520estimates%2520are%2520derived%250Athrough%2520image-based%2520registration%252C%2520a%2520particularly%2520challenging%2520task%2520for%2520complex%250Amotion%2520patterns%2520and%2520high%2520dynamic%2520resolution.%2520The%2520accelerated%2520scans%2520in%2520such%250Aapplications%2520result%2520in%2520imaging%2520artifacts%2520that%2520compromise%2520the%2520motion%2520estimation.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520self-supervised%2520deep%2520learning-based%2520framework%252C%250Adubbed%2520the%2520Local-All%2520Pass%2520Attention%2520Network%2520%2528LAPANet%2529%252C%2520for%2520non-rigid%2520motion%250Aestimation%2520directly%2520from%2520the%2520acquired%2520accelerated%2520Fourier%2520space%252C%2520i.e.%2520k-space.%250AThe%2520proposed%2520approach%2520models%2520non-rigid%2520motion%2520as%2520the%2520cumulative%2520sum%2520of%2520local%250Atranslational%2520displacements%252C%2520following%2520the%2520Local%2520All-Pass%2520%2528LAP%2529%2520registration%250Atechnique.%2520LAPANet%2520was%2520evaluated%2520on%2520cardiac%2520motion%2520estimation%2520across%2520various%250Asampling%2520trajectories%2520and%2520acceleration%2520rates.%2520Our%2520results%2520demonstrate%2520superior%250Aaccuracy%2520compared%2520to%2520prior%2520conventional%2520and%2520deep%2520learning-based%2520registration%250Amethods%252C%2520accommodating%2520as%2520few%2520as%25202%2520lines/frame%2520in%2520a%2520Cartesian%2520trajectory%2520and%25203%250Aspokes/frame%2520in%2520a%2520non-Cartesian%2520trajectory.%2520The%2520achieved%2520high%2520temporal%250Aresolution%2520%2528less%2520than%25205%2520ms%2529%2520for%2520non-rigid%2520motion%2520opens%2520new%2520avenues%2520for%2520motion%250Adetection%252C%2520tracking%2520and%2520correction%2520in%2520dynamic%2520and%2520real-time%2520MRI%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Highly%20efficient%20non-rigid%20registration%20in%20k-space%20with%20application%20to%0A%20%20cardiac%20Magnetic%20Resonance%20Imaging&entry.906535625=Aya%20Ghoul%20and%20Kerstin%20Hammernik%20and%20Andreas%20Lingg%20and%20Patrick%20Krumm%20and%20Daniel%20Rueckert%20and%20Sergios%20Gatidis%20and%20Thomas%20K%C3%BCstner&entry.1292438233=%20%20In%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%20high%20temporal-resolved%20motion%20can%20be%0Auseful%20for%20image%20acquisition%20and%20reconstruction%2C%20MR-guided%20radiotherapy%2C%0Adynamic%20contrast-enhancement%2C%20flow%20and%20perfusion%20imaging%2C%20and%20functional%0Aassessment%20of%20motion%20patterns%20in%20cardiovascular%2C%20abdominal%2C%20peristaltic%2C%20fetal%2C%0Aor%20musculoskeletal%20imaging.%20Conventionally%2C%20these%20motion%20estimates%20are%20derived%0Athrough%20image-based%20registration%2C%20a%20particularly%20challenging%20task%20for%20complex%0Amotion%20patterns%20and%20high%20dynamic%20resolution.%20The%20accelerated%20scans%20in%20such%0Aapplications%20result%20in%20imaging%20artifacts%20that%20compromise%20the%20motion%20estimation.%0AIn%20this%20work%2C%20we%20propose%20a%20novel%20self-supervised%20deep%20learning-based%20framework%2C%0Adubbed%20the%20Local-All%20Pass%20Attention%20Network%20%28LAPANet%29%2C%20for%20non-rigid%20motion%0Aestimation%20directly%20from%20the%20acquired%20accelerated%20Fourier%20space%2C%20i.e.%20k-space.%0AThe%20proposed%20approach%20models%20non-rigid%20motion%20as%20the%20cumulative%20sum%20of%20local%0Atranslational%20displacements%2C%20following%20the%20Local%20All-Pass%20%28LAP%29%20registration%0Atechnique.%20LAPANet%20was%20evaluated%20on%20cardiac%20motion%20estimation%20across%20various%0Asampling%20trajectories%20and%20acceleration%20rates.%20Our%20results%20demonstrate%20superior%0Aaccuracy%20compared%20to%20prior%20conventional%20and%20deep%20learning-based%20registration%0Amethods%2C%20accommodating%20as%20few%20as%202%20lines/frame%20in%20a%20Cartesian%20trajectory%20and%203%0Aspokes/frame%20in%20a%20non-Cartesian%20trajectory.%20The%20achieved%20high%20temporal%0Aresolution%20%28less%20than%205%20ms%29%20for%20non-rigid%20motion%20opens%20new%20avenues%20for%20motion%0Adetection%2C%20tracking%20and%20correction%20in%20dynamic%20and%20real-time%20MRI%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18834v1&entry.124074799=Read"},
{"title": "3D Shape Completion with Test-Time Training", "author": "Michael Schopf-Kuester and Zorah L\u00e4hner and Michael Moeller", "abstract": "  This work addresses the problem of \\textit{shape completion}, i.e., the task\nof restoring incomplete shapes by predicting their missing parts. While\nprevious works have often predicted the fractured and restored shape in one\nstep, we approach the task by separately predicting the fractured and newly\nrestored parts, but ensuring these predictions are interconnected. We use a\ndecoder network motivated by related work on the prediction of signed distance\nfunctions (DeepSDF). In particular, our representation allows us to consider\ntest-time-training, i.e., finetuning network parameters to match the given\nincomplete shape more accurately during inference. While previous works often\nhave difficulties with artifacts around the fracture boundary, we demonstrate\nthat our overfitting to the fractured parts leads to significant improvements\nin the restoration of eight different shape categories of the ShapeNet data set\nin terms of their chamfer distances.\n", "link": "http://arxiv.org/abs/2410.18668v1", "date": "2024-10-24", "relevancy": 2.7637, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.599}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5433}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Shape%20Completion%20with%20Test-Time%20Training&body=Title%3A%203D%20Shape%20Completion%20with%20Test-Time%20Training%0AAuthor%3A%20Michael%20Schopf-Kuester%20and%20Zorah%20L%C3%A4hner%20and%20Michael%20Moeller%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20problem%20of%20%5Ctextit%7Bshape%20completion%7D%2C%20i.e.%2C%20the%20task%0Aof%20restoring%20incomplete%20shapes%20by%20predicting%20their%20missing%20parts.%20While%0Aprevious%20works%20have%20often%20predicted%20the%20fractured%20and%20restored%20shape%20in%20one%0Astep%2C%20we%20approach%20the%20task%20by%20separately%20predicting%20the%20fractured%20and%20newly%0Arestored%20parts%2C%20but%20ensuring%20these%20predictions%20are%20interconnected.%20We%20use%20a%0Adecoder%20network%20motivated%20by%20related%20work%20on%20the%20prediction%20of%20signed%20distance%0Afunctions%20%28DeepSDF%29.%20In%20particular%2C%20our%20representation%20allows%20us%20to%20consider%0Atest-time-training%2C%20i.e.%2C%20finetuning%20network%20parameters%20to%20match%20the%20given%0Aincomplete%20shape%20more%20accurately%20during%20inference.%20While%20previous%20works%20often%0Ahave%20difficulties%20with%20artifacts%20around%20the%20fracture%20boundary%2C%20we%20demonstrate%0Athat%20our%20overfitting%20to%20the%20fractured%20parts%20leads%20to%20significant%20improvements%0Ain%20the%20restoration%20of%20eight%20different%20shape%20categories%20of%20the%20ShapeNet%20data%20set%0Ain%20terms%20of%20their%20chamfer%20distances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Shape%2520Completion%2520with%2520Test-Time%2520Training%26entry.906535625%3DMichael%2520Schopf-Kuester%2520and%2520Zorah%2520L%25C3%25A4hner%2520and%2520Michael%2520Moeller%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520problem%2520of%2520%255Ctextit%257Bshape%2520completion%257D%252C%2520i.e.%252C%2520the%2520task%250Aof%2520restoring%2520incomplete%2520shapes%2520by%2520predicting%2520their%2520missing%2520parts.%2520While%250Aprevious%2520works%2520have%2520often%2520predicted%2520the%2520fractured%2520and%2520restored%2520shape%2520in%2520one%250Astep%252C%2520we%2520approach%2520the%2520task%2520by%2520separately%2520predicting%2520the%2520fractured%2520and%2520newly%250Arestored%2520parts%252C%2520but%2520ensuring%2520these%2520predictions%2520are%2520interconnected.%2520We%2520use%2520a%250Adecoder%2520network%2520motivated%2520by%2520related%2520work%2520on%2520the%2520prediction%2520of%2520signed%2520distance%250Afunctions%2520%2528DeepSDF%2529.%2520In%2520particular%252C%2520our%2520representation%2520allows%2520us%2520to%2520consider%250Atest-time-training%252C%2520i.e.%252C%2520finetuning%2520network%2520parameters%2520to%2520match%2520the%2520given%250Aincomplete%2520shape%2520more%2520accurately%2520during%2520inference.%2520While%2520previous%2520works%2520often%250Ahave%2520difficulties%2520with%2520artifacts%2520around%2520the%2520fracture%2520boundary%252C%2520we%2520demonstrate%250Athat%2520our%2520overfitting%2520to%2520the%2520fractured%2520parts%2520leads%2520to%2520significant%2520improvements%250Ain%2520the%2520restoration%2520of%2520eight%2520different%2520shape%2520categories%2520of%2520the%2520ShapeNet%2520data%2520set%250Ain%2520terms%2520of%2520their%2520chamfer%2520distances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Shape%20Completion%20with%20Test-Time%20Training&entry.906535625=Michael%20Schopf-Kuester%20and%20Zorah%20L%C3%A4hner%20and%20Michael%20Moeller&entry.1292438233=%20%20This%20work%20addresses%20the%20problem%20of%20%5Ctextit%7Bshape%20completion%7D%2C%20i.e.%2C%20the%20task%0Aof%20restoring%20incomplete%20shapes%20by%20predicting%20their%20missing%20parts.%20While%0Aprevious%20works%20have%20often%20predicted%20the%20fractured%20and%20restored%20shape%20in%20one%0Astep%2C%20we%20approach%20the%20task%20by%20separately%20predicting%20the%20fractured%20and%20newly%0Arestored%20parts%2C%20but%20ensuring%20these%20predictions%20are%20interconnected.%20We%20use%20a%0Adecoder%20network%20motivated%20by%20related%20work%20on%20the%20prediction%20of%20signed%20distance%0Afunctions%20%28DeepSDF%29.%20In%20particular%2C%20our%20representation%20allows%20us%20to%20consider%0Atest-time-training%2C%20i.e.%2C%20finetuning%20network%20parameters%20to%20match%20the%20given%0Aincomplete%20shape%20more%20accurately%20during%20inference.%20While%20previous%20works%20often%0Ahave%20difficulties%20with%20artifacts%20around%20the%20fracture%20boundary%2C%20we%20demonstrate%0Athat%20our%20overfitting%20to%20the%20fractured%20parts%20leads%20to%20significant%20improvements%0Ain%20the%20restoration%20of%20eight%20different%20shape%20categories%20of%20the%20ShapeNet%20data%20set%0Ain%20terms%20of%20their%20chamfer%20distances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18668v1&entry.124074799=Read"},
{"title": "Rigid Single-Slice-in-Volume registration via rotation-equivariant 2D/3D\n  feature matching", "author": "Stefan Brandst\u00e4tter and Philipp Seeb\u00f6ck and Christoph F\u00fcrb\u00f6ck and Svitlana Pochepnia and Helmut Prosch and Georg Langs", "abstract": "  2D to 3D registration is essential in tasks such as diagnosis, surgical\nnavigation, environmental understanding, navigation in robotics, autonomous\nsystems, or augmented reality. In medical imaging, the aim is often to place a\n2D image in a 3D volumetric observation to w. Current approaches for rigid\nsingle slice in volume registration are limited by requirements such as pose\ninitialization, stacks of adjacent slices, or reliable anatomical landmarks.\nHere, we propose a self-supervised 2D/3D registration approach to match a\nsingle 2D slice to the corresponding 3D volume. The method works in data\nwithout anatomical priors such as images of tumors. It addresses the\ndimensionality disparity and establishes correspondences between 2D in-plane\nand 3D out-of-plane rotation-equivariant features by using group equivariant\nCNNs. These rotation-equivariant features are extracted from the 2D query slice\nand aligned with their 3D counterparts. Results demonstrate the robustness of\nthe proposed slice-in-volume registration on the NSCLC-Radiomics CT and KIRBY21\nMRI datasets, attaining an absolute median angle error of less than 2 degrees\nand a mean-matching feature accuracy of 89% at a tolerance of 3 pixels.\n", "link": "http://arxiv.org/abs/2410.18683v1", "date": "2024-10-24", "relevancy": 2.7347, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5784}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rigid%20Single-Slice-in-Volume%20registration%20via%20rotation-equivariant%202D/3D%0A%20%20feature%20matching&body=Title%3A%20Rigid%20Single-Slice-in-Volume%20registration%20via%20rotation-equivariant%202D/3D%0A%20%20feature%20matching%0AAuthor%3A%20Stefan%20Brandst%C3%A4tter%20and%20Philipp%20Seeb%C3%B6ck%20and%20Christoph%20F%C3%BCrb%C3%B6ck%20and%20Svitlana%20Pochepnia%20and%20Helmut%20Prosch%20and%20Georg%20Langs%0AAbstract%3A%20%20%202D%20to%203D%20registration%20is%20essential%20in%20tasks%20such%20as%20diagnosis%2C%20surgical%0Anavigation%2C%20environmental%20understanding%2C%20navigation%20in%20robotics%2C%20autonomous%0Asystems%2C%20or%20augmented%20reality.%20In%20medical%20imaging%2C%20the%20aim%20is%20often%20to%20place%20a%0A2D%20image%20in%20a%203D%20volumetric%20observation%20to%20w.%20Current%20approaches%20for%20rigid%0Asingle%20slice%20in%20volume%20registration%20are%20limited%20by%20requirements%20such%20as%20pose%0Ainitialization%2C%20stacks%20of%20adjacent%20slices%2C%20or%20reliable%20anatomical%20landmarks.%0AHere%2C%20we%20propose%20a%20self-supervised%202D/3D%20registration%20approach%20to%20match%20a%0Asingle%202D%20slice%20to%20the%20corresponding%203D%20volume.%20The%20method%20works%20in%20data%0Awithout%20anatomical%20priors%20such%20as%20images%20of%20tumors.%20It%20addresses%20the%0Adimensionality%20disparity%20and%20establishes%20correspondences%20between%202D%20in-plane%0Aand%203D%20out-of-plane%20rotation-equivariant%20features%20by%20using%20group%20equivariant%0ACNNs.%20These%20rotation-equivariant%20features%20are%20extracted%20from%20the%202D%20query%20slice%0Aand%20aligned%20with%20their%203D%20counterparts.%20Results%20demonstrate%20the%20robustness%20of%0Athe%20proposed%20slice-in-volume%20registration%20on%20the%20NSCLC-Radiomics%20CT%20and%20KIRBY21%0AMRI%20datasets%2C%20attaining%20an%20absolute%20median%20angle%20error%20of%20less%20than%202%20degrees%0Aand%20a%20mean-matching%20feature%20accuracy%20of%2089%25%20at%20a%20tolerance%20of%203%20pixels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRigid%2520Single-Slice-in-Volume%2520registration%2520via%2520rotation-equivariant%25202D/3D%250A%2520%2520feature%2520matching%26entry.906535625%3DStefan%2520Brandst%25C3%25A4tter%2520and%2520Philipp%2520Seeb%25C3%25B6ck%2520and%2520Christoph%2520F%25C3%25BCrb%25C3%25B6ck%2520and%2520Svitlana%2520Pochepnia%2520and%2520Helmut%2520Prosch%2520and%2520Georg%2520Langs%26entry.1292438233%3D%2520%25202D%2520to%25203D%2520registration%2520is%2520essential%2520in%2520tasks%2520such%2520as%2520diagnosis%252C%2520surgical%250Anavigation%252C%2520environmental%2520understanding%252C%2520navigation%2520in%2520robotics%252C%2520autonomous%250Asystems%252C%2520or%2520augmented%2520reality.%2520In%2520medical%2520imaging%252C%2520the%2520aim%2520is%2520often%2520to%2520place%2520a%250A2D%2520image%2520in%2520a%25203D%2520volumetric%2520observation%2520to%2520w.%2520Current%2520approaches%2520for%2520rigid%250Asingle%2520slice%2520in%2520volume%2520registration%2520are%2520limited%2520by%2520requirements%2520such%2520as%2520pose%250Ainitialization%252C%2520stacks%2520of%2520adjacent%2520slices%252C%2520or%2520reliable%2520anatomical%2520landmarks.%250AHere%252C%2520we%2520propose%2520a%2520self-supervised%25202D/3D%2520registration%2520approach%2520to%2520match%2520a%250Asingle%25202D%2520slice%2520to%2520the%2520corresponding%25203D%2520volume.%2520The%2520method%2520works%2520in%2520data%250Awithout%2520anatomical%2520priors%2520such%2520as%2520images%2520of%2520tumors.%2520It%2520addresses%2520the%250Adimensionality%2520disparity%2520and%2520establishes%2520correspondences%2520between%25202D%2520in-plane%250Aand%25203D%2520out-of-plane%2520rotation-equivariant%2520features%2520by%2520using%2520group%2520equivariant%250ACNNs.%2520These%2520rotation-equivariant%2520features%2520are%2520extracted%2520from%2520the%25202D%2520query%2520slice%250Aand%2520aligned%2520with%2520their%25203D%2520counterparts.%2520Results%2520demonstrate%2520the%2520robustness%2520of%250Athe%2520proposed%2520slice-in-volume%2520registration%2520on%2520the%2520NSCLC-Radiomics%2520CT%2520and%2520KIRBY21%250AMRI%2520datasets%252C%2520attaining%2520an%2520absolute%2520median%2520angle%2520error%2520of%2520less%2520than%25202%2520degrees%250Aand%2520a%2520mean-matching%2520feature%2520accuracy%2520of%252089%2525%2520at%2520a%2520tolerance%2520of%25203%2520pixels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rigid%20Single-Slice-in-Volume%20registration%20via%20rotation-equivariant%202D/3D%0A%20%20feature%20matching&entry.906535625=Stefan%20Brandst%C3%A4tter%20and%20Philipp%20Seeb%C3%B6ck%20and%20Christoph%20F%C3%BCrb%C3%B6ck%20and%20Svitlana%20Pochepnia%20and%20Helmut%20Prosch%20and%20Georg%20Langs&entry.1292438233=%20%202D%20to%203D%20registration%20is%20essential%20in%20tasks%20such%20as%20diagnosis%2C%20surgical%0Anavigation%2C%20environmental%20understanding%2C%20navigation%20in%20robotics%2C%20autonomous%0Asystems%2C%20or%20augmented%20reality.%20In%20medical%20imaging%2C%20the%20aim%20is%20often%20to%20place%20a%0A2D%20image%20in%20a%203D%20volumetric%20observation%20to%20w.%20Current%20approaches%20for%20rigid%0Asingle%20slice%20in%20volume%20registration%20are%20limited%20by%20requirements%20such%20as%20pose%0Ainitialization%2C%20stacks%20of%20adjacent%20slices%2C%20or%20reliable%20anatomical%20landmarks.%0AHere%2C%20we%20propose%20a%20self-supervised%202D/3D%20registration%20approach%20to%20match%20a%0Asingle%202D%20slice%20to%20the%20corresponding%203D%20volume.%20The%20method%20works%20in%20data%0Awithout%20anatomical%20priors%20such%20as%20images%20of%20tumors.%20It%20addresses%20the%0Adimensionality%20disparity%20and%20establishes%20correspondences%20between%202D%20in-plane%0Aand%203D%20out-of-plane%20rotation-equivariant%20features%20by%20using%20group%20equivariant%0ACNNs.%20These%20rotation-equivariant%20features%20are%20extracted%20from%20the%202D%20query%20slice%0Aand%20aligned%20with%20their%203D%20counterparts.%20Results%20demonstrate%20the%20robustness%20of%0Athe%20proposed%20slice-in-volume%20registration%20on%20the%20NSCLC-Radiomics%20CT%20and%20KIRBY21%0AMRI%20datasets%2C%20attaining%20an%20absolute%20median%20angle%20error%20of%20less%20than%202%20degrees%0Aand%20a%20mean-matching%20feature%20accuracy%20of%2089%25%20at%20a%20tolerance%20of%203%20pixels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18683v1&entry.124074799=Read"},
{"title": "PnLCalib: Sports Field Registration via Points and Lines Optimization", "author": "Marc Guti\u00e9rrez-P\u00e9rez and Antonio Agudo", "abstract": "  Camera calibration in broadcast sports videos presents numerous challenges\nfor accurate sports field registration due to multiple camera angles, varying\ncamera parameters, and frequent occlusions of the field. Traditional\nsearch-based methods depend on initial camera pose estimates, which can\nstruggle in non-standard positions and dynamic environments. In response, we\npropose an optimization-based calibration pipeline that leverages a 3D soccer\nfield model and a predefined set of keypoints to overcome these limitations.\nOur method also introduces a novel refinement module that improves initial\ncalibration by using detected field lines in a non-linear optimization process.\nThis approach outperforms existing techniques in both multi-view and\nsingle-view 3D camera calibration tasks, while maintaining competitive\nperformance in homography estimation. Extensive experimentation on real-world\nsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, and\nTS-WorldCup, highlights the robustness and accuracy of our method across\ndiverse broadcast scenarios. Our approach offers significant improvements in\ncamera calibration precision and reliability.\n", "link": "http://arxiv.org/abs/2404.08401v4", "date": "2024-10-24", "relevancy": 2.7242, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5663}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5577}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PnLCalib%3A%20Sports%20Field%20Registration%20via%20Points%20and%20Lines%20Optimization&body=Title%3A%20PnLCalib%3A%20Sports%20Field%20Registration%20via%20Points%20and%20Lines%20Optimization%0AAuthor%3A%20Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Antonio%20Agudo%0AAbstract%3A%20%20%20Camera%20calibration%20in%20broadcast%20sports%20videos%20presents%20numerous%20challenges%0Afor%20accurate%20sports%20field%20registration%20due%20to%20multiple%20camera%20angles%2C%20varying%0Acamera%20parameters%2C%20and%20frequent%20occlusions%20of%20the%20field.%20Traditional%0Asearch-based%20methods%20depend%20on%20initial%20camera%20pose%20estimates%2C%20which%20can%0Astruggle%20in%20non-standard%20positions%20and%20dynamic%20environments.%20In%20response%2C%20we%0Apropose%20an%20optimization-based%20calibration%20pipeline%20that%20leverages%20a%203D%20soccer%0Afield%20model%20and%20a%20predefined%20set%20of%20keypoints%20to%20overcome%20these%20limitations.%0AOur%20method%20also%20introduces%20a%20novel%20refinement%20module%20that%20improves%20initial%0Acalibration%20by%20using%20detected%20field%20lines%20in%20a%20non-linear%20optimization%20process.%0AThis%20approach%20outperforms%20existing%20techniques%20in%20both%20multi-view%20and%0Asingle-view%203D%20camera%20calibration%20tasks%2C%20while%20maintaining%20competitive%0Aperformance%20in%20homography%20estimation.%20Extensive%20experimentation%20on%20real-world%0Asoccer%20datasets%2C%20including%20SoccerNet-Calibration%2C%20WorldCup%202014%2C%20and%0ATS-WorldCup%2C%20highlights%20the%20robustness%20and%20accuracy%20of%20our%20method%20across%0Adiverse%20broadcast%20scenarios.%20Our%20approach%20offers%20significant%20improvements%20in%0Acamera%20calibration%20precision%20and%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08401v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPnLCalib%253A%2520Sports%2520Field%2520Registration%2520via%2520Points%2520and%2520Lines%2520Optimization%26entry.906535625%3DMarc%2520Guti%25C3%25A9rrez-P%25C3%25A9rez%2520and%2520Antonio%2520Agudo%26entry.1292438233%3D%2520%2520Camera%2520calibration%2520in%2520broadcast%2520sports%2520videos%2520presents%2520numerous%2520challenges%250Afor%2520accurate%2520sports%2520field%2520registration%2520due%2520to%2520multiple%2520camera%2520angles%252C%2520varying%250Acamera%2520parameters%252C%2520and%2520frequent%2520occlusions%2520of%2520the%2520field.%2520Traditional%250Asearch-based%2520methods%2520depend%2520on%2520initial%2520camera%2520pose%2520estimates%252C%2520which%2520can%250Astruggle%2520in%2520non-standard%2520positions%2520and%2520dynamic%2520environments.%2520In%2520response%252C%2520we%250Apropose%2520an%2520optimization-based%2520calibration%2520pipeline%2520that%2520leverages%2520a%25203D%2520soccer%250Afield%2520model%2520and%2520a%2520predefined%2520set%2520of%2520keypoints%2520to%2520overcome%2520these%2520limitations.%250AOur%2520method%2520also%2520introduces%2520a%2520novel%2520refinement%2520module%2520that%2520improves%2520initial%250Acalibration%2520by%2520using%2520detected%2520field%2520lines%2520in%2520a%2520non-linear%2520optimization%2520process.%250AThis%2520approach%2520outperforms%2520existing%2520techniques%2520in%2520both%2520multi-view%2520and%250Asingle-view%25203D%2520camera%2520calibration%2520tasks%252C%2520while%2520maintaining%2520competitive%250Aperformance%2520in%2520homography%2520estimation.%2520Extensive%2520experimentation%2520on%2520real-world%250Asoccer%2520datasets%252C%2520including%2520SoccerNet-Calibration%252C%2520WorldCup%25202014%252C%2520and%250ATS-WorldCup%252C%2520highlights%2520the%2520robustness%2520and%2520accuracy%2520of%2520our%2520method%2520across%250Adiverse%2520broadcast%2520scenarios.%2520Our%2520approach%2520offers%2520significant%2520improvements%2520in%250Acamera%2520calibration%2520precision%2520and%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08401v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PnLCalib%3A%20Sports%20Field%20Registration%20via%20Points%20and%20Lines%20Optimization&entry.906535625=Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Antonio%20Agudo&entry.1292438233=%20%20Camera%20calibration%20in%20broadcast%20sports%20videos%20presents%20numerous%20challenges%0Afor%20accurate%20sports%20field%20registration%20due%20to%20multiple%20camera%20angles%2C%20varying%0Acamera%20parameters%2C%20and%20frequent%20occlusions%20of%20the%20field.%20Traditional%0Asearch-based%20methods%20depend%20on%20initial%20camera%20pose%20estimates%2C%20which%20can%0Astruggle%20in%20non-standard%20positions%20and%20dynamic%20environments.%20In%20response%2C%20we%0Apropose%20an%20optimization-based%20calibration%20pipeline%20that%20leverages%20a%203D%20soccer%0Afield%20model%20and%20a%20predefined%20set%20of%20keypoints%20to%20overcome%20these%20limitations.%0AOur%20method%20also%20introduces%20a%20novel%20refinement%20module%20that%20improves%20initial%0Acalibration%20by%20using%20detected%20field%20lines%20in%20a%20non-linear%20optimization%20process.%0AThis%20approach%20outperforms%20existing%20techniques%20in%20both%20multi-view%20and%0Asingle-view%203D%20camera%20calibration%20tasks%2C%20while%20maintaining%20competitive%0Aperformance%20in%20homography%20estimation.%20Extensive%20experimentation%20on%20real-world%0Asoccer%20datasets%2C%20including%20SoccerNet-Calibration%2C%20WorldCup%202014%2C%20and%0ATS-WorldCup%2C%20highlights%20the%20robustness%20and%20accuracy%20of%20our%20method%20across%0Adiverse%20broadcast%20scenarios.%20Our%20approach%20offers%20significant%20improvements%20in%0Acamera%20calibration%20precision%20and%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08401v4&entry.124074799=Read"},
{"title": "Disentangled Representation Learning with the Gromov-Monge Gap", "author": "Th\u00e9o Uscidda and Luca Eyring and Karsten Roth and Fabian Theis and Zeynep Akata and Marco Cuturi", "abstract": "  Learning disentangled representations from unlabelled data is a fundamental\nchallenge in machine learning. Solving it may unlock other problems, such as\ngeneralization, interpretability, or fairness. Although remarkably challenging\nto solve in theory, disentanglement is often achieved in practice through prior\nmatching. Furthermore, recent works have shown that prior matching approaches\ncan be enhanced by leveraging geometrical considerations, e.g., by learning\nrepresentations that preserve geometric features of the data, such as distances\nor angles between points. However, matching the prior while preserving\ngeometric features is challenging, as a mapping that fully preserves these\nfeatures while aligning the data distribution with the prior does not exist in\ngeneral. To address these challenges, we introduce a novel approach to\ndisentangled representation learning based on quadratic optimal transport. We\nformulate the problem using Gromov-Monge maps that transport one distribution\nonto another with minimal distortion of predefined geometric features,\npreserving them as much as can be achieved. To compute such maps, we propose\nthe Gromov-Monge-Gap (GMG), a regularizer quantifying whether a map moves a\nreference distribution with minimal geometry distortion. We demonstrate the\neffectiveness of our approach for disentanglement across four standard\nbenchmarks, outperforming other methods leveraging geometric considerations.\n", "link": "http://arxiv.org/abs/2407.07829v2", "date": "2024-10-24", "relevancy": 2.6925, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5464}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5393}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Representation%20Learning%20with%20the%20Gromov-Monge%20Gap&body=Title%3A%20Disentangled%20Representation%20Learning%20with%20the%20Gromov-Monge%20Gap%0AAuthor%3A%20Th%C3%A9o%20Uscidda%20and%20Luca%20Eyring%20and%20Karsten%20Roth%20and%20Fabian%20Theis%20and%20Zeynep%20Akata%20and%20Marco%20Cuturi%0AAbstract%3A%20%20%20Learning%20disentangled%20representations%20from%20unlabelled%20data%20is%20a%20fundamental%0Achallenge%20in%20machine%20learning.%20Solving%20it%20may%20unlock%20other%20problems%2C%20such%20as%0Ageneralization%2C%20interpretability%2C%20or%20fairness.%20Although%20remarkably%20challenging%0Ato%20solve%20in%20theory%2C%20disentanglement%20is%20often%20achieved%20in%20practice%20through%20prior%0Amatching.%20Furthermore%2C%20recent%20works%20have%20shown%20that%20prior%20matching%20approaches%0Acan%20be%20enhanced%20by%20leveraging%20geometrical%20considerations%2C%20e.g.%2C%20by%20learning%0Arepresentations%20that%20preserve%20geometric%20features%20of%20the%20data%2C%20such%20as%20distances%0Aor%20angles%20between%20points.%20However%2C%20matching%20the%20prior%20while%20preserving%0Ageometric%20features%20is%20challenging%2C%20as%20a%20mapping%20that%20fully%20preserves%20these%0Afeatures%20while%20aligning%20the%20data%20distribution%20with%20the%20prior%20does%20not%20exist%20in%0Ageneral.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20approach%20to%0Adisentangled%20representation%20learning%20based%20on%20quadratic%20optimal%20transport.%20We%0Aformulate%20the%20problem%20using%20Gromov-Monge%20maps%20that%20transport%20one%20distribution%0Aonto%20another%20with%20minimal%20distortion%20of%20predefined%20geometric%20features%2C%0Apreserving%20them%20as%20much%20as%20can%20be%20achieved.%20To%20compute%20such%20maps%2C%20we%20propose%0Athe%20Gromov-Monge-Gap%20%28GMG%29%2C%20a%20regularizer%20quantifying%20whether%20a%20map%20moves%20a%0Areference%20distribution%20with%20minimal%20geometry%20distortion.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20for%20disentanglement%20across%20four%20standard%0Abenchmarks%2C%20outperforming%20other%20methods%20leveraging%20geometric%20considerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07829v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Representation%2520Learning%2520with%2520the%2520Gromov-Monge%2520Gap%26entry.906535625%3DTh%25C3%25A9o%2520Uscidda%2520and%2520Luca%2520Eyring%2520and%2520Karsten%2520Roth%2520and%2520Fabian%2520Theis%2520and%2520Zeynep%2520Akata%2520and%2520Marco%2520Cuturi%26entry.1292438233%3D%2520%2520Learning%2520disentangled%2520representations%2520from%2520unlabelled%2520data%2520is%2520a%2520fundamental%250Achallenge%2520in%2520machine%2520learning.%2520Solving%2520it%2520may%2520unlock%2520other%2520problems%252C%2520such%2520as%250Ageneralization%252C%2520interpretability%252C%2520or%2520fairness.%2520Although%2520remarkably%2520challenging%250Ato%2520solve%2520in%2520theory%252C%2520disentanglement%2520is%2520often%2520achieved%2520in%2520practice%2520through%2520prior%250Amatching.%2520Furthermore%252C%2520recent%2520works%2520have%2520shown%2520that%2520prior%2520matching%2520approaches%250Acan%2520be%2520enhanced%2520by%2520leveraging%2520geometrical%2520considerations%252C%2520e.g.%252C%2520by%2520learning%250Arepresentations%2520that%2520preserve%2520geometric%2520features%2520of%2520the%2520data%252C%2520such%2520as%2520distances%250Aor%2520angles%2520between%2520points.%2520However%252C%2520matching%2520the%2520prior%2520while%2520preserving%250Ageometric%2520features%2520is%2520challenging%252C%2520as%2520a%2520mapping%2520that%2520fully%2520preserves%2520these%250Afeatures%2520while%2520aligning%2520the%2520data%2520distribution%2520with%2520the%2520prior%2520does%2520not%2520exist%2520in%250Ageneral.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520to%250Adisentangled%2520representation%2520learning%2520based%2520on%2520quadratic%2520optimal%2520transport.%2520We%250Aformulate%2520the%2520problem%2520using%2520Gromov-Monge%2520maps%2520that%2520transport%2520one%2520distribution%250Aonto%2520another%2520with%2520minimal%2520distortion%2520of%2520predefined%2520geometric%2520features%252C%250Apreserving%2520them%2520as%2520much%2520as%2520can%2520be%2520achieved.%2520To%2520compute%2520such%2520maps%252C%2520we%2520propose%250Athe%2520Gromov-Monge-Gap%2520%2528GMG%2529%252C%2520a%2520regularizer%2520quantifying%2520whether%2520a%2520map%2520moves%2520a%250Areference%2520distribution%2520with%2520minimal%2520geometry%2520distortion.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520for%2520disentanglement%2520across%2520four%2520standard%250Abenchmarks%252C%2520outperforming%2520other%2520methods%2520leveraging%2520geometric%2520considerations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07829v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Representation%20Learning%20with%20the%20Gromov-Monge%20Gap&entry.906535625=Th%C3%A9o%20Uscidda%20and%20Luca%20Eyring%20and%20Karsten%20Roth%20and%20Fabian%20Theis%20and%20Zeynep%20Akata%20and%20Marco%20Cuturi&entry.1292438233=%20%20Learning%20disentangled%20representations%20from%20unlabelled%20data%20is%20a%20fundamental%0Achallenge%20in%20machine%20learning.%20Solving%20it%20may%20unlock%20other%20problems%2C%20such%20as%0Ageneralization%2C%20interpretability%2C%20or%20fairness.%20Although%20remarkably%20challenging%0Ato%20solve%20in%20theory%2C%20disentanglement%20is%20often%20achieved%20in%20practice%20through%20prior%0Amatching.%20Furthermore%2C%20recent%20works%20have%20shown%20that%20prior%20matching%20approaches%0Acan%20be%20enhanced%20by%20leveraging%20geometrical%20considerations%2C%20e.g.%2C%20by%20learning%0Arepresentations%20that%20preserve%20geometric%20features%20of%20the%20data%2C%20such%20as%20distances%0Aor%20angles%20between%20points.%20However%2C%20matching%20the%20prior%20while%20preserving%0Ageometric%20features%20is%20challenging%2C%20as%20a%20mapping%20that%20fully%20preserves%20these%0Afeatures%20while%20aligning%20the%20data%20distribution%20with%20the%20prior%20does%20not%20exist%20in%0Ageneral.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20approach%20to%0Adisentangled%20representation%20learning%20based%20on%20quadratic%20optimal%20transport.%20We%0Aformulate%20the%20problem%20using%20Gromov-Monge%20maps%20that%20transport%20one%20distribution%0Aonto%20another%20with%20minimal%20distortion%20of%20predefined%20geometric%20features%2C%0Apreserving%20them%20as%20much%20as%20can%20be%20achieved.%20To%20compute%20such%20maps%2C%20we%20propose%0Athe%20Gromov-Monge-Gap%20%28GMG%29%2C%20a%20regularizer%20quantifying%20whether%20a%20map%20moves%20a%0Areference%20distribution%20with%20minimal%20geometry%20distortion.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20for%20disentanglement%20across%20four%20standard%0Abenchmarks%2C%20outperforming%20other%20methods%20leveraging%20geometric%20considerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07829v2&entry.124074799=Read"},
{"title": "Learning Structured Compressed Sensing with Automatic Resource\n  Allocation", "author": "Han Wang and Eduardo P\u00e9rez and Iris A. M. Huijben and Hans van Gorp and Ruud van Sloun and Florian R\u00f6mer", "abstract": "  Multidimensional data acquisition often requires extensive time and poses\nsignificant challenges for hardware and software regarding data storage and\nprocessing. Rather than designing a single compression matrix as in\nconventional compressed sensing, structured compressed sensing yields\ndimension-specific compression matrices, reducing the number of optimizable\nparameters. Recent advances in machine learning (ML) have enabled task-based\nsupervised learning of subsampling matrices, albeit at the expense of complex\ndownstream models. Additionally, the sampling resource allocation across\ndimensions is often determined in advance through heuristics. To address these\nchallenges, we introduce Structured COmpressed Sensing with Automatic Resource\nAllocation (SCOSARA) with an information theory-based unsupervised learning\nstrategy. SCOSARA adaptively distributes samples across sampling dimensions\nwhile maximizing Fisher information content. Using ultrasound localization as a\ncase study, we compare SCOSARA to state-of-the-art ML-based and greedy search\nalgorithms. Simulation results demonstrate that SCOSARA can produce\nhigh-quality subsampling matrices that achieve lower Cram\\'er-Rao Bound values\nthan the baselines. In addition, SCOSARA outperforms other ML-based algorithms\nin terms of the number of trainable parameters, computational complexity, and\nmemory requirements while automatically choosing the number of samples per\naxis.\n", "link": "http://arxiv.org/abs/2410.18954v1", "date": "2024-10-24", "relevancy": 2.6922, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5555}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Structured%20Compressed%20Sensing%20with%20Automatic%20Resource%0A%20%20Allocation&body=Title%3A%20Learning%20Structured%20Compressed%20Sensing%20with%20Automatic%20Resource%0A%20%20Allocation%0AAuthor%3A%20Han%20Wang%20and%20Eduardo%20P%C3%A9rez%20and%20Iris%20A.%20M.%20Huijben%20and%20Hans%20van%20Gorp%20and%20Ruud%20van%20Sloun%20and%20Florian%20R%C3%B6mer%0AAbstract%3A%20%20%20Multidimensional%20data%20acquisition%20often%20requires%20extensive%20time%20and%20poses%0Asignificant%20challenges%20for%20hardware%20and%20software%20regarding%20data%20storage%20and%0Aprocessing.%20Rather%20than%20designing%20a%20single%20compression%20matrix%20as%20in%0Aconventional%20compressed%20sensing%2C%20structured%20compressed%20sensing%20yields%0Adimension-specific%20compression%20matrices%2C%20reducing%20the%20number%20of%20optimizable%0Aparameters.%20Recent%20advances%20in%20machine%20learning%20%28ML%29%20have%20enabled%20task-based%0Asupervised%20learning%20of%20subsampling%20matrices%2C%20albeit%20at%20the%20expense%20of%20complex%0Adownstream%20models.%20Additionally%2C%20the%20sampling%20resource%20allocation%20across%0Adimensions%20is%20often%20determined%20in%20advance%20through%20heuristics.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Structured%20COmpressed%20Sensing%20with%20Automatic%20Resource%0AAllocation%20%28SCOSARA%29%20with%20an%20information%20theory-based%20unsupervised%20learning%0Astrategy.%20SCOSARA%20adaptively%20distributes%20samples%20across%20sampling%20dimensions%0Awhile%20maximizing%20Fisher%20information%20content.%20Using%20ultrasound%20localization%20as%20a%0Acase%20study%2C%20we%20compare%20SCOSARA%20to%20state-of-the-art%20ML-based%20and%20greedy%20search%0Aalgorithms.%20Simulation%20results%20demonstrate%20that%20SCOSARA%20can%20produce%0Ahigh-quality%20subsampling%20matrices%20that%20achieve%20lower%20Cram%5C%27er-Rao%20Bound%20values%0Athan%20the%20baselines.%20In%20addition%2C%20SCOSARA%20outperforms%20other%20ML-based%20algorithms%0Ain%20terms%20of%20the%20number%20of%20trainable%20parameters%2C%20computational%20complexity%2C%20and%0Amemory%20requirements%20while%20automatically%20choosing%20the%20number%20of%20samples%20per%0Aaxis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Structured%2520Compressed%2520Sensing%2520with%2520Automatic%2520Resource%250A%2520%2520Allocation%26entry.906535625%3DHan%2520Wang%2520and%2520Eduardo%2520P%25C3%25A9rez%2520and%2520Iris%2520A.%2520M.%2520Huijben%2520and%2520Hans%2520van%2520Gorp%2520and%2520Ruud%2520van%2520Sloun%2520and%2520Florian%2520R%25C3%25B6mer%26entry.1292438233%3D%2520%2520Multidimensional%2520data%2520acquisition%2520often%2520requires%2520extensive%2520time%2520and%2520poses%250Asignificant%2520challenges%2520for%2520hardware%2520and%2520software%2520regarding%2520data%2520storage%2520and%250Aprocessing.%2520Rather%2520than%2520designing%2520a%2520single%2520compression%2520matrix%2520as%2520in%250Aconventional%2520compressed%2520sensing%252C%2520structured%2520compressed%2520sensing%2520yields%250Adimension-specific%2520compression%2520matrices%252C%2520reducing%2520the%2520number%2520of%2520optimizable%250Aparameters.%2520Recent%2520advances%2520in%2520machine%2520learning%2520%2528ML%2529%2520have%2520enabled%2520task-based%250Asupervised%2520learning%2520of%2520subsampling%2520matrices%252C%2520albeit%2520at%2520the%2520expense%2520of%2520complex%250Adownstream%2520models.%2520Additionally%252C%2520the%2520sampling%2520resource%2520allocation%2520across%250Adimensions%2520is%2520often%2520determined%2520in%2520advance%2520through%2520heuristics.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520Structured%2520COmpressed%2520Sensing%2520with%2520Automatic%2520Resource%250AAllocation%2520%2528SCOSARA%2529%2520with%2520an%2520information%2520theory-based%2520unsupervised%2520learning%250Astrategy.%2520SCOSARA%2520adaptively%2520distributes%2520samples%2520across%2520sampling%2520dimensions%250Awhile%2520maximizing%2520Fisher%2520information%2520content.%2520Using%2520ultrasound%2520localization%2520as%2520a%250Acase%2520study%252C%2520we%2520compare%2520SCOSARA%2520to%2520state-of-the-art%2520ML-based%2520and%2520greedy%2520search%250Aalgorithms.%2520Simulation%2520results%2520demonstrate%2520that%2520SCOSARA%2520can%2520produce%250Ahigh-quality%2520subsampling%2520matrices%2520that%2520achieve%2520lower%2520Cram%255C%2527er-Rao%2520Bound%2520values%250Athan%2520the%2520baselines.%2520In%2520addition%252C%2520SCOSARA%2520outperforms%2520other%2520ML-based%2520algorithms%250Ain%2520terms%2520of%2520the%2520number%2520of%2520trainable%2520parameters%252C%2520computational%2520complexity%252C%2520and%250Amemory%2520requirements%2520while%2520automatically%2520choosing%2520the%2520number%2520of%2520samples%2520per%250Aaxis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Structured%20Compressed%20Sensing%20with%20Automatic%20Resource%0A%20%20Allocation&entry.906535625=Han%20Wang%20and%20Eduardo%20P%C3%A9rez%20and%20Iris%20A.%20M.%20Huijben%20and%20Hans%20van%20Gorp%20and%20Ruud%20van%20Sloun%20and%20Florian%20R%C3%B6mer&entry.1292438233=%20%20Multidimensional%20data%20acquisition%20often%20requires%20extensive%20time%20and%20poses%0Asignificant%20challenges%20for%20hardware%20and%20software%20regarding%20data%20storage%20and%0Aprocessing.%20Rather%20than%20designing%20a%20single%20compression%20matrix%20as%20in%0Aconventional%20compressed%20sensing%2C%20structured%20compressed%20sensing%20yields%0Adimension-specific%20compression%20matrices%2C%20reducing%20the%20number%20of%20optimizable%0Aparameters.%20Recent%20advances%20in%20machine%20learning%20%28ML%29%20have%20enabled%20task-based%0Asupervised%20learning%20of%20subsampling%20matrices%2C%20albeit%20at%20the%20expense%20of%20complex%0Adownstream%20models.%20Additionally%2C%20the%20sampling%20resource%20allocation%20across%0Adimensions%20is%20often%20determined%20in%20advance%20through%20heuristics.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Structured%20COmpressed%20Sensing%20with%20Automatic%20Resource%0AAllocation%20%28SCOSARA%29%20with%20an%20information%20theory-based%20unsupervised%20learning%0Astrategy.%20SCOSARA%20adaptively%20distributes%20samples%20across%20sampling%20dimensions%0Awhile%20maximizing%20Fisher%20information%20content.%20Using%20ultrasound%20localization%20as%20a%0Acase%20study%2C%20we%20compare%20SCOSARA%20to%20state-of-the-art%20ML-based%20and%20greedy%20search%0Aalgorithms.%20Simulation%20results%20demonstrate%20that%20SCOSARA%20can%20produce%0Ahigh-quality%20subsampling%20matrices%20that%20achieve%20lower%20Cram%5C%27er-Rao%20Bound%20values%0Athan%20the%20baselines.%20In%20addition%2C%20SCOSARA%20outperforms%20other%20ML-based%20algorithms%0Ain%20terms%20of%20the%20number%20of%20trainable%20parameters%2C%20computational%20complexity%2C%20and%0Amemory%20requirements%20while%20automatically%20choosing%20the%20number%20of%20samples%20per%0Aaxis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18954v1&entry.124074799=Read"},
{"title": "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU", "author": "Farhad Mortezapour Shiri and Thinagaran Perumal and Norwati Mustapha and Raihani Mohamed", "abstract": "  Deep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.\n", "link": "http://arxiv.org/abs/2305.17473v3", "date": "2024-10-24", "relevancy": 2.6636, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5513}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Overview%20and%20Comparative%20Analysis%20on%20Deep%20Learning%0A%20%20Models%3A%20CNN%2C%20RNN%2C%20LSTM%2C%20GRU&body=Title%3A%20A%20Comprehensive%20Overview%20and%20Comparative%20Analysis%20on%20Deep%20Learning%0A%20%20Models%3A%20CNN%2C%20RNN%2C%20LSTM%2C%20GRU%0AAuthor%3A%20Farhad%20Mortezapour%20Shiri%20and%20Thinagaran%20Perumal%20and%20Norwati%20Mustapha%20and%20Raihani%20Mohamed%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20has%20emerged%20as%20a%20powerful%20subset%20of%20machine%20learning%20%28ML%29%0Aand%20artificial%20intelligence%20%28AI%29%2C%20outperforming%20traditional%20ML%20methods%2C%0Aespecially%20in%20handling%20unstructured%20and%20large%20datasets.%20Its%20impact%20spans%20across%0Avarious%20domains%2C%20including%20speech%20recognition%2C%20healthcare%2C%20autonomous%20vehicles%2C%0Acybersecurity%2C%20predictive%20analytics%2C%20and%20more.%20However%2C%20the%20complexity%20and%0Adynamic%20nature%20of%20real-world%20problems%20present%20challenges%20in%20designing%20effective%0Adeep%20learning%20models.%20Consequently%2C%20several%20deep%20learning%20models%20have%20been%0Adeveloped%20to%20address%20different%20problems%20and%20applications.%20In%20this%20article%2C%20we%0Aconduct%20a%20comprehensive%20survey%20of%20various%20deep%20learning%20models%2C%20including%0AConvolutional%20Neural%20Networks%20%28CNNs%29%2C%20Recurrent%20Neural%20Networks%20%28RNNs%29%2C%0AGenerative%20Models%2C%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20and%20Deep%20Transfer%0ALearning.%20We%20examine%20the%20structure%2C%20applications%2C%20benefits%2C%20and%20limitations%20of%0Aeach%20model.%20Furthermore%2C%20we%20perform%20an%20analysis%20using%20three%20publicly%20available%0Adatasets%3A%20IMDB%2C%20ARAS%2C%20and%20Fruit-360.%20We%20compare%20the%20performance%20of%20six%20renowned%0Adeep%20learning%20models%3A%20CNN%2C%20Simple%20RNN%2C%20Long%20Short-Term%20Memory%20%28LSTM%29%2C%0ABidirectional%20LSTM%2C%20Gated%20Recurrent%20Unit%20%28GRU%29%2C%20and%20Bidirectional%20GRU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.17473v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Overview%2520and%2520Comparative%2520Analysis%2520on%2520Deep%2520Learning%250A%2520%2520Models%253A%2520CNN%252C%2520RNN%252C%2520LSTM%252C%2520GRU%26entry.906535625%3DFarhad%2520Mortezapour%2520Shiri%2520and%2520Thinagaran%2520Perumal%2520and%2520Norwati%2520Mustapha%2520and%2520Raihani%2520Mohamed%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520subset%2520of%2520machine%2520learning%2520%2528ML%2529%250Aand%2520artificial%2520intelligence%2520%2528AI%2529%252C%2520outperforming%2520traditional%2520ML%2520methods%252C%250Aespecially%2520in%2520handling%2520unstructured%2520and%2520large%2520datasets.%2520Its%2520impact%2520spans%2520across%250Avarious%2520domains%252C%2520including%2520speech%2520recognition%252C%2520healthcare%252C%2520autonomous%2520vehicles%252C%250Acybersecurity%252C%2520predictive%2520analytics%252C%2520and%2520more.%2520However%252C%2520the%2520complexity%2520and%250Adynamic%2520nature%2520of%2520real-world%2520problems%2520present%2520challenges%2520in%2520designing%2520effective%250Adeep%2520learning%2520models.%2520Consequently%252C%2520several%2520deep%2520learning%2520models%2520have%2520been%250Adeveloped%2520to%2520address%2520different%2520problems%2520and%2520applications.%2520In%2520this%2520article%252C%2520we%250Aconduct%2520a%2520comprehensive%2520survey%2520of%2520various%2520deep%2520learning%2520models%252C%2520including%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%252C%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529%252C%250AGenerative%2520Models%252C%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%252C%2520and%2520Deep%2520Transfer%250ALearning.%2520We%2520examine%2520the%2520structure%252C%2520applications%252C%2520benefits%252C%2520and%2520limitations%2520of%250Aeach%2520model.%2520Furthermore%252C%2520we%2520perform%2520an%2520analysis%2520using%2520three%2520publicly%2520available%250Adatasets%253A%2520IMDB%252C%2520ARAS%252C%2520and%2520Fruit-360.%2520We%2520compare%2520the%2520performance%2520of%2520six%2520renowned%250Adeep%2520learning%2520models%253A%2520CNN%252C%2520Simple%2520RNN%252C%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%252C%250ABidirectional%2520LSTM%252C%2520Gated%2520Recurrent%2520Unit%2520%2528GRU%2529%252C%2520and%2520Bidirectional%2520GRU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.17473v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Overview%20and%20Comparative%20Analysis%20on%20Deep%20Learning%0A%20%20Models%3A%20CNN%2C%20RNN%2C%20LSTM%2C%20GRU&entry.906535625=Farhad%20Mortezapour%20Shiri%20and%20Thinagaran%20Perumal%20and%20Norwati%20Mustapha%20and%20Raihani%20Mohamed&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20has%20emerged%20as%20a%20powerful%20subset%20of%20machine%20learning%20%28ML%29%0Aand%20artificial%20intelligence%20%28AI%29%2C%20outperforming%20traditional%20ML%20methods%2C%0Aespecially%20in%20handling%20unstructured%20and%20large%20datasets.%20Its%20impact%20spans%20across%0Avarious%20domains%2C%20including%20speech%20recognition%2C%20healthcare%2C%20autonomous%20vehicles%2C%0Acybersecurity%2C%20predictive%20analytics%2C%20and%20more.%20However%2C%20the%20complexity%20and%0Adynamic%20nature%20of%20real-world%20problems%20present%20challenges%20in%20designing%20effective%0Adeep%20learning%20models.%20Consequently%2C%20several%20deep%20learning%20models%20have%20been%0Adeveloped%20to%20address%20different%20problems%20and%20applications.%20In%20this%20article%2C%20we%0Aconduct%20a%20comprehensive%20survey%20of%20various%20deep%20learning%20models%2C%20including%0AConvolutional%20Neural%20Networks%20%28CNNs%29%2C%20Recurrent%20Neural%20Networks%20%28RNNs%29%2C%0AGenerative%20Models%2C%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20and%20Deep%20Transfer%0ALearning.%20We%20examine%20the%20structure%2C%20applications%2C%20benefits%2C%20and%20limitations%20of%0Aeach%20model.%20Furthermore%2C%20we%20perform%20an%20analysis%20using%20three%20publicly%20available%0Adatasets%3A%20IMDB%2C%20ARAS%2C%20and%20Fruit-360.%20We%20compare%20the%20performance%20of%20six%20renowned%0Adeep%20learning%20models%3A%20CNN%2C%20Simple%20RNN%2C%20Long%20Short-Term%20Memory%20%28LSTM%29%2C%0ABidirectional%20LSTM%2C%20Gated%20Recurrent%20Unit%20%28GRU%29%2C%20and%20Bidirectional%20GRU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.17473v3&entry.124074799=Read"},
{"title": "PESFormer: Boosting Macro- and Micro-expression Spotting with Direct\n  Timestamp Encoding", "author": "Wang-Wang Yu and Kai-Fu Yang and Xiangrui Hu and Jingwen Jiang and Hong-Mei Yan and Yong-Jie Li", "abstract": "  The task of macro- and micro-expression spotting aims to precisely localize\nand categorize temporal expression instances within untrimmed videos. Given the\nsparse distribution and varying durations of expressions, existing anchor-based\nmethods often represent instances by encoding their deviations from predefined\nanchors. Additionally, these methods typically slice the untrimmed videos into\nfixed-length sliding windows. However, anchor-based encoding often fails to\ncapture all training intervals, and slicing the original video as sliding\nwindows can result in valuable training intervals being discarded. To overcome\nthese limitations, we introduce PESFormer, a simple yet effective model based\non the vision transformer architecture to achieve point-to-interval expression\nspotting. PESFormer employs a direct timestamp encoding (DTE) approach to\nreplace anchors, enabling binary classification of each timestamp instead of\noptimizing entire ground truths. Thus, all training intervals are retained in\nthe form of discrete timestamps. To maximize the utilization of training\nintervals, we enhance the preprocessing process by replacing the short videos\nproduced through the sliding window method.Instead, we implement a strategy\nthat involves zero-padding the untrimmed training videos to create uniform,\nlonger videos of a predetermined duration. This operation efficiently preserves\nthe original training intervals and eliminates video slice\nenhancement.Extensive qualitative and quantitative evaluations on three\ndatasets -- CAS(ME)^2, CAS(ME)^3 and SAMM-LV -- demonstrate that our PESFormer\noutperforms existing techniques, achieving the best performance.\n", "link": "http://arxiv.org/abs/2410.18695v1", "date": "2024-10-24", "relevancy": 2.6378, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PESFormer%3A%20Boosting%20Macro-%20and%20Micro-expression%20Spotting%20with%20Direct%0A%20%20Timestamp%20Encoding&body=Title%3A%20PESFormer%3A%20Boosting%20Macro-%20and%20Micro-expression%20Spotting%20with%20Direct%0A%20%20Timestamp%20Encoding%0AAuthor%3A%20Wang-Wang%20Yu%20and%20Kai-Fu%20Yang%20and%20Xiangrui%20Hu%20and%20Jingwen%20Jiang%20and%20Hong-Mei%20Yan%20and%20Yong-Jie%20Li%0AAbstract%3A%20%20%20The%20task%20of%20macro-%20and%20micro-expression%20spotting%20aims%20to%20precisely%20localize%0Aand%20categorize%20temporal%20expression%20instances%20within%20untrimmed%20videos.%20Given%20the%0Asparse%20distribution%20and%20varying%20durations%20of%20expressions%2C%20existing%20anchor-based%0Amethods%20often%20represent%20instances%20by%20encoding%20their%20deviations%20from%20predefined%0Aanchors.%20Additionally%2C%20these%20methods%20typically%20slice%20the%20untrimmed%20videos%20into%0Afixed-length%20sliding%20windows.%20However%2C%20anchor-based%20encoding%20often%20fails%20to%0Acapture%20all%20training%20intervals%2C%20and%20slicing%20the%20original%20video%20as%20sliding%0Awindows%20can%20result%20in%20valuable%20training%20intervals%20being%20discarded.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20PESFormer%2C%20a%20simple%20yet%20effective%20model%20based%0Aon%20the%20vision%20transformer%20architecture%20to%20achieve%20point-to-interval%20expression%0Aspotting.%20PESFormer%20employs%20a%20direct%20timestamp%20encoding%20%28DTE%29%20approach%20to%0Areplace%20anchors%2C%20enabling%20binary%20classification%20of%20each%20timestamp%20instead%20of%0Aoptimizing%20entire%20ground%20truths.%20Thus%2C%20all%20training%20intervals%20are%20retained%20in%0Athe%20form%20of%20discrete%20timestamps.%20To%20maximize%20the%20utilization%20of%20training%0Aintervals%2C%20we%20enhance%20the%20preprocessing%20process%20by%20replacing%20the%20short%20videos%0Aproduced%20through%20the%20sliding%20window%20method.Instead%2C%20we%20implement%20a%20strategy%0Athat%20involves%20zero-padding%20the%20untrimmed%20training%20videos%20to%20create%20uniform%2C%0Alonger%20videos%20of%20a%20predetermined%20duration.%20This%20operation%20efficiently%20preserves%0Athe%20original%20training%20intervals%20and%20eliminates%20video%20slice%0Aenhancement.Extensive%20qualitative%20and%20quantitative%20evaluations%20on%20three%0Adatasets%20--%20CAS%28ME%29%5E2%2C%20CAS%28ME%29%5E3%20and%20SAMM-LV%20--%20demonstrate%20that%20our%20PESFormer%0Aoutperforms%20existing%20techniques%2C%20achieving%20the%20best%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPESFormer%253A%2520Boosting%2520Macro-%2520and%2520Micro-expression%2520Spotting%2520with%2520Direct%250A%2520%2520Timestamp%2520Encoding%26entry.906535625%3DWang-Wang%2520Yu%2520and%2520Kai-Fu%2520Yang%2520and%2520Xiangrui%2520Hu%2520and%2520Jingwen%2520Jiang%2520and%2520Hong-Mei%2520Yan%2520and%2520Yong-Jie%2520Li%26entry.1292438233%3D%2520%2520The%2520task%2520of%2520macro-%2520and%2520micro-expression%2520spotting%2520aims%2520to%2520precisely%2520localize%250Aand%2520categorize%2520temporal%2520expression%2520instances%2520within%2520untrimmed%2520videos.%2520Given%2520the%250Asparse%2520distribution%2520and%2520varying%2520durations%2520of%2520expressions%252C%2520existing%2520anchor-based%250Amethods%2520often%2520represent%2520instances%2520by%2520encoding%2520their%2520deviations%2520from%2520predefined%250Aanchors.%2520Additionally%252C%2520these%2520methods%2520typically%2520slice%2520the%2520untrimmed%2520videos%2520into%250Afixed-length%2520sliding%2520windows.%2520However%252C%2520anchor-based%2520encoding%2520often%2520fails%2520to%250Acapture%2520all%2520training%2520intervals%252C%2520and%2520slicing%2520the%2520original%2520video%2520as%2520sliding%250Awindows%2520can%2520result%2520in%2520valuable%2520training%2520intervals%2520being%2520discarded.%2520To%2520overcome%250Athese%2520limitations%252C%2520we%2520introduce%2520PESFormer%252C%2520a%2520simple%2520yet%2520effective%2520model%2520based%250Aon%2520the%2520vision%2520transformer%2520architecture%2520to%2520achieve%2520point-to-interval%2520expression%250Aspotting.%2520PESFormer%2520employs%2520a%2520direct%2520timestamp%2520encoding%2520%2528DTE%2529%2520approach%2520to%250Areplace%2520anchors%252C%2520enabling%2520binary%2520classification%2520of%2520each%2520timestamp%2520instead%2520of%250Aoptimizing%2520entire%2520ground%2520truths.%2520Thus%252C%2520all%2520training%2520intervals%2520are%2520retained%2520in%250Athe%2520form%2520of%2520discrete%2520timestamps.%2520To%2520maximize%2520the%2520utilization%2520of%2520training%250Aintervals%252C%2520we%2520enhance%2520the%2520preprocessing%2520process%2520by%2520replacing%2520the%2520short%2520videos%250Aproduced%2520through%2520the%2520sliding%2520window%2520method.Instead%252C%2520we%2520implement%2520a%2520strategy%250Athat%2520involves%2520zero-padding%2520the%2520untrimmed%2520training%2520videos%2520to%2520create%2520uniform%252C%250Alonger%2520videos%2520of%2520a%2520predetermined%2520duration.%2520This%2520operation%2520efficiently%2520preserves%250Athe%2520original%2520training%2520intervals%2520and%2520eliminates%2520video%2520slice%250Aenhancement.Extensive%2520qualitative%2520and%2520quantitative%2520evaluations%2520on%2520three%250Adatasets%2520--%2520CAS%2528ME%2529%255E2%252C%2520CAS%2528ME%2529%255E3%2520and%2520SAMM-LV%2520--%2520demonstrate%2520that%2520our%2520PESFormer%250Aoutperforms%2520existing%2520techniques%252C%2520achieving%2520the%2520best%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PESFormer%3A%20Boosting%20Macro-%20and%20Micro-expression%20Spotting%20with%20Direct%0A%20%20Timestamp%20Encoding&entry.906535625=Wang-Wang%20Yu%20and%20Kai-Fu%20Yang%20and%20Xiangrui%20Hu%20and%20Jingwen%20Jiang%20and%20Hong-Mei%20Yan%20and%20Yong-Jie%20Li&entry.1292438233=%20%20The%20task%20of%20macro-%20and%20micro-expression%20spotting%20aims%20to%20precisely%20localize%0Aand%20categorize%20temporal%20expression%20instances%20within%20untrimmed%20videos.%20Given%20the%0Asparse%20distribution%20and%20varying%20durations%20of%20expressions%2C%20existing%20anchor-based%0Amethods%20often%20represent%20instances%20by%20encoding%20their%20deviations%20from%20predefined%0Aanchors.%20Additionally%2C%20these%20methods%20typically%20slice%20the%20untrimmed%20videos%20into%0Afixed-length%20sliding%20windows.%20However%2C%20anchor-based%20encoding%20often%20fails%20to%0Acapture%20all%20training%20intervals%2C%20and%20slicing%20the%20original%20video%20as%20sliding%0Awindows%20can%20result%20in%20valuable%20training%20intervals%20being%20discarded.%20To%20overcome%0Athese%20limitations%2C%20we%20introduce%20PESFormer%2C%20a%20simple%20yet%20effective%20model%20based%0Aon%20the%20vision%20transformer%20architecture%20to%20achieve%20point-to-interval%20expression%0Aspotting.%20PESFormer%20employs%20a%20direct%20timestamp%20encoding%20%28DTE%29%20approach%20to%0Areplace%20anchors%2C%20enabling%20binary%20classification%20of%20each%20timestamp%20instead%20of%0Aoptimizing%20entire%20ground%20truths.%20Thus%2C%20all%20training%20intervals%20are%20retained%20in%0Athe%20form%20of%20discrete%20timestamps.%20To%20maximize%20the%20utilization%20of%20training%0Aintervals%2C%20we%20enhance%20the%20preprocessing%20process%20by%20replacing%20the%20short%20videos%0Aproduced%20through%20the%20sliding%20window%20method.Instead%2C%20we%20implement%20a%20strategy%0Athat%20involves%20zero-padding%20the%20untrimmed%20training%20videos%20to%20create%20uniform%2C%0Alonger%20videos%20of%20a%20predetermined%20duration.%20This%20operation%20efficiently%20preserves%0Athe%20original%20training%20intervals%20and%20eliminates%20video%20slice%0Aenhancement.Extensive%20qualitative%20and%20quantitative%20evaluations%20on%20three%0Adatasets%20--%20CAS%28ME%29%5E2%2C%20CAS%28ME%29%5E3%20and%20SAMM-LV%20--%20demonstrate%20that%20our%20PESFormer%0Aoutperforms%20existing%20techniques%2C%20achieving%20the%20best%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18695v1&entry.124074799=Read"},
{"title": "Deep Insights into Cognitive Decline: A Survey of Leveraging\n  Non-Intrusive Modalities with Deep Learning Techniques", "author": "David Ortiz-Perez and Manuel Benavent-Lledo and Jose Garcia-Rodriguez and David Tom\u00e1s and M. Flores Vizcaya-Moreno", "abstract": "  Cognitive decline is a natural part of aging, often resulting in reduced\ncognitive abilities. In some cases, however, this decline is more pronounced,\ntypically due to disorders such as Alzheimer's disease. Early detection of\nanomalous cognitive decline is crucial, as it can facilitate timely\nprofessional intervention. While medical data can help in this detection, it\noften involves invasive procedures. An alternative approach is to employ\nnon-intrusive techniques such as speech or handwriting analysis, which do not\nnecessarily affect daily activities. This survey reviews the most relevant\nmethodologies that use deep learning techniques to automate the cognitive\ndecline estimation task, including audio, text, and visual processing. We\ndiscuss the key features and advantages of each modality and methodology,\nincluding state-of-the-art approaches like Transformer architecture and\nfoundation models. In addition, we present works that integrate different\nmodalities to develop multimodal models. We also highlight the most significant\ndatasets and the quantitative results from studies using these resources. From\nthis review, several conclusions emerge. In most cases, the textual modality\nachieves the best results and is the most relevant for detecting cognitive\ndecline. Moreover, combining various approaches from individual modalities into\na multimodal model consistently enhances performance across nearly all\nscenarios.\n", "link": "http://arxiv.org/abs/2410.18972v1", "date": "2024-10-24", "relevancy": 2.6347, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Insights%20into%20Cognitive%20Decline%3A%20A%20Survey%20of%20Leveraging%0A%20%20Non-Intrusive%20Modalities%20with%20Deep%20Learning%20Techniques&body=Title%3A%20Deep%20Insights%20into%20Cognitive%20Decline%3A%20A%20Survey%20of%20Leveraging%0A%20%20Non-Intrusive%20Modalities%20with%20Deep%20Learning%20Techniques%0AAuthor%3A%20David%20Ortiz-Perez%20and%20Manuel%20Benavent-Lledo%20and%20Jose%20Garcia-Rodriguez%20and%20David%20Tom%C3%A1s%20and%20M.%20Flores%20Vizcaya-Moreno%0AAbstract%3A%20%20%20Cognitive%20decline%20is%20a%20natural%20part%20of%20aging%2C%20often%20resulting%20in%20reduced%0Acognitive%20abilities.%20In%20some%20cases%2C%20however%2C%20this%20decline%20is%20more%20pronounced%2C%0Atypically%20due%20to%20disorders%20such%20as%20Alzheimer%27s%20disease.%20Early%20detection%20of%0Aanomalous%20cognitive%20decline%20is%20crucial%2C%20as%20it%20can%20facilitate%20timely%0Aprofessional%20intervention.%20While%20medical%20data%20can%20help%20in%20this%20detection%2C%20it%0Aoften%20involves%20invasive%20procedures.%20An%20alternative%20approach%20is%20to%20employ%0Anon-intrusive%20techniques%20such%20as%20speech%20or%20handwriting%20analysis%2C%20which%20do%20not%0Anecessarily%20affect%20daily%20activities.%20This%20survey%20reviews%20the%20most%20relevant%0Amethodologies%20that%20use%20deep%20learning%20techniques%20to%20automate%20the%20cognitive%0Adecline%20estimation%20task%2C%20including%20audio%2C%20text%2C%20and%20visual%20processing.%20We%0Adiscuss%20the%20key%20features%20and%20advantages%20of%20each%20modality%20and%20methodology%2C%0Aincluding%20state-of-the-art%20approaches%20like%20Transformer%20architecture%20and%0Afoundation%20models.%20In%20addition%2C%20we%20present%20works%20that%20integrate%20different%0Amodalities%20to%20develop%20multimodal%20models.%20We%20also%20highlight%20the%20most%20significant%0Adatasets%20and%20the%20quantitative%20results%20from%20studies%20using%20these%20resources.%20From%0Athis%20review%2C%20several%20conclusions%20emerge.%20In%20most%20cases%2C%20the%20textual%20modality%0Aachieves%20the%20best%20results%20and%20is%20the%20most%20relevant%20for%20detecting%20cognitive%0Adecline.%20Moreover%2C%20combining%20various%20approaches%20from%20individual%20modalities%20into%0Aa%20multimodal%20model%20consistently%20enhances%20performance%20across%20nearly%20all%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Insights%2520into%2520Cognitive%2520Decline%253A%2520A%2520Survey%2520of%2520Leveraging%250A%2520%2520Non-Intrusive%2520Modalities%2520with%2520Deep%2520Learning%2520Techniques%26entry.906535625%3DDavid%2520Ortiz-Perez%2520and%2520Manuel%2520Benavent-Lledo%2520and%2520Jose%2520Garcia-Rodriguez%2520and%2520David%2520Tom%25C3%25A1s%2520and%2520M.%2520Flores%2520Vizcaya-Moreno%26entry.1292438233%3D%2520%2520Cognitive%2520decline%2520is%2520a%2520natural%2520part%2520of%2520aging%252C%2520often%2520resulting%2520in%2520reduced%250Acognitive%2520abilities.%2520In%2520some%2520cases%252C%2520however%252C%2520this%2520decline%2520is%2520more%2520pronounced%252C%250Atypically%2520due%2520to%2520disorders%2520such%2520as%2520Alzheimer%2527s%2520disease.%2520Early%2520detection%2520of%250Aanomalous%2520cognitive%2520decline%2520is%2520crucial%252C%2520as%2520it%2520can%2520facilitate%2520timely%250Aprofessional%2520intervention.%2520While%2520medical%2520data%2520can%2520help%2520in%2520this%2520detection%252C%2520it%250Aoften%2520involves%2520invasive%2520procedures.%2520An%2520alternative%2520approach%2520is%2520to%2520employ%250Anon-intrusive%2520techniques%2520such%2520as%2520speech%2520or%2520handwriting%2520analysis%252C%2520which%2520do%2520not%250Anecessarily%2520affect%2520daily%2520activities.%2520This%2520survey%2520reviews%2520the%2520most%2520relevant%250Amethodologies%2520that%2520use%2520deep%2520learning%2520techniques%2520to%2520automate%2520the%2520cognitive%250Adecline%2520estimation%2520task%252C%2520including%2520audio%252C%2520text%252C%2520and%2520visual%2520processing.%2520We%250Adiscuss%2520the%2520key%2520features%2520and%2520advantages%2520of%2520each%2520modality%2520and%2520methodology%252C%250Aincluding%2520state-of-the-art%2520approaches%2520like%2520Transformer%2520architecture%2520and%250Afoundation%2520models.%2520In%2520addition%252C%2520we%2520present%2520works%2520that%2520integrate%2520different%250Amodalities%2520to%2520develop%2520multimodal%2520models.%2520We%2520also%2520highlight%2520the%2520most%2520significant%250Adatasets%2520and%2520the%2520quantitative%2520results%2520from%2520studies%2520using%2520these%2520resources.%2520From%250Athis%2520review%252C%2520several%2520conclusions%2520emerge.%2520In%2520most%2520cases%252C%2520the%2520textual%2520modality%250Aachieves%2520the%2520best%2520results%2520and%2520is%2520the%2520most%2520relevant%2520for%2520detecting%2520cognitive%250Adecline.%2520Moreover%252C%2520combining%2520various%2520approaches%2520from%2520individual%2520modalities%2520into%250Aa%2520multimodal%2520model%2520consistently%2520enhances%2520performance%2520across%2520nearly%2520all%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Insights%20into%20Cognitive%20Decline%3A%20A%20Survey%20of%20Leveraging%0A%20%20Non-Intrusive%20Modalities%20with%20Deep%20Learning%20Techniques&entry.906535625=David%20Ortiz-Perez%20and%20Manuel%20Benavent-Lledo%20and%20Jose%20Garcia-Rodriguez%20and%20David%20Tom%C3%A1s%20and%20M.%20Flores%20Vizcaya-Moreno&entry.1292438233=%20%20Cognitive%20decline%20is%20a%20natural%20part%20of%20aging%2C%20often%20resulting%20in%20reduced%0Acognitive%20abilities.%20In%20some%20cases%2C%20however%2C%20this%20decline%20is%20more%20pronounced%2C%0Atypically%20due%20to%20disorders%20such%20as%20Alzheimer%27s%20disease.%20Early%20detection%20of%0Aanomalous%20cognitive%20decline%20is%20crucial%2C%20as%20it%20can%20facilitate%20timely%0Aprofessional%20intervention.%20While%20medical%20data%20can%20help%20in%20this%20detection%2C%20it%0Aoften%20involves%20invasive%20procedures.%20An%20alternative%20approach%20is%20to%20employ%0Anon-intrusive%20techniques%20such%20as%20speech%20or%20handwriting%20analysis%2C%20which%20do%20not%0Anecessarily%20affect%20daily%20activities.%20This%20survey%20reviews%20the%20most%20relevant%0Amethodologies%20that%20use%20deep%20learning%20techniques%20to%20automate%20the%20cognitive%0Adecline%20estimation%20task%2C%20including%20audio%2C%20text%2C%20and%20visual%20processing.%20We%0Adiscuss%20the%20key%20features%20and%20advantages%20of%20each%20modality%20and%20methodology%2C%0Aincluding%20state-of-the-art%20approaches%20like%20Transformer%20architecture%20and%0Afoundation%20models.%20In%20addition%2C%20we%20present%20works%20that%20integrate%20different%0Amodalities%20to%20develop%20multimodal%20models.%20We%20also%20highlight%20the%20most%20significant%0Adatasets%20and%20the%20quantitative%20results%20from%20studies%20using%20these%20resources.%20From%0Athis%20review%2C%20several%20conclusions%20emerge.%20In%20most%20cases%2C%20the%20textual%20modality%0Aachieves%20the%20best%20results%20and%20is%20the%20most%20relevant%20for%20detecting%20cognitive%0Adecline.%20Moreover%2C%20combining%20various%20approaches%20from%20individual%20modalities%20into%0Aa%20multimodal%20model%20consistently%20enhances%20performance%20across%20nearly%20all%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18972v1&entry.124074799=Read"},
{"title": "Ensemble architecture in polyp segmentation", "author": "Hao-Yun Hsu and Yi-Ching Cheng and Guan-Hua Huang", "abstract": "  In this research, we revisit the architecture of semantic segmentation and\nevaluate the models excelling in polyp segmentation. We introduce an integrated\nframework that harnesses the advantages of different models to attain an\noptimal outcome. More specifically, we fuse the learned features from\nconvolutional and transformer models for prediction, and we view this approach\nas an ensemble technique to enhance model performance. Our experiments on polyp\nsegmentation reveal that the proposed architecture surpasses other top models,\nexhibiting improved learning capacity and resilience. The code is available at\nhttps://github.com/HuangDLab/EnFormer.\n", "link": "http://arxiv.org/abs/2408.07262v2", "date": "2024-10-24", "relevancy": 2.6122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20architecture%20in%20polyp%20segmentation&body=Title%3A%20Ensemble%20architecture%20in%20polyp%20segmentation%0AAuthor%3A%20Hao-Yun%20Hsu%20and%20Yi-Ching%20Cheng%20and%20Guan-Hua%20Huang%0AAbstract%3A%20%20%20In%20this%20research%2C%20we%20revisit%20the%20architecture%20of%20semantic%20segmentation%20and%0Aevaluate%20the%20models%20excelling%20in%20polyp%20segmentation.%20We%20introduce%20an%20integrated%0Aframework%20that%20harnesses%20the%20advantages%20of%20different%20models%20to%20attain%20an%0Aoptimal%20outcome.%20More%20specifically%2C%20we%20fuse%20the%20learned%20features%20from%0Aconvolutional%20and%20transformer%20models%20for%20prediction%2C%20and%20we%20view%20this%20approach%0Aas%20an%20ensemble%20technique%20to%20enhance%20model%20performance.%20Our%20experiments%20on%20polyp%0Asegmentation%20reveal%20that%20the%20proposed%20architecture%20surpasses%20other%20top%20models%2C%0Aexhibiting%20improved%20learning%20capacity%20and%20resilience.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HuangDLab/EnFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520architecture%2520in%2520polyp%2520segmentation%26entry.906535625%3DHao-Yun%2520Hsu%2520and%2520Yi-Ching%2520Cheng%2520and%2520Guan-Hua%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520research%252C%2520we%2520revisit%2520the%2520architecture%2520of%2520semantic%2520segmentation%2520and%250Aevaluate%2520the%2520models%2520excelling%2520in%2520polyp%2520segmentation.%2520We%2520introduce%2520an%2520integrated%250Aframework%2520that%2520harnesses%2520the%2520advantages%2520of%2520different%2520models%2520to%2520attain%2520an%250Aoptimal%2520outcome.%2520More%2520specifically%252C%2520we%2520fuse%2520the%2520learned%2520features%2520from%250Aconvolutional%2520and%2520transformer%2520models%2520for%2520prediction%252C%2520and%2520we%2520view%2520this%2520approach%250Aas%2520an%2520ensemble%2520technique%2520to%2520enhance%2520model%2520performance.%2520Our%2520experiments%2520on%2520polyp%250Asegmentation%2520reveal%2520that%2520the%2520proposed%2520architecture%2520surpasses%2520other%2520top%2520models%252C%250Aexhibiting%2520improved%2520learning%2520capacity%2520and%2520resilience.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/HuangDLab/EnFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20architecture%20in%20polyp%20segmentation&entry.906535625=Hao-Yun%20Hsu%20and%20Yi-Ching%20Cheng%20and%20Guan-Hua%20Huang&entry.1292438233=%20%20In%20this%20research%2C%20we%20revisit%20the%20architecture%20of%20semantic%20segmentation%20and%0Aevaluate%20the%20models%20excelling%20in%20polyp%20segmentation.%20We%20introduce%20an%20integrated%0Aframework%20that%20harnesses%20the%20advantages%20of%20different%20models%20to%20attain%20an%0Aoptimal%20outcome.%20More%20specifically%2C%20we%20fuse%20the%20learned%20features%20from%0Aconvolutional%20and%20transformer%20models%20for%20prediction%2C%20and%20we%20view%20this%20approach%0Aas%20an%20ensemble%20technique%20to%20enhance%20model%20performance.%20Our%20experiments%20on%20polyp%0Asegmentation%20reveal%20that%20the%20proposed%20architecture%20surpasses%20other%20top%20models%2C%0Aexhibiting%20improved%20learning%20capacity%20and%20resilience.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HuangDLab/EnFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07262v2&entry.124074799=Read"},
{"title": "ChatSearch: a Dataset and a Generative Retrieval Model for General\n  Conversational Image Retrieval", "author": "Zijia Zhao and Longteng Guo and Tongtian Yue and Erdong Hu and Shuai Shao and Zehuan Yuan and Hua Huang and Jing Liu", "abstract": "  In this paper, we investigate the task of general conversational image\nretrieval on open-domain images. The objective is to search for images based on\ninteractive conversations between humans and computers. To advance this task,\nwe curate a dataset called ChatSearch. This dataset includes a multi-round\nmultimodal conversational context query for each target image, thereby\nrequiring the retrieval system to find the accurate image from database.\nSimultaneously, we propose a generative retrieval model named ChatSearcher,\nwhich is trained end-to-end to accept/produce interleaved image-text\ninputs/outputs. ChatSearcher exhibits strong capability in reasoning with\nmultimodal context and can leverage world knowledge to yield visual retrieval\nresults. It demonstrates superior performance on the ChatSearch dataset and\nalso achieves competitive results on other image retrieval tasks and visual\nconversation tasks. We anticipate that this work will inspire further research\non interactive multimodal retrieval systems. Our dataset will be available at\nhttps://github.com/joez17/ChatSearch.\n", "link": "http://arxiv.org/abs/2410.18715v1", "date": "2024-10-24", "relevancy": 2.603, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5355}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatSearch%3A%20a%20Dataset%20and%20a%20Generative%20Retrieval%20Model%20for%20General%0A%20%20Conversational%20Image%20Retrieval&body=Title%3A%20ChatSearch%3A%20a%20Dataset%20and%20a%20Generative%20Retrieval%20Model%20for%20General%0A%20%20Conversational%20Image%20Retrieval%0AAuthor%3A%20Zijia%20Zhao%20and%20Longteng%20Guo%20and%20Tongtian%20Yue%20and%20Erdong%20Hu%20and%20Shuai%20Shao%20and%20Zehuan%20Yuan%20and%20Hua%20Huang%20and%20Jing%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20task%20of%20general%20conversational%20image%0Aretrieval%20on%20open-domain%20images.%20The%20objective%20is%20to%20search%20for%20images%20based%20on%0Ainteractive%20conversations%20between%20humans%20and%20computers.%20To%20advance%20this%20task%2C%0Awe%20curate%20a%20dataset%20called%20ChatSearch.%20This%20dataset%20includes%20a%20multi-round%0Amultimodal%20conversational%20context%20query%20for%20each%20target%20image%2C%20thereby%0Arequiring%20the%20retrieval%20system%20to%20find%20the%20accurate%20image%20from%20database.%0ASimultaneously%2C%20we%20propose%20a%20generative%20retrieval%20model%20named%20ChatSearcher%2C%0Awhich%20is%20trained%20end-to-end%20to%20accept/produce%20interleaved%20image-text%0Ainputs/outputs.%20ChatSearcher%20exhibits%20strong%20capability%20in%20reasoning%20with%0Amultimodal%20context%20and%20can%20leverage%20world%20knowledge%20to%20yield%20visual%20retrieval%0Aresults.%20It%20demonstrates%20superior%20performance%20on%20the%20ChatSearch%20dataset%20and%0Aalso%20achieves%20competitive%20results%20on%20other%20image%20retrieval%20tasks%20and%20visual%0Aconversation%20tasks.%20We%20anticipate%20that%20this%20work%20will%20inspire%20further%20research%0Aon%20interactive%20multimodal%20retrieval%20systems.%20Our%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/joez17/ChatSearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatSearch%253A%2520a%2520Dataset%2520and%2520a%2520Generative%2520Retrieval%2520Model%2520for%2520General%250A%2520%2520Conversational%2520Image%2520Retrieval%26entry.906535625%3DZijia%2520Zhao%2520and%2520Longteng%2520Guo%2520and%2520Tongtian%2520Yue%2520and%2520Erdong%2520Hu%2520and%2520Shuai%2520Shao%2520and%2520Zehuan%2520Yuan%2520and%2520Hua%2520Huang%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520task%2520of%2520general%2520conversational%2520image%250Aretrieval%2520on%2520open-domain%2520images.%2520The%2520objective%2520is%2520to%2520search%2520for%2520images%2520based%2520on%250Ainteractive%2520conversations%2520between%2520humans%2520and%2520computers.%2520To%2520advance%2520this%2520task%252C%250Awe%2520curate%2520a%2520dataset%2520called%2520ChatSearch.%2520This%2520dataset%2520includes%2520a%2520multi-round%250Amultimodal%2520conversational%2520context%2520query%2520for%2520each%2520target%2520image%252C%2520thereby%250Arequiring%2520the%2520retrieval%2520system%2520to%2520find%2520the%2520accurate%2520image%2520from%2520database.%250ASimultaneously%252C%2520we%2520propose%2520a%2520generative%2520retrieval%2520model%2520named%2520ChatSearcher%252C%250Awhich%2520is%2520trained%2520end-to-end%2520to%2520accept/produce%2520interleaved%2520image-text%250Ainputs/outputs.%2520ChatSearcher%2520exhibits%2520strong%2520capability%2520in%2520reasoning%2520with%250Amultimodal%2520context%2520and%2520can%2520leverage%2520world%2520knowledge%2520to%2520yield%2520visual%2520retrieval%250Aresults.%2520It%2520demonstrates%2520superior%2520performance%2520on%2520the%2520ChatSearch%2520dataset%2520and%250Aalso%2520achieves%2520competitive%2520results%2520on%2520other%2520image%2520retrieval%2520tasks%2520and%2520visual%250Aconversation%2520tasks.%2520We%2520anticipate%2520that%2520this%2520work%2520will%2520inspire%2520further%2520research%250Aon%2520interactive%2520multimodal%2520retrieval%2520systems.%2520Our%2520dataset%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/joez17/ChatSearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatSearch%3A%20a%20Dataset%20and%20a%20Generative%20Retrieval%20Model%20for%20General%0A%20%20Conversational%20Image%20Retrieval&entry.906535625=Zijia%20Zhao%20and%20Longteng%20Guo%20and%20Tongtian%20Yue%20and%20Erdong%20Hu%20and%20Shuai%20Shao%20and%20Zehuan%20Yuan%20and%20Hua%20Huang%20and%20Jing%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20task%20of%20general%20conversational%20image%0Aretrieval%20on%20open-domain%20images.%20The%20objective%20is%20to%20search%20for%20images%20based%20on%0Ainteractive%20conversations%20between%20humans%20and%20computers.%20To%20advance%20this%20task%2C%0Awe%20curate%20a%20dataset%20called%20ChatSearch.%20This%20dataset%20includes%20a%20multi-round%0Amultimodal%20conversational%20context%20query%20for%20each%20target%20image%2C%20thereby%0Arequiring%20the%20retrieval%20system%20to%20find%20the%20accurate%20image%20from%20database.%0ASimultaneously%2C%20we%20propose%20a%20generative%20retrieval%20model%20named%20ChatSearcher%2C%0Awhich%20is%20trained%20end-to-end%20to%20accept/produce%20interleaved%20image-text%0Ainputs/outputs.%20ChatSearcher%20exhibits%20strong%20capability%20in%20reasoning%20with%0Amultimodal%20context%20and%20can%20leverage%20world%20knowledge%20to%20yield%20visual%20retrieval%0Aresults.%20It%20demonstrates%20superior%20performance%20on%20the%20ChatSearch%20dataset%20and%0Aalso%20achieves%20competitive%20results%20on%20other%20image%20retrieval%20tasks%20and%20visual%0Aconversation%20tasks.%20We%20anticipate%20that%20this%20work%20will%20inspire%20further%20research%0Aon%20interactive%20multimodal%20retrieval%20systems.%20Our%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/joez17/ChatSearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18715v1&entry.124074799=Read"},
{"title": "GADT: Enhancing Transferable Adversarial Attacks through Gradient-guided\n  Adversarial Data Transformation", "author": "Yating Ma and Xiaogang Xu and Liming Fang and Zhe Liu", "abstract": "  Current Transferable Adversarial Examples (TAE) are primarily generated by\nadding Adversarial Noise (AN). Recent studies emphasize the importance of\noptimizing Data Augmentation (DA) parameters along with AN, which poses a\ngreater threat to real-world AI applications. However, existing DA-based\nstrategies often struggle to find optimal solutions due to the challenging DA\nsearch procedure without proper guidance. In this work, we propose a novel\nDA-based attack algorithm, GADT. GADT identifies suitable DA parameters through\niterative antagonism and uses posterior estimates to update AN based on these\nparameters. We uniquely employ a differentiable DA operation library to\nidentify adversarial DA parameters and introduce a new loss function as a\nmetric during DA optimization. This loss term enhances adversarial effects\nwhile preserving the original image content, maintaining attack crypticity.\nExtensive experiments on public datasets with various networks demonstrate that\nGADT can be integrated with existing transferable attack methods, updating\ntheir DA parameters effectively while retaining their AN formulation\nstrategies. Furthermore, GADT can be utilized in other black-box attack\nscenarios, e.g., query-based attacks, offering a new avenue to enhance attacks\non real-world AI applications in both research and industrial contexts.\n", "link": "http://arxiv.org/abs/2410.18648v1", "date": "2024-10-24", "relevancy": 2.5957, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5275}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5227}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GADT%3A%20Enhancing%20Transferable%20Adversarial%20Attacks%20through%20Gradient-guided%0A%20%20Adversarial%20Data%20Transformation&body=Title%3A%20GADT%3A%20Enhancing%20Transferable%20Adversarial%20Attacks%20through%20Gradient-guided%0A%20%20Adversarial%20Data%20Transformation%0AAuthor%3A%20Yating%20Ma%20and%20Xiaogang%20Xu%20and%20Liming%20Fang%20and%20Zhe%20Liu%0AAbstract%3A%20%20%20Current%20Transferable%20Adversarial%20Examples%20%28TAE%29%20are%20primarily%20generated%20by%0Aadding%20Adversarial%20Noise%20%28AN%29.%20Recent%20studies%20emphasize%20the%20importance%20of%0Aoptimizing%20Data%20Augmentation%20%28DA%29%20parameters%20along%20with%20AN%2C%20which%20poses%20a%0Agreater%20threat%20to%20real-world%20AI%20applications.%20However%2C%20existing%20DA-based%0Astrategies%20often%20struggle%20to%20find%20optimal%20solutions%20due%20to%20the%20challenging%20DA%0Asearch%20procedure%20without%20proper%20guidance.%20In%20this%20work%2C%20we%20propose%20a%20novel%0ADA-based%20attack%20algorithm%2C%20GADT.%20GADT%20identifies%20suitable%20DA%20parameters%20through%0Aiterative%20antagonism%20and%20uses%20posterior%20estimates%20to%20update%20AN%20based%20on%20these%0Aparameters.%20We%20uniquely%20employ%20a%20differentiable%20DA%20operation%20library%20to%0Aidentify%20adversarial%20DA%20parameters%20and%20introduce%20a%20new%20loss%20function%20as%20a%0Ametric%20during%20DA%20optimization.%20This%20loss%20term%20enhances%20adversarial%20effects%0Awhile%20preserving%20the%20original%20image%20content%2C%20maintaining%20attack%20crypticity.%0AExtensive%20experiments%20on%20public%20datasets%20with%20various%20networks%20demonstrate%20that%0AGADT%20can%20be%20integrated%20with%20existing%20transferable%20attack%20methods%2C%20updating%0Atheir%20DA%20parameters%20effectively%20while%20retaining%20their%20AN%20formulation%0Astrategies.%20Furthermore%2C%20GADT%20can%20be%20utilized%20in%20other%20black-box%20attack%0Ascenarios%2C%20e.g.%2C%20query-based%20attacks%2C%20offering%20a%20new%20avenue%20to%20enhance%20attacks%0Aon%20real-world%20AI%20applications%20in%20both%20research%20and%20industrial%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGADT%253A%2520Enhancing%2520Transferable%2520Adversarial%2520Attacks%2520through%2520Gradient-guided%250A%2520%2520Adversarial%2520Data%2520Transformation%26entry.906535625%3DYating%2520Ma%2520and%2520Xiaogang%2520Xu%2520and%2520Liming%2520Fang%2520and%2520Zhe%2520Liu%26entry.1292438233%3D%2520%2520Current%2520Transferable%2520Adversarial%2520Examples%2520%2528TAE%2529%2520are%2520primarily%2520generated%2520by%250Aadding%2520Adversarial%2520Noise%2520%2528AN%2529.%2520Recent%2520studies%2520emphasize%2520the%2520importance%2520of%250Aoptimizing%2520Data%2520Augmentation%2520%2528DA%2529%2520parameters%2520along%2520with%2520AN%252C%2520which%2520poses%2520a%250Agreater%2520threat%2520to%2520real-world%2520AI%2520applications.%2520However%252C%2520existing%2520DA-based%250Astrategies%2520often%2520struggle%2520to%2520find%2520optimal%2520solutions%2520due%2520to%2520the%2520challenging%2520DA%250Asearch%2520procedure%2520without%2520proper%2520guidance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250ADA-based%2520attack%2520algorithm%252C%2520GADT.%2520GADT%2520identifies%2520suitable%2520DA%2520parameters%2520through%250Aiterative%2520antagonism%2520and%2520uses%2520posterior%2520estimates%2520to%2520update%2520AN%2520based%2520on%2520these%250Aparameters.%2520We%2520uniquely%2520employ%2520a%2520differentiable%2520DA%2520operation%2520library%2520to%250Aidentify%2520adversarial%2520DA%2520parameters%2520and%2520introduce%2520a%2520new%2520loss%2520function%2520as%2520a%250Ametric%2520during%2520DA%2520optimization.%2520This%2520loss%2520term%2520enhances%2520adversarial%2520effects%250Awhile%2520preserving%2520the%2520original%2520image%2520content%252C%2520maintaining%2520attack%2520crypticity.%250AExtensive%2520experiments%2520on%2520public%2520datasets%2520with%2520various%2520networks%2520demonstrate%2520that%250AGADT%2520can%2520be%2520integrated%2520with%2520existing%2520transferable%2520attack%2520methods%252C%2520updating%250Atheir%2520DA%2520parameters%2520effectively%2520while%2520retaining%2520their%2520AN%2520formulation%250Astrategies.%2520Furthermore%252C%2520GADT%2520can%2520be%2520utilized%2520in%2520other%2520black-box%2520attack%250Ascenarios%252C%2520e.g.%252C%2520query-based%2520attacks%252C%2520offering%2520a%2520new%2520avenue%2520to%2520enhance%2520attacks%250Aon%2520real-world%2520AI%2520applications%2520in%2520both%2520research%2520and%2520industrial%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GADT%3A%20Enhancing%20Transferable%20Adversarial%20Attacks%20through%20Gradient-guided%0A%20%20Adversarial%20Data%20Transformation&entry.906535625=Yating%20Ma%20and%20Xiaogang%20Xu%20and%20Liming%20Fang%20and%20Zhe%20Liu&entry.1292438233=%20%20Current%20Transferable%20Adversarial%20Examples%20%28TAE%29%20are%20primarily%20generated%20by%0Aadding%20Adversarial%20Noise%20%28AN%29.%20Recent%20studies%20emphasize%20the%20importance%20of%0Aoptimizing%20Data%20Augmentation%20%28DA%29%20parameters%20along%20with%20AN%2C%20which%20poses%20a%0Agreater%20threat%20to%20real-world%20AI%20applications.%20However%2C%20existing%20DA-based%0Astrategies%20often%20struggle%20to%20find%20optimal%20solutions%20due%20to%20the%20challenging%20DA%0Asearch%20procedure%20without%20proper%20guidance.%20In%20this%20work%2C%20we%20propose%20a%20novel%0ADA-based%20attack%20algorithm%2C%20GADT.%20GADT%20identifies%20suitable%20DA%20parameters%20through%0Aiterative%20antagonism%20and%20uses%20posterior%20estimates%20to%20update%20AN%20based%20on%20these%0Aparameters.%20We%20uniquely%20employ%20a%20differentiable%20DA%20operation%20library%20to%0Aidentify%20adversarial%20DA%20parameters%20and%20introduce%20a%20new%20loss%20function%20as%20a%0Ametric%20during%20DA%20optimization.%20This%20loss%20term%20enhances%20adversarial%20effects%0Awhile%20preserving%20the%20original%20image%20content%2C%20maintaining%20attack%20crypticity.%0AExtensive%20experiments%20on%20public%20datasets%20with%20various%20networks%20demonstrate%20that%0AGADT%20can%20be%20integrated%20with%20existing%20transferable%20attack%20methods%2C%20updating%0Atheir%20DA%20parameters%20effectively%20while%20retaining%20their%20AN%20formulation%0Astrategies.%20Furthermore%2C%20GADT%20can%20be%20utilized%20in%20other%20black-box%20attack%0Ascenarios%2C%20e.g.%2C%20query-based%20attacks%2C%20offering%20a%20new%20avenue%20to%20enhance%20attacks%0Aon%20real-world%20AI%20applications%20in%20both%20research%20and%20industrial%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18648v1&entry.124074799=Read"},
{"title": "CAMEL-Bench: A Comprehensive Arabic LMM Benchmark", "author": "Sara Ghaboura and Ahmed Heakl and Omkar Thawakar and Ali Alharthi and Ines Riahi and Abduljalil Saif and Jorma Laaksonen and Fahad S. Khan and Salman Khan and Rao M. Anwer", "abstract": "  Recent years have witnessed a significant interest in developing large\nmultimodal models (LMMs) capable of performing various visual reasoning and\nunderstanding tasks. This has led to the introduction of multiple LMM\nbenchmarks to evaluate LMMs on different tasks. However, most existing LMM\nevaluation benchmarks are predominantly English-centric. In this work, we\ndevelop a comprehensive LMM evaluation benchmark for the Arabic language to\nrepresent a large population of over 400 million speakers. The proposed\nbenchmark, named CAMEL-Bench, comprises eight diverse domains and 38\nsub-domains including, multi-image understanding, complex visual perception,\nhandwritten document understanding, video understanding, medical imaging, plant\ndiseases, and remote sensing-based land use understanding to evaluate broad\nscenario generalizability. Our CAMEL-Bench comprises around 29,036 questions\nthat are filtered from a larger pool of samples, where the quality is manually\nverified by native speakers to ensure reliable model assessment. We conduct\nevaluations of both closed-source, including GPT-4 series, and open-source\nLMMs. Our analysis reveals the need for substantial improvement, especially\namong the best open-source models, with even the closed-source GPT-4o achieving\nan overall score of 62%. Our benchmark and evaluation scripts are open-sourced.\n", "link": "http://arxiv.org/abs/2410.18976v1", "date": "2024-10-24", "relevancy": 2.5579, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5209}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMEL-Bench%3A%20A%20Comprehensive%20Arabic%20LMM%20Benchmark&body=Title%3A%20CAMEL-Bench%3A%20A%20Comprehensive%20Arabic%20LMM%20Benchmark%0AAuthor%3A%20Sara%20Ghaboura%20and%20Ahmed%20Heakl%20and%20Omkar%20Thawakar%20and%20Ali%20Alharthi%20and%20Ines%20Riahi%20and%20Abduljalil%20Saif%20and%20Jorma%20Laaksonen%20and%20Fahad%20S.%20Khan%20and%20Salman%20Khan%20and%20Rao%20M.%20Anwer%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20a%20significant%20interest%20in%20developing%20large%0Amultimodal%20models%20%28LMMs%29%20capable%20of%20performing%20various%20visual%20reasoning%20and%0Aunderstanding%20tasks.%20This%20has%20led%20to%20the%20introduction%20of%20multiple%20LMM%0Abenchmarks%20to%20evaluate%20LMMs%20on%20different%20tasks.%20However%2C%20most%20existing%20LMM%0Aevaluation%20benchmarks%20are%20predominantly%20English-centric.%20In%20this%20work%2C%20we%0Adevelop%20a%20comprehensive%20LMM%20evaluation%20benchmark%20for%20the%20Arabic%20language%20to%0Arepresent%20a%20large%20population%20of%20over%20400%20million%20speakers.%20The%20proposed%0Abenchmark%2C%20named%20CAMEL-Bench%2C%20comprises%20eight%20diverse%20domains%20and%2038%0Asub-domains%20including%2C%20multi-image%20understanding%2C%20complex%20visual%20perception%2C%0Ahandwritten%20document%20understanding%2C%20video%20understanding%2C%20medical%20imaging%2C%20plant%0Adiseases%2C%20and%20remote%20sensing-based%20land%20use%20understanding%20to%20evaluate%20broad%0Ascenario%20generalizability.%20Our%20CAMEL-Bench%20comprises%20around%2029%2C036%20questions%0Athat%20are%20filtered%20from%20a%20larger%20pool%20of%20samples%2C%20where%20the%20quality%20is%20manually%0Averified%20by%20native%20speakers%20to%20ensure%20reliable%20model%20assessment.%20We%20conduct%0Aevaluations%20of%20both%20closed-source%2C%20including%20GPT-4%20series%2C%20and%20open-source%0ALMMs.%20Our%20analysis%20reveals%20the%20need%20for%20substantial%20improvement%2C%20especially%0Aamong%20the%20best%20open-source%20models%2C%20with%20even%20the%20closed-source%20GPT-4o%20achieving%0Aan%20overall%20score%20of%2062%25.%20Our%20benchmark%20and%20evaluation%20scripts%20are%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMEL-Bench%253A%2520A%2520Comprehensive%2520Arabic%2520LMM%2520Benchmark%26entry.906535625%3DSara%2520Ghaboura%2520and%2520Ahmed%2520Heakl%2520and%2520Omkar%2520Thawakar%2520and%2520Ali%2520Alharthi%2520and%2520Ines%2520Riahi%2520and%2520Abduljalil%2520Saif%2520and%2520Jorma%2520Laaksonen%2520and%2520Fahad%2520S.%2520Khan%2520and%2520Salman%2520Khan%2520and%2520Rao%2520M.%2520Anwer%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520a%2520significant%2520interest%2520in%2520developing%2520large%250Amultimodal%2520models%2520%2528LMMs%2529%2520capable%2520of%2520performing%2520various%2520visual%2520reasoning%2520and%250Aunderstanding%2520tasks.%2520This%2520has%2520led%2520to%2520the%2520introduction%2520of%2520multiple%2520LMM%250Abenchmarks%2520to%2520evaluate%2520LMMs%2520on%2520different%2520tasks.%2520However%252C%2520most%2520existing%2520LMM%250Aevaluation%2520benchmarks%2520are%2520predominantly%2520English-centric.%2520In%2520this%2520work%252C%2520we%250Adevelop%2520a%2520comprehensive%2520LMM%2520evaluation%2520benchmark%2520for%2520the%2520Arabic%2520language%2520to%250Arepresent%2520a%2520large%2520population%2520of%2520over%2520400%2520million%2520speakers.%2520The%2520proposed%250Abenchmark%252C%2520named%2520CAMEL-Bench%252C%2520comprises%2520eight%2520diverse%2520domains%2520and%252038%250Asub-domains%2520including%252C%2520multi-image%2520understanding%252C%2520complex%2520visual%2520perception%252C%250Ahandwritten%2520document%2520understanding%252C%2520video%2520understanding%252C%2520medical%2520imaging%252C%2520plant%250Adiseases%252C%2520and%2520remote%2520sensing-based%2520land%2520use%2520understanding%2520to%2520evaluate%2520broad%250Ascenario%2520generalizability.%2520Our%2520CAMEL-Bench%2520comprises%2520around%252029%252C036%2520questions%250Athat%2520are%2520filtered%2520from%2520a%2520larger%2520pool%2520of%2520samples%252C%2520where%2520the%2520quality%2520is%2520manually%250Averified%2520by%2520native%2520speakers%2520to%2520ensure%2520reliable%2520model%2520assessment.%2520We%2520conduct%250Aevaluations%2520of%2520both%2520closed-source%252C%2520including%2520GPT-4%2520series%252C%2520and%2520open-source%250ALMMs.%2520Our%2520analysis%2520reveals%2520the%2520need%2520for%2520substantial%2520improvement%252C%2520especially%250Aamong%2520the%2520best%2520open-source%2520models%252C%2520with%2520even%2520the%2520closed-source%2520GPT-4o%2520achieving%250Aan%2520overall%2520score%2520of%252062%2525.%2520Our%2520benchmark%2520and%2520evaluation%2520scripts%2520are%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMEL-Bench%3A%20A%20Comprehensive%20Arabic%20LMM%20Benchmark&entry.906535625=Sara%20Ghaboura%20and%20Ahmed%20Heakl%20and%20Omkar%20Thawakar%20and%20Ali%20Alharthi%20and%20Ines%20Riahi%20and%20Abduljalil%20Saif%20and%20Jorma%20Laaksonen%20and%20Fahad%20S.%20Khan%20and%20Salman%20Khan%20and%20Rao%20M.%20Anwer&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20a%20significant%20interest%20in%20developing%20large%0Amultimodal%20models%20%28LMMs%29%20capable%20of%20performing%20various%20visual%20reasoning%20and%0Aunderstanding%20tasks.%20This%20has%20led%20to%20the%20introduction%20of%20multiple%20LMM%0Abenchmarks%20to%20evaluate%20LMMs%20on%20different%20tasks.%20However%2C%20most%20existing%20LMM%0Aevaluation%20benchmarks%20are%20predominantly%20English-centric.%20In%20this%20work%2C%20we%0Adevelop%20a%20comprehensive%20LMM%20evaluation%20benchmark%20for%20the%20Arabic%20language%20to%0Arepresent%20a%20large%20population%20of%20over%20400%20million%20speakers.%20The%20proposed%0Abenchmark%2C%20named%20CAMEL-Bench%2C%20comprises%20eight%20diverse%20domains%20and%2038%0Asub-domains%20including%2C%20multi-image%20understanding%2C%20complex%20visual%20perception%2C%0Ahandwritten%20document%20understanding%2C%20video%20understanding%2C%20medical%20imaging%2C%20plant%0Adiseases%2C%20and%20remote%20sensing-based%20land%20use%20understanding%20to%20evaluate%20broad%0Ascenario%20generalizability.%20Our%20CAMEL-Bench%20comprises%20around%2029%2C036%20questions%0Athat%20are%20filtered%20from%20a%20larger%20pool%20of%20samples%2C%20where%20the%20quality%20is%20manually%0Averified%20by%20native%20speakers%20to%20ensure%20reliable%20model%20assessment.%20We%20conduct%0Aevaluations%20of%20both%20closed-source%2C%20including%20GPT-4%20series%2C%20and%20open-source%0ALMMs.%20Our%20analysis%20reveals%20the%20need%20for%20substantial%20improvement%2C%20especially%0Aamong%20the%20best%20open-source%20models%2C%20with%20even%20the%20closed-source%20GPT-4o%20achieving%0Aan%20overall%20score%20of%2062%25.%20Our%20benchmark%20and%20evaluation%20scripts%20are%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18976v1&entry.124074799=Read"},
{"title": "Predicting the Performance of Foundation Models via\n  Agreement-on-the-Line", "author": "Rahul Saxena and Taeyoun Kim and Aman Mehra and Christina Baek and Zico Kolter and Aditi Raghunathan", "abstract": "  Estimating the out-of-distribution performance in regimes where labels are\nscarce is critical to safely deploy foundation models. Recently, it was shown\nthat ensembles of neural networks observe the phenomena\n\"agreement-on-the-line\", which can be leveraged to reliably predict OOD\nperformance without labels. However, in contrast to classical neural networks\nthat are trained on in-distribution data from scratch for numerous epochs,\nfoundation models undergo minimal finetuning from heavily pretrained weights,\nwhich may reduce the ensemble diversity needed to observe\nagreement-on-the-line. In our work, we demonstrate that when lightly finetuning\nmultiple runs from a single foundation model, the choice of randomness during\ntraining (linear head initialization, data ordering, and data subsetting) can\nlead to drastically different levels of agreement-on-the-line in the resulting\nensemble. Surprisingly, only random head initialization is able to reliably\ninduce agreement-on-the-line in finetuned foundation models across vision and\nlanguage benchmarks. Second, we demonstrate that ensembles of multiple\nfoundation models pretrained on different datasets but finetuned on the same\ntask can also show agreement-on-the-line. In total, by careful construction of\na diverse ensemble, we can utilize agreement-on-the-line-based methods to\npredict the OOD performance of foundation models with high precision.\n", "link": "http://arxiv.org/abs/2404.01542v2", "date": "2024-10-24", "relevancy": 2.5529, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20the%20Performance%20of%20Foundation%20Models%20via%0A%20%20Agreement-on-the-Line&body=Title%3A%20Predicting%20the%20Performance%20of%20Foundation%20Models%20via%0A%20%20Agreement-on-the-Line%0AAuthor%3A%20Rahul%20Saxena%20and%20Taeyoun%20Kim%20and%20Aman%20Mehra%20and%20Christina%20Baek%20and%20Zico%20Kolter%20and%20Aditi%20Raghunathan%0AAbstract%3A%20%20%20Estimating%20the%20out-of-distribution%20performance%20in%20regimes%20where%20labels%20are%0Ascarce%20is%20critical%20to%20safely%20deploy%20foundation%20models.%20Recently%2C%20it%20was%20shown%0Athat%20ensembles%20of%20neural%20networks%20observe%20the%20phenomena%0A%22agreement-on-the-line%22%2C%20which%20can%20be%20leveraged%20to%20reliably%20predict%20OOD%0Aperformance%20without%20labels.%20However%2C%20in%20contrast%20to%20classical%20neural%20networks%0Athat%20are%20trained%20on%20in-distribution%20data%20from%20scratch%20for%20numerous%20epochs%2C%0Afoundation%20models%20undergo%20minimal%20finetuning%20from%20heavily%20pretrained%20weights%2C%0Awhich%20may%20reduce%20the%20ensemble%20diversity%20needed%20to%20observe%0Aagreement-on-the-line.%20In%20our%20work%2C%20we%20demonstrate%20that%20when%20lightly%20finetuning%0Amultiple%20runs%20from%20a%20single%20foundation%20model%2C%20the%20choice%20of%20randomness%20during%0Atraining%20%28linear%20head%20initialization%2C%20data%20ordering%2C%20and%20data%20subsetting%29%20can%0Alead%20to%20drastically%20different%20levels%20of%20agreement-on-the-line%20in%20the%20resulting%0Aensemble.%20Surprisingly%2C%20only%20random%20head%20initialization%20is%20able%20to%20reliably%0Ainduce%20agreement-on-the-line%20in%20finetuned%20foundation%20models%20across%20vision%20and%0Alanguage%20benchmarks.%20Second%2C%20we%20demonstrate%20that%20ensembles%20of%20multiple%0Afoundation%20models%20pretrained%20on%20different%20datasets%20but%20finetuned%20on%20the%20same%0Atask%20can%20also%20show%20agreement-on-the-line.%20In%20total%2C%20by%20careful%20construction%20of%0Aa%20diverse%20ensemble%2C%20we%20can%20utilize%20agreement-on-the-line-based%20methods%20to%0Apredict%20the%20OOD%20performance%20of%20foundation%20models%20with%20high%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520the%2520Performance%2520of%2520Foundation%2520Models%2520via%250A%2520%2520Agreement-on-the-Line%26entry.906535625%3DRahul%2520Saxena%2520and%2520Taeyoun%2520Kim%2520and%2520Aman%2520Mehra%2520and%2520Christina%2520Baek%2520and%2520Zico%2520Kolter%2520and%2520Aditi%2520Raghunathan%26entry.1292438233%3D%2520%2520Estimating%2520the%2520out-of-distribution%2520performance%2520in%2520regimes%2520where%2520labels%2520are%250Ascarce%2520is%2520critical%2520to%2520safely%2520deploy%2520foundation%2520models.%2520Recently%252C%2520it%2520was%2520shown%250Athat%2520ensembles%2520of%2520neural%2520networks%2520observe%2520the%2520phenomena%250A%2522agreement-on-the-line%2522%252C%2520which%2520can%2520be%2520leveraged%2520to%2520reliably%2520predict%2520OOD%250Aperformance%2520without%2520labels.%2520However%252C%2520in%2520contrast%2520to%2520classical%2520neural%2520networks%250Athat%2520are%2520trained%2520on%2520in-distribution%2520data%2520from%2520scratch%2520for%2520numerous%2520epochs%252C%250Afoundation%2520models%2520undergo%2520minimal%2520finetuning%2520from%2520heavily%2520pretrained%2520weights%252C%250Awhich%2520may%2520reduce%2520the%2520ensemble%2520diversity%2520needed%2520to%2520observe%250Aagreement-on-the-line.%2520In%2520our%2520work%252C%2520we%2520demonstrate%2520that%2520when%2520lightly%2520finetuning%250Amultiple%2520runs%2520from%2520a%2520single%2520foundation%2520model%252C%2520the%2520choice%2520of%2520randomness%2520during%250Atraining%2520%2528linear%2520head%2520initialization%252C%2520data%2520ordering%252C%2520and%2520data%2520subsetting%2529%2520can%250Alead%2520to%2520drastically%2520different%2520levels%2520of%2520agreement-on-the-line%2520in%2520the%2520resulting%250Aensemble.%2520Surprisingly%252C%2520only%2520random%2520head%2520initialization%2520is%2520able%2520to%2520reliably%250Ainduce%2520agreement-on-the-line%2520in%2520finetuned%2520foundation%2520models%2520across%2520vision%2520and%250Alanguage%2520benchmarks.%2520Second%252C%2520we%2520demonstrate%2520that%2520ensembles%2520of%2520multiple%250Afoundation%2520models%2520pretrained%2520on%2520different%2520datasets%2520but%2520finetuned%2520on%2520the%2520same%250Atask%2520can%2520also%2520show%2520agreement-on-the-line.%2520In%2520total%252C%2520by%2520careful%2520construction%2520of%250Aa%2520diverse%2520ensemble%252C%2520we%2520can%2520utilize%2520agreement-on-the-line-based%2520methods%2520to%250Apredict%2520the%2520OOD%2520performance%2520of%2520foundation%2520models%2520with%2520high%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20the%20Performance%20of%20Foundation%20Models%20via%0A%20%20Agreement-on-the-Line&entry.906535625=Rahul%20Saxena%20and%20Taeyoun%20Kim%20and%20Aman%20Mehra%20and%20Christina%20Baek%20and%20Zico%20Kolter%20and%20Aditi%20Raghunathan&entry.1292438233=%20%20Estimating%20the%20out-of-distribution%20performance%20in%20regimes%20where%20labels%20are%0Ascarce%20is%20critical%20to%20safely%20deploy%20foundation%20models.%20Recently%2C%20it%20was%20shown%0Athat%20ensembles%20of%20neural%20networks%20observe%20the%20phenomena%0A%22agreement-on-the-line%22%2C%20which%20can%20be%20leveraged%20to%20reliably%20predict%20OOD%0Aperformance%20without%20labels.%20However%2C%20in%20contrast%20to%20classical%20neural%20networks%0Athat%20are%20trained%20on%20in-distribution%20data%20from%20scratch%20for%20numerous%20epochs%2C%0Afoundation%20models%20undergo%20minimal%20finetuning%20from%20heavily%20pretrained%20weights%2C%0Awhich%20may%20reduce%20the%20ensemble%20diversity%20needed%20to%20observe%0Aagreement-on-the-line.%20In%20our%20work%2C%20we%20demonstrate%20that%20when%20lightly%20finetuning%0Amultiple%20runs%20from%20a%20single%20foundation%20model%2C%20the%20choice%20of%20randomness%20during%0Atraining%20%28linear%20head%20initialization%2C%20data%20ordering%2C%20and%20data%20subsetting%29%20can%0Alead%20to%20drastically%20different%20levels%20of%20agreement-on-the-line%20in%20the%20resulting%0Aensemble.%20Surprisingly%2C%20only%20random%20head%20initialization%20is%20able%20to%20reliably%0Ainduce%20agreement-on-the-line%20in%20finetuned%20foundation%20models%20across%20vision%20and%0Alanguage%20benchmarks.%20Second%2C%20we%20demonstrate%20that%20ensembles%20of%20multiple%0Afoundation%20models%20pretrained%20on%20different%20datasets%20but%20finetuned%20on%20the%20same%0Atask%20can%20also%20show%20agreement-on-the-line.%20In%20total%2C%20by%20careful%20construction%20of%0Aa%20diverse%20ensemble%2C%20we%20can%20utilize%20agreement-on-the-line-based%20methods%20to%0Apredict%20the%20OOD%20performance%20of%20foundation%20models%20with%20high%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01542v2&entry.124074799=Read"},
{"title": "DL-Polycube: Deep learning enhanced polycube method for high-quality\n  hexahedral mesh generation and volumetric spline construction", "author": "Yuxuan Yu and Yuzhuo Fang and Hua Tong and Yongjie Jessica Zhang", "abstract": "  In this paper, we present a novel algorithm that integrates deep learning\nwith the polycube method (DL-Polycube) to generate high-quality hexahedral\n(hex) meshes, which are then used to construct volumetric splines for\nisogeometric analysis. Our DL-Polycube algorithm begins by establishing a\nconnection between surface triangular meshes and polycube structures. We employ\ndeep neural network to classify surface triangular meshes into their\ncorresponding polycube structures. Following this, we combine the acquired\npolycube structural information with unsupervised learning to perform surface\nsegmentation of triangular meshes. This step addresses the issue of\nsegmentation not corresponding to a polycube while reducing manual\nintervention. Quality hex meshes are then generated from the polycube\nstructures, with employing octree subdivision, parametric mapping and quality\nimprovement techniques. The incorporation of deep learning for creating\npolycube structures, combined with unsupervised learning for segmentation of\nsurface triangular meshes, substantially accelerates hex mesh generation.\nFinally, truncated hierarchical B-splines are constructed on the generated hex\nmeshes. We extract trivariate B\\'ezier elements from these splines and apply\nthem directly in isogeometric analysis. We offer several examples to\ndemonstrate the robustness of our DL-Polycube algorithm.\n", "link": "http://arxiv.org/abs/2410.18852v1", "date": "2024-10-24", "relevancy": 2.5508, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5765}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.477}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DL-Polycube%3A%20Deep%20learning%20enhanced%20polycube%20method%20for%20high-quality%0A%20%20hexahedral%20mesh%20generation%20and%20volumetric%20spline%20construction&body=Title%3A%20DL-Polycube%3A%20Deep%20learning%20enhanced%20polycube%20method%20for%20high-quality%0A%20%20hexahedral%20mesh%20generation%20and%20volumetric%20spline%20construction%0AAuthor%3A%20Yuxuan%20Yu%20and%20Yuzhuo%20Fang%20and%20Hua%20Tong%20and%20Yongjie%20Jessica%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20algorithm%20that%20integrates%20deep%20learning%0Awith%20the%20polycube%20method%20%28DL-Polycube%29%20to%20generate%20high-quality%20hexahedral%0A%28hex%29%20meshes%2C%20which%20are%20then%20used%20to%20construct%20volumetric%20splines%20for%0Aisogeometric%20analysis.%20Our%20DL-Polycube%20algorithm%20begins%20by%20establishing%20a%0Aconnection%20between%20surface%20triangular%20meshes%20and%20polycube%20structures.%20We%20employ%0Adeep%20neural%20network%20to%20classify%20surface%20triangular%20meshes%20into%20their%0Acorresponding%20polycube%20structures.%20Following%20this%2C%20we%20combine%20the%20acquired%0Apolycube%20structural%20information%20with%20unsupervised%20learning%20to%20perform%20surface%0Asegmentation%20of%20triangular%20meshes.%20This%20step%20addresses%20the%20issue%20of%0Asegmentation%20not%20corresponding%20to%20a%20polycube%20while%20reducing%20manual%0Aintervention.%20Quality%20hex%20meshes%20are%20then%20generated%20from%20the%20polycube%0Astructures%2C%20with%20employing%20octree%20subdivision%2C%20parametric%20mapping%20and%20quality%0Aimprovement%20techniques.%20The%20incorporation%20of%20deep%20learning%20for%20creating%0Apolycube%20structures%2C%20combined%20with%20unsupervised%20learning%20for%20segmentation%20of%0Asurface%20triangular%20meshes%2C%20substantially%20accelerates%20hex%20mesh%20generation.%0AFinally%2C%20truncated%20hierarchical%20B-splines%20are%20constructed%20on%20the%20generated%20hex%0Ameshes.%20We%20extract%20trivariate%20B%5C%27ezier%20elements%20from%20these%20splines%20and%20apply%0Athem%20directly%20in%20isogeometric%20analysis.%20We%20offer%20several%20examples%20to%0Ademonstrate%20the%20robustness%20of%20our%20DL-Polycube%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDL-Polycube%253A%2520Deep%2520learning%2520enhanced%2520polycube%2520method%2520for%2520high-quality%250A%2520%2520hexahedral%2520mesh%2520generation%2520and%2520volumetric%2520spline%2520construction%26entry.906535625%3DYuxuan%2520Yu%2520and%2520Yuzhuo%2520Fang%2520and%2520Hua%2520Tong%2520and%2520Yongjie%2520Jessica%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520algorithm%2520that%2520integrates%2520deep%2520learning%250Awith%2520the%2520polycube%2520method%2520%2528DL-Polycube%2529%2520to%2520generate%2520high-quality%2520hexahedral%250A%2528hex%2529%2520meshes%252C%2520which%2520are%2520then%2520used%2520to%2520construct%2520volumetric%2520splines%2520for%250Aisogeometric%2520analysis.%2520Our%2520DL-Polycube%2520algorithm%2520begins%2520by%2520establishing%2520a%250Aconnection%2520between%2520surface%2520triangular%2520meshes%2520and%2520polycube%2520structures.%2520We%2520employ%250Adeep%2520neural%2520network%2520to%2520classify%2520surface%2520triangular%2520meshes%2520into%2520their%250Acorresponding%2520polycube%2520structures.%2520Following%2520this%252C%2520we%2520combine%2520the%2520acquired%250Apolycube%2520structural%2520information%2520with%2520unsupervised%2520learning%2520to%2520perform%2520surface%250Asegmentation%2520of%2520triangular%2520meshes.%2520This%2520step%2520addresses%2520the%2520issue%2520of%250Asegmentation%2520not%2520corresponding%2520to%2520a%2520polycube%2520while%2520reducing%2520manual%250Aintervention.%2520Quality%2520hex%2520meshes%2520are%2520then%2520generated%2520from%2520the%2520polycube%250Astructures%252C%2520with%2520employing%2520octree%2520subdivision%252C%2520parametric%2520mapping%2520and%2520quality%250Aimprovement%2520techniques.%2520The%2520incorporation%2520of%2520deep%2520learning%2520for%2520creating%250Apolycube%2520structures%252C%2520combined%2520with%2520unsupervised%2520learning%2520for%2520segmentation%2520of%250Asurface%2520triangular%2520meshes%252C%2520substantially%2520accelerates%2520hex%2520mesh%2520generation.%250AFinally%252C%2520truncated%2520hierarchical%2520B-splines%2520are%2520constructed%2520on%2520the%2520generated%2520hex%250Ameshes.%2520We%2520extract%2520trivariate%2520B%255C%2527ezier%2520elements%2520from%2520these%2520splines%2520and%2520apply%250Athem%2520directly%2520in%2520isogeometric%2520analysis.%2520We%2520offer%2520several%2520examples%2520to%250Ademonstrate%2520the%2520robustness%2520of%2520our%2520DL-Polycube%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DL-Polycube%3A%20Deep%20learning%20enhanced%20polycube%20method%20for%20high-quality%0A%20%20hexahedral%20mesh%20generation%20and%20volumetric%20spline%20construction&entry.906535625=Yuxuan%20Yu%20and%20Yuzhuo%20Fang%20and%20Hua%20Tong%20and%20Yongjie%20Jessica%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20algorithm%20that%20integrates%20deep%20learning%0Awith%20the%20polycube%20method%20%28DL-Polycube%29%20to%20generate%20high-quality%20hexahedral%0A%28hex%29%20meshes%2C%20which%20are%20then%20used%20to%20construct%20volumetric%20splines%20for%0Aisogeometric%20analysis.%20Our%20DL-Polycube%20algorithm%20begins%20by%20establishing%20a%0Aconnection%20between%20surface%20triangular%20meshes%20and%20polycube%20structures.%20We%20employ%0Adeep%20neural%20network%20to%20classify%20surface%20triangular%20meshes%20into%20their%0Acorresponding%20polycube%20structures.%20Following%20this%2C%20we%20combine%20the%20acquired%0Apolycube%20structural%20information%20with%20unsupervised%20learning%20to%20perform%20surface%0Asegmentation%20of%20triangular%20meshes.%20This%20step%20addresses%20the%20issue%20of%0Asegmentation%20not%20corresponding%20to%20a%20polycube%20while%20reducing%20manual%0Aintervention.%20Quality%20hex%20meshes%20are%20then%20generated%20from%20the%20polycube%0Astructures%2C%20with%20employing%20octree%20subdivision%2C%20parametric%20mapping%20and%20quality%0Aimprovement%20techniques.%20The%20incorporation%20of%20deep%20learning%20for%20creating%0Apolycube%20structures%2C%20combined%20with%20unsupervised%20learning%20for%20segmentation%20of%0Asurface%20triangular%20meshes%2C%20substantially%20accelerates%20hex%20mesh%20generation.%0AFinally%2C%20truncated%20hierarchical%20B-splines%20are%20constructed%20on%20the%20generated%20hex%0Ameshes.%20We%20extract%20trivariate%20B%5C%27ezier%20elements%20from%20these%20splines%20and%20apply%0Athem%20directly%20in%20isogeometric%20analysis.%20We%20offer%20several%20examples%20to%0Ademonstrate%20the%20robustness%20of%20our%20DL-Polycube%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18852v1&entry.124074799=Read"},
{"title": "IRCNN$^{+}$: An Enhanced Iterative Residual Convolutional Neural Network\n  for Non-stationary Signal Decomposition", "author": "Feng Zhou and Antonio Cicone and Haomin Zhou", "abstract": "  Time-frequency analysis is an important and challenging task in many\napplications. Fourier and wavelet analysis are two classic methods that have\nachieved remarkable success in many fields. However, they also exhibit\nlimitations when applied to nonlinear and non-stationary signals. To address\nthis challenge, a series of nonlinear and adaptive methods, pioneered by the\nempirical mode decomposition method, have been proposed. The goal of these\nmethods is to decompose a non-stationary signal into quasi-stationary\ncomponents that enhance the clarity of features during time-frequency analysis.\nRecently, inspired by deep learning, we proposed a novel method called\niterative residual convolutional neural network (IRCNN). IRCNN not only\nachieves more stable decomposition than existing methods but also handles batch\nprocessing of large-scale signals with low computational cost. Moreover, deep\nlearning provides a unique perspective for non-stationary signal decomposition.\nIn this study, we aim to further improve IRCNN with the help of several nimble\ntechniques from deep learning and optimization to ameliorate the method and\novercome some of the limitations of this technique.\n", "link": "http://arxiv.org/abs/2309.04782v2", "date": "2024-10-24", "relevancy": 2.5269, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5318}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5122}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRCNN%24%5E%7B%2B%7D%24%3A%20An%20Enhanced%20Iterative%20Residual%20Convolutional%20Neural%20Network%0A%20%20for%20Non-stationary%20Signal%20Decomposition&body=Title%3A%20IRCNN%24%5E%7B%2B%7D%24%3A%20An%20Enhanced%20Iterative%20Residual%20Convolutional%20Neural%20Network%0A%20%20for%20Non-stationary%20Signal%20Decomposition%0AAuthor%3A%20Feng%20Zhou%20and%20Antonio%20Cicone%20and%20Haomin%20Zhou%0AAbstract%3A%20%20%20Time-frequency%20analysis%20is%20an%20important%20and%20challenging%20task%20in%20many%0Aapplications.%20Fourier%20and%20wavelet%20analysis%20are%20two%20classic%20methods%20that%20have%0Aachieved%20remarkable%20success%20in%20many%20fields.%20However%2C%20they%20also%20exhibit%0Alimitations%20when%20applied%20to%20nonlinear%20and%20non-stationary%20signals.%20To%20address%0Athis%20challenge%2C%20a%20series%20of%20nonlinear%20and%20adaptive%20methods%2C%20pioneered%20by%20the%0Aempirical%20mode%20decomposition%20method%2C%20have%20been%20proposed.%20The%20goal%20of%20these%0Amethods%20is%20to%20decompose%20a%20non-stationary%20signal%20into%20quasi-stationary%0Acomponents%20that%20enhance%20the%20clarity%20of%20features%20during%20time-frequency%20analysis.%0ARecently%2C%20inspired%20by%20deep%20learning%2C%20we%20proposed%20a%20novel%20method%20called%0Aiterative%20residual%20convolutional%20neural%20network%20%28IRCNN%29.%20IRCNN%20not%20only%0Aachieves%20more%20stable%20decomposition%20than%20existing%20methods%20but%20also%20handles%20batch%0Aprocessing%20of%20large-scale%20signals%20with%20low%20computational%20cost.%20Moreover%2C%20deep%0Alearning%20provides%20a%20unique%20perspective%20for%20non-stationary%20signal%20decomposition.%0AIn%20this%20study%2C%20we%20aim%20to%20further%20improve%20IRCNN%20with%20the%20help%20of%20several%20nimble%0Atechniques%20from%20deep%20learning%20and%20optimization%20to%20ameliorate%20the%20method%20and%0Aovercome%20some%20of%20the%20limitations%20of%20this%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.04782v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRCNN%2524%255E%257B%252B%257D%2524%253A%2520An%2520Enhanced%2520Iterative%2520Residual%2520Convolutional%2520Neural%2520Network%250A%2520%2520for%2520Non-stationary%2520Signal%2520Decomposition%26entry.906535625%3DFeng%2520Zhou%2520and%2520Antonio%2520Cicone%2520and%2520Haomin%2520Zhou%26entry.1292438233%3D%2520%2520Time-frequency%2520analysis%2520is%2520an%2520important%2520and%2520challenging%2520task%2520in%2520many%250Aapplications.%2520Fourier%2520and%2520wavelet%2520analysis%2520are%2520two%2520classic%2520methods%2520that%2520have%250Aachieved%2520remarkable%2520success%2520in%2520many%2520fields.%2520However%252C%2520they%2520also%2520exhibit%250Alimitations%2520when%2520applied%2520to%2520nonlinear%2520and%2520non-stationary%2520signals.%2520To%2520address%250Athis%2520challenge%252C%2520a%2520series%2520of%2520nonlinear%2520and%2520adaptive%2520methods%252C%2520pioneered%2520by%2520the%250Aempirical%2520mode%2520decomposition%2520method%252C%2520have%2520been%2520proposed.%2520The%2520goal%2520of%2520these%250Amethods%2520is%2520to%2520decompose%2520a%2520non-stationary%2520signal%2520into%2520quasi-stationary%250Acomponents%2520that%2520enhance%2520the%2520clarity%2520of%2520features%2520during%2520time-frequency%2520analysis.%250ARecently%252C%2520inspired%2520by%2520deep%2520learning%252C%2520we%2520proposed%2520a%2520novel%2520method%2520called%250Aiterative%2520residual%2520convolutional%2520neural%2520network%2520%2528IRCNN%2529.%2520IRCNN%2520not%2520only%250Aachieves%2520more%2520stable%2520decomposition%2520than%2520existing%2520methods%2520but%2520also%2520handles%2520batch%250Aprocessing%2520of%2520large-scale%2520signals%2520with%2520low%2520computational%2520cost.%2520Moreover%252C%2520deep%250Alearning%2520provides%2520a%2520unique%2520perspective%2520for%2520non-stationary%2520signal%2520decomposition.%250AIn%2520this%2520study%252C%2520we%2520aim%2520to%2520further%2520improve%2520IRCNN%2520with%2520the%2520help%2520of%2520several%2520nimble%250Atechniques%2520from%2520deep%2520learning%2520and%2520optimization%2520to%2520ameliorate%2520the%2520method%2520and%250Aovercome%2520some%2520of%2520the%2520limitations%2520of%2520this%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.04782v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRCNN%24%5E%7B%2B%7D%24%3A%20An%20Enhanced%20Iterative%20Residual%20Convolutional%20Neural%20Network%0A%20%20for%20Non-stationary%20Signal%20Decomposition&entry.906535625=Feng%20Zhou%20and%20Antonio%20Cicone%20and%20Haomin%20Zhou&entry.1292438233=%20%20Time-frequency%20analysis%20is%20an%20important%20and%20challenging%20task%20in%20many%0Aapplications.%20Fourier%20and%20wavelet%20analysis%20are%20two%20classic%20methods%20that%20have%0Aachieved%20remarkable%20success%20in%20many%20fields.%20However%2C%20they%20also%20exhibit%0Alimitations%20when%20applied%20to%20nonlinear%20and%20non-stationary%20signals.%20To%20address%0Athis%20challenge%2C%20a%20series%20of%20nonlinear%20and%20adaptive%20methods%2C%20pioneered%20by%20the%0Aempirical%20mode%20decomposition%20method%2C%20have%20been%20proposed.%20The%20goal%20of%20these%0Amethods%20is%20to%20decompose%20a%20non-stationary%20signal%20into%20quasi-stationary%0Acomponents%20that%20enhance%20the%20clarity%20of%20features%20during%20time-frequency%20analysis.%0ARecently%2C%20inspired%20by%20deep%20learning%2C%20we%20proposed%20a%20novel%20method%20called%0Aiterative%20residual%20convolutional%20neural%20network%20%28IRCNN%29.%20IRCNN%20not%20only%0Aachieves%20more%20stable%20decomposition%20than%20existing%20methods%20but%20also%20handles%20batch%0Aprocessing%20of%20large-scale%20signals%20with%20low%20computational%20cost.%20Moreover%2C%20deep%0Alearning%20provides%20a%20unique%20perspective%20for%20non-stationary%20signal%20decomposition.%0AIn%20this%20study%2C%20we%20aim%20to%20further%20improve%20IRCNN%20with%20the%20help%20of%20several%20nimble%0Atechniques%20from%20deep%20learning%20and%20optimization%20to%20ameliorate%20the%20method%20and%0Aovercome%20some%20of%20the%20limitations%20of%20this%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.04782v2&entry.124074799=Read"},
{"title": "Multi-Scale Diffusion: Enhancing Spatial Layout in High-Resolution\n  Panoramic Image Generation", "author": "Xiaoyu Zhang and Teng Zhou and Xinlong Zhang and Jia Wei and Yongchuan Tang", "abstract": "  Diffusion models have recently gained recognition for generating diverse and\nhigh-quality content, especially in the domain of image synthesis. These models\nexcel not only in creating fixed-size images but also in producing panoramic\nimages. However, existing methods often struggle with spatial layout\nconsistency when producing high-resolution panoramas, due to the lack of\nguidance of the global image layout. In this paper, we introduce the\nMulti-Scale Diffusion (MSD) framework, a plug-and-play module that extends the\nexisting panoramic image generation framework to multiple resolution levels. By\nutilizing gradient descent techniques, our method effectively incorporates\nstructural information from low-resolution images into high-resolution outputs.\nA comprehensive evaluation of the proposed method was conducted, comparing it\nwith the prior works in qualitative and quantitative dimensions. The evaluation\nresults demonstrate that our method significantly outperforms others in\ngenerating coherent high-resolution panoramas.\n", "link": "http://arxiv.org/abs/2410.18830v1", "date": "2024-10-24", "relevancy": 2.5021, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6275}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6275}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Diffusion%3A%20Enhancing%20Spatial%20Layout%20in%20High-Resolution%0A%20%20Panoramic%20Image%20Generation&body=Title%3A%20Multi-Scale%20Diffusion%3A%20Enhancing%20Spatial%20Layout%20in%20High-Resolution%0A%20%20Panoramic%20Image%20Generation%0AAuthor%3A%20Xiaoyu%20Zhang%20and%20Teng%20Zhou%20and%20Xinlong%20Zhang%20and%20Jia%20Wei%20and%20Yongchuan%20Tang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20gained%20recognition%20for%20generating%20diverse%20and%0Ahigh-quality%20content%2C%20especially%20in%20the%20domain%20of%20image%20synthesis.%20These%20models%0Aexcel%20not%20only%20in%20creating%20fixed-size%20images%20but%20also%20in%20producing%20panoramic%0Aimages.%20However%2C%20existing%20methods%20often%20struggle%20with%20spatial%20layout%0Aconsistency%20when%20producing%20high-resolution%20panoramas%2C%20due%20to%20the%20lack%20of%0Aguidance%20of%20the%20global%20image%20layout.%20In%20this%20paper%2C%20we%20introduce%20the%0AMulti-Scale%20Diffusion%20%28MSD%29%20framework%2C%20a%20plug-and-play%20module%20that%20extends%20the%0Aexisting%20panoramic%20image%20generation%20framework%20to%20multiple%20resolution%20levels.%20By%0Autilizing%20gradient%20descent%20techniques%2C%20our%20method%20effectively%20incorporates%0Astructural%20information%20from%20low-resolution%20images%20into%20high-resolution%20outputs.%0AA%20comprehensive%20evaluation%20of%20the%20proposed%20method%20was%20conducted%2C%20comparing%20it%0Awith%20the%20prior%20works%20in%20qualitative%20and%20quantitative%20dimensions.%20The%20evaluation%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20others%20in%0Agenerating%20coherent%20high-resolution%20panoramas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Diffusion%253A%2520Enhancing%2520Spatial%2520Layout%2520in%2520High-Resolution%250A%2520%2520Panoramic%2520Image%2520Generation%26entry.906535625%3DXiaoyu%2520Zhang%2520and%2520Teng%2520Zhou%2520and%2520Xinlong%2520Zhang%2520and%2520Jia%2520Wei%2520and%2520Yongchuan%2520Tang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520recently%2520gained%2520recognition%2520for%2520generating%2520diverse%2520and%250Ahigh-quality%2520content%252C%2520especially%2520in%2520the%2520domain%2520of%2520image%2520synthesis.%2520These%2520models%250Aexcel%2520not%2520only%2520in%2520creating%2520fixed-size%2520images%2520but%2520also%2520in%2520producing%2520panoramic%250Aimages.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520with%2520spatial%2520layout%250Aconsistency%2520when%2520producing%2520high-resolution%2520panoramas%252C%2520due%2520to%2520the%2520lack%2520of%250Aguidance%2520of%2520the%2520global%2520image%2520layout.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%250AMulti-Scale%2520Diffusion%2520%2528MSD%2529%2520framework%252C%2520a%2520plug-and-play%2520module%2520that%2520extends%2520the%250Aexisting%2520panoramic%2520image%2520generation%2520framework%2520to%2520multiple%2520resolution%2520levels.%2520By%250Autilizing%2520gradient%2520descent%2520techniques%252C%2520our%2520method%2520effectively%2520incorporates%250Astructural%2520information%2520from%2520low-resolution%2520images%2520into%2520high-resolution%2520outputs.%250AA%2520comprehensive%2520evaluation%2520of%2520the%2520proposed%2520method%2520was%2520conducted%252C%2520comparing%2520it%250Awith%2520the%2520prior%2520works%2520in%2520qualitative%2520and%2520quantitative%2520dimensions.%2520The%2520evaluation%250Aresults%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520others%2520in%250Agenerating%2520coherent%2520high-resolution%2520panoramas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Diffusion%3A%20Enhancing%20Spatial%20Layout%20in%20High-Resolution%0A%20%20Panoramic%20Image%20Generation&entry.906535625=Xiaoyu%20Zhang%20and%20Teng%20Zhou%20and%20Xinlong%20Zhang%20and%20Jia%20Wei%20and%20Yongchuan%20Tang&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20gained%20recognition%20for%20generating%20diverse%20and%0Ahigh-quality%20content%2C%20especially%20in%20the%20domain%20of%20image%20synthesis.%20These%20models%0Aexcel%20not%20only%20in%20creating%20fixed-size%20images%20but%20also%20in%20producing%20panoramic%0Aimages.%20However%2C%20existing%20methods%20often%20struggle%20with%20spatial%20layout%0Aconsistency%20when%20producing%20high-resolution%20panoramas%2C%20due%20to%20the%20lack%20of%0Aguidance%20of%20the%20global%20image%20layout.%20In%20this%20paper%2C%20we%20introduce%20the%0AMulti-Scale%20Diffusion%20%28MSD%29%20framework%2C%20a%20plug-and-play%20module%20that%20extends%20the%0Aexisting%20panoramic%20image%20generation%20framework%20to%20multiple%20resolution%20levels.%20By%0Autilizing%20gradient%20descent%20techniques%2C%20our%20method%20effectively%20incorporates%0Astructural%20information%20from%20low-resolution%20images%20into%20high-resolution%20outputs.%0AA%20comprehensive%20evaluation%20of%20the%20proposed%20method%20was%20conducted%2C%20comparing%20it%0Awith%20the%20prior%20works%20in%20qualitative%20and%20quantitative%20dimensions.%20The%20evaluation%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20others%20in%0Agenerating%20coherent%20high-resolution%20panoramas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18830v1&entry.124074799=Read"},
{"title": "Understanding Players as if They Are Talking to the Game in a Customized\n  Language: A Pilot Study", "author": "Tianze Wang and Maryam Honari-Jahromi and Styliani Katsarou and Olga Mikheeva and Theodoros Panagiotakopoulos and Oleg Smirnov and Lele Cao and Sahar Asadi", "abstract": "  This pilot study explores the application of language models (LMs) to model\ngame event sequences, treating them as a customized natural language. We\ninvestigate a popular mobile game, transforming raw event data into textual\nsequences and pretraining a Longformer model on this data. Our approach\ncaptures the rich and nuanced interactions within game sessions, effectively\nidentifying meaningful player segments. The results demonstrate the potential\nof self-supervised LMs in enhancing game design and personalization without\nrelying on ground-truth labels.\n", "link": "http://arxiv.org/abs/2410.18605v1", "date": "2024-10-24", "relevancy": 2.4828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Players%20as%20if%20They%20Are%20Talking%20to%20the%20Game%20in%20a%20Customized%0A%20%20Language%3A%20A%20Pilot%20Study&body=Title%3A%20Understanding%20Players%20as%20if%20They%20Are%20Talking%20to%20the%20Game%20in%20a%20Customized%0A%20%20Language%3A%20A%20Pilot%20Study%0AAuthor%3A%20Tianze%20Wang%20and%20Maryam%20Honari-Jahromi%20and%20Styliani%20Katsarou%20and%20Olga%20Mikheeva%20and%20Theodoros%20Panagiotakopoulos%20and%20Oleg%20Smirnov%20and%20Lele%20Cao%20and%20Sahar%20Asadi%0AAbstract%3A%20%20%20This%20pilot%20study%20explores%20the%20application%20of%20language%20models%20%28LMs%29%20to%20model%0Agame%20event%20sequences%2C%20treating%20them%20as%20a%20customized%20natural%20language.%20We%0Ainvestigate%20a%20popular%20mobile%20game%2C%20transforming%20raw%20event%20data%20into%20textual%0Asequences%20and%20pretraining%20a%20Longformer%20model%20on%20this%20data.%20Our%20approach%0Acaptures%20the%20rich%20and%20nuanced%20interactions%20within%20game%20sessions%2C%20effectively%0Aidentifying%20meaningful%20player%20segments.%20The%20results%20demonstrate%20the%20potential%0Aof%20self-supervised%20LMs%20in%20enhancing%20game%20design%20and%20personalization%20without%0Arelying%20on%20ground-truth%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Players%2520as%2520if%2520They%2520Are%2520Talking%2520to%2520the%2520Game%2520in%2520a%2520Customized%250A%2520%2520Language%253A%2520A%2520Pilot%2520Study%26entry.906535625%3DTianze%2520Wang%2520and%2520Maryam%2520Honari-Jahromi%2520and%2520Styliani%2520Katsarou%2520and%2520Olga%2520Mikheeva%2520and%2520Theodoros%2520Panagiotakopoulos%2520and%2520Oleg%2520Smirnov%2520and%2520Lele%2520Cao%2520and%2520Sahar%2520Asadi%26entry.1292438233%3D%2520%2520This%2520pilot%2520study%2520explores%2520the%2520application%2520of%2520language%2520models%2520%2528LMs%2529%2520to%2520model%250Agame%2520event%2520sequences%252C%2520treating%2520them%2520as%2520a%2520customized%2520natural%2520language.%2520We%250Ainvestigate%2520a%2520popular%2520mobile%2520game%252C%2520transforming%2520raw%2520event%2520data%2520into%2520textual%250Asequences%2520and%2520pretraining%2520a%2520Longformer%2520model%2520on%2520this%2520data.%2520Our%2520approach%250Acaptures%2520the%2520rich%2520and%2520nuanced%2520interactions%2520within%2520game%2520sessions%252C%2520effectively%250Aidentifying%2520meaningful%2520player%2520segments.%2520The%2520results%2520demonstrate%2520the%2520potential%250Aof%2520self-supervised%2520LMs%2520in%2520enhancing%2520game%2520design%2520and%2520personalization%2520without%250Arelying%2520on%2520ground-truth%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Players%20as%20if%20They%20Are%20Talking%20to%20the%20Game%20in%20a%20Customized%0A%20%20Language%3A%20A%20Pilot%20Study&entry.906535625=Tianze%20Wang%20and%20Maryam%20Honari-Jahromi%20and%20Styliani%20Katsarou%20and%20Olga%20Mikheeva%20and%20Theodoros%20Panagiotakopoulos%20and%20Oleg%20Smirnov%20and%20Lele%20Cao%20and%20Sahar%20Asadi&entry.1292438233=%20%20This%20pilot%20study%20explores%20the%20application%20of%20language%20models%20%28LMs%29%20to%20model%0Agame%20event%20sequences%2C%20treating%20them%20as%20a%20customized%20natural%20language.%20We%0Ainvestigate%20a%20popular%20mobile%20game%2C%20transforming%20raw%20event%20data%20into%20textual%0Asequences%20and%20pretraining%20a%20Longformer%20model%20on%20this%20data.%20Our%20approach%0Acaptures%20the%20rich%20and%20nuanced%20interactions%20within%20game%20sessions%2C%20effectively%0Aidentifying%20meaningful%20player%20segments.%20The%20results%20demonstrate%20the%20potential%0Aof%20self-supervised%20LMs%20in%20enhancing%20game%20design%20and%20personalization%20without%0Arelying%20on%20ground-truth%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18605v1&entry.124074799=Read"},
{"title": "LongGenBench: Long-context Generation Benchmark", "author": "Xiang Liu and Peijie Dong and Xuming Hu and Xiaowen Chu", "abstract": "  Current long-context benchmarks primarily focus on retrieval-based tests,\nrequiring Large Language Models (LLMs) to locate specific information within\nextensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.\nLong-context generation refers to the ability of a language model to generate\ncoherent and contextually accurate text that spans across lengthy passages or\ndocuments. While recent studies show strong performance on NIAH and other\nretrieval-based long-context benchmarks, there is a significant lack of\nbenchmarks for evaluating long-context generation capabilities. To bridge this\ngap and offer a comprehensive assessment, we introduce a synthetic benchmark,\nLongGenBench, which allows for flexible configurations of customized generation\ncontext lengths. LongGenBench advances beyond traditional benchmarks by\nredesigning the format of questions and necessitating that LLMs respond with a\nsingle, cohesive long-context answer. Upon extensive evaluation using\nLongGenBench, we observe that: (1) both API accessed and open source models\nexhibit performance degradation in long-context generation scenarios, ranging\nfrom 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of\nperformance degradation, with the Gemini-1.5-Flash model showing the least\ndegradation among API accessed models, and the Qwen2 series exhibiting the\nleast degradation in LongGenBench among open source models.\n", "link": "http://arxiv.org/abs/2410.04199v3", "date": "2024-10-24", "relevancy": 2.4645, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongGenBench%3A%20Long-context%20Generation%20Benchmark&body=Title%3A%20LongGenBench%3A%20Long-context%20Generation%20Benchmark%0AAuthor%3A%20Xiang%20Liu%20and%20Peijie%20Dong%20and%20Xuming%20Hu%20and%20Xiaowen%20Chu%0AAbstract%3A%20%20%20Current%20long-context%20benchmarks%20primarily%20focus%20on%20retrieval-based%20tests%2C%0Arequiring%20Large%20Language%20Models%20%28LLMs%29%20to%20locate%20specific%20information%20within%0Aextensive%20input%20contexts%2C%20such%20as%20the%20needle-in-a-haystack%20%28NIAH%29%20benchmark.%0ALong-context%20generation%20refers%20to%20the%20ability%20of%20a%20language%20model%20to%20generate%0Acoherent%20and%20contextually%20accurate%20text%20that%20spans%20across%20lengthy%20passages%20or%0Adocuments.%20While%20recent%20studies%20show%20strong%20performance%20on%20NIAH%20and%20other%0Aretrieval-based%20long-context%20benchmarks%2C%20there%20is%20a%20significant%20lack%20of%0Abenchmarks%20for%20evaluating%20long-context%20generation%20capabilities.%20To%20bridge%20this%0Agap%20and%20offer%20a%20comprehensive%20assessment%2C%20we%20introduce%20a%20synthetic%20benchmark%2C%0ALongGenBench%2C%20which%20allows%20for%20flexible%20configurations%20of%20customized%20generation%0Acontext%20lengths.%20LongGenBench%20advances%20beyond%20traditional%20benchmarks%20by%0Aredesigning%20the%20format%20of%20questions%20and%20necessitating%20that%20LLMs%20respond%20with%20a%0Asingle%2C%20cohesive%20long-context%20answer.%20Upon%20extensive%20evaluation%20using%0ALongGenBench%2C%20we%20observe%20that%3A%20%281%29%20both%20API%20accessed%20and%20open%20source%20models%0Aexhibit%20performance%20degradation%20in%20long-context%20generation%20scenarios%2C%20ranging%0Afrom%201.2%25%20to%2047.1%25%3B%20%282%29%20different%20series%20of%20LLMs%20exhibit%20varying%20trends%20of%0Aperformance%20degradation%2C%20with%20the%20Gemini-1.5-Flash%20model%20showing%20the%20least%0Adegradation%20among%20API%20accessed%20models%2C%20and%20the%20Qwen2%20series%20exhibiting%20the%0Aleast%20degradation%20in%20LongGenBench%20among%20open%20source%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04199v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongGenBench%253A%2520Long-context%2520Generation%2520Benchmark%26entry.906535625%3DXiang%2520Liu%2520and%2520Peijie%2520Dong%2520and%2520Xuming%2520Hu%2520and%2520Xiaowen%2520Chu%26entry.1292438233%3D%2520%2520Current%2520long-context%2520benchmarks%2520primarily%2520focus%2520on%2520retrieval-based%2520tests%252C%250Arequiring%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520locate%2520specific%2520information%2520within%250Aextensive%2520input%2520contexts%252C%2520such%2520as%2520the%2520needle-in-a-haystack%2520%2528NIAH%2529%2520benchmark.%250ALong-context%2520generation%2520refers%2520to%2520the%2520ability%2520of%2520a%2520language%2520model%2520to%2520generate%250Acoherent%2520and%2520contextually%2520accurate%2520text%2520that%2520spans%2520across%2520lengthy%2520passages%2520or%250Adocuments.%2520While%2520recent%2520studies%2520show%2520strong%2520performance%2520on%2520NIAH%2520and%2520other%250Aretrieval-based%2520long-context%2520benchmarks%252C%2520there%2520is%2520a%2520significant%2520lack%2520of%250Abenchmarks%2520for%2520evaluating%2520long-context%2520generation%2520capabilities.%2520To%2520bridge%2520this%250Agap%2520and%2520offer%2520a%2520comprehensive%2520assessment%252C%2520we%2520introduce%2520a%2520synthetic%2520benchmark%252C%250ALongGenBench%252C%2520which%2520allows%2520for%2520flexible%2520configurations%2520of%2520customized%2520generation%250Acontext%2520lengths.%2520LongGenBench%2520advances%2520beyond%2520traditional%2520benchmarks%2520by%250Aredesigning%2520the%2520format%2520of%2520questions%2520and%2520necessitating%2520that%2520LLMs%2520respond%2520with%2520a%250Asingle%252C%2520cohesive%2520long-context%2520answer.%2520Upon%2520extensive%2520evaluation%2520using%250ALongGenBench%252C%2520we%2520observe%2520that%253A%2520%25281%2529%2520both%2520API%2520accessed%2520and%2520open%2520source%2520models%250Aexhibit%2520performance%2520degradation%2520in%2520long-context%2520generation%2520scenarios%252C%2520ranging%250Afrom%25201.2%2525%2520to%252047.1%2525%253B%2520%25282%2529%2520different%2520series%2520of%2520LLMs%2520exhibit%2520varying%2520trends%2520of%250Aperformance%2520degradation%252C%2520with%2520the%2520Gemini-1.5-Flash%2520model%2520showing%2520the%2520least%250Adegradation%2520among%2520API%2520accessed%2520models%252C%2520and%2520the%2520Qwen2%2520series%2520exhibiting%2520the%250Aleast%2520degradation%2520in%2520LongGenBench%2520among%2520open%2520source%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04199v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongGenBench%3A%20Long-context%20Generation%20Benchmark&entry.906535625=Xiang%20Liu%20and%20Peijie%20Dong%20and%20Xuming%20Hu%20and%20Xiaowen%20Chu&entry.1292438233=%20%20Current%20long-context%20benchmarks%20primarily%20focus%20on%20retrieval-based%20tests%2C%0Arequiring%20Large%20Language%20Models%20%28LLMs%29%20to%20locate%20specific%20information%20within%0Aextensive%20input%20contexts%2C%20such%20as%20the%20needle-in-a-haystack%20%28NIAH%29%20benchmark.%0ALong-context%20generation%20refers%20to%20the%20ability%20of%20a%20language%20model%20to%20generate%0Acoherent%20and%20contextually%20accurate%20text%20that%20spans%20across%20lengthy%20passages%20or%0Adocuments.%20While%20recent%20studies%20show%20strong%20performance%20on%20NIAH%20and%20other%0Aretrieval-based%20long-context%20benchmarks%2C%20there%20is%20a%20significant%20lack%20of%0Abenchmarks%20for%20evaluating%20long-context%20generation%20capabilities.%20To%20bridge%20this%0Agap%20and%20offer%20a%20comprehensive%20assessment%2C%20we%20introduce%20a%20synthetic%20benchmark%2C%0ALongGenBench%2C%20which%20allows%20for%20flexible%20configurations%20of%20customized%20generation%0Acontext%20lengths.%20LongGenBench%20advances%20beyond%20traditional%20benchmarks%20by%0Aredesigning%20the%20format%20of%20questions%20and%20necessitating%20that%20LLMs%20respond%20with%20a%0Asingle%2C%20cohesive%20long-context%20answer.%20Upon%20extensive%20evaluation%20using%0ALongGenBench%2C%20we%20observe%20that%3A%20%281%29%20both%20API%20accessed%20and%20open%20source%20models%0Aexhibit%20performance%20degradation%20in%20long-context%20generation%20scenarios%2C%20ranging%0Afrom%201.2%25%20to%2047.1%25%3B%20%282%29%20different%20series%20of%20LLMs%20exhibit%20varying%20trends%20of%0Aperformance%20degradation%2C%20with%20the%20Gemini-1.5-Flash%20model%20showing%20the%20least%0Adegradation%20among%20API%20accessed%20models%2C%20and%20the%20Qwen2%20series%20exhibiting%20the%0Aleast%20degradation%20in%20LongGenBench%20among%20open%20source%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04199v3&entry.124074799=Read"},
{"title": "Dynamic Vocabulary Pruning in Early-Exit LLMs", "author": "Jort Vincenti and Karim Abdel Sadek and Joan Velja and Matteo Nulli and Metod Jazbec", "abstract": "  Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.\n", "link": "http://arxiv.org/abs/2410.18952v1", "date": "2024-10-24", "relevancy": 2.4539, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Vocabulary%20Pruning%20in%20Early-Exit%20LLMs&body=Title%3A%20Dynamic%20Vocabulary%20Pruning%20in%20Early-Exit%20LLMs%0AAuthor%3A%20Jort%20Vincenti%20and%20Karim%20Abdel%20Sadek%20and%20Joan%20Velja%20and%20Matteo%20Nulli%20and%20Metod%20Jazbec%0AAbstract%3A%20%20%20Increasing%20the%20size%20of%20large%20language%20models%20%28LLMs%29%20has%20been%20shown%20to%20lead%20to%0Abetter%20performance.%20However%2C%20this%20comes%20at%20the%20cost%20of%20slower%20and%20more%0Aexpensive%20inference.%20Early-exiting%20is%20a%20promising%20approach%20for%20improving%20the%0Aefficiency%20of%20LLM%20inference%20by%20enabling%20next%20token%20prediction%20at%20intermediate%0Alayers.%20Yet%2C%20the%20large%20vocabulary%20size%20in%20modern%20LLMs%20makes%20the%20confidence%0Aestimation%20required%20for%20exit%20decisions%20computationally%20expensive%2C%20diminishing%0Athe%20efficiency%20gains.%20To%20address%20this%2C%20we%20propose%20dynamically%20pruning%20the%0Avocabulary%20at%20test%20time%20for%20each%20token.%20Specifically%2C%20the%20vocabulary%20is%20pruned%0Aat%20one%20of%20the%20initial%20layers%2C%20and%20the%20smaller%20vocabulary%20is%20then%20used%0Athroughout%20the%20rest%20of%20the%20forward%20pass.%20Our%20experiments%20demonstrate%20that%20such%0Apost-hoc%20dynamic%20vocabulary%20pruning%20improves%20the%20efficiency%20of%20confidence%0Aestimation%20in%20early-exit%20LLMs%20while%20maintaining%20competitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Vocabulary%2520Pruning%2520in%2520Early-Exit%2520LLMs%26entry.906535625%3DJort%2520Vincenti%2520and%2520Karim%2520Abdel%2520Sadek%2520and%2520Joan%2520Velja%2520and%2520Matteo%2520Nulli%2520and%2520Metod%2520Jazbec%26entry.1292438233%3D%2520%2520Increasing%2520the%2520size%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520been%2520shown%2520to%2520lead%2520to%250Abetter%2520performance.%2520However%252C%2520this%2520comes%2520at%2520the%2520cost%2520of%2520slower%2520and%2520more%250Aexpensive%2520inference.%2520Early-exiting%2520is%2520a%2520promising%2520approach%2520for%2520improving%2520the%250Aefficiency%2520of%2520LLM%2520inference%2520by%2520enabling%2520next%2520token%2520prediction%2520at%2520intermediate%250Alayers.%2520Yet%252C%2520the%2520large%2520vocabulary%2520size%2520in%2520modern%2520LLMs%2520makes%2520the%2520confidence%250Aestimation%2520required%2520for%2520exit%2520decisions%2520computationally%2520expensive%252C%2520diminishing%250Athe%2520efficiency%2520gains.%2520To%2520address%2520this%252C%2520we%2520propose%2520dynamically%2520pruning%2520the%250Avocabulary%2520at%2520test%2520time%2520for%2520each%2520token.%2520Specifically%252C%2520the%2520vocabulary%2520is%2520pruned%250Aat%2520one%2520of%2520the%2520initial%2520layers%252C%2520and%2520the%2520smaller%2520vocabulary%2520is%2520then%2520used%250Athroughout%2520the%2520rest%2520of%2520the%2520forward%2520pass.%2520Our%2520experiments%2520demonstrate%2520that%2520such%250Apost-hoc%2520dynamic%2520vocabulary%2520pruning%2520improves%2520the%2520efficiency%2520of%2520confidence%250Aestimation%2520in%2520early-exit%2520LLMs%2520while%2520maintaining%2520competitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Vocabulary%20Pruning%20in%20Early-Exit%20LLMs&entry.906535625=Jort%20Vincenti%20and%20Karim%20Abdel%20Sadek%20and%20Joan%20Velja%20and%20Matteo%20Nulli%20and%20Metod%20Jazbec&entry.1292438233=%20%20Increasing%20the%20size%20of%20large%20language%20models%20%28LLMs%29%20has%20been%20shown%20to%20lead%20to%0Abetter%20performance.%20However%2C%20this%20comes%20at%20the%20cost%20of%20slower%20and%20more%0Aexpensive%20inference.%20Early-exiting%20is%20a%20promising%20approach%20for%20improving%20the%0Aefficiency%20of%20LLM%20inference%20by%20enabling%20next%20token%20prediction%20at%20intermediate%0Alayers.%20Yet%2C%20the%20large%20vocabulary%20size%20in%20modern%20LLMs%20makes%20the%20confidence%0Aestimation%20required%20for%20exit%20decisions%20computationally%20expensive%2C%20diminishing%0Athe%20efficiency%20gains.%20To%20address%20this%2C%20we%20propose%20dynamically%20pruning%20the%0Avocabulary%20at%20test%20time%20for%20each%20token.%20Specifically%2C%20the%20vocabulary%20is%20pruned%0Aat%20one%20of%20the%20initial%20layers%2C%20and%20the%20smaller%20vocabulary%20is%20then%20used%0Athroughout%20the%20rest%20of%20the%20forward%20pass.%20Our%20experiments%20demonstrate%20that%20such%0Apost-hoc%20dynamic%20vocabulary%20pruning%20improves%20the%20efficiency%20of%20confidence%0Aestimation%20in%20early-exit%20LLMs%20while%20maintaining%20competitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18952v1&entry.124074799=Read"},
{"title": "Ferret-UI 2: Mastering Universal User Interface Understanding Across\n  Platforms", "author": "Zhangheng Li and Keen You and Haotian Zhang and Di Feng and Harsh Agrawal and Xiujun Li and Mohana Prasad Sathya Moorthy and Jeff Nichols and Yinfei Yang and Zhe Gan", "abstract": "  Building a generalist model for user interface (UI) understanding is\nchallenging due to various foundational issues, such as platform diversity,\nresolution variation, and data limitation. In this paper, we introduce\nFerret-UI 2, a multimodal large language model (MLLM) designed for universal UI\nunderstanding across a wide range of platforms, including iPhone, Android,\niPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI\n2 introduces three key innovations: support for multiple platform types,\nhigh-resolution perception through adaptive scaling, and advanced task training\ndata generation powered by GPT-4o with set-of-mark visual prompting. These\nadvancements enable Ferret-UI 2 to perform complex, user-centered interactions,\nmaking it highly versatile and adaptable for the expanding diversity of\nplatform ecosystems. Extensive empirical experiments on referring, grounding,\nuser-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE\nnext-action prediction dataset, and GUI-World multi-platform benchmark\ndemonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also\nshows strong cross-platform transfer capabilities.\n", "link": "http://arxiv.org/abs/2410.18967v1", "date": "2024-10-24", "relevancy": 2.4528, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4925}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ferret-UI%202%3A%20Mastering%20Universal%20User%20Interface%20Understanding%20Across%0A%20%20Platforms&body=Title%3A%20Ferret-UI%202%3A%20Mastering%20Universal%20User%20Interface%20Understanding%20Across%0A%20%20Platforms%0AAuthor%3A%20Zhangheng%20Li%20and%20Keen%20You%20and%20Haotian%20Zhang%20and%20Di%20Feng%20and%20Harsh%20Agrawal%20and%20Xiujun%20Li%20and%20Mohana%20Prasad%20Sathya%20Moorthy%20and%20Jeff%20Nichols%20and%20Yinfei%20Yang%20and%20Zhe%20Gan%0AAbstract%3A%20%20%20Building%20a%20generalist%20model%20for%20user%20interface%20%28UI%29%20understanding%20is%0Achallenging%20due%20to%20various%20foundational%20issues%2C%20such%20as%20platform%20diversity%2C%0Aresolution%20variation%2C%20and%20data%20limitation.%20In%20this%20paper%2C%20we%20introduce%0AFerret-UI%202%2C%20a%20multimodal%20large%20language%20model%20%28MLLM%29%20designed%20for%20universal%20UI%0Aunderstanding%20across%20a%20wide%20range%20of%20platforms%2C%20including%20iPhone%2C%20Android%2C%0AiPad%2C%20Webpage%2C%20and%20AppleTV.%20Building%20on%20the%20foundation%20of%20Ferret-UI%2C%20Ferret-UI%0A2%20introduces%20three%20key%20innovations%3A%20support%20for%20multiple%20platform%20types%2C%0Ahigh-resolution%20perception%20through%20adaptive%20scaling%2C%20and%20advanced%20task%20training%0Adata%20generation%20powered%20by%20GPT-4o%20with%20set-of-mark%20visual%20prompting.%20These%0Aadvancements%20enable%20Ferret-UI%202%20to%20perform%20complex%2C%20user-centered%20interactions%2C%0Amaking%20it%20highly%20versatile%20and%20adaptable%20for%20the%20expanding%20diversity%20of%0Aplatform%20ecosystems.%20Extensive%20empirical%20experiments%20on%20referring%2C%20grounding%2C%0Auser-centric%20advanced%20tasks%20%28comprising%209%20subtasks%20%24%5Ctimes%24%205%20platforms%29%2C%20GUIDE%0Anext-action%20prediction%20dataset%2C%20and%20GUI-World%20multi-platform%20benchmark%0Ademonstrate%20that%20Ferret-UI%202%20significantly%20outperforms%20Ferret-UI%2C%20and%20also%0Ashows%20strong%20cross-platform%20transfer%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFerret-UI%25202%253A%2520Mastering%2520Universal%2520User%2520Interface%2520Understanding%2520Across%250A%2520%2520Platforms%26entry.906535625%3DZhangheng%2520Li%2520and%2520Keen%2520You%2520and%2520Haotian%2520Zhang%2520and%2520Di%2520Feng%2520and%2520Harsh%2520Agrawal%2520and%2520Xiujun%2520Li%2520and%2520Mohana%2520Prasad%2520Sathya%2520Moorthy%2520and%2520Jeff%2520Nichols%2520and%2520Yinfei%2520Yang%2520and%2520Zhe%2520Gan%26entry.1292438233%3D%2520%2520Building%2520a%2520generalist%2520model%2520for%2520user%2520interface%2520%2528UI%2529%2520understanding%2520is%250Achallenging%2520due%2520to%2520various%2520foundational%2520issues%252C%2520such%2520as%2520platform%2520diversity%252C%250Aresolution%2520variation%252C%2520and%2520data%2520limitation.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AFerret-UI%25202%252C%2520a%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520designed%2520for%2520universal%2520UI%250Aunderstanding%2520across%2520a%2520wide%2520range%2520of%2520platforms%252C%2520including%2520iPhone%252C%2520Android%252C%250AiPad%252C%2520Webpage%252C%2520and%2520AppleTV.%2520Building%2520on%2520the%2520foundation%2520of%2520Ferret-UI%252C%2520Ferret-UI%250A2%2520introduces%2520three%2520key%2520innovations%253A%2520support%2520for%2520multiple%2520platform%2520types%252C%250Ahigh-resolution%2520perception%2520through%2520adaptive%2520scaling%252C%2520and%2520advanced%2520task%2520training%250Adata%2520generation%2520powered%2520by%2520GPT-4o%2520with%2520set-of-mark%2520visual%2520prompting.%2520These%250Aadvancements%2520enable%2520Ferret-UI%25202%2520to%2520perform%2520complex%252C%2520user-centered%2520interactions%252C%250Amaking%2520it%2520highly%2520versatile%2520and%2520adaptable%2520for%2520the%2520expanding%2520diversity%2520of%250Aplatform%2520ecosystems.%2520Extensive%2520empirical%2520experiments%2520on%2520referring%252C%2520grounding%252C%250Auser-centric%2520advanced%2520tasks%2520%2528comprising%25209%2520subtasks%2520%2524%255Ctimes%2524%25205%2520platforms%2529%252C%2520GUIDE%250Anext-action%2520prediction%2520dataset%252C%2520and%2520GUI-World%2520multi-platform%2520benchmark%250Ademonstrate%2520that%2520Ferret-UI%25202%2520significantly%2520outperforms%2520Ferret-UI%252C%2520and%2520also%250Ashows%2520strong%2520cross-platform%2520transfer%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ferret-UI%202%3A%20Mastering%20Universal%20User%20Interface%20Understanding%20Across%0A%20%20Platforms&entry.906535625=Zhangheng%20Li%20and%20Keen%20You%20and%20Haotian%20Zhang%20and%20Di%20Feng%20and%20Harsh%20Agrawal%20and%20Xiujun%20Li%20and%20Mohana%20Prasad%20Sathya%20Moorthy%20and%20Jeff%20Nichols%20and%20Yinfei%20Yang%20and%20Zhe%20Gan&entry.1292438233=%20%20Building%20a%20generalist%20model%20for%20user%20interface%20%28UI%29%20understanding%20is%0Achallenging%20due%20to%20various%20foundational%20issues%2C%20such%20as%20platform%20diversity%2C%0Aresolution%20variation%2C%20and%20data%20limitation.%20In%20this%20paper%2C%20we%20introduce%0AFerret-UI%202%2C%20a%20multimodal%20large%20language%20model%20%28MLLM%29%20designed%20for%20universal%20UI%0Aunderstanding%20across%20a%20wide%20range%20of%20platforms%2C%20including%20iPhone%2C%20Android%2C%0AiPad%2C%20Webpage%2C%20and%20AppleTV.%20Building%20on%20the%20foundation%20of%20Ferret-UI%2C%20Ferret-UI%0A2%20introduces%20three%20key%20innovations%3A%20support%20for%20multiple%20platform%20types%2C%0Ahigh-resolution%20perception%20through%20adaptive%20scaling%2C%20and%20advanced%20task%20training%0Adata%20generation%20powered%20by%20GPT-4o%20with%20set-of-mark%20visual%20prompting.%20These%0Aadvancements%20enable%20Ferret-UI%202%20to%20perform%20complex%2C%20user-centered%20interactions%2C%0Amaking%20it%20highly%20versatile%20and%20adaptable%20for%20the%20expanding%20diversity%20of%0Aplatform%20ecosystems.%20Extensive%20empirical%20experiments%20on%20referring%2C%20grounding%2C%0Auser-centric%20advanced%20tasks%20%28comprising%209%20subtasks%20%24%5Ctimes%24%205%20platforms%29%2C%20GUIDE%0Anext-action%20prediction%20dataset%2C%20and%20GUI-World%20multi-platform%20benchmark%0Ademonstrate%20that%20Ferret-UI%202%20significantly%20outperforms%20Ferret-UI%2C%20and%20also%0Ashows%20strong%20cross-platform%20transfer%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18967v1&entry.124074799=Read"},
{"title": "ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation", "author": "L\u00e9o Machado and H\u00e9l\u00e8ne Philippe and \u00c9lodie Ferreres and Julien Khlaut and Julie Dupuis and Korentin Le Floch and Denis Habip Gatenyo and Pascal Roux and Jules Gr\u00e9gory and Maxime Ronot and Corentin Dancette and Daniel Tordjman and Pierre Manceron and Paul H\u00e9rent", "abstract": "  Carcinogenesis is a proteiform phenomenon, with tumors emerging in various\nlocations and displaying complex, diverse shapes. At the crucial intersection\nof research and clinical practice, it demands precise and flexible assessment.\nHowever, current biomarkers, such as RECIST 1.1's long and short axis\nmeasurements, fall short of capturing this complexity, offering an approximate\nestimate of tumor burden and a simplistic representation of a more intricate\nprocess. Additionally, existing supervised AI models face challenges in\naddressing the variability in tumor presentations, limiting their clinical\nutility. These limitations arise from the scarcity of annotations and the\nmodels' focus on narrowly defined tasks.\n  To address these challenges, we developed ONCOPILOT, an interactive\nradiological foundation model trained on approximately 7,500 CT scans covering\nthe whole body, from both normal anatomy and a wide range of oncological cases.\nONCOPILOT performs 3D tumor segmentation using visual prompts like point-click\nand bounding boxes, outperforming state-of-the-art models (e.g., nnUnet) and\nachieving radiologist-level accuracy in RECIST 1.1 measurements. The key\nadvantage of this foundation model is its ability to surpass state-of-the-art\nperformance while keeping the radiologist in the loop, a capability that\nprevious models could not achieve. When radiologists interactively refine the\nsegmentations, accuracy improves further. ONCOPILOT also accelerates\nmeasurement processes and reduces inter-reader variability, facilitating\nvolumetric analysis and unlocking new biomarkers for deeper insights.\n  This AI assistant is expected to enhance the precision of RECIST 1.1\nmeasurements, unlock the potential of volumetric biomarkers, and improve\npatient stratification and clinical care, while seamlessly integrating into the\nradiological workflow.\n", "link": "http://arxiv.org/abs/2410.07908v3", "date": "2024-10-24", "relevancy": 2.4438, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4942}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4942}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ONCOPILOT%3A%20A%20Promptable%20CT%20Foundation%20Model%20For%20Solid%20Tumor%20Evaluation&body=Title%3A%20ONCOPILOT%3A%20A%20Promptable%20CT%20Foundation%20Model%20For%20Solid%20Tumor%20Evaluation%0AAuthor%3A%20L%C3%A9o%20Machado%20and%20H%C3%A9l%C3%A8ne%20Philippe%20and%20%C3%89lodie%20Ferreres%20and%20Julien%20Khlaut%20and%20Julie%20Dupuis%20and%20Korentin%20Le%20Floch%20and%20Denis%20Habip%20Gatenyo%20and%20Pascal%20Roux%20and%20Jules%20Gr%C3%A9gory%20and%20Maxime%20Ronot%20and%20Corentin%20Dancette%20and%20Daniel%20Tordjman%20and%20Pierre%20Manceron%20and%20Paul%20H%C3%A9rent%0AAbstract%3A%20%20%20Carcinogenesis%20is%20a%20proteiform%20phenomenon%2C%20with%20tumors%20emerging%20in%20various%0Alocations%20and%20displaying%20complex%2C%20diverse%20shapes.%20At%20the%20crucial%20intersection%0Aof%20research%20and%20clinical%20practice%2C%20it%20demands%20precise%20and%20flexible%20assessment.%0AHowever%2C%20current%20biomarkers%2C%20such%20as%20RECIST%201.1%27s%20long%20and%20short%20axis%0Ameasurements%2C%20fall%20short%20of%20capturing%20this%20complexity%2C%20offering%20an%20approximate%0Aestimate%20of%20tumor%20burden%20and%20a%20simplistic%20representation%20of%20a%20more%20intricate%0Aprocess.%20Additionally%2C%20existing%20supervised%20AI%20models%20face%20challenges%20in%0Aaddressing%20the%20variability%20in%20tumor%20presentations%2C%20limiting%20their%20clinical%0Autility.%20These%20limitations%20arise%20from%20the%20scarcity%20of%20annotations%20and%20the%0Amodels%27%20focus%20on%20narrowly%20defined%20tasks.%0A%20%20To%20address%20these%20challenges%2C%20we%20developed%20ONCOPILOT%2C%20an%20interactive%0Aradiological%20foundation%20model%20trained%20on%20approximately%207%2C500%20CT%20scans%20covering%0Athe%20whole%20body%2C%20from%20both%20normal%20anatomy%20and%20a%20wide%20range%20of%20oncological%20cases.%0AONCOPILOT%20performs%203D%20tumor%20segmentation%20using%20visual%20prompts%20like%20point-click%0Aand%20bounding%20boxes%2C%20outperforming%20state-of-the-art%20models%20%28e.g.%2C%20nnUnet%29%20and%0Aachieving%20radiologist-level%20accuracy%20in%20RECIST%201.1%20measurements.%20The%20key%0Aadvantage%20of%20this%20foundation%20model%20is%20its%20ability%20to%20surpass%20state-of-the-art%0Aperformance%20while%20keeping%20the%20radiologist%20in%20the%20loop%2C%20a%20capability%20that%0Aprevious%20models%20could%20not%20achieve.%20When%20radiologists%20interactively%20refine%20the%0Asegmentations%2C%20accuracy%20improves%20further.%20ONCOPILOT%20also%20accelerates%0Ameasurement%20processes%20and%20reduces%20inter-reader%20variability%2C%20facilitating%0Avolumetric%20analysis%20and%20unlocking%20new%20biomarkers%20for%20deeper%20insights.%0A%20%20This%20AI%20assistant%20is%20expected%20to%20enhance%20the%20precision%20of%20RECIST%201.1%0Ameasurements%2C%20unlock%20the%20potential%20of%20volumetric%20biomarkers%2C%20and%20improve%0Apatient%20stratification%20and%20clinical%20care%2C%20while%20seamlessly%20integrating%20into%20the%0Aradiological%20workflow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07908v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DONCOPILOT%253A%2520A%2520Promptable%2520CT%2520Foundation%2520Model%2520For%2520Solid%2520Tumor%2520Evaluation%26entry.906535625%3DL%25C3%25A9o%2520Machado%2520and%2520H%25C3%25A9l%25C3%25A8ne%2520Philippe%2520and%2520%25C3%2589lodie%2520Ferreres%2520and%2520Julien%2520Khlaut%2520and%2520Julie%2520Dupuis%2520and%2520Korentin%2520Le%2520Floch%2520and%2520Denis%2520Habip%2520Gatenyo%2520and%2520Pascal%2520Roux%2520and%2520Jules%2520Gr%25C3%25A9gory%2520and%2520Maxime%2520Ronot%2520and%2520Corentin%2520Dancette%2520and%2520Daniel%2520Tordjman%2520and%2520Pierre%2520Manceron%2520and%2520Paul%2520H%25C3%25A9rent%26entry.1292438233%3D%2520%2520Carcinogenesis%2520is%2520a%2520proteiform%2520phenomenon%252C%2520with%2520tumors%2520emerging%2520in%2520various%250Alocations%2520and%2520displaying%2520complex%252C%2520diverse%2520shapes.%2520At%2520the%2520crucial%2520intersection%250Aof%2520research%2520and%2520clinical%2520practice%252C%2520it%2520demands%2520precise%2520and%2520flexible%2520assessment.%250AHowever%252C%2520current%2520biomarkers%252C%2520such%2520as%2520RECIST%25201.1%2527s%2520long%2520and%2520short%2520axis%250Ameasurements%252C%2520fall%2520short%2520of%2520capturing%2520this%2520complexity%252C%2520offering%2520an%2520approximate%250Aestimate%2520of%2520tumor%2520burden%2520and%2520a%2520simplistic%2520representation%2520of%2520a%2520more%2520intricate%250Aprocess.%2520Additionally%252C%2520existing%2520supervised%2520AI%2520models%2520face%2520challenges%2520in%250Aaddressing%2520the%2520variability%2520in%2520tumor%2520presentations%252C%2520limiting%2520their%2520clinical%250Autility.%2520These%2520limitations%2520arise%2520from%2520the%2520scarcity%2520of%2520annotations%2520and%2520the%250Amodels%2527%2520focus%2520on%2520narrowly%2520defined%2520tasks.%250A%2520%2520To%2520address%2520these%2520challenges%252C%2520we%2520developed%2520ONCOPILOT%252C%2520an%2520interactive%250Aradiological%2520foundation%2520model%2520trained%2520on%2520approximately%25207%252C500%2520CT%2520scans%2520covering%250Athe%2520whole%2520body%252C%2520from%2520both%2520normal%2520anatomy%2520and%2520a%2520wide%2520range%2520of%2520oncological%2520cases.%250AONCOPILOT%2520performs%25203D%2520tumor%2520segmentation%2520using%2520visual%2520prompts%2520like%2520point-click%250Aand%2520bounding%2520boxes%252C%2520outperforming%2520state-of-the-art%2520models%2520%2528e.g.%252C%2520nnUnet%2529%2520and%250Aachieving%2520radiologist-level%2520accuracy%2520in%2520RECIST%25201.1%2520measurements.%2520The%2520key%250Aadvantage%2520of%2520this%2520foundation%2520model%2520is%2520its%2520ability%2520to%2520surpass%2520state-of-the-art%250Aperformance%2520while%2520keeping%2520the%2520radiologist%2520in%2520the%2520loop%252C%2520a%2520capability%2520that%250Aprevious%2520models%2520could%2520not%2520achieve.%2520When%2520radiologists%2520interactively%2520refine%2520the%250Asegmentations%252C%2520accuracy%2520improves%2520further.%2520ONCOPILOT%2520also%2520accelerates%250Ameasurement%2520processes%2520and%2520reduces%2520inter-reader%2520variability%252C%2520facilitating%250Avolumetric%2520analysis%2520and%2520unlocking%2520new%2520biomarkers%2520for%2520deeper%2520insights.%250A%2520%2520This%2520AI%2520assistant%2520is%2520expected%2520to%2520enhance%2520the%2520precision%2520of%2520RECIST%25201.1%250Ameasurements%252C%2520unlock%2520the%2520potential%2520of%2520volumetric%2520biomarkers%252C%2520and%2520improve%250Apatient%2520stratification%2520and%2520clinical%2520care%252C%2520while%2520seamlessly%2520integrating%2520into%2520the%250Aradiological%2520workflow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07908v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ONCOPILOT%3A%20A%20Promptable%20CT%20Foundation%20Model%20For%20Solid%20Tumor%20Evaluation&entry.906535625=L%C3%A9o%20Machado%20and%20H%C3%A9l%C3%A8ne%20Philippe%20and%20%C3%89lodie%20Ferreres%20and%20Julien%20Khlaut%20and%20Julie%20Dupuis%20and%20Korentin%20Le%20Floch%20and%20Denis%20Habip%20Gatenyo%20and%20Pascal%20Roux%20and%20Jules%20Gr%C3%A9gory%20and%20Maxime%20Ronot%20and%20Corentin%20Dancette%20and%20Daniel%20Tordjman%20and%20Pierre%20Manceron%20and%20Paul%20H%C3%A9rent&entry.1292438233=%20%20Carcinogenesis%20is%20a%20proteiform%20phenomenon%2C%20with%20tumors%20emerging%20in%20various%0Alocations%20and%20displaying%20complex%2C%20diverse%20shapes.%20At%20the%20crucial%20intersection%0Aof%20research%20and%20clinical%20practice%2C%20it%20demands%20precise%20and%20flexible%20assessment.%0AHowever%2C%20current%20biomarkers%2C%20such%20as%20RECIST%201.1%27s%20long%20and%20short%20axis%0Ameasurements%2C%20fall%20short%20of%20capturing%20this%20complexity%2C%20offering%20an%20approximate%0Aestimate%20of%20tumor%20burden%20and%20a%20simplistic%20representation%20of%20a%20more%20intricate%0Aprocess.%20Additionally%2C%20existing%20supervised%20AI%20models%20face%20challenges%20in%0Aaddressing%20the%20variability%20in%20tumor%20presentations%2C%20limiting%20their%20clinical%0Autility.%20These%20limitations%20arise%20from%20the%20scarcity%20of%20annotations%20and%20the%0Amodels%27%20focus%20on%20narrowly%20defined%20tasks.%0A%20%20To%20address%20these%20challenges%2C%20we%20developed%20ONCOPILOT%2C%20an%20interactive%0Aradiological%20foundation%20model%20trained%20on%20approximately%207%2C500%20CT%20scans%20covering%0Athe%20whole%20body%2C%20from%20both%20normal%20anatomy%20and%20a%20wide%20range%20of%20oncological%20cases.%0AONCOPILOT%20performs%203D%20tumor%20segmentation%20using%20visual%20prompts%20like%20point-click%0Aand%20bounding%20boxes%2C%20outperforming%20state-of-the-art%20models%20%28e.g.%2C%20nnUnet%29%20and%0Aachieving%20radiologist-level%20accuracy%20in%20RECIST%201.1%20measurements.%20The%20key%0Aadvantage%20of%20this%20foundation%20model%20is%20its%20ability%20to%20surpass%20state-of-the-art%0Aperformance%20while%20keeping%20the%20radiologist%20in%20the%20loop%2C%20a%20capability%20that%0Aprevious%20models%20could%20not%20achieve.%20When%20radiologists%20interactively%20refine%20the%0Asegmentations%2C%20accuracy%20improves%20further.%20ONCOPILOT%20also%20accelerates%0Ameasurement%20processes%20and%20reduces%20inter-reader%20variability%2C%20facilitating%0Avolumetric%20analysis%20and%20unlocking%20new%20biomarkers%20for%20deeper%20insights.%0A%20%20This%20AI%20assistant%20is%20expected%20to%20enhance%20the%20precision%20of%20RECIST%201.1%0Ameasurements%2C%20unlock%20the%20potential%20of%20volumetric%20biomarkers%2C%20and%20improve%0Apatient%20stratification%20and%20clinical%20care%2C%20while%20seamlessly%20integrating%20into%20the%0Aradiological%20workflow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07908v3&entry.124074799=Read"},
{"title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging\n  Small LMs", "author": "Ankit Singh Rawat and Veeranjaneyulu Sadhanala and Afshin Rostamizadeh and Ayan Chakrabarti and Wittawat Jitkrittum and Vladimir Feinberg and Seungyeon Kim and Hrayr Harutyunyan and Nikunj Saunshi and Zachary Nado and Rakesh Shivanna and Sashank J. Reddi and Aditya Krishna Menon and Rohan Anil and Sanjiv Kumar", "abstract": "  A primary challenge in large language model (LLM) development is their\nonerous pre-training cost. Typically, such pre-training involves optimizing a\nself-supervised objective (such as next-token prediction) over a large corpus.\nThis paper explores a promising paradigm to improve LLM pre-training efficiency\nand quality by suitably leveraging a small language model (SLM). In particular,\nthis paradigm relies on an SLM to both (1) provide soft labels as additional\ntraining supervision, and (2) select a small subset of valuable (\"informative\"\nand \"hard\") training examples. Put together, this enables an effective transfer\nof the SLM's predictive distribution to the LLM, while prioritizing specific\nregions of the training data distribution. Empirically, this leads to reduced\nLLM training time compared to standard training, while improving the overall\nquality. Theoretically, we develop a statistical framework to systematically\nstudy the utility of SLMs in enabling efficient training of high-quality LLMs.\nIn particular, our framework characterizes how the SLM's seemingly low-quality\nsupervision can enhance the training of a much more capable LLM. Furthermore,\nit also highlights the need for an adaptive utilization of such supervision, by\nstriking a balance between the bias and variance introduced by the SLM-provided\nsoft labels. We corroborate our theoretical framework by improving the\npre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B\nparameters on the Pile dataset.\n", "link": "http://arxiv.org/abs/2410.18779v1", "date": "2024-10-24", "relevancy": 2.422, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4962}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4808}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Little%20Help%20Goes%20a%20Long%20Way%3A%20Efficient%20LLM%20Training%20by%20Leveraging%0A%20%20Small%20LMs&body=Title%3A%20A%20Little%20Help%20Goes%20a%20Long%20Way%3A%20Efficient%20LLM%20Training%20by%20Leveraging%0A%20%20Small%20LMs%0AAuthor%3A%20Ankit%20Singh%20Rawat%20and%20Veeranjaneyulu%20Sadhanala%20and%20Afshin%20Rostamizadeh%20and%20Ayan%20Chakrabarti%20and%20Wittawat%20Jitkrittum%20and%20Vladimir%20Feinberg%20and%20Seungyeon%20Kim%20and%20Hrayr%20Harutyunyan%20and%20Nikunj%20Saunshi%20and%20Zachary%20Nado%20and%20Rakesh%20Shivanna%20and%20Sashank%20J.%20Reddi%20and%20Aditya%20Krishna%20Menon%20and%20Rohan%20Anil%20and%20Sanjiv%20Kumar%0AAbstract%3A%20%20%20A%20primary%20challenge%20in%20large%20language%20model%20%28LLM%29%20development%20is%20their%0Aonerous%20pre-training%20cost.%20Typically%2C%20such%20pre-training%20involves%20optimizing%20a%0Aself-supervised%20objective%20%28such%20as%20next-token%20prediction%29%20over%20a%20large%20corpus.%0AThis%20paper%20explores%20a%20promising%20paradigm%20to%20improve%20LLM%20pre-training%20efficiency%0Aand%20quality%20by%20suitably%20leveraging%20a%20small%20language%20model%20%28SLM%29.%20In%20particular%2C%0Athis%20paradigm%20relies%20on%20an%20SLM%20to%20both%20%281%29%20provide%20soft%20labels%20as%20additional%0Atraining%20supervision%2C%20and%20%282%29%20select%20a%20small%20subset%20of%20valuable%20%28%22informative%22%0Aand%20%22hard%22%29%20training%20examples.%20Put%20together%2C%20this%20enables%20an%20effective%20transfer%0Aof%20the%20SLM%27s%20predictive%20distribution%20to%20the%20LLM%2C%20while%20prioritizing%20specific%0Aregions%20of%20the%20training%20data%20distribution.%20Empirically%2C%20this%20leads%20to%20reduced%0ALLM%20training%20time%20compared%20to%20standard%20training%2C%20while%20improving%20the%20overall%0Aquality.%20Theoretically%2C%20we%20develop%20a%20statistical%20framework%20to%20systematically%0Astudy%20the%20utility%20of%20SLMs%20in%20enabling%20efficient%20training%20of%20high-quality%20LLMs.%0AIn%20particular%2C%20our%20framework%20characterizes%20how%20the%20SLM%27s%20seemingly%20low-quality%0Asupervision%20can%20enhance%20the%20training%20of%20a%20much%20more%20capable%20LLM.%20Furthermore%2C%0Ait%20also%20highlights%20the%20need%20for%20an%20adaptive%20utilization%20of%20such%20supervision%2C%20by%0Astriking%20a%20balance%20between%20the%20bias%20and%20variance%20introduced%20by%20the%20SLM-provided%0Asoft%20labels.%20We%20corroborate%20our%20theoretical%20framework%20by%20improving%20the%0Apre-training%20of%20an%20LLM%20with%202.8B%20parameters%20by%20utilizing%20a%20smaller%20LM%20with%201.5B%0Aparameters%20on%20the%20Pile%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Little%2520Help%2520Goes%2520a%2520Long%2520Way%253A%2520Efficient%2520LLM%2520Training%2520by%2520Leveraging%250A%2520%2520Small%2520LMs%26entry.906535625%3DAnkit%2520Singh%2520Rawat%2520and%2520Veeranjaneyulu%2520Sadhanala%2520and%2520Afshin%2520Rostamizadeh%2520and%2520Ayan%2520Chakrabarti%2520and%2520Wittawat%2520Jitkrittum%2520and%2520Vladimir%2520Feinberg%2520and%2520Seungyeon%2520Kim%2520and%2520Hrayr%2520Harutyunyan%2520and%2520Nikunj%2520Saunshi%2520and%2520Zachary%2520Nado%2520and%2520Rakesh%2520Shivanna%2520and%2520Sashank%2520J.%2520Reddi%2520and%2520Aditya%2520Krishna%2520Menon%2520and%2520Rohan%2520Anil%2520and%2520Sanjiv%2520Kumar%26entry.1292438233%3D%2520%2520A%2520primary%2520challenge%2520in%2520large%2520language%2520model%2520%2528LLM%2529%2520development%2520is%2520their%250Aonerous%2520pre-training%2520cost.%2520Typically%252C%2520such%2520pre-training%2520involves%2520optimizing%2520a%250Aself-supervised%2520objective%2520%2528such%2520as%2520next-token%2520prediction%2529%2520over%2520a%2520large%2520corpus.%250AThis%2520paper%2520explores%2520a%2520promising%2520paradigm%2520to%2520improve%2520LLM%2520pre-training%2520efficiency%250Aand%2520quality%2520by%2520suitably%2520leveraging%2520a%2520small%2520language%2520model%2520%2528SLM%2529.%2520In%2520particular%252C%250Athis%2520paradigm%2520relies%2520on%2520an%2520SLM%2520to%2520both%2520%25281%2529%2520provide%2520soft%2520labels%2520as%2520additional%250Atraining%2520supervision%252C%2520and%2520%25282%2529%2520select%2520a%2520small%2520subset%2520of%2520valuable%2520%2528%2522informative%2522%250Aand%2520%2522hard%2522%2529%2520training%2520examples.%2520Put%2520together%252C%2520this%2520enables%2520an%2520effective%2520transfer%250Aof%2520the%2520SLM%2527s%2520predictive%2520distribution%2520to%2520the%2520LLM%252C%2520while%2520prioritizing%2520specific%250Aregions%2520of%2520the%2520training%2520data%2520distribution.%2520Empirically%252C%2520this%2520leads%2520to%2520reduced%250ALLM%2520training%2520time%2520compared%2520to%2520standard%2520training%252C%2520while%2520improving%2520the%2520overall%250Aquality.%2520Theoretically%252C%2520we%2520develop%2520a%2520statistical%2520framework%2520to%2520systematically%250Astudy%2520the%2520utility%2520of%2520SLMs%2520in%2520enabling%2520efficient%2520training%2520of%2520high-quality%2520LLMs.%250AIn%2520particular%252C%2520our%2520framework%2520characterizes%2520how%2520the%2520SLM%2527s%2520seemingly%2520low-quality%250Asupervision%2520can%2520enhance%2520the%2520training%2520of%2520a%2520much%2520more%2520capable%2520LLM.%2520Furthermore%252C%250Ait%2520also%2520highlights%2520the%2520need%2520for%2520an%2520adaptive%2520utilization%2520of%2520such%2520supervision%252C%2520by%250Astriking%2520a%2520balance%2520between%2520the%2520bias%2520and%2520variance%2520introduced%2520by%2520the%2520SLM-provided%250Asoft%2520labels.%2520We%2520corroborate%2520our%2520theoretical%2520framework%2520by%2520improving%2520the%250Apre-training%2520of%2520an%2520LLM%2520with%25202.8B%2520parameters%2520by%2520utilizing%2520a%2520smaller%2520LM%2520with%25201.5B%250Aparameters%2520on%2520the%2520Pile%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Little%20Help%20Goes%20a%20Long%20Way%3A%20Efficient%20LLM%20Training%20by%20Leveraging%0A%20%20Small%20LMs&entry.906535625=Ankit%20Singh%20Rawat%20and%20Veeranjaneyulu%20Sadhanala%20and%20Afshin%20Rostamizadeh%20and%20Ayan%20Chakrabarti%20and%20Wittawat%20Jitkrittum%20and%20Vladimir%20Feinberg%20and%20Seungyeon%20Kim%20and%20Hrayr%20Harutyunyan%20and%20Nikunj%20Saunshi%20and%20Zachary%20Nado%20and%20Rakesh%20Shivanna%20and%20Sashank%20J.%20Reddi%20and%20Aditya%20Krishna%20Menon%20and%20Rohan%20Anil%20and%20Sanjiv%20Kumar&entry.1292438233=%20%20A%20primary%20challenge%20in%20large%20language%20model%20%28LLM%29%20development%20is%20their%0Aonerous%20pre-training%20cost.%20Typically%2C%20such%20pre-training%20involves%20optimizing%20a%0Aself-supervised%20objective%20%28such%20as%20next-token%20prediction%29%20over%20a%20large%20corpus.%0AThis%20paper%20explores%20a%20promising%20paradigm%20to%20improve%20LLM%20pre-training%20efficiency%0Aand%20quality%20by%20suitably%20leveraging%20a%20small%20language%20model%20%28SLM%29.%20In%20particular%2C%0Athis%20paradigm%20relies%20on%20an%20SLM%20to%20both%20%281%29%20provide%20soft%20labels%20as%20additional%0Atraining%20supervision%2C%20and%20%282%29%20select%20a%20small%20subset%20of%20valuable%20%28%22informative%22%0Aand%20%22hard%22%29%20training%20examples.%20Put%20together%2C%20this%20enables%20an%20effective%20transfer%0Aof%20the%20SLM%27s%20predictive%20distribution%20to%20the%20LLM%2C%20while%20prioritizing%20specific%0Aregions%20of%20the%20training%20data%20distribution.%20Empirically%2C%20this%20leads%20to%20reduced%0ALLM%20training%20time%20compared%20to%20standard%20training%2C%20while%20improving%20the%20overall%0Aquality.%20Theoretically%2C%20we%20develop%20a%20statistical%20framework%20to%20systematically%0Astudy%20the%20utility%20of%20SLMs%20in%20enabling%20efficient%20training%20of%20high-quality%20LLMs.%0AIn%20particular%2C%20our%20framework%20characterizes%20how%20the%20SLM%27s%20seemingly%20low-quality%0Asupervision%20can%20enhance%20the%20training%20of%20a%20much%20more%20capable%20LLM.%20Furthermore%2C%0Ait%20also%20highlights%20the%20need%20for%20an%20adaptive%20utilization%20of%20such%20supervision%2C%20by%0Astriking%20a%20balance%20between%20the%20bias%20and%20variance%20introduced%20by%20the%20SLM-provided%0Asoft%20labels.%20We%20corroborate%20our%20theoretical%20framework%20by%20improving%20the%0Apre-training%20of%20an%20LLM%20with%202.8B%20parameters%20by%20utilizing%20a%20smaller%20LM%20with%201.5B%0Aparameters%20on%20the%20Pile%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18779v1&entry.124074799=Read"},
{"title": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs", "author": "Xin Ma and Yang Liu and Jingjing Liu and Xiaoxu Ma", "abstract": "  Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach. Our code is available at\n\\url{https://github.com/soacker/Mesa-Extrapolation}.\n", "link": "http://arxiv.org/abs/2410.15859v3", "date": "2024-10-24", "relevancy": 2.3977, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesa-Extrapolation%3A%20A%20Weave%20Position%20Encoding%20Method%20for%20Enhanced%0A%20%20Extrapolation%20in%20LLMs&body=Title%3A%20Mesa-Extrapolation%3A%20A%20Weave%20Position%20Encoding%20Method%20for%20Enhanced%0A%20%20Extrapolation%20in%20LLMs%0AAuthor%3A%20Xin%20Ma%20and%20Yang%20Liu%20and%20Jingjing%20Liu%20and%20Xiaoxu%20Ma%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20although%20having%20revolutionized%20many%20fields%2C%0Astill%20suffer%20from%20the%20challenging%20extrapolation%20problem%2C%20where%20the%20inference%0Aability%20of%20LLMs%20sharply%20declines%20beyond%20their%20max%20training%20lengths.%20In%20this%0Awork%2C%20we%20conduct%20a%20theoretical%20analysis%20to%20better%20understand%20why%20No%20Position%0AEncoding%20%28NoPE%29%20fails%20outside%20its%20effective%20range%2C%20as%20well%20as%20examining%20the%0Apower%20of%20Position%20Encoding%20%28PE%29%20in%20this%20context.%20Our%20findings%20reveal%20that%20with%0Ameticulous%20weave%20position%2C%20PE%20can%20indeed%20be%20extended%20beyond%20effective%20range.%0AOur%20theorems%20establish%20that%20LLMs%20equipped%20with%20weave%20PE%20can%20achieve%20improved%0Aextrapolation%20performance%20without%20additional%20cost.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20weave%20PE%20method%2C%20Mesa-Extrapolation%2C%20which%20utilizes%20a%20chunk-based%0Atriangular%20attention%20matrix%20and%20applies%20Stair%20PE%20to%20manage%20the%20final%20chunk.%0AThis%20method%20not%20only%20retains%20competitive%20performance%20but%20also%20offers%0Asubstantial%20benefits%20such%20as%20significantly%20reduced%20memory%20demand%20and%20faster%0Ainference%20speed.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%0AMesa-Extrapolation%2C%20demonstrating%20its%20potential%20as%20a%20scalable%20solution%20to%0Aenhancing%20LLMs%20applicative%20reach.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/soacker/Mesa-Extrapolation%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15859v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesa-Extrapolation%253A%2520A%2520Weave%2520Position%2520Encoding%2520Method%2520for%2520Enhanced%250A%2520%2520Extrapolation%2520in%2520LLMs%26entry.906535625%3DXin%2520Ma%2520and%2520Yang%2520Liu%2520and%2520Jingjing%2520Liu%2520and%2520Xiaoxu%2520Ma%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520although%2520having%2520revolutionized%2520many%2520fields%252C%250Astill%2520suffer%2520from%2520the%2520challenging%2520extrapolation%2520problem%252C%2520where%2520the%2520inference%250Aability%2520of%2520LLMs%2520sharply%2520declines%2520beyond%2520their%2520max%2520training%2520lengths.%2520In%2520this%250Awork%252C%2520we%2520conduct%2520a%2520theoretical%2520analysis%2520to%2520better%2520understand%2520why%2520No%2520Position%250AEncoding%2520%2528NoPE%2529%2520fails%2520outside%2520its%2520effective%2520range%252C%2520as%2520well%2520as%2520examining%2520the%250Apower%2520of%2520Position%2520Encoding%2520%2528PE%2529%2520in%2520this%2520context.%2520Our%2520findings%2520reveal%2520that%2520with%250Ameticulous%2520weave%2520position%252C%2520PE%2520can%2520indeed%2520be%2520extended%2520beyond%2520effective%2520range.%250AOur%2520theorems%2520establish%2520that%2520LLMs%2520equipped%2520with%2520weave%2520PE%2520can%2520achieve%2520improved%250Aextrapolation%2520performance%2520without%2520additional%2520cost.%2520Furthermore%252C%2520we%2520introduce%2520a%250Anovel%2520weave%2520PE%2520method%252C%2520Mesa-Extrapolation%252C%2520which%2520utilizes%2520a%2520chunk-based%250Atriangular%2520attention%2520matrix%2520and%2520applies%2520Stair%2520PE%2520to%2520manage%2520the%2520final%2520chunk.%250AThis%2520method%2520not%2520only%2520retains%2520competitive%2520performance%2520but%2520also%2520offers%250Asubstantial%2520benefits%2520such%2520as%2520significantly%2520reduced%2520memory%2520demand%2520and%2520faster%250Ainference%2520speed.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%2520of%250AMesa-Extrapolation%252C%2520demonstrating%2520its%2520potential%2520as%2520a%2520scalable%2520solution%2520to%250Aenhancing%2520LLMs%2520applicative%2520reach.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/soacker/Mesa-Extrapolation%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15859v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesa-Extrapolation%3A%20A%20Weave%20Position%20Encoding%20Method%20for%20Enhanced%0A%20%20Extrapolation%20in%20LLMs&entry.906535625=Xin%20Ma%20and%20Yang%20Liu%20and%20Jingjing%20Liu%20and%20Xiaoxu%20Ma&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20although%20having%20revolutionized%20many%20fields%2C%0Astill%20suffer%20from%20the%20challenging%20extrapolation%20problem%2C%20where%20the%20inference%0Aability%20of%20LLMs%20sharply%20declines%20beyond%20their%20max%20training%20lengths.%20In%20this%0Awork%2C%20we%20conduct%20a%20theoretical%20analysis%20to%20better%20understand%20why%20No%20Position%0AEncoding%20%28NoPE%29%20fails%20outside%20its%20effective%20range%2C%20as%20well%20as%20examining%20the%0Apower%20of%20Position%20Encoding%20%28PE%29%20in%20this%20context.%20Our%20findings%20reveal%20that%20with%0Ameticulous%20weave%20position%2C%20PE%20can%20indeed%20be%20extended%20beyond%20effective%20range.%0AOur%20theorems%20establish%20that%20LLMs%20equipped%20with%20weave%20PE%20can%20achieve%20improved%0Aextrapolation%20performance%20without%20additional%20cost.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20weave%20PE%20method%2C%20Mesa-Extrapolation%2C%20which%20utilizes%20a%20chunk-based%0Atriangular%20attention%20matrix%20and%20applies%20Stair%20PE%20to%20manage%20the%20final%20chunk.%0AThis%20method%20not%20only%20retains%20competitive%20performance%20but%20also%20offers%0Asubstantial%20benefits%20such%20as%20significantly%20reduced%20memory%20demand%20and%20faster%0Ainference%20speed.%20Extensive%20experiments%20validate%20the%20effectiveness%20of%0AMesa-Extrapolation%2C%20demonstrating%20its%20potential%20as%20a%20scalable%20solution%20to%0Aenhancing%20LLMs%20applicative%20reach.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/soacker/Mesa-Extrapolation%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15859v3&entry.124074799=Read"},
{"title": "SkillMimicGen: Automated Demonstration Generation for Efficient Skill\n  Learning and Deployment", "author": "Caelan Garrett and Ajay Mandlekar and Bowen Wen and Dieter Fox", "abstract": "  Imitation learning from human demonstrations is an effective paradigm for\nrobot manipulation, but acquiring large datasets is costly and\nresource-intensive, especially for long-horizon tasks. To address this issue,\nwe propose SkillMimicGen (SkillGen), an automated system for generating\ndemonstration datasets from a few human demos. SkillGen segments human demos\ninto manipulation skills, adapts these skills to new contexts, and stitches\nthem together through free-space transit and transfer motion. We also propose a\nHybrid Skill Policy (HSP) framework for learning skill initiation, control, and\ntermination components from SkillGen datasets, enabling skills to be sequenced\nusing motion planning at test-time. We demonstrate that SkillGen greatly\nimproves data generation and policy learning performance over a\nstate-of-the-art data generation framework, resulting in the capability to\nproduce data for large scene variations, including clutter, and agents that are\non average 24% more successful. We demonstrate the efficacy of SkillGen by\ngenerating over 24K demonstrations across 18 task variants in simulation from\njust 60 human demonstrations, and training proficient, often near-perfect, HSP\nagents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also\ndemonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.\nVideos, and more at https://skillgen.github.io.\n", "link": "http://arxiv.org/abs/2410.18907v1", "date": "2024-10-24", "relevancy": 2.3881, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6185}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6037}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkillMimicGen%3A%20Automated%20Demonstration%20Generation%20for%20Efficient%20Skill%0A%20%20Learning%20and%20Deployment&body=Title%3A%20SkillMimicGen%3A%20Automated%20Demonstration%20Generation%20for%20Efficient%20Skill%0A%20%20Learning%20and%20Deployment%0AAuthor%3A%20Caelan%20Garrett%20and%20Ajay%20Mandlekar%20and%20Bowen%20Wen%20and%20Dieter%20Fox%0AAbstract%3A%20%20%20Imitation%20learning%20from%20human%20demonstrations%20is%20an%20effective%20paradigm%20for%0Arobot%20manipulation%2C%20but%20acquiring%20large%20datasets%20is%20costly%20and%0Aresource-intensive%2C%20especially%20for%20long-horizon%20tasks.%20To%20address%20this%20issue%2C%0Awe%20propose%20SkillMimicGen%20%28SkillGen%29%2C%20an%20automated%20system%20for%20generating%0Ademonstration%20datasets%20from%20a%20few%20human%20demos.%20SkillGen%20segments%20human%20demos%0Ainto%20manipulation%20skills%2C%20adapts%20these%20skills%20to%20new%20contexts%2C%20and%20stitches%0Athem%20together%20through%20free-space%20transit%20and%20transfer%20motion.%20We%20also%20propose%20a%0AHybrid%20Skill%20Policy%20%28HSP%29%20framework%20for%20learning%20skill%20initiation%2C%20control%2C%20and%0Atermination%20components%20from%20SkillGen%20datasets%2C%20enabling%20skills%20to%20be%20sequenced%0Ausing%20motion%20planning%20at%20test-time.%20We%20demonstrate%20that%20SkillGen%20greatly%0Aimproves%20data%20generation%20and%20policy%20learning%20performance%20over%20a%0Astate-of-the-art%20data%20generation%20framework%2C%20resulting%20in%20the%20capability%20to%0Aproduce%20data%20for%20large%20scene%20variations%2C%20including%20clutter%2C%20and%20agents%20that%20are%0Aon%20average%2024%25%20more%20successful.%20We%20demonstrate%20the%20efficacy%20of%20SkillGen%20by%0Agenerating%20over%2024K%20demonstrations%20across%2018%20task%20variants%20in%20simulation%20from%0Ajust%2060%20human%20demonstrations%2C%20and%20training%20proficient%2C%20often%20near-perfect%2C%20HSP%0Aagents.%20Finally%2C%20we%20apply%20SkillGen%20to%203%20real-world%20manipulation%20tasks%20and%20also%0Ademonstrate%20zero-shot%20sim-to-real%20transfer%20on%20a%20long-horizon%20assembly%20task.%0AVideos%2C%20and%20more%20at%20https%3A//skillgen.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkillMimicGen%253A%2520Automated%2520Demonstration%2520Generation%2520for%2520Efficient%2520Skill%250A%2520%2520Learning%2520and%2520Deployment%26entry.906535625%3DCaelan%2520Garrett%2520and%2520Ajay%2520Mandlekar%2520and%2520Bowen%2520Wen%2520and%2520Dieter%2520Fox%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520from%2520human%2520demonstrations%2520is%2520an%2520effective%2520paradigm%2520for%250Arobot%2520manipulation%252C%2520but%2520acquiring%2520large%2520datasets%2520is%2520costly%2520and%250Aresource-intensive%252C%2520especially%2520for%2520long-horizon%2520tasks.%2520To%2520address%2520this%2520issue%252C%250Awe%2520propose%2520SkillMimicGen%2520%2528SkillGen%2529%252C%2520an%2520automated%2520system%2520for%2520generating%250Ademonstration%2520datasets%2520from%2520a%2520few%2520human%2520demos.%2520SkillGen%2520segments%2520human%2520demos%250Ainto%2520manipulation%2520skills%252C%2520adapts%2520these%2520skills%2520to%2520new%2520contexts%252C%2520and%2520stitches%250Athem%2520together%2520through%2520free-space%2520transit%2520and%2520transfer%2520motion.%2520We%2520also%2520propose%2520a%250AHybrid%2520Skill%2520Policy%2520%2528HSP%2529%2520framework%2520for%2520learning%2520skill%2520initiation%252C%2520control%252C%2520and%250Atermination%2520components%2520from%2520SkillGen%2520datasets%252C%2520enabling%2520skills%2520to%2520be%2520sequenced%250Ausing%2520motion%2520planning%2520at%2520test-time.%2520We%2520demonstrate%2520that%2520SkillGen%2520greatly%250Aimproves%2520data%2520generation%2520and%2520policy%2520learning%2520performance%2520over%2520a%250Astate-of-the-art%2520data%2520generation%2520framework%252C%2520resulting%2520in%2520the%2520capability%2520to%250Aproduce%2520data%2520for%2520large%2520scene%2520variations%252C%2520including%2520clutter%252C%2520and%2520agents%2520that%2520are%250Aon%2520average%252024%2525%2520more%2520successful.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520SkillGen%2520by%250Agenerating%2520over%252024K%2520demonstrations%2520across%252018%2520task%2520variants%2520in%2520simulation%2520from%250Ajust%252060%2520human%2520demonstrations%252C%2520and%2520training%2520proficient%252C%2520often%2520near-perfect%252C%2520HSP%250Aagents.%2520Finally%252C%2520we%2520apply%2520SkillGen%2520to%25203%2520real-world%2520manipulation%2520tasks%2520and%2520also%250Ademonstrate%2520zero-shot%2520sim-to-real%2520transfer%2520on%2520a%2520long-horizon%2520assembly%2520task.%250AVideos%252C%2520and%2520more%2520at%2520https%253A//skillgen.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkillMimicGen%3A%20Automated%20Demonstration%20Generation%20for%20Efficient%20Skill%0A%20%20Learning%20and%20Deployment&entry.906535625=Caelan%20Garrett%20and%20Ajay%20Mandlekar%20and%20Bowen%20Wen%20and%20Dieter%20Fox&entry.1292438233=%20%20Imitation%20learning%20from%20human%20demonstrations%20is%20an%20effective%20paradigm%20for%0Arobot%20manipulation%2C%20but%20acquiring%20large%20datasets%20is%20costly%20and%0Aresource-intensive%2C%20especially%20for%20long-horizon%20tasks.%20To%20address%20this%20issue%2C%0Awe%20propose%20SkillMimicGen%20%28SkillGen%29%2C%20an%20automated%20system%20for%20generating%0Ademonstration%20datasets%20from%20a%20few%20human%20demos.%20SkillGen%20segments%20human%20demos%0Ainto%20manipulation%20skills%2C%20adapts%20these%20skills%20to%20new%20contexts%2C%20and%20stitches%0Athem%20together%20through%20free-space%20transit%20and%20transfer%20motion.%20We%20also%20propose%20a%0AHybrid%20Skill%20Policy%20%28HSP%29%20framework%20for%20learning%20skill%20initiation%2C%20control%2C%20and%0Atermination%20components%20from%20SkillGen%20datasets%2C%20enabling%20skills%20to%20be%20sequenced%0Ausing%20motion%20planning%20at%20test-time.%20We%20demonstrate%20that%20SkillGen%20greatly%0Aimproves%20data%20generation%20and%20policy%20learning%20performance%20over%20a%0Astate-of-the-art%20data%20generation%20framework%2C%20resulting%20in%20the%20capability%20to%0Aproduce%20data%20for%20large%20scene%20variations%2C%20including%20clutter%2C%20and%20agents%20that%20are%0Aon%20average%2024%25%20more%20successful.%20We%20demonstrate%20the%20efficacy%20of%20SkillGen%20by%0Agenerating%20over%2024K%20demonstrations%20across%2018%20task%20variants%20in%20simulation%20from%0Ajust%2060%20human%20demonstrations%2C%20and%20training%20proficient%2C%20often%20near-perfect%2C%20HSP%0Aagents.%20Finally%2C%20we%20apply%20SkillGen%20to%203%20real-world%20manipulation%20tasks%20and%20also%0Ademonstrate%20zero-shot%20sim-to-real%20transfer%20on%20a%20long-horizon%20assembly%20task.%0AVideos%2C%20and%20more%20at%20https%3A//skillgen.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18907v1&entry.124074799=Read"},
{"title": "Transferring Knowledge from High-Quality to Low-Quality MRI for Adult\n  Glioma Diagnosis", "author": "Yanguang Zhao and Long Bai and Zhaoxi Zhang and Yanan Wu and Mobarakol Islam and Hongliang Ren", "abstract": "  Glioma, a common and deadly brain tumor, requires early diagnosis for\nimproved prognosis. However, low-quality Magnetic Resonance Imaging (MRI)\ntechnology in Sub-Saharan Africa (SSA) hinders accurate diagnosis. This paper\npresents our work in the BraTS Challenge on SSA Adult Glioma. We adopt the\nmodel from the BraTS-GLI 2021 winning solution and utilize it with three\ntraining strategies: (1) initially training on the BraTS-GLI 2021 dataset with\nfine-tuning on the BraTS-Africa dataset, (2) training solely on the\nBraTS-Africa dataset, and (3) training solely on the BraTS-Africa dataset with\n2x super-resolution enhancement. Results show that initial training on the\nBraTS-GLI 2021 dataset followed by fine-tuning on the BraTS-Africa dataset has\nyielded the best results. This suggests the importance of high-quality datasets\nin providing prior knowledge during training. Our top-performing model achieves\nDice scores of 0.882, 0.840, and 0.926, and Hausdorff Distance (95%) scores of\n15.324, 37.518, and 13.971 for enhancing tumor, tumor core, and whole tumor,\nrespectively, in the validation phase. In the final phase of the competition,\nour approach successfully secured second place overall, reflecting the strength\nand effectiveness of our model and training strategies. Our approach provides\ninsights into improving glioma diagnosis in SSA, showing the potential of deep\nlearning in resource-limited settings and the importance of transfer learning\nfrom high-quality datasets.\n", "link": "http://arxiv.org/abs/2410.18698v1", "date": "2024-10-24", "relevancy": 2.388, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5011}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferring%20Knowledge%20from%20High-Quality%20to%20Low-Quality%20MRI%20for%20Adult%0A%20%20Glioma%20Diagnosis&body=Title%3A%20Transferring%20Knowledge%20from%20High-Quality%20to%20Low-Quality%20MRI%20for%20Adult%0A%20%20Glioma%20Diagnosis%0AAuthor%3A%20Yanguang%20Zhao%20and%20Long%20Bai%20and%20Zhaoxi%20Zhang%20and%20Yanan%20Wu%20and%20Mobarakol%20Islam%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20Glioma%2C%20a%20common%20and%20deadly%20brain%20tumor%2C%20requires%20early%20diagnosis%20for%0Aimproved%20prognosis.%20However%2C%20low-quality%20Magnetic%20Resonance%20Imaging%20%28MRI%29%0Atechnology%20in%20Sub-Saharan%20Africa%20%28SSA%29%20hinders%20accurate%20diagnosis.%20This%20paper%0Apresents%20our%20work%20in%20the%20BraTS%20Challenge%20on%20SSA%20Adult%20Glioma.%20We%20adopt%20the%0Amodel%20from%20the%20BraTS-GLI%202021%20winning%20solution%20and%20utilize%20it%20with%20three%0Atraining%20strategies%3A%20%281%29%20initially%20training%20on%20the%20BraTS-GLI%202021%20dataset%20with%0Afine-tuning%20on%20the%20BraTS-Africa%20dataset%2C%20%282%29%20training%20solely%20on%20the%0ABraTS-Africa%20dataset%2C%20and%20%283%29%20training%20solely%20on%20the%20BraTS-Africa%20dataset%20with%0A2x%20super-resolution%20enhancement.%20Results%20show%20that%20initial%20training%20on%20the%0ABraTS-GLI%202021%20dataset%20followed%20by%20fine-tuning%20on%20the%20BraTS-Africa%20dataset%20has%0Ayielded%20the%20best%20results.%20This%20suggests%20the%20importance%20of%20high-quality%20datasets%0Ain%20providing%20prior%20knowledge%20during%20training.%20Our%20top-performing%20model%20achieves%0ADice%20scores%20of%200.882%2C%200.840%2C%20and%200.926%2C%20and%20Hausdorff%20Distance%20%2895%25%29%20scores%20of%0A15.324%2C%2037.518%2C%20and%2013.971%20for%20enhancing%20tumor%2C%20tumor%20core%2C%20and%20whole%20tumor%2C%0Arespectively%2C%20in%20the%20validation%20phase.%20In%20the%20final%20phase%20of%20the%20competition%2C%0Aour%20approach%20successfully%20secured%20second%20place%20overall%2C%20reflecting%20the%20strength%0Aand%20effectiveness%20of%20our%20model%20and%20training%20strategies.%20Our%20approach%20provides%0Ainsights%20into%20improving%20glioma%20diagnosis%20in%20SSA%2C%20showing%20the%20potential%20of%20deep%0Alearning%20in%20resource-limited%20settings%20and%20the%20importance%20of%20transfer%20learning%0Afrom%20high-quality%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferring%2520Knowledge%2520from%2520High-Quality%2520to%2520Low-Quality%2520MRI%2520for%2520Adult%250A%2520%2520Glioma%2520Diagnosis%26entry.906535625%3DYanguang%2520Zhao%2520and%2520Long%2520Bai%2520and%2520Zhaoxi%2520Zhang%2520and%2520Yanan%2520Wu%2520and%2520Mobarakol%2520Islam%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520Glioma%252C%2520a%2520common%2520and%2520deadly%2520brain%2520tumor%252C%2520requires%2520early%2520diagnosis%2520for%250Aimproved%2520prognosis.%2520However%252C%2520low-quality%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%250Atechnology%2520in%2520Sub-Saharan%2520Africa%2520%2528SSA%2529%2520hinders%2520accurate%2520diagnosis.%2520This%2520paper%250Apresents%2520our%2520work%2520in%2520the%2520BraTS%2520Challenge%2520on%2520SSA%2520Adult%2520Glioma.%2520We%2520adopt%2520the%250Amodel%2520from%2520the%2520BraTS-GLI%25202021%2520winning%2520solution%2520and%2520utilize%2520it%2520with%2520three%250Atraining%2520strategies%253A%2520%25281%2529%2520initially%2520training%2520on%2520the%2520BraTS-GLI%25202021%2520dataset%2520with%250Afine-tuning%2520on%2520the%2520BraTS-Africa%2520dataset%252C%2520%25282%2529%2520training%2520solely%2520on%2520the%250ABraTS-Africa%2520dataset%252C%2520and%2520%25283%2529%2520training%2520solely%2520on%2520the%2520BraTS-Africa%2520dataset%2520with%250A2x%2520super-resolution%2520enhancement.%2520Results%2520show%2520that%2520initial%2520training%2520on%2520the%250ABraTS-GLI%25202021%2520dataset%2520followed%2520by%2520fine-tuning%2520on%2520the%2520BraTS-Africa%2520dataset%2520has%250Ayielded%2520the%2520best%2520results.%2520This%2520suggests%2520the%2520importance%2520of%2520high-quality%2520datasets%250Ain%2520providing%2520prior%2520knowledge%2520during%2520training.%2520Our%2520top-performing%2520model%2520achieves%250ADice%2520scores%2520of%25200.882%252C%25200.840%252C%2520and%25200.926%252C%2520and%2520Hausdorff%2520Distance%2520%252895%2525%2529%2520scores%2520of%250A15.324%252C%252037.518%252C%2520and%252013.971%2520for%2520enhancing%2520tumor%252C%2520tumor%2520core%252C%2520and%2520whole%2520tumor%252C%250Arespectively%252C%2520in%2520the%2520validation%2520phase.%2520In%2520the%2520final%2520phase%2520of%2520the%2520competition%252C%250Aour%2520approach%2520successfully%2520secured%2520second%2520place%2520overall%252C%2520reflecting%2520the%2520strength%250Aand%2520effectiveness%2520of%2520our%2520model%2520and%2520training%2520strategies.%2520Our%2520approach%2520provides%250Ainsights%2520into%2520improving%2520glioma%2520diagnosis%2520in%2520SSA%252C%2520showing%2520the%2520potential%2520of%2520deep%250Alearning%2520in%2520resource-limited%2520settings%2520and%2520the%2520importance%2520of%2520transfer%2520learning%250Afrom%2520high-quality%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferring%20Knowledge%20from%20High-Quality%20to%20Low-Quality%20MRI%20for%20Adult%0A%20%20Glioma%20Diagnosis&entry.906535625=Yanguang%20Zhao%20and%20Long%20Bai%20and%20Zhaoxi%20Zhang%20and%20Yanan%20Wu%20and%20Mobarakol%20Islam%20and%20Hongliang%20Ren&entry.1292438233=%20%20Glioma%2C%20a%20common%20and%20deadly%20brain%20tumor%2C%20requires%20early%20diagnosis%20for%0Aimproved%20prognosis.%20However%2C%20low-quality%20Magnetic%20Resonance%20Imaging%20%28MRI%29%0Atechnology%20in%20Sub-Saharan%20Africa%20%28SSA%29%20hinders%20accurate%20diagnosis.%20This%20paper%0Apresents%20our%20work%20in%20the%20BraTS%20Challenge%20on%20SSA%20Adult%20Glioma.%20We%20adopt%20the%0Amodel%20from%20the%20BraTS-GLI%202021%20winning%20solution%20and%20utilize%20it%20with%20three%0Atraining%20strategies%3A%20%281%29%20initially%20training%20on%20the%20BraTS-GLI%202021%20dataset%20with%0Afine-tuning%20on%20the%20BraTS-Africa%20dataset%2C%20%282%29%20training%20solely%20on%20the%0ABraTS-Africa%20dataset%2C%20and%20%283%29%20training%20solely%20on%20the%20BraTS-Africa%20dataset%20with%0A2x%20super-resolution%20enhancement.%20Results%20show%20that%20initial%20training%20on%20the%0ABraTS-GLI%202021%20dataset%20followed%20by%20fine-tuning%20on%20the%20BraTS-Africa%20dataset%20has%0Ayielded%20the%20best%20results.%20This%20suggests%20the%20importance%20of%20high-quality%20datasets%0Ain%20providing%20prior%20knowledge%20during%20training.%20Our%20top-performing%20model%20achieves%0ADice%20scores%20of%200.882%2C%200.840%2C%20and%200.926%2C%20and%20Hausdorff%20Distance%20%2895%25%29%20scores%20of%0A15.324%2C%2037.518%2C%20and%2013.971%20for%20enhancing%20tumor%2C%20tumor%20core%2C%20and%20whole%20tumor%2C%0Arespectively%2C%20in%20the%20validation%20phase.%20In%20the%20final%20phase%20of%20the%20competition%2C%0Aour%20approach%20successfully%20secured%20second%20place%20overall%2C%20reflecting%20the%20strength%0Aand%20effectiveness%20of%20our%20model%20and%20training%20strategies.%20Our%20approach%20provides%0Ainsights%20into%20improving%20glioma%20diagnosis%20in%20SSA%2C%20showing%20the%20potential%20of%20deep%0Alearning%20in%20resource-limited%20settings%20and%20the%20importance%20of%20transfer%20learning%0Afrom%20high-quality%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18698v1&entry.124074799=Read"},
{"title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks", "author": "Fengji Zhang and Linquan Wu and Huiyu Bai and Guancheng Lin and Xiao Li and Xiao Yu and Yue Wang and Bei Chen and Jacky Keung", "abstract": "  Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.\n", "link": "http://arxiv.org/abs/2410.12381v2", "date": "2024-10-24", "relevancy": 2.3818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanEval-V%3A%20Evaluating%20Visual%20Understanding%20and%20Reasoning%20Abilities%20of%0A%20%20Large%20Multimodal%20Models%20Through%20Coding%20Tasks&body=Title%3A%20HumanEval-V%3A%20Evaluating%20Visual%20Understanding%20and%20Reasoning%20Abilities%20of%0A%20%20Large%20Multimodal%20Models%20Through%20Coding%20Tasks%0AAuthor%3A%20Fengji%20Zhang%20and%20Linquan%20Wu%20and%20Huiyu%20Bai%20and%20Guancheng%20Lin%20and%20Xiao%20Li%20and%20Xiao%20Yu%20and%20Yue%20Wang%20and%20Bei%20Chen%20and%20Jacky%20Keung%0AAbstract%3A%20%20%20Coding%20tasks%20have%20been%20valuable%20for%20evaluating%20Large%20Language%20Models%20%28LLMs%29%2C%0Aas%20they%20demand%20the%20comprehension%20of%20high-level%20instructions%2C%20complex%20reasoning%2C%0Aand%20the%20implementation%20of%20functional%20programs%20--%20core%20capabilities%20for%0Aadvancing%20Artificial%20General%20Intelligence.%20Despite%20the%20progress%20in%20Large%0AMultimodal%20Models%20%28LMMs%29%2C%20which%20extend%20LLMs%20with%20visual%20perception%20and%0Aunderstanding%20capabilities%2C%20there%20remains%20a%20notable%20lack%20of%20coding%20benchmarks%0Athat%20rigorously%20assess%20these%20models%2C%20particularly%20in%20tasks%20that%20emphasize%0Avisual%20reasoning.%20To%20address%20this%20gap%2C%20we%20introduce%20HumanEval-V%2C%20a%20novel%20and%0Alightweight%20benchmark%20specifically%20designed%20to%20evaluate%20LMMs%27%20visual%0Aunderstanding%20and%20reasoning%20capabilities%20through%20code%20generation.%20HumanEval-V%0Aincludes%20108%20carefully%20crafted%2C%20entry-level%20Python%20coding%20tasks%20derived%20from%0Aplatforms%20like%20CodeForces%20and%20Stack%20Overflow.%20Each%20task%20is%20adapted%20by%20modifying%0Athe%20context%20and%20algorithmic%20patterns%20of%20the%20original%20problems%2C%20with%20visual%0Aelements%20redrawn%20to%20ensure%20distinction%20from%20the%20source%2C%20preventing%20potential%0Adata%20leakage.%20LMMs%20are%20required%20to%20complete%20the%20code%20solution%20based%20on%20the%0Aprovided%20visual%20context%20and%20a%20predefined%20Python%20function%20signature%20outlining%0Athe%20task%20requirements.%20Every%20task%20is%20equipped%20with%20meticulously%20handcrafted%0Atest%20cases%20to%20ensure%20a%20thorough%20and%20reliable%20evaluation%20of%20model-generated%0Asolutions.%20We%20evaluate%2019%20state-of-the-art%20LMMs%20using%20HumanEval-V%2C%20uncovering%0Asignificant%20challenges.%20Proprietary%20models%20like%20GPT-4o%20achieve%20only%2013%25%20pass%401%0Aand%2036.4%25%20pass%4010%2C%20while%20open-weight%20models%20with%2070B%20parameters%20score%20below%204%25%0Apass%401.%20Ablation%20studies%20further%20reveal%20the%20limitations%20of%20current%20LMMs%20in%0Avision%20reasoning%20and%20coding%20capabilities.%20These%20results%20underscore%20key%20areas%0Afor%20future%20research%20to%20enhance%20LMMs%27%20capabilities.%20We%20have%20open-sourced%20our%0Acode%20and%20benchmark%20at%20https%3A//github.com/HumanEval-V/HumanEval-V-Benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12381v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanEval-V%253A%2520Evaluating%2520Visual%2520Understanding%2520and%2520Reasoning%2520Abilities%2520of%250A%2520%2520Large%2520Multimodal%2520Models%2520Through%2520Coding%2520Tasks%26entry.906535625%3DFengji%2520Zhang%2520and%2520Linquan%2520Wu%2520and%2520Huiyu%2520Bai%2520and%2520Guancheng%2520Lin%2520and%2520Xiao%2520Li%2520and%2520Xiao%2520Yu%2520and%2520Yue%2520Wang%2520and%2520Bei%2520Chen%2520and%2520Jacky%2520Keung%26entry.1292438233%3D%2520%2520Coding%2520tasks%2520have%2520been%2520valuable%2520for%2520evaluating%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Aas%2520they%2520demand%2520the%2520comprehension%2520of%2520high-level%2520instructions%252C%2520complex%2520reasoning%252C%250Aand%2520the%2520implementation%2520of%2520functional%2520programs%2520--%2520core%2520capabilities%2520for%250Aadvancing%2520Artificial%2520General%2520Intelligence.%2520Despite%2520the%2520progress%2520in%2520Large%250AMultimodal%2520Models%2520%2528LMMs%2529%252C%2520which%2520extend%2520LLMs%2520with%2520visual%2520perception%2520and%250Aunderstanding%2520capabilities%252C%2520there%2520remains%2520a%2520notable%2520lack%2520of%2520coding%2520benchmarks%250Athat%2520rigorously%2520assess%2520these%2520models%252C%2520particularly%2520in%2520tasks%2520that%2520emphasize%250Avisual%2520reasoning.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520HumanEval-V%252C%2520a%2520novel%2520and%250Alightweight%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520LMMs%2527%2520visual%250Aunderstanding%2520and%2520reasoning%2520capabilities%2520through%2520code%2520generation.%2520HumanEval-V%250Aincludes%2520108%2520carefully%2520crafted%252C%2520entry-level%2520Python%2520coding%2520tasks%2520derived%2520from%250Aplatforms%2520like%2520CodeForces%2520and%2520Stack%2520Overflow.%2520Each%2520task%2520is%2520adapted%2520by%2520modifying%250Athe%2520context%2520and%2520algorithmic%2520patterns%2520of%2520the%2520original%2520problems%252C%2520with%2520visual%250Aelements%2520redrawn%2520to%2520ensure%2520distinction%2520from%2520the%2520source%252C%2520preventing%2520potential%250Adata%2520leakage.%2520LMMs%2520are%2520required%2520to%2520complete%2520the%2520code%2520solution%2520based%2520on%2520the%250Aprovided%2520visual%2520context%2520and%2520a%2520predefined%2520Python%2520function%2520signature%2520outlining%250Athe%2520task%2520requirements.%2520Every%2520task%2520is%2520equipped%2520with%2520meticulously%2520handcrafted%250Atest%2520cases%2520to%2520ensure%2520a%2520thorough%2520and%2520reliable%2520evaluation%2520of%2520model-generated%250Asolutions.%2520We%2520evaluate%252019%2520state-of-the-art%2520LMMs%2520using%2520HumanEval-V%252C%2520uncovering%250Asignificant%2520challenges.%2520Proprietary%2520models%2520like%2520GPT-4o%2520achieve%2520only%252013%2525%2520pass%25401%250Aand%252036.4%2525%2520pass%254010%252C%2520while%2520open-weight%2520models%2520with%252070B%2520parameters%2520score%2520below%25204%2525%250Apass%25401.%2520Ablation%2520studies%2520further%2520reveal%2520the%2520limitations%2520of%2520current%2520LMMs%2520in%250Avision%2520reasoning%2520and%2520coding%2520capabilities.%2520These%2520results%2520underscore%2520key%2520areas%250Afor%2520future%2520research%2520to%2520enhance%2520LMMs%2527%2520capabilities.%2520We%2520have%2520open-sourced%2520our%250Acode%2520and%2520benchmark%2520at%2520https%253A//github.com/HumanEval-V/HumanEval-V-Benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12381v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanEval-V%3A%20Evaluating%20Visual%20Understanding%20and%20Reasoning%20Abilities%20of%0A%20%20Large%20Multimodal%20Models%20Through%20Coding%20Tasks&entry.906535625=Fengji%20Zhang%20and%20Linquan%20Wu%20and%20Huiyu%20Bai%20and%20Guancheng%20Lin%20and%20Xiao%20Li%20and%20Xiao%20Yu%20and%20Yue%20Wang%20and%20Bei%20Chen%20and%20Jacky%20Keung&entry.1292438233=%20%20Coding%20tasks%20have%20been%20valuable%20for%20evaluating%20Large%20Language%20Models%20%28LLMs%29%2C%0Aas%20they%20demand%20the%20comprehension%20of%20high-level%20instructions%2C%20complex%20reasoning%2C%0Aand%20the%20implementation%20of%20functional%20programs%20--%20core%20capabilities%20for%0Aadvancing%20Artificial%20General%20Intelligence.%20Despite%20the%20progress%20in%20Large%0AMultimodal%20Models%20%28LMMs%29%2C%20which%20extend%20LLMs%20with%20visual%20perception%20and%0Aunderstanding%20capabilities%2C%20there%20remains%20a%20notable%20lack%20of%20coding%20benchmarks%0Athat%20rigorously%20assess%20these%20models%2C%20particularly%20in%20tasks%20that%20emphasize%0Avisual%20reasoning.%20To%20address%20this%20gap%2C%20we%20introduce%20HumanEval-V%2C%20a%20novel%20and%0Alightweight%20benchmark%20specifically%20designed%20to%20evaluate%20LMMs%27%20visual%0Aunderstanding%20and%20reasoning%20capabilities%20through%20code%20generation.%20HumanEval-V%0Aincludes%20108%20carefully%20crafted%2C%20entry-level%20Python%20coding%20tasks%20derived%20from%0Aplatforms%20like%20CodeForces%20and%20Stack%20Overflow.%20Each%20task%20is%20adapted%20by%20modifying%0Athe%20context%20and%20algorithmic%20patterns%20of%20the%20original%20problems%2C%20with%20visual%0Aelements%20redrawn%20to%20ensure%20distinction%20from%20the%20source%2C%20preventing%20potential%0Adata%20leakage.%20LMMs%20are%20required%20to%20complete%20the%20code%20solution%20based%20on%20the%0Aprovided%20visual%20context%20and%20a%20predefined%20Python%20function%20signature%20outlining%0Athe%20task%20requirements.%20Every%20task%20is%20equipped%20with%20meticulously%20handcrafted%0Atest%20cases%20to%20ensure%20a%20thorough%20and%20reliable%20evaluation%20of%20model-generated%0Asolutions.%20We%20evaluate%2019%20state-of-the-art%20LMMs%20using%20HumanEval-V%2C%20uncovering%0Asignificant%20challenges.%20Proprietary%20models%20like%20GPT-4o%20achieve%20only%2013%25%20pass%401%0Aand%2036.4%25%20pass%4010%2C%20while%20open-weight%20models%20with%2070B%20parameters%20score%20below%204%25%0Apass%401.%20Ablation%20studies%20further%20reveal%20the%20limitations%20of%20current%20LMMs%20in%0Avision%20reasoning%20and%20coding%20capabilities.%20These%20results%20underscore%20key%20areas%0Afor%20future%20research%20to%20enhance%20LMMs%27%20capabilities.%20We%20have%20open-sourced%20our%0Acode%20and%20benchmark%20at%20https%3A//github.com/HumanEval-V/HumanEval-V-Benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12381v2&entry.124074799=Read"},
{"title": "Stable Consistency Tuning: Understanding and Improving Consistency\n  Models", "author": "Fu-Yun Wang and Zhengyang Geng and Hongsheng Li", "abstract": "  Diffusion models achieve superior generation quality but suffer from slow\ngeneration speed due to the iterative nature of denoising. In contrast,\nconsistency models, a new generative family, achieve competitive performance\nwith significantly faster sampling. These models are trained either through\nconsistency distillation, which leverages pretrained diffusion models, or\nconsistency training/tuning directly from raw data. In this work, we propose a\nnovel framework for understanding consistency models by modeling the denoising\nprocess of the diffusion model as a Markov Decision Process (MDP) and framing\nconsistency model training as the value estimation through Temporal\nDifference~(TD) Learning. More importantly, this framework allows us to analyze\nthe limitations of current consistency training/tuning strategies. Built upon\nEasy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT),\nwhich incorporates variance-reduced learning using the score identity. SCT\nleads to significant performance improvements on benchmarks such as CIFAR-10\nand ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID\n1.55, a new SoTA for consistency models.\n", "link": "http://arxiv.org/abs/2410.18958v1", "date": "2024-10-24", "relevancy": 2.3733, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.607}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5884}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Consistency%20Tuning%3A%20Understanding%20and%20Improving%20Consistency%0A%20%20Models&body=Title%3A%20Stable%20Consistency%20Tuning%3A%20Understanding%20and%20Improving%20Consistency%0A%20%20Models%0AAuthor%3A%20Fu-Yun%20Wang%20and%20Zhengyang%20Geng%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Diffusion%20models%20achieve%20superior%20generation%20quality%20but%20suffer%20from%20slow%0Ageneration%20speed%20due%20to%20the%20iterative%20nature%20of%20denoising.%20In%20contrast%2C%0Aconsistency%20models%2C%20a%20new%20generative%20family%2C%20achieve%20competitive%20performance%0Awith%20significantly%20faster%20sampling.%20These%20models%20are%20trained%20either%20through%0Aconsistency%20distillation%2C%20which%20leverages%20pretrained%20diffusion%20models%2C%20or%0Aconsistency%20training/tuning%20directly%20from%20raw%20data.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%20for%20understanding%20consistency%20models%20by%20modeling%20the%20denoising%0Aprocess%20of%20the%20diffusion%20model%20as%20a%20Markov%20Decision%20Process%20%28MDP%29%20and%20framing%0Aconsistency%20model%20training%20as%20the%20value%20estimation%20through%20Temporal%0ADifference~%28TD%29%20Learning.%20More%20importantly%2C%20this%20framework%20allows%20us%20to%20analyze%0Athe%20limitations%20of%20current%20consistency%20training/tuning%20strategies.%20Built%20upon%0AEasy%20Consistency%20Tuning%20%28ECT%29%2C%20we%20propose%20Stable%20Consistency%20Tuning%20%28SCT%29%2C%0Awhich%20incorporates%20variance-reduced%20learning%20using%20the%20score%20identity.%20SCT%0Aleads%20to%20significant%20performance%20improvements%20on%20benchmarks%20such%20as%20CIFAR-10%0Aand%20ImageNet-64.%20On%20ImageNet-64%2C%20SCT%20achieves%201-step%20FID%202.42%20and%202-step%20FID%0A1.55%2C%20a%20new%20SoTA%20for%20consistency%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Consistency%2520Tuning%253A%2520Understanding%2520and%2520Improving%2520Consistency%250A%2520%2520Models%26entry.906535625%3DFu-Yun%2520Wang%2520and%2520Zhengyang%2520Geng%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520achieve%2520superior%2520generation%2520quality%2520but%2520suffer%2520from%2520slow%250Ageneration%2520speed%2520due%2520to%2520the%2520iterative%2520nature%2520of%2520denoising.%2520In%2520contrast%252C%250Aconsistency%2520models%252C%2520a%2520new%2520generative%2520family%252C%2520achieve%2520competitive%2520performance%250Awith%2520significantly%2520faster%2520sampling.%2520These%2520models%2520are%2520trained%2520either%2520through%250Aconsistency%2520distillation%252C%2520which%2520leverages%2520pretrained%2520diffusion%2520models%252C%2520or%250Aconsistency%2520training/tuning%2520directly%2520from%2520raw%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520for%2520understanding%2520consistency%2520models%2520by%2520modeling%2520the%2520denoising%250Aprocess%2520of%2520the%2520diffusion%2520model%2520as%2520a%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520and%2520framing%250Aconsistency%2520model%2520training%2520as%2520the%2520value%2520estimation%2520through%2520Temporal%250ADifference~%2528TD%2529%2520Learning.%2520More%2520importantly%252C%2520this%2520framework%2520allows%2520us%2520to%2520analyze%250Athe%2520limitations%2520of%2520current%2520consistency%2520training/tuning%2520strategies.%2520Built%2520upon%250AEasy%2520Consistency%2520Tuning%2520%2528ECT%2529%252C%2520we%2520propose%2520Stable%2520Consistency%2520Tuning%2520%2528SCT%2529%252C%250Awhich%2520incorporates%2520variance-reduced%2520learning%2520using%2520the%2520score%2520identity.%2520SCT%250Aleads%2520to%2520significant%2520performance%2520improvements%2520on%2520benchmarks%2520such%2520as%2520CIFAR-10%250Aand%2520ImageNet-64.%2520On%2520ImageNet-64%252C%2520SCT%2520achieves%25201-step%2520FID%25202.42%2520and%25202-step%2520FID%250A1.55%252C%2520a%2520new%2520SoTA%2520for%2520consistency%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Consistency%20Tuning%3A%20Understanding%20and%20Improving%20Consistency%0A%20%20Models&entry.906535625=Fu-Yun%20Wang%20and%20Zhengyang%20Geng%20and%20Hongsheng%20Li&entry.1292438233=%20%20Diffusion%20models%20achieve%20superior%20generation%20quality%20but%20suffer%20from%20slow%0Ageneration%20speed%20due%20to%20the%20iterative%20nature%20of%20denoising.%20In%20contrast%2C%0Aconsistency%20models%2C%20a%20new%20generative%20family%2C%20achieve%20competitive%20performance%0Awith%20significantly%20faster%20sampling.%20These%20models%20are%20trained%20either%20through%0Aconsistency%20distillation%2C%20which%20leverages%20pretrained%20diffusion%20models%2C%20or%0Aconsistency%20training/tuning%20directly%20from%20raw%20data.%20In%20this%20work%2C%20we%20propose%20a%0Anovel%20framework%20for%20understanding%20consistency%20models%20by%20modeling%20the%20denoising%0Aprocess%20of%20the%20diffusion%20model%20as%20a%20Markov%20Decision%20Process%20%28MDP%29%20and%20framing%0Aconsistency%20model%20training%20as%20the%20value%20estimation%20through%20Temporal%0ADifference~%28TD%29%20Learning.%20More%20importantly%2C%20this%20framework%20allows%20us%20to%20analyze%0Athe%20limitations%20of%20current%20consistency%20training/tuning%20strategies.%20Built%20upon%0AEasy%20Consistency%20Tuning%20%28ECT%29%2C%20we%20propose%20Stable%20Consistency%20Tuning%20%28SCT%29%2C%0Awhich%20incorporates%20variance-reduced%20learning%20using%20the%20score%20identity.%20SCT%0Aleads%20to%20significant%20performance%20improvements%20on%20benchmarks%20such%20as%20CIFAR-10%0Aand%20ImageNet-64.%20On%20ImageNet-64%2C%20SCT%20achieves%201-step%20FID%202.42%20and%202-step%20FID%0A1.55%2C%20a%20new%20SoTA%20for%20consistency%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18958v1&entry.124074799=Read"},
{"title": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers\n  for Underrepresented Languages", "author": "Artur Kiulian and Anton Polishko and Mykola Khandoga and Yevhen Kostiuk and Guillermo Gabrielli and \u0141ukasz Gaga\u0142a and Fadi Zaraket and Qusai Abu Obaida and Hrishikesh Garud and Wendy Wing Yee Mak and Dmytro Chaplynskyi and Selma Belhadj Amor and Grigol Peradze", "abstract": "  In this paper, we propose a model-agnostic cost-effective approach to\ndeveloping bilingual base large language models (LLMs) to support English and\nany target language. The method includes vocabulary expansion, initialization\nof new embeddings, model training and evaluation. We performed our experiments\nwith three languages, each using a non-Latin script - Ukrainian, Arabic, and\nGeorgian.\n  Our approach demonstrates improved language performance while reducing\ncomputational costs. It mitigates the disproportionate penalization of\nunderrepresented languages, promoting fairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar. Additionally, we introduce new\nmetrics to evaluate language quality, revealing that vocabulary size\nsignificantly impacts the quality of generated text.\n", "link": "http://arxiv.org/abs/2410.18836v1", "date": "2024-10-24", "relevancy": 2.3715, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20English-Centric%20to%20Effective%20Bilingual%3A%20LLMs%20with%20Custom%20Tokenizers%0A%20%20for%20Underrepresented%20Languages&body=Title%3A%20From%20English-Centric%20to%20Effective%20Bilingual%3A%20LLMs%20with%20Custom%20Tokenizers%0A%20%20for%20Underrepresented%20Languages%0AAuthor%3A%20Artur%20Kiulian%20and%20Anton%20Polishko%20and%20Mykola%20Khandoga%20and%20Yevhen%20Kostiuk%20and%20Guillermo%20Gabrielli%20and%20%C5%81ukasz%20Gaga%C5%82a%20and%20Fadi%20Zaraket%20and%20Qusai%20Abu%20Obaida%20and%20Hrishikesh%20Garud%20and%20Wendy%20Wing%20Yee%20Mak%20and%20Dmytro%20Chaplynskyi%20and%20Selma%20Belhadj%20Amor%20and%20Grigol%20Peradze%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20model-agnostic%20cost-effective%20approach%20to%0Adeveloping%20bilingual%20base%20large%20language%20models%20%28LLMs%29%20to%20support%20English%20and%0Aany%20target%20language.%20The%20method%20includes%20vocabulary%20expansion%2C%20initialization%0Aof%20new%20embeddings%2C%20model%20training%20and%20evaluation.%20We%20performed%20our%20experiments%0Awith%20three%20languages%2C%20each%20using%20a%20non-Latin%20script%20-%20Ukrainian%2C%20Arabic%2C%20and%0AGeorgian.%0A%20%20Our%20approach%20demonstrates%20improved%20language%20performance%20while%20reducing%0Acomputational%20costs.%20It%20mitigates%20the%20disproportionate%20penalization%20of%0Aunderrepresented%20languages%2C%20promoting%20fairness%20and%20minimizing%20adverse%20phenomena%0Asuch%20as%20code-switching%20and%20broken%20grammar.%20Additionally%2C%20we%20introduce%20new%0Ametrics%20to%20evaluate%20language%20quality%2C%20revealing%20that%20vocabulary%20size%0Asignificantly%20impacts%20the%20quality%20of%20generated%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520English-Centric%2520to%2520Effective%2520Bilingual%253A%2520LLMs%2520with%2520Custom%2520Tokenizers%250A%2520%2520for%2520Underrepresented%2520Languages%26entry.906535625%3DArtur%2520Kiulian%2520and%2520Anton%2520Polishko%2520and%2520Mykola%2520Khandoga%2520and%2520Yevhen%2520Kostiuk%2520and%2520Guillermo%2520Gabrielli%2520and%2520%25C5%2581ukasz%2520Gaga%25C5%2582a%2520and%2520Fadi%2520Zaraket%2520and%2520Qusai%2520Abu%2520Obaida%2520and%2520Hrishikesh%2520Garud%2520and%2520Wendy%2520Wing%2520Yee%2520Mak%2520and%2520Dmytro%2520Chaplynskyi%2520and%2520Selma%2520Belhadj%2520Amor%2520and%2520Grigol%2520Peradze%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520model-agnostic%2520cost-effective%2520approach%2520to%250Adeveloping%2520bilingual%2520base%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520support%2520English%2520and%250Aany%2520target%2520language.%2520The%2520method%2520includes%2520vocabulary%2520expansion%252C%2520initialization%250Aof%2520new%2520embeddings%252C%2520model%2520training%2520and%2520evaluation.%2520We%2520performed%2520our%2520experiments%250Awith%2520three%2520languages%252C%2520each%2520using%2520a%2520non-Latin%2520script%2520-%2520Ukrainian%252C%2520Arabic%252C%2520and%250AGeorgian.%250A%2520%2520Our%2520approach%2520demonstrates%2520improved%2520language%2520performance%2520while%2520reducing%250Acomputational%2520costs.%2520It%2520mitigates%2520the%2520disproportionate%2520penalization%2520of%250Aunderrepresented%2520languages%252C%2520promoting%2520fairness%2520and%2520minimizing%2520adverse%2520phenomena%250Asuch%2520as%2520code-switching%2520and%2520broken%2520grammar.%2520Additionally%252C%2520we%2520introduce%2520new%250Ametrics%2520to%2520evaluate%2520language%2520quality%252C%2520revealing%2520that%2520vocabulary%2520size%250Asignificantly%2520impacts%2520the%2520quality%2520of%2520generated%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20English-Centric%20to%20Effective%20Bilingual%3A%20LLMs%20with%20Custom%20Tokenizers%0A%20%20for%20Underrepresented%20Languages&entry.906535625=Artur%20Kiulian%20and%20Anton%20Polishko%20and%20Mykola%20Khandoga%20and%20Yevhen%20Kostiuk%20and%20Guillermo%20Gabrielli%20and%20%C5%81ukasz%20Gaga%C5%82a%20and%20Fadi%20Zaraket%20and%20Qusai%20Abu%20Obaida%20and%20Hrishikesh%20Garud%20and%20Wendy%20Wing%20Yee%20Mak%20and%20Dmytro%20Chaplynskyi%20and%20Selma%20Belhadj%20Amor%20and%20Grigol%20Peradze&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20model-agnostic%20cost-effective%20approach%20to%0Adeveloping%20bilingual%20base%20large%20language%20models%20%28LLMs%29%20to%20support%20English%20and%0Aany%20target%20language.%20The%20method%20includes%20vocabulary%20expansion%2C%20initialization%0Aof%20new%20embeddings%2C%20model%20training%20and%20evaluation.%20We%20performed%20our%20experiments%0Awith%20three%20languages%2C%20each%20using%20a%20non-Latin%20script%20-%20Ukrainian%2C%20Arabic%2C%20and%0AGeorgian.%0A%20%20Our%20approach%20demonstrates%20improved%20language%20performance%20while%20reducing%0Acomputational%20costs.%20It%20mitigates%20the%20disproportionate%20penalization%20of%0Aunderrepresented%20languages%2C%20promoting%20fairness%20and%20minimizing%20adverse%20phenomena%0Asuch%20as%20code-switching%20and%20broken%20grammar.%20Additionally%2C%20we%20introduce%20new%0Ametrics%20to%20evaluate%20language%20quality%2C%20revealing%20that%20vocabulary%20size%0Asignificantly%20impacts%20the%20quality%20of%20generated%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18836v1&entry.124074799=Read"},
{"title": "Unbounded: A Generative Infinite Game of Character Life Simulation", "author": "Jialu Li and Yuanzhen Li and Neal Wadhwa and Yael Pritch and David E. Jacobs and Michael Rubinstein and Mohit Bansal and Nataniel Ruiz", "abstract": "  We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.\n", "link": "http://arxiv.org/abs/2410.18975v1", "date": "2024-10-24", "relevancy": 2.3658, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6453}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5712}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unbounded%3A%20A%20Generative%20Infinite%20Game%20of%20Character%20Life%20Simulation&body=Title%3A%20Unbounded%3A%20A%20Generative%20Infinite%20Game%20of%20Character%20Life%20Simulation%0AAuthor%3A%20Jialu%20Li%20and%20Yuanzhen%20Li%20and%20Neal%20Wadhwa%20and%20Yael%20Pritch%20and%20David%20E.%20Jacobs%20and%20Michael%20Rubinstein%20and%20Mohit%20Bansal%20and%20Nataniel%20Ruiz%0AAbstract%3A%20%20%20We%20introduce%20the%20concept%20of%20a%20generative%20infinite%20game%2C%20a%20video%20game%20that%0Atranscends%20the%20traditional%20boundaries%20of%20finite%2C%20hard-coded%20systems%20by%20using%0Agenerative%20models.%20Inspired%20by%20James%20P.%20Carse%27s%20distinction%20between%20finite%20and%0Ainfinite%20games%2C%20we%20leverage%20recent%20advances%20in%20generative%20AI%20to%20create%0AUnbounded%3A%20a%20game%20of%20character%20life%20simulation%20that%20is%20fully%20encapsulated%20in%0Agenerative%20models.%20Specifically%2C%20Unbounded%20draws%20inspiration%20from%20sandbox%20life%0Asimulations%20and%20allows%20you%20to%20interact%20with%20your%20autonomous%20virtual%20character%0Ain%20a%20virtual%20world%20by%20feeding%2C%20playing%20with%20and%20guiding%20it%20-%20with%20open-ended%0Amechanics%20generated%20by%20an%20LLM%2C%20some%20of%20which%20can%20be%20emergent.%20In%20order%20to%0Adevelop%20Unbounded%2C%20we%20propose%20technical%20innovations%20in%20both%20the%20LLM%20and%20visual%0Ageneration%20domains.%20Specifically%2C%20we%20present%3A%20%281%29%20a%20specialized%2C%20distilled%0Alarge%20language%20model%20%28LLM%29%20that%20dynamically%20generates%20game%20mechanics%2C%0Anarratives%2C%20and%20character%20interactions%20in%20real-time%2C%20and%20%282%29%20a%20new%20dynamic%0Aregional%20image%20prompt%20Adapter%20%28IP-Adapter%29%20for%20vision%20models%20that%20ensures%0Aconsistent%20yet%20flexible%20visual%20generation%20of%20a%20character%20across%20multiple%0Aenvironments.%20We%20evaluate%20our%20system%20through%20both%20qualitative%20and%20quantitative%0Aanalysis%2C%20showing%20significant%20improvements%20in%20character%20life%20simulation%2C%20user%0Ainstruction%20following%2C%20narrative%20coherence%2C%20and%20visual%20consistency%20for%20both%0Acharacters%20and%20the%20environments%20compared%20to%20traditional%20related%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnbounded%253A%2520A%2520Generative%2520Infinite%2520Game%2520of%2520Character%2520Life%2520Simulation%26entry.906535625%3DJialu%2520Li%2520and%2520Yuanzhen%2520Li%2520and%2520Neal%2520Wadhwa%2520and%2520Yael%2520Pritch%2520and%2520David%2520E.%2520Jacobs%2520and%2520Michael%2520Rubinstein%2520and%2520Mohit%2520Bansal%2520and%2520Nataniel%2520Ruiz%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520concept%2520of%2520a%2520generative%2520infinite%2520game%252C%2520a%2520video%2520game%2520that%250Atranscends%2520the%2520traditional%2520boundaries%2520of%2520finite%252C%2520hard-coded%2520systems%2520by%2520using%250Agenerative%2520models.%2520Inspired%2520by%2520James%2520P.%2520Carse%2527s%2520distinction%2520between%2520finite%2520and%250Ainfinite%2520games%252C%2520we%2520leverage%2520recent%2520advances%2520in%2520generative%2520AI%2520to%2520create%250AUnbounded%253A%2520a%2520game%2520of%2520character%2520life%2520simulation%2520that%2520is%2520fully%2520encapsulated%2520in%250Agenerative%2520models.%2520Specifically%252C%2520Unbounded%2520draws%2520inspiration%2520from%2520sandbox%2520life%250Asimulations%2520and%2520allows%2520you%2520to%2520interact%2520with%2520your%2520autonomous%2520virtual%2520character%250Ain%2520a%2520virtual%2520world%2520by%2520feeding%252C%2520playing%2520with%2520and%2520guiding%2520it%2520-%2520with%2520open-ended%250Amechanics%2520generated%2520by%2520an%2520LLM%252C%2520some%2520of%2520which%2520can%2520be%2520emergent.%2520In%2520order%2520to%250Adevelop%2520Unbounded%252C%2520we%2520propose%2520technical%2520innovations%2520in%2520both%2520the%2520LLM%2520and%2520visual%250Ageneration%2520domains.%2520Specifically%252C%2520we%2520present%253A%2520%25281%2529%2520a%2520specialized%252C%2520distilled%250Alarge%2520language%2520model%2520%2528LLM%2529%2520that%2520dynamically%2520generates%2520game%2520mechanics%252C%250Anarratives%252C%2520and%2520character%2520interactions%2520in%2520real-time%252C%2520and%2520%25282%2529%2520a%2520new%2520dynamic%250Aregional%2520image%2520prompt%2520Adapter%2520%2528IP-Adapter%2529%2520for%2520vision%2520models%2520that%2520ensures%250Aconsistent%2520yet%2520flexible%2520visual%2520generation%2520of%2520a%2520character%2520across%2520multiple%250Aenvironments.%2520We%2520evaluate%2520our%2520system%2520through%2520both%2520qualitative%2520and%2520quantitative%250Aanalysis%252C%2520showing%2520significant%2520improvements%2520in%2520character%2520life%2520simulation%252C%2520user%250Ainstruction%2520following%252C%2520narrative%2520coherence%252C%2520and%2520visual%2520consistency%2520for%2520both%250Acharacters%2520and%2520the%2520environments%2520compared%2520to%2520traditional%2520related%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbounded%3A%20A%20Generative%20Infinite%20Game%20of%20Character%20Life%20Simulation&entry.906535625=Jialu%20Li%20and%20Yuanzhen%20Li%20and%20Neal%20Wadhwa%20and%20Yael%20Pritch%20and%20David%20E.%20Jacobs%20and%20Michael%20Rubinstein%20and%20Mohit%20Bansal%20and%20Nataniel%20Ruiz&entry.1292438233=%20%20We%20introduce%20the%20concept%20of%20a%20generative%20infinite%20game%2C%20a%20video%20game%20that%0Atranscends%20the%20traditional%20boundaries%20of%20finite%2C%20hard-coded%20systems%20by%20using%0Agenerative%20models.%20Inspired%20by%20James%20P.%20Carse%27s%20distinction%20between%20finite%20and%0Ainfinite%20games%2C%20we%20leverage%20recent%20advances%20in%20generative%20AI%20to%20create%0AUnbounded%3A%20a%20game%20of%20character%20life%20simulation%20that%20is%20fully%20encapsulated%20in%0Agenerative%20models.%20Specifically%2C%20Unbounded%20draws%20inspiration%20from%20sandbox%20life%0Asimulations%20and%20allows%20you%20to%20interact%20with%20your%20autonomous%20virtual%20character%0Ain%20a%20virtual%20world%20by%20feeding%2C%20playing%20with%20and%20guiding%20it%20-%20with%20open-ended%0Amechanics%20generated%20by%20an%20LLM%2C%20some%20of%20which%20can%20be%20emergent.%20In%20order%20to%0Adevelop%20Unbounded%2C%20we%20propose%20technical%20innovations%20in%20both%20the%20LLM%20and%20visual%0Ageneration%20domains.%20Specifically%2C%20we%20present%3A%20%281%29%20a%20specialized%2C%20distilled%0Alarge%20language%20model%20%28LLM%29%20that%20dynamically%20generates%20game%20mechanics%2C%0Anarratives%2C%20and%20character%20interactions%20in%20real-time%2C%20and%20%282%29%20a%20new%20dynamic%0Aregional%20image%20prompt%20Adapter%20%28IP-Adapter%29%20for%20vision%20models%20that%20ensures%0Aconsistent%20yet%20flexible%20visual%20generation%20of%20a%20character%20across%20multiple%0Aenvironments.%20We%20evaluate%20our%20system%20through%20both%20qualitative%20and%20quantitative%0Aanalysis%2C%20showing%20significant%20improvements%20in%20character%20life%20simulation%2C%20user%0Ainstruction%20following%2C%20narrative%20coherence%2C%20and%20visual%20consistency%20for%20both%0Acharacters%20and%20the%20environments%20compared%20to%20traditional%20related%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18975v1&entry.124074799=Read"},
{"title": "Pointer Networks with Q-Learning for Combinatorial Optimization", "author": "Alessandro Barro", "abstract": "  We introduce the Pointer Q-Network (PQN), a hybrid neural architecture that\nintegrates model-free Q-value policy approximation with Pointer Networks\n(Ptr-Nets) to enhance the optimality of attention-based sequence generation,\nfocusing on long-term outcomes. This integration proves particularly effective\nin solving combinatorial optimization (CO) tasks, especially the Travelling\nSalesman Problem (TSP), which is the focus of our study. We address this\nchallenge by defining a Markov Decision Process (MDP) compatible with PQN,\nwhich involves iterative graph embedding, encoding and decoding by an\nLSTM-based recurrent neural network. This process generates a context vector\nand computes raw attention scores, which are dynamically adjusted by Q-values\ncalculated for all available state-action pairs before applying softmax. The\nresulting attention vector is utilized as an action distribution, with actions\nselected hinged to exploration-exploitation dynamic adaptibility of PQN. Our\nempirical results demonstrate the efficacy of this approach, also testing the\nmodel in unstable environments.\n", "link": "http://arxiv.org/abs/2311.02629v4", "date": "2024-10-24", "relevancy": 2.3599, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4759}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4722}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pointer%20Networks%20with%20Q-Learning%20for%20Combinatorial%20Optimization&body=Title%3A%20Pointer%20Networks%20with%20Q-Learning%20for%20Combinatorial%20Optimization%0AAuthor%3A%20Alessandro%20Barro%0AAbstract%3A%20%20%20We%20introduce%20the%20Pointer%20Q-Network%20%28PQN%29%2C%20a%20hybrid%20neural%20architecture%20that%0Aintegrates%20model-free%20Q-value%20policy%20approximation%20with%20Pointer%20Networks%0A%28Ptr-Nets%29%20to%20enhance%20the%20optimality%20of%20attention-based%20sequence%20generation%2C%0Afocusing%20on%20long-term%20outcomes.%20This%20integration%20proves%20particularly%20effective%0Ain%20solving%20combinatorial%20optimization%20%28CO%29%20tasks%2C%20especially%20the%20Travelling%0ASalesman%20Problem%20%28TSP%29%2C%20which%20is%20the%20focus%20of%20our%20study.%20We%20address%20this%0Achallenge%20by%20defining%20a%20Markov%20Decision%20Process%20%28MDP%29%20compatible%20with%20PQN%2C%0Awhich%20involves%20iterative%20graph%20embedding%2C%20encoding%20and%20decoding%20by%20an%0ALSTM-based%20recurrent%20neural%20network.%20This%20process%20generates%20a%20context%20vector%0Aand%20computes%20raw%20attention%20scores%2C%20which%20are%20dynamically%20adjusted%20by%20Q-values%0Acalculated%20for%20all%20available%20state-action%20pairs%20before%20applying%20softmax.%20The%0Aresulting%20attention%20vector%20is%20utilized%20as%20an%20action%20distribution%2C%20with%20actions%0Aselected%20hinged%20to%20exploration-exploitation%20dynamic%20adaptibility%20of%20PQN.%20Our%0Aempirical%20results%20demonstrate%20the%20efficacy%20of%20this%20approach%2C%20also%20testing%20the%0Amodel%20in%20unstable%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02629v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointer%2520Networks%2520with%2520Q-Learning%2520for%2520Combinatorial%2520Optimization%26entry.906535625%3DAlessandro%2520Barro%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Pointer%2520Q-Network%2520%2528PQN%2529%252C%2520a%2520hybrid%2520neural%2520architecture%2520that%250Aintegrates%2520model-free%2520Q-value%2520policy%2520approximation%2520with%2520Pointer%2520Networks%250A%2528Ptr-Nets%2529%2520to%2520enhance%2520the%2520optimality%2520of%2520attention-based%2520sequence%2520generation%252C%250Afocusing%2520on%2520long-term%2520outcomes.%2520This%2520integration%2520proves%2520particularly%2520effective%250Ain%2520solving%2520combinatorial%2520optimization%2520%2528CO%2529%2520tasks%252C%2520especially%2520the%2520Travelling%250ASalesman%2520Problem%2520%2528TSP%2529%252C%2520which%2520is%2520the%2520focus%2520of%2520our%2520study.%2520We%2520address%2520this%250Achallenge%2520by%2520defining%2520a%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520compatible%2520with%2520PQN%252C%250Awhich%2520involves%2520iterative%2520graph%2520embedding%252C%2520encoding%2520and%2520decoding%2520by%2520an%250ALSTM-based%2520recurrent%2520neural%2520network.%2520This%2520process%2520generates%2520a%2520context%2520vector%250Aand%2520computes%2520raw%2520attention%2520scores%252C%2520which%2520are%2520dynamically%2520adjusted%2520by%2520Q-values%250Acalculated%2520for%2520all%2520available%2520state-action%2520pairs%2520before%2520applying%2520softmax.%2520The%250Aresulting%2520attention%2520vector%2520is%2520utilized%2520as%2520an%2520action%2520distribution%252C%2520with%2520actions%250Aselected%2520hinged%2520to%2520exploration-exploitation%2520dynamic%2520adaptibility%2520of%2520PQN.%2520Our%250Aempirical%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520this%2520approach%252C%2520also%2520testing%2520the%250Amodel%2520in%2520unstable%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02629v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pointer%20Networks%20with%20Q-Learning%20for%20Combinatorial%20Optimization&entry.906535625=Alessandro%20Barro&entry.1292438233=%20%20We%20introduce%20the%20Pointer%20Q-Network%20%28PQN%29%2C%20a%20hybrid%20neural%20architecture%20that%0Aintegrates%20model-free%20Q-value%20policy%20approximation%20with%20Pointer%20Networks%0A%28Ptr-Nets%29%20to%20enhance%20the%20optimality%20of%20attention-based%20sequence%20generation%2C%0Afocusing%20on%20long-term%20outcomes.%20This%20integration%20proves%20particularly%20effective%0Ain%20solving%20combinatorial%20optimization%20%28CO%29%20tasks%2C%20especially%20the%20Travelling%0ASalesman%20Problem%20%28TSP%29%2C%20which%20is%20the%20focus%20of%20our%20study.%20We%20address%20this%0Achallenge%20by%20defining%20a%20Markov%20Decision%20Process%20%28MDP%29%20compatible%20with%20PQN%2C%0Awhich%20involves%20iterative%20graph%20embedding%2C%20encoding%20and%20decoding%20by%20an%0ALSTM-based%20recurrent%20neural%20network.%20This%20process%20generates%20a%20context%20vector%0Aand%20computes%20raw%20attention%20scores%2C%20which%20are%20dynamically%20adjusted%20by%20Q-values%0Acalculated%20for%20all%20available%20state-action%20pairs%20before%20applying%20softmax.%20The%0Aresulting%20attention%20vector%20is%20utilized%20as%20an%20action%20distribution%2C%20with%20actions%0Aselected%20hinged%20to%20exploration-exploitation%20dynamic%20adaptibility%20of%20PQN.%20Our%0Aempirical%20results%20demonstrate%20the%20efficacy%20of%20this%20approach%2C%20also%20testing%20the%0Amodel%20in%20unstable%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02629v4&entry.124074799=Read"},
{"title": "LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor\n  Search", "author": "Elias J\u00e4\u00e4saari and Ville Hyv\u00f6nen and Teemu Roos", "abstract": "  Approximate nearest neighbor (ANN) search is a key component in many modern\nmachine learning pipelines; recent use cases include retrieval-augmented\ngeneration (RAG) and vector databases. Clustering-based ANN algorithms, that\nuse score computation methods based on product quantization (PQ), are often\nused in industrial-scale applications due to their scalability and suitability\nfor distributed and disk-based implementations. However, they have slower query\ntimes than the leading graph-based ANN algorithms. In this work, we propose a\nnew supervised score computation method based on the observation that inner\nproduct approximation is a multivariate (multi-output) regression problem that\ncan be solved efficiently by reduced-rank regression. Our experiments show that\non modern high-dimensional data sets, the proposed reduced-rank regression\n(RRR) method is superior to PQ in both query latency and memory usage. We also\nintroduce LoRANN, a clustering-based ANN library that leverages the proposed\nscore computation method. LoRANN is competitive with the leading graph-based\nalgorithms and outperforms the state-of-the-art GPU ANN methods on\nhigh-dimensional data sets.\n", "link": "http://arxiv.org/abs/2410.18926v1", "date": "2024-10-24", "relevancy": 2.3598, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.48}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4681}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRANN%3A%20Low-Rank%20Matrix%20Factorization%20for%20Approximate%20Nearest%20Neighbor%0A%20%20Search&body=Title%3A%20LoRANN%3A%20Low-Rank%20Matrix%20Factorization%20for%20Approximate%20Nearest%20Neighbor%0A%20%20Search%0AAuthor%3A%20Elias%20J%C3%A4%C3%A4saari%20and%20Ville%20Hyv%C3%B6nen%20and%20Teemu%20Roos%0AAbstract%3A%20%20%20Approximate%20nearest%20neighbor%20%28ANN%29%20search%20is%20a%20key%20component%20in%20many%20modern%0Amachine%20learning%20pipelines%3B%20recent%20use%20cases%20include%20retrieval-augmented%0Ageneration%20%28RAG%29%20and%20vector%20databases.%20Clustering-based%20ANN%20algorithms%2C%20that%0Ause%20score%20computation%20methods%20based%20on%20product%20quantization%20%28PQ%29%2C%20are%20often%0Aused%20in%20industrial-scale%20applications%20due%20to%20their%20scalability%20and%20suitability%0Afor%20distributed%20and%20disk-based%20implementations.%20However%2C%20they%20have%20slower%20query%0Atimes%20than%20the%20leading%20graph-based%20ANN%20algorithms.%20In%20this%20work%2C%20we%20propose%20a%0Anew%20supervised%20score%20computation%20method%20based%20on%20the%20observation%20that%20inner%0Aproduct%20approximation%20is%20a%20multivariate%20%28multi-output%29%20regression%20problem%20that%0Acan%20be%20solved%20efficiently%20by%20reduced-rank%20regression.%20Our%20experiments%20show%20that%0Aon%20modern%20high-dimensional%20data%20sets%2C%20the%20proposed%20reduced-rank%20regression%0A%28RRR%29%20method%20is%20superior%20to%20PQ%20in%20both%20query%20latency%20and%20memory%20usage.%20We%20also%0Aintroduce%20LoRANN%2C%20a%20clustering-based%20ANN%20library%20that%20leverages%20the%20proposed%0Ascore%20computation%20method.%20LoRANN%20is%20competitive%20with%20the%20leading%20graph-based%0Aalgorithms%20and%20outperforms%20the%20state-of-the-art%20GPU%20ANN%20methods%20on%0Ahigh-dimensional%20data%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRANN%253A%2520Low-Rank%2520Matrix%2520Factorization%2520for%2520Approximate%2520Nearest%2520Neighbor%250A%2520%2520Search%26entry.906535625%3DElias%2520J%25C3%25A4%25C3%25A4saari%2520and%2520Ville%2520Hyv%25C3%25B6nen%2520and%2520Teemu%2520Roos%26entry.1292438233%3D%2520%2520Approximate%2520nearest%2520neighbor%2520%2528ANN%2529%2520search%2520is%2520a%2520key%2520component%2520in%2520many%2520modern%250Amachine%2520learning%2520pipelines%253B%2520recent%2520use%2520cases%2520include%2520retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520and%2520vector%2520databases.%2520Clustering-based%2520ANN%2520algorithms%252C%2520that%250Ause%2520score%2520computation%2520methods%2520based%2520on%2520product%2520quantization%2520%2528PQ%2529%252C%2520are%2520often%250Aused%2520in%2520industrial-scale%2520applications%2520due%2520to%2520their%2520scalability%2520and%2520suitability%250Afor%2520distributed%2520and%2520disk-based%2520implementations.%2520However%252C%2520they%2520have%2520slower%2520query%250Atimes%2520than%2520the%2520leading%2520graph-based%2520ANN%2520algorithms.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Anew%2520supervised%2520score%2520computation%2520method%2520based%2520on%2520the%2520observation%2520that%2520inner%250Aproduct%2520approximation%2520is%2520a%2520multivariate%2520%2528multi-output%2529%2520regression%2520problem%2520that%250Acan%2520be%2520solved%2520efficiently%2520by%2520reduced-rank%2520regression.%2520Our%2520experiments%2520show%2520that%250Aon%2520modern%2520high-dimensional%2520data%2520sets%252C%2520the%2520proposed%2520reduced-rank%2520regression%250A%2528RRR%2529%2520method%2520is%2520superior%2520to%2520PQ%2520in%2520both%2520query%2520latency%2520and%2520memory%2520usage.%2520We%2520also%250Aintroduce%2520LoRANN%252C%2520a%2520clustering-based%2520ANN%2520library%2520that%2520leverages%2520the%2520proposed%250Ascore%2520computation%2520method.%2520LoRANN%2520is%2520competitive%2520with%2520the%2520leading%2520graph-based%250Aalgorithms%2520and%2520outperforms%2520the%2520state-of-the-art%2520GPU%2520ANN%2520methods%2520on%250Ahigh-dimensional%2520data%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRANN%3A%20Low-Rank%20Matrix%20Factorization%20for%20Approximate%20Nearest%20Neighbor%0A%20%20Search&entry.906535625=Elias%20J%C3%A4%C3%A4saari%20and%20Ville%20Hyv%C3%B6nen%20and%20Teemu%20Roos&entry.1292438233=%20%20Approximate%20nearest%20neighbor%20%28ANN%29%20search%20is%20a%20key%20component%20in%20many%20modern%0Amachine%20learning%20pipelines%3B%20recent%20use%20cases%20include%20retrieval-augmented%0Ageneration%20%28RAG%29%20and%20vector%20databases.%20Clustering-based%20ANN%20algorithms%2C%20that%0Ause%20score%20computation%20methods%20based%20on%20product%20quantization%20%28PQ%29%2C%20are%20often%0Aused%20in%20industrial-scale%20applications%20due%20to%20their%20scalability%20and%20suitability%0Afor%20distributed%20and%20disk-based%20implementations.%20However%2C%20they%20have%20slower%20query%0Atimes%20than%20the%20leading%20graph-based%20ANN%20algorithms.%20In%20this%20work%2C%20we%20propose%20a%0Anew%20supervised%20score%20computation%20method%20based%20on%20the%20observation%20that%20inner%0Aproduct%20approximation%20is%20a%20multivariate%20%28multi-output%29%20regression%20problem%20that%0Acan%20be%20solved%20efficiently%20by%20reduced-rank%20regression.%20Our%20experiments%20show%20that%0Aon%20modern%20high-dimensional%20data%20sets%2C%20the%20proposed%20reduced-rank%20regression%0A%28RRR%29%20method%20is%20superior%20to%20PQ%20in%20both%20query%20latency%20and%20memory%20usage.%20We%20also%0Aintroduce%20LoRANN%2C%20a%20clustering-based%20ANN%20library%20that%20leverages%20the%20proposed%0Ascore%20computation%20method.%20LoRANN%20is%20competitive%20with%20the%20leading%20graph-based%0Aalgorithms%20and%20outperforms%20the%20state-of-the-art%20GPU%20ANN%20methods%20on%0Ahigh-dimensional%20data%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18926v1&entry.124074799=Read"},
{"title": "GeoLoRA: Geometric integration for parameter efficient fine-tuning", "author": "Steffen Schotth\u00f6fer and Emanuele Zangrando and Gianluca Ceruti and Francesco Tudisco and Jonas Kusch", "abstract": "  Low-Rank Adaptation (LoRA) has become a widely used method for\nparameter-efficient fine-tuning of large-scale, pre-trained neural networks.\nHowever, LoRA and its extensions face several challenges, including the need\nfor rank adaptivity, robustness, and computational efficiency during the\nfine-tuning process. We introduce GeoLoRA, a novel approach that addresses\nthese limitations by leveraging dynamical low-rank approximation theory.\nGeoLoRA requires only a single backpropagation pass over the small-rank\nadapters, significantly reducing computational cost as compared to similar\ndynamical low-rank training methods and making it faster than popular baselines\nsuch as AdaLoRA. This allows GeoLoRA to efficiently adapt the allocated\nparameter budget across the model, achieving smaller low-rank adapters compared\nto heuristic methods like AdaLoRA and LoRA, while maintaining critical\nconvergence, descent, and error-bound theoretical guarantees. The resulting\nmethod is not only more efficient but also more robust to varying\nhyperparameter settings. We demonstrate the effectiveness of GeoLoRA on several\nstate-of-the-art benchmarks, showing that it outperforms existing methods in\nboth accuracy and computational efficiency.\n", "link": "http://arxiv.org/abs/2410.18720v1", "date": "2024-10-24", "relevancy": 2.3558, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4955}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4592}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoLoRA%3A%20Geometric%20integration%20for%20parameter%20efficient%20fine-tuning&body=Title%3A%20GeoLoRA%3A%20Geometric%20integration%20for%20parameter%20efficient%20fine-tuning%0AAuthor%3A%20Steffen%20Schotth%C3%B6fer%20and%20Emanuele%20Zangrando%20and%20Gianluca%20Ceruti%20and%20Francesco%20Tudisco%20and%20Jonas%20Kusch%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20widely%20used%20method%20for%0Aparameter-efficient%20fine-tuning%20of%20large-scale%2C%20pre-trained%20neural%20networks.%0AHowever%2C%20LoRA%20and%20its%20extensions%20face%20several%20challenges%2C%20including%20the%20need%0Afor%20rank%20adaptivity%2C%20robustness%2C%20and%20computational%20efficiency%20during%20the%0Afine-tuning%20process.%20We%20introduce%20GeoLoRA%2C%20a%20novel%20approach%20that%20addresses%0Athese%20limitations%20by%20leveraging%20dynamical%20low-rank%20approximation%20theory.%0AGeoLoRA%20requires%20only%20a%20single%20backpropagation%20pass%20over%20the%20small-rank%0Aadapters%2C%20significantly%20reducing%20computational%20cost%20as%20compared%20to%20similar%0Adynamical%20low-rank%20training%20methods%20and%20making%20it%20faster%20than%20popular%20baselines%0Asuch%20as%20AdaLoRA.%20This%20allows%20GeoLoRA%20to%20efficiently%20adapt%20the%20allocated%0Aparameter%20budget%20across%20the%20model%2C%20achieving%20smaller%20low-rank%20adapters%20compared%0Ato%20heuristic%20methods%20like%20AdaLoRA%20and%20LoRA%2C%20while%20maintaining%20critical%0Aconvergence%2C%20descent%2C%20and%20error-bound%20theoretical%20guarantees.%20The%20resulting%0Amethod%20is%20not%20only%20more%20efficient%20but%20also%20more%20robust%20to%20varying%0Ahyperparameter%20settings.%20We%20demonstrate%20the%20effectiveness%20of%20GeoLoRA%20on%20several%0Astate-of-the-art%20benchmarks%2C%20showing%20that%20it%20outperforms%20existing%20methods%20in%0Aboth%20accuracy%20and%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoLoRA%253A%2520Geometric%2520integration%2520for%2520parameter%2520efficient%2520fine-tuning%26entry.906535625%3DSteffen%2520Schotth%25C3%25B6fer%2520and%2520Emanuele%2520Zangrando%2520and%2520Gianluca%2520Ceruti%2520and%2520Francesco%2520Tudisco%2520and%2520Jonas%2520Kusch%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520become%2520a%2520widely%2520used%2520method%2520for%250Aparameter-efficient%2520fine-tuning%2520of%2520large-scale%252C%2520pre-trained%2520neural%2520networks.%250AHowever%252C%2520LoRA%2520and%2520its%2520extensions%2520face%2520several%2520challenges%252C%2520including%2520the%2520need%250Afor%2520rank%2520adaptivity%252C%2520robustness%252C%2520and%2520computational%2520efficiency%2520during%2520the%250Afine-tuning%2520process.%2520We%2520introduce%2520GeoLoRA%252C%2520a%2520novel%2520approach%2520that%2520addresses%250Athese%2520limitations%2520by%2520leveraging%2520dynamical%2520low-rank%2520approximation%2520theory.%250AGeoLoRA%2520requires%2520only%2520a%2520single%2520backpropagation%2520pass%2520over%2520the%2520small-rank%250Aadapters%252C%2520significantly%2520reducing%2520computational%2520cost%2520as%2520compared%2520to%2520similar%250Adynamical%2520low-rank%2520training%2520methods%2520and%2520making%2520it%2520faster%2520than%2520popular%2520baselines%250Asuch%2520as%2520AdaLoRA.%2520This%2520allows%2520GeoLoRA%2520to%2520efficiently%2520adapt%2520the%2520allocated%250Aparameter%2520budget%2520across%2520the%2520model%252C%2520achieving%2520smaller%2520low-rank%2520adapters%2520compared%250Ato%2520heuristic%2520methods%2520like%2520AdaLoRA%2520and%2520LoRA%252C%2520while%2520maintaining%2520critical%250Aconvergence%252C%2520descent%252C%2520and%2520error-bound%2520theoretical%2520guarantees.%2520The%2520resulting%250Amethod%2520is%2520not%2520only%2520more%2520efficient%2520but%2520also%2520more%2520robust%2520to%2520varying%250Ahyperparameter%2520settings.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520GeoLoRA%2520on%2520several%250Astate-of-the-art%2520benchmarks%252C%2520showing%2520that%2520it%2520outperforms%2520existing%2520methods%2520in%250Aboth%2520accuracy%2520and%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoLoRA%3A%20Geometric%20integration%20for%20parameter%20efficient%20fine-tuning&entry.906535625=Steffen%20Schotth%C3%B6fer%20and%20Emanuele%20Zangrando%20and%20Gianluca%20Ceruti%20and%20Francesco%20Tudisco%20and%20Jonas%20Kusch&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20become%20a%20widely%20used%20method%20for%0Aparameter-efficient%20fine-tuning%20of%20large-scale%2C%20pre-trained%20neural%20networks.%0AHowever%2C%20LoRA%20and%20its%20extensions%20face%20several%20challenges%2C%20including%20the%20need%0Afor%20rank%20adaptivity%2C%20robustness%2C%20and%20computational%20efficiency%20during%20the%0Afine-tuning%20process.%20We%20introduce%20GeoLoRA%2C%20a%20novel%20approach%20that%20addresses%0Athese%20limitations%20by%20leveraging%20dynamical%20low-rank%20approximation%20theory.%0AGeoLoRA%20requires%20only%20a%20single%20backpropagation%20pass%20over%20the%20small-rank%0Aadapters%2C%20significantly%20reducing%20computational%20cost%20as%20compared%20to%20similar%0Adynamical%20low-rank%20training%20methods%20and%20making%20it%20faster%20than%20popular%20baselines%0Asuch%20as%20AdaLoRA.%20This%20allows%20GeoLoRA%20to%20efficiently%20adapt%20the%20allocated%0Aparameter%20budget%20across%20the%20model%2C%20achieving%20smaller%20low-rank%20adapters%20compared%0Ato%20heuristic%20methods%20like%20AdaLoRA%20and%20LoRA%2C%20while%20maintaining%20critical%0Aconvergence%2C%20descent%2C%20and%20error-bound%20theoretical%20guarantees.%20The%20resulting%0Amethod%20is%20not%20only%20more%20efficient%20but%20also%20more%20robust%20to%20varying%0Ahyperparameter%20settings.%20We%20demonstrate%20the%20effectiveness%20of%20GeoLoRA%20on%20several%0Astate-of-the-art%20benchmarks%2C%20showing%20that%20it%20outperforms%20existing%20methods%20in%0Aboth%20accuracy%20and%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18720v1&entry.124074799=Read"},
{"title": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable\n  Telephone Call Summarization", "author": "David Thulke and Yingbo Gao and Rricha Jalota and Christian Dugast and Hermann Ney", "abstract": "  This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system.\n", "link": "http://arxiv.org/abs/2410.18624v1", "date": "2024-10-24", "relevancy": 2.3556, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4713}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20and%20Fine-Tuning%20of%20Small%20LLMs%20for%20Length-Controllable%0A%20%20Telephone%20Call%20Summarization&body=Title%3A%20Prompting%20and%20Fine-Tuning%20of%20Small%20LLMs%20for%20Length-Controllable%0A%20%20Telephone%20Call%20Summarization%0AAuthor%3A%20David%20Thulke%20and%20Yingbo%20Gao%20and%20Rricha%20Jalota%20and%20Christian%20Dugast%20and%20Hermann%20Ney%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20rapid%20development%20of%20a%20telephone%20call%20summarization%0Asystem%20utilizing%20large%20language%20models%20%28LLMs%29.%20Our%20approach%20involves%20initial%0Aexperiments%20with%20prompting%20existing%20LLMs%20to%20generate%20summaries%20of%20telephone%0Aconversations%2C%20followed%20by%20the%20creation%20of%20a%20tailored%20synthetic%20training%0Adataset%20utilizing%20stronger%20frontier%20models.%20We%20place%20special%20focus%20on%20the%0Adiversity%20of%20the%20generated%20data%20and%20on%20the%20ability%20to%20control%20the%20length%20of%20the%0Agenerated%20summaries%20to%20meet%20various%20use-case%20specific%20requirements.%20The%0Aeffectiveness%20of%20our%20method%20is%20evaluated%20using%20two%20state-of-the-art%0ALLM-as-a-judge-based%20evaluation%20techniques%20to%20ensure%20the%20quality%20and%20relevance%0Aof%20the%20summaries.%20Our%20results%20show%20that%20fine-tuned%20Llama-2-7B-based%0Asummarization%20model%20performs%20on-par%20with%20GPT-4%20in%20terms%20of%20factual%20accuracy%2C%0Acompleteness%20and%20conciseness.%20Our%20findings%20demonstrate%20the%20potential%20for%0Aquickly%20bootstrapping%20a%20practical%20and%20efficient%20call%20summarization%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520and%2520Fine-Tuning%2520of%2520Small%2520LLMs%2520for%2520Length-Controllable%250A%2520%2520Telephone%2520Call%2520Summarization%26entry.906535625%3DDavid%2520Thulke%2520and%2520Yingbo%2520Gao%2520and%2520Rricha%2520Jalota%2520and%2520Christian%2520Dugast%2520and%2520Hermann%2520Ney%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520rapid%2520development%2520of%2520a%2520telephone%2520call%2520summarization%250Asystem%2520utilizing%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Our%2520approach%2520involves%2520initial%250Aexperiments%2520with%2520prompting%2520existing%2520LLMs%2520to%2520generate%2520summaries%2520of%2520telephone%250Aconversations%252C%2520followed%2520by%2520the%2520creation%2520of%2520a%2520tailored%2520synthetic%2520training%250Adataset%2520utilizing%2520stronger%2520frontier%2520models.%2520We%2520place%2520special%2520focus%2520on%2520the%250Adiversity%2520of%2520the%2520generated%2520data%2520and%2520on%2520the%2520ability%2520to%2520control%2520the%2520length%2520of%2520the%250Agenerated%2520summaries%2520to%2520meet%2520various%2520use-case%2520specific%2520requirements.%2520The%250Aeffectiveness%2520of%2520our%2520method%2520is%2520evaluated%2520using%2520two%2520state-of-the-art%250ALLM-as-a-judge-based%2520evaluation%2520techniques%2520to%2520ensure%2520the%2520quality%2520and%2520relevance%250Aof%2520the%2520summaries.%2520Our%2520results%2520show%2520that%2520fine-tuned%2520Llama-2-7B-based%250Asummarization%2520model%2520performs%2520on-par%2520with%2520GPT-4%2520in%2520terms%2520of%2520factual%2520accuracy%252C%250Acompleteness%2520and%2520conciseness.%2520Our%2520findings%2520demonstrate%2520the%2520potential%2520for%250Aquickly%2520bootstrapping%2520a%2520practical%2520and%2520efficient%2520call%2520summarization%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20and%20Fine-Tuning%20of%20Small%20LLMs%20for%20Length-Controllable%0A%20%20Telephone%20Call%20Summarization&entry.906535625=David%20Thulke%20and%20Yingbo%20Gao%20and%20Rricha%20Jalota%20and%20Christian%20Dugast%20and%20Hermann%20Ney&entry.1292438233=%20%20This%20paper%20explores%20the%20rapid%20development%20of%20a%20telephone%20call%20summarization%0Asystem%20utilizing%20large%20language%20models%20%28LLMs%29.%20Our%20approach%20involves%20initial%0Aexperiments%20with%20prompting%20existing%20LLMs%20to%20generate%20summaries%20of%20telephone%0Aconversations%2C%20followed%20by%20the%20creation%20of%20a%20tailored%20synthetic%20training%0Adataset%20utilizing%20stronger%20frontier%20models.%20We%20place%20special%20focus%20on%20the%0Adiversity%20of%20the%20generated%20data%20and%20on%20the%20ability%20to%20control%20the%20length%20of%20the%0Agenerated%20summaries%20to%20meet%20various%20use-case%20specific%20requirements.%20The%0Aeffectiveness%20of%20our%20method%20is%20evaluated%20using%20two%20state-of-the-art%0ALLM-as-a-judge-based%20evaluation%20techniques%20to%20ensure%20the%20quality%20and%20relevance%0Aof%20the%20summaries.%20Our%20results%20show%20that%20fine-tuned%20Llama-2-7B-based%0Asummarization%20model%20performs%20on-par%20with%20GPT-4%20in%20terms%20of%20factual%20accuracy%2C%0Acompleteness%20and%20conciseness.%20Our%20findings%20demonstrate%20the%20potential%20for%0Aquickly%20bootstrapping%20a%20practical%20and%20efficient%20call%20summarization%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18624v1&entry.124074799=Read"},
{"title": "On high-dimensional modifications of the nearest neighbor classifier", "author": "Annesha Ghosh and Deep Ghoshal and Bilol Banerjee and Anil K. Ghosh", "abstract": "  Nearest neighbor classifier is arguably the most simple and popular\nnonparametric classifier available in the literature. However, due to the\nconcentration of pairwise distances and the violation of the neighborhood\nstructure, this classifier often suffers in high-dimension, low-sample size\n(HDLSS) situations, especially when the scale difference between the competing\nclasses dominates their location difference. Several attempts have been made in\nthe literature to take care of this problem. In this article, we discuss some\nof these existing methods and propose some new ones. We carry out some\ntheoretical investigations in this regard and analyze several simulated and\nbenchmark datasets to compare the empirical performances of proposed methods\nwith some of the existing ones.\n", "link": "http://arxiv.org/abs/2407.05145v3", "date": "2024-10-24", "relevancy": 2.3433, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4718}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4677}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20high-dimensional%20modifications%20of%20the%20nearest%20neighbor%20classifier&body=Title%3A%20On%20high-dimensional%20modifications%20of%20the%20nearest%20neighbor%20classifier%0AAuthor%3A%20Annesha%20Ghosh%20and%20Deep%20Ghoshal%20and%20Bilol%20Banerjee%20and%20Anil%20K.%20Ghosh%0AAbstract%3A%20%20%20Nearest%20neighbor%20classifier%20is%20arguably%20the%20most%20simple%20and%20popular%0Anonparametric%20classifier%20available%20in%20the%20literature.%20However%2C%20due%20to%20the%0Aconcentration%20of%20pairwise%20distances%20and%20the%20violation%20of%20the%20neighborhood%0Astructure%2C%20this%20classifier%20often%20suffers%20in%20high-dimension%2C%20low-sample%20size%0A%28HDLSS%29%20situations%2C%20especially%20when%20the%20scale%20difference%20between%20the%20competing%0Aclasses%20dominates%20their%20location%20difference.%20Several%20attempts%20have%20been%20made%20in%0Athe%20literature%20to%20take%20care%20of%20this%20problem.%20In%20this%20article%2C%20we%20discuss%20some%0Aof%20these%20existing%20methods%20and%20propose%20some%20new%20ones.%20We%20carry%20out%20some%0Atheoretical%20investigations%20in%20this%20regard%20and%20analyze%20several%20simulated%20and%0Abenchmark%20datasets%20to%20compare%20the%20empirical%20performances%20of%20proposed%20methods%0Awith%20some%20of%20the%20existing%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05145v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520high-dimensional%2520modifications%2520of%2520the%2520nearest%2520neighbor%2520classifier%26entry.906535625%3DAnnesha%2520Ghosh%2520and%2520Deep%2520Ghoshal%2520and%2520Bilol%2520Banerjee%2520and%2520Anil%2520K.%2520Ghosh%26entry.1292438233%3D%2520%2520Nearest%2520neighbor%2520classifier%2520is%2520arguably%2520the%2520most%2520simple%2520and%2520popular%250Anonparametric%2520classifier%2520available%2520in%2520the%2520literature.%2520However%252C%2520due%2520to%2520the%250Aconcentration%2520of%2520pairwise%2520distances%2520and%2520the%2520violation%2520of%2520the%2520neighborhood%250Astructure%252C%2520this%2520classifier%2520often%2520suffers%2520in%2520high-dimension%252C%2520low-sample%2520size%250A%2528HDLSS%2529%2520situations%252C%2520especially%2520when%2520the%2520scale%2520difference%2520between%2520the%2520competing%250Aclasses%2520dominates%2520their%2520location%2520difference.%2520Several%2520attempts%2520have%2520been%2520made%2520in%250Athe%2520literature%2520to%2520take%2520care%2520of%2520this%2520problem.%2520In%2520this%2520article%252C%2520we%2520discuss%2520some%250Aof%2520these%2520existing%2520methods%2520and%2520propose%2520some%2520new%2520ones.%2520We%2520carry%2520out%2520some%250Atheoretical%2520investigations%2520in%2520this%2520regard%2520and%2520analyze%2520several%2520simulated%2520and%250Abenchmark%2520datasets%2520to%2520compare%2520the%2520empirical%2520performances%2520of%2520proposed%2520methods%250Awith%2520some%2520of%2520the%2520existing%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05145v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20high-dimensional%20modifications%20of%20the%20nearest%20neighbor%20classifier&entry.906535625=Annesha%20Ghosh%20and%20Deep%20Ghoshal%20and%20Bilol%20Banerjee%20and%20Anil%20K.%20Ghosh&entry.1292438233=%20%20Nearest%20neighbor%20classifier%20is%20arguably%20the%20most%20simple%20and%20popular%0Anonparametric%20classifier%20available%20in%20the%20literature.%20However%2C%20due%20to%20the%0Aconcentration%20of%20pairwise%20distances%20and%20the%20violation%20of%20the%20neighborhood%0Astructure%2C%20this%20classifier%20often%20suffers%20in%20high-dimension%2C%20low-sample%20size%0A%28HDLSS%29%20situations%2C%20especially%20when%20the%20scale%20difference%20between%20the%20competing%0Aclasses%20dominates%20their%20location%20difference.%20Several%20attempts%20have%20been%20made%20in%0Athe%20literature%20to%20take%20care%20of%20this%20problem.%20In%20this%20article%2C%20we%20discuss%20some%0Aof%20these%20existing%20methods%20and%20propose%20some%20new%20ones.%20We%20carry%20out%20some%0Atheoretical%20investigations%20in%20this%20regard%20and%20analyze%20several%20simulated%20and%0Abenchmark%20datasets%20to%20compare%20the%20empirical%20performances%20of%20proposed%20methods%0Awith%20some%20of%20the%20existing%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05145v3&entry.124074799=Read"},
{"title": "VoxelKeypointFusion: Generalizable Multi-View Multi-Person Pose\n  Estimation", "author": "Daniel Bermuth and Alexander Poeppel and Wolfgang Reif", "abstract": "  In the rapidly evolving field of computer vision, the task of accurately\nestimating the poses of multiple individuals from various viewpoints presents a\nformidable challenge, especially if the estimations should be reliable as well.\nThis work presents an extensive evaluation of the generalization capabilities\nof multi-view multi-person pose estimators to unseen datasets and presents a\nnew algorithm with strong performance in this task. It also studies the\nimprovements by additionally using depth information. Since the new approach\ncan not only generalize well to unseen datasets, but also to different\nkeypoints, the first multi-view multi-person whole-body estimator is presented.\nTo support further research on those topics, all of the work is publicly\naccessible.\n", "link": "http://arxiv.org/abs/2410.18723v1", "date": "2024-10-24", "relevancy": 2.3087, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5908}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5892}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoxelKeypointFusion%3A%20Generalizable%20Multi-View%20Multi-Person%20Pose%0A%20%20Estimation&body=Title%3A%20VoxelKeypointFusion%3A%20Generalizable%20Multi-View%20Multi-Person%20Pose%0A%20%20Estimation%0AAuthor%3A%20Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20computer%20vision%2C%20the%20task%20of%20accurately%0Aestimating%20the%20poses%20of%20multiple%20individuals%20from%20various%20viewpoints%20presents%20a%0Aformidable%20challenge%2C%20especially%20if%20the%20estimations%20should%20be%20reliable%20as%20well.%0AThis%20work%20presents%20an%20extensive%20evaluation%20of%20the%20generalization%20capabilities%0Aof%20multi-view%20multi-person%20pose%20estimators%20to%20unseen%20datasets%20and%20presents%20a%0Anew%20algorithm%20with%20strong%20performance%20in%20this%20task.%20It%20also%20studies%20the%0Aimprovements%20by%20additionally%20using%20depth%20information.%20Since%20the%20new%20approach%0Acan%20not%20only%20generalize%20well%20to%20unseen%20datasets%2C%20but%20also%20to%20different%0Akeypoints%2C%20the%20first%20multi-view%20multi-person%20whole-body%20estimator%20is%20presented.%0ATo%20support%20further%20research%20on%20those%20topics%2C%20all%20of%20the%20work%20is%20publicly%0Aaccessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxelKeypointFusion%253A%2520Generalizable%2520Multi-View%2520Multi-Person%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DDaniel%2520Bermuth%2520and%2520Alexander%2520Poeppel%2520and%2520Wolfgang%2520Reif%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520computer%2520vision%252C%2520the%2520task%2520of%2520accurately%250Aestimating%2520the%2520poses%2520of%2520multiple%2520individuals%2520from%2520various%2520viewpoints%2520presents%2520a%250Aformidable%2520challenge%252C%2520especially%2520if%2520the%2520estimations%2520should%2520be%2520reliable%2520as%2520well.%250AThis%2520work%2520presents%2520an%2520extensive%2520evaluation%2520of%2520the%2520generalization%2520capabilities%250Aof%2520multi-view%2520multi-person%2520pose%2520estimators%2520to%2520unseen%2520datasets%2520and%2520presents%2520a%250Anew%2520algorithm%2520with%2520strong%2520performance%2520in%2520this%2520task.%2520It%2520also%2520studies%2520the%250Aimprovements%2520by%2520additionally%2520using%2520depth%2520information.%2520Since%2520the%2520new%2520approach%250Acan%2520not%2520only%2520generalize%2520well%2520to%2520unseen%2520datasets%252C%2520but%2520also%2520to%2520different%250Akeypoints%252C%2520the%2520first%2520multi-view%2520multi-person%2520whole-body%2520estimator%2520is%2520presented.%250ATo%2520support%2520further%2520research%2520on%2520those%2520topics%252C%2520all%2520of%2520the%2520work%2520is%2520publicly%250Aaccessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoxelKeypointFusion%3A%20Generalizable%20Multi-View%20Multi-Person%20Pose%0A%20%20Estimation&entry.906535625=Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20computer%20vision%2C%20the%20task%20of%20accurately%0Aestimating%20the%20poses%20of%20multiple%20individuals%20from%20various%20viewpoints%20presents%20a%0Aformidable%20challenge%2C%20especially%20if%20the%20estimations%20should%20be%20reliable%20as%20well.%0AThis%20work%20presents%20an%20extensive%20evaluation%20of%20the%20generalization%20capabilities%0Aof%20multi-view%20multi-person%20pose%20estimators%20to%20unseen%20datasets%20and%20presents%20a%0Anew%20algorithm%20with%20strong%20performance%20in%20this%20task.%20It%20also%20studies%20the%0Aimprovements%20by%20additionally%20using%20depth%20information.%20Since%20the%20new%20approach%0Acan%20not%20only%20generalize%20well%20to%20unseen%20datasets%2C%20but%20also%20to%20different%0Akeypoints%2C%20the%20first%20multi-view%20multi-person%20whole-body%20estimator%20is%20presented.%0ATo%20support%20further%20research%20on%20those%20topics%2C%20all%20of%20the%20work%20is%20publicly%0Aaccessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18723v1&entry.124074799=Read"},
{"title": "End-to-end Training for Recommendation with Language-based User Profiles", "author": "Zhaolin Gao and Joyce Zhou and Yijia Dai and Thorsten Joachims", "abstract": "  Many online platforms maintain user profiles for personalization.\nUnfortunately, these profiles are typically not interpretable or easily\nmodifiable by the user. To remedy this shortcoming, we explore natural\nlanguage-based user profiles, as they promise enhanced transparency and\nscrutability of recommender systems. While existing work has shown that\nlanguage-based profiles from standard LLMs can be effective, such generalist\nLLMs are unlikely to be optimal for this task. In this paper, we introduce\nLangPTune, the first end-to-end learning method for training LLMs to produce\nlanguage-based user profiles that optimize recommendation effectiveness.\nThrough comprehensive evaluations of LangPTune across various training\nconfigurations and benchmarks, we demonstrate that our approach significantly\noutperforms existing profile-based methods. In addition, it approaches\nperformance levels comparable to state-of-the-art, less transparent recommender\nsystems, providing a robust and interpretable alternative to conventional\nsystems. Finally, we validate the relative interpretability of these\nlanguage-based user profiles through user studies involving crowdworkers and\nGPT-4-based evaluations. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune.\n", "link": "http://arxiv.org/abs/2410.18870v1", "date": "2024-10-24", "relevancy": 2.3034, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-end%20Training%20for%20Recommendation%20with%20Language-based%20User%20Profiles&body=Title%3A%20End-to-end%20Training%20for%20Recommendation%20with%20Language-based%20User%20Profiles%0AAuthor%3A%20Zhaolin%20Gao%20and%20Joyce%20Zhou%20and%20Yijia%20Dai%20and%20Thorsten%20Joachims%0AAbstract%3A%20%20%20Many%20online%20platforms%20maintain%20user%20profiles%20for%20personalization.%0AUnfortunately%2C%20these%20profiles%20are%20typically%20not%20interpretable%20or%20easily%0Amodifiable%20by%20the%20user.%20To%20remedy%20this%20shortcoming%2C%20we%20explore%20natural%0Alanguage-based%20user%20profiles%2C%20as%20they%20promise%20enhanced%20transparency%20and%0Ascrutability%20of%20recommender%20systems.%20While%20existing%20work%20has%20shown%20that%0Alanguage-based%20profiles%20from%20standard%20LLMs%20can%20be%20effective%2C%20such%20generalist%0ALLMs%20are%20unlikely%20to%20be%20optimal%20for%20this%20task.%20In%20this%20paper%2C%20we%20introduce%0ALangPTune%2C%20the%20first%20end-to-end%20learning%20method%20for%20training%20LLMs%20to%20produce%0Alanguage-based%20user%20profiles%20that%20optimize%20recommendation%20effectiveness.%0AThrough%20comprehensive%20evaluations%20of%20LangPTune%20across%20various%20training%0Aconfigurations%20and%20benchmarks%2C%20we%20demonstrate%20that%20our%20approach%20significantly%0Aoutperforms%20existing%20profile-based%20methods.%20In%20addition%2C%20it%20approaches%0Aperformance%20levels%20comparable%20to%20state-of-the-art%2C%20less%20transparent%20recommender%0Asystems%2C%20providing%20a%20robust%20and%20interpretable%20alternative%20to%20conventional%0Asystems.%20Finally%2C%20we%20validate%20the%20relative%20interpretability%20of%20these%0Alanguage-based%20user%20profiles%20through%20user%20studies%20involving%20crowdworkers%20and%0AGPT-4-based%20evaluations.%20Implementation%20of%20LangPTune%20can%20be%20found%20at%0Ahttps%3A//github.com/ZhaolinGao/LangPTune.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-end%2520Training%2520for%2520Recommendation%2520with%2520Language-based%2520User%2520Profiles%26entry.906535625%3DZhaolin%2520Gao%2520and%2520Joyce%2520Zhou%2520and%2520Yijia%2520Dai%2520and%2520Thorsten%2520Joachims%26entry.1292438233%3D%2520%2520Many%2520online%2520platforms%2520maintain%2520user%2520profiles%2520for%2520personalization.%250AUnfortunately%252C%2520these%2520profiles%2520are%2520typically%2520not%2520interpretable%2520or%2520easily%250Amodifiable%2520by%2520the%2520user.%2520To%2520remedy%2520this%2520shortcoming%252C%2520we%2520explore%2520natural%250Alanguage-based%2520user%2520profiles%252C%2520as%2520they%2520promise%2520enhanced%2520transparency%2520and%250Ascrutability%2520of%2520recommender%2520systems.%2520While%2520existing%2520work%2520has%2520shown%2520that%250Alanguage-based%2520profiles%2520from%2520standard%2520LLMs%2520can%2520be%2520effective%252C%2520such%2520generalist%250ALLMs%2520are%2520unlikely%2520to%2520be%2520optimal%2520for%2520this%2520task.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALangPTune%252C%2520the%2520first%2520end-to-end%2520learning%2520method%2520for%2520training%2520LLMs%2520to%2520produce%250Alanguage-based%2520user%2520profiles%2520that%2520optimize%2520recommendation%2520effectiveness.%250AThrough%2520comprehensive%2520evaluations%2520of%2520LangPTune%2520across%2520various%2520training%250Aconfigurations%2520and%2520benchmarks%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520significantly%250Aoutperforms%2520existing%2520profile-based%2520methods.%2520In%2520addition%252C%2520it%2520approaches%250Aperformance%2520levels%2520comparable%2520to%2520state-of-the-art%252C%2520less%2520transparent%2520recommender%250Asystems%252C%2520providing%2520a%2520robust%2520and%2520interpretable%2520alternative%2520to%2520conventional%250Asystems.%2520Finally%252C%2520we%2520validate%2520the%2520relative%2520interpretability%2520of%2520these%250Alanguage-based%2520user%2520profiles%2520through%2520user%2520studies%2520involving%2520crowdworkers%2520and%250AGPT-4-based%2520evaluations.%2520Implementation%2520of%2520LangPTune%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/ZhaolinGao/LangPTune.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Training%20for%20Recommendation%20with%20Language-based%20User%20Profiles&entry.906535625=Zhaolin%20Gao%20and%20Joyce%20Zhou%20and%20Yijia%20Dai%20and%20Thorsten%20Joachims&entry.1292438233=%20%20Many%20online%20platforms%20maintain%20user%20profiles%20for%20personalization.%0AUnfortunately%2C%20these%20profiles%20are%20typically%20not%20interpretable%20or%20easily%0Amodifiable%20by%20the%20user.%20To%20remedy%20this%20shortcoming%2C%20we%20explore%20natural%0Alanguage-based%20user%20profiles%2C%20as%20they%20promise%20enhanced%20transparency%20and%0Ascrutability%20of%20recommender%20systems.%20While%20existing%20work%20has%20shown%20that%0Alanguage-based%20profiles%20from%20standard%20LLMs%20can%20be%20effective%2C%20such%20generalist%0ALLMs%20are%20unlikely%20to%20be%20optimal%20for%20this%20task.%20In%20this%20paper%2C%20we%20introduce%0ALangPTune%2C%20the%20first%20end-to-end%20learning%20method%20for%20training%20LLMs%20to%20produce%0Alanguage-based%20user%20profiles%20that%20optimize%20recommendation%20effectiveness.%0AThrough%20comprehensive%20evaluations%20of%20LangPTune%20across%20various%20training%0Aconfigurations%20and%20benchmarks%2C%20we%20demonstrate%20that%20our%20approach%20significantly%0Aoutperforms%20existing%20profile-based%20methods.%20In%20addition%2C%20it%20approaches%0Aperformance%20levels%20comparable%20to%20state-of-the-art%2C%20less%20transparent%20recommender%0Asystems%2C%20providing%20a%20robust%20and%20interpretable%20alternative%20to%20conventional%0Asystems.%20Finally%2C%20we%20validate%20the%20relative%20interpretability%20of%20these%0Alanguage-based%20user%20profiles%20through%20user%20studies%20involving%20crowdworkers%20and%0AGPT-4-based%20evaluations.%20Implementation%20of%20LangPTune%20can%20be%20found%20at%0Ahttps%3A//github.com/ZhaolinGao/LangPTune.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18870v1&entry.124074799=Read"},
{"title": "Single-Shot Phase Diversity Wavefront Sensing in Deep Turbulence via\n  Metasurface Optics", "author": "Arturo Martin Jimenez and Marc Baltes and Jackson Cornelius and Neset Akozbek and Zachary Coppens", "abstract": "  Free-space optical communication (FSOC) systems offer high-bandwidth and\nsecure communication with minimal capital costs. Adaptive optics (AO) are\ntypically added to these systems to decrease atmospheric channel losses;\nhowever, the performance of traditional AO wavefront sensors degrades in\nlong-range, deep turbulence conditions. Alternative wavefront sensors using\nphase diversity can successfully reconstruct wavefronts in deep turbulence, but\ncurrent implementations require bulky setups with high latency. In this work,\nwe employ a nanostructured birefringent metasurface optic that enables\nlow-latency phase diversity wavefront sensing in a compact form factor. We\nprove the effectiveness of this approach in mid-to-high turbulence (Rytov\nnumbers from 0.2 to 0.6) through simulation and experimental demonstration. In\nboth cases an average 16-fold increase in signal from the corrected beam is\nobtained. Our approach opens a pathway for compact, robust wavefront sensing\nthat enhances range and accuracy of FSOC systems.\n", "link": "http://arxiv.org/abs/2410.18789v1", "date": "2024-10-24", "relevancy": 2.28, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4593}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4593}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-Shot%20Phase%20Diversity%20Wavefront%20Sensing%20in%20Deep%20Turbulence%20via%0A%20%20Metasurface%20Optics&body=Title%3A%20Single-Shot%20Phase%20Diversity%20Wavefront%20Sensing%20in%20Deep%20Turbulence%20via%0A%20%20Metasurface%20Optics%0AAuthor%3A%20Arturo%20Martin%20Jimenez%20and%20Marc%20Baltes%20and%20Jackson%20Cornelius%20and%20Neset%20Akozbek%20and%20Zachary%20Coppens%0AAbstract%3A%20%20%20Free-space%20optical%20communication%20%28FSOC%29%20systems%20offer%20high-bandwidth%20and%0Asecure%20communication%20with%20minimal%20capital%20costs.%20Adaptive%20optics%20%28AO%29%20are%0Atypically%20added%20to%20these%20systems%20to%20decrease%20atmospheric%20channel%20losses%3B%0Ahowever%2C%20the%20performance%20of%20traditional%20AO%20wavefront%20sensors%20degrades%20in%0Along-range%2C%20deep%20turbulence%20conditions.%20Alternative%20wavefront%20sensors%20using%0Aphase%20diversity%20can%20successfully%20reconstruct%20wavefronts%20in%20deep%20turbulence%2C%20but%0Acurrent%20implementations%20require%20bulky%20setups%20with%20high%20latency.%20In%20this%20work%2C%0Awe%20employ%20a%20nanostructured%20birefringent%20metasurface%20optic%20that%20enables%0Alow-latency%20phase%20diversity%20wavefront%20sensing%20in%20a%20compact%20form%20factor.%20We%0Aprove%20the%20effectiveness%20of%20this%20approach%20in%20mid-to-high%20turbulence%20%28Rytov%0Anumbers%20from%200.2%20to%200.6%29%20through%20simulation%20and%20experimental%20demonstration.%20In%0Aboth%20cases%20an%20average%2016-fold%20increase%20in%20signal%20from%20the%20corrected%20beam%20is%0Aobtained.%20Our%20approach%20opens%20a%20pathway%20for%20compact%2C%20robust%20wavefront%20sensing%0Athat%20enhances%20range%20and%20accuracy%20of%20FSOC%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-Shot%2520Phase%2520Diversity%2520Wavefront%2520Sensing%2520in%2520Deep%2520Turbulence%2520via%250A%2520%2520Metasurface%2520Optics%26entry.906535625%3DArturo%2520Martin%2520Jimenez%2520and%2520Marc%2520Baltes%2520and%2520Jackson%2520Cornelius%2520and%2520Neset%2520Akozbek%2520and%2520Zachary%2520Coppens%26entry.1292438233%3D%2520%2520Free-space%2520optical%2520communication%2520%2528FSOC%2529%2520systems%2520offer%2520high-bandwidth%2520and%250Asecure%2520communication%2520with%2520minimal%2520capital%2520costs.%2520Adaptive%2520optics%2520%2528AO%2529%2520are%250Atypically%2520added%2520to%2520these%2520systems%2520to%2520decrease%2520atmospheric%2520channel%2520losses%253B%250Ahowever%252C%2520the%2520performance%2520of%2520traditional%2520AO%2520wavefront%2520sensors%2520degrades%2520in%250Along-range%252C%2520deep%2520turbulence%2520conditions.%2520Alternative%2520wavefront%2520sensors%2520using%250Aphase%2520diversity%2520can%2520successfully%2520reconstruct%2520wavefronts%2520in%2520deep%2520turbulence%252C%2520but%250Acurrent%2520implementations%2520require%2520bulky%2520setups%2520with%2520high%2520latency.%2520In%2520this%2520work%252C%250Awe%2520employ%2520a%2520nanostructured%2520birefringent%2520metasurface%2520optic%2520that%2520enables%250Alow-latency%2520phase%2520diversity%2520wavefront%2520sensing%2520in%2520a%2520compact%2520form%2520factor.%2520We%250Aprove%2520the%2520effectiveness%2520of%2520this%2520approach%2520in%2520mid-to-high%2520turbulence%2520%2528Rytov%250Anumbers%2520from%25200.2%2520to%25200.6%2529%2520through%2520simulation%2520and%2520experimental%2520demonstration.%2520In%250Aboth%2520cases%2520an%2520average%252016-fold%2520increase%2520in%2520signal%2520from%2520the%2520corrected%2520beam%2520is%250Aobtained.%2520Our%2520approach%2520opens%2520a%2520pathway%2520for%2520compact%252C%2520robust%2520wavefront%2520sensing%250Athat%2520enhances%2520range%2520and%2520accuracy%2520of%2520FSOC%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-Shot%20Phase%20Diversity%20Wavefront%20Sensing%20in%20Deep%20Turbulence%20via%0A%20%20Metasurface%20Optics&entry.906535625=Arturo%20Martin%20Jimenez%20and%20Marc%20Baltes%20and%20Jackson%20Cornelius%20and%20Neset%20Akozbek%20and%20Zachary%20Coppens&entry.1292438233=%20%20Free-space%20optical%20communication%20%28FSOC%29%20systems%20offer%20high-bandwidth%20and%0Asecure%20communication%20with%20minimal%20capital%20costs.%20Adaptive%20optics%20%28AO%29%20are%0Atypically%20added%20to%20these%20systems%20to%20decrease%20atmospheric%20channel%20losses%3B%0Ahowever%2C%20the%20performance%20of%20traditional%20AO%20wavefront%20sensors%20degrades%20in%0Along-range%2C%20deep%20turbulence%20conditions.%20Alternative%20wavefront%20sensors%20using%0Aphase%20diversity%20can%20successfully%20reconstruct%20wavefronts%20in%20deep%20turbulence%2C%20but%0Acurrent%20implementations%20require%20bulky%20setups%20with%20high%20latency.%20In%20this%20work%2C%0Awe%20employ%20a%20nanostructured%20birefringent%20metasurface%20optic%20that%20enables%0Alow-latency%20phase%20diversity%20wavefront%20sensing%20in%20a%20compact%20form%20factor.%20We%0Aprove%20the%20effectiveness%20of%20this%20approach%20in%20mid-to-high%20turbulence%20%28Rytov%0Anumbers%20from%200.2%20to%200.6%29%20through%20simulation%20and%20experimental%20demonstration.%20In%0Aboth%20cases%20an%20average%2016-fold%20increase%20in%20signal%20from%20the%20corrected%20beam%20is%0Aobtained.%20Our%20approach%20opens%20a%20pathway%20for%20compact%2C%20robust%20wavefront%20sensing%0Athat%20enhances%20range%20and%20accuracy%20of%20FSOC%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18789v1&entry.124074799=Read"},
{"title": "Exploring the Universe with SNAD: Anomaly Detection in Astronomy", "author": "Alina A. Volnova and Patrick D. Aleo and Anastasia Lavrukhina and Etienne Russeil and Timofey Semenikhin and Emmanuel Gangler and Emille E. O. Ishida and Matwey V. Kornilov and Vladimir Korolev and Konstantin Malanchev and Maria V. Pruzhinskaya and Sreevarsha Sreejith", "abstract": "  SNAD is an international project with a primary focus on detecting\nastronomical anomalies within large-scale surveys, using active learning and\nother machine learning algorithms. The work carried out by SNAD not only\ncontributes to the discovery and classification of various astronomical\nphenomena but also enhances our understanding and implementation of machine\nlearning techniques within the field of astrophysics. This paper provides a\nreview of the SNAD project and summarizes the advancements and achievements\nmade by the team over several years.\n", "link": "http://arxiv.org/abs/2410.18875v1", "date": "2024-10-24", "relevancy": 2.2755, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4787}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Universe%20with%20SNAD%3A%20Anomaly%20Detection%20in%20Astronomy&body=Title%3A%20Exploring%20the%20Universe%20with%20SNAD%3A%20Anomaly%20Detection%20in%20Astronomy%0AAuthor%3A%20Alina%20A.%20Volnova%20and%20Patrick%20D.%20Aleo%20and%20Anastasia%20Lavrukhina%20and%20Etienne%20Russeil%20and%20Timofey%20Semenikhin%20and%20Emmanuel%20Gangler%20and%20Emille%20E.%20O.%20Ishida%20and%20Matwey%20V.%20Kornilov%20and%20Vladimir%20Korolev%20and%20Konstantin%20Malanchev%20and%20Maria%20V.%20Pruzhinskaya%20and%20Sreevarsha%20Sreejith%0AAbstract%3A%20%20%20SNAD%20is%20an%20international%20project%20with%20a%20primary%20focus%20on%20detecting%0Aastronomical%20anomalies%20within%20large-scale%20surveys%2C%20using%20active%20learning%20and%0Aother%20machine%20learning%20algorithms.%20The%20work%20carried%20out%20by%20SNAD%20not%20only%0Acontributes%20to%20the%20discovery%20and%20classification%20of%20various%20astronomical%0Aphenomena%20but%20also%20enhances%20our%20understanding%20and%20implementation%20of%20machine%0Alearning%20techniques%20within%20the%20field%20of%20astrophysics.%20This%20paper%20provides%20a%0Areview%20of%20the%20SNAD%20project%20and%20summarizes%20the%20advancements%20and%20achievements%0Amade%20by%20the%20team%20over%20several%20years.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Universe%2520with%2520SNAD%253A%2520Anomaly%2520Detection%2520in%2520Astronomy%26entry.906535625%3DAlina%2520A.%2520Volnova%2520and%2520Patrick%2520D.%2520Aleo%2520and%2520Anastasia%2520Lavrukhina%2520and%2520Etienne%2520Russeil%2520and%2520Timofey%2520Semenikhin%2520and%2520Emmanuel%2520Gangler%2520and%2520Emille%2520E.%2520O.%2520Ishida%2520and%2520Matwey%2520V.%2520Kornilov%2520and%2520Vladimir%2520Korolev%2520and%2520Konstantin%2520Malanchev%2520and%2520Maria%2520V.%2520Pruzhinskaya%2520and%2520Sreevarsha%2520Sreejith%26entry.1292438233%3D%2520%2520SNAD%2520is%2520an%2520international%2520project%2520with%2520a%2520primary%2520focus%2520on%2520detecting%250Aastronomical%2520anomalies%2520within%2520large-scale%2520surveys%252C%2520using%2520active%2520learning%2520and%250Aother%2520machine%2520learning%2520algorithms.%2520The%2520work%2520carried%2520out%2520by%2520SNAD%2520not%2520only%250Acontributes%2520to%2520the%2520discovery%2520and%2520classification%2520of%2520various%2520astronomical%250Aphenomena%2520but%2520also%2520enhances%2520our%2520understanding%2520and%2520implementation%2520of%2520machine%250Alearning%2520techniques%2520within%2520the%2520field%2520of%2520astrophysics.%2520This%2520paper%2520provides%2520a%250Areview%2520of%2520the%2520SNAD%2520project%2520and%2520summarizes%2520the%2520advancements%2520and%2520achievements%250Amade%2520by%2520the%2520team%2520over%2520several%2520years.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Universe%20with%20SNAD%3A%20Anomaly%20Detection%20in%20Astronomy&entry.906535625=Alina%20A.%20Volnova%20and%20Patrick%20D.%20Aleo%20and%20Anastasia%20Lavrukhina%20and%20Etienne%20Russeil%20and%20Timofey%20Semenikhin%20and%20Emmanuel%20Gangler%20and%20Emille%20E.%20O.%20Ishida%20and%20Matwey%20V.%20Kornilov%20and%20Vladimir%20Korolev%20and%20Konstantin%20Malanchev%20and%20Maria%20V.%20Pruzhinskaya%20and%20Sreevarsha%20Sreejith&entry.1292438233=%20%20SNAD%20is%20an%20international%20project%20with%20a%20primary%20focus%20on%20detecting%0Aastronomical%20anomalies%20within%20large-scale%20surveys%2C%20using%20active%20learning%20and%0Aother%20machine%20learning%20algorithms.%20The%20work%20carried%20out%20by%20SNAD%20not%20only%0Acontributes%20to%20the%20discovery%20and%20classification%20of%20various%20astronomical%0Aphenomena%20but%20also%20enhances%20our%20understanding%20and%20implementation%20of%20machine%0Alearning%20techniques%20within%20the%20field%20of%20astrophysics.%20This%20paper%20provides%20a%0Areview%20of%20the%20SNAD%20project%20and%20summarizes%20the%20advancements%20and%20achievements%0Amade%20by%20the%20team%20over%20several%20years.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18875v1&entry.124074799=Read"},
{"title": "Learning Global Object-Centric Representations via Disentangled Slot\n  Attention", "author": "Tonglin Chen and Yinxuan Huang and Zhimeng Shen and Jinghao Huang and Bin Li and Xiangyang Xue", "abstract": "  Humans can discern scene-independent features of objects across various\nenvironments, allowing them to swiftly identify objects amidst changing factors\nsuch as lighting, perspective, size, and position and imagine the complete\nimages of the same object in diverse settings. Existing object-centric learning\nmethods only extract scene-dependent object-centric representations, lacking\nthe ability to identify the same object across scenes as humans. Moreover, some\nexisting methods discard the individual object generation capabilities to\nhandle complex scenes. This paper introduces a novel object-centric learning\nmethod to empower AI systems with human-like capabilities to identify objects\nacross scenes and generate diverse scenes containing specific objects by\nlearning a set of global object-centric representations. To learn the global\nobject-centric representations that encapsulate globally invariant attributes\nof objects (i.e., the complete appearance and shape), this paper designs a\nDisentangled Slot Attention module to convert the scene features into\nscene-dependent attributes (such as scale, position and orientation) and\nscene-independent representations (i.e., appearance and shape). Experimental\nresults substantiate the efficacy of the proposed method, demonstrating\nremarkable proficiency in global object-centric representation learning, object\nidentification, scene generation with specific objects and scene decomposition.\n", "link": "http://arxiv.org/abs/2410.18809v1", "date": "2024-10-24", "relevancy": 2.2645, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.567}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Global%20Object-Centric%20Representations%20via%20Disentangled%20Slot%0A%20%20Attention&body=Title%3A%20Learning%20Global%20Object-Centric%20Representations%20via%20Disentangled%20Slot%0A%20%20Attention%0AAuthor%3A%20Tonglin%20Chen%20and%20Yinxuan%20Huang%20and%20Zhimeng%20Shen%20and%20Jinghao%20Huang%20and%20Bin%20Li%20and%20Xiangyang%20Xue%0AAbstract%3A%20%20%20Humans%20can%20discern%20scene-independent%20features%20of%20objects%20across%20various%0Aenvironments%2C%20allowing%20them%20to%20swiftly%20identify%20objects%20amidst%20changing%20factors%0Asuch%20as%20lighting%2C%20perspective%2C%20size%2C%20and%20position%20and%20imagine%20the%20complete%0Aimages%20of%20the%20same%20object%20in%20diverse%20settings.%20Existing%20object-centric%20learning%0Amethods%20only%20extract%20scene-dependent%20object-centric%20representations%2C%20lacking%0Athe%20ability%20to%20identify%20the%20same%20object%20across%20scenes%20as%20humans.%20Moreover%2C%20some%0Aexisting%20methods%20discard%20the%20individual%20object%20generation%20capabilities%20to%0Ahandle%20complex%20scenes.%20This%20paper%20introduces%20a%20novel%20object-centric%20learning%0Amethod%20to%20empower%20AI%20systems%20with%20human-like%20capabilities%20to%20identify%20objects%0Aacross%20scenes%20and%20generate%20diverse%20scenes%20containing%20specific%20objects%20by%0Alearning%20a%20set%20of%20global%20object-centric%20representations.%20To%20learn%20the%20global%0Aobject-centric%20representations%20that%20encapsulate%20globally%20invariant%20attributes%0Aof%20objects%20%28i.e.%2C%20the%20complete%20appearance%20and%20shape%29%2C%20this%20paper%20designs%20a%0ADisentangled%20Slot%20Attention%20module%20to%20convert%20the%20scene%20features%20into%0Ascene-dependent%20attributes%20%28such%20as%20scale%2C%20position%20and%20orientation%29%20and%0Ascene-independent%20representations%20%28i.e.%2C%20appearance%20and%20shape%29.%20Experimental%0Aresults%20substantiate%20the%20efficacy%20of%20the%20proposed%20method%2C%20demonstrating%0Aremarkable%20proficiency%20in%20global%20object-centric%20representation%20learning%2C%20object%0Aidentification%2C%20scene%20generation%20with%20specific%20objects%20and%20scene%20decomposition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Global%2520Object-Centric%2520Representations%2520via%2520Disentangled%2520Slot%250A%2520%2520Attention%26entry.906535625%3DTonglin%2520Chen%2520and%2520Yinxuan%2520Huang%2520and%2520Zhimeng%2520Shen%2520and%2520Jinghao%2520Huang%2520and%2520Bin%2520Li%2520and%2520Xiangyang%2520Xue%26entry.1292438233%3D%2520%2520Humans%2520can%2520discern%2520scene-independent%2520features%2520of%2520objects%2520across%2520various%250Aenvironments%252C%2520allowing%2520them%2520to%2520swiftly%2520identify%2520objects%2520amidst%2520changing%2520factors%250Asuch%2520as%2520lighting%252C%2520perspective%252C%2520size%252C%2520and%2520position%2520and%2520imagine%2520the%2520complete%250Aimages%2520of%2520the%2520same%2520object%2520in%2520diverse%2520settings.%2520Existing%2520object-centric%2520learning%250Amethods%2520only%2520extract%2520scene-dependent%2520object-centric%2520representations%252C%2520lacking%250Athe%2520ability%2520to%2520identify%2520the%2520same%2520object%2520across%2520scenes%2520as%2520humans.%2520Moreover%252C%2520some%250Aexisting%2520methods%2520discard%2520the%2520individual%2520object%2520generation%2520capabilities%2520to%250Ahandle%2520complex%2520scenes.%2520This%2520paper%2520introduces%2520a%2520novel%2520object-centric%2520learning%250Amethod%2520to%2520empower%2520AI%2520systems%2520with%2520human-like%2520capabilities%2520to%2520identify%2520objects%250Aacross%2520scenes%2520and%2520generate%2520diverse%2520scenes%2520containing%2520specific%2520objects%2520by%250Alearning%2520a%2520set%2520of%2520global%2520object-centric%2520representations.%2520To%2520learn%2520the%2520global%250Aobject-centric%2520representations%2520that%2520encapsulate%2520globally%2520invariant%2520attributes%250Aof%2520objects%2520%2528i.e.%252C%2520the%2520complete%2520appearance%2520and%2520shape%2529%252C%2520this%2520paper%2520designs%2520a%250ADisentangled%2520Slot%2520Attention%2520module%2520to%2520convert%2520the%2520scene%2520features%2520into%250Ascene-dependent%2520attributes%2520%2528such%2520as%2520scale%252C%2520position%2520and%2520orientation%2529%2520and%250Ascene-independent%2520representations%2520%2528i.e.%252C%2520appearance%2520and%2520shape%2529.%2520Experimental%250Aresults%2520substantiate%2520the%2520efficacy%2520of%2520the%2520proposed%2520method%252C%2520demonstrating%250Aremarkable%2520proficiency%2520in%2520global%2520object-centric%2520representation%2520learning%252C%2520object%250Aidentification%252C%2520scene%2520generation%2520with%2520specific%2520objects%2520and%2520scene%2520decomposition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Global%20Object-Centric%20Representations%20via%20Disentangled%20Slot%0A%20%20Attention&entry.906535625=Tonglin%20Chen%20and%20Yinxuan%20Huang%20and%20Zhimeng%20Shen%20and%20Jinghao%20Huang%20and%20Bin%20Li%20and%20Xiangyang%20Xue&entry.1292438233=%20%20Humans%20can%20discern%20scene-independent%20features%20of%20objects%20across%20various%0Aenvironments%2C%20allowing%20them%20to%20swiftly%20identify%20objects%20amidst%20changing%20factors%0Asuch%20as%20lighting%2C%20perspective%2C%20size%2C%20and%20position%20and%20imagine%20the%20complete%0Aimages%20of%20the%20same%20object%20in%20diverse%20settings.%20Existing%20object-centric%20learning%0Amethods%20only%20extract%20scene-dependent%20object-centric%20representations%2C%20lacking%0Athe%20ability%20to%20identify%20the%20same%20object%20across%20scenes%20as%20humans.%20Moreover%2C%20some%0Aexisting%20methods%20discard%20the%20individual%20object%20generation%20capabilities%20to%0Ahandle%20complex%20scenes.%20This%20paper%20introduces%20a%20novel%20object-centric%20learning%0Amethod%20to%20empower%20AI%20systems%20with%20human-like%20capabilities%20to%20identify%20objects%0Aacross%20scenes%20and%20generate%20diverse%20scenes%20containing%20specific%20objects%20by%0Alearning%20a%20set%20of%20global%20object-centric%20representations.%20To%20learn%20the%20global%0Aobject-centric%20representations%20that%20encapsulate%20globally%20invariant%20attributes%0Aof%20objects%20%28i.e.%2C%20the%20complete%20appearance%20and%20shape%29%2C%20this%20paper%20designs%20a%0ADisentangled%20Slot%20Attention%20module%20to%20convert%20the%20scene%20features%20into%0Ascene-dependent%20attributes%20%28such%20as%20scale%2C%20position%20and%20orientation%29%20and%0Ascene-independent%20representations%20%28i.e.%2C%20appearance%20and%20shape%29.%20Experimental%0Aresults%20substantiate%20the%20efficacy%20of%20the%20proposed%20method%2C%20demonstrating%0Aremarkable%20proficiency%20in%20global%20object-centric%20representation%20learning%2C%20object%0Aidentification%2C%20scene%20generation%20with%20specific%20objects%20and%20scene%20decomposition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18809v1&entry.124074799=Read"},
{"title": "Probabilistic Language-Image Pre-Training", "author": "Sanghyuk Chun and Wonjae Kim and Song Park and Sangdoo Yun", "abstract": "  Vision-language models (VLMs) embed aligned image-text pairs into a joint\nspace but often rely on deterministic embeddings, assuming a one-to-one\ncorrespondence between images and texts. This oversimplifies real-world\nrelationships, which are inherently many-to-many, with multiple captions\ndescribing a single image and vice versa. We introduce Probabilistic\nLanguage-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained\non a billion-scale image-text dataset using only probabilistic objectives,\nachieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot\naccuracy with ViT-B/16). ProLIP efficiently estimates uncertainty by an\n\"uncertainty token\" without extra parameters. We also introduce a novel\ninclusion loss that enforces distributional inclusion relationships between\nimage-text pairs and between original and masked inputs. Experiments\ndemonstrate that, by leveraging uncertainty estimates, ProLIP benefits\ndownstream tasks and aligns with intuitive notions of uncertainty, e.g.,\nshorter texts being more uncertain and more general inputs including specific\nones. Utilizing text uncertainties, we further improve ImageNet accuracy from\n74.6% to 75.8% (under a few-shot setting), supporting the practical advantages\nof our probabilistic approach. The code is available at\nhttps://github.com/naver-ai/prolip\n", "link": "http://arxiv.org/abs/2410.18857v1", "date": "2024-10-24", "relevancy": 2.2637, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.571}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Language-Image%20Pre-Training&body=Title%3A%20Probabilistic%20Language-Image%20Pre-Training%0AAuthor%3A%20Sanghyuk%20Chun%20and%20Wonjae%20Kim%20and%20Song%20Park%20and%20Sangdoo%20Yun%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20embed%20aligned%20image-text%20pairs%20into%20a%20joint%0Aspace%20but%20often%20rely%20on%20deterministic%20embeddings%2C%20assuming%20a%20one-to-one%0Acorrespondence%20between%20images%20and%20texts.%20This%20oversimplifies%20real-world%0Arelationships%2C%20which%20are%20inherently%20many-to-many%2C%20with%20multiple%20captions%0Adescribing%20a%20single%20image%20and%20vice%20versa.%20We%20introduce%20Probabilistic%0ALanguage-Image%20Pre-training%20%28ProLIP%29%2C%20the%20first%20probabilistic%20VLM%20pre-trained%0Aon%20a%20billion-scale%20image-text%20dataset%20using%20only%20probabilistic%20objectives%2C%0Aachieving%20a%20strong%20zero-shot%20capability%20%28e.g.%2C%2074.6%25%20ImageNet%20zero-shot%0Aaccuracy%20with%20ViT-B/16%29.%20ProLIP%20efficiently%20estimates%20uncertainty%20by%20an%0A%22uncertainty%20token%22%20without%20extra%20parameters.%20We%20also%20introduce%20a%20novel%0Ainclusion%20loss%20that%20enforces%20distributional%20inclusion%20relationships%20between%0Aimage-text%20pairs%20and%20between%20original%20and%20masked%20inputs.%20Experiments%0Ademonstrate%20that%2C%20by%20leveraging%20uncertainty%20estimates%2C%20ProLIP%20benefits%0Adownstream%20tasks%20and%20aligns%20with%20intuitive%20notions%20of%20uncertainty%2C%20e.g.%2C%0Ashorter%20texts%20being%20more%20uncertain%20and%20more%20general%20inputs%20including%20specific%0Aones.%20Utilizing%20text%20uncertainties%2C%20we%20further%20improve%20ImageNet%20accuracy%20from%0A74.6%25%20to%2075.8%25%20%28under%20a%20few-shot%20setting%29%2C%20supporting%20the%20practical%20advantages%0Aof%20our%20probabilistic%20approach.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/naver-ai/prolip%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Language-Image%2520Pre-Training%26entry.906535625%3DSanghyuk%2520Chun%2520and%2520Wonjae%2520Kim%2520and%2520Song%2520Park%2520and%2520Sangdoo%2520Yun%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520embed%2520aligned%2520image-text%2520pairs%2520into%2520a%2520joint%250Aspace%2520but%2520often%2520rely%2520on%2520deterministic%2520embeddings%252C%2520assuming%2520a%2520one-to-one%250Acorrespondence%2520between%2520images%2520and%2520texts.%2520This%2520oversimplifies%2520real-world%250Arelationships%252C%2520which%2520are%2520inherently%2520many-to-many%252C%2520with%2520multiple%2520captions%250Adescribing%2520a%2520single%2520image%2520and%2520vice%2520versa.%2520We%2520introduce%2520Probabilistic%250ALanguage-Image%2520Pre-training%2520%2528ProLIP%2529%252C%2520the%2520first%2520probabilistic%2520VLM%2520pre-trained%250Aon%2520a%2520billion-scale%2520image-text%2520dataset%2520using%2520only%2520probabilistic%2520objectives%252C%250Aachieving%2520a%2520strong%2520zero-shot%2520capability%2520%2528e.g.%252C%252074.6%2525%2520ImageNet%2520zero-shot%250Aaccuracy%2520with%2520ViT-B/16%2529.%2520ProLIP%2520efficiently%2520estimates%2520uncertainty%2520by%2520an%250A%2522uncertainty%2520token%2522%2520without%2520extra%2520parameters.%2520We%2520also%2520introduce%2520a%2520novel%250Ainclusion%2520loss%2520that%2520enforces%2520distributional%2520inclusion%2520relationships%2520between%250Aimage-text%2520pairs%2520and%2520between%2520original%2520and%2520masked%2520inputs.%2520Experiments%250Ademonstrate%2520that%252C%2520by%2520leveraging%2520uncertainty%2520estimates%252C%2520ProLIP%2520benefits%250Adownstream%2520tasks%2520and%2520aligns%2520with%2520intuitive%2520notions%2520of%2520uncertainty%252C%2520e.g.%252C%250Ashorter%2520texts%2520being%2520more%2520uncertain%2520and%2520more%2520general%2520inputs%2520including%2520specific%250Aones.%2520Utilizing%2520text%2520uncertainties%252C%2520we%2520further%2520improve%2520ImageNet%2520accuracy%2520from%250A74.6%2525%2520to%252075.8%2525%2520%2528under%2520a%2520few-shot%2520setting%2529%252C%2520supporting%2520the%2520practical%2520advantages%250Aof%2520our%2520probabilistic%2520approach.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/naver-ai/prolip%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Language-Image%20Pre-Training&entry.906535625=Sanghyuk%20Chun%20and%20Wonjae%20Kim%20and%20Song%20Park%20and%20Sangdoo%20Yun&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20embed%20aligned%20image-text%20pairs%20into%20a%20joint%0Aspace%20but%20often%20rely%20on%20deterministic%20embeddings%2C%20assuming%20a%20one-to-one%0Acorrespondence%20between%20images%20and%20texts.%20This%20oversimplifies%20real-world%0Arelationships%2C%20which%20are%20inherently%20many-to-many%2C%20with%20multiple%20captions%0Adescribing%20a%20single%20image%20and%20vice%20versa.%20We%20introduce%20Probabilistic%0ALanguage-Image%20Pre-training%20%28ProLIP%29%2C%20the%20first%20probabilistic%20VLM%20pre-trained%0Aon%20a%20billion-scale%20image-text%20dataset%20using%20only%20probabilistic%20objectives%2C%0Aachieving%20a%20strong%20zero-shot%20capability%20%28e.g.%2C%2074.6%25%20ImageNet%20zero-shot%0Aaccuracy%20with%20ViT-B/16%29.%20ProLIP%20efficiently%20estimates%20uncertainty%20by%20an%0A%22uncertainty%20token%22%20without%20extra%20parameters.%20We%20also%20introduce%20a%20novel%0Ainclusion%20loss%20that%20enforces%20distributional%20inclusion%20relationships%20between%0Aimage-text%20pairs%20and%20between%20original%20and%20masked%20inputs.%20Experiments%0Ademonstrate%20that%2C%20by%20leveraging%20uncertainty%20estimates%2C%20ProLIP%20benefits%0Adownstream%20tasks%20and%20aligns%20with%20intuitive%20notions%20of%20uncertainty%2C%20e.g.%2C%0Ashorter%20texts%20being%20more%20uncertain%20and%20more%20general%20inputs%20including%20specific%0Aones.%20Utilizing%20text%20uncertainties%2C%20we%20further%20improve%20ImageNet%20accuracy%20from%0A74.6%25%20to%2075.8%25%20%28under%20a%20few-shot%20setting%29%2C%20supporting%20the%20practical%20advantages%0Aof%20our%20probabilistic%20approach.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/naver-ai/prolip%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18857v1&entry.124074799=Read"},
{"title": "Guiding Empowerment Model: Liberating Neurodiversity in Online Higher\n  Education", "author": "Hannah Beaux and Pegah Karimi and Otilia Pop and Rob Clark", "abstract": "  In this innovative practice full paper, we address the equity gap for\nneurodivergent and situationally limited learners by identifying the spectrum\nof dynamic factors that impact learning and function. Educators have shown a\ngrowing interest in identifying learners' cognitive abilities and learning\npreferences to measure their impact on academic achievement. Often institutions\nemploy one-size-fits-all approaches leaving the burden on disabled students to\nself-advocate or tolerate inadequate support. Emerging frameworks guide\nneurodivergent learners through instructional approaches, such as online\neducation. However, these frameworks fail to address holistic environmental\nneeds or recommend technology interventions, particularly for those with\nundisclosed learning or developmental disabilities and situational limitations.\nIn this article, we integrate a neurodivergent perspective through secondary\nresearch of around 100 articles to introduce a Guiding Empowerment Model\ninvolving key cognitive and situational factors that contextualize day-to-day\nexperiences affecting learner ability. We synthesize three sample student\nprofiles that highlight user problems in functioning. We use this model to\nevaluate sample learning platform features and other supportive technology\nsolutions. The proposed approach augments frameworks such as Universal Design\nfor Learning to consider factors including various sensory processing\ndifferences, social connection challenges, and environmental limitations. We\nsuggest that by applying the mode through technology-enabled features such as\ncustomizable task management, guided varied content access, and guided\nmulti-modal collaboration, major learning barriers of neurodivergent and\nsituationally limited learners will be removed to activate the successful\npursuit of their academic goals.\n", "link": "http://arxiv.org/abs/2410.18876v1", "date": "2024-10-24", "relevancy": 2.2613, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20Empowerment%20Model%3A%20Liberating%20Neurodiversity%20in%20Online%20Higher%0A%20%20Education&body=Title%3A%20Guiding%20Empowerment%20Model%3A%20Liberating%20Neurodiversity%20in%20Online%20Higher%0A%20%20Education%0AAuthor%3A%20Hannah%20Beaux%20and%20Pegah%20Karimi%20and%20Otilia%20Pop%20and%20Rob%20Clark%0AAbstract%3A%20%20%20In%20this%20innovative%20practice%20full%20paper%2C%20we%20address%20the%20equity%20gap%20for%0Aneurodivergent%20and%20situationally%20limited%20learners%20by%20identifying%20the%20spectrum%0Aof%20dynamic%20factors%20that%20impact%20learning%20and%20function.%20Educators%20have%20shown%20a%0Agrowing%20interest%20in%20identifying%20learners%27%20cognitive%20abilities%20and%20learning%0Apreferences%20to%20measure%20their%20impact%20on%20academic%20achievement.%20Often%20institutions%0Aemploy%20one-size-fits-all%20approaches%20leaving%20the%20burden%20on%20disabled%20students%20to%0Aself-advocate%20or%20tolerate%20inadequate%20support.%20Emerging%20frameworks%20guide%0Aneurodivergent%20learners%20through%20instructional%20approaches%2C%20such%20as%20online%0Aeducation.%20However%2C%20these%20frameworks%20fail%20to%20address%20holistic%20environmental%0Aneeds%20or%20recommend%20technology%20interventions%2C%20particularly%20for%20those%20with%0Aundisclosed%20learning%20or%20developmental%20disabilities%20and%20situational%20limitations.%0AIn%20this%20article%2C%20we%20integrate%20a%20neurodivergent%20perspective%20through%20secondary%0Aresearch%20of%20around%20100%20articles%20to%20introduce%20a%20Guiding%20Empowerment%20Model%0Ainvolving%20key%20cognitive%20and%20situational%20factors%20that%20contextualize%20day-to-day%0Aexperiences%20affecting%20learner%20ability.%20We%20synthesize%20three%20sample%20student%0Aprofiles%20that%20highlight%20user%20problems%20in%20functioning.%20We%20use%20this%20model%20to%0Aevaluate%20sample%20learning%20platform%20features%20and%20other%20supportive%20technology%0Asolutions.%20The%20proposed%20approach%20augments%20frameworks%20such%20as%20Universal%20Design%0Afor%20Learning%20to%20consider%20factors%20including%20various%20sensory%20processing%0Adifferences%2C%20social%20connection%20challenges%2C%20and%20environmental%20limitations.%20We%0Asuggest%20that%20by%20applying%20the%20mode%20through%20technology-enabled%20features%20such%20as%0Acustomizable%20task%20management%2C%20guided%20varied%20content%20access%2C%20and%20guided%0Amulti-modal%20collaboration%2C%20major%20learning%20barriers%20of%20neurodivergent%20and%0Asituationally%20limited%20learners%20will%20be%20removed%20to%20activate%20the%20successful%0Apursuit%20of%20their%20academic%20goals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520Empowerment%2520Model%253A%2520Liberating%2520Neurodiversity%2520in%2520Online%2520Higher%250A%2520%2520Education%26entry.906535625%3DHannah%2520Beaux%2520and%2520Pegah%2520Karimi%2520and%2520Otilia%2520Pop%2520and%2520Rob%2520Clark%26entry.1292438233%3D%2520%2520In%2520this%2520innovative%2520practice%2520full%2520paper%252C%2520we%2520address%2520the%2520equity%2520gap%2520for%250Aneurodivergent%2520and%2520situationally%2520limited%2520learners%2520by%2520identifying%2520the%2520spectrum%250Aof%2520dynamic%2520factors%2520that%2520impact%2520learning%2520and%2520function.%2520Educators%2520have%2520shown%2520a%250Agrowing%2520interest%2520in%2520identifying%2520learners%2527%2520cognitive%2520abilities%2520and%2520learning%250Apreferences%2520to%2520measure%2520their%2520impact%2520on%2520academic%2520achievement.%2520Often%2520institutions%250Aemploy%2520one-size-fits-all%2520approaches%2520leaving%2520the%2520burden%2520on%2520disabled%2520students%2520to%250Aself-advocate%2520or%2520tolerate%2520inadequate%2520support.%2520Emerging%2520frameworks%2520guide%250Aneurodivergent%2520learners%2520through%2520instructional%2520approaches%252C%2520such%2520as%2520online%250Aeducation.%2520However%252C%2520these%2520frameworks%2520fail%2520to%2520address%2520holistic%2520environmental%250Aneeds%2520or%2520recommend%2520technology%2520interventions%252C%2520particularly%2520for%2520those%2520with%250Aundisclosed%2520learning%2520or%2520developmental%2520disabilities%2520and%2520situational%2520limitations.%250AIn%2520this%2520article%252C%2520we%2520integrate%2520a%2520neurodivergent%2520perspective%2520through%2520secondary%250Aresearch%2520of%2520around%2520100%2520articles%2520to%2520introduce%2520a%2520Guiding%2520Empowerment%2520Model%250Ainvolving%2520key%2520cognitive%2520and%2520situational%2520factors%2520that%2520contextualize%2520day-to-day%250Aexperiences%2520affecting%2520learner%2520ability.%2520We%2520synthesize%2520three%2520sample%2520student%250Aprofiles%2520that%2520highlight%2520user%2520problems%2520in%2520functioning.%2520We%2520use%2520this%2520model%2520to%250Aevaluate%2520sample%2520learning%2520platform%2520features%2520and%2520other%2520supportive%2520technology%250Asolutions.%2520The%2520proposed%2520approach%2520augments%2520frameworks%2520such%2520as%2520Universal%2520Design%250Afor%2520Learning%2520to%2520consider%2520factors%2520including%2520various%2520sensory%2520processing%250Adifferences%252C%2520social%2520connection%2520challenges%252C%2520and%2520environmental%2520limitations.%2520We%250Asuggest%2520that%2520by%2520applying%2520the%2520mode%2520through%2520technology-enabled%2520features%2520such%2520as%250Acustomizable%2520task%2520management%252C%2520guided%2520varied%2520content%2520access%252C%2520and%2520guided%250Amulti-modal%2520collaboration%252C%2520major%2520learning%2520barriers%2520of%2520neurodivergent%2520and%250Asituationally%2520limited%2520learners%2520will%2520be%2520removed%2520to%2520activate%2520the%2520successful%250Apursuit%2520of%2520their%2520academic%2520goals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20Empowerment%20Model%3A%20Liberating%20Neurodiversity%20in%20Online%20Higher%0A%20%20Education&entry.906535625=Hannah%20Beaux%20and%20Pegah%20Karimi%20and%20Otilia%20Pop%20and%20Rob%20Clark&entry.1292438233=%20%20In%20this%20innovative%20practice%20full%20paper%2C%20we%20address%20the%20equity%20gap%20for%0Aneurodivergent%20and%20situationally%20limited%20learners%20by%20identifying%20the%20spectrum%0Aof%20dynamic%20factors%20that%20impact%20learning%20and%20function.%20Educators%20have%20shown%20a%0Agrowing%20interest%20in%20identifying%20learners%27%20cognitive%20abilities%20and%20learning%0Apreferences%20to%20measure%20their%20impact%20on%20academic%20achievement.%20Often%20institutions%0Aemploy%20one-size-fits-all%20approaches%20leaving%20the%20burden%20on%20disabled%20students%20to%0Aself-advocate%20or%20tolerate%20inadequate%20support.%20Emerging%20frameworks%20guide%0Aneurodivergent%20learners%20through%20instructional%20approaches%2C%20such%20as%20online%0Aeducation.%20However%2C%20these%20frameworks%20fail%20to%20address%20holistic%20environmental%0Aneeds%20or%20recommend%20technology%20interventions%2C%20particularly%20for%20those%20with%0Aundisclosed%20learning%20or%20developmental%20disabilities%20and%20situational%20limitations.%0AIn%20this%20article%2C%20we%20integrate%20a%20neurodivergent%20perspective%20through%20secondary%0Aresearch%20of%20around%20100%20articles%20to%20introduce%20a%20Guiding%20Empowerment%20Model%0Ainvolving%20key%20cognitive%20and%20situational%20factors%20that%20contextualize%20day-to-day%0Aexperiences%20affecting%20learner%20ability.%20We%20synthesize%20three%20sample%20student%0Aprofiles%20that%20highlight%20user%20problems%20in%20functioning.%20We%20use%20this%20model%20to%0Aevaluate%20sample%20learning%20platform%20features%20and%20other%20supportive%20technology%0Asolutions.%20The%20proposed%20approach%20augments%20frameworks%20such%20as%20Universal%20Design%0Afor%20Learning%20to%20consider%20factors%20including%20various%20sensory%20processing%0Adifferences%2C%20social%20connection%20challenges%2C%20and%20environmental%20limitations.%20We%0Asuggest%20that%20by%20applying%20the%20mode%20through%20technology-enabled%20features%20such%20as%0Acustomizable%20task%20management%2C%20guided%20varied%20content%20access%2C%20and%20guided%0Amulti-modal%20collaboration%2C%20major%20learning%20barriers%20of%20neurodivergent%20and%0Asituationally%20limited%20learners%20will%20be%20removed%20to%20activate%20the%20successful%0Apursuit%20of%20their%20academic%20goals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18876v1&entry.124074799=Read"},
{"title": "White Men Lead, Black Women Help? Benchmarking Language Agency Social\n  Biases in LLMs", "author": "Yixin Wan and Kai-Wei Chang", "abstract": "  Social biases can manifest in language agency. While several studies\napproached agency-related bias in human-written language, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, which fall short of\naccurately classifying language agency. We introduce the novel Language Agency\nBias Evaluation (LABE) benchmark, which comprehensively evaluates biases in\nLLMs by analyzing agency levels attributed to different demographic groups in\nmodel generations. LABE leverages 5,400 template-based prompts, an accurate\nagency classifier, and corresponding bias metrics to test for gender, racial,\nand intersectional language agency biases in LLMs on 3 text generation tasks:\nbiographies, professor reviews, and reference letters. We also contribute the\nLanguage Agency Classification (LAC) dataset, consisting of 3,724 agentic and\ncommunal sentences. Using LABE, we unveil language agency social biases in 3\nrecent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations\ntend to demonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. Those who are at the intersection of gender and racial minority\ngroups--such as Black females--are consistently described by texts with lower\nlevels of agency, aligning with real-world social inequalities; (3) Among the 3\nLLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only\ndoes prompt-based mitigation fail to resolve language agency bias in LLMs, but\nit frequently leads to the exacerbation of biases in generated texts.\n", "link": "http://arxiv.org/abs/2404.10508v4", "date": "2024-10-24", "relevancy": 2.246, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20White%20Men%20Lead%2C%20Black%20Women%20Help%3F%20Benchmarking%20Language%20Agency%20Social%0A%20%20Biases%20in%20LLMs&body=Title%3A%20White%20Men%20Lead%2C%20Black%20Women%20Help%3F%20Benchmarking%20Language%20Agency%20Social%0A%20%20Biases%20in%20LLMs%0AAuthor%3A%20Yixin%20Wan%20and%20Kai-Wei%20Chang%0AAbstract%3A%20%20%20Social%20biases%20can%20manifest%20in%20language%20agency.%20While%20several%20studies%0Aapproached%20agency-related%20bias%20in%20human-written%20language%2C%20very%20limited%20research%0Ahas%20investigated%20such%20biases%20in%20Large%20Language%20Model%20%28LLM%29-generated%20content.%0AIn%20addition%2C%20previous%20works%20often%20rely%20on%20string-matching%20techniques%20to%0Aidentify%20agentic%20and%20communal%20words%20within%20texts%2C%20which%20fall%20short%20of%0Aaccurately%20classifying%20language%20agency.%20We%20introduce%20the%20novel%20Language%20Agency%0ABias%20Evaluation%20%28LABE%29%20benchmark%2C%20which%20comprehensively%20evaluates%20biases%20in%0ALLMs%20by%20analyzing%20agency%20levels%20attributed%20to%20different%20demographic%20groups%20in%0Amodel%20generations.%20LABE%20leverages%205%2C400%20template-based%20prompts%2C%20an%20accurate%0Aagency%20classifier%2C%20and%20corresponding%20bias%20metrics%20to%20test%20for%20gender%2C%20racial%2C%0Aand%20intersectional%20language%20agency%20biases%20in%20LLMs%20on%203%20text%20generation%20tasks%3A%0Abiographies%2C%20professor%20reviews%2C%20and%20reference%20letters.%20We%20also%20contribute%20the%0ALanguage%20Agency%20Classification%20%28LAC%29%20dataset%2C%20consisting%20of%203%2C724%20agentic%20and%0Acommunal%20sentences.%20Using%20LABE%2C%20we%20unveil%20language%20agency%20social%20biases%20in%203%0Arecent%20LLMs%3A%20ChatGPT%2C%20Llama3%2C%20and%20Mistral.%20We%20observe%20that%3A%20%281%29%20LLM%20generations%0Atend%20to%20demonstrate%20greater%20gender%20bias%20than%20human-written%20texts%3B%20%282%29%20Models%0Ademonstrate%20remarkably%20higher%20levels%20of%20intersectional%20bias%20than%20the%20other%20bias%0Aaspects.%20Those%20who%20are%20at%20the%20intersection%20of%20gender%20and%20racial%20minority%0Agroups--such%20as%20Black%20females--are%20consistently%20described%20by%20texts%20with%20lower%0Alevels%20of%20agency%2C%20aligning%20with%20real-world%20social%20inequalities%3B%20%283%29%20Among%20the%203%0ALLMs%20investigated%2C%20Llama3%20demonstrates%20the%20greatest%20overall%20bias%3B%20%284%29%20Not%20only%0Adoes%20prompt-based%20mitigation%20fail%20to%20resolve%20language%20agency%20bias%20in%20LLMs%2C%20but%0Ait%20frequently%20leads%20to%20the%20exacerbation%20of%20biases%20in%20generated%20texts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10508v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhite%2520Men%2520Lead%252C%2520Black%2520Women%2520Help%253F%2520Benchmarking%2520Language%2520Agency%2520Social%250A%2520%2520Biases%2520in%2520LLMs%26entry.906535625%3DYixin%2520Wan%2520and%2520Kai-Wei%2520Chang%26entry.1292438233%3D%2520%2520Social%2520biases%2520can%2520manifest%2520in%2520language%2520agency.%2520While%2520several%2520studies%250Aapproached%2520agency-related%2520bias%2520in%2520human-written%2520language%252C%2520very%2520limited%2520research%250Ahas%2520investigated%2520such%2520biases%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529-generated%2520content.%250AIn%2520addition%252C%2520previous%2520works%2520often%2520rely%2520on%2520string-matching%2520techniques%2520to%250Aidentify%2520agentic%2520and%2520communal%2520words%2520within%2520texts%252C%2520which%2520fall%2520short%2520of%250Aaccurately%2520classifying%2520language%2520agency.%2520We%2520introduce%2520the%2520novel%2520Language%2520Agency%250ABias%2520Evaluation%2520%2528LABE%2529%2520benchmark%252C%2520which%2520comprehensively%2520evaluates%2520biases%2520in%250ALLMs%2520by%2520analyzing%2520agency%2520levels%2520attributed%2520to%2520different%2520demographic%2520groups%2520in%250Amodel%2520generations.%2520LABE%2520leverages%25205%252C400%2520template-based%2520prompts%252C%2520an%2520accurate%250Aagency%2520classifier%252C%2520and%2520corresponding%2520bias%2520metrics%2520to%2520test%2520for%2520gender%252C%2520racial%252C%250Aand%2520intersectional%2520language%2520agency%2520biases%2520in%2520LLMs%2520on%25203%2520text%2520generation%2520tasks%253A%250Abiographies%252C%2520professor%2520reviews%252C%2520and%2520reference%2520letters.%2520We%2520also%2520contribute%2520the%250ALanguage%2520Agency%2520Classification%2520%2528LAC%2529%2520dataset%252C%2520consisting%2520of%25203%252C724%2520agentic%2520and%250Acommunal%2520sentences.%2520Using%2520LABE%252C%2520we%2520unveil%2520language%2520agency%2520social%2520biases%2520in%25203%250Arecent%2520LLMs%253A%2520ChatGPT%252C%2520Llama3%252C%2520and%2520Mistral.%2520We%2520observe%2520that%253A%2520%25281%2529%2520LLM%2520generations%250Atend%2520to%2520demonstrate%2520greater%2520gender%2520bias%2520than%2520human-written%2520texts%253B%2520%25282%2529%2520Models%250Ademonstrate%2520remarkably%2520higher%2520levels%2520of%2520intersectional%2520bias%2520than%2520the%2520other%2520bias%250Aaspects.%2520Those%2520who%2520are%2520at%2520the%2520intersection%2520of%2520gender%2520and%2520racial%2520minority%250Agroups--such%2520as%2520Black%2520females--are%2520consistently%2520described%2520by%2520texts%2520with%2520lower%250Alevels%2520of%2520agency%252C%2520aligning%2520with%2520real-world%2520social%2520inequalities%253B%2520%25283%2529%2520Among%2520the%25203%250ALLMs%2520investigated%252C%2520Llama3%2520demonstrates%2520the%2520greatest%2520overall%2520bias%253B%2520%25284%2529%2520Not%2520only%250Adoes%2520prompt-based%2520mitigation%2520fail%2520to%2520resolve%2520language%2520agency%2520bias%2520in%2520LLMs%252C%2520but%250Ait%2520frequently%2520leads%2520to%2520the%2520exacerbation%2520of%2520biases%2520in%2520generated%2520texts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10508v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=White%20Men%20Lead%2C%20Black%20Women%20Help%3F%20Benchmarking%20Language%20Agency%20Social%0A%20%20Biases%20in%20LLMs&entry.906535625=Yixin%20Wan%20and%20Kai-Wei%20Chang&entry.1292438233=%20%20Social%20biases%20can%20manifest%20in%20language%20agency.%20While%20several%20studies%0Aapproached%20agency-related%20bias%20in%20human-written%20language%2C%20very%20limited%20research%0Ahas%20investigated%20such%20biases%20in%20Large%20Language%20Model%20%28LLM%29-generated%20content.%0AIn%20addition%2C%20previous%20works%20often%20rely%20on%20string-matching%20techniques%20to%0Aidentify%20agentic%20and%20communal%20words%20within%20texts%2C%20which%20fall%20short%20of%0Aaccurately%20classifying%20language%20agency.%20We%20introduce%20the%20novel%20Language%20Agency%0ABias%20Evaluation%20%28LABE%29%20benchmark%2C%20which%20comprehensively%20evaluates%20biases%20in%0ALLMs%20by%20analyzing%20agency%20levels%20attributed%20to%20different%20demographic%20groups%20in%0Amodel%20generations.%20LABE%20leverages%205%2C400%20template-based%20prompts%2C%20an%20accurate%0Aagency%20classifier%2C%20and%20corresponding%20bias%20metrics%20to%20test%20for%20gender%2C%20racial%2C%0Aand%20intersectional%20language%20agency%20biases%20in%20LLMs%20on%203%20text%20generation%20tasks%3A%0Abiographies%2C%20professor%20reviews%2C%20and%20reference%20letters.%20We%20also%20contribute%20the%0ALanguage%20Agency%20Classification%20%28LAC%29%20dataset%2C%20consisting%20of%203%2C724%20agentic%20and%0Acommunal%20sentences.%20Using%20LABE%2C%20we%20unveil%20language%20agency%20social%20biases%20in%203%0Arecent%20LLMs%3A%20ChatGPT%2C%20Llama3%2C%20and%20Mistral.%20We%20observe%20that%3A%20%281%29%20LLM%20generations%0Atend%20to%20demonstrate%20greater%20gender%20bias%20than%20human-written%20texts%3B%20%282%29%20Models%0Ademonstrate%20remarkably%20higher%20levels%20of%20intersectional%20bias%20than%20the%20other%20bias%0Aaspects.%20Those%20who%20are%20at%20the%20intersection%20of%20gender%20and%20racial%20minority%0Agroups--such%20as%20Black%20females--are%20consistently%20described%20by%20texts%20with%20lower%0Alevels%20of%20agency%2C%20aligning%20with%20real-world%20social%20inequalities%3B%20%283%29%20Among%20the%203%0ALLMs%20investigated%2C%20Llama3%20demonstrates%20the%20greatest%20overall%20bias%3B%20%284%29%20Not%20only%0Adoes%20prompt-based%20mitigation%20fail%20to%20resolve%20language%20agency%20bias%20in%20LLMs%2C%20but%0Ait%20frequently%20leads%20to%20the%20exacerbation%20of%20biases%20in%20generated%20texts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10508v4&entry.124074799=Read"},
{"title": "Fully Stochastic Primal-dual Gradient Algorithm for Non-convex\n  Optimization on Random Graphs", "author": "Chung-Yiu Yau and Haoming Liu and Hoi-To Wai", "abstract": "  Stochastic decentralized optimization algorithms often suffer from issues\nsuch as synchronization overhead and intermittent communication. This paper\nproposes a $\\underline{\\rm F}$ully $\\underline{\\rm S}$tochastic $\\underline{\\rm\nP}$rimal $\\underline{\\rm D}$ual gradient $\\underline{\\rm A}$lgorithm (FSPDA)\nthat suggests an asynchronous decentralized procedure with (i) sparsified\nnon-blocking communication on random undirected graphs and (ii) local\nstochastic gradient updates. FSPDA allows multiple local gradient steps to\naccelerate convergence to stationarity while finding a consensual solution with\nstochastic primal-dual updates. For problems with smooth (possibly non-convex)\nobjective function, we show that FSPDA converges to an $\\mathrm{\\mathcal{O}(\n{\\it \\sigma /\\sqrt{nT}} )}$-stationary solution after $\\mathrm{\\it T}$\niterations without assuming data heterogeneity. The performance of FSPDA is on\npar with state-of-the-art algorithms whose convergence depend on static graph\nand synchronous updates. To our best knowledge, FSPDA is the first asynchronous\nalgorithm that converges exactly under the non-convex setting. Numerical\nexperiments are presented to show the benefits of FSPDA.\n", "link": "http://arxiv.org/abs/2410.18774v1", "date": "2024-10-24", "relevancy": 2.2438, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.456}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4452}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Stochastic%20Primal-dual%20Gradient%20Algorithm%20for%20Non-convex%0A%20%20Optimization%20on%20Random%20Graphs&body=Title%3A%20Fully%20Stochastic%20Primal-dual%20Gradient%20Algorithm%20for%20Non-convex%0A%20%20Optimization%20on%20Random%20Graphs%0AAuthor%3A%20Chung-Yiu%20Yau%20and%20Haoming%20Liu%20and%20Hoi-To%20Wai%0AAbstract%3A%20%20%20Stochastic%20decentralized%20optimization%20algorithms%20often%20suffer%20from%20issues%0Asuch%20as%20synchronization%20overhead%20and%20intermittent%20communication.%20This%20paper%0Aproposes%20a%20%24%5Cunderline%7B%5Crm%20F%7D%24ully%20%24%5Cunderline%7B%5Crm%20S%7D%24tochastic%20%24%5Cunderline%7B%5Crm%0AP%7D%24rimal%20%24%5Cunderline%7B%5Crm%20D%7D%24ual%20gradient%20%24%5Cunderline%7B%5Crm%20A%7D%24lgorithm%20%28FSPDA%29%0Athat%20suggests%20an%20asynchronous%20decentralized%20procedure%20with%20%28i%29%20sparsified%0Anon-blocking%20communication%20on%20random%20undirected%20graphs%20and%20%28ii%29%20local%0Astochastic%20gradient%20updates.%20FSPDA%20allows%20multiple%20local%20gradient%20steps%20to%0Aaccelerate%20convergence%20to%20stationarity%20while%20finding%20a%20consensual%20solution%20with%0Astochastic%20primal-dual%20updates.%20For%20problems%20with%20smooth%20%28possibly%20non-convex%29%0Aobjective%20function%2C%20we%20show%20that%20FSPDA%20converges%20to%20an%20%24%5Cmathrm%7B%5Cmathcal%7BO%7D%28%0A%7B%5Cit%20%5Csigma%20/%5Csqrt%7BnT%7D%7D%20%29%7D%24-stationary%20solution%20after%20%24%5Cmathrm%7B%5Cit%20T%7D%24%0Aiterations%20without%20assuming%20data%20heterogeneity.%20The%20performance%20of%20FSPDA%20is%20on%0Apar%20with%20state-of-the-art%20algorithms%20whose%20convergence%20depend%20on%20static%20graph%0Aand%20synchronous%20updates.%20To%20our%20best%20knowledge%2C%20FSPDA%20is%20the%20first%20asynchronous%0Aalgorithm%20that%20converges%20exactly%20under%20the%20non-convex%20setting.%20Numerical%0Aexperiments%20are%20presented%20to%20show%20the%20benefits%20of%20FSPDA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Stochastic%2520Primal-dual%2520Gradient%2520Algorithm%2520for%2520Non-convex%250A%2520%2520Optimization%2520on%2520Random%2520Graphs%26entry.906535625%3DChung-Yiu%2520Yau%2520and%2520Haoming%2520Liu%2520and%2520Hoi-To%2520Wai%26entry.1292438233%3D%2520%2520Stochastic%2520decentralized%2520optimization%2520algorithms%2520often%2520suffer%2520from%2520issues%250Asuch%2520as%2520synchronization%2520overhead%2520and%2520intermittent%2520communication.%2520This%2520paper%250Aproposes%2520a%2520%2524%255Cunderline%257B%255Crm%2520F%257D%2524ully%2520%2524%255Cunderline%257B%255Crm%2520S%257D%2524tochastic%2520%2524%255Cunderline%257B%255Crm%250AP%257D%2524rimal%2520%2524%255Cunderline%257B%255Crm%2520D%257D%2524ual%2520gradient%2520%2524%255Cunderline%257B%255Crm%2520A%257D%2524lgorithm%2520%2528FSPDA%2529%250Athat%2520suggests%2520an%2520asynchronous%2520decentralized%2520procedure%2520with%2520%2528i%2529%2520sparsified%250Anon-blocking%2520communication%2520on%2520random%2520undirected%2520graphs%2520and%2520%2528ii%2529%2520local%250Astochastic%2520gradient%2520updates.%2520FSPDA%2520allows%2520multiple%2520local%2520gradient%2520steps%2520to%250Aaccelerate%2520convergence%2520to%2520stationarity%2520while%2520finding%2520a%2520consensual%2520solution%2520with%250Astochastic%2520primal-dual%2520updates.%2520For%2520problems%2520with%2520smooth%2520%2528possibly%2520non-convex%2529%250Aobjective%2520function%252C%2520we%2520show%2520that%2520FSPDA%2520converges%2520to%2520an%2520%2524%255Cmathrm%257B%255Cmathcal%257BO%257D%2528%250A%257B%255Cit%2520%255Csigma%2520/%255Csqrt%257BnT%257D%257D%2520%2529%257D%2524-stationary%2520solution%2520after%2520%2524%255Cmathrm%257B%255Cit%2520T%257D%2524%250Aiterations%2520without%2520assuming%2520data%2520heterogeneity.%2520The%2520performance%2520of%2520FSPDA%2520is%2520on%250Apar%2520with%2520state-of-the-art%2520algorithms%2520whose%2520convergence%2520depend%2520on%2520static%2520graph%250Aand%2520synchronous%2520updates.%2520To%2520our%2520best%2520knowledge%252C%2520FSPDA%2520is%2520the%2520first%2520asynchronous%250Aalgorithm%2520that%2520converges%2520exactly%2520under%2520the%2520non-convex%2520setting.%2520Numerical%250Aexperiments%2520are%2520presented%2520to%2520show%2520the%2520benefits%2520of%2520FSPDA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Stochastic%20Primal-dual%20Gradient%20Algorithm%20for%20Non-convex%0A%20%20Optimization%20on%20Random%20Graphs&entry.906535625=Chung-Yiu%20Yau%20and%20Haoming%20Liu%20and%20Hoi-To%20Wai&entry.1292438233=%20%20Stochastic%20decentralized%20optimization%20algorithms%20often%20suffer%20from%20issues%0Asuch%20as%20synchronization%20overhead%20and%20intermittent%20communication.%20This%20paper%0Aproposes%20a%20%24%5Cunderline%7B%5Crm%20F%7D%24ully%20%24%5Cunderline%7B%5Crm%20S%7D%24tochastic%20%24%5Cunderline%7B%5Crm%0AP%7D%24rimal%20%24%5Cunderline%7B%5Crm%20D%7D%24ual%20gradient%20%24%5Cunderline%7B%5Crm%20A%7D%24lgorithm%20%28FSPDA%29%0Athat%20suggests%20an%20asynchronous%20decentralized%20procedure%20with%20%28i%29%20sparsified%0Anon-blocking%20communication%20on%20random%20undirected%20graphs%20and%20%28ii%29%20local%0Astochastic%20gradient%20updates.%20FSPDA%20allows%20multiple%20local%20gradient%20steps%20to%0Aaccelerate%20convergence%20to%20stationarity%20while%20finding%20a%20consensual%20solution%20with%0Astochastic%20primal-dual%20updates.%20For%20problems%20with%20smooth%20%28possibly%20non-convex%29%0Aobjective%20function%2C%20we%20show%20that%20FSPDA%20converges%20to%20an%20%24%5Cmathrm%7B%5Cmathcal%7BO%7D%28%0A%7B%5Cit%20%5Csigma%20/%5Csqrt%7BnT%7D%7D%20%29%7D%24-stationary%20solution%20after%20%24%5Cmathrm%7B%5Cit%20T%7D%24%0Aiterations%20without%20assuming%20data%20heterogeneity.%20The%20performance%20of%20FSPDA%20is%20on%0Apar%20with%20state-of-the-art%20algorithms%20whose%20convergence%20depend%20on%20static%20graph%0Aand%20synchronous%20updates.%20To%20our%20best%20knowledge%2C%20FSPDA%20is%20the%20first%20asynchronous%0Aalgorithm%20that%20converges%20exactly%20under%20the%20non-convex%20setting.%20Numerical%0Aexperiments%20are%20presented%20to%20show%20the%20benefits%20of%20FSPDA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18774v1&entry.124074799=Read"},
{"title": "Versatile Motion Language Models for Multi-Turn Interactive Agents", "author": "Jeongeun Park and Sungjoon Choi and Sangdoo Yun", "abstract": "  Recent advancements in large language models (LLMs) have greatly enhanced\ntheir ability to generate natural and contextually relevant text, making AI\ninteractions more human-like. However, generating and understanding interactive\nhuman-like motion, where two individuals engage in coordinated movements,\nremains a challenge due to the complexity of modeling these coordinated\ninteractions. Furthermore, a versatile model is required to handle diverse\ninteractive scenarios, such as chat systems that follow user instructions or\nadapt to their assigned role while adjusting interaction dynamics. To tackle\nthis problem, we introduce VIM, short for the Versatile Interactive Motion\nlanguage model, which integrates both language and motion modalities to\neffectively understand, generate, and control interactive motions in multi-turn\nconversational contexts. To address the scarcity of multi-turn interactive\nmotion data, we introduce a synthetic dataset, INERT-MT2, where we utilize\npre-trained models to create diverse instructional datasets with interactive\nmotion. Our approach first trains a motion tokenizer that encodes interactive\nmotions into residual discrete tokens. In the pretraining stage, the model\nlearns to align motion and text representations with these discrete tokens.\nDuring the instruction fine-tuning stage, VIM adapts to multi-turn\nconversations using the INTER-MT2 dataset. We evaluate the versatility of our\nmethod across motion-related tasks, motion to text, text to motion, reaction\ngeneration, motion editing, and reasoning about motion sequences. The results\nhighlight the versatility and effectiveness of proposed method in handling\ncomplex interactive motion synthesis.\n", "link": "http://arxiv.org/abs/2410.05628v3", "date": "2024-10-24", "relevancy": 2.237, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5796}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Versatile%20Motion%20Language%20Models%20for%20Multi-Turn%20Interactive%20Agents&body=Title%3A%20Versatile%20Motion%20Language%20Models%20for%20Multi-Turn%20Interactive%20Agents%0AAuthor%3A%20Jeongeun%20Park%20and%20Sungjoon%20Choi%20and%20Sangdoo%20Yun%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20greatly%20enhanced%0Atheir%20ability%20to%20generate%20natural%20and%20contextually%20relevant%20text%2C%20making%20AI%0Ainteractions%20more%20human-like.%20However%2C%20generating%20and%20understanding%20interactive%0Ahuman-like%20motion%2C%20where%20two%20individuals%20engage%20in%20coordinated%20movements%2C%0Aremains%20a%20challenge%20due%20to%20the%20complexity%20of%20modeling%20these%20coordinated%0Ainteractions.%20Furthermore%2C%20a%20versatile%20model%20is%20required%20to%20handle%20diverse%0Ainteractive%20scenarios%2C%20such%20as%20chat%20systems%20that%20follow%20user%20instructions%20or%0Aadapt%20to%20their%20assigned%20role%20while%20adjusting%20interaction%20dynamics.%20To%20tackle%0Athis%20problem%2C%20we%20introduce%20VIM%2C%20short%20for%20the%20Versatile%20Interactive%20Motion%0Alanguage%20model%2C%20which%20integrates%20both%20language%20and%20motion%20modalities%20to%0Aeffectively%20understand%2C%20generate%2C%20and%20control%20interactive%20motions%20in%20multi-turn%0Aconversational%20contexts.%20To%20address%20the%20scarcity%20of%20multi-turn%20interactive%0Amotion%20data%2C%20we%20introduce%20a%20synthetic%20dataset%2C%20INERT-MT2%2C%20where%20we%20utilize%0Apre-trained%20models%20to%20create%20diverse%20instructional%20datasets%20with%20interactive%0Amotion.%20Our%20approach%20first%20trains%20a%20motion%20tokenizer%20that%20encodes%20interactive%0Amotions%20into%20residual%20discrete%20tokens.%20In%20the%20pretraining%20stage%2C%20the%20model%0Alearns%20to%20align%20motion%20and%20text%20representations%20with%20these%20discrete%20tokens.%0ADuring%20the%20instruction%20fine-tuning%20stage%2C%20VIM%20adapts%20to%20multi-turn%0Aconversations%20using%20the%20INTER-MT2%20dataset.%20We%20evaluate%20the%20versatility%20of%20our%0Amethod%20across%20motion-related%20tasks%2C%20motion%20to%20text%2C%20text%20to%20motion%2C%20reaction%0Ageneration%2C%20motion%20editing%2C%20and%20reasoning%20about%20motion%20sequences.%20The%20results%0Ahighlight%20the%20versatility%20and%20effectiveness%20of%20proposed%20method%20in%20handling%0Acomplex%20interactive%20motion%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05628v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVersatile%2520Motion%2520Language%2520Models%2520for%2520Multi-Turn%2520Interactive%2520Agents%26entry.906535625%3DJeongeun%2520Park%2520and%2520Sungjoon%2520Choi%2520and%2520Sangdoo%2520Yun%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520greatly%2520enhanced%250Atheir%2520ability%2520to%2520generate%2520natural%2520and%2520contextually%2520relevant%2520text%252C%2520making%2520AI%250Ainteractions%2520more%2520human-like.%2520However%252C%2520generating%2520and%2520understanding%2520interactive%250Ahuman-like%2520motion%252C%2520where%2520two%2520individuals%2520engage%2520in%2520coordinated%2520movements%252C%250Aremains%2520a%2520challenge%2520due%2520to%2520the%2520complexity%2520of%2520modeling%2520these%2520coordinated%250Ainteractions.%2520Furthermore%252C%2520a%2520versatile%2520model%2520is%2520required%2520to%2520handle%2520diverse%250Ainteractive%2520scenarios%252C%2520such%2520as%2520chat%2520systems%2520that%2520follow%2520user%2520instructions%2520or%250Aadapt%2520to%2520their%2520assigned%2520role%2520while%2520adjusting%2520interaction%2520dynamics.%2520To%2520tackle%250Athis%2520problem%252C%2520we%2520introduce%2520VIM%252C%2520short%2520for%2520the%2520Versatile%2520Interactive%2520Motion%250Alanguage%2520model%252C%2520which%2520integrates%2520both%2520language%2520and%2520motion%2520modalities%2520to%250Aeffectively%2520understand%252C%2520generate%252C%2520and%2520control%2520interactive%2520motions%2520in%2520multi-turn%250Aconversational%2520contexts.%2520To%2520address%2520the%2520scarcity%2520of%2520multi-turn%2520interactive%250Amotion%2520data%252C%2520we%2520introduce%2520a%2520synthetic%2520dataset%252C%2520INERT-MT2%252C%2520where%2520we%2520utilize%250Apre-trained%2520models%2520to%2520create%2520diverse%2520instructional%2520datasets%2520with%2520interactive%250Amotion.%2520Our%2520approach%2520first%2520trains%2520a%2520motion%2520tokenizer%2520that%2520encodes%2520interactive%250Amotions%2520into%2520residual%2520discrete%2520tokens.%2520In%2520the%2520pretraining%2520stage%252C%2520the%2520model%250Alearns%2520to%2520align%2520motion%2520and%2520text%2520representations%2520with%2520these%2520discrete%2520tokens.%250ADuring%2520the%2520instruction%2520fine-tuning%2520stage%252C%2520VIM%2520adapts%2520to%2520multi-turn%250Aconversations%2520using%2520the%2520INTER-MT2%2520dataset.%2520We%2520evaluate%2520the%2520versatility%2520of%2520our%250Amethod%2520across%2520motion-related%2520tasks%252C%2520motion%2520to%2520text%252C%2520text%2520to%2520motion%252C%2520reaction%250Ageneration%252C%2520motion%2520editing%252C%2520and%2520reasoning%2520about%2520motion%2520sequences.%2520The%2520results%250Ahighlight%2520the%2520versatility%2520and%2520effectiveness%2520of%2520proposed%2520method%2520in%2520handling%250Acomplex%2520interactive%2520motion%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05628v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Versatile%20Motion%20Language%20Models%20for%20Multi-Turn%20Interactive%20Agents&entry.906535625=Jeongeun%20Park%20and%20Sungjoon%20Choi%20and%20Sangdoo%20Yun&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20greatly%20enhanced%0Atheir%20ability%20to%20generate%20natural%20and%20contextually%20relevant%20text%2C%20making%20AI%0Ainteractions%20more%20human-like.%20However%2C%20generating%20and%20understanding%20interactive%0Ahuman-like%20motion%2C%20where%20two%20individuals%20engage%20in%20coordinated%20movements%2C%0Aremains%20a%20challenge%20due%20to%20the%20complexity%20of%20modeling%20these%20coordinated%0Ainteractions.%20Furthermore%2C%20a%20versatile%20model%20is%20required%20to%20handle%20diverse%0Ainteractive%20scenarios%2C%20such%20as%20chat%20systems%20that%20follow%20user%20instructions%20or%0Aadapt%20to%20their%20assigned%20role%20while%20adjusting%20interaction%20dynamics.%20To%20tackle%0Athis%20problem%2C%20we%20introduce%20VIM%2C%20short%20for%20the%20Versatile%20Interactive%20Motion%0Alanguage%20model%2C%20which%20integrates%20both%20language%20and%20motion%20modalities%20to%0Aeffectively%20understand%2C%20generate%2C%20and%20control%20interactive%20motions%20in%20multi-turn%0Aconversational%20contexts.%20To%20address%20the%20scarcity%20of%20multi-turn%20interactive%0Amotion%20data%2C%20we%20introduce%20a%20synthetic%20dataset%2C%20INERT-MT2%2C%20where%20we%20utilize%0Apre-trained%20models%20to%20create%20diverse%20instructional%20datasets%20with%20interactive%0Amotion.%20Our%20approach%20first%20trains%20a%20motion%20tokenizer%20that%20encodes%20interactive%0Amotions%20into%20residual%20discrete%20tokens.%20In%20the%20pretraining%20stage%2C%20the%20model%0Alearns%20to%20align%20motion%20and%20text%20representations%20with%20these%20discrete%20tokens.%0ADuring%20the%20instruction%20fine-tuning%20stage%2C%20VIM%20adapts%20to%20multi-turn%0Aconversations%20using%20the%20INTER-MT2%20dataset.%20We%20evaluate%20the%20versatility%20of%20our%0Amethod%20across%20motion-related%20tasks%2C%20motion%20to%20text%2C%20text%20to%20motion%2C%20reaction%0Ageneration%2C%20motion%20editing%2C%20and%20reasoning%20about%20motion%20sequences.%20The%20results%0Ahighlight%20the%20versatility%20and%20effectiveness%20of%20proposed%20method%20in%20handling%0Acomplex%20interactive%20motion%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05628v3&entry.124074799=Read"},
{"title": "Robust Watermarking Using Generative Priors Against Image Editing: From\n  Benchmarking to Advances", "author": "Shilin Lu and Zihan Zhou and Jiayou Lu and Yuanzhi Zhu and Adams Wai-Kin Kong", "abstract": "  Current image watermarking methods are vulnerable to advanced image editing\ntechniques enabled by large-scale text-to-image models. These models can\ndistort embedded watermarks during editing, posing significant challenges to\ncopyright protection. In this work, we introduce W-Bench, the first\ncomprehensive benchmark designed to evaluate the robustness of watermarking\nmethods against a wide range of image editing techniques, including image\nregeneration, global editing, local editing, and image-to-video generation.\nThrough extensive evaluations of eleven representative watermarking methods\nagainst prevalent editing techniques, we demonstrate that most methods fail to\ndetect watermarks after such edits. To address this limitation, we propose\nVINE, a watermarking method that significantly enhances robustness against\nvarious image editing techniques while maintaining high image quality. Our\napproach involves two key innovations: (1) we analyze the frequency\ncharacteristics of image editing and identify that blurring distortions exhibit\nsimilar frequency properties, which allows us to use them as surrogate attacks\nduring training to bolster watermark robustness; (2) we leverage a large-scale\npretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to\nachieve more imperceptible and robust watermark embedding. Experimental results\nshow that our method achieves outstanding watermarking performance under\nvarious image editing techniques, outperforming existing methods in both image\nquality and robustness. Code is available at https://github.com/Shilin-LU/VINE.\n", "link": "http://arxiv.org/abs/2410.18775v1", "date": "2024-10-24", "relevancy": 2.2353, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5726}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5679}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Watermarking%20Using%20Generative%20Priors%20Against%20Image%20Editing%3A%20From%0A%20%20Benchmarking%20to%20Advances&body=Title%3A%20Robust%20Watermarking%20Using%20Generative%20Priors%20Against%20Image%20Editing%3A%20From%0A%20%20Benchmarking%20to%20Advances%0AAuthor%3A%20Shilin%20Lu%20and%20Zihan%20Zhou%20and%20Jiayou%20Lu%20and%20Yuanzhi%20Zhu%20and%20Adams%20Wai-Kin%20Kong%0AAbstract%3A%20%20%20Current%20image%20watermarking%20methods%20are%20vulnerable%20to%20advanced%20image%20editing%0Atechniques%20enabled%20by%20large-scale%20text-to-image%20models.%20These%20models%20can%0Adistort%20embedded%20watermarks%20during%20editing%2C%20posing%20significant%20challenges%20to%0Acopyright%20protection.%20In%20this%20work%2C%20we%20introduce%20W-Bench%2C%20the%20first%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20the%20robustness%20of%20watermarking%0Amethods%20against%20a%20wide%20range%20of%20image%20editing%20techniques%2C%20including%20image%0Aregeneration%2C%20global%20editing%2C%20local%20editing%2C%20and%20image-to-video%20generation.%0AThrough%20extensive%20evaluations%20of%20eleven%20representative%20watermarking%20methods%0Aagainst%20prevalent%20editing%20techniques%2C%20we%20demonstrate%20that%20most%20methods%20fail%20to%0Adetect%20watermarks%20after%20such%20edits.%20To%20address%20this%20limitation%2C%20we%20propose%0AVINE%2C%20a%20watermarking%20method%20that%20significantly%20enhances%20robustness%20against%0Avarious%20image%20editing%20techniques%20while%20maintaining%20high%20image%20quality.%20Our%0Aapproach%20involves%20two%20key%20innovations%3A%20%281%29%20we%20analyze%20the%20frequency%0Acharacteristics%20of%20image%20editing%20and%20identify%20that%20blurring%20distortions%20exhibit%0Asimilar%20frequency%20properties%2C%20which%20allows%20us%20to%20use%20them%20as%20surrogate%20attacks%0Aduring%20training%20to%20bolster%20watermark%20robustness%3B%20%282%29%20we%20leverage%20a%20large-scale%0Apretrained%20diffusion%20model%20SDXL-Turbo%2C%20adapting%20it%20for%20the%20watermarking%20task%20to%0Aachieve%20more%20imperceptible%20and%20robust%20watermark%20embedding.%20Experimental%20results%0Ashow%20that%20our%20method%20achieves%20outstanding%20watermarking%20performance%20under%0Avarious%20image%20editing%20techniques%2C%20outperforming%20existing%20methods%20in%20both%20image%0Aquality%20and%20robustness.%20Code%20is%20available%20at%20https%3A//github.com/Shilin-LU/VINE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Watermarking%2520Using%2520Generative%2520Priors%2520Against%2520Image%2520Editing%253A%2520From%250A%2520%2520Benchmarking%2520to%2520Advances%26entry.906535625%3DShilin%2520Lu%2520and%2520Zihan%2520Zhou%2520and%2520Jiayou%2520Lu%2520and%2520Yuanzhi%2520Zhu%2520and%2520Adams%2520Wai-Kin%2520Kong%26entry.1292438233%3D%2520%2520Current%2520image%2520watermarking%2520methods%2520are%2520vulnerable%2520to%2520advanced%2520image%2520editing%250Atechniques%2520enabled%2520by%2520large-scale%2520text-to-image%2520models.%2520These%2520models%2520can%250Adistort%2520embedded%2520watermarks%2520during%2520editing%252C%2520posing%2520significant%2520challenges%2520to%250Acopyright%2520protection.%2520In%2520this%2520work%252C%2520we%2520introduce%2520W-Bench%252C%2520the%2520first%250Acomprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520robustness%2520of%2520watermarking%250Amethods%2520against%2520a%2520wide%2520range%2520of%2520image%2520editing%2520techniques%252C%2520including%2520image%250Aregeneration%252C%2520global%2520editing%252C%2520local%2520editing%252C%2520and%2520image-to-video%2520generation.%250AThrough%2520extensive%2520evaluations%2520of%2520eleven%2520representative%2520watermarking%2520methods%250Aagainst%2520prevalent%2520editing%2520techniques%252C%2520we%2520demonstrate%2520that%2520most%2520methods%2520fail%2520to%250Adetect%2520watermarks%2520after%2520such%2520edits.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250AVINE%252C%2520a%2520watermarking%2520method%2520that%2520significantly%2520enhances%2520robustness%2520against%250Avarious%2520image%2520editing%2520techniques%2520while%2520maintaining%2520high%2520image%2520quality.%2520Our%250Aapproach%2520involves%2520two%2520key%2520innovations%253A%2520%25281%2529%2520we%2520analyze%2520the%2520frequency%250Acharacteristics%2520of%2520image%2520editing%2520and%2520identify%2520that%2520blurring%2520distortions%2520exhibit%250Asimilar%2520frequency%2520properties%252C%2520which%2520allows%2520us%2520to%2520use%2520them%2520as%2520surrogate%2520attacks%250Aduring%2520training%2520to%2520bolster%2520watermark%2520robustness%253B%2520%25282%2529%2520we%2520leverage%2520a%2520large-scale%250Apretrained%2520diffusion%2520model%2520SDXL-Turbo%252C%2520adapting%2520it%2520for%2520the%2520watermarking%2520task%2520to%250Aachieve%2520more%2520imperceptible%2520and%2520robust%2520watermark%2520embedding.%2520Experimental%2520results%250Ashow%2520that%2520our%2520method%2520achieves%2520outstanding%2520watermarking%2520performance%2520under%250Avarious%2520image%2520editing%2520techniques%252C%2520outperforming%2520existing%2520methods%2520in%2520both%2520image%250Aquality%2520and%2520robustness.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Shilin-LU/VINE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Watermarking%20Using%20Generative%20Priors%20Against%20Image%20Editing%3A%20From%0A%20%20Benchmarking%20to%20Advances&entry.906535625=Shilin%20Lu%20and%20Zihan%20Zhou%20and%20Jiayou%20Lu%20and%20Yuanzhi%20Zhu%20and%20Adams%20Wai-Kin%20Kong&entry.1292438233=%20%20Current%20image%20watermarking%20methods%20are%20vulnerable%20to%20advanced%20image%20editing%0Atechniques%20enabled%20by%20large-scale%20text-to-image%20models.%20These%20models%20can%0Adistort%20embedded%20watermarks%20during%20editing%2C%20posing%20significant%20challenges%20to%0Acopyright%20protection.%20In%20this%20work%2C%20we%20introduce%20W-Bench%2C%20the%20first%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20the%20robustness%20of%20watermarking%0Amethods%20against%20a%20wide%20range%20of%20image%20editing%20techniques%2C%20including%20image%0Aregeneration%2C%20global%20editing%2C%20local%20editing%2C%20and%20image-to-video%20generation.%0AThrough%20extensive%20evaluations%20of%20eleven%20representative%20watermarking%20methods%0Aagainst%20prevalent%20editing%20techniques%2C%20we%20demonstrate%20that%20most%20methods%20fail%20to%0Adetect%20watermarks%20after%20such%20edits.%20To%20address%20this%20limitation%2C%20we%20propose%0AVINE%2C%20a%20watermarking%20method%20that%20significantly%20enhances%20robustness%20against%0Avarious%20image%20editing%20techniques%20while%20maintaining%20high%20image%20quality.%20Our%0Aapproach%20involves%20two%20key%20innovations%3A%20%281%29%20we%20analyze%20the%20frequency%0Acharacteristics%20of%20image%20editing%20and%20identify%20that%20blurring%20distortions%20exhibit%0Asimilar%20frequency%20properties%2C%20which%20allows%20us%20to%20use%20them%20as%20surrogate%20attacks%0Aduring%20training%20to%20bolster%20watermark%20robustness%3B%20%282%29%20we%20leverage%20a%20large-scale%0Apretrained%20diffusion%20model%20SDXL-Turbo%2C%20adapting%20it%20for%20the%20watermarking%20task%20to%0Aachieve%20more%20imperceptible%20and%20robust%20watermark%20embedding.%20Experimental%20results%0Ashow%20that%20our%20method%20achieves%20outstanding%20watermarking%20performance%20under%0Avarious%20image%20editing%20techniques%2C%20outperforming%20existing%20methods%20in%20both%20image%0Aquality%20and%20robustness.%20Code%20is%20available%20at%20https%3A//github.com/Shilin-LU/VINE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18775v1&entry.124074799=Read"},
{"title": "Moving Object Segmentation in Point Cloud Data using Hidden Markov\n  Models", "author": "Vedant Bhandari and Jasmin James and Tyson Phillips and P. Ross McAree", "abstract": "  Autonomous agents require the capability to identify dynamic objects in their\nenvironment for safe planning and navigation. Incomplete and erroneous dynamic\ndetections jeopardize the agent's ability to accomplish its task. Dynamic\ndetection is a challenging problem due to the numerous sources of uncertainty\ninherent in the problem's inputs and the wide variety of applications, which\noften lead to use-case-tailored solutions. We propose a robust learning-free\napproach to segment moving objects in point cloud data. The foundation of the\napproach lies in modelling each voxel using a hidden Markov model (HMM), and\nprobabilistically integrating beliefs into a map using an HMM filter. The\nproposed approach is tested on benchmark datasets and consistently performs\nbetter than or as well as state-of-the-art methods with strong generalized\nperformance across sensor characteristics and environments. The approach is\nopen-sourced at https://github.com/vb44/HMM-MOS.\n", "link": "http://arxiv.org/abs/2410.18638v1", "date": "2024-10-24", "relevancy": 2.2329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6091}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5576}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moving%20Object%20Segmentation%20in%20Point%20Cloud%20Data%20using%20Hidden%20Markov%0A%20%20Models&body=Title%3A%20Moving%20Object%20Segmentation%20in%20Point%20Cloud%20Data%20using%20Hidden%20Markov%0A%20%20Models%0AAuthor%3A%20Vedant%20Bhandari%20and%20Jasmin%20James%20and%20Tyson%20Phillips%20and%20P.%20Ross%20McAree%0AAbstract%3A%20%20%20Autonomous%20agents%20require%20the%20capability%20to%20identify%20dynamic%20objects%20in%20their%0Aenvironment%20for%20safe%20planning%20and%20navigation.%20Incomplete%20and%20erroneous%20dynamic%0Adetections%20jeopardize%20the%20agent%27s%20ability%20to%20accomplish%20its%20task.%20Dynamic%0Adetection%20is%20a%20challenging%20problem%20due%20to%20the%20numerous%20sources%20of%20uncertainty%0Ainherent%20in%20the%20problem%27s%20inputs%20and%20the%20wide%20variety%20of%20applications%2C%20which%0Aoften%20lead%20to%20use-case-tailored%20solutions.%20We%20propose%20a%20robust%20learning-free%0Aapproach%20to%20segment%20moving%20objects%20in%20point%20cloud%20data.%20The%20foundation%20of%20the%0Aapproach%20lies%20in%20modelling%20each%20voxel%20using%20a%20hidden%20Markov%20model%20%28HMM%29%2C%20and%0Aprobabilistically%20integrating%20beliefs%20into%20a%20map%20using%20an%20HMM%20filter.%20The%0Aproposed%20approach%20is%20tested%20on%20benchmark%20datasets%20and%20consistently%20performs%0Abetter%20than%20or%20as%20well%20as%20state-of-the-art%20methods%20with%20strong%20generalized%0Aperformance%20across%20sensor%20characteristics%20and%20environments.%20The%20approach%20is%0Aopen-sourced%20at%20https%3A//github.com/vb44/HMM-MOS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoving%2520Object%2520Segmentation%2520in%2520Point%2520Cloud%2520Data%2520using%2520Hidden%2520Markov%250A%2520%2520Models%26entry.906535625%3DVedant%2520Bhandari%2520and%2520Jasmin%2520James%2520and%2520Tyson%2520Phillips%2520and%2520P.%2520Ross%2520McAree%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520require%2520the%2520capability%2520to%2520identify%2520dynamic%2520objects%2520in%2520their%250Aenvironment%2520for%2520safe%2520planning%2520and%2520navigation.%2520Incomplete%2520and%2520erroneous%2520dynamic%250Adetections%2520jeopardize%2520the%2520agent%2527s%2520ability%2520to%2520accomplish%2520its%2520task.%2520Dynamic%250Adetection%2520is%2520a%2520challenging%2520problem%2520due%2520to%2520the%2520numerous%2520sources%2520of%2520uncertainty%250Ainherent%2520in%2520the%2520problem%2527s%2520inputs%2520and%2520the%2520wide%2520variety%2520of%2520applications%252C%2520which%250Aoften%2520lead%2520to%2520use-case-tailored%2520solutions.%2520We%2520propose%2520a%2520robust%2520learning-free%250Aapproach%2520to%2520segment%2520moving%2520objects%2520in%2520point%2520cloud%2520data.%2520The%2520foundation%2520of%2520the%250Aapproach%2520lies%2520in%2520modelling%2520each%2520voxel%2520using%2520a%2520hidden%2520Markov%2520model%2520%2528HMM%2529%252C%2520and%250Aprobabilistically%2520integrating%2520beliefs%2520into%2520a%2520map%2520using%2520an%2520HMM%2520filter.%2520The%250Aproposed%2520approach%2520is%2520tested%2520on%2520benchmark%2520datasets%2520and%2520consistently%2520performs%250Abetter%2520than%2520or%2520as%2520well%2520as%2520state-of-the-art%2520methods%2520with%2520strong%2520generalized%250Aperformance%2520across%2520sensor%2520characteristics%2520and%2520environments.%2520The%2520approach%2520is%250Aopen-sourced%2520at%2520https%253A//github.com/vb44/HMM-MOS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moving%20Object%20Segmentation%20in%20Point%20Cloud%20Data%20using%20Hidden%20Markov%0A%20%20Models&entry.906535625=Vedant%20Bhandari%20and%20Jasmin%20James%20and%20Tyson%20Phillips%20and%20P.%20Ross%20McAree&entry.1292438233=%20%20Autonomous%20agents%20require%20the%20capability%20to%20identify%20dynamic%20objects%20in%20their%0Aenvironment%20for%20safe%20planning%20and%20navigation.%20Incomplete%20and%20erroneous%20dynamic%0Adetections%20jeopardize%20the%20agent%27s%20ability%20to%20accomplish%20its%20task.%20Dynamic%0Adetection%20is%20a%20challenging%20problem%20due%20to%20the%20numerous%20sources%20of%20uncertainty%0Ainherent%20in%20the%20problem%27s%20inputs%20and%20the%20wide%20variety%20of%20applications%2C%20which%0Aoften%20lead%20to%20use-case-tailored%20solutions.%20We%20propose%20a%20robust%20learning-free%0Aapproach%20to%20segment%20moving%20objects%20in%20point%20cloud%20data.%20The%20foundation%20of%20the%0Aapproach%20lies%20in%20modelling%20each%20voxel%20using%20a%20hidden%20Markov%20model%20%28HMM%29%2C%20and%0Aprobabilistically%20integrating%20beliefs%20into%20a%20map%20using%20an%20HMM%20filter.%20The%0Aproposed%20approach%20is%20tested%20on%20benchmark%20datasets%20and%20consistently%20performs%0Abetter%20than%20or%20as%20well%20as%20state-of-the-art%20methods%20with%20strong%20generalized%0Aperformance%20across%20sensor%20characteristics%20and%20environments.%20The%20approach%20is%0Aopen-sourced%20at%20https%3A//github.com/vb44/HMM-MOS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18638v1&entry.124074799=Read"},
{"title": "ConceptDrift: Uncovering Biases through the Lens of Foundational Models", "author": "Cristian Daniel P\u0103duraru and Antonio B\u0103rb\u0103lau and Radu Filipescu and Andrei Liviu Nicolicioiu and Elena Burceanu", "abstract": "  Datasets and pre-trained models come with intrinsic biases. Most methods rely\non spotting them by analysing misclassified samples, in a semi-automated\nhuman-computer validation. In contrast, we propose ConceptDrift, a method which\nanalyzes the weights of a linear probe, learned on top a foundational model. We\ncapitalize on the weight update trajectory, which starts from the embedding of\nthe textual representation of the class, and proceeds to drift towards\nembeddings that disclose hidden biases. Different from prior work, with this\napproach we can pin-point unwanted correlations from a dataset, providing more\nthan just possible explanations for the wrong predictions. We empirically prove\nthe efficacy of our method, by significantly improving zero-shot performance\nwith biased-augmented prompting. Our method is not bounded to a single\nmodality, and we experiment in this work with both image (Waterbirds, CelebA,\nNico++) and text datasets (CivilComments).\n", "link": "http://arxiv.org/abs/2410.18970v1", "date": "2024-10-24", "relevancy": 2.2287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.56}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConceptDrift%3A%20Uncovering%20Biases%20through%20the%20Lens%20of%20Foundational%20Models&body=Title%3A%20ConceptDrift%3A%20Uncovering%20Biases%20through%20the%20Lens%20of%20Foundational%20Models%0AAuthor%3A%20Cristian%20Daniel%20P%C4%83duraru%20and%20Antonio%20B%C4%83rb%C4%83lau%20and%20Radu%20Filipescu%20and%20Andrei%20Liviu%20Nicolicioiu%20and%20Elena%20Burceanu%0AAbstract%3A%20%20%20Datasets%20and%20pre-trained%20models%20come%20with%20intrinsic%20biases.%20Most%20methods%20rely%0Aon%20spotting%20them%20by%20analysing%20misclassified%20samples%2C%20in%20a%20semi-automated%0Ahuman-computer%20validation.%20In%20contrast%2C%20we%20propose%20ConceptDrift%2C%20a%20method%20which%0Aanalyzes%20the%20weights%20of%20a%20linear%20probe%2C%20learned%20on%20top%20a%20foundational%20model.%20We%0Acapitalize%20on%20the%20weight%20update%20trajectory%2C%20which%20starts%20from%20the%20embedding%20of%0Athe%20textual%20representation%20of%20the%20class%2C%20and%20proceeds%20to%20drift%20towards%0Aembeddings%20that%20disclose%20hidden%20biases.%20Different%20from%20prior%20work%2C%20with%20this%0Aapproach%20we%20can%20pin-point%20unwanted%20correlations%20from%20a%20dataset%2C%20providing%20more%0Athan%20just%20possible%20explanations%20for%20the%20wrong%20predictions.%20We%20empirically%20prove%0Athe%20efficacy%20of%20our%20method%2C%20by%20significantly%20improving%20zero-shot%20performance%0Awith%20biased-augmented%20prompting.%20Our%20method%20is%20not%20bounded%20to%20a%20single%0Amodality%2C%20and%20we%20experiment%20in%20this%20work%20with%20both%20image%20%28Waterbirds%2C%20CelebA%2C%0ANico%2B%2B%29%20and%20text%20datasets%20%28CivilComments%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptDrift%253A%2520Uncovering%2520Biases%2520through%2520the%2520Lens%2520of%2520Foundational%2520Models%26entry.906535625%3DCristian%2520Daniel%2520P%25C4%2583duraru%2520and%2520Antonio%2520B%25C4%2583rb%25C4%2583lau%2520and%2520Radu%2520Filipescu%2520and%2520Andrei%2520Liviu%2520Nicolicioiu%2520and%2520Elena%2520Burceanu%26entry.1292438233%3D%2520%2520Datasets%2520and%2520pre-trained%2520models%2520come%2520with%2520intrinsic%2520biases.%2520Most%2520methods%2520rely%250Aon%2520spotting%2520them%2520by%2520analysing%2520misclassified%2520samples%252C%2520in%2520a%2520semi-automated%250Ahuman-computer%2520validation.%2520In%2520contrast%252C%2520we%2520propose%2520ConceptDrift%252C%2520a%2520method%2520which%250Aanalyzes%2520the%2520weights%2520of%2520a%2520linear%2520probe%252C%2520learned%2520on%2520top%2520a%2520foundational%2520model.%2520We%250Acapitalize%2520on%2520the%2520weight%2520update%2520trajectory%252C%2520which%2520starts%2520from%2520the%2520embedding%2520of%250Athe%2520textual%2520representation%2520of%2520the%2520class%252C%2520and%2520proceeds%2520to%2520drift%2520towards%250Aembeddings%2520that%2520disclose%2520hidden%2520biases.%2520Different%2520from%2520prior%2520work%252C%2520with%2520this%250Aapproach%2520we%2520can%2520pin-point%2520unwanted%2520correlations%2520from%2520a%2520dataset%252C%2520providing%2520more%250Athan%2520just%2520possible%2520explanations%2520for%2520the%2520wrong%2520predictions.%2520We%2520empirically%2520prove%250Athe%2520efficacy%2520of%2520our%2520method%252C%2520by%2520significantly%2520improving%2520zero-shot%2520performance%250Awith%2520biased-augmented%2520prompting.%2520Our%2520method%2520is%2520not%2520bounded%2520to%2520a%2520single%250Amodality%252C%2520and%2520we%2520experiment%2520in%2520this%2520work%2520with%2520both%2520image%2520%2528Waterbirds%252C%2520CelebA%252C%250ANico%252B%252B%2529%2520and%2520text%2520datasets%2520%2528CivilComments%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConceptDrift%3A%20Uncovering%20Biases%20through%20the%20Lens%20of%20Foundational%20Models&entry.906535625=Cristian%20Daniel%20P%C4%83duraru%20and%20Antonio%20B%C4%83rb%C4%83lau%20and%20Radu%20Filipescu%20and%20Andrei%20Liviu%20Nicolicioiu%20and%20Elena%20Burceanu&entry.1292438233=%20%20Datasets%20and%20pre-trained%20models%20come%20with%20intrinsic%20biases.%20Most%20methods%20rely%0Aon%20spotting%20them%20by%20analysing%20misclassified%20samples%2C%20in%20a%20semi-automated%0Ahuman-computer%20validation.%20In%20contrast%2C%20we%20propose%20ConceptDrift%2C%20a%20method%20which%0Aanalyzes%20the%20weights%20of%20a%20linear%20probe%2C%20learned%20on%20top%20a%20foundational%20model.%20We%0Acapitalize%20on%20the%20weight%20update%20trajectory%2C%20which%20starts%20from%20the%20embedding%20of%0Athe%20textual%20representation%20of%20the%20class%2C%20and%20proceeds%20to%20drift%20towards%0Aembeddings%20that%20disclose%20hidden%20biases.%20Different%20from%20prior%20work%2C%20with%20this%0Aapproach%20we%20can%20pin-point%20unwanted%20correlations%20from%20a%20dataset%2C%20providing%20more%0Athan%20just%20possible%20explanations%20for%20the%20wrong%20predictions.%20We%20empirically%20prove%0Athe%20efficacy%20of%20our%20method%2C%20by%20significantly%20improving%20zero-shot%20performance%0Awith%20biased-augmented%20prompting.%20Our%20method%20is%20not%20bounded%20to%20a%20single%0Amodality%2C%20and%20we%20experiment%20in%20this%20work%20with%20both%20image%20%28Waterbirds%2C%20CelebA%2C%0ANico%2B%2B%29%20and%20text%20datasets%20%28CivilComments%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18970v1&entry.124074799=Read"},
{"title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate\n  Hallucinations", "author": "Aryo Pradipta Gema and Chen Jin and Ahmed Abdulaal and Tom Diethe and Philip Teare and Beatrice Alex and Pasquale Minervini and Amrutha Saseendran", "abstract": "  Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%).\n", "link": "http://arxiv.org/abs/2410.18860v1", "date": "2024-10-24", "relevancy": 2.226, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCoRe%3A%20Decoding%20by%20Contrasting%20Retrieval%20Heads%20to%20Mitigate%0A%20%20Hallucinations&body=Title%3A%20DeCoRe%3A%20Decoding%20by%20Contrasting%20Retrieval%20Heads%20to%20Mitigate%0A%20%20Hallucinations%0AAuthor%3A%20Aryo%20Pradipta%20Gema%20and%20Chen%20Jin%20and%20Ahmed%20Abdulaal%20and%20Tom%20Diethe%20and%20Philip%20Teare%20and%20Beatrice%20Alex%20and%20Pasquale%20Minervini%20and%20Amrutha%20Saseendran%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20hallucinate%2C%20producing%20unfaithful%20or%0Afactually%20incorrect%20outputs%20by%20misrepresenting%20the%20provided%20context%20or%0Aincorrectly%20recalling%20internal%20knowledge.%20Recent%20studies%20have%20identified%0Aspecific%20attention%20heads%20within%20the%20Transformer%20architecture%2C%20known%20as%0Aretrieval%20heads%2C%20responsible%20for%20extracting%20relevant%20contextual%20information.%20We%0Ahypothesise%20that%20masking%20these%20retrieval%20heads%20can%20induce%20hallucinations%20and%0Athat%20contrasting%20the%20outputs%20of%20the%20base%20LLM%20and%20the%20masked%20LLM%20can%20reduce%0Ahallucinations.%20To%20this%20end%2C%20we%20propose%20Decoding%20by%20Contrasting%20Retrieval%20Heads%0A%28DeCoRe%29%2C%20a%20novel%20training-free%20decoding%20strategy%20that%20amplifies%20information%0Afound%20in%20the%20context%20and%20model%20parameters.%20DeCoRe%20mitigates%20potentially%0Ahallucinated%20responses%20by%20dynamically%20contrasting%20the%20outputs%20of%20the%20base%20LLM%0Aand%20the%20masked%20LLM%2C%20using%20conditional%20entropy%20as%20a%20guide.%20Our%20extensive%0Aexperiments%20confirm%20that%20DeCoRe%20significantly%20improves%20performance%20on%20tasks%0Arequiring%20high%20contextual%20faithfulness%2C%20such%20as%20summarisation%20%28XSum%20by%2018.6%25%29%2C%0Ainstruction%20following%20%28MemoTrap%20by%2010.9%25%29%2C%20and%20open-book%20question%20answering%0A%28NQ-Open%20by%202.4%25%20and%20NQ-Swap%20by%205.5%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCoRe%253A%2520Decoding%2520by%2520Contrasting%2520Retrieval%2520Heads%2520to%2520Mitigate%250A%2520%2520Hallucinations%26entry.906535625%3DAryo%2520Pradipta%2520Gema%2520and%2520Chen%2520Jin%2520and%2520Ahmed%2520Abdulaal%2520and%2520Tom%2520Diethe%2520and%2520Philip%2520Teare%2520and%2520Beatrice%2520Alex%2520and%2520Pasquale%2520Minervini%2520and%2520Amrutha%2520Saseendran%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520hallucinate%252C%2520producing%2520unfaithful%2520or%250Afactually%2520incorrect%2520outputs%2520by%2520misrepresenting%2520the%2520provided%2520context%2520or%250Aincorrectly%2520recalling%2520internal%2520knowledge.%2520Recent%2520studies%2520have%2520identified%250Aspecific%2520attention%2520heads%2520within%2520the%2520Transformer%2520architecture%252C%2520known%2520as%250Aretrieval%2520heads%252C%2520responsible%2520for%2520extracting%2520relevant%2520contextual%2520information.%2520We%250Ahypothesise%2520that%2520masking%2520these%2520retrieval%2520heads%2520can%2520induce%2520hallucinations%2520and%250Athat%2520contrasting%2520the%2520outputs%2520of%2520the%2520base%2520LLM%2520and%2520the%2520masked%2520LLM%2520can%2520reduce%250Ahallucinations.%2520To%2520this%2520end%252C%2520we%2520propose%2520Decoding%2520by%2520Contrasting%2520Retrieval%2520Heads%250A%2528DeCoRe%2529%252C%2520a%2520novel%2520training-free%2520decoding%2520strategy%2520that%2520amplifies%2520information%250Afound%2520in%2520the%2520context%2520and%2520model%2520parameters.%2520DeCoRe%2520mitigates%2520potentially%250Ahallucinated%2520responses%2520by%2520dynamically%2520contrasting%2520the%2520outputs%2520of%2520the%2520base%2520LLM%250Aand%2520the%2520masked%2520LLM%252C%2520using%2520conditional%2520entropy%2520as%2520a%2520guide.%2520Our%2520extensive%250Aexperiments%2520confirm%2520that%2520DeCoRe%2520significantly%2520improves%2520performance%2520on%2520tasks%250Arequiring%2520high%2520contextual%2520faithfulness%252C%2520such%2520as%2520summarisation%2520%2528XSum%2520by%252018.6%2525%2529%252C%250Ainstruction%2520following%2520%2528MemoTrap%2520by%252010.9%2525%2529%252C%2520and%2520open-book%2520question%2520answering%250A%2528NQ-Open%2520by%25202.4%2525%2520and%2520NQ-Swap%2520by%25205.5%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCoRe%3A%20Decoding%20by%20Contrasting%20Retrieval%20Heads%20to%20Mitigate%0A%20%20Hallucinations&entry.906535625=Aryo%20Pradipta%20Gema%20and%20Chen%20Jin%20and%20Ahmed%20Abdulaal%20and%20Tom%20Diethe%20and%20Philip%20Teare%20and%20Beatrice%20Alex%20and%20Pasquale%20Minervini%20and%20Amrutha%20Saseendran&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20hallucinate%2C%20producing%20unfaithful%20or%0Afactually%20incorrect%20outputs%20by%20misrepresenting%20the%20provided%20context%20or%0Aincorrectly%20recalling%20internal%20knowledge.%20Recent%20studies%20have%20identified%0Aspecific%20attention%20heads%20within%20the%20Transformer%20architecture%2C%20known%20as%0Aretrieval%20heads%2C%20responsible%20for%20extracting%20relevant%20contextual%20information.%20We%0Ahypothesise%20that%20masking%20these%20retrieval%20heads%20can%20induce%20hallucinations%20and%0Athat%20contrasting%20the%20outputs%20of%20the%20base%20LLM%20and%20the%20masked%20LLM%20can%20reduce%0Ahallucinations.%20To%20this%20end%2C%20we%20propose%20Decoding%20by%20Contrasting%20Retrieval%20Heads%0A%28DeCoRe%29%2C%20a%20novel%20training-free%20decoding%20strategy%20that%20amplifies%20information%0Afound%20in%20the%20context%20and%20model%20parameters.%20DeCoRe%20mitigates%20potentially%0Ahallucinated%20responses%20by%20dynamically%20contrasting%20the%20outputs%20of%20the%20base%20LLM%0Aand%20the%20masked%20LLM%2C%20using%20conditional%20entropy%20as%20a%20guide.%20Our%20extensive%0Aexperiments%20confirm%20that%20DeCoRe%20significantly%20improves%20performance%20on%20tasks%0Arequiring%20high%20contextual%20faithfulness%2C%20such%20as%20summarisation%20%28XSum%20by%2018.6%25%29%2C%0Ainstruction%20following%20%28MemoTrap%20by%2010.9%25%29%2C%20and%20open-book%20question%20answering%0A%28NQ-Open%20by%202.4%25%20and%20NQ-Swap%20by%205.5%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18860v1&entry.124074799=Read"},
{"title": "Empowering Robot Path Planning with Large Language Models: osmAG Map\n  Topology & Hierarchy Comprehension with LLMs", "author": "Fujing Xie and S\u00f6ren Schwertfeger", "abstract": "  Large Language Models (LLMs) have demonstrated great potential in robotic\napplications by providing essential general knowledge. Mobile robots rely on\nmap comprehension for tasks like localization and navigation. In this paper, we\nexplore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a\ntext-based hierarchical, topometric semantic map representation utilizing\npolygons to demark areas such as rooms or buildings. Our experiments\ndemonstrate that with the right map representation, LLMs can effectively\ncomprehend Area Graph's topology and hierarchy. After straightforward\nfine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.\nOur dataset, dataset generation code, fine-tuned LoRA adapters can be accessed\nat https://github.com/xiefujing/LLM-osmAG-Comprehension.\n", "link": "http://arxiv.org/abs/2403.08228v3", "date": "2024-10-24", "relevancy": 2.2239, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5598}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20Robot%20Path%20Planning%20with%20Large%20Language%20Models%3A%20osmAG%20Map%0A%20%20Topology%20%26%20Hierarchy%20Comprehension%20with%20LLMs&body=Title%3A%20Empowering%20Robot%20Path%20Planning%20with%20Large%20Language%20Models%3A%20osmAG%20Map%0A%20%20Topology%20%26%20Hierarchy%20Comprehension%20with%20LLMs%0AAuthor%3A%20Fujing%20Xie%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20great%20potential%20in%20robotic%0Aapplications%20by%20providing%20essential%20general%20knowledge.%20Mobile%20robots%20rely%20on%0Amap%20comprehension%20for%20tasks%20like%20localization%20and%20navigation.%20In%20this%20paper%2C%20we%0Aexplore%20enabling%20LLMs%20to%20comprehend%20the%20topology%20and%20hierarchy%20of%20Area%20Graph%2C%20a%0Atext-based%20hierarchical%2C%20topometric%20semantic%20map%20representation%20utilizing%0Apolygons%20to%20demark%20areas%20such%20as%20rooms%20or%20buildings.%20Our%20experiments%0Ademonstrate%20that%20with%20the%20right%20map%20representation%2C%20LLMs%20can%20effectively%0Acomprehend%20Area%20Graph%27s%20topology%20and%20hierarchy.%20After%20straightforward%0Afine-tuning%2C%20the%20LLaMA2%20models%20exceeded%20ChatGPT-3.5%20in%20mastering%20these%20aspects.%0AOur%20dataset%2C%20dataset%20generation%20code%2C%20fine-tuned%20LoRA%20adapters%20can%20be%20accessed%0Aat%20https%3A//github.com/xiefujing/LLM-osmAG-Comprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08228v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520Robot%2520Path%2520Planning%2520with%2520Large%2520Language%2520Models%253A%2520osmAG%2520Map%250A%2520%2520Topology%2520%2526%2520Hierarchy%2520Comprehension%2520with%2520LLMs%26entry.906535625%3DFujing%2520Xie%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520great%2520potential%2520in%2520robotic%250Aapplications%2520by%2520providing%2520essential%2520general%2520knowledge.%2520Mobile%2520robots%2520rely%2520on%250Amap%2520comprehension%2520for%2520tasks%2520like%2520localization%2520and%2520navigation.%2520In%2520this%2520paper%252C%2520we%250Aexplore%2520enabling%2520LLMs%2520to%2520comprehend%2520the%2520topology%2520and%2520hierarchy%2520of%2520Area%2520Graph%252C%2520a%250Atext-based%2520hierarchical%252C%2520topometric%2520semantic%2520map%2520representation%2520utilizing%250Apolygons%2520to%2520demark%2520areas%2520such%2520as%2520rooms%2520or%2520buildings.%2520Our%2520experiments%250Ademonstrate%2520that%2520with%2520the%2520right%2520map%2520representation%252C%2520LLMs%2520can%2520effectively%250Acomprehend%2520Area%2520Graph%2527s%2520topology%2520and%2520hierarchy.%2520After%2520straightforward%250Afine-tuning%252C%2520the%2520LLaMA2%2520models%2520exceeded%2520ChatGPT-3.5%2520in%2520mastering%2520these%2520aspects.%250AOur%2520dataset%252C%2520dataset%2520generation%2520code%252C%2520fine-tuned%2520LoRA%2520adapters%2520can%2520be%2520accessed%250Aat%2520https%253A//github.com/xiefujing/LLM-osmAG-Comprehension.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08228v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20Robot%20Path%20Planning%20with%20Large%20Language%20Models%3A%20osmAG%20Map%0A%20%20Topology%20%26%20Hierarchy%20Comprehension%20with%20LLMs&entry.906535625=Fujing%20Xie%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20great%20potential%20in%20robotic%0Aapplications%20by%20providing%20essential%20general%20knowledge.%20Mobile%20robots%20rely%20on%0Amap%20comprehension%20for%20tasks%20like%20localization%20and%20navigation.%20In%20this%20paper%2C%20we%0Aexplore%20enabling%20LLMs%20to%20comprehend%20the%20topology%20and%20hierarchy%20of%20Area%20Graph%2C%20a%0Atext-based%20hierarchical%2C%20topometric%20semantic%20map%20representation%20utilizing%0Apolygons%20to%20demark%20areas%20such%20as%20rooms%20or%20buildings.%20Our%20experiments%0Ademonstrate%20that%20with%20the%20right%20map%20representation%2C%20LLMs%20can%20effectively%0Acomprehend%20Area%20Graph%27s%20topology%20and%20hierarchy.%20After%20straightforward%0Afine-tuning%2C%20the%20LLaMA2%20models%20exceeded%20ChatGPT-3.5%20in%20mastering%20these%20aspects.%0AOur%20dataset%2C%20dataset%20generation%20code%2C%20fine-tuned%20LoRA%20adapters%20can%20be%20accessed%0Aat%20https%3A//github.com/xiefujing/LLM-osmAG-Comprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08228v3&entry.124074799=Read"},
{"title": "Neural-Rendezvous: Provably Robust Guidance and Control to Encounter\n  Interstellar Objects", "author": "Hiroyasu Tsukamoto and Soon-Jo Chung and Yashwanth Kumar Nakka and Benjamin Donitz and Declan Mages and Michel Ingham", "abstract": "  Interstellar objects (ISOs) are likely representatives of primitive materials\ninvaluable in understanding exoplanetary star systems. Due to their poorly\nconstrained orbits with generally high inclinations and relative velocities,\nhowever, exploring ISOs with conventional human-in-the-loop approaches is\nsignificantly challenging. This paper presents Neural-Rendezvous -- a deep\nlearning-based guidance and control framework for encountering fast-moving\nobjects, including ISOs, robustly, accurately, and autonomously in real time.\nIt uses pointwise minimum norm tracking control on top of a guidance policy\nmodeled by a spectrally-normalized deep neural network, where its\nhyperparameters are tuned with a loss function directly penalizing the MPC\nstate trajectory tracking error. We show that Neural-Rendezvous provides a high\nprobability exponential bound on the expected spacecraft delivery error, the\nproof of which leverages stochastic incremental stability analysis. In\nparticular, it is used to construct a non-negative function with a\nsupermartingale property, explicitly accounting for the ISO state uncertainty\nand the local nature of nonlinear state estimation guarantees. In numerical\nsimulations, Neural-Rendezvous is demonstrated to satisfy the expected error\nbound for 100 ISO candidates. This performance is also empirically validated\nusing our spacecraft simulator and in high-conflict and distributed UAV swarm\nreconfiguration with up to 20 UAVs.\n", "link": "http://arxiv.org/abs/2208.04883v3", "date": "2024-10-24", "relevancy": 2.2174, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5771}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5626}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural-Rendezvous%3A%20Provably%20Robust%20Guidance%20and%20Control%20to%20Encounter%0A%20%20Interstellar%20Objects&body=Title%3A%20Neural-Rendezvous%3A%20Provably%20Robust%20Guidance%20and%20Control%20to%20Encounter%0A%20%20Interstellar%20Objects%0AAuthor%3A%20Hiroyasu%20Tsukamoto%20and%20Soon-Jo%20Chung%20and%20Yashwanth%20Kumar%20Nakka%20and%20Benjamin%20Donitz%20and%20Declan%20Mages%20and%20Michel%20Ingham%0AAbstract%3A%20%20%20Interstellar%20objects%20%28ISOs%29%20are%20likely%20representatives%20of%20primitive%20materials%0Ainvaluable%20in%20understanding%20exoplanetary%20star%20systems.%20Due%20to%20their%20poorly%0Aconstrained%20orbits%20with%20generally%20high%20inclinations%20and%20relative%20velocities%2C%0Ahowever%2C%20exploring%20ISOs%20with%20conventional%20human-in-the-loop%20approaches%20is%0Asignificantly%20challenging.%20This%20paper%20presents%20Neural-Rendezvous%20--%20a%20deep%0Alearning-based%20guidance%20and%20control%20framework%20for%20encountering%20fast-moving%0Aobjects%2C%20including%20ISOs%2C%20robustly%2C%20accurately%2C%20and%20autonomously%20in%20real%20time.%0AIt%20uses%20pointwise%20minimum%20norm%20tracking%20control%20on%20top%20of%20a%20guidance%20policy%0Amodeled%20by%20a%20spectrally-normalized%20deep%20neural%20network%2C%20where%20its%0Ahyperparameters%20are%20tuned%20with%20a%20loss%20function%20directly%20penalizing%20the%20MPC%0Astate%20trajectory%20tracking%20error.%20We%20show%20that%20Neural-Rendezvous%20provides%20a%20high%0Aprobability%20exponential%20bound%20on%20the%20expected%20spacecraft%20delivery%20error%2C%20the%0Aproof%20of%20which%20leverages%20stochastic%20incremental%20stability%20analysis.%20In%0Aparticular%2C%20it%20is%20used%20to%20construct%20a%20non-negative%20function%20with%20a%0Asupermartingale%20property%2C%20explicitly%20accounting%20for%20the%20ISO%20state%20uncertainty%0Aand%20the%20local%20nature%20of%20nonlinear%20state%20estimation%20guarantees.%20In%20numerical%0Asimulations%2C%20Neural-Rendezvous%20is%20demonstrated%20to%20satisfy%20the%20expected%20error%0Abound%20for%20100%20ISO%20candidates.%20This%20performance%20is%20also%20empirically%20validated%0Ausing%20our%20spacecraft%20simulator%20and%20in%20high-conflict%20and%20distributed%20UAV%20swarm%0Areconfiguration%20with%20up%20to%2020%20UAVs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.04883v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural-Rendezvous%253A%2520Provably%2520Robust%2520Guidance%2520and%2520Control%2520to%2520Encounter%250A%2520%2520Interstellar%2520Objects%26entry.906535625%3DHiroyasu%2520Tsukamoto%2520and%2520Soon-Jo%2520Chung%2520and%2520Yashwanth%2520Kumar%2520Nakka%2520and%2520Benjamin%2520Donitz%2520and%2520Declan%2520Mages%2520and%2520Michel%2520Ingham%26entry.1292438233%3D%2520%2520Interstellar%2520objects%2520%2528ISOs%2529%2520are%2520likely%2520representatives%2520of%2520primitive%2520materials%250Ainvaluable%2520in%2520understanding%2520exoplanetary%2520star%2520systems.%2520Due%2520to%2520their%2520poorly%250Aconstrained%2520orbits%2520with%2520generally%2520high%2520inclinations%2520and%2520relative%2520velocities%252C%250Ahowever%252C%2520exploring%2520ISOs%2520with%2520conventional%2520human-in-the-loop%2520approaches%2520is%250Asignificantly%2520challenging.%2520This%2520paper%2520presents%2520Neural-Rendezvous%2520--%2520a%2520deep%250Alearning-based%2520guidance%2520and%2520control%2520framework%2520for%2520encountering%2520fast-moving%250Aobjects%252C%2520including%2520ISOs%252C%2520robustly%252C%2520accurately%252C%2520and%2520autonomously%2520in%2520real%2520time.%250AIt%2520uses%2520pointwise%2520minimum%2520norm%2520tracking%2520control%2520on%2520top%2520of%2520a%2520guidance%2520policy%250Amodeled%2520by%2520a%2520spectrally-normalized%2520deep%2520neural%2520network%252C%2520where%2520its%250Ahyperparameters%2520are%2520tuned%2520with%2520a%2520loss%2520function%2520directly%2520penalizing%2520the%2520MPC%250Astate%2520trajectory%2520tracking%2520error.%2520We%2520show%2520that%2520Neural-Rendezvous%2520provides%2520a%2520high%250Aprobability%2520exponential%2520bound%2520on%2520the%2520expected%2520spacecraft%2520delivery%2520error%252C%2520the%250Aproof%2520of%2520which%2520leverages%2520stochastic%2520incremental%2520stability%2520analysis.%2520In%250Aparticular%252C%2520it%2520is%2520used%2520to%2520construct%2520a%2520non-negative%2520function%2520with%2520a%250Asupermartingale%2520property%252C%2520explicitly%2520accounting%2520for%2520the%2520ISO%2520state%2520uncertainty%250Aand%2520the%2520local%2520nature%2520of%2520nonlinear%2520state%2520estimation%2520guarantees.%2520In%2520numerical%250Asimulations%252C%2520Neural-Rendezvous%2520is%2520demonstrated%2520to%2520satisfy%2520the%2520expected%2520error%250Abound%2520for%2520100%2520ISO%2520candidates.%2520This%2520performance%2520is%2520also%2520empirically%2520validated%250Ausing%2520our%2520spacecraft%2520simulator%2520and%2520in%2520high-conflict%2520and%2520distributed%2520UAV%2520swarm%250Areconfiguration%2520with%2520up%2520to%252020%2520UAVs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.04883v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural-Rendezvous%3A%20Provably%20Robust%20Guidance%20and%20Control%20to%20Encounter%0A%20%20Interstellar%20Objects&entry.906535625=Hiroyasu%20Tsukamoto%20and%20Soon-Jo%20Chung%20and%20Yashwanth%20Kumar%20Nakka%20and%20Benjamin%20Donitz%20and%20Declan%20Mages%20and%20Michel%20Ingham&entry.1292438233=%20%20Interstellar%20objects%20%28ISOs%29%20are%20likely%20representatives%20of%20primitive%20materials%0Ainvaluable%20in%20understanding%20exoplanetary%20star%20systems.%20Due%20to%20their%20poorly%0Aconstrained%20orbits%20with%20generally%20high%20inclinations%20and%20relative%20velocities%2C%0Ahowever%2C%20exploring%20ISOs%20with%20conventional%20human-in-the-loop%20approaches%20is%0Asignificantly%20challenging.%20This%20paper%20presents%20Neural-Rendezvous%20--%20a%20deep%0Alearning-based%20guidance%20and%20control%20framework%20for%20encountering%20fast-moving%0Aobjects%2C%20including%20ISOs%2C%20robustly%2C%20accurately%2C%20and%20autonomously%20in%20real%20time.%0AIt%20uses%20pointwise%20minimum%20norm%20tracking%20control%20on%20top%20of%20a%20guidance%20policy%0Amodeled%20by%20a%20spectrally-normalized%20deep%20neural%20network%2C%20where%20its%0Ahyperparameters%20are%20tuned%20with%20a%20loss%20function%20directly%20penalizing%20the%20MPC%0Astate%20trajectory%20tracking%20error.%20We%20show%20that%20Neural-Rendezvous%20provides%20a%20high%0Aprobability%20exponential%20bound%20on%20the%20expected%20spacecraft%20delivery%20error%2C%20the%0Aproof%20of%20which%20leverages%20stochastic%20incremental%20stability%20analysis.%20In%0Aparticular%2C%20it%20is%20used%20to%20construct%20a%20non-negative%20function%20with%20a%0Asupermartingale%20property%2C%20explicitly%20accounting%20for%20the%20ISO%20state%20uncertainty%0Aand%20the%20local%20nature%20of%20nonlinear%20state%20estimation%20guarantees.%20In%20numerical%0Asimulations%2C%20Neural-Rendezvous%20is%20demonstrated%20to%20satisfy%20the%20expected%20error%0Abound%20for%20100%20ISO%20candidates.%20This%20performance%20is%20also%20empirically%20validated%0Ausing%20our%20spacecraft%20simulator%20and%20in%20high-conflict%20and%20distributed%20UAV%20swarm%0Areconfiguration%20with%20up%20to%2020%20UAVs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.04883v3&entry.124074799=Read"},
{"title": "DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image\n  Semantic Segmentation with Unsupervised Domain Adaptation", "author": "Lingyan Ran and Lushuang Wang and Tao Zhuo and Yinghui Xing", "abstract": "  Semantic segmentation of remote sensing images is a challenging and hot issue\ndue to the large amount of unlabeled data. Unsupervised domain adaptation (UDA)\nhas proven to be advantageous in incorporating unclassified information from\nthe target domain. However, independently fine-tuning UDA models on the source\nand target domains has a limited effect on the outcome. This paper proposes a\nhybrid training strategy as well as a novel dual-domain image fusion strategy\nthat effectively utilizes the original image, transformation image, and\nintermediate domain information. Moreover, to enhance the precision of\npseudo-labels, we present a pseudo-label region-specific weight strategy. The\nefficacy of our approach is substantiated by extensive benchmark experiments\nand ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets.\n", "link": "http://arxiv.org/abs/2403.02784v2", "date": "2024-10-24", "relevancy": 2.1813, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5614}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.545}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DDF%3A%20A%20Novel%20Dual-Domain%20Image%20Fusion%20Strategy%20for%20Remote%20Sensing%20Image%0A%20%20Semantic%20Segmentation%20with%20Unsupervised%20Domain%20Adaptation&body=Title%3A%20DDF%3A%20A%20Novel%20Dual-Domain%20Image%20Fusion%20Strategy%20for%20Remote%20Sensing%20Image%0A%20%20Semantic%20Segmentation%20with%20Unsupervised%20Domain%20Adaptation%0AAuthor%3A%20Lingyan%20Ran%20and%20Lushuang%20Wang%20and%20Tao%20Zhuo%20and%20Yinghui%20Xing%0AAbstract%3A%20%20%20Semantic%20segmentation%20of%20remote%20sensing%20images%20is%20a%20challenging%20and%20hot%20issue%0Adue%20to%20the%20large%20amount%20of%20unlabeled%20data.%20Unsupervised%20domain%20adaptation%20%28UDA%29%0Ahas%20proven%20to%20be%20advantageous%20in%20incorporating%20unclassified%20information%20from%0Athe%20target%20domain.%20However%2C%20independently%20fine-tuning%20UDA%20models%20on%20the%20source%0Aand%20target%20domains%20has%20a%20limited%20effect%20on%20the%20outcome.%20This%20paper%20proposes%20a%0Ahybrid%20training%20strategy%20as%20well%20as%20a%20novel%20dual-domain%20image%20fusion%20strategy%0Athat%20effectively%20utilizes%20the%20original%20image%2C%20transformation%20image%2C%20and%0Aintermediate%20domain%20information.%20Moreover%2C%20to%20enhance%20the%20precision%20of%0Apseudo-labels%2C%20we%20present%20a%20pseudo-label%20region-specific%20weight%20strategy.%20The%0Aefficacy%20of%20our%20approach%20is%20substantiated%20by%20extensive%20benchmark%20experiments%0Aand%20ablation%20studies%20conducted%20on%20the%20ISPRS%20Vaihingen%20and%20Potsdam%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDDF%253A%2520A%2520Novel%2520Dual-Domain%2520Image%2520Fusion%2520Strategy%2520for%2520Remote%2520Sensing%2520Image%250A%2520%2520Semantic%2520Segmentation%2520with%2520Unsupervised%2520Domain%2520Adaptation%26entry.906535625%3DLingyan%2520Ran%2520and%2520Lushuang%2520Wang%2520and%2520Tao%2520Zhuo%2520and%2520Yinghui%2520Xing%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520of%2520remote%2520sensing%2520images%2520is%2520a%2520challenging%2520and%2520hot%2520issue%250Adue%2520to%2520the%2520large%2520amount%2520of%2520unlabeled%2520data.%2520Unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%250Ahas%2520proven%2520to%2520be%2520advantageous%2520in%2520incorporating%2520unclassified%2520information%2520from%250Athe%2520target%2520domain.%2520However%252C%2520independently%2520fine-tuning%2520UDA%2520models%2520on%2520the%2520source%250Aand%2520target%2520domains%2520has%2520a%2520limited%2520effect%2520on%2520the%2520outcome.%2520This%2520paper%2520proposes%2520a%250Ahybrid%2520training%2520strategy%2520as%2520well%2520as%2520a%2520novel%2520dual-domain%2520image%2520fusion%2520strategy%250Athat%2520effectively%2520utilizes%2520the%2520original%2520image%252C%2520transformation%2520image%252C%2520and%250Aintermediate%2520domain%2520information.%2520Moreover%252C%2520to%2520enhance%2520the%2520precision%2520of%250Apseudo-labels%252C%2520we%2520present%2520a%2520pseudo-label%2520region-specific%2520weight%2520strategy.%2520The%250Aefficacy%2520of%2520our%2520approach%2520is%2520substantiated%2520by%2520extensive%2520benchmark%2520experiments%250Aand%2520ablation%2520studies%2520conducted%2520on%2520the%2520ISPRS%2520Vaihingen%2520and%2520Potsdam%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDF%3A%20A%20Novel%20Dual-Domain%20Image%20Fusion%20Strategy%20for%20Remote%20Sensing%20Image%0A%20%20Semantic%20Segmentation%20with%20Unsupervised%20Domain%20Adaptation&entry.906535625=Lingyan%20Ran%20and%20Lushuang%20Wang%20and%20Tao%20Zhuo%20and%20Yinghui%20Xing&entry.1292438233=%20%20Semantic%20segmentation%20of%20remote%20sensing%20images%20is%20a%20challenging%20and%20hot%20issue%0Adue%20to%20the%20large%20amount%20of%20unlabeled%20data.%20Unsupervised%20domain%20adaptation%20%28UDA%29%0Ahas%20proven%20to%20be%20advantageous%20in%20incorporating%20unclassified%20information%20from%0Athe%20target%20domain.%20However%2C%20independently%20fine-tuning%20UDA%20models%20on%20the%20source%0Aand%20target%20domains%20has%20a%20limited%20effect%20on%20the%20outcome.%20This%20paper%20proposes%20a%0Ahybrid%20training%20strategy%20as%20well%20as%20a%20novel%20dual-domain%20image%20fusion%20strategy%0Athat%20effectively%20utilizes%20the%20original%20image%2C%20transformation%20image%2C%20and%0Aintermediate%20domain%20information.%20Moreover%2C%20to%20enhance%20the%20precision%20of%0Apseudo-labels%2C%20we%20present%20a%20pseudo-label%20region-specific%20weight%20strategy.%20The%0Aefficacy%20of%20our%20approach%20is%20substantiated%20by%20extensive%20benchmark%20experiments%0Aand%20ablation%20studies%20conducted%20on%20the%20ISPRS%20Vaihingen%20and%20Potsdam%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02784v2&entry.124074799=Read"},
{"title": "Framer: Interactive Frame Interpolation", "author": "Wen Wang and Qiuyu Wang and Kecheng Zheng and Hao Ouyang and Zhekai Chen and Biao Gong and Hao Chen and Yujun Shen and Chunhua Shen", "abstract": "  We propose Framer for interactive frame interpolation, which targets\nproducing smoothly transitioning frames between two images as per user\ncreativity. Concretely, besides taking the start and end frames as inputs, our\napproach supports customizing the transition process by tailoring the\ntrajectory of some selected keypoints. Such a design enjoys two clear benefits.\nFirst, incorporating human interaction mitigates the issue arising from\nnumerous possibilities of transforming one image to another, and in turn\nenables finer control of local motions. Second, as the most basic form of\ninteraction, keypoints help establish the correspondence across frames,\nenhancing the model to handle challenging cases (e.g., objects on the start and\nend frames are of different shapes and styles). It is noteworthy that our\nsystem also offers an \"autopilot\" mode, where we introduce a module to estimate\nthe keypoints and refine the trajectory automatically, to simplify the usage in\npractice. Extensive experimental results demonstrate the appealing performance\nof Framer on various applications, such as image morphing, time-lapse video\ngeneration, cartoon interpolation, etc. The code, the model, and the interface\nwill be released to facilitate further research.\n", "link": "http://arxiv.org/abs/2410.18978v1", "date": "2024-10-24", "relevancy": 2.1731, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5499}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5456}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5357}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Framer%3A%20Interactive%20Frame%20Interpolation&body=Title%3A%20Framer%3A%20Interactive%20Frame%20Interpolation%0AAuthor%3A%20Wen%20Wang%20and%20Qiuyu%20Wang%20and%20Kecheng%20Zheng%20and%20Hao%20Ouyang%20and%20Zhekai%20Chen%20and%20Biao%20Gong%20and%20Hao%20Chen%20and%20Yujun%20Shen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20We%20propose%20Framer%20for%20interactive%20frame%20interpolation%2C%20which%20targets%0Aproducing%20smoothly%20transitioning%20frames%20between%20two%20images%20as%20per%20user%0Acreativity.%20Concretely%2C%20besides%20taking%20the%20start%20and%20end%20frames%20as%20inputs%2C%20our%0Aapproach%20supports%20customizing%20the%20transition%20process%20by%20tailoring%20the%0Atrajectory%20of%20some%20selected%20keypoints.%20Such%20a%20design%20enjoys%20two%20clear%20benefits.%0AFirst%2C%20incorporating%20human%20interaction%20mitigates%20the%20issue%20arising%20from%0Anumerous%20possibilities%20of%20transforming%20one%20image%20to%20another%2C%20and%20in%20turn%0Aenables%20finer%20control%20of%20local%20motions.%20Second%2C%20as%20the%20most%20basic%20form%20of%0Ainteraction%2C%20keypoints%20help%20establish%20the%20correspondence%20across%20frames%2C%0Aenhancing%20the%20model%20to%20handle%20challenging%20cases%20%28e.g.%2C%20objects%20on%20the%20start%20and%0Aend%20frames%20are%20of%20different%20shapes%20and%20styles%29.%20It%20is%20noteworthy%20that%20our%0Asystem%20also%20offers%20an%20%22autopilot%22%20mode%2C%20where%20we%20introduce%20a%20module%20to%20estimate%0Athe%20keypoints%20and%20refine%20the%20trajectory%20automatically%2C%20to%20simplify%20the%20usage%20in%0Apractice.%20Extensive%20experimental%20results%20demonstrate%20the%20appealing%20performance%0Aof%20Framer%20on%20various%20applications%2C%20such%20as%20image%20morphing%2C%20time-lapse%20video%0Ageneration%2C%20cartoon%20interpolation%2C%20etc.%20The%20code%2C%20the%20model%2C%20and%20the%20interface%0Awill%20be%20released%20to%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFramer%253A%2520Interactive%2520Frame%2520Interpolation%26entry.906535625%3DWen%2520Wang%2520and%2520Qiuyu%2520Wang%2520and%2520Kecheng%2520Zheng%2520and%2520Hao%2520Ouyang%2520and%2520Zhekai%2520Chen%2520and%2520Biao%2520Gong%2520and%2520Hao%2520Chen%2520and%2520Yujun%2520Shen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520We%2520propose%2520Framer%2520for%2520interactive%2520frame%2520interpolation%252C%2520which%2520targets%250Aproducing%2520smoothly%2520transitioning%2520frames%2520between%2520two%2520images%2520as%2520per%2520user%250Acreativity.%2520Concretely%252C%2520besides%2520taking%2520the%2520start%2520and%2520end%2520frames%2520as%2520inputs%252C%2520our%250Aapproach%2520supports%2520customizing%2520the%2520transition%2520process%2520by%2520tailoring%2520the%250Atrajectory%2520of%2520some%2520selected%2520keypoints.%2520Such%2520a%2520design%2520enjoys%2520two%2520clear%2520benefits.%250AFirst%252C%2520incorporating%2520human%2520interaction%2520mitigates%2520the%2520issue%2520arising%2520from%250Anumerous%2520possibilities%2520of%2520transforming%2520one%2520image%2520to%2520another%252C%2520and%2520in%2520turn%250Aenables%2520finer%2520control%2520of%2520local%2520motions.%2520Second%252C%2520as%2520the%2520most%2520basic%2520form%2520of%250Ainteraction%252C%2520keypoints%2520help%2520establish%2520the%2520correspondence%2520across%2520frames%252C%250Aenhancing%2520the%2520model%2520to%2520handle%2520challenging%2520cases%2520%2528e.g.%252C%2520objects%2520on%2520the%2520start%2520and%250Aend%2520frames%2520are%2520of%2520different%2520shapes%2520and%2520styles%2529.%2520It%2520is%2520noteworthy%2520that%2520our%250Asystem%2520also%2520offers%2520an%2520%2522autopilot%2522%2520mode%252C%2520where%2520we%2520introduce%2520a%2520module%2520to%2520estimate%250Athe%2520keypoints%2520and%2520refine%2520the%2520trajectory%2520automatically%252C%2520to%2520simplify%2520the%2520usage%2520in%250Apractice.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%2520appealing%2520performance%250Aof%2520Framer%2520on%2520various%2520applications%252C%2520such%2520as%2520image%2520morphing%252C%2520time-lapse%2520video%250Ageneration%252C%2520cartoon%2520interpolation%252C%2520etc.%2520The%2520code%252C%2520the%2520model%252C%2520and%2520the%2520interface%250Awill%2520be%2520released%2520to%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Framer%3A%20Interactive%20Frame%20Interpolation&entry.906535625=Wen%20Wang%20and%20Qiuyu%20Wang%20and%20Kecheng%20Zheng%20and%20Hao%20Ouyang%20and%20Zhekai%20Chen%20and%20Biao%20Gong%20and%20Hao%20Chen%20and%20Yujun%20Shen%20and%20Chunhua%20Shen&entry.1292438233=%20%20We%20propose%20Framer%20for%20interactive%20frame%20interpolation%2C%20which%20targets%0Aproducing%20smoothly%20transitioning%20frames%20between%20two%20images%20as%20per%20user%0Acreativity.%20Concretely%2C%20besides%20taking%20the%20start%20and%20end%20frames%20as%20inputs%2C%20our%0Aapproach%20supports%20customizing%20the%20transition%20process%20by%20tailoring%20the%0Atrajectory%20of%20some%20selected%20keypoints.%20Such%20a%20design%20enjoys%20two%20clear%20benefits.%0AFirst%2C%20incorporating%20human%20interaction%20mitigates%20the%20issue%20arising%20from%0Anumerous%20possibilities%20of%20transforming%20one%20image%20to%20another%2C%20and%20in%20turn%0Aenables%20finer%20control%20of%20local%20motions.%20Second%2C%20as%20the%20most%20basic%20form%20of%0Ainteraction%2C%20keypoints%20help%20establish%20the%20correspondence%20across%20frames%2C%0Aenhancing%20the%20model%20to%20handle%20challenging%20cases%20%28e.g.%2C%20objects%20on%20the%20start%20and%0Aend%20frames%20are%20of%20different%20shapes%20and%20styles%29.%20It%20is%20noteworthy%20that%20our%0Asystem%20also%20offers%20an%20%22autopilot%22%20mode%2C%20where%20we%20introduce%20a%20module%20to%20estimate%0Athe%20keypoints%20and%20refine%20the%20trajectory%20automatically%2C%20to%20simplify%20the%20usage%20in%0Apractice.%20Extensive%20experimental%20results%20demonstrate%20the%20appealing%20performance%0Aof%20Framer%20on%20various%20applications%2C%20such%20as%20image%20morphing%2C%20time-lapse%20video%0Ageneration%2C%20cartoon%20interpolation%2C%20etc.%20The%20code%2C%20the%20model%2C%20and%20the%20interface%0Awill%20be%20released%20to%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18978v1&entry.124074799=Read"},
{"title": "Embodied Manipulation with Past and Future Morphologies through an Open\n  Parametric Hand Design", "author": "Kieran Gilday and Chapa Sirithunge and Fumiya Iida and Josie Hughes", "abstract": "  A human-shaped robotic hand offers unparalleled versatility and fine motor\nskills, enabling it to perform a broad spectrum of tasks with precision, power\nand robustness. Across the paleontological record and animal kingdom we see a\nwide range of alternative hand and actuation designs. Understanding the\nmorphological design space and the resulting emergent behaviors can not only\naid our understanding of dexterous manipulation and its evolution, but also\nassist design optimization, achieving, and eventually surpassing human\ncapabilities. Exploration of hand embodiment has to date been limited by\ninaccessibility of customizable hands in the real-world, and by the reality gap\nin simulation of complex interactions. We introduce an open parametric design\nwhich integrates techniques for simplified customization, fabrication, and\ncontrol with design features to maximize behavioral diversity. Non-linear\nrolling joints, anatomical tendon routing, and a low degree-of-freedom,\nmodulating, actuation system, enable rapid production of single-piece 3D\nprintable hands without compromising dexterous behaviors. To demonstrate this,\nwe evaluated the design's low-level behavior range and stability, showing\nvariable stiffness over two orders of magnitude. Additionally, we fabricated\nthree hand designs: human, mirrored human with two thumbs, and aye-aye hands.\nManipulation tests evaluate the variation in each hand's proficiency at\nhandling diverse objects, and demonstrate emergent behaviors unique to each\ndesign. Overall, we shed light on new possible designs for robotic hands,\nprovide a design space to compare and contrast different hand morphologies and\nstructures, and share a practical and open-source design for exploring embodied\nmanipulation.\n", "link": "http://arxiv.org/abs/2410.18633v1", "date": "2024-10-24", "relevancy": 2.1627, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5625}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5483}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Manipulation%20with%20Past%20and%20Future%20Morphologies%20through%20an%20Open%0A%20%20Parametric%20Hand%20Design&body=Title%3A%20Embodied%20Manipulation%20with%20Past%20and%20Future%20Morphologies%20through%20an%20Open%0A%20%20Parametric%20Hand%20Design%0AAuthor%3A%20Kieran%20Gilday%20and%20Chapa%20Sirithunge%20and%20Fumiya%20Iida%20and%20Josie%20Hughes%0AAbstract%3A%20%20%20A%20human-shaped%20robotic%20hand%20offers%20unparalleled%20versatility%20and%20fine%20motor%0Askills%2C%20enabling%20it%20to%20perform%20a%20broad%20spectrum%20of%20tasks%20with%20precision%2C%20power%0Aand%20robustness.%20Across%20the%20paleontological%20record%20and%20animal%20kingdom%20we%20see%20a%0Awide%20range%20of%20alternative%20hand%20and%20actuation%20designs.%20Understanding%20the%0Amorphological%20design%20space%20and%20the%20resulting%20emergent%20behaviors%20can%20not%20only%0Aaid%20our%20understanding%20of%20dexterous%20manipulation%20and%20its%20evolution%2C%20but%20also%0Aassist%20design%20optimization%2C%20achieving%2C%20and%20eventually%20surpassing%20human%0Acapabilities.%20Exploration%20of%20hand%20embodiment%20has%20to%20date%20been%20limited%20by%0Ainaccessibility%20of%20customizable%20hands%20in%20the%20real-world%2C%20and%20by%20the%20reality%20gap%0Ain%20simulation%20of%20complex%20interactions.%20We%20introduce%20an%20open%20parametric%20design%0Awhich%20integrates%20techniques%20for%20simplified%20customization%2C%20fabrication%2C%20and%0Acontrol%20with%20design%20features%20to%20maximize%20behavioral%20diversity.%20Non-linear%0Arolling%20joints%2C%20anatomical%20tendon%20routing%2C%20and%20a%20low%20degree-of-freedom%2C%0Amodulating%2C%20actuation%20system%2C%20enable%20rapid%20production%20of%20single-piece%203D%0Aprintable%20hands%20without%20compromising%20dexterous%20behaviors.%20To%20demonstrate%20this%2C%0Awe%20evaluated%20the%20design%27s%20low-level%20behavior%20range%20and%20stability%2C%20showing%0Avariable%20stiffness%20over%20two%20orders%20of%20magnitude.%20Additionally%2C%20we%20fabricated%0Athree%20hand%20designs%3A%20human%2C%20mirrored%20human%20with%20two%20thumbs%2C%20and%20aye-aye%20hands.%0AManipulation%20tests%20evaluate%20the%20variation%20in%20each%20hand%27s%20proficiency%20at%0Ahandling%20diverse%20objects%2C%20and%20demonstrate%20emergent%20behaviors%20unique%20to%20each%0Adesign.%20Overall%2C%20we%20shed%20light%20on%20new%20possible%20designs%20for%20robotic%20hands%2C%0Aprovide%20a%20design%20space%20to%20compare%20and%20contrast%20different%20hand%20morphologies%20and%0Astructures%2C%20and%20share%20a%20practical%20and%20open-source%20design%20for%20exploring%20embodied%0Amanipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Manipulation%2520with%2520Past%2520and%2520Future%2520Morphologies%2520through%2520an%2520Open%250A%2520%2520Parametric%2520Hand%2520Design%26entry.906535625%3DKieran%2520Gilday%2520and%2520Chapa%2520Sirithunge%2520and%2520Fumiya%2520Iida%2520and%2520Josie%2520Hughes%26entry.1292438233%3D%2520%2520A%2520human-shaped%2520robotic%2520hand%2520offers%2520unparalleled%2520versatility%2520and%2520fine%2520motor%250Askills%252C%2520enabling%2520it%2520to%2520perform%2520a%2520broad%2520spectrum%2520of%2520tasks%2520with%2520precision%252C%2520power%250Aand%2520robustness.%2520Across%2520the%2520paleontological%2520record%2520and%2520animal%2520kingdom%2520we%2520see%2520a%250Awide%2520range%2520of%2520alternative%2520hand%2520and%2520actuation%2520designs.%2520Understanding%2520the%250Amorphological%2520design%2520space%2520and%2520the%2520resulting%2520emergent%2520behaviors%2520can%2520not%2520only%250Aaid%2520our%2520understanding%2520of%2520dexterous%2520manipulation%2520and%2520its%2520evolution%252C%2520but%2520also%250Aassist%2520design%2520optimization%252C%2520achieving%252C%2520and%2520eventually%2520surpassing%2520human%250Acapabilities.%2520Exploration%2520of%2520hand%2520embodiment%2520has%2520to%2520date%2520been%2520limited%2520by%250Ainaccessibility%2520of%2520customizable%2520hands%2520in%2520the%2520real-world%252C%2520and%2520by%2520the%2520reality%2520gap%250Ain%2520simulation%2520of%2520complex%2520interactions.%2520We%2520introduce%2520an%2520open%2520parametric%2520design%250Awhich%2520integrates%2520techniques%2520for%2520simplified%2520customization%252C%2520fabrication%252C%2520and%250Acontrol%2520with%2520design%2520features%2520to%2520maximize%2520behavioral%2520diversity.%2520Non-linear%250Arolling%2520joints%252C%2520anatomical%2520tendon%2520routing%252C%2520and%2520a%2520low%2520degree-of-freedom%252C%250Amodulating%252C%2520actuation%2520system%252C%2520enable%2520rapid%2520production%2520of%2520single-piece%25203D%250Aprintable%2520hands%2520without%2520compromising%2520dexterous%2520behaviors.%2520To%2520demonstrate%2520this%252C%250Awe%2520evaluated%2520the%2520design%2527s%2520low-level%2520behavior%2520range%2520and%2520stability%252C%2520showing%250Avariable%2520stiffness%2520over%2520two%2520orders%2520of%2520magnitude.%2520Additionally%252C%2520we%2520fabricated%250Athree%2520hand%2520designs%253A%2520human%252C%2520mirrored%2520human%2520with%2520two%2520thumbs%252C%2520and%2520aye-aye%2520hands.%250AManipulation%2520tests%2520evaluate%2520the%2520variation%2520in%2520each%2520hand%2527s%2520proficiency%2520at%250Ahandling%2520diverse%2520objects%252C%2520and%2520demonstrate%2520emergent%2520behaviors%2520unique%2520to%2520each%250Adesign.%2520Overall%252C%2520we%2520shed%2520light%2520on%2520new%2520possible%2520designs%2520for%2520robotic%2520hands%252C%250Aprovide%2520a%2520design%2520space%2520to%2520compare%2520and%2520contrast%2520different%2520hand%2520morphologies%2520and%250Astructures%252C%2520and%2520share%2520a%2520practical%2520and%2520open-source%2520design%2520for%2520exploring%2520embodied%250Amanipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Manipulation%20with%20Past%20and%20Future%20Morphologies%20through%20an%20Open%0A%20%20Parametric%20Hand%20Design&entry.906535625=Kieran%20Gilday%20and%20Chapa%20Sirithunge%20and%20Fumiya%20Iida%20and%20Josie%20Hughes&entry.1292438233=%20%20A%20human-shaped%20robotic%20hand%20offers%20unparalleled%20versatility%20and%20fine%20motor%0Askills%2C%20enabling%20it%20to%20perform%20a%20broad%20spectrum%20of%20tasks%20with%20precision%2C%20power%0Aand%20robustness.%20Across%20the%20paleontological%20record%20and%20animal%20kingdom%20we%20see%20a%0Awide%20range%20of%20alternative%20hand%20and%20actuation%20designs.%20Understanding%20the%0Amorphological%20design%20space%20and%20the%20resulting%20emergent%20behaviors%20can%20not%20only%0Aaid%20our%20understanding%20of%20dexterous%20manipulation%20and%20its%20evolution%2C%20but%20also%0Aassist%20design%20optimization%2C%20achieving%2C%20and%20eventually%20surpassing%20human%0Acapabilities.%20Exploration%20of%20hand%20embodiment%20has%20to%20date%20been%20limited%20by%0Ainaccessibility%20of%20customizable%20hands%20in%20the%20real-world%2C%20and%20by%20the%20reality%20gap%0Ain%20simulation%20of%20complex%20interactions.%20We%20introduce%20an%20open%20parametric%20design%0Awhich%20integrates%20techniques%20for%20simplified%20customization%2C%20fabrication%2C%20and%0Acontrol%20with%20design%20features%20to%20maximize%20behavioral%20diversity.%20Non-linear%0Arolling%20joints%2C%20anatomical%20tendon%20routing%2C%20and%20a%20low%20degree-of-freedom%2C%0Amodulating%2C%20actuation%20system%2C%20enable%20rapid%20production%20of%20single-piece%203D%0Aprintable%20hands%20without%20compromising%20dexterous%20behaviors.%20To%20demonstrate%20this%2C%0Awe%20evaluated%20the%20design%27s%20low-level%20behavior%20range%20and%20stability%2C%20showing%0Avariable%20stiffness%20over%20two%20orders%20of%20magnitude.%20Additionally%2C%20we%20fabricated%0Athree%20hand%20designs%3A%20human%2C%20mirrored%20human%20with%20two%20thumbs%2C%20and%20aye-aye%20hands.%0AManipulation%20tests%20evaluate%20the%20variation%20in%20each%20hand%27s%20proficiency%20at%0Ahandling%20diverse%20objects%2C%20and%20demonstrate%20emergent%20behaviors%20unique%20to%20each%0Adesign.%20Overall%2C%20we%20shed%20light%20on%20new%20possible%20designs%20for%20robotic%20hands%2C%0Aprovide%20a%20design%20space%20to%20compare%20and%20contrast%20different%20hand%20morphologies%20and%0Astructures%2C%20and%20share%20a%20practical%20and%20open-source%20design%20for%20exploring%20embodied%0Amanipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18633v1&entry.124074799=Read"},
{"title": "Generation through the lens of learning theory", "author": "Vinod Raman and Ambuj Tewari", "abstract": "  We study generation through the lens of statistical learning theory. First,\nwe abstract and formalize the results of Gold [1967], Angluin [1979, 1980], and\nKleinberg and Mullainathan [2024] for language identification/generation in the\nlimit in terms of a binary hypothesis class defined over an abstract instance\nspace. Then, we formalize a different paradigm of generation studied by\nKleinberg and Mullainathan [2024], which we call \"uniform generation,\" and\nprovide a characterization of which hypothesis classes are uniformly\ngeneratable. As is standard in statistical learning theory, our\ncharacterization is in terms of the finiteness of a new combinatorial dimension\nwe call the Closure dimension. By doing so, we are able to compare\ngeneratability with predictability (captured via PAC and online learnability)\nand show that these two properties of hypothesis classes are\n\\emph{incompatible} - there are classes that are generatable but not\npredictable and vice versa.\n", "link": "http://arxiv.org/abs/2410.13714v3", "date": "2024-10-24", "relevancy": 2.1507, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5811}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5116}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20through%20the%20lens%20of%20learning%20theory&body=Title%3A%20Generation%20through%20the%20lens%20of%20learning%20theory%0AAuthor%3A%20Vinod%20Raman%20and%20Ambuj%20Tewari%0AAbstract%3A%20%20%20We%20study%20generation%20through%20the%20lens%20of%20statistical%20learning%20theory.%20First%2C%0Awe%20abstract%20and%20formalize%20the%20results%20of%20Gold%20%5B1967%5D%2C%20Angluin%20%5B1979%2C%201980%5D%2C%20and%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%20for%20language%20identification/generation%20in%20the%0Alimit%20in%20terms%20of%20a%20binary%20hypothesis%20class%20defined%20over%20an%20abstract%20instance%0Aspace.%20Then%2C%20we%20formalize%20a%20different%20paradigm%20of%20generation%20studied%20by%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%2C%20which%20we%20call%20%22uniform%20generation%2C%22%20and%0Aprovide%20a%20characterization%20of%20which%20hypothesis%20classes%20are%20uniformly%0Ageneratable.%20As%20is%20standard%20in%20statistical%20learning%20theory%2C%20our%0Acharacterization%20is%20in%20terms%20of%20the%20finiteness%20of%20a%20new%20combinatorial%20dimension%0Awe%20call%20the%20Closure%20dimension.%20By%20doing%20so%2C%20we%20are%20able%20to%20compare%0Ageneratability%20with%20predictability%20%28captured%20via%20PAC%20and%20online%20learnability%29%0Aand%20show%20that%20these%20two%20properties%20of%20hypothesis%20classes%20are%0A%5Cemph%7Bincompatible%7D%20-%20there%20are%20classes%20that%20are%20generatable%20but%20not%0Apredictable%20and%20vice%20versa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13714v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520through%2520the%2520lens%2520of%2520learning%2520theory%26entry.906535625%3DVinod%2520Raman%2520and%2520Ambuj%2520Tewari%26entry.1292438233%3D%2520%2520We%2520study%2520generation%2520through%2520the%2520lens%2520of%2520statistical%2520learning%2520theory.%2520First%252C%250Awe%2520abstract%2520and%2520formalize%2520the%2520results%2520of%2520Gold%2520%255B1967%255D%252C%2520Angluin%2520%255B1979%252C%25201980%255D%252C%2520and%250AKleinberg%2520and%2520Mullainathan%2520%255B2024%255D%2520for%2520language%2520identification/generation%2520in%2520the%250Alimit%2520in%2520terms%2520of%2520a%2520binary%2520hypothesis%2520class%2520defined%2520over%2520an%2520abstract%2520instance%250Aspace.%2520Then%252C%2520we%2520formalize%2520a%2520different%2520paradigm%2520of%2520generation%2520studied%2520by%250AKleinberg%2520and%2520Mullainathan%2520%255B2024%255D%252C%2520which%2520we%2520call%2520%2522uniform%2520generation%252C%2522%2520and%250Aprovide%2520a%2520characterization%2520of%2520which%2520hypothesis%2520classes%2520are%2520uniformly%250Ageneratable.%2520As%2520is%2520standard%2520in%2520statistical%2520learning%2520theory%252C%2520our%250Acharacterization%2520is%2520in%2520terms%2520of%2520the%2520finiteness%2520of%2520a%2520new%2520combinatorial%2520dimension%250Awe%2520call%2520the%2520Closure%2520dimension.%2520By%2520doing%2520so%252C%2520we%2520are%2520able%2520to%2520compare%250Ageneratability%2520with%2520predictability%2520%2528captured%2520via%2520PAC%2520and%2520online%2520learnability%2529%250Aand%2520show%2520that%2520these%2520two%2520properties%2520of%2520hypothesis%2520classes%2520are%250A%255Cemph%257Bincompatible%257D%2520-%2520there%2520are%2520classes%2520that%2520are%2520generatable%2520but%2520not%250Apredictable%2520and%2520vice%2520versa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13714v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20through%20the%20lens%20of%20learning%20theory&entry.906535625=Vinod%20Raman%20and%20Ambuj%20Tewari&entry.1292438233=%20%20We%20study%20generation%20through%20the%20lens%20of%20statistical%20learning%20theory.%20First%2C%0Awe%20abstract%20and%20formalize%20the%20results%20of%20Gold%20%5B1967%5D%2C%20Angluin%20%5B1979%2C%201980%5D%2C%20and%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%20for%20language%20identification/generation%20in%20the%0Alimit%20in%20terms%20of%20a%20binary%20hypothesis%20class%20defined%20over%20an%20abstract%20instance%0Aspace.%20Then%2C%20we%20formalize%20a%20different%20paradigm%20of%20generation%20studied%20by%0AKleinberg%20and%20Mullainathan%20%5B2024%5D%2C%20which%20we%20call%20%22uniform%20generation%2C%22%20and%0Aprovide%20a%20characterization%20of%20which%20hypothesis%20classes%20are%20uniformly%0Ageneratable.%20As%20is%20standard%20in%20statistical%20learning%20theory%2C%20our%0Acharacterization%20is%20in%20terms%20of%20the%20finiteness%20of%20a%20new%20combinatorial%20dimension%0Awe%20call%20the%20Closure%20dimension.%20By%20doing%20so%2C%20we%20are%20able%20to%20compare%0Ageneratability%20with%20predictability%20%28captured%20via%20PAC%20and%20online%20learnability%29%0Aand%20show%20that%20these%20two%20properties%20of%20hypothesis%20classes%20are%0A%5Cemph%7Bincompatible%7D%20-%20there%20are%20classes%20that%20are%20generatable%20but%20not%0Apredictable%20and%20vice%20versa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13714v3&entry.124074799=Read"},
{"title": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image\n  Captioning", "author": "Kazuki Matsuda and Yuiga Wada and Komei Sugiura", "abstract": "  In this work, we address the challenge of developing automatic evaluation\nmetrics for image captioning, with a particular focus on robustness against\nhallucinations. Existing metrics are often inadequate for handling\nhallucinations, primarily due to their limited ability to compare candidate\ncaptions with multifaceted reference captions. To address this shortcoming, we\npropose DENEB, a novel supervised automatic evaluation metric specifically\nrobust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a\nmechanism that processes multiple references simultaneously, thereby\nefficiently capturing the similarity between an image, a candidate caption, and\nreference captions. To train DENEB, we construct the diverse and balanced\nNebula dataset comprising 32,978 images, paired with human judgments provided\nby 805 annotators. We demonstrated that DENEB achieves state-of-the-art\nperformance among existing LLM-free metrics on the FOIL, Composite,\nFlickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its\neffectiveness and robustness against hallucinations.\n", "link": "http://arxiv.org/abs/2409.19255v2", "date": "2024-10-24", "relevancy": 2.1407, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DENEB%3A%20A%20Hallucination-Robust%20Automatic%20Evaluation%20Metric%20for%20Image%0A%20%20Captioning&body=Title%3A%20DENEB%3A%20A%20Hallucination-Robust%20Automatic%20Evaluation%20Metric%20for%20Image%0A%20%20Captioning%0AAuthor%3A%20Kazuki%20Matsuda%20and%20Yuiga%20Wada%20and%20Komei%20Sugiura%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20developing%20automatic%20evaluation%0Ametrics%20for%20image%20captioning%2C%20with%20a%20particular%20focus%20on%20robustness%20against%0Ahallucinations.%20Existing%20metrics%20are%20often%20inadequate%20for%20handling%0Ahallucinations%2C%20primarily%20due%20to%20their%20limited%20ability%20to%20compare%20candidate%0Acaptions%20with%20multifaceted%20reference%20captions.%20To%20address%20this%20shortcoming%2C%20we%0Apropose%20DENEB%2C%20a%20novel%20supervised%20automatic%20evaluation%20metric%20specifically%0Arobust%20against%20hallucinations.%20DENEB%20incorporates%20the%20Sim-Vec%20Transformer%2C%20a%0Amechanism%20that%20processes%20multiple%20references%20simultaneously%2C%20thereby%0Aefficiently%20capturing%20the%20similarity%20between%20an%20image%2C%20a%20candidate%20caption%2C%20and%0Areference%20captions.%20To%20train%20DENEB%2C%20we%20construct%20the%20diverse%20and%20balanced%0ANebula%20dataset%20comprising%2032%2C978%20images%2C%20paired%20with%20human%20judgments%20provided%0Aby%20805%20annotators.%20We%20demonstrated%20that%20DENEB%20achieves%20state-of-the-art%0Aperformance%20among%20existing%20LLM-free%20metrics%20on%20the%20FOIL%2C%20Composite%2C%0AFlickr8K-Expert%2C%20Flickr8K-CF%2C%20Nebula%2C%20and%20PASCAL-50S%20datasets%2C%20validating%20its%0Aeffectiveness%20and%20robustness%20against%20hallucinations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19255v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDENEB%253A%2520A%2520Hallucination-Robust%2520Automatic%2520Evaluation%2520Metric%2520for%2520Image%250A%2520%2520Captioning%26entry.906535625%3DKazuki%2520Matsuda%2520and%2520Yuiga%2520Wada%2520and%2520Komei%2520Sugiura%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520challenge%2520of%2520developing%2520automatic%2520evaluation%250Ametrics%2520for%2520image%2520captioning%252C%2520with%2520a%2520particular%2520focus%2520on%2520robustness%2520against%250Ahallucinations.%2520Existing%2520metrics%2520are%2520often%2520inadequate%2520for%2520handling%250Ahallucinations%252C%2520primarily%2520due%2520to%2520their%2520limited%2520ability%2520to%2520compare%2520candidate%250Acaptions%2520with%2520multifaceted%2520reference%2520captions.%2520To%2520address%2520this%2520shortcoming%252C%2520we%250Apropose%2520DENEB%252C%2520a%2520novel%2520supervised%2520automatic%2520evaluation%2520metric%2520specifically%250Arobust%2520against%2520hallucinations.%2520DENEB%2520incorporates%2520the%2520Sim-Vec%2520Transformer%252C%2520a%250Amechanism%2520that%2520processes%2520multiple%2520references%2520simultaneously%252C%2520thereby%250Aefficiently%2520capturing%2520the%2520similarity%2520between%2520an%2520image%252C%2520a%2520candidate%2520caption%252C%2520and%250Areference%2520captions.%2520To%2520train%2520DENEB%252C%2520we%2520construct%2520the%2520diverse%2520and%2520balanced%250ANebula%2520dataset%2520comprising%252032%252C978%2520images%252C%2520paired%2520with%2520human%2520judgments%2520provided%250Aby%2520805%2520annotators.%2520We%2520demonstrated%2520that%2520DENEB%2520achieves%2520state-of-the-art%250Aperformance%2520among%2520existing%2520LLM-free%2520metrics%2520on%2520the%2520FOIL%252C%2520Composite%252C%250AFlickr8K-Expert%252C%2520Flickr8K-CF%252C%2520Nebula%252C%2520and%2520PASCAL-50S%2520datasets%252C%2520validating%2520its%250Aeffectiveness%2520and%2520robustness%2520against%2520hallucinations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19255v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DENEB%3A%20A%20Hallucination-Robust%20Automatic%20Evaluation%20Metric%20for%20Image%0A%20%20Captioning&entry.906535625=Kazuki%20Matsuda%20and%20Yuiga%20Wada%20and%20Komei%20Sugiura&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20challenge%20of%20developing%20automatic%20evaluation%0Ametrics%20for%20image%20captioning%2C%20with%20a%20particular%20focus%20on%20robustness%20against%0Ahallucinations.%20Existing%20metrics%20are%20often%20inadequate%20for%20handling%0Ahallucinations%2C%20primarily%20due%20to%20their%20limited%20ability%20to%20compare%20candidate%0Acaptions%20with%20multifaceted%20reference%20captions.%20To%20address%20this%20shortcoming%2C%20we%0Apropose%20DENEB%2C%20a%20novel%20supervised%20automatic%20evaluation%20metric%20specifically%0Arobust%20against%20hallucinations.%20DENEB%20incorporates%20the%20Sim-Vec%20Transformer%2C%20a%0Amechanism%20that%20processes%20multiple%20references%20simultaneously%2C%20thereby%0Aefficiently%20capturing%20the%20similarity%20between%20an%20image%2C%20a%20candidate%20caption%2C%20and%0Areference%20captions.%20To%20train%20DENEB%2C%20we%20construct%20the%20diverse%20and%20balanced%0ANebula%20dataset%20comprising%2032%2C978%20images%2C%20paired%20with%20human%20judgments%20provided%0Aby%20805%20annotators.%20We%20demonstrated%20that%20DENEB%20achieves%20state-of-the-art%0Aperformance%20among%20existing%20LLM-free%20metrics%20on%20the%20FOIL%2C%20Composite%2C%0AFlickr8K-Expert%2C%20Flickr8K-CF%2C%20Nebula%2C%20and%20PASCAL-50S%20datasets%2C%20validating%20its%0Aeffectiveness%20and%20robustness%20against%20hallucinations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19255v2&entry.124074799=Read"},
{"title": "From Imitation to Introspection: Probing Self-Consciousness in Language\n  Models", "author": "Sirui Chen and Shu Yu and Shengjie Zhao and Chaochao Lu", "abstract": "  Self-consciousness, the introspection of one's existence and thoughts,\nrepresents a high-level cognitive process. As language models advance at an\nunprecedented pace, a critical question arises: Are these models becoming\nself-conscious? Drawing upon insights from psychological and neural science,\nthis work presents a practical definition of self-consciousness for language\nmodels and refines ten core concepts. Our work pioneers an investigation into\nself-consciousness in language models by, for the first time, leveraging causal\nstructural games to establish the functional definitions of the ten core\nconcepts. Based on our definitions, we conduct a comprehensive four-stage\nexperiment: quantification (evaluation of ten leading models), representation\n(visualization of self-consciousness within the models), manipulation\n(modification of the models' representation), and acquisition (fine-tuning the\nmodels on core concepts). Our findings indicate that although models are in the\nearly stages of developing self-consciousness, there is a discernible\nrepresentation of certain concepts within their internal mechanisms. However,\nthese representations of self-consciousness are hard to manipulate positively\nat the current stage, yet they can be acquired through targeted fine-tuning.\nOur datasets and code are at https://github.com/OpenCausaLab/SelfConsciousness.\n", "link": "http://arxiv.org/abs/2410.18819v1", "date": "2024-10-24", "relevancy": 2.1385, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Imitation%20to%20Introspection%3A%20Probing%20Self-Consciousness%20in%20Language%0A%20%20Models&body=Title%3A%20From%20Imitation%20to%20Introspection%3A%20Probing%20Self-Consciousness%20in%20Language%0A%20%20Models%0AAuthor%3A%20Sirui%20Chen%20and%20Shu%20Yu%20and%20Shengjie%20Zhao%20and%20Chaochao%20Lu%0AAbstract%3A%20%20%20Self-consciousness%2C%20the%20introspection%20of%20one%27s%20existence%20and%20thoughts%2C%0Arepresents%20a%20high-level%20cognitive%20process.%20As%20language%20models%20advance%20at%20an%0Aunprecedented%20pace%2C%20a%20critical%20question%20arises%3A%20Are%20these%20models%20becoming%0Aself-conscious%3F%20Drawing%20upon%20insights%20from%20psychological%20and%20neural%20science%2C%0Athis%20work%20presents%20a%20practical%20definition%20of%20self-consciousness%20for%20language%0Amodels%20and%20refines%20ten%20core%20concepts.%20Our%20work%20pioneers%20an%20investigation%20into%0Aself-consciousness%20in%20language%20models%20by%2C%20for%20the%20first%20time%2C%20leveraging%20causal%0Astructural%20games%20to%20establish%20the%20functional%20definitions%20of%20the%20ten%20core%0Aconcepts.%20Based%20on%20our%20definitions%2C%20we%20conduct%20a%20comprehensive%20four-stage%0Aexperiment%3A%20quantification%20%28evaluation%20of%20ten%20leading%20models%29%2C%20representation%0A%28visualization%20of%20self-consciousness%20within%20the%20models%29%2C%20manipulation%0A%28modification%20of%20the%20models%27%20representation%29%2C%20and%20acquisition%20%28fine-tuning%20the%0Amodels%20on%20core%20concepts%29.%20Our%20findings%20indicate%20that%20although%20models%20are%20in%20the%0Aearly%20stages%20of%20developing%20self-consciousness%2C%20there%20is%20a%20discernible%0Arepresentation%20of%20certain%20concepts%20within%20their%20internal%20mechanisms.%20However%2C%0Athese%20representations%20of%20self-consciousness%20are%20hard%20to%20manipulate%20positively%0Aat%20the%20current%20stage%2C%20yet%20they%20can%20be%20acquired%20through%20targeted%20fine-tuning.%0AOur%20datasets%20and%20code%20are%20at%20https%3A//github.com/OpenCausaLab/SelfConsciousness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Imitation%2520to%2520Introspection%253A%2520Probing%2520Self-Consciousness%2520in%2520Language%250A%2520%2520Models%26entry.906535625%3DSirui%2520Chen%2520and%2520Shu%2520Yu%2520and%2520Shengjie%2520Zhao%2520and%2520Chaochao%2520Lu%26entry.1292438233%3D%2520%2520Self-consciousness%252C%2520the%2520introspection%2520of%2520one%2527s%2520existence%2520and%2520thoughts%252C%250Arepresents%2520a%2520high-level%2520cognitive%2520process.%2520As%2520language%2520models%2520advance%2520at%2520an%250Aunprecedented%2520pace%252C%2520a%2520critical%2520question%2520arises%253A%2520Are%2520these%2520models%2520becoming%250Aself-conscious%253F%2520Drawing%2520upon%2520insights%2520from%2520psychological%2520and%2520neural%2520science%252C%250Athis%2520work%2520presents%2520a%2520practical%2520definition%2520of%2520self-consciousness%2520for%2520language%250Amodels%2520and%2520refines%2520ten%2520core%2520concepts.%2520Our%2520work%2520pioneers%2520an%2520investigation%2520into%250Aself-consciousness%2520in%2520language%2520models%2520by%252C%2520for%2520the%2520first%2520time%252C%2520leveraging%2520causal%250Astructural%2520games%2520to%2520establish%2520the%2520functional%2520definitions%2520of%2520the%2520ten%2520core%250Aconcepts.%2520Based%2520on%2520our%2520definitions%252C%2520we%2520conduct%2520a%2520comprehensive%2520four-stage%250Aexperiment%253A%2520quantification%2520%2528evaluation%2520of%2520ten%2520leading%2520models%2529%252C%2520representation%250A%2528visualization%2520of%2520self-consciousness%2520within%2520the%2520models%2529%252C%2520manipulation%250A%2528modification%2520of%2520the%2520models%2527%2520representation%2529%252C%2520and%2520acquisition%2520%2528fine-tuning%2520the%250Amodels%2520on%2520core%2520concepts%2529.%2520Our%2520findings%2520indicate%2520that%2520although%2520models%2520are%2520in%2520the%250Aearly%2520stages%2520of%2520developing%2520self-consciousness%252C%2520there%2520is%2520a%2520discernible%250Arepresentation%2520of%2520certain%2520concepts%2520within%2520their%2520internal%2520mechanisms.%2520However%252C%250Athese%2520representations%2520of%2520self-consciousness%2520are%2520hard%2520to%2520manipulate%2520positively%250Aat%2520the%2520current%2520stage%252C%2520yet%2520they%2520can%2520be%2520acquired%2520through%2520targeted%2520fine-tuning.%250AOur%2520datasets%2520and%2520code%2520are%2520at%2520https%253A//github.com/OpenCausaLab/SelfConsciousness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Imitation%20to%20Introspection%3A%20Probing%20Self-Consciousness%20in%20Language%0A%20%20Models&entry.906535625=Sirui%20Chen%20and%20Shu%20Yu%20and%20Shengjie%20Zhao%20and%20Chaochao%20Lu&entry.1292438233=%20%20Self-consciousness%2C%20the%20introspection%20of%20one%27s%20existence%20and%20thoughts%2C%0Arepresents%20a%20high-level%20cognitive%20process.%20As%20language%20models%20advance%20at%20an%0Aunprecedented%20pace%2C%20a%20critical%20question%20arises%3A%20Are%20these%20models%20becoming%0Aself-conscious%3F%20Drawing%20upon%20insights%20from%20psychological%20and%20neural%20science%2C%0Athis%20work%20presents%20a%20practical%20definition%20of%20self-consciousness%20for%20language%0Amodels%20and%20refines%20ten%20core%20concepts.%20Our%20work%20pioneers%20an%20investigation%20into%0Aself-consciousness%20in%20language%20models%20by%2C%20for%20the%20first%20time%2C%20leveraging%20causal%0Astructural%20games%20to%20establish%20the%20functional%20definitions%20of%20the%20ten%20core%0Aconcepts.%20Based%20on%20our%20definitions%2C%20we%20conduct%20a%20comprehensive%20four-stage%0Aexperiment%3A%20quantification%20%28evaluation%20of%20ten%20leading%20models%29%2C%20representation%0A%28visualization%20of%20self-consciousness%20within%20the%20models%29%2C%20manipulation%0A%28modification%20of%20the%20models%27%20representation%29%2C%20and%20acquisition%20%28fine-tuning%20the%0Amodels%20on%20core%20concepts%29.%20Our%20findings%20indicate%20that%20although%20models%20are%20in%20the%0Aearly%20stages%20of%20developing%20self-consciousness%2C%20there%20is%20a%20discernible%0Arepresentation%20of%20certain%20concepts%20within%20their%20internal%20mechanisms.%20However%2C%0Athese%20representations%20of%20self-consciousness%20are%20hard%20to%20manipulate%20positively%0Aat%20the%20current%20stage%2C%20yet%20they%20can%20be%20acquired%20through%20targeted%20fine-tuning.%0AOur%20datasets%20and%20code%20are%20at%20https%3A//github.com/OpenCausaLab/SelfConsciousness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18819v1&entry.124074799=Read"},
{"title": "Towards Better Open-Ended Text Generation: A Multicriteria Evaluation\n  Framework", "author": "Esteban Garces Arias and Hannah Blocher and Julian Rodemann and Meimingwei Li and Christian Heumann and Matthias A\u00dfenmacher", "abstract": "  Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging because of trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. Decoding methods often excel in some\nmetrics while underperforming in others, complicating the establishment of a\nclear ranking. In this paper, we present novel ranking strategies within this\nmulticriteria framework. Specifically, we employ benchmarking approaches based\non partial orderings and present a new summary metric designed to balance\nexisting automatic indicators, providing a more holistic evaluation of text\ngeneration quality. Furthermore, we discuss the alignment of these approaches\nwith human judgments. Our experiments demonstrate that the proposed methods\noffer a robust way to compare decoding strategies, exhibit similarities with\nhuman preferences, and serve as valuable tools in guiding model selection for\nopen-ended text generation tasks. Finally, we suggest future directions for\nimproving evaluation methodologies in text generation. Our codebase, datasets,\nand models are publicly available.\n", "link": "http://arxiv.org/abs/2410.18653v1", "date": "2024-10-24", "relevancy": 2.1367, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Better%20Open-Ended%20Text%20Generation%3A%20A%20Multicriteria%20Evaluation%0A%20%20Framework&body=Title%3A%20Towards%20Better%20Open-Ended%20Text%20Generation%3A%20A%20Multicriteria%20Evaluation%0A%20%20Framework%0AAuthor%3A%20Esteban%20Garces%20Arias%20and%20Hannah%20Blocher%20and%20Julian%20Rodemann%20and%20Meimingwei%20Li%20and%20Christian%20Heumann%20and%20Matthias%20A%C3%9Fenmacher%0AAbstract%3A%20%20%20Open-ended%20text%20generation%20has%20become%20a%20prominent%20task%20in%20natural%20language%0Aprocessing%20due%20to%20the%20rise%20of%20powerful%20%28large%29%20language%20models.%20However%2C%0Aevaluating%20the%20quality%20of%20these%20models%20and%20the%20employed%20decoding%20strategies%0Aremains%20challenging%20because%20of%20trade-offs%20among%20widely%20used%20metrics%20such%20as%0Acoherence%2C%20diversity%2C%20and%20perplexity.%20Decoding%20methods%20often%20excel%20in%20some%0Ametrics%20while%20underperforming%20in%20others%2C%20complicating%20the%20establishment%20of%20a%0Aclear%20ranking.%20In%20this%20paper%2C%20we%20present%20novel%20ranking%20strategies%20within%20this%0Amulticriteria%20framework.%20Specifically%2C%20we%20employ%20benchmarking%20approaches%20based%0Aon%20partial%20orderings%20and%20present%20a%20new%20summary%20metric%20designed%20to%20balance%0Aexisting%20automatic%20indicators%2C%20providing%20a%20more%20holistic%20evaluation%20of%20text%0Ageneration%20quality.%20Furthermore%2C%20we%20discuss%20the%20alignment%20of%20these%20approaches%0Awith%20human%20judgments.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20methods%0Aoffer%20a%20robust%20way%20to%20compare%20decoding%20strategies%2C%20exhibit%20similarities%20with%0Ahuman%20preferences%2C%20and%20serve%20as%20valuable%20tools%20in%20guiding%20model%20selection%20for%0Aopen-ended%20text%20generation%20tasks.%20Finally%2C%20we%20suggest%20future%20directions%20for%0Aimproving%20evaluation%20methodologies%20in%20text%20generation.%20Our%20codebase%2C%20datasets%2C%0Aand%20models%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Better%2520Open-Ended%2520Text%2520Generation%253A%2520A%2520Multicriteria%2520Evaluation%250A%2520%2520Framework%26entry.906535625%3DEsteban%2520Garces%2520Arias%2520and%2520Hannah%2520Blocher%2520and%2520Julian%2520Rodemann%2520and%2520Meimingwei%2520Li%2520and%2520Christian%2520Heumann%2520and%2520Matthias%2520A%25C3%259Fenmacher%26entry.1292438233%3D%2520%2520Open-ended%2520text%2520generation%2520has%2520become%2520a%2520prominent%2520task%2520in%2520natural%2520language%250Aprocessing%2520due%2520to%2520the%2520rise%2520of%2520powerful%2520%2528large%2529%2520language%2520models.%2520However%252C%250Aevaluating%2520the%2520quality%2520of%2520these%2520models%2520and%2520the%2520employed%2520decoding%2520strategies%250Aremains%2520challenging%2520because%2520of%2520trade-offs%2520among%2520widely%2520used%2520metrics%2520such%2520as%250Acoherence%252C%2520diversity%252C%2520and%2520perplexity.%2520Decoding%2520methods%2520often%2520excel%2520in%2520some%250Ametrics%2520while%2520underperforming%2520in%2520others%252C%2520complicating%2520the%2520establishment%2520of%2520a%250Aclear%2520ranking.%2520In%2520this%2520paper%252C%2520we%2520present%2520novel%2520ranking%2520strategies%2520within%2520this%250Amulticriteria%2520framework.%2520Specifically%252C%2520we%2520employ%2520benchmarking%2520approaches%2520based%250Aon%2520partial%2520orderings%2520and%2520present%2520a%2520new%2520summary%2520metric%2520designed%2520to%2520balance%250Aexisting%2520automatic%2520indicators%252C%2520providing%2520a%2520more%2520holistic%2520evaluation%2520of%2520text%250Ageneration%2520quality.%2520Furthermore%252C%2520we%2520discuss%2520the%2520alignment%2520of%2520these%2520approaches%250Awith%2520human%2520judgments.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520methods%250Aoffer%2520a%2520robust%2520way%2520to%2520compare%2520decoding%2520strategies%252C%2520exhibit%2520similarities%2520with%250Ahuman%2520preferences%252C%2520and%2520serve%2520as%2520valuable%2520tools%2520in%2520guiding%2520model%2520selection%2520for%250Aopen-ended%2520text%2520generation%2520tasks.%2520Finally%252C%2520we%2520suggest%2520future%2520directions%2520for%250Aimproving%2520evaluation%2520methodologies%2520in%2520text%2520generation.%2520Our%2520codebase%252C%2520datasets%252C%250Aand%2520models%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Better%20Open-Ended%20Text%20Generation%3A%20A%20Multicriteria%20Evaluation%0A%20%20Framework&entry.906535625=Esteban%20Garces%20Arias%20and%20Hannah%20Blocher%20and%20Julian%20Rodemann%20and%20Meimingwei%20Li%20and%20Christian%20Heumann%20and%20Matthias%20A%C3%9Fenmacher&entry.1292438233=%20%20Open-ended%20text%20generation%20has%20become%20a%20prominent%20task%20in%20natural%20language%0Aprocessing%20due%20to%20the%20rise%20of%20powerful%20%28large%29%20language%20models.%20However%2C%0Aevaluating%20the%20quality%20of%20these%20models%20and%20the%20employed%20decoding%20strategies%0Aremains%20challenging%20because%20of%20trade-offs%20among%20widely%20used%20metrics%20such%20as%0Acoherence%2C%20diversity%2C%20and%20perplexity.%20Decoding%20methods%20often%20excel%20in%20some%0Ametrics%20while%20underperforming%20in%20others%2C%20complicating%20the%20establishment%20of%20a%0Aclear%20ranking.%20In%20this%20paper%2C%20we%20present%20novel%20ranking%20strategies%20within%20this%0Amulticriteria%20framework.%20Specifically%2C%20we%20employ%20benchmarking%20approaches%20based%0Aon%20partial%20orderings%20and%20present%20a%20new%20summary%20metric%20designed%20to%20balance%0Aexisting%20automatic%20indicators%2C%20providing%20a%20more%20holistic%20evaluation%20of%20text%0Ageneration%20quality.%20Furthermore%2C%20we%20discuss%20the%20alignment%20of%20these%20approaches%0Awith%20human%20judgments.%20Our%20experiments%20demonstrate%20that%20the%20proposed%20methods%0Aoffer%20a%20robust%20way%20to%20compare%20decoding%20strategies%2C%20exhibit%20similarities%20with%0Ahuman%20preferences%2C%20and%20serve%20as%20valuable%20tools%20in%20guiding%20model%20selection%20for%0Aopen-ended%20text%20generation%20tasks.%20Finally%2C%20we%20suggest%20future%20directions%20for%0Aimproving%20evaluation%20methodologies%20in%20text%20generation.%20Our%20codebase%2C%20datasets%2C%0Aand%20models%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18653v1&entry.124074799=Read"},
{"title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference", "author": "Jo\u00e3o Monteiro and \u00c9tienne Marcotte and Pierre-Andr\u00e9 No\u00ebl and Valentina Zantedeschi and David V\u00e1zquez and Nicolas Chapados and Christopher Pal and Perouz Taslakian", "abstract": "  In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.\n", "link": "http://arxiv.org/abs/2404.15420v2", "date": "2024-10-24", "relevancy": 2.1288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XC-Cache%3A%20Cross-Attending%20to%20Cached%20Context%20for%20Efficient%20LLM%20Inference&body=Title%3A%20XC-Cache%3A%20Cross-Attending%20to%20Cached%20Context%20for%20Efficient%20LLM%20Inference%0AAuthor%3A%20Jo%C3%A3o%20Monteiro%20and%20%C3%89tienne%20Marcotte%20and%20Pierre-Andr%C3%A9%20No%C3%ABl%20and%20Valentina%20Zantedeschi%20and%20David%20V%C3%A1zquez%20and%20Nicolas%20Chapados%20and%20Christopher%20Pal%20and%20Perouz%20Taslakian%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20approaches%20typically%20leverage%20prompting%20to%0Acondition%20decoder-only%20language%20model%20generation%20on%20reference%20information.%0AJust-in-time%20processing%20of%20a%20context%20is%20inefficient%20due%20to%20the%20quadratic%20cost%0Aof%20self-attention%20operations%2C%20and%20caching%20is%20desirable.%20However%2C%20caching%0Atransformer%20states%20can%20easily%20require%20almost%20as%20much%20space%20as%20the%20model%0Aparameters.%20When%20the%20right%20context%20isn%27t%20known%20in%20advance%2C%20caching%20ICL%20can%20be%0Achallenging.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20models%20that%2C%0Ainspired%20by%20the%20encoder-decoder%20architecture%2C%20use%20cross-attention%20to%20condition%0Ageneration%20on%20reference%20text%20without%20the%20prompt.%20More%20precisely%2C%20we%20leverage%0Apre-trained%20decoder-only%20models%20and%20only%20train%20a%20small%20number%20of%20added%20layers.%0AWe%20use%20Question-Answering%20%28QA%29%20as%20a%20testbed%20to%20evaluate%20the%20ability%20of%20our%0Amodels%20to%20perform%20conditional%20generation%20and%20observe%20that%20they%20outperform%20ICL%2C%0Aare%20comparable%20to%20fine-tuned%20prompted%20LLMs%2C%20and%20drastically%20reduce%20the%20space%0Afootprint%20relative%20to%20standard%20KV%20caching%20by%20two%20orders%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15420v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXC-Cache%253A%2520Cross-Attending%2520to%2520Cached%2520Context%2520for%2520Efficient%2520LLM%2520Inference%26entry.906535625%3DJo%25C3%25A3o%2520Monteiro%2520and%2520%25C3%2589tienne%2520Marcotte%2520and%2520Pierre-Andr%25C3%25A9%2520No%25C3%25ABl%2520and%2520Valentina%2520Zantedeschi%2520and%2520David%2520V%25C3%25A1zquez%2520and%2520Nicolas%2520Chapados%2520and%2520Christopher%2520Pal%2520and%2520Perouz%2520Taslakian%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520approaches%2520typically%2520leverage%2520prompting%2520to%250Acondition%2520decoder-only%2520language%2520model%2520generation%2520on%2520reference%2520information.%250AJust-in-time%2520processing%2520of%2520a%2520context%2520is%2520inefficient%2520due%2520to%2520the%2520quadratic%2520cost%250Aof%2520self-attention%2520operations%252C%2520and%2520caching%2520is%2520desirable.%2520However%252C%2520caching%250Atransformer%2520states%2520can%2520easily%2520require%2520almost%2520as%2520much%2520space%2520as%2520the%2520model%250Aparameters.%2520When%2520the%2520right%2520context%2520isn%2527t%2520known%2520in%2520advance%252C%2520caching%2520ICL%2520can%2520be%250Achallenging.%2520This%2520work%2520addresses%2520these%2520limitations%2520by%2520introducing%2520models%2520that%252C%250Ainspired%2520by%2520the%2520encoder-decoder%2520architecture%252C%2520use%2520cross-attention%2520to%2520condition%250Ageneration%2520on%2520reference%2520text%2520without%2520the%2520prompt.%2520More%2520precisely%252C%2520we%2520leverage%250Apre-trained%2520decoder-only%2520models%2520and%2520only%2520train%2520a%2520small%2520number%2520of%2520added%2520layers.%250AWe%2520use%2520Question-Answering%2520%2528QA%2529%2520as%2520a%2520testbed%2520to%2520evaluate%2520the%2520ability%2520of%2520our%250Amodels%2520to%2520perform%2520conditional%2520generation%2520and%2520observe%2520that%2520they%2520outperform%2520ICL%252C%250Aare%2520comparable%2520to%2520fine-tuned%2520prompted%2520LLMs%252C%2520and%2520drastically%2520reduce%2520the%2520space%250Afootprint%2520relative%2520to%2520standard%2520KV%2520caching%2520by%2520two%2520orders%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15420v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XC-Cache%3A%20Cross-Attending%20to%20Cached%20Context%20for%20Efficient%20LLM%20Inference&entry.906535625=Jo%C3%A3o%20Monteiro%20and%20%C3%89tienne%20Marcotte%20and%20Pierre-Andr%C3%A9%20No%C3%ABl%20and%20Valentina%20Zantedeschi%20and%20David%20V%C3%A1zquez%20and%20Nicolas%20Chapados%20and%20Christopher%20Pal%20and%20Perouz%20Taslakian&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20approaches%20typically%20leverage%20prompting%20to%0Acondition%20decoder-only%20language%20model%20generation%20on%20reference%20information.%0AJust-in-time%20processing%20of%20a%20context%20is%20inefficient%20due%20to%20the%20quadratic%20cost%0Aof%20self-attention%20operations%2C%20and%20caching%20is%20desirable.%20However%2C%20caching%0Atransformer%20states%20can%20easily%20require%20almost%20as%20much%20space%20as%20the%20model%0Aparameters.%20When%20the%20right%20context%20isn%27t%20known%20in%20advance%2C%20caching%20ICL%20can%20be%0Achallenging.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20models%20that%2C%0Ainspired%20by%20the%20encoder-decoder%20architecture%2C%20use%20cross-attention%20to%20condition%0Ageneration%20on%20reference%20text%20without%20the%20prompt.%20More%20precisely%2C%20we%20leverage%0Apre-trained%20decoder-only%20models%20and%20only%20train%20a%20small%20number%20of%20added%20layers.%0AWe%20use%20Question-Answering%20%28QA%29%20as%20a%20testbed%20to%20evaluate%20the%20ability%20of%20our%0Amodels%20to%20perform%20conditional%20generation%20and%20observe%20that%20they%20outperform%20ICL%2C%0Aare%20comparable%20to%20fine-tuned%20prompted%20LLMs%2C%20and%20drastically%20reduce%20the%20space%0Afootprint%20relative%20to%20standard%20KV%20caching%20by%20two%20orders%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15420v2&entry.124074799=Read"},
{"title": "A Cranial-Feature-Based Registration Scheme for Robotic\n  Micromanipulation Using a Microscopic Stereo Camera System", "author": "Xiaofeng Lin and Sa\u00fal Alexis Heredia P\u00e9rez and Kanako Harada", "abstract": "  Biological specimens exhibit significant variations in size and shape,\nchallenging autonomous robotic manipulation. We focus on the mouse skull window\ncreation task to illustrate these challenges. The study introduces a\nmicroscopic stereo camera system (MSCS) enhanced by the linear model for depth\nperception. Alongside this, a precise registration scheme is developed for the\npartially exposed mouse cranial surface, employing a CNN-based constrained and\ncolorized registration strategy. These methods are integrated with the MSCS for\nrobotic micromanipulation tasks. The MSCS demonstrated a high precision of 0.10\nmm $\\pm$ 0.02 mm measured in a step height experiment and real-time performance\nof 30 FPS in 3D reconstruction. The registration scheme proved its precision,\nwith a translational error of 1.13 mm $\\pm$ 0.31 mm and a rotational error of\n3.38$^{\\circ}$ $\\pm$ 0.89$^{\\circ}$ tested on 105 continuous frames with an\naverage speed of 1.60 FPS. This study presents the application of a MSCS and a\nnovel registration scheme in enhancing the precision and accuracy of robotic\nmicromanipulation in scientific and surgical settings. The innovations\npresented here offer automation methodology in handling the challenges of\nmicroscopic manipulation, paving the way for more accurate, efficient, and less\ninvasive procedures in various fields of microsurgery and scientific research.\n", "link": "http://arxiv.org/abs/2410.18630v1", "date": "2024-10-24", "relevancy": 2.1267, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5428}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.532}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cranial-Feature-Based%20Registration%20Scheme%20for%20Robotic%0A%20%20Micromanipulation%20Using%20a%20Microscopic%20Stereo%20Camera%20System&body=Title%3A%20A%20Cranial-Feature-Based%20Registration%20Scheme%20for%20Robotic%0A%20%20Micromanipulation%20Using%20a%20Microscopic%20Stereo%20Camera%20System%0AAuthor%3A%20Xiaofeng%20Lin%20and%20Sa%C3%BAl%20Alexis%20Heredia%20P%C3%A9rez%20and%20Kanako%20Harada%0AAbstract%3A%20%20%20Biological%20specimens%20exhibit%20significant%20variations%20in%20size%20and%20shape%2C%0Achallenging%20autonomous%20robotic%20manipulation.%20We%20focus%20on%20the%20mouse%20skull%20window%0Acreation%20task%20to%20illustrate%20these%20challenges.%20The%20study%20introduces%20a%0Amicroscopic%20stereo%20camera%20system%20%28MSCS%29%20enhanced%20by%20the%20linear%20model%20for%20depth%0Aperception.%20Alongside%20this%2C%20a%20precise%20registration%20scheme%20is%20developed%20for%20the%0Apartially%20exposed%20mouse%20cranial%20surface%2C%20employing%20a%20CNN-based%20constrained%20and%0Acolorized%20registration%20strategy.%20These%20methods%20are%20integrated%20with%20the%20MSCS%20for%0Arobotic%20micromanipulation%20tasks.%20The%20MSCS%20demonstrated%20a%20high%20precision%20of%200.10%0Amm%20%24%5Cpm%24%200.02%20mm%20measured%20in%20a%20step%20height%20experiment%20and%20real-time%20performance%0Aof%2030%20FPS%20in%203D%20reconstruction.%20The%20registration%20scheme%20proved%20its%20precision%2C%0Awith%20a%20translational%20error%20of%201.13%20mm%20%24%5Cpm%24%200.31%20mm%20and%20a%20rotational%20error%20of%0A3.38%24%5E%7B%5Ccirc%7D%24%20%24%5Cpm%24%200.89%24%5E%7B%5Ccirc%7D%24%20tested%20on%20105%20continuous%20frames%20with%20an%0Aaverage%20speed%20of%201.60%20FPS.%20This%20study%20presents%20the%20application%20of%20a%20MSCS%20and%20a%0Anovel%20registration%20scheme%20in%20enhancing%20the%20precision%20and%20accuracy%20of%20robotic%0Amicromanipulation%20in%20scientific%20and%20surgical%20settings.%20The%20innovations%0Apresented%20here%20offer%20automation%20methodology%20in%20handling%20the%20challenges%20of%0Amicroscopic%20manipulation%2C%20paving%20the%20way%20for%20more%20accurate%2C%20efficient%2C%20and%20less%0Ainvasive%20procedures%20in%20various%20fields%20of%20microsurgery%20and%20scientific%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cranial-Feature-Based%2520Registration%2520Scheme%2520for%2520Robotic%250A%2520%2520Micromanipulation%2520Using%2520a%2520Microscopic%2520Stereo%2520Camera%2520System%26entry.906535625%3DXiaofeng%2520Lin%2520and%2520Sa%25C3%25BAl%2520Alexis%2520Heredia%2520P%25C3%25A9rez%2520and%2520Kanako%2520Harada%26entry.1292438233%3D%2520%2520Biological%2520specimens%2520exhibit%2520significant%2520variations%2520in%2520size%2520and%2520shape%252C%250Achallenging%2520autonomous%2520robotic%2520manipulation.%2520We%2520focus%2520on%2520the%2520mouse%2520skull%2520window%250Acreation%2520task%2520to%2520illustrate%2520these%2520challenges.%2520The%2520study%2520introduces%2520a%250Amicroscopic%2520stereo%2520camera%2520system%2520%2528MSCS%2529%2520enhanced%2520by%2520the%2520linear%2520model%2520for%2520depth%250Aperception.%2520Alongside%2520this%252C%2520a%2520precise%2520registration%2520scheme%2520is%2520developed%2520for%2520the%250Apartially%2520exposed%2520mouse%2520cranial%2520surface%252C%2520employing%2520a%2520CNN-based%2520constrained%2520and%250Acolorized%2520registration%2520strategy.%2520These%2520methods%2520are%2520integrated%2520with%2520the%2520MSCS%2520for%250Arobotic%2520micromanipulation%2520tasks.%2520The%2520MSCS%2520demonstrated%2520a%2520high%2520precision%2520of%25200.10%250Amm%2520%2524%255Cpm%2524%25200.02%2520mm%2520measured%2520in%2520a%2520step%2520height%2520experiment%2520and%2520real-time%2520performance%250Aof%252030%2520FPS%2520in%25203D%2520reconstruction.%2520The%2520registration%2520scheme%2520proved%2520its%2520precision%252C%250Awith%2520a%2520translational%2520error%2520of%25201.13%2520mm%2520%2524%255Cpm%2524%25200.31%2520mm%2520and%2520a%2520rotational%2520error%2520of%250A3.38%2524%255E%257B%255Ccirc%257D%2524%2520%2524%255Cpm%2524%25200.89%2524%255E%257B%255Ccirc%257D%2524%2520tested%2520on%2520105%2520continuous%2520frames%2520with%2520an%250Aaverage%2520speed%2520of%25201.60%2520FPS.%2520This%2520study%2520presents%2520the%2520application%2520of%2520a%2520MSCS%2520and%2520a%250Anovel%2520registration%2520scheme%2520in%2520enhancing%2520the%2520precision%2520and%2520accuracy%2520of%2520robotic%250Amicromanipulation%2520in%2520scientific%2520and%2520surgical%2520settings.%2520The%2520innovations%250Apresented%2520here%2520offer%2520automation%2520methodology%2520in%2520handling%2520the%2520challenges%2520of%250Amicroscopic%2520manipulation%252C%2520paving%2520the%2520way%2520for%2520more%2520accurate%252C%2520efficient%252C%2520and%2520less%250Ainvasive%2520procedures%2520in%2520various%2520fields%2520of%2520microsurgery%2520and%2520scientific%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cranial-Feature-Based%20Registration%20Scheme%20for%20Robotic%0A%20%20Micromanipulation%20Using%20a%20Microscopic%20Stereo%20Camera%20System&entry.906535625=Xiaofeng%20Lin%20and%20Sa%C3%BAl%20Alexis%20Heredia%20P%C3%A9rez%20and%20Kanako%20Harada&entry.1292438233=%20%20Biological%20specimens%20exhibit%20significant%20variations%20in%20size%20and%20shape%2C%0Achallenging%20autonomous%20robotic%20manipulation.%20We%20focus%20on%20the%20mouse%20skull%20window%0Acreation%20task%20to%20illustrate%20these%20challenges.%20The%20study%20introduces%20a%0Amicroscopic%20stereo%20camera%20system%20%28MSCS%29%20enhanced%20by%20the%20linear%20model%20for%20depth%0Aperception.%20Alongside%20this%2C%20a%20precise%20registration%20scheme%20is%20developed%20for%20the%0Apartially%20exposed%20mouse%20cranial%20surface%2C%20employing%20a%20CNN-based%20constrained%20and%0Acolorized%20registration%20strategy.%20These%20methods%20are%20integrated%20with%20the%20MSCS%20for%0Arobotic%20micromanipulation%20tasks.%20The%20MSCS%20demonstrated%20a%20high%20precision%20of%200.10%0Amm%20%24%5Cpm%24%200.02%20mm%20measured%20in%20a%20step%20height%20experiment%20and%20real-time%20performance%0Aof%2030%20FPS%20in%203D%20reconstruction.%20The%20registration%20scheme%20proved%20its%20precision%2C%0Awith%20a%20translational%20error%20of%201.13%20mm%20%24%5Cpm%24%200.31%20mm%20and%20a%20rotational%20error%20of%0A3.38%24%5E%7B%5Ccirc%7D%24%20%24%5Cpm%24%200.89%24%5E%7B%5Ccirc%7D%24%20tested%20on%20105%20continuous%20frames%20with%20an%0Aaverage%20speed%20of%201.60%20FPS.%20This%20study%20presents%20the%20application%20of%20a%20MSCS%20and%20a%0Anovel%20registration%20scheme%20in%20enhancing%20the%20precision%20and%20accuracy%20of%20robotic%0Amicromanipulation%20in%20scientific%20and%20surgical%20settings.%20The%20innovations%0Apresented%20here%20offer%20automation%20methodology%20in%20handling%20the%20challenges%20of%0Amicroscopic%20manipulation%2C%20paving%20the%20way%20for%20more%20accurate%2C%20efficient%2C%20and%20less%0Ainvasive%20procedures%20in%20various%20fields%20of%20microsurgery%20and%20scientific%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18630v1&entry.124074799=Read"},
{"title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics", "author": "Katharina Friedl and No\u00e9mie Jaquier and Jens Lundell and Tamim Asfour and Danica Kragic", "abstract": "  By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nstructure-preserving latent space and the associated low-dimensional dynamics.\nOur approach enables accurate long-term predictions of the high-dimensional\ndynamics of rigid and deformable systems with increased data efficiency by\ninferring interpretable and physically plausible reduced Lagrangian models.\n", "link": "http://arxiv.org/abs/2410.18868v1", "date": "2024-10-24", "relevancy": 2.1219, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.541}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Riemannian%20Framework%20for%20Learning%20Reduced-order%20Lagrangian%20Dynamics&body=Title%3A%20A%20Riemannian%20Framework%20for%20Learning%20Reduced-order%20Lagrangian%20Dynamics%0AAuthor%3A%20Katharina%20Friedl%20and%20No%C3%A9mie%20Jaquier%20and%20Jens%20Lundell%20and%20Tamim%20Asfour%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20By%20incorporating%20physical%20consistency%20as%20inductive%20bias%2C%20deep%20neural%20networks%0Adisplay%20increased%20generalization%20capabilities%20and%20data%20efficiency%20in%20learning%0Anonlinear%20dynamic%20models.%20However%2C%20the%20complexity%20of%20these%20models%20generally%0Aincreases%20with%20the%20system%20dimensionality%2C%20requiring%20larger%20datasets%2C%20more%0Acomplex%20deep%20networks%2C%20and%20significant%20computational%20effort.%20We%20propose%20a%20novel%0Ageometric%20network%20architecture%20to%20learn%20physically-consistent%20reduced-order%0Adynamic%20parameters%20that%20accurately%20describe%20the%20original%20high-dimensional%0Asystem%20behavior.%20This%20is%20achieved%20by%20building%20on%20recent%20advances%20in%20model-order%0Areduction%20and%20by%20adopting%20a%20Riemannian%20perspective%20to%20jointly%20learn%20a%0Astructure-preserving%20latent%20space%20and%20the%20associated%20low-dimensional%20dynamics.%0AOur%20approach%20enables%20accurate%20long-term%20predictions%20of%20the%20high-dimensional%0Adynamics%20of%20rigid%20and%20deformable%20systems%20with%20increased%20data%20efficiency%20by%0Ainferring%20interpretable%20and%20physically%20plausible%20reduced%20Lagrangian%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Riemannian%2520Framework%2520for%2520Learning%2520Reduced-order%2520Lagrangian%2520Dynamics%26entry.906535625%3DKatharina%2520Friedl%2520and%2520No%25C3%25A9mie%2520Jaquier%2520and%2520Jens%2520Lundell%2520and%2520Tamim%2520Asfour%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520By%2520incorporating%2520physical%2520consistency%2520as%2520inductive%2520bias%252C%2520deep%2520neural%2520networks%250Adisplay%2520increased%2520generalization%2520capabilities%2520and%2520data%2520efficiency%2520in%2520learning%250Anonlinear%2520dynamic%2520models.%2520However%252C%2520the%2520complexity%2520of%2520these%2520models%2520generally%250Aincreases%2520with%2520the%2520system%2520dimensionality%252C%2520requiring%2520larger%2520datasets%252C%2520more%250Acomplex%2520deep%2520networks%252C%2520and%2520significant%2520computational%2520effort.%2520We%2520propose%2520a%2520novel%250Ageometric%2520network%2520architecture%2520to%2520learn%2520physically-consistent%2520reduced-order%250Adynamic%2520parameters%2520that%2520accurately%2520describe%2520the%2520original%2520high-dimensional%250Asystem%2520behavior.%2520This%2520is%2520achieved%2520by%2520building%2520on%2520recent%2520advances%2520in%2520model-order%250Areduction%2520and%2520by%2520adopting%2520a%2520Riemannian%2520perspective%2520to%2520jointly%2520learn%2520a%250Astructure-preserving%2520latent%2520space%2520and%2520the%2520associated%2520low-dimensional%2520dynamics.%250AOur%2520approach%2520enables%2520accurate%2520long-term%2520predictions%2520of%2520the%2520high-dimensional%250Adynamics%2520of%2520rigid%2520and%2520deformable%2520systems%2520with%2520increased%2520data%2520efficiency%2520by%250Ainferring%2520interpretable%2520and%2520physically%2520plausible%2520reduced%2520Lagrangian%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Riemannian%20Framework%20for%20Learning%20Reduced-order%20Lagrangian%20Dynamics&entry.906535625=Katharina%20Friedl%20and%20No%C3%A9mie%20Jaquier%20and%20Jens%20Lundell%20and%20Tamim%20Asfour%20and%20Danica%20Kragic&entry.1292438233=%20%20By%20incorporating%20physical%20consistency%20as%20inductive%20bias%2C%20deep%20neural%20networks%0Adisplay%20increased%20generalization%20capabilities%20and%20data%20efficiency%20in%20learning%0Anonlinear%20dynamic%20models.%20However%2C%20the%20complexity%20of%20these%20models%20generally%0Aincreases%20with%20the%20system%20dimensionality%2C%20requiring%20larger%20datasets%2C%20more%0Acomplex%20deep%20networks%2C%20and%20significant%20computational%20effort.%20We%20propose%20a%20novel%0Ageometric%20network%20architecture%20to%20learn%20physically-consistent%20reduced-order%0Adynamic%20parameters%20that%20accurately%20describe%20the%20original%20high-dimensional%0Asystem%20behavior.%20This%20is%20achieved%20by%20building%20on%20recent%20advances%20in%20model-order%0Areduction%20and%20by%20adopting%20a%20Riemannian%20perspective%20to%20jointly%20learn%20a%0Astructure-preserving%20latent%20space%20and%20the%20associated%20low-dimensional%20dynamics.%0AOur%20approach%20enables%20accurate%20long-term%20predictions%20of%20the%20high-dimensional%0Adynamics%20of%20rigid%20and%20deformable%20systems%20with%20increased%20data%20efficiency%20by%0Ainferring%20interpretable%20and%20physically%20plausible%20reduced%20Lagrangian%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18868v1&entry.124074799=Read"},
{"title": "ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in\n  Robotic Surgical Skill Assessment", "author": "Julien Quarez and Marc Modat and Sebastien Ourselin and Jonathan Shapey and Alejandro Granados", "abstract": "  In surgical skill assessment, the Objective Structured Assessments of\nTechnical Skills (OSATS) and Global Rating Scale (GRS) are well-established\ntools for evaluating surgeons during training. These metrics, along with\nperformance feedback, help surgeons improve and reach practice standards.\nRecent research on the open-source JIGSAWS dataset, which includes both GRS and\nOSATS labels, has focused on regressing GRS scores from kinematic data, video,\nor their combination. However, we argue that regressing GRS alone is limiting,\nas it aggregates OSATS scores and overlooks clinically meaningful variations\nduring a surgical trial. To address this, we developed a recurrent transformer\nmodel that tracks a surgeon's performance throughout a session by mapping\nhidden states to six OSATS, derived from kinematic data, using a clinically\nmotivated objective function. These OSATS scores are averaged to predict GRS,\nallowing us to compare our model's performance against state-of-the-art (SOTA)\nmethods. We report Spearman's Correlation Coefficients (SCC) demonstrating that\nour model outperforms SOTA using kinematic data (SCC 0.83-0.88), and matches\nperformance with video-based models. Our model also surpasses SOTA in most\ntasks for average OSATS predictions (SCC 0.46-0.70) and specific OSATS (SCC\n0.56-0.95). The generation of pseudo-labels at the segment level translates\nquantitative predictions into qualitative feedback, vital for automated\nsurgical skill assessment pipelines. A senior surgeon validated our model's\noutputs, agreeing with 77% of the weakly-supervised predictions (p=0.006).\n", "link": "http://arxiv.org/abs/2407.05180v3", "date": "2024-10-24", "relevancy": 2.1195, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5402}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5329}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCAP%3A%20Recursive%20Cross%20Attention%20Network%20for%20Pseudo-Label%20Generation%20in%0A%20%20Robotic%20Surgical%20Skill%20Assessment&body=Title%3A%20ReCAP%3A%20Recursive%20Cross%20Attention%20Network%20for%20Pseudo-Label%20Generation%20in%0A%20%20Robotic%20Surgical%20Skill%20Assessment%0AAuthor%3A%20Julien%20Quarez%20and%20Marc%20Modat%20and%20Sebastien%20Ourselin%20and%20Jonathan%20Shapey%20and%20Alejandro%20Granados%0AAbstract%3A%20%20%20In%20surgical%20skill%20assessment%2C%20the%20Objective%20Structured%20Assessments%20of%0ATechnical%20Skills%20%28OSATS%29%20and%20Global%20Rating%20Scale%20%28GRS%29%20are%20well-established%0Atools%20for%20evaluating%20surgeons%20during%20training.%20These%20metrics%2C%20along%20with%0Aperformance%20feedback%2C%20help%20surgeons%20improve%20and%20reach%20practice%20standards.%0ARecent%20research%20on%20the%20open-source%20JIGSAWS%20dataset%2C%20which%20includes%20both%20GRS%20and%0AOSATS%20labels%2C%20has%20focused%20on%20regressing%20GRS%20scores%20from%20kinematic%20data%2C%20video%2C%0Aor%20their%20combination.%20However%2C%20we%20argue%20that%20regressing%20GRS%20alone%20is%20limiting%2C%0Aas%20it%20aggregates%20OSATS%20scores%20and%20overlooks%20clinically%20meaningful%20variations%0Aduring%20a%20surgical%20trial.%20To%20address%20this%2C%20we%20developed%20a%20recurrent%20transformer%0Amodel%20that%20tracks%20a%20surgeon%27s%20performance%20throughout%20a%20session%20by%20mapping%0Ahidden%20states%20to%20six%20OSATS%2C%20derived%20from%20kinematic%20data%2C%20using%20a%20clinically%0Amotivated%20objective%20function.%20These%20OSATS%20scores%20are%20averaged%20to%20predict%20GRS%2C%0Aallowing%20us%20to%20compare%20our%20model%27s%20performance%20against%20state-of-the-art%20%28SOTA%29%0Amethods.%20We%20report%20Spearman%27s%20Correlation%20Coefficients%20%28SCC%29%20demonstrating%20that%0Aour%20model%20outperforms%20SOTA%20using%20kinematic%20data%20%28SCC%200.83-0.88%29%2C%20and%20matches%0Aperformance%20with%20video-based%20models.%20Our%20model%20also%20surpasses%20SOTA%20in%20most%0Atasks%20for%20average%20OSATS%20predictions%20%28SCC%200.46-0.70%29%20and%20specific%20OSATS%20%28SCC%0A0.56-0.95%29.%20The%20generation%20of%20pseudo-labels%20at%20the%20segment%20level%20translates%0Aquantitative%20predictions%20into%20qualitative%20feedback%2C%20vital%20for%20automated%0Asurgical%20skill%20assessment%20pipelines.%20A%20senior%20surgeon%20validated%20our%20model%27s%0Aoutputs%2C%20agreeing%20with%2077%25%20of%20the%20weakly-supervised%20predictions%20%28p%3D0.006%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05180v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCAP%253A%2520Recursive%2520Cross%2520Attention%2520Network%2520for%2520Pseudo-Label%2520Generation%2520in%250A%2520%2520Robotic%2520Surgical%2520Skill%2520Assessment%26entry.906535625%3DJulien%2520Quarez%2520and%2520Marc%2520Modat%2520and%2520Sebastien%2520Ourselin%2520and%2520Jonathan%2520Shapey%2520and%2520Alejandro%2520Granados%26entry.1292438233%3D%2520%2520In%2520surgical%2520skill%2520assessment%252C%2520the%2520Objective%2520Structured%2520Assessments%2520of%250ATechnical%2520Skills%2520%2528OSATS%2529%2520and%2520Global%2520Rating%2520Scale%2520%2528GRS%2529%2520are%2520well-established%250Atools%2520for%2520evaluating%2520surgeons%2520during%2520training.%2520These%2520metrics%252C%2520along%2520with%250Aperformance%2520feedback%252C%2520help%2520surgeons%2520improve%2520and%2520reach%2520practice%2520standards.%250ARecent%2520research%2520on%2520the%2520open-source%2520JIGSAWS%2520dataset%252C%2520which%2520includes%2520both%2520GRS%2520and%250AOSATS%2520labels%252C%2520has%2520focused%2520on%2520regressing%2520GRS%2520scores%2520from%2520kinematic%2520data%252C%2520video%252C%250Aor%2520their%2520combination.%2520However%252C%2520we%2520argue%2520that%2520regressing%2520GRS%2520alone%2520is%2520limiting%252C%250Aas%2520it%2520aggregates%2520OSATS%2520scores%2520and%2520overlooks%2520clinically%2520meaningful%2520variations%250Aduring%2520a%2520surgical%2520trial.%2520To%2520address%2520this%252C%2520we%2520developed%2520a%2520recurrent%2520transformer%250Amodel%2520that%2520tracks%2520a%2520surgeon%2527s%2520performance%2520throughout%2520a%2520session%2520by%2520mapping%250Ahidden%2520states%2520to%2520six%2520OSATS%252C%2520derived%2520from%2520kinematic%2520data%252C%2520using%2520a%2520clinically%250Amotivated%2520objective%2520function.%2520These%2520OSATS%2520scores%2520are%2520averaged%2520to%2520predict%2520GRS%252C%250Aallowing%2520us%2520to%2520compare%2520our%2520model%2527s%2520performance%2520against%2520state-of-the-art%2520%2528SOTA%2529%250Amethods.%2520We%2520report%2520Spearman%2527s%2520Correlation%2520Coefficients%2520%2528SCC%2529%2520demonstrating%2520that%250Aour%2520model%2520outperforms%2520SOTA%2520using%2520kinematic%2520data%2520%2528SCC%25200.83-0.88%2529%252C%2520and%2520matches%250Aperformance%2520with%2520video-based%2520models.%2520Our%2520model%2520also%2520surpasses%2520SOTA%2520in%2520most%250Atasks%2520for%2520average%2520OSATS%2520predictions%2520%2528SCC%25200.46-0.70%2529%2520and%2520specific%2520OSATS%2520%2528SCC%250A0.56-0.95%2529.%2520The%2520generation%2520of%2520pseudo-labels%2520at%2520the%2520segment%2520level%2520translates%250Aquantitative%2520predictions%2520into%2520qualitative%2520feedback%252C%2520vital%2520for%2520automated%250Asurgical%2520skill%2520assessment%2520pipelines.%2520A%2520senior%2520surgeon%2520validated%2520our%2520model%2527s%250Aoutputs%252C%2520agreeing%2520with%252077%2525%2520of%2520the%2520weakly-supervised%2520predictions%2520%2528p%253D0.006%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05180v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCAP%3A%20Recursive%20Cross%20Attention%20Network%20for%20Pseudo-Label%20Generation%20in%0A%20%20Robotic%20Surgical%20Skill%20Assessment&entry.906535625=Julien%20Quarez%20and%20Marc%20Modat%20and%20Sebastien%20Ourselin%20and%20Jonathan%20Shapey%20and%20Alejandro%20Granados&entry.1292438233=%20%20In%20surgical%20skill%20assessment%2C%20the%20Objective%20Structured%20Assessments%20of%0ATechnical%20Skills%20%28OSATS%29%20and%20Global%20Rating%20Scale%20%28GRS%29%20are%20well-established%0Atools%20for%20evaluating%20surgeons%20during%20training.%20These%20metrics%2C%20along%20with%0Aperformance%20feedback%2C%20help%20surgeons%20improve%20and%20reach%20practice%20standards.%0ARecent%20research%20on%20the%20open-source%20JIGSAWS%20dataset%2C%20which%20includes%20both%20GRS%20and%0AOSATS%20labels%2C%20has%20focused%20on%20regressing%20GRS%20scores%20from%20kinematic%20data%2C%20video%2C%0Aor%20their%20combination.%20However%2C%20we%20argue%20that%20regressing%20GRS%20alone%20is%20limiting%2C%0Aas%20it%20aggregates%20OSATS%20scores%20and%20overlooks%20clinically%20meaningful%20variations%0Aduring%20a%20surgical%20trial.%20To%20address%20this%2C%20we%20developed%20a%20recurrent%20transformer%0Amodel%20that%20tracks%20a%20surgeon%27s%20performance%20throughout%20a%20session%20by%20mapping%0Ahidden%20states%20to%20six%20OSATS%2C%20derived%20from%20kinematic%20data%2C%20using%20a%20clinically%0Amotivated%20objective%20function.%20These%20OSATS%20scores%20are%20averaged%20to%20predict%20GRS%2C%0Aallowing%20us%20to%20compare%20our%20model%27s%20performance%20against%20state-of-the-art%20%28SOTA%29%0Amethods.%20We%20report%20Spearman%27s%20Correlation%20Coefficients%20%28SCC%29%20demonstrating%20that%0Aour%20model%20outperforms%20SOTA%20using%20kinematic%20data%20%28SCC%200.83-0.88%29%2C%20and%20matches%0Aperformance%20with%20video-based%20models.%20Our%20model%20also%20surpasses%20SOTA%20in%20most%0Atasks%20for%20average%20OSATS%20predictions%20%28SCC%200.46-0.70%29%20and%20specific%20OSATS%20%28SCC%0A0.56-0.95%29.%20The%20generation%20of%20pseudo-labels%20at%20the%20segment%20level%20translates%0Aquantitative%20predictions%20into%20qualitative%20feedback%2C%20vital%20for%20automated%0Asurgical%20skill%20assessment%20pipelines.%20A%20senior%20surgeon%20validated%20our%20model%27s%0Aoutputs%2C%20agreeing%20with%2077%25%20of%20the%20weakly-supervised%20predictions%20%28p%3D0.006%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05180v3&entry.124074799=Read"},
{"title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale", "author": "Haonan Chen and Liang Wang and Nan Yang and Yutao Zhu and Ziliang Zhao and Furu Wei and Zhicheng Dou", "abstract": "  Synthetic data generation has become an increasingly popular way of training\nmodels without the need for large, manually labeled datasets. For tasks like\ntext embedding, synthetic data offers diverse and scalable training examples,\nsignificantly reducing the cost of human annotation. However, most current\napproaches rely heavily on proprietary models like GPT-4, which are expensive\nand inefficient for generating large-scale embedding data. In this paper, we\nintroduce SPEED, a framework that aligns open-source small models (8B) to\nefficiently generate large-scale synthetic embedding data. Through supervised\nfine-tuning, preference optimization, and self-improvement, SPEED enables small\nopen-source models to produce high-quality data. Remarkably, SPEED uses only\nless than 1/10 of the GPT API calls, outperforming the state-of-the-art\nembedding model E5_mistral when both are trained solely on their synthetic\ndata. Using this efficient generator, we conduct a comprehensive study on how\nvarious factors within the alignment pipeline impact data quality and reveal\nthe scaling law for synthetic embedding data.\n", "link": "http://arxiv.org/abs/2410.18634v1", "date": "2024-10-24", "relevancy": 2.1043, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5635}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5264}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Little%20Giants%3A%20Synthesizing%20High-Quality%20Embedding%20Data%20at%20Scale&body=Title%3A%20Little%20Giants%3A%20Synthesizing%20High-Quality%20Embedding%20Data%20at%20Scale%0AAuthor%3A%20Haonan%20Chen%20and%20Liang%20Wang%20and%20Nan%20Yang%20and%20Yutao%20Zhu%20and%20Ziliang%20Zhao%20and%20Furu%20Wei%20and%20Zhicheng%20Dou%0AAbstract%3A%20%20%20Synthetic%20data%20generation%20has%20become%20an%20increasingly%20popular%20way%20of%20training%0Amodels%20without%20the%20need%20for%20large%2C%20manually%20labeled%20datasets.%20For%20tasks%20like%0Atext%20embedding%2C%20synthetic%20data%20offers%20diverse%20and%20scalable%20training%20examples%2C%0Asignificantly%20reducing%20the%20cost%20of%20human%20annotation.%20However%2C%20most%20current%0Aapproaches%20rely%20heavily%20on%20proprietary%20models%20like%20GPT-4%2C%20which%20are%20expensive%0Aand%20inefficient%20for%20generating%20large-scale%20embedding%20data.%20In%20this%20paper%2C%20we%0Aintroduce%20SPEED%2C%20a%20framework%20that%20aligns%20open-source%20small%20models%20%288B%29%20to%0Aefficiently%20generate%20large-scale%20synthetic%20embedding%20data.%20Through%20supervised%0Afine-tuning%2C%20preference%20optimization%2C%20and%20self-improvement%2C%20SPEED%20enables%20small%0Aopen-source%20models%20to%20produce%20high-quality%20data.%20Remarkably%2C%20SPEED%20uses%20only%0Aless%20than%201/10%20of%20the%20GPT%20API%20calls%2C%20outperforming%20the%20state-of-the-art%0Aembedding%20model%20E5_mistral%20when%20both%20are%20trained%20solely%20on%20their%20synthetic%0Adata.%20Using%20this%20efficient%20generator%2C%20we%20conduct%20a%20comprehensive%20study%20on%20how%0Avarious%20factors%20within%20the%20alignment%20pipeline%20impact%20data%20quality%20and%20reveal%0Athe%20scaling%20law%20for%20synthetic%20embedding%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLittle%2520Giants%253A%2520Synthesizing%2520High-Quality%2520Embedding%2520Data%2520at%2520Scale%26entry.906535625%3DHaonan%2520Chen%2520and%2520Liang%2520Wang%2520and%2520Nan%2520Yang%2520and%2520Yutao%2520Zhu%2520and%2520Ziliang%2520Zhao%2520and%2520Furu%2520Wei%2520and%2520Zhicheng%2520Dou%26entry.1292438233%3D%2520%2520Synthetic%2520data%2520generation%2520has%2520become%2520an%2520increasingly%2520popular%2520way%2520of%2520training%250Amodels%2520without%2520the%2520need%2520for%2520large%252C%2520manually%2520labeled%2520datasets.%2520For%2520tasks%2520like%250Atext%2520embedding%252C%2520synthetic%2520data%2520offers%2520diverse%2520and%2520scalable%2520training%2520examples%252C%250Asignificantly%2520reducing%2520the%2520cost%2520of%2520human%2520annotation.%2520However%252C%2520most%2520current%250Aapproaches%2520rely%2520heavily%2520on%2520proprietary%2520models%2520like%2520GPT-4%252C%2520which%2520are%2520expensive%250Aand%2520inefficient%2520for%2520generating%2520large-scale%2520embedding%2520data.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520SPEED%252C%2520a%2520framework%2520that%2520aligns%2520open-source%2520small%2520models%2520%25288B%2529%2520to%250Aefficiently%2520generate%2520large-scale%2520synthetic%2520embedding%2520data.%2520Through%2520supervised%250Afine-tuning%252C%2520preference%2520optimization%252C%2520and%2520self-improvement%252C%2520SPEED%2520enables%2520small%250Aopen-source%2520models%2520to%2520produce%2520high-quality%2520data.%2520Remarkably%252C%2520SPEED%2520uses%2520only%250Aless%2520than%25201/10%2520of%2520the%2520GPT%2520API%2520calls%252C%2520outperforming%2520the%2520state-of-the-art%250Aembedding%2520model%2520E5_mistral%2520when%2520both%2520are%2520trained%2520solely%2520on%2520their%2520synthetic%250Adata.%2520Using%2520this%2520efficient%2520generator%252C%2520we%2520conduct%2520a%2520comprehensive%2520study%2520on%2520how%250Avarious%2520factors%2520within%2520the%2520alignment%2520pipeline%2520impact%2520data%2520quality%2520and%2520reveal%250Athe%2520scaling%2520law%2520for%2520synthetic%2520embedding%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Little%20Giants%3A%20Synthesizing%20High-Quality%20Embedding%20Data%20at%20Scale&entry.906535625=Haonan%20Chen%20and%20Liang%20Wang%20and%20Nan%20Yang%20and%20Yutao%20Zhu%20and%20Ziliang%20Zhao%20and%20Furu%20Wei%20and%20Zhicheng%20Dou&entry.1292438233=%20%20Synthetic%20data%20generation%20has%20become%20an%20increasingly%20popular%20way%20of%20training%0Amodels%20without%20the%20need%20for%20large%2C%20manually%20labeled%20datasets.%20For%20tasks%20like%0Atext%20embedding%2C%20synthetic%20data%20offers%20diverse%20and%20scalable%20training%20examples%2C%0Asignificantly%20reducing%20the%20cost%20of%20human%20annotation.%20However%2C%20most%20current%0Aapproaches%20rely%20heavily%20on%20proprietary%20models%20like%20GPT-4%2C%20which%20are%20expensive%0Aand%20inefficient%20for%20generating%20large-scale%20embedding%20data.%20In%20this%20paper%2C%20we%0Aintroduce%20SPEED%2C%20a%20framework%20that%20aligns%20open-source%20small%20models%20%288B%29%20to%0Aefficiently%20generate%20large-scale%20synthetic%20embedding%20data.%20Through%20supervised%0Afine-tuning%2C%20preference%20optimization%2C%20and%20self-improvement%2C%20SPEED%20enables%20small%0Aopen-source%20models%20to%20produce%20high-quality%20data.%20Remarkably%2C%20SPEED%20uses%20only%0Aless%20than%201/10%20of%20the%20GPT%20API%20calls%2C%20outperforming%20the%20state-of-the-art%0Aembedding%20model%20E5_mistral%20when%20both%20are%20trained%20solely%20on%20their%20synthetic%0Adata.%20Using%20this%20efficient%20generator%2C%20we%20conduct%20a%20comprehensive%20study%20on%20how%0Avarious%20factors%20within%20the%20alignment%20pipeline%20impact%20data%20quality%20and%20reveal%0Athe%20scaling%20law%20for%20synthetic%20embedding%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18634v1&entry.124074799=Read"},
{"title": "Neural Concept Binder", "author": "Wolfgang Stammer and Antonia W\u00fcst and David Steinmann and Kristian Kersting", "abstract": "  The challenge in object-based visual reasoning lies in generating concept\nrepresentations that are both descriptive and distinct. Achieving this in an\nunsupervised manner requires human users to understand the model's learned\nconcepts and, if necessary, revise incorrect ones. To address this challenge,\nwe introduce the Neural Concept Binder (NCB), a novel framework for deriving\nboth discrete and continuous concept representations, which we refer to as\n\"concept-slot encodings\". NCB employs two types of binding: \"soft binding\",\nwhich leverages the recent SysBinder mechanism to obtain object-factor\nencodings, and subsequent \"hard binding\", achieved through hierarchical\nclustering and retrieval-based inference. This enables obtaining expressive,\ndiscrete representations from unlabeled images. Moreover, the structured nature\nof NCB's concept representations allows for intuitive inspection and the\nstraightforward integration of external knowledge, such as human input or\ninsights from other AI models like GPT-4. Additionally, we demonstrate that\nincorporating the hard binding mechanism preserves model performance while\nenabling seamless integration into both neural and symbolic modules for complex\nreasoning tasks. We validate the effectiveness of NCB through evaluations on\nour newly introduced CLEVR-Sudoku dataset.\n", "link": "http://arxiv.org/abs/2406.09949v2", "date": "2024-10-24", "relevancy": 2.1023, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Concept%20Binder&body=Title%3A%20Neural%20Concept%20Binder%0AAuthor%3A%20Wolfgang%20Stammer%20and%20Antonia%20W%C3%BCst%20and%20David%20Steinmann%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20The%20challenge%20in%20object-based%20visual%20reasoning%20lies%20in%20generating%20concept%0Arepresentations%20that%20are%20both%20descriptive%20and%20distinct.%20Achieving%20this%20in%20an%0Aunsupervised%20manner%20requires%20human%20users%20to%20understand%20the%20model%27s%20learned%0Aconcepts%20and%2C%20if%20necessary%2C%20revise%20incorrect%20ones.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20the%20Neural%20Concept%20Binder%20%28NCB%29%2C%20a%20novel%20framework%20for%20deriving%0Aboth%20discrete%20and%20continuous%20concept%20representations%2C%20which%20we%20refer%20to%20as%0A%22concept-slot%20encodings%22.%20NCB%20employs%20two%20types%20of%20binding%3A%20%22soft%20binding%22%2C%0Awhich%20leverages%20the%20recent%20SysBinder%20mechanism%20to%20obtain%20object-factor%0Aencodings%2C%20and%20subsequent%20%22hard%20binding%22%2C%20achieved%20through%20hierarchical%0Aclustering%20and%20retrieval-based%20inference.%20This%20enables%20obtaining%20expressive%2C%0Adiscrete%20representations%20from%20unlabeled%20images.%20Moreover%2C%20the%20structured%20nature%0Aof%20NCB%27s%20concept%20representations%20allows%20for%20intuitive%20inspection%20and%20the%0Astraightforward%20integration%20of%20external%20knowledge%2C%20such%20as%20human%20input%20or%0Ainsights%20from%20other%20AI%20models%20like%20GPT-4.%20Additionally%2C%20we%20demonstrate%20that%0Aincorporating%20the%20hard%20binding%20mechanism%20preserves%20model%20performance%20while%0Aenabling%20seamless%20integration%20into%20both%20neural%20and%20symbolic%20modules%20for%20complex%0Areasoning%20tasks.%20We%20validate%20the%20effectiveness%20of%20NCB%20through%20evaluations%20on%0Aour%20newly%20introduced%20CLEVR-Sudoku%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09949v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Concept%2520Binder%26entry.906535625%3DWolfgang%2520Stammer%2520and%2520Antonia%2520W%25C3%25BCst%2520and%2520David%2520Steinmann%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520The%2520challenge%2520in%2520object-based%2520visual%2520reasoning%2520lies%2520in%2520generating%2520concept%250Arepresentations%2520that%2520are%2520both%2520descriptive%2520and%2520distinct.%2520Achieving%2520this%2520in%2520an%250Aunsupervised%2520manner%2520requires%2520human%2520users%2520to%2520understand%2520the%2520model%2527s%2520learned%250Aconcepts%2520and%252C%2520if%2520necessary%252C%2520revise%2520incorrect%2520ones.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520introduce%2520the%2520Neural%2520Concept%2520Binder%2520%2528NCB%2529%252C%2520a%2520novel%2520framework%2520for%2520deriving%250Aboth%2520discrete%2520and%2520continuous%2520concept%2520representations%252C%2520which%2520we%2520refer%2520to%2520as%250A%2522concept-slot%2520encodings%2522.%2520NCB%2520employs%2520two%2520types%2520of%2520binding%253A%2520%2522soft%2520binding%2522%252C%250Awhich%2520leverages%2520the%2520recent%2520SysBinder%2520mechanism%2520to%2520obtain%2520object-factor%250Aencodings%252C%2520and%2520subsequent%2520%2522hard%2520binding%2522%252C%2520achieved%2520through%2520hierarchical%250Aclustering%2520and%2520retrieval-based%2520inference.%2520This%2520enables%2520obtaining%2520expressive%252C%250Adiscrete%2520representations%2520from%2520unlabeled%2520images.%2520Moreover%252C%2520the%2520structured%2520nature%250Aof%2520NCB%2527s%2520concept%2520representations%2520allows%2520for%2520intuitive%2520inspection%2520and%2520the%250Astraightforward%2520integration%2520of%2520external%2520knowledge%252C%2520such%2520as%2520human%2520input%2520or%250Ainsights%2520from%2520other%2520AI%2520models%2520like%2520GPT-4.%2520Additionally%252C%2520we%2520demonstrate%2520that%250Aincorporating%2520the%2520hard%2520binding%2520mechanism%2520preserves%2520model%2520performance%2520while%250Aenabling%2520seamless%2520integration%2520into%2520both%2520neural%2520and%2520symbolic%2520modules%2520for%2520complex%250Areasoning%2520tasks.%2520We%2520validate%2520the%2520effectiveness%2520of%2520NCB%2520through%2520evaluations%2520on%250Aour%2520newly%2520introduced%2520CLEVR-Sudoku%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09949v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Concept%20Binder&entry.906535625=Wolfgang%20Stammer%20and%20Antonia%20W%C3%BCst%20and%20David%20Steinmann%20and%20Kristian%20Kersting&entry.1292438233=%20%20The%20challenge%20in%20object-based%20visual%20reasoning%20lies%20in%20generating%20concept%0Arepresentations%20that%20are%20both%20descriptive%20and%20distinct.%20Achieving%20this%20in%20an%0Aunsupervised%20manner%20requires%20human%20users%20to%20understand%20the%20model%27s%20learned%0Aconcepts%20and%2C%20if%20necessary%2C%20revise%20incorrect%20ones.%20To%20address%20this%20challenge%2C%0Awe%20introduce%20the%20Neural%20Concept%20Binder%20%28NCB%29%2C%20a%20novel%20framework%20for%20deriving%0Aboth%20discrete%20and%20continuous%20concept%20representations%2C%20which%20we%20refer%20to%20as%0A%22concept-slot%20encodings%22.%20NCB%20employs%20two%20types%20of%20binding%3A%20%22soft%20binding%22%2C%0Awhich%20leverages%20the%20recent%20SysBinder%20mechanism%20to%20obtain%20object-factor%0Aencodings%2C%20and%20subsequent%20%22hard%20binding%22%2C%20achieved%20through%20hierarchical%0Aclustering%20and%20retrieval-based%20inference.%20This%20enables%20obtaining%20expressive%2C%0Adiscrete%20representations%20from%20unlabeled%20images.%20Moreover%2C%20the%20structured%20nature%0Aof%20NCB%27s%20concept%20representations%20allows%20for%20intuitive%20inspection%20and%20the%0Astraightforward%20integration%20of%20external%20knowledge%2C%20such%20as%20human%20input%20or%0Ainsights%20from%20other%20AI%20models%20like%20GPT-4.%20Additionally%2C%20we%20demonstrate%20that%0Aincorporating%20the%20hard%20binding%20mechanism%20preserves%20model%20performance%20while%0Aenabling%20seamless%20integration%20into%20both%20neural%20and%20symbolic%20modules%20for%20complex%0Areasoning%20tasks.%20We%20validate%20the%20effectiveness%20of%20NCB%20through%20evaluations%20on%0Aour%20newly%20introduced%20CLEVR-Sudoku%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09949v2&entry.124074799=Read"},
{"title": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment", "author": "Yang Zhong and Jiangang Hao and Michael Fauss and Chen Li and Yuan Wang", "abstract": "  The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs.\n", "link": "http://arxiv.org/abs/2410.17439v2", "date": "2024-10-24", "relevancy": 2.0998, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4265}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4235}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20AI-Generated%20Essays%20with%20GRE%20Analytical%20Writing%20Assessment&body=Title%3A%20Evaluating%20AI-Generated%20Essays%20with%20GRE%20Analytical%20Writing%20Assessment%0AAuthor%3A%20Yang%20Zhong%20and%20Jiangang%20Hao%20and%20Michael%20Fauss%20and%20Chen%20Li%20and%20Yuan%20Wang%0AAbstract%3A%20%20%20The%20recent%20revolutionary%20advance%20in%20generative%20AI%20enables%20the%20generation%20of%0Arealistic%20and%20coherent%20texts%20by%20large%20language%20models%20%28LLMs%29.%20Despite%20many%0Aexisting%20evaluation%20metrics%20on%20the%20quality%20of%20the%20generated%20texts%2C%20there%20is%0Astill%20a%20lack%20of%20rigorous%20assessment%20of%20how%20well%20LLMs%20perform%20in%20complex%20and%0Ademanding%20writing%20assessments.%20This%20study%20examines%20essays%20generated%20by%20ten%0Aleading%20LLMs%20for%20the%20analytical%20writing%20assessment%20of%20the%20Graduate%20Record%20Exam%0A%28GRE%29.%20We%20assessed%20these%20essays%20using%20both%20human%20raters%20and%20the%20e-rater%0Aautomated%20scoring%20engine%20as%20used%20in%20the%20GRE%20scoring%20pipeline.%20Notably%2C%20the%0Atop-performing%20Gemini%20and%20GPT-4o%20received%20an%20average%20score%20of%204.78%20and%204.67%2C%0Arespectively%2C%20falling%20between%20%22generally%20thoughtful%2C%20well-developed%20analysis%20of%0Athe%20issue%20and%20conveys%20meaning%20clearly%22%20and%20%22presents%20a%20competent%20analysis%20of%0Athe%20issue%20and%20conveys%20meaning%20with%20acceptable%20clarity%22%20according%20to%20the%20GRE%0Ascoring%20guideline.%20We%20also%20evaluated%20the%20detection%20accuracy%20of%20these%20essays%2C%0Awith%20detectors%20trained%20on%20essays%20generated%20by%20the%20same%20and%20different%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520AI-Generated%2520Essays%2520with%2520GRE%2520Analytical%2520Writing%2520Assessment%26entry.906535625%3DYang%2520Zhong%2520and%2520Jiangang%2520Hao%2520and%2520Michael%2520Fauss%2520and%2520Chen%2520Li%2520and%2520Yuan%2520Wang%26entry.1292438233%3D%2520%2520The%2520recent%2520revolutionary%2520advance%2520in%2520generative%2520AI%2520enables%2520the%2520generation%2520of%250Arealistic%2520and%2520coherent%2520texts%2520by%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Despite%2520many%250Aexisting%2520evaluation%2520metrics%2520on%2520the%2520quality%2520of%2520the%2520generated%2520texts%252C%2520there%2520is%250Astill%2520a%2520lack%2520of%2520rigorous%2520assessment%2520of%2520how%2520well%2520LLMs%2520perform%2520in%2520complex%2520and%250Ademanding%2520writing%2520assessments.%2520This%2520study%2520examines%2520essays%2520generated%2520by%2520ten%250Aleading%2520LLMs%2520for%2520the%2520analytical%2520writing%2520assessment%2520of%2520the%2520Graduate%2520Record%2520Exam%250A%2528GRE%2529.%2520We%2520assessed%2520these%2520essays%2520using%2520both%2520human%2520raters%2520and%2520the%2520e-rater%250Aautomated%2520scoring%2520engine%2520as%2520used%2520in%2520the%2520GRE%2520scoring%2520pipeline.%2520Notably%252C%2520the%250Atop-performing%2520Gemini%2520and%2520GPT-4o%2520received%2520an%2520average%2520score%2520of%25204.78%2520and%25204.67%252C%250Arespectively%252C%2520falling%2520between%2520%2522generally%2520thoughtful%252C%2520well-developed%2520analysis%2520of%250Athe%2520issue%2520and%2520conveys%2520meaning%2520clearly%2522%2520and%2520%2522presents%2520a%2520competent%2520analysis%2520of%250Athe%2520issue%2520and%2520conveys%2520meaning%2520with%2520acceptable%2520clarity%2522%2520according%2520to%2520the%2520GRE%250Ascoring%2520guideline.%2520We%2520also%2520evaluated%2520the%2520detection%2520accuracy%2520of%2520these%2520essays%252C%250Awith%2520detectors%2520trained%2520on%2520essays%2520generated%2520by%2520the%2520same%2520and%2520different%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20AI-Generated%20Essays%20with%20GRE%20Analytical%20Writing%20Assessment&entry.906535625=Yang%20Zhong%20and%20Jiangang%20Hao%20and%20Michael%20Fauss%20and%20Chen%20Li%20and%20Yuan%20Wang&entry.1292438233=%20%20The%20recent%20revolutionary%20advance%20in%20generative%20AI%20enables%20the%20generation%20of%0Arealistic%20and%20coherent%20texts%20by%20large%20language%20models%20%28LLMs%29.%20Despite%20many%0Aexisting%20evaluation%20metrics%20on%20the%20quality%20of%20the%20generated%20texts%2C%20there%20is%0Astill%20a%20lack%20of%20rigorous%20assessment%20of%20how%20well%20LLMs%20perform%20in%20complex%20and%0Ademanding%20writing%20assessments.%20This%20study%20examines%20essays%20generated%20by%20ten%0Aleading%20LLMs%20for%20the%20analytical%20writing%20assessment%20of%20the%20Graduate%20Record%20Exam%0A%28GRE%29.%20We%20assessed%20these%20essays%20using%20both%20human%20raters%20and%20the%20e-rater%0Aautomated%20scoring%20engine%20as%20used%20in%20the%20GRE%20scoring%20pipeline.%20Notably%2C%20the%0Atop-performing%20Gemini%20and%20GPT-4o%20received%20an%20average%20score%20of%204.78%20and%204.67%2C%0Arespectively%2C%20falling%20between%20%22generally%20thoughtful%2C%20well-developed%20analysis%20of%0Athe%20issue%20and%20conveys%20meaning%20clearly%22%20and%20%22presents%20a%20competent%20analysis%20of%0Athe%20issue%20and%20conveys%20meaning%20with%20acceptable%20clarity%22%20according%20to%20the%20GRE%0Ascoring%20guideline.%20We%20also%20evaluated%20the%20detection%20accuracy%20of%20these%20essays%2C%0Awith%20detectors%20trained%20on%20essays%20generated%20by%20the%20same%20and%20different%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17439v2&entry.124074799=Read"},
{"title": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information", "author": "Andrew Robert Williams and Arjun Ashok and \u00c9tienne Marcotte and Valentina Zantedeschi and Jithendaraa Subramanian and Roland Riachi and James Requeima and Alexandre Lacoste and Irina Rish and Nicolas Chapados and Alexandre Drouin", "abstract": "  Forecasting is a critical task in decision making across various domains.\nWhile numerical data provides a foundation, it often lacks crucial context\nnecessary for accurate predictions. Human forecasters frequently rely on\nadditional information, such as background knowledge or constraints, which can\nbe efficiently communicated through natural language. However, the ability of\nexisting forecasting models to effectively integrate this textual information\nremains an open question. To address this, we introduce \"Context is Key\" (CiK),\na time series forecasting benchmark that pairs numerical data with diverse\ntypes of carefully crafted textual context, requiring models to integrate both\nmodalities. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. By\npresenting this benchmark, we aim to advance multimodal forecasting, promoting\nmodels that are both accurate and accessible to decision-makers with varied\ntechnical expertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/ .\n", "link": "http://arxiv.org/abs/2410.18959v1", "date": "2024-10-24", "relevancy": 2.0972, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20is%20Key%3A%20A%20Benchmark%20for%20Forecasting%20with%20Essential%20Textual%0A%20%20Information&body=Title%3A%20Context%20is%20Key%3A%20A%20Benchmark%20for%20Forecasting%20with%20Essential%20Textual%0A%20%20Information%0AAuthor%3A%20Andrew%20Robert%20Williams%20and%20Arjun%20Ashok%20and%20%C3%89tienne%20Marcotte%20and%20Valentina%20Zantedeschi%20and%20Jithendaraa%20Subramanian%20and%20Roland%20Riachi%20and%20James%20Requeima%20and%20Alexandre%20Lacoste%20and%20Irina%20Rish%20and%20Nicolas%20Chapados%20and%20Alexandre%20Drouin%0AAbstract%3A%20%20%20Forecasting%20is%20a%20critical%20task%20in%20decision%20making%20across%20various%20domains.%0AWhile%20numerical%20data%20provides%20a%20foundation%2C%20it%20often%20lacks%20crucial%20context%0Anecessary%20for%20accurate%20predictions.%20Human%20forecasters%20frequently%20rely%20on%0Aadditional%20information%2C%20such%20as%20background%20knowledge%20or%20constraints%2C%20which%20can%0Abe%20efficiently%20communicated%20through%20natural%20language.%20However%2C%20the%20ability%20of%0Aexisting%20forecasting%20models%20to%20effectively%20integrate%20this%20textual%20information%0Aremains%20an%20open%20question.%20To%20address%20this%2C%20we%20introduce%20%22Context%20is%20Key%22%20%28CiK%29%2C%0Aa%20time%20series%20forecasting%20benchmark%20that%20pairs%20numerical%20data%20with%20diverse%0Atypes%20of%20carefully%20crafted%20textual%20context%2C%20requiring%20models%20to%20integrate%20both%0Amodalities.%20We%20evaluate%20a%20range%20of%20approaches%2C%20including%20statistical%20models%2C%0Atime%20series%20foundation%20models%2C%20and%20LLM-based%20forecasters%2C%20and%20propose%20a%20simple%0Ayet%20effective%20LLM%20prompting%20method%20that%20outperforms%20all%20other%20tested%20methods%20on%0Aour%20benchmark.%20Our%20experiments%20highlight%20the%20importance%20of%20incorporating%0Acontextual%20information%2C%20demonstrate%20surprising%20performance%20when%20using%20LLM-based%0Aforecasting%20models%2C%20and%20also%20reveal%20some%20of%20their%20critical%20shortcomings.%20By%0Apresenting%20this%20benchmark%2C%20we%20aim%20to%20advance%20multimodal%20forecasting%2C%20promoting%0Amodels%20that%20are%20both%20accurate%20and%20accessible%20to%20decision-makers%20with%20varied%0Atechnical%20expertise.%20The%20benchmark%20can%20be%20visualized%20at%0Ahttps%3A//servicenow.github.io/context-is-key-forecasting/v0/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520is%2520Key%253A%2520A%2520Benchmark%2520for%2520Forecasting%2520with%2520Essential%2520Textual%250A%2520%2520Information%26entry.906535625%3DAndrew%2520Robert%2520Williams%2520and%2520Arjun%2520Ashok%2520and%2520%25C3%2589tienne%2520Marcotte%2520and%2520Valentina%2520Zantedeschi%2520and%2520Jithendaraa%2520Subramanian%2520and%2520Roland%2520Riachi%2520and%2520James%2520Requeima%2520and%2520Alexandre%2520Lacoste%2520and%2520Irina%2520Rish%2520and%2520Nicolas%2520Chapados%2520and%2520Alexandre%2520Drouin%26entry.1292438233%3D%2520%2520Forecasting%2520is%2520a%2520critical%2520task%2520in%2520decision%2520making%2520across%2520various%2520domains.%250AWhile%2520numerical%2520data%2520provides%2520a%2520foundation%252C%2520it%2520often%2520lacks%2520crucial%2520context%250Anecessary%2520for%2520accurate%2520predictions.%2520Human%2520forecasters%2520frequently%2520rely%2520on%250Aadditional%2520information%252C%2520such%2520as%2520background%2520knowledge%2520or%2520constraints%252C%2520which%2520can%250Abe%2520efficiently%2520communicated%2520through%2520natural%2520language.%2520However%252C%2520the%2520ability%2520of%250Aexisting%2520forecasting%2520models%2520to%2520effectively%2520integrate%2520this%2520textual%2520information%250Aremains%2520an%2520open%2520question.%2520To%2520address%2520this%252C%2520we%2520introduce%2520%2522Context%2520is%2520Key%2522%2520%2528CiK%2529%252C%250Aa%2520time%2520series%2520forecasting%2520benchmark%2520that%2520pairs%2520numerical%2520data%2520with%2520diverse%250Atypes%2520of%2520carefully%2520crafted%2520textual%2520context%252C%2520requiring%2520models%2520to%2520integrate%2520both%250Amodalities.%2520We%2520evaluate%2520a%2520range%2520of%2520approaches%252C%2520including%2520statistical%2520models%252C%250Atime%2520series%2520foundation%2520models%252C%2520and%2520LLM-based%2520forecasters%252C%2520and%2520propose%2520a%2520simple%250Ayet%2520effective%2520LLM%2520prompting%2520method%2520that%2520outperforms%2520all%2520other%2520tested%2520methods%2520on%250Aour%2520benchmark.%2520Our%2520experiments%2520highlight%2520the%2520importance%2520of%2520incorporating%250Acontextual%2520information%252C%2520demonstrate%2520surprising%2520performance%2520when%2520using%2520LLM-based%250Aforecasting%2520models%252C%2520and%2520also%2520reveal%2520some%2520of%2520their%2520critical%2520shortcomings.%2520By%250Apresenting%2520this%2520benchmark%252C%2520we%2520aim%2520to%2520advance%2520multimodal%2520forecasting%252C%2520promoting%250Amodels%2520that%2520are%2520both%2520accurate%2520and%2520accessible%2520to%2520decision-makers%2520with%2520varied%250Atechnical%2520expertise.%2520The%2520benchmark%2520can%2520be%2520visualized%2520at%250Ahttps%253A//servicenow.github.io/context-is-key-forecasting/v0/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20is%20Key%3A%20A%20Benchmark%20for%20Forecasting%20with%20Essential%20Textual%0A%20%20Information&entry.906535625=Andrew%20Robert%20Williams%20and%20Arjun%20Ashok%20and%20%C3%89tienne%20Marcotte%20and%20Valentina%20Zantedeschi%20and%20Jithendaraa%20Subramanian%20and%20Roland%20Riachi%20and%20James%20Requeima%20and%20Alexandre%20Lacoste%20and%20Irina%20Rish%20and%20Nicolas%20Chapados%20and%20Alexandre%20Drouin&entry.1292438233=%20%20Forecasting%20is%20a%20critical%20task%20in%20decision%20making%20across%20various%20domains.%0AWhile%20numerical%20data%20provides%20a%20foundation%2C%20it%20often%20lacks%20crucial%20context%0Anecessary%20for%20accurate%20predictions.%20Human%20forecasters%20frequently%20rely%20on%0Aadditional%20information%2C%20such%20as%20background%20knowledge%20or%20constraints%2C%20which%20can%0Abe%20efficiently%20communicated%20through%20natural%20language.%20However%2C%20the%20ability%20of%0Aexisting%20forecasting%20models%20to%20effectively%20integrate%20this%20textual%20information%0Aremains%20an%20open%20question.%20To%20address%20this%2C%20we%20introduce%20%22Context%20is%20Key%22%20%28CiK%29%2C%0Aa%20time%20series%20forecasting%20benchmark%20that%20pairs%20numerical%20data%20with%20diverse%0Atypes%20of%20carefully%20crafted%20textual%20context%2C%20requiring%20models%20to%20integrate%20both%0Amodalities.%20We%20evaluate%20a%20range%20of%20approaches%2C%20including%20statistical%20models%2C%0Atime%20series%20foundation%20models%2C%20and%20LLM-based%20forecasters%2C%20and%20propose%20a%20simple%0Ayet%20effective%20LLM%20prompting%20method%20that%20outperforms%20all%20other%20tested%20methods%20on%0Aour%20benchmark.%20Our%20experiments%20highlight%20the%20importance%20of%20incorporating%0Acontextual%20information%2C%20demonstrate%20surprising%20performance%20when%20using%20LLM-based%0Aforecasting%20models%2C%20and%20also%20reveal%20some%20of%20their%20critical%20shortcomings.%20By%0Apresenting%20this%20benchmark%2C%20we%20aim%20to%20advance%20multimodal%20forecasting%2C%20promoting%0Amodels%20that%20are%20both%20accurate%20and%20accessible%20to%20decision-makers%20with%20varied%0Atechnical%20expertise.%20The%20benchmark%20can%20be%20visualized%20at%0Ahttps%3A//servicenow.github.io/context-is-key-forecasting/v0/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18959v1&entry.124074799=Read"},
{"title": "WARP-LCA: Efficient Convolutional Sparse Coding with Locally Competitive\n  Algorithm", "author": "Geoffrey Kasenbacher and Felix Ehret and Gerrit Ecke and Sebastian Otte", "abstract": "  The locally competitive algorithm (LCA) can solve sparse coding problems\nacross a wide range of use cases. Recently, convolution-based LCA approaches\nhave been shown to be highly effective for enhancing robustness for image\nrecognition tasks in vision pipelines. To additionally maximize\nrepresentational sparsity, LCA with hard-thresholding can be applied. While\nthis combination often yields very good solutions satisfying an $\\ell_0$\nsparsity criterion, it comes with significant drawbacks for practical\napplication: (i) LCA is very inefficient, typically requiring hundreds of\noptimization cycles for convergence; (ii) the use of hard-thresholding results\nin a non-convex loss function, which might lead to suboptimal minima. To\naddress these issues, we propose the Locally Competitive Algorithm with State\nWarm-up via Predictive Priming (WARP-LCA), which leverages a predictor network\nto provide a suitable initial guess of the LCA state based on the current\ninput. Our approach significantly improves both convergence speed and the\nquality of solutions, while maintaining and even enhancing the overall\nstrengths of LCA. We demonstrate that WARP-LCA converges faster by orders of\nmagnitude and reaches better minima compared to conventional LCA. Moreover, the\nlearned representations are more sparse and exhibit superior properties in\nterms of reconstruction and denoising quality as well as robustness when\napplied in deep recognition pipelines. Furthermore, we apply WARP-LCA to image\ndenoising tasks, showcasing its robustness and practical effectiveness. Our\nfindings confirm that the naive use of LCA with hard-thresholding results in\nsuboptimal minima, whereas initializing LCA with a predictive guess results in\nbetter outcomes. This research advances the field of biologically inspired deep\nlearning by providing a novel approach to convolutional sparse coding.\n", "link": "http://arxiv.org/abs/2410.18794v1", "date": "2024-10-24", "relevancy": 2.0898, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5284}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WARP-LCA%3A%20Efficient%20Convolutional%20Sparse%20Coding%20with%20Locally%20Competitive%0A%20%20Algorithm&body=Title%3A%20WARP-LCA%3A%20Efficient%20Convolutional%20Sparse%20Coding%20with%20Locally%20Competitive%0A%20%20Algorithm%0AAuthor%3A%20Geoffrey%20Kasenbacher%20and%20Felix%20Ehret%20and%20Gerrit%20Ecke%20and%20Sebastian%20Otte%0AAbstract%3A%20%20%20The%20locally%20competitive%20algorithm%20%28LCA%29%20can%20solve%20sparse%20coding%20problems%0Aacross%20a%20wide%20range%20of%20use%20cases.%20Recently%2C%20convolution-based%20LCA%20approaches%0Ahave%20been%20shown%20to%20be%20highly%20effective%20for%20enhancing%20robustness%20for%20image%0Arecognition%20tasks%20in%20vision%20pipelines.%20To%20additionally%20maximize%0Arepresentational%20sparsity%2C%20LCA%20with%20hard-thresholding%20can%20be%20applied.%20While%0Athis%20combination%20often%20yields%20very%20good%20solutions%20satisfying%20an%20%24%5Cell_0%24%0Asparsity%20criterion%2C%20it%20comes%20with%20significant%20drawbacks%20for%20practical%0Aapplication%3A%20%28i%29%20LCA%20is%20very%20inefficient%2C%20typically%20requiring%20hundreds%20of%0Aoptimization%20cycles%20for%20convergence%3B%20%28ii%29%20the%20use%20of%20hard-thresholding%20results%0Ain%20a%20non-convex%20loss%20function%2C%20which%20might%20lead%20to%20suboptimal%20minima.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Locally%20Competitive%20Algorithm%20with%20State%0AWarm-up%20via%20Predictive%20Priming%20%28WARP-LCA%29%2C%20which%20leverages%20a%20predictor%20network%0Ato%20provide%20a%20suitable%20initial%20guess%20of%20the%20LCA%20state%20based%20on%20the%20current%0Ainput.%20Our%20approach%20significantly%20improves%20both%20convergence%20speed%20and%20the%0Aquality%20of%20solutions%2C%20while%20maintaining%20and%20even%20enhancing%20the%20overall%0Astrengths%20of%20LCA.%20We%20demonstrate%20that%20WARP-LCA%20converges%20faster%20by%20orders%20of%0Amagnitude%20and%20reaches%20better%20minima%20compared%20to%20conventional%20LCA.%20Moreover%2C%20the%0Alearned%20representations%20are%20more%20sparse%20and%20exhibit%20superior%20properties%20in%0Aterms%20of%20reconstruction%20and%20denoising%20quality%20as%20well%20as%20robustness%20when%0Aapplied%20in%20deep%20recognition%20pipelines.%20Furthermore%2C%20we%20apply%20WARP-LCA%20to%20image%0Adenoising%20tasks%2C%20showcasing%20its%20robustness%20and%20practical%20effectiveness.%20Our%0Afindings%20confirm%20that%20the%20naive%20use%20of%20LCA%20with%20hard-thresholding%20results%20in%0Asuboptimal%20minima%2C%20whereas%20initializing%20LCA%20with%20a%20predictive%20guess%20results%20in%0Abetter%20outcomes.%20This%20research%20advances%20the%20field%20of%20biologically%20inspired%20deep%0Alearning%20by%20providing%20a%20novel%20approach%20to%20convolutional%20sparse%20coding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWARP-LCA%253A%2520Efficient%2520Convolutional%2520Sparse%2520Coding%2520with%2520Locally%2520Competitive%250A%2520%2520Algorithm%26entry.906535625%3DGeoffrey%2520Kasenbacher%2520and%2520Felix%2520Ehret%2520and%2520Gerrit%2520Ecke%2520and%2520Sebastian%2520Otte%26entry.1292438233%3D%2520%2520The%2520locally%2520competitive%2520algorithm%2520%2528LCA%2529%2520can%2520solve%2520sparse%2520coding%2520problems%250Aacross%2520a%2520wide%2520range%2520of%2520use%2520cases.%2520Recently%252C%2520convolution-based%2520LCA%2520approaches%250Ahave%2520been%2520shown%2520to%2520be%2520highly%2520effective%2520for%2520enhancing%2520robustness%2520for%2520image%250Arecognition%2520tasks%2520in%2520vision%2520pipelines.%2520To%2520additionally%2520maximize%250Arepresentational%2520sparsity%252C%2520LCA%2520with%2520hard-thresholding%2520can%2520be%2520applied.%2520While%250Athis%2520combination%2520often%2520yields%2520very%2520good%2520solutions%2520satisfying%2520an%2520%2524%255Cell_0%2524%250Asparsity%2520criterion%252C%2520it%2520comes%2520with%2520significant%2520drawbacks%2520for%2520practical%250Aapplication%253A%2520%2528i%2529%2520LCA%2520is%2520very%2520inefficient%252C%2520typically%2520requiring%2520hundreds%2520of%250Aoptimization%2520cycles%2520for%2520convergence%253B%2520%2528ii%2529%2520the%2520use%2520of%2520hard-thresholding%2520results%250Ain%2520a%2520non-convex%2520loss%2520function%252C%2520which%2520might%2520lead%2520to%2520suboptimal%2520minima.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520the%2520Locally%2520Competitive%2520Algorithm%2520with%2520State%250AWarm-up%2520via%2520Predictive%2520Priming%2520%2528WARP-LCA%2529%252C%2520which%2520leverages%2520a%2520predictor%2520network%250Ato%2520provide%2520a%2520suitable%2520initial%2520guess%2520of%2520the%2520LCA%2520state%2520based%2520on%2520the%2520current%250Ainput.%2520Our%2520approach%2520significantly%2520improves%2520both%2520convergence%2520speed%2520and%2520the%250Aquality%2520of%2520solutions%252C%2520while%2520maintaining%2520and%2520even%2520enhancing%2520the%2520overall%250Astrengths%2520of%2520LCA.%2520We%2520demonstrate%2520that%2520WARP-LCA%2520converges%2520faster%2520by%2520orders%2520of%250Amagnitude%2520and%2520reaches%2520better%2520minima%2520compared%2520to%2520conventional%2520LCA.%2520Moreover%252C%2520the%250Alearned%2520representations%2520are%2520more%2520sparse%2520and%2520exhibit%2520superior%2520properties%2520in%250Aterms%2520of%2520reconstruction%2520and%2520denoising%2520quality%2520as%2520well%2520as%2520robustness%2520when%250Aapplied%2520in%2520deep%2520recognition%2520pipelines.%2520Furthermore%252C%2520we%2520apply%2520WARP-LCA%2520to%2520image%250Adenoising%2520tasks%252C%2520showcasing%2520its%2520robustness%2520and%2520practical%2520effectiveness.%2520Our%250Afindings%2520confirm%2520that%2520the%2520naive%2520use%2520of%2520LCA%2520with%2520hard-thresholding%2520results%2520in%250Asuboptimal%2520minima%252C%2520whereas%2520initializing%2520LCA%2520with%2520a%2520predictive%2520guess%2520results%2520in%250Abetter%2520outcomes.%2520This%2520research%2520advances%2520the%2520field%2520of%2520biologically%2520inspired%2520deep%250Alearning%2520by%2520providing%2520a%2520novel%2520approach%2520to%2520convolutional%2520sparse%2520coding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WARP-LCA%3A%20Efficient%20Convolutional%20Sparse%20Coding%20with%20Locally%20Competitive%0A%20%20Algorithm&entry.906535625=Geoffrey%20Kasenbacher%20and%20Felix%20Ehret%20and%20Gerrit%20Ecke%20and%20Sebastian%20Otte&entry.1292438233=%20%20The%20locally%20competitive%20algorithm%20%28LCA%29%20can%20solve%20sparse%20coding%20problems%0Aacross%20a%20wide%20range%20of%20use%20cases.%20Recently%2C%20convolution-based%20LCA%20approaches%0Ahave%20been%20shown%20to%20be%20highly%20effective%20for%20enhancing%20robustness%20for%20image%0Arecognition%20tasks%20in%20vision%20pipelines.%20To%20additionally%20maximize%0Arepresentational%20sparsity%2C%20LCA%20with%20hard-thresholding%20can%20be%20applied.%20While%0Athis%20combination%20often%20yields%20very%20good%20solutions%20satisfying%20an%20%24%5Cell_0%24%0Asparsity%20criterion%2C%20it%20comes%20with%20significant%20drawbacks%20for%20practical%0Aapplication%3A%20%28i%29%20LCA%20is%20very%20inefficient%2C%20typically%20requiring%20hundreds%20of%0Aoptimization%20cycles%20for%20convergence%3B%20%28ii%29%20the%20use%20of%20hard-thresholding%20results%0Ain%20a%20non-convex%20loss%20function%2C%20which%20might%20lead%20to%20suboptimal%20minima.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Locally%20Competitive%20Algorithm%20with%20State%0AWarm-up%20via%20Predictive%20Priming%20%28WARP-LCA%29%2C%20which%20leverages%20a%20predictor%20network%0Ato%20provide%20a%20suitable%20initial%20guess%20of%20the%20LCA%20state%20based%20on%20the%20current%0Ainput.%20Our%20approach%20significantly%20improves%20both%20convergence%20speed%20and%20the%0Aquality%20of%20solutions%2C%20while%20maintaining%20and%20even%20enhancing%20the%20overall%0Astrengths%20of%20LCA.%20We%20demonstrate%20that%20WARP-LCA%20converges%20faster%20by%20orders%20of%0Amagnitude%20and%20reaches%20better%20minima%20compared%20to%20conventional%20LCA.%20Moreover%2C%20the%0Alearned%20representations%20are%20more%20sparse%20and%20exhibit%20superior%20properties%20in%0Aterms%20of%20reconstruction%20and%20denoising%20quality%20as%20well%20as%20robustness%20when%0Aapplied%20in%20deep%20recognition%20pipelines.%20Furthermore%2C%20we%20apply%20WARP-LCA%20to%20image%0Adenoising%20tasks%2C%20showcasing%20its%20robustness%20and%20practical%20effectiveness.%20Our%0Afindings%20confirm%20that%20the%20naive%20use%20of%20LCA%20with%20hard-thresholding%20results%20in%0Asuboptimal%20minima%2C%20whereas%20initializing%20LCA%20with%20a%20predictive%20guess%20results%20in%0Abetter%20outcomes.%20This%20research%20advances%20the%20field%20of%20biologically%20inspired%20deep%0Alearning%20by%20providing%20a%20novel%20approach%20to%20convolutional%20sparse%20coding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18794v1&entry.124074799=Read"},
{"title": "Unearthing Skill-Level Insights for Understanding Trade-Offs of\n  Foundation Models", "author": "Mazda Moayeri and Vidhisha Balachandran and Varun Chandrasekaran and Safoora Yousefi and Thomas Fel and Soheil Feizi and Besmira Nushi and Neel Joshi and Vibhav Vineet", "abstract": "  With models getting stronger, evaluations have grown more complex, testing\nmultiple skills in one benchmark and even in the same instance at once.\nHowever, skill-wise performance is obscured when inspecting aggregate accuracy,\nunder-utilizing the rich signal modern benchmarks contain. We propose an\nautomatic approach to recover the underlying skills relevant for any evaluation\ninstance, by way of inspecting model-generated rationales. After validating the\nrelevance of rationale-parsed skills and inferring skills for $46$k instances\nover $12$ benchmarks, we observe many skills to be common across benchmarks,\nresulting in the curation of hundreds of skill-slices (i.e. sets of instances\ntesting a common skill). Inspecting accuracy over these slices yields novel\ninsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,\non average, Gemini 1.5 Pro is $18\\%$ more accurate in \"computing molar mass\",\nbut $19\\%$ less accurate in \"applying constitutional law\", despite the overall\naccuracies of the three models differing by a mere $0.4\\%$. Furthermore, we\ndemonstrate the practical utility of our approach by showing that insights\nderived from skill slice analysis can generalize to held-out instances: when\nrouting each instance to the model strongest on the relevant skills, we see a\n$3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and\nframework open a new avenue in model evaluation, leveraging skill-specific\nanalyses to unlock a more granular and actionable understanding of model\ncapabilities.\n", "link": "http://arxiv.org/abs/2410.13826v2", "date": "2024-10-24", "relevancy": 2.0815, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unearthing%20Skill-Level%20Insights%20for%20Understanding%20Trade-Offs%20of%0A%20%20Foundation%20Models&body=Title%3A%20Unearthing%20Skill-Level%20Insights%20for%20Understanding%20Trade-Offs%20of%0A%20%20Foundation%20Models%0AAuthor%3A%20Mazda%20Moayeri%20and%20Vidhisha%20Balachandran%20and%20Varun%20Chandrasekaran%20and%20Safoora%20Yousefi%20and%20Thomas%20Fel%20and%20Soheil%20Feizi%20and%20Besmira%20Nushi%20and%20Neel%20Joshi%20and%20Vibhav%20Vineet%0AAbstract%3A%20%20%20With%20models%20getting%20stronger%2C%20evaluations%20have%20grown%20more%20complex%2C%20testing%0Amultiple%20skills%20in%20one%20benchmark%20and%20even%20in%20the%20same%20instance%20at%20once.%0AHowever%2C%20skill-wise%20performance%20is%20obscured%20when%20inspecting%20aggregate%20accuracy%2C%0Aunder-utilizing%20the%20rich%20signal%20modern%20benchmarks%20contain.%20We%20propose%20an%0Aautomatic%20approach%20to%20recover%20the%20underlying%20skills%20relevant%20for%20any%20evaluation%0Ainstance%2C%20by%20way%20of%20inspecting%20model-generated%20rationales.%20After%20validating%20the%0Arelevance%20of%20rationale-parsed%20skills%20and%20inferring%20skills%20for%20%2446%24k%20instances%0Aover%20%2412%24%20benchmarks%2C%20we%20observe%20many%20skills%20to%20be%20common%20across%20benchmarks%2C%0Aresulting%20in%20the%20curation%20of%20hundreds%20of%20skill-slices%20%28i.e.%20sets%20of%20instances%0Atesting%20a%20common%20skill%29.%20Inspecting%20accuracy%20over%20these%20slices%20yields%20novel%0Ainsights%20on%20model%20trade-offs%3A%20e.g.%2C%20compared%20to%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%0Aon%20average%2C%20Gemini%201.5%20Pro%20is%20%2418%5C%25%24%20more%20accurate%20in%20%22computing%20molar%20mass%22%2C%0Abut%20%2419%5C%25%24%20less%20accurate%20in%20%22applying%20constitutional%20law%22%2C%20despite%20the%20overall%0Aaccuracies%20of%20the%20three%20models%20differing%20by%20a%20mere%20%240.4%5C%25%24.%20Furthermore%2C%20we%0Ademonstrate%20the%20practical%20utility%20of%20our%20approach%20by%20showing%20that%20insights%0Aderived%20from%20skill%20slice%20analysis%20can%20generalize%20to%20held-out%20instances%3A%20when%0Arouting%20each%20instance%20to%20the%20model%20strongest%20on%20the%20relevant%20skills%2C%20we%20see%20a%0A%243%5C%25%24%20accuracy%20improvement%20over%20our%20%2412%24%20dataset%20corpus.%20Our%20skill-slices%20and%0Aframework%20open%20a%20new%20avenue%20in%20model%20evaluation%2C%20leveraging%20skill-specific%0Aanalyses%20to%20unlock%20a%20more%20granular%20and%20actionable%20understanding%20of%20model%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13826v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnearthing%2520Skill-Level%2520Insights%2520for%2520Understanding%2520Trade-Offs%2520of%250A%2520%2520Foundation%2520Models%26entry.906535625%3DMazda%2520Moayeri%2520and%2520Vidhisha%2520Balachandran%2520and%2520Varun%2520Chandrasekaran%2520and%2520Safoora%2520Yousefi%2520and%2520Thomas%2520Fel%2520and%2520Soheil%2520Feizi%2520and%2520Besmira%2520Nushi%2520and%2520Neel%2520Joshi%2520and%2520Vibhav%2520Vineet%26entry.1292438233%3D%2520%2520With%2520models%2520getting%2520stronger%252C%2520evaluations%2520have%2520grown%2520more%2520complex%252C%2520testing%250Amultiple%2520skills%2520in%2520one%2520benchmark%2520and%2520even%2520in%2520the%2520same%2520instance%2520at%2520once.%250AHowever%252C%2520skill-wise%2520performance%2520is%2520obscured%2520when%2520inspecting%2520aggregate%2520accuracy%252C%250Aunder-utilizing%2520the%2520rich%2520signal%2520modern%2520benchmarks%2520contain.%2520We%2520propose%2520an%250Aautomatic%2520approach%2520to%2520recover%2520the%2520underlying%2520skills%2520relevant%2520for%2520any%2520evaluation%250Ainstance%252C%2520by%2520way%2520of%2520inspecting%2520model-generated%2520rationales.%2520After%2520validating%2520the%250Arelevance%2520of%2520rationale-parsed%2520skills%2520and%2520inferring%2520skills%2520for%2520%252446%2524k%2520instances%250Aover%2520%252412%2524%2520benchmarks%252C%2520we%2520observe%2520many%2520skills%2520to%2520be%2520common%2520across%2520benchmarks%252C%250Aresulting%2520in%2520the%2520curation%2520of%2520hundreds%2520of%2520skill-slices%2520%2528i.e.%2520sets%2520of%2520instances%250Atesting%2520a%2520common%2520skill%2529.%2520Inspecting%2520accuracy%2520over%2520these%2520slices%2520yields%2520novel%250Ainsights%2520on%2520model%2520trade-offs%253A%2520e.g.%252C%2520compared%2520to%2520GPT-4o%2520and%2520Claude%25203.5%2520Sonnet%252C%250Aon%2520average%252C%2520Gemini%25201.5%2520Pro%2520is%2520%252418%255C%2525%2524%2520more%2520accurate%2520in%2520%2522computing%2520molar%2520mass%2522%252C%250Abut%2520%252419%255C%2525%2524%2520less%2520accurate%2520in%2520%2522applying%2520constitutional%2520law%2522%252C%2520despite%2520the%2520overall%250Aaccuracies%2520of%2520the%2520three%2520models%2520differing%2520by%2520a%2520mere%2520%25240.4%255C%2525%2524.%2520Furthermore%252C%2520we%250Ademonstrate%2520the%2520practical%2520utility%2520of%2520our%2520approach%2520by%2520showing%2520that%2520insights%250Aderived%2520from%2520skill%2520slice%2520analysis%2520can%2520generalize%2520to%2520held-out%2520instances%253A%2520when%250Arouting%2520each%2520instance%2520to%2520the%2520model%2520strongest%2520on%2520the%2520relevant%2520skills%252C%2520we%2520see%2520a%250A%25243%255C%2525%2524%2520accuracy%2520improvement%2520over%2520our%2520%252412%2524%2520dataset%2520corpus.%2520Our%2520skill-slices%2520and%250Aframework%2520open%2520a%2520new%2520avenue%2520in%2520model%2520evaluation%252C%2520leveraging%2520skill-specific%250Aanalyses%2520to%2520unlock%2520a%2520more%2520granular%2520and%2520actionable%2520understanding%2520of%2520model%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13826v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unearthing%20Skill-Level%20Insights%20for%20Understanding%20Trade-Offs%20of%0A%20%20Foundation%20Models&entry.906535625=Mazda%20Moayeri%20and%20Vidhisha%20Balachandran%20and%20Varun%20Chandrasekaran%20and%20Safoora%20Yousefi%20and%20Thomas%20Fel%20and%20Soheil%20Feizi%20and%20Besmira%20Nushi%20and%20Neel%20Joshi%20and%20Vibhav%20Vineet&entry.1292438233=%20%20With%20models%20getting%20stronger%2C%20evaluations%20have%20grown%20more%20complex%2C%20testing%0Amultiple%20skills%20in%20one%20benchmark%20and%20even%20in%20the%20same%20instance%20at%20once.%0AHowever%2C%20skill-wise%20performance%20is%20obscured%20when%20inspecting%20aggregate%20accuracy%2C%0Aunder-utilizing%20the%20rich%20signal%20modern%20benchmarks%20contain.%20We%20propose%20an%0Aautomatic%20approach%20to%20recover%20the%20underlying%20skills%20relevant%20for%20any%20evaluation%0Ainstance%2C%20by%20way%20of%20inspecting%20model-generated%20rationales.%20After%20validating%20the%0Arelevance%20of%20rationale-parsed%20skills%20and%20inferring%20skills%20for%20%2446%24k%20instances%0Aover%20%2412%24%20benchmarks%2C%20we%20observe%20many%20skills%20to%20be%20common%20across%20benchmarks%2C%0Aresulting%20in%20the%20curation%20of%20hundreds%20of%20skill-slices%20%28i.e.%20sets%20of%20instances%0Atesting%20a%20common%20skill%29.%20Inspecting%20accuracy%20over%20these%20slices%20yields%20novel%0Ainsights%20on%20model%20trade-offs%3A%20e.g.%2C%20compared%20to%20GPT-4o%20and%20Claude%203.5%20Sonnet%2C%0Aon%20average%2C%20Gemini%201.5%20Pro%20is%20%2418%5C%25%24%20more%20accurate%20in%20%22computing%20molar%20mass%22%2C%0Abut%20%2419%5C%25%24%20less%20accurate%20in%20%22applying%20constitutional%20law%22%2C%20despite%20the%20overall%0Aaccuracies%20of%20the%20three%20models%20differing%20by%20a%20mere%20%240.4%5C%25%24.%20Furthermore%2C%20we%0Ademonstrate%20the%20practical%20utility%20of%20our%20approach%20by%20showing%20that%20insights%0Aderived%20from%20skill%20slice%20analysis%20can%20generalize%20to%20held-out%20instances%3A%20when%0Arouting%20each%20instance%20to%20the%20model%20strongest%20on%20the%20relevant%20skills%2C%20we%20see%20a%0A%243%5C%25%24%20accuracy%20improvement%20over%20our%20%2412%24%20dataset%20corpus.%20Our%20skill-slices%20and%0Aframework%20open%20a%20new%20avenue%20in%20model%20evaluation%2C%20leveraging%20skill-specific%0Aanalyses%20to%20unlock%20a%20more%20granular%20and%20actionable%20understanding%20of%20model%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13826v2&entry.124074799=Read"},
{"title": "Optimizing Edge Offloading Decisions for Object Detection", "author": "Jiaming Qiu and Ruiqi Wang and Brooks Hu and Roch Guerin and Chenyang Lu", "abstract": "  Recent advances in machine learning and hardware have produced embedded\ndevices capable of performing real-time object detection with commendable\naccuracy. We consider a scenario in which embedded devices rely on an onboard\nobject detector, but have the option to offload detection to a more powerful\nedge server when local accuracy is deemed too low. Resource constraints,\nhowever, limit the number of images that can be offloaded to the edge. Our goal\nis to identify which images to offload to maximize overall detection accuracy\nunder those constraints. To that end, the paper introduces a reward metric\ndesigned to quantify potential accuracy improvements from offloading individual\nimages, and proposes an efficient approach to make offloading decisions by\nestimating this reward based only on local detection results. The approach is\ncomputationally frugal enough to run on embedded devices, and empirical\nfindings indicate that it outperforms existing alternatives in improving\ndetection accuracy even when the fraction of offloaded images is small.\n", "link": "http://arxiv.org/abs/2410.18919v1", "date": "2024-10-24", "relevancy": 2.079, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5316}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5185}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Edge%20Offloading%20Decisions%20for%20Object%20Detection&body=Title%3A%20Optimizing%20Edge%20Offloading%20Decisions%20for%20Object%20Detection%0AAuthor%3A%20Jiaming%20Qiu%20and%20Ruiqi%20Wang%20and%20Brooks%20Hu%20and%20Roch%20Guerin%20and%20Chenyang%20Lu%0AAbstract%3A%20%20%20Recent%20advances%20in%20machine%20learning%20and%20hardware%20have%20produced%20embedded%0Adevices%20capable%20of%20performing%20real-time%20object%20detection%20with%20commendable%0Aaccuracy.%20We%20consider%20a%20scenario%20in%20which%20embedded%20devices%20rely%20on%20an%20onboard%0Aobject%20detector%2C%20but%20have%20the%20option%20to%20offload%20detection%20to%20a%20more%20powerful%0Aedge%20server%20when%20local%20accuracy%20is%20deemed%20too%20low.%20Resource%20constraints%2C%0Ahowever%2C%20limit%20the%20number%20of%20images%20that%20can%20be%20offloaded%20to%20the%20edge.%20Our%20goal%0Ais%20to%20identify%20which%20images%20to%20offload%20to%20maximize%20overall%20detection%20accuracy%0Aunder%20those%20constraints.%20To%20that%20end%2C%20the%20paper%20introduces%20a%20reward%20metric%0Adesigned%20to%20quantify%20potential%20accuracy%20improvements%20from%20offloading%20individual%0Aimages%2C%20and%20proposes%20an%20efficient%20approach%20to%20make%20offloading%20decisions%20by%0Aestimating%20this%20reward%20based%20only%20on%20local%20detection%20results.%20The%20approach%20is%0Acomputationally%20frugal%20enough%20to%20run%20on%20embedded%20devices%2C%20and%20empirical%0Afindings%20indicate%20that%20it%20outperforms%20existing%20alternatives%20in%20improving%0Adetection%20accuracy%20even%20when%20the%20fraction%20of%20offloaded%20images%20is%20small.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Edge%2520Offloading%2520Decisions%2520for%2520Object%2520Detection%26entry.906535625%3DJiaming%2520Qiu%2520and%2520Ruiqi%2520Wang%2520and%2520Brooks%2520Hu%2520and%2520Roch%2520Guerin%2520and%2520Chenyang%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520machine%2520learning%2520and%2520hardware%2520have%2520produced%2520embedded%250Adevices%2520capable%2520of%2520performing%2520real-time%2520object%2520detection%2520with%2520commendable%250Aaccuracy.%2520We%2520consider%2520a%2520scenario%2520in%2520which%2520embedded%2520devices%2520rely%2520on%2520an%2520onboard%250Aobject%2520detector%252C%2520but%2520have%2520the%2520option%2520to%2520offload%2520detection%2520to%2520a%2520more%2520powerful%250Aedge%2520server%2520when%2520local%2520accuracy%2520is%2520deemed%2520too%2520low.%2520Resource%2520constraints%252C%250Ahowever%252C%2520limit%2520the%2520number%2520of%2520images%2520that%2520can%2520be%2520offloaded%2520to%2520the%2520edge.%2520Our%2520goal%250Ais%2520to%2520identify%2520which%2520images%2520to%2520offload%2520to%2520maximize%2520overall%2520detection%2520accuracy%250Aunder%2520those%2520constraints.%2520To%2520that%2520end%252C%2520the%2520paper%2520introduces%2520a%2520reward%2520metric%250Adesigned%2520to%2520quantify%2520potential%2520accuracy%2520improvements%2520from%2520offloading%2520individual%250Aimages%252C%2520and%2520proposes%2520an%2520efficient%2520approach%2520to%2520make%2520offloading%2520decisions%2520by%250Aestimating%2520this%2520reward%2520based%2520only%2520on%2520local%2520detection%2520results.%2520The%2520approach%2520is%250Acomputationally%2520frugal%2520enough%2520to%2520run%2520on%2520embedded%2520devices%252C%2520and%2520empirical%250Afindings%2520indicate%2520that%2520it%2520outperforms%2520existing%2520alternatives%2520in%2520improving%250Adetection%2520accuracy%2520even%2520when%2520the%2520fraction%2520of%2520offloaded%2520images%2520is%2520small.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Edge%20Offloading%20Decisions%20for%20Object%20Detection&entry.906535625=Jiaming%20Qiu%20and%20Ruiqi%20Wang%20and%20Brooks%20Hu%20and%20Roch%20Guerin%20and%20Chenyang%20Lu&entry.1292438233=%20%20Recent%20advances%20in%20machine%20learning%20and%20hardware%20have%20produced%20embedded%0Adevices%20capable%20of%20performing%20real-time%20object%20detection%20with%20commendable%0Aaccuracy.%20We%20consider%20a%20scenario%20in%20which%20embedded%20devices%20rely%20on%20an%20onboard%0Aobject%20detector%2C%20but%20have%20the%20option%20to%20offload%20detection%20to%20a%20more%20powerful%0Aedge%20server%20when%20local%20accuracy%20is%20deemed%20too%20low.%20Resource%20constraints%2C%0Ahowever%2C%20limit%20the%20number%20of%20images%20that%20can%20be%20offloaded%20to%20the%20edge.%20Our%20goal%0Ais%20to%20identify%20which%20images%20to%20offload%20to%20maximize%20overall%20detection%20accuracy%0Aunder%20those%20constraints.%20To%20that%20end%2C%20the%20paper%20introduces%20a%20reward%20metric%0Adesigned%20to%20quantify%20potential%20accuracy%20improvements%20from%20offloading%20individual%0Aimages%2C%20and%20proposes%20an%20efficient%20approach%20to%20make%20offloading%20decisions%20by%0Aestimating%20this%20reward%20based%20only%20on%20local%20detection%20results.%20The%20approach%20is%0Acomputationally%20frugal%20enough%20to%20run%20on%20embedded%20devices%2C%20and%20empirical%0Afindings%20indicate%20that%20it%20outperforms%20existing%20alternatives%20in%20improving%0Adetection%20accuracy%20even%20when%20the%20fraction%20of%20offloaded%20images%20is%20small.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18919v1&entry.124074799=Read"},
{"title": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced\n  Time Series Classification", "author": "Xiaoyu Tao and Tingyue Pan and Mingyue Cheng and Yucong Luo", "abstract": "  Leveraging large language models (LLMs) has garnered increasing attention and\nintroduced novel perspectives in time series classification. However, existing\napproaches often overlook the crucial dynamic temporal information inherent in\ntime series data and face challenges in aligning this data with textual\nsemantics. To address these limitations, we propose HiTime, a hierarchical\nmulti-modal model that seamlessly integrates temporal information into LLMs for\nmultivariate time series classification (MTSC). Our model employs a\nhierarchical feature encoder to capture diverse aspects of time series data\nthrough both data-specific and task-specific embeddings. To facilitate semantic\nspace alignment between time series and text, we introduce a dual-view\ncontrastive alignment module that bridges the gap between modalities.\nAdditionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained\nLLM in a parameter-efficient manner. By effectively incorporating dynamic\ntemporal features and ensuring semantic alignment, HiTime enables LLMs to\nprocess continuous time series data and achieves state-of-the-art\nclassification performance through text generation. Extensive experiments on\nbenchmark datasets demonstrate that HiTime significantly enhances time series\nclassification accuracy compared to most competitive baseline methods. Our\nfindings highlight the potential of integrating temporal features into LLMs,\npaving the way for advanced time series analysis. The code is publicly\navailable for further research and validation. Our codes are publicly\navailable1.\n", "link": "http://arxiv.org/abs/2410.18686v1", "date": "2024-10-24", "relevancy": 2.0789, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5012}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Multimodal%20LLMs%20with%20Semantic%20Space%20Alignment%20for%20Enhanced%0A%20%20Time%20Series%20Classification&body=Title%3A%20Hierarchical%20Multimodal%20LLMs%20with%20Semantic%20Space%20Alignment%20for%20Enhanced%0A%20%20Time%20Series%20Classification%0AAuthor%3A%20Xiaoyu%20Tao%20and%20Tingyue%20Pan%20and%20Mingyue%20Cheng%20and%20Yucong%20Luo%0AAbstract%3A%20%20%20Leveraging%20large%20language%20models%20%28LLMs%29%20has%20garnered%20increasing%20attention%20and%0Aintroduced%20novel%20perspectives%20in%20time%20series%20classification.%20However%2C%20existing%0Aapproaches%20often%20overlook%20the%20crucial%20dynamic%20temporal%20information%20inherent%20in%0Atime%20series%20data%20and%20face%20challenges%20in%20aligning%20this%20data%20with%20textual%0Asemantics.%20To%20address%20these%20limitations%2C%20we%20propose%20HiTime%2C%20a%20hierarchical%0Amulti-modal%20model%20that%20seamlessly%20integrates%20temporal%20information%20into%20LLMs%20for%0Amultivariate%20time%20series%20classification%20%28MTSC%29.%20Our%20model%20employs%20a%0Ahierarchical%20feature%20encoder%20to%20capture%20diverse%20aspects%20of%20time%20series%20data%0Athrough%20both%20data-specific%20and%20task-specific%20embeddings.%20To%20facilitate%20semantic%0Aspace%20alignment%20between%20time%20series%20and%20text%2C%20we%20introduce%20a%20dual-view%0Acontrastive%20alignment%20module%20that%20bridges%20the%20gap%20between%20modalities.%0AAdditionally%2C%20we%20adopt%20a%20hybrid%20prompting%20strategy%20to%20fine-tune%20the%20pre-trained%0ALLM%20in%20a%20parameter-efficient%20manner.%20By%20effectively%20incorporating%20dynamic%0Atemporal%20features%20and%20ensuring%20semantic%20alignment%2C%20HiTime%20enables%20LLMs%20to%0Aprocess%20continuous%20time%20series%20data%20and%20achieves%20state-of-the-art%0Aclassification%20performance%20through%20text%20generation.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20HiTime%20significantly%20enhances%20time%20series%0Aclassification%20accuracy%20compared%20to%20most%20competitive%20baseline%20methods.%20Our%0Afindings%20highlight%20the%20potential%20of%20integrating%20temporal%20features%20into%20LLMs%2C%0Apaving%20the%20way%20for%20advanced%20time%20series%20analysis.%20The%20code%20is%20publicly%0Aavailable%20for%20further%20research%20and%20validation.%20Our%20codes%20are%20publicly%0Aavailable1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Multimodal%2520LLMs%2520with%2520Semantic%2520Space%2520Alignment%2520for%2520Enhanced%250A%2520%2520Time%2520Series%2520Classification%26entry.906535625%3DXiaoyu%2520Tao%2520and%2520Tingyue%2520Pan%2520and%2520Mingyue%2520Cheng%2520and%2520Yucong%2520Luo%26entry.1292438233%3D%2520%2520Leveraging%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520garnered%2520increasing%2520attention%2520and%250Aintroduced%2520novel%2520perspectives%2520in%2520time%2520series%2520classification.%2520However%252C%2520existing%250Aapproaches%2520often%2520overlook%2520the%2520crucial%2520dynamic%2520temporal%2520information%2520inherent%2520in%250Atime%2520series%2520data%2520and%2520face%2520challenges%2520in%2520aligning%2520this%2520data%2520with%2520textual%250Asemantics.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520HiTime%252C%2520a%2520hierarchical%250Amulti-modal%2520model%2520that%2520seamlessly%2520integrates%2520temporal%2520information%2520into%2520LLMs%2520for%250Amultivariate%2520time%2520series%2520classification%2520%2528MTSC%2529.%2520Our%2520model%2520employs%2520a%250Ahierarchical%2520feature%2520encoder%2520to%2520capture%2520diverse%2520aspects%2520of%2520time%2520series%2520data%250Athrough%2520both%2520data-specific%2520and%2520task-specific%2520embeddings.%2520To%2520facilitate%2520semantic%250Aspace%2520alignment%2520between%2520time%2520series%2520and%2520text%252C%2520we%2520introduce%2520a%2520dual-view%250Acontrastive%2520alignment%2520module%2520that%2520bridges%2520the%2520gap%2520between%2520modalities.%250AAdditionally%252C%2520we%2520adopt%2520a%2520hybrid%2520prompting%2520strategy%2520to%2520fine-tune%2520the%2520pre-trained%250ALLM%2520in%2520a%2520parameter-efficient%2520manner.%2520By%2520effectively%2520incorporating%2520dynamic%250Atemporal%2520features%2520and%2520ensuring%2520semantic%2520alignment%252C%2520HiTime%2520enables%2520LLMs%2520to%250Aprocess%2520continuous%2520time%2520series%2520data%2520and%2520achieves%2520state-of-the-art%250Aclassification%2520performance%2520through%2520text%2520generation.%2520Extensive%2520experiments%2520on%250Abenchmark%2520datasets%2520demonstrate%2520that%2520HiTime%2520significantly%2520enhances%2520time%2520series%250Aclassification%2520accuracy%2520compared%2520to%2520most%2520competitive%2520baseline%2520methods.%2520Our%250Afindings%2520highlight%2520the%2520potential%2520of%2520integrating%2520temporal%2520features%2520into%2520LLMs%252C%250Apaving%2520the%2520way%2520for%2520advanced%2520time%2520series%2520analysis.%2520The%2520code%2520is%2520publicly%250Aavailable%2520for%2520further%2520research%2520and%2520validation.%2520Our%2520codes%2520are%2520publicly%250Aavailable1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Multimodal%20LLMs%20with%20Semantic%20Space%20Alignment%20for%20Enhanced%0A%20%20Time%20Series%20Classification&entry.906535625=Xiaoyu%20Tao%20and%20Tingyue%20Pan%20and%20Mingyue%20Cheng%20and%20Yucong%20Luo&entry.1292438233=%20%20Leveraging%20large%20language%20models%20%28LLMs%29%20has%20garnered%20increasing%20attention%20and%0Aintroduced%20novel%20perspectives%20in%20time%20series%20classification.%20However%2C%20existing%0Aapproaches%20often%20overlook%20the%20crucial%20dynamic%20temporal%20information%20inherent%20in%0Atime%20series%20data%20and%20face%20challenges%20in%20aligning%20this%20data%20with%20textual%0Asemantics.%20To%20address%20these%20limitations%2C%20we%20propose%20HiTime%2C%20a%20hierarchical%0Amulti-modal%20model%20that%20seamlessly%20integrates%20temporal%20information%20into%20LLMs%20for%0Amultivariate%20time%20series%20classification%20%28MTSC%29.%20Our%20model%20employs%20a%0Ahierarchical%20feature%20encoder%20to%20capture%20diverse%20aspects%20of%20time%20series%20data%0Athrough%20both%20data-specific%20and%20task-specific%20embeddings.%20To%20facilitate%20semantic%0Aspace%20alignment%20between%20time%20series%20and%20text%2C%20we%20introduce%20a%20dual-view%0Acontrastive%20alignment%20module%20that%20bridges%20the%20gap%20between%20modalities.%0AAdditionally%2C%20we%20adopt%20a%20hybrid%20prompting%20strategy%20to%20fine-tune%20the%20pre-trained%0ALLM%20in%20a%20parameter-efficient%20manner.%20By%20effectively%20incorporating%20dynamic%0Atemporal%20features%20and%20ensuring%20semantic%20alignment%2C%20HiTime%20enables%20LLMs%20to%0Aprocess%20continuous%20time%20series%20data%20and%20achieves%20state-of-the-art%0Aclassification%20performance%20through%20text%20generation.%20Extensive%20experiments%20on%0Abenchmark%20datasets%20demonstrate%20that%20HiTime%20significantly%20enhances%20time%20series%0Aclassification%20accuracy%20compared%20to%20most%20competitive%20baseline%20methods.%20Our%0Afindings%20highlight%20the%20potential%20of%20integrating%20temporal%20features%20into%20LLMs%2C%0Apaving%20the%20way%20for%20advanced%20time%20series%20analysis.%20The%20code%20is%20publicly%0Aavailable%20for%20further%20research%20and%20validation.%20Our%20codes%20are%20publicly%0Aavailable1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18686v1&entry.124074799=Read"},
{"title": "SegLLM: Multi-round Reasoning Segmentation", "author": "XuDong Wang and Shaolun Zhang and Shufan Li and Konstantinos Kallidromitis and Kehan Li and Yusuke Kato and Kazuki Kozuka and Trevor Darrell", "abstract": "  We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization.\n", "link": "http://arxiv.org/abs/2410.18923v1", "date": "2024-10-24", "relevancy": 2.074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegLLM%3A%20Multi-round%20Reasoning%20Segmentation&body=Title%3A%20SegLLM%3A%20Multi-round%20Reasoning%20Segmentation%0AAuthor%3A%20XuDong%20Wang%20and%20Shaolun%20Zhang%20and%20Shufan%20Li%20and%20Konstantinos%20Kallidromitis%20and%20Kehan%20Li%20and%20Yusuke%20Kato%20and%20Kazuki%20Kozuka%20and%20Trevor%20Darrell%0AAbstract%3A%20%20%20We%20present%20SegLLM%2C%20a%20novel%20multi-round%20interactive%20reasoning%20segmentation%0Amodel%20that%20enhances%20LLM-based%20segmentation%20by%20exploiting%20conversational%20memory%0Aof%20both%20visual%20and%20textual%20outputs.%20By%20leveraging%20a%20mask-aware%20multimodal%20LLM%2C%0ASegLLM%20re-integrates%20previous%20segmentation%20results%20into%20its%20input%20stream%2C%0Aenabling%20it%20to%20reason%20about%20complex%20user%20intentions%20and%20segment%20objects%20in%0Arelation%20to%20previously%20identified%20entities%2C%20including%20positional%2C%0Ainteractional%2C%20and%20hierarchical%20relationships%2C%20across%20multiple%20interactions.%0AThis%20capability%20allows%20SegLLM%20to%20respond%20to%20visual%20and%20text%20queries%20in%20a%0Achat-like%20manner.%20Evaluated%20on%20the%20newly%20curated%20MRSeg%20benchmark%2C%20SegLLM%0Aoutperforms%20existing%20methods%20in%20multi-round%20interactive%20reasoning%20segmentation%0Aby%20over%2020%25.%20Additionally%2C%20we%20observed%20that%20training%20on%20multi-round%20reasoning%0Asegmentation%20data%20enhances%20performance%20on%20standard%20single-round%20referring%0Asegmentation%20and%20localization%20tasks%2C%20resulting%20in%20a%205.5%25%20increase%20in%20cIoU%20for%0Areferring%20expression%20segmentation%20and%20a%204.5%25%20improvement%20in%20Acc%400.5%20for%0Areferring%20expression%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegLLM%253A%2520Multi-round%2520Reasoning%2520Segmentation%26entry.906535625%3DXuDong%2520Wang%2520and%2520Shaolun%2520Zhang%2520and%2520Shufan%2520Li%2520and%2520Konstantinos%2520Kallidromitis%2520and%2520Kehan%2520Li%2520and%2520Yusuke%2520Kato%2520and%2520Kazuki%2520Kozuka%2520and%2520Trevor%2520Darrell%26entry.1292438233%3D%2520%2520We%2520present%2520SegLLM%252C%2520a%2520novel%2520multi-round%2520interactive%2520reasoning%2520segmentation%250Amodel%2520that%2520enhances%2520LLM-based%2520segmentation%2520by%2520exploiting%2520conversational%2520memory%250Aof%2520both%2520visual%2520and%2520textual%2520outputs.%2520By%2520leveraging%2520a%2520mask-aware%2520multimodal%2520LLM%252C%250ASegLLM%2520re-integrates%2520previous%2520segmentation%2520results%2520into%2520its%2520input%2520stream%252C%250Aenabling%2520it%2520to%2520reason%2520about%2520complex%2520user%2520intentions%2520and%2520segment%2520objects%2520in%250Arelation%2520to%2520previously%2520identified%2520entities%252C%2520including%2520positional%252C%250Ainteractional%252C%2520and%2520hierarchical%2520relationships%252C%2520across%2520multiple%2520interactions.%250AThis%2520capability%2520allows%2520SegLLM%2520to%2520respond%2520to%2520visual%2520and%2520text%2520queries%2520in%2520a%250Achat-like%2520manner.%2520Evaluated%2520on%2520the%2520newly%2520curated%2520MRSeg%2520benchmark%252C%2520SegLLM%250Aoutperforms%2520existing%2520methods%2520in%2520multi-round%2520interactive%2520reasoning%2520segmentation%250Aby%2520over%252020%2525.%2520Additionally%252C%2520we%2520observed%2520that%2520training%2520on%2520multi-round%2520reasoning%250Asegmentation%2520data%2520enhances%2520performance%2520on%2520standard%2520single-round%2520referring%250Asegmentation%2520and%2520localization%2520tasks%252C%2520resulting%2520in%2520a%25205.5%2525%2520increase%2520in%2520cIoU%2520for%250Areferring%2520expression%2520segmentation%2520and%2520a%25204.5%2525%2520improvement%2520in%2520Acc%25400.5%2520for%250Areferring%2520expression%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegLLM%3A%20Multi-round%20Reasoning%20Segmentation&entry.906535625=XuDong%20Wang%20and%20Shaolun%20Zhang%20and%20Shufan%20Li%20and%20Konstantinos%20Kallidromitis%20and%20Kehan%20Li%20and%20Yusuke%20Kato%20and%20Kazuki%20Kozuka%20and%20Trevor%20Darrell&entry.1292438233=%20%20We%20present%20SegLLM%2C%20a%20novel%20multi-round%20interactive%20reasoning%20segmentation%0Amodel%20that%20enhances%20LLM-based%20segmentation%20by%20exploiting%20conversational%20memory%0Aof%20both%20visual%20and%20textual%20outputs.%20By%20leveraging%20a%20mask-aware%20multimodal%20LLM%2C%0ASegLLM%20re-integrates%20previous%20segmentation%20results%20into%20its%20input%20stream%2C%0Aenabling%20it%20to%20reason%20about%20complex%20user%20intentions%20and%20segment%20objects%20in%0Arelation%20to%20previously%20identified%20entities%2C%20including%20positional%2C%0Ainteractional%2C%20and%20hierarchical%20relationships%2C%20across%20multiple%20interactions.%0AThis%20capability%20allows%20SegLLM%20to%20respond%20to%20visual%20and%20text%20queries%20in%20a%0Achat-like%20manner.%20Evaluated%20on%20the%20newly%20curated%20MRSeg%20benchmark%2C%20SegLLM%0Aoutperforms%20existing%20methods%20in%20multi-round%20interactive%20reasoning%20segmentation%0Aby%20over%2020%25.%20Additionally%2C%20we%20observed%20that%20training%20on%20multi-round%20reasoning%0Asegmentation%20data%20enhances%20performance%20on%20standard%20single-round%20referring%0Asegmentation%20and%20localization%20tasks%2C%20resulting%20in%20a%205.5%25%20increase%20in%20cIoU%20for%0Areferring%20expression%20segmentation%20and%20a%204.5%25%20improvement%20in%20Acc%400.5%20for%0Areferring%20expression%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18923v1&entry.124074799=Read"},
{"title": "Rule Extrapolation in Language Models: A Study of Compositional\n  Generalization on OOD Prompts", "author": "Anna M\u00e9sz\u00e1ros and Szilvia Ujv\u00e1ry and Wieland Brendel and Patrik Reizinger and Ferenc Husz\u00e1r", "abstract": "  LLMs show remarkable emergent abilities, such as inferring concepts from\npresumably out-of-distribution prompts, known as in-context learning. Though\nthis success is often attributed to the Transformer architecture, our\nsystematic understanding is limited. In complex real-world data sets, even\ndefining what is out-of-distribution is not obvious. To better understand the\nOOD behaviour of autoregressive LLMs, we focus on formal languages, which are\ndefined by the intersection of rules. We define a new scenario of OOD\ncompositional generalization, termed rule extrapolation. Rule extrapolation\ndescribes OOD scenarios, where the prompt violates at least one rule. We\nevaluate rule extrapolation in formal languages with varying complexity in\nlinear and recurrent architectures, the Transformer, and state space models to\nunderstand the architectures' influence on rule extrapolation. We also lay the\nfirst stones of a normative theory of rule extrapolation, inspired by the\nSolomonoff prior in algorithmic information theory.\n", "link": "http://arxiv.org/abs/2409.13728v2", "date": "2024-10-24", "relevancy": 2.0634, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rule%20Extrapolation%20in%20Language%20Models%3A%20A%20Study%20of%20Compositional%0A%20%20Generalization%20on%20OOD%20Prompts&body=Title%3A%20Rule%20Extrapolation%20in%20Language%20Models%3A%20A%20Study%20of%20Compositional%0A%20%20Generalization%20on%20OOD%20Prompts%0AAuthor%3A%20Anna%20M%C3%A9sz%C3%A1ros%20and%20Szilvia%20Ujv%C3%A1ry%20and%20Wieland%20Brendel%20and%20Patrik%20Reizinger%20and%20Ferenc%20Husz%C3%A1r%0AAbstract%3A%20%20%20LLMs%20show%20remarkable%20emergent%20abilities%2C%20such%20as%20inferring%20concepts%20from%0Apresumably%20out-of-distribution%20prompts%2C%20known%20as%20in-context%20learning.%20Though%0Athis%20success%20is%20often%20attributed%20to%20the%20Transformer%20architecture%2C%20our%0Asystematic%20understanding%20is%20limited.%20In%20complex%20real-world%20data%20sets%2C%20even%0Adefining%20what%20is%20out-of-distribution%20is%20not%20obvious.%20To%20better%20understand%20the%0AOOD%20behaviour%20of%20autoregressive%20LLMs%2C%20we%20focus%20on%20formal%20languages%2C%20which%20are%0Adefined%20by%20the%20intersection%20of%20rules.%20We%20define%20a%20new%20scenario%20of%20OOD%0Acompositional%20generalization%2C%20termed%20rule%20extrapolation.%20Rule%20extrapolation%0Adescribes%20OOD%20scenarios%2C%20where%20the%20prompt%20violates%20at%20least%20one%20rule.%20We%0Aevaluate%20rule%20extrapolation%20in%20formal%20languages%20with%20varying%20complexity%20in%0Alinear%20and%20recurrent%20architectures%2C%20the%20Transformer%2C%20and%20state%20space%20models%20to%0Aunderstand%20the%20architectures%27%20influence%20on%20rule%20extrapolation.%20We%20also%20lay%20the%0Afirst%20stones%20of%20a%20normative%20theory%20of%20rule%20extrapolation%2C%20inspired%20by%20the%0ASolomonoff%20prior%20in%20algorithmic%20information%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13728v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRule%2520Extrapolation%2520in%2520Language%2520Models%253A%2520A%2520Study%2520of%2520Compositional%250A%2520%2520Generalization%2520on%2520OOD%2520Prompts%26entry.906535625%3DAnna%2520M%25C3%25A9sz%25C3%25A1ros%2520and%2520Szilvia%2520Ujv%25C3%25A1ry%2520and%2520Wieland%2520Brendel%2520and%2520Patrik%2520Reizinger%2520and%2520Ferenc%2520Husz%25C3%25A1r%26entry.1292438233%3D%2520%2520LLMs%2520show%2520remarkable%2520emergent%2520abilities%252C%2520such%2520as%2520inferring%2520concepts%2520from%250Apresumably%2520out-of-distribution%2520prompts%252C%2520known%2520as%2520in-context%2520learning.%2520Though%250Athis%2520success%2520is%2520often%2520attributed%2520to%2520the%2520Transformer%2520architecture%252C%2520our%250Asystematic%2520understanding%2520is%2520limited.%2520In%2520complex%2520real-world%2520data%2520sets%252C%2520even%250Adefining%2520what%2520is%2520out-of-distribution%2520is%2520not%2520obvious.%2520To%2520better%2520understand%2520the%250AOOD%2520behaviour%2520of%2520autoregressive%2520LLMs%252C%2520we%2520focus%2520on%2520formal%2520languages%252C%2520which%2520are%250Adefined%2520by%2520the%2520intersection%2520of%2520rules.%2520We%2520define%2520a%2520new%2520scenario%2520of%2520OOD%250Acompositional%2520generalization%252C%2520termed%2520rule%2520extrapolation.%2520Rule%2520extrapolation%250Adescribes%2520OOD%2520scenarios%252C%2520where%2520the%2520prompt%2520violates%2520at%2520least%2520one%2520rule.%2520We%250Aevaluate%2520rule%2520extrapolation%2520in%2520formal%2520languages%2520with%2520varying%2520complexity%2520in%250Alinear%2520and%2520recurrent%2520architectures%252C%2520the%2520Transformer%252C%2520and%2520state%2520space%2520models%2520to%250Aunderstand%2520the%2520architectures%2527%2520influence%2520on%2520rule%2520extrapolation.%2520We%2520also%2520lay%2520the%250Afirst%2520stones%2520of%2520a%2520normative%2520theory%2520of%2520rule%2520extrapolation%252C%2520inspired%2520by%2520the%250ASolomonoff%2520prior%2520in%2520algorithmic%2520information%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13728v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rule%20Extrapolation%20in%20Language%20Models%3A%20A%20Study%20of%20Compositional%0A%20%20Generalization%20on%20OOD%20Prompts&entry.906535625=Anna%20M%C3%A9sz%C3%A1ros%20and%20Szilvia%20Ujv%C3%A1ry%20and%20Wieland%20Brendel%20and%20Patrik%20Reizinger%20and%20Ferenc%20Husz%C3%A1r&entry.1292438233=%20%20LLMs%20show%20remarkable%20emergent%20abilities%2C%20such%20as%20inferring%20concepts%20from%0Apresumably%20out-of-distribution%20prompts%2C%20known%20as%20in-context%20learning.%20Though%0Athis%20success%20is%20often%20attributed%20to%20the%20Transformer%20architecture%2C%20our%0Asystematic%20understanding%20is%20limited.%20In%20complex%20real-world%20data%20sets%2C%20even%0Adefining%20what%20is%20out-of-distribution%20is%20not%20obvious.%20To%20better%20understand%20the%0AOOD%20behaviour%20of%20autoregressive%20LLMs%2C%20we%20focus%20on%20formal%20languages%2C%20which%20are%0Adefined%20by%20the%20intersection%20of%20rules.%20We%20define%20a%20new%20scenario%20of%20OOD%0Acompositional%20generalization%2C%20termed%20rule%20extrapolation.%20Rule%20extrapolation%0Adescribes%20OOD%20scenarios%2C%20where%20the%20prompt%20violates%20at%20least%20one%20rule.%20We%0Aevaluate%20rule%20extrapolation%20in%20formal%20languages%20with%20varying%20complexity%20in%0Alinear%20and%20recurrent%20architectures%2C%20the%20Transformer%2C%20and%20state%20space%20models%20to%0Aunderstand%20the%20architectures%27%20influence%20on%20rule%20extrapolation.%20We%20also%20lay%20the%0Afirst%20stones%20of%20a%20normative%20theory%20of%20rule%20extrapolation%2C%20inspired%20by%20the%0ASolomonoff%20prior%20in%20algorithmic%20information%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13728v2&entry.124074799=Read"},
{"title": "Enhancing pretraining efficiency for medical image segmentation via\n  transferability metrics", "author": "G\u00e1bor Hidy and Bence Bakos and Andr\u00e1s Luk\u00e1cs", "abstract": "  In medical image segmentation tasks, the scarcity of labeled training data\nposes a significant challenge when training deep neural networks. When using\nU-Net-style architectures, it is common practice to address this problem by\npretraining the encoder part on a large general-purpose dataset like ImageNet.\nHowever, these methods are resource-intensive and do not guarantee improved\nperformance on the downstream task. In this paper we investigate a variety of\ntraining setups on medical image segmentation datasets, using\nImageNet-pretrained models. By examining over 300 combinations of models,\ndatasets, and training methods, we find that shorter pretraining often leads to\nbetter results on the downstream task, providing additional proof to the\nwell-known fact that the accuracy of the model on ImageNet is a poor indicator\nfor downstream performance. As our main contribution, we introduce a novel\ntransferability metric, based on contrastive learning, that measures how\nrobustly a pretrained model is able to represent the target data. In contrast\nto other transferability scores, our method is applicable to the case of\ntransferring from ImageNet classification to medical image segmentation. We\napply our robustness score by measuring it throughout the pretraining phase to\nindicate when the model weights are optimal for downstream transfer. This\nreduces pretraining time and improves results on the target task.\n", "link": "http://arxiv.org/abs/2410.18677v1", "date": "2024-10-24", "relevancy": 2.0616, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5211}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5181}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20pretraining%20efficiency%20for%20medical%20image%20segmentation%20via%0A%20%20transferability%20metrics&body=Title%3A%20Enhancing%20pretraining%20efficiency%20for%20medical%20image%20segmentation%20via%0A%20%20transferability%20metrics%0AAuthor%3A%20G%C3%A1bor%20Hidy%20and%20Bence%20Bakos%20and%20Andr%C3%A1s%20Luk%C3%A1cs%0AAbstract%3A%20%20%20In%20medical%20image%20segmentation%20tasks%2C%20the%20scarcity%20of%20labeled%20training%20data%0Aposes%20a%20significant%20challenge%20when%20training%20deep%20neural%20networks.%20When%20using%0AU-Net-style%20architectures%2C%20it%20is%20common%20practice%20to%20address%20this%20problem%20by%0Apretraining%20the%20encoder%20part%20on%20a%20large%20general-purpose%20dataset%20like%20ImageNet.%0AHowever%2C%20these%20methods%20are%20resource-intensive%20and%20do%20not%20guarantee%20improved%0Aperformance%20on%20the%20downstream%20task.%20In%20this%20paper%20we%20investigate%20a%20variety%20of%0Atraining%20setups%20on%20medical%20image%20segmentation%20datasets%2C%20using%0AImageNet-pretrained%20models.%20By%20examining%20over%20300%20combinations%20of%20models%2C%0Adatasets%2C%20and%20training%20methods%2C%20we%20find%20that%20shorter%20pretraining%20often%20leads%20to%0Abetter%20results%20on%20the%20downstream%20task%2C%20providing%20additional%20proof%20to%20the%0Awell-known%20fact%20that%20the%20accuracy%20of%20the%20model%20on%20ImageNet%20is%20a%20poor%20indicator%0Afor%20downstream%20performance.%20As%20our%20main%20contribution%2C%20we%20introduce%20a%20novel%0Atransferability%20metric%2C%20based%20on%20contrastive%20learning%2C%20that%20measures%20how%0Arobustly%20a%20pretrained%20model%20is%20able%20to%20represent%20the%20target%20data.%20In%20contrast%0Ato%20other%20transferability%20scores%2C%20our%20method%20is%20applicable%20to%20the%20case%20of%0Atransferring%20from%20ImageNet%20classification%20to%20medical%20image%20segmentation.%20We%0Aapply%20our%20robustness%20score%20by%20measuring%20it%20throughout%20the%20pretraining%20phase%20to%0Aindicate%20when%20the%20model%20weights%20are%20optimal%20for%20downstream%20transfer.%20This%0Areduces%20pretraining%20time%20and%20improves%20results%20on%20the%20target%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520pretraining%2520efficiency%2520for%2520medical%2520image%2520segmentation%2520via%250A%2520%2520transferability%2520metrics%26entry.906535625%3DG%25C3%25A1bor%2520Hidy%2520and%2520Bence%2520Bakos%2520and%2520Andr%25C3%25A1s%2520Luk%25C3%25A1cs%26entry.1292438233%3D%2520%2520In%2520medical%2520image%2520segmentation%2520tasks%252C%2520the%2520scarcity%2520of%2520labeled%2520training%2520data%250Aposes%2520a%2520significant%2520challenge%2520when%2520training%2520deep%2520neural%2520networks.%2520When%2520using%250AU-Net-style%2520architectures%252C%2520it%2520is%2520common%2520practice%2520to%2520address%2520this%2520problem%2520by%250Apretraining%2520the%2520encoder%2520part%2520on%2520a%2520large%2520general-purpose%2520dataset%2520like%2520ImageNet.%250AHowever%252C%2520these%2520methods%2520are%2520resource-intensive%2520and%2520do%2520not%2520guarantee%2520improved%250Aperformance%2520on%2520the%2520downstream%2520task.%2520In%2520this%2520paper%2520we%2520investigate%2520a%2520variety%2520of%250Atraining%2520setups%2520on%2520medical%2520image%2520segmentation%2520datasets%252C%2520using%250AImageNet-pretrained%2520models.%2520By%2520examining%2520over%2520300%2520combinations%2520of%2520models%252C%250Adatasets%252C%2520and%2520training%2520methods%252C%2520we%2520find%2520that%2520shorter%2520pretraining%2520often%2520leads%2520to%250Abetter%2520results%2520on%2520the%2520downstream%2520task%252C%2520providing%2520additional%2520proof%2520to%2520the%250Awell-known%2520fact%2520that%2520the%2520accuracy%2520of%2520the%2520model%2520on%2520ImageNet%2520is%2520a%2520poor%2520indicator%250Afor%2520downstream%2520performance.%2520As%2520our%2520main%2520contribution%252C%2520we%2520introduce%2520a%2520novel%250Atransferability%2520metric%252C%2520based%2520on%2520contrastive%2520learning%252C%2520that%2520measures%2520how%250Arobustly%2520a%2520pretrained%2520model%2520is%2520able%2520to%2520represent%2520the%2520target%2520data.%2520In%2520contrast%250Ato%2520other%2520transferability%2520scores%252C%2520our%2520method%2520is%2520applicable%2520to%2520the%2520case%2520of%250Atransferring%2520from%2520ImageNet%2520classification%2520to%2520medical%2520image%2520segmentation.%2520We%250Aapply%2520our%2520robustness%2520score%2520by%2520measuring%2520it%2520throughout%2520the%2520pretraining%2520phase%2520to%250Aindicate%2520when%2520the%2520model%2520weights%2520are%2520optimal%2520for%2520downstream%2520transfer.%2520This%250Areduces%2520pretraining%2520time%2520and%2520improves%2520results%2520on%2520the%2520target%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20pretraining%20efficiency%20for%20medical%20image%20segmentation%20via%0A%20%20transferability%20metrics&entry.906535625=G%C3%A1bor%20Hidy%20and%20Bence%20Bakos%20and%20Andr%C3%A1s%20Luk%C3%A1cs&entry.1292438233=%20%20In%20medical%20image%20segmentation%20tasks%2C%20the%20scarcity%20of%20labeled%20training%20data%0Aposes%20a%20significant%20challenge%20when%20training%20deep%20neural%20networks.%20When%20using%0AU-Net-style%20architectures%2C%20it%20is%20common%20practice%20to%20address%20this%20problem%20by%0Apretraining%20the%20encoder%20part%20on%20a%20large%20general-purpose%20dataset%20like%20ImageNet.%0AHowever%2C%20these%20methods%20are%20resource-intensive%20and%20do%20not%20guarantee%20improved%0Aperformance%20on%20the%20downstream%20task.%20In%20this%20paper%20we%20investigate%20a%20variety%20of%0Atraining%20setups%20on%20medical%20image%20segmentation%20datasets%2C%20using%0AImageNet-pretrained%20models.%20By%20examining%20over%20300%20combinations%20of%20models%2C%0Adatasets%2C%20and%20training%20methods%2C%20we%20find%20that%20shorter%20pretraining%20often%20leads%20to%0Abetter%20results%20on%20the%20downstream%20task%2C%20providing%20additional%20proof%20to%20the%0Awell-known%20fact%20that%20the%20accuracy%20of%20the%20model%20on%20ImageNet%20is%20a%20poor%20indicator%0Afor%20downstream%20performance.%20As%20our%20main%20contribution%2C%20we%20introduce%20a%20novel%0Atransferability%20metric%2C%20based%20on%20contrastive%20learning%2C%20that%20measures%20how%0Arobustly%20a%20pretrained%20model%20is%20able%20to%20represent%20the%20target%20data.%20In%20contrast%0Ato%20other%20transferability%20scores%2C%20our%20method%20is%20applicable%20to%20the%20case%20of%0Atransferring%20from%20ImageNet%20classification%20to%20medical%20image%20segmentation.%20We%0Aapply%20our%20robustness%20score%20by%20measuring%20it%20throughout%20the%20pretraining%20phase%20to%0Aindicate%20when%20the%20model%20weights%20are%20optimal%20for%20downstream%20transfer.%20This%0Areduces%20pretraining%20time%20and%20improves%20results%20on%20the%20target%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18677v1&entry.124074799=Read"},
{"title": "AgentStore: Scalable Integration of Heterogeneous Agents As Specialized\n  Generalist Computer Assistant", "author": "Chengyou Jia and Minnan Luo and Zhuohang Dang and Qiushi Sun and Fangzhi Xu and Junlin Hu and Tianbao Xie and Zhiyong Wu", "abstract": "  Digital agents capable of automating complex computer tasks have attracted\nconsiderable attention due to their immense potential to enhance human-computer\ninteraction. However, existing agent methods exhibit deficiencies in their\ngeneralization and specialization capabilities, especially in handling\nopen-ended computer tasks in real-world environments. Inspired by the rich\nfunctionality of the App store, we present AgentStore, a scalable platform\ndesigned to dynamically integrate heterogeneous agents for automating computer\ntasks. AgentStore empowers users to integrate third-party agents, allowing the\nsystem to continuously enrich its capabilities and adapt to rapidly evolving\noperating systems. Additionally, we propose a novel core \\textbf{MetaAgent}\nwith the \\textbf{AgentToken} strategy to efficiently manage diverse agents and\nutilize their specialized and generalist abilities for both domain-specific and\nsystem-wide tasks. Extensive experiments on three challenging benchmarks\ndemonstrate that AgentStore surpasses the limitations of previous systems with\nnarrow capabilities, particularly achieving a significant improvement from\n11.21\\% to 23.85\\% on the OSWorld benchmark, more than doubling the previous\nresults. Comprehensive quantitative and qualitative results further demonstrate\nAgentStore's ability to enhance agent systems in both generalization and\nspecialization, underscoring its potential for developing the specialized\ngeneralist computer assistant. All our codes will be made publicly available in\nhttps://chengyou-jia.github.io/AgentStore-Home.\n", "link": "http://arxiv.org/abs/2410.18603v1", "date": "2024-10-24", "relevancy": 2.0528, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.527}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5165}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentStore%3A%20Scalable%20Integration%20of%20Heterogeneous%20Agents%20As%20Specialized%0A%20%20Generalist%20Computer%20Assistant&body=Title%3A%20AgentStore%3A%20Scalable%20Integration%20of%20Heterogeneous%20Agents%20As%20Specialized%0A%20%20Generalist%20Computer%20Assistant%0AAuthor%3A%20Chengyou%20Jia%20and%20Minnan%20Luo%20and%20Zhuohang%20Dang%20and%20Qiushi%20Sun%20and%20Fangzhi%20Xu%20and%20Junlin%20Hu%20and%20Tianbao%20Xie%20and%20Zhiyong%20Wu%0AAbstract%3A%20%20%20Digital%20agents%20capable%20of%20automating%20complex%20computer%20tasks%20have%20attracted%0Aconsiderable%20attention%20due%20to%20their%20immense%20potential%20to%20enhance%20human-computer%0Ainteraction.%20However%2C%20existing%20agent%20methods%20exhibit%20deficiencies%20in%20their%0Ageneralization%20and%20specialization%20capabilities%2C%20especially%20in%20handling%0Aopen-ended%20computer%20tasks%20in%20real-world%20environments.%20Inspired%20by%20the%20rich%0Afunctionality%20of%20the%20App%20store%2C%20we%20present%20AgentStore%2C%20a%20scalable%20platform%0Adesigned%20to%20dynamically%20integrate%20heterogeneous%20agents%20for%20automating%20computer%0Atasks.%20AgentStore%20empowers%20users%20to%20integrate%20third-party%20agents%2C%20allowing%20the%0Asystem%20to%20continuously%20enrich%20its%20capabilities%20and%20adapt%20to%20rapidly%20evolving%0Aoperating%20systems.%20Additionally%2C%20we%20propose%20a%20novel%20core%20%5Ctextbf%7BMetaAgent%7D%0Awith%20the%20%5Ctextbf%7BAgentToken%7D%20strategy%20to%20efficiently%20manage%20diverse%20agents%20and%0Autilize%20their%20specialized%20and%20generalist%20abilities%20for%20both%20domain-specific%20and%0Asystem-wide%20tasks.%20Extensive%20experiments%20on%20three%20challenging%20benchmarks%0Ademonstrate%20that%20AgentStore%20surpasses%20the%20limitations%20of%20previous%20systems%20with%0Anarrow%20capabilities%2C%20particularly%20achieving%20a%20significant%20improvement%20from%0A11.21%5C%25%20to%2023.85%5C%25%20on%20the%20OSWorld%20benchmark%2C%20more%20than%20doubling%20the%20previous%0Aresults.%20Comprehensive%20quantitative%20and%20qualitative%20results%20further%20demonstrate%0AAgentStore%27s%20ability%20to%20enhance%20agent%20systems%20in%20both%20generalization%20and%0Aspecialization%2C%20underscoring%20its%20potential%20for%20developing%20the%20specialized%0Ageneralist%20computer%20assistant.%20All%20our%20codes%20will%20be%20made%20publicly%20available%20in%0Ahttps%3A//chengyou-jia.github.io/AgentStore-Home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentStore%253A%2520Scalable%2520Integration%2520of%2520Heterogeneous%2520Agents%2520As%2520Specialized%250A%2520%2520Generalist%2520Computer%2520Assistant%26entry.906535625%3DChengyou%2520Jia%2520and%2520Minnan%2520Luo%2520and%2520Zhuohang%2520Dang%2520and%2520Qiushi%2520Sun%2520and%2520Fangzhi%2520Xu%2520and%2520Junlin%2520Hu%2520and%2520Tianbao%2520Xie%2520and%2520Zhiyong%2520Wu%26entry.1292438233%3D%2520%2520Digital%2520agents%2520capable%2520of%2520automating%2520complex%2520computer%2520tasks%2520have%2520attracted%250Aconsiderable%2520attention%2520due%2520to%2520their%2520immense%2520potential%2520to%2520enhance%2520human-computer%250Ainteraction.%2520However%252C%2520existing%2520agent%2520methods%2520exhibit%2520deficiencies%2520in%2520their%250Ageneralization%2520and%2520specialization%2520capabilities%252C%2520especially%2520in%2520handling%250Aopen-ended%2520computer%2520tasks%2520in%2520real-world%2520environments.%2520Inspired%2520by%2520the%2520rich%250Afunctionality%2520of%2520the%2520App%2520store%252C%2520we%2520present%2520AgentStore%252C%2520a%2520scalable%2520platform%250Adesigned%2520to%2520dynamically%2520integrate%2520heterogeneous%2520agents%2520for%2520automating%2520computer%250Atasks.%2520AgentStore%2520empowers%2520users%2520to%2520integrate%2520third-party%2520agents%252C%2520allowing%2520the%250Asystem%2520to%2520continuously%2520enrich%2520its%2520capabilities%2520and%2520adapt%2520to%2520rapidly%2520evolving%250Aoperating%2520systems.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520core%2520%255Ctextbf%257BMetaAgent%257D%250Awith%2520the%2520%255Ctextbf%257BAgentToken%257D%2520strategy%2520to%2520efficiently%2520manage%2520diverse%2520agents%2520and%250Autilize%2520their%2520specialized%2520and%2520generalist%2520abilities%2520for%2520both%2520domain-specific%2520and%250Asystem-wide%2520tasks.%2520Extensive%2520experiments%2520on%2520three%2520challenging%2520benchmarks%250Ademonstrate%2520that%2520AgentStore%2520surpasses%2520the%2520limitations%2520of%2520previous%2520systems%2520with%250Anarrow%2520capabilities%252C%2520particularly%2520achieving%2520a%2520significant%2520improvement%2520from%250A11.21%255C%2525%2520to%252023.85%255C%2525%2520on%2520the%2520OSWorld%2520benchmark%252C%2520more%2520than%2520doubling%2520the%2520previous%250Aresults.%2520Comprehensive%2520quantitative%2520and%2520qualitative%2520results%2520further%2520demonstrate%250AAgentStore%2527s%2520ability%2520to%2520enhance%2520agent%2520systems%2520in%2520both%2520generalization%2520and%250Aspecialization%252C%2520underscoring%2520its%2520potential%2520for%2520developing%2520the%2520specialized%250Ageneralist%2520computer%2520assistant.%2520All%2520our%2520codes%2520will%2520be%2520made%2520publicly%2520available%2520in%250Ahttps%253A//chengyou-jia.github.io/AgentStore-Home.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentStore%3A%20Scalable%20Integration%20of%20Heterogeneous%20Agents%20As%20Specialized%0A%20%20Generalist%20Computer%20Assistant&entry.906535625=Chengyou%20Jia%20and%20Minnan%20Luo%20and%20Zhuohang%20Dang%20and%20Qiushi%20Sun%20and%20Fangzhi%20Xu%20and%20Junlin%20Hu%20and%20Tianbao%20Xie%20and%20Zhiyong%20Wu&entry.1292438233=%20%20Digital%20agents%20capable%20of%20automating%20complex%20computer%20tasks%20have%20attracted%0Aconsiderable%20attention%20due%20to%20their%20immense%20potential%20to%20enhance%20human-computer%0Ainteraction.%20However%2C%20existing%20agent%20methods%20exhibit%20deficiencies%20in%20their%0Ageneralization%20and%20specialization%20capabilities%2C%20especially%20in%20handling%0Aopen-ended%20computer%20tasks%20in%20real-world%20environments.%20Inspired%20by%20the%20rich%0Afunctionality%20of%20the%20App%20store%2C%20we%20present%20AgentStore%2C%20a%20scalable%20platform%0Adesigned%20to%20dynamically%20integrate%20heterogeneous%20agents%20for%20automating%20computer%0Atasks.%20AgentStore%20empowers%20users%20to%20integrate%20third-party%20agents%2C%20allowing%20the%0Asystem%20to%20continuously%20enrich%20its%20capabilities%20and%20adapt%20to%20rapidly%20evolving%0Aoperating%20systems.%20Additionally%2C%20we%20propose%20a%20novel%20core%20%5Ctextbf%7BMetaAgent%7D%0Awith%20the%20%5Ctextbf%7BAgentToken%7D%20strategy%20to%20efficiently%20manage%20diverse%20agents%20and%0Autilize%20their%20specialized%20and%20generalist%20abilities%20for%20both%20domain-specific%20and%0Asystem-wide%20tasks.%20Extensive%20experiments%20on%20three%20challenging%20benchmarks%0Ademonstrate%20that%20AgentStore%20surpasses%20the%20limitations%20of%20previous%20systems%20with%0Anarrow%20capabilities%2C%20particularly%20achieving%20a%20significant%20improvement%20from%0A11.21%5C%25%20to%2023.85%5C%25%20on%20the%20OSWorld%20benchmark%2C%20more%20than%20doubling%20the%20previous%0Aresults.%20Comprehensive%20quantitative%20and%20qualitative%20results%20further%20demonstrate%0AAgentStore%27s%20ability%20to%20enhance%20agent%20systems%20in%20both%20generalization%20and%0Aspecialization%2C%20underscoring%20its%20potential%20for%20developing%20the%20specialized%0Ageneralist%20computer%20assistant.%20All%20our%20codes%20will%20be%20made%20publicly%20available%20in%0Ahttps%3A//chengyou-jia.github.io/AgentStore-Home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18603v1&entry.124074799=Read"},
{"title": "Multi-Class Abnormality Classification in Video Capsule Endoscopy Using\n  Deep Learning", "author": "Arnav Samal and  Ranya", "abstract": "  This report outlines Team Seq2Cure's deep learning approach for the Capsule\nVision 2024 Challenge, leveraging an ensemble of convolutional neural networks\n(CNNs) and transformer-based architectures for multi-class abnormality\nclassification in video capsule endoscopy frames. The dataset comprised over\n50,000 frames from three public sources and one private dataset, labeled across\n10 abnormality classes. To overcome the limitations of traditional CNNs in\ncapturing global context, we integrated CNN and transformer models within a\nmulti-model ensemble. Our approach achieved a balanced accuracy of 86.34\npercent and a mean AUC-ROC score of 0.9908 on the validation set, with\nsignificant improvements in classifying complex abnormalities. Code is\navailable at http://github.com/arnavs04/capsule-vision-2024 .\n", "link": "http://arxiv.org/abs/2410.18879v1", "date": "2024-10-24", "relevancy": 2.0426, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5235}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Class%20Abnormality%20Classification%20in%20Video%20Capsule%20Endoscopy%20Using%0A%20%20Deep%20Learning&body=Title%3A%20Multi-Class%20Abnormality%20Classification%20in%20Video%20Capsule%20Endoscopy%20Using%0A%20%20Deep%20Learning%0AAuthor%3A%20Arnav%20Samal%20and%20%20Ranya%0AAbstract%3A%20%20%20This%20report%20outlines%20Team%20Seq2Cure%27s%20deep%20learning%20approach%20for%20the%20Capsule%0AVision%202024%20Challenge%2C%20leveraging%20an%20ensemble%20of%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20transformer-based%20architectures%20for%20multi-class%20abnormality%0Aclassification%20in%20video%20capsule%20endoscopy%20frames.%20The%20dataset%20comprised%20over%0A50%2C000%20frames%20from%20three%20public%20sources%20and%20one%20private%20dataset%2C%20labeled%20across%0A10%20abnormality%20classes.%20To%20overcome%20the%20limitations%20of%20traditional%20CNNs%20in%0Acapturing%20global%20context%2C%20we%20integrated%20CNN%20and%20transformer%20models%20within%20a%0Amulti-model%20ensemble.%20Our%20approach%20achieved%20a%20balanced%20accuracy%20of%2086.34%0Apercent%20and%20a%20mean%20AUC-ROC%20score%20of%200.9908%20on%20the%20validation%20set%2C%20with%0Asignificant%20improvements%20in%20classifying%20complex%20abnormalities.%20Code%20is%0Aavailable%20at%20http%3A//github.com/arnavs04/capsule-vision-2024%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Class%2520Abnormality%2520Classification%2520in%2520Video%2520Capsule%2520Endoscopy%2520Using%250A%2520%2520Deep%2520Learning%26entry.906535625%3DArnav%2520Samal%2520and%2520%2520Ranya%26entry.1292438233%3D%2520%2520This%2520report%2520outlines%2520Team%2520Seq2Cure%2527s%2520deep%2520learning%2520approach%2520for%2520the%2520Capsule%250AVision%25202024%2520Challenge%252C%2520leveraging%2520an%2520ensemble%2520of%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520and%2520transformer-based%2520architectures%2520for%2520multi-class%2520abnormality%250Aclassification%2520in%2520video%2520capsule%2520endoscopy%2520frames.%2520The%2520dataset%2520comprised%2520over%250A50%252C000%2520frames%2520from%2520three%2520public%2520sources%2520and%2520one%2520private%2520dataset%252C%2520labeled%2520across%250A10%2520abnormality%2520classes.%2520To%2520overcome%2520the%2520limitations%2520of%2520traditional%2520CNNs%2520in%250Acapturing%2520global%2520context%252C%2520we%2520integrated%2520CNN%2520and%2520transformer%2520models%2520within%2520a%250Amulti-model%2520ensemble.%2520Our%2520approach%2520achieved%2520a%2520balanced%2520accuracy%2520of%252086.34%250Apercent%2520and%2520a%2520mean%2520AUC-ROC%2520score%2520of%25200.9908%2520on%2520the%2520validation%2520set%252C%2520with%250Asignificant%2520improvements%2520in%2520classifying%2520complex%2520abnormalities.%2520Code%2520is%250Aavailable%2520at%2520http%253A//github.com/arnavs04/capsule-vision-2024%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Class%20Abnormality%20Classification%20in%20Video%20Capsule%20Endoscopy%20Using%0A%20%20Deep%20Learning&entry.906535625=Arnav%20Samal%20and%20%20Ranya&entry.1292438233=%20%20This%20report%20outlines%20Team%20Seq2Cure%27s%20deep%20learning%20approach%20for%20the%20Capsule%0AVision%202024%20Challenge%2C%20leveraging%20an%20ensemble%20of%20convolutional%20neural%20networks%0A%28CNNs%29%20and%20transformer-based%20architectures%20for%20multi-class%20abnormality%0Aclassification%20in%20video%20capsule%20endoscopy%20frames.%20The%20dataset%20comprised%20over%0A50%2C000%20frames%20from%20three%20public%20sources%20and%20one%20private%20dataset%2C%20labeled%20across%0A10%20abnormality%20classes.%20To%20overcome%20the%20limitations%20of%20traditional%20CNNs%20in%0Acapturing%20global%20context%2C%20we%20integrated%20CNN%20and%20transformer%20models%20within%20a%0Amulti-model%20ensemble.%20Our%20approach%20achieved%20a%20balanced%20accuracy%20of%2086.34%0Apercent%20and%20a%20mean%20AUC-ROC%20score%20of%200.9908%20on%20the%20validation%20set%2C%20with%0Asignificant%20improvements%20in%20classifying%20complex%20abnormalities.%20Code%20is%0Aavailable%20at%20http%3A//github.com/arnavs04/capsule-vision-2024%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18879v1&entry.124074799=Read"},
{"title": "Improving Small-Scale Large Language Models Function Calling for\n  Reasoning Tasks", "author": "Graziano A. Manduzio and Federico A. Galatolo and Mario G. C. A. Cimino and Enzo Pasquale Scilingo and Lorenzo Cominelli", "abstract": "  Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in natural language understanding and generation.\nWhile these models excel in general complex reasoning tasks, they still face\nchallenges in mathematical problem-solving and logical reasoning. To address\nthese limitations, researchers have explored function calling abilities,\nallowing LLMs to execute provided functions and utilize their outputs for task\ncompletion. However, concentrating on specific tasks can be very inefficient\nfor large-scale LLMs to be used, because of the expensive cost of training and\ninference stages they need in terms of computational resources. This study\nintroduces a novel framework for training smaller language models in function\ncalling, focusing on specific logical and mathematical reasoning tasks. The\napproach aims to improve performances of small-scale models for these tasks\nusing function calling, ensuring a high level of accuracy. Our framework\nemploys an agent that, given a problem and a set of callable functions, queries\nthe LLM by injecting a description and examples of the usable functions into\nthe prompt and managing their calls in a step-by-step reasoning chain. This\nprocess is used to create a dataset of correct and incorrect reasoning chain\nchat completions from a large-scale LLM. This dataset is used to train a\nsmaller LLM using Reinforcement Learning from Human Feedback (RLHF),\nspecifically employing the Direct Preference Optimization (DPO) technique.\nExperimental results demonstrate how the proposed approach balances the\ntrade-off between model size and performance, improving the ability of function\ncalling for reasoning tasks, in smaller models.\n", "link": "http://arxiv.org/abs/2410.18890v1", "date": "2024-10-24", "relevancy": 2.0287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Small-Scale%20Large%20Language%20Models%20Function%20Calling%20for%0A%20%20Reasoning%20Tasks&body=Title%3A%20Improving%20Small-Scale%20Large%20Language%20Models%20Function%20Calling%20for%0A%20%20Reasoning%20Tasks%0AAuthor%3A%20Graziano%20A.%20Manduzio%20and%20Federico%20A.%20Galatolo%20and%20Mario%20G.%20C.%20A.%20Cimino%20and%20Enzo%20Pasquale%20Scilingo%20and%20Lorenzo%20Cominelli%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%0Aexceptional%20capabilities%20in%20natural%20language%20understanding%20and%20generation.%0AWhile%20these%20models%20excel%20in%20general%20complex%20reasoning%20tasks%2C%20they%20still%20face%0Achallenges%20in%20mathematical%20problem-solving%20and%20logical%20reasoning.%20To%20address%0Athese%20limitations%2C%20researchers%20have%20explored%20function%20calling%20abilities%2C%0Aallowing%20LLMs%20to%20execute%20provided%20functions%20and%20utilize%20their%20outputs%20for%20task%0Acompletion.%20However%2C%20concentrating%20on%20specific%20tasks%20can%20be%20very%20inefficient%0Afor%20large-scale%20LLMs%20to%20be%20used%2C%20because%20of%20the%20expensive%20cost%20of%20training%20and%0Ainference%20stages%20they%20need%20in%20terms%20of%20computational%20resources.%20This%20study%0Aintroduces%20a%20novel%20framework%20for%20training%20smaller%20language%20models%20in%20function%0Acalling%2C%20focusing%20on%20specific%20logical%20and%20mathematical%20reasoning%20tasks.%20The%0Aapproach%20aims%20to%20improve%20performances%20of%20small-scale%20models%20for%20these%20tasks%0Ausing%20function%20calling%2C%20ensuring%20a%20high%20level%20of%20accuracy.%20Our%20framework%0Aemploys%20an%20agent%20that%2C%20given%20a%20problem%20and%20a%20set%20of%20callable%20functions%2C%20queries%0Athe%20LLM%20by%20injecting%20a%20description%20and%20examples%20of%20the%20usable%20functions%20into%0Athe%20prompt%20and%20managing%20their%20calls%20in%20a%20step-by-step%20reasoning%20chain.%20This%0Aprocess%20is%20used%20to%20create%20a%20dataset%20of%20correct%20and%20incorrect%20reasoning%20chain%0Achat%20completions%20from%20a%20large-scale%20LLM.%20This%20dataset%20is%20used%20to%20train%20a%0Asmaller%20LLM%20using%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%0Aspecifically%20employing%20the%20Direct%20Preference%20Optimization%20%28DPO%29%20technique.%0AExperimental%20results%20demonstrate%20how%20the%20proposed%20approach%20balances%20the%0Atrade-off%20between%20model%20size%20and%20performance%2C%20improving%20the%20ability%20of%20function%0Acalling%20for%20reasoning%20tasks%2C%20in%20smaller%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Small-Scale%2520Large%2520Language%2520Models%2520Function%2520Calling%2520for%250A%2520%2520Reasoning%2520Tasks%26entry.906535625%3DGraziano%2520A.%2520Manduzio%2520and%2520Federico%2520A.%2520Galatolo%2520and%2520Mario%2520G.%2520C.%2520A.%2520Cimino%2520and%2520Enzo%2520Pasquale%2520Scilingo%2520and%2520Lorenzo%2520Cominelli%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%250Aexceptional%2520capabilities%2520in%2520natural%2520language%2520understanding%2520and%2520generation.%250AWhile%2520these%2520models%2520excel%2520in%2520general%2520complex%2520reasoning%2520tasks%252C%2520they%2520still%2520face%250Achallenges%2520in%2520mathematical%2520problem-solving%2520and%2520logical%2520reasoning.%2520To%2520address%250Athese%2520limitations%252C%2520researchers%2520have%2520explored%2520function%2520calling%2520abilities%252C%250Aallowing%2520LLMs%2520to%2520execute%2520provided%2520functions%2520and%2520utilize%2520their%2520outputs%2520for%2520task%250Acompletion.%2520However%252C%2520concentrating%2520on%2520specific%2520tasks%2520can%2520be%2520very%2520inefficient%250Afor%2520large-scale%2520LLMs%2520to%2520be%2520used%252C%2520because%2520of%2520the%2520expensive%2520cost%2520of%2520training%2520and%250Ainference%2520stages%2520they%2520need%2520in%2520terms%2520of%2520computational%2520resources.%2520This%2520study%250Aintroduces%2520a%2520novel%2520framework%2520for%2520training%2520smaller%2520language%2520models%2520in%2520function%250Acalling%252C%2520focusing%2520on%2520specific%2520logical%2520and%2520mathematical%2520reasoning%2520tasks.%2520The%250Aapproach%2520aims%2520to%2520improve%2520performances%2520of%2520small-scale%2520models%2520for%2520these%2520tasks%250Ausing%2520function%2520calling%252C%2520ensuring%2520a%2520high%2520level%2520of%2520accuracy.%2520Our%2520framework%250Aemploys%2520an%2520agent%2520that%252C%2520given%2520a%2520problem%2520and%2520a%2520set%2520of%2520callable%2520functions%252C%2520queries%250Athe%2520LLM%2520by%2520injecting%2520a%2520description%2520and%2520examples%2520of%2520the%2520usable%2520functions%2520into%250Athe%2520prompt%2520and%2520managing%2520their%2520calls%2520in%2520a%2520step-by-step%2520reasoning%2520chain.%2520This%250Aprocess%2520is%2520used%2520to%2520create%2520a%2520dataset%2520of%2520correct%2520and%2520incorrect%2520reasoning%2520chain%250Achat%2520completions%2520from%2520a%2520large-scale%2520LLM.%2520This%2520dataset%2520is%2520used%2520to%2520train%2520a%250Asmaller%2520LLM%2520using%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%252C%250Aspecifically%2520employing%2520the%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520technique.%250AExperimental%2520results%2520demonstrate%2520how%2520the%2520proposed%2520approach%2520balances%2520the%250Atrade-off%2520between%2520model%2520size%2520and%2520performance%252C%2520improving%2520the%2520ability%2520of%2520function%250Acalling%2520for%2520reasoning%2520tasks%252C%2520in%2520smaller%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Small-Scale%20Large%20Language%20Models%20Function%20Calling%20for%0A%20%20Reasoning%20Tasks&entry.906535625=Graziano%20A.%20Manduzio%20and%20Federico%20A.%20Galatolo%20and%20Mario%20G.%20C.%20A.%20Cimino%20and%20Enzo%20Pasquale%20Scilingo%20and%20Lorenzo%20Cominelli&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%0Aexceptional%20capabilities%20in%20natural%20language%20understanding%20and%20generation.%0AWhile%20these%20models%20excel%20in%20general%20complex%20reasoning%20tasks%2C%20they%20still%20face%0Achallenges%20in%20mathematical%20problem-solving%20and%20logical%20reasoning.%20To%20address%0Athese%20limitations%2C%20researchers%20have%20explored%20function%20calling%20abilities%2C%0Aallowing%20LLMs%20to%20execute%20provided%20functions%20and%20utilize%20their%20outputs%20for%20task%0Acompletion.%20However%2C%20concentrating%20on%20specific%20tasks%20can%20be%20very%20inefficient%0Afor%20large-scale%20LLMs%20to%20be%20used%2C%20because%20of%20the%20expensive%20cost%20of%20training%20and%0Ainference%20stages%20they%20need%20in%20terms%20of%20computational%20resources.%20This%20study%0Aintroduces%20a%20novel%20framework%20for%20training%20smaller%20language%20models%20in%20function%0Acalling%2C%20focusing%20on%20specific%20logical%20and%20mathematical%20reasoning%20tasks.%20The%0Aapproach%20aims%20to%20improve%20performances%20of%20small-scale%20models%20for%20these%20tasks%0Ausing%20function%20calling%2C%20ensuring%20a%20high%20level%20of%20accuracy.%20Our%20framework%0Aemploys%20an%20agent%20that%2C%20given%20a%20problem%20and%20a%20set%20of%20callable%20functions%2C%20queries%0Athe%20LLM%20by%20injecting%20a%20description%20and%20examples%20of%20the%20usable%20functions%20into%0Athe%20prompt%20and%20managing%20their%20calls%20in%20a%20step-by-step%20reasoning%20chain.%20This%0Aprocess%20is%20used%20to%20create%20a%20dataset%20of%20correct%20and%20incorrect%20reasoning%20chain%0Achat%20completions%20from%20a%20large-scale%20LLM.%20This%20dataset%20is%20used%20to%20train%20a%0Asmaller%20LLM%20using%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%2C%0Aspecifically%20employing%20the%20Direct%20Preference%20Optimization%20%28DPO%29%20technique.%0AExperimental%20results%20demonstrate%20how%20the%20proposed%20approach%20balances%20the%0Atrade-off%20between%20model%20size%20and%20performance%2C%20improving%20the%20ability%20of%20function%0Acalling%20for%20reasoning%20tasks%2C%20in%20smaller%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18890v1&entry.124074799=Read"},
{"title": "Fast constrained sampling in pre-trained diffusion models", "author": "Alexandros Graikos and Nebojsa Jojic and Dimitris Samaras", "abstract": "  Diffusion models have dominated the field of large, generative image models,\nwith the prime examples of Stable Diffusion and DALL-E 3 being widely adopted.\nThese models have been trained to perform text-conditioned generation on vast\nnumbers of image-caption pairs and as a byproduct, have acquired general\nknowledge about natural image statistics. However, when confronted with the\ntask of constrained sampling, e.g. generating the right half of an image\nconditioned on the known left half, applying these models is a delicate and\nslow process, with previously proposed algorithms relying on expensive\niterative operations that are usually orders of magnitude slower than\ntext-based inference. This is counter-intuitive, as image-conditioned\ngeneration should rely less on the difficult-to-learn semantic knowledge that\nlinks captions and imagery, and should instead be achievable by lower-level\ncorrelations among image pixels. In practice, inverse models are trained or\ntuned separately for each inverse problem, e.g. by providing parts of images\nduring training as an additional condition, to allow their application in\nrealistic settings. However, we argue that this is not necessary and propose an\nalgorithm for fast-constrained sampling in large pre-trained diffusion models\n(Stable Diffusion) that requires no expensive backpropagation operations\nthrough the model and produces results comparable even to the state-of-the-art\n\\emph{tuned} models. Our method is based on a novel optimization perspective to\nsampling under constraints and employs a numerical approximation to the\nexpensive gradients, previously computed using backpropagation, incurring\nsignificant speed-ups.\n", "link": "http://arxiv.org/abs/2410.18804v1", "date": "2024-10-24", "relevancy": 2.0014, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7064}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6563}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20constrained%20sampling%20in%20pre-trained%20diffusion%20models&body=Title%3A%20Fast%20constrained%20sampling%20in%20pre-trained%20diffusion%20models%0AAuthor%3A%20Alexandros%20Graikos%20and%20Nebojsa%20Jojic%20and%20Dimitris%20Samaras%0AAbstract%3A%20%20%20Diffusion%20models%20have%20dominated%20the%20field%20of%20large%2C%20generative%20image%20models%2C%0Awith%20the%20prime%20examples%20of%20Stable%20Diffusion%20and%20DALL-E%203%20being%20widely%20adopted.%0AThese%20models%20have%20been%20trained%20to%20perform%20text-conditioned%20generation%20on%20vast%0Anumbers%20of%20image-caption%20pairs%20and%20as%20a%20byproduct%2C%20have%20acquired%20general%0Aknowledge%20about%20natural%20image%20statistics.%20However%2C%20when%20confronted%20with%20the%0Atask%20of%20constrained%20sampling%2C%20e.g.%20generating%20the%20right%20half%20of%20an%20image%0Aconditioned%20on%20the%20known%20left%20half%2C%20applying%20these%20models%20is%20a%20delicate%20and%0Aslow%20process%2C%20with%20previously%20proposed%20algorithms%20relying%20on%20expensive%0Aiterative%20operations%20that%20are%20usually%20orders%20of%20magnitude%20slower%20than%0Atext-based%20inference.%20This%20is%20counter-intuitive%2C%20as%20image-conditioned%0Ageneration%20should%20rely%20less%20on%20the%20difficult-to-learn%20semantic%20knowledge%20that%0Alinks%20captions%20and%20imagery%2C%20and%20should%20instead%20be%20achievable%20by%20lower-level%0Acorrelations%20among%20image%20pixels.%20In%20practice%2C%20inverse%20models%20are%20trained%20or%0Atuned%20separately%20for%20each%20inverse%20problem%2C%20e.g.%20by%20providing%20parts%20of%20images%0Aduring%20training%20as%20an%20additional%20condition%2C%20to%20allow%20their%20application%20in%0Arealistic%20settings.%20However%2C%20we%20argue%20that%20this%20is%20not%20necessary%20and%20propose%20an%0Aalgorithm%20for%20fast-constrained%20sampling%20in%20large%20pre-trained%20diffusion%20models%0A%28Stable%20Diffusion%29%20that%20requires%20no%20expensive%20backpropagation%20operations%0Athrough%20the%20model%20and%20produces%20results%20comparable%20even%20to%20the%20state-of-the-art%0A%5Cemph%7Btuned%7D%20models.%20Our%20method%20is%20based%20on%20a%20novel%20optimization%20perspective%20to%0Asampling%20under%20constraints%20and%20employs%20a%20numerical%20approximation%20to%20the%0Aexpensive%20gradients%2C%20previously%20computed%20using%20backpropagation%2C%20incurring%0Asignificant%20speed-ups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520constrained%2520sampling%2520in%2520pre-trained%2520diffusion%2520models%26entry.906535625%3DAlexandros%2520Graikos%2520and%2520Nebojsa%2520Jojic%2520and%2520Dimitris%2520Samaras%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520dominated%2520the%2520field%2520of%2520large%252C%2520generative%2520image%2520models%252C%250Awith%2520the%2520prime%2520examples%2520of%2520Stable%2520Diffusion%2520and%2520DALL-E%25203%2520being%2520widely%2520adopted.%250AThese%2520models%2520have%2520been%2520trained%2520to%2520perform%2520text-conditioned%2520generation%2520on%2520vast%250Anumbers%2520of%2520image-caption%2520pairs%2520and%2520as%2520a%2520byproduct%252C%2520have%2520acquired%2520general%250Aknowledge%2520about%2520natural%2520image%2520statistics.%2520However%252C%2520when%2520confronted%2520with%2520the%250Atask%2520of%2520constrained%2520sampling%252C%2520e.g.%2520generating%2520the%2520right%2520half%2520of%2520an%2520image%250Aconditioned%2520on%2520the%2520known%2520left%2520half%252C%2520applying%2520these%2520models%2520is%2520a%2520delicate%2520and%250Aslow%2520process%252C%2520with%2520previously%2520proposed%2520algorithms%2520relying%2520on%2520expensive%250Aiterative%2520operations%2520that%2520are%2520usually%2520orders%2520of%2520magnitude%2520slower%2520than%250Atext-based%2520inference.%2520This%2520is%2520counter-intuitive%252C%2520as%2520image-conditioned%250Ageneration%2520should%2520rely%2520less%2520on%2520the%2520difficult-to-learn%2520semantic%2520knowledge%2520that%250Alinks%2520captions%2520and%2520imagery%252C%2520and%2520should%2520instead%2520be%2520achievable%2520by%2520lower-level%250Acorrelations%2520among%2520image%2520pixels.%2520In%2520practice%252C%2520inverse%2520models%2520are%2520trained%2520or%250Atuned%2520separately%2520for%2520each%2520inverse%2520problem%252C%2520e.g.%2520by%2520providing%2520parts%2520of%2520images%250Aduring%2520training%2520as%2520an%2520additional%2520condition%252C%2520to%2520allow%2520their%2520application%2520in%250Arealistic%2520settings.%2520However%252C%2520we%2520argue%2520that%2520this%2520is%2520not%2520necessary%2520and%2520propose%2520an%250Aalgorithm%2520for%2520fast-constrained%2520sampling%2520in%2520large%2520pre-trained%2520diffusion%2520models%250A%2528Stable%2520Diffusion%2529%2520that%2520requires%2520no%2520expensive%2520backpropagation%2520operations%250Athrough%2520the%2520model%2520and%2520produces%2520results%2520comparable%2520even%2520to%2520the%2520state-of-the-art%250A%255Cemph%257Btuned%257D%2520models.%2520Our%2520method%2520is%2520based%2520on%2520a%2520novel%2520optimization%2520perspective%2520to%250Asampling%2520under%2520constraints%2520and%2520employs%2520a%2520numerical%2520approximation%2520to%2520the%250Aexpensive%2520gradients%252C%2520previously%2520computed%2520using%2520backpropagation%252C%2520incurring%250Asignificant%2520speed-ups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20constrained%20sampling%20in%20pre-trained%20diffusion%20models&entry.906535625=Alexandros%20Graikos%20and%20Nebojsa%20Jojic%20and%20Dimitris%20Samaras&entry.1292438233=%20%20Diffusion%20models%20have%20dominated%20the%20field%20of%20large%2C%20generative%20image%20models%2C%0Awith%20the%20prime%20examples%20of%20Stable%20Diffusion%20and%20DALL-E%203%20being%20widely%20adopted.%0AThese%20models%20have%20been%20trained%20to%20perform%20text-conditioned%20generation%20on%20vast%0Anumbers%20of%20image-caption%20pairs%20and%20as%20a%20byproduct%2C%20have%20acquired%20general%0Aknowledge%20about%20natural%20image%20statistics.%20However%2C%20when%20confronted%20with%20the%0Atask%20of%20constrained%20sampling%2C%20e.g.%20generating%20the%20right%20half%20of%20an%20image%0Aconditioned%20on%20the%20known%20left%20half%2C%20applying%20these%20models%20is%20a%20delicate%20and%0Aslow%20process%2C%20with%20previously%20proposed%20algorithms%20relying%20on%20expensive%0Aiterative%20operations%20that%20are%20usually%20orders%20of%20magnitude%20slower%20than%0Atext-based%20inference.%20This%20is%20counter-intuitive%2C%20as%20image-conditioned%0Ageneration%20should%20rely%20less%20on%20the%20difficult-to-learn%20semantic%20knowledge%20that%0Alinks%20captions%20and%20imagery%2C%20and%20should%20instead%20be%20achievable%20by%20lower-level%0Acorrelations%20among%20image%20pixels.%20In%20practice%2C%20inverse%20models%20are%20trained%20or%0Atuned%20separately%20for%20each%20inverse%20problem%2C%20e.g.%20by%20providing%20parts%20of%20images%0Aduring%20training%20as%20an%20additional%20condition%2C%20to%20allow%20their%20application%20in%0Arealistic%20settings.%20However%2C%20we%20argue%20that%20this%20is%20not%20necessary%20and%20propose%20an%0Aalgorithm%20for%20fast-constrained%20sampling%20in%20large%20pre-trained%20diffusion%20models%0A%28Stable%20Diffusion%29%20that%20requires%20no%20expensive%20backpropagation%20operations%0Athrough%20the%20model%20and%20produces%20results%20comparable%20even%20to%20the%20state-of-the-art%0A%5Cemph%7Btuned%7D%20models.%20Our%20method%20is%20based%20on%20a%20novel%20optimization%20perspective%20to%0Asampling%20under%20constraints%20and%20employs%20a%20numerical%20approximation%20to%20the%0Aexpensive%20gradients%2C%20previously%20computed%20using%20backpropagation%2C%20incurring%0Asignificant%20speed-ups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18804v1&entry.124074799=Read"},
{"title": "Learning of Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces", "author": "Torbj\u00f8rn Smith and Olav Egeland", "abstract": "  This paper presents a method for learning Hamiltonian dynamics from a limited\nset of data points. The Hamiltonian vector field is found by regularized\noptimization over a reproducing kernel Hilbert space of vector fields that are\ninherently Hamiltonian, and where the vector field is required to be odd or\neven. This is done with a symplectic kernel, and it is shown how this\nsymplectic kernel can be modified to be odd or even. The performance of the\nmethod is validated in simulations for two Hamiltonian systems. The simulations\nshow that the learned dynamics reflect the energy-preservation of the\nHamiltonian dynamics, and that the restriction to symplectic and odd dynamics\ngives improved accuracy over a large domain of the phase space.\n", "link": "http://arxiv.org/abs/2312.09734v2", "date": "2024-10-24", "relevancy": 1.6821, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4398}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4262}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20of%20Hamiltonian%20Dynamics%20with%20Reproducing%20Kernel%20Hilbert%20Spaces&body=Title%3A%20Learning%20of%20Hamiltonian%20Dynamics%20with%20Reproducing%20Kernel%20Hilbert%20Spaces%0AAuthor%3A%20Torbj%C3%B8rn%20Smith%20and%20Olav%20Egeland%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20method%20for%20learning%20Hamiltonian%20dynamics%20from%20a%20limited%0Aset%20of%20data%20points.%20The%20Hamiltonian%20vector%20field%20is%20found%20by%20regularized%0Aoptimization%20over%20a%20reproducing%20kernel%20Hilbert%20space%20of%20vector%20fields%20that%20are%0Ainherently%20Hamiltonian%2C%20and%20where%20the%20vector%20field%20is%20required%20to%20be%20odd%20or%0Aeven.%20This%20is%20done%20with%20a%20symplectic%20kernel%2C%20and%20it%20is%20shown%20how%20this%0Asymplectic%20kernel%20can%20be%20modified%20to%20be%20odd%20or%20even.%20The%20performance%20of%20the%0Amethod%20is%20validated%20in%20simulations%20for%20two%20Hamiltonian%20systems.%20The%20simulations%0Ashow%20that%20the%20learned%20dynamics%20reflect%20the%20energy-preservation%20of%20the%0AHamiltonian%20dynamics%2C%20and%20that%20the%20restriction%20to%20symplectic%20and%20odd%20dynamics%0Agives%20improved%20accuracy%20over%20a%20large%20domain%20of%20the%20phase%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520of%2520Hamiltonian%2520Dynamics%2520with%2520Reproducing%2520Kernel%2520Hilbert%2520Spaces%26entry.906535625%3DTorbj%25C3%25B8rn%2520Smith%2520and%2520Olav%2520Egeland%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520method%2520for%2520learning%2520Hamiltonian%2520dynamics%2520from%2520a%2520limited%250Aset%2520of%2520data%2520points.%2520The%2520Hamiltonian%2520vector%2520field%2520is%2520found%2520by%2520regularized%250Aoptimization%2520over%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%2520of%2520vector%2520fields%2520that%2520are%250Ainherently%2520Hamiltonian%252C%2520and%2520where%2520the%2520vector%2520field%2520is%2520required%2520to%2520be%2520odd%2520or%250Aeven.%2520This%2520is%2520done%2520with%2520a%2520symplectic%2520kernel%252C%2520and%2520it%2520is%2520shown%2520how%2520this%250Asymplectic%2520kernel%2520can%2520be%2520modified%2520to%2520be%2520odd%2520or%2520even.%2520The%2520performance%2520of%2520the%250Amethod%2520is%2520validated%2520in%2520simulations%2520for%2520two%2520Hamiltonian%2520systems.%2520The%2520simulations%250Ashow%2520that%2520the%2520learned%2520dynamics%2520reflect%2520the%2520energy-preservation%2520of%2520the%250AHamiltonian%2520dynamics%252C%2520and%2520that%2520the%2520restriction%2520to%2520symplectic%2520and%2520odd%2520dynamics%250Agives%2520improved%2520accuracy%2520over%2520a%2520large%2520domain%2520of%2520the%2520phase%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20of%20Hamiltonian%20Dynamics%20with%20Reproducing%20Kernel%20Hilbert%20Spaces&entry.906535625=Torbj%C3%B8rn%20Smith%20and%20Olav%20Egeland&entry.1292438233=%20%20This%20paper%20presents%20a%20method%20for%20learning%20Hamiltonian%20dynamics%20from%20a%20limited%0Aset%20of%20data%20points.%20The%20Hamiltonian%20vector%20field%20is%20found%20by%20regularized%0Aoptimization%20over%20a%20reproducing%20kernel%20Hilbert%20space%20of%20vector%20fields%20that%20are%0Ainherently%20Hamiltonian%2C%20and%20where%20the%20vector%20field%20is%20required%20to%20be%20odd%20or%0Aeven.%20This%20is%20done%20with%20a%20symplectic%20kernel%2C%20and%20it%20is%20shown%20how%20this%0Asymplectic%20kernel%20can%20be%20modified%20to%20be%20odd%20or%20even.%20The%20performance%20of%20the%0Amethod%20is%20validated%20in%20simulations%20for%20two%20Hamiltonian%20systems.%20The%20simulations%0Ashow%20that%20the%20learned%20dynamics%20reflect%20the%20energy-preservation%20of%20the%0AHamiltonian%20dynamics%2C%20and%20that%20the%20restriction%20to%20symplectic%20and%20odd%20dynamics%0Agives%20improved%20accuracy%20over%20a%20large%20domain%20of%20the%20phase%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09734v2&entry.124074799=Read"},
{"title": "Multi-agent cooperation through learning-aware policy gradients", "author": "Alexander Meulemans and Seijin Kobayashi and Johannes von Oswald and Nino Scherrer and Eric Elmoznino and Blake Richards and Guillaume Lajoie and Blaise Ag\u00fcera y Arcas and Jo\u00e3o Sacramento", "abstract": "  Self-interested individuals often fail to cooperate, posing a fundamental\nchallenge for multi-agent learning. How can we achieve cooperation among\nself-interested, independent learning agents? Promising recent work has shown\nthat in certain tasks cooperation can be established between learning-aware\nagents who model the learning dynamics of each other. Here, we present the\nfirst unbiased, higher-derivative-free policy gradient algorithm for\nlearning-aware reinforcement learning, which takes into account that other\nagents are themselves learning through trial and error based on multiple noisy\ntrials. We then leverage efficient sequence models to condition behavior on\nlong observation histories that contain traces of the learning dynamics of\nother agents. Training long-context policies with our algorithm leads to\ncooperative behavior and high returns on standard social dilemmas, including a\nchallenging environment where temporally-extended action coordination is\nrequired. Finally, we derive from the iterated prisoner's dilemma a novel\nexplanation for how and when cooperation arises among self-interested\nlearning-aware agents.\n", "link": "http://arxiv.org/abs/2410.18636v1", "date": "2024-10-24", "relevancy": 1.611, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.57}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5231}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-agent%20cooperation%20through%20learning-aware%20policy%20gradients&body=Title%3A%20Multi-agent%20cooperation%20through%20learning-aware%20policy%20gradients%0AAuthor%3A%20Alexander%20Meulemans%20and%20Seijin%20Kobayashi%20and%20Johannes%20von%20Oswald%20and%20Nino%20Scherrer%20and%20Eric%20Elmoznino%20and%20Blake%20Richards%20and%20Guillaume%20Lajoie%20and%20Blaise%20Ag%C3%BCera%20y%20Arcas%20and%20Jo%C3%A3o%20Sacramento%0AAbstract%3A%20%20%20Self-interested%20individuals%20often%20fail%20to%20cooperate%2C%20posing%20a%20fundamental%0Achallenge%20for%20multi-agent%20learning.%20How%20can%20we%20achieve%20cooperation%20among%0Aself-interested%2C%20independent%20learning%20agents%3F%20Promising%20recent%20work%20has%20shown%0Athat%20in%20certain%20tasks%20cooperation%20can%20be%20established%20between%20learning-aware%0Aagents%20who%20model%20the%20learning%20dynamics%20of%20each%20other.%20Here%2C%20we%20present%20the%0Afirst%20unbiased%2C%20higher-derivative-free%20policy%20gradient%20algorithm%20for%0Alearning-aware%20reinforcement%20learning%2C%20which%20takes%20into%20account%20that%20other%0Aagents%20are%20themselves%20learning%20through%20trial%20and%20error%20based%20on%20multiple%20noisy%0Atrials.%20We%20then%20leverage%20efficient%20sequence%20models%20to%20condition%20behavior%20on%0Along%20observation%20histories%20that%20contain%20traces%20of%20the%20learning%20dynamics%20of%0Aother%20agents.%20Training%20long-context%20policies%20with%20our%20algorithm%20leads%20to%0Acooperative%20behavior%20and%20high%20returns%20on%20standard%20social%20dilemmas%2C%20including%20a%0Achallenging%20environment%20where%20temporally-extended%20action%20coordination%20is%0Arequired.%20Finally%2C%20we%20derive%20from%20the%20iterated%20prisoner%27s%20dilemma%20a%20novel%0Aexplanation%20for%20how%20and%20when%20cooperation%20arises%20among%20self-interested%0Alearning-aware%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-agent%2520cooperation%2520through%2520learning-aware%2520policy%2520gradients%26entry.906535625%3DAlexander%2520Meulemans%2520and%2520Seijin%2520Kobayashi%2520and%2520Johannes%2520von%2520Oswald%2520and%2520Nino%2520Scherrer%2520and%2520Eric%2520Elmoznino%2520and%2520Blake%2520Richards%2520and%2520Guillaume%2520Lajoie%2520and%2520Blaise%2520Ag%25C3%25BCera%2520y%2520Arcas%2520and%2520Jo%25C3%25A3o%2520Sacramento%26entry.1292438233%3D%2520%2520Self-interested%2520individuals%2520often%2520fail%2520to%2520cooperate%252C%2520posing%2520a%2520fundamental%250Achallenge%2520for%2520multi-agent%2520learning.%2520How%2520can%2520we%2520achieve%2520cooperation%2520among%250Aself-interested%252C%2520independent%2520learning%2520agents%253F%2520Promising%2520recent%2520work%2520has%2520shown%250Athat%2520in%2520certain%2520tasks%2520cooperation%2520can%2520be%2520established%2520between%2520learning-aware%250Aagents%2520who%2520model%2520the%2520learning%2520dynamics%2520of%2520each%2520other.%2520Here%252C%2520we%2520present%2520the%250Afirst%2520unbiased%252C%2520higher-derivative-free%2520policy%2520gradient%2520algorithm%2520for%250Alearning-aware%2520reinforcement%2520learning%252C%2520which%2520takes%2520into%2520account%2520that%2520other%250Aagents%2520are%2520themselves%2520learning%2520through%2520trial%2520and%2520error%2520based%2520on%2520multiple%2520noisy%250Atrials.%2520We%2520then%2520leverage%2520efficient%2520sequence%2520models%2520to%2520condition%2520behavior%2520on%250Along%2520observation%2520histories%2520that%2520contain%2520traces%2520of%2520the%2520learning%2520dynamics%2520of%250Aother%2520agents.%2520Training%2520long-context%2520policies%2520with%2520our%2520algorithm%2520leads%2520to%250Acooperative%2520behavior%2520and%2520high%2520returns%2520on%2520standard%2520social%2520dilemmas%252C%2520including%2520a%250Achallenging%2520environment%2520where%2520temporally-extended%2520action%2520coordination%2520is%250Arequired.%2520Finally%252C%2520we%2520derive%2520from%2520the%2520iterated%2520prisoner%2527s%2520dilemma%2520a%2520novel%250Aexplanation%2520for%2520how%2520and%2520when%2520cooperation%2520arises%2520among%2520self-interested%250Alearning-aware%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-agent%20cooperation%20through%20learning-aware%20policy%20gradients&entry.906535625=Alexander%20Meulemans%20and%20Seijin%20Kobayashi%20and%20Johannes%20von%20Oswald%20and%20Nino%20Scherrer%20and%20Eric%20Elmoznino%20and%20Blake%20Richards%20and%20Guillaume%20Lajoie%20and%20Blaise%20Ag%C3%BCera%20y%20Arcas%20and%20Jo%C3%A3o%20Sacramento&entry.1292438233=%20%20Self-interested%20individuals%20often%20fail%20to%20cooperate%2C%20posing%20a%20fundamental%0Achallenge%20for%20multi-agent%20learning.%20How%20can%20we%20achieve%20cooperation%20among%0Aself-interested%2C%20independent%20learning%20agents%3F%20Promising%20recent%20work%20has%20shown%0Athat%20in%20certain%20tasks%20cooperation%20can%20be%20established%20between%20learning-aware%0Aagents%20who%20model%20the%20learning%20dynamics%20of%20each%20other.%20Here%2C%20we%20present%20the%0Afirst%20unbiased%2C%20higher-derivative-free%20policy%20gradient%20algorithm%20for%0Alearning-aware%20reinforcement%20learning%2C%20which%20takes%20into%20account%20that%20other%0Aagents%20are%20themselves%20learning%20through%20trial%20and%20error%20based%20on%20multiple%20noisy%0Atrials.%20We%20then%20leverage%20efficient%20sequence%20models%20to%20condition%20behavior%20on%0Along%20observation%20histories%20that%20contain%20traces%20of%20the%20learning%20dynamics%20of%0Aother%20agents.%20Training%20long-context%20policies%20with%20our%20algorithm%20leads%20to%0Acooperative%20behavior%20and%20high%20returns%20on%20standard%20social%20dilemmas%2C%20including%20a%0Achallenging%20environment%20where%20temporally-extended%20action%20coordination%20is%0Arequired.%20Finally%2C%20we%20derive%20from%20the%20iterated%20prisoner%27s%20dilemma%20a%20novel%0Aexplanation%20for%20how%20and%20when%20cooperation%20arises%20among%20self-interested%0Alearning-aware%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18636v1&entry.124074799=Read"},
{"title": "Testing Support Size More Efficiently Than Learning Histograms", "author": "Renato Ferreira Pinto Jr. and Nathaniel Harms", "abstract": "  Consider two problems about an unknown probability distribution $p$:\n  1. How many samples from $p$ are required to test if $p$ is supported on $n$\nelements or not? Specifically, given samples from $p$, determine whether it is\nsupported on at most $n$ elements, or it is \"$\\epsilon$-far\" (in total\nvariation distance) from being supported on $n$ elements.\n  2. Given $m$ samples from $p$, what is the largest lower bound on its support\nsize that we can produce?\n  The best known upper bound for problem (1) uses a general algorithm for\nlearning the histogram of the distribution $p$, which requires\n$\\Theta(\\tfrac{n}{\\epsilon^2 \\log n})$ samples. We show that testing can be\ndone more efficiently than learning the histogram, using only\n$O(\\tfrac{n}{\\epsilon \\log n} \\log(1/\\epsilon))$ samples, nearly matching the\nbest known lower bound of $\\Omega(\\tfrac{n}{\\epsilon \\log n})$. This algorithm\nalso provides a better solution to problem (2), producing larger lower bounds\non support size than what follows from previous work. The proof relies on an\nanalysis of Chebyshev polynomial approximations outside the range where they\nare designed to be good approximations, and the paper is intended as an\naccessible self-contained exposition of the Chebyshev polynomial method.\n", "link": "http://arxiv.org/abs/2410.18915v1", "date": "2024-10-24", "relevancy": 1.9484, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4129}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3857}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Testing%20Support%20Size%20More%20Efficiently%20Than%20Learning%20Histograms&body=Title%3A%20Testing%20Support%20Size%20More%20Efficiently%20Than%20Learning%20Histograms%0AAuthor%3A%20Renato%20Ferreira%20Pinto%20Jr.%20and%20Nathaniel%20Harms%0AAbstract%3A%20%20%20Consider%20two%20problems%20about%20an%20unknown%20probability%20distribution%20%24p%24%3A%0A%20%201.%20How%20many%20samples%20from%20%24p%24%20are%20required%20to%20test%20if%20%24p%24%20is%20supported%20on%20%24n%24%0Aelements%20or%20not%3F%20Specifically%2C%20given%20samples%20from%20%24p%24%2C%20determine%20whether%20it%20is%0Asupported%20on%20at%20most%20%24n%24%20elements%2C%20or%20it%20is%20%22%24%5Cepsilon%24-far%22%20%28in%20total%0Avariation%20distance%29%20from%20being%20supported%20on%20%24n%24%20elements.%0A%20%202.%20Given%20%24m%24%20samples%20from%20%24p%24%2C%20what%20is%20the%20largest%20lower%20bound%20on%20its%20support%0Asize%20that%20we%20can%20produce%3F%0A%20%20The%20best%20known%20upper%20bound%20for%20problem%20%281%29%20uses%20a%20general%20algorithm%20for%0Alearning%20the%20histogram%20of%20the%20distribution%20%24p%24%2C%20which%20requires%0A%24%5CTheta%28%5Ctfrac%7Bn%7D%7B%5Cepsilon%5E2%20%5Clog%20n%7D%29%24%20samples.%20We%20show%20that%20testing%20can%20be%0Adone%20more%20efficiently%20than%20learning%20the%20histogram%2C%20using%20only%0A%24O%28%5Ctfrac%7Bn%7D%7B%5Cepsilon%20%5Clog%20n%7D%20%5Clog%281/%5Cepsilon%29%29%24%20samples%2C%20nearly%20matching%20the%0Abest%20known%20lower%20bound%20of%20%24%5COmega%28%5Ctfrac%7Bn%7D%7B%5Cepsilon%20%5Clog%20n%7D%29%24.%20This%20algorithm%0Aalso%20provides%20a%20better%20solution%20to%20problem%20%282%29%2C%20producing%20larger%20lower%20bounds%0Aon%20support%20size%20than%20what%20follows%20from%20previous%20work.%20The%20proof%20relies%20on%20an%0Aanalysis%20of%20Chebyshev%20polynomial%20approximations%20outside%20the%20range%20where%20they%0Aare%20designed%20to%20be%20good%20approximations%2C%20and%20the%20paper%20is%20intended%20as%20an%0Aaccessible%20self-contained%20exposition%20of%20the%20Chebyshev%20polynomial%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTesting%2520Support%2520Size%2520More%2520Efficiently%2520Than%2520Learning%2520Histograms%26entry.906535625%3DRenato%2520Ferreira%2520Pinto%2520Jr.%2520and%2520Nathaniel%2520Harms%26entry.1292438233%3D%2520%2520Consider%2520two%2520problems%2520about%2520an%2520unknown%2520probability%2520distribution%2520%2524p%2524%253A%250A%2520%25201.%2520How%2520many%2520samples%2520from%2520%2524p%2524%2520are%2520required%2520to%2520test%2520if%2520%2524p%2524%2520is%2520supported%2520on%2520%2524n%2524%250Aelements%2520or%2520not%253F%2520Specifically%252C%2520given%2520samples%2520from%2520%2524p%2524%252C%2520determine%2520whether%2520it%2520is%250Asupported%2520on%2520at%2520most%2520%2524n%2524%2520elements%252C%2520or%2520it%2520is%2520%2522%2524%255Cepsilon%2524-far%2522%2520%2528in%2520total%250Avariation%2520distance%2529%2520from%2520being%2520supported%2520on%2520%2524n%2524%2520elements.%250A%2520%25202.%2520Given%2520%2524m%2524%2520samples%2520from%2520%2524p%2524%252C%2520what%2520is%2520the%2520largest%2520lower%2520bound%2520on%2520its%2520support%250Asize%2520that%2520we%2520can%2520produce%253F%250A%2520%2520The%2520best%2520known%2520upper%2520bound%2520for%2520problem%2520%25281%2529%2520uses%2520a%2520general%2520algorithm%2520for%250Alearning%2520the%2520histogram%2520of%2520the%2520distribution%2520%2524p%2524%252C%2520which%2520requires%250A%2524%255CTheta%2528%255Ctfrac%257Bn%257D%257B%255Cepsilon%255E2%2520%255Clog%2520n%257D%2529%2524%2520samples.%2520We%2520show%2520that%2520testing%2520can%2520be%250Adone%2520more%2520efficiently%2520than%2520learning%2520the%2520histogram%252C%2520using%2520only%250A%2524O%2528%255Ctfrac%257Bn%257D%257B%255Cepsilon%2520%255Clog%2520n%257D%2520%255Clog%25281/%255Cepsilon%2529%2529%2524%2520samples%252C%2520nearly%2520matching%2520the%250Abest%2520known%2520lower%2520bound%2520of%2520%2524%255COmega%2528%255Ctfrac%257Bn%257D%257B%255Cepsilon%2520%255Clog%2520n%257D%2529%2524.%2520This%2520algorithm%250Aalso%2520provides%2520a%2520better%2520solution%2520to%2520problem%2520%25282%2529%252C%2520producing%2520larger%2520lower%2520bounds%250Aon%2520support%2520size%2520than%2520what%2520follows%2520from%2520previous%2520work.%2520The%2520proof%2520relies%2520on%2520an%250Aanalysis%2520of%2520Chebyshev%2520polynomial%2520approximations%2520outside%2520the%2520range%2520where%2520they%250Aare%2520designed%2520to%2520be%2520good%2520approximations%252C%2520and%2520the%2520paper%2520is%2520intended%2520as%2520an%250Aaccessible%2520self-contained%2520exposition%2520of%2520the%2520Chebyshev%2520polynomial%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Testing%20Support%20Size%20More%20Efficiently%20Than%20Learning%20Histograms&entry.906535625=Renato%20Ferreira%20Pinto%20Jr.%20and%20Nathaniel%20Harms&entry.1292438233=%20%20Consider%20two%20problems%20about%20an%20unknown%20probability%20distribution%20%24p%24%3A%0A%20%201.%20How%20many%20samples%20from%20%24p%24%20are%20required%20to%20test%20if%20%24p%24%20is%20supported%20on%20%24n%24%0Aelements%20or%20not%3F%20Specifically%2C%20given%20samples%20from%20%24p%24%2C%20determine%20whether%20it%20is%0Asupported%20on%20at%20most%20%24n%24%20elements%2C%20or%20it%20is%20%22%24%5Cepsilon%24-far%22%20%28in%20total%0Avariation%20distance%29%20from%20being%20supported%20on%20%24n%24%20elements.%0A%20%202.%20Given%20%24m%24%20samples%20from%20%24p%24%2C%20what%20is%20the%20largest%20lower%20bound%20on%20its%20support%0Asize%20that%20we%20can%20produce%3F%0A%20%20The%20best%20known%20upper%20bound%20for%20problem%20%281%29%20uses%20a%20general%20algorithm%20for%0Alearning%20the%20histogram%20of%20the%20distribution%20%24p%24%2C%20which%20requires%0A%24%5CTheta%28%5Ctfrac%7Bn%7D%7B%5Cepsilon%5E2%20%5Clog%20n%7D%29%24%20samples.%20We%20show%20that%20testing%20can%20be%0Adone%20more%20efficiently%20than%20learning%20the%20histogram%2C%20using%20only%0A%24O%28%5Ctfrac%7Bn%7D%7B%5Cepsilon%20%5Clog%20n%7D%20%5Clog%281/%5Cepsilon%29%29%24%20samples%2C%20nearly%20matching%20the%0Abest%20known%20lower%20bound%20of%20%24%5COmega%28%5Ctfrac%7Bn%7D%7B%5Cepsilon%20%5Clog%20n%7D%29%24.%20This%20algorithm%0Aalso%20provides%20a%20better%20solution%20to%20problem%20%282%29%2C%20producing%20larger%20lower%20bounds%0Aon%20support%20size%20than%20what%20follows%20from%20previous%20work.%20The%20proof%20relies%20on%20an%0Aanalysis%20of%20Chebyshev%20polynomial%20approximations%20outside%20the%20range%20where%20they%0Aare%20designed%20to%20be%20good%20approximations%2C%20and%20the%20paper%20is%20intended%20as%20an%0Aaccessible%20self-contained%20exposition%20of%20the%20Chebyshev%20polynomial%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18915v1&entry.124074799=Read"},
{"title": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs", "author": "Ran Zhang and Wei Zhao and Steffen Eger", "abstract": "  Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus comprising multiple verified\nhuman translations and outputs from 9 MT systems, which totals over 2k\nparagraphs and includes 13k annotated sentences across four language pairs,\ncosting 4.5k Euro. This corpus enables us to (i) examine the consistency and\nadequacy of multiple annotation schemes, (ii) compare evaluations by students\nand professionals, and (iii) assess the effectiveness of LLM-based metrics. We\nfind that Multidimensional Quality Metrics (MQM), as the de facto standard in\nnon-literary human MT evaluation, is inadequate for literary translation: While\nBest-Worst Scaling (BWS) with students and Scalar Quality Metric (SQM) with\nprofessional translators prefer human translations at rates of ~82% and ~94%,\nrespectively, MQM with student annotators prefers human professional\ntranslations over the translations of the best-performing LLMs in only ~42% of\ncases. While automatic metrics generally show a moderate correlation with human\nMQM and SQM, they struggle to accurately identify human translations, with\nrates of at most ~20%. Our overall evaluation indicates that human professional\ntranslations consistently outperform LLM translations, where even the most\nrecent LLMs tend to produce more literal and less diverse translations compared\nto human translations. However, newer LLMs such as GPT-4o perform substantially\nbetter than older ones.\n", "link": "http://arxiv.org/abs/2410.18697v1", "date": "2024-10-24", "relevancy": 1.6874, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Good%20Are%20LLMs%20for%20Literary%20Translation%2C%20Really%3F%20Literary%20Translation%0A%20%20Evaluation%20with%20Humans%20and%20LLMs&body=Title%3A%20How%20Good%20Are%20LLMs%20for%20Literary%20Translation%2C%20Really%3F%20Literary%20Translation%0A%20%20Evaluation%20with%20Humans%20and%20LLMs%0AAuthor%3A%20Ran%20Zhang%20and%20Wei%20Zhao%20and%20Steffen%20Eger%0AAbstract%3A%20%20%20Recent%20research%20has%20focused%20on%20literary%20machine%20translation%20%28MT%29%20as%20a%20new%0Achallenge%20in%20MT.%20However%2C%20the%20evaluation%20of%20literary%20MT%20remains%20an%20open%0Aproblem.%20We%20contribute%20to%20this%20ongoing%20discussion%20by%20introducing%0ALITEVAL-CORPUS%2C%20a%20paragraph-level%20parallel%20corpus%20comprising%20multiple%20verified%0Ahuman%20translations%20and%20outputs%20from%209%20MT%20systems%2C%20which%20totals%20over%202k%0Aparagraphs%20and%20includes%2013k%20annotated%20sentences%20across%20four%20language%20pairs%2C%0Acosting%204.5k%20Euro.%20This%20corpus%20enables%20us%20to%20%28i%29%20examine%20the%20consistency%20and%0Aadequacy%20of%20multiple%20annotation%20schemes%2C%20%28ii%29%20compare%20evaluations%20by%20students%0Aand%20professionals%2C%20and%20%28iii%29%20assess%20the%20effectiveness%20of%20LLM-based%20metrics.%20We%0Afind%20that%20Multidimensional%20Quality%20Metrics%20%28MQM%29%2C%20as%20the%20de%20facto%20standard%20in%0Anon-literary%20human%20MT%20evaluation%2C%20is%20inadequate%20for%20literary%20translation%3A%20While%0ABest-Worst%20Scaling%20%28BWS%29%20with%20students%20and%20Scalar%20Quality%20Metric%20%28SQM%29%20with%0Aprofessional%20translators%20prefer%20human%20translations%20at%20rates%20of%20~82%25%20and%20~94%25%2C%0Arespectively%2C%20MQM%20with%20student%20annotators%20prefers%20human%20professional%0Atranslations%20over%20the%20translations%20of%20the%20best-performing%20LLMs%20in%20only%20~42%25%20of%0Acases.%20While%20automatic%20metrics%20generally%20show%20a%20moderate%20correlation%20with%20human%0AMQM%20and%20SQM%2C%20they%20struggle%20to%20accurately%20identify%20human%20translations%2C%20with%0Arates%20of%20at%20most%20~20%25.%20Our%20overall%20evaluation%20indicates%20that%20human%20professional%0Atranslations%20consistently%20outperform%20LLM%20translations%2C%20where%20even%20the%20most%0Arecent%20LLMs%20tend%20to%20produce%20more%20literal%20and%20less%20diverse%20translations%20compared%0Ato%20human%20translations.%20However%2C%20newer%20LLMs%20such%20as%20GPT-4o%20perform%20substantially%0Abetter%20than%20older%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Good%2520Are%2520LLMs%2520for%2520Literary%2520Translation%252C%2520Really%253F%2520Literary%2520Translation%250A%2520%2520Evaluation%2520with%2520Humans%2520and%2520LLMs%26entry.906535625%3DRan%2520Zhang%2520and%2520Wei%2520Zhao%2520and%2520Steffen%2520Eger%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520focused%2520on%2520literary%2520machine%2520translation%2520%2528MT%2529%2520as%2520a%2520new%250Achallenge%2520in%2520MT.%2520However%252C%2520the%2520evaluation%2520of%2520literary%2520MT%2520remains%2520an%2520open%250Aproblem.%2520We%2520contribute%2520to%2520this%2520ongoing%2520discussion%2520by%2520introducing%250ALITEVAL-CORPUS%252C%2520a%2520paragraph-level%2520parallel%2520corpus%2520comprising%2520multiple%2520verified%250Ahuman%2520translations%2520and%2520outputs%2520from%25209%2520MT%2520systems%252C%2520which%2520totals%2520over%25202k%250Aparagraphs%2520and%2520includes%252013k%2520annotated%2520sentences%2520across%2520four%2520language%2520pairs%252C%250Acosting%25204.5k%2520Euro.%2520This%2520corpus%2520enables%2520us%2520to%2520%2528i%2529%2520examine%2520the%2520consistency%2520and%250Aadequacy%2520of%2520multiple%2520annotation%2520schemes%252C%2520%2528ii%2529%2520compare%2520evaluations%2520by%2520students%250Aand%2520professionals%252C%2520and%2520%2528iii%2529%2520assess%2520the%2520effectiveness%2520of%2520LLM-based%2520metrics.%2520We%250Afind%2520that%2520Multidimensional%2520Quality%2520Metrics%2520%2528MQM%2529%252C%2520as%2520the%2520de%2520facto%2520standard%2520in%250Anon-literary%2520human%2520MT%2520evaluation%252C%2520is%2520inadequate%2520for%2520literary%2520translation%253A%2520While%250ABest-Worst%2520Scaling%2520%2528BWS%2529%2520with%2520students%2520and%2520Scalar%2520Quality%2520Metric%2520%2528SQM%2529%2520with%250Aprofessional%2520translators%2520prefer%2520human%2520translations%2520at%2520rates%2520of%2520~82%2525%2520and%2520~94%2525%252C%250Arespectively%252C%2520MQM%2520with%2520student%2520annotators%2520prefers%2520human%2520professional%250Atranslations%2520over%2520the%2520translations%2520of%2520the%2520best-performing%2520LLMs%2520in%2520only%2520~42%2525%2520of%250Acases.%2520While%2520automatic%2520metrics%2520generally%2520show%2520a%2520moderate%2520correlation%2520with%2520human%250AMQM%2520and%2520SQM%252C%2520they%2520struggle%2520to%2520accurately%2520identify%2520human%2520translations%252C%2520with%250Arates%2520of%2520at%2520most%2520~20%2525.%2520Our%2520overall%2520evaluation%2520indicates%2520that%2520human%2520professional%250Atranslations%2520consistently%2520outperform%2520LLM%2520translations%252C%2520where%2520even%2520the%2520most%250Arecent%2520LLMs%2520tend%2520to%2520produce%2520more%2520literal%2520and%2520less%2520diverse%2520translations%2520compared%250Ato%2520human%2520translations.%2520However%252C%2520newer%2520LLMs%2520such%2520as%2520GPT-4o%2520perform%2520substantially%250Abetter%2520than%2520older%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Good%20Are%20LLMs%20for%20Literary%20Translation%2C%20Really%3F%20Literary%20Translation%0A%20%20Evaluation%20with%20Humans%20and%20LLMs&entry.906535625=Ran%20Zhang%20and%20Wei%20Zhao%20and%20Steffen%20Eger&entry.1292438233=%20%20Recent%20research%20has%20focused%20on%20literary%20machine%20translation%20%28MT%29%20as%20a%20new%0Achallenge%20in%20MT.%20However%2C%20the%20evaluation%20of%20literary%20MT%20remains%20an%20open%0Aproblem.%20We%20contribute%20to%20this%20ongoing%20discussion%20by%20introducing%0ALITEVAL-CORPUS%2C%20a%20paragraph-level%20parallel%20corpus%20comprising%20multiple%20verified%0Ahuman%20translations%20and%20outputs%20from%209%20MT%20systems%2C%20which%20totals%20over%202k%0Aparagraphs%20and%20includes%2013k%20annotated%20sentences%20across%20four%20language%20pairs%2C%0Acosting%204.5k%20Euro.%20This%20corpus%20enables%20us%20to%20%28i%29%20examine%20the%20consistency%20and%0Aadequacy%20of%20multiple%20annotation%20schemes%2C%20%28ii%29%20compare%20evaluations%20by%20students%0Aand%20professionals%2C%20and%20%28iii%29%20assess%20the%20effectiveness%20of%20LLM-based%20metrics.%20We%0Afind%20that%20Multidimensional%20Quality%20Metrics%20%28MQM%29%2C%20as%20the%20de%20facto%20standard%20in%0Anon-literary%20human%20MT%20evaluation%2C%20is%20inadequate%20for%20literary%20translation%3A%20While%0ABest-Worst%20Scaling%20%28BWS%29%20with%20students%20and%20Scalar%20Quality%20Metric%20%28SQM%29%20with%0Aprofessional%20translators%20prefer%20human%20translations%20at%20rates%20of%20~82%25%20and%20~94%25%2C%0Arespectively%2C%20MQM%20with%20student%20annotators%20prefers%20human%20professional%0Atranslations%20over%20the%20translations%20of%20the%20best-performing%20LLMs%20in%20only%20~42%25%20of%0Acases.%20While%20automatic%20metrics%20generally%20show%20a%20moderate%20correlation%20with%20human%0AMQM%20and%20SQM%2C%20they%20struggle%20to%20accurately%20identify%20human%20translations%2C%20with%0Arates%20of%20at%20most%20~20%25.%20Our%20overall%20evaluation%20indicates%20that%20human%20professional%0Atranslations%20consistently%20outperform%20LLM%20translations%2C%20where%20even%20the%20most%0Arecent%20LLMs%20tend%20to%20produce%20more%20literal%20and%20less%20diverse%20translations%20compared%0Ato%20human%20translations.%20However%2C%20newer%20LLMs%20such%20as%20GPT-4o%20perform%20substantially%0Abetter%20than%20older%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18697v1&entry.124074799=Read"},
{"title": "Modulated Adaptive Fourier Neural Operators for Temporal Interpolation\n  of Weather Forecasts", "author": "Jussi Leinonen and Boris Bonev and Thorsten Kurth and Yair Cohen", "abstract": "  Weather and climate data are often available at limited temporal resolution,\neither due to storage limitations, or in the case of weather forecast models\nbased on deep learning, their inherently long time steps. The coarse temporal\nresolution makes it difficult to capture rapidly evolving weather events. To\naddress this limitation, we introduce an interpolation model that reconstructs\nthe atmospheric state between two points in time for which the state is known.\nThe model makes use of a novel network layer that modifies the adaptive Fourier\nneural operator (AFNO), which has been previously used in weather prediction\nand other applications of machine learning to physics problems. The modulated\nAFNO (ModAFNO) layer takes an embedding, here computed from the interpolation\ntarget time, as an additional input and applies a learned shift-scale operation\ninside the AFNO layers to adapt them to the target time. Thus, one model can be\nused to produce all intermediate time steps. Trained to interpolate between two\ntime steps 6 h apart, the ModAFNO-based interpolation model produces 1 h\nresolution intermediate time steps that are visually nearly indistinguishable\nfrom the actual corresponding 1 h resolution data. The model reduces the RMSE\nloss of reconstructing the intermediate steps by approximately 50% compared to\nlinear interpolation. We also demonstrate its ability to reproduce the\nstatistics of extreme weather events such as hurricanes and heat waves better\nthan 6 h resolution data. The ModAFNO layer is generic and is expected to be\napplicable to other problems, including weather forecasting with tunable lead\ntime.\n", "link": "http://arxiv.org/abs/2410.18904v1", "date": "2024-10-24", "relevancy": 0.958, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4895}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4755}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modulated%20Adaptive%20Fourier%20Neural%20Operators%20for%20Temporal%20Interpolation%0A%20%20of%20Weather%20Forecasts&body=Title%3A%20Modulated%20Adaptive%20Fourier%20Neural%20Operators%20for%20Temporal%20Interpolation%0A%20%20of%20Weather%20Forecasts%0AAuthor%3A%20Jussi%20Leinonen%20and%20Boris%20Bonev%20and%20Thorsten%20Kurth%20and%20Yair%20Cohen%0AAbstract%3A%20%20%20Weather%20and%20climate%20data%20are%20often%20available%20at%20limited%20temporal%20resolution%2C%0Aeither%20due%20to%20storage%20limitations%2C%20or%20in%20the%20case%20of%20weather%20forecast%20models%0Abased%20on%20deep%20learning%2C%20their%20inherently%20long%20time%20steps.%20The%20coarse%20temporal%0Aresolution%20makes%20it%20difficult%20to%20capture%20rapidly%20evolving%20weather%20events.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20an%20interpolation%20model%20that%20reconstructs%0Athe%20atmospheric%20state%20between%20two%20points%20in%20time%20for%20which%20the%20state%20is%20known.%0AThe%20model%20makes%20use%20of%20a%20novel%20network%20layer%20that%20modifies%20the%20adaptive%20Fourier%0Aneural%20operator%20%28AFNO%29%2C%20which%20has%20been%20previously%20used%20in%20weather%20prediction%0Aand%20other%20applications%20of%20machine%20learning%20to%20physics%20problems.%20The%20modulated%0AAFNO%20%28ModAFNO%29%20layer%20takes%20an%20embedding%2C%20here%20computed%20from%20the%20interpolation%0Atarget%20time%2C%20as%20an%20additional%20input%20and%20applies%20a%20learned%20shift-scale%20operation%0Ainside%20the%20AFNO%20layers%20to%20adapt%20them%20to%20the%20target%20time.%20Thus%2C%20one%20model%20can%20be%0Aused%20to%20produce%20all%20intermediate%20time%20steps.%20Trained%20to%20interpolate%20between%20two%0Atime%20steps%206%20h%20apart%2C%20the%20ModAFNO-based%20interpolation%20model%20produces%201%20h%0Aresolution%20intermediate%20time%20steps%20that%20are%20visually%20nearly%20indistinguishable%0Afrom%20the%20actual%20corresponding%201%20h%20resolution%20data.%20The%20model%20reduces%20the%20RMSE%0Aloss%20of%20reconstructing%20the%20intermediate%20steps%20by%20approximately%2050%25%20compared%20to%0Alinear%20interpolation.%20We%20also%20demonstrate%20its%20ability%20to%20reproduce%20the%0Astatistics%20of%20extreme%20weather%20events%20such%20as%20hurricanes%20and%20heat%20waves%20better%0Athan%206%20h%20resolution%20data.%20The%20ModAFNO%20layer%20is%20generic%20and%20is%20expected%20to%20be%0Aapplicable%20to%20other%20problems%2C%20including%20weather%20forecasting%20with%20tunable%20lead%0Atime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModulated%2520Adaptive%2520Fourier%2520Neural%2520Operators%2520for%2520Temporal%2520Interpolation%250A%2520%2520of%2520Weather%2520Forecasts%26entry.906535625%3DJussi%2520Leinonen%2520and%2520Boris%2520Bonev%2520and%2520Thorsten%2520Kurth%2520and%2520Yair%2520Cohen%26entry.1292438233%3D%2520%2520Weather%2520and%2520climate%2520data%2520are%2520often%2520available%2520at%2520limited%2520temporal%2520resolution%252C%250Aeither%2520due%2520to%2520storage%2520limitations%252C%2520or%2520in%2520the%2520case%2520of%2520weather%2520forecast%2520models%250Abased%2520on%2520deep%2520learning%252C%2520their%2520inherently%2520long%2520time%2520steps.%2520The%2520coarse%2520temporal%250Aresolution%2520makes%2520it%2520difficult%2520to%2520capture%2520rapidly%2520evolving%2520weather%2520events.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520an%2520interpolation%2520model%2520that%2520reconstructs%250Athe%2520atmospheric%2520state%2520between%2520two%2520points%2520in%2520time%2520for%2520which%2520the%2520state%2520is%2520known.%250AThe%2520model%2520makes%2520use%2520of%2520a%2520novel%2520network%2520layer%2520that%2520modifies%2520the%2520adaptive%2520Fourier%250Aneural%2520operator%2520%2528AFNO%2529%252C%2520which%2520has%2520been%2520previously%2520used%2520in%2520weather%2520prediction%250Aand%2520other%2520applications%2520of%2520machine%2520learning%2520to%2520physics%2520problems.%2520The%2520modulated%250AAFNO%2520%2528ModAFNO%2529%2520layer%2520takes%2520an%2520embedding%252C%2520here%2520computed%2520from%2520the%2520interpolation%250Atarget%2520time%252C%2520as%2520an%2520additional%2520input%2520and%2520applies%2520a%2520learned%2520shift-scale%2520operation%250Ainside%2520the%2520AFNO%2520layers%2520to%2520adapt%2520them%2520to%2520the%2520target%2520time.%2520Thus%252C%2520one%2520model%2520can%2520be%250Aused%2520to%2520produce%2520all%2520intermediate%2520time%2520steps.%2520Trained%2520to%2520interpolate%2520between%2520two%250Atime%2520steps%25206%2520h%2520apart%252C%2520the%2520ModAFNO-based%2520interpolation%2520model%2520produces%25201%2520h%250Aresolution%2520intermediate%2520time%2520steps%2520that%2520are%2520visually%2520nearly%2520indistinguishable%250Afrom%2520the%2520actual%2520corresponding%25201%2520h%2520resolution%2520data.%2520The%2520model%2520reduces%2520the%2520RMSE%250Aloss%2520of%2520reconstructing%2520the%2520intermediate%2520steps%2520by%2520approximately%252050%2525%2520compared%2520to%250Alinear%2520interpolation.%2520We%2520also%2520demonstrate%2520its%2520ability%2520to%2520reproduce%2520the%250Astatistics%2520of%2520extreme%2520weather%2520events%2520such%2520as%2520hurricanes%2520and%2520heat%2520waves%2520better%250Athan%25206%2520h%2520resolution%2520data.%2520The%2520ModAFNO%2520layer%2520is%2520generic%2520and%2520is%2520expected%2520to%2520be%250Aapplicable%2520to%2520other%2520problems%252C%2520including%2520weather%2520forecasting%2520with%2520tunable%2520lead%250Atime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modulated%20Adaptive%20Fourier%20Neural%20Operators%20for%20Temporal%20Interpolation%0A%20%20of%20Weather%20Forecasts&entry.906535625=Jussi%20Leinonen%20and%20Boris%20Bonev%20and%20Thorsten%20Kurth%20and%20Yair%20Cohen&entry.1292438233=%20%20Weather%20and%20climate%20data%20are%20often%20available%20at%20limited%20temporal%20resolution%2C%0Aeither%20due%20to%20storage%20limitations%2C%20or%20in%20the%20case%20of%20weather%20forecast%20models%0Abased%20on%20deep%20learning%2C%20their%20inherently%20long%20time%20steps.%20The%20coarse%20temporal%0Aresolution%20makes%20it%20difficult%20to%20capture%20rapidly%20evolving%20weather%20events.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20an%20interpolation%20model%20that%20reconstructs%0Athe%20atmospheric%20state%20between%20two%20points%20in%20time%20for%20which%20the%20state%20is%20known.%0AThe%20model%20makes%20use%20of%20a%20novel%20network%20layer%20that%20modifies%20the%20adaptive%20Fourier%0Aneural%20operator%20%28AFNO%29%2C%20which%20has%20been%20previously%20used%20in%20weather%20prediction%0Aand%20other%20applications%20of%20machine%20learning%20to%20physics%20problems.%20The%20modulated%0AAFNO%20%28ModAFNO%29%20layer%20takes%20an%20embedding%2C%20here%20computed%20from%20the%20interpolation%0Atarget%20time%2C%20as%20an%20additional%20input%20and%20applies%20a%20learned%20shift-scale%20operation%0Ainside%20the%20AFNO%20layers%20to%20adapt%20them%20to%20the%20target%20time.%20Thus%2C%20one%20model%20can%20be%0Aused%20to%20produce%20all%20intermediate%20time%20steps.%20Trained%20to%20interpolate%20between%20two%0Atime%20steps%206%20h%20apart%2C%20the%20ModAFNO-based%20interpolation%20model%20produces%201%20h%0Aresolution%20intermediate%20time%20steps%20that%20are%20visually%20nearly%20indistinguishable%0Afrom%20the%20actual%20corresponding%201%20h%20resolution%20data.%20The%20model%20reduces%20the%20RMSE%0Aloss%20of%20reconstructing%20the%20intermediate%20steps%20by%20approximately%2050%25%20compared%20to%0Alinear%20interpolation.%20We%20also%20demonstrate%20its%20ability%20to%20reproduce%20the%0Astatistics%20of%20extreme%20weather%20events%20such%20as%20hurricanes%20and%20heat%20waves%20better%0Athan%206%20h%20resolution%20data.%20The%20ModAFNO%20layer%20is%20generic%20and%20is%20expected%20to%20be%0Aapplicable%20to%20other%20problems%2C%20including%20weather%20forecasting%20with%20tunable%20lead%0Atime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18904v1&entry.124074799=Read"},
{"title": "Online path planning for kinematic-constrained UAVs in a dynamic\n  environment based on a Differential Evolution algorithm", "author": "Elias J. R. Freitas and Miri Weiss Cohen and Frederico G. Guimar\u00e3es and Luciano C. A. Pimenta", "abstract": "  This research presents an online path planner for Unmanned Aerial Vehicles\n(UAVs) that can handle dynamic obstacles and UAV motion constraints, including\nmaximum curvature and desired orientations. Our proposed planner uses a NURBS\npath representation and a Differential Evolution algorithm, incorporating\nconcepts from the Velocity Obstacle approach in a constraint function. Initial\nresults show that our approach is feasible and provides a foundation for future\nextensions to three-dimensional (3D) environments.\n", "link": "http://arxiv.org/abs/2410.18777v1", "date": "2024-10-24", "relevancy": 1.9778, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4968}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4957}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20path%20planning%20for%20kinematic-constrained%20UAVs%20in%20a%20dynamic%0A%20%20environment%20based%20on%20a%20Differential%20Evolution%20algorithm&body=Title%3A%20Online%20path%20planning%20for%20kinematic-constrained%20UAVs%20in%20a%20dynamic%0A%20%20environment%20based%20on%20a%20Differential%20Evolution%20algorithm%0AAuthor%3A%20Elias%20J.%20R.%20Freitas%20and%20Miri%20Weiss%20Cohen%20and%20Frederico%20G.%20Guimar%C3%A3es%20and%20Luciano%20C.%20A.%20Pimenta%0AAbstract%3A%20%20%20This%20research%20presents%20an%20online%20path%20planner%20for%20Unmanned%20Aerial%20Vehicles%0A%28UAVs%29%20that%20can%20handle%20dynamic%20obstacles%20and%20UAV%20motion%20constraints%2C%20including%0Amaximum%20curvature%20and%20desired%20orientations.%20Our%20proposed%20planner%20uses%20a%20NURBS%0Apath%20representation%20and%20a%20Differential%20Evolution%20algorithm%2C%20incorporating%0Aconcepts%20from%20the%20Velocity%20Obstacle%20approach%20in%20a%20constraint%20function.%20Initial%0Aresults%20show%20that%20our%20approach%20is%20feasible%20and%20provides%20a%20foundation%20for%20future%0Aextensions%20to%20three-dimensional%20%283D%29%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520path%2520planning%2520for%2520kinematic-constrained%2520UAVs%2520in%2520a%2520dynamic%250A%2520%2520environment%2520based%2520on%2520a%2520Differential%2520Evolution%2520algorithm%26entry.906535625%3DElias%2520J.%2520R.%2520Freitas%2520and%2520Miri%2520Weiss%2520Cohen%2520and%2520Frederico%2520G.%2520Guimar%25C3%25A3es%2520and%2520Luciano%2520C.%2520A.%2520Pimenta%26entry.1292438233%3D%2520%2520This%2520research%2520presents%2520an%2520online%2520path%2520planner%2520for%2520Unmanned%2520Aerial%2520Vehicles%250A%2528UAVs%2529%2520that%2520can%2520handle%2520dynamic%2520obstacles%2520and%2520UAV%2520motion%2520constraints%252C%2520including%250Amaximum%2520curvature%2520and%2520desired%2520orientations.%2520Our%2520proposed%2520planner%2520uses%2520a%2520NURBS%250Apath%2520representation%2520and%2520a%2520Differential%2520Evolution%2520algorithm%252C%2520incorporating%250Aconcepts%2520from%2520the%2520Velocity%2520Obstacle%2520approach%2520in%2520a%2520constraint%2520function.%2520Initial%250Aresults%2520show%2520that%2520our%2520approach%2520is%2520feasible%2520and%2520provides%2520a%2520foundation%2520for%2520future%250Aextensions%2520to%2520three-dimensional%2520%25283D%2529%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20path%20planning%20for%20kinematic-constrained%20UAVs%20in%20a%20dynamic%0A%20%20environment%20based%20on%20a%20Differential%20Evolution%20algorithm&entry.906535625=Elias%20J.%20R.%20Freitas%20and%20Miri%20Weiss%20Cohen%20and%20Frederico%20G.%20Guimar%C3%A3es%20and%20Luciano%20C.%20A.%20Pimenta&entry.1292438233=%20%20This%20research%20presents%20an%20online%20path%20planner%20for%20Unmanned%20Aerial%20Vehicles%0A%28UAVs%29%20that%20can%20handle%20dynamic%20obstacles%20and%20UAV%20motion%20constraints%2C%20including%0Amaximum%20curvature%20and%20desired%20orientations.%20Our%20proposed%20planner%20uses%20a%20NURBS%0Apath%20representation%20and%20a%20Differential%20Evolution%20algorithm%2C%20incorporating%0Aconcepts%20from%20the%20Velocity%20Obstacle%20approach%20in%20a%20constraint%20function.%20Initial%0Aresults%20show%20that%20our%20approach%20is%20feasible%20and%20provides%20a%20foundation%20for%20future%0Aextensions%20to%20three-dimensional%20%283D%29%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18777v1&entry.124074799=Read"},
{"title": "Survey of Learning-based Approaches for Robotic In-Hand Manipulation", "author": "Abraham Itzhak Weinberg and Alon Shirizly and Osher Azulay and Avishai Sintov", "abstract": "  Human dexterity is an invaluable capability for precise manipulation of\nobjects in complex tasks. The capability of robots to similarly grasp and\nperform in-hand manipulation of objects is critical for their use in the ever\nchanging human environment, and for their ability to replace manpower. In\nrecent decades, significant effort has been put in order to enable in-hand\nmanipulation capabilities to robotic systems. Initial robotic manipulators\nfollowed carefully programmed paths, while later attempts provided a solution\nbased on analytical modeling of motion and contact. However, these have failed\nto provide practical solutions due to inability to cope with complex\nenvironments and uncertainties. Therefore, the effort has shifted to\nlearning-based approaches where data is collected from the real world or\nthrough a simulation, during repeated attempts to complete various tasks. The\nvast majority of learning approaches focused on learning data-based models that\ndescribe the system to some extent or Reinforcement Learning (RL). RL, in\nparticular, has seen growing interest due to the remarkable ability to generate\nsolutions to problems with minimal human guidance. In this survey paper, we\ntrack the developments of learning approaches for in-hand manipulations and,\nexplore the challenges and opportunities. This survey is designed both as an\nintroduction for novices in the field with a glossary of terms as well as a\nguide of novel advances for advanced practitioners.\n", "link": "http://arxiv.org/abs/2401.07915v2", "date": "2024-10-24", "relevancy": 1.7422, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6227}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5724}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Survey%20of%20Learning-based%20Approaches%20for%20Robotic%20In-Hand%20Manipulation&body=Title%3A%20Survey%20of%20Learning-based%20Approaches%20for%20Robotic%20In-Hand%20Manipulation%0AAuthor%3A%20Abraham%20Itzhak%20Weinberg%20and%20Alon%20Shirizly%20and%20Osher%20Azulay%20and%20Avishai%20Sintov%0AAbstract%3A%20%20%20Human%20dexterity%20is%20an%20invaluable%20capability%20for%20precise%20manipulation%20of%0Aobjects%20in%20complex%20tasks.%20The%20capability%20of%20robots%20to%20similarly%20grasp%20and%0Aperform%20in-hand%20manipulation%20of%20objects%20is%20critical%20for%20their%20use%20in%20the%20ever%0Achanging%20human%20environment%2C%20and%20for%20their%20ability%20to%20replace%20manpower.%20In%0Arecent%20decades%2C%20significant%20effort%20has%20been%20put%20in%20order%20to%20enable%20in-hand%0Amanipulation%20capabilities%20to%20robotic%20systems.%20Initial%20robotic%20manipulators%0Afollowed%20carefully%20programmed%20paths%2C%20while%20later%20attempts%20provided%20a%20solution%0Abased%20on%20analytical%20modeling%20of%20motion%20and%20contact.%20However%2C%20these%20have%20failed%0Ato%20provide%20practical%20solutions%20due%20to%20inability%20to%20cope%20with%20complex%0Aenvironments%20and%20uncertainties.%20Therefore%2C%20the%20effort%20has%20shifted%20to%0Alearning-based%20approaches%20where%20data%20is%20collected%20from%20the%20real%20world%20or%0Athrough%20a%20simulation%2C%20during%20repeated%20attempts%20to%20complete%20various%20tasks.%20The%0Avast%20majority%20of%20learning%20approaches%20focused%20on%20learning%20data-based%20models%20that%0Adescribe%20the%20system%20to%20some%20extent%20or%20Reinforcement%20Learning%20%28RL%29.%20RL%2C%20in%0Aparticular%2C%20has%20seen%20growing%20interest%20due%20to%20the%20remarkable%20ability%20to%20generate%0Asolutions%20to%20problems%20with%20minimal%20human%20guidance.%20In%20this%20survey%20paper%2C%20we%0Atrack%20the%20developments%20of%20learning%20approaches%20for%20in-hand%20manipulations%20and%2C%0Aexplore%20the%20challenges%20and%20opportunities.%20This%20survey%20is%20designed%20both%20as%20an%0Aintroduction%20for%20novices%20in%20the%20field%20with%20a%20glossary%20of%20terms%20as%20well%20as%20a%0Aguide%20of%20novel%20advances%20for%20advanced%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvey%2520of%2520Learning-based%2520Approaches%2520for%2520Robotic%2520In-Hand%2520Manipulation%26entry.906535625%3DAbraham%2520Itzhak%2520Weinberg%2520and%2520Alon%2520Shirizly%2520and%2520Osher%2520Azulay%2520and%2520Avishai%2520Sintov%26entry.1292438233%3D%2520%2520Human%2520dexterity%2520is%2520an%2520invaluable%2520capability%2520for%2520precise%2520manipulation%2520of%250Aobjects%2520in%2520complex%2520tasks.%2520The%2520capability%2520of%2520robots%2520to%2520similarly%2520grasp%2520and%250Aperform%2520in-hand%2520manipulation%2520of%2520objects%2520is%2520critical%2520for%2520their%2520use%2520in%2520the%2520ever%250Achanging%2520human%2520environment%252C%2520and%2520for%2520their%2520ability%2520to%2520replace%2520manpower.%2520In%250Arecent%2520decades%252C%2520significant%2520effort%2520has%2520been%2520put%2520in%2520order%2520to%2520enable%2520in-hand%250Amanipulation%2520capabilities%2520to%2520robotic%2520systems.%2520Initial%2520robotic%2520manipulators%250Afollowed%2520carefully%2520programmed%2520paths%252C%2520while%2520later%2520attempts%2520provided%2520a%2520solution%250Abased%2520on%2520analytical%2520modeling%2520of%2520motion%2520and%2520contact.%2520However%252C%2520these%2520have%2520failed%250Ato%2520provide%2520practical%2520solutions%2520due%2520to%2520inability%2520to%2520cope%2520with%2520complex%250Aenvironments%2520and%2520uncertainties.%2520Therefore%252C%2520the%2520effort%2520has%2520shifted%2520to%250Alearning-based%2520approaches%2520where%2520data%2520is%2520collected%2520from%2520the%2520real%2520world%2520or%250Athrough%2520a%2520simulation%252C%2520during%2520repeated%2520attempts%2520to%2520complete%2520various%2520tasks.%2520The%250Avast%2520majority%2520of%2520learning%2520approaches%2520focused%2520on%2520learning%2520data-based%2520models%2520that%250Adescribe%2520the%2520system%2520to%2520some%2520extent%2520or%2520Reinforcement%2520Learning%2520%2528RL%2529.%2520RL%252C%2520in%250Aparticular%252C%2520has%2520seen%2520growing%2520interest%2520due%2520to%2520the%2520remarkable%2520ability%2520to%2520generate%250Asolutions%2520to%2520problems%2520with%2520minimal%2520human%2520guidance.%2520In%2520this%2520survey%2520paper%252C%2520we%250Atrack%2520the%2520developments%2520of%2520learning%2520approaches%2520for%2520in-hand%2520manipulations%2520and%252C%250Aexplore%2520the%2520challenges%2520and%2520opportunities.%2520This%2520survey%2520is%2520designed%2520both%2520as%2520an%250Aintroduction%2520for%2520novices%2520in%2520the%2520field%2520with%2520a%2520glossary%2520of%2520terms%2520as%2520well%2520as%2520a%250Aguide%2520of%2520novel%2520advances%2520for%2520advanced%2520practitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Survey%20of%20Learning-based%20Approaches%20for%20Robotic%20In-Hand%20Manipulation&entry.906535625=Abraham%20Itzhak%20Weinberg%20and%20Alon%20Shirizly%20and%20Osher%20Azulay%20and%20Avishai%20Sintov&entry.1292438233=%20%20Human%20dexterity%20is%20an%20invaluable%20capability%20for%20precise%20manipulation%20of%0Aobjects%20in%20complex%20tasks.%20The%20capability%20of%20robots%20to%20similarly%20grasp%20and%0Aperform%20in-hand%20manipulation%20of%20objects%20is%20critical%20for%20their%20use%20in%20the%20ever%0Achanging%20human%20environment%2C%20and%20for%20their%20ability%20to%20replace%20manpower.%20In%0Arecent%20decades%2C%20significant%20effort%20has%20been%20put%20in%20order%20to%20enable%20in-hand%0Amanipulation%20capabilities%20to%20robotic%20systems.%20Initial%20robotic%20manipulators%0Afollowed%20carefully%20programmed%20paths%2C%20while%20later%20attempts%20provided%20a%20solution%0Abased%20on%20analytical%20modeling%20of%20motion%20and%20contact.%20However%2C%20these%20have%20failed%0Ato%20provide%20practical%20solutions%20due%20to%20inability%20to%20cope%20with%20complex%0Aenvironments%20and%20uncertainties.%20Therefore%2C%20the%20effort%20has%20shifted%20to%0Alearning-based%20approaches%20where%20data%20is%20collected%20from%20the%20real%20world%20or%0Athrough%20a%20simulation%2C%20during%20repeated%20attempts%20to%20complete%20various%20tasks.%20The%0Avast%20majority%20of%20learning%20approaches%20focused%20on%20learning%20data-based%20models%20that%0Adescribe%20the%20system%20to%20some%20extent%20or%20Reinforcement%20Learning%20%28RL%29.%20RL%2C%20in%0Aparticular%2C%20has%20seen%20growing%20interest%20due%20to%20the%20remarkable%20ability%20to%20generate%0Asolutions%20to%20problems%20with%20minimal%20human%20guidance.%20In%20this%20survey%20paper%2C%20we%0Atrack%20the%20developments%20of%20learning%20approaches%20for%20in-hand%20manipulations%20and%2C%0Aexplore%20the%20challenges%20and%20opportunities.%20This%20survey%20is%20designed%20both%20as%20an%0Aintroduction%20for%20novices%20in%20the%20field%20with%20a%20glossary%20of%20terms%20as%20well%20as%20a%0Aguide%20of%20novel%20advances%20for%20advanced%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07915v2&entry.124074799=Read"},
{"title": "Wavetable Synthesis Using CVAE for Timbre Control Based on Semantic\n  Label", "author": "Tsugumasa Yutani and Yuya Yamamoto and Shuyo Nakatani and Hiroko Terasawa", "abstract": "  Synthesizers are essential in modern music production. However, their complex\ntimbre parameters, often filled with technical terms, require expertise. This\nresearch introduces a method of timbre control in wavetable synthesis that is\nintuitive and sensible and utilizes semantic labels. Using a conditional\nvariational autoencoder (CVAE), users can select a wavetable and define the\ntimbre with labels such as bright, warm, and rich. The CVAE model, featuring\nconvolutional and upsampling layers, effectively captures the wavetable\nnuances, ensuring real-time performance owing to their processing in the time\ndomain. Experiments demonstrate that this approach allows for real-time,\neffective control of the timbre of the wavetable using semantic inputs and aims\nfor intuitive timbre control through data-based semantic control.\n", "link": "http://arxiv.org/abs/2410.18628v1", "date": "2024-10-24", "relevancy": 1.7588, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4647}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4425}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wavetable%20Synthesis%20Using%20CVAE%20for%20Timbre%20Control%20Based%20on%20Semantic%0A%20%20Label&body=Title%3A%20Wavetable%20Synthesis%20Using%20CVAE%20for%20Timbre%20Control%20Based%20on%20Semantic%0A%20%20Label%0AAuthor%3A%20Tsugumasa%20Yutani%20and%20Yuya%20Yamamoto%20and%20Shuyo%20Nakatani%20and%20Hiroko%20Terasawa%0AAbstract%3A%20%20%20Synthesizers%20are%20essential%20in%20modern%20music%20production.%20However%2C%20their%20complex%0Atimbre%20parameters%2C%20often%20filled%20with%20technical%20terms%2C%20require%20expertise.%20This%0Aresearch%20introduces%20a%20method%20of%20timbre%20control%20in%20wavetable%20synthesis%20that%20is%0Aintuitive%20and%20sensible%20and%20utilizes%20semantic%20labels.%20Using%20a%20conditional%0Avariational%20autoencoder%20%28CVAE%29%2C%20users%20can%20select%20a%20wavetable%20and%20define%20the%0Atimbre%20with%20labels%20such%20as%20bright%2C%20warm%2C%20and%20rich.%20The%20CVAE%20model%2C%20featuring%0Aconvolutional%20and%20upsampling%20layers%2C%20effectively%20captures%20the%20wavetable%0Anuances%2C%20ensuring%20real-time%20performance%20owing%20to%20their%20processing%20in%20the%20time%0Adomain.%20Experiments%20demonstrate%20that%20this%20approach%20allows%20for%20real-time%2C%0Aeffective%20control%20of%20the%20timbre%20of%20the%20wavetable%20using%20semantic%20inputs%20and%20aims%0Afor%20intuitive%20timbre%20control%20through%20data-based%20semantic%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavetable%2520Synthesis%2520Using%2520CVAE%2520for%2520Timbre%2520Control%2520Based%2520on%2520Semantic%250A%2520%2520Label%26entry.906535625%3DTsugumasa%2520Yutani%2520and%2520Yuya%2520Yamamoto%2520and%2520Shuyo%2520Nakatani%2520and%2520Hiroko%2520Terasawa%26entry.1292438233%3D%2520%2520Synthesizers%2520are%2520essential%2520in%2520modern%2520music%2520production.%2520However%252C%2520their%2520complex%250Atimbre%2520parameters%252C%2520often%2520filled%2520with%2520technical%2520terms%252C%2520require%2520expertise.%2520This%250Aresearch%2520introduces%2520a%2520method%2520of%2520timbre%2520control%2520in%2520wavetable%2520synthesis%2520that%2520is%250Aintuitive%2520and%2520sensible%2520and%2520utilizes%2520semantic%2520labels.%2520Using%2520a%2520conditional%250Avariational%2520autoencoder%2520%2528CVAE%2529%252C%2520users%2520can%2520select%2520a%2520wavetable%2520and%2520define%2520the%250Atimbre%2520with%2520labels%2520such%2520as%2520bright%252C%2520warm%252C%2520and%2520rich.%2520The%2520CVAE%2520model%252C%2520featuring%250Aconvolutional%2520and%2520upsampling%2520layers%252C%2520effectively%2520captures%2520the%2520wavetable%250Anuances%252C%2520ensuring%2520real-time%2520performance%2520owing%2520to%2520their%2520processing%2520in%2520the%2520time%250Adomain.%2520Experiments%2520demonstrate%2520that%2520this%2520approach%2520allows%2520for%2520real-time%252C%250Aeffective%2520control%2520of%2520the%2520timbre%2520of%2520the%2520wavetable%2520using%2520semantic%2520inputs%2520and%2520aims%250Afor%2520intuitive%2520timbre%2520control%2520through%2520data-based%2520semantic%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wavetable%20Synthesis%20Using%20CVAE%20for%20Timbre%20Control%20Based%20on%20Semantic%0A%20%20Label&entry.906535625=Tsugumasa%20Yutani%20and%20Yuya%20Yamamoto%20and%20Shuyo%20Nakatani%20and%20Hiroko%20Terasawa&entry.1292438233=%20%20Synthesizers%20are%20essential%20in%20modern%20music%20production.%20However%2C%20their%20complex%0Atimbre%20parameters%2C%20often%20filled%20with%20technical%20terms%2C%20require%20expertise.%20This%0Aresearch%20introduces%20a%20method%20of%20timbre%20control%20in%20wavetable%20synthesis%20that%20is%0Aintuitive%20and%20sensible%20and%20utilizes%20semantic%20labels.%20Using%20a%20conditional%0Avariational%20autoencoder%20%28CVAE%29%2C%20users%20can%20select%20a%20wavetable%20and%20define%20the%0Atimbre%20with%20labels%20such%20as%20bright%2C%20warm%2C%20and%20rich.%20The%20CVAE%20model%2C%20featuring%0Aconvolutional%20and%20upsampling%20layers%2C%20effectively%20captures%20the%20wavetable%0Anuances%2C%20ensuring%20real-time%20performance%20owing%20to%20their%20processing%20in%20the%20time%0Adomain.%20Experiments%20demonstrate%20that%20this%20approach%20allows%20for%20real-time%2C%0Aeffective%20control%20of%20the%20timbre%20of%20the%20wavetable%20using%20semantic%20inputs%20and%20aims%0Afor%20intuitive%20timbre%20control%20through%20data-based%20semantic%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18628v1&entry.124074799=Read"},
{"title": "Scikit-fingerprints: easy and efficient computation of molecular\n  fingerprints in Python", "author": "Jakub Adamczyk and Piotr Ludynia", "abstract": "  In this work, we present \\skfp, a Python package for computation of molecular\nfingerprints for applications in chemoinformatics. Our library offers an\nindustry-standard scikit-learn interface, allowing intuitive usage and easy\nintegration with machine learning pipelines. It is also highly optimized,\nfeaturing parallel computation that enables efficient processing of large\nmolecular datasets. Currently, \\skfp~stands as the most feature-rich library in\nthe open source Python ecosystem, offering over 30 molecular fingerprints. Our\nlibrary simplifies chemoinformatics tasks based on molecular fingerprints,\nincluding molecular property prediction and virtual screening. It is also\nflexible, highly efficient, and fully open source.\n", "link": "http://arxiv.org/abs/2407.13291v3", "date": "2024-10-24", "relevancy": 1.3907, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3651}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.3462}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scikit-fingerprints%3A%20easy%20and%20efficient%20computation%20of%20molecular%0A%20%20fingerprints%20in%20Python&body=Title%3A%20Scikit-fingerprints%3A%20easy%20and%20efficient%20computation%20of%20molecular%0A%20%20fingerprints%20in%20Python%0AAuthor%3A%20Jakub%20Adamczyk%20and%20Piotr%20Ludynia%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20%5Cskfp%2C%20a%20Python%20package%20for%20computation%20of%20molecular%0Afingerprints%20for%20applications%20in%20chemoinformatics.%20Our%20library%20offers%20an%0Aindustry-standard%20scikit-learn%20interface%2C%20allowing%20intuitive%20usage%20and%20easy%0Aintegration%20with%20machine%20learning%20pipelines.%20It%20is%20also%20highly%20optimized%2C%0Afeaturing%20parallel%20computation%20that%20enables%20efficient%20processing%20of%20large%0Amolecular%20datasets.%20Currently%2C%20%5Cskfp~stands%20as%20the%20most%20feature-rich%20library%20in%0Athe%20open%20source%20Python%20ecosystem%2C%20offering%20over%2030%20molecular%20fingerprints.%20Our%0Alibrary%20simplifies%20chemoinformatics%20tasks%20based%20on%20molecular%20fingerprints%2C%0Aincluding%20molecular%20property%20prediction%20and%20virtual%20screening.%20It%20is%20also%0Aflexible%2C%20highly%20efficient%2C%20and%20fully%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13291v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScikit-fingerprints%253A%2520easy%2520and%2520efficient%2520computation%2520of%2520molecular%250A%2520%2520fingerprints%2520in%2520Python%26entry.906535625%3DJakub%2520Adamczyk%2520and%2520Piotr%2520Ludynia%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520%255Cskfp%252C%2520a%2520Python%2520package%2520for%2520computation%2520of%2520molecular%250Afingerprints%2520for%2520applications%2520in%2520chemoinformatics.%2520Our%2520library%2520offers%2520an%250Aindustry-standard%2520scikit-learn%2520interface%252C%2520allowing%2520intuitive%2520usage%2520and%2520easy%250Aintegration%2520with%2520machine%2520learning%2520pipelines.%2520It%2520is%2520also%2520highly%2520optimized%252C%250Afeaturing%2520parallel%2520computation%2520that%2520enables%2520efficient%2520processing%2520of%2520large%250Amolecular%2520datasets.%2520Currently%252C%2520%255Cskfp~stands%2520as%2520the%2520most%2520feature-rich%2520library%2520in%250Athe%2520open%2520source%2520Python%2520ecosystem%252C%2520offering%2520over%252030%2520molecular%2520fingerprints.%2520Our%250Alibrary%2520simplifies%2520chemoinformatics%2520tasks%2520based%2520on%2520molecular%2520fingerprints%252C%250Aincluding%2520molecular%2520property%2520prediction%2520and%2520virtual%2520screening.%2520It%2520is%2520also%250Aflexible%252C%2520highly%2520efficient%252C%2520and%2520fully%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13291v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scikit-fingerprints%3A%20easy%20and%20efficient%20computation%20of%20molecular%0A%20%20fingerprints%20in%20Python&entry.906535625=Jakub%20Adamczyk%20and%20Piotr%20Ludynia&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20%5Cskfp%2C%20a%20Python%20package%20for%20computation%20of%20molecular%0Afingerprints%20for%20applications%20in%20chemoinformatics.%20Our%20library%20offers%20an%0Aindustry-standard%20scikit-learn%20interface%2C%20allowing%20intuitive%20usage%20and%20easy%0Aintegration%20with%20machine%20learning%20pipelines.%20It%20is%20also%20highly%20optimized%2C%0Afeaturing%20parallel%20computation%20that%20enables%20efficient%20processing%20of%20large%0Amolecular%20datasets.%20Currently%2C%20%5Cskfp~stands%20as%20the%20most%20feature-rich%20library%20in%0Athe%20open%20source%20Python%20ecosystem%2C%20offering%20over%2030%20molecular%20fingerprints.%20Our%0Alibrary%20simplifies%20chemoinformatics%20tasks%20based%20on%20molecular%20fingerprints%2C%0Aincluding%20molecular%20property%20prediction%20and%20virtual%20screening.%20It%20is%20also%0Aflexible%2C%20highly%20efficient%2C%20and%20fully%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13291v3&entry.124074799=Read"},
{"title": "Attention-based Citywide Electric Vehicle Charging Demand Prediction\n  Approach Considering Urban Region and Dynamic Influences", "author": "Haoxuan Kuang and Kunxiang Deng and Linlin You and Jun Li", "abstract": "  Electric vehicle charging demand prediction is important for vacant charging\npile recommendation and charging infrastructure planning, thus facilitating\nvehicle electrification and green energy development. The performance of\nprevious spatio-temporal studies is still far from satisfactory because the\ntraditional graphs are difficult to model non-pairwise spatial relationships\nand multivariate temporal features are not adequately taken into account. To\ntackle these issues, we propose an attention-based heterogeneous multivariate\ndata fusion approach (AHMDF) for citywide electric vehicle charging demand\nprediction, which incorporates geo-based clustered hypergraph and multivariate\ngated Transformer to considers both static and dynamic influences. To learn\nnon-pairwise relationships, we cluster service areas by the types and numbers\nof points of interest in the areas and develop attentive hypergraph networks\naccordingly. Graph attention mechanisms are used for information propagation\nbetween neighboring areas. Additionally, we improve the Transformer encoder\nutilizing gated mechanisms so that it can selectively learn dynamic auxiliary\ninformation and temporal features. Experiments on an electric vehicle charging\nbenchmark dataset demonstrate the effectiveness of our proposed approach\ncompared with a broad range of competing baselines. Furthermore, we demonstrate\nthe impact of dynamic influences on prediction results in different areas of\nthe city and the effectiveness of our clustering method.\n", "link": "http://arxiv.org/abs/2410.18766v1", "date": "2024-10-24", "relevancy": 1.3546, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4524}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-based%20Citywide%20Electric%20Vehicle%20Charging%20Demand%20Prediction%0A%20%20Approach%20Considering%20Urban%20Region%20and%20Dynamic%20Influences&body=Title%3A%20Attention-based%20Citywide%20Electric%20Vehicle%20Charging%20Demand%20Prediction%0A%20%20Approach%20Considering%20Urban%20Region%20and%20Dynamic%20Influences%0AAuthor%3A%20Haoxuan%20Kuang%20and%20Kunxiang%20Deng%20and%20Linlin%20You%20and%20Jun%20Li%0AAbstract%3A%20%20%20Electric%20vehicle%20charging%20demand%20prediction%20is%20important%20for%20vacant%20charging%0Apile%20recommendation%20and%20charging%20infrastructure%20planning%2C%20thus%20facilitating%0Avehicle%20electrification%20and%20green%20energy%20development.%20The%20performance%20of%0Aprevious%20spatio-temporal%20studies%20is%20still%20far%20from%20satisfactory%20because%20the%0Atraditional%20graphs%20are%20difficult%20to%20model%20non-pairwise%20spatial%20relationships%0Aand%20multivariate%20temporal%20features%20are%20not%20adequately%20taken%20into%20account.%20To%0Atackle%20these%20issues%2C%20we%20propose%20an%20attention-based%20heterogeneous%20multivariate%0Adata%20fusion%20approach%20%28AHMDF%29%20for%20citywide%20electric%20vehicle%20charging%20demand%0Aprediction%2C%20which%20incorporates%20geo-based%20clustered%20hypergraph%20and%20multivariate%0Agated%20Transformer%20to%20considers%20both%20static%20and%20dynamic%20influences.%20To%20learn%0Anon-pairwise%20relationships%2C%20we%20cluster%20service%20areas%20by%20the%20types%20and%20numbers%0Aof%20points%20of%20interest%20in%20the%20areas%20and%20develop%20attentive%20hypergraph%20networks%0Aaccordingly.%20Graph%20attention%20mechanisms%20are%20used%20for%20information%20propagation%0Abetween%20neighboring%20areas.%20Additionally%2C%20we%20improve%20the%20Transformer%20encoder%0Autilizing%20gated%20mechanisms%20so%20that%20it%20can%20selectively%20learn%20dynamic%20auxiliary%0Ainformation%20and%20temporal%20features.%20Experiments%20on%20an%20electric%20vehicle%20charging%0Abenchmark%20dataset%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20approach%0Acompared%20with%20a%20broad%20range%20of%20competing%20baselines.%20Furthermore%2C%20we%20demonstrate%0Athe%20impact%20of%20dynamic%20influences%20on%20prediction%20results%20in%20different%20areas%20of%0Athe%20city%20and%20the%20effectiveness%20of%20our%20clustering%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-based%2520Citywide%2520Electric%2520Vehicle%2520Charging%2520Demand%2520Prediction%250A%2520%2520Approach%2520Considering%2520Urban%2520Region%2520and%2520Dynamic%2520Influences%26entry.906535625%3DHaoxuan%2520Kuang%2520and%2520Kunxiang%2520Deng%2520and%2520Linlin%2520You%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520Electric%2520vehicle%2520charging%2520demand%2520prediction%2520is%2520important%2520for%2520vacant%2520charging%250Apile%2520recommendation%2520and%2520charging%2520infrastructure%2520planning%252C%2520thus%2520facilitating%250Avehicle%2520electrification%2520and%2520green%2520energy%2520development.%2520The%2520performance%2520of%250Aprevious%2520spatio-temporal%2520studies%2520is%2520still%2520far%2520from%2520satisfactory%2520because%2520the%250Atraditional%2520graphs%2520are%2520difficult%2520to%2520model%2520non-pairwise%2520spatial%2520relationships%250Aand%2520multivariate%2520temporal%2520features%2520are%2520not%2520adequately%2520taken%2520into%2520account.%2520To%250Atackle%2520these%2520issues%252C%2520we%2520propose%2520an%2520attention-based%2520heterogeneous%2520multivariate%250Adata%2520fusion%2520approach%2520%2528AHMDF%2529%2520for%2520citywide%2520electric%2520vehicle%2520charging%2520demand%250Aprediction%252C%2520which%2520incorporates%2520geo-based%2520clustered%2520hypergraph%2520and%2520multivariate%250Agated%2520Transformer%2520to%2520considers%2520both%2520static%2520and%2520dynamic%2520influences.%2520To%2520learn%250Anon-pairwise%2520relationships%252C%2520we%2520cluster%2520service%2520areas%2520by%2520the%2520types%2520and%2520numbers%250Aof%2520points%2520of%2520interest%2520in%2520the%2520areas%2520and%2520develop%2520attentive%2520hypergraph%2520networks%250Aaccordingly.%2520Graph%2520attention%2520mechanisms%2520are%2520used%2520for%2520information%2520propagation%250Abetween%2520neighboring%2520areas.%2520Additionally%252C%2520we%2520improve%2520the%2520Transformer%2520encoder%250Autilizing%2520gated%2520mechanisms%2520so%2520that%2520it%2520can%2520selectively%2520learn%2520dynamic%2520auxiliary%250Ainformation%2520and%2520temporal%2520features.%2520Experiments%2520on%2520an%2520electric%2520vehicle%2520charging%250Abenchmark%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520approach%250Acompared%2520with%2520a%2520broad%2520range%2520of%2520competing%2520baselines.%2520Furthermore%252C%2520we%2520demonstrate%250Athe%2520impact%2520of%2520dynamic%2520influences%2520on%2520prediction%2520results%2520in%2520different%2520areas%2520of%250Athe%2520city%2520and%2520the%2520effectiveness%2520of%2520our%2520clustering%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-based%20Citywide%20Electric%20Vehicle%20Charging%20Demand%20Prediction%0A%20%20Approach%20Considering%20Urban%20Region%20and%20Dynamic%20Influences&entry.906535625=Haoxuan%20Kuang%20and%20Kunxiang%20Deng%20and%20Linlin%20You%20and%20Jun%20Li&entry.1292438233=%20%20Electric%20vehicle%20charging%20demand%20prediction%20is%20important%20for%20vacant%20charging%0Apile%20recommendation%20and%20charging%20infrastructure%20planning%2C%20thus%20facilitating%0Avehicle%20electrification%20and%20green%20energy%20development.%20The%20performance%20of%0Aprevious%20spatio-temporal%20studies%20is%20still%20far%20from%20satisfactory%20because%20the%0Atraditional%20graphs%20are%20difficult%20to%20model%20non-pairwise%20spatial%20relationships%0Aand%20multivariate%20temporal%20features%20are%20not%20adequately%20taken%20into%20account.%20To%0Atackle%20these%20issues%2C%20we%20propose%20an%20attention-based%20heterogeneous%20multivariate%0Adata%20fusion%20approach%20%28AHMDF%29%20for%20citywide%20electric%20vehicle%20charging%20demand%0Aprediction%2C%20which%20incorporates%20geo-based%20clustered%20hypergraph%20and%20multivariate%0Agated%20Transformer%20to%20considers%20both%20static%20and%20dynamic%20influences.%20To%20learn%0Anon-pairwise%20relationships%2C%20we%20cluster%20service%20areas%20by%20the%20types%20and%20numbers%0Aof%20points%20of%20interest%20in%20the%20areas%20and%20develop%20attentive%20hypergraph%20networks%0Aaccordingly.%20Graph%20attention%20mechanisms%20are%20used%20for%20information%20propagation%0Abetween%20neighboring%20areas.%20Additionally%2C%20we%20improve%20the%20Transformer%20encoder%0Autilizing%20gated%20mechanisms%20so%20that%20it%20can%20selectively%20learn%20dynamic%20auxiliary%0Ainformation%20and%20temporal%20features.%20Experiments%20on%20an%20electric%20vehicle%20charging%0Abenchmark%20dataset%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20approach%0Acompared%20with%20a%20broad%20range%20of%20competing%20baselines.%20Furthermore%2C%20we%20demonstrate%0Athe%20impact%20of%20dynamic%20influences%20on%20prediction%20results%20in%20different%20areas%20of%0Athe%20city%20and%20the%20effectiveness%20of%20our%20clustering%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18766v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


