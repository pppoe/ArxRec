<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250113.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance", "author": "Dimitrios Gerogiannis and Foivos Paraperas Papantoniou and Rolandos Alexandros Potamias and Alexandros Lattas and Stefanos Zafeiriou", "abstract": "  Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail. Please visit https://arc2avatar.github.io for more\nresources.\n", "link": "http://arxiv.org/abs/2501.05379v2", "date": "2025-01-13", "relevancy": 3.4589, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7187}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7187}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arc2Avatar%3A%20Generating%20Expressive%203D%20Avatars%20from%20a%20Single%20Image%20via%20ID%0A%20%20Guidance&body=Title%3A%20Arc2Avatar%3A%20Generating%20Expressive%203D%20Avatars%20from%20a%20Single%20Image%20via%20ID%0A%20%20Guidance%0AAuthor%3A%20Dimitrios%20Gerogiannis%20and%20Foivos%20Paraperas%20Papantoniou%20and%20Rolandos%20Alexandros%20Potamias%20and%20Alexandros%20Lattas%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20Inspired%20by%20the%20effectiveness%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%0Areconstructing%20detailed%203D%20scenes%20within%20multi-view%20setups%20and%20the%20emergence%20of%0Alarge%202D%20human%20foundation%20models%2C%20we%20introduce%20Arc2Avatar%2C%20the%20first%20SDS-based%0Amethod%20utilizing%20a%20human%20face%20foundation%20model%20as%20guidance%20with%20just%20a%20single%0Aimage%20as%20input.%20To%20achieve%20that%2C%20we%20extend%20such%20a%20model%20for%20diverse-view%20human%0Ahead%20generation%20by%20fine-tuning%20on%20synthetic%20data%20and%20modifying%20its%0Aconditioning.%20Our%20avatars%20maintain%20a%20dense%20correspondence%20with%20a%20human%20face%0Amesh%20template%2C%20allowing%20blendshape-based%20expression%20generation.%20This%20is%0Aachieved%20through%20a%20modified%203DGS%20approach%2C%20connectivity%20regularizers%2C%20and%20a%0Astrategic%20initialization%20tailored%20for%20our%20task.%20Additionally%2C%20we%20propose%20an%0Aoptional%20efficient%20SDS-based%20correction%20step%20to%20refine%20the%20blendshape%0Aexpressions%2C%20enhancing%20realism%20and%20diversity.%20Experiments%20demonstrate%20that%0AArc2Avatar%20achieves%20state-of-the-art%20realism%20and%20identity%20preservation%2C%0Aeffectively%20addressing%20color%20issues%20by%20allowing%20the%20use%20of%20very%20low%20guidance%2C%0Aenabled%20by%20our%20strong%20identity%20prior%20and%20initialization%20strategy%2C%20without%0Acompromising%20detail.%20Please%20visit%20https%3A//arc2avatar.github.io%20for%20more%0Aresources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArc2Avatar%253A%2520Generating%2520Expressive%25203D%2520Avatars%2520from%2520a%2520Single%2520Image%2520via%2520ID%250A%2520%2520Guidance%26entry.906535625%3DDimitrios%2520Gerogiannis%2520and%2520Foivos%2520Paraperas%2520Papantoniou%2520and%2520Rolandos%2520Alexandros%2520Potamias%2520and%2520Alexandros%2520Lattas%2520and%2520Stefanos%2520Zafeiriou%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520effectiveness%2520of%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520in%250Areconstructing%2520detailed%25203D%2520scenes%2520within%2520multi-view%2520setups%2520and%2520the%2520emergence%2520of%250Alarge%25202D%2520human%2520foundation%2520models%252C%2520we%2520introduce%2520Arc2Avatar%252C%2520the%2520first%2520SDS-based%250Amethod%2520utilizing%2520a%2520human%2520face%2520foundation%2520model%2520as%2520guidance%2520with%2520just%2520a%2520single%250Aimage%2520as%2520input.%2520To%2520achieve%2520that%252C%2520we%2520extend%2520such%2520a%2520model%2520for%2520diverse-view%2520human%250Ahead%2520generation%2520by%2520fine-tuning%2520on%2520synthetic%2520data%2520and%2520modifying%2520its%250Aconditioning.%2520Our%2520avatars%2520maintain%2520a%2520dense%2520correspondence%2520with%2520a%2520human%2520face%250Amesh%2520template%252C%2520allowing%2520blendshape-based%2520expression%2520generation.%2520This%2520is%250Aachieved%2520through%2520a%2520modified%25203DGS%2520approach%252C%2520connectivity%2520regularizers%252C%2520and%2520a%250Astrategic%2520initialization%2520tailored%2520for%2520our%2520task.%2520Additionally%252C%2520we%2520propose%2520an%250Aoptional%2520efficient%2520SDS-based%2520correction%2520step%2520to%2520refine%2520the%2520blendshape%250Aexpressions%252C%2520enhancing%2520realism%2520and%2520diversity.%2520Experiments%2520demonstrate%2520that%250AArc2Avatar%2520achieves%2520state-of-the-art%2520realism%2520and%2520identity%2520preservation%252C%250Aeffectively%2520addressing%2520color%2520issues%2520by%2520allowing%2520the%2520use%2520of%2520very%2520low%2520guidance%252C%250Aenabled%2520by%2520our%2520strong%2520identity%2520prior%2520and%2520initialization%2520strategy%252C%2520without%250Acompromising%2520detail.%2520Please%2520visit%2520https%253A//arc2avatar.github.io%2520for%2520more%250Aresources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arc2Avatar%3A%20Generating%20Expressive%203D%20Avatars%20from%20a%20Single%20Image%20via%20ID%0A%20%20Guidance&entry.906535625=Dimitrios%20Gerogiannis%20and%20Foivos%20Paraperas%20Papantoniou%20and%20Rolandos%20Alexandros%20Potamias%20and%20Alexandros%20Lattas%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20Inspired%20by%20the%20effectiveness%20of%203D%20Gaussian%20Splatting%20%283DGS%29%20in%0Areconstructing%20detailed%203D%20scenes%20within%20multi-view%20setups%20and%20the%20emergence%20of%0Alarge%202D%20human%20foundation%20models%2C%20we%20introduce%20Arc2Avatar%2C%20the%20first%20SDS-based%0Amethod%20utilizing%20a%20human%20face%20foundation%20model%20as%20guidance%20with%20just%20a%20single%0Aimage%20as%20input.%20To%20achieve%20that%2C%20we%20extend%20such%20a%20model%20for%20diverse-view%20human%0Ahead%20generation%20by%20fine-tuning%20on%20synthetic%20data%20and%20modifying%20its%0Aconditioning.%20Our%20avatars%20maintain%20a%20dense%20correspondence%20with%20a%20human%20face%0Amesh%20template%2C%20allowing%20blendshape-based%20expression%20generation.%20This%20is%0Aachieved%20through%20a%20modified%203DGS%20approach%2C%20connectivity%20regularizers%2C%20and%20a%0Astrategic%20initialization%20tailored%20for%20our%20task.%20Additionally%2C%20we%20propose%20an%0Aoptional%20efficient%20SDS-based%20correction%20step%20to%20refine%20the%20blendshape%0Aexpressions%2C%20enhancing%20realism%20and%20diversity.%20Experiments%20demonstrate%20that%0AArc2Avatar%20achieves%20state-of-the-art%20realism%20and%20identity%20preservation%2C%0Aeffectively%20addressing%20color%20issues%20by%20allowing%20the%20use%20of%20very%20low%20guidance%2C%0Aenabled%20by%20our%20strong%20identity%20prior%20and%20initialization%20strategy%2C%20without%0Acompromising%20detail.%20Please%20visit%20https%3A//arc2avatar.github.io%20for%20more%0Aresources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05379v2&entry.124074799=Read"},
{"title": "3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point\n  Cloud or Mesh", "author": "Lewis A G Stuart and Michael P Pound", "abstract": "  3D Gaussian Splatting (3DGS) excels at producing highly detailed 3D\nreconstructions, but these scenes often require specialised renderers for\neffective visualisation. In contrast, point clouds are a widely used 3D\nrepresentation and are compatible with most popular 3D processing software, yet\nconverting 3DGS scenes into point clouds is a complex challenge. In this work\nwe introduce 3DGS-to-PC, a flexible and highly customisable framework that is\ncapable of transforming 3DGS scenes into dense, high-accuracy point clouds. We\nsample points probabilistically from each Gaussian as a 3D density function. We\nadditionally threshold new points using the Mahalanobis distance to the\nGaussian centre, preventing extreme outliers. The result is a point cloud that\nclosely represents the shape encoded into the 3D Gaussian scene. Individual\nGaussians use spherical harmonics to adapt colours depending on view, and each\npoint may contribute only subtle colour hints to the resulting rendered scene.\nTo avoid spurious or incorrect colours that do not fit with the final point\ncloud, we recalculate Gaussian colours via a customised image rendering\napproach, assigning each Gaussian the colour of the pixel to which it\ncontributes most across all views. 3DGS-to-PC also supports mesh generation\nthrough Poisson Surface Reconstruction, applied to points sampled from\npredicted surface Gaussians. This allows coloured meshes to be generated from\n3DGS scenes without the need for re-training. This package is highly\ncustomisable and capability of simple integration into existing 3DGS pipelines.\n3DGS-to-PC provides a powerful tool for converting 3DGS data into point cloud\nand surface-based formats.\n", "link": "http://arxiv.org/abs/2501.07478v1", "date": "2025-01-13", "relevancy": 3.2741, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6738}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGS-to-PC%3A%20Convert%20a%203D%20Gaussian%20Splatting%20Scene%20into%20a%20Dense%20Point%0A%20%20Cloud%20or%20Mesh&body=Title%3A%203DGS-to-PC%3A%20Convert%20a%203D%20Gaussian%20Splatting%20Scene%20into%20a%20Dense%20Point%0A%20%20Cloud%20or%20Mesh%0AAuthor%3A%20Lewis%20A%20G%20Stuart%20and%20Michael%20P%20Pound%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20excels%20at%20producing%20highly%20detailed%203D%0Areconstructions%2C%20but%20these%20scenes%20often%20require%20specialised%20renderers%20for%0Aeffective%20visualisation.%20In%20contrast%2C%20point%20clouds%20are%20a%20widely%20used%203D%0Arepresentation%20and%20are%20compatible%20with%20most%20popular%203D%20processing%20software%2C%20yet%0Aconverting%203DGS%20scenes%20into%20point%20clouds%20is%20a%20complex%20challenge.%20In%20this%20work%0Awe%20introduce%203DGS-to-PC%2C%20a%20flexible%20and%20highly%20customisable%20framework%20that%20is%0Acapable%20of%20transforming%203DGS%20scenes%20into%20dense%2C%20high-accuracy%20point%20clouds.%20We%0Asample%20points%20probabilistically%20from%20each%20Gaussian%20as%20a%203D%20density%20function.%20We%0Aadditionally%20threshold%20new%20points%20using%20the%20Mahalanobis%20distance%20to%20the%0AGaussian%20centre%2C%20preventing%20extreme%20outliers.%20The%20result%20is%20a%20point%20cloud%20that%0Aclosely%20represents%20the%20shape%20encoded%20into%20the%203D%20Gaussian%20scene.%20Individual%0AGaussians%20use%20spherical%20harmonics%20to%20adapt%20colours%20depending%20on%20view%2C%20and%20each%0Apoint%20may%20contribute%20only%20subtle%20colour%20hints%20to%20the%20resulting%20rendered%20scene.%0ATo%20avoid%20spurious%20or%20incorrect%20colours%20that%20do%20not%20fit%20with%20the%20final%20point%0Acloud%2C%20we%20recalculate%20Gaussian%20colours%20via%20a%20customised%20image%20rendering%0Aapproach%2C%20assigning%20each%20Gaussian%20the%20colour%20of%20the%20pixel%20to%20which%20it%0Acontributes%20most%20across%20all%20views.%203DGS-to-PC%20also%20supports%20mesh%20generation%0Athrough%20Poisson%20Surface%20Reconstruction%2C%20applied%20to%20points%20sampled%20from%0Apredicted%20surface%20Gaussians.%20This%20allows%20coloured%20meshes%20to%20be%20generated%20from%0A3DGS%20scenes%20without%20the%20need%20for%20re-training.%20This%20package%20is%20highly%0Acustomisable%20and%20capability%20of%20simple%20integration%20into%20existing%203DGS%20pipelines.%0A3DGS-to-PC%20provides%20a%20powerful%20tool%20for%20converting%203DGS%20data%20into%20point%20cloud%0Aand%20surface-based%20formats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGS-to-PC%253A%2520Convert%2520a%25203D%2520Gaussian%2520Splatting%2520Scene%2520into%2520a%2520Dense%2520Point%250A%2520%2520Cloud%2520or%2520Mesh%26entry.906535625%3DLewis%2520A%2520G%2520Stuart%2520and%2520Michael%2520P%2520Pound%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520excels%2520at%2520producing%2520highly%2520detailed%25203D%250Areconstructions%252C%2520but%2520these%2520scenes%2520often%2520require%2520specialised%2520renderers%2520for%250Aeffective%2520visualisation.%2520In%2520contrast%252C%2520point%2520clouds%2520are%2520a%2520widely%2520used%25203D%250Arepresentation%2520and%2520are%2520compatible%2520with%2520most%2520popular%25203D%2520processing%2520software%252C%2520yet%250Aconverting%25203DGS%2520scenes%2520into%2520point%2520clouds%2520is%2520a%2520complex%2520challenge.%2520In%2520this%2520work%250Awe%2520introduce%25203DGS-to-PC%252C%2520a%2520flexible%2520and%2520highly%2520customisable%2520framework%2520that%2520is%250Acapable%2520of%2520transforming%25203DGS%2520scenes%2520into%2520dense%252C%2520high-accuracy%2520point%2520clouds.%2520We%250Asample%2520points%2520probabilistically%2520from%2520each%2520Gaussian%2520as%2520a%25203D%2520density%2520function.%2520We%250Aadditionally%2520threshold%2520new%2520points%2520using%2520the%2520Mahalanobis%2520distance%2520to%2520the%250AGaussian%2520centre%252C%2520preventing%2520extreme%2520outliers.%2520The%2520result%2520is%2520a%2520point%2520cloud%2520that%250Aclosely%2520represents%2520the%2520shape%2520encoded%2520into%2520the%25203D%2520Gaussian%2520scene.%2520Individual%250AGaussians%2520use%2520spherical%2520harmonics%2520to%2520adapt%2520colours%2520depending%2520on%2520view%252C%2520and%2520each%250Apoint%2520may%2520contribute%2520only%2520subtle%2520colour%2520hints%2520to%2520the%2520resulting%2520rendered%2520scene.%250ATo%2520avoid%2520spurious%2520or%2520incorrect%2520colours%2520that%2520do%2520not%2520fit%2520with%2520the%2520final%2520point%250Acloud%252C%2520we%2520recalculate%2520Gaussian%2520colours%2520via%2520a%2520customised%2520image%2520rendering%250Aapproach%252C%2520assigning%2520each%2520Gaussian%2520the%2520colour%2520of%2520the%2520pixel%2520to%2520which%2520it%250Acontributes%2520most%2520across%2520all%2520views.%25203DGS-to-PC%2520also%2520supports%2520mesh%2520generation%250Athrough%2520Poisson%2520Surface%2520Reconstruction%252C%2520applied%2520to%2520points%2520sampled%2520from%250Apredicted%2520surface%2520Gaussians.%2520This%2520allows%2520coloured%2520meshes%2520to%2520be%2520generated%2520from%250A3DGS%2520scenes%2520without%2520the%2520need%2520for%2520re-training.%2520This%2520package%2520is%2520highly%250Acustomisable%2520and%2520capability%2520of%2520simple%2520integration%2520into%2520existing%25203DGS%2520pipelines.%250A3DGS-to-PC%2520provides%2520a%2520powerful%2520tool%2520for%2520converting%25203DGS%2520data%2520into%2520point%2520cloud%250Aand%2520surface-based%2520formats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGS-to-PC%3A%20Convert%20a%203D%20Gaussian%20Splatting%20Scene%20into%20a%20Dense%20Point%0A%20%20Cloud%20or%20Mesh&entry.906535625=Lewis%20A%20G%20Stuart%20and%20Michael%20P%20Pound&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20excels%20at%20producing%20highly%20detailed%203D%0Areconstructions%2C%20but%20these%20scenes%20often%20require%20specialised%20renderers%20for%0Aeffective%20visualisation.%20In%20contrast%2C%20point%20clouds%20are%20a%20widely%20used%203D%0Arepresentation%20and%20are%20compatible%20with%20most%20popular%203D%20processing%20software%2C%20yet%0Aconverting%203DGS%20scenes%20into%20point%20clouds%20is%20a%20complex%20challenge.%20In%20this%20work%0Awe%20introduce%203DGS-to-PC%2C%20a%20flexible%20and%20highly%20customisable%20framework%20that%20is%0Acapable%20of%20transforming%203DGS%20scenes%20into%20dense%2C%20high-accuracy%20point%20clouds.%20We%0Asample%20points%20probabilistically%20from%20each%20Gaussian%20as%20a%203D%20density%20function.%20We%0Aadditionally%20threshold%20new%20points%20using%20the%20Mahalanobis%20distance%20to%20the%0AGaussian%20centre%2C%20preventing%20extreme%20outliers.%20The%20result%20is%20a%20point%20cloud%20that%0Aclosely%20represents%20the%20shape%20encoded%20into%20the%203D%20Gaussian%20scene.%20Individual%0AGaussians%20use%20spherical%20harmonics%20to%20adapt%20colours%20depending%20on%20view%2C%20and%20each%0Apoint%20may%20contribute%20only%20subtle%20colour%20hints%20to%20the%20resulting%20rendered%20scene.%0ATo%20avoid%20spurious%20or%20incorrect%20colours%20that%20do%20not%20fit%20with%20the%20final%20point%0Acloud%2C%20we%20recalculate%20Gaussian%20colours%20via%20a%20customised%20image%20rendering%0Aapproach%2C%20assigning%20each%20Gaussian%20the%20colour%20of%20the%20pixel%20to%20which%20it%0Acontributes%20most%20across%20all%20views.%203DGS-to-PC%20also%20supports%20mesh%20generation%0Athrough%20Poisson%20Surface%20Reconstruction%2C%20applied%20to%20points%20sampled%20from%0Apredicted%20surface%20Gaussians.%20This%20allows%20coloured%20meshes%20to%20be%20generated%20from%0A3DGS%20scenes%20without%20the%20need%20for%20re-training.%20This%20package%20is%20highly%0Acustomisable%20and%20capability%20of%20simple%20integration%20into%20existing%203DGS%20pipelines.%0A3DGS-to-PC%20provides%20a%20powerful%20tool%20for%20converting%203DGS%20data%20into%20point%20cloud%0Aand%20surface-based%20formats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07478v1&entry.124074799=Read"},
{"title": "CMAR-Net: Accurate Cross-Modal 3D SAR Reconstruction of Vehicle Targets\n  with Sparse Multi-Baseline Data", "author": "Da Li and Guoqiang Zhao and Houjun Sun and Jiacheng Bao", "abstract": "  Multi-baseline Synthetic Aperture Radar (SAR) three-dimensional (3D)\ntomography is a crucial remote sensing technique that provides 3D resolution\nunavailable in conventional SAR imaging. However, achieving high-quality\nimaging typically requires multi-angle or full-aperture data, resulting in\nsignificant imaging costs. Recent advancements in sparse 3D SAR, which rely on\ndata from limited apertures, have gained attention as a cost-effective\nalternative. Notably, deep learning techniques have markedly enhanced the\nimaging quality of sparse 3D SAR. Despite these advancements, existing methods\nprimarily depend on high-resolution radar images for supervising the training\nof deep neural networks (DNNs). This exclusive dependence on single-modal data\nprevents the introduction of complementary information from other data sources,\nlimiting further improvements in imaging performance. In this paper, we\nintroduce a Cross-Modal 3D-SAR Reconstruction Network (CMAR-Net) to enhance 3D\nSAR imaging by integrating heterogeneous information. Leveraging cross-modal\nsupervision from 2D optical images and error transfer guaranteed by\ndifferentiable rendering, CMAR-Net achieves efficient training and reconstructs\nhighly sparse multi-baseline SAR data into visually structured and accurate 3D\nimages, particularly for vehicle targets. Extensive experiments on simulated\nand real-world datasets demonstrate that CMAR-Net significantly outperforms\nSOTA sparse reconstruction algorithms based on compressed sensing (CS) and deep\nlearning (DL). Furthermore, our method eliminates the need for time-consuming\nfull-aperture data preprocessing and relies solely on computer-rendered optical\nimages, significantly reducing dataset construction costs. This work highlights\nthe potential of deep learning for multi-baseline SAR 3D imaging and introduces\na novel framework for radar imaging research through cross-modal learning.\n", "link": "http://arxiv.org/abs/2406.04158v3", "date": "2025-01-13", "relevancy": 3.0119, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6061}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6005}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMAR-Net%3A%20Accurate%20Cross-Modal%203D%20SAR%20Reconstruction%20of%20Vehicle%20Targets%0A%20%20with%20Sparse%20Multi-Baseline%20Data&body=Title%3A%20CMAR-Net%3A%20Accurate%20Cross-Modal%203D%20SAR%20Reconstruction%20of%20Vehicle%20Targets%0A%20%20with%20Sparse%20Multi-Baseline%20Data%0AAuthor%3A%20Da%20Li%20and%20Guoqiang%20Zhao%20and%20Houjun%20Sun%20and%20Jiacheng%20Bao%0AAbstract%3A%20%20%20Multi-baseline%20Synthetic%20Aperture%20Radar%20%28SAR%29%20three-dimensional%20%283D%29%0Atomography%20is%20a%20crucial%20remote%20sensing%20technique%20that%20provides%203D%20resolution%0Aunavailable%20in%20conventional%20SAR%20imaging.%20However%2C%20achieving%20high-quality%0Aimaging%20typically%20requires%20multi-angle%20or%20full-aperture%20data%2C%20resulting%20in%0Asignificant%20imaging%20costs.%20Recent%20advancements%20in%20sparse%203D%20SAR%2C%20which%20rely%20on%0Adata%20from%20limited%20apertures%2C%20have%20gained%20attention%20as%20a%20cost-effective%0Aalternative.%20Notably%2C%20deep%20learning%20techniques%20have%20markedly%20enhanced%20the%0Aimaging%20quality%20of%20sparse%203D%20SAR.%20Despite%20these%20advancements%2C%20existing%20methods%0Aprimarily%20depend%20on%20high-resolution%20radar%20images%20for%20supervising%20the%20training%0Aof%20deep%20neural%20networks%20%28DNNs%29.%20This%20exclusive%20dependence%20on%20single-modal%20data%0Aprevents%20the%20introduction%20of%20complementary%20information%20from%20other%20data%20sources%2C%0Alimiting%20further%20improvements%20in%20imaging%20performance.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20Cross-Modal%203D-SAR%20Reconstruction%20Network%20%28CMAR-Net%29%20to%20enhance%203D%0ASAR%20imaging%20by%20integrating%20heterogeneous%20information.%20Leveraging%20cross-modal%0Asupervision%20from%202D%20optical%20images%20and%20error%20transfer%20guaranteed%20by%0Adifferentiable%20rendering%2C%20CMAR-Net%20achieves%20efficient%20training%20and%20reconstructs%0Ahighly%20sparse%20multi-baseline%20SAR%20data%20into%20visually%20structured%20and%20accurate%203D%0Aimages%2C%20particularly%20for%20vehicle%20targets.%20Extensive%20experiments%20on%20simulated%0Aand%20real-world%20datasets%20demonstrate%20that%20CMAR-Net%20significantly%20outperforms%0ASOTA%20sparse%20reconstruction%20algorithms%20based%20on%20compressed%20sensing%20%28CS%29%20and%20deep%0Alearning%20%28DL%29.%20Furthermore%2C%20our%20method%20eliminates%20the%20need%20for%20time-consuming%0Afull-aperture%20data%20preprocessing%20and%20relies%20solely%20on%20computer-rendered%20optical%0Aimages%2C%20significantly%20reducing%20dataset%20construction%20costs.%20This%20work%20highlights%0Athe%20potential%20of%20deep%20learning%20for%20multi-baseline%20SAR%203D%20imaging%20and%20introduces%0Aa%20novel%20framework%20for%20radar%20imaging%20research%20through%20cross-modal%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04158v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMAR-Net%253A%2520Accurate%2520Cross-Modal%25203D%2520SAR%2520Reconstruction%2520of%2520Vehicle%2520Targets%250A%2520%2520with%2520Sparse%2520Multi-Baseline%2520Data%26entry.906535625%3DDa%2520Li%2520and%2520Guoqiang%2520Zhao%2520and%2520Houjun%2520Sun%2520and%2520Jiacheng%2520Bao%26entry.1292438233%3D%2520%2520Multi-baseline%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520three-dimensional%2520%25283D%2529%250Atomography%2520is%2520a%2520crucial%2520remote%2520sensing%2520technique%2520that%2520provides%25203D%2520resolution%250Aunavailable%2520in%2520conventional%2520SAR%2520imaging.%2520However%252C%2520achieving%2520high-quality%250Aimaging%2520typically%2520requires%2520multi-angle%2520or%2520full-aperture%2520data%252C%2520resulting%2520in%250Asignificant%2520imaging%2520costs.%2520Recent%2520advancements%2520in%2520sparse%25203D%2520SAR%252C%2520which%2520rely%2520on%250Adata%2520from%2520limited%2520apertures%252C%2520have%2520gained%2520attention%2520as%2520a%2520cost-effective%250Aalternative.%2520Notably%252C%2520deep%2520learning%2520techniques%2520have%2520markedly%2520enhanced%2520the%250Aimaging%2520quality%2520of%2520sparse%25203D%2520SAR.%2520Despite%2520these%2520advancements%252C%2520existing%2520methods%250Aprimarily%2520depend%2520on%2520high-resolution%2520radar%2520images%2520for%2520supervising%2520the%2520training%250Aof%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520This%2520exclusive%2520dependence%2520on%2520single-modal%2520data%250Aprevents%2520the%2520introduction%2520of%2520complementary%2520information%2520from%2520other%2520data%2520sources%252C%250Alimiting%2520further%2520improvements%2520in%2520imaging%2520performance.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520Cross-Modal%25203D-SAR%2520Reconstruction%2520Network%2520%2528CMAR-Net%2529%2520to%2520enhance%25203D%250ASAR%2520imaging%2520by%2520integrating%2520heterogeneous%2520information.%2520Leveraging%2520cross-modal%250Asupervision%2520from%25202D%2520optical%2520images%2520and%2520error%2520transfer%2520guaranteed%2520by%250Adifferentiable%2520rendering%252C%2520CMAR-Net%2520achieves%2520efficient%2520training%2520and%2520reconstructs%250Ahighly%2520sparse%2520multi-baseline%2520SAR%2520data%2520into%2520visually%2520structured%2520and%2520accurate%25203D%250Aimages%252C%2520particularly%2520for%2520vehicle%2520targets.%2520Extensive%2520experiments%2520on%2520simulated%250Aand%2520real-world%2520datasets%2520demonstrate%2520that%2520CMAR-Net%2520significantly%2520outperforms%250ASOTA%2520sparse%2520reconstruction%2520algorithms%2520based%2520on%2520compressed%2520sensing%2520%2528CS%2529%2520and%2520deep%250Alearning%2520%2528DL%2529.%2520Furthermore%252C%2520our%2520method%2520eliminates%2520the%2520need%2520for%2520time-consuming%250Afull-aperture%2520data%2520preprocessing%2520and%2520relies%2520solely%2520on%2520computer-rendered%2520optical%250Aimages%252C%2520significantly%2520reducing%2520dataset%2520construction%2520costs.%2520This%2520work%2520highlights%250Athe%2520potential%2520of%2520deep%2520learning%2520for%2520multi-baseline%2520SAR%25203D%2520imaging%2520and%2520introduces%250Aa%2520novel%2520framework%2520for%2520radar%2520imaging%2520research%2520through%2520cross-modal%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04158v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMAR-Net%3A%20Accurate%20Cross-Modal%203D%20SAR%20Reconstruction%20of%20Vehicle%20Targets%0A%20%20with%20Sparse%20Multi-Baseline%20Data&entry.906535625=Da%20Li%20and%20Guoqiang%20Zhao%20and%20Houjun%20Sun%20and%20Jiacheng%20Bao&entry.1292438233=%20%20Multi-baseline%20Synthetic%20Aperture%20Radar%20%28SAR%29%20three-dimensional%20%283D%29%0Atomography%20is%20a%20crucial%20remote%20sensing%20technique%20that%20provides%203D%20resolution%0Aunavailable%20in%20conventional%20SAR%20imaging.%20However%2C%20achieving%20high-quality%0Aimaging%20typically%20requires%20multi-angle%20or%20full-aperture%20data%2C%20resulting%20in%0Asignificant%20imaging%20costs.%20Recent%20advancements%20in%20sparse%203D%20SAR%2C%20which%20rely%20on%0Adata%20from%20limited%20apertures%2C%20have%20gained%20attention%20as%20a%20cost-effective%0Aalternative.%20Notably%2C%20deep%20learning%20techniques%20have%20markedly%20enhanced%20the%0Aimaging%20quality%20of%20sparse%203D%20SAR.%20Despite%20these%20advancements%2C%20existing%20methods%0Aprimarily%20depend%20on%20high-resolution%20radar%20images%20for%20supervising%20the%20training%0Aof%20deep%20neural%20networks%20%28DNNs%29.%20This%20exclusive%20dependence%20on%20single-modal%20data%0Aprevents%20the%20introduction%20of%20complementary%20information%20from%20other%20data%20sources%2C%0Alimiting%20further%20improvements%20in%20imaging%20performance.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20Cross-Modal%203D-SAR%20Reconstruction%20Network%20%28CMAR-Net%29%20to%20enhance%203D%0ASAR%20imaging%20by%20integrating%20heterogeneous%20information.%20Leveraging%20cross-modal%0Asupervision%20from%202D%20optical%20images%20and%20error%20transfer%20guaranteed%20by%0Adifferentiable%20rendering%2C%20CMAR-Net%20achieves%20efficient%20training%20and%20reconstructs%0Ahighly%20sparse%20multi-baseline%20SAR%20data%20into%20visually%20structured%20and%20accurate%203D%0Aimages%2C%20particularly%20for%20vehicle%20targets.%20Extensive%20experiments%20on%20simulated%0Aand%20real-world%20datasets%20demonstrate%20that%20CMAR-Net%20significantly%20outperforms%0ASOTA%20sparse%20reconstruction%20algorithms%20based%20on%20compressed%20sensing%20%28CS%29%20and%20deep%0Alearning%20%28DL%29.%20Furthermore%2C%20our%20method%20eliminates%20the%20need%20for%20time-consuming%0Afull-aperture%20data%20preprocessing%20and%20relies%20solely%20on%20computer-rendered%20optical%0Aimages%2C%20significantly%20reducing%20dataset%20construction%20costs.%20This%20work%20highlights%0Athe%20potential%20of%20deep%20learning%20for%20multi-baseline%20SAR%203D%20imaging%20and%20introduces%0Aa%20novel%20framework%20for%20radar%20imaging%20research%20through%20cross-modal%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04158v3&entry.124074799=Read"},
{"title": "Point-JEPA: A Joint Embedding Predictive Architecture for\n  Self-Supervised Learning on Point Cloud", "author": "Ayumu Saito and Prachi Kudeshia and Jiju Poovvancheri", "abstract": "  Recent advancements in self-supervised learning in the point cloud domain\nhave demonstrated significant potential. However, these methods often suffer\nfrom drawbacks, including lengthy pre-training time, the necessity of\nreconstruction in the input space, or the necessity of additional modalities.\nIn order to address these issues, we introduce Point-JEPA, a joint embedding\npredictive architecture designed specifically for point cloud data. To this\nend, we introduce a sequencer that orders point cloud patch embeddings to\nefficiently compute and utilize their proximity based on the indices during\ntarget and context selection. The sequencer also allows shared computations of\nthe patch embeddings' proximity between context and target selection, further\nimproving the efficiency. Experimentally, our method achieves competitive\nresults with state-of-the-art methods while avoiding the reconstruction in the\ninput space or additional modality.\n", "link": "http://arxiv.org/abs/2404.16432v5", "date": "2025-01-13", "relevancy": 2.9712, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6619}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5705}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point-JEPA%3A%20A%20Joint%20Embedding%20Predictive%20Architecture%20for%0A%20%20Self-Supervised%20Learning%20on%20Point%20Cloud&body=Title%3A%20Point-JEPA%3A%20A%20Joint%20Embedding%20Predictive%20Architecture%20for%0A%20%20Self-Supervised%20Learning%20on%20Point%20Cloud%0AAuthor%3A%20Ayumu%20Saito%20and%20Prachi%20Kudeshia%20and%20Jiju%20Poovvancheri%0AAbstract%3A%20%20%20Recent%20advancements%20in%20self-supervised%20learning%20in%20the%20point%20cloud%20domain%0Ahave%20demonstrated%20significant%20potential.%20However%2C%20these%20methods%20often%20suffer%0Afrom%20drawbacks%2C%20including%20lengthy%20pre-training%20time%2C%20the%20necessity%20of%0Areconstruction%20in%20the%20input%20space%2C%20or%20the%20necessity%20of%20additional%20modalities.%0AIn%20order%20to%20address%20these%20issues%2C%20we%20introduce%20Point-JEPA%2C%20a%20joint%20embedding%0Apredictive%20architecture%20designed%20specifically%20for%20point%20cloud%20data.%20To%20this%0Aend%2C%20we%20introduce%20a%20sequencer%20that%20orders%20point%20cloud%20patch%20embeddings%20to%0Aefficiently%20compute%20and%20utilize%20their%20proximity%20based%20on%20the%20indices%20during%0Atarget%20and%20context%20selection.%20The%20sequencer%20also%20allows%20shared%20computations%20of%0Athe%20patch%20embeddings%27%20proximity%20between%20context%20and%20target%20selection%2C%20further%0Aimproving%20the%20efficiency.%20Experimentally%2C%20our%20method%20achieves%20competitive%0Aresults%20with%20state-of-the-art%20methods%20while%20avoiding%20the%20reconstruction%20in%20the%0Ainput%20space%20or%20additional%20modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16432v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint-JEPA%253A%2520A%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520for%250A%2520%2520Self-Supervised%2520Learning%2520on%2520Point%2520Cloud%26entry.906535625%3DAyumu%2520Saito%2520and%2520Prachi%2520Kudeshia%2520and%2520Jiju%2520Poovvancheri%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520self-supervised%2520learning%2520in%2520the%2520point%2520cloud%2520domain%250Ahave%2520demonstrated%2520significant%2520potential.%2520However%252C%2520these%2520methods%2520often%2520suffer%250Afrom%2520drawbacks%252C%2520including%2520lengthy%2520pre-training%2520time%252C%2520the%2520necessity%2520of%250Areconstruction%2520in%2520the%2520input%2520space%252C%2520or%2520the%2520necessity%2520of%2520additional%2520modalities.%250AIn%2520order%2520to%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Point-JEPA%252C%2520a%2520joint%2520embedding%250Apredictive%2520architecture%2520designed%2520specifically%2520for%2520point%2520cloud%2520data.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520a%2520sequencer%2520that%2520orders%2520point%2520cloud%2520patch%2520embeddings%2520to%250Aefficiently%2520compute%2520and%2520utilize%2520their%2520proximity%2520based%2520on%2520the%2520indices%2520during%250Atarget%2520and%2520context%2520selection.%2520The%2520sequencer%2520also%2520allows%2520shared%2520computations%2520of%250Athe%2520patch%2520embeddings%2527%2520proximity%2520between%2520context%2520and%2520target%2520selection%252C%2520further%250Aimproving%2520the%2520efficiency.%2520Experimentally%252C%2520our%2520method%2520achieves%2520competitive%250Aresults%2520with%2520state-of-the-art%2520methods%2520while%2520avoiding%2520the%2520reconstruction%2520in%2520the%250Ainput%2520space%2520or%2520additional%2520modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16432v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-JEPA%3A%20A%20Joint%20Embedding%20Predictive%20Architecture%20for%0A%20%20Self-Supervised%20Learning%20on%20Point%20Cloud&entry.906535625=Ayumu%20Saito%20and%20Prachi%20Kudeshia%20and%20Jiju%20Poovvancheri&entry.1292438233=%20%20Recent%20advancements%20in%20self-supervised%20learning%20in%20the%20point%20cloud%20domain%0Ahave%20demonstrated%20significant%20potential.%20However%2C%20these%20methods%20often%20suffer%0Afrom%20drawbacks%2C%20including%20lengthy%20pre-training%20time%2C%20the%20necessity%20of%0Areconstruction%20in%20the%20input%20space%2C%20or%20the%20necessity%20of%20additional%20modalities.%0AIn%20order%20to%20address%20these%20issues%2C%20we%20introduce%20Point-JEPA%2C%20a%20joint%20embedding%0Apredictive%20architecture%20designed%20specifically%20for%20point%20cloud%20data.%20To%20this%0Aend%2C%20we%20introduce%20a%20sequencer%20that%20orders%20point%20cloud%20patch%20embeddings%20to%0Aefficiently%20compute%20and%20utilize%20their%20proximity%20based%20on%20the%20indices%20during%0Atarget%20and%20context%20selection.%20The%20sequencer%20also%20allows%20shared%20computations%20of%0Athe%20patch%20embeddings%27%20proximity%20between%20context%20and%20target%20selection%2C%20further%0Aimproving%20the%20efficiency.%20Experimentally%2C%20our%20method%20achieves%20competitive%0Aresults%20with%20state-of-the-art%20methods%20while%20avoiding%20the%20reconstruction%20in%20the%0Ainput%20space%20or%20additional%20modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16432v5&entry.124074799=Read"},
{"title": "RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text\n  Supervision", "author": "Fernando P\u00e9rez-Garc\u00eda and Harshita Sharma and Sam Bond-Taylor and Kenza Bouzid and Valentina Salvatelli and Maximilian Ilse and Shruthi Bannur and Daniel C. Castro and Anton Schwaighofer and Matthew P. Lungren and Maria Wetscherek and Noel Codella and Stephanie L. Hyland and Javier Alvarez-Valle and Ozan Oktay", "abstract": "  Language-supervised pre-training has proven to be a valuable method for\nextracting semantically meaningful features from images, serving as a\nfoundational element in multimodal systems within the computer vision and\nmedical imaging domains. However, the computed features are limited by the\ninformation contained in the text, which is particularly problematic in medical\nimaging, where the findings described by radiologists focus on specific\nobservations. This challenge is compounded by the scarcity of paired\nimaging-text data due to concerns over leakage of personal health information.\nIn this work, we fundamentally challenge the prevailing reliance on language\nsupervision for learning general-purpose biomedical imaging encoders. We\nintroduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal\nbiomedical imaging data that obtains similar or greater performance than\nstate-of-the-art biomedical language-supervised models on a diverse range of\nbenchmarks. Specifically, the quality of learned representations is evaluated\non standard imaging tasks (classification and semantic segmentation), and a\nvision-language alignment task (text report generation from images). To further\ndemonstrate the drawback of language supervision, we show that features from\nRAD-DINO correlate with other medical records (e.g., sex or age) better than\nlanguage-supervised models, which are generally not mentioned in radiology\nreports. Finally, we conduct a series of ablations determining the factors in\nRAD-DINO's performance; notably, we observe that RAD-DINO's downstream\nperformance scales well with the quantity and diversity of training data,\ndemonstrating that image-only supervision is a scalable approach for training a\nfoundational biomedical image encoder. Model weights of RAD-DINO trained on\npublicly available datasets are available at\nhttps://huggingface.co/microsoft/rad-dino.\n", "link": "http://arxiv.org/abs/2401.10815v2", "date": "2025-01-13", "relevancy": 2.9511, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAD-DINO%3A%20Exploring%20Scalable%20Medical%20Image%20Encoders%20Beyond%20Text%0A%20%20Supervision&body=Title%3A%20RAD-DINO%3A%20Exploring%20Scalable%20Medical%20Image%20Encoders%20Beyond%20Text%0A%20%20Supervision%0AAuthor%3A%20Fernando%20P%C3%A9rez-Garc%C3%ADa%20and%20Harshita%20Sharma%20and%20Sam%20Bond-Taylor%20and%20Kenza%20Bouzid%20and%20Valentina%20Salvatelli%20and%20Maximilian%20Ilse%20and%20Shruthi%20Bannur%20and%20Daniel%20C.%20Castro%20and%20Anton%20Schwaighofer%20and%20Matthew%20P.%20Lungren%20and%20Maria%20Wetscherek%20and%20Noel%20Codella%20and%20Stephanie%20L.%20Hyland%20and%20Javier%20Alvarez-Valle%20and%20Ozan%20Oktay%0AAbstract%3A%20%20%20Language-supervised%20pre-training%20has%20proven%20to%20be%20a%20valuable%20method%20for%0Aextracting%20semantically%20meaningful%20features%20from%20images%2C%20serving%20as%20a%0Afoundational%20element%20in%20multimodal%20systems%20within%20the%20computer%20vision%20and%0Amedical%20imaging%20domains.%20However%2C%20the%20computed%20features%20are%20limited%20by%20the%0Ainformation%20contained%20in%20the%20text%2C%20which%20is%20particularly%20problematic%20in%20medical%0Aimaging%2C%20where%20the%20findings%20described%20by%20radiologists%20focus%20on%20specific%0Aobservations.%20This%20challenge%20is%20compounded%20by%20the%20scarcity%20of%20paired%0Aimaging-text%20data%20due%20to%20concerns%20over%20leakage%20of%20personal%20health%20information.%0AIn%20this%20work%2C%20we%20fundamentally%20challenge%20the%20prevailing%20reliance%20on%20language%0Asupervision%20for%20learning%20general-purpose%20biomedical%20imaging%20encoders.%20We%0Aintroduce%20RAD-DINO%2C%20a%20biomedical%20image%20encoder%20pre-trained%20solely%20on%20unimodal%0Abiomedical%20imaging%20data%20that%20obtains%20similar%20or%20greater%20performance%20than%0Astate-of-the-art%20biomedical%20language-supervised%20models%20on%20a%20diverse%20range%20of%0Abenchmarks.%20Specifically%2C%20the%20quality%20of%20learned%20representations%20is%20evaluated%0Aon%20standard%20imaging%20tasks%20%28classification%20and%20semantic%20segmentation%29%2C%20and%20a%0Avision-language%20alignment%20task%20%28text%20report%20generation%20from%20images%29.%20To%20further%0Ademonstrate%20the%20drawback%20of%20language%20supervision%2C%20we%20show%20that%20features%20from%0ARAD-DINO%20correlate%20with%20other%20medical%20records%20%28e.g.%2C%20sex%20or%20age%29%20better%20than%0Alanguage-supervised%20models%2C%20which%20are%20generally%20not%20mentioned%20in%20radiology%0Areports.%20Finally%2C%20we%20conduct%20a%20series%20of%20ablations%20determining%20the%20factors%20in%0ARAD-DINO%27s%20performance%3B%20notably%2C%20we%20observe%20that%20RAD-DINO%27s%20downstream%0Aperformance%20scales%20well%20with%20the%20quantity%20and%20diversity%20of%20training%20data%2C%0Ademonstrating%20that%20image-only%20supervision%20is%20a%20scalable%20approach%20for%20training%20a%0Afoundational%20biomedical%20image%20encoder.%20Model%20weights%20of%20RAD-DINO%20trained%20on%0Apublicly%20available%20datasets%20are%20available%20at%0Ahttps%3A//huggingface.co/microsoft/rad-dino.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAD-DINO%253A%2520Exploring%2520Scalable%2520Medical%2520Image%2520Encoders%2520Beyond%2520Text%250A%2520%2520Supervision%26entry.906535625%3DFernando%2520P%25C3%25A9rez-Garc%25C3%25ADa%2520and%2520Harshita%2520Sharma%2520and%2520Sam%2520Bond-Taylor%2520and%2520Kenza%2520Bouzid%2520and%2520Valentina%2520Salvatelli%2520and%2520Maximilian%2520Ilse%2520and%2520Shruthi%2520Bannur%2520and%2520Daniel%2520C.%2520Castro%2520and%2520Anton%2520Schwaighofer%2520and%2520Matthew%2520P.%2520Lungren%2520and%2520Maria%2520Wetscherek%2520and%2520Noel%2520Codella%2520and%2520Stephanie%2520L.%2520Hyland%2520and%2520Javier%2520Alvarez-Valle%2520and%2520Ozan%2520Oktay%26entry.1292438233%3D%2520%2520Language-supervised%2520pre-training%2520has%2520proven%2520to%2520be%2520a%2520valuable%2520method%2520for%250Aextracting%2520semantically%2520meaningful%2520features%2520from%2520images%252C%2520serving%2520as%2520a%250Afoundational%2520element%2520in%2520multimodal%2520systems%2520within%2520the%2520computer%2520vision%2520and%250Amedical%2520imaging%2520domains.%2520However%252C%2520the%2520computed%2520features%2520are%2520limited%2520by%2520the%250Ainformation%2520contained%2520in%2520the%2520text%252C%2520which%2520is%2520particularly%2520problematic%2520in%2520medical%250Aimaging%252C%2520where%2520the%2520findings%2520described%2520by%2520radiologists%2520focus%2520on%2520specific%250Aobservations.%2520This%2520challenge%2520is%2520compounded%2520by%2520the%2520scarcity%2520of%2520paired%250Aimaging-text%2520data%2520due%2520to%2520concerns%2520over%2520leakage%2520of%2520personal%2520health%2520information.%250AIn%2520this%2520work%252C%2520we%2520fundamentally%2520challenge%2520the%2520prevailing%2520reliance%2520on%2520language%250Asupervision%2520for%2520learning%2520general-purpose%2520biomedical%2520imaging%2520encoders.%2520We%250Aintroduce%2520RAD-DINO%252C%2520a%2520biomedical%2520image%2520encoder%2520pre-trained%2520solely%2520on%2520unimodal%250Abiomedical%2520imaging%2520data%2520that%2520obtains%2520similar%2520or%2520greater%2520performance%2520than%250Astate-of-the-art%2520biomedical%2520language-supervised%2520models%2520on%2520a%2520diverse%2520range%2520of%250Abenchmarks.%2520Specifically%252C%2520the%2520quality%2520of%2520learned%2520representations%2520is%2520evaluated%250Aon%2520standard%2520imaging%2520tasks%2520%2528classification%2520and%2520semantic%2520segmentation%2529%252C%2520and%2520a%250Avision-language%2520alignment%2520task%2520%2528text%2520report%2520generation%2520from%2520images%2529.%2520To%2520further%250Ademonstrate%2520the%2520drawback%2520of%2520language%2520supervision%252C%2520we%2520show%2520that%2520features%2520from%250ARAD-DINO%2520correlate%2520with%2520other%2520medical%2520records%2520%2528e.g.%252C%2520sex%2520or%2520age%2529%2520better%2520than%250Alanguage-supervised%2520models%252C%2520which%2520are%2520generally%2520not%2520mentioned%2520in%2520radiology%250Areports.%2520Finally%252C%2520we%2520conduct%2520a%2520series%2520of%2520ablations%2520determining%2520the%2520factors%2520in%250ARAD-DINO%2527s%2520performance%253B%2520notably%252C%2520we%2520observe%2520that%2520RAD-DINO%2527s%2520downstream%250Aperformance%2520scales%2520well%2520with%2520the%2520quantity%2520and%2520diversity%2520of%2520training%2520data%252C%250Ademonstrating%2520that%2520image-only%2520supervision%2520is%2520a%2520scalable%2520approach%2520for%2520training%2520a%250Afoundational%2520biomedical%2520image%2520encoder.%2520Model%2520weights%2520of%2520RAD-DINO%2520trained%2520on%250Apublicly%2520available%2520datasets%2520are%2520available%2520at%250Ahttps%253A//huggingface.co/microsoft/rad-dino.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAD-DINO%3A%20Exploring%20Scalable%20Medical%20Image%20Encoders%20Beyond%20Text%0A%20%20Supervision&entry.906535625=Fernando%20P%C3%A9rez-Garc%C3%ADa%20and%20Harshita%20Sharma%20and%20Sam%20Bond-Taylor%20and%20Kenza%20Bouzid%20and%20Valentina%20Salvatelli%20and%20Maximilian%20Ilse%20and%20Shruthi%20Bannur%20and%20Daniel%20C.%20Castro%20and%20Anton%20Schwaighofer%20and%20Matthew%20P.%20Lungren%20and%20Maria%20Wetscherek%20and%20Noel%20Codella%20and%20Stephanie%20L.%20Hyland%20and%20Javier%20Alvarez-Valle%20and%20Ozan%20Oktay&entry.1292438233=%20%20Language-supervised%20pre-training%20has%20proven%20to%20be%20a%20valuable%20method%20for%0Aextracting%20semantically%20meaningful%20features%20from%20images%2C%20serving%20as%20a%0Afoundational%20element%20in%20multimodal%20systems%20within%20the%20computer%20vision%20and%0Amedical%20imaging%20domains.%20However%2C%20the%20computed%20features%20are%20limited%20by%20the%0Ainformation%20contained%20in%20the%20text%2C%20which%20is%20particularly%20problematic%20in%20medical%0Aimaging%2C%20where%20the%20findings%20described%20by%20radiologists%20focus%20on%20specific%0Aobservations.%20This%20challenge%20is%20compounded%20by%20the%20scarcity%20of%20paired%0Aimaging-text%20data%20due%20to%20concerns%20over%20leakage%20of%20personal%20health%20information.%0AIn%20this%20work%2C%20we%20fundamentally%20challenge%20the%20prevailing%20reliance%20on%20language%0Asupervision%20for%20learning%20general-purpose%20biomedical%20imaging%20encoders.%20We%0Aintroduce%20RAD-DINO%2C%20a%20biomedical%20image%20encoder%20pre-trained%20solely%20on%20unimodal%0Abiomedical%20imaging%20data%20that%20obtains%20similar%20or%20greater%20performance%20than%0Astate-of-the-art%20biomedical%20language-supervised%20models%20on%20a%20diverse%20range%20of%0Abenchmarks.%20Specifically%2C%20the%20quality%20of%20learned%20representations%20is%20evaluated%0Aon%20standard%20imaging%20tasks%20%28classification%20and%20semantic%20segmentation%29%2C%20and%20a%0Avision-language%20alignment%20task%20%28text%20report%20generation%20from%20images%29.%20To%20further%0Ademonstrate%20the%20drawback%20of%20language%20supervision%2C%20we%20show%20that%20features%20from%0ARAD-DINO%20correlate%20with%20other%20medical%20records%20%28e.g.%2C%20sex%20or%20age%29%20better%20than%0Alanguage-supervised%20models%2C%20which%20are%20generally%20not%20mentioned%20in%20radiology%0Areports.%20Finally%2C%20we%20conduct%20a%20series%20of%20ablations%20determining%20the%20factors%20in%0ARAD-DINO%27s%20performance%3B%20notably%2C%20we%20observe%20that%20RAD-DINO%27s%20downstream%0Aperformance%20scales%20well%20with%20the%20quantity%20and%20diversity%20of%20training%20data%2C%0Ademonstrating%20that%20image-only%20supervision%20is%20a%20scalable%20approach%20for%20training%20a%0Afoundational%20biomedical%20image%20encoder.%20Model%20weights%20of%20RAD-DINO%20trained%20on%0Apublicly%20available%20datasets%20are%20available%20at%0Ahttps%3A//huggingface.co/microsoft/rad-dino.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10815v2&entry.124074799=Read"},
{"title": "MatchAnything: Universal Cross-Modality Image Matching with Large-Scale\n  Pre-Training", "author": "Xingyi He and Hao Yu and Sida Peng and Dongli Tan and Zehong Shen and Hujun Bao and Xiaowei Zhou", "abstract": "  Image matching, which aims to identify corresponding pixel locations between\nimages, is crucial in a wide range of scientific disciplines, aiding in image\nregistration, fusion, and analysis. In recent years, deep learning-based image\nmatching algorithms have dramatically outperformed humans in rapidly and\naccurately finding large amounts of correspondences. However, when dealing with\nimages captured under different imaging modalities that result in significant\nappearance changes, the performance of these algorithms often deteriorates due\nto the scarcity of annotated cross-modal training data. This limitation hinders\napplications in various fields that rely on multiple image modalities to obtain\ncomplementary information. To address this challenge, we propose a large-scale\npre-training framework that utilizes synthetic cross-modal training signals,\nincorporating diverse data from various sources, to train models to recognize\nand match fundamental structures across images. This capability is transferable\nto real-world, unseen cross-modality image matching tasks. Our key finding is\nthat the matching model trained with our framework achieves remarkable\ngeneralizability across more than eight unseen cross-modality registration\ntasks using the same network weight, substantially outperforming existing\nmethods, whether designed for generalization or tailored for specific tasks.\nThis advancement significantly enhances the applicability of image matching\ntechnologies across various scientific disciplines and paves the way for new\napplications in multi-modality human and artificial intelligence analysis and\nbeyond.\n", "link": "http://arxiv.org/abs/2501.07556v1", "date": "2025-01-13", "relevancy": 2.9195, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6207}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatchAnything%3A%20Universal%20Cross-Modality%20Image%20Matching%20with%20Large-Scale%0A%20%20Pre-Training&body=Title%3A%20MatchAnything%3A%20Universal%20Cross-Modality%20Image%20Matching%20with%20Large-Scale%0A%20%20Pre-Training%0AAuthor%3A%20Xingyi%20He%20and%20Hao%20Yu%20and%20Sida%20Peng%20and%20Dongli%20Tan%20and%20Zehong%20Shen%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%20Image%20matching%2C%20which%20aims%20to%20identify%20corresponding%20pixel%20locations%20between%0Aimages%2C%20is%20crucial%20in%20a%20wide%20range%20of%20scientific%20disciplines%2C%20aiding%20in%20image%0Aregistration%2C%20fusion%2C%20and%20analysis.%20In%20recent%20years%2C%20deep%20learning-based%20image%0Amatching%20algorithms%20have%20dramatically%20outperformed%20humans%20in%20rapidly%20and%0Aaccurately%20finding%20large%20amounts%20of%20correspondences.%20However%2C%20when%20dealing%20with%0Aimages%20captured%20under%20different%20imaging%20modalities%20that%20result%20in%20significant%0Aappearance%20changes%2C%20the%20performance%20of%20these%20algorithms%20often%20deteriorates%20due%0Ato%20the%20scarcity%20of%20annotated%20cross-modal%20training%20data.%20This%20limitation%20hinders%0Aapplications%20in%20various%20fields%20that%20rely%20on%20multiple%20image%20modalities%20to%20obtain%0Acomplementary%20information.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20large-scale%0Apre-training%20framework%20that%20utilizes%20synthetic%20cross-modal%20training%20signals%2C%0Aincorporating%20diverse%20data%20from%20various%20sources%2C%20to%20train%20models%20to%20recognize%0Aand%20match%20fundamental%20structures%20across%20images.%20This%20capability%20is%20transferable%0Ato%20real-world%2C%20unseen%20cross-modality%20image%20matching%20tasks.%20Our%20key%20finding%20is%0Athat%20the%20matching%20model%20trained%20with%20our%20framework%20achieves%20remarkable%0Ageneralizability%20across%20more%20than%20eight%20unseen%20cross-modality%20registration%0Atasks%20using%20the%20same%20network%20weight%2C%20substantially%20outperforming%20existing%0Amethods%2C%20whether%20designed%20for%20generalization%20or%20tailored%20for%20specific%20tasks.%0AThis%20advancement%20significantly%20enhances%20the%20applicability%20of%20image%20matching%0Atechnologies%20across%20various%20scientific%20disciplines%20and%20paves%20the%20way%20for%20new%0Aapplications%20in%20multi-modality%20human%20and%20artificial%20intelligence%20analysis%20and%0Abeyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatchAnything%253A%2520Universal%2520Cross-Modality%2520Image%2520Matching%2520with%2520Large-Scale%250A%2520%2520Pre-Training%26entry.906535625%3DXingyi%2520He%2520and%2520Hao%2520Yu%2520and%2520Sida%2520Peng%2520and%2520Dongli%2520Tan%2520and%2520Zehong%2520Shen%2520and%2520Hujun%2520Bao%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%2520Image%2520matching%252C%2520which%2520aims%2520to%2520identify%2520corresponding%2520pixel%2520locations%2520between%250Aimages%252C%2520is%2520crucial%2520in%2520a%2520wide%2520range%2520of%2520scientific%2520disciplines%252C%2520aiding%2520in%2520image%250Aregistration%252C%2520fusion%252C%2520and%2520analysis.%2520In%2520recent%2520years%252C%2520deep%2520learning-based%2520image%250Amatching%2520algorithms%2520have%2520dramatically%2520outperformed%2520humans%2520in%2520rapidly%2520and%250Aaccurately%2520finding%2520large%2520amounts%2520of%2520correspondences.%2520However%252C%2520when%2520dealing%2520with%250Aimages%2520captured%2520under%2520different%2520imaging%2520modalities%2520that%2520result%2520in%2520significant%250Aappearance%2520changes%252C%2520the%2520performance%2520of%2520these%2520algorithms%2520often%2520deteriorates%2520due%250Ato%2520the%2520scarcity%2520of%2520annotated%2520cross-modal%2520training%2520data.%2520This%2520limitation%2520hinders%250Aapplications%2520in%2520various%2520fields%2520that%2520rely%2520on%2520multiple%2520image%2520modalities%2520to%2520obtain%250Acomplementary%2520information.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520large-scale%250Apre-training%2520framework%2520that%2520utilizes%2520synthetic%2520cross-modal%2520training%2520signals%252C%250Aincorporating%2520diverse%2520data%2520from%2520various%2520sources%252C%2520to%2520train%2520models%2520to%2520recognize%250Aand%2520match%2520fundamental%2520structures%2520across%2520images.%2520This%2520capability%2520is%2520transferable%250Ato%2520real-world%252C%2520unseen%2520cross-modality%2520image%2520matching%2520tasks.%2520Our%2520key%2520finding%2520is%250Athat%2520the%2520matching%2520model%2520trained%2520with%2520our%2520framework%2520achieves%2520remarkable%250Ageneralizability%2520across%2520more%2520than%2520eight%2520unseen%2520cross-modality%2520registration%250Atasks%2520using%2520the%2520same%2520network%2520weight%252C%2520substantially%2520outperforming%2520existing%250Amethods%252C%2520whether%2520designed%2520for%2520generalization%2520or%2520tailored%2520for%2520specific%2520tasks.%250AThis%2520advancement%2520significantly%2520enhances%2520the%2520applicability%2520of%2520image%2520matching%250Atechnologies%2520across%2520various%2520scientific%2520disciplines%2520and%2520paves%2520the%2520way%2520for%2520new%250Aapplications%2520in%2520multi-modality%2520human%2520and%2520artificial%2520intelligence%2520analysis%2520and%250Abeyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatchAnything%3A%20Universal%20Cross-Modality%20Image%20Matching%20with%20Large-Scale%0A%20%20Pre-Training&entry.906535625=Xingyi%20He%20and%20Hao%20Yu%20and%20Sida%20Peng%20and%20Dongli%20Tan%20and%20Zehong%20Shen%20and%20Hujun%20Bao%20and%20Xiaowei%20Zhou&entry.1292438233=%20%20Image%20matching%2C%20which%20aims%20to%20identify%20corresponding%20pixel%20locations%20between%0Aimages%2C%20is%20crucial%20in%20a%20wide%20range%20of%20scientific%20disciplines%2C%20aiding%20in%20image%0Aregistration%2C%20fusion%2C%20and%20analysis.%20In%20recent%20years%2C%20deep%20learning-based%20image%0Amatching%20algorithms%20have%20dramatically%20outperformed%20humans%20in%20rapidly%20and%0Aaccurately%20finding%20large%20amounts%20of%20correspondences.%20However%2C%20when%20dealing%20with%0Aimages%20captured%20under%20different%20imaging%20modalities%20that%20result%20in%20significant%0Aappearance%20changes%2C%20the%20performance%20of%20these%20algorithms%20often%20deteriorates%20due%0Ato%20the%20scarcity%20of%20annotated%20cross-modal%20training%20data.%20This%20limitation%20hinders%0Aapplications%20in%20various%20fields%20that%20rely%20on%20multiple%20image%20modalities%20to%20obtain%0Acomplementary%20information.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20large-scale%0Apre-training%20framework%20that%20utilizes%20synthetic%20cross-modal%20training%20signals%2C%0Aincorporating%20diverse%20data%20from%20various%20sources%2C%20to%20train%20models%20to%20recognize%0Aand%20match%20fundamental%20structures%20across%20images.%20This%20capability%20is%20transferable%0Ato%20real-world%2C%20unseen%20cross-modality%20image%20matching%20tasks.%20Our%20key%20finding%20is%0Athat%20the%20matching%20model%20trained%20with%20our%20framework%20achieves%20remarkable%0Ageneralizability%20across%20more%20than%20eight%20unseen%20cross-modality%20registration%0Atasks%20using%20the%20same%20network%20weight%2C%20substantially%20outperforming%20existing%0Amethods%2C%20whether%20designed%20for%20generalization%20or%20tailored%20for%20specific%20tasks.%0AThis%20advancement%20significantly%20enhances%20the%20applicability%20of%20image%20matching%0Atechnologies%20across%20various%20scientific%20disciplines%20and%20paves%20the%20way%20for%20new%0Aapplications%20in%20multi-modality%20human%20and%20artificial%20intelligence%20analysis%20and%0Abeyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07556v1&entry.124074799=Read"},
{"title": "Zero-Shot Scene Understanding for Automatic Target Recognition Using\n  Large Vision-Language Models", "author": "Yasiru Ranasinghe and Vibashan VS and James Uplinger and Celso De Melo and Vishal M. Patel", "abstract": "  Automatic target recognition (ATR) plays a critical role in tasks such as\nnavigation and surveillance, where safety and accuracy are paramount. In\nextreme use cases, such as military applications, these factors are often\nchallenged due to the presence of unknown terrains, environmental conditions,\nand novel object categories. Current object detectors, including open-world\ndetectors, lack the ability to confidently recognize novel objects or operate\nin unknown environments, as they have not been exposed to these new conditions.\nHowever, Large Vision-Language Models (LVLMs) exhibit emergent properties that\nenable them to recognize objects in varying conditions in a zero-shot manner.\nDespite this, LVLMs struggle to localize objects effectively within a scene. To\naddress these limitations, we propose a novel pipeline that combines the\ndetection capabilities of open-world detectors with the recognition confidence\nof LVLMs, creating a robust system for zero-shot ATR of novel classes and\nunknown domains. In this study, we compare the performance of various LVLMs for\nrecognizing military vehicles, which are often underrepresented in training\ndatasets. Additionally, we examine the impact of factors such as distance\nrange, modality, and prompting methods on the recognition performance,\nproviding insights into the development of more reliable ATR systems for novel\nconditions and classes.\n", "link": "http://arxiv.org/abs/2501.07396v1", "date": "2025-01-13", "relevancy": 2.9015, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Scene%20Understanding%20for%20Automatic%20Target%20Recognition%20Using%0A%20%20Large%20Vision-Language%20Models&body=Title%3A%20Zero-Shot%20Scene%20Understanding%20for%20Automatic%20Target%20Recognition%20Using%0A%20%20Large%20Vision-Language%20Models%0AAuthor%3A%20Yasiru%20Ranasinghe%20and%20Vibashan%20VS%20and%20James%20Uplinger%20and%20Celso%20De%20Melo%20and%20Vishal%20M.%20Patel%0AAbstract%3A%20%20%20Automatic%20target%20recognition%20%28ATR%29%20plays%20a%20critical%20role%20in%20tasks%20such%20as%0Anavigation%20and%20surveillance%2C%20where%20safety%20and%20accuracy%20are%20paramount.%20In%0Aextreme%20use%20cases%2C%20such%20as%20military%20applications%2C%20these%20factors%20are%20often%0Achallenged%20due%20to%20the%20presence%20of%20unknown%20terrains%2C%20environmental%20conditions%2C%0Aand%20novel%20object%20categories.%20Current%20object%20detectors%2C%20including%20open-world%0Adetectors%2C%20lack%20the%20ability%20to%20confidently%20recognize%20novel%20objects%20or%20operate%0Ain%20unknown%20environments%2C%20as%20they%20have%20not%20been%20exposed%20to%20these%20new%20conditions.%0AHowever%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20exhibit%20emergent%20properties%20that%0Aenable%20them%20to%20recognize%20objects%20in%20varying%20conditions%20in%20a%20zero-shot%20manner.%0ADespite%20this%2C%20LVLMs%20struggle%20to%20localize%20objects%20effectively%20within%20a%20scene.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20pipeline%20that%20combines%20the%0Adetection%20capabilities%20of%20open-world%20detectors%20with%20the%20recognition%20confidence%0Aof%20LVLMs%2C%20creating%20a%20robust%20system%20for%20zero-shot%20ATR%20of%20novel%20classes%20and%0Aunknown%20domains.%20In%20this%20study%2C%20we%20compare%20the%20performance%20of%20various%20LVLMs%20for%0Arecognizing%20military%20vehicles%2C%20which%20are%20often%20underrepresented%20in%20training%0Adatasets.%20Additionally%2C%20we%20examine%20the%20impact%20of%20factors%20such%20as%20distance%0Arange%2C%20modality%2C%20and%20prompting%20methods%20on%20the%20recognition%20performance%2C%0Aproviding%20insights%20into%20the%20development%20of%20more%20reliable%20ATR%20systems%20for%20novel%0Aconditions%20and%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Scene%2520Understanding%2520for%2520Automatic%2520Target%2520Recognition%2520Using%250A%2520%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DYasiru%2520Ranasinghe%2520and%2520Vibashan%2520VS%2520and%2520James%2520Uplinger%2520and%2520Celso%2520De%2520Melo%2520and%2520Vishal%2520M.%2520Patel%26entry.1292438233%3D%2520%2520Automatic%2520target%2520recognition%2520%2528ATR%2529%2520plays%2520a%2520critical%2520role%2520in%2520tasks%2520such%2520as%250Anavigation%2520and%2520surveillance%252C%2520where%2520safety%2520and%2520accuracy%2520are%2520paramount.%2520In%250Aextreme%2520use%2520cases%252C%2520such%2520as%2520military%2520applications%252C%2520these%2520factors%2520are%2520often%250Achallenged%2520due%2520to%2520the%2520presence%2520of%2520unknown%2520terrains%252C%2520environmental%2520conditions%252C%250Aand%2520novel%2520object%2520categories.%2520Current%2520object%2520detectors%252C%2520including%2520open-world%250Adetectors%252C%2520lack%2520the%2520ability%2520to%2520confidently%2520recognize%2520novel%2520objects%2520or%2520operate%250Ain%2520unknown%2520environments%252C%2520as%2520they%2520have%2520not%2520been%2520exposed%2520to%2520these%2520new%2520conditions.%250AHowever%252C%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520exhibit%2520emergent%2520properties%2520that%250Aenable%2520them%2520to%2520recognize%2520objects%2520in%2520varying%2520conditions%2520in%2520a%2520zero-shot%2520manner.%250ADespite%2520this%252C%2520LVLMs%2520struggle%2520to%2520localize%2520objects%2520effectively%2520within%2520a%2520scene.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520pipeline%2520that%2520combines%2520the%250Adetection%2520capabilities%2520of%2520open-world%2520detectors%2520with%2520the%2520recognition%2520confidence%250Aof%2520LVLMs%252C%2520creating%2520a%2520robust%2520system%2520for%2520zero-shot%2520ATR%2520of%2520novel%2520classes%2520and%250Aunknown%2520domains.%2520In%2520this%2520study%252C%2520we%2520compare%2520the%2520performance%2520of%2520various%2520LVLMs%2520for%250Arecognizing%2520military%2520vehicles%252C%2520which%2520are%2520often%2520underrepresented%2520in%2520training%250Adatasets.%2520Additionally%252C%2520we%2520examine%2520the%2520impact%2520of%2520factors%2520such%2520as%2520distance%250Arange%252C%2520modality%252C%2520and%2520prompting%2520methods%2520on%2520the%2520recognition%2520performance%252C%250Aproviding%2520insights%2520into%2520the%2520development%2520of%2520more%2520reliable%2520ATR%2520systems%2520for%2520novel%250Aconditions%2520and%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Scene%20Understanding%20for%20Automatic%20Target%20Recognition%20Using%0A%20%20Large%20Vision-Language%20Models&entry.906535625=Yasiru%20Ranasinghe%20and%20Vibashan%20VS%20and%20James%20Uplinger%20and%20Celso%20De%20Melo%20and%20Vishal%20M.%20Patel&entry.1292438233=%20%20Automatic%20target%20recognition%20%28ATR%29%20plays%20a%20critical%20role%20in%20tasks%20such%20as%0Anavigation%20and%20surveillance%2C%20where%20safety%20and%20accuracy%20are%20paramount.%20In%0Aextreme%20use%20cases%2C%20such%20as%20military%20applications%2C%20these%20factors%20are%20often%0Achallenged%20due%20to%20the%20presence%20of%20unknown%20terrains%2C%20environmental%20conditions%2C%0Aand%20novel%20object%20categories.%20Current%20object%20detectors%2C%20including%20open-world%0Adetectors%2C%20lack%20the%20ability%20to%20confidently%20recognize%20novel%20objects%20or%20operate%0Ain%20unknown%20environments%2C%20as%20they%20have%20not%20been%20exposed%20to%20these%20new%20conditions.%0AHowever%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20exhibit%20emergent%20properties%20that%0Aenable%20them%20to%20recognize%20objects%20in%20varying%20conditions%20in%20a%20zero-shot%20manner.%0ADespite%20this%2C%20LVLMs%20struggle%20to%20localize%20objects%20effectively%20within%20a%20scene.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20pipeline%20that%20combines%20the%0Adetection%20capabilities%20of%20open-world%20detectors%20with%20the%20recognition%20confidence%0Aof%20LVLMs%2C%20creating%20a%20robust%20system%20for%20zero-shot%20ATR%20of%20novel%20classes%20and%0Aunknown%20domains.%20In%20this%20study%2C%20we%20compare%20the%20performance%20of%20various%20LVLMs%20for%0Arecognizing%20military%20vehicles%2C%20which%20are%20often%20underrepresented%20in%20training%0Adatasets.%20Additionally%2C%20we%20examine%20the%20impact%20of%20factors%20such%20as%20distance%0Arange%2C%20modality%2C%20and%20prompting%20methods%20on%20the%20recognition%20performance%2C%0Aproviding%20insights%20into%20the%20development%20of%20more%20reliable%20ATR%20systems%20for%20novel%0Aconditions%20and%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07396v1&entry.124074799=Read"},
{"title": "Efficiently Closing Loops in LiDAR-Based SLAM Using Point Cloud Density\n  Maps", "author": "Saurabh Gupta and Tiziano Guadagnino and Benedikt Mersch and Niklas Trekel and Meher V. R. Malladi and Cyrill Stachniss", "abstract": "  Consistent maps are key for most autonomous mobile robots. They often use\nSLAM approaches to build such maps. Loop closures via place recognition help\nmaintain accurate pose estimates by mitigating global drift. This paper\npresents a robust loop closure detection pipeline for outdoor SLAM with\nLiDAR-equipped robots. The method handles various LiDAR sensors with different\nscanning patterns, field of views and resolutions. It generates local maps from\nLiDAR scans and aligns them using a ground alignment module to handle both\nplanar and non-planar motion of the LiDAR, ensuring applicability across\nplatforms. The method uses density-preserving bird's eye view projections of\nthese local maps and extracts ORB feature descriptors from them for place\nrecognition. It stores the feature descriptors in a binary search tree for\nefficient retrieval, and self-similarity pruning addresses perceptual aliasing\nin repetitive environments. Extensive experiments on public and self-recorded\ndatasets demonstrate accurate loop closure detection, long-term localization,\nand cross-platform multi-map alignment, agnostic to the LiDAR scanning\npatterns, fields of view, and motion profiles.\n", "link": "http://arxiv.org/abs/2501.07399v1", "date": "2025-01-13", "relevancy": 2.8783, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6232}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Closing%20Loops%20in%20LiDAR-Based%20SLAM%20Using%20Point%20Cloud%20Density%0A%20%20Maps&body=Title%3A%20Efficiently%20Closing%20Loops%20in%20LiDAR-Based%20SLAM%20Using%20Point%20Cloud%20Density%0A%20%20Maps%0AAuthor%3A%20Saurabh%20Gupta%20and%20Tiziano%20Guadagnino%20and%20Benedikt%20Mersch%20and%20Niklas%20Trekel%20and%20Meher%20V.%20R.%20Malladi%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Consistent%20maps%20are%20key%20for%20most%20autonomous%20mobile%20robots.%20They%20often%20use%0ASLAM%20approaches%20to%20build%20such%20maps.%20Loop%20closures%20via%20place%20recognition%20help%0Amaintain%20accurate%20pose%20estimates%20by%20mitigating%20global%20drift.%20This%20paper%0Apresents%20a%20robust%20loop%20closure%20detection%20pipeline%20for%20outdoor%20SLAM%20with%0ALiDAR-equipped%20robots.%20The%20method%20handles%20various%20LiDAR%20sensors%20with%20different%0Ascanning%20patterns%2C%20field%20of%20views%20and%20resolutions.%20It%20generates%20local%20maps%20from%0ALiDAR%20scans%20and%20aligns%20them%20using%20a%20ground%20alignment%20module%20to%20handle%20both%0Aplanar%20and%20non-planar%20motion%20of%20the%20LiDAR%2C%20ensuring%20applicability%20across%0Aplatforms.%20The%20method%20uses%20density-preserving%20bird%27s%20eye%20view%20projections%20of%0Athese%20local%20maps%20and%20extracts%20ORB%20feature%20descriptors%20from%20them%20for%20place%0Arecognition.%20It%20stores%20the%20feature%20descriptors%20in%20a%20binary%20search%20tree%20for%0Aefficient%20retrieval%2C%20and%20self-similarity%20pruning%20addresses%20perceptual%20aliasing%0Ain%20repetitive%20environments.%20Extensive%20experiments%20on%20public%20and%20self-recorded%0Adatasets%20demonstrate%20accurate%20loop%20closure%20detection%2C%20long-term%20localization%2C%0Aand%20cross-platform%20multi-map%20alignment%2C%20agnostic%20to%20the%20LiDAR%20scanning%0Apatterns%2C%20fields%20of%20view%2C%20and%20motion%20profiles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Closing%2520Loops%2520in%2520LiDAR-Based%2520SLAM%2520Using%2520Point%2520Cloud%2520Density%250A%2520%2520Maps%26entry.906535625%3DSaurabh%2520Gupta%2520and%2520Tiziano%2520Guadagnino%2520and%2520Benedikt%2520Mersch%2520and%2520Niklas%2520Trekel%2520and%2520Meher%2520V.%2520R.%2520Malladi%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520Consistent%2520maps%2520are%2520key%2520for%2520most%2520autonomous%2520mobile%2520robots.%2520They%2520often%2520use%250ASLAM%2520approaches%2520to%2520build%2520such%2520maps.%2520Loop%2520closures%2520via%2520place%2520recognition%2520help%250Amaintain%2520accurate%2520pose%2520estimates%2520by%2520mitigating%2520global%2520drift.%2520This%2520paper%250Apresents%2520a%2520robust%2520loop%2520closure%2520detection%2520pipeline%2520for%2520outdoor%2520SLAM%2520with%250ALiDAR-equipped%2520robots.%2520The%2520method%2520handles%2520various%2520LiDAR%2520sensors%2520with%2520different%250Ascanning%2520patterns%252C%2520field%2520of%2520views%2520and%2520resolutions.%2520It%2520generates%2520local%2520maps%2520from%250ALiDAR%2520scans%2520and%2520aligns%2520them%2520using%2520a%2520ground%2520alignment%2520module%2520to%2520handle%2520both%250Aplanar%2520and%2520non-planar%2520motion%2520of%2520the%2520LiDAR%252C%2520ensuring%2520applicability%2520across%250Aplatforms.%2520The%2520method%2520uses%2520density-preserving%2520bird%2527s%2520eye%2520view%2520projections%2520of%250Athese%2520local%2520maps%2520and%2520extracts%2520ORB%2520feature%2520descriptors%2520from%2520them%2520for%2520place%250Arecognition.%2520It%2520stores%2520the%2520feature%2520descriptors%2520in%2520a%2520binary%2520search%2520tree%2520for%250Aefficient%2520retrieval%252C%2520and%2520self-similarity%2520pruning%2520addresses%2520perceptual%2520aliasing%250Ain%2520repetitive%2520environments.%2520Extensive%2520experiments%2520on%2520public%2520and%2520self-recorded%250Adatasets%2520demonstrate%2520accurate%2520loop%2520closure%2520detection%252C%2520long-term%2520localization%252C%250Aand%2520cross-platform%2520multi-map%2520alignment%252C%2520agnostic%2520to%2520the%2520LiDAR%2520scanning%250Apatterns%252C%2520fields%2520of%2520view%252C%2520and%2520motion%2520profiles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Closing%20Loops%20in%20LiDAR-Based%20SLAM%20Using%20Point%20Cloud%20Density%0A%20%20Maps&entry.906535625=Saurabh%20Gupta%20and%20Tiziano%20Guadagnino%20and%20Benedikt%20Mersch%20and%20Niklas%20Trekel%20and%20Meher%20V.%20R.%20Malladi%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Consistent%20maps%20are%20key%20for%20most%20autonomous%20mobile%20robots.%20They%20often%20use%0ASLAM%20approaches%20to%20build%20such%20maps.%20Loop%20closures%20via%20place%20recognition%20help%0Amaintain%20accurate%20pose%20estimates%20by%20mitigating%20global%20drift.%20This%20paper%0Apresents%20a%20robust%20loop%20closure%20detection%20pipeline%20for%20outdoor%20SLAM%20with%0ALiDAR-equipped%20robots.%20The%20method%20handles%20various%20LiDAR%20sensors%20with%20different%0Ascanning%20patterns%2C%20field%20of%20views%20and%20resolutions.%20It%20generates%20local%20maps%20from%0ALiDAR%20scans%20and%20aligns%20them%20using%20a%20ground%20alignment%20module%20to%20handle%20both%0Aplanar%20and%20non-planar%20motion%20of%20the%20LiDAR%2C%20ensuring%20applicability%20across%0Aplatforms.%20The%20method%20uses%20density-preserving%20bird%27s%20eye%20view%20projections%20of%0Athese%20local%20maps%20and%20extracts%20ORB%20feature%20descriptors%20from%20them%20for%20place%0Arecognition.%20It%20stores%20the%20feature%20descriptors%20in%20a%20binary%20search%20tree%20for%0Aefficient%20retrieval%2C%20and%20self-similarity%20pruning%20addresses%20perceptual%20aliasing%0Ain%20repetitive%20environments.%20Extensive%20experiments%20on%20public%20and%20self-recorded%0Adatasets%20demonstrate%20accurate%20loop%20closure%20detection%2C%20long-term%20localization%2C%0Aand%20cross-platform%20multi-map%20alignment%2C%20agnostic%20to%20the%20LiDAR%20scanning%0Apatterns%2C%20fields%20of%20view%2C%20and%20motion%20profiles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07399v1&entry.124074799=Read"},
{"title": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment", "author": "Zhendong Liu and Yuanbi Nie and Yingshui Tan and Jiaheng Liu and Xiangyu Yue and Qiushi Cui and Chongjun Wang and Xiaoyong Zhu and Bo Zheng", "abstract": "  Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.\n", "link": "http://arxiv.org/abs/2411.11543v4", "date": "2025-01-13", "relevancy": 2.8421, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSA-VLM%3A%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment&body=Title%3A%20PSA-VLM%3A%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment%0AAuthor%3A%20Zhendong%20Liu%20and%20Yuanbi%20Nie%20and%20Yingshui%20Tan%20and%20Jiaheng%20Liu%20and%20Xiangyu%20Yue%20and%20Qiushi%20Cui%20and%20Chongjun%20Wang%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Benefiting%20from%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Apre-trained%20visual%20encoder%20models%20connected%20to%20LLMs%20form%20Vision%20Language%20Models%0A%28VLMs%29.%20However%2C%20recent%20research%20shows%20that%20the%20visual%20modality%20in%20VLMs%20is%0Ahighly%20vulnerable%2C%20allowing%20attackers%20to%20bypass%20safety%20alignment%20in%20LLMs%0Athrough%20visually%20transmitted%20content%2C%20launching%20harmful%20attacks.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20progressive%20concept-based%20alignment%20strategy%2C%0APSA-VLM%2C%20which%20incorporates%20safety%20modules%20as%20concept%20bottlenecks%20to%20enhance%0Avisual%20modality%20safety%20alignment.%20By%20aligning%20model%20predictions%20with%20specific%0Asafety%20concepts%2C%20we%20improve%20defenses%20against%20risky%20images%2C%20enhancing%0Aexplainability%20and%20controllability%20while%20minimally%20impacting%20general%0Aperformance.%20Our%20method%20is%20obtained%20through%20two-stage%20training.%20The%20low%0Acomputational%20cost%20of%20the%20first%20stage%20brings%20very%20effective%20performance%0Aimprovement%2C%20and%20the%20fine-tuning%20of%20the%20language%20model%20in%20the%20second%20stage%0Afurther%20improves%20the%20safety%20performance.%20Our%20method%20achieves%20state-of-the-art%0Aresults%20on%20popular%20VLM%20safety%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11543v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSA-VLM%253A%2520Enhancing%2520Vision-Language%2520Model%2520Safety%2520through%2520Progressive%250A%2520%2520Concept-Bottleneck-Driven%2520Alignment%26entry.906535625%3DZhendong%2520Liu%2520and%2520Yuanbi%2520Nie%2520and%2520Yingshui%2520Tan%2520and%2520Jiaheng%2520Liu%2520and%2520Xiangyu%2520Yue%2520and%2520Qiushi%2520Cui%2520and%2520Chongjun%2520Wang%2520and%2520Xiaoyong%2520Zhu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Benefiting%2520from%2520the%2520powerful%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Apre-trained%2520visual%2520encoder%2520models%2520connected%2520to%2520LLMs%2520form%2520Vision%2520Language%2520Models%250A%2528VLMs%2529.%2520However%252C%2520recent%2520research%2520shows%2520that%2520the%2520visual%2520modality%2520in%2520VLMs%2520is%250Ahighly%2520vulnerable%252C%2520allowing%2520attackers%2520to%2520bypass%2520safety%2520alignment%2520in%2520LLMs%250Athrough%2520visually%2520transmitted%2520content%252C%2520launching%2520harmful%2520attacks.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520progressive%2520concept-based%2520alignment%2520strategy%252C%250APSA-VLM%252C%2520which%2520incorporates%2520safety%2520modules%2520as%2520concept%2520bottlenecks%2520to%2520enhance%250Avisual%2520modality%2520safety%2520alignment.%2520By%2520aligning%2520model%2520predictions%2520with%2520specific%250Asafety%2520concepts%252C%2520we%2520improve%2520defenses%2520against%2520risky%2520images%252C%2520enhancing%250Aexplainability%2520and%2520controllability%2520while%2520minimally%2520impacting%2520general%250Aperformance.%2520Our%2520method%2520is%2520obtained%2520through%2520two-stage%2520training.%2520The%2520low%250Acomputational%2520cost%2520of%2520the%2520first%2520stage%2520brings%2520very%2520effective%2520performance%250Aimprovement%252C%2520and%2520the%2520fine-tuning%2520of%2520the%2520language%2520model%2520in%2520the%2520second%2520stage%250Afurther%2520improves%2520the%2520safety%2520performance.%2520Our%2520method%2520achieves%2520state-of-the-art%250Aresults%2520on%2520popular%2520VLM%2520safety%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11543v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSA-VLM%3A%20Enhancing%20Vision-Language%20Model%20Safety%20through%20Progressive%0A%20%20Concept-Bottleneck-Driven%20Alignment&entry.906535625=Zhendong%20Liu%20and%20Yuanbi%20Nie%20and%20Yingshui%20Tan%20and%20Jiaheng%20Liu%20and%20Xiangyu%20Yue%20and%20Qiushi%20Cui%20and%20Chongjun%20Wang%20and%20Xiaoyong%20Zhu%20and%20Bo%20Zheng&entry.1292438233=%20%20Benefiting%20from%20the%20powerful%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%2C%0Apre-trained%20visual%20encoder%20models%20connected%20to%20LLMs%20form%20Vision%20Language%20Models%0A%28VLMs%29.%20However%2C%20recent%20research%20shows%20that%20the%20visual%20modality%20in%20VLMs%20is%0Ahighly%20vulnerable%2C%20allowing%20attackers%20to%20bypass%20safety%20alignment%20in%20LLMs%0Athrough%20visually%20transmitted%20content%2C%20launching%20harmful%20attacks.%20To%20address%0Athis%20challenge%2C%20we%20propose%20a%20progressive%20concept-based%20alignment%20strategy%2C%0APSA-VLM%2C%20which%20incorporates%20safety%20modules%20as%20concept%20bottlenecks%20to%20enhance%0Avisual%20modality%20safety%20alignment.%20By%20aligning%20model%20predictions%20with%20specific%0Asafety%20concepts%2C%20we%20improve%20defenses%20against%20risky%20images%2C%20enhancing%0Aexplainability%20and%20controllability%20while%20minimally%20impacting%20general%0Aperformance.%20Our%20method%20is%20obtained%20through%20two-stage%20training.%20The%20low%0Acomputational%20cost%20of%20the%20first%20stage%20brings%20very%20effective%20performance%0Aimprovement%2C%20and%20the%20fine-tuning%20of%20the%20language%20model%20in%20the%20second%20stage%0Afurther%20improves%20the%20safety%20performance.%20Our%20method%20achieves%20state-of-the-art%0Aresults%20on%20popular%20VLM%20safety%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11543v4&entry.124074799=Read"},
{"title": "Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling", "author": "Zhe Chen and Weiyun Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Erfei Cui and Jinguo Zhu and Shenglong Ye and Hao Tian and Zhaoyang Liu and Lixin Gu and Xuehui Wang and Qingyun Li and Yimin Ren and Zixuan Chen and Jiapeng Luo and Jiahao Wang and Tan Jiang and Bo Wang and Conghui He and Botian Shi and Xingcheng Zhang and Han Lv and Yi Wang and Wenqi Shao and Pei Chu and Zhongying Tu and Tong He and Zhiyong Wu and Huipeng Deng and Jiaye Ge and Kai Chen and Kaipeng Zhang and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang", "abstract": "  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n", "link": "http://arxiv.org/abs/2412.05271v4", "date": "2025-01-13", "relevancy": 2.7766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling&body=Title%3A%20Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling%0AAuthor%3A%20Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jinguo%20Zhu%20and%20Shenglong%20Ye%20and%20Hao%20Tian%20and%20Zhaoyang%20Liu%20and%20Lixin%20Gu%20and%20Xuehui%20Wang%20and%20Qingyun%20Li%20and%20Yimin%20Ren%20and%20Zixuan%20Chen%20and%20Jiapeng%20Luo%20and%20Jiahao%20Wang%20and%20Tan%20Jiang%20and%20Bo%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Xingcheng%20Zhang%20and%20Han%20Lv%20and%20Yi%20Wang%20and%20Wenqi%20Shao%20and%20Pei%20Chu%20and%20Zhongying%20Tu%20and%20Tong%20He%20and%20Zhiyong%20Wu%20and%20Huipeng%20Deng%20and%20Jiaye%20Ge%20and%20Kai%20Chen%20and%20Kaipeng%20Zhang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang%0AAbstract%3A%20%20%20We%20introduce%20InternVL%202.5%2C%20an%20advanced%20multimodal%20large%20language%20model%20%28MLLM%29%0Aseries%20that%20builds%20upon%20InternVL%202.0%2C%20maintaining%20its%20core%20model%20architecture%0Awhile%20introducing%20significant%20enhancements%20in%20training%20and%20testing%20strategies%0Aas%20well%20as%20data%20quality.%20In%20this%20work%2C%20we%20delve%20into%20the%20relationship%20between%0Amodel%20scaling%20and%20performance%2C%20systematically%20exploring%20the%20performance%20trends%0Ain%20vision%20encoders%2C%20language%20models%2C%20dataset%20sizes%2C%20and%20test-time%0Aconfigurations.%20Through%20extensive%20evaluations%20on%20a%20wide%20range%20of%20benchmarks%2C%0Aincluding%20multi-discipline%20reasoning%2C%20document%20understanding%2C%20multi-image%20/%0Avideo%20understanding%2C%20real-world%20comprehension%2C%20multimodal%20hallucination%0Adetection%2C%20visual%20grounding%2C%20multilingual%20capabilities%2C%20and%20pure%20language%0Aprocessing%2C%20InternVL%202.5%20exhibits%20competitive%20performance%2C%20rivaling%20leading%0Acommercial%20models%20such%20as%20GPT-4o%20and%20Claude-3.5-Sonnet.%20Notably%2C%20our%20model%20is%0Athe%20first%20open-source%20MLLMs%20to%20surpass%2070%25%20on%20the%20MMMU%20benchmark%2C%20achieving%20a%0A3.7-point%20improvement%20through%20Chain-of-Thought%20%28CoT%29%20reasoning%20and%20showcasing%0Astrong%20potential%20for%20test-time%20scaling.%20We%20hope%20this%20model%20contributes%20to%20the%0Aopen-source%20community%20by%20setting%20new%20standards%20for%20developing%20and%20applying%0Amultimodal%20AI%20systems.%20HuggingFace%20demo%20see%0Ahttps%3A//huggingface.co/spaces/OpenGVLab/InternVL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05271v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpanding%2520Performance%2520Boundaries%2520of%2520Open-Source%2520Multimodal%2520Models%2520with%250A%2520%2520Model%252C%2520Data%252C%2520and%2520Test-Time%2520Scaling%26entry.906535625%3DZhe%2520Chen%2520and%2520Weiyun%2520Wang%2520and%2520Yue%2520Cao%2520and%2520Yangzhou%2520Liu%2520and%2520Zhangwei%2520Gao%2520and%2520Erfei%2520Cui%2520and%2520Jinguo%2520Zhu%2520and%2520Shenglong%2520Ye%2520and%2520Hao%2520Tian%2520and%2520Zhaoyang%2520Liu%2520and%2520Lixin%2520Gu%2520and%2520Xuehui%2520Wang%2520and%2520Qingyun%2520Li%2520and%2520Yimin%2520Ren%2520and%2520Zixuan%2520Chen%2520and%2520Jiapeng%2520Luo%2520and%2520Jiahao%2520Wang%2520and%2520Tan%2520Jiang%2520and%2520Bo%2520Wang%2520and%2520Conghui%2520He%2520and%2520Botian%2520Shi%2520and%2520Xingcheng%2520Zhang%2520and%2520Han%2520Lv%2520and%2520Yi%2520Wang%2520and%2520Wenqi%2520Shao%2520and%2520Pei%2520Chu%2520and%2520Zhongying%2520Tu%2520and%2520Tong%2520He%2520and%2520Zhiyong%2520Wu%2520and%2520Huipeng%2520Deng%2520and%2520Jiaye%2520Ge%2520and%2520Kai%2520Chen%2520and%2520Kaipeng%2520Zhang%2520and%2520Limin%2520Wang%2520and%2520Min%2520Dou%2520and%2520Lewei%2520Lu%2520and%2520Xizhou%2520Zhu%2520and%2520Tong%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%2520and%2520Wenhai%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520InternVL%25202.5%252C%2520an%2520advanced%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%250Aseries%2520that%2520builds%2520upon%2520InternVL%25202.0%252C%2520maintaining%2520its%2520core%2520model%2520architecture%250Awhile%2520introducing%2520significant%2520enhancements%2520in%2520training%2520and%2520testing%2520strategies%250Aas%2520well%2520as%2520data%2520quality.%2520In%2520this%2520work%252C%2520we%2520delve%2520into%2520the%2520relationship%2520between%250Amodel%2520scaling%2520and%2520performance%252C%2520systematically%2520exploring%2520the%2520performance%2520trends%250Ain%2520vision%2520encoders%252C%2520language%2520models%252C%2520dataset%2520sizes%252C%2520and%2520test-time%250Aconfigurations.%2520Through%2520extensive%2520evaluations%2520on%2520a%2520wide%2520range%2520of%2520benchmarks%252C%250Aincluding%2520multi-discipline%2520reasoning%252C%2520document%2520understanding%252C%2520multi-image%2520/%250Avideo%2520understanding%252C%2520real-world%2520comprehension%252C%2520multimodal%2520hallucination%250Adetection%252C%2520visual%2520grounding%252C%2520multilingual%2520capabilities%252C%2520and%2520pure%2520language%250Aprocessing%252C%2520InternVL%25202.5%2520exhibits%2520competitive%2520performance%252C%2520rivaling%2520leading%250Acommercial%2520models%2520such%2520as%2520GPT-4o%2520and%2520Claude-3.5-Sonnet.%2520Notably%252C%2520our%2520model%2520is%250Athe%2520first%2520open-source%2520MLLMs%2520to%2520surpass%252070%2525%2520on%2520the%2520MMMU%2520benchmark%252C%2520achieving%2520a%250A3.7-point%2520improvement%2520through%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520and%2520showcasing%250Astrong%2520potential%2520for%2520test-time%2520scaling.%2520We%2520hope%2520this%2520model%2520contributes%2520to%2520the%250Aopen-source%2520community%2520by%2520setting%2520new%2520standards%2520for%2520developing%2520and%2520applying%250Amultimodal%2520AI%2520systems.%2520HuggingFace%2520demo%2520see%250Ahttps%253A//huggingface.co/spaces/OpenGVLab/InternVL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05271v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling&entry.906535625=Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jinguo%20Zhu%20and%20Shenglong%20Ye%20and%20Hao%20Tian%20and%20Zhaoyang%20Liu%20and%20Lixin%20Gu%20and%20Xuehui%20Wang%20and%20Qingyun%20Li%20and%20Yimin%20Ren%20and%20Zixuan%20Chen%20and%20Jiapeng%20Luo%20and%20Jiahao%20Wang%20and%20Tan%20Jiang%20and%20Bo%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Xingcheng%20Zhang%20and%20Han%20Lv%20and%20Yi%20Wang%20and%20Wenqi%20Shao%20and%20Pei%20Chu%20and%20Zhongying%20Tu%20and%20Tong%20He%20and%20Zhiyong%20Wu%20and%20Huipeng%20Deng%20and%20Jiaye%20Ge%20and%20Kai%20Chen%20and%20Kaipeng%20Zhang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang&entry.1292438233=%20%20We%20introduce%20InternVL%202.5%2C%20an%20advanced%20multimodal%20large%20language%20model%20%28MLLM%29%0Aseries%20that%20builds%20upon%20InternVL%202.0%2C%20maintaining%20its%20core%20model%20architecture%0Awhile%20introducing%20significant%20enhancements%20in%20training%20and%20testing%20strategies%0Aas%20well%20as%20data%20quality.%20In%20this%20work%2C%20we%20delve%20into%20the%20relationship%20between%0Amodel%20scaling%20and%20performance%2C%20systematically%20exploring%20the%20performance%20trends%0Ain%20vision%20encoders%2C%20language%20models%2C%20dataset%20sizes%2C%20and%20test-time%0Aconfigurations.%20Through%20extensive%20evaluations%20on%20a%20wide%20range%20of%20benchmarks%2C%0Aincluding%20multi-discipline%20reasoning%2C%20document%20understanding%2C%20multi-image%20/%0Avideo%20understanding%2C%20real-world%20comprehension%2C%20multimodal%20hallucination%0Adetection%2C%20visual%20grounding%2C%20multilingual%20capabilities%2C%20and%20pure%20language%0Aprocessing%2C%20InternVL%202.5%20exhibits%20competitive%20performance%2C%20rivaling%20leading%0Acommercial%20models%20such%20as%20GPT-4o%20and%20Claude-3.5-Sonnet.%20Notably%2C%20our%20model%20is%0Athe%20first%20open-source%20MLLMs%20to%20surpass%2070%25%20on%20the%20MMMU%20benchmark%2C%20achieving%20a%0A3.7-point%20improvement%20through%20Chain-of-Thought%20%28CoT%29%20reasoning%20and%20showcasing%0Astrong%20potential%20for%20test-time%20scaling.%20We%20hope%20this%20model%20contributes%20to%20the%0Aopen-source%20community%20by%20setting%20new%20standards%20for%20developing%20and%20applying%0Amultimodal%20AI%20systems.%20HuggingFace%20demo%20see%0Ahttps%3A//huggingface.co/spaces/OpenGVLab/InternVL%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05271v4&entry.124074799=Read"},
{"title": "IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion", "author": "Tharun Anand and Aryan Garg and Kaushik Mitra", "abstract": "  Facial video editing has become increasingly important for content creators,\nenabling the manipulation of facial expressions and attributes. However,\nexisting models encounter challenges such as poor editing quality, high\ncomputational costs and difficulties in preserving facial identity across\ndiverse edits. Additionally, these models are often constrained to editing\npredefined facial attributes, limiting their flexibility to diverse editing\nprompts. To address these challenges, we propose a novel facial video editing\nframework that leverages the rich latent space of pre-trained text-to-image\n(T2I) diffusion models and fine-tune them specifically for facial video editing\ntasks. Our approach introduces a targeted fine-tuning scheme that enables high\nquality, localized, text-driven edits while ensuring identity preservation\nacross video frames. Additionally, by using pre-trained T2I models during\ninference, our approach significantly reduces editing time by 80%, while\nmaintaining temporal consistency throughout the video sequence. We evaluate the\neffectiveness of our approach through extensive testing across a wide range of\nchallenging scenarios, including varying head poses, complex action sequences,\nand diverse facial expressions. Our method consistently outperforms existing\ntechniques, demonstrating superior performance across a broad set of metrics\nand benchmarks.\n", "link": "http://arxiv.org/abs/2501.07530v1", "date": "2025-01-13", "relevancy": 2.7329, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.7202}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7195}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IP-FaceDiff%3A%20Identity-Preserving%20Facial%20Video%20Editing%20with%20Diffusion&body=Title%3A%20IP-FaceDiff%3A%20Identity-Preserving%20Facial%20Video%20Editing%20with%20Diffusion%0AAuthor%3A%20Tharun%20Anand%20and%20Aryan%20Garg%20and%20Kaushik%20Mitra%0AAbstract%3A%20%20%20Facial%20video%20editing%20has%20become%20increasingly%20important%20for%20content%20creators%2C%0Aenabling%20the%20manipulation%20of%20facial%20expressions%20and%20attributes.%20However%2C%0Aexisting%20models%20encounter%20challenges%20such%20as%20poor%20editing%20quality%2C%20high%0Acomputational%20costs%20and%20difficulties%20in%20preserving%20facial%20identity%20across%0Adiverse%20edits.%20Additionally%2C%20these%20models%20are%20often%20constrained%20to%20editing%0Apredefined%20facial%20attributes%2C%20limiting%20their%20flexibility%20to%20diverse%20editing%0Aprompts.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20facial%20video%20editing%0Aframework%20that%20leverages%20the%20rich%20latent%20space%20of%20pre-trained%20text-to-image%0A%28T2I%29%20diffusion%20models%20and%20fine-tune%20them%20specifically%20for%20facial%20video%20editing%0Atasks.%20Our%20approach%20introduces%20a%20targeted%20fine-tuning%20scheme%20that%20enables%20high%0Aquality%2C%20localized%2C%20text-driven%20edits%20while%20ensuring%20identity%20preservation%0Aacross%20video%20frames.%20Additionally%2C%20by%20using%20pre-trained%20T2I%20models%20during%0Ainference%2C%20our%20approach%20significantly%20reduces%20editing%20time%20by%2080%25%2C%20while%0Amaintaining%20temporal%20consistency%20throughout%20the%20video%20sequence.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20approach%20through%20extensive%20testing%20across%20a%20wide%20range%20of%0Achallenging%20scenarios%2C%20including%20varying%20head%20poses%2C%20complex%20action%20sequences%2C%0Aand%20diverse%20facial%20expressions.%20Our%20method%20consistently%20outperforms%20existing%0Atechniques%2C%20demonstrating%20superior%20performance%20across%20a%20broad%20set%20of%20metrics%0Aand%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIP-FaceDiff%253A%2520Identity-Preserving%2520Facial%2520Video%2520Editing%2520with%2520Diffusion%26entry.906535625%3DTharun%2520Anand%2520and%2520Aryan%2520Garg%2520and%2520Kaushik%2520Mitra%26entry.1292438233%3D%2520%2520Facial%2520video%2520editing%2520has%2520become%2520increasingly%2520important%2520for%2520content%2520creators%252C%250Aenabling%2520the%2520manipulation%2520of%2520facial%2520expressions%2520and%2520attributes.%2520However%252C%250Aexisting%2520models%2520encounter%2520challenges%2520such%2520as%2520poor%2520editing%2520quality%252C%2520high%250Acomputational%2520costs%2520and%2520difficulties%2520in%2520preserving%2520facial%2520identity%2520across%250Adiverse%2520edits.%2520Additionally%252C%2520these%2520models%2520are%2520often%2520constrained%2520to%2520editing%250Apredefined%2520facial%2520attributes%252C%2520limiting%2520their%2520flexibility%2520to%2520diverse%2520editing%250Aprompts.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520facial%2520video%2520editing%250Aframework%2520that%2520leverages%2520the%2520rich%2520latent%2520space%2520of%2520pre-trained%2520text-to-image%250A%2528T2I%2529%2520diffusion%2520models%2520and%2520fine-tune%2520them%2520specifically%2520for%2520facial%2520video%2520editing%250Atasks.%2520Our%2520approach%2520introduces%2520a%2520targeted%2520fine-tuning%2520scheme%2520that%2520enables%2520high%250Aquality%252C%2520localized%252C%2520text-driven%2520edits%2520while%2520ensuring%2520identity%2520preservation%250Aacross%2520video%2520frames.%2520Additionally%252C%2520by%2520using%2520pre-trained%2520T2I%2520models%2520during%250Ainference%252C%2520our%2520approach%2520significantly%2520reduces%2520editing%2520time%2520by%252080%2525%252C%2520while%250Amaintaining%2520temporal%2520consistency%2520throughout%2520the%2520video%2520sequence.%2520We%2520evaluate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520through%2520extensive%2520testing%2520across%2520a%2520wide%2520range%2520of%250Achallenging%2520scenarios%252C%2520including%2520varying%2520head%2520poses%252C%2520complex%2520action%2520sequences%252C%250Aand%2520diverse%2520facial%2520expressions.%2520Our%2520method%2520consistently%2520outperforms%2520existing%250Atechniques%252C%2520demonstrating%2520superior%2520performance%2520across%2520a%2520broad%2520set%2520of%2520metrics%250Aand%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IP-FaceDiff%3A%20Identity-Preserving%20Facial%20Video%20Editing%20with%20Diffusion&entry.906535625=Tharun%20Anand%20and%20Aryan%20Garg%20and%20Kaushik%20Mitra&entry.1292438233=%20%20Facial%20video%20editing%20has%20become%20increasingly%20important%20for%20content%20creators%2C%0Aenabling%20the%20manipulation%20of%20facial%20expressions%20and%20attributes.%20However%2C%0Aexisting%20models%20encounter%20challenges%20such%20as%20poor%20editing%20quality%2C%20high%0Acomputational%20costs%20and%20difficulties%20in%20preserving%20facial%20identity%20across%0Adiverse%20edits.%20Additionally%2C%20these%20models%20are%20often%20constrained%20to%20editing%0Apredefined%20facial%20attributes%2C%20limiting%20their%20flexibility%20to%20diverse%20editing%0Aprompts.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20facial%20video%20editing%0Aframework%20that%20leverages%20the%20rich%20latent%20space%20of%20pre-trained%20text-to-image%0A%28T2I%29%20diffusion%20models%20and%20fine-tune%20them%20specifically%20for%20facial%20video%20editing%0Atasks.%20Our%20approach%20introduces%20a%20targeted%20fine-tuning%20scheme%20that%20enables%20high%0Aquality%2C%20localized%2C%20text-driven%20edits%20while%20ensuring%20identity%20preservation%0Aacross%20video%20frames.%20Additionally%2C%20by%20using%20pre-trained%20T2I%20models%20during%0Ainference%2C%20our%20approach%20significantly%20reduces%20editing%20time%20by%2080%25%2C%20while%0Amaintaining%20temporal%20consistency%20throughout%20the%20video%20sequence.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20approach%20through%20extensive%20testing%20across%20a%20wide%20range%20of%0Achallenging%20scenarios%2C%20including%20varying%20head%20poses%2C%20complex%20action%20sequences%2C%0Aand%20diverse%20facial%20expressions.%20Our%20method%20consistently%20outperforms%20existing%0Atechniques%2C%20demonstrating%20superior%20performance%20across%20a%20broad%20set%20of%20metrics%0Aand%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07530v1&entry.124074799=Read"},
{"title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling", "author": "Jun Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong", "abstract": "  Transformer has achieved remarkable success in language, image, and speech\nprocessing. Recently, various efficient attention architectures have been\nproposed to improve transformer's efficiency while largely preserving its\nefficacy, especially in modeling long sequences. A widely-used benchmark to\ntest these efficient methods' capability on long-range modeling is Long Range\nArena (LRA). However, LRA only focuses on the standard bidirectional (or\nnoncausal) self attention, and completely ignores cross attentions and\nunidirectional (or causal) attentions, which are equally important to\ndownstream applications. In this paper, we propose Comprehensive Attention\nBenchmark (CAB) under a fine-grained attention taxonomy with four\ndistinguishable attention patterns, namely, noncausal self, causal self,\nnoncausal cross, and causal cross attentions. CAB collects seven real-world\ntasks from different research areas to evaluate efficient attentions under the\nfour attention patterns. Among these tasks, CAB validates efficient attentions\nin eight backbone networks to show their generalization across neural\narchitectures. We conduct exhaustive experiments to benchmark the performances\nof nine widely-used efficient attention architectures designed with different\nphilosophies on CAB. Extensive experimental results also shed light on the\nfundamental problems of efficient attentions, such as efficiency length against\nvanilla attention, performance consistency across attention patterns, the\nbenefit of attention mechanisms, and interpolation/extrapolation on\nlong-context language modeling.\n", "link": "http://arxiv.org/abs/2210.07661v4", "date": "2025-01-13", "relevancy": 2.7266, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAB%3A%20Comprehensive%20Attention%20Benchmarking%20on%20Long%20Sequence%20Modeling&body=Title%3A%20CAB%3A%20Comprehensive%20Attention%20Benchmarking%20on%20Long%20Sequence%20Modeling%0AAuthor%3A%20Jun%20Zhang%20and%20Shuyang%20Jiang%20and%20Jiangtao%20Feng%20and%20Lin%20Zheng%20and%20Lingpeng%20Kong%0AAbstract%3A%20%20%20Transformer%20has%20achieved%20remarkable%20success%20in%20language%2C%20image%2C%20and%20speech%0Aprocessing.%20Recently%2C%20various%20efficient%20attention%20architectures%20have%20been%0Aproposed%20to%20improve%20transformer%27s%20efficiency%20while%20largely%20preserving%20its%0Aefficacy%2C%20especially%20in%20modeling%20long%20sequences.%20A%20widely-used%20benchmark%20to%0Atest%20these%20efficient%20methods%27%20capability%20on%20long-range%20modeling%20is%20Long%20Range%0AArena%20%28LRA%29.%20However%2C%20LRA%20only%20focuses%20on%20the%20standard%20bidirectional%20%28or%0Anoncausal%29%20self%20attention%2C%20and%20completely%20ignores%20cross%20attentions%20and%0Aunidirectional%20%28or%20causal%29%20attentions%2C%20which%20are%20equally%20important%20to%0Adownstream%20applications.%20In%20this%20paper%2C%20we%20propose%20Comprehensive%20Attention%0ABenchmark%20%28CAB%29%20under%20a%20fine-grained%20attention%20taxonomy%20with%20four%0Adistinguishable%20attention%20patterns%2C%20namely%2C%20noncausal%20self%2C%20causal%20self%2C%0Anoncausal%20cross%2C%20and%20causal%20cross%20attentions.%20CAB%20collects%20seven%20real-world%0Atasks%20from%20different%20research%20areas%20to%20evaluate%20efficient%20attentions%20under%20the%0Afour%20attention%20patterns.%20Among%20these%20tasks%2C%20CAB%20validates%20efficient%20attentions%0Ain%20eight%20backbone%20networks%20to%20show%20their%20generalization%20across%20neural%0Aarchitectures.%20We%20conduct%20exhaustive%20experiments%20to%20benchmark%20the%20performances%0Aof%20nine%20widely-used%20efficient%20attention%20architectures%20designed%20with%20different%0Aphilosophies%20on%20CAB.%20Extensive%20experimental%20results%20also%20shed%20light%20on%20the%0Afundamental%20problems%20of%20efficient%20attentions%2C%20such%20as%20efficiency%20length%20against%0Avanilla%20attention%2C%20performance%20consistency%20across%20attention%20patterns%2C%20the%0Abenefit%20of%20attention%20mechanisms%2C%20and%20interpolation/extrapolation%20on%0Along-context%20language%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.07661v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAB%253A%2520Comprehensive%2520Attention%2520Benchmarking%2520on%2520Long%2520Sequence%2520Modeling%26entry.906535625%3DJun%2520Zhang%2520and%2520Shuyang%2520Jiang%2520and%2520Jiangtao%2520Feng%2520and%2520Lin%2520Zheng%2520and%2520Lingpeng%2520Kong%26entry.1292438233%3D%2520%2520Transformer%2520has%2520achieved%2520remarkable%2520success%2520in%2520language%252C%2520image%252C%2520and%2520speech%250Aprocessing.%2520Recently%252C%2520various%2520efficient%2520attention%2520architectures%2520have%2520been%250Aproposed%2520to%2520improve%2520transformer%2527s%2520efficiency%2520while%2520largely%2520preserving%2520its%250Aefficacy%252C%2520especially%2520in%2520modeling%2520long%2520sequences.%2520A%2520widely-used%2520benchmark%2520to%250Atest%2520these%2520efficient%2520methods%2527%2520capability%2520on%2520long-range%2520modeling%2520is%2520Long%2520Range%250AArena%2520%2528LRA%2529.%2520However%252C%2520LRA%2520only%2520focuses%2520on%2520the%2520standard%2520bidirectional%2520%2528or%250Anoncausal%2529%2520self%2520attention%252C%2520and%2520completely%2520ignores%2520cross%2520attentions%2520and%250Aunidirectional%2520%2528or%2520causal%2529%2520attentions%252C%2520which%2520are%2520equally%2520important%2520to%250Adownstream%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Comprehensive%2520Attention%250ABenchmark%2520%2528CAB%2529%2520under%2520a%2520fine-grained%2520attention%2520taxonomy%2520with%2520four%250Adistinguishable%2520attention%2520patterns%252C%2520namely%252C%2520noncausal%2520self%252C%2520causal%2520self%252C%250Anoncausal%2520cross%252C%2520and%2520causal%2520cross%2520attentions.%2520CAB%2520collects%2520seven%2520real-world%250Atasks%2520from%2520different%2520research%2520areas%2520to%2520evaluate%2520efficient%2520attentions%2520under%2520the%250Afour%2520attention%2520patterns.%2520Among%2520these%2520tasks%252C%2520CAB%2520validates%2520efficient%2520attentions%250Ain%2520eight%2520backbone%2520networks%2520to%2520show%2520their%2520generalization%2520across%2520neural%250Aarchitectures.%2520We%2520conduct%2520exhaustive%2520experiments%2520to%2520benchmark%2520the%2520performances%250Aof%2520nine%2520widely-used%2520efficient%2520attention%2520architectures%2520designed%2520with%2520different%250Aphilosophies%2520on%2520CAB.%2520Extensive%2520experimental%2520results%2520also%2520shed%2520light%2520on%2520the%250Afundamental%2520problems%2520of%2520efficient%2520attentions%252C%2520such%2520as%2520efficiency%2520length%2520against%250Avanilla%2520attention%252C%2520performance%2520consistency%2520across%2520attention%2520patterns%252C%2520the%250Abenefit%2520of%2520attention%2520mechanisms%252C%2520and%2520interpolation/extrapolation%2520on%250Along-context%2520language%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.07661v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAB%3A%20Comprehensive%20Attention%20Benchmarking%20on%20Long%20Sequence%20Modeling&entry.906535625=Jun%20Zhang%20and%20Shuyang%20Jiang%20and%20Jiangtao%20Feng%20and%20Lin%20Zheng%20and%20Lingpeng%20Kong&entry.1292438233=%20%20Transformer%20has%20achieved%20remarkable%20success%20in%20language%2C%20image%2C%20and%20speech%0Aprocessing.%20Recently%2C%20various%20efficient%20attention%20architectures%20have%20been%0Aproposed%20to%20improve%20transformer%27s%20efficiency%20while%20largely%20preserving%20its%0Aefficacy%2C%20especially%20in%20modeling%20long%20sequences.%20A%20widely-used%20benchmark%20to%0Atest%20these%20efficient%20methods%27%20capability%20on%20long-range%20modeling%20is%20Long%20Range%0AArena%20%28LRA%29.%20However%2C%20LRA%20only%20focuses%20on%20the%20standard%20bidirectional%20%28or%0Anoncausal%29%20self%20attention%2C%20and%20completely%20ignores%20cross%20attentions%20and%0Aunidirectional%20%28or%20causal%29%20attentions%2C%20which%20are%20equally%20important%20to%0Adownstream%20applications.%20In%20this%20paper%2C%20we%20propose%20Comprehensive%20Attention%0ABenchmark%20%28CAB%29%20under%20a%20fine-grained%20attention%20taxonomy%20with%20four%0Adistinguishable%20attention%20patterns%2C%20namely%2C%20noncausal%20self%2C%20causal%20self%2C%0Anoncausal%20cross%2C%20and%20causal%20cross%20attentions.%20CAB%20collects%20seven%20real-world%0Atasks%20from%20different%20research%20areas%20to%20evaluate%20efficient%20attentions%20under%20the%0Afour%20attention%20patterns.%20Among%20these%20tasks%2C%20CAB%20validates%20efficient%20attentions%0Ain%20eight%20backbone%20networks%20to%20show%20their%20generalization%20across%20neural%0Aarchitectures.%20We%20conduct%20exhaustive%20experiments%20to%20benchmark%20the%20performances%0Aof%20nine%20widely-used%20efficient%20attention%20architectures%20designed%20with%20different%0Aphilosophies%20on%20CAB.%20Extensive%20experimental%20results%20also%20shed%20light%20on%20the%0Afundamental%20problems%20of%20efficient%20attentions%2C%20such%20as%20efficiency%20length%20against%0Avanilla%20attention%2C%20performance%20consistency%20across%20attention%20patterns%2C%20the%0Abenefit%20of%20attention%20mechanisms%2C%20and%20interpolation/extrapolation%20on%0Along-context%20language%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.07661v4&entry.124074799=Read"},
{"title": "ScVLM: Enhancing Vision-Language Model for Safety-Critical Event\n  Understanding", "author": "Liang Shi and Boyu Jiang and Tong Zeng and Feng Guo", "abstract": "  Accurately identifying, understanding and describing traffic safety-critical\nevents (SCEs), including crashes, tire strikes, and near-crashes, is crucial\nfor advanced driver assistance systems, automated driving systems, and traffic\nsafety. As SCEs are rare events, most general vision-language models (VLMs)\nhave not been trained sufficiently to link SCE videos and narratives, which\ncould lead to hallucinations and missing key safety characteristics. Here, we\nintroduce ScVLM, a novel hybrid methodology that integrates supervised and\ncontrastive learning techniques to classify the severity and types of SCEs, as\nwell as to generate narrative descriptions of SCEs. This approach utilizes\nclassification to enhance VLMs' comprehension of driving videos and improve the\nrationality of event descriptions. The proposed approach is trained on and\nevaluated by more than 8,600 SCEs from the Second Strategic Highway Research\nProgram Naturalistic Driving Study dataset, the largest publicly accessible\ndriving dataset with videos and SCE annotations. The results demonstrate the\nsuperiority of the proposed approach in generating contextually accurate event\ndescriptions and mitigating VLM hallucinations. The code will be available at\nhttps://github.com/datadrivenwheels/ScVLM.\n", "link": "http://arxiv.org/abs/2410.00982v2", "date": "2025-01-13", "relevancy": 2.7169, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScVLM%3A%20Enhancing%20Vision-Language%20Model%20for%20Safety-Critical%20Event%0A%20%20Understanding&body=Title%3A%20ScVLM%3A%20Enhancing%20Vision-Language%20Model%20for%20Safety-Critical%20Event%0A%20%20Understanding%0AAuthor%3A%20Liang%20Shi%20and%20Boyu%20Jiang%20and%20Tong%20Zeng%20and%20Feng%20Guo%0AAbstract%3A%20%20%20Accurately%20identifying%2C%20understanding%20and%20describing%20traffic%20safety-critical%0Aevents%20%28SCEs%29%2C%20including%20crashes%2C%20tire%20strikes%2C%20and%20near-crashes%2C%20is%20crucial%0Afor%20advanced%20driver%20assistance%20systems%2C%20automated%20driving%20systems%2C%20and%20traffic%0Asafety.%20As%20SCEs%20are%20rare%20events%2C%20most%20general%20vision-language%20models%20%28VLMs%29%0Ahave%20not%20been%20trained%20sufficiently%20to%20link%20SCE%20videos%20and%20narratives%2C%20which%0Acould%20lead%20to%20hallucinations%20and%20missing%20key%20safety%20characteristics.%20Here%2C%20we%0Aintroduce%20ScVLM%2C%20a%20novel%20hybrid%20methodology%20that%20integrates%20supervised%20and%0Acontrastive%20learning%20techniques%20to%20classify%20the%20severity%20and%20types%20of%20SCEs%2C%20as%0Awell%20as%20to%20generate%20narrative%20descriptions%20of%20SCEs.%20This%20approach%20utilizes%0Aclassification%20to%20enhance%20VLMs%27%20comprehension%20of%20driving%20videos%20and%20improve%20the%0Arationality%20of%20event%20descriptions.%20The%20proposed%20approach%20is%20trained%20on%20and%0Aevaluated%20by%20more%20than%208%2C600%20SCEs%20from%20the%20Second%20Strategic%20Highway%20Research%0AProgram%20Naturalistic%20Driving%20Study%20dataset%2C%20the%20largest%20publicly%20accessible%0Adriving%20dataset%20with%20videos%20and%20SCE%20annotations.%20The%20results%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20approach%20in%20generating%20contextually%20accurate%20event%0Adescriptions%20and%20mitigating%20VLM%20hallucinations.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/datadrivenwheels/ScVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScVLM%253A%2520Enhancing%2520Vision-Language%2520Model%2520for%2520Safety-Critical%2520Event%250A%2520%2520Understanding%26entry.906535625%3DLiang%2520Shi%2520and%2520Boyu%2520Jiang%2520and%2520Tong%2520Zeng%2520and%2520Feng%2520Guo%26entry.1292438233%3D%2520%2520Accurately%2520identifying%252C%2520understanding%2520and%2520describing%2520traffic%2520safety-critical%250Aevents%2520%2528SCEs%2529%252C%2520including%2520crashes%252C%2520tire%2520strikes%252C%2520and%2520near-crashes%252C%2520is%2520crucial%250Afor%2520advanced%2520driver%2520assistance%2520systems%252C%2520automated%2520driving%2520systems%252C%2520and%2520traffic%250Asafety.%2520As%2520SCEs%2520are%2520rare%2520events%252C%2520most%2520general%2520vision-language%2520models%2520%2528VLMs%2529%250Ahave%2520not%2520been%2520trained%2520sufficiently%2520to%2520link%2520SCE%2520videos%2520and%2520narratives%252C%2520which%250Acould%2520lead%2520to%2520hallucinations%2520and%2520missing%2520key%2520safety%2520characteristics.%2520Here%252C%2520we%250Aintroduce%2520ScVLM%252C%2520a%2520novel%2520hybrid%2520methodology%2520that%2520integrates%2520supervised%2520and%250Acontrastive%2520learning%2520techniques%2520to%2520classify%2520the%2520severity%2520and%2520types%2520of%2520SCEs%252C%2520as%250Awell%2520as%2520to%2520generate%2520narrative%2520descriptions%2520of%2520SCEs.%2520This%2520approach%2520utilizes%250Aclassification%2520to%2520enhance%2520VLMs%2527%2520comprehension%2520of%2520driving%2520videos%2520and%2520improve%2520the%250Arationality%2520of%2520event%2520descriptions.%2520The%2520proposed%2520approach%2520is%2520trained%2520on%2520and%250Aevaluated%2520by%2520more%2520than%25208%252C600%2520SCEs%2520from%2520the%2520Second%2520Strategic%2520Highway%2520Research%250AProgram%2520Naturalistic%2520Driving%2520Study%2520dataset%252C%2520the%2520largest%2520publicly%2520accessible%250Adriving%2520dataset%2520with%2520videos%2520and%2520SCE%2520annotations.%2520The%2520results%2520demonstrate%2520the%250Asuperiority%2520of%2520the%2520proposed%2520approach%2520in%2520generating%2520contextually%2520accurate%2520event%250Adescriptions%2520and%2520mitigating%2520VLM%2520hallucinations.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/datadrivenwheels/ScVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScVLM%3A%20Enhancing%20Vision-Language%20Model%20for%20Safety-Critical%20Event%0A%20%20Understanding&entry.906535625=Liang%20Shi%20and%20Boyu%20Jiang%20and%20Tong%20Zeng%20and%20Feng%20Guo&entry.1292438233=%20%20Accurately%20identifying%2C%20understanding%20and%20describing%20traffic%20safety-critical%0Aevents%20%28SCEs%29%2C%20including%20crashes%2C%20tire%20strikes%2C%20and%20near-crashes%2C%20is%20crucial%0Afor%20advanced%20driver%20assistance%20systems%2C%20automated%20driving%20systems%2C%20and%20traffic%0Asafety.%20As%20SCEs%20are%20rare%20events%2C%20most%20general%20vision-language%20models%20%28VLMs%29%0Ahave%20not%20been%20trained%20sufficiently%20to%20link%20SCE%20videos%20and%20narratives%2C%20which%0Acould%20lead%20to%20hallucinations%20and%20missing%20key%20safety%20characteristics.%20Here%2C%20we%0Aintroduce%20ScVLM%2C%20a%20novel%20hybrid%20methodology%20that%20integrates%20supervised%20and%0Acontrastive%20learning%20techniques%20to%20classify%20the%20severity%20and%20types%20of%20SCEs%2C%20as%0Awell%20as%20to%20generate%20narrative%20descriptions%20of%20SCEs.%20This%20approach%20utilizes%0Aclassification%20to%20enhance%20VLMs%27%20comprehension%20of%20driving%20videos%20and%20improve%20the%0Arationality%20of%20event%20descriptions.%20The%20proposed%20approach%20is%20trained%20on%20and%0Aevaluated%20by%20more%20than%208%2C600%20SCEs%20from%20the%20Second%20Strategic%20Highway%20Research%0AProgram%20Naturalistic%20Driving%20Study%20dataset%2C%20the%20largest%20publicly%20accessible%0Adriving%20dataset%20with%20videos%20and%20SCE%20annotations.%20The%20results%20demonstrate%20the%0Asuperiority%20of%20the%20proposed%20approach%20in%20generating%20contextually%20accurate%20event%0Adescriptions%20and%20mitigating%20VLM%20hallucinations.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/datadrivenwheels/ScVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00982v2&entry.124074799=Read"},
{"title": "Exploring the Use of Contrastive Language-Image Pre-Training for Human\n  Posture Classification: Insights from Yoga Pose Analysis", "author": "Andrzej D. Dobrzycki and Ana M. Bernardos and Luca Bergesio and Andrzej Pomirski and Daniel S\u00e1ez-Trigueros", "abstract": "  Accurate human posture classification in images and videos is crucial for\nautomated applications across various fields, including work safety, physical\nrehabilitation, sports training, or daily assisted living. Recently, multimodal\nlearning methods, such as Contrastive Language-Image Pretraining (CLIP), have\nadvanced significantly in jointly understanding images and text. This study\naims to assess the effectiveness of CLIP in classifying human postures,\nfocusing on its application in yoga. Despite the initial limitations of the\nzero-shot approach, applying transfer learning on 15,301 images (real and\nsynthetic) with 82 classes has shown promising results. The article describes\nthe full procedure for fine-tuning, including the choice for image description\nsyntax, models and hyperparameters adjustment. The fine-tuned CLIP model,\ntested on 3826 images, achieves an accuracy of over 85%, surpassing the current\nstate-of-the-art of previous works on the same dataset by approximately 6%, its\ntraining time being 3.5 times lower than what is needed to fine-tune a\nYOLOv8-based model. For more application-oriented scenarios, with smaller\ndatasets of six postures each, containing 1301 and 401 training images, the\nfine-tuned models attain an accuracy of 98.8% and 99.1%, respectively.\nFurthermore, our experiments indicate that training with as few as 20 images\nper pose can yield around 90% accuracy in a six-class dataset. This study\ndemonstrates that this multimodal technique can be effectively used for yoga\npose classification, and possibly for human posture classification, in general.\nAdditionally, CLIP inference time (around 7 ms) supports that the model can be\nintegrated into automated systems for posture evaluation, e.g., for developing\na real-time personal yoga assistant for performance assessment.\n", "link": "http://arxiv.org/abs/2501.07221v1", "date": "2025-01-13", "relevancy": 2.7005, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5743}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5327}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Use%20of%20Contrastive%20Language-Image%20Pre-Training%20for%20Human%0A%20%20Posture%20Classification%3A%20Insights%20from%20Yoga%20Pose%20Analysis&body=Title%3A%20Exploring%20the%20Use%20of%20Contrastive%20Language-Image%20Pre-Training%20for%20Human%0A%20%20Posture%20Classification%3A%20Insights%20from%20Yoga%20Pose%20Analysis%0AAuthor%3A%20Andrzej%20D.%20Dobrzycki%20and%20Ana%20M.%20Bernardos%20and%20Luca%20Bergesio%20and%20Andrzej%20Pomirski%20and%20Daniel%20S%C3%A1ez-Trigueros%0AAbstract%3A%20%20%20Accurate%20human%20posture%20classification%20in%20images%20and%20videos%20is%20crucial%20for%0Aautomated%20applications%20across%20various%20fields%2C%20including%20work%20safety%2C%20physical%0Arehabilitation%2C%20sports%20training%2C%20or%20daily%20assisted%20living.%20Recently%2C%20multimodal%0Alearning%20methods%2C%20such%20as%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%2C%20have%0Aadvanced%20significantly%20in%20jointly%20understanding%20images%20and%20text.%20This%20study%0Aaims%20to%20assess%20the%20effectiveness%20of%20CLIP%20in%20classifying%20human%20postures%2C%0Afocusing%20on%20its%20application%20in%20yoga.%20Despite%20the%20initial%20limitations%20of%20the%0Azero-shot%20approach%2C%20applying%20transfer%20learning%20on%2015%2C301%20images%20%28real%20and%0Asynthetic%29%20with%2082%20classes%20has%20shown%20promising%20results.%20The%20article%20describes%0Athe%20full%20procedure%20for%20fine-tuning%2C%20including%20the%20choice%20for%20image%20description%0Asyntax%2C%20models%20and%20hyperparameters%20adjustment.%20The%20fine-tuned%20CLIP%20model%2C%0Atested%20on%203826%20images%2C%20achieves%20an%20accuracy%20of%20over%2085%25%2C%20surpassing%20the%20current%0Astate-of-the-art%20of%20previous%20works%20on%20the%20same%20dataset%20by%20approximately%206%25%2C%20its%0Atraining%20time%20being%203.5%20times%20lower%20than%20what%20is%20needed%20to%20fine-tune%20a%0AYOLOv8-based%20model.%20For%20more%20application-oriented%20scenarios%2C%20with%20smaller%0Adatasets%20of%20six%20postures%20each%2C%20containing%201301%20and%20401%20training%20images%2C%20the%0Afine-tuned%20models%20attain%20an%20accuracy%20of%2098.8%25%20and%2099.1%25%2C%20respectively.%0AFurthermore%2C%20our%20experiments%20indicate%20that%20training%20with%20as%20few%20as%2020%20images%0Aper%20pose%20can%20yield%20around%2090%25%20accuracy%20in%20a%20six-class%20dataset.%20This%20study%0Ademonstrates%20that%20this%20multimodal%20technique%20can%20be%20effectively%20used%20for%20yoga%0Apose%20classification%2C%20and%20possibly%20for%20human%20posture%20classification%2C%20in%20general.%0AAdditionally%2C%20CLIP%20inference%20time%20%28around%207%20ms%29%20supports%20that%20the%20model%20can%20be%0Aintegrated%20into%20automated%20systems%20for%20posture%20evaluation%2C%20e.g.%2C%20for%20developing%0Aa%20real-time%20personal%20yoga%20assistant%20for%20performance%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Use%2520of%2520Contrastive%2520Language-Image%2520Pre-Training%2520for%2520Human%250A%2520%2520Posture%2520Classification%253A%2520Insights%2520from%2520Yoga%2520Pose%2520Analysis%26entry.906535625%3DAndrzej%2520D.%2520Dobrzycki%2520and%2520Ana%2520M.%2520Bernardos%2520and%2520Luca%2520Bergesio%2520and%2520Andrzej%2520Pomirski%2520and%2520Daniel%2520S%25C3%25A1ez-Trigueros%26entry.1292438233%3D%2520%2520Accurate%2520human%2520posture%2520classification%2520in%2520images%2520and%2520videos%2520is%2520crucial%2520for%250Aautomated%2520applications%2520across%2520various%2520fields%252C%2520including%2520work%2520safety%252C%2520physical%250Arehabilitation%252C%2520sports%2520training%252C%2520or%2520daily%2520assisted%2520living.%2520Recently%252C%2520multimodal%250Alearning%2520methods%252C%2520such%2520as%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%252C%2520have%250Aadvanced%2520significantly%2520in%2520jointly%2520understanding%2520images%2520and%2520text.%2520This%2520study%250Aaims%2520to%2520assess%2520the%2520effectiveness%2520of%2520CLIP%2520in%2520classifying%2520human%2520postures%252C%250Afocusing%2520on%2520its%2520application%2520in%2520yoga.%2520Despite%2520the%2520initial%2520limitations%2520of%2520the%250Azero-shot%2520approach%252C%2520applying%2520transfer%2520learning%2520on%252015%252C301%2520images%2520%2528real%2520and%250Asynthetic%2529%2520with%252082%2520classes%2520has%2520shown%2520promising%2520results.%2520The%2520article%2520describes%250Athe%2520full%2520procedure%2520for%2520fine-tuning%252C%2520including%2520the%2520choice%2520for%2520image%2520description%250Asyntax%252C%2520models%2520and%2520hyperparameters%2520adjustment.%2520The%2520fine-tuned%2520CLIP%2520model%252C%250Atested%2520on%25203826%2520images%252C%2520achieves%2520an%2520accuracy%2520of%2520over%252085%2525%252C%2520surpassing%2520the%2520current%250Astate-of-the-art%2520of%2520previous%2520works%2520on%2520the%2520same%2520dataset%2520by%2520approximately%25206%2525%252C%2520its%250Atraining%2520time%2520being%25203.5%2520times%2520lower%2520than%2520what%2520is%2520needed%2520to%2520fine-tune%2520a%250AYOLOv8-based%2520model.%2520For%2520more%2520application-oriented%2520scenarios%252C%2520with%2520smaller%250Adatasets%2520of%2520six%2520postures%2520each%252C%2520containing%25201301%2520and%2520401%2520training%2520images%252C%2520the%250Afine-tuned%2520models%2520attain%2520an%2520accuracy%2520of%252098.8%2525%2520and%252099.1%2525%252C%2520respectively.%250AFurthermore%252C%2520our%2520experiments%2520indicate%2520that%2520training%2520with%2520as%2520few%2520as%252020%2520images%250Aper%2520pose%2520can%2520yield%2520around%252090%2525%2520accuracy%2520in%2520a%2520six-class%2520dataset.%2520This%2520study%250Ademonstrates%2520that%2520this%2520multimodal%2520technique%2520can%2520be%2520effectively%2520used%2520for%2520yoga%250Apose%2520classification%252C%2520and%2520possibly%2520for%2520human%2520posture%2520classification%252C%2520in%2520general.%250AAdditionally%252C%2520CLIP%2520inference%2520time%2520%2528around%25207%2520ms%2529%2520supports%2520that%2520the%2520model%2520can%2520be%250Aintegrated%2520into%2520automated%2520systems%2520for%2520posture%2520evaluation%252C%2520e.g.%252C%2520for%2520developing%250Aa%2520real-time%2520personal%2520yoga%2520assistant%2520for%2520performance%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Use%20of%20Contrastive%20Language-Image%20Pre-Training%20for%20Human%0A%20%20Posture%20Classification%3A%20Insights%20from%20Yoga%20Pose%20Analysis&entry.906535625=Andrzej%20D.%20Dobrzycki%20and%20Ana%20M.%20Bernardos%20and%20Luca%20Bergesio%20and%20Andrzej%20Pomirski%20and%20Daniel%20S%C3%A1ez-Trigueros&entry.1292438233=%20%20Accurate%20human%20posture%20classification%20in%20images%20and%20videos%20is%20crucial%20for%0Aautomated%20applications%20across%20various%20fields%2C%20including%20work%20safety%2C%20physical%0Arehabilitation%2C%20sports%20training%2C%20or%20daily%20assisted%20living.%20Recently%2C%20multimodal%0Alearning%20methods%2C%20such%20as%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%2C%20have%0Aadvanced%20significantly%20in%20jointly%20understanding%20images%20and%20text.%20This%20study%0Aaims%20to%20assess%20the%20effectiveness%20of%20CLIP%20in%20classifying%20human%20postures%2C%0Afocusing%20on%20its%20application%20in%20yoga.%20Despite%20the%20initial%20limitations%20of%20the%0Azero-shot%20approach%2C%20applying%20transfer%20learning%20on%2015%2C301%20images%20%28real%20and%0Asynthetic%29%20with%2082%20classes%20has%20shown%20promising%20results.%20The%20article%20describes%0Athe%20full%20procedure%20for%20fine-tuning%2C%20including%20the%20choice%20for%20image%20description%0Asyntax%2C%20models%20and%20hyperparameters%20adjustment.%20The%20fine-tuned%20CLIP%20model%2C%0Atested%20on%203826%20images%2C%20achieves%20an%20accuracy%20of%20over%2085%25%2C%20surpassing%20the%20current%0Astate-of-the-art%20of%20previous%20works%20on%20the%20same%20dataset%20by%20approximately%206%25%2C%20its%0Atraining%20time%20being%203.5%20times%20lower%20than%20what%20is%20needed%20to%20fine-tune%20a%0AYOLOv8-based%20model.%20For%20more%20application-oriented%20scenarios%2C%20with%20smaller%0Adatasets%20of%20six%20postures%20each%2C%20containing%201301%20and%20401%20training%20images%2C%20the%0Afine-tuned%20models%20attain%20an%20accuracy%20of%2098.8%25%20and%2099.1%25%2C%20respectively.%0AFurthermore%2C%20our%20experiments%20indicate%20that%20training%20with%20as%20few%20as%2020%20images%0Aper%20pose%20can%20yield%20around%2090%25%20accuracy%20in%20a%20six-class%20dataset.%20This%20study%0Ademonstrates%20that%20this%20multimodal%20technique%20can%20be%20effectively%20used%20for%20yoga%0Apose%20classification%2C%20and%20possibly%20for%20human%20posture%20classification%2C%20in%20general.%0AAdditionally%2C%20CLIP%20inference%20time%20%28around%207%20ms%29%20supports%20that%20the%20model%20can%20be%0Aintegrated%20into%20automated%20systems%20for%20posture%20evaluation%2C%20e.g.%2C%20for%20developing%0Aa%20real-time%20personal%20yoga%20assistant%20for%20performance%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07221v1&entry.124074799=Read"},
{"title": "Text-Guided Coarse-to-Fine Fusion Network for Robust Remote Sensing\n  Visual Question Answering", "author": "Zhicheng Zhao and Changfu Zhou and Yu Zhang and Chenglong Li and Xiaoliang Ma and Jin Tang", "abstract": "  Remote Sensing Visual Question Answering (RSVQA) has gained significant\nresearch interest. However, current RSVQA methods are limited by the imaging\nmechanisms of optical sensors, particularly under challenging conditions such\nas cloud-covered and low-light scenarios. Given the all-time and all-weather\nimaging capabilities of Synthetic Aperture Radar (SAR), it is crucial to\ninvestigate the integration of optical-SAR images to improve RSVQA performance.\nIn this work, we propose a Text-guided Coarse-to-Fine Fusion Network (TGFNet),\nwhich leverages the semantic relationships between question text and\nmulti-source images to guide the network toward complementary fusion at the\nfeature level. Specifically, we develop a Text-guided Coarse-to-Fine Attention\nRefinement (CFAR) module to focus on key areas related to the question in\ncomplex remote sensing images. This module progressively directs attention from\nbroad areas to finer details through key region routing, enhancing the model's\nability to focus on relevant regions. Furthermore, we propose an Adaptive\nMulti-Expert Fusion (AMEF) module that dynamically integrates different\nexperts, enabling the adaptive fusion of optical and SAR features. In addition,\nwe create the first large-scale benchmark dataset for evaluating optical-SAR\nRSVQA methods, comprising 6,008 well-aligned optical-SAR image pairs and\n1,036,694 well-labeled question-answer pairs across 16 diverse question types,\nincluding complex relational reasoning questions. Extensive experiments on the\nproposed dataset demonstrate that our TGFNet effectively integrates\ncomplementary information between optical and SAR images, significantly\nimproving the model's performance in challenging scenarios. The dataset is\navailable at: https://github.com/mmic-lcl/.\n  Index Terms: Remote Sensing Visual Question Answering, Multi-source Data\nFusion, Multimodal, Remote Sensing, OPT-SAR.\n", "link": "http://arxiv.org/abs/2411.15770v2", "date": "2025-01-13", "relevancy": 2.6945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5406}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Guided%20Coarse-to-Fine%20Fusion%20Network%20for%20Robust%20Remote%20Sensing%0A%20%20Visual%20Question%20Answering&body=Title%3A%20Text-Guided%20Coarse-to-Fine%20Fusion%20Network%20for%20Robust%20Remote%20Sensing%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Zhicheng%20Zhao%20and%20Changfu%20Zhou%20and%20Yu%20Zhang%20and%20Chenglong%20Li%20and%20Xiaoliang%20Ma%20and%20Jin%20Tang%0AAbstract%3A%20%20%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RSVQA%29%20has%20gained%20significant%0Aresearch%20interest.%20However%2C%20current%20RSVQA%20methods%20are%20limited%20by%20the%20imaging%0Amechanisms%20of%20optical%20sensors%2C%20particularly%20under%20challenging%20conditions%20such%0Aas%20cloud-covered%20and%20low-light%20scenarios.%20Given%20the%20all-time%20and%20all-weather%0Aimaging%20capabilities%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%2C%20it%20is%20crucial%20to%0Ainvestigate%20the%20integration%20of%20optical-SAR%20images%20to%20improve%20RSVQA%20performance.%0AIn%20this%20work%2C%20we%20propose%20a%20Text-guided%20Coarse-to-Fine%20Fusion%20Network%20%28TGFNet%29%2C%0Awhich%20leverages%20the%20semantic%20relationships%20between%20question%20text%20and%0Amulti-source%20images%20to%20guide%20the%20network%20toward%20complementary%20fusion%20at%20the%0Afeature%20level.%20Specifically%2C%20we%20develop%20a%20Text-guided%20Coarse-to-Fine%20Attention%0ARefinement%20%28CFAR%29%20module%20to%20focus%20on%20key%20areas%20related%20to%20the%20question%20in%0Acomplex%20remote%20sensing%20images.%20This%20module%20progressively%20directs%20attention%20from%0Abroad%20areas%20to%20finer%20details%20through%20key%20region%20routing%2C%20enhancing%20the%20model%27s%0Aability%20to%20focus%20on%20relevant%20regions.%20Furthermore%2C%20we%20propose%20an%20Adaptive%0AMulti-Expert%20Fusion%20%28AMEF%29%20module%20that%20dynamically%20integrates%20different%0Aexperts%2C%20enabling%20the%20adaptive%20fusion%20of%20optical%20and%20SAR%20features.%20In%20addition%2C%0Awe%20create%20the%20first%20large-scale%20benchmark%20dataset%20for%20evaluating%20optical-SAR%0ARSVQA%20methods%2C%20comprising%206%2C008%20well-aligned%20optical-SAR%20image%20pairs%20and%0A1%2C036%2C694%20well-labeled%20question-answer%20pairs%20across%2016%20diverse%20question%20types%2C%0Aincluding%20complex%20relational%20reasoning%20questions.%20Extensive%20experiments%20on%20the%0Aproposed%20dataset%20demonstrate%20that%20our%20TGFNet%20effectively%20integrates%0Acomplementary%20information%20between%20optical%20and%20SAR%20images%2C%20significantly%0Aimproving%20the%20model%27s%20performance%20in%20challenging%20scenarios.%20The%20dataset%20is%0Aavailable%20at%3A%20https%3A//github.com/mmic-lcl/.%0A%20%20Index%20Terms%3A%20Remote%20Sensing%20Visual%20Question%20Answering%2C%20Multi-source%20Data%0AFusion%2C%20Multimodal%2C%20Remote%20Sensing%2C%20OPT-SAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15770v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Guided%2520Coarse-to-Fine%2520Fusion%2520Network%2520for%2520Robust%2520Remote%2520Sensing%250A%2520%2520Visual%2520Question%2520Answering%26entry.906535625%3DZhicheng%2520Zhao%2520and%2520Changfu%2520Zhou%2520and%2520Yu%2520Zhang%2520and%2520Chenglong%2520Li%2520and%2520Xiaoliang%2520Ma%2520and%2520Jin%2520Tang%26entry.1292438233%3D%2520%2520Remote%2520Sensing%2520Visual%2520Question%2520Answering%2520%2528RSVQA%2529%2520has%2520gained%2520significant%250Aresearch%2520interest.%2520However%252C%2520current%2520RSVQA%2520methods%2520are%2520limited%2520by%2520the%2520imaging%250Amechanisms%2520of%2520optical%2520sensors%252C%2520particularly%2520under%2520challenging%2520conditions%2520such%250Aas%2520cloud-covered%2520and%2520low-light%2520scenarios.%2520Given%2520the%2520all-time%2520and%2520all-weather%250Aimaging%2520capabilities%2520of%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%252C%2520it%2520is%2520crucial%2520to%250Ainvestigate%2520the%2520integration%2520of%2520optical-SAR%2520images%2520to%2520improve%2520RSVQA%2520performance.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520Text-guided%2520Coarse-to-Fine%2520Fusion%2520Network%2520%2528TGFNet%2529%252C%250Awhich%2520leverages%2520the%2520semantic%2520relationships%2520between%2520question%2520text%2520and%250Amulti-source%2520images%2520to%2520guide%2520the%2520network%2520toward%2520complementary%2520fusion%2520at%2520the%250Afeature%2520level.%2520Specifically%252C%2520we%2520develop%2520a%2520Text-guided%2520Coarse-to-Fine%2520Attention%250ARefinement%2520%2528CFAR%2529%2520module%2520to%2520focus%2520on%2520key%2520areas%2520related%2520to%2520the%2520question%2520in%250Acomplex%2520remote%2520sensing%2520images.%2520This%2520module%2520progressively%2520directs%2520attention%2520from%250Abroad%2520areas%2520to%2520finer%2520details%2520through%2520key%2520region%2520routing%252C%2520enhancing%2520the%2520model%2527s%250Aability%2520to%2520focus%2520on%2520relevant%2520regions.%2520Furthermore%252C%2520we%2520propose%2520an%2520Adaptive%250AMulti-Expert%2520Fusion%2520%2528AMEF%2529%2520module%2520that%2520dynamically%2520integrates%2520different%250Aexperts%252C%2520enabling%2520the%2520adaptive%2520fusion%2520of%2520optical%2520and%2520SAR%2520features.%2520In%2520addition%252C%250Awe%2520create%2520the%2520first%2520large-scale%2520benchmark%2520dataset%2520for%2520evaluating%2520optical-SAR%250ARSVQA%2520methods%252C%2520comprising%25206%252C008%2520well-aligned%2520optical-SAR%2520image%2520pairs%2520and%250A1%252C036%252C694%2520well-labeled%2520question-answer%2520pairs%2520across%252016%2520diverse%2520question%2520types%252C%250Aincluding%2520complex%2520relational%2520reasoning%2520questions.%2520Extensive%2520experiments%2520on%2520the%250Aproposed%2520dataset%2520demonstrate%2520that%2520our%2520TGFNet%2520effectively%2520integrates%250Acomplementary%2520information%2520between%2520optical%2520and%2520SAR%2520images%252C%2520significantly%250Aimproving%2520the%2520model%2527s%2520performance%2520in%2520challenging%2520scenarios.%2520The%2520dataset%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/mmic-lcl/.%250A%2520%2520Index%2520Terms%253A%2520Remote%2520Sensing%2520Visual%2520Question%2520Answering%252C%2520Multi-source%2520Data%250AFusion%252C%2520Multimodal%252C%2520Remote%2520Sensing%252C%2520OPT-SAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15770v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Guided%20Coarse-to-Fine%20Fusion%20Network%20for%20Robust%20Remote%20Sensing%0A%20%20Visual%20Question%20Answering&entry.906535625=Zhicheng%20Zhao%20and%20Changfu%20Zhou%20and%20Yu%20Zhang%20and%20Chenglong%20Li%20and%20Xiaoliang%20Ma%20and%20Jin%20Tang&entry.1292438233=%20%20Remote%20Sensing%20Visual%20Question%20Answering%20%28RSVQA%29%20has%20gained%20significant%0Aresearch%20interest.%20However%2C%20current%20RSVQA%20methods%20are%20limited%20by%20the%20imaging%0Amechanisms%20of%20optical%20sensors%2C%20particularly%20under%20challenging%20conditions%20such%0Aas%20cloud-covered%20and%20low-light%20scenarios.%20Given%20the%20all-time%20and%20all-weather%0Aimaging%20capabilities%20of%20Synthetic%20Aperture%20Radar%20%28SAR%29%2C%20it%20is%20crucial%20to%0Ainvestigate%20the%20integration%20of%20optical-SAR%20images%20to%20improve%20RSVQA%20performance.%0AIn%20this%20work%2C%20we%20propose%20a%20Text-guided%20Coarse-to-Fine%20Fusion%20Network%20%28TGFNet%29%2C%0Awhich%20leverages%20the%20semantic%20relationships%20between%20question%20text%20and%0Amulti-source%20images%20to%20guide%20the%20network%20toward%20complementary%20fusion%20at%20the%0Afeature%20level.%20Specifically%2C%20we%20develop%20a%20Text-guided%20Coarse-to-Fine%20Attention%0ARefinement%20%28CFAR%29%20module%20to%20focus%20on%20key%20areas%20related%20to%20the%20question%20in%0Acomplex%20remote%20sensing%20images.%20This%20module%20progressively%20directs%20attention%20from%0Abroad%20areas%20to%20finer%20details%20through%20key%20region%20routing%2C%20enhancing%20the%20model%27s%0Aability%20to%20focus%20on%20relevant%20regions.%20Furthermore%2C%20we%20propose%20an%20Adaptive%0AMulti-Expert%20Fusion%20%28AMEF%29%20module%20that%20dynamically%20integrates%20different%0Aexperts%2C%20enabling%20the%20adaptive%20fusion%20of%20optical%20and%20SAR%20features.%20In%20addition%2C%0Awe%20create%20the%20first%20large-scale%20benchmark%20dataset%20for%20evaluating%20optical-SAR%0ARSVQA%20methods%2C%20comprising%206%2C008%20well-aligned%20optical-SAR%20image%20pairs%20and%0A1%2C036%2C694%20well-labeled%20question-answer%20pairs%20across%2016%20diverse%20question%20types%2C%0Aincluding%20complex%20relational%20reasoning%20questions.%20Extensive%20experiments%20on%20the%0Aproposed%20dataset%20demonstrate%20that%20our%20TGFNet%20effectively%20integrates%0Acomplementary%20information%20between%20optical%20and%20SAR%20images%2C%20significantly%0Aimproving%20the%20model%27s%20performance%20in%20challenging%20scenarios.%20The%20dataset%20is%0Aavailable%20at%3A%20https%3A//github.com/mmic-lcl/.%0A%20%20Index%20Terms%3A%20Remote%20Sensing%20Visual%20Question%20Answering%2C%20Multi-source%20Data%0AFusion%2C%20Multimodal%2C%20Remote%20Sensing%2C%20OPT-SAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15770v2&entry.124074799=Read"},
{"title": "MVICAD2: Multi-View Independent Component Analysis with Delays and\n  Dilations", "author": "Ambroise Heurtebise and Omar Chehab and Pierre Ablin and Alexandre Gramfort", "abstract": "  Machine learning techniques in multi-view settings face significant\nchallenges, particularly when integrating heterogeneous data, aligning feature\nspaces, and managing view-specific biases. These issues are prominent in\nneuroscience, where data from multiple subjects exposed to the same stimuli are\nanalyzed to uncover brain activity dynamics. In magnetoencephalography (MEG),\nwhere signals are captured at the scalp level, estimating the brain's\nunderlying sources is crucial, especially in group studies where sources are\nassumed to be similar for all subjects. Common methods, such as Multi-View\nIndependent Component Analysis (MVICA), assume identical sources across\nsubjects, but this assumption is often too restrictive due to individual\nvariability and age-related changes. Multi-View Independent Component Analysis\nwith Delays (MVICAD) addresses this by allowing sources to differ up to a\ntemporal delay. However, temporal dilation effects, particularly in auditory\nstimuli, are common in brain dynamics, making the estimation of time delays\nalone insufficient. To address this, we propose Multi-View Independent\nComponent Analysis with Delays and Dilations (MVICAD2), which allows sources to\ndiffer across subjects in both temporal delays and dilations. We present a\nmodel with identifiable sources, derive an approximation of its likelihood in\nclosed form, and use regularization and optimization techniques to enhance\nperformance. Through simulations, we demonstrate that MVICAD2 outperforms\nexisting multi-view ICA methods. We further validate its effectiveness using\nthe Cam-CAN dataset, and showing how delays and dilations are related to aging.\n", "link": "http://arxiv.org/abs/2501.07426v1", "date": "2025-01-13", "relevancy": 2.6925, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5461}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5461}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVICAD2%3A%20Multi-View%20Independent%20Component%20Analysis%20with%20Delays%20and%0A%20%20Dilations&body=Title%3A%20MVICAD2%3A%20Multi-View%20Independent%20Component%20Analysis%20with%20Delays%20and%0A%20%20Dilations%0AAuthor%3A%20Ambroise%20Heurtebise%20and%20Omar%20Chehab%20and%20Pierre%20Ablin%20and%20Alexandre%20Gramfort%0AAbstract%3A%20%20%20Machine%20learning%20techniques%20in%20multi-view%20settings%20face%20significant%0Achallenges%2C%20particularly%20when%20integrating%20heterogeneous%20data%2C%20aligning%20feature%0Aspaces%2C%20and%20managing%20view-specific%20biases.%20These%20issues%20are%20prominent%20in%0Aneuroscience%2C%20where%20data%20from%20multiple%20subjects%20exposed%20to%20the%20same%20stimuli%20are%0Aanalyzed%20to%20uncover%20brain%20activity%20dynamics.%20In%20magnetoencephalography%20%28MEG%29%2C%0Awhere%20signals%20are%20captured%20at%20the%20scalp%20level%2C%20estimating%20the%20brain%27s%0Aunderlying%20sources%20is%20crucial%2C%20especially%20in%20group%20studies%20where%20sources%20are%0Aassumed%20to%20be%20similar%20for%20all%20subjects.%20Common%20methods%2C%20such%20as%20Multi-View%0AIndependent%20Component%20Analysis%20%28MVICA%29%2C%20assume%20identical%20sources%20across%0Asubjects%2C%20but%20this%20assumption%20is%20often%20too%20restrictive%20due%20to%20individual%0Avariability%20and%20age-related%20changes.%20Multi-View%20Independent%20Component%20Analysis%0Awith%20Delays%20%28MVICAD%29%20addresses%20this%20by%20allowing%20sources%20to%20differ%20up%20to%20a%0Atemporal%20delay.%20However%2C%20temporal%20dilation%20effects%2C%20particularly%20in%20auditory%0Astimuli%2C%20are%20common%20in%20brain%20dynamics%2C%20making%20the%20estimation%20of%20time%20delays%0Aalone%20insufficient.%20To%20address%20this%2C%20we%20propose%20Multi-View%20Independent%0AComponent%20Analysis%20with%20Delays%20and%20Dilations%20%28MVICAD2%29%2C%20which%20allows%20sources%20to%0Adiffer%20across%20subjects%20in%20both%20temporal%20delays%20and%20dilations.%20We%20present%20a%0Amodel%20with%20identifiable%20sources%2C%20derive%20an%20approximation%20of%20its%20likelihood%20in%0Aclosed%20form%2C%20and%20use%20regularization%20and%20optimization%20techniques%20to%20enhance%0Aperformance.%20Through%20simulations%2C%20we%20demonstrate%20that%20MVICAD2%20outperforms%0Aexisting%20multi-view%20ICA%20methods.%20We%20further%20validate%20its%20effectiveness%20using%0Athe%20Cam-CAN%20dataset%2C%20and%20showing%20how%20delays%20and%20dilations%20are%20related%20to%20aging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVICAD2%253A%2520Multi-View%2520Independent%2520Component%2520Analysis%2520with%2520Delays%2520and%250A%2520%2520Dilations%26entry.906535625%3DAmbroise%2520Heurtebise%2520and%2520Omar%2520Chehab%2520and%2520Pierre%2520Ablin%2520and%2520Alexandre%2520Gramfort%26entry.1292438233%3D%2520%2520Machine%2520learning%2520techniques%2520in%2520multi-view%2520settings%2520face%2520significant%250Achallenges%252C%2520particularly%2520when%2520integrating%2520heterogeneous%2520data%252C%2520aligning%2520feature%250Aspaces%252C%2520and%2520managing%2520view-specific%2520biases.%2520These%2520issues%2520are%2520prominent%2520in%250Aneuroscience%252C%2520where%2520data%2520from%2520multiple%2520subjects%2520exposed%2520to%2520the%2520same%2520stimuli%2520are%250Aanalyzed%2520to%2520uncover%2520brain%2520activity%2520dynamics.%2520In%2520magnetoencephalography%2520%2528MEG%2529%252C%250Awhere%2520signals%2520are%2520captured%2520at%2520the%2520scalp%2520level%252C%2520estimating%2520the%2520brain%2527s%250Aunderlying%2520sources%2520is%2520crucial%252C%2520especially%2520in%2520group%2520studies%2520where%2520sources%2520are%250Aassumed%2520to%2520be%2520similar%2520for%2520all%2520subjects.%2520Common%2520methods%252C%2520such%2520as%2520Multi-View%250AIndependent%2520Component%2520Analysis%2520%2528MVICA%2529%252C%2520assume%2520identical%2520sources%2520across%250Asubjects%252C%2520but%2520this%2520assumption%2520is%2520often%2520too%2520restrictive%2520due%2520to%2520individual%250Avariability%2520and%2520age-related%2520changes.%2520Multi-View%2520Independent%2520Component%2520Analysis%250Awith%2520Delays%2520%2528MVICAD%2529%2520addresses%2520this%2520by%2520allowing%2520sources%2520to%2520differ%2520up%2520to%2520a%250Atemporal%2520delay.%2520However%252C%2520temporal%2520dilation%2520effects%252C%2520particularly%2520in%2520auditory%250Astimuli%252C%2520are%2520common%2520in%2520brain%2520dynamics%252C%2520making%2520the%2520estimation%2520of%2520time%2520delays%250Aalone%2520insufficient.%2520To%2520address%2520this%252C%2520we%2520propose%2520Multi-View%2520Independent%250AComponent%2520Analysis%2520with%2520Delays%2520and%2520Dilations%2520%2528MVICAD2%2529%252C%2520which%2520allows%2520sources%2520to%250Adiffer%2520across%2520subjects%2520in%2520both%2520temporal%2520delays%2520and%2520dilations.%2520We%2520present%2520a%250Amodel%2520with%2520identifiable%2520sources%252C%2520derive%2520an%2520approximation%2520of%2520its%2520likelihood%2520in%250Aclosed%2520form%252C%2520and%2520use%2520regularization%2520and%2520optimization%2520techniques%2520to%2520enhance%250Aperformance.%2520Through%2520simulations%252C%2520we%2520demonstrate%2520that%2520MVICAD2%2520outperforms%250Aexisting%2520multi-view%2520ICA%2520methods.%2520We%2520further%2520validate%2520its%2520effectiveness%2520using%250Athe%2520Cam-CAN%2520dataset%252C%2520and%2520showing%2520how%2520delays%2520and%2520dilations%2520are%2520related%2520to%2520aging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVICAD2%3A%20Multi-View%20Independent%20Component%20Analysis%20with%20Delays%20and%0A%20%20Dilations&entry.906535625=Ambroise%20Heurtebise%20and%20Omar%20Chehab%20and%20Pierre%20Ablin%20and%20Alexandre%20Gramfort&entry.1292438233=%20%20Machine%20learning%20techniques%20in%20multi-view%20settings%20face%20significant%0Achallenges%2C%20particularly%20when%20integrating%20heterogeneous%20data%2C%20aligning%20feature%0Aspaces%2C%20and%20managing%20view-specific%20biases.%20These%20issues%20are%20prominent%20in%0Aneuroscience%2C%20where%20data%20from%20multiple%20subjects%20exposed%20to%20the%20same%20stimuli%20are%0Aanalyzed%20to%20uncover%20brain%20activity%20dynamics.%20In%20magnetoencephalography%20%28MEG%29%2C%0Awhere%20signals%20are%20captured%20at%20the%20scalp%20level%2C%20estimating%20the%20brain%27s%0Aunderlying%20sources%20is%20crucial%2C%20especially%20in%20group%20studies%20where%20sources%20are%0Aassumed%20to%20be%20similar%20for%20all%20subjects.%20Common%20methods%2C%20such%20as%20Multi-View%0AIndependent%20Component%20Analysis%20%28MVICA%29%2C%20assume%20identical%20sources%20across%0Asubjects%2C%20but%20this%20assumption%20is%20often%20too%20restrictive%20due%20to%20individual%0Avariability%20and%20age-related%20changes.%20Multi-View%20Independent%20Component%20Analysis%0Awith%20Delays%20%28MVICAD%29%20addresses%20this%20by%20allowing%20sources%20to%20differ%20up%20to%20a%0Atemporal%20delay.%20However%2C%20temporal%20dilation%20effects%2C%20particularly%20in%20auditory%0Astimuli%2C%20are%20common%20in%20brain%20dynamics%2C%20making%20the%20estimation%20of%20time%20delays%0Aalone%20insufficient.%20To%20address%20this%2C%20we%20propose%20Multi-View%20Independent%0AComponent%20Analysis%20with%20Delays%20and%20Dilations%20%28MVICAD2%29%2C%20which%20allows%20sources%20to%0Adiffer%20across%20subjects%20in%20both%20temporal%20delays%20and%20dilations.%20We%20present%20a%0Amodel%20with%20identifiable%20sources%2C%20derive%20an%20approximation%20of%20its%20likelihood%20in%0Aclosed%20form%2C%20and%20use%20regularization%20and%20optimization%20techniques%20to%20enhance%0Aperformance.%20Through%20simulations%2C%20we%20demonstrate%20that%20MVICAD2%20outperforms%0Aexisting%20multi-view%20ICA%20methods.%20We%20further%20validate%20its%20effectiveness%20using%0Athe%20Cam-CAN%20dataset%2C%20and%20showing%20how%20delays%20and%20dilations%20are%20related%20to%20aging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07426v1&entry.124074799=Read"},
{"title": "InstructOCR: Instruction Boosting Scene Text Spotting", "author": "Chen Duan and Qianyi Jiang and Pei Fu and Jiamin Chen and Shengxi Li and Zining Wang and Shan Guo and Junfeng Luo", "abstract": "  In the field of scene text spotting, previous OCR methods primarily relied on\nimage encoders and pre-trained text information, but they often overlooked the\nadvantages of incorporating human language instructions. To address this gap,\nwe propose InstructOCR, an innovative instruction-based scene text spotting\nmodel that leverages human language instructions to enhance the understanding\nof text within images. Our framework employs both text and image encoders\nduring training and inference, along with instructions meticulously designed\nbased on text attributes. This approach enables the model to interpret text\nmore accurately and flexibly. Extensive experiments demonstrate the\neffectiveness of our model and we achieve state-of-the-art results on widely\nused benchmarks. Furthermore, the proposed framework can be seamlessly applied\nto scene text VQA tasks. By leveraging instruction strategies during\npre-training, the performance on downstream VQA tasks can be significantly\nimproved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on\nthe ST-VQA dataset. These experimental results provide insights into the\nbenefits of incorporating human language instructions for OCR-related tasks.\n", "link": "http://arxiv.org/abs/2412.15523v2", "date": "2025-01-13", "relevancy": 2.6838, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5609}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructOCR%3A%20Instruction%20Boosting%20Scene%20Text%20Spotting&body=Title%3A%20InstructOCR%3A%20Instruction%20Boosting%20Scene%20Text%20Spotting%0AAuthor%3A%20Chen%20Duan%20and%20Qianyi%20Jiang%20and%20Pei%20Fu%20and%20Jiamin%20Chen%20and%20Shengxi%20Li%20and%20Zining%20Wang%20and%20Shan%20Guo%20and%20Junfeng%20Luo%0AAbstract%3A%20%20%20In%20the%20field%20of%20scene%20text%20spotting%2C%20previous%20OCR%20methods%20primarily%20relied%20on%0Aimage%20encoders%20and%20pre-trained%20text%20information%2C%20but%20they%20often%20overlooked%20the%0Aadvantages%20of%20incorporating%20human%20language%20instructions.%20To%20address%20this%20gap%2C%0Awe%20propose%20InstructOCR%2C%20an%20innovative%20instruction-based%20scene%20text%20spotting%0Amodel%20that%20leverages%20human%20language%20instructions%20to%20enhance%20the%20understanding%0Aof%20text%20within%20images.%20Our%20framework%20employs%20both%20text%20and%20image%20encoders%0Aduring%20training%20and%20inference%2C%20along%20with%20instructions%20meticulously%20designed%0Abased%20on%20text%20attributes.%20This%20approach%20enables%20the%20model%20to%20interpret%20text%0Amore%20accurately%20and%20flexibly.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20and%20we%20achieve%20state-of-the-art%20results%20on%20widely%0Aused%20benchmarks.%20Furthermore%2C%20the%20proposed%20framework%20can%20be%20seamlessly%20applied%0Ato%20scene%20text%20VQA%20tasks.%20By%20leveraging%20instruction%20strategies%20during%0Apre-training%2C%20the%20performance%20on%20downstream%20VQA%20tasks%20can%20be%20significantly%0Aimproved%2C%20with%20a%202.6%25%20increase%20on%20the%20TextVQA%20dataset%20and%20a%202.1%25%20increase%20on%0Athe%20ST-VQA%20dataset.%20These%20experimental%20results%20provide%20insights%20into%20the%0Abenefits%20of%20incorporating%20human%20language%20instructions%20for%20OCR-related%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructOCR%253A%2520Instruction%2520Boosting%2520Scene%2520Text%2520Spotting%26entry.906535625%3DChen%2520Duan%2520and%2520Qianyi%2520Jiang%2520and%2520Pei%2520Fu%2520and%2520Jiamin%2520Chen%2520and%2520Shengxi%2520Li%2520and%2520Zining%2520Wang%2520and%2520Shan%2520Guo%2520and%2520Junfeng%2520Luo%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520scene%2520text%2520spotting%252C%2520previous%2520OCR%2520methods%2520primarily%2520relied%2520on%250Aimage%2520encoders%2520and%2520pre-trained%2520text%2520information%252C%2520but%2520they%2520often%2520overlooked%2520the%250Aadvantages%2520of%2520incorporating%2520human%2520language%2520instructions.%2520To%2520address%2520this%2520gap%252C%250Awe%2520propose%2520InstructOCR%252C%2520an%2520innovative%2520instruction-based%2520scene%2520text%2520spotting%250Amodel%2520that%2520leverages%2520human%2520language%2520instructions%2520to%2520enhance%2520the%2520understanding%250Aof%2520text%2520within%2520images.%2520Our%2520framework%2520employs%2520both%2520text%2520and%2520image%2520encoders%250Aduring%2520training%2520and%2520inference%252C%2520along%2520with%2520instructions%2520meticulously%2520designed%250Abased%2520on%2520text%2520attributes.%2520This%2520approach%2520enables%2520the%2520model%2520to%2520interpret%2520text%250Amore%2520accurately%2520and%2520flexibly.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520model%2520and%2520we%2520achieve%2520state-of-the-art%2520results%2520on%2520widely%250Aused%2520benchmarks.%2520Furthermore%252C%2520the%2520proposed%2520framework%2520can%2520be%2520seamlessly%2520applied%250Ato%2520scene%2520text%2520VQA%2520tasks.%2520By%2520leveraging%2520instruction%2520strategies%2520during%250Apre-training%252C%2520the%2520performance%2520on%2520downstream%2520VQA%2520tasks%2520can%2520be%2520significantly%250Aimproved%252C%2520with%2520a%25202.6%2525%2520increase%2520on%2520the%2520TextVQA%2520dataset%2520and%2520a%25202.1%2525%2520increase%2520on%250Athe%2520ST-VQA%2520dataset.%2520These%2520experimental%2520results%2520provide%2520insights%2520into%2520the%250Abenefits%2520of%2520incorporating%2520human%2520language%2520instructions%2520for%2520OCR-related%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructOCR%3A%20Instruction%20Boosting%20Scene%20Text%20Spotting&entry.906535625=Chen%20Duan%20and%20Qianyi%20Jiang%20and%20Pei%20Fu%20and%20Jiamin%20Chen%20and%20Shengxi%20Li%20and%20Zining%20Wang%20and%20Shan%20Guo%20and%20Junfeng%20Luo&entry.1292438233=%20%20In%20the%20field%20of%20scene%20text%20spotting%2C%20previous%20OCR%20methods%20primarily%20relied%20on%0Aimage%20encoders%20and%20pre-trained%20text%20information%2C%20but%20they%20often%20overlooked%20the%0Aadvantages%20of%20incorporating%20human%20language%20instructions.%20To%20address%20this%20gap%2C%0Awe%20propose%20InstructOCR%2C%20an%20innovative%20instruction-based%20scene%20text%20spotting%0Amodel%20that%20leverages%20human%20language%20instructions%20to%20enhance%20the%20understanding%0Aof%20text%20within%20images.%20Our%20framework%20employs%20both%20text%20and%20image%20encoders%0Aduring%20training%20and%20inference%2C%20along%20with%20instructions%20meticulously%20designed%0Abased%20on%20text%20attributes.%20This%20approach%20enables%20the%20model%20to%20interpret%20text%0Amore%20accurately%20and%20flexibly.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20model%20and%20we%20achieve%20state-of-the-art%20results%20on%20widely%0Aused%20benchmarks.%20Furthermore%2C%20the%20proposed%20framework%20can%20be%20seamlessly%20applied%0Ato%20scene%20text%20VQA%20tasks.%20By%20leveraging%20instruction%20strategies%20during%0Apre-training%2C%20the%20performance%20on%20downstream%20VQA%20tasks%20can%20be%20significantly%0Aimproved%2C%20with%20a%202.6%25%20increase%20on%20the%20TextVQA%20dataset%20and%20a%202.1%25%20increase%20on%0Athe%20ST-VQA%20dataset.%20These%20experimental%20results%20provide%20insights%20into%20the%0Abenefits%20of%20incorporating%20human%20language%20instructions%20for%20OCR-related%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15523v2&entry.124074799=Read"},
{"title": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images", "author": "Virmarie Maquiling and Sean Anthony Byrne and Diederick C. Niehorster and Marco Carminati and Enkelejda Kasneci", "abstract": "  We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research.\n", "link": "http://arxiv.org/abs/2410.08926v3", "date": "2025-01-13", "relevancy": 2.6579, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5413}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5392}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Pupil%20Segmentation%20with%20SAM%202%3A%20A%20Case%20Study%20of%20Over%2014%20Million%0A%20%20Images&body=Title%3A%20Zero-Shot%20Pupil%20Segmentation%20with%20SAM%202%3A%20A%20Case%20Study%20of%20Over%2014%20Million%0A%20%20Images%0AAuthor%3A%20Virmarie%20Maquiling%20and%20Sean%20Anthony%20Byrne%20and%20Diederick%20C.%20Niehorster%20and%20Marco%20Carminati%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20We%20explore%20the%20transformative%20potential%20of%20SAM%202%2C%20a%20vision%20foundation%20model%2C%0Ain%20advancing%20gaze%20estimation%20and%20eye%20tracking%20technologies.%20By%20significantly%0Areducing%20annotation%20time%2C%20lowering%20technical%20barriers%20through%20its%20ease%20of%0Adeployment%2C%20and%20enhancing%20segmentation%20accuracy%2C%20SAM%202%20addresses%20critical%0Achallenges%20faced%20by%20researchers%20and%20practitioners.%20Utilizing%20its%20zero-shot%0Asegmentation%20capabilities%20with%20minimal%20user%20input-a%20single%20click%20per%20video-we%0Atested%20SAM%202%20on%20over%2014%20million%20eye%20images%20from%20diverse%20datasets%2C%20including%0Avirtual%20reality%20setups%20and%20the%20world%27s%20largest%20unified%20dataset%20recorded%20using%0Awearable%20eye%20trackers.%20Remarkably%2C%20in%20pupil%20segmentation%20tasks%2C%20SAM%202%20matches%0Athe%20performance%20of%20domain-specific%20models%20trained%20solely%20on%20eye%20images%2C%0Aachieving%20competitive%20mean%20Intersection%20over%20Union%20%28mIoU%29%20scores%20of%20up%20to%2093%25%0Awithout%20fine-tuning.%20Additionally%2C%20we%20provide%20our%20code%20and%20segmentation%20masks%0Afor%20these%20widely%20used%20datasets%20to%20promote%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08926v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Pupil%2520Segmentation%2520with%2520SAM%25202%253A%2520A%2520Case%2520Study%2520of%2520Over%252014%2520Million%250A%2520%2520Images%26entry.906535625%3DVirmarie%2520Maquiling%2520and%2520Sean%2520Anthony%2520Byrne%2520and%2520Diederick%2520C.%2520Niehorster%2520and%2520Marco%2520Carminati%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520transformative%2520potential%2520of%2520SAM%25202%252C%2520a%2520vision%2520foundation%2520model%252C%250Ain%2520advancing%2520gaze%2520estimation%2520and%2520eye%2520tracking%2520technologies.%2520By%2520significantly%250Areducing%2520annotation%2520time%252C%2520lowering%2520technical%2520barriers%2520through%2520its%2520ease%2520of%250Adeployment%252C%2520and%2520enhancing%2520segmentation%2520accuracy%252C%2520SAM%25202%2520addresses%2520critical%250Achallenges%2520faced%2520by%2520researchers%2520and%2520practitioners.%2520Utilizing%2520its%2520zero-shot%250Asegmentation%2520capabilities%2520with%2520minimal%2520user%2520input-a%2520single%2520click%2520per%2520video-we%250Atested%2520SAM%25202%2520on%2520over%252014%2520million%2520eye%2520images%2520from%2520diverse%2520datasets%252C%2520including%250Avirtual%2520reality%2520setups%2520and%2520the%2520world%2527s%2520largest%2520unified%2520dataset%2520recorded%2520using%250Awearable%2520eye%2520trackers.%2520Remarkably%252C%2520in%2520pupil%2520segmentation%2520tasks%252C%2520SAM%25202%2520matches%250Athe%2520performance%2520of%2520domain-specific%2520models%2520trained%2520solely%2520on%2520eye%2520images%252C%250Aachieving%2520competitive%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520scores%2520of%2520up%2520to%252093%2525%250Awithout%2520fine-tuning.%2520Additionally%252C%2520we%2520provide%2520our%2520code%2520and%2520segmentation%2520masks%250Afor%2520these%2520widely%2520used%2520datasets%2520to%2520promote%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08926v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Pupil%20Segmentation%20with%20SAM%202%3A%20A%20Case%20Study%20of%20Over%2014%20Million%0A%20%20Images&entry.906535625=Virmarie%20Maquiling%20and%20Sean%20Anthony%20Byrne%20and%20Diederick%20C.%20Niehorster%20and%20Marco%20Carminati%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20We%20explore%20the%20transformative%20potential%20of%20SAM%202%2C%20a%20vision%20foundation%20model%2C%0Ain%20advancing%20gaze%20estimation%20and%20eye%20tracking%20technologies.%20By%20significantly%0Areducing%20annotation%20time%2C%20lowering%20technical%20barriers%20through%20its%20ease%20of%0Adeployment%2C%20and%20enhancing%20segmentation%20accuracy%2C%20SAM%202%20addresses%20critical%0Achallenges%20faced%20by%20researchers%20and%20practitioners.%20Utilizing%20its%20zero-shot%0Asegmentation%20capabilities%20with%20minimal%20user%20input-a%20single%20click%20per%20video-we%0Atested%20SAM%202%20on%20over%2014%20million%20eye%20images%20from%20diverse%20datasets%2C%20including%0Avirtual%20reality%20setups%20and%20the%20world%27s%20largest%20unified%20dataset%20recorded%20using%0Awearable%20eye%20trackers.%20Remarkably%2C%20in%20pupil%20segmentation%20tasks%2C%20SAM%202%20matches%0Athe%20performance%20of%20domain-specific%20models%20trained%20solely%20on%20eye%20images%2C%0Aachieving%20competitive%20mean%20Intersection%20over%20Union%20%28mIoU%29%20scores%20of%20up%20to%2093%25%0Awithout%20fine-tuning.%20Additionally%2C%20we%20provide%20our%20code%20and%20segmentation%20masks%0Afor%20these%20widely%20used%20datasets%20to%20promote%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08926v3&entry.124074799=Read"},
{"title": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and\n  Vision-Language Models Derived from Scientific Literature", "author": "Alejandro Lozano and Min Woo Sun and James Burgess and Liangyu Chen and Jeffrey J Nirschl and Jeffrey Gu and Ivan Lopez and Josiah Aklilu and Austin Wolfgang Katzer and Collin Chiu and Anita Rau and Xiaohan Wang and Yuhui Zhang and Alfred Seunghoon Song and Robert Tibshirani and Serena Yeung-Levy", "abstract": "  The development of vision-language models (VLMs) is driven by large-scale and\ndiverse multimodal datasets. However, progress toward generalist biomedical\nVLMs is limited by the lack of annotated, publicly accessible datasets across\nbiology and medicine. Existing efforts are restricted to narrow domains,\nmissing the full diversity of biomedical knowledge encoded in scientific\nliterature. To address this gap, we introduce BIOMEDICA, a scalable,\nopen-source framework to extract, annotate, and serialize the entirety of the\nPubMed Central Open Access subset into an easy-to-use, publicly accessible\ndataset.Our framework produces a comprehensive archive with over 24 million\nunique image-text pairs from over 6 million articles. Metadata and\nexpert-guided annotations are also provided. We demonstrate the utility and\naccessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style\nmodels continuously pre-trained on the BIOMEDICA dataset via streaming,\neliminating the need to download 27 TB of data locally.On average, our models\nachieve state-of-the-art performance across 40 tasks - spanning pathology,\nradiology, ophthalmology, dermatology, surgery, molecular biology,\nparasitology, and cell biology - excelling in zero-shot classification with a\n6.56% average improvement (as high as 29.8% and 17.5% in dermatology and\nophthalmology, respectively), and stronger image-text retrieval, all while\nusing 10x less compute. To foster reproducibility and collaboration, we release\nour codebase and dataset for the broader research community.\n", "link": "http://arxiv.org/abs/2501.07171v1", "date": "2025-01-13", "relevancy": 2.6294, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIOMEDICA%3A%20An%20Open%20Biomedical%20Image-Caption%20Archive%2C%20Dataset%2C%20and%0A%20%20Vision-Language%20Models%20Derived%20from%20Scientific%20Literature&body=Title%3A%20BIOMEDICA%3A%20An%20Open%20Biomedical%20Image-Caption%20Archive%2C%20Dataset%2C%20and%0A%20%20Vision-Language%20Models%20Derived%20from%20Scientific%20Literature%0AAuthor%3A%20Alejandro%20Lozano%20and%20Min%20Woo%20Sun%20and%20James%20Burgess%20and%20Liangyu%20Chen%20and%20Jeffrey%20J%20Nirschl%20and%20Jeffrey%20Gu%20and%20Ivan%20Lopez%20and%20Josiah%20Aklilu%20and%20Austin%20Wolfgang%20Katzer%20and%20Collin%20Chiu%20and%20Anita%20Rau%20and%20Xiaohan%20Wang%20and%20Yuhui%20Zhang%20and%20Alfred%20Seunghoon%20Song%20and%20Robert%20Tibshirani%20and%20Serena%20Yeung-Levy%0AAbstract%3A%20%20%20The%20development%20of%20vision-language%20models%20%28VLMs%29%20is%20driven%20by%20large-scale%20and%0Adiverse%20multimodal%20datasets.%20However%2C%20progress%20toward%20generalist%20biomedical%0AVLMs%20is%20limited%20by%20the%20lack%20of%20annotated%2C%20publicly%20accessible%20datasets%20across%0Abiology%20and%20medicine.%20Existing%20efforts%20are%20restricted%20to%20narrow%20domains%2C%0Amissing%20the%20full%20diversity%20of%20biomedical%20knowledge%20encoded%20in%20scientific%0Aliterature.%20To%20address%20this%20gap%2C%20we%20introduce%20BIOMEDICA%2C%20a%20scalable%2C%0Aopen-source%20framework%20to%20extract%2C%20annotate%2C%20and%20serialize%20the%20entirety%20of%20the%0APubMed%20Central%20Open%20Access%20subset%20into%20an%20easy-to-use%2C%20publicly%20accessible%0Adataset.Our%20framework%20produces%20a%20comprehensive%20archive%20with%20over%2024%20million%0Aunique%20image-text%20pairs%20from%20over%206%20million%20articles.%20Metadata%20and%0Aexpert-guided%20annotations%20are%20also%20provided.%20We%20demonstrate%20the%20utility%20and%0Aaccessibility%20of%20our%20resource%20by%20releasing%20BMCA-CLIP%2C%20a%20suite%20of%20CLIP-style%0Amodels%20continuously%20pre-trained%20on%20the%20BIOMEDICA%20dataset%20via%20streaming%2C%0Aeliminating%20the%20need%20to%20download%2027%20TB%20of%20data%20locally.On%20average%2C%20our%20models%0Aachieve%20state-of-the-art%20performance%20across%2040%20tasks%20-%20spanning%20pathology%2C%0Aradiology%2C%20ophthalmology%2C%20dermatology%2C%20surgery%2C%20molecular%20biology%2C%0Aparasitology%2C%20and%20cell%20biology%20-%20excelling%20in%20zero-shot%20classification%20with%20a%0A6.56%25%20average%20improvement%20%28as%20high%20as%2029.8%25%20and%2017.5%25%20in%20dermatology%20and%0Aophthalmology%2C%20respectively%29%2C%20and%20stronger%20image-text%20retrieval%2C%20all%20while%0Ausing%2010x%20less%20compute.%20To%20foster%20reproducibility%20and%20collaboration%2C%20we%20release%0Aour%20codebase%20and%20dataset%20for%20the%20broader%20research%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIOMEDICA%253A%2520An%2520Open%2520Biomedical%2520Image-Caption%2520Archive%252C%2520Dataset%252C%2520and%250A%2520%2520Vision-Language%2520Models%2520Derived%2520from%2520Scientific%2520Literature%26entry.906535625%3DAlejandro%2520Lozano%2520and%2520Min%2520Woo%2520Sun%2520and%2520James%2520Burgess%2520and%2520Liangyu%2520Chen%2520and%2520Jeffrey%2520J%2520Nirschl%2520and%2520Jeffrey%2520Gu%2520and%2520Ivan%2520Lopez%2520and%2520Josiah%2520Aklilu%2520and%2520Austin%2520Wolfgang%2520Katzer%2520and%2520Collin%2520Chiu%2520and%2520Anita%2520Rau%2520and%2520Xiaohan%2520Wang%2520and%2520Yuhui%2520Zhang%2520and%2520Alfred%2520Seunghoon%2520Song%2520and%2520Robert%2520Tibshirani%2520and%2520Serena%2520Yeung-Levy%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520is%2520driven%2520by%2520large-scale%2520and%250Adiverse%2520multimodal%2520datasets.%2520However%252C%2520progress%2520toward%2520generalist%2520biomedical%250AVLMs%2520is%2520limited%2520by%2520the%2520lack%2520of%2520annotated%252C%2520publicly%2520accessible%2520datasets%2520across%250Abiology%2520and%2520medicine.%2520Existing%2520efforts%2520are%2520restricted%2520to%2520narrow%2520domains%252C%250Amissing%2520the%2520full%2520diversity%2520of%2520biomedical%2520knowledge%2520encoded%2520in%2520scientific%250Aliterature.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520BIOMEDICA%252C%2520a%2520scalable%252C%250Aopen-source%2520framework%2520to%2520extract%252C%2520annotate%252C%2520and%2520serialize%2520the%2520entirety%2520of%2520the%250APubMed%2520Central%2520Open%2520Access%2520subset%2520into%2520an%2520easy-to-use%252C%2520publicly%2520accessible%250Adataset.Our%2520framework%2520produces%2520a%2520comprehensive%2520archive%2520with%2520over%252024%2520million%250Aunique%2520image-text%2520pairs%2520from%2520over%25206%2520million%2520articles.%2520Metadata%2520and%250Aexpert-guided%2520annotations%2520are%2520also%2520provided.%2520We%2520demonstrate%2520the%2520utility%2520and%250Aaccessibility%2520of%2520our%2520resource%2520by%2520releasing%2520BMCA-CLIP%252C%2520a%2520suite%2520of%2520CLIP-style%250Amodels%2520continuously%2520pre-trained%2520on%2520the%2520BIOMEDICA%2520dataset%2520via%2520streaming%252C%250Aeliminating%2520the%2520need%2520to%2520download%252027%2520TB%2520of%2520data%2520locally.On%2520average%252C%2520our%2520models%250Aachieve%2520state-of-the-art%2520performance%2520across%252040%2520tasks%2520-%2520spanning%2520pathology%252C%250Aradiology%252C%2520ophthalmology%252C%2520dermatology%252C%2520surgery%252C%2520molecular%2520biology%252C%250Aparasitology%252C%2520and%2520cell%2520biology%2520-%2520excelling%2520in%2520zero-shot%2520classification%2520with%2520a%250A6.56%2525%2520average%2520improvement%2520%2528as%2520high%2520as%252029.8%2525%2520and%252017.5%2525%2520in%2520dermatology%2520and%250Aophthalmology%252C%2520respectively%2529%252C%2520and%2520stronger%2520image-text%2520retrieval%252C%2520all%2520while%250Ausing%252010x%2520less%2520compute.%2520To%2520foster%2520reproducibility%2520and%2520collaboration%252C%2520we%2520release%250Aour%2520codebase%2520and%2520dataset%2520for%2520the%2520broader%2520research%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIOMEDICA%3A%20An%20Open%20Biomedical%20Image-Caption%20Archive%2C%20Dataset%2C%20and%0A%20%20Vision-Language%20Models%20Derived%20from%20Scientific%20Literature&entry.906535625=Alejandro%20Lozano%20and%20Min%20Woo%20Sun%20and%20James%20Burgess%20and%20Liangyu%20Chen%20and%20Jeffrey%20J%20Nirschl%20and%20Jeffrey%20Gu%20and%20Ivan%20Lopez%20and%20Josiah%20Aklilu%20and%20Austin%20Wolfgang%20Katzer%20and%20Collin%20Chiu%20and%20Anita%20Rau%20and%20Xiaohan%20Wang%20and%20Yuhui%20Zhang%20and%20Alfred%20Seunghoon%20Song%20and%20Robert%20Tibshirani%20and%20Serena%20Yeung-Levy&entry.1292438233=%20%20The%20development%20of%20vision-language%20models%20%28VLMs%29%20is%20driven%20by%20large-scale%20and%0Adiverse%20multimodal%20datasets.%20However%2C%20progress%20toward%20generalist%20biomedical%0AVLMs%20is%20limited%20by%20the%20lack%20of%20annotated%2C%20publicly%20accessible%20datasets%20across%0Abiology%20and%20medicine.%20Existing%20efforts%20are%20restricted%20to%20narrow%20domains%2C%0Amissing%20the%20full%20diversity%20of%20biomedical%20knowledge%20encoded%20in%20scientific%0Aliterature.%20To%20address%20this%20gap%2C%20we%20introduce%20BIOMEDICA%2C%20a%20scalable%2C%0Aopen-source%20framework%20to%20extract%2C%20annotate%2C%20and%20serialize%20the%20entirety%20of%20the%0APubMed%20Central%20Open%20Access%20subset%20into%20an%20easy-to-use%2C%20publicly%20accessible%0Adataset.Our%20framework%20produces%20a%20comprehensive%20archive%20with%20over%2024%20million%0Aunique%20image-text%20pairs%20from%20over%206%20million%20articles.%20Metadata%20and%0Aexpert-guided%20annotations%20are%20also%20provided.%20We%20demonstrate%20the%20utility%20and%0Aaccessibility%20of%20our%20resource%20by%20releasing%20BMCA-CLIP%2C%20a%20suite%20of%20CLIP-style%0Amodels%20continuously%20pre-trained%20on%20the%20BIOMEDICA%20dataset%20via%20streaming%2C%0Aeliminating%20the%20need%20to%20download%2027%20TB%20of%20data%20locally.On%20average%2C%20our%20models%0Aachieve%20state-of-the-art%20performance%20across%2040%20tasks%20-%20spanning%20pathology%2C%0Aradiology%2C%20ophthalmology%2C%20dermatology%2C%20surgery%2C%20molecular%20biology%2C%0Aparasitology%2C%20and%20cell%20biology%20-%20excelling%20in%20zero-shot%20classification%20with%20a%0A6.56%25%20average%20improvement%20%28as%20high%20as%2029.8%25%20and%2017.5%25%20in%20dermatology%20and%0Aophthalmology%2C%20respectively%29%2C%20and%20stronger%20image-text%20retrieval%2C%20all%20while%0Ausing%2010x%20less%20compute.%20To%20foster%20reproducibility%20and%20collaboration%2C%20we%20release%0Aour%20codebase%20and%20dataset%20for%20the%20broader%20research%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07171v1&entry.124074799=Read"},
{"title": "ViewVR: Visual Feedback Modes to Achieve Quality of VR-based\n  Telemanipulation", "author": "A. Erkhov and A. Bazhenov and S. Satsevich and D. Belov and F. Khabibullin and S. Egorov and M. Gromakov and M. Altamirano Cabrera and D. Tsetserukou", "abstract": "  The paper focuses on an immersive teleoperation system that enhances\noperator's ability to actively perceive the robot's surroundings. A\nconsumer-grade HTC Vive VR system was used to synchronize the operator's hand\nand head movements with a UR3 robot and a custom-built robotic head with two\ndegrees of freedom (2-DoF). The system's usability, manipulation efficiency,\nand intuitiveness of control were evaluated in comparison with static head\ncamera positioning across three distinct tasks. Code and other supplementary\nmaterials can be accessed by link: https://github.com/ErkhovArtem/ViewVR\n", "link": "http://arxiv.org/abs/2501.07299v1", "date": "2025-01-13", "relevancy": 2.5923, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5581}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4987}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewVR%3A%20Visual%20Feedback%20Modes%20to%20Achieve%20Quality%20of%20VR-based%0A%20%20Telemanipulation&body=Title%3A%20ViewVR%3A%20Visual%20Feedback%20Modes%20to%20Achieve%20Quality%20of%20VR-based%0A%20%20Telemanipulation%0AAuthor%3A%20A.%20Erkhov%20and%20A.%20Bazhenov%20and%20S.%20Satsevich%20and%20D.%20Belov%20and%20F.%20Khabibullin%20and%20S.%20Egorov%20and%20M.%20Gromakov%20and%20M.%20Altamirano%20Cabrera%20and%20D.%20Tsetserukou%0AAbstract%3A%20%20%20The%20paper%20focuses%20on%20an%20immersive%20teleoperation%20system%20that%20enhances%0Aoperator%27s%20ability%20to%20actively%20perceive%20the%20robot%27s%20surroundings.%20A%0Aconsumer-grade%20HTC%20Vive%20VR%20system%20was%20used%20to%20synchronize%20the%20operator%27s%20hand%0Aand%20head%20movements%20with%20a%20UR3%20robot%20and%20a%20custom-built%20robotic%20head%20with%20two%0Adegrees%20of%20freedom%20%282-DoF%29.%20The%20system%27s%20usability%2C%20manipulation%20efficiency%2C%0Aand%20intuitiveness%20of%20control%20were%20evaluated%20in%20comparison%20with%20static%20head%0Acamera%20positioning%20across%20three%20distinct%20tasks.%20Code%20and%20other%20supplementary%0Amaterials%20can%20be%20accessed%20by%20link%3A%20https%3A//github.com/ErkhovArtem/ViewVR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewVR%253A%2520Visual%2520Feedback%2520Modes%2520to%2520Achieve%2520Quality%2520of%2520VR-based%250A%2520%2520Telemanipulation%26entry.906535625%3DA.%2520Erkhov%2520and%2520A.%2520Bazhenov%2520and%2520S.%2520Satsevich%2520and%2520D.%2520Belov%2520and%2520F.%2520Khabibullin%2520and%2520S.%2520Egorov%2520and%2520M.%2520Gromakov%2520and%2520M.%2520Altamirano%2520Cabrera%2520and%2520D.%2520Tsetserukou%26entry.1292438233%3D%2520%2520The%2520paper%2520focuses%2520on%2520an%2520immersive%2520teleoperation%2520system%2520that%2520enhances%250Aoperator%2527s%2520ability%2520to%2520actively%2520perceive%2520the%2520robot%2527s%2520surroundings.%2520A%250Aconsumer-grade%2520HTC%2520Vive%2520VR%2520system%2520was%2520used%2520to%2520synchronize%2520the%2520operator%2527s%2520hand%250Aand%2520head%2520movements%2520with%2520a%2520UR3%2520robot%2520and%2520a%2520custom-built%2520robotic%2520head%2520with%2520two%250Adegrees%2520of%2520freedom%2520%25282-DoF%2529.%2520The%2520system%2527s%2520usability%252C%2520manipulation%2520efficiency%252C%250Aand%2520intuitiveness%2520of%2520control%2520were%2520evaluated%2520in%2520comparison%2520with%2520static%2520head%250Acamera%2520positioning%2520across%2520three%2520distinct%2520tasks.%2520Code%2520and%2520other%2520supplementary%250Amaterials%2520can%2520be%2520accessed%2520by%2520link%253A%2520https%253A//github.com/ErkhovArtem/ViewVR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewVR%3A%20Visual%20Feedback%20Modes%20to%20Achieve%20Quality%20of%20VR-based%0A%20%20Telemanipulation&entry.906535625=A.%20Erkhov%20and%20A.%20Bazhenov%20and%20S.%20Satsevich%20and%20D.%20Belov%20and%20F.%20Khabibullin%20and%20S.%20Egorov%20and%20M.%20Gromakov%20and%20M.%20Altamirano%20Cabrera%20and%20D.%20Tsetserukou&entry.1292438233=%20%20The%20paper%20focuses%20on%20an%20immersive%20teleoperation%20system%20that%20enhances%0Aoperator%27s%20ability%20to%20actively%20perceive%20the%20robot%27s%20surroundings.%20A%0Aconsumer-grade%20HTC%20Vive%20VR%20system%20was%20used%20to%20synchronize%20the%20operator%27s%20hand%0Aand%20head%20movements%20with%20a%20UR3%20robot%20and%20a%20custom-built%20robotic%20head%20with%20two%0Adegrees%20of%20freedom%20%282-DoF%29.%20The%20system%27s%20usability%2C%20manipulation%20efficiency%2C%0Aand%20intuitiveness%20of%20control%20were%20evaluated%20in%20comparison%20with%20static%20head%0Acamera%20positioning%20across%20three%20distinct%20tasks.%20Code%20and%20other%20supplementary%0Amaterials%20can%20be%20accessed%20by%20link%3A%20https%3A//github.com/ErkhovArtem/ViewVR%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07299v1&entry.124074799=Read"},
{"title": "Multi-Head Explainer: A General Framework to Improve Explainability in\n  CNNs and Transformers", "author": "Bohang Sun and Pietro Li\u00f2", "abstract": "  In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and\nmodular framework that enhances both the explainability and accuracy of\nConvolutional Neural Networks (CNNs) and Transformer-based models. MHEX\nconsists of three core components: an Attention Gate that dynamically\nhighlights task-relevant features, Deep Supervision that guides early layers to\ncapture fine-grained details pertinent to the target class, and an Equivalent\nMatrix that unifies refined local and global representations to generate\ncomprehensive saliency maps. Our approach demonstrates superior compatibility,\nenabling effortless integration into existing residual networks like ResNet and\nTransformer architectures such as BERT with minimal modifications. Extensive\nexperiments on benchmark datasets in medical imaging and text classification\nshow that MHEX not only improves classification accuracy but also produces\nhighly interpretable and detailed saliency scores.\n", "link": "http://arxiv.org/abs/2501.01311v2", "date": "2025-01-13", "relevancy": 2.5821, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Head%20Explainer%3A%20A%20General%20Framework%20to%20Improve%20Explainability%20in%0A%20%20CNNs%20and%20Transformers&body=Title%3A%20Multi-Head%20Explainer%3A%20A%20General%20Framework%20to%20Improve%20Explainability%20in%0A%20%20CNNs%20and%20Transformers%0AAuthor%3A%20Bohang%20Sun%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20the%20Multi-Head%20Explainer%20%28MHEX%29%2C%20a%20versatile%20and%0Amodular%20framework%20that%20enhances%20both%20the%20explainability%20and%20accuracy%20of%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformer-based%20models.%20MHEX%0Aconsists%20of%20three%20core%20components%3A%20an%20Attention%20Gate%20that%20dynamically%0Ahighlights%20task-relevant%20features%2C%20Deep%20Supervision%20that%20guides%20early%20layers%20to%0Acapture%20fine-grained%20details%20pertinent%20to%20the%20target%20class%2C%20and%20an%20Equivalent%0AMatrix%20that%20unifies%20refined%20local%20and%20global%20representations%20to%20generate%0Acomprehensive%20saliency%20maps.%20Our%20approach%20demonstrates%20superior%20compatibility%2C%0Aenabling%20effortless%20integration%20into%20existing%20residual%20networks%20like%20ResNet%20and%0ATransformer%20architectures%20such%20as%20BERT%20with%20minimal%20modifications.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20in%20medical%20imaging%20and%20text%20classification%0Ashow%20that%20MHEX%20not%20only%20improves%20classification%20accuracy%20but%20also%20produces%0Ahighly%20interpretable%20and%20detailed%20saliency%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01311v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Head%2520Explainer%253A%2520A%2520General%2520Framework%2520to%2520Improve%2520Explainability%2520in%250A%2520%2520CNNs%2520and%2520Transformers%26entry.906535625%3DBohang%2520Sun%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520introduce%2520the%2520Multi-Head%2520Explainer%2520%2528MHEX%2529%252C%2520a%2520versatile%2520and%250Amodular%2520framework%2520that%2520enhances%2520both%2520the%2520explainability%2520and%2520accuracy%2520of%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Transformer-based%2520models.%2520MHEX%250Aconsists%2520of%2520three%2520core%2520components%253A%2520an%2520Attention%2520Gate%2520that%2520dynamically%250Ahighlights%2520task-relevant%2520features%252C%2520Deep%2520Supervision%2520that%2520guides%2520early%2520layers%2520to%250Acapture%2520fine-grained%2520details%2520pertinent%2520to%2520the%2520target%2520class%252C%2520and%2520an%2520Equivalent%250AMatrix%2520that%2520unifies%2520refined%2520local%2520and%2520global%2520representations%2520to%2520generate%250Acomprehensive%2520saliency%2520maps.%2520Our%2520approach%2520demonstrates%2520superior%2520compatibility%252C%250Aenabling%2520effortless%2520integration%2520into%2520existing%2520residual%2520networks%2520like%2520ResNet%2520and%250ATransformer%2520architectures%2520such%2520as%2520BERT%2520with%2520minimal%2520modifications.%2520Extensive%250Aexperiments%2520on%2520benchmark%2520datasets%2520in%2520medical%2520imaging%2520and%2520text%2520classification%250Ashow%2520that%2520MHEX%2520not%2520only%2520improves%2520classification%2520accuracy%2520but%2520also%2520produces%250Ahighly%2520interpretable%2520and%2520detailed%2520saliency%2520scores.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01311v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Head%20Explainer%3A%20A%20General%20Framework%20to%20Improve%20Explainability%20in%0A%20%20CNNs%20and%20Transformers&entry.906535625=Bohang%20Sun%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20the%20Multi-Head%20Explainer%20%28MHEX%29%2C%20a%20versatile%20and%0Amodular%20framework%20that%20enhances%20both%20the%20explainability%20and%20accuracy%20of%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformer-based%20models.%20MHEX%0Aconsists%20of%20three%20core%20components%3A%20an%20Attention%20Gate%20that%20dynamically%0Ahighlights%20task-relevant%20features%2C%20Deep%20Supervision%20that%20guides%20early%20layers%20to%0Acapture%20fine-grained%20details%20pertinent%20to%20the%20target%20class%2C%20and%20an%20Equivalent%0AMatrix%20that%20unifies%20refined%20local%20and%20global%20representations%20to%20generate%0Acomprehensive%20saliency%20maps.%20Our%20approach%20demonstrates%20superior%20compatibility%2C%0Aenabling%20effortless%20integration%20into%20existing%20residual%20networks%20like%20ResNet%20and%0ATransformer%20architectures%20such%20as%20BERT%20with%20minimal%20modifications.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20in%20medical%20imaging%20and%20text%20classification%0Ashow%20that%20MHEX%20not%20only%20improves%20classification%20accuracy%20but%20also%20produces%0Ahighly%20interpretable%20and%20detailed%20saliency%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01311v2&entry.124074799=Read"},
{"title": "Kriging and Gaussian Process Interpolation for Georeferenced Data\n  Augmentation", "author": "Fr\u00e9d\u00e9rick Fabre Ferber and Dominique Gay and Jean-Christophe Souli\u00e9 and Jean Diatta and Odalric-Ambrym Maillard", "abstract": "  Data augmentation is a crucial step in the development of robust supervised\nlearning models, especially when dealing with limited datasets. This study\nexplores interpolation techniques for the augmentation of geo-referenced data,\nwith the aim of predicting the presence of Commelina benghalensis L. in\nsugarcane plots in La R{\\'e}union. Given the spatial nature of the data and the\nhigh cost of data collection, we evaluated two interpolation approaches:\nGaussian processes (GPs) with different kernels and kriging with various\nvariograms. The objectives of this work are threefold: (i) to identify which\ninterpolation methods offer the best predictive performance for various\nregression algorithms, (ii) to analyze the evolution of performance as a\nfunction of the number of observations added, and (iii) to assess the spatial\nconsistency of augmented datasets. The results show that GP-based methods, in\nparticular with combined kernels (GP-COMB), significantly improve the\nperformance of regression algorithms while requiring less additional data.\nAlthough kriging shows slightly lower performance, it is distinguished by a\nmore homogeneous spatial coverage, a potential advantage in certain contexts.\n", "link": "http://arxiv.org/abs/2501.07183v1", "date": "2025-01-13", "relevancy": 2.5809, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5217}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5194}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kriging%20and%20Gaussian%20Process%20Interpolation%20for%20Georeferenced%20Data%0A%20%20Augmentation&body=Title%3A%20Kriging%20and%20Gaussian%20Process%20Interpolation%20for%20Georeferenced%20Data%0A%20%20Augmentation%0AAuthor%3A%20Fr%C3%A9d%C3%A9rick%20Fabre%20Ferber%20and%20Dominique%20Gay%20and%20Jean-Christophe%20Souli%C3%A9%20and%20Jean%20Diatta%20and%20Odalric-Ambrym%20Maillard%0AAbstract%3A%20%20%20Data%20augmentation%20is%20a%20crucial%20step%20in%20the%20development%20of%20robust%20supervised%0Alearning%20models%2C%20especially%20when%20dealing%20with%20limited%20datasets.%20This%20study%0Aexplores%20interpolation%20techniques%20for%20the%20augmentation%20of%20geo-referenced%20data%2C%0Awith%20the%20aim%20of%20predicting%20the%20presence%20of%20Commelina%20benghalensis%20L.%20in%0Asugarcane%20plots%20in%20La%20R%7B%5C%27e%7Dunion.%20Given%20the%20spatial%20nature%20of%20the%20data%20and%20the%0Ahigh%20cost%20of%20data%20collection%2C%20we%20evaluated%20two%20interpolation%20approaches%3A%0AGaussian%20processes%20%28GPs%29%20with%20different%20kernels%20and%20kriging%20with%20various%0Avariograms.%20The%20objectives%20of%20this%20work%20are%20threefold%3A%20%28i%29%20to%20identify%20which%0Ainterpolation%20methods%20offer%20the%20best%20predictive%20performance%20for%20various%0Aregression%20algorithms%2C%20%28ii%29%20to%20analyze%20the%20evolution%20of%20performance%20as%20a%0Afunction%20of%20the%20number%20of%20observations%20added%2C%20and%20%28iii%29%20to%20assess%20the%20spatial%0Aconsistency%20of%20augmented%20datasets.%20The%20results%20show%20that%20GP-based%20methods%2C%20in%0Aparticular%20with%20combined%20kernels%20%28GP-COMB%29%2C%20significantly%20improve%20the%0Aperformance%20of%20regression%20algorithms%20while%20requiring%20less%20additional%20data.%0AAlthough%20kriging%20shows%20slightly%20lower%20performance%2C%20it%20is%20distinguished%20by%20a%0Amore%20homogeneous%20spatial%20coverage%2C%20a%20potential%20advantage%20in%20certain%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKriging%2520and%2520Gaussian%2520Process%2520Interpolation%2520for%2520Georeferenced%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DFr%25C3%25A9d%25C3%25A9rick%2520Fabre%2520Ferber%2520and%2520Dominique%2520Gay%2520and%2520Jean-Christophe%2520Souli%25C3%25A9%2520and%2520Jean%2520Diatta%2520and%2520Odalric-Ambrym%2520Maillard%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520a%2520crucial%2520step%2520in%2520the%2520development%2520of%2520robust%2520supervised%250Alearning%2520models%252C%2520especially%2520when%2520dealing%2520with%2520limited%2520datasets.%2520This%2520study%250Aexplores%2520interpolation%2520techniques%2520for%2520the%2520augmentation%2520of%2520geo-referenced%2520data%252C%250Awith%2520the%2520aim%2520of%2520predicting%2520the%2520presence%2520of%2520Commelina%2520benghalensis%2520L.%2520in%250Asugarcane%2520plots%2520in%2520La%2520R%257B%255C%2527e%257Dunion.%2520Given%2520the%2520spatial%2520nature%2520of%2520the%2520data%2520and%2520the%250Ahigh%2520cost%2520of%2520data%2520collection%252C%2520we%2520evaluated%2520two%2520interpolation%2520approaches%253A%250AGaussian%2520processes%2520%2528GPs%2529%2520with%2520different%2520kernels%2520and%2520kriging%2520with%2520various%250Avariograms.%2520The%2520objectives%2520of%2520this%2520work%2520are%2520threefold%253A%2520%2528i%2529%2520to%2520identify%2520which%250Ainterpolation%2520methods%2520offer%2520the%2520best%2520predictive%2520performance%2520for%2520various%250Aregression%2520algorithms%252C%2520%2528ii%2529%2520to%2520analyze%2520the%2520evolution%2520of%2520performance%2520as%2520a%250Afunction%2520of%2520the%2520number%2520of%2520observations%2520added%252C%2520and%2520%2528iii%2529%2520to%2520assess%2520the%2520spatial%250Aconsistency%2520of%2520augmented%2520datasets.%2520The%2520results%2520show%2520that%2520GP-based%2520methods%252C%2520in%250Aparticular%2520with%2520combined%2520kernels%2520%2528GP-COMB%2529%252C%2520significantly%2520improve%2520the%250Aperformance%2520of%2520regression%2520algorithms%2520while%2520requiring%2520less%2520additional%2520data.%250AAlthough%2520kriging%2520shows%2520slightly%2520lower%2520performance%252C%2520it%2520is%2520distinguished%2520by%2520a%250Amore%2520homogeneous%2520spatial%2520coverage%252C%2520a%2520potential%2520advantage%2520in%2520certain%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kriging%20and%20Gaussian%20Process%20Interpolation%20for%20Georeferenced%20Data%0A%20%20Augmentation&entry.906535625=Fr%C3%A9d%C3%A9rick%20Fabre%20Ferber%20and%20Dominique%20Gay%20and%20Jean-Christophe%20Souli%C3%A9%20and%20Jean%20Diatta%20and%20Odalric-Ambrym%20Maillard&entry.1292438233=%20%20Data%20augmentation%20is%20a%20crucial%20step%20in%20the%20development%20of%20robust%20supervised%0Alearning%20models%2C%20especially%20when%20dealing%20with%20limited%20datasets.%20This%20study%0Aexplores%20interpolation%20techniques%20for%20the%20augmentation%20of%20geo-referenced%20data%2C%0Awith%20the%20aim%20of%20predicting%20the%20presence%20of%20Commelina%20benghalensis%20L.%20in%0Asugarcane%20plots%20in%20La%20R%7B%5C%27e%7Dunion.%20Given%20the%20spatial%20nature%20of%20the%20data%20and%20the%0Ahigh%20cost%20of%20data%20collection%2C%20we%20evaluated%20two%20interpolation%20approaches%3A%0AGaussian%20processes%20%28GPs%29%20with%20different%20kernels%20and%20kriging%20with%20various%0Avariograms.%20The%20objectives%20of%20this%20work%20are%20threefold%3A%20%28i%29%20to%20identify%20which%0Ainterpolation%20methods%20offer%20the%20best%20predictive%20performance%20for%20various%0Aregression%20algorithms%2C%20%28ii%29%20to%20analyze%20the%20evolution%20of%20performance%20as%20a%0Afunction%20of%20the%20number%20of%20observations%20added%2C%20and%20%28iii%29%20to%20assess%20the%20spatial%0Aconsistency%20of%20augmented%20datasets.%20The%20results%20show%20that%20GP-based%20methods%2C%20in%0Aparticular%20with%20combined%20kernels%20%28GP-COMB%29%2C%20significantly%20improve%20the%0Aperformance%20of%20regression%20algorithms%20while%20requiring%20less%20additional%20data.%0AAlthough%20kriging%20shows%20slightly%20lower%20performance%2C%20it%20is%20distinguished%20by%20a%0Amore%20homogeneous%20spatial%20coverage%2C%20a%20potential%20advantage%20in%20certain%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07183v1&entry.124074799=Read"},
{"title": "Three-view Focal Length Recovery From Homographies", "author": "Yaqing Ding and Viktor Kocur and Zuzana Berger Haladov\u00e1 and Qianliang Wu and Shen Cai and Jian Yang and Zuzana Kukelova", "abstract": "  In this paper, we propose a novel approach for recovering focal lengths from\nthree-view homographies. By examining the consistency of normal vectors between\ntwo homographies, we derive new explicit constraints between the focal lengths\nand homographies using an elimination technique. We demonstrate that three-view\nhomographies provide two additional constraints, enabling the recovery of one\nor two focal lengths. We discuss four possible cases, including three cameras\nhaving an unknown equal focal length, three cameras having two different\nunknown focal lengths, three cameras where one focal length is known, and the\nother two cameras have equal or different unknown focal lengths. All the\nproblems can be converted into solving polynomials in one or two unknowns,\nwhich can be efficiently solved using Sturm sequence or hidden variable\ntechnique. Evaluation using both synthetic and real data shows that the\nproposed solvers are both faster and more accurate than methods relying on\nexisting two-view solvers. The code and data are available on\nhttps://github.com/kocurvik/hf\n", "link": "http://arxiv.org/abs/2501.07499v1", "date": "2025-01-13", "relevancy": 2.5745, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5161}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5161}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Three-view%20Focal%20Length%20Recovery%20From%20Homographies&body=Title%3A%20Three-view%20Focal%20Length%20Recovery%20From%20Homographies%0AAuthor%3A%20Yaqing%20Ding%20and%20Viktor%20Kocur%20and%20Zuzana%20Berger%20Haladov%C3%A1%20and%20Qianliang%20Wu%20and%20Shen%20Cai%20and%20Jian%20Yang%20and%20Zuzana%20Kukelova%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20for%20recovering%20focal%20lengths%20from%0Athree-view%20homographies.%20By%20examining%20the%20consistency%20of%20normal%20vectors%20between%0Atwo%20homographies%2C%20we%20derive%20new%20explicit%20constraints%20between%20the%20focal%20lengths%0Aand%20homographies%20using%20an%20elimination%20technique.%20We%20demonstrate%20that%20three-view%0Ahomographies%20provide%20two%20additional%20constraints%2C%20enabling%20the%20recovery%20of%20one%0Aor%20two%20focal%20lengths.%20We%20discuss%20four%20possible%20cases%2C%20including%20three%20cameras%0Ahaving%20an%20unknown%20equal%20focal%20length%2C%20three%20cameras%20having%20two%20different%0Aunknown%20focal%20lengths%2C%20three%20cameras%20where%20one%20focal%20length%20is%20known%2C%20and%20the%0Aother%20two%20cameras%20have%20equal%20or%20different%20unknown%20focal%20lengths.%20All%20the%0Aproblems%20can%20be%20converted%20into%20solving%20polynomials%20in%20one%20or%20two%20unknowns%2C%0Awhich%20can%20be%20efficiently%20solved%20using%20Sturm%20sequence%20or%20hidden%20variable%0Atechnique.%20Evaluation%20using%20both%20synthetic%20and%20real%20data%20shows%20that%20the%0Aproposed%20solvers%20are%20both%20faster%20and%20more%20accurate%20than%20methods%20relying%20on%0Aexisting%20two-view%20solvers.%20The%20code%20and%20data%20are%20available%20on%0Ahttps%3A//github.com/kocurvik/hf%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThree-view%2520Focal%2520Length%2520Recovery%2520From%2520Homographies%26entry.906535625%3DYaqing%2520Ding%2520and%2520Viktor%2520Kocur%2520and%2520Zuzana%2520Berger%2520Haladov%25C3%25A1%2520and%2520Qianliang%2520Wu%2520and%2520Shen%2520Cai%2520and%2520Jian%2520Yang%2520and%2520Zuzana%2520Kukelova%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%2520recovering%2520focal%2520lengths%2520from%250Athree-view%2520homographies.%2520By%2520examining%2520the%2520consistency%2520of%2520normal%2520vectors%2520between%250Atwo%2520homographies%252C%2520we%2520derive%2520new%2520explicit%2520constraints%2520between%2520the%2520focal%2520lengths%250Aand%2520homographies%2520using%2520an%2520elimination%2520technique.%2520We%2520demonstrate%2520that%2520three-view%250Ahomographies%2520provide%2520two%2520additional%2520constraints%252C%2520enabling%2520the%2520recovery%2520of%2520one%250Aor%2520two%2520focal%2520lengths.%2520We%2520discuss%2520four%2520possible%2520cases%252C%2520including%2520three%2520cameras%250Ahaving%2520an%2520unknown%2520equal%2520focal%2520length%252C%2520three%2520cameras%2520having%2520two%2520different%250Aunknown%2520focal%2520lengths%252C%2520three%2520cameras%2520where%2520one%2520focal%2520length%2520is%2520known%252C%2520and%2520the%250Aother%2520two%2520cameras%2520have%2520equal%2520or%2520different%2520unknown%2520focal%2520lengths.%2520All%2520the%250Aproblems%2520can%2520be%2520converted%2520into%2520solving%2520polynomials%2520in%2520one%2520or%2520two%2520unknowns%252C%250Awhich%2520can%2520be%2520efficiently%2520solved%2520using%2520Sturm%2520sequence%2520or%2520hidden%2520variable%250Atechnique.%2520Evaluation%2520using%2520both%2520synthetic%2520and%2520real%2520data%2520shows%2520that%2520the%250Aproposed%2520solvers%2520are%2520both%2520faster%2520and%2520more%2520accurate%2520than%2520methods%2520relying%2520on%250Aexisting%2520two-view%2520solvers.%2520The%2520code%2520and%2520data%2520are%2520available%2520on%250Ahttps%253A//github.com/kocurvik/hf%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Three-view%20Focal%20Length%20Recovery%20From%20Homographies&entry.906535625=Yaqing%20Ding%20and%20Viktor%20Kocur%20and%20Zuzana%20Berger%20Haladov%C3%A1%20and%20Qianliang%20Wu%20and%20Shen%20Cai%20and%20Jian%20Yang%20and%20Zuzana%20Kukelova&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20for%20recovering%20focal%20lengths%20from%0Athree-view%20homographies.%20By%20examining%20the%20consistency%20of%20normal%20vectors%20between%0Atwo%20homographies%2C%20we%20derive%20new%20explicit%20constraints%20between%20the%20focal%20lengths%0Aand%20homographies%20using%20an%20elimination%20technique.%20We%20demonstrate%20that%20three-view%0Ahomographies%20provide%20two%20additional%20constraints%2C%20enabling%20the%20recovery%20of%20one%0Aor%20two%20focal%20lengths.%20We%20discuss%20four%20possible%20cases%2C%20including%20three%20cameras%0Ahaving%20an%20unknown%20equal%20focal%20length%2C%20three%20cameras%20having%20two%20different%0Aunknown%20focal%20lengths%2C%20three%20cameras%20where%20one%20focal%20length%20is%20known%2C%20and%20the%0Aother%20two%20cameras%20have%20equal%20or%20different%20unknown%20focal%20lengths.%20All%20the%0Aproblems%20can%20be%20converted%20into%20solving%20polynomials%20in%20one%20or%20two%20unknowns%2C%0Awhich%20can%20be%20efficiently%20solved%20using%20Sturm%20sequence%20or%20hidden%20variable%0Atechnique.%20Evaluation%20using%20both%20synthetic%20and%20real%20data%20shows%20that%20the%0Aproposed%20solvers%20are%20both%20faster%20and%20more%20accurate%20than%20methods%20relying%20on%0Aexisting%20two-view%20solvers.%20The%20code%20and%20data%20are%20available%20on%0Ahttps%3A//github.com/kocurvik/hf%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07499v1&entry.124074799=Read"},
{"title": "Lifelong Learning of Large Language Model based Agents: A Roadmap", "author": "Junhao Zheng and Chengming Shi and Xidi Cai and Qiuke Li and Duzhen Zhang and Chenxing Li and Dong Yu and Qianli Ma", "abstract": "  Lifelong learning, also known as continual or incremental learning, is a\ncrucial component for advancing Artificial General Intelligence (AGI) by\nenabling systems to continuously adapt in dynamic environments. While large\nlanguage models (LLMs) have demonstrated impressive capabilities in natural\nlanguage processing, existing LLM agents are typically designed for static\nsystems and lack the ability to adapt over time in response to new challenges.\nThis survey is the first to systematically summarize the potential techniques\nfor incorporating lifelong learning into LLM-based agents. We categorize the\ncore components of these agents into three modules: the perception module for\nmultimodal input integration, the memory module for storing and retrieving\nevolving knowledge, and the action module for grounded interactions with the\ndynamic environment. We highlight how these pillars collectively enable\ncontinuous adaptation, mitigate catastrophic forgetting, and improve long-term\nperformance. This survey provides a roadmap for researchers and practitioners\nworking to develop lifelong learning capabilities in LLM agents, offering\ninsights into emerging trends, evaluation metrics, and application scenarios.\nRelevant literature and resources are available at \\href{this\nurl}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}.\n", "link": "http://arxiv.org/abs/2501.07278v1", "date": "2025-01-13", "relevancy": 2.5668, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lifelong%20Learning%20of%20Large%20Language%20Model%20based%20Agents%3A%20A%20Roadmap&body=Title%3A%20Lifelong%20Learning%20of%20Large%20Language%20Model%20based%20Agents%3A%20A%20Roadmap%0AAuthor%3A%20Junhao%20Zheng%20and%20Chengming%20Shi%20and%20Xidi%20Cai%20and%20Qiuke%20Li%20and%20Duzhen%20Zhang%20and%20Chenxing%20Li%20and%20Dong%20Yu%20and%20Qianli%20Ma%0AAbstract%3A%20%20%20Lifelong%20learning%2C%20also%20known%20as%20continual%20or%20incremental%20learning%2C%20is%20a%0Acrucial%20component%20for%20advancing%20Artificial%20General%20Intelligence%20%28AGI%29%20by%0Aenabling%20systems%20to%20continuously%20adapt%20in%20dynamic%20environments.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%20natural%0Alanguage%20processing%2C%20existing%20LLM%20agents%20are%20typically%20designed%20for%20static%0Asystems%20and%20lack%20the%20ability%20to%20adapt%20over%20time%20in%20response%20to%20new%20challenges.%0AThis%20survey%20is%20the%20first%20to%20systematically%20summarize%20the%20potential%20techniques%0Afor%20incorporating%20lifelong%20learning%20into%20LLM-based%20agents.%20We%20categorize%20the%0Acore%20components%20of%20these%20agents%20into%20three%20modules%3A%20the%20perception%20module%20for%0Amultimodal%20input%20integration%2C%20the%20memory%20module%20for%20storing%20and%20retrieving%0Aevolving%20knowledge%2C%20and%20the%20action%20module%20for%20grounded%20interactions%20with%20the%0Adynamic%20environment.%20We%20highlight%20how%20these%20pillars%20collectively%20enable%0Acontinuous%20adaptation%2C%20mitigate%20catastrophic%20forgetting%2C%20and%20improve%20long-term%0Aperformance.%20This%20survey%20provides%20a%20roadmap%20for%20researchers%20and%20practitioners%0Aworking%20to%20develop%20lifelong%20learning%20capabilities%20in%20LLM%20agents%2C%20offering%0Ainsights%20into%20emerging%20trends%2C%20evaluation%20metrics%2C%20and%20application%20scenarios.%0ARelevant%20literature%20and%20resources%20are%20available%20at%20%5Chref%7Bthis%0Aurl%7D%7Bhttps%3A//github.com/qianlima-lab/awesome-lifelong-llm-agent%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLifelong%2520Learning%2520of%2520Large%2520Language%2520Model%2520based%2520Agents%253A%2520A%2520Roadmap%26entry.906535625%3DJunhao%2520Zheng%2520and%2520Chengming%2520Shi%2520and%2520Xidi%2520Cai%2520and%2520Qiuke%2520Li%2520and%2520Duzhen%2520Zhang%2520and%2520Chenxing%2520Li%2520and%2520Dong%2520Yu%2520and%2520Qianli%2520Ma%26entry.1292438233%3D%2520%2520Lifelong%2520learning%252C%2520also%2520known%2520as%2520continual%2520or%2520incremental%2520learning%252C%2520is%2520a%250Acrucial%2520component%2520for%2520advancing%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529%2520by%250Aenabling%2520systems%2520to%2520continuously%2520adapt%2520in%2520dynamic%2520environments.%2520While%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%2520natural%250Alanguage%2520processing%252C%2520existing%2520LLM%2520agents%2520are%2520typically%2520designed%2520for%2520static%250Asystems%2520and%2520lack%2520the%2520ability%2520to%2520adapt%2520over%2520time%2520in%2520response%2520to%2520new%2520challenges.%250AThis%2520survey%2520is%2520the%2520first%2520to%2520systematically%2520summarize%2520the%2520potential%2520techniques%250Afor%2520incorporating%2520lifelong%2520learning%2520into%2520LLM-based%2520agents.%2520We%2520categorize%2520the%250Acore%2520components%2520of%2520these%2520agents%2520into%2520three%2520modules%253A%2520the%2520perception%2520module%2520for%250Amultimodal%2520input%2520integration%252C%2520the%2520memory%2520module%2520for%2520storing%2520and%2520retrieving%250Aevolving%2520knowledge%252C%2520and%2520the%2520action%2520module%2520for%2520grounded%2520interactions%2520with%2520the%250Adynamic%2520environment.%2520We%2520highlight%2520how%2520these%2520pillars%2520collectively%2520enable%250Acontinuous%2520adaptation%252C%2520mitigate%2520catastrophic%2520forgetting%252C%2520and%2520improve%2520long-term%250Aperformance.%2520This%2520survey%2520provides%2520a%2520roadmap%2520for%2520researchers%2520and%2520practitioners%250Aworking%2520to%2520develop%2520lifelong%2520learning%2520capabilities%2520in%2520LLM%2520agents%252C%2520offering%250Ainsights%2520into%2520emerging%2520trends%252C%2520evaluation%2520metrics%252C%2520and%2520application%2520scenarios.%250ARelevant%2520literature%2520and%2520resources%2520are%2520available%2520at%2520%255Chref%257Bthis%250Aurl%257D%257Bhttps%253A//github.com/qianlima-lab/awesome-lifelong-llm-agent%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lifelong%20Learning%20of%20Large%20Language%20Model%20based%20Agents%3A%20A%20Roadmap&entry.906535625=Junhao%20Zheng%20and%20Chengming%20Shi%20and%20Xidi%20Cai%20and%20Qiuke%20Li%20and%20Duzhen%20Zhang%20and%20Chenxing%20Li%20and%20Dong%20Yu%20and%20Qianli%20Ma&entry.1292438233=%20%20Lifelong%20learning%2C%20also%20known%20as%20continual%20or%20incremental%20learning%2C%20is%20a%0Acrucial%20component%20for%20advancing%20Artificial%20General%20Intelligence%20%28AGI%29%20by%0Aenabling%20systems%20to%20continuously%20adapt%20in%20dynamic%20environments.%20While%20large%0Alanguage%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%20natural%0Alanguage%20processing%2C%20existing%20LLM%20agents%20are%20typically%20designed%20for%20static%0Asystems%20and%20lack%20the%20ability%20to%20adapt%20over%20time%20in%20response%20to%20new%20challenges.%0AThis%20survey%20is%20the%20first%20to%20systematically%20summarize%20the%20potential%20techniques%0Afor%20incorporating%20lifelong%20learning%20into%20LLM-based%20agents.%20We%20categorize%20the%0Acore%20components%20of%20these%20agents%20into%20three%20modules%3A%20the%20perception%20module%20for%0Amultimodal%20input%20integration%2C%20the%20memory%20module%20for%20storing%20and%20retrieving%0Aevolving%20knowledge%2C%20and%20the%20action%20module%20for%20grounded%20interactions%20with%20the%0Adynamic%20environment.%20We%20highlight%20how%20these%20pillars%20collectively%20enable%0Acontinuous%20adaptation%2C%20mitigate%20catastrophic%20forgetting%2C%20and%20improve%20long-term%0Aperformance.%20This%20survey%20provides%20a%20roadmap%20for%20researchers%20and%20practitioners%0Aworking%20to%20develop%20lifelong%20learning%20capabilities%20in%20LLM%20agents%2C%20offering%0Ainsights%20into%20emerging%20trends%2C%20evaluation%20metrics%2C%20and%20application%20scenarios.%0ARelevant%20literature%20and%20resources%20are%20available%20at%20%5Chref%7Bthis%0Aurl%7D%7Bhttps%3A//github.com/qianlima-lab/awesome-lifelong-llm-agent%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07278v1&entry.124074799=Read"},
{"title": "Light Transport-aware Diffusion Posterior Sampling for Single-View\n  Reconstruction of 3D Volumes", "author": "Ludwic Leonard and Nils Thuerey and Ruediger Westermann", "abstract": "  We introduce a single-view reconstruction technique of volumetric fields in\nwhich multiple light scattering effects are omnipresent, such as in clouds. We\nmodel the unknown distribution of volumetric fields using an unconditional\ndiffusion model trained on a novel benchmark dataset comprising 1,000\nsynthetically simulated volumetric density fields. The neural diffusion model\nis trained on the latent codes of a novel, diffusion-friendly, monoplanar\nrepresentation. The generative model is used to incorporate a tailored\nparametric diffusion posterior sampling technique into different reconstruction\ntasks. A physically-based differentiable volume renderer is employed to provide\ngradients with respect to light transport in the latent space. This stands in\ncontrast to classic NeRF approaches and makes the reconstructions better\naligned with observed data. Through various experiments, we demonstrate\nsingle-view reconstruction of volumetric clouds at a previously unattainable\nquality.\n", "link": "http://arxiv.org/abs/2501.05226v2", "date": "2025-01-13", "relevancy": 2.541, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6422}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6422}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light%20Transport-aware%20Diffusion%20Posterior%20Sampling%20for%20Single-View%0A%20%20Reconstruction%20of%203D%20Volumes&body=Title%3A%20Light%20Transport-aware%20Diffusion%20Posterior%20Sampling%20for%20Single-View%0A%20%20Reconstruction%20of%203D%20Volumes%0AAuthor%3A%20Ludwic%20Leonard%20and%20Nils%20Thuerey%20and%20Ruediger%20Westermann%0AAbstract%3A%20%20%20We%20introduce%20a%20single-view%20reconstruction%20technique%20of%20volumetric%20fields%20in%0Awhich%20multiple%20light%20scattering%20effects%20are%20omnipresent%2C%20such%20as%20in%20clouds.%20We%0Amodel%20the%20unknown%20distribution%20of%20volumetric%20fields%20using%20an%20unconditional%0Adiffusion%20model%20trained%20on%20a%20novel%20benchmark%20dataset%20comprising%201%2C000%0Asynthetically%20simulated%20volumetric%20density%20fields.%20The%20neural%20diffusion%20model%0Ais%20trained%20on%20the%20latent%20codes%20of%20a%20novel%2C%20diffusion-friendly%2C%20monoplanar%0Arepresentation.%20The%20generative%20model%20is%20used%20to%20incorporate%20a%20tailored%0Aparametric%20diffusion%20posterior%20sampling%20technique%20into%20different%20reconstruction%0Atasks.%20A%20physically-based%20differentiable%20volume%20renderer%20is%20employed%20to%20provide%0Agradients%20with%20respect%20to%20light%20transport%20in%20the%20latent%20space.%20This%20stands%20in%0Acontrast%20to%20classic%20NeRF%20approaches%20and%20makes%20the%20reconstructions%20better%0Aaligned%20with%20observed%20data.%20Through%20various%20experiments%2C%20we%20demonstrate%0Asingle-view%20reconstruction%20of%20volumetric%20clouds%20at%20a%20previously%20unattainable%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05226v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight%2520Transport-aware%2520Diffusion%2520Posterior%2520Sampling%2520for%2520Single-View%250A%2520%2520Reconstruction%2520of%25203D%2520Volumes%26entry.906535625%3DLudwic%2520Leonard%2520and%2520Nils%2520Thuerey%2520and%2520Ruediger%2520Westermann%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520single-view%2520reconstruction%2520technique%2520of%2520volumetric%2520fields%2520in%250Awhich%2520multiple%2520light%2520scattering%2520effects%2520are%2520omnipresent%252C%2520such%2520as%2520in%2520clouds.%2520We%250Amodel%2520the%2520unknown%2520distribution%2520of%2520volumetric%2520fields%2520using%2520an%2520unconditional%250Adiffusion%2520model%2520trained%2520on%2520a%2520novel%2520benchmark%2520dataset%2520comprising%25201%252C000%250Asynthetically%2520simulated%2520volumetric%2520density%2520fields.%2520The%2520neural%2520diffusion%2520model%250Ais%2520trained%2520on%2520the%2520latent%2520codes%2520of%2520a%2520novel%252C%2520diffusion-friendly%252C%2520monoplanar%250Arepresentation.%2520The%2520generative%2520model%2520is%2520used%2520to%2520incorporate%2520a%2520tailored%250Aparametric%2520diffusion%2520posterior%2520sampling%2520technique%2520into%2520different%2520reconstruction%250Atasks.%2520A%2520physically-based%2520differentiable%2520volume%2520renderer%2520is%2520employed%2520to%2520provide%250Agradients%2520with%2520respect%2520to%2520light%2520transport%2520in%2520the%2520latent%2520space.%2520This%2520stands%2520in%250Acontrast%2520to%2520classic%2520NeRF%2520approaches%2520and%2520makes%2520the%2520reconstructions%2520better%250Aaligned%2520with%2520observed%2520data.%2520Through%2520various%2520experiments%252C%2520we%2520demonstrate%250Asingle-view%2520reconstruction%2520of%2520volumetric%2520clouds%2520at%2520a%2520previously%2520unattainable%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05226v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light%20Transport-aware%20Diffusion%20Posterior%20Sampling%20for%20Single-View%0A%20%20Reconstruction%20of%203D%20Volumes&entry.906535625=Ludwic%20Leonard%20and%20Nils%20Thuerey%20and%20Ruediger%20Westermann&entry.1292438233=%20%20We%20introduce%20a%20single-view%20reconstruction%20technique%20of%20volumetric%20fields%20in%0Awhich%20multiple%20light%20scattering%20effects%20are%20omnipresent%2C%20such%20as%20in%20clouds.%20We%0Amodel%20the%20unknown%20distribution%20of%20volumetric%20fields%20using%20an%20unconditional%0Adiffusion%20model%20trained%20on%20a%20novel%20benchmark%20dataset%20comprising%201%2C000%0Asynthetically%20simulated%20volumetric%20density%20fields.%20The%20neural%20diffusion%20model%0Ais%20trained%20on%20the%20latent%20codes%20of%20a%20novel%2C%20diffusion-friendly%2C%20monoplanar%0Arepresentation.%20The%20generative%20model%20is%20used%20to%20incorporate%20a%20tailored%0Aparametric%20diffusion%20posterior%20sampling%20technique%20into%20different%20reconstruction%0Atasks.%20A%20physically-based%20differentiable%20volume%20renderer%20is%20employed%20to%20provide%0Agradients%20with%20respect%20to%20light%20transport%20in%20the%20latent%20space.%20This%20stands%20in%0Acontrast%20to%20classic%20NeRF%20approaches%20and%20makes%20the%20reconstructions%20better%0Aaligned%20with%20observed%20data.%20Through%20various%20experiments%2C%20we%20demonstrate%0Asingle-view%20reconstruction%20of%20volumetric%20clouds%20at%20a%20previously%20unattainable%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05226v2&entry.124074799=Read"},
{"title": "QuadWBG: Generalizable Quadrupedal Whole-Body Grasping", "author": "Jilong Wang and Javokhirbek Rajabov and Chaoyi Xu and Yiming Zheng and He Wang", "abstract": "  Legged robots with advanced manipulation capabilities have the potential to\nsignificantly improve household duties and urban maintenance. Despite\nconsiderable progress in developing robust locomotion and precise manipulation\nmethods, seamlessly integrating these into cohesive whole-body control for\nreal-world applications remains challenging. In this paper, we present a\nmodular framework for robust and generalizable whole-body loco-manipulation\ncontroller based on a single arm-mounted camera. By using reinforcement\nlearning (RL), we enable a robust low-level policy for command execution over 5\ndimensions (5D) and a grasp-aware high-level policy guided by a novel metric,\nGeneralized Oriented Reachability Map (GORM). The proposed system achieves\nstate-of-the-art one-time grasping accuracy of 89% in the real world, including\nchallenging tasks such as grasping transparent objects. Through extensive\nsimulations and real-world experiments, we demonstrate that our system can\neffectively manage a large workspace, from floor level to above body height,\nand perform diverse whole-body loco-manipulation tasks.\n", "link": "http://arxiv.org/abs/2411.06782v2", "date": "2025-01-13", "relevancy": 2.5382, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6493}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuadWBG%3A%20Generalizable%20Quadrupedal%20Whole-Body%20Grasping&body=Title%3A%20QuadWBG%3A%20Generalizable%20Quadrupedal%20Whole-Body%20Grasping%0AAuthor%3A%20Jilong%20Wang%20and%20Javokhirbek%20Rajabov%20and%20Chaoyi%20Xu%20and%20Yiming%20Zheng%20and%20He%20Wang%0AAbstract%3A%20%20%20Legged%20robots%20with%20advanced%20manipulation%20capabilities%20have%20the%20potential%20to%0Asignificantly%20improve%20household%20duties%20and%20urban%20maintenance.%20Despite%0Aconsiderable%20progress%20in%20developing%20robust%20locomotion%20and%20precise%20manipulation%0Amethods%2C%20seamlessly%20integrating%20these%20into%20cohesive%20whole-body%20control%20for%0Areal-world%20applications%20remains%20challenging.%20In%20this%20paper%2C%20we%20present%20a%0Amodular%20framework%20for%20robust%20and%20generalizable%20whole-body%20loco-manipulation%0Acontroller%20based%20on%20a%20single%20arm-mounted%20camera.%20By%20using%20reinforcement%0Alearning%20%28RL%29%2C%20we%20enable%20a%20robust%20low-level%20policy%20for%20command%20execution%20over%205%0Adimensions%20%285D%29%20and%20a%20grasp-aware%20high-level%20policy%20guided%20by%20a%20novel%20metric%2C%0AGeneralized%20Oriented%20Reachability%20Map%20%28GORM%29.%20The%20proposed%20system%20achieves%0Astate-of-the-art%20one-time%20grasping%20accuracy%20of%2089%25%20in%20the%20real%20world%2C%20including%0Achallenging%20tasks%20such%20as%20grasping%20transparent%20objects.%20Through%20extensive%0Asimulations%20and%20real-world%20experiments%2C%20we%20demonstrate%20that%20our%20system%20can%0Aeffectively%20manage%20a%20large%20workspace%2C%20from%20floor%20level%20to%20above%20body%20height%2C%0Aand%20perform%20diverse%20whole-body%20loco-manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06782v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuadWBG%253A%2520Generalizable%2520Quadrupedal%2520Whole-Body%2520Grasping%26entry.906535625%3DJilong%2520Wang%2520and%2520Javokhirbek%2520Rajabov%2520and%2520Chaoyi%2520Xu%2520and%2520Yiming%2520Zheng%2520and%2520He%2520Wang%26entry.1292438233%3D%2520%2520Legged%2520robots%2520with%2520advanced%2520manipulation%2520capabilities%2520have%2520the%2520potential%2520to%250Asignificantly%2520improve%2520household%2520duties%2520and%2520urban%2520maintenance.%2520Despite%250Aconsiderable%2520progress%2520in%2520developing%2520robust%2520locomotion%2520and%2520precise%2520manipulation%250Amethods%252C%2520seamlessly%2520integrating%2520these%2520into%2520cohesive%2520whole-body%2520control%2520for%250Areal-world%2520applications%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Amodular%2520framework%2520for%2520robust%2520and%2520generalizable%2520whole-body%2520loco-manipulation%250Acontroller%2520based%2520on%2520a%2520single%2520arm-mounted%2520camera.%2520By%2520using%2520reinforcement%250Alearning%2520%2528RL%2529%252C%2520we%2520enable%2520a%2520robust%2520low-level%2520policy%2520for%2520command%2520execution%2520over%25205%250Adimensions%2520%25285D%2529%2520and%2520a%2520grasp-aware%2520high-level%2520policy%2520guided%2520by%2520a%2520novel%2520metric%252C%250AGeneralized%2520Oriented%2520Reachability%2520Map%2520%2528GORM%2529.%2520The%2520proposed%2520system%2520achieves%250Astate-of-the-art%2520one-time%2520grasping%2520accuracy%2520of%252089%2525%2520in%2520the%2520real%2520world%252C%2520including%250Achallenging%2520tasks%2520such%2520as%2520grasping%2520transparent%2520objects.%2520Through%2520extensive%250Asimulations%2520and%2520real-world%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520system%2520can%250Aeffectively%2520manage%2520a%2520large%2520workspace%252C%2520from%2520floor%2520level%2520to%2520above%2520body%2520height%252C%250Aand%2520perform%2520diverse%2520whole-body%2520loco-manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06782v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuadWBG%3A%20Generalizable%20Quadrupedal%20Whole-Body%20Grasping&entry.906535625=Jilong%20Wang%20and%20Javokhirbek%20Rajabov%20and%20Chaoyi%20Xu%20and%20Yiming%20Zheng%20and%20He%20Wang&entry.1292438233=%20%20Legged%20robots%20with%20advanced%20manipulation%20capabilities%20have%20the%20potential%20to%0Asignificantly%20improve%20household%20duties%20and%20urban%20maintenance.%20Despite%0Aconsiderable%20progress%20in%20developing%20robust%20locomotion%20and%20precise%20manipulation%0Amethods%2C%20seamlessly%20integrating%20these%20into%20cohesive%20whole-body%20control%20for%0Areal-world%20applications%20remains%20challenging.%20In%20this%20paper%2C%20we%20present%20a%0Amodular%20framework%20for%20robust%20and%20generalizable%20whole-body%20loco-manipulation%0Acontroller%20based%20on%20a%20single%20arm-mounted%20camera.%20By%20using%20reinforcement%0Alearning%20%28RL%29%2C%20we%20enable%20a%20robust%20low-level%20policy%20for%20command%20execution%20over%205%0Adimensions%20%285D%29%20and%20a%20grasp-aware%20high-level%20policy%20guided%20by%20a%20novel%20metric%2C%0AGeneralized%20Oriented%20Reachability%20Map%20%28GORM%29.%20The%20proposed%20system%20achieves%0Astate-of-the-art%20one-time%20grasping%20accuracy%20of%2089%25%20in%20the%20real%20world%2C%20including%0Achallenging%20tasks%20such%20as%20grasping%20transparent%20objects.%20Through%20extensive%0Asimulations%20and%20real-world%20experiments%2C%20we%20demonstrate%20that%20our%20system%20can%0Aeffectively%20manage%20a%20large%20workspace%2C%20from%20floor%20level%20to%20above%20body%20height%2C%0Aand%20perform%20diverse%20whole-body%20loco-manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06782v2&entry.124074799=Read"},
{"title": "VAGeo: View-specific Attention for Cross-View Object Geo-Localization", "author": "Zhongyang Li and Xin Yuan and Wei Liu and Xin Xu", "abstract": "  Cross-view object geo-localization (CVOGL) aims to locate an object of\ninterest in a captured ground- or drone-view image within the satellite image.\nHowever, existing works treat ground-view and drone-view query images\nequivalently, overlooking their inherent viewpoint discrepancies and the\nspatial correlation between the query image and the satellite-view reference\nimage. To this end, this paper proposes a novel View-specific Attention\nGeo-localization method (VAGeo) for accurate CVOGL. Specifically, VAGeo\ncontains two key modules: view-specific positional encoding (VSPE) module and\nchannel-spatial hybrid attention (CSHA) module. In object-level, according to\nthe characteristics of different viewpoints of ground and drone query images,\nviewpoint-specific positional codings are designed to more accurately identify\nthe click-point object of the query image in the VSPE module. In feature-level,\na hybrid attention in the CSHA module is introduced by combining channel\nattention and spatial attention mechanisms simultaneously for learning\ndiscriminative features. Extensive experimental results demonstrate that the\nproposed VAGeo gains a significant performance improvement, i.e., improving\nacc@0.25/acc@0.5 on the CVOGL dataset from 45.43%/42.24% to 48.21%/45.22% for\nground-view, and from 61.97%/57.66% to 66.19%/61.87% for drone-view.\n", "link": "http://arxiv.org/abs/2501.07194v1", "date": "2025-01-13", "relevancy": 2.5058, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VAGeo%3A%20View-specific%20Attention%20for%20Cross-View%20Object%20Geo-Localization&body=Title%3A%20VAGeo%3A%20View-specific%20Attention%20for%20Cross-View%20Object%20Geo-Localization%0AAuthor%3A%20Zhongyang%20Li%20and%20Xin%20Yuan%20and%20Wei%20Liu%20and%20Xin%20Xu%0AAbstract%3A%20%20%20Cross-view%20object%20geo-localization%20%28CVOGL%29%20aims%20to%20locate%20an%20object%20of%0Ainterest%20in%20a%20captured%20ground-%20or%20drone-view%20image%20within%20the%20satellite%20image.%0AHowever%2C%20existing%20works%20treat%20ground-view%20and%20drone-view%20query%20images%0Aequivalently%2C%20overlooking%20their%20inherent%20viewpoint%20discrepancies%20and%20the%0Aspatial%20correlation%20between%20the%20query%20image%20and%20the%20satellite-view%20reference%0Aimage.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20View-specific%20Attention%0AGeo-localization%20method%20%28VAGeo%29%20for%20accurate%20CVOGL.%20Specifically%2C%20VAGeo%0Acontains%20two%20key%20modules%3A%20view-specific%20positional%20encoding%20%28VSPE%29%20module%20and%0Achannel-spatial%20hybrid%20attention%20%28CSHA%29%20module.%20In%20object-level%2C%20according%20to%0Athe%20characteristics%20of%20different%20viewpoints%20of%20ground%20and%20drone%20query%20images%2C%0Aviewpoint-specific%20positional%20codings%20are%20designed%20to%20more%20accurately%20identify%0Athe%20click-point%20object%20of%20the%20query%20image%20in%20the%20VSPE%20module.%20In%20feature-level%2C%0Aa%20hybrid%20attention%20in%20the%20CSHA%20module%20is%20introduced%20by%20combining%20channel%0Aattention%20and%20spatial%20attention%20mechanisms%20simultaneously%20for%20learning%0Adiscriminative%20features.%20Extensive%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20VAGeo%20gains%20a%20significant%20performance%20improvement%2C%20i.e.%2C%20improving%0Aacc%400.25/acc%400.5%20on%20the%20CVOGL%20dataset%20from%2045.43%25/42.24%25%20to%2048.21%25/45.22%25%20for%0Aground-view%2C%20and%20from%2061.97%25/57.66%25%20to%2066.19%25/61.87%25%20for%20drone-view.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07194v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVAGeo%253A%2520View-specific%2520Attention%2520for%2520Cross-View%2520Object%2520Geo-Localization%26entry.906535625%3DZhongyang%2520Li%2520and%2520Xin%2520Yuan%2520and%2520Wei%2520Liu%2520and%2520Xin%2520Xu%26entry.1292438233%3D%2520%2520Cross-view%2520object%2520geo-localization%2520%2528CVOGL%2529%2520aims%2520to%2520locate%2520an%2520object%2520of%250Ainterest%2520in%2520a%2520captured%2520ground-%2520or%2520drone-view%2520image%2520within%2520the%2520satellite%2520image.%250AHowever%252C%2520existing%2520works%2520treat%2520ground-view%2520and%2520drone-view%2520query%2520images%250Aequivalently%252C%2520overlooking%2520their%2520inherent%2520viewpoint%2520discrepancies%2520and%2520the%250Aspatial%2520correlation%2520between%2520the%2520query%2520image%2520and%2520the%2520satellite-view%2520reference%250Aimage.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520View-specific%2520Attention%250AGeo-localization%2520method%2520%2528VAGeo%2529%2520for%2520accurate%2520CVOGL.%2520Specifically%252C%2520VAGeo%250Acontains%2520two%2520key%2520modules%253A%2520view-specific%2520positional%2520encoding%2520%2528VSPE%2529%2520module%2520and%250Achannel-spatial%2520hybrid%2520attention%2520%2528CSHA%2529%2520module.%2520In%2520object-level%252C%2520according%2520to%250Athe%2520characteristics%2520of%2520different%2520viewpoints%2520of%2520ground%2520and%2520drone%2520query%2520images%252C%250Aviewpoint-specific%2520positional%2520codings%2520are%2520designed%2520to%2520more%2520accurately%2520identify%250Athe%2520click-point%2520object%2520of%2520the%2520query%2520image%2520in%2520the%2520VSPE%2520module.%2520In%2520feature-level%252C%250Aa%2520hybrid%2520attention%2520in%2520the%2520CSHA%2520module%2520is%2520introduced%2520by%2520combining%2520channel%250Aattention%2520and%2520spatial%2520attention%2520mechanisms%2520simultaneously%2520for%2520learning%250Adiscriminative%2520features.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520VAGeo%2520gains%2520a%2520significant%2520performance%2520improvement%252C%2520i.e.%252C%2520improving%250Aacc%25400.25/acc%25400.5%2520on%2520the%2520CVOGL%2520dataset%2520from%252045.43%2525/42.24%2525%2520to%252048.21%2525/45.22%2525%2520for%250Aground-view%252C%2520and%2520from%252061.97%2525/57.66%2525%2520to%252066.19%2525/61.87%2525%2520for%2520drone-view.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07194v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VAGeo%3A%20View-specific%20Attention%20for%20Cross-View%20Object%20Geo-Localization&entry.906535625=Zhongyang%20Li%20and%20Xin%20Yuan%20and%20Wei%20Liu%20and%20Xin%20Xu&entry.1292438233=%20%20Cross-view%20object%20geo-localization%20%28CVOGL%29%20aims%20to%20locate%20an%20object%20of%0Ainterest%20in%20a%20captured%20ground-%20or%20drone-view%20image%20within%20the%20satellite%20image.%0AHowever%2C%20existing%20works%20treat%20ground-view%20and%20drone-view%20query%20images%0Aequivalently%2C%20overlooking%20their%20inherent%20viewpoint%20discrepancies%20and%20the%0Aspatial%20correlation%20between%20the%20query%20image%20and%20the%20satellite-view%20reference%0Aimage.%20To%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20View-specific%20Attention%0AGeo-localization%20method%20%28VAGeo%29%20for%20accurate%20CVOGL.%20Specifically%2C%20VAGeo%0Acontains%20two%20key%20modules%3A%20view-specific%20positional%20encoding%20%28VSPE%29%20module%20and%0Achannel-spatial%20hybrid%20attention%20%28CSHA%29%20module.%20In%20object-level%2C%20according%20to%0Athe%20characteristics%20of%20different%20viewpoints%20of%20ground%20and%20drone%20query%20images%2C%0Aviewpoint-specific%20positional%20codings%20are%20designed%20to%20more%20accurately%20identify%0Athe%20click-point%20object%20of%20the%20query%20image%20in%20the%20VSPE%20module.%20In%20feature-level%2C%0Aa%20hybrid%20attention%20in%20the%20CSHA%20module%20is%20introduced%20by%20combining%20channel%0Aattention%20and%20spatial%20attention%20mechanisms%20simultaneously%20for%20learning%0Adiscriminative%20features.%20Extensive%20experimental%20results%20demonstrate%20that%20the%0Aproposed%20VAGeo%20gains%20a%20significant%20performance%20improvement%2C%20i.e.%2C%20improving%0Aacc%400.25/acc%400.5%20on%20the%20CVOGL%20dataset%20from%2045.43%25/42.24%25%20to%2048.21%25/45.22%25%20for%0Aground-view%2C%20and%20from%2061.97%25/57.66%25%20to%2066.19%25/61.87%25%20for%20drone-view.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07194v1&entry.124074799=Read"},
{"title": "Training-Free Motion-Guided Video Generation with Enhanced Temporal\n  Consistency Using Motion Consistency Loss", "author": "Xinyu Zhang and Zicheng Duan and Dong Gong and Lingqiao Liu", "abstract": "  In this paper, we address the challenge of generating temporally consistent\nvideos with motion guidance. While many existing methods depend on additional\ncontrol modules or inference-time fine-tuning, recent studies suggest that\neffective motion guidance is achievable without altering the model architecture\nor requiring extra training. Such approaches offer promising compatibility with\nvarious video generation foundation models. However, existing training-free\nmethods often struggle to maintain consistent temporal coherence across frames\nor to follow guided motion accurately. In this work, we propose a simple yet\neffective solution that combines an initial-noise-based approach with a novel\nmotion consistency loss, the latter being our key innovation. Specifically, we\ncapture the inter-frame feature correlation patterns of intermediate features\nfrom a video diffusion model to represent the motion pattern of the reference\nvideo. We then design a motion consistency loss to maintain similar feature\ncorrelation patterns in the generated video, using the gradient of this loss in\nthe latent space to guide the generation process for precise motion control.\nThis approach improves temporal consistency across various motion control tasks\nwhile preserving the benefits of a training-free setup. Extensive experiments\nshow that our method sets a new standard for efficient, temporally coherent\nvideo generation.\n", "link": "http://arxiv.org/abs/2501.07563v1", "date": "2025-01-13", "relevancy": 2.5055, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6461}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6354}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Motion-Guided%20Video%20Generation%20with%20Enhanced%20Temporal%0A%20%20Consistency%20Using%20Motion%20Consistency%20Loss&body=Title%3A%20Training-Free%20Motion-Guided%20Video%20Generation%20with%20Enhanced%20Temporal%0A%20%20Consistency%20Using%20Motion%20Consistency%20Loss%0AAuthor%3A%20Xinyu%20Zhang%20and%20Zicheng%20Duan%20and%20Dong%20Gong%20and%20Lingqiao%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20generating%20temporally%20consistent%0Avideos%20with%20motion%20guidance.%20While%20many%20existing%20methods%20depend%20on%20additional%0Acontrol%20modules%20or%20inference-time%20fine-tuning%2C%20recent%20studies%20suggest%20that%0Aeffective%20motion%20guidance%20is%20achievable%20without%20altering%20the%20model%20architecture%0Aor%20requiring%20extra%20training.%20Such%20approaches%20offer%20promising%20compatibility%20with%0Avarious%20video%20generation%20foundation%20models.%20However%2C%20existing%20training-free%0Amethods%20often%20struggle%20to%20maintain%20consistent%20temporal%20coherence%20across%20frames%0Aor%20to%20follow%20guided%20motion%20accurately.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20solution%20that%20combines%20an%20initial-noise-based%20approach%20with%20a%20novel%0Amotion%20consistency%20loss%2C%20the%20latter%20being%20our%20key%20innovation.%20Specifically%2C%20we%0Acapture%20the%20inter-frame%20feature%20correlation%20patterns%20of%20intermediate%20features%0Afrom%20a%20video%20diffusion%20model%20to%20represent%20the%20motion%20pattern%20of%20the%20reference%0Avideo.%20We%20then%20design%20a%20motion%20consistency%20loss%20to%20maintain%20similar%20feature%0Acorrelation%20patterns%20in%20the%20generated%20video%2C%20using%20the%20gradient%20of%20this%20loss%20in%0Athe%20latent%20space%20to%20guide%20the%20generation%20process%20for%20precise%20motion%20control.%0AThis%20approach%20improves%20temporal%20consistency%20across%20various%20motion%20control%20tasks%0Awhile%20preserving%20the%20benefits%20of%20a%20training-free%20setup.%20Extensive%20experiments%0Ashow%20that%20our%20method%20sets%20a%20new%20standard%20for%20efficient%2C%20temporally%20coherent%0Avideo%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Motion-Guided%2520Video%2520Generation%2520with%2520Enhanced%2520Temporal%250A%2520%2520Consistency%2520Using%2520Motion%2520Consistency%2520Loss%26entry.906535625%3DXinyu%2520Zhang%2520and%2520Zicheng%2520Duan%2520and%2520Dong%2520Gong%2520and%2520Lingqiao%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenge%2520of%2520generating%2520temporally%2520consistent%250Avideos%2520with%2520motion%2520guidance.%2520While%2520many%2520existing%2520methods%2520depend%2520on%2520additional%250Acontrol%2520modules%2520or%2520inference-time%2520fine-tuning%252C%2520recent%2520studies%2520suggest%2520that%250Aeffective%2520motion%2520guidance%2520is%2520achievable%2520without%2520altering%2520the%2520model%2520architecture%250Aor%2520requiring%2520extra%2520training.%2520Such%2520approaches%2520offer%2520promising%2520compatibility%2520with%250Avarious%2520video%2520generation%2520foundation%2520models.%2520However%252C%2520existing%2520training-free%250Amethods%2520often%2520struggle%2520to%2520maintain%2520consistent%2520temporal%2520coherence%2520across%2520frames%250Aor%2520to%2520follow%2520guided%2520motion%2520accurately.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520yet%250Aeffective%2520solution%2520that%2520combines%2520an%2520initial-noise-based%2520approach%2520with%2520a%2520novel%250Amotion%2520consistency%2520loss%252C%2520the%2520latter%2520being%2520our%2520key%2520innovation.%2520Specifically%252C%2520we%250Acapture%2520the%2520inter-frame%2520feature%2520correlation%2520patterns%2520of%2520intermediate%2520features%250Afrom%2520a%2520video%2520diffusion%2520model%2520to%2520represent%2520the%2520motion%2520pattern%2520of%2520the%2520reference%250Avideo.%2520We%2520then%2520design%2520a%2520motion%2520consistency%2520loss%2520to%2520maintain%2520similar%2520feature%250Acorrelation%2520patterns%2520in%2520the%2520generated%2520video%252C%2520using%2520the%2520gradient%2520of%2520this%2520loss%2520in%250Athe%2520latent%2520space%2520to%2520guide%2520the%2520generation%2520process%2520for%2520precise%2520motion%2520control.%250AThis%2520approach%2520improves%2520temporal%2520consistency%2520across%2520various%2520motion%2520control%2520tasks%250Awhile%2520preserving%2520the%2520benefits%2520of%2520a%2520training-free%2520setup.%2520Extensive%2520experiments%250Ashow%2520that%2520our%2520method%2520sets%2520a%2520new%2520standard%2520for%2520efficient%252C%2520temporally%2520coherent%250Avideo%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Motion-Guided%20Video%20Generation%20with%20Enhanced%20Temporal%0A%20%20Consistency%20Using%20Motion%20Consistency%20Loss&entry.906535625=Xinyu%20Zhang%20and%20Zicheng%20Duan%20and%20Dong%20Gong%20and%20Lingqiao%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20generating%20temporally%20consistent%0Avideos%20with%20motion%20guidance.%20While%20many%20existing%20methods%20depend%20on%20additional%0Acontrol%20modules%20or%20inference-time%20fine-tuning%2C%20recent%20studies%20suggest%20that%0Aeffective%20motion%20guidance%20is%20achievable%20without%20altering%20the%20model%20architecture%0Aor%20requiring%20extra%20training.%20Such%20approaches%20offer%20promising%20compatibility%20with%0Avarious%20video%20generation%20foundation%20models.%20However%2C%20existing%20training-free%0Amethods%20often%20struggle%20to%20maintain%20consistent%20temporal%20coherence%20across%20frames%0Aor%20to%20follow%20guided%20motion%20accurately.%20In%20this%20work%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20solution%20that%20combines%20an%20initial-noise-based%20approach%20with%20a%20novel%0Amotion%20consistency%20loss%2C%20the%20latter%20being%20our%20key%20innovation.%20Specifically%2C%20we%0Acapture%20the%20inter-frame%20feature%20correlation%20patterns%20of%20intermediate%20features%0Afrom%20a%20video%20diffusion%20model%20to%20represent%20the%20motion%20pattern%20of%20the%20reference%0Avideo.%20We%20then%20design%20a%20motion%20consistency%20loss%20to%20maintain%20similar%20feature%0Acorrelation%20patterns%20in%20the%20generated%20video%2C%20using%20the%20gradient%20of%20this%20loss%20in%0Athe%20latent%20space%20to%20guide%20the%20generation%20process%20for%20precise%20motion%20control.%0AThis%20approach%20improves%20temporal%20consistency%20across%20various%20motion%20control%20tasks%0Awhile%20preserving%20the%20benefits%20of%20a%20training-free%20setup.%20Extensive%20experiments%0Ashow%20that%20our%20method%20sets%20a%20new%20standard%20for%20efficient%2C%20temporally%20coherent%0Avideo%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07563v1&entry.124074799=Read"},
{"title": "SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object\n  Interaction Synthesis", "author": "Wenkun He and Yun Liu and Ruitao Liu and Li Yi", "abstract": "  Synthesizing realistic human-object interaction motions is a critical problem\nin VR/AR and human animation. Unlike the commonly studied scenarios involving a\nsingle human or hand interacting with one object, we address a more generic\nmulti-body setting with arbitrary numbers of humans, hands, and objects. This\ncomplexity introduces significant challenges in synchronizing motions due to\nthe high correlations and mutual influences among bodies. To address these\nchallenges, we introduce SyncDiff, a novel method for multi-body interaction\nsynthesis using a synchronized motion diffusion strategy. SyncDiff employs a\nsingle diffusion model to capture the joint distribution of multi-body motions.\nTo enhance motion fidelity, we propose a frequency-domain motion decomposition\nscheme. Additionally, we introduce a new set of alignment scores to emphasize\nthe synchronization of different body motions. SyncDiff jointly optimizes both\ndata sample likelihood and alignment likelihood through an explicit\nsynchronization strategy. Extensive experiments across four datasets with\nvarious multi-body configurations demonstrate the superiority of SyncDiff over\nexisting state-of-the-art motion synthesis methods.\n", "link": "http://arxiv.org/abs/2412.20104v2", "date": "2025-01-13", "relevancy": 2.4891, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7226}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5641}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyncDiff%3A%20Synchronized%20Motion%20Diffusion%20for%20Multi-Body%20Human-Object%0A%20%20Interaction%20Synthesis&body=Title%3A%20SyncDiff%3A%20Synchronized%20Motion%20Diffusion%20for%20Multi-Body%20Human-Object%0A%20%20Interaction%20Synthesis%0AAuthor%3A%20Wenkun%20He%20and%20Yun%20Liu%20and%20Ruitao%20Liu%20and%20Li%20Yi%0AAbstract%3A%20%20%20Synthesizing%20realistic%20human-object%20interaction%20motions%20is%20a%20critical%20problem%0Ain%20VR/AR%20and%20human%20animation.%20Unlike%20the%20commonly%20studied%20scenarios%20involving%20a%0Asingle%20human%20or%20hand%20interacting%20with%20one%20object%2C%20we%20address%20a%20more%20generic%0Amulti-body%20setting%20with%20arbitrary%20numbers%20of%20humans%2C%20hands%2C%20and%20objects.%20This%0Acomplexity%20introduces%20significant%20challenges%20in%20synchronizing%20motions%20due%20to%0Athe%20high%20correlations%20and%20mutual%20influences%20among%20bodies.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20SyncDiff%2C%20a%20novel%20method%20for%20multi-body%20interaction%0Asynthesis%20using%20a%20synchronized%20motion%20diffusion%20strategy.%20SyncDiff%20employs%20a%0Asingle%20diffusion%20model%20to%20capture%20the%20joint%20distribution%20of%20multi-body%20motions.%0ATo%20enhance%20motion%20fidelity%2C%20we%20propose%20a%20frequency-domain%20motion%20decomposition%0Ascheme.%20Additionally%2C%20we%20introduce%20a%20new%20set%20of%20alignment%20scores%20to%20emphasize%0Athe%20synchronization%20of%20different%20body%20motions.%20SyncDiff%20jointly%20optimizes%20both%0Adata%20sample%20likelihood%20and%20alignment%20likelihood%20through%20an%20explicit%0Asynchronization%20strategy.%20Extensive%20experiments%20across%20four%20datasets%20with%0Avarious%20multi-body%20configurations%20demonstrate%20the%20superiority%20of%20SyncDiff%20over%0Aexisting%20state-of-the-art%20motion%20synthesis%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20104v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyncDiff%253A%2520Synchronized%2520Motion%2520Diffusion%2520for%2520Multi-Body%2520Human-Object%250A%2520%2520Interaction%2520Synthesis%26entry.906535625%3DWenkun%2520He%2520and%2520Yun%2520Liu%2520and%2520Ruitao%2520Liu%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520Synthesizing%2520realistic%2520human-object%2520interaction%2520motions%2520is%2520a%2520critical%2520problem%250Ain%2520VR/AR%2520and%2520human%2520animation.%2520Unlike%2520the%2520commonly%2520studied%2520scenarios%2520involving%2520a%250Asingle%2520human%2520or%2520hand%2520interacting%2520with%2520one%2520object%252C%2520we%2520address%2520a%2520more%2520generic%250Amulti-body%2520setting%2520with%2520arbitrary%2520numbers%2520of%2520humans%252C%2520hands%252C%2520and%2520objects.%2520This%250Acomplexity%2520introduces%2520significant%2520challenges%2520in%2520synchronizing%2520motions%2520due%2520to%250Athe%2520high%2520correlations%2520and%2520mutual%2520influences%2520among%2520bodies.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520SyncDiff%252C%2520a%2520novel%2520method%2520for%2520multi-body%2520interaction%250Asynthesis%2520using%2520a%2520synchronized%2520motion%2520diffusion%2520strategy.%2520SyncDiff%2520employs%2520a%250Asingle%2520diffusion%2520model%2520to%2520capture%2520the%2520joint%2520distribution%2520of%2520multi-body%2520motions.%250ATo%2520enhance%2520motion%2520fidelity%252C%2520we%2520propose%2520a%2520frequency-domain%2520motion%2520decomposition%250Ascheme.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%2520set%2520of%2520alignment%2520scores%2520to%2520emphasize%250Athe%2520synchronization%2520of%2520different%2520body%2520motions.%2520SyncDiff%2520jointly%2520optimizes%2520both%250Adata%2520sample%2520likelihood%2520and%2520alignment%2520likelihood%2520through%2520an%2520explicit%250Asynchronization%2520strategy.%2520Extensive%2520experiments%2520across%2520four%2520datasets%2520with%250Avarious%2520multi-body%2520configurations%2520demonstrate%2520the%2520superiority%2520of%2520SyncDiff%2520over%250Aexisting%2520state-of-the-art%2520motion%2520synthesis%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20104v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncDiff%3A%20Synchronized%20Motion%20Diffusion%20for%20Multi-Body%20Human-Object%0A%20%20Interaction%20Synthesis&entry.906535625=Wenkun%20He%20and%20Yun%20Liu%20and%20Ruitao%20Liu%20and%20Li%20Yi&entry.1292438233=%20%20Synthesizing%20realistic%20human-object%20interaction%20motions%20is%20a%20critical%20problem%0Ain%20VR/AR%20and%20human%20animation.%20Unlike%20the%20commonly%20studied%20scenarios%20involving%20a%0Asingle%20human%20or%20hand%20interacting%20with%20one%20object%2C%20we%20address%20a%20more%20generic%0Amulti-body%20setting%20with%20arbitrary%20numbers%20of%20humans%2C%20hands%2C%20and%20objects.%20This%0Acomplexity%20introduces%20significant%20challenges%20in%20synchronizing%20motions%20due%20to%0Athe%20high%20correlations%20and%20mutual%20influences%20among%20bodies.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20SyncDiff%2C%20a%20novel%20method%20for%20multi-body%20interaction%0Asynthesis%20using%20a%20synchronized%20motion%20diffusion%20strategy.%20SyncDiff%20employs%20a%0Asingle%20diffusion%20model%20to%20capture%20the%20joint%20distribution%20of%20multi-body%20motions.%0ATo%20enhance%20motion%20fidelity%2C%20we%20propose%20a%20frequency-domain%20motion%20decomposition%0Ascheme.%20Additionally%2C%20we%20introduce%20a%20new%20set%20of%20alignment%20scores%20to%20emphasize%0Athe%20synchronization%20of%20different%20body%20motions.%20SyncDiff%20jointly%20optimizes%20both%0Adata%20sample%20likelihood%20and%20alignment%20likelihood%20through%20an%20explicit%0Asynchronization%20strategy.%20Extensive%20experiments%20across%20four%20datasets%20with%0Avarious%20multi-body%20configurations%20demonstrate%20the%20superiority%20of%20SyncDiff%20over%0Aexisting%20state-of-the-art%20motion%20synthesis%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20104v2&entry.124074799=Read"},
{"title": "Implicit Neural Representations for Registration of Left Ventricle\n  Myocardium During a Cardiac Cycle", "author": "Mathias Micheelsen Lowes and Jonas Jalili Pedersen and Bj\u00f8rn S. Hansen and Klaus Fuglsang Kofoed and Maxime Sermesant and Rasmus R. Paulsen", "abstract": "  Understanding the movement of the left ventricle myocardium (LVmyo) during\nthe cardiac cycle is essential for assessing cardiac function. One way to model\nthis movement is through a series of deformable image registrations (DIRs) of\nthe LVmyo. Traditional deep learning methods for DIRs, such as those based on\nconvolutional neural networks, often require substantial memory and\ncomputational resources. In contrast, implicit neural representations (INRs)\noffer an efficient approach by operating on any number of continuous points.\nThis study extends the use of INRs for DIR to cardiac computed tomography (CT),\nfocusing on LVmyo registration. To enhance the precision of the registration\naround the LVmyo, we incorporate the signed distance field of the LVmyo with\nthe Hounsfield Unit values from the CT frames. This guides the registration of\nthe LVmyo, while keeping the tissue information from the CT frames. Our\nframework demonstrates high registration accuracy and provides a robust method\nfor temporal registration that facilitates further analysis of LVmyo motion.\n", "link": "http://arxiv.org/abs/2501.07248v1", "date": "2025-01-13", "relevancy": 2.4773, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5113}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4949}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Neural%20Representations%20for%20Registration%20of%20Left%20Ventricle%0A%20%20Myocardium%20During%20a%20Cardiac%20Cycle&body=Title%3A%20Implicit%20Neural%20Representations%20for%20Registration%20of%20Left%20Ventricle%0A%20%20Myocardium%20During%20a%20Cardiac%20Cycle%0AAuthor%3A%20Mathias%20Micheelsen%20Lowes%20and%20Jonas%20Jalili%20Pedersen%20and%20Bj%C3%B8rn%20S.%20Hansen%20and%20Klaus%20Fuglsang%20Kofoed%20and%20Maxime%20Sermesant%20and%20Rasmus%20R.%20Paulsen%0AAbstract%3A%20%20%20Understanding%20the%20movement%20of%20the%20left%20ventricle%20myocardium%20%28LVmyo%29%20during%0Athe%20cardiac%20cycle%20is%20essential%20for%20assessing%20cardiac%20function.%20One%20way%20to%20model%0Athis%20movement%20is%20through%20a%20series%20of%20deformable%20image%20registrations%20%28DIRs%29%20of%0Athe%20LVmyo.%20Traditional%20deep%20learning%20methods%20for%20DIRs%2C%20such%20as%20those%20based%20on%0Aconvolutional%20neural%20networks%2C%20often%20require%20substantial%20memory%20and%0Acomputational%20resources.%20In%20contrast%2C%20implicit%20neural%20representations%20%28INRs%29%0Aoffer%20an%20efficient%20approach%20by%20operating%20on%20any%20number%20of%20continuous%20points.%0AThis%20study%20extends%20the%20use%20of%20INRs%20for%20DIR%20to%20cardiac%20computed%20tomography%20%28CT%29%2C%0Afocusing%20on%20LVmyo%20registration.%20To%20enhance%20the%20precision%20of%20the%20registration%0Aaround%20the%20LVmyo%2C%20we%20incorporate%20the%20signed%20distance%20field%20of%20the%20LVmyo%20with%0Athe%20Hounsfield%20Unit%20values%20from%20the%20CT%20frames.%20This%20guides%20the%20registration%20of%0Athe%20LVmyo%2C%20while%20keeping%20the%20tissue%20information%20from%20the%20CT%20frames.%20Our%0Aframework%20demonstrates%20high%20registration%20accuracy%20and%20provides%20a%20robust%20method%0Afor%20temporal%20registration%20that%20facilitates%20further%20analysis%20of%20LVmyo%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07248v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Neural%2520Representations%2520for%2520Registration%2520of%2520Left%2520Ventricle%250A%2520%2520Myocardium%2520During%2520a%2520Cardiac%2520Cycle%26entry.906535625%3DMathias%2520Micheelsen%2520Lowes%2520and%2520Jonas%2520Jalili%2520Pedersen%2520and%2520Bj%25C3%25B8rn%2520S.%2520Hansen%2520and%2520Klaus%2520Fuglsang%2520Kofoed%2520and%2520Maxime%2520Sermesant%2520and%2520Rasmus%2520R.%2520Paulsen%26entry.1292438233%3D%2520%2520Understanding%2520the%2520movement%2520of%2520the%2520left%2520ventricle%2520myocardium%2520%2528LVmyo%2529%2520during%250Athe%2520cardiac%2520cycle%2520is%2520essential%2520for%2520assessing%2520cardiac%2520function.%2520One%2520way%2520to%2520model%250Athis%2520movement%2520is%2520through%2520a%2520series%2520of%2520deformable%2520image%2520registrations%2520%2528DIRs%2529%2520of%250Athe%2520LVmyo.%2520Traditional%2520deep%2520learning%2520methods%2520for%2520DIRs%252C%2520such%2520as%2520those%2520based%2520on%250Aconvolutional%2520neural%2520networks%252C%2520often%2520require%2520substantial%2520memory%2520and%250Acomputational%2520resources.%2520In%2520contrast%252C%2520implicit%2520neural%2520representations%2520%2528INRs%2529%250Aoffer%2520an%2520efficient%2520approach%2520by%2520operating%2520on%2520any%2520number%2520of%2520continuous%2520points.%250AThis%2520study%2520extends%2520the%2520use%2520of%2520INRs%2520for%2520DIR%2520to%2520cardiac%2520computed%2520tomography%2520%2528CT%2529%252C%250Afocusing%2520on%2520LVmyo%2520registration.%2520To%2520enhance%2520the%2520precision%2520of%2520the%2520registration%250Aaround%2520the%2520LVmyo%252C%2520we%2520incorporate%2520the%2520signed%2520distance%2520field%2520of%2520the%2520LVmyo%2520with%250Athe%2520Hounsfield%2520Unit%2520values%2520from%2520the%2520CT%2520frames.%2520This%2520guides%2520the%2520registration%2520of%250Athe%2520LVmyo%252C%2520while%2520keeping%2520the%2520tissue%2520information%2520from%2520the%2520CT%2520frames.%2520Our%250Aframework%2520demonstrates%2520high%2520registration%2520accuracy%2520and%2520provides%2520a%2520robust%2520method%250Afor%2520temporal%2520registration%2520that%2520facilitates%2520further%2520analysis%2520of%2520LVmyo%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07248v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Neural%20Representations%20for%20Registration%20of%20Left%20Ventricle%0A%20%20Myocardium%20During%20a%20Cardiac%20Cycle&entry.906535625=Mathias%20Micheelsen%20Lowes%20and%20Jonas%20Jalili%20Pedersen%20and%20Bj%C3%B8rn%20S.%20Hansen%20and%20Klaus%20Fuglsang%20Kofoed%20and%20Maxime%20Sermesant%20and%20Rasmus%20R.%20Paulsen&entry.1292438233=%20%20Understanding%20the%20movement%20of%20the%20left%20ventricle%20myocardium%20%28LVmyo%29%20during%0Athe%20cardiac%20cycle%20is%20essential%20for%20assessing%20cardiac%20function.%20One%20way%20to%20model%0Athis%20movement%20is%20through%20a%20series%20of%20deformable%20image%20registrations%20%28DIRs%29%20of%0Athe%20LVmyo.%20Traditional%20deep%20learning%20methods%20for%20DIRs%2C%20such%20as%20those%20based%20on%0Aconvolutional%20neural%20networks%2C%20often%20require%20substantial%20memory%20and%0Acomputational%20resources.%20In%20contrast%2C%20implicit%20neural%20representations%20%28INRs%29%0Aoffer%20an%20efficient%20approach%20by%20operating%20on%20any%20number%20of%20continuous%20points.%0AThis%20study%20extends%20the%20use%20of%20INRs%20for%20DIR%20to%20cardiac%20computed%20tomography%20%28CT%29%2C%0Afocusing%20on%20LVmyo%20registration.%20To%20enhance%20the%20precision%20of%20the%20registration%0Aaround%20the%20LVmyo%2C%20we%20incorporate%20the%20signed%20distance%20field%20of%20the%20LVmyo%20with%0Athe%20Hounsfield%20Unit%20values%20from%20the%20CT%20frames.%20This%20guides%20the%20registration%20of%0Athe%20LVmyo%2C%20while%20keeping%20the%20tissue%20information%20from%20the%20CT%20frames.%20Our%0Aframework%20demonstrates%20high%20registration%20accuracy%20and%20provides%20a%20robust%20method%0Afor%20temporal%20registration%20that%20facilitates%20further%20analysis%20of%20LVmyo%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07248v1&entry.124074799=Read"},
{"title": "UnCommon Objects in 3D", "author": "Xingchen Liu and Piyush Tayal and Jianyuan Wang and Jesus Zarzar and Tom Monnier and Konstantinos Tertikas and Jiali Duan and Antoine Toisoul and Jason Y. Zhang and Natalia Neverova and Andrea Vedaldi and Roman Shapovalov and David Novotny", "abstract": "  We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for\n3D deep learning and 3D generative AI. uCO3D is the largest publicly-available\ncollection of high-resolution videos of objects with 3D annotations that\nensures full-360$^{\\circ}$ coverage. uCO3D is significantly more diverse than\nMVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of\nhigher quality, due to extensive quality checks of both the collected videos\nand the 3D annotations. Similar to analogous datasets, uCO3D contains\nannotations for 3D camera poses, depth maps and sparse point clouds. In\naddition, each object is equipped with a caption and a 3D Gaussian Splat\nreconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D\nand obtain superior results using the latter, showing that uCO3D is better for\nlearning applications.\n", "link": "http://arxiv.org/abs/2501.07574v1", "date": "2025-01-13", "relevancy": 2.4746, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnCommon%20Objects%20in%203D&body=Title%3A%20UnCommon%20Objects%20in%203D%0AAuthor%3A%20Xingchen%20Liu%20and%20Piyush%20Tayal%20and%20Jianyuan%20Wang%20and%20Jesus%20Zarzar%20and%20Tom%20Monnier%20and%20Konstantinos%20Tertikas%20and%20Jiali%20Duan%20and%20Antoine%20Toisoul%20and%20Jason%20Y.%20Zhang%20and%20Natalia%20Neverova%20and%20Andrea%20Vedaldi%20and%20Roman%20Shapovalov%20and%20David%20Novotny%0AAbstract%3A%20%20%20We%20introduce%20Uncommon%20Objects%20in%203D%20%28uCO3D%29%2C%20a%20new%20object-centric%20dataset%20for%0A3D%20deep%20learning%20and%203D%20generative%20AI.%20uCO3D%20is%20the%20largest%20publicly-available%0Acollection%20of%20high-resolution%20videos%20of%20objects%20with%203D%20annotations%20that%0Aensures%20full-360%24%5E%7B%5Ccirc%7D%24%20coverage.%20uCO3D%20is%20significantly%20more%20diverse%20than%0AMVImgNet%20and%20CO3Dv2%2C%20covering%20more%20than%201%2C000%20object%20categories.%20It%20is%20also%20of%0Ahigher%20quality%2C%20due%20to%20extensive%20quality%20checks%20of%20both%20the%20collected%20videos%0Aand%20the%203D%20annotations.%20Similar%20to%20analogous%20datasets%2C%20uCO3D%20contains%0Aannotations%20for%203D%20camera%20poses%2C%20depth%20maps%20and%20sparse%20point%20clouds.%20In%0Aaddition%2C%20each%20object%20is%20equipped%20with%20a%20caption%20and%20a%203D%20Gaussian%20Splat%0Areconstruction.%20We%20train%20several%20large%203D%20models%20on%20MVImgNet%2C%20CO3Dv2%2C%20and%20uCO3D%0Aand%20obtain%20superior%20results%20using%20the%20latter%2C%20showing%20that%20uCO3D%20is%20better%20for%0Alearning%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnCommon%2520Objects%2520in%25203D%26entry.906535625%3DXingchen%2520Liu%2520and%2520Piyush%2520Tayal%2520and%2520Jianyuan%2520Wang%2520and%2520Jesus%2520Zarzar%2520and%2520Tom%2520Monnier%2520and%2520Konstantinos%2520Tertikas%2520and%2520Jiali%2520Duan%2520and%2520Antoine%2520Toisoul%2520and%2520Jason%2520Y.%2520Zhang%2520and%2520Natalia%2520Neverova%2520and%2520Andrea%2520Vedaldi%2520and%2520Roman%2520Shapovalov%2520and%2520David%2520Novotny%26entry.1292438233%3D%2520%2520We%2520introduce%2520Uncommon%2520Objects%2520in%25203D%2520%2528uCO3D%2529%252C%2520a%2520new%2520object-centric%2520dataset%2520for%250A3D%2520deep%2520learning%2520and%25203D%2520generative%2520AI.%2520uCO3D%2520is%2520the%2520largest%2520publicly-available%250Acollection%2520of%2520high-resolution%2520videos%2520of%2520objects%2520with%25203D%2520annotations%2520that%250Aensures%2520full-360%2524%255E%257B%255Ccirc%257D%2524%2520coverage.%2520uCO3D%2520is%2520significantly%2520more%2520diverse%2520than%250AMVImgNet%2520and%2520CO3Dv2%252C%2520covering%2520more%2520than%25201%252C000%2520object%2520categories.%2520It%2520is%2520also%2520of%250Ahigher%2520quality%252C%2520due%2520to%2520extensive%2520quality%2520checks%2520of%2520both%2520the%2520collected%2520videos%250Aand%2520the%25203D%2520annotations.%2520Similar%2520to%2520analogous%2520datasets%252C%2520uCO3D%2520contains%250Aannotations%2520for%25203D%2520camera%2520poses%252C%2520depth%2520maps%2520and%2520sparse%2520point%2520clouds.%2520In%250Aaddition%252C%2520each%2520object%2520is%2520equipped%2520with%2520a%2520caption%2520and%2520a%25203D%2520Gaussian%2520Splat%250Areconstruction.%2520We%2520train%2520several%2520large%25203D%2520models%2520on%2520MVImgNet%252C%2520CO3Dv2%252C%2520and%2520uCO3D%250Aand%2520obtain%2520superior%2520results%2520using%2520the%2520latter%252C%2520showing%2520that%2520uCO3D%2520is%2520better%2520for%250Alearning%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnCommon%20Objects%20in%203D&entry.906535625=Xingchen%20Liu%20and%20Piyush%20Tayal%20and%20Jianyuan%20Wang%20and%20Jesus%20Zarzar%20and%20Tom%20Monnier%20and%20Konstantinos%20Tertikas%20and%20Jiali%20Duan%20and%20Antoine%20Toisoul%20and%20Jason%20Y.%20Zhang%20and%20Natalia%20Neverova%20and%20Andrea%20Vedaldi%20and%20Roman%20Shapovalov%20and%20David%20Novotny&entry.1292438233=%20%20We%20introduce%20Uncommon%20Objects%20in%203D%20%28uCO3D%29%2C%20a%20new%20object-centric%20dataset%20for%0A3D%20deep%20learning%20and%203D%20generative%20AI.%20uCO3D%20is%20the%20largest%20publicly-available%0Acollection%20of%20high-resolution%20videos%20of%20objects%20with%203D%20annotations%20that%0Aensures%20full-360%24%5E%7B%5Ccirc%7D%24%20coverage.%20uCO3D%20is%20significantly%20more%20diverse%20than%0AMVImgNet%20and%20CO3Dv2%2C%20covering%20more%20than%201%2C000%20object%20categories.%20It%20is%20also%20of%0Ahigher%20quality%2C%20due%20to%20extensive%20quality%20checks%20of%20both%20the%20collected%20videos%0Aand%20the%203D%20annotations.%20Similar%20to%20analogous%20datasets%2C%20uCO3D%20contains%0Aannotations%20for%203D%20camera%20poses%2C%20depth%20maps%20and%20sparse%20point%20clouds.%20In%0Aaddition%2C%20each%20object%20is%20equipped%20with%20a%20caption%20and%20a%203D%20Gaussian%20Splat%0Areconstruction.%20We%20train%20several%20large%203D%20models%20on%20MVImgNet%2C%20CO3Dv2%2C%20and%20uCO3D%0Aand%20obtain%20superior%20results%20using%20the%20latter%2C%20showing%20that%20uCO3D%20is%20better%20for%0Alearning%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07574v1&entry.124074799=Read"},
{"title": "Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion", "author": "Li Liang and Naveed Akhtar and Jordan Vice and Xiangrui Kong and Ajmal Saeed Mian", "abstract": "  3D semantic scene completion is critical for multiple downstream tasks in\nautonomous systems. It estimates missing geometric and semantic information in\nthe acquired scene data. Due to the challenging real-world conditions, this\ntask usually demands complex models that process multi-modal data to achieve\nacceptable performance. We propose a unique neural model, leveraging advances\nfrom the state space and diffusion generative modeling to achieve remarkable 3D\nsemantic scene completion performance with monocular image input. Our technique\nprocesses the data in the conditioned latent space of a variational autoencoder\nwhere diffusion modeling is carried out with an innovative state space\ntechnique. A key component of our neural network is the proposed Skimba (Skip\nMamba) denoiser, which is adept at efficiently processing long-sequence data.\nThe Skimba diffusion model is integral to our 3D scene completion network,\nincorporating a triple Mamba structure, dimensional decomposition residuals and\nvarying dilations along three directions. We also adopt a variant of this\nnetwork for the subsequent semantic segmentation stage of our method. Extensive\nevaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show\nthat our approach not only outperforms other monocular techniques by a large\nmargin, it also achieves competitive performance against stereo methods. The\ncode is available at https://github.com/xrkong/skimba\n", "link": "http://arxiv.org/abs/2501.07260v1", "date": "2025-01-13", "relevancy": 2.4602, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6152}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6152}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skip%20Mamba%20Diffusion%20for%20Monocular%203D%20Semantic%20Scene%20Completion&body=Title%3A%20Skip%20Mamba%20Diffusion%20for%20Monocular%203D%20Semantic%20Scene%20Completion%0AAuthor%3A%20Li%20Liang%20and%20Naveed%20Akhtar%20and%20Jordan%20Vice%20and%20Xiangrui%20Kong%20and%20Ajmal%20Saeed%20Mian%0AAbstract%3A%20%20%203D%20semantic%20scene%20completion%20is%20critical%20for%20multiple%20downstream%20tasks%20in%0Aautonomous%20systems.%20It%20estimates%20missing%20geometric%20and%20semantic%20information%20in%0Athe%20acquired%20scene%20data.%20Due%20to%20the%20challenging%20real-world%20conditions%2C%20this%0Atask%20usually%20demands%20complex%20models%20that%20process%20multi-modal%20data%20to%20achieve%0Aacceptable%20performance.%20We%20propose%20a%20unique%20neural%20model%2C%20leveraging%20advances%0Afrom%20the%20state%20space%20and%20diffusion%20generative%20modeling%20to%20achieve%20remarkable%203D%0Asemantic%20scene%20completion%20performance%20with%20monocular%20image%20input.%20Our%20technique%0Aprocesses%20the%20data%20in%20the%20conditioned%20latent%20space%20of%20a%20variational%20autoencoder%0Awhere%20diffusion%20modeling%20is%20carried%20out%20with%20an%20innovative%20state%20space%0Atechnique.%20A%20key%20component%20of%20our%20neural%20network%20is%20the%20proposed%20Skimba%20%28Skip%0AMamba%29%20denoiser%2C%20which%20is%20adept%20at%20efficiently%20processing%20long-sequence%20data.%0AThe%20Skimba%20diffusion%20model%20is%20integral%20to%20our%203D%20scene%20completion%20network%2C%0Aincorporating%20a%20triple%20Mamba%20structure%2C%20dimensional%20decomposition%20residuals%20and%0Avarying%20dilations%20along%20three%20directions.%20We%20also%20adopt%20a%20variant%20of%20this%0Anetwork%20for%20the%20subsequent%20semantic%20segmentation%20stage%20of%20our%20method.%20Extensive%0Aevaluation%20on%20the%20standard%20SemanticKITTI%20and%20SSCBench-KITTI360%20datasets%20show%0Athat%20our%20approach%20not%20only%20outperforms%20other%20monocular%20techniques%20by%20a%20large%0Amargin%2C%20it%20also%20achieves%20competitive%20performance%20against%20stereo%20methods.%20The%0Acode%20is%20available%20at%20https%3A//github.com/xrkong/skimba%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkip%2520Mamba%2520Diffusion%2520for%2520Monocular%25203D%2520Semantic%2520Scene%2520Completion%26entry.906535625%3DLi%2520Liang%2520and%2520Naveed%2520Akhtar%2520and%2520Jordan%2520Vice%2520and%2520Xiangrui%2520Kong%2520and%2520Ajmal%2520Saeed%2520Mian%26entry.1292438233%3D%2520%25203D%2520semantic%2520scene%2520completion%2520is%2520critical%2520for%2520multiple%2520downstream%2520tasks%2520in%250Aautonomous%2520systems.%2520It%2520estimates%2520missing%2520geometric%2520and%2520semantic%2520information%2520in%250Athe%2520acquired%2520scene%2520data.%2520Due%2520to%2520the%2520challenging%2520real-world%2520conditions%252C%2520this%250Atask%2520usually%2520demands%2520complex%2520models%2520that%2520process%2520multi-modal%2520data%2520to%2520achieve%250Aacceptable%2520performance.%2520We%2520propose%2520a%2520unique%2520neural%2520model%252C%2520leveraging%2520advances%250Afrom%2520the%2520state%2520space%2520and%2520diffusion%2520generative%2520modeling%2520to%2520achieve%2520remarkable%25203D%250Asemantic%2520scene%2520completion%2520performance%2520with%2520monocular%2520image%2520input.%2520Our%2520technique%250Aprocesses%2520the%2520data%2520in%2520the%2520conditioned%2520latent%2520space%2520of%2520a%2520variational%2520autoencoder%250Awhere%2520diffusion%2520modeling%2520is%2520carried%2520out%2520with%2520an%2520innovative%2520state%2520space%250Atechnique.%2520A%2520key%2520component%2520of%2520our%2520neural%2520network%2520is%2520the%2520proposed%2520Skimba%2520%2528Skip%250AMamba%2529%2520denoiser%252C%2520which%2520is%2520adept%2520at%2520efficiently%2520processing%2520long-sequence%2520data.%250AThe%2520Skimba%2520diffusion%2520model%2520is%2520integral%2520to%2520our%25203D%2520scene%2520completion%2520network%252C%250Aincorporating%2520a%2520triple%2520Mamba%2520structure%252C%2520dimensional%2520decomposition%2520residuals%2520and%250Avarying%2520dilations%2520along%2520three%2520directions.%2520We%2520also%2520adopt%2520a%2520variant%2520of%2520this%250Anetwork%2520for%2520the%2520subsequent%2520semantic%2520segmentation%2520stage%2520of%2520our%2520method.%2520Extensive%250Aevaluation%2520on%2520the%2520standard%2520SemanticKITTI%2520and%2520SSCBench-KITTI360%2520datasets%2520show%250Athat%2520our%2520approach%2520not%2520only%2520outperforms%2520other%2520monocular%2520techniques%2520by%2520a%2520large%250Amargin%252C%2520it%2520also%2520achieves%2520competitive%2520performance%2520against%2520stereo%2520methods.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/xrkong/skimba%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skip%20Mamba%20Diffusion%20for%20Monocular%203D%20Semantic%20Scene%20Completion&entry.906535625=Li%20Liang%20and%20Naveed%20Akhtar%20and%20Jordan%20Vice%20and%20Xiangrui%20Kong%20and%20Ajmal%20Saeed%20Mian&entry.1292438233=%20%203D%20semantic%20scene%20completion%20is%20critical%20for%20multiple%20downstream%20tasks%20in%0Aautonomous%20systems.%20It%20estimates%20missing%20geometric%20and%20semantic%20information%20in%0Athe%20acquired%20scene%20data.%20Due%20to%20the%20challenging%20real-world%20conditions%2C%20this%0Atask%20usually%20demands%20complex%20models%20that%20process%20multi-modal%20data%20to%20achieve%0Aacceptable%20performance.%20We%20propose%20a%20unique%20neural%20model%2C%20leveraging%20advances%0Afrom%20the%20state%20space%20and%20diffusion%20generative%20modeling%20to%20achieve%20remarkable%203D%0Asemantic%20scene%20completion%20performance%20with%20monocular%20image%20input.%20Our%20technique%0Aprocesses%20the%20data%20in%20the%20conditioned%20latent%20space%20of%20a%20variational%20autoencoder%0Awhere%20diffusion%20modeling%20is%20carried%20out%20with%20an%20innovative%20state%20space%0Atechnique.%20A%20key%20component%20of%20our%20neural%20network%20is%20the%20proposed%20Skimba%20%28Skip%0AMamba%29%20denoiser%2C%20which%20is%20adept%20at%20efficiently%20processing%20long-sequence%20data.%0AThe%20Skimba%20diffusion%20model%20is%20integral%20to%20our%203D%20scene%20completion%20network%2C%0Aincorporating%20a%20triple%20Mamba%20structure%2C%20dimensional%20decomposition%20residuals%20and%0Avarying%20dilations%20along%20three%20directions.%20We%20also%20adopt%20a%20variant%20of%20this%0Anetwork%20for%20the%20subsequent%20semantic%20segmentation%20stage%20of%20our%20method.%20Extensive%0Aevaluation%20on%20the%20standard%20SemanticKITTI%20and%20SSCBench-KITTI360%20datasets%20show%0Athat%20our%20approach%20not%20only%20outperforms%20other%20monocular%20techniques%20by%20a%20large%0Amargin%2C%20it%20also%20achieves%20competitive%20performance%20against%20stereo%20methods.%20The%0Acode%20is%20available%20at%20https%3A//github.com/xrkong/skimba%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07260v1&entry.124074799=Read"},
{"title": "Dynamic Prototype Rehearsal for Continual Learning in ECG Arrhythmia\n  Detection", "author": "Sana Rahmani and Reetam Chatterjee and Ali Etemad and Javad Hashemi", "abstract": "  Continual Learning (CL) methods aim to learn from a sequence of tasks while\navoiding the challenge of forgetting previous knowledge. We present DREAM-CL, a\nnovel CL method for ECG arrhythmia detection that introduces dynamic prototype\nrehearsal memory. DREAM-CL selects representative prototypes by clustering data\nbased on learning behavior during each training session. Within each cluster,\nwe apply a smooth sorting operation that ranks samples by training difficulty,\ncompressing extreme values and removing outliers. The more challenging samples\nare then chosen as prototypes for the rehearsal memory, ensuring effective\nknowledge retention across sessions. We evaluate our method on\ntime-incremental, class-incremental, and lead-incremental scenarios using two\nwidely used ECG arrhythmia datasets, Chapman and PTB-XL. The results\ndemonstrate that DREAM-CL outperforms the state-of-the-art in CL for ECG\narrhythmia detection. Detailed ablation and sensitivity studies are performed\nto validate the different design choices of our method.\n", "link": "http://arxiv.org/abs/2501.07555v1", "date": "2025-01-13", "relevancy": 2.4399, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4967}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4885}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Prototype%20Rehearsal%20for%20Continual%20Learning%20in%20ECG%20Arrhythmia%0A%20%20Detection&body=Title%3A%20Dynamic%20Prototype%20Rehearsal%20for%20Continual%20Learning%20in%20ECG%20Arrhythmia%0A%20%20Detection%0AAuthor%3A%20Sana%20Rahmani%20and%20Reetam%20Chatterjee%20and%20Ali%20Etemad%20and%20Javad%20Hashemi%0AAbstract%3A%20%20%20Continual%20Learning%20%28CL%29%20methods%20aim%20to%20learn%20from%20a%20sequence%20of%20tasks%20while%0Aavoiding%20the%20challenge%20of%20forgetting%20previous%20knowledge.%20We%20present%20DREAM-CL%2C%20a%0Anovel%20CL%20method%20for%20ECG%20arrhythmia%20detection%20that%20introduces%20dynamic%20prototype%0Arehearsal%20memory.%20DREAM-CL%20selects%20representative%20prototypes%20by%20clustering%20data%0Abased%20on%20learning%20behavior%20during%20each%20training%20session.%20Within%20each%20cluster%2C%0Awe%20apply%20a%20smooth%20sorting%20operation%20that%20ranks%20samples%20by%20training%20difficulty%2C%0Acompressing%20extreme%20values%20and%20removing%20outliers.%20The%20more%20challenging%20samples%0Aare%20then%20chosen%20as%20prototypes%20for%20the%20rehearsal%20memory%2C%20ensuring%20effective%0Aknowledge%20retention%20across%20sessions.%20We%20evaluate%20our%20method%20on%0Atime-incremental%2C%20class-incremental%2C%20and%20lead-incremental%20scenarios%20using%20two%0Awidely%20used%20ECG%20arrhythmia%20datasets%2C%20Chapman%20and%20PTB-XL.%20The%20results%0Ademonstrate%20that%20DREAM-CL%20outperforms%20the%20state-of-the-art%20in%20CL%20for%20ECG%0Aarrhythmia%20detection.%20Detailed%20ablation%20and%20sensitivity%20studies%20are%20performed%0Ato%20validate%20the%20different%20design%20choices%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Prototype%2520Rehearsal%2520for%2520Continual%2520Learning%2520in%2520ECG%2520Arrhythmia%250A%2520%2520Detection%26entry.906535625%3DSana%2520Rahmani%2520and%2520Reetam%2520Chatterjee%2520and%2520Ali%2520Etemad%2520and%2520Javad%2520Hashemi%26entry.1292438233%3D%2520%2520Continual%2520Learning%2520%2528CL%2529%2520methods%2520aim%2520to%2520learn%2520from%2520a%2520sequence%2520of%2520tasks%2520while%250Aavoiding%2520the%2520challenge%2520of%2520forgetting%2520previous%2520knowledge.%2520We%2520present%2520DREAM-CL%252C%2520a%250Anovel%2520CL%2520method%2520for%2520ECG%2520arrhythmia%2520detection%2520that%2520introduces%2520dynamic%2520prototype%250Arehearsal%2520memory.%2520DREAM-CL%2520selects%2520representative%2520prototypes%2520by%2520clustering%2520data%250Abased%2520on%2520learning%2520behavior%2520during%2520each%2520training%2520session.%2520Within%2520each%2520cluster%252C%250Awe%2520apply%2520a%2520smooth%2520sorting%2520operation%2520that%2520ranks%2520samples%2520by%2520training%2520difficulty%252C%250Acompressing%2520extreme%2520values%2520and%2520removing%2520outliers.%2520The%2520more%2520challenging%2520samples%250Aare%2520then%2520chosen%2520as%2520prototypes%2520for%2520the%2520rehearsal%2520memory%252C%2520ensuring%2520effective%250Aknowledge%2520retention%2520across%2520sessions.%2520We%2520evaluate%2520our%2520method%2520on%250Atime-incremental%252C%2520class-incremental%252C%2520and%2520lead-incremental%2520scenarios%2520using%2520two%250Awidely%2520used%2520ECG%2520arrhythmia%2520datasets%252C%2520Chapman%2520and%2520PTB-XL.%2520The%2520results%250Ademonstrate%2520that%2520DREAM-CL%2520outperforms%2520the%2520state-of-the-art%2520in%2520CL%2520for%2520ECG%250Aarrhythmia%2520detection.%2520Detailed%2520ablation%2520and%2520sensitivity%2520studies%2520are%2520performed%250Ato%2520validate%2520the%2520different%2520design%2520choices%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Prototype%20Rehearsal%20for%20Continual%20Learning%20in%20ECG%20Arrhythmia%0A%20%20Detection&entry.906535625=Sana%20Rahmani%20and%20Reetam%20Chatterjee%20and%20Ali%20Etemad%20and%20Javad%20Hashemi&entry.1292438233=%20%20Continual%20Learning%20%28CL%29%20methods%20aim%20to%20learn%20from%20a%20sequence%20of%20tasks%20while%0Aavoiding%20the%20challenge%20of%20forgetting%20previous%20knowledge.%20We%20present%20DREAM-CL%2C%20a%0Anovel%20CL%20method%20for%20ECG%20arrhythmia%20detection%20that%20introduces%20dynamic%20prototype%0Arehearsal%20memory.%20DREAM-CL%20selects%20representative%20prototypes%20by%20clustering%20data%0Abased%20on%20learning%20behavior%20during%20each%20training%20session.%20Within%20each%20cluster%2C%0Awe%20apply%20a%20smooth%20sorting%20operation%20that%20ranks%20samples%20by%20training%20difficulty%2C%0Acompressing%20extreme%20values%20and%20removing%20outliers.%20The%20more%20challenging%20samples%0Aare%20then%20chosen%20as%20prototypes%20for%20the%20rehearsal%20memory%2C%20ensuring%20effective%0Aknowledge%20retention%20across%20sessions.%20We%20evaluate%20our%20method%20on%0Atime-incremental%2C%20class-incremental%2C%20and%20lead-incremental%20scenarios%20using%20two%0Awidely%20used%20ECG%20arrhythmia%20datasets%2C%20Chapman%20and%20PTB-XL.%20The%20results%0Ademonstrate%20that%20DREAM-CL%20outperforms%20the%20state-of-the-art%20in%20CL%20for%20ECG%0Aarrhythmia%20detection.%20Detailed%20ablation%20and%20sensitivity%20studies%20are%20performed%0Ato%20validate%20the%20different%20design%20choices%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07555v1&entry.124074799=Read"},
{"title": "GazeGrasp: DNN-Driven Robotic Grasping with Wearable Eye-Gaze Interface", "author": "Issatay Tokmurziyev and Miguel Altamirano Cabrera and Luis Moreno and Muhammad Haris Khan and Dzmitry Tsetserukou", "abstract": "  We present GazeGrasp, a gaze-based manipulation system enabling individuals\nwith motor impairments to control collaborative robots using eye-gaze. The\nsystem employs an ESP32 CAM for eye tracking, MediaPipe for gaze detection, and\nYOLOv8 for object localization, integrated with a Universal Robot UR10 for\nmanipulation tasks. After user-specific calibration, the system allows\nintuitive object selection with a magnetic snapping effect and robot control\nvia eye gestures. Experimental evaluation involving 13 participants\ndemonstrated that the magnetic snapping effect significantly reduced gaze\nalignment time, improving task efficiency by 31%. GazeGrasp provides a robust,\nhands-free interface for assistive robotics, enhancing accessibility and\nautonomy for users.\n", "link": "http://arxiv.org/abs/2501.07255v1", "date": "2025-01-13", "relevancy": 2.4314, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6333}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6053}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GazeGrasp%3A%20DNN-Driven%20Robotic%20Grasping%20with%20Wearable%20Eye-Gaze%20Interface&body=Title%3A%20GazeGrasp%3A%20DNN-Driven%20Robotic%20Grasping%20with%20Wearable%20Eye-Gaze%20Interface%0AAuthor%3A%20Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Luis%20Moreno%20and%20Muhammad%20Haris%20Khan%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20We%20present%20GazeGrasp%2C%20a%20gaze-based%20manipulation%20system%20enabling%20individuals%0Awith%20motor%20impairments%20to%20control%20collaborative%20robots%20using%20eye-gaze.%20The%0Asystem%20employs%20an%20ESP32%20CAM%20for%20eye%20tracking%2C%20MediaPipe%20for%20gaze%20detection%2C%20and%0AYOLOv8%20for%20object%20localization%2C%20integrated%20with%20a%20Universal%20Robot%20UR10%20for%0Amanipulation%20tasks.%20After%20user-specific%20calibration%2C%20the%20system%20allows%0Aintuitive%20object%20selection%20with%20a%20magnetic%20snapping%20effect%20and%20robot%20control%0Avia%20eye%20gestures.%20Experimental%20evaluation%20involving%2013%20participants%0Ademonstrated%20that%20the%20magnetic%20snapping%20effect%20significantly%20reduced%20gaze%0Aalignment%20time%2C%20improving%20task%20efficiency%20by%2031%25.%20GazeGrasp%20provides%20a%20robust%2C%0Ahands-free%20interface%20for%20assistive%20robotics%2C%20enhancing%20accessibility%20and%0Aautonomy%20for%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGazeGrasp%253A%2520DNN-Driven%2520Robotic%2520Grasping%2520with%2520Wearable%2520Eye-Gaze%2520Interface%26entry.906535625%3DIssatay%2520Tokmurziyev%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Luis%2520Moreno%2520and%2520Muhammad%2520Haris%2520Khan%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520We%2520present%2520GazeGrasp%252C%2520a%2520gaze-based%2520manipulation%2520system%2520enabling%2520individuals%250Awith%2520motor%2520impairments%2520to%2520control%2520collaborative%2520robots%2520using%2520eye-gaze.%2520The%250Asystem%2520employs%2520an%2520ESP32%2520CAM%2520for%2520eye%2520tracking%252C%2520MediaPipe%2520for%2520gaze%2520detection%252C%2520and%250AYOLOv8%2520for%2520object%2520localization%252C%2520integrated%2520with%2520a%2520Universal%2520Robot%2520UR10%2520for%250Amanipulation%2520tasks.%2520After%2520user-specific%2520calibration%252C%2520the%2520system%2520allows%250Aintuitive%2520object%2520selection%2520with%2520a%2520magnetic%2520snapping%2520effect%2520and%2520robot%2520control%250Avia%2520eye%2520gestures.%2520Experimental%2520evaluation%2520involving%252013%2520participants%250Ademonstrated%2520that%2520the%2520magnetic%2520snapping%2520effect%2520significantly%2520reduced%2520gaze%250Aalignment%2520time%252C%2520improving%2520task%2520efficiency%2520by%252031%2525.%2520GazeGrasp%2520provides%2520a%2520robust%252C%250Ahands-free%2520interface%2520for%2520assistive%2520robotics%252C%2520enhancing%2520accessibility%2520and%250Aautonomy%2520for%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GazeGrasp%3A%20DNN-Driven%20Robotic%20Grasping%20with%20Wearable%20Eye-Gaze%20Interface&entry.906535625=Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Luis%20Moreno%20and%20Muhammad%20Haris%20Khan%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20We%20present%20GazeGrasp%2C%20a%20gaze-based%20manipulation%20system%20enabling%20individuals%0Awith%20motor%20impairments%20to%20control%20collaborative%20robots%20using%20eye-gaze.%20The%0Asystem%20employs%20an%20ESP32%20CAM%20for%20eye%20tracking%2C%20MediaPipe%20for%20gaze%20detection%2C%20and%0AYOLOv8%20for%20object%20localization%2C%20integrated%20with%20a%20Universal%20Robot%20UR10%20for%0Amanipulation%20tasks.%20After%20user-specific%20calibration%2C%20the%20system%20allows%0Aintuitive%20object%20selection%20with%20a%20magnetic%20snapping%20effect%20and%20robot%20control%0Avia%20eye%20gestures.%20Experimental%20evaluation%20involving%2013%20participants%0Ademonstrated%20that%20the%20magnetic%20snapping%20effect%20significantly%20reduced%20gaze%0Aalignment%20time%2C%20improving%20task%20efficiency%20by%2031%25.%20GazeGrasp%20provides%20a%20robust%2C%0Ahands-free%20interface%20for%20assistive%20robotics%2C%20enhancing%20accessibility%20and%0Aautonomy%20for%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07255v1&entry.124074799=Read"},
{"title": "Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for\n  Volume-to-Volume Medical Image Translation", "author": "Xiyue Zhu and Dou Hoon Kwark and Ruike Zhu and Kaiwen Hong and Yiqi Tao and Shirui Luo and Yudu Li and Zhi-Pei Liang and Volodymyr Kindratenko", "abstract": "  Despite success in volume-to-volume translations in medical images, most\nexisting models struggle to effectively capture the inherent volumetric\ndistribution using 3D representations. The current state-of-the-art approach\ncombines multiple 2D-based networks through weighted averaging, thereby\nneglecting the 3D spatial structures. Directly training 3D models in medical\nimaging presents significant challenges due to high computational demands and\nthe need for large-scale datasets. To address these challenges, we introduce\nDiff-Ensembler, a novel hybrid 2D-3D model for efficient and effective\nvolumetric translations by ensembling perpendicularly trained 2D diffusion\nmodels with a 3D network in each diffusion step. Moreover, our model can\nnaturally be used to ensemble diffusion models conditioned on different\nmodalities, allowing flexible and accurate fusion of input conditions.\nExtensive experiments demonstrate that Diff-Ensembler attains superior accuracy\nand volumetric realism in 3D medical image super-resolution and modality\ntranslation. We further demonstrate the strength of our model's volumetric\nrealism using tumor segmentation as a downstream task.\n", "link": "http://arxiv.org/abs/2501.07430v1", "date": "2025-01-13", "relevancy": 2.4217, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6213}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6022}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff-Ensembler%3A%20Learning%20to%20Ensemble%202D%20Diffusion%20Models%20for%0A%20%20Volume-to-Volume%20Medical%20Image%20Translation&body=Title%3A%20Diff-Ensembler%3A%20Learning%20to%20Ensemble%202D%20Diffusion%20Models%20for%0A%20%20Volume-to-Volume%20Medical%20Image%20Translation%0AAuthor%3A%20Xiyue%20Zhu%20and%20Dou%20Hoon%20Kwark%20and%20Ruike%20Zhu%20and%20Kaiwen%20Hong%20and%20Yiqi%20Tao%20and%20Shirui%20Luo%20and%20Yudu%20Li%20and%20Zhi-Pei%20Liang%20and%20Volodymyr%20Kindratenko%0AAbstract%3A%20%20%20Despite%20success%20in%20volume-to-volume%20translations%20in%20medical%20images%2C%20most%0Aexisting%20models%20struggle%20to%20effectively%20capture%20the%20inherent%20volumetric%0Adistribution%20using%203D%20representations.%20The%20current%20state-of-the-art%20approach%0Acombines%20multiple%202D-based%20networks%20through%20weighted%20averaging%2C%20thereby%0Aneglecting%20the%203D%20spatial%20structures.%20Directly%20training%203D%20models%20in%20medical%0Aimaging%20presents%20significant%20challenges%20due%20to%20high%20computational%20demands%20and%0Athe%20need%20for%20large-scale%20datasets.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADiff-Ensembler%2C%20a%20novel%20hybrid%202D-3D%20model%20for%20efficient%20and%20effective%0Avolumetric%20translations%20by%20ensembling%20perpendicularly%20trained%202D%20diffusion%0Amodels%20with%20a%203D%20network%20in%20each%20diffusion%20step.%20Moreover%2C%20our%20model%20can%0Anaturally%20be%20used%20to%20ensemble%20diffusion%20models%20conditioned%20on%20different%0Amodalities%2C%20allowing%20flexible%20and%20accurate%20fusion%20of%20input%20conditions.%0AExtensive%20experiments%20demonstrate%20that%20Diff-Ensembler%20attains%20superior%20accuracy%0Aand%20volumetric%20realism%20in%203D%20medical%20image%20super-resolution%20and%20modality%0Atranslation.%20We%20further%20demonstrate%20the%20strength%20of%20our%20model%27s%20volumetric%0Arealism%20using%20tumor%20segmentation%20as%20a%20downstream%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff-Ensembler%253A%2520Learning%2520to%2520Ensemble%25202D%2520Diffusion%2520Models%2520for%250A%2520%2520Volume-to-Volume%2520Medical%2520Image%2520Translation%26entry.906535625%3DXiyue%2520Zhu%2520and%2520Dou%2520Hoon%2520Kwark%2520and%2520Ruike%2520Zhu%2520and%2520Kaiwen%2520Hong%2520and%2520Yiqi%2520Tao%2520and%2520Shirui%2520Luo%2520and%2520Yudu%2520Li%2520and%2520Zhi-Pei%2520Liang%2520and%2520Volodymyr%2520Kindratenko%26entry.1292438233%3D%2520%2520Despite%2520success%2520in%2520volume-to-volume%2520translations%2520in%2520medical%2520images%252C%2520most%250Aexisting%2520models%2520struggle%2520to%2520effectively%2520capture%2520the%2520inherent%2520volumetric%250Adistribution%2520using%25203D%2520representations.%2520The%2520current%2520state-of-the-art%2520approach%250Acombines%2520multiple%25202D-based%2520networks%2520through%2520weighted%2520averaging%252C%2520thereby%250Aneglecting%2520the%25203D%2520spatial%2520structures.%2520Directly%2520training%25203D%2520models%2520in%2520medical%250Aimaging%2520presents%2520significant%2520challenges%2520due%2520to%2520high%2520computational%2520demands%2520and%250Athe%2520need%2520for%2520large-scale%2520datasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250ADiff-Ensembler%252C%2520a%2520novel%2520hybrid%25202D-3D%2520model%2520for%2520efficient%2520and%2520effective%250Avolumetric%2520translations%2520by%2520ensembling%2520perpendicularly%2520trained%25202D%2520diffusion%250Amodels%2520with%2520a%25203D%2520network%2520in%2520each%2520diffusion%2520step.%2520Moreover%252C%2520our%2520model%2520can%250Anaturally%2520be%2520used%2520to%2520ensemble%2520diffusion%2520models%2520conditioned%2520on%2520different%250Amodalities%252C%2520allowing%2520flexible%2520and%2520accurate%2520fusion%2520of%2520input%2520conditions.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Diff-Ensembler%2520attains%2520superior%2520accuracy%250Aand%2520volumetric%2520realism%2520in%25203D%2520medical%2520image%2520super-resolution%2520and%2520modality%250Atranslation.%2520We%2520further%2520demonstrate%2520the%2520strength%2520of%2520our%2520model%2527s%2520volumetric%250Arealism%2520using%2520tumor%2520segmentation%2520as%2520a%2520downstream%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff-Ensembler%3A%20Learning%20to%20Ensemble%202D%20Diffusion%20Models%20for%0A%20%20Volume-to-Volume%20Medical%20Image%20Translation&entry.906535625=Xiyue%20Zhu%20and%20Dou%20Hoon%20Kwark%20and%20Ruike%20Zhu%20and%20Kaiwen%20Hong%20and%20Yiqi%20Tao%20and%20Shirui%20Luo%20and%20Yudu%20Li%20and%20Zhi-Pei%20Liang%20and%20Volodymyr%20Kindratenko&entry.1292438233=%20%20Despite%20success%20in%20volume-to-volume%20translations%20in%20medical%20images%2C%20most%0Aexisting%20models%20struggle%20to%20effectively%20capture%20the%20inherent%20volumetric%0Adistribution%20using%203D%20representations.%20The%20current%20state-of-the-art%20approach%0Acombines%20multiple%202D-based%20networks%20through%20weighted%20averaging%2C%20thereby%0Aneglecting%20the%203D%20spatial%20structures.%20Directly%20training%203D%20models%20in%20medical%0Aimaging%20presents%20significant%20challenges%20due%20to%20high%20computational%20demands%20and%0Athe%20need%20for%20large-scale%20datasets.%20To%20address%20these%20challenges%2C%20we%20introduce%0ADiff-Ensembler%2C%20a%20novel%20hybrid%202D-3D%20model%20for%20efficient%20and%20effective%0Avolumetric%20translations%20by%20ensembling%20perpendicularly%20trained%202D%20diffusion%0Amodels%20with%20a%203D%20network%20in%20each%20diffusion%20step.%20Moreover%2C%20our%20model%20can%0Anaturally%20be%20used%20to%20ensemble%20diffusion%20models%20conditioned%20on%20different%0Amodalities%2C%20allowing%20flexible%20and%20accurate%20fusion%20of%20input%20conditions.%0AExtensive%20experiments%20demonstrate%20that%20Diff-Ensembler%20attains%20superior%20accuracy%0Aand%20volumetric%20realism%20in%203D%20medical%20image%20super-resolution%20and%20modality%0Atranslation.%20We%20further%20demonstrate%20the%20strength%20of%20our%20model%27s%20volumetric%0Arealism%20using%20tumor%20segmentation%20as%20a%20downstream%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07430v1&entry.124074799=Read"},
{"title": "FaceOracle: Chat with a Face Image Oracle", "author": "Wassim Kabbani and Kiran Raja and Raghavendra Ramachandra and Christoph Busch", "abstract": "  A face image is a mandatory part of ID and travel documents. Obtaining\nhigh-quality face images when issuing such documents is crucial for both human\nexaminers and automated face recognition systems. In several international\nstandards, face image quality requirements are intricate and defined in detail.\nIdentifying and understanding non-compliance or defects in the submitted face\nimages is crucial for both issuing authorities and applicants. In this work, we\nintroduce FaceOracle, an LLM-powered AI assistant that helps its users analyze\na face image in a natural conversational manner using standard compliant\nalgorithms. Leveraging the power of LLMs, users can get explanations of various\nface image quality concepts as well as interpret the outcome of face image\nquality assessment (FIQA) algorithms. We implement a proof-of-concept that\ndemonstrates how experts at an issuing authority could integrate FaceOracle\ninto their workflow to analyze, understand, and communicate their decisions\nmore efficiently, resulting in enhanced productivity.\n", "link": "http://arxiv.org/abs/2501.07202v1", "date": "2025-01-13", "relevancy": 2.4142, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4973}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4778}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceOracle%3A%20Chat%20with%20a%20Face%20Image%20Oracle&body=Title%3A%20FaceOracle%3A%20Chat%20with%20a%20Face%20Image%20Oracle%0AAuthor%3A%20Wassim%20Kabbani%20and%20Kiran%20Raja%20and%20Raghavendra%20Ramachandra%20and%20Christoph%20Busch%0AAbstract%3A%20%20%20A%20face%20image%20is%20a%20mandatory%20part%20of%20ID%20and%20travel%20documents.%20Obtaining%0Ahigh-quality%20face%20images%20when%20issuing%20such%20documents%20is%20crucial%20for%20both%20human%0Aexaminers%20and%20automated%20face%20recognition%20systems.%20In%20several%20international%0Astandards%2C%20face%20image%20quality%20requirements%20are%20intricate%20and%20defined%20in%20detail.%0AIdentifying%20and%20understanding%20non-compliance%20or%20defects%20in%20the%20submitted%20face%0Aimages%20is%20crucial%20for%20both%20issuing%20authorities%20and%20applicants.%20In%20this%20work%2C%20we%0Aintroduce%20FaceOracle%2C%20an%20LLM-powered%20AI%20assistant%20that%20helps%20its%20users%20analyze%0Aa%20face%20image%20in%20a%20natural%20conversational%20manner%20using%20standard%20compliant%0Aalgorithms.%20Leveraging%20the%20power%20of%20LLMs%2C%20users%20can%20get%20explanations%20of%20various%0Aface%20image%20quality%20concepts%20as%20well%20as%20interpret%20the%20outcome%20of%20face%20image%0Aquality%20assessment%20%28FIQA%29%20algorithms.%20We%20implement%20a%20proof-of-concept%20that%0Ademonstrates%20how%20experts%20at%20an%20issuing%20authority%20could%20integrate%20FaceOracle%0Ainto%20their%20workflow%20to%20analyze%2C%20understand%2C%20and%20communicate%20their%20decisions%0Amore%20efficiently%2C%20resulting%20in%20enhanced%20productivity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceOracle%253A%2520Chat%2520with%2520a%2520Face%2520Image%2520Oracle%26entry.906535625%3DWassim%2520Kabbani%2520and%2520Kiran%2520Raja%2520and%2520Raghavendra%2520Ramachandra%2520and%2520Christoph%2520Busch%26entry.1292438233%3D%2520%2520A%2520face%2520image%2520is%2520a%2520mandatory%2520part%2520of%2520ID%2520and%2520travel%2520documents.%2520Obtaining%250Ahigh-quality%2520face%2520images%2520when%2520issuing%2520such%2520documents%2520is%2520crucial%2520for%2520both%2520human%250Aexaminers%2520and%2520automated%2520face%2520recognition%2520systems.%2520In%2520several%2520international%250Astandards%252C%2520face%2520image%2520quality%2520requirements%2520are%2520intricate%2520and%2520defined%2520in%2520detail.%250AIdentifying%2520and%2520understanding%2520non-compliance%2520or%2520defects%2520in%2520the%2520submitted%2520face%250Aimages%2520is%2520crucial%2520for%2520both%2520issuing%2520authorities%2520and%2520applicants.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520FaceOracle%252C%2520an%2520LLM-powered%2520AI%2520assistant%2520that%2520helps%2520its%2520users%2520analyze%250Aa%2520face%2520image%2520in%2520a%2520natural%2520conversational%2520manner%2520using%2520standard%2520compliant%250Aalgorithms.%2520Leveraging%2520the%2520power%2520of%2520LLMs%252C%2520users%2520can%2520get%2520explanations%2520of%2520various%250Aface%2520image%2520quality%2520concepts%2520as%2520well%2520as%2520interpret%2520the%2520outcome%2520of%2520face%2520image%250Aquality%2520assessment%2520%2528FIQA%2529%2520algorithms.%2520We%2520implement%2520a%2520proof-of-concept%2520that%250Ademonstrates%2520how%2520experts%2520at%2520an%2520issuing%2520authority%2520could%2520integrate%2520FaceOracle%250Ainto%2520their%2520workflow%2520to%2520analyze%252C%2520understand%252C%2520and%2520communicate%2520their%2520decisions%250Amore%2520efficiently%252C%2520resulting%2520in%2520enhanced%2520productivity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceOracle%3A%20Chat%20with%20a%20Face%20Image%20Oracle&entry.906535625=Wassim%20Kabbani%20and%20Kiran%20Raja%20and%20Raghavendra%20Ramachandra%20and%20Christoph%20Busch&entry.1292438233=%20%20A%20face%20image%20is%20a%20mandatory%20part%20of%20ID%20and%20travel%20documents.%20Obtaining%0Ahigh-quality%20face%20images%20when%20issuing%20such%20documents%20is%20crucial%20for%20both%20human%0Aexaminers%20and%20automated%20face%20recognition%20systems.%20In%20several%20international%0Astandards%2C%20face%20image%20quality%20requirements%20are%20intricate%20and%20defined%20in%20detail.%0AIdentifying%20and%20understanding%20non-compliance%20or%20defects%20in%20the%20submitted%20face%0Aimages%20is%20crucial%20for%20both%20issuing%20authorities%20and%20applicants.%20In%20this%20work%2C%20we%0Aintroduce%20FaceOracle%2C%20an%20LLM-powered%20AI%20assistant%20that%20helps%20its%20users%20analyze%0Aa%20face%20image%20in%20a%20natural%20conversational%20manner%20using%20standard%20compliant%0Aalgorithms.%20Leveraging%20the%20power%20of%20LLMs%2C%20users%20can%20get%20explanations%20of%20various%0Aface%20image%20quality%20concepts%20as%20well%20as%20interpret%20the%20outcome%20of%20face%20image%0Aquality%20assessment%20%28FIQA%29%20algorithms.%20We%20implement%20a%20proof-of-concept%20that%0Ademonstrates%20how%20experts%20at%20an%20issuing%20authority%20could%20integrate%20FaceOracle%0Ainto%20their%20workflow%20to%20analyze%2C%20understand%2C%20and%20communicate%20their%20decisions%0Amore%20efficiently%2C%20resulting%20in%20enhanced%20productivity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07202v1&entry.124074799=Read"},
{"title": "MusicLIME: Explainable Multimodal Music Understanding", "author": "Theodoros Sotirou and Vassilis Lyberatos and Orfeas Menis Mastromichalakis and Giorgos Stamou", "abstract": "  Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.\n", "link": "http://arxiv.org/abs/2409.10496v2", "date": "2025-01-13", "relevancy": 2.4132, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4763}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MusicLIME%3A%20Explainable%20Multimodal%20Music%20Understanding&body=Title%3A%20MusicLIME%3A%20Explainable%20Multimodal%20Music%20Understanding%0AAuthor%3A%20Theodoros%20Sotirou%20and%20Vassilis%20Lyberatos%20and%20Orfeas%20Menis%20Mastromichalakis%20and%20Giorgos%20Stamou%0AAbstract%3A%20%20%20Multimodal%20models%20are%20critical%20for%20music%20understanding%20tasks%2C%20as%20they%20capture%0Athe%20complex%20interplay%20between%20audio%20and%20lyrics.%20However%2C%20as%20these%20models%20become%0Amore%20prevalent%2C%20the%20need%20for%20explainability%20grows-understanding%20how%20these%0Asystems%20make%20decisions%20is%20vital%20for%20ensuring%20fairness%2C%20reducing%20bias%2C%20and%0Afostering%20trust.%20In%20this%20paper%2C%20we%20introduce%20MusicLIME%2C%20a%20model-agnostic%0Afeature%20importance%20explanation%20method%20designed%20for%20multimodal%20music%20models.%0AUnlike%20traditional%20unimodal%20methods%2C%20which%20analyze%20each%20modality%20separately%0Awithout%20considering%20the%20interaction%20between%20them%2C%20often%20leading%20to%20incomplete%0Aor%20misleading%20explanations%2C%20MusicLIME%20reveals%20how%20audio%20and%20lyrical%20features%0Ainteract%20and%20contribute%20to%20predictions%2C%20providing%20a%20holistic%20view%20of%20the%0Amodel%27s%20decision-making.%20Additionally%2C%20we%20enhance%20local%20explanations%20by%0Aaggregating%20them%20into%20global%20explanations%2C%20giving%20users%20a%20broader%20perspective%0Aof%20model%20behavior.%20Through%20this%20work%2C%20we%20contribute%20to%20improving%20the%0Ainterpretability%20of%20multimodal%20music%20models%2C%20empowering%20users%20to%20make%20informed%0Achoices%2C%20and%20fostering%20more%20equitable%2C%20fair%2C%20and%20transparent%20music%0Aunderstanding%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMusicLIME%253A%2520Explainable%2520Multimodal%2520Music%2520Understanding%26entry.906535625%3DTheodoros%2520Sotirou%2520and%2520Vassilis%2520Lyberatos%2520and%2520Orfeas%2520Menis%2520Mastromichalakis%2520and%2520Giorgos%2520Stamou%26entry.1292438233%3D%2520%2520Multimodal%2520models%2520are%2520critical%2520for%2520music%2520understanding%2520tasks%252C%2520as%2520they%2520capture%250Athe%2520complex%2520interplay%2520between%2520audio%2520and%2520lyrics.%2520However%252C%2520as%2520these%2520models%2520become%250Amore%2520prevalent%252C%2520the%2520need%2520for%2520explainability%2520grows-understanding%2520how%2520these%250Asystems%2520make%2520decisions%2520is%2520vital%2520for%2520ensuring%2520fairness%252C%2520reducing%2520bias%252C%2520and%250Afostering%2520trust.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MusicLIME%252C%2520a%2520model-agnostic%250Afeature%2520importance%2520explanation%2520method%2520designed%2520for%2520multimodal%2520music%2520models.%250AUnlike%2520traditional%2520unimodal%2520methods%252C%2520which%2520analyze%2520each%2520modality%2520separately%250Awithout%2520considering%2520the%2520interaction%2520between%2520them%252C%2520often%2520leading%2520to%2520incomplete%250Aor%2520misleading%2520explanations%252C%2520MusicLIME%2520reveals%2520how%2520audio%2520and%2520lyrical%2520features%250Ainteract%2520and%2520contribute%2520to%2520predictions%252C%2520providing%2520a%2520holistic%2520view%2520of%2520the%250Amodel%2527s%2520decision-making.%2520Additionally%252C%2520we%2520enhance%2520local%2520explanations%2520by%250Aaggregating%2520them%2520into%2520global%2520explanations%252C%2520giving%2520users%2520a%2520broader%2520perspective%250Aof%2520model%2520behavior.%2520Through%2520this%2520work%252C%2520we%2520contribute%2520to%2520improving%2520the%250Ainterpretability%2520of%2520multimodal%2520music%2520models%252C%2520empowering%2520users%2520to%2520make%2520informed%250Achoices%252C%2520and%2520fostering%2520more%2520equitable%252C%2520fair%252C%2520and%2520transparent%2520music%250Aunderstanding%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MusicLIME%3A%20Explainable%20Multimodal%20Music%20Understanding&entry.906535625=Theodoros%20Sotirou%20and%20Vassilis%20Lyberatos%20and%20Orfeas%20Menis%20Mastromichalakis%20and%20Giorgos%20Stamou&entry.1292438233=%20%20Multimodal%20models%20are%20critical%20for%20music%20understanding%20tasks%2C%20as%20they%20capture%0Athe%20complex%20interplay%20between%20audio%20and%20lyrics.%20However%2C%20as%20these%20models%20become%0Amore%20prevalent%2C%20the%20need%20for%20explainability%20grows-understanding%20how%20these%0Asystems%20make%20decisions%20is%20vital%20for%20ensuring%20fairness%2C%20reducing%20bias%2C%20and%0Afostering%20trust.%20In%20this%20paper%2C%20we%20introduce%20MusicLIME%2C%20a%20model-agnostic%0Afeature%20importance%20explanation%20method%20designed%20for%20multimodal%20music%20models.%0AUnlike%20traditional%20unimodal%20methods%2C%20which%20analyze%20each%20modality%20separately%0Awithout%20considering%20the%20interaction%20between%20them%2C%20often%20leading%20to%20incomplete%0Aor%20misleading%20explanations%2C%20MusicLIME%20reveals%20how%20audio%20and%20lyrical%20features%0Ainteract%20and%20contribute%20to%20predictions%2C%20providing%20a%20holistic%20view%20of%20the%0Amodel%27s%20decision-making.%20Additionally%2C%20we%20enhance%20local%20explanations%20by%0Aaggregating%20them%20into%20global%20explanations%2C%20giving%20users%20a%20broader%20perspective%0Aof%20model%20behavior.%20Through%20this%20work%2C%20we%20contribute%20to%20improving%20the%0Ainterpretability%20of%20multimodal%20music%20models%2C%20empowering%20users%20to%20make%20informed%0Achoices%2C%20and%20fostering%20more%20equitable%2C%20fair%2C%20and%20transparent%20music%0Aunderstanding%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10496v2&entry.124074799=Read"},
{"title": "Higher-Order Topological Directionality and Directed Simplicial Neural\n  Networks", "author": "Manuel Lecha and Andrea Cavallo and Francesca Dominici and Elvin Isufi and Claudio Battiloro", "abstract": "  Topological Deep Learning (TDL) has emerged as a paradigm to process and\nlearn from signals defined on higher-order combinatorial topological spaces,\nsuch as simplicial or cell complexes. Although many complex systems have an\nasymmetric relational structure, most TDL models forcibly symmetrize these\nrelationships. In this paper, we first introduce a novel notion of higher-order\ndirectionality and we then design Directed Simplicial Neural Networks\n(Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating on\ndirected simplicial complexes able to leverage directed and possibly asymmetric\ninteractions among the simplices. To our knowledge, this is the first TDL model\nusing a notion of higher-order directionality. We theoretically and empirically\nprove that Dir-SNNs are more expressive than their directed graph counterpart\nin distinguishing isomorphic directed graphs. Experiments on a synthetic source\nlocalization task demonstrate that Dir-SNNs outperform undirected SNNs when the\nunderlying complex is directed, and perform comparably when the underlying\ncomplex is undirected.\n", "link": "http://arxiv.org/abs/2409.08389v2", "date": "2025-01-13", "relevancy": 2.4085, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4813}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher-Order%20Topological%20Directionality%20and%20Directed%20Simplicial%20Neural%0A%20%20Networks&body=Title%3A%20Higher-Order%20Topological%20Directionality%20and%20Directed%20Simplicial%20Neural%0A%20%20Networks%0AAuthor%3A%20Manuel%20Lecha%20and%20Andrea%20Cavallo%20and%20Francesca%20Dominici%20and%20Elvin%20Isufi%20and%20Claudio%20Battiloro%0AAbstract%3A%20%20%20Topological%20Deep%20Learning%20%28TDL%29%20has%20emerged%20as%20a%20paradigm%20to%20process%20and%0Alearn%20from%20signals%20defined%20on%20higher-order%20combinatorial%20topological%20spaces%2C%0Asuch%20as%20simplicial%20or%20cell%20complexes.%20Although%20many%20complex%20systems%20have%20an%0Aasymmetric%20relational%20structure%2C%20most%20TDL%20models%20forcibly%20symmetrize%20these%0Arelationships.%20In%20this%20paper%2C%20we%20first%20introduce%20a%20novel%20notion%20of%20higher-order%0Adirectionality%20and%20we%20then%20design%20Directed%20Simplicial%20Neural%20Networks%0A%28Dir-SNNs%29%20based%20on%20it.%20Dir-SNNs%20are%20message-passing%20networks%20operating%20on%0Adirected%20simplicial%20complexes%20able%20to%20leverage%20directed%20and%20possibly%20asymmetric%0Ainteractions%20among%20the%20simplices.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20TDL%20model%0Ausing%20a%20notion%20of%20higher-order%20directionality.%20We%20theoretically%20and%20empirically%0Aprove%20that%20Dir-SNNs%20are%20more%20expressive%20than%20their%20directed%20graph%20counterpart%0Ain%20distinguishing%20isomorphic%20directed%20graphs.%20Experiments%20on%20a%20synthetic%20source%0Alocalization%20task%20demonstrate%20that%20Dir-SNNs%20outperform%20undirected%20SNNs%20when%20the%0Aunderlying%20complex%20is%20directed%2C%20and%20perform%20comparably%20when%20the%20underlying%0Acomplex%20is%20undirected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher-Order%2520Topological%2520Directionality%2520and%2520Directed%2520Simplicial%2520Neural%250A%2520%2520Networks%26entry.906535625%3DManuel%2520Lecha%2520and%2520Andrea%2520Cavallo%2520and%2520Francesca%2520Dominici%2520and%2520Elvin%2520Isufi%2520and%2520Claudio%2520Battiloro%26entry.1292438233%3D%2520%2520Topological%2520Deep%2520Learning%2520%2528TDL%2529%2520has%2520emerged%2520as%2520a%2520paradigm%2520to%2520process%2520and%250Alearn%2520from%2520signals%2520defined%2520on%2520higher-order%2520combinatorial%2520topological%2520spaces%252C%250Asuch%2520as%2520simplicial%2520or%2520cell%2520complexes.%2520Although%2520many%2520complex%2520systems%2520have%2520an%250Aasymmetric%2520relational%2520structure%252C%2520most%2520TDL%2520models%2520forcibly%2520symmetrize%2520these%250Arelationships.%2520In%2520this%2520paper%252C%2520we%2520first%2520introduce%2520a%2520novel%2520notion%2520of%2520higher-order%250Adirectionality%2520and%2520we%2520then%2520design%2520Directed%2520Simplicial%2520Neural%2520Networks%250A%2528Dir-SNNs%2529%2520based%2520on%2520it.%2520Dir-SNNs%2520are%2520message-passing%2520networks%2520operating%2520on%250Adirected%2520simplicial%2520complexes%2520able%2520to%2520leverage%2520directed%2520and%2520possibly%2520asymmetric%250Ainteractions%2520among%2520the%2520simplices.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520TDL%2520model%250Ausing%2520a%2520notion%2520of%2520higher-order%2520directionality.%2520We%2520theoretically%2520and%2520empirically%250Aprove%2520that%2520Dir-SNNs%2520are%2520more%2520expressive%2520than%2520their%2520directed%2520graph%2520counterpart%250Ain%2520distinguishing%2520isomorphic%2520directed%2520graphs.%2520Experiments%2520on%2520a%2520synthetic%2520source%250Alocalization%2520task%2520demonstrate%2520that%2520Dir-SNNs%2520outperform%2520undirected%2520SNNs%2520when%2520the%250Aunderlying%2520complex%2520is%2520directed%252C%2520and%2520perform%2520comparably%2520when%2520the%2520underlying%250Acomplex%2520is%2520undirected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher-Order%20Topological%20Directionality%20and%20Directed%20Simplicial%20Neural%0A%20%20Networks&entry.906535625=Manuel%20Lecha%20and%20Andrea%20Cavallo%20and%20Francesca%20Dominici%20and%20Elvin%20Isufi%20and%20Claudio%20Battiloro&entry.1292438233=%20%20Topological%20Deep%20Learning%20%28TDL%29%20has%20emerged%20as%20a%20paradigm%20to%20process%20and%0Alearn%20from%20signals%20defined%20on%20higher-order%20combinatorial%20topological%20spaces%2C%0Asuch%20as%20simplicial%20or%20cell%20complexes.%20Although%20many%20complex%20systems%20have%20an%0Aasymmetric%20relational%20structure%2C%20most%20TDL%20models%20forcibly%20symmetrize%20these%0Arelationships.%20In%20this%20paper%2C%20we%20first%20introduce%20a%20novel%20notion%20of%20higher-order%0Adirectionality%20and%20we%20then%20design%20Directed%20Simplicial%20Neural%20Networks%0A%28Dir-SNNs%29%20based%20on%20it.%20Dir-SNNs%20are%20message-passing%20networks%20operating%20on%0Adirected%20simplicial%20complexes%20able%20to%20leverage%20directed%20and%20possibly%20asymmetric%0Ainteractions%20among%20the%20simplices.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20TDL%20model%0Ausing%20a%20notion%20of%20higher-order%20directionality.%20We%20theoretically%20and%20empirically%0Aprove%20that%20Dir-SNNs%20are%20more%20expressive%20than%20their%20directed%20graph%20counterpart%0Ain%20distinguishing%20isomorphic%20directed%20graphs.%20Experiments%20on%20a%20synthetic%20source%0Alocalization%20task%20demonstrate%20that%20Dir-SNNs%20outperform%20undirected%20SNNs%20when%20the%0Aunderlying%20complex%20is%20directed%2C%20and%20perform%20comparably%20when%20the%20underlying%0Acomplex%20is%20undirected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08389v2&entry.124074799=Read"},
{"title": "Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is\n  Not AGI", "author": "Rolf Pfister and Hansueli Jud", "abstract": "  OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposed\nto measure intelligence. This raises the question whether systems based on\nLarge Language Models (LLMs), particularly o3, demonstrate intelligence and\nprogress towards artificial general intelligence (AGI). Building on the\ndistinction between skills and intelligence made by Fran\\c{c}ois Chollet, the\ncreator of ARC-AGI, a new understanding of intelligence is introduced: an agent\nis the more intelligent, the more efficiently it can achieve the more diverse\ngoals in the more diverse worlds with the less knowledge. An analysis of the\nARC-AGI benchmark shows that its tasks represent a very specific type of\nproblem that can be solved by massive trialling of combinations of predefined\noperations. This method is also applied by o3, achieving its high score through\nthe extensive use of computing power. However, for most problems in the\nphysical world and in the human domain, solutions cannot be tested in advance\nand predefined operations are not available. Consequently, massive trialling of\npredefined operations, as o3 does, cannot be a basis for AGI - instead, new\napproaches are required that can reliably solve a wide variety of problems\nwithout existing skills. To support this development, a new benchmark for\nintelligence is outlined that covers a much higher diversity of unknown tasks\nto be solved, thus enabling a comprehensive assessment of intelligence and of\nprogress towards AGI.\n", "link": "http://arxiv.org/abs/2501.07458v1", "date": "2025-01-13", "relevancy": 2.3916, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4922}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Benchmarking%20Artificial%20Intelligence%3A%20OpenAI%27s%20o3%20Is%0A%20%20Not%20AGI&body=Title%3A%20Understanding%20and%20Benchmarking%20Artificial%20Intelligence%3A%20OpenAI%27s%20o3%20Is%0A%20%20Not%20AGI%0AAuthor%3A%20Rolf%20Pfister%20and%20Hansueli%20Jud%0AAbstract%3A%20%20%20OpenAI%27s%20o3%20achieves%20a%20high%20score%20of%2087.5%20%25%20on%20ARC-AGI%2C%20a%20benchmark%20proposed%0Ato%20measure%20intelligence.%20This%20raises%20the%20question%20whether%20systems%20based%20on%0ALarge%20Language%20Models%20%28LLMs%29%2C%20particularly%20o3%2C%20demonstrate%20intelligence%20and%0Aprogress%20towards%20artificial%20general%20intelligence%20%28AGI%29.%20Building%20on%20the%0Adistinction%20between%20skills%20and%20intelligence%20made%20by%20Fran%5Cc%7Bc%7Dois%20Chollet%2C%20the%0Acreator%20of%20ARC-AGI%2C%20a%20new%20understanding%20of%20intelligence%20is%20introduced%3A%20an%20agent%0Ais%20the%20more%20intelligent%2C%20the%20more%20efficiently%20it%20can%20achieve%20the%20more%20diverse%0Agoals%20in%20the%20more%20diverse%20worlds%20with%20the%20less%20knowledge.%20An%20analysis%20of%20the%0AARC-AGI%20benchmark%20shows%20that%20its%20tasks%20represent%20a%20very%20specific%20type%20of%0Aproblem%20that%20can%20be%20solved%20by%20massive%20trialling%20of%20combinations%20of%20predefined%0Aoperations.%20This%20method%20is%20also%20applied%20by%20o3%2C%20achieving%20its%20high%20score%20through%0Athe%20extensive%20use%20of%20computing%20power.%20However%2C%20for%20most%20problems%20in%20the%0Aphysical%20world%20and%20in%20the%20human%20domain%2C%20solutions%20cannot%20be%20tested%20in%20advance%0Aand%20predefined%20operations%20are%20not%20available.%20Consequently%2C%20massive%20trialling%20of%0Apredefined%20operations%2C%20as%20o3%20does%2C%20cannot%20be%20a%20basis%20for%20AGI%20-%20instead%2C%20new%0Aapproaches%20are%20required%20that%20can%20reliably%20solve%20a%20wide%20variety%20of%20problems%0Awithout%20existing%20skills.%20To%20support%20this%20development%2C%20a%20new%20benchmark%20for%0Aintelligence%20is%20outlined%20that%20covers%20a%20much%20higher%20diversity%20of%20unknown%20tasks%0Ato%20be%20solved%2C%20thus%20enabling%20a%20comprehensive%20assessment%20of%20intelligence%20and%20of%0Aprogress%20towards%20AGI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Benchmarking%2520Artificial%2520Intelligence%253A%2520OpenAI%2527s%2520o3%2520Is%250A%2520%2520Not%2520AGI%26entry.906535625%3DRolf%2520Pfister%2520and%2520Hansueli%2520Jud%26entry.1292438233%3D%2520%2520OpenAI%2527s%2520o3%2520achieves%2520a%2520high%2520score%2520of%252087.5%2520%2525%2520on%2520ARC-AGI%252C%2520a%2520benchmark%2520proposed%250Ato%2520measure%2520intelligence.%2520This%2520raises%2520the%2520question%2520whether%2520systems%2520based%2520on%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520particularly%2520o3%252C%2520demonstrate%2520intelligence%2520and%250Aprogress%2520towards%2520artificial%2520general%2520intelligence%2520%2528AGI%2529.%2520Building%2520on%2520the%250Adistinction%2520between%2520skills%2520and%2520intelligence%2520made%2520by%2520Fran%255Cc%257Bc%257Dois%2520Chollet%252C%2520the%250Acreator%2520of%2520ARC-AGI%252C%2520a%2520new%2520understanding%2520of%2520intelligence%2520is%2520introduced%253A%2520an%2520agent%250Ais%2520the%2520more%2520intelligent%252C%2520the%2520more%2520efficiently%2520it%2520can%2520achieve%2520the%2520more%2520diverse%250Agoals%2520in%2520the%2520more%2520diverse%2520worlds%2520with%2520the%2520less%2520knowledge.%2520An%2520analysis%2520of%2520the%250AARC-AGI%2520benchmark%2520shows%2520that%2520its%2520tasks%2520represent%2520a%2520very%2520specific%2520type%2520of%250Aproblem%2520that%2520can%2520be%2520solved%2520by%2520massive%2520trialling%2520of%2520combinations%2520of%2520predefined%250Aoperations.%2520This%2520method%2520is%2520also%2520applied%2520by%2520o3%252C%2520achieving%2520its%2520high%2520score%2520through%250Athe%2520extensive%2520use%2520of%2520computing%2520power.%2520However%252C%2520for%2520most%2520problems%2520in%2520the%250Aphysical%2520world%2520and%2520in%2520the%2520human%2520domain%252C%2520solutions%2520cannot%2520be%2520tested%2520in%2520advance%250Aand%2520predefined%2520operations%2520are%2520not%2520available.%2520Consequently%252C%2520massive%2520trialling%2520of%250Apredefined%2520operations%252C%2520as%2520o3%2520does%252C%2520cannot%2520be%2520a%2520basis%2520for%2520AGI%2520-%2520instead%252C%2520new%250Aapproaches%2520are%2520required%2520that%2520can%2520reliably%2520solve%2520a%2520wide%2520variety%2520of%2520problems%250Awithout%2520existing%2520skills.%2520To%2520support%2520this%2520development%252C%2520a%2520new%2520benchmark%2520for%250Aintelligence%2520is%2520outlined%2520that%2520covers%2520a%2520much%2520higher%2520diversity%2520of%2520unknown%2520tasks%250Ato%2520be%2520solved%252C%2520thus%2520enabling%2520a%2520comprehensive%2520assessment%2520of%2520intelligence%2520and%2520of%250Aprogress%2520towards%2520AGI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Benchmarking%20Artificial%20Intelligence%3A%20OpenAI%27s%20o3%20Is%0A%20%20Not%20AGI&entry.906535625=Rolf%20Pfister%20and%20Hansueli%20Jud&entry.1292438233=%20%20OpenAI%27s%20o3%20achieves%20a%20high%20score%20of%2087.5%20%25%20on%20ARC-AGI%2C%20a%20benchmark%20proposed%0Ato%20measure%20intelligence.%20This%20raises%20the%20question%20whether%20systems%20based%20on%0ALarge%20Language%20Models%20%28LLMs%29%2C%20particularly%20o3%2C%20demonstrate%20intelligence%20and%0Aprogress%20towards%20artificial%20general%20intelligence%20%28AGI%29.%20Building%20on%20the%0Adistinction%20between%20skills%20and%20intelligence%20made%20by%20Fran%5Cc%7Bc%7Dois%20Chollet%2C%20the%0Acreator%20of%20ARC-AGI%2C%20a%20new%20understanding%20of%20intelligence%20is%20introduced%3A%20an%20agent%0Ais%20the%20more%20intelligent%2C%20the%20more%20efficiently%20it%20can%20achieve%20the%20more%20diverse%0Agoals%20in%20the%20more%20diverse%20worlds%20with%20the%20less%20knowledge.%20An%20analysis%20of%20the%0AARC-AGI%20benchmark%20shows%20that%20its%20tasks%20represent%20a%20very%20specific%20type%20of%0Aproblem%20that%20can%20be%20solved%20by%20massive%20trialling%20of%20combinations%20of%20predefined%0Aoperations.%20This%20method%20is%20also%20applied%20by%20o3%2C%20achieving%20its%20high%20score%20through%0Athe%20extensive%20use%20of%20computing%20power.%20However%2C%20for%20most%20problems%20in%20the%0Aphysical%20world%20and%20in%20the%20human%20domain%2C%20solutions%20cannot%20be%20tested%20in%20advance%0Aand%20predefined%20operations%20are%20not%20available.%20Consequently%2C%20massive%20trialling%20of%0Apredefined%20operations%2C%20as%20o3%20does%2C%20cannot%20be%20a%20basis%20for%20AGI%20-%20instead%2C%20new%0Aapproaches%20are%20required%20that%20can%20reliably%20solve%20a%20wide%20variety%20of%20problems%0Awithout%20existing%20skills.%20To%20support%20this%20development%2C%20a%20new%20benchmark%20for%0Aintelligence%20is%20outlined%20that%20covers%20a%20much%20higher%20diversity%20of%20unknown%20tasks%0Ato%20be%20solved%2C%20thus%20enabling%20a%20comprehensive%20assessment%20of%20intelligence%20and%20of%0Aprogress%20towards%20AGI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07458v1&entry.124074799=Read"},
{"title": "BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation", "author": "Pablo Morales-\u00c1lvarez and Stergios Christodoulidis and Maria Vakalopoulou and Pablo Piantanida and Jose Dolz", "abstract": "  The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code will be publicly available upon acceptance of the\npaper.\n", "link": "http://arxiv.org/abs/2412.09718v2", "date": "2025-01-13", "relevancy": 2.3605, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BayesAdapter%3A%20enhanced%20uncertainty%20estimation%20in%20CLIP%20few-shot%0A%20%20adaptation&body=Title%3A%20BayesAdapter%3A%20enhanced%20uncertainty%20estimation%20in%20CLIP%20few-shot%0A%20%20adaptation%0AAuthor%3A%20Pablo%20Morales-%C3%81lvarez%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou%20and%20Pablo%20Piantanida%20and%20Jose%20Dolz%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20pre-trained%20vision-language%20models%20%28VLMs%29%20represents%20a%0Aparadigm%20shift%20in%20machine%20learning%2C%20with%20unprecedented%20results%20in%20a%20broad%20span%0Aof%20visual%20recognition%20tasks.%20CLIP%2C%20one%20of%20the%20most%20popular%20VLMs%2C%20has%20exhibited%0Aremarkable%20zero-shot%20and%20transfer%20learning%20capabilities%20in%20classification.%20To%0Atransfer%20CLIP%20to%20downstream%20tasks%2C%20adapters%20constitute%20a%20parameter-efficient%0Aapproach%20that%20avoids%20backpropagation%20through%20the%20large%20model%20%28unlike%20related%0Aprompt%20learning%20methods%29.%20However%2C%20CLIP%20adapters%20have%20been%20developed%20to%20target%0Adiscriminative%20performance%2C%20and%20the%20quality%20of%20their%20uncertainty%20estimates%20has%0Abeen%20overlooked.%20In%20this%20work%20we%20show%20that%20the%20discriminative%20performance%20of%0Astate-of-the-art%20CLIP%20adapters%20does%20not%20always%20correlate%20with%20their%20uncertainty%0Aestimation%20capabilities%2C%20which%20are%20essential%20for%20a%20safe%20deployment%20in%0Areal-world%20scenarios.%20We%20also%20demonstrate%20that%20one%20of%20such%20adapters%20is%20obtained%0Athrough%20MAP%20inference%20from%20a%20more%20general%20probabilistic%20framework.%20Based%20on%0Athis%20observation%20we%20introduce%20BayesAdapter%2C%20which%20leverages%20Bayesian%20inference%0Ato%20estimate%20a%20full%20probability%20distribution%20instead%20of%20a%20single%20point%2C%20better%0Acapturing%20the%20variability%20inherent%20in%20the%20parameter%20space.%20In%20a%20comprehensive%0Aempirical%20evaluation%20we%20show%20that%20our%20approach%20obtains%20high%20quality%20uncertainty%0Aestimates%20in%20the%20predictions%2C%20standing%20out%20in%20calibration%20and%20selective%0Aclassification.%20Our%20code%20will%20be%20publicly%20available%20upon%20acceptance%20of%20the%0Apaper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesAdapter%253A%2520enhanced%2520uncertainty%2520estimation%2520in%2520CLIP%2520few-shot%250A%2520%2520adaptation%26entry.906535625%3DPablo%2520Morales-%25C3%2581lvarez%2520and%2520Stergios%2520Christodoulidis%2520and%2520Maria%2520Vakalopoulou%2520and%2520Pablo%2520Piantanida%2520and%2520Jose%2520Dolz%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520represents%2520a%250Aparadigm%2520shift%2520in%2520machine%2520learning%252C%2520with%2520unprecedented%2520results%2520in%2520a%2520broad%2520span%250Aof%2520visual%2520recognition%2520tasks.%2520CLIP%252C%2520one%2520of%2520the%2520most%2520popular%2520VLMs%252C%2520has%2520exhibited%250Aremarkable%2520zero-shot%2520and%2520transfer%2520learning%2520capabilities%2520in%2520classification.%2520To%250Atransfer%2520CLIP%2520to%2520downstream%2520tasks%252C%2520adapters%2520constitute%2520a%2520parameter-efficient%250Aapproach%2520that%2520avoids%2520backpropagation%2520through%2520the%2520large%2520model%2520%2528unlike%2520related%250Aprompt%2520learning%2520methods%2529.%2520However%252C%2520CLIP%2520adapters%2520have%2520been%2520developed%2520to%2520target%250Adiscriminative%2520performance%252C%2520and%2520the%2520quality%2520of%2520their%2520uncertainty%2520estimates%2520has%250Abeen%2520overlooked.%2520In%2520this%2520work%2520we%2520show%2520that%2520the%2520discriminative%2520performance%2520of%250Astate-of-the-art%2520CLIP%2520adapters%2520does%2520not%2520always%2520correlate%2520with%2520their%2520uncertainty%250Aestimation%2520capabilities%252C%2520which%2520are%2520essential%2520for%2520a%2520safe%2520deployment%2520in%250Areal-world%2520scenarios.%2520We%2520also%2520demonstrate%2520that%2520one%2520of%2520such%2520adapters%2520is%2520obtained%250Athrough%2520MAP%2520inference%2520from%2520a%2520more%2520general%2520probabilistic%2520framework.%2520Based%2520on%250Athis%2520observation%2520we%2520introduce%2520BayesAdapter%252C%2520which%2520leverages%2520Bayesian%2520inference%250Ato%2520estimate%2520a%2520full%2520probability%2520distribution%2520instead%2520of%2520a%2520single%2520point%252C%2520better%250Acapturing%2520the%2520variability%2520inherent%2520in%2520the%2520parameter%2520space.%2520In%2520a%2520comprehensive%250Aempirical%2520evaluation%2520we%2520show%2520that%2520our%2520approach%2520obtains%2520high%2520quality%2520uncertainty%250Aestimates%2520in%2520the%2520predictions%252C%2520standing%2520out%2520in%2520calibration%2520and%2520selective%250Aclassification.%2520Our%2520code%2520will%2520be%2520publicly%2520available%2520upon%2520acceptance%2520of%2520the%250Apaper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BayesAdapter%3A%20enhanced%20uncertainty%20estimation%20in%20CLIP%20few-shot%0A%20%20adaptation&entry.906535625=Pablo%20Morales-%C3%81lvarez%20and%20Stergios%20Christodoulidis%20and%20Maria%20Vakalopoulou%20and%20Pablo%20Piantanida%20and%20Jose%20Dolz&entry.1292438233=%20%20The%20emergence%20of%20large%20pre-trained%20vision-language%20models%20%28VLMs%29%20represents%20a%0Aparadigm%20shift%20in%20machine%20learning%2C%20with%20unprecedented%20results%20in%20a%20broad%20span%0Aof%20visual%20recognition%20tasks.%20CLIP%2C%20one%20of%20the%20most%20popular%20VLMs%2C%20has%20exhibited%0Aremarkable%20zero-shot%20and%20transfer%20learning%20capabilities%20in%20classification.%20To%0Atransfer%20CLIP%20to%20downstream%20tasks%2C%20adapters%20constitute%20a%20parameter-efficient%0Aapproach%20that%20avoids%20backpropagation%20through%20the%20large%20model%20%28unlike%20related%0Aprompt%20learning%20methods%29.%20However%2C%20CLIP%20adapters%20have%20been%20developed%20to%20target%0Adiscriminative%20performance%2C%20and%20the%20quality%20of%20their%20uncertainty%20estimates%20has%0Abeen%20overlooked.%20In%20this%20work%20we%20show%20that%20the%20discriminative%20performance%20of%0Astate-of-the-art%20CLIP%20adapters%20does%20not%20always%20correlate%20with%20their%20uncertainty%0Aestimation%20capabilities%2C%20which%20are%20essential%20for%20a%20safe%20deployment%20in%0Areal-world%20scenarios.%20We%20also%20demonstrate%20that%20one%20of%20such%20adapters%20is%20obtained%0Athrough%20MAP%20inference%20from%20a%20more%20general%20probabilistic%20framework.%20Based%20on%0Athis%20observation%20we%20introduce%20BayesAdapter%2C%20which%20leverages%20Bayesian%20inference%0Ato%20estimate%20a%20full%20probability%20distribution%20instead%20of%20a%20single%20point%2C%20better%0Acapturing%20the%20variability%20inherent%20in%20the%20parameter%20space.%20In%20a%20comprehensive%0Aempirical%20evaluation%20we%20show%20that%20our%20approach%20obtains%20high%20quality%20uncertainty%0Aestimates%20in%20the%20predictions%2C%20standing%20out%20in%20calibration%20and%20selective%0Aclassification.%20Our%20code%20will%20be%20publicly%20available%20upon%20acceptance%20of%20the%0Apaper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09718v2&entry.124074799=Read"},
{"title": "OCTolyzer: Fully automatic toolkit for segmentation and feature\n  extracting in optical coherence tomography and scanning laser ophthalmoscopy\n  data", "author": "Jamie Burke and Justin Engelmann and Samuel Gibbon and Charlene Hamid and Diana Moukaddem and Dan Pugh and Tariq Farrah and Niall Strang and Neeraj Dhaun and Tom MacGillivray and Stuart King and Ian J. C. MacCormick", "abstract": "  Optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) of\nthe eye has become essential to ophthalmology and the emerging field of\noculomics, thus requiring a need for transparent, reproducible, and rapid\nanalysis of this data for clinical research and the wider research community.\nHere, we introduce OCTolyzer, the first open-source toolkit for retinochoroidal\nanalysis in OCT/SLO data. It features two analysis suites for OCT and SLO data,\nfacilitating deep learning-based anatomical segmentation and feature extraction\nof the cross-sectional retinal and choroidal layers and en face retinal\nvessels. We describe OCTolyzer and evaluate the reproducibility of its OCT\nchoroid analysis. At the population level, metrics for choroid region thickness\nwere highly reproducible, with a mean absolute error (MAE)/Pearson correlation\nfor macular volume choroid thickness (CT) of 6.7$\\mu$m/0.99, macular B-scan CT\nof 11.6$\\mu$m/0.99, and peripapillary CT of 5.0$\\mu$m/0.99. Macular choroid\nvascular index (CVI) also showed strong reproducibility, with MAE/Pearson for\nvolume CVI yielding 0.0271/0.97 and B-scan CVI 0.0130/0.91. At the eye level,\nmeasurement noise for regional and vessel metrics was below 5% and 20% of the\npopulation's variability, respectively. Outliers were caused by poor-quality\nB-scans with thick choroids and invisible choroid-sclera boundary. Processing\ntimes on a laptop CPU were under three seconds for macular/peripapillary\nB-scans and 85 seconds for volume scans. OCTolyzer can convert OCT/SLO data\ninto reproducible and clinically meaningful retinochoroidal features and will\nimprove the standardisation of ocular measurements in OCT/SLO image analysis,\nrequiring no specialised training or proprietary software to be used. OCTolyzer\nis freely available here: https://github.com/jaburke166/OCTolyzer.\n", "link": "http://arxiv.org/abs/2407.14128v2", "date": "2025-01-13", "relevancy": 2.3545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4727}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCTolyzer%3A%20Fully%20automatic%20toolkit%20for%20segmentation%20and%20feature%0A%20%20extracting%20in%20optical%20coherence%20tomography%20and%20scanning%20laser%20ophthalmoscopy%0A%20%20data&body=Title%3A%20OCTolyzer%3A%20Fully%20automatic%20toolkit%20for%20segmentation%20and%20feature%0A%20%20extracting%20in%20optical%20coherence%20tomography%20and%20scanning%20laser%20ophthalmoscopy%0A%20%20data%0AAuthor%3A%20Jamie%20Burke%20and%20Justin%20Engelmann%20and%20Samuel%20Gibbon%20and%20Charlene%20Hamid%20and%20Diana%20Moukaddem%20and%20Dan%20Pugh%20and%20Tariq%20Farrah%20and%20Niall%20Strang%20and%20Neeraj%20Dhaun%20and%20Tom%20MacGillivray%20and%20Stuart%20King%20and%20Ian%20J.%20C.%20MacCormick%0AAbstract%3A%20%20%20Optical%20coherence%20tomography%20%28OCT%29%20and%20scanning%20laser%20ophthalmoscopy%20%28SLO%29%20of%0Athe%20eye%20has%20become%20essential%20to%20ophthalmology%20and%20the%20emerging%20field%20of%0Aoculomics%2C%20thus%20requiring%20a%20need%20for%20transparent%2C%20reproducible%2C%20and%20rapid%0Aanalysis%20of%20this%20data%20for%20clinical%20research%20and%20the%20wider%20research%20community.%0AHere%2C%20we%20introduce%20OCTolyzer%2C%20the%20first%20open-source%20toolkit%20for%20retinochoroidal%0Aanalysis%20in%20OCT/SLO%20data.%20It%20features%20two%20analysis%20suites%20for%20OCT%20and%20SLO%20data%2C%0Afacilitating%20deep%20learning-based%20anatomical%20segmentation%20and%20feature%20extraction%0Aof%20the%20cross-sectional%20retinal%20and%20choroidal%20layers%20and%20en%20face%20retinal%0Avessels.%20We%20describe%20OCTolyzer%20and%20evaluate%20the%20reproducibility%20of%20its%20OCT%0Achoroid%20analysis.%20At%20the%20population%20level%2C%20metrics%20for%20choroid%20region%20thickness%0Awere%20highly%20reproducible%2C%20with%20a%20mean%20absolute%20error%20%28MAE%29/Pearson%20correlation%0Afor%20macular%20volume%20choroid%20thickness%20%28CT%29%20of%206.7%24%5Cmu%24m/0.99%2C%20macular%20B-scan%20CT%0Aof%2011.6%24%5Cmu%24m/0.99%2C%20and%20peripapillary%20CT%20of%205.0%24%5Cmu%24m/0.99.%20Macular%20choroid%0Avascular%20index%20%28CVI%29%20also%20showed%20strong%20reproducibility%2C%20with%20MAE/Pearson%20for%0Avolume%20CVI%20yielding%200.0271/0.97%20and%20B-scan%20CVI%200.0130/0.91.%20At%20the%20eye%20level%2C%0Ameasurement%20noise%20for%20regional%20and%20vessel%20metrics%20was%20below%205%25%20and%2020%25%20of%20the%0Apopulation%27s%20variability%2C%20respectively.%20Outliers%20were%20caused%20by%20poor-quality%0AB-scans%20with%20thick%20choroids%20and%20invisible%20choroid-sclera%20boundary.%20Processing%0Atimes%20on%20a%20laptop%20CPU%20were%20under%20three%20seconds%20for%20macular/peripapillary%0AB-scans%20and%2085%20seconds%20for%20volume%20scans.%20OCTolyzer%20can%20convert%20OCT/SLO%20data%0Ainto%20reproducible%20and%20clinically%20meaningful%20retinochoroidal%20features%20and%20will%0Aimprove%20the%20standardisation%20of%20ocular%20measurements%20in%20OCT/SLO%20image%20analysis%2C%0Arequiring%20no%20specialised%20training%20or%20proprietary%20software%20to%20be%20used.%20OCTolyzer%0Ais%20freely%20available%20here%3A%20https%3A//github.com/jaburke166/OCTolyzer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCTolyzer%253A%2520Fully%2520automatic%2520toolkit%2520for%2520segmentation%2520and%2520feature%250A%2520%2520extracting%2520in%2520optical%2520coherence%2520tomography%2520and%2520scanning%2520laser%2520ophthalmoscopy%250A%2520%2520data%26entry.906535625%3DJamie%2520Burke%2520and%2520Justin%2520Engelmann%2520and%2520Samuel%2520Gibbon%2520and%2520Charlene%2520Hamid%2520and%2520Diana%2520Moukaddem%2520and%2520Dan%2520Pugh%2520and%2520Tariq%2520Farrah%2520and%2520Niall%2520Strang%2520and%2520Neeraj%2520Dhaun%2520and%2520Tom%2520MacGillivray%2520and%2520Stuart%2520King%2520and%2520Ian%2520J.%2520C.%2520MacCormick%26entry.1292438233%3D%2520%2520Optical%2520coherence%2520tomography%2520%2528OCT%2529%2520and%2520scanning%2520laser%2520ophthalmoscopy%2520%2528SLO%2529%2520of%250Athe%2520eye%2520has%2520become%2520essential%2520to%2520ophthalmology%2520and%2520the%2520emerging%2520field%2520of%250Aoculomics%252C%2520thus%2520requiring%2520a%2520need%2520for%2520transparent%252C%2520reproducible%252C%2520and%2520rapid%250Aanalysis%2520of%2520this%2520data%2520for%2520clinical%2520research%2520and%2520the%2520wider%2520research%2520community.%250AHere%252C%2520we%2520introduce%2520OCTolyzer%252C%2520the%2520first%2520open-source%2520toolkit%2520for%2520retinochoroidal%250Aanalysis%2520in%2520OCT/SLO%2520data.%2520It%2520features%2520two%2520analysis%2520suites%2520for%2520OCT%2520and%2520SLO%2520data%252C%250Afacilitating%2520deep%2520learning-based%2520anatomical%2520segmentation%2520and%2520feature%2520extraction%250Aof%2520the%2520cross-sectional%2520retinal%2520and%2520choroidal%2520layers%2520and%2520en%2520face%2520retinal%250Avessels.%2520We%2520describe%2520OCTolyzer%2520and%2520evaluate%2520the%2520reproducibility%2520of%2520its%2520OCT%250Achoroid%2520analysis.%2520At%2520the%2520population%2520level%252C%2520metrics%2520for%2520choroid%2520region%2520thickness%250Awere%2520highly%2520reproducible%252C%2520with%2520a%2520mean%2520absolute%2520error%2520%2528MAE%2529/Pearson%2520correlation%250Afor%2520macular%2520volume%2520choroid%2520thickness%2520%2528CT%2529%2520of%25206.7%2524%255Cmu%2524m/0.99%252C%2520macular%2520B-scan%2520CT%250Aof%252011.6%2524%255Cmu%2524m/0.99%252C%2520and%2520peripapillary%2520CT%2520of%25205.0%2524%255Cmu%2524m/0.99.%2520Macular%2520choroid%250Avascular%2520index%2520%2528CVI%2529%2520also%2520showed%2520strong%2520reproducibility%252C%2520with%2520MAE/Pearson%2520for%250Avolume%2520CVI%2520yielding%25200.0271/0.97%2520and%2520B-scan%2520CVI%25200.0130/0.91.%2520At%2520the%2520eye%2520level%252C%250Ameasurement%2520noise%2520for%2520regional%2520and%2520vessel%2520metrics%2520was%2520below%25205%2525%2520and%252020%2525%2520of%2520the%250Apopulation%2527s%2520variability%252C%2520respectively.%2520Outliers%2520were%2520caused%2520by%2520poor-quality%250AB-scans%2520with%2520thick%2520choroids%2520and%2520invisible%2520choroid-sclera%2520boundary.%2520Processing%250Atimes%2520on%2520a%2520laptop%2520CPU%2520were%2520under%2520three%2520seconds%2520for%2520macular/peripapillary%250AB-scans%2520and%252085%2520seconds%2520for%2520volume%2520scans.%2520OCTolyzer%2520can%2520convert%2520OCT/SLO%2520data%250Ainto%2520reproducible%2520and%2520clinically%2520meaningful%2520retinochoroidal%2520features%2520and%2520will%250Aimprove%2520the%2520standardisation%2520of%2520ocular%2520measurements%2520in%2520OCT/SLO%2520image%2520analysis%252C%250Arequiring%2520no%2520specialised%2520training%2520or%2520proprietary%2520software%2520to%2520be%2520used.%2520OCTolyzer%250Ais%2520freely%2520available%2520here%253A%2520https%253A//github.com/jaburke166/OCTolyzer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCTolyzer%3A%20Fully%20automatic%20toolkit%20for%20segmentation%20and%20feature%0A%20%20extracting%20in%20optical%20coherence%20tomography%20and%20scanning%20laser%20ophthalmoscopy%0A%20%20data&entry.906535625=Jamie%20Burke%20and%20Justin%20Engelmann%20and%20Samuel%20Gibbon%20and%20Charlene%20Hamid%20and%20Diana%20Moukaddem%20and%20Dan%20Pugh%20and%20Tariq%20Farrah%20and%20Niall%20Strang%20and%20Neeraj%20Dhaun%20and%20Tom%20MacGillivray%20and%20Stuart%20King%20and%20Ian%20J.%20C.%20MacCormick&entry.1292438233=%20%20Optical%20coherence%20tomography%20%28OCT%29%20and%20scanning%20laser%20ophthalmoscopy%20%28SLO%29%20of%0Athe%20eye%20has%20become%20essential%20to%20ophthalmology%20and%20the%20emerging%20field%20of%0Aoculomics%2C%20thus%20requiring%20a%20need%20for%20transparent%2C%20reproducible%2C%20and%20rapid%0Aanalysis%20of%20this%20data%20for%20clinical%20research%20and%20the%20wider%20research%20community.%0AHere%2C%20we%20introduce%20OCTolyzer%2C%20the%20first%20open-source%20toolkit%20for%20retinochoroidal%0Aanalysis%20in%20OCT/SLO%20data.%20It%20features%20two%20analysis%20suites%20for%20OCT%20and%20SLO%20data%2C%0Afacilitating%20deep%20learning-based%20anatomical%20segmentation%20and%20feature%20extraction%0Aof%20the%20cross-sectional%20retinal%20and%20choroidal%20layers%20and%20en%20face%20retinal%0Avessels.%20We%20describe%20OCTolyzer%20and%20evaluate%20the%20reproducibility%20of%20its%20OCT%0Achoroid%20analysis.%20At%20the%20population%20level%2C%20metrics%20for%20choroid%20region%20thickness%0Awere%20highly%20reproducible%2C%20with%20a%20mean%20absolute%20error%20%28MAE%29/Pearson%20correlation%0Afor%20macular%20volume%20choroid%20thickness%20%28CT%29%20of%206.7%24%5Cmu%24m/0.99%2C%20macular%20B-scan%20CT%0Aof%2011.6%24%5Cmu%24m/0.99%2C%20and%20peripapillary%20CT%20of%205.0%24%5Cmu%24m/0.99.%20Macular%20choroid%0Avascular%20index%20%28CVI%29%20also%20showed%20strong%20reproducibility%2C%20with%20MAE/Pearson%20for%0Avolume%20CVI%20yielding%200.0271/0.97%20and%20B-scan%20CVI%200.0130/0.91.%20At%20the%20eye%20level%2C%0Ameasurement%20noise%20for%20regional%20and%20vessel%20metrics%20was%20below%205%25%20and%2020%25%20of%20the%0Apopulation%27s%20variability%2C%20respectively.%20Outliers%20were%20caused%20by%20poor-quality%0AB-scans%20with%20thick%20choroids%20and%20invisible%20choroid-sclera%20boundary.%20Processing%0Atimes%20on%20a%20laptop%20CPU%20were%20under%20three%20seconds%20for%20macular/peripapillary%0AB-scans%20and%2085%20seconds%20for%20volume%20scans.%20OCTolyzer%20can%20convert%20OCT/SLO%20data%0Ainto%20reproducible%20and%20clinically%20meaningful%20retinochoroidal%20features%20and%20will%0Aimprove%20the%20standardisation%20of%20ocular%20measurements%20in%20OCT/SLO%20image%20analysis%2C%0Arequiring%20no%20specialised%20training%20or%20proprietary%20software%20to%20be%20used.%20OCTolyzer%0Ais%20freely%20available%20here%3A%20https%3A//github.com/jaburke166/OCTolyzer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14128v2&entry.124074799=Read"},
{"title": "Distance Measure Based on an Embedding of the Manifold of K-Component\n  Gaussian Mixture Models into the Manifold of Symmetric Positive Definite\n  Matrices", "author": "Amit Vishwakarma and KS Subrahamanian Moosath", "abstract": "  In this paper, a distance between the Gaussian Mixture Models(GMMs) is\nobtained based on an embedding of the K-component Gaussian Mixture Model into\nthe manifold of the symmetric positive definite matrices. Proof of embedding of\nK-component GMMs into the manifold of symmetric positive definite matrices is\ngiven and shown that it is a submanifold. Then, proved that the manifold of\nGMMs with the pullback of induced metric is isometric to the submanifold with\nthe induced metric. Through this embedding we obtain a general lower bound for\nthe Fisher-Rao metric. This lower bound is a distance measure on the manifold\nof GMMs and we employ it for the similarity measure of GMMs. The effectiveness\nof this framework is demonstrated through an experiment on standard machine\nlearning benchmarks, achieving accuracy of 98%, 92%, and 93.33% on the UIUC,\nKTH-TIPS, and UMD texture recognition datasets respectively.\n", "link": "http://arxiv.org/abs/2501.07429v1", "date": "2025-01-13", "relevancy": 2.3328, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4729}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.472}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distance%20Measure%20Based%20on%20an%20Embedding%20of%20the%20Manifold%20of%20K-Component%0A%20%20Gaussian%20Mixture%20Models%20into%20the%20Manifold%20of%20Symmetric%20Positive%20Definite%0A%20%20Matrices&body=Title%3A%20Distance%20Measure%20Based%20on%20an%20Embedding%20of%20the%20Manifold%20of%20K-Component%0A%20%20Gaussian%20Mixture%20Models%20into%20the%20Manifold%20of%20Symmetric%20Positive%20Definite%0A%20%20Matrices%0AAuthor%3A%20Amit%20Vishwakarma%20and%20KS%20Subrahamanian%20Moosath%0AAbstract%3A%20%20%20In%20this%20paper%2C%20a%20distance%20between%20the%20Gaussian%20Mixture%20Models%28GMMs%29%20is%0Aobtained%20based%20on%20an%20embedding%20of%20the%20K-component%20Gaussian%20Mixture%20Model%20into%0Athe%20manifold%20of%20the%20symmetric%20positive%20definite%20matrices.%20Proof%20of%20embedding%20of%0AK-component%20GMMs%20into%20the%20manifold%20of%20symmetric%20positive%20definite%20matrices%20is%0Agiven%20and%20shown%20that%20it%20is%20a%20submanifold.%20Then%2C%20proved%20that%20the%20manifold%20of%0AGMMs%20with%20the%20pullback%20of%20induced%20metric%20is%20isometric%20to%20the%20submanifold%20with%0Athe%20induced%20metric.%20Through%20this%20embedding%20we%20obtain%20a%20general%20lower%20bound%20for%0Athe%20Fisher-Rao%20metric.%20This%20lower%20bound%20is%20a%20distance%20measure%20on%20the%20manifold%0Aof%20GMMs%20and%20we%20employ%20it%20for%20the%20similarity%20measure%20of%20GMMs.%20The%20effectiveness%0Aof%20this%20framework%20is%20demonstrated%20through%20an%20experiment%20on%20standard%20machine%0Alearning%20benchmarks%2C%20achieving%20accuracy%20of%2098%25%2C%2092%25%2C%20and%2093.33%25%20on%20the%20UIUC%2C%0AKTH-TIPS%2C%20and%20UMD%20texture%20recognition%20datasets%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistance%2520Measure%2520Based%2520on%2520an%2520Embedding%2520of%2520the%2520Manifold%2520of%2520K-Component%250A%2520%2520Gaussian%2520Mixture%2520Models%2520into%2520the%2520Manifold%2520of%2520Symmetric%2520Positive%2520Definite%250A%2520%2520Matrices%26entry.906535625%3DAmit%2520Vishwakarma%2520and%2520KS%2520Subrahamanian%2520Moosath%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520a%2520distance%2520between%2520the%2520Gaussian%2520Mixture%2520Models%2528GMMs%2529%2520is%250Aobtained%2520based%2520on%2520an%2520embedding%2520of%2520the%2520K-component%2520Gaussian%2520Mixture%2520Model%2520into%250Athe%2520manifold%2520of%2520the%2520symmetric%2520positive%2520definite%2520matrices.%2520Proof%2520of%2520embedding%2520of%250AK-component%2520GMMs%2520into%2520the%2520manifold%2520of%2520symmetric%2520positive%2520definite%2520matrices%2520is%250Agiven%2520and%2520shown%2520that%2520it%2520is%2520a%2520submanifold.%2520Then%252C%2520proved%2520that%2520the%2520manifold%2520of%250AGMMs%2520with%2520the%2520pullback%2520of%2520induced%2520metric%2520is%2520isometric%2520to%2520the%2520submanifold%2520with%250Athe%2520induced%2520metric.%2520Through%2520this%2520embedding%2520we%2520obtain%2520a%2520general%2520lower%2520bound%2520for%250Athe%2520Fisher-Rao%2520metric.%2520This%2520lower%2520bound%2520is%2520a%2520distance%2520measure%2520on%2520the%2520manifold%250Aof%2520GMMs%2520and%2520we%2520employ%2520it%2520for%2520the%2520similarity%2520measure%2520of%2520GMMs.%2520The%2520effectiveness%250Aof%2520this%2520framework%2520is%2520demonstrated%2520through%2520an%2520experiment%2520on%2520standard%2520machine%250Alearning%2520benchmarks%252C%2520achieving%2520accuracy%2520of%252098%2525%252C%252092%2525%252C%2520and%252093.33%2525%2520on%2520the%2520UIUC%252C%250AKTH-TIPS%252C%2520and%2520UMD%2520texture%2520recognition%2520datasets%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distance%20Measure%20Based%20on%20an%20Embedding%20of%20the%20Manifold%20of%20K-Component%0A%20%20Gaussian%20Mixture%20Models%20into%20the%20Manifold%20of%20Symmetric%20Positive%20Definite%0A%20%20Matrices&entry.906535625=Amit%20Vishwakarma%20and%20KS%20Subrahamanian%20Moosath&entry.1292438233=%20%20In%20this%20paper%2C%20a%20distance%20between%20the%20Gaussian%20Mixture%20Models%28GMMs%29%20is%0Aobtained%20based%20on%20an%20embedding%20of%20the%20K-component%20Gaussian%20Mixture%20Model%20into%0Athe%20manifold%20of%20the%20symmetric%20positive%20definite%20matrices.%20Proof%20of%20embedding%20of%0AK-component%20GMMs%20into%20the%20manifold%20of%20symmetric%20positive%20definite%20matrices%20is%0Agiven%20and%20shown%20that%20it%20is%20a%20submanifold.%20Then%2C%20proved%20that%20the%20manifold%20of%0AGMMs%20with%20the%20pullback%20of%20induced%20metric%20is%20isometric%20to%20the%20submanifold%20with%0Athe%20induced%20metric.%20Through%20this%20embedding%20we%20obtain%20a%20general%20lower%20bound%20for%0Athe%20Fisher-Rao%20metric.%20This%20lower%20bound%20is%20a%20distance%20measure%20on%20the%20manifold%0Aof%20GMMs%20and%20we%20employ%20it%20for%20the%20similarity%20measure%20of%20GMMs.%20The%20effectiveness%0Aof%20this%20framework%20is%20demonstrated%20through%20an%20experiment%20on%20standard%20machine%0Alearning%20benchmarks%2C%20achieving%20accuracy%20of%2098%25%2C%2092%25%2C%20and%2093.33%25%20on%20the%20UIUC%2C%0AKTH-TIPS%2C%20and%20UMD%20texture%20recognition%20datasets%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07429v1&entry.124074799=Read"},
{"title": "GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization", "author": "Yirui Chen and Xudong Huang and Quan Zhang and Wei Li and Mingjian Zhu and Qiangyu Yan and Simiao Li and Hanting Chen and Hailin Hu and Jie Yang and Wei Liu and Jie Hu", "abstract": "  The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location (IMDL). However, the lack of a large-scale\ndata foundation makes the IMDL task unattainable. In this paper, we build a\nlocal manipulation data generation pipeline that integrates the powerful\ncapabilities of SAM, LLM, and generative models. Upon this basis, we propose\nthe GIM dataset, which has the following advantages: 1) Large scale, GIM\nincludes over one million pairs of AI-manipulated images and real images. 2)\nRich image content, GIM encompasses a broad range of image classes. 3) Diverse\ngenerative manipulation, the images are manipulated images with\nstate-of-the-art generators and various manipulation tasks. The aforementioned\nadvantages allow for a more comprehensive evaluation of IMDL methods, extending\ntheir applicability to diverse images. We introduce the GIM benchmark with two\nsettings to evaluate existing IMDL methods. In addition, we propose a novel\nIMDL framework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)\nmodule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nthe previous state-of-the-art approach on two different benchmarks.\n", "link": "http://arxiv.org/abs/2406.16531v2", "date": "2025-01-13", "relevancy": 2.3271, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6231}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5776}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization&body=Title%3A%20GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization%0AAuthor%3A%20Yirui%20Chen%20and%20Xudong%20Huang%20and%20Quan%20Zhang%20and%20Wei%20Li%20and%20Mingjian%20Zhu%20and%20Qiangyu%20Yan%20and%20Simiao%20Li%20and%20Hanting%20Chen%20and%20Hailin%20Hu%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20Jie%20Hu%0AAbstract%3A%20%20%20The%20extraordinary%20ability%20of%20generative%20models%20emerges%20as%20a%20new%20trend%20in%0Aimage%20editing%20and%20generating%20realistic%20images%2C%20posing%20a%20serious%20threat%20to%20the%0Atrustworthiness%20of%20multimedia%20data%20and%20driving%20the%20research%20of%20image%0Amanipulation%20detection%20and%20location%20%28IMDL%29.%20However%2C%20the%20lack%20of%20a%20large-scale%0Adata%20foundation%20makes%20the%20IMDL%20task%20unattainable.%20In%20this%20paper%2C%20we%20build%20a%0Alocal%20manipulation%20data%20generation%20pipeline%20that%20integrates%20the%20powerful%0Acapabilities%20of%20SAM%2C%20LLM%2C%20and%20generative%20models.%20Upon%20this%20basis%2C%20we%20propose%0Athe%20GIM%20dataset%2C%20which%20has%20the%20following%20advantages%3A%201%29%20Large%20scale%2C%20GIM%0Aincludes%20over%20one%20million%20pairs%20of%20AI-manipulated%20images%20and%20real%20images.%202%29%0ARich%20image%20content%2C%20GIM%20encompasses%20a%20broad%20range%20of%20image%20classes.%203%29%20Diverse%0Agenerative%20manipulation%2C%20the%20images%20are%20manipulated%20images%20with%0Astate-of-the-art%20generators%20and%20various%20manipulation%20tasks.%20The%20aforementioned%0Aadvantages%20allow%20for%20a%20more%20comprehensive%20evaluation%20of%20IMDL%20methods%2C%20extending%0Atheir%20applicability%20to%20diverse%20images.%20We%20introduce%20the%20GIM%20benchmark%20with%20two%0Asettings%20to%20evaluate%20existing%20IMDL%20methods.%20In%20addition%2C%20we%20propose%20a%20novel%0AIMDL%20framework%2C%20termed%20GIMFormer%2C%20which%20consists%20of%20a%20ShadowTracer%2C%0AFrequency-Spatial%20block%20%28FSB%29%2C%20and%20a%20Multi-Window%20Anomalous%20Modeling%20%28MWAM%29%0Amodule.%20Extensive%20experiments%20on%20the%20GIM%20demonstrate%20that%20GIMFormer%20surpasses%0Athe%20previous%20state-of-the-art%20approach%20on%20two%20different%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16531v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIM%253A%2520A%2520Million-scale%2520Benchmark%2520for%2520Generative%2520Image%2520Manipulation%250A%2520%2520Detection%2520and%2520Localization%26entry.906535625%3DYirui%2520Chen%2520and%2520Xudong%2520Huang%2520and%2520Quan%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Mingjian%2520Zhu%2520and%2520Qiangyu%2520Yan%2520and%2520Simiao%2520Li%2520and%2520Hanting%2520Chen%2520and%2520Hailin%2520Hu%2520and%2520Jie%2520Yang%2520and%2520Wei%2520Liu%2520and%2520Jie%2520Hu%26entry.1292438233%3D%2520%2520The%2520extraordinary%2520ability%2520of%2520generative%2520models%2520emerges%2520as%2520a%2520new%2520trend%2520in%250Aimage%2520editing%2520and%2520generating%2520realistic%2520images%252C%2520posing%2520a%2520serious%2520threat%2520to%2520the%250Atrustworthiness%2520of%2520multimedia%2520data%2520and%2520driving%2520the%2520research%2520of%2520image%250Amanipulation%2520detection%2520and%2520location%2520%2528IMDL%2529.%2520However%252C%2520the%2520lack%2520of%2520a%2520large-scale%250Adata%2520foundation%2520makes%2520the%2520IMDL%2520task%2520unattainable.%2520In%2520this%2520paper%252C%2520we%2520build%2520a%250Alocal%2520manipulation%2520data%2520generation%2520pipeline%2520that%2520integrates%2520the%2520powerful%250Acapabilities%2520of%2520SAM%252C%2520LLM%252C%2520and%2520generative%2520models.%2520Upon%2520this%2520basis%252C%2520we%2520propose%250Athe%2520GIM%2520dataset%252C%2520which%2520has%2520the%2520following%2520advantages%253A%25201%2529%2520Large%2520scale%252C%2520GIM%250Aincludes%2520over%2520one%2520million%2520pairs%2520of%2520AI-manipulated%2520images%2520and%2520real%2520images.%25202%2529%250ARich%2520image%2520content%252C%2520GIM%2520encompasses%2520a%2520broad%2520range%2520of%2520image%2520classes.%25203%2529%2520Diverse%250Agenerative%2520manipulation%252C%2520the%2520images%2520are%2520manipulated%2520images%2520with%250Astate-of-the-art%2520generators%2520and%2520various%2520manipulation%2520tasks.%2520The%2520aforementioned%250Aadvantages%2520allow%2520for%2520a%2520more%2520comprehensive%2520evaluation%2520of%2520IMDL%2520methods%252C%2520extending%250Atheir%2520applicability%2520to%2520diverse%2520images.%2520We%2520introduce%2520the%2520GIM%2520benchmark%2520with%2520two%250Asettings%2520to%2520evaluate%2520existing%2520IMDL%2520methods.%2520In%2520addition%252C%2520we%2520propose%2520a%2520novel%250AIMDL%2520framework%252C%2520termed%2520GIMFormer%252C%2520which%2520consists%2520of%2520a%2520ShadowTracer%252C%250AFrequency-Spatial%2520block%2520%2528FSB%2529%252C%2520and%2520a%2520Multi-Window%2520Anomalous%2520Modeling%2520%2528MWAM%2529%250Amodule.%2520Extensive%2520experiments%2520on%2520the%2520GIM%2520demonstrate%2520that%2520GIMFormer%2520surpasses%250Athe%2520previous%2520state-of-the-art%2520approach%2520on%2520two%2520different%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16531v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization&entry.906535625=Yirui%20Chen%20and%20Xudong%20Huang%20and%20Quan%20Zhang%20and%20Wei%20Li%20and%20Mingjian%20Zhu%20and%20Qiangyu%20Yan%20and%20Simiao%20Li%20and%20Hanting%20Chen%20and%20Hailin%20Hu%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20Jie%20Hu&entry.1292438233=%20%20The%20extraordinary%20ability%20of%20generative%20models%20emerges%20as%20a%20new%20trend%20in%0Aimage%20editing%20and%20generating%20realistic%20images%2C%20posing%20a%20serious%20threat%20to%20the%0Atrustworthiness%20of%20multimedia%20data%20and%20driving%20the%20research%20of%20image%0Amanipulation%20detection%20and%20location%20%28IMDL%29.%20However%2C%20the%20lack%20of%20a%20large-scale%0Adata%20foundation%20makes%20the%20IMDL%20task%20unattainable.%20In%20this%20paper%2C%20we%20build%20a%0Alocal%20manipulation%20data%20generation%20pipeline%20that%20integrates%20the%20powerful%0Acapabilities%20of%20SAM%2C%20LLM%2C%20and%20generative%20models.%20Upon%20this%20basis%2C%20we%20propose%0Athe%20GIM%20dataset%2C%20which%20has%20the%20following%20advantages%3A%201%29%20Large%20scale%2C%20GIM%0Aincludes%20over%20one%20million%20pairs%20of%20AI-manipulated%20images%20and%20real%20images.%202%29%0ARich%20image%20content%2C%20GIM%20encompasses%20a%20broad%20range%20of%20image%20classes.%203%29%20Diverse%0Agenerative%20manipulation%2C%20the%20images%20are%20manipulated%20images%20with%0Astate-of-the-art%20generators%20and%20various%20manipulation%20tasks.%20The%20aforementioned%0Aadvantages%20allow%20for%20a%20more%20comprehensive%20evaluation%20of%20IMDL%20methods%2C%20extending%0Atheir%20applicability%20to%20diverse%20images.%20We%20introduce%20the%20GIM%20benchmark%20with%20two%0Asettings%20to%20evaluate%20existing%20IMDL%20methods.%20In%20addition%2C%20we%20propose%20a%20novel%0AIMDL%20framework%2C%20termed%20GIMFormer%2C%20which%20consists%20of%20a%20ShadowTracer%2C%0AFrequency-Spatial%20block%20%28FSB%29%2C%20and%20a%20Multi-Window%20Anomalous%20Modeling%20%28MWAM%29%0Amodule.%20Extensive%20experiments%20on%20the%20GIM%20demonstrate%20that%20GIMFormer%20surpasses%0Athe%20previous%20state-of-the-art%20approach%20on%20two%20different%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16531v2&entry.124074799=Read"},
{"title": "GFairHint: Improving Individual Fairness for Graph Neural Networks via\n  Fairness Hint", "author": "Paiheng Xu and Yuhang Zhou and Bang An and Wei Ai and Furong Huang", "abstract": "  Given the growing concerns about fairness in machine learning and the\nimpressive performance of Graph Neural Networks (GNNs) on graph data learning,\nalgorithmic fairness in GNNs has attracted significant attention. While many\nexisting studies improve fairness at the group level, only a few works promote\nindividual fairness, which renders similar outcomes for similar individuals. A\ndesirable framework that promotes individual fairness should (1) balance\nbetween fairness and performance, (2) accommodate two commonly-used individual\nsimilarity measures (externally annotated and computed from input features),\n(3) generalize across various GNN models, and (4) be computationally efficient.\nUnfortunately, none of the prior work achieves all the desirables. In this\nwork, we propose a novel method, GFairHint, which promotes individual fairness\nin GNNs and achieves all aforementioned desirables. GFairHint learns fairness\nrepresentations through an auxiliary link prediction task, and then\nconcatenates the representations with the learned node embeddings in original\nGNNs as a \"fairness hint\". Through extensive experimental investigations on\nfive real-world graph datasets under three prevalent GNN models covering both\nindividual similarity measures above, GFairHint achieves the best fairness\nresults in almost all combinations of datasets with various backbone models,\nwhile generating comparable utility results, with much less computational cost\ncompared to the previous state-of-the-art (SoTA) method.\n", "link": "http://arxiv.org/abs/2305.15622v2", "date": "2025-01-13", "relevancy": 2.3032, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4762}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4564}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GFairHint%3A%20Improving%20Individual%20Fairness%20for%20Graph%20Neural%20Networks%20via%0A%20%20Fairness%20Hint&body=Title%3A%20GFairHint%3A%20Improving%20Individual%20Fairness%20for%20Graph%20Neural%20Networks%20via%0A%20%20Fairness%20Hint%0AAuthor%3A%20Paiheng%20Xu%20and%20Yuhang%20Zhou%20and%20Bang%20An%20and%20Wei%20Ai%20and%20Furong%20Huang%0AAbstract%3A%20%20%20Given%20the%20growing%20concerns%20about%20fairness%20in%20machine%20learning%20and%20the%0Aimpressive%20performance%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20on%20graph%20data%20learning%2C%0Aalgorithmic%20fairness%20in%20GNNs%20has%20attracted%20significant%20attention.%20While%20many%0Aexisting%20studies%20improve%20fairness%20at%20the%20group%20level%2C%20only%20a%20few%20works%20promote%0Aindividual%20fairness%2C%20which%20renders%20similar%20outcomes%20for%20similar%20individuals.%20A%0Adesirable%20framework%20that%20promotes%20individual%20fairness%20should%20%281%29%20balance%0Abetween%20fairness%20and%20performance%2C%20%282%29%20accommodate%20two%20commonly-used%20individual%0Asimilarity%20measures%20%28externally%20annotated%20and%20computed%20from%20input%20features%29%2C%0A%283%29%20generalize%20across%20various%20GNN%20models%2C%20and%20%284%29%20be%20computationally%20efficient.%0AUnfortunately%2C%20none%20of%20the%20prior%20work%20achieves%20all%20the%20desirables.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20method%2C%20GFairHint%2C%20which%20promotes%20individual%20fairness%0Ain%20GNNs%20and%20achieves%20all%20aforementioned%20desirables.%20GFairHint%20learns%20fairness%0Arepresentations%20through%20an%20auxiliary%20link%20prediction%20task%2C%20and%20then%0Aconcatenates%20the%20representations%20with%20the%20learned%20node%20embeddings%20in%20original%0AGNNs%20as%20a%20%22fairness%20hint%22.%20Through%20extensive%20experimental%20investigations%20on%0Afive%20real-world%20graph%20datasets%20under%20three%20prevalent%20GNN%20models%20covering%20both%0Aindividual%20similarity%20measures%20above%2C%20GFairHint%20achieves%20the%20best%20fairness%0Aresults%20in%20almost%20all%20combinations%20of%20datasets%20with%20various%20backbone%20models%2C%0Awhile%20generating%20comparable%20utility%20results%2C%20with%20much%20less%20computational%20cost%0Acompared%20to%20the%20previous%20state-of-the-art%20%28SoTA%29%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGFairHint%253A%2520Improving%2520Individual%2520Fairness%2520for%2520Graph%2520Neural%2520Networks%2520via%250A%2520%2520Fairness%2520Hint%26entry.906535625%3DPaiheng%2520Xu%2520and%2520Yuhang%2520Zhou%2520and%2520Bang%2520An%2520and%2520Wei%2520Ai%2520and%2520Furong%2520Huang%26entry.1292438233%3D%2520%2520Given%2520the%2520growing%2520concerns%2520about%2520fairness%2520in%2520machine%2520learning%2520and%2520the%250Aimpressive%2520performance%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520on%2520graph%2520data%2520learning%252C%250Aalgorithmic%2520fairness%2520in%2520GNNs%2520has%2520attracted%2520significant%2520attention.%2520While%2520many%250Aexisting%2520studies%2520improve%2520fairness%2520at%2520the%2520group%2520level%252C%2520only%2520a%2520few%2520works%2520promote%250Aindividual%2520fairness%252C%2520which%2520renders%2520similar%2520outcomes%2520for%2520similar%2520individuals.%2520A%250Adesirable%2520framework%2520that%2520promotes%2520individual%2520fairness%2520should%2520%25281%2529%2520balance%250Abetween%2520fairness%2520and%2520performance%252C%2520%25282%2529%2520accommodate%2520two%2520commonly-used%2520individual%250Asimilarity%2520measures%2520%2528externally%2520annotated%2520and%2520computed%2520from%2520input%2520features%2529%252C%250A%25283%2529%2520generalize%2520across%2520various%2520GNN%2520models%252C%2520and%2520%25284%2529%2520be%2520computationally%2520efficient.%250AUnfortunately%252C%2520none%2520of%2520the%2520prior%2520work%2520achieves%2520all%2520the%2520desirables.%2520In%2520this%250Awork%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520GFairHint%252C%2520which%2520promotes%2520individual%2520fairness%250Ain%2520GNNs%2520and%2520achieves%2520all%2520aforementioned%2520desirables.%2520GFairHint%2520learns%2520fairness%250Arepresentations%2520through%2520an%2520auxiliary%2520link%2520prediction%2520task%252C%2520and%2520then%250Aconcatenates%2520the%2520representations%2520with%2520the%2520learned%2520node%2520embeddings%2520in%2520original%250AGNNs%2520as%2520a%2520%2522fairness%2520hint%2522.%2520Through%2520extensive%2520experimental%2520investigations%2520on%250Afive%2520real-world%2520graph%2520datasets%2520under%2520three%2520prevalent%2520GNN%2520models%2520covering%2520both%250Aindividual%2520similarity%2520measures%2520above%252C%2520GFairHint%2520achieves%2520the%2520best%2520fairness%250Aresults%2520in%2520almost%2520all%2520combinations%2520of%2520datasets%2520with%2520various%2520backbone%2520models%252C%250Awhile%2520generating%2520comparable%2520utility%2520results%252C%2520with%2520much%2520less%2520computational%2520cost%250Acompared%2520to%2520the%2520previous%2520state-of-the-art%2520%2528SoTA%2529%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GFairHint%3A%20Improving%20Individual%20Fairness%20for%20Graph%20Neural%20Networks%20via%0A%20%20Fairness%20Hint&entry.906535625=Paiheng%20Xu%20and%20Yuhang%20Zhou%20and%20Bang%20An%20and%20Wei%20Ai%20and%20Furong%20Huang&entry.1292438233=%20%20Given%20the%20growing%20concerns%20about%20fairness%20in%20machine%20learning%20and%20the%0Aimpressive%20performance%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20on%20graph%20data%20learning%2C%0Aalgorithmic%20fairness%20in%20GNNs%20has%20attracted%20significant%20attention.%20While%20many%0Aexisting%20studies%20improve%20fairness%20at%20the%20group%20level%2C%20only%20a%20few%20works%20promote%0Aindividual%20fairness%2C%20which%20renders%20similar%20outcomes%20for%20similar%20individuals.%20A%0Adesirable%20framework%20that%20promotes%20individual%20fairness%20should%20%281%29%20balance%0Abetween%20fairness%20and%20performance%2C%20%282%29%20accommodate%20two%20commonly-used%20individual%0Asimilarity%20measures%20%28externally%20annotated%20and%20computed%20from%20input%20features%29%2C%0A%283%29%20generalize%20across%20various%20GNN%20models%2C%20and%20%284%29%20be%20computationally%20efficient.%0AUnfortunately%2C%20none%20of%20the%20prior%20work%20achieves%20all%20the%20desirables.%20In%20this%0Awork%2C%20we%20propose%20a%20novel%20method%2C%20GFairHint%2C%20which%20promotes%20individual%20fairness%0Ain%20GNNs%20and%20achieves%20all%20aforementioned%20desirables.%20GFairHint%20learns%20fairness%0Arepresentations%20through%20an%20auxiliary%20link%20prediction%20task%2C%20and%20then%0Aconcatenates%20the%20representations%20with%20the%20learned%20node%20embeddings%20in%20original%0AGNNs%20as%20a%20%22fairness%20hint%22.%20Through%20extensive%20experimental%20investigations%20on%0Afive%20real-world%20graph%20datasets%20under%20three%20prevalent%20GNN%20models%20covering%20both%0Aindividual%20similarity%20measures%20above%2C%20GFairHint%20achieves%20the%20best%20fairness%0Aresults%20in%20almost%20all%20combinations%20of%20datasets%20with%20various%20backbone%20models%2C%0Awhile%20generating%20comparable%20utility%20results%2C%20with%20much%20less%20computational%20cost%0Acompared%20to%20the%20previous%20state-of-the-art%20%28SoTA%29%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15622v2&entry.124074799=Read"},
{"title": "Rethinking Decoders for Transformer-based Semantic Segmentation: A\n  Compression Perspective", "author": "Qishuai Wen and Chun-Guang Li", "abstract": "  State-of-the-art methods for Transformer-based semantic segmentation\ntypically adopt Transformer decoders that are used to extract additional\nembeddings from image embeddings via cross-attention, refine either or both\ntypes of embeddings via self-attention, and project image embeddings onto the\nadditional embeddings via dot-product. Despite their remarkable success, these\nempirical designs still lack theoretical justifications or interpretations,\nthus hindering potentially principled improvements. In this paper, we argue\nthat there are fundamental connections between semantic segmentation and\ncompression, especially between the Transformer decoders and Principal\nComponent Analysis (PCA). From such a perspective, we derive a white-box, fully\nattentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the\ninterpretations as follows: 1) the self-attention operator refines image\nembeddings to construct an ideal principal subspace that aligns with the\nsupervision and retains most information; 2) the cross-attention operator seeks\nto find a low-rank approximation of the refined image embeddings, which is\nexpected to be a set of orthonormal bases of the principal subspace and\ncorresponds to the predefined classes; 3) the dot-product operation yields\ncompact representation for image embeddings as segmentation masks. Experiments\nconducted on dataset ADE20K find that DEPICT consistently outperforms its\nblack-box counterpart, Segmenter, and it is light weight and more robust.\n", "link": "http://arxiv.org/abs/2411.03033v2", "date": "2025-01-13", "relevancy": 2.2993, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5802}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%20A%0A%20%20Compression%20Perspective&body=Title%3A%20Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%20A%0A%20%20Compression%20Perspective%0AAuthor%3A%20Qishuai%20Wen%20and%20Chun-Guang%20Li%0AAbstract%3A%20%20%20State-of-the-art%20methods%20for%20Transformer-based%20semantic%20segmentation%0Atypically%20adopt%20Transformer%20decoders%20that%20are%20used%20to%20extract%20additional%0Aembeddings%20from%20image%20embeddings%20via%20cross-attention%2C%20refine%20either%20or%20both%0Atypes%20of%20embeddings%20via%20self-attention%2C%20and%20project%20image%20embeddings%20onto%20the%0Aadditional%20embeddings%20via%20dot-product.%20Despite%20their%20remarkable%20success%2C%20these%0Aempirical%20designs%20still%20lack%20theoretical%20justifications%20or%20interpretations%2C%0Athus%20hindering%20potentially%20principled%20improvements.%20In%20this%20paper%2C%20we%20argue%0Athat%20there%20are%20fundamental%20connections%20between%20semantic%20segmentation%20and%0Acompression%2C%20especially%20between%20the%20Transformer%20decoders%20and%20Principal%0AComponent%20Analysis%20%28PCA%29.%20From%20such%20a%20perspective%2C%20we%20derive%20a%20white-box%2C%20fully%0Aattentional%20DEcoder%20for%20PrIncipled%20semantiC%20segemenTation%20%28DEPICT%29%2C%20with%20the%0Ainterpretations%20as%20follows%3A%201%29%20the%20self-attention%20operator%20refines%20image%0Aembeddings%20to%20construct%20an%20ideal%20principal%20subspace%20that%20aligns%20with%20the%0Asupervision%20and%20retains%20most%20information%3B%202%29%20the%20cross-attention%20operator%20seeks%0Ato%20find%20a%20low-rank%20approximation%20of%20the%20refined%20image%20embeddings%2C%20which%20is%0Aexpected%20to%20be%20a%20set%20of%20orthonormal%20bases%20of%20the%20principal%20subspace%20and%0Acorresponds%20to%20the%20predefined%20classes%3B%203%29%20the%20dot-product%20operation%20yields%0Acompact%20representation%20for%20image%20embeddings%20as%20segmentation%20masks.%20Experiments%0Aconducted%20on%20dataset%20ADE20K%20find%20that%20DEPICT%20consistently%20outperforms%20its%0Ablack-box%20counterpart%2C%20Segmenter%2C%20and%20it%20is%20light%20weight%20and%20more%20robust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03033v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Decoders%2520for%2520Transformer-based%2520Semantic%2520Segmentation%253A%2520A%250A%2520%2520Compression%2520Perspective%26entry.906535625%3DQishuai%2520Wen%2520and%2520Chun-Guang%2520Li%26entry.1292438233%3D%2520%2520State-of-the-art%2520methods%2520for%2520Transformer-based%2520semantic%2520segmentation%250Atypically%2520adopt%2520Transformer%2520decoders%2520that%2520are%2520used%2520to%2520extract%2520additional%250Aembeddings%2520from%2520image%2520embeddings%2520via%2520cross-attention%252C%2520refine%2520either%2520or%2520both%250Atypes%2520of%2520embeddings%2520via%2520self-attention%252C%2520and%2520project%2520image%2520embeddings%2520onto%2520the%250Aadditional%2520embeddings%2520via%2520dot-product.%2520Despite%2520their%2520remarkable%2520success%252C%2520these%250Aempirical%2520designs%2520still%2520lack%2520theoretical%2520justifications%2520or%2520interpretations%252C%250Athus%2520hindering%2520potentially%2520principled%2520improvements.%2520In%2520this%2520paper%252C%2520we%2520argue%250Athat%2520there%2520are%2520fundamental%2520connections%2520between%2520semantic%2520segmentation%2520and%250Acompression%252C%2520especially%2520between%2520the%2520Transformer%2520decoders%2520and%2520Principal%250AComponent%2520Analysis%2520%2528PCA%2529.%2520From%2520such%2520a%2520perspective%252C%2520we%2520derive%2520a%2520white-box%252C%2520fully%250Aattentional%2520DEcoder%2520for%2520PrIncipled%2520semantiC%2520segemenTation%2520%2528DEPICT%2529%252C%2520with%2520the%250Ainterpretations%2520as%2520follows%253A%25201%2529%2520the%2520self-attention%2520operator%2520refines%2520image%250Aembeddings%2520to%2520construct%2520an%2520ideal%2520principal%2520subspace%2520that%2520aligns%2520with%2520the%250Asupervision%2520and%2520retains%2520most%2520information%253B%25202%2529%2520the%2520cross-attention%2520operator%2520seeks%250Ato%2520find%2520a%2520low-rank%2520approximation%2520of%2520the%2520refined%2520image%2520embeddings%252C%2520which%2520is%250Aexpected%2520to%2520be%2520a%2520set%2520of%2520orthonormal%2520bases%2520of%2520the%2520principal%2520subspace%2520and%250Acorresponds%2520to%2520the%2520predefined%2520classes%253B%25203%2529%2520the%2520dot-product%2520operation%2520yields%250Acompact%2520representation%2520for%2520image%2520embeddings%2520as%2520segmentation%2520masks.%2520Experiments%250Aconducted%2520on%2520dataset%2520ADE20K%2520find%2520that%2520DEPICT%2520consistently%2520outperforms%2520its%250Ablack-box%2520counterpart%252C%2520Segmenter%252C%2520and%2520it%2520is%2520light%2520weight%2520and%2520more%2520robust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03033v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Decoders%20for%20Transformer-based%20Semantic%20Segmentation%3A%20A%0A%20%20Compression%20Perspective&entry.906535625=Qishuai%20Wen%20and%20Chun-Guang%20Li&entry.1292438233=%20%20State-of-the-art%20methods%20for%20Transformer-based%20semantic%20segmentation%0Atypically%20adopt%20Transformer%20decoders%20that%20are%20used%20to%20extract%20additional%0Aembeddings%20from%20image%20embeddings%20via%20cross-attention%2C%20refine%20either%20or%20both%0Atypes%20of%20embeddings%20via%20self-attention%2C%20and%20project%20image%20embeddings%20onto%20the%0Aadditional%20embeddings%20via%20dot-product.%20Despite%20their%20remarkable%20success%2C%20these%0Aempirical%20designs%20still%20lack%20theoretical%20justifications%20or%20interpretations%2C%0Athus%20hindering%20potentially%20principled%20improvements.%20In%20this%20paper%2C%20we%20argue%0Athat%20there%20are%20fundamental%20connections%20between%20semantic%20segmentation%20and%0Acompression%2C%20especially%20between%20the%20Transformer%20decoders%20and%20Principal%0AComponent%20Analysis%20%28PCA%29.%20From%20such%20a%20perspective%2C%20we%20derive%20a%20white-box%2C%20fully%0Aattentional%20DEcoder%20for%20PrIncipled%20semantiC%20segemenTation%20%28DEPICT%29%2C%20with%20the%0Ainterpretations%20as%20follows%3A%201%29%20the%20self-attention%20operator%20refines%20image%0Aembeddings%20to%20construct%20an%20ideal%20principal%20subspace%20that%20aligns%20with%20the%0Asupervision%20and%20retains%20most%20information%3B%202%29%20the%20cross-attention%20operator%20seeks%0Ato%20find%20a%20low-rank%20approximation%20of%20the%20refined%20image%20embeddings%2C%20which%20is%0Aexpected%20to%20be%20a%20set%20of%20orthonormal%20bases%20of%20the%20principal%20subspace%20and%0Acorresponds%20to%20the%20predefined%20classes%3B%203%29%20the%20dot-product%20operation%20yields%0Acompact%20representation%20for%20image%20embeddings%20as%20segmentation%20masks.%20Experiments%0Aconducted%20on%20dataset%20ADE20K%20find%20that%20DEPICT%20consistently%20outperforms%20its%0Ablack-box%20counterpart%2C%20Segmenter%2C%20and%20it%20is%20light%20weight%20and%20more%20robust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03033v2&entry.124074799=Read"},
{"title": "Can Vision-Language Models Evaluate Handwritten Math?", "author": "Oikantik Nath and Hanani Bathina and Mohammed Safi Ur Rahman Khan and Mitesh M. Khapra", "abstract": "  Recent advancements in Vision-Language Models (VLMs) have opened new\npossibilities in automatic grading of handwritten student responses,\nparticularly in mathematics. However, a comprehensive study to test the ability\nof VLMs to evaluate and reason over handwritten content remains absent. To\naddress this gap, we introduce FERMAT, a benchmark designed to assess the\nability of VLMs to detect, localize and correct errors in handwritten\nmathematical content. FERMAT spans four key error dimensions - computational,\nconceptual, notational, and presentation - and comprises over 2,200 handwritten\nmath solutions derived from 609 manually curated problems from grades 7-12 with\nintentionally introduced perturbations. Using FERMAT we benchmark nine VLMs\nacross three tasks: error detection, localization, and correction. Our results\nreveal significant shortcomings in current VLMs in reasoning over handwritten\ntext, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We\nalso observed that some models struggle with processing handwritten content, as\ntheir accuracy improves when handwritten inputs are replaced with printed text\nor images. These findings highlight the limitations of current VLMs and reveal\nnew avenues for improvement. We release FERMAT and all the associated resources\nin the open-source to drive further research.\n", "link": "http://arxiv.org/abs/2501.07244v1", "date": "2025-01-13", "relevancy": 2.276, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Vision-Language%20Models%20Evaluate%20Handwritten%20Math%3F&body=Title%3A%20Can%20Vision-Language%20Models%20Evaluate%20Handwritten%20Math%3F%0AAuthor%3A%20Oikantik%20Nath%20and%20Hanani%20Bathina%20and%20Mohammed%20Safi%20Ur%20Rahman%20Khan%20and%20Mitesh%20M.%20Khapra%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20opened%20new%0Apossibilities%20in%20automatic%20grading%20of%20handwritten%20student%20responses%2C%0Aparticularly%20in%20mathematics.%20However%2C%20a%20comprehensive%20study%20to%20test%20the%20ability%0Aof%20VLMs%20to%20evaluate%20and%20reason%20over%20handwritten%20content%20remains%20absent.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20FERMAT%2C%20a%20benchmark%20designed%20to%20assess%20the%0Aability%20of%20VLMs%20to%20detect%2C%20localize%20and%20correct%20errors%20in%20handwritten%0Amathematical%20content.%20FERMAT%20spans%20four%20key%20error%20dimensions%20-%20computational%2C%0Aconceptual%2C%20notational%2C%20and%20presentation%20-%20and%20comprises%20over%202%2C200%20handwritten%0Amath%20solutions%20derived%20from%20609%20manually%20curated%20problems%20from%20grades%207-12%20with%0Aintentionally%20introduced%20perturbations.%20Using%20FERMAT%20we%20benchmark%20nine%20VLMs%0Aacross%20three%20tasks%3A%20error%20detection%2C%20localization%2C%20and%20correction.%20Our%20results%0Areveal%20significant%20shortcomings%20in%20current%20VLMs%20in%20reasoning%20over%20handwritten%0Atext%2C%20with%20Gemini-1.5-Pro%20achieving%20the%20highest%20error%20correction%20rate%20%2877%25%29.%20We%0Aalso%20observed%20that%20some%20models%20struggle%20with%20processing%20handwritten%20content%2C%20as%0Atheir%20accuracy%20improves%20when%20handwritten%20inputs%20are%20replaced%20with%20printed%20text%0Aor%20images.%20These%20findings%20highlight%20the%20limitations%20of%20current%20VLMs%20and%20reveal%0Anew%20avenues%20for%20improvement.%20We%20release%20FERMAT%20and%20all%20the%20associated%20resources%0Ain%20the%20open-source%20to%20drive%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Vision-Language%2520Models%2520Evaluate%2520Handwritten%2520Math%253F%26entry.906535625%3DOikantik%2520Nath%2520and%2520Hanani%2520Bathina%2520and%2520Mohammed%2520Safi%2520Ur%2520Rahman%2520Khan%2520and%2520Mitesh%2520M.%2520Khapra%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520opened%2520new%250Apossibilities%2520in%2520automatic%2520grading%2520of%2520handwritten%2520student%2520responses%252C%250Aparticularly%2520in%2520mathematics.%2520However%252C%2520a%2520comprehensive%2520study%2520to%2520test%2520the%2520ability%250Aof%2520VLMs%2520to%2520evaluate%2520and%2520reason%2520over%2520handwritten%2520content%2520remains%2520absent.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520FERMAT%252C%2520a%2520benchmark%2520designed%2520to%2520assess%2520the%250Aability%2520of%2520VLMs%2520to%2520detect%252C%2520localize%2520and%2520correct%2520errors%2520in%2520handwritten%250Amathematical%2520content.%2520FERMAT%2520spans%2520four%2520key%2520error%2520dimensions%2520-%2520computational%252C%250Aconceptual%252C%2520notational%252C%2520and%2520presentation%2520-%2520and%2520comprises%2520over%25202%252C200%2520handwritten%250Amath%2520solutions%2520derived%2520from%2520609%2520manually%2520curated%2520problems%2520from%2520grades%25207-12%2520with%250Aintentionally%2520introduced%2520perturbations.%2520Using%2520FERMAT%2520we%2520benchmark%2520nine%2520VLMs%250Aacross%2520three%2520tasks%253A%2520error%2520detection%252C%2520localization%252C%2520and%2520correction.%2520Our%2520results%250Areveal%2520significant%2520shortcomings%2520in%2520current%2520VLMs%2520in%2520reasoning%2520over%2520handwritten%250Atext%252C%2520with%2520Gemini-1.5-Pro%2520achieving%2520the%2520highest%2520error%2520correction%2520rate%2520%252877%2525%2529.%2520We%250Aalso%2520observed%2520that%2520some%2520models%2520struggle%2520with%2520processing%2520handwritten%2520content%252C%2520as%250Atheir%2520accuracy%2520improves%2520when%2520handwritten%2520inputs%2520are%2520replaced%2520with%2520printed%2520text%250Aor%2520images.%2520These%2520findings%2520highlight%2520the%2520limitations%2520of%2520current%2520VLMs%2520and%2520reveal%250Anew%2520avenues%2520for%2520improvement.%2520We%2520release%2520FERMAT%2520and%2520all%2520the%2520associated%2520resources%250Ain%2520the%2520open-source%2520to%2520drive%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Vision-Language%20Models%20Evaluate%20Handwritten%20Math%3F&entry.906535625=Oikantik%20Nath%20and%20Hanani%20Bathina%20and%20Mohammed%20Safi%20Ur%20Rahman%20Khan%20and%20Mitesh%20M.%20Khapra&entry.1292438233=%20%20Recent%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20have%20opened%20new%0Apossibilities%20in%20automatic%20grading%20of%20handwritten%20student%20responses%2C%0Aparticularly%20in%20mathematics.%20However%2C%20a%20comprehensive%20study%20to%20test%20the%20ability%0Aof%20VLMs%20to%20evaluate%20and%20reason%20over%20handwritten%20content%20remains%20absent.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20FERMAT%2C%20a%20benchmark%20designed%20to%20assess%20the%0Aability%20of%20VLMs%20to%20detect%2C%20localize%20and%20correct%20errors%20in%20handwritten%0Amathematical%20content.%20FERMAT%20spans%20four%20key%20error%20dimensions%20-%20computational%2C%0Aconceptual%2C%20notational%2C%20and%20presentation%20-%20and%20comprises%20over%202%2C200%20handwritten%0Amath%20solutions%20derived%20from%20609%20manually%20curated%20problems%20from%20grades%207-12%20with%0Aintentionally%20introduced%20perturbations.%20Using%20FERMAT%20we%20benchmark%20nine%20VLMs%0Aacross%20three%20tasks%3A%20error%20detection%2C%20localization%2C%20and%20correction.%20Our%20results%0Areveal%20significant%20shortcomings%20in%20current%20VLMs%20in%20reasoning%20over%20handwritten%0Atext%2C%20with%20Gemini-1.5-Pro%20achieving%20the%20highest%20error%20correction%20rate%20%2877%25%29.%20We%0Aalso%20observed%20that%20some%20models%20struggle%20with%20processing%20handwritten%20content%2C%20as%0Atheir%20accuracy%20improves%20when%20handwritten%20inputs%20are%20replaced%20with%20printed%20text%0Aor%20images.%20These%20findings%20highlight%20the%20limitations%20of%20current%20VLMs%20and%20reveal%0Anew%20avenues%20for%20improvement.%20We%20release%20FERMAT%20and%20all%20the%20associated%20resources%0Ain%20the%20open-source%20to%20drive%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07244v1&entry.124074799=Read"},
{"title": "Improving Forward Compatibility in Class Incremental Learning by\n  Increasing Representation Rank and Feature Richness", "author": "Jaeill Kim and Wonseok Lee and Moonjung Eo and Wonjong Rhee", "abstract": "  Class Incremental Learning (CIL) constitutes a pivotal subfield within\ncontinual learning, aimed at enabling models to progressively learn new\nclassification tasks while retaining knowledge obtained from prior tasks.\nAlthough previous studies have predominantly focused on backward compatible\napproaches to mitigate catastrophic forgetting, recent investigations have\nintroduced forward compatible methods to enhance performance on novel tasks and\ncomplement existing backward compatible methods. In this study, we introduce an\neffective-Rank based Feature Richness enhancement (RFR) method, designed for\nimproving forward compatibility. Specifically, this method increases the\neffective rank of representations during the base session, thereby facilitating\nthe incorporation of more informative features pertinent to unseen novel tasks.\nConsequently, RFR achieves dual objectives in backward and forward\ncompatibility: minimizing feature extractor modifications and enhancing novel\ntask performance, respectively. To validate the efficacy of our approach, we\nestablish a theoretical connection between effective rank and the Shannon\nentropy of representations. Subsequently, we conduct comprehensive experiments\nby integrating RFR into eleven well-known CIL methods. Our results demonstrate\nthe effectiveness of our approach in enhancing novel-task performance while\nmitigating catastrophic forgetting. Furthermore, our method notably improves\nthe average incremental accuracy across all eleven cases examined.\n", "link": "http://arxiv.org/abs/2403.15517v2", "date": "2025-01-13", "relevancy": 2.2757, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4737}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4477}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Forward%20Compatibility%20in%20Class%20Incremental%20Learning%20by%0A%20%20Increasing%20Representation%20Rank%20and%20Feature%20Richness&body=Title%3A%20Improving%20Forward%20Compatibility%20in%20Class%20Incremental%20Learning%20by%0A%20%20Increasing%20Representation%20Rank%20and%20Feature%20Richness%0AAuthor%3A%20Jaeill%20Kim%20and%20Wonseok%20Lee%20and%20Moonjung%20Eo%20and%20Wonjong%20Rhee%0AAbstract%3A%20%20%20Class%20Incremental%20Learning%20%28CIL%29%20constitutes%20a%20pivotal%20subfield%20within%0Acontinual%20learning%2C%20aimed%20at%20enabling%20models%20to%20progressively%20learn%20new%0Aclassification%20tasks%20while%20retaining%20knowledge%20obtained%20from%20prior%20tasks.%0AAlthough%20previous%20studies%20have%20predominantly%20focused%20on%20backward%20compatible%0Aapproaches%20to%20mitigate%20catastrophic%20forgetting%2C%20recent%20investigations%20have%0Aintroduced%20forward%20compatible%20methods%20to%20enhance%20performance%20on%20novel%20tasks%20and%0Acomplement%20existing%20backward%20compatible%20methods.%20In%20this%20study%2C%20we%20introduce%20an%0Aeffective-Rank%20based%20Feature%20Richness%20enhancement%20%28RFR%29%20method%2C%20designed%20for%0Aimproving%20forward%20compatibility.%20Specifically%2C%20this%20method%20increases%20the%0Aeffective%20rank%20of%20representations%20during%20the%20base%20session%2C%20thereby%20facilitating%0Athe%20incorporation%20of%20more%20informative%20features%20pertinent%20to%20unseen%20novel%20tasks.%0AConsequently%2C%20RFR%20achieves%20dual%20objectives%20in%20backward%20and%20forward%0Acompatibility%3A%20minimizing%20feature%20extractor%20modifications%20and%20enhancing%20novel%0Atask%20performance%2C%20respectively.%20To%20validate%20the%20efficacy%20of%20our%20approach%2C%20we%0Aestablish%20a%20theoretical%20connection%20between%20effective%20rank%20and%20the%20Shannon%0Aentropy%20of%20representations.%20Subsequently%2C%20we%20conduct%20comprehensive%20experiments%0Aby%20integrating%20RFR%20into%20eleven%20well-known%20CIL%20methods.%20Our%20results%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20in%20enhancing%20novel-task%20performance%20while%0Amitigating%20catastrophic%20forgetting.%20Furthermore%2C%20our%20method%20notably%20improves%0Athe%20average%20incremental%20accuracy%20across%20all%20eleven%20cases%20examined.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15517v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Forward%2520Compatibility%2520in%2520Class%2520Incremental%2520Learning%2520by%250A%2520%2520Increasing%2520Representation%2520Rank%2520and%2520Feature%2520Richness%26entry.906535625%3DJaeill%2520Kim%2520and%2520Wonseok%2520Lee%2520and%2520Moonjung%2520Eo%2520and%2520Wonjong%2520Rhee%26entry.1292438233%3D%2520%2520Class%2520Incremental%2520Learning%2520%2528CIL%2529%2520constitutes%2520a%2520pivotal%2520subfield%2520within%250Acontinual%2520learning%252C%2520aimed%2520at%2520enabling%2520models%2520to%2520progressively%2520learn%2520new%250Aclassification%2520tasks%2520while%2520retaining%2520knowledge%2520obtained%2520from%2520prior%2520tasks.%250AAlthough%2520previous%2520studies%2520have%2520predominantly%2520focused%2520on%2520backward%2520compatible%250Aapproaches%2520to%2520mitigate%2520catastrophic%2520forgetting%252C%2520recent%2520investigations%2520have%250Aintroduced%2520forward%2520compatible%2520methods%2520to%2520enhance%2520performance%2520on%2520novel%2520tasks%2520and%250Acomplement%2520existing%2520backward%2520compatible%2520methods.%2520In%2520this%2520study%252C%2520we%2520introduce%2520an%250Aeffective-Rank%2520based%2520Feature%2520Richness%2520enhancement%2520%2528RFR%2529%2520method%252C%2520designed%2520for%250Aimproving%2520forward%2520compatibility.%2520Specifically%252C%2520this%2520method%2520increases%2520the%250Aeffective%2520rank%2520of%2520representations%2520during%2520the%2520base%2520session%252C%2520thereby%2520facilitating%250Athe%2520incorporation%2520of%2520more%2520informative%2520features%2520pertinent%2520to%2520unseen%2520novel%2520tasks.%250AConsequently%252C%2520RFR%2520achieves%2520dual%2520objectives%2520in%2520backward%2520and%2520forward%250Acompatibility%253A%2520minimizing%2520feature%2520extractor%2520modifications%2520and%2520enhancing%2520novel%250Atask%2520performance%252C%2520respectively.%2520To%2520validate%2520the%2520efficacy%2520of%2520our%2520approach%252C%2520we%250Aestablish%2520a%2520theoretical%2520connection%2520between%2520effective%2520rank%2520and%2520the%2520Shannon%250Aentropy%2520of%2520representations.%2520Subsequently%252C%2520we%2520conduct%2520comprehensive%2520experiments%250Aby%2520integrating%2520RFR%2520into%2520eleven%2520well-known%2520CIL%2520methods.%2520Our%2520results%2520demonstrate%250Athe%2520effectiveness%2520of%2520our%2520approach%2520in%2520enhancing%2520novel-task%2520performance%2520while%250Amitigating%2520catastrophic%2520forgetting.%2520Furthermore%252C%2520our%2520method%2520notably%2520improves%250Athe%2520average%2520incremental%2520accuracy%2520across%2520all%2520eleven%2520cases%2520examined.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15517v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Forward%20Compatibility%20in%20Class%20Incremental%20Learning%20by%0A%20%20Increasing%20Representation%20Rank%20and%20Feature%20Richness&entry.906535625=Jaeill%20Kim%20and%20Wonseok%20Lee%20and%20Moonjung%20Eo%20and%20Wonjong%20Rhee&entry.1292438233=%20%20Class%20Incremental%20Learning%20%28CIL%29%20constitutes%20a%20pivotal%20subfield%20within%0Acontinual%20learning%2C%20aimed%20at%20enabling%20models%20to%20progressively%20learn%20new%0Aclassification%20tasks%20while%20retaining%20knowledge%20obtained%20from%20prior%20tasks.%0AAlthough%20previous%20studies%20have%20predominantly%20focused%20on%20backward%20compatible%0Aapproaches%20to%20mitigate%20catastrophic%20forgetting%2C%20recent%20investigations%20have%0Aintroduced%20forward%20compatible%20methods%20to%20enhance%20performance%20on%20novel%20tasks%20and%0Acomplement%20existing%20backward%20compatible%20methods.%20In%20this%20study%2C%20we%20introduce%20an%0Aeffective-Rank%20based%20Feature%20Richness%20enhancement%20%28RFR%29%20method%2C%20designed%20for%0Aimproving%20forward%20compatibility.%20Specifically%2C%20this%20method%20increases%20the%0Aeffective%20rank%20of%20representations%20during%20the%20base%20session%2C%20thereby%20facilitating%0Athe%20incorporation%20of%20more%20informative%20features%20pertinent%20to%20unseen%20novel%20tasks.%0AConsequently%2C%20RFR%20achieves%20dual%20objectives%20in%20backward%20and%20forward%0Acompatibility%3A%20minimizing%20feature%20extractor%20modifications%20and%20enhancing%20novel%0Atask%20performance%2C%20respectively.%20To%20validate%20the%20efficacy%20of%20our%20approach%2C%20we%0Aestablish%20a%20theoretical%20connection%20between%20effective%20rank%20and%20the%20Shannon%0Aentropy%20of%20representations.%20Subsequently%2C%20we%20conduct%20comprehensive%20experiments%0Aby%20integrating%20RFR%20into%20eleven%20well-known%20CIL%20methods.%20Our%20results%20demonstrate%0Athe%20effectiveness%20of%20our%20approach%20in%20enhancing%20novel-task%20performance%20while%0Amitigating%20catastrophic%20forgetting.%20Furthermore%2C%20our%20method%20notably%20improves%0Athe%20average%20incremental%20accuracy%20across%20all%20eleven%20cases%20examined.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15517v2&entry.124074799=Read"},
{"title": "OCORD: Open-Campus Object Removal Dataset", "author": "Shuo Zhang and Runpu Wei and Kongming Liang", "abstract": "  The rapid advancements in generative models, particularly diffusion-based\ntechniques, have revolutionized image inpainting tasks by enabling the\ngeneration of high-fidelity and diverse content. However, object removal\nremains under-explored as a specific subset of inpainting, facing challenges\nsuch as inadequate semantic understanding and the unintended generation of\nartifacts. Existing datasets for object removal often rely on synthetic data,\nwhich fails to align with real-world scenarios, limiting model performance.\nAlthough some real-world datasets address these issues partially, they suffer\nfrom scalability, annotation inefficiencies, and limited realism in physical\nphenomena such as lighting and shadows. To address these limitations, this\npaper introduces a novel approach to object removal by constructing a\nhigh-resolution real-world dataset through long-duration video capture with\nfixed camera settings. Leveraging advanced tools such as Grounding-DINO,\nSegment-Anything-Model, and MASA for automated annotation, we provides image,\nbackground, and mask pairs while significantly reducing annotation time and\nlabor. With our efficient annotation pipeline, we release the first fully open,\nhigh-resolution real-world dataset for object removal, and improved performance\nin object removal tasks through fine-tuning of pre-trained diffusion models.\n", "link": "http://arxiv.org/abs/2501.07397v1", "date": "2025-01-13", "relevancy": 2.2686, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6266}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5555}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCORD%3A%20Open-Campus%20Object%20Removal%20Dataset&body=Title%3A%20OCORD%3A%20Open-Campus%20Object%20Removal%20Dataset%0AAuthor%3A%20Shuo%20Zhang%20and%20Runpu%20Wei%20and%20Kongming%20Liang%0AAbstract%3A%20%20%20The%20rapid%20advancements%20in%20generative%20models%2C%20particularly%20diffusion-based%0Atechniques%2C%20have%20revolutionized%20image%20inpainting%20tasks%20by%20enabling%20the%0Ageneration%20of%20high-fidelity%20and%20diverse%20content.%20However%2C%20object%20removal%0Aremains%20under-explored%20as%20a%20specific%20subset%20of%20inpainting%2C%20facing%20challenges%0Asuch%20as%20inadequate%20semantic%20understanding%20and%20the%20unintended%20generation%20of%0Aartifacts.%20Existing%20datasets%20for%20object%20removal%20often%20rely%20on%20synthetic%20data%2C%0Awhich%20fails%20to%20align%20with%20real-world%20scenarios%2C%20limiting%20model%20performance.%0AAlthough%20some%20real-world%20datasets%20address%20these%20issues%20partially%2C%20they%20suffer%0Afrom%20scalability%2C%20annotation%20inefficiencies%2C%20and%20limited%20realism%20in%20physical%0Aphenomena%20such%20as%20lighting%20and%20shadows.%20To%20address%20these%20limitations%2C%20this%0Apaper%20introduces%20a%20novel%20approach%20to%20object%20removal%20by%20constructing%20a%0Ahigh-resolution%20real-world%20dataset%20through%20long-duration%20video%20capture%20with%0Afixed%20camera%20settings.%20Leveraging%20advanced%20tools%20such%20as%20Grounding-DINO%2C%0ASegment-Anything-Model%2C%20and%20MASA%20for%20automated%20annotation%2C%20we%20provides%20image%2C%0Abackground%2C%20and%20mask%20pairs%20while%20significantly%20reducing%20annotation%20time%20and%0Alabor.%20With%20our%20efficient%20annotation%20pipeline%2C%20we%20release%20the%20first%20fully%20open%2C%0Ahigh-resolution%20real-world%20dataset%20for%20object%20removal%2C%20and%20improved%20performance%0Ain%20object%20removal%20tasks%20through%20fine-tuning%20of%20pre-trained%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCORD%253A%2520Open-Campus%2520Object%2520Removal%2520Dataset%26entry.906535625%3DShuo%2520Zhang%2520and%2520Runpu%2520Wei%2520and%2520Kongming%2520Liang%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancements%2520in%2520generative%2520models%252C%2520particularly%2520diffusion-based%250Atechniques%252C%2520have%2520revolutionized%2520image%2520inpainting%2520tasks%2520by%2520enabling%2520the%250Ageneration%2520of%2520high-fidelity%2520and%2520diverse%2520content.%2520However%252C%2520object%2520removal%250Aremains%2520under-explored%2520as%2520a%2520specific%2520subset%2520of%2520inpainting%252C%2520facing%2520challenges%250Asuch%2520as%2520inadequate%2520semantic%2520understanding%2520and%2520the%2520unintended%2520generation%2520of%250Aartifacts.%2520Existing%2520datasets%2520for%2520object%2520removal%2520often%2520rely%2520on%2520synthetic%2520data%252C%250Awhich%2520fails%2520to%2520align%2520with%2520real-world%2520scenarios%252C%2520limiting%2520model%2520performance.%250AAlthough%2520some%2520real-world%2520datasets%2520address%2520these%2520issues%2520partially%252C%2520they%2520suffer%250Afrom%2520scalability%252C%2520annotation%2520inefficiencies%252C%2520and%2520limited%2520realism%2520in%2520physical%250Aphenomena%2520such%2520as%2520lighting%2520and%2520shadows.%2520To%2520address%2520these%2520limitations%252C%2520this%250Apaper%2520introduces%2520a%2520novel%2520approach%2520to%2520object%2520removal%2520by%2520constructing%2520a%250Ahigh-resolution%2520real-world%2520dataset%2520through%2520long-duration%2520video%2520capture%2520with%250Afixed%2520camera%2520settings.%2520Leveraging%2520advanced%2520tools%2520such%2520as%2520Grounding-DINO%252C%250ASegment-Anything-Model%252C%2520and%2520MASA%2520for%2520automated%2520annotation%252C%2520we%2520provides%2520image%252C%250Abackground%252C%2520and%2520mask%2520pairs%2520while%2520significantly%2520reducing%2520annotation%2520time%2520and%250Alabor.%2520With%2520our%2520efficient%2520annotation%2520pipeline%252C%2520we%2520release%2520the%2520first%2520fully%2520open%252C%250Ahigh-resolution%2520real-world%2520dataset%2520for%2520object%2520removal%252C%2520and%2520improved%2520performance%250Ain%2520object%2520removal%2520tasks%2520through%2520fine-tuning%2520of%2520pre-trained%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCORD%3A%20Open-Campus%20Object%20Removal%20Dataset&entry.906535625=Shuo%20Zhang%20and%20Runpu%20Wei%20and%20Kongming%20Liang&entry.1292438233=%20%20The%20rapid%20advancements%20in%20generative%20models%2C%20particularly%20diffusion-based%0Atechniques%2C%20have%20revolutionized%20image%20inpainting%20tasks%20by%20enabling%20the%0Ageneration%20of%20high-fidelity%20and%20diverse%20content.%20However%2C%20object%20removal%0Aremains%20under-explored%20as%20a%20specific%20subset%20of%20inpainting%2C%20facing%20challenges%0Asuch%20as%20inadequate%20semantic%20understanding%20and%20the%20unintended%20generation%20of%0Aartifacts.%20Existing%20datasets%20for%20object%20removal%20often%20rely%20on%20synthetic%20data%2C%0Awhich%20fails%20to%20align%20with%20real-world%20scenarios%2C%20limiting%20model%20performance.%0AAlthough%20some%20real-world%20datasets%20address%20these%20issues%20partially%2C%20they%20suffer%0Afrom%20scalability%2C%20annotation%20inefficiencies%2C%20and%20limited%20realism%20in%20physical%0Aphenomena%20such%20as%20lighting%20and%20shadows.%20To%20address%20these%20limitations%2C%20this%0Apaper%20introduces%20a%20novel%20approach%20to%20object%20removal%20by%20constructing%20a%0Ahigh-resolution%20real-world%20dataset%20through%20long-duration%20video%20capture%20with%0Afixed%20camera%20settings.%20Leveraging%20advanced%20tools%20such%20as%20Grounding-DINO%2C%0ASegment-Anything-Model%2C%20and%20MASA%20for%20automated%20annotation%2C%20we%20provides%20image%2C%0Abackground%2C%20and%20mask%20pairs%20while%20significantly%20reducing%20annotation%20time%20and%0Alabor.%20With%20our%20efficient%20annotation%20pipeline%2C%20we%20release%20the%20first%20fully%20open%2C%0Ahigh-resolution%20real-world%20dataset%20for%20object%20removal%2C%20and%20improved%20performance%0Ain%20object%20removal%20tasks%20through%20fine-tuning%20of%20pre-trained%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07397v1&entry.124074799=Read"},
{"title": "Few-Shot Task Learning through Inverse Generative Modeling", "author": "Aviv Netanyahu and Yilun Du and Antonia Bronars and Jyothish Pari and Joshua Tenenbaum and Tianmin Shu and Pulkit Agrawal", "abstract": "  Learning the intents of an agent, defined by its goals or motion style, is\noften extremely challenging from just a few examples. We refer to this problem\nas task concept learning and present our approach, Few-Shot Task Learning\nthrough Inverse Generative Modeling (FTL-IGM), which learns new task concepts\nby leveraging invertible neural generative models. The core idea is to pretrain\na generative model on a set of basic concepts and their demonstrations. Then,\ngiven a few demonstrations of a new concept (such as a new goal or a new\naction), our method learns the underlying concepts through backpropagation\nwithout updating the model weights, thanks to the invertibility of the\ngenerative model. We evaluate our method in five domains -- object\nrearrangement, goal-oriented navigation, motion caption of human actions,\nautonomous driving, and real-world table-top manipulation. Our experimental\nresults demonstrate that via the pretrained generative model, we successfully\nlearn novel concepts and generate agent plans or motion corresponding to these\nconcepts in (1) unseen environments and (2) in composition with training\nconcepts.\n", "link": "http://arxiv.org/abs/2411.04987v2", "date": "2025-01-13", "relevancy": 2.2682, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6066}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5456}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Task%20Learning%20through%20Inverse%20Generative%20Modeling&body=Title%3A%20Few-Shot%20Task%20Learning%20through%20Inverse%20Generative%20Modeling%0AAuthor%3A%20Aviv%20Netanyahu%20and%20Yilun%20Du%20and%20Antonia%20Bronars%20and%20Jyothish%20Pari%20and%20Joshua%20Tenenbaum%20and%20Tianmin%20Shu%20and%20Pulkit%20Agrawal%0AAbstract%3A%20%20%20Learning%20the%20intents%20of%20an%20agent%2C%20defined%20by%20its%20goals%20or%20motion%20style%2C%20is%0Aoften%20extremely%20challenging%20from%20just%20a%20few%20examples.%20We%20refer%20to%20this%20problem%0Aas%20task%20concept%20learning%20and%20present%20our%20approach%2C%20Few-Shot%20Task%20Learning%0Athrough%20Inverse%20Generative%20Modeling%20%28FTL-IGM%29%2C%20which%20learns%20new%20task%20concepts%0Aby%20leveraging%20invertible%20neural%20generative%20models.%20The%20core%20idea%20is%20to%20pretrain%0Aa%20generative%20model%20on%20a%20set%20of%20basic%20concepts%20and%20their%20demonstrations.%20Then%2C%0Agiven%20a%20few%20demonstrations%20of%20a%20new%20concept%20%28such%20as%20a%20new%20goal%20or%20a%20new%0Aaction%29%2C%20our%20method%20learns%20the%20underlying%20concepts%20through%20backpropagation%0Awithout%20updating%20the%20model%20weights%2C%20thanks%20to%20the%20invertibility%20of%20the%0Agenerative%20model.%20We%20evaluate%20our%20method%20in%20five%20domains%20--%20object%0Arearrangement%2C%20goal-oriented%20navigation%2C%20motion%20caption%20of%20human%20actions%2C%0Aautonomous%20driving%2C%20and%20real-world%20table-top%20manipulation.%20Our%20experimental%0Aresults%20demonstrate%20that%20via%20the%20pretrained%20generative%20model%2C%20we%20successfully%0Alearn%20novel%20concepts%20and%20generate%20agent%20plans%20or%20motion%20corresponding%20to%20these%0Aconcepts%20in%20%281%29%20unseen%20environments%20and%20%282%29%20in%20composition%20with%20training%0Aconcepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04987v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Task%2520Learning%2520through%2520Inverse%2520Generative%2520Modeling%26entry.906535625%3DAviv%2520Netanyahu%2520and%2520Yilun%2520Du%2520and%2520Antonia%2520Bronars%2520and%2520Jyothish%2520Pari%2520and%2520Joshua%2520Tenenbaum%2520and%2520Tianmin%2520Shu%2520and%2520Pulkit%2520Agrawal%26entry.1292438233%3D%2520%2520Learning%2520the%2520intents%2520of%2520an%2520agent%252C%2520defined%2520by%2520its%2520goals%2520or%2520motion%2520style%252C%2520is%250Aoften%2520extremely%2520challenging%2520from%2520just%2520a%2520few%2520examples.%2520We%2520refer%2520to%2520this%2520problem%250Aas%2520task%2520concept%2520learning%2520and%2520present%2520our%2520approach%252C%2520Few-Shot%2520Task%2520Learning%250Athrough%2520Inverse%2520Generative%2520Modeling%2520%2528FTL-IGM%2529%252C%2520which%2520learns%2520new%2520task%2520concepts%250Aby%2520leveraging%2520invertible%2520neural%2520generative%2520models.%2520The%2520core%2520idea%2520is%2520to%2520pretrain%250Aa%2520generative%2520model%2520on%2520a%2520set%2520of%2520basic%2520concepts%2520and%2520their%2520demonstrations.%2520Then%252C%250Agiven%2520a%2520few%2520demonstrations%2520of%2520a%2520new%2520concept%2520%2528such%2520as%2520a%2520new%2520goal%2520or%2520a%2520new%250Aaction%2529%252C%2520our%2520method%2520learns%2520the%2520underlying%2520concepts%2520through%2520backpropagation%250Awithout%2520updating%2520the%2520model%2520weights%252C%2520thanks%2520to%2520the%2520invertibility%2520of%2520the%250Agenerative%2520model.%2520We%2520evaluate%2520our%2520method%2520in%2520five%2520domains%2520--%2520object%250Arearrangement%252C%2520goal-oriented%2520navigation%252C%2520motion%2520caption%2520of%2520human%2520actions%252C%250Aautonomous%2520driving%252C%2520and%2520real-world%2520table-top%2520manipulation.%2520Our%2520experimental%250Aresults%2520demonstrate%2520that%2520via%2520the%2520pretrained%2520generative%2520model%252C%2520we%2520successfully%250Alearn%2520novel%2520concepts%2520and%2520generate%2520agent%2520plans%2520or%2520motion%2520corresponding%2520to%2520these%250Aconcepts%2520in%2520%25281%2529%2520unseen%2520environments%2520and%2520%25282%2529%2520in%2520composition%2520with%2520training%250Aconcepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04987v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Task%20Learning%20through%20Inverse%20Generative%20Modeling&entry.906535625=Aviv%20Netanyahu%20and%20Yilun%20Du%20and%20Antonia%20Bronars%20and%20Jyothish%20Pari%20and%20Joshua%20Tenenbaum%20and%20Tianmin%20Shu%20and%20Pulkit%20Agrawal&entry.1292438233=%20%20Learning%20the%20intents%20of%20an%20agent%2C%20defined%20by%20its%20goals%20or%20motion%20style%2C%20is%0Aoften%20extremely%20challenging%20from%20just%20a%20few%20examples.%20We%20refer%20to%20this%20problem%0Aas%20task%20concept%20learning%20and%20present%20our%20approach%2C%20Few-Shot%20Task%20Learning%0Athrough%20Inverse%20Generative%20Modeling%20%28FTL-IGM%29%2C%20which%20learns%20new%20task%20concepts%0Aby%20leveraging%20invertible%20neural%20generative%20models.%20The%20core%20idea%20is%20to%20pretrain%0Aa%20generative%20model%20on%20a%20set%20of%20basic%20concepts%20and%20their%20demonstrations.%20Then%2C%0Agiven%20a%20few%20demonstrations%20of%20a%20new%20concept%20%28such%20as%20a%20new%20goal%20or%20a%20new%0Aaction%29%2C%20our%20method%20learns%20the%20underlying%20concepts%20through%20backpropagation%0Awithout%20updating%20the%20model%20weights%2C%20thanks%20to%20the%20invertibility%20of%20the%0Agenerative%20model.%20We%20evaluate%20our%20method%20in%20five%20domains%20--%20object%0Arearrangement%2C%20goal-oriented%20navigation%2C%20motion%20caption%20of%20human%20actions%2C%0Aautonomous%20driving%2C%20and%20real-world%20table-top%20manipulation.%20Our%20experimental%0Aresults%20demonstrate%20that%20via%20the%20pretrained%20generative%20model%2C%20we%20successfully%0Alearn%20novel%20concepts%20and%20generate%20agent%20plans%20or%20motion%20corresponding%20to%20these%0Aconcepts%20in%20%281%29%20unseen%20environments%20and%20%282%29%20in%20composition%20with%20training%0Aconcepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04987v2&entry.124074799=Read"},
{"title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought", "author": "Chengzu Li and Wenshan Wu and Huanyu Zhang and Yan Xia and Shaoguang Mao and Li Dong and Ivan Vuli\u0107 and Furu Wei", "abstract": "  Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.\n", "link": "http://arxiv.org/abs/2501.07542v1", "date": "2025-01-13", "relevancy": 2.2643, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagine%20while%20Reasoning%20in%20Space%3A%20Multimodal%20Visualization-of-Thought&body=Title%3A%20Imagine%20while%20Reasoning%20in%20Space%3A%20Multimodal%20Visualization-of-Thought%0AAuthor%3A%20Chengzu%20Li%20and%20Wenshan%20Wu%20and%20Huanyu%20Zhang%20and%20Yan%20Xia%20and%20Shaoguang%20Mao%20and%20Li%20Dong%20and%20Ivan%20Vuli%C4%87%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20prompting%20has%20proven%20highly%20effective%20for%20enhancing%0Acomplex%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20Yet%2C%20it%20struggles%20in%20complex%20spatial%20reasoning%20tasks.%0ANonetheless%2C%20human%20cognition%20extends%20beyond%20language%20alone%2C%20enabling%20the%0Aremarkable%20capability%20to%20think%20in%20both%20words%20and%20images.%20Inspired%20by%20this%0Amechanism%2C%20we%20propose%20a%20new%20reasoning%20paradigm%2C%20Multimodal%0AVisualization-of-Thought%20%28MVoT%29.%20It%20enables%20visual%20thinking%20in%20MLLMs%20by%0Agenerating%20image%20visualizations%20of%20their%20reasoning%20traces.%20To%20ensure%0Ahigh-quality%20visualization%2C%20we%20introduce%20token%20discrepancy%20loss%20into%0Aautoregressive%20MLLMs.%20This%20innovation%20significantly%20improves%20both%20visual%0Acoherence%20and%20fidelity.%20We%20validate%20this%20approach%20through%20several%20dynamic%0Aspatial%20reasoning%20tasks.%20Experimental%20results%20reveal%20that%20MVoT%20demonstrates%0Acompetitive%20performance%20across%20tasks.%20Moreover%2C%20it%20exhibits%20robust%20and%20reliable%0Aimprovements%20in%20the%20most%20challenging%20scenarios%20where%20CoT%20fails.%20Ultimately%2C%0AMVoT%20establishes%20new%20possibilities%20for%20complex%20reasoning%20tasks%20where%20visual%0Athinking%20can%20effectively%20complement%20verbal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagine%2520while%2520Reasoning%2520in%2520Space%253A%2520Multimodal%2520Visualization-of-Thought%26entry.906535625%3DChengzu%2520Li%2520and%2520Wenshan%2520Wu%2520and%2520Huanyu%2520Zhang%2520and%2520Yan%2520Xia%2520and%2520Shaoguang%2520Mao%2520and%2520Li%2520Dong%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520has%2520proven%2520highly%2520effective%2520for%2520enhancing%250Acomplex%2520reasoning%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Multimodal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529.%2520Yet%252C%2520it%2520struggles%2520in%2520complex%2520spatial%2520reasoning%2520tasks.%250ANonetheless%252C%2520human%2520cognition%2520extends%2520beyond%2520language%2520alone%252C%2520enabling%2520the%250Aremarkable%2520capability%2520to%2520think%2520in%2520both%2520words%2520and%2520images.%2520Inspired%2520by%2520this%250Amechanism%252C%2520we%2520propose%2520a%2520new%2520reasoning%2520paradigm%252C%2520Multimodal%250AVisualization-of-Thought%2520%2528MVoT%2529.%2520It%2520enables%2520visual%2520thinking%2520in%2520MLLMs%2520by%250Agenerating%2520image%2520visualizations%2520of%2520their%2520reasoning%2520traces.%2520To%2520ensure%250Ahigh-quality%2520visualization%252C%2520we%2520introduce%2520token%2520discrepancy%2520loss%2520into%250Aautoregressive%2520MLLMs.%2520This%2520innovation%2520significantly%2520improves%2520both%2520visual%250Acoherence%2520and%2520fidelity.%2520We%2520validate%2520this%2520approach%2520through%2520several%2520dynamic%250Aspatial%2520reasoning%2520tasks.%2520Experimental%2520results%2520reveal%2520that%2520MVoT%2520demonstrates%250Acompetitive%2520performance%2520across%2520tasks.%2520Moreover%252C%2520it%2520exhibits%2520robust%2520and%2520reliable%250Aimprovements%2520in%2520the%2520most%2520challenging%2520scenarios%2520where%2520CoT%2520fails.%2520Ultimately%252C%250AMVoT%2520establishes%2520new%2520possibilities%2520for%2520complex%2520reasoning%2520tasks%2520where%2520visual%250Athinking%2520can%2520effectively%2520complement%2520verbal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagine%20while%20Reasoning%20in%20Space%3A%20Multimodal%20Visualization-of-Thought&entry.906535625=Chengzu%20Li%20and%20Wenshan%20Wu%20and%20Huanyu%20Zhang%20and%20Yan%20Xia%20and%20Shaoguang%20Mao%20and%20Li%20Dong%20and%20Ivan%20Vuli%C4%87%20and%20Furu%20Wei&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20prompting%20has%20proven%20highly%20effective%20for%20enhancing%0Acomplex%20reasoning%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%0AModels%20%28MLLMs%29.%20Yet%2C%20it%20struggles%20in%20complex%20spatial%20reasoning%20tasks.%0ANonetheless%2C%20human%20cognition%20extends%20beyond%20language%20alone%2C%20enabling%20the%0Aremarkable%20capability%20to%20think%20in%20both%20words%20and%20images.%20Inspired%20by%20this%0Amechanism%2C%20we%20propose%20a%20new%20reasoning%20paradigm%2C%20Multimodal%0AVisualization-of-Thought%20%28MVoT%29.%20It%20enables%20visual%20thinking%20in%20MLLMs%20by%0Agenerating%20image%20visualizations%20of%20their%20reasoning%20traces.%20To%20ensure%0Ahigh-quality%20visualization%2C%20we%20introduce%20token%20discrepancy%20loss%20into%0Aautoregressive%20MLLMs.%20This%20innovation%20significantly%20improves%20both%20visual%0Acoherence%20and%20fidelity.%20We%20validate%20this%20approach%20through%20several%20dynamic%0Aspatial%20reasoning%20tasks.%20Experimental%20results%20reveal%20that%20MVoT%20demonstrates%0Acompetitive%20performance%20across%20tasks.%20Moreover%2C%20it%20exhibits%20robust%20and%20reliable%0Aimprovements%20in%20the%20most%20challenging%20scenarios%20where%20CoT%20fails.%20Ultimately%2C%0AMVoT%20establishes%20new%20possibilities%20for%20complex%20reasoning%20tasks%20where%20visual%0Athinking%20can%20effectively%20complement%20verbal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07542v1&entry.124074799=Read"},
{"title": "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems", "author": "Vinay Sharma and Olga Fink", "abstract": "  Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces.\n", "link": "http://arxiv.org/abs/2501.07373v1", "date": "2025-01-13", "relevancy": 2.2564, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5753}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynami-CAL%20GraphNet%3A%20A%20Physics-Informed%20Graph%20Neural%20Network%20Conserving%0A%20%20Linear%20and%20Angular%20Momentum%20for%20Dynamical%20Systems&body=Title%3A%20Dynami-CAL%20GraphNet%3A%20A%20Physics-Informed%20Graph%20Neural%20Network%20Conserving%0A%20%20Linear%20and%20Angular%20Momentum%20for%20Dynamical%20Systems%0AAuthor%3A%20Vinay%20Sharma%20and%20Olga%20Fink%0AAbstract%3A%20%20%20Accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20multi-body%20dynamical%0Asystems%20is%20essential%20for%20predicting%20behaviors%20and%20inferring%20physical%20properties%0Ain%20natural%20and%20engineered%20environments.%20Traditional%20physics-based%20models%20face%0Ascalability%20challenges%20and%20are%20computationally%20demanding%2C%20while%20data-driven%0Aapproaches%20like%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20lack%20physical%20consistency%2C%0Ainterpretability%2C%20and%20generalization.%20In%20this%20paper%2C%20we%20propose%20Dynami-CAL%0AGraphNet%2C%20a%20Physics-Informed%20Graph%20Neural%20Network%20that%20integrates%20the%20learning%0Acapabilities%20of%20GNNs%20with%20physics-based%20inductive%20biases%20to%20address%20these%0Alimitations.%20Dynami-CAL%20GraphNet%20enforces%20pairwise%20conservation%20of%20linear%20and%0Aangular%20momentum%20for%20interacting%20nodes%20using%20edge-local%20reference%20frames%20that%0Aare%20equivariant%20to%20rotational%20symmetries%2C%20invariant%20to%20translations%2C%20and%0Aequivariant%20to%20node%20permutations.%20This%20design%20ensures%20physically%20consistent%0Apredictions%20of%20node%20dynamics%20while%20offering%20interpretable%2C%20edge-wise%20linear%20and%0Aangular%20impulses%20resulting%20from%20pairwise%20interactions.%20Evaluated%20on%20a%203D%0Agranular%20system%20with%20inelastic%20collisions%2C%20Dynami-CAL%20GraphNet%20demonstrates%0Astable%20error%20accumulation%20over%20extended%20rollouts%2C%20effective%20extrapolations%20to%0Aunseen%20configurations%2C%20and%20robust%20handling%20of%20heterogeneous%20interactions%20and%0Aexternal%20forces.%20Dynami-CAL%20GraphNet%20offers%20significant%20advantages%20in%20fields%0Arequiring%20accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20complex%20multi-body%0Adynamical%20systems%2C%20such%20as%20robotics%2C%20aerospace%20engineering%2C%20and%20materials%0Ascience.%20By%20providing%20physically%20consistent%20and%20scalable%20predictions%20that%0Aadhere%20to%20fundamental%20conservation%20laws%2C%20it%20enables%20the%20inference%20of%20forces%20and%0Amoments%20while%20efficiently%20handling%20heterogeneous%20interactions%20and%20external%0Aforces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynami-CAL%2520GraphNet%253A%2520A%2520Physics-Informed%2520Graph%2520Neural%2520Network%2520Conserving%250A%2520%2520Linear%2520and%2520Angular%2520Momentum%2520for%2520Dynamical%2520Systems%26entry.906535625%3DVinay%2520Sharma%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520Accurate%252C%2520interpretable%252C%2520and%2520real-time%2520modeling%2520of%2520multi-body%2520dynamical%250Asystems%2520is%2520essential%2520for%2520predicting%2520behaviors%2520and%2520inferring%2520physical%2520properties%250Ain%2520natural%2520and%2520engineered%2520environments.%2520Traditional%2520physics-based%2520models%2520face%250Ascalability%2520challenges%2520and%2520are%2520computationally%2520demanding%252C%2520while%2520data-driven%250Aapproaches%2520like%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520often%2520lack%2520physical%2520consistency%252C%250Ainterpretability%252C%2520and%2520generalization.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Dynami-CAL%250AGraphNet%252C%2520a%2520Physics-Informed%2520Graph%2520Neural%2520Network%2520that%2520integrates%2520the%2520learning%250Acapabilities%2520of%2520GNNs%2520with%2520physics-based%2520inductive%2520biases%2520to%2520address%2520these%250Alimitations.%2520Dynami-CAL%2520GraphNet%2520enforces%2520pairwise%2520conservation%2520of%2520linear%2520and%250Aangular%2520momentum%2520for%2520interacting%2520nodes%2520using%2520edge-local%2520reference%2520frames%2520that%250Aare%2520equivariant%2520to%2520rotational%2520symmetries%252C%2520invariant%2520to%2520translations%252C%2520and%250Aequivariant%2520to%2520node%2520permutations.%2520This%2520design%2520ensures%2520physically%2520consistent%250Apredictions%2520of%2520node%2520dynamics%2520while%2520offering%2520interpretable%252C%2520edge-wise%2520linear%2520and%250Aangular%2520impulses%2520resulting%2520from%2520pairwise%2520interactions.%2520Evaluated%2520on%2520a%25203D%250Agranular%2520system%2520with%2520inelastic%2520collisions%252C%2520Dynami-CAL%2520GraphNet%2520demonstrates%250Astable%2520error%2520accumulation%2520over%2520extended%2520rollouts%252C%2520effective%2520extrapolations%2520to%250Aunseen%2520configurations%252C%2520and%2520robust%2520handling%2520of%2520heterogeneous%2520interactions%2520and%250Aexternal%2520forces.%2520Dynami-CAL%2520GraphNet%2520offers%2520significant%2520advantages%2520in%2520fields%250Arequiring%2520accurate%252C%2520interpretable%252C%2520and%2520real-time%2520modeling%2520of%2520complex%2520multi-body%250Adynamical%2520systems%252C%2520such%2520as%2520robotics%252C%2520aerospace%2520engineering%252C%2520and%2520materials%250Ascience.%2520By%2520providing%2520physically%2520consistent%2520and%2520scalable%2520predictions%2520that%250Aadhere%2520to%2520fundamental%2520conservation%2520laws%252C%2520it%2520enables%2520the%2520inference%2520of%2520forces%2520and%250Amoments%2520while%2520efficiently%2520handling%2520heterogeneous%2520interactions%2520and%2520external%250Aforces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynami-CAL%20GraphNet%3A%20A%20Physics-Informed%20Graph%20Neural%20Network%20Conserving%0A%20%20Linear%20and%20Angular%20Momentum%20for%20Dynamical%20Systems&entry.906535625=Vinay%20Sharma%20and%20Olga%20Fink&entry.1292438233=%20%20Accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20multi-body%20dynamical%0Asystems%20is%20essential%20for%20predicting%20behaviors%20and%20inferring%20physical%20properties%0Ain%20natural%20and%20engineered%20environments.%20Traditional%20physics-based%20models%20face%0Ascalability%20challenges%20and%20are%20computationally%20demanding%2C%20while%20data-driven%0Aapproaches%20like%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20lack%20physical%20consistency%2C%0Ainterpretability%2C%20and%20generalization.%20In%20this%20paper%2C%20we%20propose%20Dynami-CAL%0AGraphNet%2C%20a%20Physics-Informed%20Graph%20Neural%20Network%20that%20integrates%20the%20learning%0Acapabilities%20of%20GNNs%20with%20physics-based%20inductive%20biases%20to%20address%20these%0Alimitations.%20Dynami-CAL%20GraphNet%20enforces%20pairwise%20conservation%20of%20linear%20and%0Aangular%20momentum%20for%20interacting%20nodes%20using%20edge-local%20reference%20frames%20that%0Aare%20equivariant%20to%20rotational%20symmetries%2C%20invariant%20to%20translations%2C%20and%0Aequivariant%20to%20node%20permutations.%20This%20design%20ensures%20physically%20consistent%0Apredictions%20of%20node%20dynamics%20while%20offering%20interpretable%2C%20edge-wise%20linear%20and%0Aangular%20impulses%20resulting%20from%20pairwise%20interactions.%20Evaluated%20on%20a%203D%0Agranular%20system%20with%20inelastic%20collisions%2C%20Dynami-CAL%20GraphNet%20demonstrates%0Astable%20error%20accumulation%20over%20extended%20rollouts%2C%20effective%20extrapolations%20to%0Aunseen%20configurations%2C%20and%20robust%20handling%20of%20heterogeneous%20interactions%20and%0Aexternal%20forces.%20Dynami-CAL%20GraphNet%20offers%20significant%20advantages%20in%20fields%0Arequiring%20accurate%2C%20interpretable%2C%20and%20real-time%20modeling%20of%20complex%20multi-body%0Adynamical%20systems%2C%20such%20as%20robotics%2C%20aerospace%20engineering%2C%20and%20materials%0Ascience.%20By%20providing%20physically%20consistent%20and%20scalable%20predictions%20that%0Aadhere%20to%20fundamental%20conservation%20laws%2C%20it%20enables%20the%20inference%20of%20forces%20and%0Amoments%20while%20efficiently%20handling%20heterogeneous%20interactions%20and%20external%0Aforces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07373v1&entry.124074799=Read"},
{"title": "Event-based Video Person Re-identification via Cross-Modality and\n  Temporal Collaboration", "author": "Renkai Li and Xin Yuan and Wei Liu and Xin Xu", "abstract": "  Video-based person re-identification (ReID) has become increasingly important\ndue to its applications in video surveillance applications. By employing events\nin video-based person ReID, more motion information can be provided between\ncontinuous frames to improve recognition accuracy. Previous approaches have\nassisted by introducing event data into the video person ReID task, but they\nstill cannot avoid the privacy leakage problem caused by RGB images. In order\nto avoid privacy attacks and to take advantage of the benefits of event data,\nwe consider using only event data. To make full use of the information in the\nevent stream, we propose a Cross-Modality and Temporal Collaboration (CMTC)\nnetwork for event-based video person ReID. First, we design an event transform\nnetwork to obtain corresponding auxiliary information from the input of raw\nevents. Additionally, we propose a differential modality collaboration module\nto balance the roles of events and auxiliaries to achieve complementary\neffects. Furthermore, we introduce a temporal collaboration module to exploit\nmotion information and appearance cues. Experimental results demonstrate that\nour method outperforms others in the task of event-based video person ReID.\n", "link": "http://arxiv.org/abs/2501.07296v1", "date": "2025-01-13", "relevancy": 2.2497, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6053}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-based%20Video%20Person%20Re-identification%20via%20Cross-Modality%20and%0A%20%20Temporal%20Collaboration&body=Title%3A%20Event-based%20Video%20Person%20Re-identification%20via%20Cross-Modality%20and%0A%20%20Temporal%20Collaboration%0AAuthor%3A%20Renkai%20Li%20and%20Xin%20Yuan%20and%20Wei%20Liu%20and%20Xin%20Xu%0AAbstract%3A%20%20%20Video-based%20person%20re-identification%20%28ReID%29%20has%20become%20increasingly%20important%0Adue%20to%20its%20applications%20in%20video%20surveillance%20applications.%20By%20employing%20events%0Ain%20video-based%20person%20ReID%2C%20more%20motion%20information%20can%20be%20provided%20between%0Acontinuous%20frames%20to%20improve%20recognition%20accuracy.%20Previous%20approaches%20have%0Aassisted%20by%20introducing%20event%20data%20into%20the%20video%20person%20ReID%20task%2C%20but%20they%0Astill%20cannot%20avoid%20the%20privacy%20leakage%20problem%20caused%20by%20RGB%20images.%20In%20order%0Ato%20avoid%20privacy%20attacks%20and%20to%20take%20advantage%20of%20the%20benefits%20of%20event%20data%2C%0Awe%20consider%20using%20only%20event%20data.%20To%20make%20full%20use%20of%20the%20information%20in%20the%0Aevent%20stream%2C%20we%20propose%20a%20Cross-Modality%20and%20Temporal%20Collaboration%20%28CMTC%29%0Anetwork%20for%20event-based%20video%20person%20ReID.%20First%2C%20we%20design%20an%20event%20transform%0Anetwork%20to%20obtain%20corresponding%20auxiliary%20information%20from%20the%20input%20of%20raw%0Aevents.%20Additionally%2C%20we%20propose%20a%20differential%20modality%20collaboration%20module%0Ato%20balance%20the%20roles%20of%20events%20and%20auxiliaries%20to%20achieve%20complementary%0Aeffects.%20Furthermore%2C%20we%20introduce%20a%20temporal%20collaboration%20module%20to%20exploit%0Amotion%20information%20and%20appearance%20cues.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20outperforms%20others%20in%20the%20task%20of%20event-based%20video%20person%20ReID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-based%2520Video%2520Person%2520Re-identification%2520via%2520Cross-Modality%2520and%250A%2520%2520Temporal%2520Collaboration%26entry.906535625%3DRenkai%2520Li%2520and%2520Xin%2520Yuan%2520and%2520Wei%2520Liu%2520and%2520Xin%2520Xu%26entry.1292438233%3D%2520%2520Video-based%2520person%2520re-identification%2520%2528ReID%2529%2520has%2520become%2520increasingly%2520important%250Adue%2520to%2520its%2520applications%2520in%2520video%2520surveillance%2520applications.%2520By%2520employing%2520events%250Ain%2520video-based%2520person%2520ReID%252C%2520more%2520motion%2520information%2520can%2520be%2520provided%2520between%250Acontinuous%2520frames%2520to%2520improve%2520recognition%2520accuracy.%2520Previous%2520approaches%2520have%250Aassisted%2520by%2520introducing%2520event%2520data%2520into%2520the%2520video%2520person%2520ReID%2520task%252C%2520but%2520they%250Astill%2520cannot%2520avoid%2520the%2520privacy%2520leakage%2520problem%2520caused%2520by%2520RGB%2520images.%2520In%2520order%250Ato%2520avoid%2520privacy%2520attacks%2520and%2520to%2520take%2520advantage%2520of%2520the%2520benefits%2520of%2520event%2520data%252C%250Awe%2520consider%2520using%2520only%2520event%2520data.%2520To%2520make%2520full%2520use%2520of%2520the%2520information%2520in%2520the%250Aevent%2520stream%252C%2520we%2520propose%2520a%2520Cross-Modality%2520and%2520Temporal%2520Collaboration%2520%2528CMTC%2529%250Anetwork%2520for%2520event-based%2520video%2520person%2520ReID.%2520First%252C%2520we%2520design%2520an%2520event%2520transform%250Anetwork%2520to%2520obtain%2520corresponding%2520auxiliary%2520information%2520from%2520the%2520input%2520of%2520raw%250Aevents.%2520Additionally%252C%2520we%2520propose%2520a%2520differential%2520modality%2520collaboration%2520module%250Ato%2520balance%2520the%2520roles%2520of%2520events%2520and%2520auxiliaries%2520to%2520achieve%2520complementary%250Aeffects.%2520Furthermore%252C%2520we%2520introduce%2520a%2520temporal%2520collaboration%2520module%2520to%2520exploit%250Amotion%2520information%2520and%2520appearance%2520cues.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520method%2520outperforms%2520others%2520in%2520the%2520task%2520of%2520event-based%2520video%2520person%2520ReID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-based%20Video%20Person%20Re-identification%20via%20Cross-Modality%20and%0A%20%20Temporal%20Collaboration&entry.906535625=Renkai%20Li%20and%20Xin%20Yuan%20and%20Wei%20Liu%20and%20Xin%20Xu&entry.1292438233=%20%20Video-based%20person%20re-identification%20%28ReID%29%20has%20become%20increasingly%20important%0Adue%20to%20its%20applications%20in%20video%20surveillance%20applications.%20By%20employing%20events%0Ain%20video-based%20person%20ReID%2C%20more%20motion%20information%20can%20be%20provided%20between%0Acontinuous%20frames%20to%20improve%20recognition%20accuracy.%20Previous%20approaches%20have%0Aassisted%20by%20introducing%20event%20data%20into%20the%20video%20person%20ReID%20task%2C%20but%20they%0Astill%20cannot%20avoid%20the%20privacy%20leakage%20problem%20caused%20by%20RGB%20images.%20In%20order%0Ato%20avoid%20privacy%20attacks%20and%20to%20take%20advantage%20of%20the%20benefits%20of%20event%20data%2C%0Awe%20consider%20using%20only%20event%20data.%20To%20make%20full%20use%20of%20the%20information%20in%20the%0Aevent%20stream%2C%20we%20propose%20a%20Cross-Modality%20and%20Temporal%20Collaboration%20%28CMTC%29%0Anetwork%20for%20event-based%20video%20person%20ReID.%20First%2C%20we%20design%20an%20event%20transform%0Anetwork%20to%20obtain%20corresponding%20auxiliary%20information%20from%20the%20input%20of%20raw%0Aevents.%20Additionally%2C%20we%20propose%20a%20differential%20modality%20collaboration%20module%0Ato%20balance%20the%20roles%20of%20events%20and%20auxiliaries%20to%20achieve%20complementary%0Aeffects.%20Furthermore%2C%20we%20introduce%20a%20temporal%20collaboration%20module%20to%20exploit%0Amotion%20information%20and%20appearance%20cues.%20Experimental%20results%20demonstrate%20that%0Aour%20method%20outperforms%20others%20in%20the%20task%20of%20event-based%20video%20person%20ReID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07296v1&entry.124074799=Read"},
{"title": "Empirical Comparison of Four Stereoscopic Depth Sensing Cameras for\n  Robotics Applications", "author": "Lukas Rustler and Vojtech Volprecht and Matej Hoffmann", "abstract": "  Depth sensing is an essential technology in robotics and many other fields.\nMany depth sensing (or RGB-D) cameras are available on the market and selecting\nthe best one for your application can be challenging. In this work, we tested\nfour stereoscopic RGB-D cameras that sense the distance by using two images\nfrom slightly different views. We empirically compared four cameras (Intel\nRealSense D435, Intel RealSense D455, StereoLabs ZED 2, and Luxonis OAK-D Pro)\nin three scenarios: (i) planar surface perception, (ii) plastic doll\nperception, (iii) household object perception (YCB dataset). We recorded and\nevaluated more than 3,000 RGB-D frames for each camera. For table-top robotics\nscenarios with distance to objects up to one meter, the best performance is\nprovided by the D435 camera. For longer distances, the other three models\nperform better, making them more suitable for some mobile robotics\napplications. OAK-D Pro additionally offers integrated AI modules (e.g., object\nand human keypoint detection). ZED 2 is not a standalone device and requires a\ncomputer with a GPU for depth data acquisition. All data (more than 12,000\nRGB-D frames) are publicly available at https://osf.io/f2seb.\n", "link": "http://arxiv.org/abs/2501.07421v1", "date": "2025-01-13", "relevancy": 2.2474, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Comparison%20of%20Four%20Stereoscopic%20Depth%20Sensing%20Cameras%20for%0A%20%20Robotics%20Applications&body=Title%3A%20Empirical%20Comparison%20of%20Four%20Stereoscopic%20Depth%20Sensing%20Cameras%20for%0A%20%20Robotics%20Applications%0AAuthor%3A%20Lukas%20Rustler%20and%20Vojtech%20Volprecht%20and%20Matej%20Hoffmann%0AAbstract%3A%20%20%20Depth%20sensing%20is%20an%20essential%20technology%20in%20robotics%20and%20many%20other%20fields.%0AMany%20depth%20sensing%20%28or%20RGB-D%29%20cameras%20are%20available%20on%20the%20market%20and%20selecting%0Athe%20best%20one%20for%20your%20application%20can%20be%20challenging.%20In%20this%20work%2C%20we%20tested%0Afour%20stereoscopic%20RGB-D%20cameras%20that%20sense%20the%20distance%20by%20using%20two%20images%0Afrom%20slightly%20different%20views.%20We%20empirically%20compared%20four%20cameras%20%28Intel%0ARealSense%20D435%2C%20Intel%20RealSense%20D455%2C%20StereoLabs%20ZED%202%2C%20and%20Luxonis%20OAK-D%20Pro%29%0Ain%20three%20scenarios%3A%20%28i%29%20planar%20surface%20perception%2C%20%28ii%29%20plastic%20doll%0Aperception%2C%20%28iii%29%20household%20object%20perception%20%28YCB%20dataset%29.%20We%20recorded%20and%0Aevaluated%20more%20than%203%2C000%20RGB-D%20frames%20for%20each%20camera.%20For%20table-top%20robotics%0Ascenarios%20with%20distance%20to%20objects%20up%20to%20one%20meter%2C%20the%20best%20performance%20is%0Aprovided%20by%20the%20D435%20camera.%20For%20longer%20distances%2C%20the%20other%20three%20models%0Aperform%20better%2C%20making%20them%20more%20suitable%20for%20some%20mobile%20robotics%0Aapplications.%20OAK-D%20Pro%20additionally%20offers%20integrated%20AI%20modules%20%28e.g.%2C%20object%0Aand%20human%20keypoint%20detection%29.%20ZED%202%20is%20not%20a%20standalone%20device%20and%20requires%20a%0Acomputer%20with%20a%20GPU%20for%20depth%20data%20acquisition.%20All%20data%20%28more%20than%2012%2C000%0ARGB-D%20frames%29%20are%20publicly%20available%20at%20https%3A//osf.io/f2seb.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Comparison%2520of%2520Four%2520Stereoscopic%2520Depth%2520Sensing%2520Cameras%2520for%250A%2520%2520Robotics%2520Applications%26entry.906535625%3DLukas%2520Rustler%2520and%2520Vojtech%2520Volprecht%2520and%2520Matej%2520Hoffmann%26entry.1292438233%3D%2520%2520Depth%2520sensing%2520is%2520an%2520essential%2520technology%2520in%2520robotics%2520and%2520many%2520other%2520fields.%250AMany%2520depth%2520sensing%2520%2528or%2520RGB-D%2529%2520cameras%2520are%2520available%2520on%2520the%2520market%2520and%2520selecting%250Athe%2520best%2520one%2520for%2520your%2520application%2520can%2520be%2520challenging.%2520In%2520this%2520work%252C%2520we%2520tested%250Afour%2520stereoscopic%2520RGB-D%2520cameras%2520that%2520sense%2520the%2520distance%2520by%2520using%2520two%2520images%250Afrom%2520slightly%2520different%2520views.%2520We%2520empirically%2520compared%2520four%2520cameras%2520%2528Intel%250ARealSense%2520D435%252C%2520Intel%2520RealSense%2520D455%252C%2520StereoLabs%2520ZED%25202%252C%2520and%2520Luxonis%2520OAK-D%2520Pro%2529%250Ain%2520three%2520scenarios%253A%2520%2528i%2529%2520planar%2520surface%2520perception%252C%2520%2528ii%2529%2520plastic%2520doll%250Aperception%252C%2520%2528iii%2529%2520household%2520object%2520perception%2520%2528YCB%2520dataset%2529.%2520We%2520recorded%2520and%250Aevaluated%2520more%2520than%25203%252C000%2520RGB-D%2520frames%2520for%2520each%2520camera.%2520For%2520table-top%2520robotics%250Ascenarios%2520with%2520distance%2520to%2520objects%2520up%2520to%2520one%2520meter%252C%2520the%2520best%2520performance%2520is%250Aprovided%2520by%2520the%2520D435%2520camera.%2520For%2520longer%2520distances%252C%2520the%2520other%2520three%2520models%250Aperform%2520better%252C%2520making%2520them%2520more%2520suitable%2520for%2520some%2520mobile%2520robotics%250Aapplications.%2520OAK-D%2520Pro%2520additionally%2520offers%2520integrated%2520AI%2520modules%2520%2528e.g.%252C%2520object%250Aand%2520human%2520keypoint%2520detection%2529.%2520ZED%25202%2520is%2520not%2520a%2520standalone%2520device%2520and%2520requires%2520a%250Acomputer%2520with%2520a%2520GPU%2520for%2520depth%2520data%2520acquisition.%2520All%2520data%2520%2528more%2520than%252012%252C000%250ARGB-D%2520frames%2529%2520are%2520publicly%2520available%2520at%2520https%253A//osf.io/f2seb.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Comparison%20of%20Four%20Stereoscopic%20Depth%20Sensing%20Cameras%20for%0A%20%20Robotics%20Applications&entry.906535625=Lukas%20Rustler%20and%20Vojtech%20Volprecht%20and%20Matej%20Hoffmann&entry.1292438233=%20%20Depth%20sensing%20is%20an%20essential%20technology%20in%20robotics%20and%20many%20other%20fields.%0AMany%20depth%20sensing%20%28or%20RGB-D%29%20cameras%20are%20available%20on%20the%20market%20and%20selecting%0Athe%20best%20one%20for%20your%20application%20can%20be%20challenging.%20In%20this%20work%2C%20we%20tested%0Afour%20stereoscopic%20RGB-D%20cameras%20that%20sense%20the%20distance%20by%20using%20two%20images%0Afrom%20slightly%20different%20views.%20We%20empirically%20compared%20four%20cameras%20%28Intel%0ARealSense%20D435%2C%20Intel%20RealSense%20D455%2C%20StereoLabs%20ZED%202%2C%20and%20Luxonis%20OAK-D%20Pro%29%0Ain%20three%20scenarios%3A%20%28i%29%20planar%20surface%20perception%2C%20%28ii%29%20plastic%20doll%0Aperception%2C%20%28iii%29%20household%20object%20perception%20%28YCB%20dataset%29.%20We%20recorded%20and%0Aevaluated%20more%20than%203%2C000%20RGB-D%20frames%20for%20each%20camera.%20For%20table-top%20robotics%0Ascenarios%20with%20distance%20to%20objects%20up%20to%20one%20meter%2C%20the%20best%20performance%20is%0Aprovided%20by%20the%20D435%20camera.%20For%20longer%20distances%2C%20the%20other%20three%20models%0Aperform%20better%2C%20making%20them%20more%20suitable%20for%20some%20mobile%20robotics%0Aapplications.%20OAK-D%20Pro%20additionally%20offers%20integrated%20AI%20modules%20%28e.g.%2C%20object%0Aand%20human%20keypoint%20detection%29.%20ZED%202%20is%20not%20a%20standalone%20device%20and%20requires%20a%0Acomputer%20with%20a%20GPU%20for%20depth%20data%20acquisition.%20All%20data%20%28more%20than%2012%2C000%0ARGB-D%20frames%29%20are%20publicly%20available%20at%20https%3A//osf.io/f2seb.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07421v1&entry.124074799=Read"},
{"title": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano\n  Transcription", "author": "Hounsu Kim and Taegyun Kwon and Juhan Nam", "abstract": "  Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm.\n", "link": "http://arxiv.org/abs/2501.05068v2", "date": "2025-01-13", "relevancy": 2.2448, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5686}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5562}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D3RM%3A%20A%20Discrete%20Denoising%20Diffusion%20Refinement%20Model%20for%20Piano%0A%20%20Transcription&body=Title%3A%20D3RM%3A%20A%20Discrete%20Denoising%20Diffusion%20Refinement%20Model%20for%20Piano%0A%20%20Transcription%0AAuthor%3A%20Hounsu%20Kim%20and%20Taegyun%20Kwon%20and%20Juhan%20Nam%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20widely%20used%20in%20the%20generative%20domain%20due%20to%20their%0Aconvincing%20performance%20in%20modeling%20complex%20data%20distributions.%20Moreover%2C%20they%0Ahave%20shown%20competitive%20results%20on%20discriminative%20tasks%2C%20such%20as%20image%0Asegmentation.%20While%20diffusion%20models%20have%20also%20been%20explored%20for%20automatic%0Amusic%20transcription%2C%20their%20performance%20has%20yet%20to%20reach%20a%20competitive%20level.%20In%0Athis%20paper%2C%20we%20focus%20on%20discrete%20diffusion%20model%27s%20refinement%20capabilities%20and%0Apresent%20a%20novel%20architecture%20for%20piano%20transcription.%20Our%20model%20utilizes%0ANeighborhood%20Attention%20layers%20as%20the%20denoising%20module%2C%20gradually%20predicting%20the%0Atarget%20high-resolution%20piano%20roll%2C%20conditioned%20on%20the%20finetuned%20features%20of%20a%0Apretrained%20acoustic%20model.%20To%20further%20enhance%20refinement%2C%20we%20devise%20a%20novel%0Astrategy%20which%20applies%20distinct%20transition%20states%20during%20training%20and%20inference%0Astage%20of%20discrete%20diffusion%20models.%20Experiments%20on%20the%20MAESTRO%20dataset%20show%0Athat%20our%20approach%20outperforms%20previous%20diffusion-based%20piano%20transcription%0Amodels%20and%20the%20baseline%20model%20in%20terms%20of%20F1%20score.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/hanshounsu/d3rm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05068v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD3RM%253A%2520A%2520Discrete%2520Denoising%2520Diffusion%2520Refinement%2520Model%2520for%2520Piano%250A%2520%2520Transcription%26entry.906535625%3DHounsu%2520Kim%2520and%2520Taegyun%2520Kwon%2520and%2520Juhan%2520Nam%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520been%2520widely%2520used%2520in%2520the%2520generative%2520domain%2520due%2520to%2520their%250Aconvincing%2520performance%2520in%2520modeling%2520complex%2520data%2520distributions.%2520Moreover%252C%2520they%250Ahave%2520shown%2520competitive%2520results%2520on%2520discriminative%2520tasks%252C%2520such%2520as%2520image%250Asegmentation.%2520While%2520diffusion%2520models%2520have%2520also%2520been%2520explored%2520for%2520automatic%250Amusic%2520transcription%252C%2520their%2520performance%2520has%2520yet%2520to%2520reach%2520a%2520competitive%2520level.%2520In%250Athis%2520paper%252C%2520we%2520focus%2520on%2520discrete%2520diffusion%2520model%2527s%2520refinement%2520capabilities%2520and%250Apresent%2520a%2520novel%2520architecture%2520for%2520piano%2520transcription.%2520Our%2520model%2520utilizes%250ANeighborhood%2520Attention%2520layers%2520as%2520the%2520denoising%2520module%252C%2520gradually%2520predicting%2520the%250Atarget%2520high-resolution%2520piano%2520roll%252C%2520conditioned%2520on%2520the%2520finetuned%2520features%2520of%2520a%250Apretrained%2520acoustic%2520model.%2520To%2520further%2520enhance%2520refinement%252C%2520we%2520devise%2520a%2520novel%250Astrategy%2520which%2520applies%2520distinct%2520transition%2520states%2520during%2520training%2520and%2520inference%250Astage%2520of%2520discrete%2520diffusion%2520models.%2520Experiments%2520on%2520the%2520MAESTRO%2520dataset%2520show%250Athat%2520our%2520approach%2520outperforms%2520previous%2520diffusion-based%2520piano%2520transcription%250Amodels%2520and%2520the%2520baseline%2520model%2520in%2520terms%2520of%2520F1%2520score.%2520Our%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/hanshounsu/d3rm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05068v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D3RM%3A%20A%20Discrete%20Denoising%20Diffusion%20Refinement%20Model%20for%20Piano%0A%20%20Transcription&entry.906535625=Hounsu%20Kim%20and%20Taegyun%20Kwon%20and%20Juhan%20Nam&entry.1292438233=%20%20Diffusion%20models%20have%20been%20widely%20used%20in%20the%20generative%20domain%20due%20to%20their%0Aconvincing%20performance%20in%20modeling%20complex%20data%20distributions.%20Moreover%2C%20they%0Ahave%20shown%20competitive%20results%20on%20discriminative%20tasks%2C%20such%20as%20image%0Asegmentation.%20While%20diffusion%20models%20have%20also%20been%20explored%20for%20automatic%0Amusic%20transcription%2C%20their%20performance%20has%20yet%20to%20reach%20a%20competitive%20level.%20In%0Athis%20paper%2C%20we%20focus%20on%20discrete%20diffusion%20model%27s%20refinement%20capabilities%20and%0Apresent%20a%20novel%20architecture%20for%20piano%20transcription.%20Our%20model%20utilizes%0ANeighborhood%20Attention%20layers%20as%20the%20denoising%20module%2C%20gradually%20predicting%20the%0Atarget%20high-resolution%20piano%20roll%2C%20conditioned%20on%20the%20finetuned%20features%20of%20a%0Apretrained%20acoustic%20model.%20To%20further%20enhance%20refinement%2C%20we%20devise%20a%20novel%0Astrategy%20which%20applies%20distinct%20transition%20states%20during%20training%20and%20inference%0Astage%20of%20discrete%20diffusion%20models.%20Experiments%20on%20the%20MAESTRO%20dataset%20show%0Athat%20our%20approach%20outperforms%20previous%20diffusion-based%20piano%20transcription%0Amodels%20and%20the%20baseline%20model%20in%20terms%20of%20F1%20score.%20Our%20code%20is%20available%20in%0Ahttps%3A//github.com/hanshounsu/d3rm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05068v2&entry.124074799=Read"},
{"title": "An Adaptive Sliding Window Estimator for Positioning of Unmanned Aerial\n  Vehicle Using a Single Anchor", "author": "Kaiwen Xiong and Sijia Chen and Wei Dong", "abstract": "  Localization using a single range anchor combined with onboard\noptical-inertial odometry offers a lightweight solution that provides\nmultidimensional measurements for the positioning of unmanned aerial vehicles.\nUnfortunately, the performance of such lightweight sensors varies with the\ndynamic environment, and the fidelity of the dynamic model is also severely\naffected by environmental aerial flow. To address this challenge, we propose an\nadaptive sliding window estimator equipped with an estimation reliability\nevaluator, where the states, noise covariance matrices and aerial drag are\nestimated simultaneously. The aerial drag effects are first evaluated based on\nposterior states and covariance. Then, an augmented Kalman filter is designed\nto pre-process multidimensional measurements and inherit historical\ninformation. Subsequently, an inverse-Wishart smoother is employed to estimate\nposterior states and covariance matrices. To further suppress potential\ndivergence, a reliability evaluator is devised to infer estimation errors. We\nfurther determine the fidelity of each sensor based on the error propagation.\nExtensive experiments are conducted in both standard and harsh environments,\ndemonstrating the adaptability and robustness of the proposed method. The root\nmean square error reaches 0.15 m, outperforming the state-of-the-art approach.\n", "link": "http://arxiv.org/abs/2409.06501v3", "date": "2025-01-13", "relevancy": 2.2393, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6017}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5652}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Sliding%20Window%20Estimator%20for%20Positioning%20of%20Unmanned%20Aerial%0A%20%20Vehicle%20Using%20a%20Single%20Anchor&body=Title%3A%20An%20Adaptive%20Sliding%20Window%20Estimator%20for%20Positioning%20of%20Unmanned%20Aerial%0A%20%20Vehicle%20Using%20a%20Single%20Anchor%0AAuthor%3A%20Kaiwen%20Xiong%20and%20Sijia%20Chen%20and%20Wei%20Dong%0AAbstract%3A%20%20%20Localization%20using%20a%20single%20range%20anchor%20combined%20with%20onboard%0Aoptical-inertial%20odometry%20offers%20a%20lightweight%20solution%20that%20provides%0Amultidimensional%20measurements%20for%20the%20positioning%20of%20unmanned%20aerial%20vehicles.%0AUnfortunately%2C%20the%20performance%20of%20such%20lightweight%20sensors%20varies%20with%20the%0Adynamic%20environment%2C%20and%20the%20fidelity%20of%20the%20dynamic%20model%20is%20also%20severely%0Aaffected%20by%20environmental%20aerial%20flow.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aadaptive%20sliding%20window%20estimator%20equipped%20with%20an%20estimation%20reliability%0Aevaluator%2C%20where%20the%20states%2C%20noise%20covariance%20matrices%20and%20aerial%20drag%20are%0Aestimated%20simultaneously.%20The%20aerial%20drag%20effects%20are%20first%20evaluated%20based%20on%0Aposterior%20states%20and%20covariance.%20Then%2C%20an%20augmented%20Kalman%20filter%20is%20designed%0Ato%20pre-process%20multidimensional%20measurements%20and%20inherit%20historical%0Ainformation.%20Subsequently%2C%20an%20inverse-Wishart%20smoother%20is%20employed%20to%20estimate%0Aposterior%20states%20and%20covariance%20matrices.%20To%20further%20suppress%20potential%0Adivergence%2C%20a%20reliability%20evaluator%20is%20devised%20to%20infer%20estimation%20errors.%20We%0Afurther%20determine%20the%20fidelity%20of%20each%20sensor%20based%20on%20the%20error%20propagation.%0AExtensive%20experiments%20are%20conducted%20in%20both%20standard%20and%20harsh%20environments%2C%0Ademonstrating%20the%20adaptability%20and%20robustness%20of%20the%20proposed%20method.%20The%20root%0Amean%20square%20error%20reaches%200.15%20m%2C%20outperforming%20the%20state-of-the-art%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.06501v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Sliding%2520Window%2520Estimator%2520for%2520Positioning%2520of%2520Unmanned%2520Aerial%250A%2520%2520Vehicle%2520Using%2520a%2520Single%2520Anchor%26entry.906535625%3DKaiwen%2520Xiong%2520and%2520Sijia%2520Chen%2520and%2520Wei%2520Dong%26entry.1292438233%3D%2520%2520Localization%2520using%2520a%2520single%2520range%2520anchor%2520combined%2520with%2520onboard%250Aoptical-inertial%2520odometry%2520offers%2520a%2520lightweight%2520solution%2520that%2520provides%250Amultidimensional%2520measurements%2520for%2520the%2520positioning%2520of%2520unmanned%2520aerial%2520vehicles.%250AUnfortunately%252C%2520the%2520performance%2520of%2520such%2520lightweight%2520sensors%2520varies%2520with%2520the%250Adynamic%2520environment%252C%2520and%2520the%2520fidelity%2520of%2520the%2520dynamic%2520model%2520is%2520also%2520severely%250Aaffected%2520by%2520environmental%2520aerial%2520flow.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%250Aadaptive%2520sliding%2520window%2520estimator%2520equipped%2520with%2520an%2520estimation%2520reliability%250Aevaluator%252C%2520where%2520the%2520states%252C%2520noise%2520covariance%2520matrices%2520and%2520aerial%2520drag%2520are%250Aestimated%2520simultaneously.%2520The%2520aerial%2520drag%2520effects%2520are%2520first%2520evaluated%2520based%2520on%250Aposterior%2520states%2520and%2520covariance.%2520Then%252C%2520an%2520augmented%2520Kalman%2520filter%2520is%2520designed%250Ato%2520pre-process%2520multidimensional%2520measurements%2520and%2520inherit%2520historical%250Ainformation.%2520Subsequently%252C%2520an%2520inverse-Wishart%2520smoother%2520is%2520employed%2520to%2520estimate%250Aposterior%2520states%2520and%2520covariance%2520matrices.%2520To%2520further%2520suppress%2520potential%250Adivergence%252C%2520a%2520reliability%2520evaluator%2520is%2520devised%2520to%2520infer%2520estimation%2520errors.%2520We%250Afurther%2520determine%2520the%2520fidelity%2520of%2520each%2520sensor%2520based%2520on%2520the%2520error%2520propagation.%250AExtensive%2520experiments%2520are%2520conducted%2520in%2520both%2520standard%2520and%2520harsh%2520environments%252C%250Ademonstrating%2520the%2520adaptability%2520and%2520robustness%2520of%2520the%2520proposed%2520method.%2520The%2520root%250Amean%2520square%2520error%2520reaches%25200.15%2520m%252C%2520outperforming%2520the%2520state-of-the-art%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.06501v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Sliding%20Window%20Estimator%20for%20Positioning%20of%20Unmanned%20Aerial%0A%20%20Vehicle%20Using%20a%20Single%20Anchor&entry.906535625=Kaiwen%20Xiong%20and%20Sijia%20Chen%20and%20Wei%20Dong&entry.1292438233=%20%20Localization%20using%20a%20single%20range%20anchor%20combined%20with%20onboard%0Aoptical-inertial%20odometry%20offers%20a%20lightweight%20solution%20that%20provides%0Amultidimensional%20measurements%20for%20the%20positioning%20of%20unmanned%20aerial%20vehicles.%0AUnfortunately%2C%20the%20performance%20of%20such%20lightweight%20sensors%20varies%20with%20the%0Adynamic%20environment%2C%20and%20the%20fidelity%20of%20the%20dynamic%20model%20is%20also%20severely%0Aaffected%20by%20environmental%20aerial%20flow.%20To%20address%20this%20challenge%2C%20we%20propose%20an%0Aadaptive%20sliding%20window%20estimator%20equipped%20with%20an%20estimation%20reliability%0Aevaluator%2C%20where%20the%20states%2C%20noise%20covariance%20matrices%20and%20aerial%20drag%20are%0Aestimated%20simultaneously.%20The%20aerial%20drag%20effects%20are%20first%20evaluated%20based%20on%0Aposterior%20states%20and%20covariance.%20Then%2C%20an%20augmented%20Kalman%20filter%20is%20designed%0Ato%20pre-process%20multidimensional%20measurements%20and%20inherit%20historical%0Ainformation.%20Subsequently%2C%20an%20inverse-Wishart%20smoother%20is%20employed%20to%20estimate%0Aposterior%20states%20and%20covariance%20matrices.%20To%20further%20suppress%20potential%0Adivergence%2C%20a%20reliability%20evaluator%20is%20devised%20to%20infer%20estimation%20errors.%20We%0Afurther%20determine%20the%20fidelity%20of%20each%20sensor%20based%20on%20the%20error%20propagation.%0AExtensive%20experiments%20are%20conducted%20in%20both%20standard%20and%20harsh%20environments%2C%0Ademonstrating%20the%20adaptability%20and%20robustness%20of%20the%20proposed%20method.%20The%20root%0Amean%20square%20error%20reaches%200.15%20m%2C%20outperforming%20the%20state-of-the-art%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.06501v3&entry.124074799=Read"},
{"title": "GestLLM: Advanced Hand Gesture Interpretation via Large Language Models\n  for Human-Robot Interaction", "author": "Oleg Kobzarev and Artem Lykov and Dzmitry Tsetserukou", "abstract": "  This paper introduces GestLLM, an advanced system for human-robot interaction\nthat enables intuitive robot control through hand gestures. Unlike conventional\nsystems, which rely on a limited set of predefined gestures, GestLLM leverages\nlarge language models and feature extraction via MediaPipe to interpret a\ndiverse range of gestures. This integration addresses key limitations in\nexisting systems, such as restricted gesture flexibility and the inability to\nrecognize complex or unconventional gestures commonly used in human\ncommunication.\n  By combining state-of-the-art feature extraction and language model\ncapabilities, GestLLM achieves performance comparable to leading\nvision-language models while supporting gestures underrepresented in\ntraditional datasets. For example, this includes gestures from popular culture,\nsuch as the ``Vulcan salute\" from Star Trek, without any additional\npretraining, prompt engineering, etc. This flexibility enhances the naturalness\nand inclusivity of robot control, making interactions more intuitive and\nuser-friendly.\n  GestLLM provides a significant step forward in gesture-based interaction,\nenabling robots to understand and respond to a wide variety of hand gestures\neffectively. This paper outlines its design, implementation, and evaluation,\ndemonstrating its potential applications in advanced human-robot collaboration,\nassistive robotics, and interactive entertainment.\n", "link": "http://arxiv.org/abs/2501.07295v1", "date": "2025-01-13", "relevancy": 2.23, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5743}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5622}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GestLLM%3A%20Advanced%20Hand%20Gesture%20Interpretation%20via%20Large%20Language%20Models%0A%20%20for%20Human-Robot%20Interaction&body=Title%3A%20GestLLM%3A%20Advanced%20Hand%20Gesture%20Interpretation%20via%20Large%20Language%20Models%0A%20%20for%20Human-Robot%20Interaction%0AAuthor%3A%20Oleg%20Kobzarev%20and%20Artem%20Lykov%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20This%20paper%20introduces%20GestLLM%2C%20an%20advanced%20system%20for%20human-robot%20interaction%0Athat%20enables%20intuitive%20robot%20control%20through%20hand%20gestures.%20Unlike%20conventional%0Asystems%2C%20which%20rely%20on%20a%20limited%20set%20of%20predefined%20gestures%2C%20GestLLM%20leverages%0Alarge%20language%20models%20and%20feature%20extraction%20via%20MediaPipe%20to%20interpret%20a%0Adiverse%20range%20of%20gestures.%20This%20integration%20addresses%20key%20limitations%20in%0Aexisting%20systems%2C%20such%20as%20restricted%20gesture%20flexibility%20and%20the%20inability%20to%0Arecognize%20complex%20or%20unconventional%20gestures%20commonly%20used%20in%20human%0Acommunication.%0A%20%20By%20combining%20state-of-the-art%20feature%20extraction%20and%20language%20model%0Acapabilities%2C%20GestLLM%20achieves%20performance%20comparable%20to%20leading%0Avision-language%20models%20while%20supporting%20gestures%20underrepresented%20in%0Atraditional%20datasets.%20For%20example%2C%20this%20includes%20gestures%20from%20popular%20culture%2C%0Asuch%20as%20the%20%60%60Vulcan%20salute%22%20from%20Star%20Trek%2C%20without%20any%20additional%0Apretraining%2C%20prompt%20engineering%2C%20etc.%20This%20flexibility%20enhances%20the%20naturalness%0Aand%20inclusivity%20of%20robot%20control%2C%20making%20interactions%20more%20intuitive%20and%0Auser-friendly.%0A%20%20GestLLM%20provides%20a%20significant%20step%20forward%20in%20gesture-based%20interaction%2C%0Aenabling%20robots%20to%20understand%20and%20respond%20to%20a%20wide%20variety%20of%20hand%20gestures%0Aeffectively.%20This%20paper%20outlines%20its%20design%2C%20implementation%2C%20and%20evaluation%2C%0Ademonstrating%20its%20potential%20applications%20in%20advanced%20human-robot%20collaboration%2C%0Aassistive%20robotics%2C%20and%20interactive%20entertainment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGestLLM%253A%2520Advanced%2520Hand%2520Gesture%2520Interpretation%2520via%2520Large%2520Language%2520Models%250A%2520%2520for%2520Human-Robot%2520Interaction%26entry.906535625%3DOleg%2520Kobzarev%2520and%2520Artem%2520Lykov%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520GestLLM%252C%2520an%2520advanced%2520system%2520for%2520human-robot%2520interaction%250Athat%2520enables%2520intuitive%2520robot%2520control%2520through%2520hand%2520gestures.%2520Unlike%2520conventional%250Asystems%252C%2520which%2520rely%2520on%2520a%2520limited%2520set%2520of%2520predefined%2520gestures%252C%2520GestLLM%2520leverages%250Alarge%2520language%2520models%2520and%2520feature%2520extraction%2520via%2520MediaPipe%2520to%2520interpret%2520a%250Adiverse%2520range%2520of%2520gestures.%2520This%2520integration%2520addresses%2520key%2520limitations%2520in%250Aexisting%2520systems%252C%2520such%2520as%2520restricted%2520gesture%2520flexibility%2520and%2520the%2520inability%2520to%250Arecognize%2520complex%2520or%2520unconventional%2520gestures%2520commonly%2520used%2520in%2520human%250Acommunication.%250A%2520%2520By%2520combining%2520state-of-the-art%2520feature%2520extraction%2520and%2520language%2520model%250Acapabilities%252C%2520GestLLM%2520achieves%2520performance%2520comparable%2520to%2520leading%250Avision-language%2520models%2520while%2520supporting%2520gestures%2520underrepresented%2520in%250Atraditional%2520datasets.%2520For%2520example%252C%2520this%2520includes%2520gestures%2520from%2520popular%2520culture%252C%250Asuch%2520as%2520the%2520%2560%2560Vulcan%2520salute%2522%2520from%2520Star%2520Trek%252C%2520without%2520any%2520additional%250Apretraining%252C%2520prompt%2520engineering%252C%2520etc.%2520This%2520flexibility%2520enhances%2520the%2520naturalness%250Aand%2520inclusivity%2520of%2520robot%2520control%252C%2520making%2520interactions%2520more%2520intuitive%2520and%250Auser-friendly.%250A%2520%2520GestLLM%2520provides%2520a%2520significant%2520step%2520forward%2520in%2520gesture-based%2520interaction%252C%250Aenabling%2520robots%2520to%2520understand%2520and%2520respond%2520to%2520a%2520wide%2520variety%2520of%2520hand%2520gestures%250Aeffectively.%2520This%2520paper%2520outlines%2520its%2520design%252C%2520implementation%252C%2520and%2520evaluation%252C%250Ademonstrating%2520its%2520potential%2520applications%2520in%2520advanced%2520human-robot%2520collaboration%252C%250Aassistive%2520robotics%252C%2520and%2520interactive%2520entertainment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GestLLM%3A%20Advanced%20Hand%20Gesture%20Interpretation%20via%20Large%20Language%20Models%0A%20%20for%20Human-Robot%20Interaction&entry.906535625=Oleg%20Kobzarev%20and%20Artem%20Lykov%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20This%20paper%20introduces%20GestLLM%2C%20an%20advanced%20system%20for%20human-robot%20interaction%0Athat%20enables%20intuitive%20robot%20control%20through%20hand%20gestures.%20Unlike%20conventional%0Asystems%2C%20which%20rely%20on%20a%20limited%20set%20of%20predefined%20gestures%2C%20GestLLM%20leverages%0Alarge%20language%20models%20and%20feature%20extraction%20via%20MediaPipe%20to%20interpret%20a%0Adiverse%20range%20of%20gestures.%20This%20integration%20addresses%20key%20limitations%20in%0Aexisting%20systems%2C%20such%20as%20restricted%20gesture%20flexibility%20and%20the%20inability%20to%0Arecognize%20complex%20or%20unconventional%20gestures%20commonly%20used%20in%20human%0Acommunication.%0A%20%20By%20combining%20state-of-the-art%20feature%20extraction%20and%20language%20model%0Acapabilities%2C%20GestLLM%20achieves%20performance%20comparable%20to%20leading%0Avision-language%20models%20while%20supporting%20gestures%20underrepresented%20in%0Atraditional%20datasets.%20For%20example%2C%20this%20includes%20gestures%20from%20popular%20culture%2C%0Asuch%20as%20the%20%60%60Vulcan%20salute%22%20from%20Star%20Trek%2C%20without%20any%20additional%0Apretraining%2C%20prompt%20engineering%2C%20etc.%20This%20flexibility%20enhances%20the%20naturalness%0Aand%20inclusivity%20of%20robot%20control%2C%20making%20interactions%20more%20intuitive%20and%0Auser-friendly.%0A%20%20GestLLM%20provides%20a%20significant%20step%20forward%20in%20gesture-based%20interaction%2C%0Aenabling%20robots%20to%20understand%20and%20respond%20to%20a%20wide%20variety%20of%20hand%20gestures%0Aeffectively.%20This%20paper%20outlines%20its%20design%2C%20implementation%2C%20and%20evaluation%2C%0Ademonstrating%20its%20potential%20applications%20in%20advanced%20human-robot%20collaboration%2C%0Aassistive%20robotics%2C%20and%20interactive%20entertainment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07295v1&entry.124074799=Read"},
{"title": "QUACK: Quantum Aligned Centroid Kernel", "author": "Kilian Tscharke and Sebastian Issel and Pascal Debus", "abstract": "  Quantum computing (QC) seems to show potential for application in machine\nlearning (ML). In particular quantum kernel methods (QKM) exhibit promising\nproperties for use in supervised ML tasks. However, a major disadvantage of\nkernel methods is their unfavorable quadratic scaling with the number of\ntraining samples. Together with the limits imposed by currently available\nquantum hardware (NISQ devices) with their low qubit coherence times, small\nnumber of qubits, and high error rates, the use of QC in ML at an industrially\nrelevant scale is currently impossible. As a small step in improving the\npotential applications of QKMs, we introduce QUACK, a quantum kernel algorithm\nwhose time complexity scales linear with the number of samples during training,\nand independent of the number of training samples in the inference stage. In\nthe training process, only the kernel entries for the samples and the centers\nof the classes are calculated, i.e. the maximum shape of the kernel for n\nsamples and c classes is (n, c). During training, the parameters of the quantum\nkernel and the positions of the centroids are optimized iteratively. In the\ninference stage, for every new sample the circuit is only evaluated for every\ncentroid, i.e. c times. We show that the QUACK algorithm nevertheless provides\nsatisfactory results and can perform at a similar level as classical kernel\nmethods with quadratic scaling during training. In addition, our (simulated)\nalgorithm is able to handle high-dimensional datasets such as MNIST with 784\nfeatures without any dimensionality reduction.\n", "link": "http://arxiv.org/abs/2405.00304v3", "date": "2025-01-13", "relevancy": 2.2229, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.436}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QUACK%3A%20Quantum%20Aligned%20Centroid%20Kernel&body=Title%3A%20QUACK%3A%20Quantum%20Aligned%20Centroid%20Kernel%0AAuthor%3A%20Kilian%20Tscharke%20and%20Sebastian%20Issel%20and%20Pascal%20Debus%0AAbstract%3A%20%20%20Quantum%20computing%20%28QC%29%20seems%20to%20show%20potential%20for%20application%20in%20machine%0Alearning%20%28ML%29.%20In%20particular%20quantum%20kernel%20methods%20%28QKM%29%20exhibit%20promising%0Aproperties%20for%20use%20in%20supervised%20ML%20tasks.%20However%2C%20a%20major%20disadvantage%20of%0Akernel%20methods%20is%20their%20unfavorable%20quadratic%20scaling%20with%20the%20number%20of%0Atraining%20samples.%20Together%20with%20the%20limits%20imposed%20by%20currently%20available%0Aquantum%20hardware%20%28NISQ%20devices%29%20with%20their%20low%20qubit%20coherence%20times%2C%20small%0Anumber%20of%20qubits%2C%20and%20high%20error%20rates%2C%20the%20use%20of%20QC%20in%20ML%20at%20an%20industrially%0Arelevant%20scale%20is%20currently%20impossible.%20As%20a%20small%20step%20in%20improving%20the%0Apotential%20applications%20of%20QKMs%2C%20we%20introduce%20QUACK%2C%20a%20quantum%20kernel%20algorithm%0Awhose%20time%20complexity%20scales%20linear%20with%20the%20number%20of%20samples%20during%20training%2C%0Aand%20independent%20of%20the%20number%20of%20training%20samples%20in%20the%20inference%20stage.%20In%0Athe%20training%20process%2C%20only%20the%20kernel%20entries%20for%20the%20samples%20and%20the%20centers%0Aof%20the%20classes%20are%20calculated%2C%20i.e.%20the%20maximum%20shape%20of%20the%20kernel%20for%20n%0Asamples%20and%20c%20classes%20is%20%28n%2C%20c%29.%20During%20training%2C%20the%20parameters%20of%20the%20quantum%0Akernel%20and%20the%20positions%20of%20the%20centroids%20are%20optimized%20iteratively.%20In%20the%0Ainference%20stage%2C%20for%20every%20new%20sample%20the%20circuit%20is%20only%20evaluated%20for%20every%0Acentroid%2C%20i.e.%20c%20times.%20We%20show%20that%20the%20QUACK%20algorithm%20nevertheless%20provides%0Asatisfactory%20results%20and%20can%20perform%20at%20a%20similar%20level%20as%20classical%20kernel%0Amethods%20with%20quadratic%20scaling%20during%20training.%20In%20addition%2C%20our%20%28simulated%29%0Aalgorithm%20is%20able%20to%20handle%20high-dimensional%20datasets%20such%20as%20MNIST%20with%20784%0Afeatures%20without%20any%20dimensionality%20reduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00304v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQUACK%253A%2520Quantum%2520Aligned%2520Centroid%2520Kernel%26entry.906535625%3DKilian%2520Tscharke%2520and%2520Sebastian%2520Issel%2520and%2520Pascal%2520Debus%26entry.1292438233%3D%2520%2520Quantum%2520computing%2520%2528QC%2529%2520seems%2520to%2520show%2520potential%2520for%2520application%2520in%2520machine%250Alearning%2520%2528ML%2529.%2520In%2520particular%2520quantum%2520kernel%2520methods%2520%2528QKM%2529%2520exhibit%2520promising%250Aproperties%2520for%2520use%2520in%2520supervised%2520ML%2520tasks.%2520However%252C%2520a%2520major%2520disadvantage%2520of%250Akernel%2520methods%2520is%2520their%2520unfavorable%2520quadratic%2520scaling%2520with%2520the%2520number%2520of%250Atraining%2520samples.%2520Together%2520with%2520the%2520limits%2520imposed%2520by%2520currently%2520available%250Aquantum%2520hardware%2520%2528NISQ%2520devices%2529%2520with%2520their%2520low%2520qubit%2520coherence%2520times%252C%2520small%250Anumber%2520of%2520qubits%252C%2520and%2520high%2520error%2520rates%252C%2520the%2520use%2520of%2520QC%2520in%2520ML%2520at%2520an%2520industrially%250Arelevant%2520scale%2520is%2520currently%2520impossible.%2520As%2520a%2520small%2520step%2520in%2520improving%2520the%250Apotential%2520applications%2520of%2520QKMs%252C%2520we%2520introduce%2520QUACK%252C%2520a%2520quantum%2520kernel%2520algorithm%250Awhose%2520time%2520complexity%2520scales%2520linear%2520with%2520the%2520number%2520of%2520samples%2520during%2520training%252C%250Aand%2520independent%2520of%2520the%2520number%2520of%2520training%2520samples%2520in%2520the%2520inference%2520stage.%2520In%250Athe%2520training%2520process%252C%2520only%2520the%2520kernel%2520entries%2520for%2520the%2520samples%2520and%2520the%2520centers%250Aof%2520the%2520classes%2520are%2520calculated%252C%2520i.e.%2520the%2520maximum%2520shape%2520of%2520the%2520kernel%2520for%2520n%250Asamples%2520and%2520c%2520classes%2520is%2520%2528n%252C%2520c%2529.%2520During%2520training%252C%2520the%2520parameters%2520of%2520the%2520quantum%250Akernel%2520and%2520the%2520positions%2520of%2520the%2520centroids%2520are%2520optimized%2520iteratively.%2520In%2520the%250Ainference%2520stage%252C%2520for%2520every%2520new%2520sample%2520the%2520circuit%2520is%2520only%2520evaluated%2520for%2520every%250Acentroid%252C%2520i.e.%2520c%2520times.%2520We%2520show%2520that%2520the%2520QUACK%2520algorithm%2520nevertheless%2520provides%250Asatisfactory%2520results%2520and%2520can%2520perform%2520at%2520a%2520similar%2520level%2520as%2520classical%2520kernel%250Amethods%2520with%2520quadratic%2520scaling%2520during%2520training.%2520In%2520addition%252C%2520our%2520%2528simulated%2529%250Aalgorithm%2520is%2520able%2520to%2520handle%2520high-dimensional%2520datasets%2520such%2520as%2520MNIST%2520with%2520784%250Afeatures%2520without%2520any%2520dimensionality%2520reduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00304v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QUACK%3A%20Quantum%20Aligned%20Centroid%20Kernel&entry.906535625=Kilian%20Tscharke%20and%20Sebastian%20Issel%20and%20Pascal%20Debus&entry.1292438233=%20%20Quantum%20computing%20%28QC%29%20seems%20to%20show%20potential%20for%20application%20in%20machine%0Alearning%20%28ML%29.%20In%20particular%20quantum%20kernel%20methods%20%28QKM%29%20exhibit%20promising%0Aproperties%20for%20use%20in%20supervised%20ML%20tasks.%20However%2C%20a%20major%20disadvantage%20of%0Akernel%20methods%20is%20their%20unfavorable%20quadratic%20scaling%20with%20the%20number%20of%0Atraining%20samples.%20Together%20with%20the%20limits%20imposed%20by%20currently%20available%0Aquantum%20hardware%20%28NISQ%20devices%29%20with%20their%20low%20qubit%20coherence%20times%2C%20small%0Anumber%20of%20qubits%2C%20and%20high%20error%20rates%2C%20the%20use%20of%20QC%20in%20ML%20at%20an%20industrially%0Arelevant%20scale%20is%20currently%20impossible.%20As%20a%20small%20step%20in%20improving%20the%0Apotential%20applications%20of%20QKMs%2C%20we%20introduce%20QUACK%2C%20a%20quantum%20kernel%20algorithm%0Awhose%20time%20complexity%20scales%20linear%20with%20the%20number%20of%20samples%20during%20training%2C%0Aand%20independent%20of%20the%20number%20of%20training%20samples%20in%20the%20inference%20stage.%20In%0Athe%20training%20process%2C%20only%20the%20kernel%20entries%20for%20the%20samples%20and%20the%20centers%0Aof%20the%20classes%20are%20calculated%2C%20i.e.%20the%20maximum%20shape%20of%20the%20kernel%20for%20n%0Asamples%20and%20c%20classes%20is%20%28n%2C%20c%29.%20During%20training%2C%20the%20parameters%20of%20the%20quantum%0Akernel%20and%20the%20positions%20of%20the%20centroids%20are%20optimized%20iteratively.%20In%20the%0Ainference%20stage%2C%20for%20every%20new%20sample%20the%20circuit%20is%20only%20evaluated%20for%20every%0Acentroid%2C%20i.e.%20c%20times.%20We%20show%20that%20the%20QUACK%20algorithm%20nevertheless%20provides%0Asatisfactory%20results%20and%20can%20perform%20at%20a%20similar%20level%20as%20classical%20kernel%0Amethods%20with%20quadratic%20scaling%20during%20training.%20In%20addition%2C%20our%20%28simulated%29%0Aalgorithm%20is%20able%20to%20handle%20high-dimensional%20datasets%20such%20as%20MNIST%20with%20784%0Afeatures%20without%20any%20dimensionality%20reduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00304v3&entry.124074799=Read"},
{"title": "Initial Findings on Sensor based Open Vocabulary Activity Recognition\n  via Text Embedding Inversion", "author": "Lala Shakti Swarup Ray and Bo Zhou and Sungho Suh and Paul Lukowicz", "abstract": "  Conventional human activity recognition (HAR) relies on classifiers trained\nto predict discrete activity classes, inherently limiting recognition to\nactivities explicitly present in the training set. Such classifiers would\ninvariably fail, putting zero likelihood, when encountering unseen activities.\nWe propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this\nlimitation by first converting each activity into natural language and breaking\nit into a sequence of elementary motions. This descriptive text is then encoded\ninto a fixed-size embedding. The model is trained to regress this embedding,\nwhich is subsequently decoded back into natural language using a pre-trained\nembedding inversion model. Unlike other works that rely on auto-regressive\nlarge language models (LLMs) at their core, OV-HAR achieves open vocabulary\nrecognition without the computational overhead of such models. The generated\ntext can be transformed into a single activity class using LLM prompt\nengineering. We have evaluated our approach on different modalities, including\nvision (pose), IMU, and pressure sensors, demonstrating robust generalization\nacross unseen activities and modalities, offering a fundamentally different\nparadigm from contemporary classifiers.\n", "link": "http://arxiv.org/abs/2501.07408v1", "date": "2025-01-13", "relevancy": 2.2127, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Initial%20Findings%20on%20Sensor%20based%20Open%20Vocabulary%20Activity%20Recognition%0A%20%20via%20Text%20Embedding%20Inversion&body=Title%3A%20Initial%20Findings%20on%20Sensor%20based%20Open%20Vocabulary%20Activity%20Recognition%0A%20%20via%20Text%20Embedding%20Inversion%0AAuthor%3A%20Lala%20Shakti%20Swarup%20Ray%20and%20Bo%20Zhou%20and%20Sungho%20Suh%20and%20Paul%20Lukowicz%0AAbstract%3A%20%20%20Conventional%20human%20activity%20recognition%20%28HAR%29%20relies%20on%20classifiers%20trained%0Ato%20predict%20discrete%20activity%20classes%2C%20inherently%20limiting%20recognition%20to%0Aactivities%20explicitly%20present%20in%20the%20training%20set.%20Such%20classifiers%20would%0Ainvariably%20fail%2C%20putting%20zero%20likelihood%2C%20when%20encountering%20unseen%20activities.%0AWe%20propose%20Open%20Vocabulary%20HAR%20%28OV-HAR%29%2C%20a%20framework%20that%20overcomes%20this%0Alimitation%20by%20first%20converting%20each%20activity%20into%20natural%20language%20and%20breaking%0Ait%20into%20a%20sequence%20of%20elementary%20motions.%20This%20descriptive%20text%20is%20then%20encoded%0Ainto%20a%20fixed-size%20embedding.%20The%20model%20is%20trained%20to%20regress%20this%20embedding%2C%0Awhich%20is%20subsequently%20decoded%20back%20into%20natural%20language%20using%20a%20pre-trained%0Aembedding%20inversion%20model.%20Unlike%20other%20works%20that%20rely%20on%20auto-regressive%0Alarge%20language%20models%20%28LLMs%29%20at%20their%20core%2C%20OV-HAR%20achieves%20open%20vocabulary%0Arecognition%20without%20the%20computational%20overhead%20of%20such%20models.%20The%20generated%0Atext%20can%20be%20transformed%20into%20a%20single%20activity%20class%20using%20LLM%20prompt%0Aengineering.%20We%20have%20evaluated%20our%20approach%20on%20different%20modalities%2C%20including%0Avision%20%28pose%29%2C%20IMU%2C%20and%20pressure%20sensors%2C%20demonstrating%20robust%20generalization%0Aacross%20unseen%20activities%20and%20modalities%2C%20offering%20a%20fundamentally%20different%0Aparadigm%20from%20contemporary%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInitial%2520Findings%2520on%2520Sensor%2520based%2520Open%2520Vocabulary%2520Activity%2520Recognition%250A%2520%2520via%2520Text%2520Embedding%2520Inversion%26entry.906535625%3DLala%2520Shakti%2520Swarup%2520Ray%2520and%2520Bo%2520Zhou%2520and%2520Sungho%2520Suh%2520and%2520Paul%2520Lukowicz%26entry.1292438233%3D%2520%2520Conventional%2520human%2520activity%2520recognition%2520%2528HAR%2529%2520relies%2520on%2520classifiers%2520trained%250Ato%2520predict%2520discrete%2520activity%2520classes%252C%2520inherently%2520limiting%2520recognition%2520to%250Aactivities%2520explicitly%2520present%2520in%2520the%2520training%2520set.%2520Such%2520classifiers%2520would%250Ainvariably%2520fail%252C%2520putting%2520zero%2520likelihood%252C%2520when%2520encountering%2520unseen%2520activities.%250AWe%2520propose%2520Open%2520Vocabulary%2520HAR%2520%2528OV-HAR%2529%252C%2520a%2520framework%2520that%2520overcomes%2520this%250Alimitation%2520by%2520first%2520converting%2520each%2520activity%2520into%2520natural%2520language%2520and%2520breaking%250Ait%2520into%2520a%2520sequence%2520of%2520elementary%2520motions.%2520This%2520descriptive%2520text%2520is%2520then%2520encoded%250Ainto%2520a%2520fixed-size%2520embedding.%2520The%2520model%2520is%2520trained%2520to%2520regress%2520this%2520embedding%252C%250Awhich%2520is%2520subsequently%2520decoded%2520back%2520into%2520natural%2520language%2520using%2520a%2520pre-trained%250Aembedding%2520inversion%2520model.%2520Unlike%2520other%2520works%2520that%2520rely%2520on%2520auto-regressive%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520at%2520their%2520core%252C%2520OV-HAR%2520achieves%2520open%2520vocabulary%250Arecognition%2520without%2520the%2520computational%2520overhead%2520of%2520such%2520models.%2520The%2520generated%250Atext%2520can%2520be%2520transformed%2520into%2520a%2520single%2520activity%2520class%2520using%2520LLM%2520prompt%250Aengineering.%2520We%2520have%2520evaluated%2520our%2520approach%2520on%2520different%2520modalities%252C%2520including%250Avision%2520%2528pose%2529%252C%2520IMU%252C%2520and%2520pressure%2520sensors%252C%2520demonstrating%2520robust%2520generalization%250Aacross%2520unseen%2520activities%2520and%2520modalities%252C%2520offering%2520a%2520fundamentally%2520different%250Aparadigm%2520from%2520contemporary%2520classifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Initial%20Findings%20on%20Sensor%20based%20Open%20Vocabulary%20Activity%20Recognition%0A%20%20via%20Text%20Embedding%20Inversion&entry.906535625=Lala%20Shakti%20Swarup%20Ray%20and%20Bo%20Zhou%20and%20Sungho%20Suh%20and%20Paul%20Lukowicz&entry.1292438233=%20%20Conventional%20human%20activity%20recognition%20%28HAR%29%20relies%20on%20classifiers%20trained%0Ato%20predict%20discrete%20activity%20classes%2C%20inherently%20limiting%20recognition%20to%0Aactivities%20explicitly%20present%20in%20the%20training%20set.%20Such%20classifiers%20would%0Ainvariably%20fail%2C%20putting%20zero%20likelihood%2C%20when%20encountering%20unseen%20activities.%0AWe%20propose%20Open%20Vocabulary%20HAR%20%28OV-HAR%29%2C%20a%20framework%20that%20overcomes%20this%0Alimitation%20by%20first%20converting%20each%20activity%20into%20natural%20language%20and%20breaking%0Ait%20into%20a%20sequence%20of%20elementary%20motions.%20This%20descriptive%20text%20is%20then%20encoded%0Ainto%20a%20fixed-size%20embedding.%20The%20model%20is%20trained%20to%20regress%20this%20embedding%2C%0Awhich%20is%20subsequently%20decoded%20back%20into%20natural%20language%20using%20a%20pre-trained%0Aembedding%20inversion%20model.%20Unlike%20other%20works%20that%20rely%20on%20auto-regressive%0Alarge%20language%20models%20%28LLMs%29%20at%20their%20core%2C%20OV-HAR%20achieves%20open%20vocabulary%0Arecognition%20without%20the%20computational%20overhead%20of%20such%20models.%20The%20generated%0Atext%20can%20be%20transformed%20into%20a%20single%20activity%20class%20using%20LLM%20prompt%0Aengineering.%20We%20have%20evaluated%20our%20approach%20on%20different%20modalities%2C%20including%0Avision%20%28pose%29%2C%20IMU%2C%20and%20pressure%20sensors%2C%20demonstrating%20robust%20generalization%0Aacross%20unseen%20activities%20and%20modalities%2C%20offering%20a%20fundamentally%20different%0Aparadigm%20from%20contemporary%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07408v1&entry.124074799=Read"},
{"title": "A4O: All Trigger for One sample", "author": "Duc Anh Vu and Anh Tuan Tran and Cong Tran and Cuong Pham", "abstract": "  Backdoor attacks have become a critical threat to deep neural networks\n(DNNs), drawing many research interests. However, most of the studied attacks\nemploy a single type of trigger. Consequently, proposed backdoor defenders\noften rely on the assumption that triggers would appear in a unified way. In\nthis paper, we show that this naive assumption can create a loophole, allowing\nmore sophisticated backdoor attacks to bypass. We design a novel backdoor\nattack mechanism that incorporates multiple types of backdoor triggers,\nfocusing on stealthiness and effectiveness. Our journey begins with the\nintriguing observation that the performance of a backdoor attack in deep\nlearning models, as well as its detectability and removability, are all\nproportional to the magnitude of the trigger. Based on this correlation, we\npropose reducing the magnitude of each trigger type and combining them to\nachieve a strong backdoor relying on the combined trigger while still staying\nsafely under the radar of defenders. Extensive experiments on three standard\ndatasets demonstrate that our method can achieve high attack success rates\n(ASRs) while consistently bypassing state-of-the-art defenses.\n", "link": "http://arxiv.org/abs/2501.07192v1", "date": "2025-01-13", "relevancy": 2.2096, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4609}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4415}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A4O%3A%20All%20Trigger%20for%20One%20sample&body=Title%3A%20A4O%3A%20All%20Trigger%20for%20One%20sample%0AAuthor%3A%20Duc%20Anh%20Vu%20and%20Anh%20Tuan%20Tran%20and%20Cong%20Tran%20and%20Cuong%20Pham%0AAbstract%3A%20%20%20Backdoor%20attacks%20have%20become%20a%20critical%20threat%20to%20deep%20neural%20networks%0A%28DNNs%29%2C%20drawing%20many%20research%20interests.%20However%2C%20most%20of%20the%20studied%20attacks%0Aemploy%20a%20single%20type%20of%20trigger.%20Consequently%2C%20proposed%20backdoor%20defenders%0Aoften%20rely%20on%20the%20assumption%20that%20triggers%20would%20appear%20in%20a%20unified%20way.%20In%0Athis%20paper%2C%20we%20show%20that%20this%20naive%20assumption%20can%20create%20a%20loophole%2C%20allowing%0Amore%20sophisticated%20backdoor%20attacks%20to%20bypass.%20We%20design%20a%20novel%20backdoor%0Aattack%20mechanism%20that%20incorporates%20multiple%20types%20of%20backdoor%20triggers%2C%0Afocusing%20on%20stealthiness%20and%20effectiveness.%20Our%20journey%20begins%20with%20the%0Aintriguing%20observation%20that%20the%20performance%20of%20a%20backdoor%20attack%20in%20deep%0Alearning%20models%2C%20as%20well%20as%20its%20detectability%20and%20removability%2C%20are%20all%0Aproportional%20to%20the%20magnitude%20of%20the%20trigger.%20Based%20on%20this%20correlation%2C%20we%0Apropose%20reducing%20the%20magnitude%20of%20each%20trigger%20type%20and%20combining%20them%20to%0Aachieve%20a%20strong%20backdoor%20relying%20on%20the%20combined%20trigger%20while%20still%20staying%0Asafely%20under%20the%20radar%20of%20defenders.%20Extensive%20experiments%20on%20three%20standard%0Adatasets%20demonstrate%20that%20our%20method%20can%20achieve%20high%20attack%20success%20rates%0A%28ASRs%29%20while%20consistently%20bypassing%20state-of-the-art%20defenses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA4O%253A%2520All%2520Trigger%2520for%2520One%2520sample%26entry.906535625%3DDuc%2520Anh%2520Vu%2520and%2520Anh%2520Tuan%2520Tran%2520and%2520Cong%2520Tran%2520and%2520Cuong%2520Pham%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520have%2520become%2520a%2520critical%2520threat%2520to%2520deep%2520neural%2520networks%250A%2528DNNs%2529%252C%2520drawing%2520many%2520research%2520interests.%2520However%252C%2520most%2520of%2520the%2520studied%2520attacks%250Aemploy%2520a%2520single%2520type%2520of%2520trigger.%2520Consequently%252C%2520proposed%2520backdoor%2520defenders%250Aoften%2520rely%2520on%2520the%2520assumption%2520that%2520triggers%2520would%2520appear%2520in%2520a%2520unified%2520way.%2520In%250Athis%2520paper%252C%2520we%2520show%2520that%2520this%2520naive%2520assumption%2520can%2520create%2520a%2520loophole%252C%2520allowing%250Amore%2520sophisticated%2520backdoor%2520attacks%2520to%2520bypass.%2520We%2520design%2520a%2520novel%2520backdoor%250Aattack%2520mechanism%2520that%2520incorporates%2520multiple%2520types%2520of%2520backdoor%2520triggers%252C%250Afocusing%2520on%2520stealthiness%2520and%2520effectiveness.%2520Our%2520journey%2520begins%2520with%2520the%250Aintriguing%2520observation%2520that%2520the%2520performance%2520of%2520a%2520backdoor%2520attack%2520in%2520deep%250Alearning%2520models%252C%2520as%2520well%2520as%2520its%2520detectability%2520and%2520removability%252C%2520are%2520all%250Aproportional%2520to%2520the%2520magnitude%2520of%2520the%2520trigger.%2520Based%2520on%2520this%2520correlation%252C%2520we%250Apropose%2520reducing%2520the%2520magnitude%2520of%2520each%2520trigger%2520type%2520and%2520combining%2520them%2520to%250Aachieve%2520a%2520strong%2520backdoor%2520relying%2520on%2520the%2520combined%2520trigger%2520while%2520still%2520staying%250Asafely%2520under%2520the%2520radar%2520of%2520defenders.%2520Extensive%2520experiments%2520on%2520three%2520standard%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520can%2520achieve%2520high%2520attack%2520success%2520rates%250A%2528ASRs%2529%2520while%2520consistently%2520bypassing%2520state-of-the-art%2520defenses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A4O%3A%20All%20Trigger%20for%20One%20sample&entry.906535625=Duc%20Anh%20Vu%20and%20Anh%20Tuan%20Tran%20and%20Cong%20Tran%20and%20Cuong%20Pham&entry.1292438233=%20%20Backdoor%20attacks%20have%20become%20a%20critical%20threat%20to%20deep%20neural%20networks%0A%28DNNs%29%2C%20drawing%20many%20research%20interests.%20However%2C%20most%20of%20the%20studied%20attacks%0Aemploy%20a%20single%20type%20of%20trigger.%20Consequently%2C%20proposed%20backdoor%20defenders%0Aoften%20rely%20on%20the%20assumption%20that%20triggers%20would%20appear%20in%20a%20unified%20way.%20In%0Athis%20paper%2C%20we%20show%20that%20this%20naive%20assumption%20can%20create%20a%20loophole%2C%20allowing%0Amore%20sophisticated%20backdoor%20attacks%20to%20bypass.%20We%20design%20a%20novel%20backdoor%0Aattack%20mechanism%20that%20incorporates%20multiple%20types%20of%20backdoor%20triggers%2C%0Afocusing%20on%20stealthiness%20and%20effectiveness.%20Our%20journey%20begins%20with%20the%0Aintriguing%20observation%20that%20the%20performance%20of%20a%20backdoor%20attack%20in%20deep%0Alearning%20models%2C%20as%20well%20as%20its%20detectability%20and%20removability%2C%20are%20all%0Aproportional%20to%20the%20magnitude%20of%20the%20trigger.%20Based%20on%20this%20correlation%2C%20we%0Apropose%20reducing%20the%20magnitude%20of%20each%20trigger%20type%20and%20combining%20them%20to%0Aachieve%20a%20strong%20backdoor%20relying%20on%20the%20combined%20trigger%20while%20still%20staying%0Asafely%20under%20the%20radar%20of%20defenders.%20Extensive%20experiments%20on%20three%20standard%0Adatasets%20demonstrate%20that%20our%20method%20can%20achieve%20high%20attack%20success%20rates%0A%28ASRs%29%20while%20consistently%20bypassing%20state-of-the-art%20defenses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07192v1&entry.124074799=Read"},
{"title": "TimberVision: A Multi-Task Dataset and Framework for Log-Component\n  Segmentation and Tracking in Autonomous Forestry Operations", "author": "Daniel Steininger and Julia Simon and Andreas Trondl and Markus Murschitz", "abstract": "  Timber represents an increasingly valuable and versatile resource. However,\nforestry operations such as harvesting, handling and measuring logs still\nrequire substantial human labor in remote environments posing significant\nsafety risks. Progressively automating these tasks has the potential of\nincreasing their efficiency as well as safety, but requires an accurate\ndetection of individual logs as well as live trees and their context. Although\ninitial approaches have been proposed for this challenging application domain,\nspecialized data and algorithms are still too scarce to develop robust\nsolutions. To mitigate this gap, we introduce the TimberVision dataset,\nconsisting of more than 2k annotated RGB images containing a total of 51k trunk\ncomponents including cut and lateral surfaces, thereby surpassing any existing\ndataset in this domain in terms of both quantity and detail by a large margin.\nBased on this data, we conduct a series of ablation experiments for oriented\nobject detection and instance segmentation and evaluate the influence of\nmultiple scene parameters on model performance. We introduce a generic\nframework to fuse the components detected by our models for both tasks into\nunified trunk representations. Furthermore, we automatically derive geometric\nproperties and apply multi-object tracking to further enhance robustness. Our\ndetection and tracking approach provides highly descriptive and accurate trunk\nrepresentations solely from RGB image data, even under challenging\nenvironmental conditions. Our solution is suitable for a wide range of\napplication scenarios and can be readily combined with other sensor modalities.\n", "link": "http://arxiv.org/abs/2501.07360v1", "date": "2025-01-13", "relevancy": 2.2092, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimberVision%3A%20A%20Multi-Task%20Dataset%20and%20Framework%20for%20Log-Component%0A%20%20Segmentation%20and%20Tracking%20in%20Autonomous%20Forestry%20Operations&body=Title%3A%20TimberVision%3A%20A%20Multi-Task%20Dataset%20and%20Framework%20for%20Log-Component%0A%20%20Segmentation%20and%20Tracking%20in%20Autonomous%20Forestry%20Operations%0AAuthor%3A%20Daniel%20Steininger%20and%20Julia%20Simon%20and%20Andreas%20Trondl%20and%20Markus%20Murschitz%0AAbstract%3A%20%20%20Timber%20represents%20an%20increasingly%20valuable%20and%20versatile%20resource.%20However%2C%0Aforestry%20operations%20such%20as%20harvesting%2C%20handling%20and%20measuring%20logs%20still%0Arequire%20substantial%20human%20labor%20in%20remote%20environments%20posing%20significant%0Asafety%20risks.%20Progressively%20automating%20these%20tasks%20has%20the%20potential%20of%0Aincreasing%20their%20efficiency%20as%20well%20as%20safety%2C%20but%20requires%20an%20accurate%0Adetection%20of%20individual%20logs%20as%20well%20as%20live%20trees%20and%20their%20context.%20Although%0Ainitial%20approaches%20have%20been%20proposed%20for%20this%20challenging%20application%20domain%2C%0Aspecialized%20data%20and%20algorithms%20are%20still%20too%20scarce%20to%20develop%20robust%0Asolutions.%20To%20mitigate%20this%20gap%2C%20we%20introduce%20the%20TimberVision%20dataset%2C%0Aconsisting%20of%20more%20than%202k%20annotated%20RGB%20images%20containing%20a%20total%20of%2051k%20trunk%0Acomponents%20including%20cut%20and%20lateral%20surfaces%2C%20thereby%20surpassing%20any%20existing%0Adataset%20in%20this%20domain%20in%20terms%20of%20both%20quantity%20and%20detail%20by%20a%20large%20margin.%0ABased%20on%20this%20data%2C%20we%20conduct%20a%20series%20of%20ablation%20experiments%20for%20oriented%0Aobject%20detection%20and%20instance%20segmentation%20and%20evaluate%20the%20influence%20of%0Amultiple%20scene%20parameters%20on%20model%20performance.%20We%20introduce%20a%20generic%0Aframework%20to%20fuse%20the%20components%20detected%20by%20our%20models%20for%20both%20tasks%20into%0Aunified%20trunk%20representations.%20Furthermore%2C%20we%20automatically%20derive%20geometric%0Aproperties%20and%20apply%20multi-object%20tracking%20to%20further%20enhance%20robustness.%20Our%0Adetection%20and%20tracking%20approach%20provides%20highly%20descriptive%20and%20accurate%20trunk%0Arepresentations%20solely%20from%20RGB%20image%20data%2C%20even%20under%20challenging%0Aenvironmental%20conditions.%20Our%20solution%20is%20suitable%20for%20a%20wide%20range%20of%0Aapplication%20scenarios%20and%20can%20be%20readily%20combined%20with%20other%20sensor%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimberVision%253A%2520A%2520Multi-Task%2520Dataset%2520and%2520Framework%2520for%2520Log-Component%250A%2520%2520Segmentation%2520and%2520Tracking%2520in%2520Autonomous%2520Forestry%2520Operations%26entry.906535625%3DDaniel%2520Steininger%2520and%2520Julia%2520Simon%2520and%2520Andreas%2520Trondl%2520and%2520Markus%2520Murschitz%26entry.1292438233%3D%2520%2520Timber%2520represents%2520an%2520increasingly%2520valuable%2520and%2520versatile%2520resource.%2520However%252C%250Aforestry%2520operations%2520such%2520as%2520harvesting%252C%2520handling%2520and%2520measuring%2520logs%2520still%250Arequire%2520substantial%2520human%2520labor%2520in%2520remote%2520environments%2520posing%2520significant%250Asafety%2520risks.%2520Progressively%2520automating%2520these%2520tasks%2520has%2520the%2520potential%2520of%250Aincreasing%2520their%2520efficiency%2520as%2520well%2520as%2520safety%252C%2520but%2520requires%2520an%2520accurate%250Adetection%2520of%2520individual%2520logs%2520as%2520well%2520as%2520live%2520trees%2520and%2520their%2520context.%2520Although%250Ainitial%2520approaches%2520have%2520been%2520proposed%2520for%2520this%2520challenging%2520application%2520domain%252C%250Aspecialized%2520data%2520and%2520algorithms%2520are%2520still%2520too%2520scarce%2520to%2520develop%2520robust%250Asolutions.%2520To%2520mitigate%2520this%2520gap%252C%2520we%2520introduce%2520the%2520TimberVision%2520dataset%252C%250Aconsisting%2520of%2520more%2520than%25202k%2520annotated%2520RGB%2520images%2520containing%2520a%2520total%2520of%252051k%2520trunk%250Acomponents%2520including%2520cut%2520and%2520lateral%2520surfaces%252C%2520thereby%2520surpassing%2520any%2520existing%250Adataset%2520in%2520this%2520domain%2520in%2520terms%2520of%2520both%2520quantity%2520and%2520detail%2520by%2520a%2520large%2520margin.%250ABased%2520on%2520this%2520data%252C%2520we%2520conduct%2520a%2520series%2520of%2520ablation%2520experiments%2520for%2520oriented%250Aobject%2520detection%2520and%2520instance%2520segmentation%2520and%2520evaluate%2520the%2520influence%2520of%250Amultiple%2520scene%2520parameters%2520on%2520model%2520performance.%2520We%2520introduce%2520a%2520generic%250Aframework%2520to%2520fuse%2520the%2520components%2520detected%2520by%2520our%2520models%2520for%2520both%2520tasks%2520into%250Aunified%2520trunk%2520representations.%2520Furthermore%252C%2520we%2520automatically%2520derive%2520geometric%250Aproperties%2520and%2520apply%2520multi-object%2520tracking%2520to%2520further%2520enhance%2520robustness.%2520Our%250Adetection%2520and%2520tracking%2520approach%2520provides%2520highly%2520descriptive%2520and%2520accurate%2520trunk%250Arepresentations%2520solely%2520from%2520RGB%2520image%2520data%252C%2520even%2520under%2520challenging%250Aenvironmental%2520conditions.%2520Our%2520solution%2520is%2520suitable%2520for%2520a%2520wide%2520range%2520of%250Aapplication%2520scenarios%2520and%2520can%2520be%2520readily%2520combined%2520with%2520other%2520sensor%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimberVision%3A%20A%20Multi-Task%20Dataset%20and%20Framework%20for%20Log-Component%0A%20%20Segmentation%20and%20Tracking%20in%20Autonomous%20Forestry%20Operations&entry.906535625=Daniel%20Steininger%20and%20Julia%20Simon%20and%20Andreas%20Trondl%20and%20Markus%20Murschitz&entry.1292438233=%20%20Timber%20represents%20an%20increasingly%20valuable%20and%20versatile%20resource.%20However%2C%0Aforestry%20operations%20such%20as%20harvesting%2C%20handling%20and%20measuring%20logs%20still%0Arequire%20substantial%20human%20labor%20in%20remote%20environments%20posing%20significant%0Asafety%20risks.%20Progressively%20automating%20these%20tasks%20has%20the%20potential%20of%0Aincreasing%20their%20efficiency%20as%20well%20as%20safety%2C%20but%20requires%20an%20accurate%0Adetection%20of%20individual%20logs%20as%20well%20as%20live%20trees%20and%20their%20context.%20Although%0Ainitial%20approaches%20have%20been%20proposed%20for%20this%20challenging%20application%20domain%2C%0Aspecialized%20data%20and%20algorithms%20are%20still%20too%20scarce%20to%20develop%20robust%0Asolutions.%20To%20mitigate%20this%20gap%2C%20we%20introduce%20the%20TimberVision%20dataset%2C%0Aconsisting%20of%20more%20than%202k%20annotated%20RGB%20images%20containing%20a%20total%20of%2051k%20trunk%0Acomponents%20including%20cut%20and%20lateral%20surfaces%2C%20thereby%20surpassing%20any%20existing%0Adataset%20in%20this%20domain%20in%20terms%20of%20both%20quantity%20and%20detail%20by%20a%20large%20margin.%0ABased%20on%20this%20data%2C%20we%20conduct%20a%20series%20of%20ablation%20experiments%20for%20oriented%0Aobject%20detection%20and%20instance%20segmentation%20and%20evaluate%20the%20influence%20of%0Amultiple%20scene%20parameters%20on%20model%20performance.%20We%20introduce%20a%20generic%0Aframework%20to%20fuse%20the%20components%20detected%20by%20our%20models%20for%20both%20tasks%20into%0Aunified%20trunk%20representations.%20Furthermore%2C%20we%20automatically%20derive%20geometric%0Aproperties%20and%20apply%20multi-object%20tracking%20to%20further%20enhance%20robustness.%20Our%0Adetection%20and%20tracking%20approach%20provides%20highly%20descriptive%20and%20accurate%20trunk%0Arepresentations%20solely%20from%20RGB%20image%20data%2C%20even%20under%20challenging%0Aenvironmental%20conditions.%20Our%20solution%20is%20suitable%20for%20a%20wide%20range%20of%0Aapplication%20scenarios%20and%20can%20be%20readily%20combined%20with%20other%20sensor%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07360v1&entry.124074799=Read"},
{"title": "Are LLMs Good Cryptic Crossword Solvers?", "author": "Abdelrahman Sadallah and Daria Kotova and Ekaterina Kochmar", "abstract": "  Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans.\n", "link": "http://arxiv.org/abs/2403.12094v2", "date": "2025-01-13", "relevancy": 2.2071, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4694}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20LLMs%20Good%20Cryptic%20Crossword%20Solvers%3F&body=Title%3A%20Are%20LLMs%20Good%20Cryptic%20Crossword%20Solvers%3F%0AAuthor%3A%20Abdelrahman%20Sadallah%20and%20Daria%20Kotova%20and%20Ekaterina%20Kochmar%0AAbstract%3A%20%20%20Cryptic%20crosswords%20are%20puzzles%20that%20rely%20not%20only%20on%20general%20knowledge%20but%0Aalso%20on%20the%20solver%27s%20ability%20to%20manipulate%20language%20on%20different%20levels%20and%0Adeal%20with%20various%20types%20of%20wordplay.%20Previous%20research%20suggests%20that%20solving%0Asuch%20puzzles%20is%20a%20challenge%20even%20for%20modern%20NLP%20models.%20However%2C%20the%20abilities%0Aof%20large%20language%20models%20%28LLMs%29%20have%20not%20yet%20been%20tested%20on%20this%20task.%20In%20this%0Apaper%2C%20we%20establish%20the%20benchmark%20results%20for%20three%20popular%20LLMs%20--%20LLaMA2%2C%0AMistral%2C%20and%20ChatGPT%20--%20showing%20that%20their%20performance%20on%20this%20task%20is%20still%0Afar%20from%20that%20of%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520LLMs%2520Good%2520Cryptic%2520Crossword%2520Solvers%253F%26entry.906535625%3DAbdelrahman%2520Sadallah%2520and%2520Daria%2520Kotova%2520and%2520Ekaterina%2520Kochmar%26entry.1292438233%3D%2520%2520Cryptic%2520crosswords%2520are%2520puzzles%2520that%2520rely%2520not%2520only%2520on%2520general%2520knowledge%2520but%250Aalso%2520on%2520the%2520solver%2527s%2520ability%2520to%2520manipulate%2520language%2520on%2520different%2520levels%2520and%250Adeal%2520with%2520various%2520types%2520of%2520wordplay.%2520Previous%2520research%2520suggests%2520that%2520solving%250Asuch%2520puzzles%2520is%2520a%2520challenge%2520even%2520for%2520modern%2520NLP%2520models.%2520However%252C%2520the%2520abilities%250Aof%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520not%2520yet%2520been%2520tested%2520on%2520this%2520task.%2520In%2520this%250Apaper%252C%2520we%2520establish%2520the%2520benchmark%2520results%2520for%2520three%2520popular%2520LLMs%2520--%2520LLaMA2%252C%250AMistral%252C%2520and%2520ChatGPT%2520--%2520showing%2520that%2520their%2520performance%2520on%2520this%2520task%2520is%2520still%250Afar%2520from%2520that%2520of%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20LLMs%20Good%20Cryptic%20Crossword%20Solvers%3F&entry.906535625=Abdelrahman%20Sadallah%20and%20Daria%20Kotova%20and%20Ekaterina%20Kochmar&entry.1292438233=%20%20Cryptic%20crosswords%20are%20puzzles%20that%20rely%20not%20only%20on%20general%20knowledge%20but%0Aalso%20on%20the%20solver%27s%20ability%20to%20manipulate%20language%20on%20different%20levels%20and%0Adeal%20with%20various%20types%20of%20wordplay.%20Previous%20research%20suggests%20that%20solving%0Asuch%20puzzles%20is%20a%20challenge%20even%20for%20modern%20NLP%20models.%20However%2C%20the%20abilities%0Aof%20large%20language%20models%20%28LLMs%29%20have%20not%20yet%20been%20tested%20on%20this%20task.%20In%20this%0Apaper%2C%20we%20establish%20the%20benchmark%20results%20for%20three%20popular%20LLMs%20--%20LLaMA2%2C%0AMistral%2C%20and%20ChatGPT%20--%20showing%20that%20their%20performance%20on%20this%20task%20is%20still%0Afar%20from%20that%20of%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12094v2&entry.124074799=Read"},
{"title": "CSTA: Spatial-Temporal Causal Adaptive Learning for Exemplar-Free Video\n  Class-Incremental Learning", "author": "Tieyuan Chen and Huabin Liu and Chern Hong Lim and John See and Xing Gao and Junhui Hou and Weiyao Lin", "abstract": "  Continual learning aims to acquire new knowledge while retaining past\ninformation. Class-incremental learning (CIL) presents a challenging scenario\nwhere classes are introduced sequentially. For video data, the task becomes\nmore complex than image data because it requires learning and preserving both\nspatial appearance and temporal action involvement. To address this challenge,\nwe propose a novel exemplar-free framework that equips separate spatiotemporal\nadapters to learn new class patterns, accommodating the incremental information\nrepresentation requirements unique to each class. While separate adapters are\nproven to mitigate forgetting and fit unique requirements, naively applying\nthem hinders the intrinsic connection between spatial and temporal information\nincrements, affecting the efficiency of representing newly learned class\ninformation. Motivated by this, we introduce two key innovations from a causal\nperspective. First, a causal distillation module is devised to maintain the\nrelation between spatial-temporal knowledge for a more efficient\nrepresentation. Second, a causal compensation mechanism is proposed to reduce\nthe conflicts during increment and memorization between different types of\ninformation. Extensive experiments conducted on benchmark datasets demonstrate\nthat our framework can achieve new state-of-the-art results, surpassing current\nexample-based methods by 4.2% in accuracy on average.\n", "link": "http://arxiv.org/abs/2501.07236v1", "date": "2025-01-13", "relevancy": 2.2054, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5787}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.556}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CSTA%3A%20Spatial-Temporal%20Causal%20Adaptive%20Learning%20for%20Exemplar-Free%20Video%0A%20%20Class-Incremental%20Learning&body=Title%3A%20CSTA%3A%20Spatial-Temporal%20Causal%20Adaptive%20Learning%20for%20Exemplar-Free%20Video%0A%20%20Class-Incremental%20Learning%0AAuthor%3A%20Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Chern%20Hong%20Lim%20and%20John%20See%20and%20Xing%20Gao%20and%20Junhui%20Hou%20and%20Weiyao%20Lin%0AAbstract%3A%20%20%20Continual%20learning%20aims%20to%20acquire%20new%20knowledge%20while%20retaining%20past%0Ainformation.%20Class-incremental%20learning%20%28CIL%29%20presents%20a%20challenging%20scenario%0Awhere%20classes%20are%20introduced%20sequentially.%20For%20video%20data%2C%20the%20task%20becomes%0Amore%20complex%20than%20image%20data%20because%20it%20requires%20learning%20and%20preserving%20both%0Aspatial%20appearance%20and%20temporal%20action%20involvement.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20novel%20exemplar-free%20framework%20that%20equips%20separate%20spatiotemporal%0Aadapters%20to%20learn%20new%20class%20patterns%2C%20accommodating%20the%20incremental%20information%0Arepresentation%20requirements%20unique%20to%20each%20class.%20While%20separate%20adapters%20are%0Aproven%20to%20mitigate%20forgetting%20and%20fit%20unique%20requirements%2C%20naively%20applying%0Athem%20hinders%20the%20intrinsic%20connection%20between%20spatial%20and%20temporal%20information%0Aincrements%2C%20affecting%20the%20efficiency%20of%20representing%20newly%20learned%20class%0Ainformation.%20Motivated%20by%20this%2C%20we%20introduce%20two%20key%20innovations%20from%20a%20causal%0Aperspective.%20First%2C%20a%20causal%20distillation%20module%20is%20devised%20to%20maintain%20the%0Arelation%20between%20spatial-temporal%20knowledge%20for%20a%20more%20efficient%0Arepresentation.%20Second%2C%20a%20causal%20compensation%20mechanism%20is%20proposed%20to%20reduce%0Athe%20conflicts%20during%20increment%20and%20memorization%20between%20different%20types%20of%0Ainformation.%20Extensive%20experiments%20conducted%20on%20benchmark%20datasets%20demonstrate%0Athat%20our%20framework%20can%20achieve%20new%20state-of-the-art%20results%2C%20surpassing%20current%0Aexample-based%20methods%20by%204.2%25%20in%20accuracy%20on%20average.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCSTA%253A%2520Spatial-Temporal%2520Causal%2520Adaptive%2520Learning%2520for%2520Exemplar-Free%2520Video%250A%2520%2520Class-Incremental%2520Learning%26entry.906535625%3DTieyuan%2520Chen%2520and%2520Huabin%2520Liu%2520and%2520Chern%2520Hong%2520Lim%2520and%2520John%2520See%2520and%2520Xing%2520Gao%2520and%2520Junhui%2520Hou%2520and%2520Weiyao%2520Lin%26entry.1292438233%3D%2520%2520Continual%2520learning%2520aims%2520to%2520acquire%2520new%2520knowledge%2520while%2520retaining%2520past%250Ainformation.%2520Class-incremental%2520learning%2520%2528CIL%2529%2520presents%2520a%2520challenging%2520scenario%250Awhere%2520classes%2520are%2520introduced%2520sequentially.%2520For%2520video%2520data%252C%2520the%2520task%2520becomes%250Amore%2520complex%2520than%2520image%2520data%2520because%2520it%2520requires%2520learning%2520and%2520preserving%2520both%250Aspatial%2520appearance%2520and%2520temporal%2520action%2520involvement.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520a%2520novel%2520exemplar-free%2520framework%2520that%2520equips%2520separate%2520spatiotemporal%250Aadapters%2520to%2520learn%2520new%2520class%2520patterns%252C%2520accommodating%2520the%2520incremental%2520information%250Arepresentation%2520requirements%2520unique%2520to%2520each%2520class.%2520While%2520separate%2520adapters%2520are%250Aproven%2520to%2520mitigate%2520forgetting%2520and%2520fit%2520unique%2520requirements%252C%2520naively%2520applying%250Athem%2520hinders%2520the%2520intrinsic%2520connection%2520between%2520spatial%2520and%2520temporal%2520information%250Aincrements%252C%2520affecting%2520the%2520efficiency%2520of%2520representing%2520newly%2520learned%2520class%250Ainformation.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%2520two%2520key%2520innovations%2520from%2520a%2520causal%250Aperspective.%2520First%252C%2520a%2520causal%2520distillation%2520module%2520is%2520devised%2520to%2520maintain%2520the%250Arelation%2520between%2520spatial-temporal%2520knowledge%2520for%2520a%2520more%2520efficient%250Arepresentation.%2520Second%252C%2520a%2520causal%2520compensation%2520mechanism%2520is%2520proposed%2520to%2520reduce%250Athe%2520conflicts%2520during%2520increment%2520and%2520memorization%2520between%2520different%2520types%2520of%250Ainformation.%2520Extensive%2520experiments%2520conducted%2520on%2520benchmark%2520datasets%2520demonstrate%250Athat%2520our%2520framework%2520can%2520achieve%2520new%2520state-of-the-art%2520results%252C%2520surpassing%2520current%250Aexample-based%2520methods%2520by%25204.2%2525%2520in%2520accuracy%2520on%2520average.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSTA%3A%20Spatial-Temporal%20Causal%20Adaptive%20Learning%20for%20Exemplar-Free%20Video%0A%20%20Class-Incremental%20Learning&entry.906535625=Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Chern%20Hong%20Lim%20and%20John%20See%20and%20Xing%20Gao%20and%20Junhui%20Hou%20and%20Weiyao%20Lin&entry.1292438233=%20%20Continual%20learning%20aims%20to%20acquire%20new%20knowledge%20while%20retaining%20past%0Ainformation.%20Class-incremental%20learning%20%28CIL%29%20presents%20a%20challenging%20scenario%0Awhere%20classes%20are%20introduced%20sequentially.%20For%20video%20data%2C%20the%20task%20becomes%0Amore%20complex%20than%20image%20data%20because%20it%20requires%20learning%20and%20preserving%20both%0Aspatial%20appearance%20and%20temporal%20action%20involvement.%20To%20address%20this%20challenge%2C%0Awe%20propose%20a%20novel%20exemplar-free%20framework%20that%20equips%20separate%20spatiotemporal%0Aadapters%20to%20learn%20new%20class%20patterns%2C%20accommodating%20the%20incremental%20information%0Arepresentation%20requirements%20unique%20to%20each%20class.%20While%20separate%20adapters%20are%0Aproven%20to%20mitigate%20forgetting%20and%20fit%20unique%20requirements%2C%20naively%20applying%0Athem%20hinders%20the%20intrinsic%20connection%20between%20spatial%20and%20temporal%20information%0Aincrements%2C%20affecting%20the%20efficiency%20of%20representing%20newly%20learned%20class%0Ainformation.%20Motivated%20by%20this%2C%20we%20introduce%20two%20key%20innovations%20from%20a%20causal%0Aperspective.%20First%2C%20a%20causal%20distillation%20module%20is%20devised%20to%20maintain%20the%0Arelation%20between%20spatial-temporal%20knowledge%20for%20a%20more%20efficient%0Arepresentation.%20Second%2C%20a%20causal%20compensation%20mechanism%20is%20proposed%20to%20reduce%0Athe%20conflicts%20during%20increment%20and%20memorization%20between%20different%20types%20of%0Ainformation.%20Extensive%20experiments%20conducted%20on%20benchmark%20datasets%20demonstrate%0Athat%20our%20framework%20can%20achieve%20new%20state-of-the-art%20results%2C%20surpassing%20current%0Aexample-based%20methods%20by%204.2%25%20in%20accuracy%20on%20average.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07236v1&entry.124074799=Read"},
{"title": "Fast-Revisit Coverage Path Planning for Autonomous Mobile Patrol Robots\n  Using Long-Range Sensor Information", "author": "Srinivas Kachavarapu and Tobias Doernbach and Reinhard Gerndt", "abstract": "  The utilization of Unmanned Ground Vehicles (UGVs) for patrolling industrial\nsites has expanded significantly. These UGVs typically are equipped with\nperception systems, e.g., computer vision, with limited range due to sensor\nlimitations or site topology. High-level control of the UGVs requires Coverage\nPath Planning (CPP) algorithms that navigate all relevant waypoints and\npromptly start the next cycle. In this paper, we propose the novel Fast-Revisit\nCoverage Path Planning (FaRe-CPP) algorithm using a greedy heuristic approach\nto propose waypoints for maximum coverage area and a random search-based path\noptimization technique to obtain a path along the proposed waypoints with\nminimum revisit time. We evaluated the algorithm in a simulated environment\nusing Gazebo and a camera-equipped TurtleBot3 against a number of existing\nalgorithms. Compared to their average revisit times and path lengths, our\nFaRe-CPP algorithm approximately showed a 45% and 40% reduction, respectively,\nin these highly relevant performance indicators.\n", "link": "http://arxiv.org/abs/2501.07343v1", "date": "2025-01-13", "relevancy": 2.1922, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5522}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast-Revisit%20Coverage%20Path%20Planning%20for%20Autonomous%20Mobile%20Patrol%20Robots%0A%20%20Using%20Long-Range%20Sensor%20Information&body=Title%3A%20Fast-Revisit%20Coverage%20Path%20Planning%20for%20Autonomous%20Mobile%20Patrol%20Robots%0A%20%20Using%20Long-Range%20Sensor%20Information%0AAuthor%3A%20Srinivas%20Kachavarapu%20and%20Tobias%20Doernbach%20and%20Reinhard%20Gerndt%0AAbstract%3A%20%20%20The%20utilization%20of%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20for%20patrolling%20industrial%0Asites%20has%20expanded%20significantly.%20These%20UGVs%20typically%20are%20equipped%20with%0Aperception%20systems%2C%20e.g.%2C%20computer%20vision%2C%20with%20limited%20range%20due%20to%20sensor%0Alimitations%20or%20site%20topology.%20High-level%20control%20of%20the%20UGVs%20requires%20Coverage%0APath%20Planning%20%28CPP%29%20algorithms%20that%20navigate%20all%20relevant%20waypoints%20and%0Apromptly%20start%20the%20next%20cycle.%20In%20this%20paper%2C%20we%20propose%20the%20novel%20Fast-Revisit%0ACoverage%20Path%20Planning%20%28FaRe-CPP%29%20algorithm%20using%20a%20greedy%20heuristic%20approach%0Ato%20propose%20waypoints%20for%20maximum%20coverage%20area%20and%20a%20random%20search-based%20path%0Aoptimization%20technique%20to%20obtain%20a%20path%20along%20the%20proposed%20waypoints%20with%0Aminimum%20revisit%20time.%20We%20evaluated%20the%20algorithm%20in%20a%20simulated%20environment%0Ausing%20Gazebo%20and%20a%20camera-equipped%20TurtleBot3%20against%20a%20number%20of%20existing%0Aalgorithms.%20Compared%20to%20their%20average%20revisit%20times%20and%20path%20lengths%2C%20our%0AFaRe-CPP%20algorithm%20approximately%20showed%20a%2045%25%20and%2040%25%20reduction%2C%20respectively%2C%0Ain%20these%20highly%20relevant%20performance%20indicators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast-Revisit%2520Coverage%2520Path%2520Planning%2520for%2520Autonomous%2520Mobile%2520Patrol%2520Robots%250A%2520%2520Using%2520Long-Range%2520Sensor%2520Information%26entry.906535625%3DSrinivas%2520Kachavarapu%2520and%2520Tobias%2520Doernbach%2520and%2520Reinhard%2520Gerndt%26entry.1292438233%3D%2520%2520The%2520utilization%2520of%2520Unmanned%2520Ground%2520Vehicles%2520%2528UGVs%2529%2520for%2520patrolling%2520industrial%250Asites%2520has%2520expanded%2520significantly.%2520These%2520UGVs%2520typically%2520are%2520equipped%2520with%250Aperception%2520systems%252C%2520e.g.%252C%2520computer%2520vision%252C%2520with%2520limited%2520range%2520due%2520to%2520sensor%250Alimitations%2520or%2520site%2520topology.%2520High-level%2520control%2520of%2520the%2520UGVs%2520requires%2520Coverage%250APath%2520Planning%2520%2528CPP%2529%2520algorithms%2520that%2520navigate%2520all%2520relevant%2520waypoints%2520and%250Apromptly%2520start%2520the%2520next%2520cycle.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520novel%2520Fast-Revisit%250ACoverage%2520Path%2520Planning%2520%2528FaRe-CPP%2529%2520algorithm%2520using%2520a%2520greedy%2520heuristic%2520approach%250Ato%2520propose%2520waypoints%2520for%2520maximum%2520coverage%2520area%2520and%2520a%2520random%2520search-based%2520path%250Aoptimization%2520technique%2520to%2520obtain%2520a%2520path%2520along%2520the%2520proposed%2520waypoints%2520with%250Aminimum%2520revisit%2520time.%2520We%2520evaluated%2520the%2520algorithm%2520in%2520a%2520simulated%2520environment%250Ausing%2520Gazebo%2520and%2520a%2520camera-equipped%2520TurtleBot3%2520against%2520a%2520number%2520of%2520existing%250Aalgorithms.%2520Compared%2520to%2520their%2520average%2520revisit%2520times%2520and%2520path%2520lengths%252C%2520our%250AFaRe-CPP%2520algorithm%2520approximately%2520showed%2520a%252045%2525%2520and%252040%2525%2520reduction%252C%2520respectively%252C%250Ain%2520these%2520highly%2520relevant%2520performance%2520indicators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast-Revisit%20Coverage%20Path%20Planning%20for%20Autonomous%20Mobile%20Patrol%20Robots%0A%20%20Using%20Long-Range%20Sensor%20Information&entry.906535625=Srinivas%20Kachavarapu%20and%20Tobias%20Doernbach%20and%20Reinhard%20Gerndt&entry.1292438233=%20%20The%20utilization%20of%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20for%20patrolling%20industrial%0Asites%20has%20expanded%20significantly.%20These%20UGVs%20typically%20are%20equipped%20with%0Aperception%20systems%2C%20e.g.%2C%20computer%20vision%2C%20with%20limited%20range%20due%20to%20sensor%0Alimitations%20or%20site%20topology.%20High-level%20control%20of%20the%20UGVs%20requires%20Coverage%0APath%20Planning%20%28CPP%29%20algorithms%20that%20navigate%20all%20relevant%20waypoints%20and%0Apromptly%20start%20the%20next%20cycle.%20In%20this%20paper%2C%20we%20propose%20the%20novel%20Fast-Revisit%0ACoverage%20Path%20Planning%20%28FaRe-CPP%29%20algorithm%20using%20a%20greedy%20heuristic%20approach%0Ato%20propose%20waypoints%20for%20maximum%20coverage%20area%20and%20a%20random%20search-based%20path%0Aoptimization%20technique%20to%20obtain%20a%20path%20along%20the%20proposed%20waypoints%20with%0Aminimum%20revisit%20time.%20We%20evaluated%20the%20algorithm%20in%20a%20simulated%20environment%0Ausing%20Gazebo%20and%20a%20camera-equipped%20TurtleBot3%20against%20a%20number%20of%20existing%0Aalgorithms.%20Compared%20to%20their%20average%20revisit%20times%20and%20path%20lengths%2C%20our%0AFaRe-CPP%20algorithm%20approximately%20showed%20a%2045%25%20and%2040%25%20reduction%2C%20respectively%2C%0Ain%20these%20highly%20relevant%20performance%20indicators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07343v1&entry.124074799=Read"},
{"title": "Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal\n  Violence Detection Method", "author": "Wenping Jin and Li Zhu and Jing Sun", "abstract": "  Weakly supervised violence detection refers to the technique of training\nmodels to identify violent segments in videos using only video-level labels.\nAmong these approaches, multimodal violence detection, which integrates\nmodalities such as audio and optical flow, holds great potential. Existing\nmethods in this domain primarily focus on designing multimodal fusion models to\naddress modality discrepancies. In contrast, we take a different approach;\nleveraging the inherent discrepancies across modalities in violence event\nrepresentation to propose a novel multimodal semantic feature alignment method.\nThis method sparsely maps the semantic features of local, transient, and less\ninformative modalities ( such as audio and optical flow ) into the more\ninformative RGB semantic feature space. Through an iterative process, the\nmethod identifies the suitable no-zero feature matching subspace and aligns the\nmodality-specific event representations based on this subspace, enabling the\nfull exploitation of information from all modalities during the subsequent\nmodality fusion stage. Building on this, we design a new weakly supervised\nviolence detection framework that consists of unimodal multiple-instance\nlearning for extracting unimodal semantic features, multimodal alignment,\nmultimodal fusion, and final detection. Experimental results on benchmark\ndatasets demonstrate the effectiveness of our method, achieving an average\nprecision (AP) of 86.07% on the XD-Violence dataset. Our code is available at\nhttps://github.com/xjpp2016/MAVD.\n", "link": "http://arxiv.org/abs/2501.07496v1", "date": "2025-01-13", "relevancy": 2.1918, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5552}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20First%2C%20Then%20Fusing%3A%20A%20Novel%20Weakly%20Supervised%20Multimodal%0A%20%20Violence%20Detection%20Method&body=Title%3A%20Aligning%20First%2C%20Then%20Fusing%3A%20A%20Novel%20Weakly%20Supervised%20Multimodal%0A%20%20Violence%20Detection%20Method%0AAuthor%3A%20Wenping%20Jin%20and%20Li%20Zhu%20and%20Jing%20Sun%0AAbstract%3A%20%20%20Weakly%20supervised%20violence%20detection%20refers%20to%20the%20technique%20of%20training%0Amodels%20to%20identify%20violent%20segments%20in%20videos%20using%20only%20video-level%20labels.%0AAmong%20these%20approaches%2C%20multimodal%20violence%20detection%2C%20which%20integrates%0Amodalities%20such%20as%20audio%20and%20optical%20flow%2C%20holds%20great%20potential.%20Existing%0Amethods%20in%20this%20domain%20primarily%20focus%20on%20designing%20multimodal%20fusion%20models%20to%0Aaddress%20modality%20discrepancies.%20In%20contrast%2C%20we%20take%20a%20different%20approach%3B%0Aleveraging%20the%20inherent%20discrepancies%20across%20modalities%20in%20violence%20event%0Arepresentation%20to%20propose%20a%20novel%20multimodal%20semantic%20feature%20alignment%20method.%0AThis%20method%20sparsely%20maps%20the%20semantic%20features%20of%20local%2C%20transient%2C%20and%20less%0Ainformative%20modalities%20%28%20such%20as%20audio%20and%20optical%20flow%20%29%20into%20the%20more%0Ainformative%20RGB%20semantic%20feature%20space.%20Through%20an%20iterative%20process%2C%20the%0Amethod%20identifies%20the%20suitable%20no-zero%20feature%20matching%20subspace%20and%20aligns%20the%0Amodality-specific%20event%20representations%20based%20on%20this%20subspace%2C%20enabling%20the%0Afull%20exploitation%20of%20information%20from%20all%20modalities%20during%20the%20subsequent%0Amodality%20fusion%20stage.%20Building%20on%20this%2C%20we%20design%20a%20new%20weakly%20supervised%0Aviolence%20detection%20framework%20that%20consists%20of%20unimodal%20multiple-instance%0Alearning%20for%20extracting%20unimodal%20semantic%20features%2C%20multimodal%20alignment%2C%0Amultimodal%20fusion%2C%20and%20final%20detection.%20Experimental%20results%20on%20benchmark%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20achieving%20an%20average%0Aprecision%20%28AP%29%20of%2086.07%25%20on%20the%20XD-Violence%20dataset.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xjpp2016/MAVD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520First%252C%2520Then%2520Fusing%253A%2520A%2520Novel%2520Weakly%2520Supervised%2520Multimodal%250A%2520%2520Violence%2520Detection%2520Method%26entry.906535625%3DWenping%2520Jin%2520and%2520Li%2520Zhu%2520and%2520Jing%2520Sun%26entry.1292438233%3D%2520%2520Weakly%2520supervised%2520violence%2520detection%2520refers%2520to%2520the%2520technique%2520of%2520training%250Amodels%2520to%2520identify%2520violent%2520segments%2520in%2520videos%2520using%2520only%2520video-level%2520labels.%250AAmong%2520these%2520approaches%252C%2520multimodal%2520violence%2520detection%252C%2520which%2520integrates%250Amodalities%2520such%2520as%2520audio%2520and%2520optical%2520flow%252C%2520holds%2520great%2520potential.%2520Existing%250Amethods%2520in%2520this%2520domain%2520primarily%2520focus%2520on%2520designing%2520multimodal%2520fusion%2520models%2520to%250Aaddress%2520modality%2520discrepancies.%2520In%2520contrast%252C%2520we%2520take%2520a%2520different%2520approach%253B%250Aleveraging%2520the%2520inherent%2520discrepancies%2520across%2520modalities%2520in%2520violence%2520event%250Arepresentation%2520to%2520propose%2520a%2520novel%2520multimodal%2520semantic%2520feature%2520alignment%2520method.%250AThis%2520method%2520sparsely%2520maps%2520the%2520semantic%2520features%2520of%2520local%252C%2520transient%252C%2520and%2520less%250Ainformative%2520modalities%2520%2528%2520such%2520as%2520audio%2520and%2520optical%2520flow%2520%2529%2520into%2520the%2520more%250Ainformative%2520RGB%2520semantic%2520feature%2520space.%2520Through%2520an%2520iterative%2520process%252C%2520the%250Amethod%2520identifies%2520the%2520suitable%2520no-zero%2520feature%2520matching%2520subspace%2520and%2520aligns%2520the%250Amodality-specific%2520event%2520representations%2520based%2520on%2520this%2520subspace%252C%2520enabling%2520the%250Afull%2520exploitation%2520of%2520information%2520from%2520all%2520modalities%2520during%2520the%2520subsequent%250Amodality%2520fusion%2520stage.%2520Building%2520on%2520this%252C%2520we%2520design%2520a%2520new%2520weakly%2520supervised%250Aviolence%2520detection%2520framework%2520that%2520consists%2520of%2520unimodal%2520multiple-instance%250Alearning%2520for%2520extracting%2520unimodal%2520semantic%2520features%252C%2520multimodal%2520alignment%252C%250Amultimodal%2520fusion%252C%2520and%2520final%2520detection.%2520Experimental%2520results%2520on%2520benchmark%250Adatasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520achieving%2520an%2520average%250Aprecision%2520%2528AP%2529%2520of%252086.07%2525%2520on%2520the%2520XD-Violence%2520dataset.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/xjpp2016/MAVD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20First%2C%20Then%20Fusing%3A%20A%20Novel%20Weakly%20Supervised%20Multimodal%0A%20%20Violence%20Detection%20Method&entry.906535625=Wenping%20Jin%20and%20Li%20Zhu%20and%20Jing%20Sun&entry.1292438233=%20%20Weakly%20supervised%20violence%20detection%20refers%20to%20the%20technique%20of%20training%0Amodels%20to%20identify%20violent%20segments%20in%20videos%20using%20only%20video-level%20labels.%0AAmong%20these%20approaches%2C%20multimodal%20violence%20detection%2C%20which%20integrates%0Amodalities%20such%20as%20audio%20and%20optical%20flow%2C%20holds%20great%20potential.%20Existing%0Amethods%20in%20this%20domain%20primarily%20focus%20on%20designing%20multimodal%20fusion%20models%20to%0Aaddress%20modality%20discrepancies.%20In%20contrast%2C%20we%20take%20a%20different%20approach%3B%0Aleveraging%20the%20inherent%20discrepancies%20across%20modalities%20in%20violence%20event%0Arepresentation%20to%20propose%20a%20novel%20multimodal%20semantic%20feature%20alignment%20method.%0AThis%20method%20sparsely%20maps%20the%20semantic%20features%20of%20local%2C%20transient%2C%20and%20less%0Ainformative%20modalities%20%28%20such%20as%20audio%20and%20optical%20flow%20%29%20into%20the%20more%0Ainformative%20RGB%20semantic%20feature%20space.%20Through%20an%20iterative%20process%2C%20the%0Amethod%20identifies%20the%20suitable%20no-zero%20feature%20matching%20subspace%20and%20aligns%20the%0Amodality-specific%20event%20representations%20based%20on%20this%20subspace%2C%20enabling%20the%0Afull%20exploitation%20of%20information%20from%20all%20modalities%20during%20the%20subsequent%0Amodality%20fusion%20stage.%20Building%20on%20this%2C%20we%20design%20a%20new%20weakly%20supervised%0Aviolence%20detection%20framework%20that%20consists%20of%20unimodal%20multiple-instance%0Alearning%20for%20extracting%20unimodal%20semantic%20features%2C%20multimodal%20alignment%2C%0Amultimodal%20fusion%2C%20and%20final%20detection.%20Experimental%20results%20on%20benchmark%0Adatasets%20demonstrate%20the%20effectiveness%20of%20our%20method%2C%20achieving%20an%20average%0Aprecision%20%28AP%29%20of%2086.07%25%20on%20the%20XD-Violence%20dataset.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xjpp2016/MAVD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07496v1&entry.124074799=Read"},
{"title": "Tiny Models are the Computational Saver for Large Models", "author": "Qingyuan Wang and Barry Cardiff and Antoine Frapp\u00e9 and Benoit Larras and Deepu John", "abstract": "  This paper introduces TinySaver, an early-exit-like dynamic model compression\napproach which employs tiny models to substitute large models adaptively.\nDistinct from traditional compression techniques, dynamic methods like\nTinySaver can leverage the difficulty differences to allow certain inputs to\ncomplete their inference processes early, thereby conserving computational\nresources. Most existing early exit designs are implemented by attaching\nadditional network branches to the model's backbone. Our study, however,\nreveals that completely independent tiny models can replace a substantial\nportion of the larger models' job with minimal impact on performance. Employing\nthem as the first exit can remarkably enhance computational efficiency. By\nsearching and employing the most appropriate tiny model as the computational\nsaver for a given large model, the proposed approaches work as a novel and\ngeneric method to model compression. This finding will help the research\ncommunity in exploring new compression methods to address the escalating\ncomputational demands posed by rapidly evolving AI models. Our evaluation of\nthis approach in ImageNet-1k classification demonstrates its potential to\nreduce the number of compute operations by up to 90\\%, with only negligible\nlosses in performance, across various modern vision models.\n", "link": "http://arxiv.org/abs/2403.17726v4", "date": "2025-01-13", "relevancy": 2.1814, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5681}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5498}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiny%20Models%20are%20the%20Computational%20Saver%20for%20Large%20Models&body=Title%3A%20Tiny%20Models%20are%20the%20Computational%20Saver%20for%20Large%20Models%0AAuthor%3A%20Qingyuan%20Wang%20and%20Barry%20Cardiff%20and%20Antoine%20Frapp%C3%A9%20and%20Benoit%20Larras%20and%20Deepu%20John%0AAbstract%3A%20%20%20This%20paper%20introduces%20TinySaver%2C%20an%20early-exit-like%20dynamic%20model%20compression%0Aapproach%20which%20employs%20tiny%20models%20to%20substitute%20large%20models%20adaptively.%0ADistinct%20from%20traditional%20compression%20techniques%2C%20dynamic%20methods%20like%0ATinySaver%20can%20leverage%20the%20difficulty%20differences%20to%20allow%20certain%20inputs%20to%0Acomplete%20their%20inference%20processes%20early%2C%20thereby%20conserving%20computational%0Aresources.%20Most%20existing%20early%20exit%20designs%20are%20implemented%20by%20attaching%0Aadditional%20network%20branches%20to%20the%20model%27s%20backbone.%20Our%20study%2C%20however%2C%0Areveals%20that%20completely%20independent%20tiny%20models%20can%20replace%20a%20substantial%0Aportion%20of%20the%20larger%20models%27%20job%20with%20minimal%20impact%20on%20performance.%20Employing%0Athem%20as%20the%20first%20exit%20can%20remarkably%20enhance%20computational%20efficiency.%20By%0Asearching%20and%20employing%20the%20most%20appropriate%20tiny%20model%20as%20the%20computational%0Asaver%20for%20a%20given%20large%20model%2C%20the%20proposed%20approaches%20work%20as%20a%20novel%20and%0Ageneric%20method%20to%20model%20compression.%20This%20finding%20will%20help%20the%20research%0Acommunity%20in%20exploring%20new%20compression%20methods%20to%20address%20the%20escalating%0Acomputational%20demands%20posed%20by%20rapidly%20evolving%20AI%20models.%20Our%20evaluation%20of%0Athis%20approach%20in%20ImageNet-1k%20classification%20demonstrates%20its%20potential%20to%0Areduce%20the%20number%20of%20compute%20operations%20by%20up%20to%2090%5C%25%2C%20with%20only%20negligible%0Alosses%20in%20performance%2C%20across%20various%20modern%20vision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17726v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiny%2520Models%2520are%2520the%2520Computational%2520Saver%2520for%2520Large%2520Models%26entry.906535625%3DQingyuan%2520Wang%2520and%2520Barry%2520Cardiff%2520and%2520Antoine%2520Frapp%25C3%25A9%2520and%2520Benoit%2520Larras%2520and%2520Deepu%2520John%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520TinySaver%252C%2520an%2520early-exit-like%2520dynamic%2520model%2520compression%250Aapproach%2520which%2520employs%2520tiny%2520models%2520to%2520substitute%2520large%2520models%2520adaptively.%250ADistinct%2520from%2520traditional%2520compression%2520techniques%252C%2520dynamic%2520methods%2520like%250ATinySaver%2520can%2520leverage%2520the%2520difficulty%2520differences%2520to%2520allow%2520certain%2520inputs%2520to%250Acomplete%2520their%2520inference%2520processes%2520early%252C%2520thereby%2520conserving%2520computational%250Aresources.%2520Most%2520existing%2520early%2520exit%2520designs%2520are%2520implemented%2520by%2520attaching%250Aadditional%2520network%2520branches%2520to%2520the%2520model%2527s%2520backbone.%2520Our%2520study%252C%2520however%252C%250Areveals%2520that%2520completely%2520independent%2520tiny%2520models%2520can%2520replace%2520a%2520substantial%250Aportion%2520of%2520the%2520larger%2520models%2527%2520job%2520with%2520minimal%2520impact%2520on%2520performance.%2520Employing%250Athem%2520as%2520the%2520first%2520exit%2520can%2520remarkably%2520enhance%2520computational%2520efficiency.%2520By%250Asearching%2520and%2520employing%2520the%2520most%2520appropriate%2520tiny%2520model%2520as%2520the%2520computational%250Asaver%2520for%2520a%2520given%2520large%2520model%252C%2520the%2520proposed%2520approaches%2520work%2520as%2520a%2520novel%2520and%250Ageneric%2520method%2520to%2520model%2520compression.%2520This%2520finding%2520will%2520help%2520the%2520research%250Acommunity%2520in%2520exploring%2520new%2520compression%2520methods%2520to%2520address%2520the%2520escalating%250Acomputational%2520demands%2520posed%2520by%2520rapidly%2520evolving%2520AI%2520models.%2520Our%2520evaluation%2520of%250Athis%2520approach%2520in%2520ImageNet-1k%2520classification%2520demonstrates%2520its%2520potential%2520to%250Areduce%2520the%2520number%2520of%2520compute%2520operations%2520by%2520up%2520to%252090%255C%2525%252C%2520with%2520only%2520negligible%250Alosses%2520in%2520performance%252C%2520across%2520various%2520modern%2520vision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17726v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiny%20Models%20are%20the%20Computational%20Saver%20for%20Large%20Models&entry.906535625=Qingyuan%20Wang%20and%20Barry%20Cardiff%20and%20Antoine%20Frapp%C3%A9%20and%20Benoit%20Larras%20and%20Deepu%20John&entry.1292438233=%20%20This%20paper%20introduces%20TinySaver%2C%20an%20early-exit-like%20dynamic%20model%20compression%0Aapproach%20which%20employs%20tiny%20models%20to%20substitute%20large%20models%20adaptively.%0ADistinct%20from%20traditional%20compression%20techniques%2C%20dynamic%20methods%20like%0ATinySaver%20can%20leverage%20the%20difficulty%20differences%20to%20allow%20certain%20inputs%20to%0Acomplete%20their%20inference%20processes%20early%2C%20thereby%20conserving%20computational%0Aresources.%20Most%20existing%20early%20exit%20designs%20are%20implemented%20by%20attaching%0Aadditional%20network%20branches%20to%20the%20model%27s%20backbone.%20Our%20study%2C%20however%2C%0Areveals%20that%20completely%20independent%20tiny%20models%20can%20replace%20a%20substantial%0Aportion%20of%20the%20larger%20models%27%20job%20with%20minimal%20impact%20on%20performance.%20Employing%0Athem%20as%20the%20first%20exit%20can%20remarkably%20enhance%20computational%20efficiency.%20By%0Asearching%20and%20employing%20the%20most%20appropriate%20tiny%20model%20as%20the%20computational%0Asaver%20for%20a%20given%20large%20model%2C%20the%20proposed%20approaches%20work%20as%20a%20novel%20and%0Ageneric%20method%20to%20model%20compression.%20This%20finding%20will%20help%20the%20research%0Acommunity%20in%20exploring%20new%20compression%20methods%20to%20address%20the%20escalating%0Acomputational%20demands%20posed%20by%20rapidly%20evolving%20AI%20models.%20Our%20evaluation%20of%0Athis%20approach%20in%20ImageNet-1k%20classification%20demonstrates%20its%20potential%20to%0Areduce%20the%20number%20of%20compute%20operations%20by%20up%20to%2090%5C%25%2C%20with%20only%20negligible%0Alosses%20in%20performance%2C%20across%20various%20modern%20vision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17726v4&entry.124074799=Read"},
{"title": "The importance of visual modelling languages in generative software\n  engineering", "author": "Roberto Rossi", "abstract": "  Multimodal GPTs represent a watershed in the interplay between Software\nEngineering and Generative Artificial Intelligence. GPT-4 accepts image and\ntext inputs, rather than simply natural language. We investigate relevant use\ncases stemming from these enhanced capabilities of GPT-4. To the best of our\nknowledge, no other work has investigated similar use cases involving Software\nEngineering tasks carried out via multimodal GPTs prompted with a mix of\ndiagrams and natural language.\n", "link": "http://arxiv.org/abs/2411.17976v3", "date": "2025-01-13", "relevancy": 2.1782, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5656}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5336}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5279}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20importance%20of%20visual%20modelling%20languages%20in%20generative%20software%0A%20%20engineering&body=Title%3A%20The%20importance%20of%20visual%20modelling%20languages%20in%20generative%20software%0A%20%20engineering%0AAuthor%3A%20Roberto%20Rossi%0AAbstract%3A%20%20%20Multimodal%20GPTs%20represent%20a%20watershed%20in%20the%20interplay%20between%20Software%0AEngineering%20and%20Generative%20Artificial%20Intelligence.%20GPT-4%20accepts%20image%20and%0Atext%20inputs%2C%20rather%20than%20simply%20natural%20language.%20We%20investigate%20relevant%20use%0Acases%20stemming%20from%20these%20enhanced%20capabilities%20of%20GPT-4.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20other%20work%20has%20investigated%20similar%20use%20cases%20involving%20Software%0AEngineering%20tasks%20carried%20out%20via%20multimodal%20GPTs%20prompted%20with%20a%20mix%20of%0Adiagrams%20and%20natural%20language.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17976v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520importance%2520of%2520visual%2520modelling%2520languages%2520in%2520generative%2520software%250A%2520%2520engineering%26entry.906535625%3DRoberto%2520Rossi%26entry.1292438233%3D%2520%2520Multimodal%2520GPTs%2520represent%2520a%2520watershed%2520in%2520the%2520interplay%2520between%2520Software%250AEngineering%2520and%2520Generative%2520Artificial%2520Intelligence.%2520GPT-4%2520accepts%2520image%2520and%250Atext%2520inputs%252C%2520rather%2520than%2520simply%2520natural%2520language.%2520We%2520investigate%2520relevant%2520use%250Acases%2520stemming%2520from%2520these%2520enhanced%2520capabilities%2520of%2520GPT-4.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520no%2520other%2520work%2520has%2520investigated%2520similar%2520use%2520cases%2520involving%2520Software%250AEngineering%2520tasks%2520carried%2520out%2520via%2520multimodal%2520GPTs%2520prompted%2520with%2520a%2520mix%2520of%250Adiagrams%2520and%2520natural%2520language.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17976v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20importance%20of%20visual%20modelling%20languages%20in%20generative%20software%0A%20%20engineering&entry.906535625=Roberto%20Rossi&entry.1292438233=%20%20Multimodal%20GPTs%20represent%20a%20watershed%20in%20the%20interplay%20between%20Software%0AEngineering%20and%20Generative%20Artificial%20Intelligence.%20GPT-4%20accepts%20image%20and%0Atext%20inputs%2C%20rather%20than%20simply%20natural%20language.%20We%20investigate%20relevant%20use%0Acases%20stemming%20from%20these%20enhanced%20capabilities%20of%20GPT-4.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20other%20work%20has%20investigated%20similar%20use%20cases%20involving%20Software%0AEngineering%20tasks%20carried%20out%20via%20multimodal%20GPTs%20prompted%20with%20a%20mix%20of%0Adiagrams%20and%20natural%20language.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17976v3&entry.124074799=Read"},
{"title": "Comparative analysis of optical character recognition methods for S\u00e1mi\n  texts from the National Library of Norway", "author": "Tita Enstad and Trond Trosterud and Marie Iversdatter R\u00f8sok and Yngvil Beyer and Marie Roald", "abstract": "  Optical Character Recognition (OCR) is crucial to the National Library of\nNorway's (NLN) digitisation process as it converts scanned documents into\nmachine-readable text. However, for the S\\'ami documents in NLN's collection,\nthe OCR accuracy is insufficient. Given that OCR quality affects downstream\nprocesses, evaluating and improving OCR for text written in S\\'ami languages is\nnecessary to make these resources accessible. To address this need, this work\nfine-tunes and evaluates three established OCR approaches, Transkribus,\nTesseract and TrOCR, for transcribing S\\'ami texts from NLN's collection. Our\nresults show that Transkribus and TrOCR outperform Tesseract on this task,\nwhile Tesseract achieves superior performance on an out-of-domain dataset.\nFurthermore, we show that fine-tuning pre-trained models and supplementing\nmanual annotations with machine annotations and synthetic text images can yield\naccurate OCR for S\\'ami languages, even with a moderate amount of manually\nannotated data.\n", "link": "http://arxiv.org/abs/2501.07300v1", "date": "2025-01-13", "relevancy": 2.1677, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20analysis%20of%20optical%20character%20recognition%20methods%20for%20S%C3%A1mi%0A%20%20texts%20from%20the%20National%20Library%20of%20Norway&body=Title%3A%20Comparative%20analysis%20of%20optical%20character%20recognition%20methods%20for%20S%C3%A1mi%0A%20%20texts%20from%20the%20National%20Library%20of%20Norway%0AAuthor%3A%20Tita%20Enstad%20and%20Trond%20Trosterud%20and%20Marie%20Iversdatter%20R%C3%B8sok%20and%20Yngvil%20Beyer%20and%20Marie%20Roald%0AAbstract%3A%20%20%20Optical%20Character%20Recognition%20%28OCR%29%20is%20crucial%20to%20the%20National%20Library%20of%0ANorway%27s%20%28NLN%29%20digitisation%20process%20as%20it%20converts%20scanned%20documents%20into%0Amachine-readable%20text.%20However%2C%20for%20the%20S%5C%27ami%20documents%20in%20NLN%27s%20collection%2C%0Athe%20OCR%20accuracy%20is%20insufficient.%20Given%20that%20OCR%20quality%20affects%20downstream%0Aprocesses%2C%20evaluating%20and%20improving%20OCR%20for%20text%20written%20in%20S%5C%27ami%20languages%20is%0Anecessary%20to%20make%20these%20resources%20accessible.%20To%20address%20this%20need%2C%20this%20work%0Afine-tunes%20and%20evaluates%20three%20established%20OCR%20approaches%2C%20Transkribus%2C%0ATesseract%20and%20TrOCR%2C%20for%20transcribing%20S%5C%27ami%20texts%20from%20NLN%27s%20collection.%20Our%0Aresults%20show%20that%20Transkribus%20and%20TrOCR%20outperform%20Tesseract%20on%20this%20task%2C%0Awhile%20Tesseract%20achieves%20superior%20performance%20on%20an%20out-of-domain%20dataset.%0AFurthermore%2C%20we%20show%20that%20fine-tuning%20pre-trained%20models%20and%20supplementing%0Amanual%20annotations%20with%20machine%20annotations%20and%20synthetic%20text%20images%20can%20yield%0Aaccurate%20OCR%20for%20S%5C%27ami%20languages%2C%20even%20with%20a%20moderate%20amount%20of%20manually%0Aannotated%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520analysis%2520of%2520optical%2520character%2520recognition%2520methods%2520for%2520S%25C3%25A1mi%250A%2520%2520texts%2520from%2520the%2520National%2520Library%2520of%2520Norway%26entry.906535625%3DTita%2520Enstad%2520and%2520Trond%2520Trosterud%2520and%2520Marie%2520Iversdatter%2520R%25C3%25B8sok%2520and%2520Yngvil%2520Beyer%2520and%2520Marie%2520Roald%26entry.1292438233%3D%2520%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%2520is%2520crucial%2520to%2520the%2520National%2520Library%2520of%250ANorway%2527s%2520%2528NLN%2529%2520digitisation%2520process%2520as%2520it%2520converts%2520scanned%2520documents%2520into%250Amachine-readable%2520text.%2520However%252C%2520for%2520the%2520S%255C%2527ami%2520documents%2520in%2520NLN%2527s%2520collection%252C%250Athe%2520OCR%2520accuracy%2520is%2520insufficient.%2520Given%2520that%2520OCR%2520quality%2520affects%2520downstream%250Aprocesses%252C%2520evaluating%2520and%2520improving%2520OCR%2520for%2520text%2520written%2520in%2520S%255C%2527ami%2520languages%2520is%250Anecessary%2520to%2520make%2520these%2520resources%2520accessible.%2520To%2520address%2520this%2520need%252C%2520this%2520work%250Afine-tunes%2520and%2520evaluates%2520three%2520established%2520OCR%2520approaches%252C%2520Transkribus%252C%250ATesseract%2520and%2520TrOCR%252C%2520for%2520transcribing%2520S%255C%2527ami%2520texts%2520from%2520NLN%2527s%2520collection.%2520Our%250Aresults%2520show%2520that%2520Transkribus%2520and%2520TrOCR%2520outperform%2520Tesseract%2520on%2520this%2520task%252C%250Awhile%2520Tesseract%2520achieves%2520superior%2520performance%2520on%2520an%2520out-of-domain%2520dataset.%250AFurthermore%252C%2520we%2520show%2520that%2520fine-tuning%2520pre-trained%2520models%2520and%2520supplementing%250Amanual%2520annotations%2520with%2520machine%2520annotations%2520and%2520synthetic%2520text%2520images%2520can%2520yield%250Aaccurate%2520OCR%2520for%2520S%255C%2527ami%2520languages%252C%2520even%2520with%2520a%2520moderate%2520amount%2520of%2520manually%250Aannotated%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20analysis%20of%20optical%20character%20recognition%20methods%20for%20S%C3%A1mi%0A%20%20texts%20from%20the%20National%20Library%20of%20Norway&entry.906535625=Tita%20Enstad%20and%20Trond%20Trosterud%20and%20Marie%20Iversdatter%20R%C3%B8sok%20and%20Yngvil%20Beyer%20and%20Marie%20Roald&entry.1292438233=%20%20Optical%20Character%20Recognition%20%28OCR%29%20is%20crucial%20to%20the%20National%20Library%20of%0ANorway%27s%20%28NLN%29%20digitisation%20process%20as%20it%20converts%20scanned%20documents%20into%0Amachine-readable%20text.%20However%2C%20for%20the%20S%5C%27ami%20documents%20in%20NLN%27s%20collection%2C%0Athe%20OCR%20accuracy%20is%20insufficient.%20Given%20that%20OCR%20quality%20affects%20downstream%0Aprocesses%2C%20evaluating%20and%20improving%20OCR%20for%20text%20written%20in%20S%5C%27ami%20languages%20is%0Anecessary%20to%20make%20these%20resources%20accessible.%20To%20address%20this%20need%2C%20this%20work%0Afine-tunes%20and%20evaluates%20three%20established%20OCR%20approaches%2C%20Transkribus%2C%0ATesseract%20and%20TrOCR%2C%20for%20transcribing%20S%5C%27ami%20texts%20from%20NLN%27s%20collection.%20Our%0Aresults%20show%20that%20Transkribus%20and%20TrOCR%20outperform%20Tesseract%20on%20this%20task%2C%0Awhile%20Tesseract%20achieves%20superior%20performance%20on%20an%20out-of-domain%20dataset.%0AFurthermore%2C%20we%20show%20that%20fine-tuning%20pre-trained%20models%20and%20supplementing%0Amanual%20annotations%20with%20machine%20annotations%20and%20synthetic%20text%20images%20can%20yield%0Aaccurate%20OCR%20for%20S%5C%27ami%20languages%2C%20even%20with%20a%20moderate%20amount%20of%20manually%0Aannotated%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07300v1&entry.124074799=Read"},
{"title": "Situational Scene Graph for Structured Human-centric Situation\n  Understanding", "author": "Chinthani Sugandhika and Chen Li and Deepu Rajan and Basura Fernando", "abstract": "  Graph based representation has been widely used in modelling spatio-temporal\nrelationships in video understanding. Although effective, existing graph-based\napproaches focus on capturing the human-object relationships while ignoring\nfine-grained semantic properties of the action components. These semantic\nproperties are crucial for understanding the current situation, such as where\ndoes the action takes place, what tools are used and functional properties of\nthe objects. In this work, we propose a graph-based representation called\nSituational Scene Graph (SSG) to encode both human-object relationships and the\ncorresponding semantic properties. The semantic details are represented as\npredefined roles and values inspired by situation frame, which is originally\ndesigned to represent a single action. Based on our proposed representation, we\nintroduce the task of situational scene graph generation and propose a\nmulti-stage pipeline Interactive and Complementary Network (InComNet) to\naddress the task. Given that the existing datasets are not applicable to the\ntask, we further introduce a SSG dataset whose annotations consist of semantic\nrole-value frames for human, objects and verb predicates of human-object\nrelations. Finally, we demonstrate the effectiveness of our proposed SSG\nrepresentation by testing on different downstream tasks. Experimental results\nshow that the unified representation can not only benefit predicate\nclassification and semantic role-value classification, but also benefit\nreasoning tasks on human-centric situation understanding. We will release the\ncode and the dataset soon.\n", "link": "http://arxiv.org/abs/2410.22829v2", "date": "2025-01-13", "relevancy": 2.1671, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6105}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Situational%20Scene%20Graph%20for%20Structured%20Human-centric%20Situation%0A%20%20Understanding&body=Title%3A%20Situational%20Scene%20Graph%20for%20Structured%20Human-centric%20Situation%0A%20%20Understanding%0AAuthor%3A%20Chinthani%20Sugandhika%20and%20Chen%20Li%20and%20Deepu%20Rajan%20and%20Basura%20Fernando%0AAbstract%3A%20%20%20Graph%20based%20representation%20has%20been%20widely%20used%20in%20modelling%20spatio-temporal%0Arelationships%20in%20video%20understanding.%20Although%20effective%2C%20existing%20graph-based%0Aapproaches%20focus%20on%20capturing%20the%20human-object%20relationships%20while%20ignoring%0Afine-grained%20semantic%20properties%20of%20the%20action%20components.%20These%20semantic%0Aproperties%20are%20crucial%20for%20understanding%20the%20current%20situation%2C%20such%20as%20where%0Adoes%20the%20action%20takes%20place%2C%20what%20tools%20are%20used%20and%20functional%20properties%20of%0Athe%20objects.%20In%20this%20work%2C%20we%20propose%20a%20graph-based%20representation%20called%0ASituational%20Scene%20Graph%20%28SSG%29%20to%20encode%20both%20human-object%20relationships%20and%20the%0Acorresponding%20semantic%20properties.%20The%20semantic%20details%20are%20represented%20as%0Apredefined%20roles%20and%20values%20inspired%20by%20situation%20frame%2C%20which%20is%20originally%0Adesigned%20to%20represent%20a%20single%20action.%20Based%20on%20our%20proposed%20representation%2C%20we%0Aintroduce%20the%20task%20of%20situational%20scene%20graph%20generation%20and%20propose%20a%0Amulti-stage%20pipeline%20Interactive%20and%20Complementary%20Network%20%28InComNet%29%20to%0Aaddress%20the%20task.%20Given%20that%20the%20existing%20datasets%20are%20not%20applicable%20to%20the%0Atask%2C%20we%20further%20introduce%20a%20SSG%20dataset%20whose%20annotations%20consist%20of%20semantic%0Arole-value%20frames%20for%20human%2C%20objects%20and%20verb%20predicates%20of%20human-object%0Arelations.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20SSG%0Arepresentation%20by%20testing%20on%20different%20downstream%20tasks.%20Experimental%20results%0Ashow%20that%20the%20unified%20representation%20can%20not%20only%20benefit%20predicate%0Aclassification%20and%20semantic%20role-value%20classification%2C%20but%20also%20benefit%0Areasoning%20tasks%20on%20human-centric%20situation%20understanding.%20We%20will%20release%20the%0Acode%20and%20the%20dataset%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22829v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSituational%2520Scene%2520Graph%2520for%2520Structured%2520Human-centric%2520Situation%250A%2520%2520Understanding%26entry.906535625%3DChinthani%2520Sugandhika%2520and%2520Chen%2520Li%2520and%2520Deepu%2520Rajan%2520and%2520Basura%2520Fernando%26entry.1292438233%3D%2520%2520Graph%2520based%2520representation%2520has%2520been%2520widely%2520used%2520in%2520modelling%2520spatio-temporal%250Arelationships%2520in%2520video%2520understanding.%2520Although%2520effective%252C%2520existing%2520graph-based%250Aapproaches%2520focus%2520on%2520capturing%2520the%2520human-object%2520relationships%2520while%2520ignoring%250Afine-grained%2520semantic%2520properties%2520of%2520the%2520action%2520components.%2520These%2520semantic%250Aproperties%2520are%2520crucial%2520for%2520understanding%2520the%2520current%2520situation%252C%2520such%2520as%2520where%250Adoes%2520the%2520action%2520takes%2520place%252C%2520what%2520tools%2520are%2520used%2520and%2520functional%2520properties%2520of%250Athe%2520objects.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520graph-based%2520representation%2520called%250ASituational%2520Scene%2520Graph%2520%2528SSG%2529%2520to%2520encode%2520both%2520human-object%2520relationships%2520and%2520the%250Acorresponding%2520semantic%2520properties.%2520The%2520semantic%2520details%2520are%2520represented%2520as%250Apredefined%2520roles%2520and%2520values%2520inspired%2520by%2520situation%2520frame%252C%2520which%2520is%2520originally%250Adesigned%2520to%2520represent%2520a%2520single%2520action.%2520Based%2520on%2520our%2520proposed%2520representation%252C%2520we%250Aintroduce%2520the%2520task%2520of%2520situational%2520scene%2520graph%2520generation%2520and%2520propose%2520a%250Amulti-stage%2520pipeline%2520Interactive%2520and%2520Complementary%2520Network%2520%2528InComNet%2529%2520to%250Aaddress%2520the%2520task.%2520Given%2520that%2520the%2520existing%2520datasets%2520are%2520not%2520applicable%2520to%2520the%250Atask%252C%2520we%2520further%2520introduce%2520a%2520SSG%2520dataset%2520whose%2520annotations%2520consist%2520of%2520semantic%250Arole-value%2520frames%2520for%2520human%252C%2520objects%2520and%2520verb%2520predicates%2520of%2520human-object%250Arelations.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520SSG%250Arepresentation%2520by%2520testing%2520on%2520different%2520downstream%2520tasks.%2520Experimental%2520results%250Ashow%2520that%2520the%2520unified%2520representation%2520can%2520not%2520only%2520benefit%2520predicate%250Aclassification%2520and%2520semantic%2520role-value%2520classification%252C%2520but%2520also%2520benefit%250Areasoning%2520tasks%2520on%2520human-centric%2520situation%2520understanding.%2520We%2520will%2520release%2520the%250Acode%2520and%2520the%2520dataset%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22829v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Situational%20Scene%20Graph%20for%20Structured%20Human-centric%20Situation%0A%20%20Understanding&entry.906535625=Chinthani%20Sugandhika%20and%20Chen%20Li%20and%20Deepu%20Rajan%20and%20Basura%20Fernando&entry.1292438233=%20%20Graph%20based%20representation%20has%20been%20widely%20used%20in%20modelling%20spatio-temporal%0Arelationships%20in%20video%20understanding.%20Although%20effective%2C%20existing%20graph-based%0Aapproaches%20focus%20on%20capturing%20the%20human-object%20relationships%20while%20ignoring%0Afine-grained%20semantic%20properties%20of%20the%20action%20components.%20These%20semantic%0Aproperties%20are%20crucial%20for%20understanding%20the%20current%20situation%2C%20such%20as%20where%0Adoes%20the%20action%20takes%20place%2C%20what%20tools%20are%20used%20and%20functional%20properties%20of%0Athe%20objects.%20In%20this%20work%2C%20we%20propose%20a%20graph-based%20representation%20called%0ASituational%20Scene%20Graph%20%28SSG%29%20to%20encode%20both%20human-object%20relationships%20and%20the%0Acorresponding%20semantic%20properties.%20The%20semantic%20details%20are%20represented%20as%0Apredefined%20roles%20and%20values%20inspired%20by%20situation%20frame%2C%20which%20is%20originally%0Adesigned%20to%20represent%20a%20single%20action.%20Based%20on%20our%20proposed%20representation%2C%20we%0Aintroduce%20the%20task%20of%20situational%20scene%20graph%20generation%20and%20propose%20a%0Amulti-stage%20pipeline%20Interactive%20and%20Complementary%20Network%20%28InComNet%29%20to%0Aaddress%20the%20task.%20Given%20that%20the%20existing%20datasets%20are%20not%20applicable%20to%20the%0Atask%2C%20we%20further%20introduce%20a%20SSG%20dataset%20whose%20annotations%20consist%20of%20semantic%0Arole-value%20frames%20for%20human%2C%20objects%20and%20verb%20predicates%20of%20human-object%0Arelations.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20SSG%0Arepresentation%20by%20testing%20on%20different%20downstream%20tasks.%20Experimental%20results%0Ashow%20that%20the%20unified%20representation%20can%20not%20only%20benefit%20predicate%0Aclassification%20and%20semantic%20role-value%20classification%2C%20but%20also%20benefit%0Areasoning%20tasks%20on%20human-centric%20situation%20understanding.%20We%20will%20release%20the%0Acode%20and%20the%20dataset%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22829v2&entry.124074799=Read"},
{"title": "Emergent effects of scaling on the functional hierarchies within large\n  language models", "author": "Paul C. Bogdan", "abstract": "  Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.\n", "link": "http://arxiv.org/abs/2501.07359v1", "date": "2025-01-13", "relevancy": 2.165, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5451}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20effects%20of%20scaling%20on%20the%20functional%20hierarchies%20within%20large%0A%20%20language%20models&body=Title%3A%20Emergent%20effects%20of%20scaling%20on%20the%20functional%20hierarchies%20within%20large%0A%20%20language%20models%0AAuthor%3A%20Paul%20C.%20Bogdan%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20architectures%20are%20often%20described%20as%20functionally%0Ahierarchical%3A%20Early%20layers%20process%20syntax%2C%20middle%20layers%20begin%20to%20parse%0Asemantics%2C%20and%20late%20layers%20integrate%20information.%20The%20present%20work%20revisits%0Athese%20ideas.%20This%20research%20submits%20simple%20texts%20to%20an%20LLM%20%28e.g.%2C%20%22A%20church%20and%0Aorgan%22%29%20and%20extracts%20the%20resulting%20activations.%20Then%2C%20for%20each%20layer%2C%20support%0Avector%20machines%20and%20ridge%20regressions%20are%20fit%20to%20predict%20a%20text%27s%20label%20and%0Athus%20examine%20whether%20a%20given%20layer%20encodes%20some%20information.%20Analyses%20using%20a%0Asmall%20model%20%28Llama-3.2-3b%3B%2028%20layers%29%20partly%20bolster%20the%20common%20hierarchical%0Aperspective%3A%20Item-level%20semantics%20are%20most%20strongly%20represented%20early%20%28layers%0A2-7%29%2C%20then%20two-item%20relations%20%28layers%208-12%29%2C%20and%20then%20four-item%20analogies%0A%28layers%2010-15%29.%20Afterward%2C%20the%20representation%20of%20items%20and%20simple%20relations%0Agradually%20decreases%20in%20deeper%20layers%20that%20focus%20on%20more%20global%20information.%0AHowever%2C%20several%20findings%20run%20counter%20to%20a%20steady%20hierarchy%20view%3A%20First%2C%0Aalthough%20deep%20layers%20can%20represent%20document-wide%20abstractions%2C%20deep%20layers%20also%0Acompress%20information%20from%20early%20portions%20of%20the%20context%20window%20without%0Ameaningful%20abstraction.%20Second%2C%20when%20examining%20a%20larger%20model%0A%28Llama-3.3-70b-Instruct%29%2C%20stark%20fluctuations%20in%20abstraction%20level%20appear%3A%20As%0Adepth%20increases%2C%20two-item%20relations%20and%20four-item%20analogies%20initially%20increase%0Ain%20their%20representation%2C%20then%20markedly%20decrease%2C%20and%20afterward%20increase%20again%0Amomentarily.%20This%20peculiar%20pattern%20consistently%20emerges%20across%20several%0Aexperiments.%20Third%2C%20another%20emergent%20effect%20of%20scaling%20is%20coordination%20between%0Athe%20attention%20mechanisms%20of%20adjacent%20layers.%20Across%20multiple%20experiments%20using%0Athe%20larger%20model%2C%20adjacent%20layers%20fluctuate%20between%20what%20information%20they%20each%0Aspecialize%20in%20representing.%20In%20sum%2C%20an%20abstraction%20hierarchy%20often%20manifests%0Aacross%20layers%2C%20but%20large%20models%20also%20deviate%20from%20this%20structure%20in%20curious%0Aways.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520effects%2520of%2520scaling%2520on%2520the%2520functional%2520hierarchies%2520within%2520large%250A%2520%2520language%2520models%26entry.906535625%3DPaul%2520C.%2520Bogdan%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520architectures%2520are%2520often%2520described%2520as%2520functionally%250Ahierarchical%253A%2520Early%2520layers%2520process%2520syntax%252C%2520middle%2520layers%2520begin%2520to%2520parse%250Asemantics%252C%2520and%2520late%2520layers%2520integrate%2520information.%2520The%2520present%2520work%2520revisits%250Athese%2520ideas.%2520This%2520research%2520submits%2520simple%2520texts%2520to%2520an%2520LLM%2520%2528e.g.%252C%2520%2522A%2520church%2520and%250Aorgan%2522%2529%2520and%2520extracts%2520the%2520resulting%2520activations.%2520Then%252C%2520for%2520each%2520layer%252C%2520support%250Avector%2520machines%2520and%2520ridge%2520regressions%2520are%2520fit%2520to%2520predict%2520a%2520text%2527s%2520label%2520and%250Athus%2520examine%2520whether%2520a%2520given%2520layer%2520encodes%2520some%2520information.%2520Analyses%2520using%2520a%250Asmall%2520model%2520%2528Llama-3.2-3b%253B%252028%2520layers%2529%2520partly%2520bolster%2520the%2520common%2520hierarchical%250Aperspective%253A%2520Item-level%2520semantics%2520are%2520most%2520strongly%2520represented%2520early%2520%2528layers%250A2-7%2529%252C%2520then%2520two-item%2520relations%2520%2528layers%25208-12%2529%252C%2520and%2520then%2520four-item%2520analogies%250A%2528layers%252010-15%2529.%2520Afterward%252C%2520the%2520representation%2520of%2520items%2520and%2520simple%2520relations%250Agradually%2520decreases%2520in%2520deeper%2520layers%2520that%2520focus%2520on%2520more%2520global%2520information.%250AHowever%252C%2520several%2520findings%2520run%2520counter%2520to%2520a%2520steady%2520hierarchy%2520view%253A%2520First%252C%250Aalthough%2520deep%2520layers%2520can%2520represent%2520document-wide%2520abstractions%252C%2520deep%2520layers%2520also%250Acompress%2520information%2520from%2520early%2520portions%2520of%2520the%2520context%2520window%2520without%250Ameaningful%2520abstraction.%2520Second%252C%2520when%2520examining%2520a%2520larger%2520model%250A%2528Llama-3.3-70b-Instruct%2529%252C%2520stark%2520fluctuations%2520in%2520abstraction%2520level%2520appear%253A%2520As%250Adepth%2520increases%252C%2520two-item%2520relations%2520and%2520four-item%2520analogies%2520initially%2520increase%250Ain%2520their%2520representation%252C%2520then%2520markedly%2520decrease%252C%2520and%2520afterward%2520increase%2520again%250Amomentarily.%2520This%2520peculiar%2520pattern%2520consistently%2520emerges%2520across%2520several%250Aexperiments.%2520Third%252C%2520another%2520emergent%2520effect%2520of%2520scaling%2520is%2520coordination%2520between%250Athe%2520attention%2520mechanisms%2520of%2520adjacent%2520layers.%2520Across%2520multiple%2520experiments%2520using%250Athe%2520larger%2520model%252C%2520adjacent%2520layers%2520fluctuate%2520between%2520what%2520information%2520they%2520each%250Aspecialize%2520in%2520representing.%2520In%2520sum%252C%2520an%2520abstraction%2520hierarchy%2520often%2520manifests%250Aacross%2520layers%252C%2520but%2520large%2520models%2520also%2520deviate%2520from%2520this%2520structure%2520in%2520curious%250Aways.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20effects%20of%20scaling%20on%20the%20functional%20hierarchies%20within%20large%0A%20%20language%20models&entry.906535625=Paul%20C.%20Bogdan&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20architectures%20are%20often%20described%20as%20functionally%0Ahierarchical%3A%20Early%20layers%20process%20syntax%2C%20middle%20layers%20begin%20to%20parse%0Asemantics%2C%20and%20late%20layers%20integrate%20information.%20The%20present%20work%20revisits%0Athese%20ideas.%20This%20research%20submits%20simple%20texts%20to%20an%20LLM%20%28e.g.%2C%20%22A%20church%20and%0Aorgan%22%29%20and%20extracts%20the%20resulting%20activations.%20Then%2C%20for%20each%20layer%2C%20support%0Avector%20machines%20and%20ridge%20regressions%20are%20fit%20to%20predict%20a%20text%27s%20label%20and%0Athus%20examine%20whether%20a%20given%20layer%20encodes%20some%20information.%20Analyses%20using%20a%0Asmall%20model%20%28Llama-3.2-3b%3B%2028%20layers%29%20partly%20bolster%20the%20common%20hierarchical%0Aperspective%3A%20Item-level%20semantics%20are%20most%20strongly%20represented%20early%20%28layers%0A2-7%29%2C%20then%20two-item%20relations%20%28layers%208-12%29%2C%20and%20then%20four-item%20analogies%0A%28layers%2010-15%29.%20Afterward%2C%20the%20representation%20of%20items%20and%20simple%20relations%0Agradually%20decreases%20in%20deeper%20layers%20that%20focus%20on%20more%20global%20information.%0AHowever%2C%20several%20findings%20run%20counter%20to%20a%20steady%20hierarchy%20view%3A%20First%2C%0Aalthough%20deep%20layers%20can%20represent%20document-wide%20abstractions%2C%20deep%20layers%20also%0Acompress%20information%20from%20early%20portions%20of%20the%20context%20window%20without%0Ameaningful%20abstraction.%20Second%2C%20when%20examining%20a%20larger%20model%0A%28Llama-3.3-70b-Instruct%29%2C%20stark%20fluctuations%20in%20abstraction%20level%20appear%3A%20As%0Adepth%20increases%2C%20two-item%20relations%20and%20four-item%20analogies%20initially%20increase%0Ain%20their%20representation%2C%20then%20markedly%20decrease%2C%20and%20afterward%20increase%20again%0Amomentarily.%20This%20peculiar%20pattern%20consistently%20emerges%20across%20several%0Aexperiments.%20Third%2C%20another%20emergent%20effect%20of%20scaling%20is%20coordination%20between%0Athe%20attention%20mechanisms%20of%20adjacent%20layers.%20Across%20multiple%20experiments%20using%0Athe%20larger%20model%2C%20adjacent%20layers%20fluctuate%20between%20what%20information%20they%20each%0Aspecialize%20in%20representing.%20In%20sum%2C%20an%20abstraction%20hierarchy%20often%20manifests%0Aacross%20layers%2C%20but%20large%20models%20also%20deviate%20from%20this%20structure%20in%20curious%0Aways.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07359v1&entry.124074799=Read"},
{"title": "A Unified Approach to Extract Interpretable Rules from Tree Ensembles\n  via Integer Programming", "author": "Lorenzo Bonasera and Emilio Carrizosa", "abstract": "  Tree ensembles are very popular machine learning models, known for their\neffectiveness in supervised classification and regression tasks. Their\nperformance derives from aggregating predictions of multiple decision trees,\nwhich are renowned for their interpretability properties. However, tree\nensemble models do not reliably exhibit interpretable output. Our work aims to\nextract an optimized list of rules from a trained tree ensemble, providing the\nuser with a condensed, interpretable model that retains most of the predictive\npower of the full model. Our approach consists of solving a set partitioning\nproblem formulated through Integer Programming. The proposed method works with\neither tabular or time series data, for both classification and regression\ntasks, and its flexible formulation can include any arbitrary loss or\nregularization functions. Our extensive computational experiments offer\nstatistically significant evidence that our method is competitive with other\nrule extraction methods in terms of predictive performance and fidelity towards\nthe tree ensemble. Moreover, we empirically show that the proposed method\neffectively extracts interpretable rules from tree ensemble that are designed\nfor time series data.\n", "link": "http://arxiv.org/abs/2407.00843v3", "date": "2025-01-13", "relevancy": 2.1648, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4403}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Approach%20to%20Extract%20Interpretable%20Rules%20from%20Tree%20Ensembles%0A%20%20via%20Integer%20Programming&body=Title%3A%20A%20Unified%20Approach%20to%20Extract%20Interpretable%20Rules%20from%20Tree%20Ensembles%0A%20%20via%20Integer%20Programming%0AAuthor%3A%20Lorenzo%20Bonasera%20and%20Emilio%20Carrizosa%0AAbstract%3A%20%20%20Tree%20ensembles%20are%20very%20popular%20machine%20learning%20models%2C%20known%20for%20their%0Aeffectiveness%20in%20supervised%20classification%20and%20regression%20tasks.%20Their%0Aperformance%20derives%20from%20aggregating%20predictions%20of%20multiple%20decision%20trees%2C%0Awhich%20are%20renowned%20for%20their%20interpretability%20properties.%20However%2C%20tree%0Aensemble%20models%20do%20not%20reliably%20exhibit%20interpretable%20output.%20Our%20work%20aims%20to%0Aextract%20an%20optimized%20list%20of%20rules%20from%20a%20trained%20tree%20ensemble%2C%20providing%20the%0Auser%20with%20a%20condensed%2C%20interpretable%20model%20that%20retains%20most%20of%20the%20predictive%0Apower%20of%20the%20full%20model.%20Our%20approach%20consists%20of%20solving%20a%20set%20partitioning%0Aproblem%20formulated%20through%20Integer%20Programming.%20The%20proposed%20method%20works%20with%0Aeither%20tabular%20or%20time%20series%20data%2C%20for%20both%20classification%20and%20regression%0Atasks%2C%20and%20its%20flexible%20formulation%20can%20include%20any%20arbitrary%20loss%20or%0Aregularization%20functions.%20Our%20extensive%20computational%20experiments%20offer%0Astatistically%20significant%20evidence%20that%20our%20method%20is%20competitive%20with%20other%0Arule%20extraction%20methods%20in%20terms%20of%20predictive%20performance%20and%20fidelity%20towards%0Athe%20tree%20ensemble.%20Moreover%2C%20we%20empirically%20show%20that%20the%20proposed%20method%0Aeffectively%20extracts%20interpretable%20rules%20from%20tree%20ensemble%20that%20are%20designed%0Afor%20time%20series%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00843v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Approach%2520to%2520Extract%2520Interpretable%2520Rules%2520from%2520Tree%2520Ensembles%250A%2520%2520via%2520Integer%2520Programming%26entry.906535625%3DLorenzo%2520Bonasera%2520and%2520Emilio%2520Carrizosa%26entry.1292438233%3D%2520%2520Tree%2520ensembles%2520are%2520very%2520popular%2520machine%2520learning%2520models%252C%2520known%2520for%2520their%250Aeffectiveness%2520in%2520supervised%2520classification%2520and%2520regression%2520tasks.%2520Their%250Aperformance%2520derives%2520from%2520aggregating%2520predictions%2520of%2520multiple%2520decision%2520trees%252C%250Awhich%2520are%2520renowned%2520for%2520their%2520interpretability%2520properties.%2520However%252C%2520tree%250Aensemble%2520models%2520do%2520not%2520reliably%2520exhibit%2520interpretable%2520output.%2520Our%2520work%2520aims%2520to%250Aextract%2520an%2520optimized%2520list%2520of%2520rules%2520from%2520a%2520trained%2520tree%2520ensemble%252C%2520providing%2520the%250Auser%2520with%2520a%2520condensed%252C%2520interpretable%2520model%2520that%2520retains%2520most%2520of%2520the%2520predictive%250Apower%2520of%2520the%2520full%2520model.%2520Our%2520approach%2520consists%2520of%2520solving%2520a%2520set%2520partitioning%250Aproblem%2520formulated%2520through%2520Integer%2520Programming.%2520The%2520proposed%2520method%2520works%2520with%250Aeither%2520tabular%2520or%2520time%2520series%2520data%252C%2520for%2520both%2520classification%2520and%2520regression%250Atasks%252C%2520and%2520its%2520flexible%2520formulation%2520can%2520include%2520any%2520arbitrary%2520loss%2520or%250Aregularization%2520functions.%2520Our%2520extensive%2520computational%2520experiments%2520offer%250Astatistically%2520significant%2520evidence%2520that%2520our%2520method%2520is%2520competitive%2520with%2520other%250Arule%2520extraction%2520methods%2520in%2520terms%2520of%2520predictive%2520performance%2520and%2520fidelity%2520towards%250Athe%2520tree%2520ensemble.%2520Moreover%252C%2520we%2520empirically%2520show%2520that%2520the%2520proposed%2520method%250Aeffectively%2520extracts%2520interpretable%2520rules%2520from%2520tree%2520ensemble%2520that%2520are%2520designed%250Afor%2520time%2520series%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00843v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Approach%20to%20Extract%20Interpretable%20Rules%20from%20Tree%20Ensembles%0A%20%20via%20Integer%20Programming&entry.906535625=Lorenzo%20Bonasera%20and%20Emilio%20Carrizosa&entry.1292438233=%20%20Tree%20ensembles%20are%20very%20popular%20machine%20learning%20models%2C%20known%20for%20their%0Aeffectiveness%20in%20supervised%20classification%20and%20regression%20tasks.%20Their%0Aperformance%20derives%20from%20aggregating%20predictions%20of%20multiple%20decision%20trees%2C%0Awhich%20are%20renowned%20for%20their%20interpretability%20properties.%20However%2C%20tree%0Aensemble%20models%20do%20not%20reliably%20exhibit%20interpretable%20output.%20Our%20work%20aims%20to%0Aextract%20an%20optimized%20list%20of%20rules%20from%20a%20trained%20tree%20ensemble%2C%20providing%20the%0Auser%20with%20a%20condensed%2C%20interpretable%20model%20that%20retains%20most%20of%20the%20predictive%0Apower%20of%20the%20full%20model.%20Our%20approach%20consists%20of%20solving%20a%20set%20partitioning%0Aproblem%20formulated%20through%20Integer%20Programming.%20The%20proposed%20method%20works%20with%0Aeither%20tabular%20or%20time%20series%20data%2C%20for%20both%20classification%20and%20regression%0Atasks%2C%20and%20its%20flexible%20formulation%20can%20include%20any%20arbitrary%20loss%20or%0Aregularization%20functions.%20Our%20extensive%20computational%20experiments%20offer%0Astatistically%20significant%20evidence%20that%20our%20method%20is%20competitive%20with%20other%0Arule%20extraction%20methods%20in%20terms%20of%20predictive%20performance%20and%20fidelity%20towards%0Athe%20tree%20ensemble.%20Moreover%2C%20we%20empirically%20show%20that%20the%20proposed%20method%0Aeffectively%20extracts%20interpretable%20rules%20from%20tree%20ensemble%20that%20are%20designed%0Afor%20time%20series%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00843v3&entry.124074799=Read"},
{"title": "RGB-D Indiscernible Object Counting in Underwater Scenes", "author": "Guolei Sun and Xiaogang Cheng and Zhaochong An and Xiaokang Wang and Yun Liu and Deng-Ping Fan and Ming-Ming Cheng and Luc Van Gool", "abstract": "  Recently, indiscernible/camouflaged scene understanding has attracted lots of\nresearch attention in the vision community. We further advance the frontier of\nthis field by systematically studying a new challenge named indiscernible\nobject counting (IOC), the goal of which is to count objects that are blended\nwith respect to their surroundings. Due to a lack of appropriate IOC datasets,\nwe present a large-scale dataset IOCfish5K which contains a total of 5,637\nhigh-resolution images and 659,024 annotated center points. Our dataset\nconsists of a large number of indiscernible objects (mainly fish) in underwater\nscenes, making the annotation process all the more challenging. IOCfish5K is\nsuperior to existing datasets with indiscernible scenes because of its larger\nscale, higher image resolutions, more annotations, and denser scenes. All these\naspects make it the most challenging dataset for IOC so far, supporting\nprogress in this area. Benefiting from the recent advancements of depth\nestimation foundation models, we construct high-quality depth maps for\nIOCfish5K by generating pseudo labels using the Depth Anything V2 model. The\nRGB-D version of IOCfish5K is named IOCfish5K-D. For benchmarking purposes on\nIOCfish5K, we select 14 mainstream methods for object counting and carefully\nevaluate them. For multimodal IOCfish5K-D, we evaluate other 4 popular\nmultimodal counting methods. Furthermore, we propose IOCFormer, a new strong\nbaseline that combines density and regression branches in a unified framework\nand can effectively tackle object counting under concealed scenes. We also\npropose IOCFormer-D to enable the effective usage of depth modality in helping\ndetect and count objects hidden in their environments. Experiments show that\nIOCFormer and IOCFormer-D achieve state-of-the-art scores on IOCfish5K and\nIOCfish5K-D, respectively.\n", "link": "http://arxiv.org/abs/2304.11677v2", "date": "2025-01-13", "relevancy": 2.1574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5414}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-D%20Indiscernible%20Object%20Counting%20in%20Underwater%20Scenes&body=Title%3A%20RGB-D%20Indiscernible%20Object%20Counting%20in%20Underwater%20Scenes%0AAuthor%3A%20Guolei%20Sun%20and%20Xiaogang%20Cheng%20and%20Zhaochong%20An%20and%20Xiaokang%20Wang%20and%20Yun%20Liu%20and%20Deng-Ping%20Fan%20and%20Ming-Ming%20Cheng%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Recently%2C%20indiscernible/camouflaged%20scene%20understanding%20has%20attracted%20lots%20of%0Aresearch%20attention%20in%20the%20vision%20community.%20We%20further%20advance%20the%20frontier%20of%0Athis%20field%20by%20systematically%20studying%20a%20new%20challenge%20named%20indiscernible%0Aobject%20counting%20%28IOC%29%2C%20the%20goal%20of%20which%20is%20to%20count%20objects%20that%20are%20blended%0Awith%20respect%20to%20their%20surroundings.%20Due%20to%20a%20lack%20of%20appropriate%20IOC%20datasets%2C%0Awe%20present%20a%20large-scale%20dataset%20IOCfish5K%20which%20contains%20a%20total%20of%205%2C637%0Ahigh-resolution%20images%20and%20659%2C024%20annotated%20center%20points.%20Our%20dataset%0Aconsists%20of%20a%20large%20number%20of%20indiscernible%20objects%20%28mainly%20fish%29%20in%20underwater%0Ascenes%2C%20making%20the%20annotation%20process%20all%20the%20more%20challenging.%20IOCfish5K%20is%0Asuperior%20to%20existing%20datasets%20with%20indiscernible%20scenes%20because%20of%20its%20larger%0Ascale%2C%20higher%20image%20resolutions%2C%20more%20annotations%2C%20and%20denser%20scenes.%20All%20these%0Aaspects%20make%20it%20the%20most%20challenging%20dataset%20for%20IOC%20so%20far%2C%20supporting%0Aprogress%20in%20this%20area.%20Benefiting%20from%20the%20recent%20advancements%20of%20depth%0Aestimation%20foundation%20models%2C%20we%20construct%20high-quality%20depth%20maps%20for%0AIOCfish5K%20by%20generating%20pseudo%20labels%20using%20the%20Depth%20Anything%20V2%20model.%20The%0ARGB-D%20version%20of%20IOCfish5K%20is%20named%20IOCfish5K-D.%20For%20benchmarking%20purposes%20on%0AIOCfish5K%2C%20we%20select%2014%20mainstream%20methods%20for%20object%20counting%20and%20carefully%0Aevaluate%20them.%20For%20multimodal%20IOCfish5K-D%2C%20we%20evaluate%20other%204%20popular%0Amultimodal%20counting%20methods.%20Furthermore%2C%20we%20propose%20IOCFormer%2C%20a%20new%20strong%0Abaseline%20that%20combines%20density%20and%20regression%20branches%20in%20a%20unified%20framework%0Aand%20can%20effectively%20tackle%20object%20counting%20under%20concealed%20scenes.%20We%20also%0Apropose%20IOCFormer-D%20to%20enable%20the%20effective%20usage%20of%20depth%20modality%20in%20helping%0Adetect%20and%20count%20objects%20hidden%20in%20their%20environments.%20Experiments%20show%20that%0AIOCFormer%20and%20IOCFormer-D%20achieve%20state-of-the-art%20scores%20on%20IOCfish5K%20and%0AIOCfish5K-D%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.11677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-D%2520Indiscernible%2520Object%2520Counting%2520in%2520Underwater%2520Scenes%26entry.906535625%3DGuolei%2520Sun%2520and%2520Xiaogang%2520Cheng%2520and%2520Zhaochong%2520An%2520and%2520Xiaokang%2520Wang%2520and%2520Yun%2520Liu%2520and%2520Deng-Ping%2520Fan%2520and%2520Ming-Ming%2520Cheng%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Recently%252C%2520indiscernible/camouflaged%2520scene%2520understanding%2520has%2520attracted%2520lots%2520of%250Aresearch%2520attention%2520in%2520the%2520vision%2520community.%2520We%2520further%2520advance%2520the%2520frontier%2520of%250Athis%2520field%2520by%2520systematically%2520studying%2520a%2520new%2520challenge%2520named%2520indiscernible%250Aobject%2520counting%2520%2528IOC%2529%252C%2520the%2520goal%2520of%2520which%2520is%2520to%2520count%2520objects%2520that%2520are%2520blended%250Awith%2520respect%2520to%2520their%2520surroundings.%2520Due%2520to%2520a%2520lack%2520of%2520appropriate%2520IOC%2520datasets%252C%250Awe%2520present%2520a%2520large-scale%2520dataset%2520IOCfish5K%2520which%2520contains%2520a%2520total%2520of%25205%252C637%250Ahigh-resolution%2520images%2520and%2520659%252C024%2520annotated%2520center%2520points.%2520Our%2520dataset%250Aconsists%2520of%2520a%2520large%2520number%2520of%2520indiscernible%2520objects%2520%2528mainly%2520fish%2529%2520in%2520underwater%250Ascenes%252C%2520making%2520the%2520annotation%2520process%2520all%2520the%2520more%2520challenging.%2520IOCfish5K%2520is%250Asuperior%2520to%2520existing%2520datasets%2520with%2520indiscernible%2520scenes%2520because%2520of%2520its%2520larger%250Ascale%252C%2520higher%2520image%2520resolutions%252C%2520more%2520annotations%252C%2520and%2520denser%2520scenes.%2520All%2520these%250Aaspects%2520make%2520it%2520the%2520most%2520challenging%2520dataset%2520for%2520IOC%2520so%2520far%252C%2520supporting%250Aprogress%2520in%2520this%2520area.%2520Benefiting%2520from%2520the%2520recent%2520advancements%2520of%2520depth%250Aestimation%2520foundation%2520models%252C%2520we%2520construct%2520high-quality%2520depth%2520maps%2520for%250AIOCfish5K%2520by%2520generating%2520pseudo%2520labels%2520using%2520the%2520Depth%2520Anything%2520V2%2520model.%2520The%250ARGB-D%2520version%2520of%2520IOCfish5K%2520is%2520named%2520IOCfish5K-D.%2520For%2520benchmarking%2520purposes%2520on%250AIOCfish5K%252C%2520we%2520select%252014%2520mainstream%2520methods%2520for%2520object%2520counting%2520and%2520carefully%250Aevaluate%2520them.%2520For%2520multimodal%2520IOCfish5K-D%252C%2520we%2520evaluate%2520other%25204%2520popular%250Amultimodal%2520counting%2520methods.%2520Furthermore%252C%2520we%2520propose%2520IOCFormer%252C%2520a%2520new%2520strong%250Abaseline%2520that%2520combines%2520density%2520and%2520regression%2520branches%2520in%2520a%2520unified%2520framework%250Aand%2520can%2520effectively%2520tackle%2520object%2520counting%2520under%2520concealed%2520scenes.%2520We%2520also%250Apropose%2520IOCFormer-D%2520to%2520enable%2520the%2520effective%2520usage%2520of%2520depth%2520modality%2520in%2520helping%250Adetect%2520and%2520count%2520objects%2520hidden%2520in%2520their%2520environments.%2520Experiments%2520show%2520that%250AIOCFormer%2520and%2520IOCFormer-D%2520achieve%2520state-of-the-art%2520scores%2520on%2520IOCfish5K%2520and%250AIOCfish5K-D%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.11677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-D%20Indiscernible%20Object%20Counting%20in%20Underwater%20Scenes&entry.906535625=Guolei%20Sun%20and%20Xiaogang%20Cheng%20and%20Zhaochong%20An%20and%20Xiaokang%20Wang%20and%20Yun%20Liu%20and%20Deng-Ping%20Fan%20and%20Ming-Ming%20Cheng%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Recently%2C%20indiscernible/camouflaged%20scene%20understanding%20has%20attracted%20lots%20of%0Aresearch%20attention%20in%20the%20vision%20community.%20We%20further%20advance%20the%20frontier%20of%0Athis%20field%20by%20systematically%20studying%20a%20new%20challenge%20named%20indiscernible%0Aobject%20counting%20%28IOC%29%2C%20the%20goal%20of%20which%20is%20to%20count%20objects%20that%20are%20blended%0Awith%20respect%20to%20their%20surroundings.%20Due%20to%20a%20lack%20of%20appropriate%20IOC%20datasets%2C%0Awe%20present%20a%20large-scale%20dataset%20IOCfish5K%20which%20contains%20a%20total%20of%205%2C637%0Ahigh-resolution%20images%20and%20659%2C024%20annotated%20center%20points.%20Our%20dataset%0Aconsists%20of%20a%20large%20number%20of%20indiscernible%20objects%20%28mainly%20fish%29%20in%20underwater%0Ascenes%2C%20making%20the%20annotation%20process%20all%20the%20more%20challenging.%20IOCfish5K%20is%0Asuperior%20to%20existing%20datasets%20with%20indiscernible%20scenes%20because%20of%20its%20larger%0Ascale%2C%20higher%20image%20resolutions%2C%20more%20annotations%2C%20and%20denser%20scenes.%20All%20these%0Aaspects%20make%20it%20the%20most%20challenging%20dataset%20for%20IOC%20so%20far%2C%20supporting%0Aprogress%20in%20this%20area.%20Benefiting%20from%20the%20recent%20advancements%20of%20depth%0Aestimation%20foundation%20models%2C%20we%20construct%20high-quality%20depth%20maps%20for%0AIOCfish5K%20by%20generating%20pseudo%20labels%20using%20the%20Depth%20Anything%20V2%20model.%20The%0ARGB-D%20version%20of%20IOCfish5K%20is%20named%20IOCfish5K-D.%20For%20benchmarking%20purposes%20on%0AIOCfish5K%2C%20we%20select%2014%20mainstream%20methods%20for%20object%20counting%20and%20carefully%0Aevaluate%20them.%20For%20multimodal%20IOCfish5K-D%2C%20we%20evaluate%20other%204%20popular%0Amultimodal%20counting%20methods.%20Furthermore%2C%20we%20propose%20IOCFormer%2C%20a%20new%20strong%0Abaseline%20that%20combines%20density%20and%20regression%20branches%20in%20a%20unified%20framework%0Aand%20can%20effectively%20tackle%20object%20counting%20under%20concealed%20scenes.%20We%20also%0Apropose%20IOCFormer-D%20to%20enable%20the%20effective%20usage%20of%20depth%20modality%20in%20helping%0Adetect%20and%20count%20objects%20hidden%20in%20their%20environments.%20Experiments%20show%20that%0AIOCFormer%20and%20IOCFormer-D%20achieve%20state-of-the-art%20scores%20on%20IOCfish5K%20and%0AIOCfish5K-D%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.11677v2&entry.124074799=Read"},
{"title": "Toward Realistic Camouflaged Object Detection: Benchmarks and Method", "author": "Zhimeng Xin and Tianxu Wu and Shiming Chen and Shuo Ye and Zijing Xie and Yixiong Zou and Xinge You and Yufei Guo", "abstract": "  Camouflaged object detection (COD) primarily relies on semantic or instance\nsegmentation methods. While these methods have made significant advancements in\nidentifying the contours of camouflaged objects, they may be inefficient or\ncost-effective for tasks that only require the specific location of the object.\nObject detection algorithms offer an optimized solution for Realistic\nCamouflaged Object Detection (RCOD) in such cases. However, detecting\ncamouflaged objects remains a formidable challenge due to the high degree of\nsimilarity between the features of the objects and their backgrounds. Unlike\nsegmentation methods that perform pixel-wise comparisons to differentiate\nbetween foreground and background, object detectors omit this analysis, further\naggravating the challenge. To solve this problem, we propose a camouflage-aware\nfeature refinement (CAFR) strategy. Since camouflaged objects are not rare\ncategories, CAFR fully utilizes a clear perception of the current object within\nthe prior knowledge of large models to assist detectors in deeply understanding\nthe distinctions between background and foreground. Specifically, in CAFR, we\nintroduce the Adaptive Gradient Propagation (AGP) module that fine-tunes all\nfeature extractor layers in large detection models to fully refine\nclass-specific features from camouflaged contexts. We then design the Sparse\nFeature Refinement (SFR) module that optimizes the transformer-based feature\nextractor to focus primarily on capturing class-specific features in\ncamouflaged scenarios. To facilitate the assessment of RCOD tasks, we manually\nannotate the labels required for detection on three existing segmentation COD\ndatasets, creating a new benchmark for RCOD tasks. Code and datasets are\navailable at: https://github.com/zhimengXin/RCOD.\n", "link": "http://arxiv.org/abs/2501.07297v1", "date": "2025-01-13", "relevancy": 2.1441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5382}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Realistic%20Camouflaged%20Object%20Detection%3A%20Benchmarks%20and%20Method&body=Title%3A%20Toward%20Realistic%20Camouflaged%20Object%20Detection%3A%20Benchmarks%20and%20Method%0AAuthor%3A%20Zhimeng%20Xin%20and%20Tianxu%20Wu%20and%20Shiming%20Chen%20and%20Shuo%20Ye%20and%20Zijing%20Xie%20and%20Yixiong%20Zou%20and%20Xinge%20You%20and%20Yufei%20Guo%0AAbstract%3A%20%20%20Camouflaged%20object%20detection%20%28COD%29%20primarily%20relies%20on%20semantic%20or%20instance%0Asegmentation%20methods.%20While%20these%20methods%20have%20made%20significant%20advancements%20in%0Aidentifying%20the%20contours%20of%20camouflaged%20objects%2C%20they%20may%20be%20inefficient%20or%0Acost-effective%20for%20tasks%20that%20only%20require%20the%20specific%20location%20of%20the%20object.%0AObject%20detection%20algorithms%20offer%20an%20optimized%20solution%20for%20Realistic%0ACamouflaged%20Object%20Detection%20%28RCOD%29%20in%20such%20cases.%20However%2C%20detecting%0Acamouflaged%20objects%20remains%20a%20formidable%20challenge%20due%20to%20the%20high%20degree%20of%0Asimilarity%20between%20the%20features%20of%20the%20objects%20and%20their%20backgrounds.%20Unlike%0Asegmentation%20methods%20that%20perform%20pixel-wise%20comparisons%20to%20differentiate%0Abetween%20foreground%20and%20background%2C%20object%20detectors%20omit%20this%20analysis%2C%20further%0Aaggravating%20the%20challenge.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20camouflage-aware%0Afeature%20refinement%20%28CAFR%29%20strategy.%20Since%20camouflaged%20objects%20are%20not%20rare%0Acategories%2C%20CAFR%20fully%20utilizes%20a%20clear%20perception%20of%20the%20current%20object%20within%0Athe%20prior%20knowledge%20of%20large%20models%20to%20assist%20detectors%20in%20deeply%20understanding%0Athe%20distinctions%20between%20background%20and%20foreground.%20Specifically%2C%20in%20CAFR%2C%20we%0Aintroduce%20the%20Adaptive%20Gradient%20Propagation%20%28AGP%29%20module%20that%20fine-tunes%20all%0Afeature%20extractor%20layers%20in%20large%20detection%20models%20to%20fully%20refine%0Aclass-specific%20features%20from%20camouflaged%20contexts.%20We%20then%20design%20the%20Sparse%0AFeature%20Refinement%20%28SFR%29%20module%20that%20optimizes%20the%20transformer-based%20feature%0Aextractor%20to%20focus%20primarily%20on%20capturing%20class-specific%20features%20in%0Acamouflaged%20scenarios.%20To%20facilitate%20the%20assessment%20of%20RCOD%20tasks%2C%20we%20manually%0Aannotate%20the%20labels%20required%20for%20detection%20on%20three%20existing%20segmentation%20COD%0Adatasets%2C%20creating%20a%20new%20benchmark%20for%20RCOD%20tasks.%20Code%20and%20datasets%20are%0Aavailable%20at%3A%20https%3A//github.com/zhimengXin/RCOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Realistic%2520Camouflaged%2520Object%2520Detection%253A%2520Benchmarks%2520and%2520Method%26entry.906535625%3DZhimeng%2520Xin%2520and%2520Tianxu%2520Wu%2520and%2520Shiming%2520Chen%2520and%2520Shuo%2520Ye%2520and%2520Zijing%2520Xie%2520and%2520Yixiong%2520Zou%2520and%2520Xinge%2520You%2520and%2520Yufei%2520Guo%26entry.1292438233%3D%2520%2520Camouflaged%2520object%2520detection%2520%2528COD%2529%2520primarily%2520relies%2520on%2520semantic%2520or%2520instance%250Asegmentation%2520methods.%2520While%2520these%2520methods%2520have%2520made%2520significant%2520advancements%2520in%250Aidentifying%2520the%2520contours%2520of%2520camouflaged%2520objects%252C%2520they%2520may%2520be%2520inefficient%2520or%250Acost-effective%2520for%2520tasks%2520that%2520only%2520require%2520the%2520specific%2520location%2520of%2520the%2520object.%250AObject%2520detection%2520algorithms%2520offer%2520an%2520optimized%2520solution%2520for%2520Realistic%250ACamouflaged%2520Object%2520Detection%2520%2528RCOD%2529%2520in%2520such%2520cases.%2520However%252C%2520detecting%250Acamouflaged%2520objects%2520remains%2520a%2520formidable%2520challenge%2520due%2520to%2520the%2520high%2520degree%2520of%250Asimilarity%2520between%2520the%2520features%2520of%2520the%2520objects%2520and%2520their%2520backgrounds.%2520Unlike%250Asegmentation%2520methods%2520that%2520perform%2520pixel-wise%2520comparisons%2520to%2520differentiate%250Abetween%2520foreground%2520and%2520background%252C%2520object%2520detectors%2520omit%2520this%2520analysis%252C%2520further%250Aaggravating%2520the%2520challenge.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%2520a%2520camouflage-aware%250Afeature%2520refinement%2520%2528CAFR%2529%2520strategy.%2520Since%2520camouflaged%2520objects%2520are%2520not%2520rare%250Acategories%252C%2520CAFR%2520fully%2520utilizes%2520a%2520clear%2520perception%2520of%2520the%2520current%2520object%2520within%250Athe%2520prior%2520knowledge%2520of%2520large%2520models%2520to%2520assist%2520detectors%2520in%2520deeply%2520understanding%250Athe%2520distinctions%2520between%2520background%2520and%2520foreground.%2520Specifically%252C%2520in%2520CAFR%252C%2520we%250Aintroduce%2520the%2520Adaptive%2520Gradient%2520Propagation%2520%2528AGP%2529%2520module%2520that%2520fine-tunes%2520all%250Afeature%2520extractor%2520layers%2520in%2520large%2520detection%2520models%2520to%2520fully%2520refine%250Aclass-specific%2520features%2520from%2520camouflaged%2520contexts.%2520We%2520then%2520design%2520the%2520Sparse%250AFeature%2520Refinement%2520%2528SFR%2529%2520module%2520that%2520optimizes%2520the%2520transformer-based%2520feature%250Aextractor%2520to%2520focus%2520primarily%2520on%2520capturing%2520class-specific%2520features%2520in%250Acamouflaged%2520scenarios.%2520To%2520facilitate%2520the%2520assessment%2520of%2520RCOD%2520tasks%252C%2520we%2520manually%250Aannotate%2520the%2520labels%2520required%2520for%2520detection%2520on%2520three%2520existing%2520segmentation%2520COD%250Adatasets%252C%2520creating%2520a%2520new%2520benchmark%2520for%2520RCOD%2520tasks.%2520Code%2520and%2520datasets%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/zhimengXin/RCOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Realistic%20Camouflaged%20Object%20Detection%3A%20Benchmarks%20and%20Method&entry.906535625=Zhimeng%20Xin%20and%20Tianxu%20Wu%20and%20Shiming%20Chen%20and%20Shuo%20Ye%20and%20Zijing%20Xie%20and%20Yixiong%20Zou%20and%20Xinge%20You%20and%20Yufei%20Guo&entry.1292438233=%20%20Camouflaged%20object%20detection%20%28COD%29%20primarily%20relies%20on%20semantic%20or%20instance%0Asegmentation%20methods.%20While%20these%20methods%20have%20made%20significant%20advancements%20in%0Aidentifying%20the%20contours%20of%20camouflaged%20objects%2C%20they%20may%20be%20inefficient%20or%0Acost-effective%20for%20tasks%20that%20only%20require%20the%20specific%20location%20of%20the%20object.%0AObject%20detection%20algorithms%20offer%20an%20optimized%20solution%20for%20Realistic%0ACamouflaged%20Object%20Detection%20%28RCOD%29%20in%20such%20cases.%20However%2C%20detecting%0Acamouflaged%20objects%20remains%20a%20formidable%20challenge%20due%20to%20the%20high%20degree%20of%0Asimilarity%20between%20the%20features%20of%20the%20objects%20and%20their%20backgrounds.%20Unlike%0Asegmentation%20methods%20that%20perform%20pixel-wise%20comparisons%20to%20differentiate%0Abetween%20foreground%20and%20background%2C%20object%20detectors%20omit%20this%20analysis%2C%20further%0Aaggravating%20the%20challenge.%20To%20solve%20this%20problem%2C%20we%20propose%20a%20camouflage-aware%0Afeature%20refinement%20%28CAFR%29%20strategy.%20Since%20camouflaged%20objects%20are%20not%20rare%0Acategories%2C%20CAFR%20fully%20utilizes%20a%20clear%20perception%20of%20the%20current%20object%20within%0Athe%20prior%20knowledge%20of%20large%20models%20to%20assist%20detectors%20in%20deeply%20understanding%0Athe%20distinctions%20between%20background%20and%20foreground.%20Specifically%2C%20in%20CAFR%2C%20we%0Aintroduce%20the%20Adaptive%20Gradient%20Propagation%20%28AGP%29%20module%20that%20fine-tunes%20all%0Afeature%20extractor%20layers%20in%20large%20detection%20models%20to%20fully%20refine%0Aclass-specific%20features%20from%20camouflaged%20contexts.%20We%20then%20design%20the%20Sparse%0AFeature%20Refinement%20%28SFR%29%20module%20that%20optimizes%20the%20transformer-based%20feature%0Aextractor%20to%20focus%20primarily%20on%20capturing%20class-specific%20features%20in%0Acamouflaged%20scenarios.%20To%20facilitate%20the%20assessment%20of%20RCOD%20tasks%2C%20we%20manually%0Aannotate%20the%20labels%20required%20for%20detection%20on%20three%20existing%20segmentation%20COD%0Adatasets%2C%20creating%20a%20new%20benchmark%20for%20RCOD%20tasks.%20Code%20and%20datasets%20are%0Aavailable%20at%3A%20https%3A//github.com/zhimengXin/RCOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07297v1&entry.124074799=Read"},
{"title": "Efficient Large Foundation Models Design: A Perspective From Model and\n  System Co-Design", "author": "Dong Liu and Yanxuan Yu and Zhixin Lai and Yite Wang and Jing Wu and Zhongwei Wan and Sina Alinejad and Benjamin Lengerich and Ying Nian Wu", "abstract": "  This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\n\\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}\n", "link": "http://arxiv.org/abs/2409.01990v3", "date": "2025-01-13", "relevancy": 2.1378, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Large%20Foundation%20Models%20Design%3A%20A%20Perspective%20From%20Model%20and%0A%20%20System%20Co-Design&body=Title%3A%20Efficient%20Large%20Foundation%20Models%20Design%3A%20A%20Perspective%20From%20Model%20and%0A%20%20System%20Co-Design%0AAuthor%3A%20Dong%20Liu%20and%20Yanxuan%20Yu%20and%20Zhixin%20Lai%20and%20Yite%20Wang%20and%20Jing%20Wu%20and%20Zhongwei%20Wan%20and%20Sina%20Alinejad%20and%20Benjamin%20Lengerich%20and%20Ying%20Nian%20Wu%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20modern%20efficient%20training%20and%20inference%20technologies%20on%0Afoundation%20models%20and%20illustrates%20them%20from%20two%20perspectives%3A%20model%20and%20system%0Adesign.%20Model%20and%20System%20Design%20optimize%20LLM%20training%20and%20inference%20from%0Adifferent%20aspects%20to%20save%20computational%20resources%2C%20making%20LLMs%20more%20efficient%2C%0Aaffordable%2C%20and%20more%20accessible.%20The%20paper%20list%20repository%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/NoakLiu/Efficient-Foundation-Models-Survey%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01990v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Large%2520Foundation%2520Models%2520Design%253A%2520A%2520Perspective%2520From%2520Model%2520and%250A%2520%2520System%2520Co-Design%26entry.906535625%3DDong%2520Liu%2520and%2520Yanxuan%2520Yu%2520and%2520Zhixin%2520Lai%2520and%2520Yite%2520Wang%2520and%2520Jing%2520Wu%2520and%2520Zhongwei%2520Wan%2520and%2520Sina%2520Alinejad%2520and%2520Benjamin%2520Lengerich%2520and%2520Ying%2520Nian%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520modern%2520efficient%2520training%2520and%2520inference%2520technologies%2520on%250Afoundation%2520models%2520and%2520illustrates%2520them%2520from%2520two%2520perspectives%253A%2520model%2520and%2520system%250Adesign.%2520Model%2520and%2520System%2520Design%2520optimize%2520LLM%2520training%2520and%2520inference%2520from%250Adifferent%2520aspects%2520to%2520save%2520computational%2520resources%252C%2520making%2520LLMs%2520more%2520efficient%252C%250Aaffordable%252C%2520and%2520more%2520accessible.%2520The%2520paper%2520list%2520repository%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/NoakLiu/Efficient-Foundation-Models-Survey%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01990v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Large%20Foundation%20Models%20Design%3A%20A%20Perspective%20From%20Model%20and%0A%20%20System%20Co-Design&entry.906535625=Dong%20Liu%20and%20Yanxuan%20Yu%20and%20Zhixin%20Lai%20and%20Yite%20Wang%20and%20Jing%20Wu%20and%20Zhongwei%20Wan%20and%20Sina%20Alinejad%20and%20Benjamin%20Lengerich%20and%20Ying%20Nian%20Wu&entry.1292438233=%20%20This%20paper%20focuses%20on%20modern%20efficient%20training%20and%20inference%20technologies%20on%0Afoundation%20models%20and%20illustrates%20them%20from%20two%20perspectives%3A%20model%20and%20system%0Adesign.%20Model%20and%20System%20Design%20optimize%20LLM%20training%20and%20inference%20from%0Adifferent%20aspects%20to%20save%20computational%20resources%2C%20making%20LLMs%20more%20efficient%2C%0Aaffordable%2C%20and%20more%20accessible.%20The%20paper%20list%20repository%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/NoakLiu/Efficient-Foundation-Models-Survey%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01990v3&entry.124074799=Read"},
{"title": "PO-GVINS: Tightly Coupled GNSS-Visual-Inertial Integration with\n  Pose-Only Representation", "author": "Zhuo Xu and Feng Zhu and Zihang Zhang and Chang Jian and Jiarui Lv and Yuantai Zhang and Xiaohong Zhang", "abstract": "  Accurate and reliable positioning is crucial for perception, decision-making,\nand other high-level applications in autonomous driving, unmanned aerial\nvehicles, and intelligent robots. Given the inherent limitations of standalone\nsensors, integrating heterogeneous sensors with complementary capabilities is\none of the most effective approaches to achieving this goal. In this paper, we\npropose a filtering-based, tightly coupled global navigation satellite system\n(GNSS)-visual-inertial positioning framework with a pose-only formulation\napplied to the visual-inertial system (VINS), termed PO-GVINS. Specifically,\nmultiple-view imaging used in current VINS requires a priori of 3D feature,\nthen jointly estimate camera poses and 3D feature position, which inevitably\nintroduces linearization error of the feature as well as facing dimensional\nexplosion. However, the pose-only (PO) formulation, which is demonstrated to be\nequivalent to the multiple-view imaging and has been applied in visual\nreconstruction, represent feature depth using two camera poses and thus 3D\nfeature position is removed from state vector avoiding aforementioned\ndifficulties. Inspired by this, we first apply PO formulation in our VINS,\ni.e., PO-VINS. GNSS raw measurements are then incorporated with integer\nambiguity resolved to achieve accurate and drift-free estimation. Extensive\nexperiments demonstrate that the proposed PO-VINS significantly outperforms the\nmulti-state constrained Kalman filter (MSCKF). By incorporating GNSS\nmeasurements, PO-GVINS achieves accurate, drift-free state estimation, making\nit a robust solution for positioning in challenging environments.\n", "link": "http://arxiv.org/abs/2501.07259v1", "date": "2025-01-13", "relevancy": 2.1377, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5506}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.529}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PO-GVINS%3A%20Tightly%20Coupled%20GNSS-Visual-Inertial%20Integration%20with%0A%20%20Pose-Only%20Representation&body=Title%3A%20PO-GVINS%3A%20Tightly%20Coupled%20GNSS-Visual-Inertial%20Integration%20with%0A%20%20Pose-Only%20Representation%0AAuthor%3A%20Zhuo%20Xu%20and%20Feng%20Zhu%20and%20Zihang%20Zhang%20and%20Chang%20Jian%20and%20Jiarui%20Lv%20and%20Yuantai%20Zhang%20and%20Xiaohong%20Zhang%0AAbstract%3A%20%20%20Accurate%20and%20reliable%20positioning%20is%20crucial%20for%20perception%2C%20decision-making%2C%0Aand%20other%20high-level%20applications%20in%20autonomous%20driving%2C%20unmanned%20aerial%0Avehicles%2C%20and%20intelligent%20robots.%20Given%20the%20inherent%20limitations%20of%20standalone%0Asensors%2C%20integrating%20heterogeneous%20sensors%20with%20complementary%20capabilities%20is%0Aone%20of%20the%20most%20effective%20approaches%20to%20achieving%20this%20goal.%20In%20this%20paper%2C%20we%0Apropose%20a%20filtering-based%2C%20tightly%20coupled%20global%20navigation%20satellite%20system%0A%28GNSS%29-visual-inertial%20positioning%20framework%20with%20a%20pose-only%20formulation%0Aapplied%20to%20the%20visual-inertial%20system%20%28VINS%29%2C%20termed%20PO-GVINS.%20Specifically%2C%0Amultiple-view%20imaging%20used%20in%20current%20VINS%20requires%20a%20priori%20of%203D%20feature%2C%0Athen%20jointly%20estimate%20camera%20poses%20and%203D%20feature%20position%2C%20which%20inevitably%0Aintroduces%20linearization%20error%20of%20the%20feature%20as%20well%20as%20facing%20dimensional%0Aexplosion.%20However%2C%20the%20pose-only%20%28PO%29%20formulation%2C%20which%20is%20demonstrated%20to%20be%0Aequivalent%20to%20the%20multiple-view%20imaging%20and%20has%20been%20applied%20in%20visual%0Areconstruction%2C%20represent%20feature%20depth%20using%20two%20camera%20poses%20and%20thus%203D%0Afeature%20position%20is%20removed%20from%20state%20vector%20avoiding%20aforementioned%0Adifficulties.%20Inspired%20by%20this%2C%20we%20first%20apply%20PO%20formulation%20in%20our%20VINS%2C%0Ai.e.%2C%20PO-VINS.%20GNSS%20raw%20measurements%20are%20then%20incorporated%20with%20integer%0Aambiguity%20resolved%20to%20achieve%20accurate%20and%20drift-free%20estimation.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20PO-VINS%20significantly%20outperforms%20the%0Amulti-state%20constrained%20Kalman%20filter%20%28MSCKF%29.%20By%20incorporating%20GNSS%0Ameasurements%2C%20PO-GVINS%20achieves%20accurate%2C%20drift-free%20state%20estimation%2C%20making%0Ait%20a%20robust%20solution%20for%20positioning%20in%20challenging%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPO-GVINS%253A%2520Tightly%2520Coupled%2520GNSS-Visual-Inertial%2520Integration%2520with%250A%2520%2520Pose-Only%2520Representation%26entry.906535625%3DZhuo%2520Xu%2520and%2520Feng%2520Zhu%2520and%2520Zihang%2520Zhang%2520and%2520Chang%2520Jian%2520and%2520Jiarui%2520Lv%2520and%2520Yuantai%2520Zhang%2520and%2520Xiaohong%2520Zhang%26entry.1292438233%3D%2520%2520Accurate%2520and%2520reliable%2520positioning%2520is%2520crucial%2520for%2520perception%252C%2520decision-making%252C%250Aand%2520other%2520high-level%2520applications%2520in%2520autonomous%2520driving%252C%2520unmanned%2520aerial%250Avehicles%252C%2520and%2520intelligent%2520robots.%2520Given%2520the%2520inherent%2520limitations%2520of%2520standalone%250Asensors%252C%2520integrating%2520heterogeneous%2520sensors%2520with%2520complementary%2520capabilities%2520is%250Aone%2520of%2520the%2520most%2520effective%2520approaches%2520to%2520achieving%2520this%2520goal.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520filtering-based%252C%2520tightly%2520coupled%2520global%2520navigation%2520satellite%2520system%250A%2528GNSS%2529-visual-inertial%2520positioning%2520framework%2520with%2520a%2520pose-only%2520formulation%250Aapplied%2520to%2520the%2520visual-inertial%2520system%2520%2528VINS%2529%252C%2520termed%2520PO-GVINS.%2520Specifically%252C%250Amultiple-view%2520imaging%2520used%2520in%2520current%2520VINS%2520requires%2520a%2520priori%2520of%25203D%2520feature%252C%250Athen%2520jointly%2520estimate%2520camera%2520poses%2520and%25203D%2520feature%2520position%252C%2520which%2520inevitably%250Aintroduces%2520linearization%2520error%2520of%2520the%2520feature%2520as%2520well%2520as%2520facing%2520dimensional%250Aexplosion.%2520However%252C%2520the%2520pose-only%2520%2528PO%2529%2520formulation%252C%2520which%2520is%2520demonstrated%2520to%2520be%250Aequivalent%2520to%2520the%2520multiple-view%2520imaging%2520and%2520has%2520been%2520applied%2520in%2520visual%250Areconstruction%252C%2520represent%2520feature%2520depth%2520using%2520two%2520camera%2520poses%2520and%2520thus%25203D%250Afeature%2520position%2520is%2520removed%2520from%2520state%2520vector%2520avoiding%2520aforementioned%250Adifficulties.%2520Inspired%2520by%2520this%252C%2520we%2520first%2520apply%2520PO%2520formulation%2520in%2520our%2520VINS%252C%250Ai.e.%252C%2520PO-VINS.%2520GNSS%2520raw%2520measurements%2520are%2520then%2520incorporated%2520with%2520integer%250Aambiguity%2520resolved%2520to%2520achieve%2520accurate%2520and%2520drift-free%2520estimation.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520PO-VINS%2520significantly%2520outperforms%2520the%250Amulti-state%2520constrained%2520Kalman%2520filter%2520%2528MSCKF%2529.%2520By%2520incorporating%2520GNSS%250Ameasurements%252C%2520PO-GVINS%2520achieves%2520accurate%252C%2520drift-free%2520state%2520estimation%252C%2520making%250Ait%2520a%2520robust%2520solution%2520for%2520positioning%2520in%2520challenging%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PO-GVINS%3A%20Tightly%20Coupled%20GNSS-Visual-Inertial%20Integration%20with%0A%20%20Pose-Only%20Representation&entry.906535625=Zhuo%20Xu%20and%20Feng%20Zhu%20and%20Zihang%20Zhang%20and%20Chang%20Jian%20and%20Jiarui%20Lv%20and%20Yuantai%20Zhang%20and%20Xiaohong%20Zhang&entry.1292438233=%20%20Accurate%20and%20reliable%20positioning%20is%20crucial%20for%20perception%2C%20decision-making%2C%0Aand%20other%20high-level%20applications%20in%20autonomous%20driving%2C%20unmanned%20aerial%0Avehicles%2C%20and%20intelligent%20robots.%20Given%20the%20inherent%20limitations%20of%20standalone%0Asensors%2C%20integrating%20heterogeneous%20sensors%20with%20complementary%20capabilities%20is%0Aone%20of%20the%20most%20effective%20approaches%20to%20achieving%20this%20goal.%20In%20this%20paper%2C%20we%0Apropose%20a%20filtering-based%2C%20tightly%20coupled%20global%20navigation%20satellite%20system%0A%28GNSS%29-visual-inertial%20positioning%20framework%20with%20a%20pose-only%20formulation%0Aapplied%20to%20the%20visual-inertial%20system%20%28VINS%29%2C%20termed%20PO-GVINS.%20Specifically%2C%0Amultiple-view%20imaging%20used%20in%20current%20VINS%20requires%20a%20priori%20of%203D%20feature%2C%0Athen%20jointly%20estimate%20camera%20poses%20and%203D%20feature%20position%2C%20which%20inevitably%0Aintroduces%20linearization%20error%20of%20the%20feature%20as%20well%20as%20facing%20dimensional%0Aexplosion.%20However%2C%20the%20pose-only%20%28PO%29%20formulation%2C%20which%20is%20demonstrated%20to%20be%0Aequivalent%20to%20the%20multiple-view%20imaging%20and%20has%20been%20applied%20in%20visual%0Areconstruction%2C%20represent%20feature%20depth%20using%20two%20camera%20poses%20and%20thus%203D%0Afeature%20position%20is%20removed%20from%20state%20vector%20avoiding%20aforementioned%0Adifficulties.%20Inspired%20by%20this%2C%20we%20first%20apply%20PO%20formulation%20in%20our%20VINS%2C%0Ai.e.%2C%20PO-VINS.%20GNSS%20raw%20measurements%20are%20then%20incorporated%20with%20integer%0Aambiguity%20resolved%20to%20achieve%20accurate%20and%20drift-free%20estimation.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20PO-VINS%20significantly%20outperforms%20the%0Amulti-state%20constrained%20Kalman%20filter%20%28MSCKF%29.%20By%20incorporating%20GNSS%0Ameasurements%2C%20PO-GVINS%20achieves%20accurate%2C%20drift-free%20state%20estimation%2C%20making%0Ait%20a%20robust%20solution%20for%20positioning%20in%20challenging%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07259v1&entry.124074799=Read"},
{"title": "Kolmogorov-Arnold Network for Remote Sensing Image Semantic Segmentation", "author": "Xianping Ma and Ziyao Wang and Yin Hu and Xiaokang Zhang and Man-On Pun", "abstract": "  Semantic segmentation plays a crucial role in remote sensing applications,\nwhere the accurate extraction and representation of features are essential for\nhigh-quality results. Despite the widespread use of encoder-decoder\narchitectures, existing methods often struggle with fully utilizing the\nhigh-dimensional features extracted by the encoder and efficiently recovering\ndetailed information during decoding. To address these problems, we propose a\nnovel semantic segmentation network, namely DeepKANSeg, including two key\ninnovations based on the emerging Kolmogorov Arnold Network (KAN). Notably, the\nadvantage of KAN lies in its ability to decompose high-dimensional complex\nfunctions into univariate transformations, enabling efficient and flexible\nrepresentation of intricate relationships in data. First, we introduce a\nKAN-based deep feature refinement module, namely DeepKAN to effectively capture\ncomplex spatial and rich semantic relationships from high-dimensional features.\nSecond, we replace the traditional multi-layer perceptron (MLP) layers in the\nglobal-local combined decoder with KAN-based linear layers, namely GLKAN. This\nmodule enhances the decoder's ability to capture fine-grained details during\ndecoding. To evaluate the effectiveness of the proposed method, experiments are\nconducted on two well-known fine-resolution remote sensing benchmark datasets,\nnamely ISPRS Vaihingen and ISPRS Potsdam. The results demonstrate that the\nKAN-enhanced segmentation model achieves superior performance in terms of\naccuracy compared to state-of-the-art methods. They highlight the potential of\nKANs as a powerful alternative to traditional architectures in semantic\nsegmentation tasks. Moreover, the explicit univariate decomposition provides\nimproved interpretability, which is particularly beneficial for applications\nrequiring explainable learning in remote sensing.\n", "link": "http://arxiv.org/abs/2501.07390v1", "date": "2025-01-13", "relevancy": 2.1359, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kolmogorov-Arnold%20Network%20for%20Remote%20Sensing%20Image%20Semantic%20Segmentation&body=Title%3A%20Kolmogorov-Arnold%20Network%20for%20Remote%20Sensing%20Image%20Semantic%20Segmentation%0AAuthor%3A%20Xianping%20Ma%20and%20Ziyao%20Wang%20and%20Yin%20Hu%20and%20Xiaokang%20Zhang%20and%20Man-On%20Pun%0AAbstract%3A%20%20%20Semantic%20segmentation%20plays%20a%20crucial%20role%20in%20remote%20sensing%20applications%2C%0Awhere%20the%20accurate%20extraction%20and%20representation%20of%20features%20are%20essential%20for%0Ahigh-quality%20results.%20Despite%20the%20widespread%20use%20of%20encoder-decoder%0Aarchitectures%2C%20existing%20methods%20often%20struggle%20with%20fully%20utilizing%20the%0Ahigh-dimensional%20features%20extracted%20by%20the%20encoder%20and%20efficiently%20recovering%0Adetailed%20information%20during%20decoding.%20To%20address%20these%20problems%2C%20we%20propose%20a%0Anovel%20semantic%20segmentation%20network%2C%20namely%20DeepKANSeg%2C%20including%20two%20key%0Ainnovations%20based%20on%20the%20emerging%20Kolmogorov%20Arnold%20Network%20%28KAN%29.%20Notably%2C%20the%0Aadvantage%20of%20KAN%20lies%20in%20its%20ability%20to%20decompose%20high-dimensional%20complex%0Afunctions%20into%20univariate%20transformations%2C%20enabling%20efficient%20and%20flexible%0Arepresentation%20of%20intricate%20relationships%20in%20data.%20First%2C%20we%20introduce%20a%0AKAN-based%20deep%20feature%20refinement%20module%2C%20namely%20DeepKAN%20to%20effectively%20capture%0Acomplex%20spatial%20and%20rich%20semantic%20relationships%20from%20high-dimensional%20features.%0ASecond%2C%20we%20replace%20the%20traditional%20multi-layer%20perceptron%20%28MLP%29%20layers%20in%20the%0Aglobal-local%20combined%20decoder%20with%20KAN-based%20linear%20layers%2C%20namely%20GLKAN.%20This%0Amodule%20enhances%20the%20decoder%27s%20ability%20to%20capture%20fine-grained%20details%20during%0Adecoding.%20To%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20experiments%20are%0Aconducted%20on%20two%20well-known%20fine-resolution%20remote%20sensing%20benchmark%20datasets%2C%0Anamely%20ISPRS%20Vaihingen%20and%20ISPRS%20Potsdam.%20The%20results%20demonstrate%20that%20the%0AKAN-enhanced%20segmentation%20model%20achieves%20superior%20performance%20in%20terms%20of%0Aaccuracy%20compared%20to%20state-of-the-art%20methods.%20They%20highlight%20the%20potential%20of%0AKANs%20as%20a%20powerful%20alternative%20to%20traditional%20architectures%20in%20semantic%0Asegmentation%20tasks.%20Moreover%2C%20the%20explicit%20univariate%20decomposition%20provides%0Aimproved%20interpretability%2C%20which%20is%20particularly%20beneficial%20for%20applications%0Arequiring%20explainable%20learning%20in%20remote%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKolmogorov-Arnold%2520Network%2520for%2520Remote%2520Sensing%2520Image%2520Semantic%2520Segmentation%26entry.906535625%3DXianping%2520Ma%2520and%2520Ziyao%2520Wang%2520and%2520Yin%2520Hu%2520and%2520Xiaokang%2520Zhang%2520and%2520Man-On%2520Pun%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520plays%2520a%2520crucial%2520role%2520in%2520remote%2520sensing%2520applications%252C%250Awhere%2520the%2520accurate%2520extraction%2520and%2520representation%2520of%2520features%2520are%2520essential%2520for%250Ahigh-quality%2520results.%2520Despite%2520the%2520widespread%2520use%2520of%2520encoder-decoder%250Aarchitectures%252C%2520existing%2520methods%2520often%2520struggle%2520with%2520fully%2520utilizing%2520the%250Ahigh-dimensional%2520features%2520extracted%2520by%2520the%2520encoder%2520and%2520efficiently%2520recovering%250Adetailed%2520information%2520during%2520decoding.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520a%250Anovel%2520semantic%2520segmentation%2520network%252C%2520namely%2520DeepKANSeg%252C%2520including%2520two%2520key%250Ainnovations%2520based%2520on%2520the%2520emerging%2520Kolmogorov%2520Arnold%2520Network%2520%2528KAN%2529.%2520Notably%252C%2520the%250Aadvantage%2520of%2520KAN%2520lies%2520in%2520its%2520ability%2520to%2520decompose%2520high-dimensional%2520complex%250Afunctions%2520into%2520univariate%2520transformations%252C%2520enabling%2520efficient%2520and%2520flexible%250Arepresentation%2520of%2520intricate%2520relationships%2520in%2520data.%2520First%252C%2520we%2520introduce%2520a%250AKAN-based%2520deep%2520feature%2520refinement%2520module%252C%2520namely%2520DeepKAN%2520to%2520effectively%2520capture%250Acomplex%2520spatial%2520and%2520rich%2520semantic%2520relationships%2520from%2520high-dimensional%2520features.%250ASecond%252C%2520we%2520replace%2520the%2520traditional%2520multi-layer%2520perceptron%2520%2528MLP%2529%2520layers%2520in%2520the%250Aglobal-local%2520combined%2520decoder%2520with%2520KAN-based%2520linear%2520layers%252C%2520namely%2520GLKAN.%2520This%250Amodule%2520enhances%2520the%2520decoder%2527s%2520ability%2520to%2520capture%2520fine-grained%2520details%2520during%250Adecoding.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520experiments%2520are%250Aconducted%2520on%2520two%2520well-known%2520fine-resolution%2520remote%2520sensing%2520benchmark%2520datasets%252C%250Anamely%2520ISPRS%2520Vaihingen%2520and%2520ISPRS%2520Potsdam.%2520The%2520results%2520demonstrate%2520that%2520the%250AKAN-enhanced%2520segmentation%2520model%2520achieves%2520superior%2520performance%2520in%2520terms%2520of%250Aaccuracy%2520compared%2520to%2520state-of-the-art%2520methods.%2520They%2520highlight%2520the%2520potential%2520of%250AKANs%2520as%2520a%2520powerful%2520alternative%2520to%2520traditional%2520architectures%2520in%2520semantic%250Asegmentation%2520tasks.%2520Moreover%252C%2520the%2520explicit%2520univariate%2520decomposition%2520provides%250Aimproved%2520interpretability%252C%2520which%2520is%2520particularly%2520beneficial%2520for%2520applications%250Arequiring%2520explainable%2520learning%2520in%2520remote%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kolmogorov-Arnold%20Network%20for%20Remote%20Sensing%20Image%20Semantic%20Segmentation&entry.906535625=Xianping%20Ma%20and%20Ziyao%20Wang%20and%20Yin%20Hu%20and%20Xiaokang%20Zhang%20and%20Man-On%20Pun&entry.1292438233=%20%20Semantic%20segmentation%20plays%20a%20crucial%20role%20in%20remote%20sensing%20applications%2C%0Awhere%20the%20accurate%20extraction%20and%20representation%20of%20features%20are%20essential%20for%0Ahigh-quality%20results.%20Despite%20the%20widespread%20use%20of%20encoder-decoder%0Aarchitectures%2C%20existing%20methods%20often%20struggle%20with%20fully%20utilizing%20the%0Ahigh-dimensional%20features%20extracted%20by%20the%20encoder%20and%20efficiently%20recovering%0Adetailed%20information%20during%20decoding.%20To%20address%20these%20problems%2C%20we%20propose%20a%0Anovel%20semantic%20segmentation%20network%2C%20namely%20DeepKANSeg%2C%20including%20two%20key%0Ainnovations%20based%20on%20the%20emerging%20Kolmogorov%20Arnold%20Network%20%28KAN%29.%20Notably%2C%20the%0Aadvantage%20of%20KAN%20lies%20in%20its%20ability%20to%20decompose%20high-dimensional%20complex%0Afunctions%20into%20univariate%20transformations%2C%20enabling%20efficient%20and%20flexible%0Arepresentation%20of%20intricate%20relationships%20in%20data.%20First%2C%20we%20introduce%20a%0AKAN-based%20deep%20feature%20refinement%20module%2C%20namely%20DeepKAN%20to%20effectively%20capture%0Acomplex%20spatial%20and%20rich%20semantic%20relationships%20from%20high-dimensional%20features.%0ASecond%2C%20we%20replace%20the%20traditional%20multi-layer%20perceptron%20%28MLP%29%20layers%20in%20the%0Aglobal-local%20combined%20decoder%20with%20KAN-based%20linear%20layers%2C%20namely%20GLKAN.%20This%0Amodule%20enhances%20the%20decoder%27s%20ability%20to%20capture%20fine-grained%20details%20during%0Adecoding.%20To%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20experiments%20are%0Aconducted%20on%20two%20well-known%20fine-resolution%20remote%20sensing%20benchmark%20datasets%2C%0Anamely%20ISPRS%20Vaihingen%20and%20ISPRS%20Potsdam.%20The%20results%20demonstrate%20that%20the%0AKAN-enhanced%20segmentation%20model%20achieves%20superior%20performance%20in%20terms%20of%0Aaccuracy%20compared%20to%20state-of-the-art%20methods.%20They%20highlight%20the%20potential%20of%0AKANs%20as%20a%20powerful%20alternative%20to%20traditional%20architectures%20in%20semantic%0Asegmentation%20tasks.%20Moreover%2C%20the%20explicit%20univariate%20decomposition%20provides%0Aimproved%20interpretability%2C%20which%20is%20particularly%20beneficial%20for%20applications%0Arequiring%20explainable%20learning%20in%20remote%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07390v1&entry.124074799=Read"},
{"title": "SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing", "author": "Varun Biyyala and Bharat Chanderprakash Kathuria and Jialu Li and Youshan Zhang", "abstract": "  Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}.\n", "link": "http://arxiv.org/abs/2501.07554v1", "date": "2025-01-13", "relevancy": 2.1286, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SST-EM%3A%20Advanced%20Metrics%20for%20Evaluating%20Semantic%2C%20Spatial%20and%20Temporal%0A%20%20Aspects%20in%20Video%20Editing&body=Title%3A%20SST-EM%3A%20Advanced%20Metrics%20for%20Evaluating%20Semantic%2C%20Spatial%20and%20Temporal%0A%20%20Aspects%20in%20Video%20Editing%0AAuthor%3A%20Varun%20Biyyala%20and%20Bharat%20Chanderprakash%20Kathuria%20and%20Jialu%20Li%20and%20Youshan%20Zhang%0AAbstract%3A%20%20%20Video%20editing%20models%20have%20advanced%20significantly%2C%20but%20evaluating%20their%0Aperformance%20remains%20challenging.%20Traditional%20metrics%2C%20such%20as%20CLIP%20text%20and%0Aimage%20scores%2C%20often%20fall%20short%3A%20text%20scores%20are%20limited%20by%20inadequate%20training%0Adata%20and%20hierarchical%20dependencies%2C%20while%20image%20scores%20fail%20to%20assess%20temporal%0Aconsistency.%20We%20present%20SST-EM%20%28Semantic%2C%20Spatial%2C%20and%20Temporal%20Evaluation%0AMetric%29%2C%20a%20novel%20evaluation%20framework%20that%20leverages%20modern%20Vision-Language%0AModels%20%28VLMs%29%2C%20Object%20Detection%2C%20and%20Temporal%20Consistency%20checks.%20SST-EM%0Acomprises%20four%20components%3A%20%281%29%20semantic%20extraction%20from%20frames%20using%20a%20VLM%2C%20%282%29%0Aprimary%20object%20tracking%20with%20Object%20Detection%2C%20%283%29%20focused%20object%20refinement%0Avia%20an%20LLM%20agent%2C%20and%20%284%29%20temporal%20consistency%20assessment%20using%20a%20Vision%0ATransformer%20%28ViT%29.%20These%20components%20are%20integrated%20into%20a%20unified%20metric%20with%0Aweights%20derived%20from%20human%20evaluations%20and%20regression%20analysis.%20The%20name%20SST-EM%0Areflects%20its%20focus%20on%20Semantic%2C%20Spatial%2C%20and%20Temporal%20aspects%20of%20video%0Aevaluation.%20SST-EM%20provides%20a%20comprehensive%20evaluation%20of%20semantic%20fidelity%20and%0Atemporal%20smoothness%20in%20video%20editing.%20The%20source%20code%20is%20available%20in%20the%0A%5Ctextbf%7B%5Chref%7Bhttps%3A//github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git%7D%7BGitHub%0ARepository%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSST-EM%253A%2520Advanced%2520Metrics%2520for%2520Evaluating%2520Semantic%252C%2520Spatial%2520and%2520Temporal%250A%2520%2520Aspects%2520in%2520Video%2520Editing%26entry.906535625%3DVarun%2520Biyyala%2520and%2520Bharat%2520Chanderprakash%2520Kathuria%2520and%2520Jialu%2520Li%2520and%2520Youshan%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520editing%2520models%2520have%2520advanced%2520significantly%252C%2520but%2520evaluating%2520their%250Aperformance%2520remains%2520challenging.%2520Traditional%2520metrics%252C%2520such%2520as%2520CLIP%2520text%2520and%250Aimage%2520scores%252C%2520often%2520fall%2520short%253A%2520text%2520scores%2520are%2520limited%2520by%2520inadequate%2520training%250Adata%2520and%2520hierarchical%2520dependencies%252C%2520while%2520image%2520scores%2520fail%2520to%2520assess%2520temporal%250Aconsistency.%2520We%2520present%2520SST-EM%2520%2528Semantic%252C%2520Spatial%252C%2520and%2520Temporal%2520Evaluation%250AMetric%2529%252C%2520a%2520novel%2520evaluation%2520framework%2520that%2520leverages%2520modern%2520Vision-Language%250AModels%2520%2528VLMs%2529%252C%2520Object%2520Detection%252C%2520and%2520Temporal%2520Consistency%2520checks.%2520SST-EM%250Acomprises%2520four%2520components%253A%2520%25281%2529%2520semantic%2520extraction%2520from%2520frames%2520using%2520a%2520VLM%252C%2520%25282%2529%250Aprimary%2520object%2520tracking%2520with%2520Object%2520Detection%252C%2520%25283%2529%2520focused%2520object%2520refinement%250Avia%2520an%2520LLM%2520agent%252C%2520and%2520%25284%2529%2520temporal%2520consistency%2520assessment%2520using%2520a%2520Vision%250ATransformer%2520%2528ViT%2529.%2520These%2520components%2520are%2520integrated%2520into%2520a%2520unified%2520metric%2520with%250Aweights%2520derived%2520from%2520human%2520evaluations%2520and%2520regression%2520analysis.%2520The%2520name%2520SST-EM%250Areflects%2520its%2520focus%2520on%2520Semantic%252C%2520Spatial%252C%2520and%2520Temporal%2520aspects%2520of%2520video%250Aevaluation.%2520SST-EM%2520provides%2520a%2520comprehensive%2520evaluation%2520of%2520semantic%2520fidelity%2520and%250Atemporal%2520smoothness%2520in%2520video%2520editing.%2520The%2520source%2520code%2520is%2520available%2520in%2520the%250A%255Ctextbf%257B%255Chref%257Bhttps%253A//github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git%257D%257BGitHub%250ARepository%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SST-EM%3A%20Advanced%20Metrics%20for%20Evaluating%20Semantic%2C%20Spatial%20and%20Temporal%0A%20%20Aspects%20in%20Video%20Editing&entry.906535625=Varun%20Biyyala%20and%20Bharat%20Chanderprakash%20Kathuria%20and%20Jialu%20Li%20and%20Youshan%20Zhang&entry.1292438233=%20%20Video%20editing%20models%20have%20advanced%20significantly%2C%20but%20evaluating%20their%0Aperformance%20remains%20challenging.%20Traditional%20metrics%2C%20such%20as%20CLIP%20text%20and%0Aimage%20scores%2C%20often%20fall%20short%3A%20text%20scores%20are%20limited%20by%20inadequate%20training%0Adata%20and%20hierarchical%20dependencies%2C%20while%20image%20scores%20fail%20to%20assess%20temporal%0Aconsistency.%20We%20present%20SST-EM%20%28Semantic%2C%20Spatial%2C%20and%20Temporal%20Evaluation%0AMetric%29%2C%20a%20novel%20evaluation%20framework%20that%20leverages%20modern%20Vision-Language%0AModels%20%28VLMs%29%2C%20Object%20Detection%2C%20and%20Temporal%20Consistency%20checks.%20SST-EM%0Acomprises%20four%20components%3A%20%281%29%20semantic%20extraction%20from%20frames%20using%20a%20VLM%2C%20%282%29%0Aprimary%20object%20tracking%20with%20Object%20Detection%2C%20%283%29%20focused%20object%20refinement%0Avia%20an%20LLM%20agent%2C%20and%20%284%29%20temporal%20consistency%20assessment%20using%20a%20Vision%0ATransformer%20%28ViT%29.%20These%20components%20are%20integrated%20into%20a%20unified%20metric%20with%0Aweights%20derived%20from%20human%20evaluations%20and%20regression%20analysis.%20The%20name%20SST-EM%0Areflects%20its%20focus%20on%20Semantic%2C%20Spatial%2C%20and%20Temporal%20aspects%20of%20video%0Aevaluation.%20SST-EM%20provides%20a%20comprehensive%20evaluation%20of%20semantic%20fidelity%20and%0Atemporal%20smoothness%20in%20video%20editing.%20The%20source%20code%20is%20available%20in%20the%0A%5Ctextbf%7B%5Chref%7Bhttps%3A//github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git%7D%7BGitHub%0ARepository%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07554v1&entry.124074799=Read"},
{"title": "Guided SAM: Label-Efficient Part Segmentation", "author": "S. B. van Rooij and G. J. Burghouts", "abstract": "  Localizing object parts precisely is essential for tasks such as object\nrecognition and robotic manipulation. Recent part segmentation methods require\nextensive training data and labor-intensive annotations. Segment-Anything Model\n(SAM) has demonstrated good performance on a wide range of segmentation\nproblems, but requires (manual) positional prompts to guide it where to\nsegment. Furthermore, since it has been trained on full objects instead of\nobject parts, it is prone to over-segmentation of parts. To address this, we\npropose a novel approach that guides SAM towards the relevant object parts. Our\nmethod learns positional prompts from coarse patch annotations that are easier\nand cheaper to acquire. We train classifiers on image patches to identify part\nclasses and aggregate patches into regions of interest (ROIs) with positional\nprompts. SAM is conditioned on these ROIs and prompts. This approach, termed\n`Guided SAM', enhances efficiency and reduces manual effort, allowing effective\npart segmentation with minimal labeled data. We demonstrate the efficacy of\nGuided SAM on a dataset of car parts, improving the average IoU on state of the\nart models from 0.37 to 0.49 with annotations that are on average five times\nmore efficient to acquire.\n", "link": "http://arxiv.org/abs/2501.07434v1", "date": "2025-01-13", "relevancy": 2.1258, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5293}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20SAM%3A%20Label-Efficient%20Part%20Segmentation&body=Title%3A%20Guided%20SAM%3A%20Label-Efficient%20Part%20Segmentation%0AAuthor%3A%20S.%20B.%20van%20Rooij%20and%20G.%20J.%20Burghouts%0AAbstract%3A%20%20%20Localizing%20object%20parts%20precisely%20is%20essential%20for%20tasks%20such%20as%20object%0Arecognition%20and%20robotic%20manipulation.%20Recent%20part%20segmentation%20methods%20require%0Aextensive%20training%20data%20and%20labor-intensive%20annotations.%20Segment-Anything%20Model%0A%28SAM%29%20has%20demonstrated%20good%20performance%20on%20a%20wide%20range%20of%20segmentation%0Aproblems%2C%20but%20requires%20%28manual%29%20positional%20prompts%20to%20guide%20it%20where%20to%0Asegment.%20Furthermore%2C%20since%20it%20has%20been%20trained%20on%20full%20objects%20instead%20of%0Aobject%20parts%2C%20it%20is%20prone%20to%20over-segmentation%20of%20parts.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20approach%20that%20guides%20SAM%20towards%20the%20relevant%20object%20parts.%20Our%0Amethod%20learns%20positional%20prompts%20from%20coarse%20patch%20annotations%20that%20are%20easier%0Aand%20cheaper%20to%20acquire.%20We%20train%20classifiers%20on%20image%20patches%20to%20identify%20part%0Aclasses%20and%20aggregate%20patches%20into%20regions%20of%20interest%20%28ROIs%29%20with%20positional%0Aprompts.%20SAM%20is%20conditioned%20on%20these%20ROIs%20and%20prompts.%20This%20approach%2C%20termed%0A%60Guided%20SAM%27%2C%20enhances%20efficiency%20and%20reduces%20manual%20effort%2C%20allowing%20effective%0Apart%20segmentation%20with%20minimal%20labeled%20data.%20We%20demonstrate%20the%20efficacy%20of%0AGuided%20SAM%20on%20a%20dataset%20of%20car%20parts%2C%20improving%20the%20average%20IoU%20on%20state%20of%20the%0Aart%20models%20from%200.37%20to%200.49%20with%20annotations%20that%20are%20on%20average%20five%20times%0Amore%20efficient%20to%20acquire.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520SAM%253A%2520Label-Efficient%2520Part%2520Segmentation%26entry.906535625%3DS.%2520B.%2520van%2520Rooij%2520and%2520G.%2520J.%2520Burghouts%26entry.1292438233%3D%2520%2520Localizing%2520object%2520parts%2520precisely%2520is%2520essential%2520for%2520tasks%2520such%2520as%2520object%250Arecognition%2520and%2520robotic%2520manipulation.%2520Recent%2520part%2520segmentation%2520methods%2520require%250Aextensive%2520training%2520data%2520and%2520labor-intensive%2520annotations.%2520Segment-Anything%2520Model%250A%2528SAM%2529%2520has%2520demonstrated%2520good%2520performance%2520on%2520a%2520wide%2520range%2520of%2520segmentation%250Aproblems%252C%2520but%2520requires%2520%2528manual%2529%2520positional%2520prompts%2520to%2520guide%2520it%2520where%2520to%250Asegment.%2520Furthermore%252C%2520since%2520it%2520has%2520been%2520trained%2520on%2520full%2520objects%2520instead%2520of%250Aobject%2520parts%252C%2520it%2520is%2520prone%2520to%2520over-segmentation%2520of%2520parts.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520that%2520guides%2520SAM%2520towards%2520the%2520relevant%2520object%2520parts.%2520Our%250Amethod%2520learns%2520positional%2520prompts%2520from%2520coarse%2520patch%2520annotations%2520that%2520are%2520easier%250Aand%2520cheaper%2520to%2520acquire.%2520We%2520train%2520classifiers%2520on%2520image%2520patches%2520to%2520identify%2520part%250Aclasses%2520and%2520aggregate%2520patches%2520into%2520regions%2520of%2520interest%2520%2528ROIs%2529%2520with%2520positional%250Aprompts.%2520SAM%2520is%2520conditioned%2520on%2520these%2520ROIs%2520and%2520prompts.%2520This%2520approach%252C%2520termed%250A%2560Guided%2520SAM%2527%252C%2520enhances%2520efficiency%2520and%2520reduces%2520manual%2520effort%252C%2520allowing%2520effective%250Apart%2520segmentation%2520with%2520minimal%2520labeled%2520data.%2520We%2520demonstrate%2520the%2520efficacy%2520of%250AGuided%2520SAM%2520on%2520a%2520dataset%2520of%2520car%2520parts%252C%2520improving%2520the%2520average%2520IoU%2520on%2520state%2520of%2520the%250Aart%2520models%2520from%25200.37%2520to%25200.49%2520with%2520annotations%2520that%2520are%2520on%2520average%2520five%2520times%250Amore%2520efficient%2520to%2520acquire.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20SAM%3A%20Label-Efficient%20Part%20Segmentation&entry.906535625=S.%20B.%20van%20Rooij%20and%20G.%20J.%20Burghouts&entry.1292438233=%20%20Localizing%20object%20parts%20precisely%20is%20essential%20for%20tasks%20such%20as%20object%0Arecognition%20and%20robotic%20manipulation.%20Recent%20part%20segmentation%20methods%20require%0Aextensive%20training%20data%20and%20labor-intensive%20annotations.%20Segment-Anything%20Model%0A%28SAM%29%20has%20demonstrated%20good%20performance%20on%20a%20wide%20range%20of%20segmentation%0Aproblems%2C%20but%20requires%20%28manual%29%20positional%20prompts%20to%20guide%20it%20where%20to%0Asegment.%20Furthermore%2C%20since%20it%20has%20been%20trained%20on%20full%20objects%20instead%20of%0Aobject%20parts%2C%20it%20is%20prone%20to%20over-segmentation%20of%20parts.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20approach%20that%20guides%20SAM%20towards%20the%20relevant%20object%20parts.%20Our%0Amethod%20learns%20positional%20prompts%20from%20coarse%20patch%20annotations%20that%20are%20easier%0Aand%20cheaper%20to%20acquire.%20We%20train%20classifiers%20on%20image%20patches%20to%20identify%20part%0Aclasses%20and%20aggregate%20patches%20into%20regions%20of%20interest%20%28ROIs%29%20with%20positional%0Aprompts.%20SAM%20is%20conditioned%20on%20these%20ROIs%20and%20prompts.%20This%20approach%2C%20termed%0A%60Guided%20SAM%27%2C%20enhances%20efficiency%20and%20reduces%20manual%20effort%2C%20allowing%20effective%0Apart%20segmentation%20with%20minimal%20labeled%20data.%20We%20demonstrate%20the%20efficacy%20of%0AGuided%20SAM%20on%20a%20dataset%20of%20car%20parts%2C%20improving%20the%20average%20IoU%20on%20state%20of%20the%0Aart%20models%20from%200.37%20to%200.49%20with%20annotations%20that%20are%20on%20average%20five%20times%0Amore%20efficient%20to%20acquire.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07434v1&entry.124074799=Read"},
{"title": "The Devil is in the Spurious Correlation: Boosting Moment Retrieval via\n  Temporal Dynamic Learning", "author": "Xinyang Zhou and Fanyue Wei and Lixin Duan and Wen Li", "abstract": "  Given a textual query along with a corresponding video, the objective of\nmoment retrieval aims to localize the moments relevant to the query within the\nvideo. While commendable results have been demonstrated by existing\ntransformer-based approaches, predicting the accurate temporal span of the\ntarget moment is currently still a major challenge. In this paper, we reveal\nthat a crucial reason stems from the spurious correlation between the text\nqueries and the moment context. Namely, the model may associate the textual\nquery with the background frames rather than the target moment. To address this\nissue, we propose a temporal dynamic learning approach for moment retrieval,\nwhere two strategies are designed to mitigate the spurious correlation. First,\nwe introduce a novel video synthesis approach to construct a dynamic context\nfor the relevant moment. With separate yet similar videos mixed up, the\nsynthesis approach empowers our model to attend to the target moment of the\ncorresponding query under various dynamic contexts. Second, we enhance the\nrepresentation by learning temporal dynamics. Besides the visual\nrepresentation, text queries are aligned with temporal dynamic representations,\nwhich enables our model to establish a non-spurious correlation between the\nquery-related moment and context. With the aforementioned proposed method, the\nspurious correlation issue in moment retrieval can be largely alleviated. Our\nmethod establishes a new state-of-the-art performance on two popular benchmarks\nof moment retrieval, \\ie, QVHighlights and Charades-STA. In addition, the\ndetailed ablation analyses demonstrate the effectiveness of the proposed\nstrategies. Our code will be publicly available.\n", "link": "http://arxiv.org/abs/2501.07305v1", "date": "2025-01-13", "relevancy": 2.1179, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5514}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5252}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Devil%20is%20in%20the%20Spurious%20Correlation%3A%20Boosting%20Moment%20Retrieval%20via%0A%20%20Temporal%20Dynamic%20Learning&body=Title%3A%20The%20Devil%20is%20in%20the%20Spurious%20Correlation%3A%20Boosting%20Moment%20Retrieval%20via%0A%20%20Temporal%20Dynamic%20Learning%0AAuthor%3A%20Xinyang%20Zhou%20and%20Fanyue%20Wei%20and%20Lixin%20Duan%20and%20Wen%20Li%0AAbstract%3A%20%20%20Given%20a%20textual%20query%20along%20with%20a%20corresponding%20video%2C%20the%20objective%20of%0Amoment%20retrieval%20aims%20to%20localize%20the%20moments%20relevant%20to%20the%20query%20within%20the%0Avideo.%20While%20commendable%20results%20have%20been%20demonstrated%20by%20existing%0Atransformer-based%20approaches%2C%20predicting%20the%20accurate%20temporal%20span%20of%20the%0Atarget%20moment%20is%20currently%20still%20a%20major%20challenge.%20In%20this%20paper%2C%20we%20reveal%0Athat%20a%20crucial%20reason%20stems%20from%20the%20spurious%20correlation%20between%20the%20text%0Aqueries%20and%20the%20moment%20context.%20Namely%2C%20the%20model%20may%20associate%20the%20textual%0Aquery%20with%20the%20background%20frames%20rather%20than%20the%20target%20moment.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20temporal%20dynamic%20learning%20approach%20for%20moment%20retrieval%2C%0Awhere%20two%20strategies%20are%20designed%20to%20mitigate%20the%20spurious%20correlation.%20First%2C%0Awe%20introduce%20a%20novel%20video%20synthesis%20approach%20to%20construct%20a%20dynamic%20context%0Afor%20the%20relevant%20moment.%20With%20separate%20yet%20similar%20videos%20mixed%20up%2C%20the%0Asynthesis%20approach%20empowers%20our%20model%20to%20attend%20to%20the%20target%20moment%20of%20the%0Acorresponding%20query%20under%20various%20dynamic%20contexts.%20Second%2C%20we%20enhance%20the%0Arepresentation%20by%20learning%20temporal%20dynamics.%20Besides%20the%20visual%0Arepresentation%2C%20text%20queries%20are%20aligned%20with%20temporal%20dynamic%20representations%2C%0Awhich%20enables%20our%20model%20to%20establish%20a%20non-spurious%20correlation%20between%20the%0Aquery-related%20moment%20and%20context.%20With%20the%20aforementioned%20proposed%20method%2C%20the%0Aspurious%20correlation%20issue%20in%20moment%20retrieval%20can%20be%20largely%20alleviated.%20Our%0Amethod%20establishes%20a%20new%20state-of-the-art%20performance%20on%20two%20popular%20benchmarks%0Aof%20moment%20retrieval%2C%20%5Cie%2C%20QVHighlights%20and%20Charades-STA.%20In%20addition%2C%20the%0Adetailed%20ablation%20analyses%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Astrategies.%20Our%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Devil%2520is%2520in%2520the%2520Spurious%2520Correlation%253A%2520Boosting%2520Moment%2520Retrieval%2520via%250A%2520%2520Temporal%2520Dynamic%2520Learning%26entry.906535625%3DXinyang%2520Zhou%2520and%2520Fanyue%2520Wei%2520and%2520Lixin%2520Duan%2520and%2520Wen%2520Li%26entry.1292438233%3D%2520%2520Given%2520a%2520textual%2520query%2520along%2520with%2520a%2520corresponding%2520video%252C%2520the%2520objective%2520of%250Amoment%2520retrieval%2520aims%2520to%2520localize%2520the%2520moments%2520relevant%2520to%2520the%2520query%2520within%2520the%250Avideo.%2520While%2520commendable%2520results%2520have%2520been%2520demonstrated%2520by%2520existing%250Atransformer-based%2520approaches%252C%2520predicting%2520the%2520accurate%2520temporal%2520span%2520of%2520the%250Atarget%2520moment%2520is%2520currently%2520still%2520a%2520major%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520reveal%250Athat%2520a%2520crucial%2520reason%2520stems%2520from%2520the%2520spurious%2520correlation%2520between%2520the%2520text%250Aqueries%2520and%2520the%2520moment%2520context.%2520Namely%252C%2520the%2520model%2520may%2520associate%2520the%2520textual%250Aquery%2520with%2520the%2520background%2520frames%2520rather%2520than%2520the%2520target%2520moment.%2520To%2520address%2520this%250Aissue%252C%2520we%2520propose%2520a%2520temporal%2520dynamic%2520learning%2520approach%2520for%2520moment%2520retrieval%252C%250Awhere%2520two%2520strategies%2520are%2520designed%2520to%2520mitigate%2520the%2520spurious%2520correlation.%2520First%252C%250Awe%2520introduce%2520a%2520novel%2520video%2520synthesis%2520approach%2520to%2520construct%2520a%2520dynamic%2520context%250Afor%2520the%2520relevant%2520moment.%2520With%2520separate%2520yet%2520similar%2520videos%2520mixed%2520up%252C%2520the%250Asynthesis%2520approach%2520empowers%2520our%2520model%2520to%2520attend%2520to%2520the%2520target%2520moment%2520of%2520the%250Acorresponding%2520query%2520under%2520various%2520dynamic%2520contexts.%2520Second%252C%2520we%2520enhance%2520the%250Arepresentation%2520by%2520learning%2520temporal%2520dynamics.%2520Besides%2520the%2520visual%250Arepresentation%252C%2520text%2520queries%2520are%2520aligned%2520with%2520temporal%2520dynamic%2520representations%252C%250Awhich%2520enables%2520our%2520model%2520to%2520establish%2520a%2520non-spurious%2520correlation%2520between%2520the%250Aquery-related%2520moment%2520and%2520context.%2520With%2520the%2520aforementioned%2520proposed%2520method%252C%2520the%250Aspurious%2520correlation%2520issue%2520in%2520moment%2520retrieval%2520can%2520be%2520largely%2520alleviated.%2520Our%250Amethod%2520establishes%2520a%2520new%2520state-of-the-art%2520performance%2520on%2520two%2520popular%2520benchmarks%250Aof%2520moment%2520retrieval%252C%2520%255Cie%252C%2520QVHighlights%2520and%2520Charades-STA.%2520In%2520addition%252C%2520the%250Adetailed%2520ablation%2520analyses%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Astrategies.%2520Our%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Devil%20is%20in%20the%20Spurious%20Correlation%3A%20Boosting%20Moment%20Retrieval%20via%0A%20%20Temporal%20Dynamic%20Learning&entry.906535625=Xinyang%20Zhou%20and%20Fanyue%20Wei%20and%20Lixin%20Duan%20and%20Wen%20Li&entry.1292438233=%20%20Given%20a%20textual%20query%20along%20with%20a%20corresponding%20video%2C%20the%20objective%20of%0Amoment%20retrieval%20aims%20to%20localize%20the%20moments%20relevant%20to%20the%20query%20within%20the%0Avideo.%20While%20commendable%20results%20have%20been%20demonstrated%20by%20existing%0Atransformer-based%20approaches%2C%20predicting%20the%20accurate%20temporal%20span%20of%20the%0Atarget%20moment%20is%20currently%20still%20a%20major%20challenge.%20In%20this%20paper%2C%20we%20reveal%0Athat%20a%20crucial%20reason%20stems%20from%20the%20spurious%20correlation%20between%20the%20text%0Aqueries%20and%20the%20moment%20context.%20Namely%2C%20the%20model%20may%20associate%20the%20textual%0Aquery%20with%20the%20background%20frames%20rather%20than%20the%20target%20moment.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20temporal%20dynamic%20learning%20approach%20for%20moment%20retrieval%2C%0Awhere%20two%20strategies%20are%20designed%20to%20mitigate%20the%20spurious%20correlation.%20First%2C%0Awe%20introduce%20a%20novel%20video%20synthesis%20approach%20to%20construct%20a%20dynamic%20context%0Afor%20the%20relevant%20moment.%20With%20separate%20yet%20similar%20videos%20mixed%20up%2C%20the%0Asynthesis%20approach%20empowers%20our%20model%20to%20attend%20to%20the%20target%20moment%20of%20the%0Acorresponding%20query%20under%20various%20dynamic%20contexts.%20Second%2C%20we%20enhance%20the%0Arepresentation%20by%20learning%20temporal%20dynamics.%20Besides%20the%20visual%0Arepresentation%2C%20text%20queries%20are%20aligned%20with%20temporal%20dynamic%20representations%2C%0Awhich%20enables%20our%20model%20to%20establish%20a%20non-spurious%20correlation%20between%20the%0Aquery-related%20moment%20and%20context.%20With%20the%20aforementioned%20proposed%20method%2C%20the%0Aspurious%20correlation%20issue%20in%20moment%20retrieval%20can%20be%20largely%20alleviated.%20Our%0Amethod%20establishes%20a%20new%20state-of-the-art%20performance%20on%20two%20popular%20benchmarks%0Aof%20moment%20retrieval%2C%20%5Cie%2C%20QVHighlights%20and%20Charades-STA.%20In%20addition%2C%20the%0Adetailed%20ablation%20analyses%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Astrategies.%20Our%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07305v1&entry.124074799=Read"},
{"title": "Design of 2D Skyrmionic Metamaterial Through Controlled Assembly", "author": "Qichen Xu and Zhuanglin Shen and Alexander Edstr\u00f6m and I. P. Miranda and Zhiwei Lu and Anders Bergman and Danny Thonig and Wanjian Yin and Olle Eriksson and Anna Delin", "abstract": "  Despite extensive research on magnetic skyrmions and antiskyrmions, a\nsignificant challenge remains in crafting nontrivial high-order skyrmionic\ntextures with varying, or even tailor-made, topologies. We address this\nchallenge, by focusing on a construction pathway of skyrmionic metamaterials\nwithin a monolayer thin film and suggest several skyrmionic metamaterials that\nare surprisingly stable, i.e., long-lived, due to a self-stabilization\nmechanism. This makes these new textures promising for applications. Central to\nour approach is the concept of 'simulated controlled assembly', in short, a\nprotocol inspired by 'click chemistry' that allows for positioning topological\nmagnetic structures where one likes, and then allowing for energy minimization\nto elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic\nsimulations alongside state-of-the-art AI-driven tools, we have isolated\nskyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium\n(Q=0). These entities serve as foundational 'skyrmionic building blocks' to\nform the here reported intricate textures. In this work, two key contributions\nare introduced to the field of skyrmionic systems. First, we present a a novel\ncombination of atomistic spin dynamics simulations and controlled assembly\nprotocols for the stabilization and investigation of new topological magnets.\nSecond, using the aforementioned methods we report on the discovery of\nskyrmionic metamaterials.\n", "link": "http://arxiv.org/abs/2402.10874v2", "date": "2025-01-13", "relevancy": 2.1063, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4244}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4244}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20of%202D%20Skyrmionic%20Metamaterial%20Through%20Controlled%20Assembly&body=Title%3A%20Design%20of%202D%20Skyrmionic%20Metamaterial%20Through%20Controlled%20Assembly%0AAuthor%3A%20Qichen%20Xu%20and%20Zhuanglin%20Shen%20and%20Alexander%20Edstr%C3%B6m%20and%20I.%20P.%20Miranda%20and%20Zhiwei%20Lu%20and%20Anders%20Bergman%20and%20Danny%20Thonig%20and%20Wanjian%20Yin%20and%20Olle%20Eriksson%20and%20Anna%20Delin%0AAbstract%3A%20%20%20Despite%20extensive%20research%20on%20magnetic%20skyrmions%20and%20antiskyrmions%2C%20a%0Asignificant%20challenge%20remains%20in%20crafting%20nontrivial%20high-order%20skyrmionic%0Atextures%20with%20varying%2C%20or%20even%20tailor-made%2C%20topologies.%20We%20address%20this%0Achallenge%2C%20by%20focusing%20on%20a%20construction%20pathway%20of%20skyrmionic%20metamaterials%0Awithin%20a%20monolayer%20thin%20film%20and%20suggest%20several%20skyrmionic%20metamaterials%20that%0Aare%20surprisingly%20stable%2C%20i.e.%2C%20long-lived%2C%20due%20to%20a%20self-stabilization%0Amechanism.%20This%20makes%20these%20new%20textures%20promising%20for%20applications.%20Central%20to%0Aour%20approach%20is%20the%20concept%20of%20%27simulated%20controlled%20assembly%27%2C%20in%20short%2C%20a%0Aprotocol%20inspired%20by%20%27click%20chemistry%27%20that%20allows%20for%20positioning%20topological%0Amagnetic%20structures%20where%20one%20likes%2C%20and%20then%20allowing%20for%20energy%20minimization%0Ato%20elucidate%20the%20stability.%20Utilizing%20high-throughput%20atomistic-spin-dynamic%0Asimulations%20alongside%20state-of-the-art%20AI-driven%20tools%2C%20we%20have%20isolated%0Askyrmions%20%28topological%20charge%20Q%3D1%29%2C%20antiskyrmions%20%28Q%3D-1%29%2C%20and%20skyrmionium%0A%28Q%3D0%29.%20These%20entities%20serve%20as%20foundational%20%27skyrmionic%20building%20blocks%27%20to%0Aform%20the%20here%20reported%20intricate%20textures.%20In%20this%20work%2C%20two%20key%20contributions%0Aare%20introduced%20to%20the%20field%20of%20skyrmionic%20systems.%20First%2C%20we%20present%20a%20a%20novel%0Acombination%20of%20atomistic%20spin%20dynamics%20simulations%20and%20controlled%20assembly%0Aprotocols%20for%20the%20stabilization%20and%20investigation%20of%20new%20topological%20magnets.%0ASecond%2C%20using%20the%20aforementioned%20methods%20we%20report%20on%20the%20discovery%20of%0Askyrmionic%20metamaterials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10874v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520of%25202D%2520Skyrmionic%2520Metamaterial%2520Through%2520Controlled%2520Assembly%26entry.906535625%3DQichen%2520Xu%2520and%2520Zhuanglin%2520Shen%2520and%2520Alexander%2520Edstr%25C3%25B6m%2520and%2520I.%2520P.%2520Miranda%2520and%2520Zhiwei%2520Lu%2520and%2520Anders%2520Bergman%2520and%2520Danny%2520Thonig%2520and%2520Wanjian%2520Yin%2520and%2520Olle%2520Eriksson%2520and%2520Anna%2520Delin%26entry.1292438233%3D%2520%2520Despite%2520extensive%2520research%2520on%2520magnetic%2520skyrmions%2520and%2520antiskyrmions%252C%2520a%250Asignificant%2520challenge%2520remains%2520in%2520crafting%2520nontrivial%2520high-order%2520skyrmionic%250Atextures%2520with%2520varying%252C%2520or%2520even%2520tailor-made%252C%2520topologies.%2520We%2520address%2520this%250Achallenge%252C%2520by%2520focusing%2520on%2520a%2520construction%2520pathway%2520of%2520skyrmionic%2520metamaterials%250Awithin%2520a%2520monolayer%2520thin%2520film%2520and%2520suggest%2520several%2520skyrmionic%2520metamaterials%2520that%250Aare%2520surprisingly%2520stable%252C%2520i.e.%252C%2520long-lived%252C%2520due%2520to%2520a%2520self-stabilization%250Amechanism.%2520This%2520makes%2520these%2520new%2520textures%2520promising%2520for%2520applications.%2520Central%2520to%250Aour%2520approach%2520is%2520the%2520concept%2520of%2520%2527simulated%2520controlled%2520assembly%2527%252C%2520in%2520short%252C%2520a%250Aprotocol%2520inspired%2520by%2520%2527click%2520chemistry%2527%2520that%2520allows%2520for%2520positioning%2520topological%250Amagnetic%2520structures%2520where%2520one%2520likes%252C%2520and%2520then%2520allowing%2520for%2520energy%2520minimization%250Ato%2520elucidate%2520the%2520stability.%2520Utilizing%2520high-throughput%2520atomistic-spin-dynamic%250Asimulations%2520alongside%2520state-of-the-art%2520AI-driven%2520tools%252C%2520we%2520have%2520isolated%250Askyrmions%2520%2528topological%2520charge%2520Q%253D1%2529%252C%2520antiskyrmions%2520%2528Q%253D-1%2529%252C%2520and%2520skyrmionium%250A%2528Q%253D0%2529.%2520These%2520entities%2520serve%2520as%2520foundational%2520%2527skyrmionic%2520building%2520blocks%2527%2520to%250Aform%2520the%2520here%2520reported%2520intricate%2520textures.%2520In%2520this%2520work%252C%2520two%2520key%2520contributions%250Aare%2520introduced%2520to%2520the%2520field%2520of%2520skyrmionic%2520systems.%2520First%252C%2520we%2520present%2520a%2520a%2520novel%250Acombination%2520of%2520atomistic%2520spin%2520dynamics%2520simulations%2520and%2520controlled%2520assembly%250Aprotocols%2520for%2520the%2520stabilization%2520and%2520investigation%2520of%2520new%2520topological%2520magnets.%250ASecond%252C%2520using%2520the%2520aforementioned%2520methods%2520we%2520report%2520on%2520the%2520discovery%2520of%250Askyrmionic%2520metamaterials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10874v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20of%202D%20Skyrmionic%20Metamaterial%20Through%20Controlled%20Assembly&entry.906535625=Qichen%20Xu%20and%20Zhuanglin%20Shen%20and%20Alexander%20Edstr%C3%B6m%20and%20I.%20P.%20Miranda%20and%20Zhiwei%20Lu%20and%20Anders%20Bergman%20and%20Danny%20Thonig%20and%20Wanjian%20Yin%20and%20Olle%20Eriksson%20and%20Anna%20Delin&entry.1292438233=%20%20Despite%20extensive%20research%20on%20magnetic%20skyrmions%20and%20antiskyrmions%2C%20a%0Asignificant%20challenge%20remains%20in%20crafting%20nontrivial%20high-order%20skyrmionic%0Atextures%20with%20varying%2C%20or%20even%20tailor-made%2C%20topologies.%20We%20address%20this%0Achallenge%2C%20by%20focusing%20on%20a%20construction%20pathway%20of%20skyrmionic%20metamaterials%0Awithin%20a%20monolayer%20thin%20film%20and%20suggest%20several%20skyrmionic%20metamaterials%20that%0Aare%20surprisingly%20stable%2C%20i.e.%2C%20long-lived%2C%20due%20to%20a%20self-stabilization%0Amechanism.%20This%20makes%20these%20new%20textures%20promising%20for%20applications.%20Central%20to%0Aour%20approach%20is%20the%20concept%20of%20%27simulated%20controlled%20assembly%27%2C%20in%20short%2C%20a%0Aprotocol%20inspired%20by%20%27click%20chemistry%27%20that%20allows%20for%20positioning%20topological%0Amagnetic%20structures%20where%20one%20likes%2C%20and%20then%20allowing%20for%20energy%20minimization%0Ato%20elucidate%20the%20stability.%20Utilizing%20high-throughput%20atomistic-spin-dynamic%0Asimulations%20alongside%20state-of-the-art%20AI-driven%20tools%2C%20we%20have%20isolated%0Askyrmions%20%28topological%20charge%20Q%3D1%29%2C%20antiskyrmions%20%28Q%3D-1%29%2C%20and%20skyrmionium%0A%28Q%3D0%29.%20These%20entities%20serve%20as%20foundational%20%27skyrmionic%20building%20blocks%27%20to%0Aform%20the%20here%20reported%20intricate%20textures.%20In%20this%20work%2C%20two%20key%20contributions%0Aare%20introduced%20to%20the%20field%20of%20skyrmionic%20systems.%20First%2C%20we%20present%20a%20a%20novel%0Acombination%20of%20atomistic%20spin%20dynamics%20simulations%20and%20controlled%20assembly%0Aprotocols%20for%20the%20stabilization%20and%20investigation%20of%20new%20topological%20magnets.%0ASecond%2C%20using%20the%20aforementioned%20methods%20we%20report%20on%20the%20discovery%20of%0Askyrmionic%20metamaterials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10874v2&entry.124074799=Read"},
{"title": "Localization-Aware Multi-Scale Representation Learning for Repetitive\n  Action Counting", "author": "Sujia Wang and Xiangwei Shen and Yansong Tang and Xin Dong and Wenjia Geng and Lei Chen", "abstract": "  Repetitive action counting (RAC) aims to estimate the number of\nclass-agnostic action occurrences in a video without exemplars. Most current\nRAC methods rely on a raw frame-to-frame similarity representation for period\nprediction. However, this approach can be significantly disrupted by common\nnoise such as action interruptions and inconsistencies, leading to sub-optimal\ncounting performance in realistic scenarios. In this paper, we introduce a\nforeground localization optimization objective into similarity representation\nlearning to obtain more robust and efficient video features. We propose a\nLocalization-Aware Multi-Scale Representation Learning (LMRL) framework.\nSpecifically, we apply a Multi-Scale Period-Aware Representation (MPR) with a\nscale-specific design to accommodate various action frequencies and learn more\nflexible temporal correlations. Furthermore, we introduce the Repetition\nForeground Localization (RFL) method, which enhances the representation by\ncoarsely identifying periodic actions and incorporating global semantic\ninformation. These two modules can be jointly optimized, resulting in a more\ndiscerning periodic action representation. Our approach significantly reduces\nthe impact of noise, thereby improving counting accuracy. Additionally, the\nframework is designed to be scalable and adaptable to different types of video\ncontent. Experimental results on the RepCountA and UCFRep datasets demonstrate\nthat our proposed method effectively handles repetitive action counting.\n", "link": "http://arxiv.org/abs/2501.07312v1", "date": "2025-01-13", "relevancy": 2.1062, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5284}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5278}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localization-Aware%20Multi-Scale%20Representation%20Learning%20for%20Repetitive%0A%20%20Action%20Counting&body=Title%3A%20Localization-Aware%20Multi-Scale%20Representation%20Learning%20for%20Repetitive%0A%20%20Action%20Counting%0AAuthor%3A%20Sujia%20Wang%20and%20Xiangwei%20Shen%20and%20Yansong%20Tang%20and%20Xin%20Dong%20and%20Wenjia%20Geng%20and%20Lei%20Chen%0AAbstract%3A%20%20%20Repetitive%20action%20counting%20%28RAC%29%20aims%20to%20estimate%20the%20number%20of%0Aclass-agnostic%20action%20occurrences%20in%20a%20video%20without%20exemplars.%20Most%20current%0ARAC%20methods%20rely%20on%20a%20raw%20frame-to-frame%20similarity%20representation%20for%20period%0Aprediction.%20However%2C%20this%20approach%20can%20be%20significantly%20disrupted%20by%20common%0Anoise%20such%20as%20action%20interruptions%20and%20inconsistencies%2C%20leading%20to%20sub-optimal%0Acounting%20performance%20in%20realistic%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20a%0Aforeground%20localization%20optimization%20objective%20into%20similarity%20representation%0Alearning%20to%20obtain%20more%20robust%20and%20efficient%20video%20features.%20We%20propose%20a%0ALocalization-Aware%20Multi-Scale%20Representation%20Learning%20%28LMRL%29%20framework.%0ASpecifically%2C%20we%20apply%20a%20Multi-Scale%20Period-Aware%20Representation%20%28MPR%29%20with%20a%0Ascale-specific%20design%20to%20accommodate%20various%20action%20frequencies%20and%20learn%20more%0Aflexible%20temporal%20correlations.%20Furthermore%2C%20we%20introduce%20the%20Repetition%0AForeground%20Localization%20%28RFL%29%20method%2C%20which%20enhances%20the%20representation%20by%0Acoarsely%20identifying%20periodic%20actions%20and%20incorporating%20global%20semantic%0Ainformation.%20These%20two%20modules%20can%20be%20jointly%20optimized%2C%20resulting%20in%20a%20more%0Adiscerning%20periodic%20action%20representation.%20Our%20approach%20significantly%20reduces%0Athe%20impact%20of%20noise%2C%20thereby%20improving%20counting%20accuracy.%20Additionally%2C%20the%0Aframework%20is%20designed%20to%20be%20scalable%20and%20adaptable%20to%20different%20types%20of%20video%0Acontent.%20Experimental%20results%20on%20the%20RepCountA%20and%20UCFRep%20datasets%20demonstrate%0Athat%20our%20proposed%20method%20effectively%20handles%20repetitive%20action%20counting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalization-Aware%2520Multi-Scale%2520Representation%2520Learning%2520for%2520Repetitive%250A%2520%2520Action%2520Counting%26entry.906535625%3DSujia%2520Wang%2520and%2520Xiangwei%2520Shen%2520and%2520Yansong%2520Tang%2520and%2520Xin%2520Dong%2520and%2520Wenjia%2520Geng%2520and%2520Lei%2520Chen%26entry.1292438233%3D%2520%2520Repetitive%2520action%2520counting%2520%2528RAC%2529%2520aims%2520to%2520estimate%2520the%2520number%2520of%250Aclass-agnostic%2520action%2520occurrences%2520in%2520a%2520video%2520without%2520exemplars.%2520Most%2520current%250ARAC%2520methods%2520rely%2520on%2520a%2520raw%2520frame-to-frame%2520similarity%2520representation%2520for%2520period%250Aprediction.%2520However%252C%2520this%2520approach%2520can%2520be%2520significantly%2520disrupted%2520by%2520common%250Anoise%2520such%2520as%2520action%2520interruptions%2520and%2520inconsistencies%252C%2520leading%2520to%2520sub-optimal%250Acounting%2520performance%2520in%2520realistic%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Aforeground%2520localization%2520optimization%2520objective%2520into%2520similarity%2520representation%250Alearning%2520to%2520obtain%2520more%2520robust%2520and%2520efficient%2520video%2520features.%2520We%2520propose%2520a%250ALocalization-Aware%2520Multi-Scale%2520Representation%2520Learning%2520%2528LMRL%2529%2520framework.%250ASpecifically%252C%2520we%2520apply%2520a%2520Multi-Scale%2520Period-Aware%2520Representation%2520%2528MPR%2529%2520with%2520a%250Ascale-specific%2520design%2520to%2520accommodate%2520various%2520action%2520frequencies%2520and%2520learn%2520more%250Aflexible%2520temporal%2520correlations.%2520Furthermore%252C%2520we%2520introduce%2520the%2520Repetition%250AForeground%2520Localization%2520%2528RFL%2529%2520method%252C%2520which%2520enhances%2520the%2520representation%2520by%250Acoarsely%2520identifying%2520periodic%2520actions%2520and%2520incorporating%2520global%2520semantic%250Ainformation.%2520These%2520two%2520modules%2520can%2520be%2520jointly%2520optimized%252C%2520resulting%2520in%2520a%2520more%250Adiscerning%2520periodic%2520action%2520representation.%2520Our%2520approach%2520significantly%2520reduces%250Athe%2520impact%2520of%2520noise%252C%2520thereby%2520improving%2520counting%2520accuracy.%2520Additionally%252C%2520the%250Aframework%2520is%2520designed%2520to%2520be%2520scalable%2520and%2520adaptable%2520to%2520different%2520types%2520of%2520video%250Acontent.%2520Experimental%2520results%2520on%2520the%2520RepCountA%2520and%2520UCFRep%2520datasets%2520demonstrate%250Athat%2520our%2520proposed%2520method%2520effectively%2520handles%2520repetitive%2520action%2520counting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization-Aware%20Multi-Scale%20Representation%20Learning%20for%20Repetitive%0A%20%20Action%20Counting&entry.906535625=Sujia%20Wang%20and%20Xiangwei%20Shen%20and%20Yansong%20Tang%20and%20Xin%20Dong%20and%20Wenjia%20Geng%20and%20Lei%20Chen&entry.1292438233=%20%20Repetitive%20action%20counting%20%28RAC%29%20aims%20to%20estimate%20the%20number%20of%0Aclass-agnostic%20action%20occurrences%20in%20a%20video%20without%20exemplars.%20Most%20current%0ARAC%20methods%20rely%20on%20a%20raw%20frame-to-frame%20similarity%20representation%20for%20period%0Aprediction.%20However%2C%20this%20approach%20can%20be%20significantly%20disrupted%20by%20common%0Anoise%20such%20as%20action%20interruptions%20and%20inconsistencies%2C%20leading%20to%20sub-optimal%0Acounting%20performance%20in%20realistic%20scenarios.%20In%20this%20paper%2C%20we%20introduce%20a%0Aforeground%20localization%20optimization%20objective%20into%20similarity%20representation%0Alearning%20to%20obtain%20more%20robust%20and%20efficient%20video%20features.%20We%20propose%20a%0ALocalization-Aware%20Multi-Scale%20Representation%20Learning%20%28LMRL%29%20framework.%0ASpecifically%2C%20we%20apply%20a%20Multi-Scale%20Period-Aware%20Representation%20%28MPR%29%20with%20a%0Ascale-specific%20design%20to%20accommodate%20various%20action%20frequencies%20and%20learn%20more%0Aflexible%20temporal%20correlations.%20Furthermore%2C%20we%20introduce%20the%20Repetition%0AForeground%20Localization%20%28RFL%29%20method%2C%20which%20enhances%20the%20representation%20by%0Acoarsely%20identifying%20periodic%20actions%20and%20incorporating%20global%20semantic%0Ainformation.%20These%20two%20modules%20can%20be%20jointly%20optimized%2C%20resulting%20in%20a%20more%0Adiscerning%20periodic%20action%20representation.%20Our%20approach%20significantly%20reduces%0Athe%20impact%20of%20noise%2C%20thereby%20improving%20counting%20accuracy.%20Additionally%2C%20the%0Aframework%20is%20designed%20to%20be%20scalable%20and%20adaptable%20to%20different%20types%20of%20video%0Acontent.%20Experimental%20results%20on%20the%20RepCountA%20and%20UCFRep%20datasets%20demonstrate%0Athat%20our%20proposed%20method%20effectively%20handles%20repetitive%20action%20counting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07312v1&entry.124074799=Read"},
{"title": "TempoGPT: Enhancing Temporal Reasoning via Quantizing Embedding", "author": "Haochuan Zhang and Chunhua Yang and Jie Han and Liyang Qin and Xiaoli Wang", "abstract": "  Multi-modal language model has made advanced progress in vision and audio,\nbut still faces significant challenges in dealing with complex reasoning tasks\nin the time series domain. The reasons are twofold. First, labels for\nmulti-modal time series data are coarse and devoid of analysis or reasoning\nprocesses. Training with these data cannot improve the model's reasoning\ncapabilities. Second, due to the lack of precise tokenization in processing\ntime series, the representation patterns for temporal and textual information\nare inconsistent, which hampers the effectiveness of multi-modal alignment. To\naddress these challenges, we propose a multi-modal time series data\nconstruction approach and a multi-modal time series language model (TLM),\nTempoGPT. Specially, we construct multi-modal data for complex reasoning tasks\nby analyzing the variable-system relationships within a white-box system.\nAdditionally, proposed TempoGPT achieves consistent representation between\ntemporal and textual information by quantizing temporal embeddings, where\ntemporal embeddings are quantized into a series of discrete tokens using a\npredefined codebook; subsequently, a shared embedding layer processes both\ntemporal and textual tokens. Extensive experiments demonstrate that TempoGPT\naccurately perceives temporal information, logically infers conclusions, and\nachieves state-of-the-art in the constructed complex time series reasoning\ntasks. Moreover, we quantitatively demonstrate the effectiveness of quantizing\ntemporal embeddings in enhancing multi-modal alignment and the reasoning\ncapabilities of TLMs. Code and data are available at\nhttps://github.com/zhanghaochuan20/TempoGPT.\n", "link": "http://arxiv.org/abs/2501.07335v1", "date": "2025-01-13", "relevancy": 2.103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5506}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5165}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TempoGPT%3A%20Enhancing%20Temporal%20Reasoning%20via%20Quantizing%20Embedding&body=Title%3A%20TempoGPT%3A%20Enhancing%20Temporal%20Reasoning%20via%20Quantizing%20Embedding%0AAuthor%3A%20Haochuan%20Zhang%20and%20Chunhua%20Yang%20and%20Jie%20Han%20and%20Liyang%20Qin%20and%20Xiaoli%20Wang%0AAbstract%3A%20%20%20Multi-modal%20language%20model%20has%20made%20advanced%20progress%20in%20vision%20and%20audio%2C%0Abut%20still%20faces%20significant%20challenges%20in%20dealing%20with%20complex%20reasoning%20tasks%0Ain%20the%20time%20series%20domain.%20The%20reasons%20are%20twofold.%20First%2C%20labels%20for%0Amulti-modal%20time%20series%20data%20are%20coarse%20and%20devoid%20of%20analysis%20or%20reasoning%0Aprocesses.%20Training%20with%20these%20data%20cannot%20improve%20the%20model%27s%20reasoning%0Acapabilities.%20Second%2C%20due%20to%20the%20lack%20of%20precise%20tokenization%20in%20processing%0Atime%20series%2C%20the%20representation%20patterns%20for%20temporal%20and%20textual%20information%0Aare%20inconsistent%2C%20which%20hampers%20the%20effectiveness%20of%20multi-modal%20alignment.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20multi-modal%20time%20series%20data%0Aconstruction%20approach%20and%20a%20multi-modal%20time%20series%20language%20model%20%28TLM%29%2C%0ATempoGPT.%20Specially%2C%20we%20construct%20multi-modal%20data%20for%20complex%20reasoning%20tasks%0Aby%20analyzing%20the%20variable-system%20relationships%20within%20a%20white-box%20system.%0AAdditionally%2C%20proposed%20TempoGPT%20achieves%20consistent%20representation%20between%0Atemporal%20and%20textual%20information%20by%20quantizing%20temporal%20embeddings%2C%20where%0Atemporal%20embeddings%20are%20quantized%20into%20a%20series%20of%20discrete%20tokens%20using%20a%0Apredefined%20codebook%3B%20subsequently%2C%20a%20shared%20embedding%20layer%20processes%20both%0Atemporal%20and%20textual%20tokens.%20Extensive%20experiments%20demonstrate%20that%20TempoGPT%0Aaccurately%20perceives%20temporal%20information%2C%20logically%20infers%20conclusions%2C%20and%0Aachieves%20state-of-the-art%20in%20the%20constructed%20complex%20time%20series%20reasoning%0Atasks.%20Moreover%2C%20we%20quantitatively%20demonstrate%20the%20effectiveness%20of%20quantizing%0Atemporal%20embeddings%20in%20enhancing%20multi-modal%20alignment%20and%20the%20reasoning%0Acapabilities%20of%20TLMs.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/zhanghaochuan20/TempoGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTempoGPT%253A%2520Enhancing%2520Temporal%2520Reasoning%2520via%2520Quantizing%2520Embedding%26entry.906535625%3DHaochuan%2520Zhang%2520and%2520Chunhua%2520Yang%2520and%2520Jie%2520Han%2520and%2520Liyang%2520Qin%2520and%2520Xiaoli%2520Wang%26entry.1292438233%3D%2520%2520Multi-modal%2520language%2520model%2520has%2520made%2520advanced%2520progress%2520in%2520vision%2520and%2520audio%252C%250Abut%2520still%2520faces%2520significant%2520challenges%2520in%2520dealing%2520with%2520complex%2520reasoning%2520tasks%250Ain%2520the%2520time%2520series%2520domain.%2520The%2520reasons%2520are%2520twofold.%2520First%252C%2520labels%2520for%250Amulti-modal%2520time%2520series%2520data%2520are%2520coarse%2520and%2520devoid%2520of%2520analysis%2520or%2520reasoning%250Aprocesses.%2520Training%2520with%2520these%2520data%2520cannot%2520improve%2520the%2520model%2527s%2520reasoning%250Acapabilities.%2520Second%252C%2520due%2520to%2520the%2520lack%2520of%2520precise%2520tokenization%2520in%2520processing%250Atime%2520series%252C%2520the%2520representation%2520patterns%2520for%2520temporal%2520and%2520textual%2520information%250Aare%2520inconsistent%252C%2520which%2520hampers%2520the%2520effectiveness%2520of%2520multi-modal%2520alignment.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520multi-modal%2520time%2520series%2520data%250Aconstruction%2520approach%2520and%2520a%2520multi-modal%2520time%2520series%2520language%2520model%2520%2528TLM%2529%252C%250ATempoGPT.%2520Specially%252C%2520we%2520construct%2520multi-modal%2520data%2520for%2520complex%2520reasoning%2520tasks%250Aby%2520analyzing%2520the%2520variable-system%2520relationships%2520within%2520a%2520white-box%2520system.%250AAdditionally%252C%2520proposed%2520TempoGPT%2520achieves%2520consistent%2520representation%2520between%250Atemporal%2520and%2520textual%2520information%2520by%2520quantizing%2520temporal%2520embeddings%252C%2520where%250Atemporal%2520embeddings%2520are%2520quantized%2520into%2520a%2520series%2520of%2520discrete%2520tokens%2520using%2520a%250Apredefined%2520codebook%253B%2520subsequently%252C%2520a%2520shared%2520embedding%2520layer%2520processes%2520both%250Atemporal%2520and%2520textual%2520tokens.%2520Extensive%2520experiments%2520demonstrate%2520that%2520TempoGPT%250Aaccurately%2520perceives%2520temporal%2520information%252C%2520logically%2520infers%2520conclusions%252C%2520and%250Aachieves%2520state-of-the-art%2520in%2520the%2520constructed%2520complex%2520time%2520series%2520reasoning%250Atasks.%2520Moreover%252C%2520we%2520quantitatively%2520demonstrate%2520the%2520effectiveness%2520of%2520quantizing%250Atemporal%2520embeddings%2520in%2520enhancing%2520multi-modal%2520alignment%2520and%2520the%2520reasoning%250Acapabilities%2520of%2520TLMs.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/zhanghaochuan20/TempoGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TempoGPT%3A%20Enhancing%20Temporal%20Reasoning%20via%20Quantizing%20Embedding&entry.906535625=Haochuan%20Zhang%20and%20Chunhua%20Yang%20and%20Jie%20Han%20and%20Liyang%20Qin%20and%20Xiaoli%20Wang&entry.1292438233=%20%20Multi-modal%20language%20model%20has%20made%20advanced%20progress%20in%20vision%20and%20audio%2C%0Abut%20still%20faces%20significant%20challenges%20in%20dealing%20with%20complex%20reasoning%20tasks%0Ain%20the%20time%20series%20domain.%20The%20reasons%20are%20twofold.%20First%2C%20labels%20for%0Amulti-modal%20time%20series%20data%20are%20coarse%20and%20devoid%20of%20analysis%20or%20reasoning%0Aprocesses.%20Training%20with%20these%20data%20cannot%20improve%20the%20model%27s%20reasoning%0Acapabilities.%20Second%2C%20due%20to%20the%20lack%20of%20precise%20tokenization%20in%20processing%0Atime%20series%2C%20the%20representation%20patterns%20for%20temporal%20and%20textual%20information%0Aare%20inconsistent%2C%20which%20hampers%20the%20effectiveness%20of%20multi-modal%20alignment.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20multi-modal%20time%20series%20data%0Aconstruction%20approach%20and%20a%20multi-modal%20time%20series%20language%20model%20%28TLM%29%2C%0ATempoGPT.%20Specially%2C%20we%20construct%20multi-modal%20data%20for%20complex%20reasoning%20tasks%0Aby%20analyzing%20the%20variable-system%20relationships%20within%20a%20white-box%20system.%0AAdditionally%2C%20proposed%20TempoGPT%20achieves%20consistent%20representation%20between%0Atemporal%20and%20textual%20information%20by%20quantizing%20temporal%20embeddings%2C%20where%0Atemporal%20embeddings%20are%20quantized%20into%20a%20series%20of%20discrete%20tokens%20using%20a%0Apredefined%20codebook%3B%20subsequently%2C%20a%20shared%20embedding%20layer%20processes%20both%0Atemporal%20and%20textual%20tokens.%20Extensive%20experiments%20demonstrate%20that%20TempoGPT%0Aaccurately%20perceives%20temporal%20information%2C%20logically%20infers%20conclusions%2C%20and%0Aachieves%20state-of-the-art%20in%20the%20constructed%20complex%20time%20series%20reasoning%0Atasks.%20Moreover%2C%20we%20quantitatively%20demonstrate%20the%20effectiveness%20of%20quantizing%0Atemporal%20embeddings%20in%20enhancing%20multi-modal%20alignment%20and%20the%20reasoning%0Acapabilities%20of%20TLMs.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/zhanghaochuan20/TempoGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07335v1&entry.124074799=Read"},
{"title": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning", "author": "Tieyuan Chen and Huabin Liu and Yi Wang and Yihang Chen and Tianyao He and Chaofan Gan and Huanyu He and Weiyao Lin", "abstract": "  Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.\n", "link": "http://arxiv.org/abs/2501.07227v1", "date": "2025-01-13", "relevancy": 2.0959, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5346}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MECD%2B%3A%20Unlocking%20Event-Level%20Causal%20Graph%20Discovery%20for%20Video%20Reasoning&body=Title%3A%20MECD%2B%3A%20Unlocking%20Event-Level%20Causal%20Graph%20Discovery%20for%20Video%20Reasoning%0AAuthor%3A%20Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Yi%20Wang%20and%20Yihang%20Chen%20and%20Tianyao%20He%20and%20Chaofan%20Gan%20and%20Huanyu%20He%20and%20Weiyao%20Lin%0AAbstract%3A%20%20%20Video%20causal%20reasoning%20aims%20to%20achieve%20a%20high-level%20understanding%20of%20videos%0Afrom%20a%20causal%20perspective.%20However%2C%20it%20exhibits%20limitations%20in%20its%20scope%2C%0Aprimarily%20executed%20in%20a%20question-answering%20paradigm%20and%20focusing%20on%20brief%20video%0Asegments%20containing%20isolated%20events%20and%20basic%20causal%20relations%2C%20lacking%0Acomprehensive%20and%20structured%20causality%20analysis%20for%20videos%20with%20multiple%0Ainterconnected%20events.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20new%20task%20and%20dataset%2C%0AMulti-Event%20Causal%20Discovery%20%28MECD%29.%20It%20aims%20to%20uncover%20the%20causal%20relations%0Abetween%20events%20distributed%20chronologically%20across%20long%20videos.%20Given%20visual%0Asegments%20and%20textual%20descriptions%20of%20events%2C%20MECD%20identifies%20the%20causal%0Aassociations%20between%20these%20events%20to%20derive%20a%20comprehensive%20and%20structured%0Aevent-level%20video%20causal%20graph%20explaining%20why%20and%20how%20the%20result%20event%0Aoccurred.%20To%20address%20the%20challenges%20of%20MECD%2C%20we%20devise%20a%20novel%20framework%0Ainspired%20by%20the%20Granger%20Causality%20method%2C%20incorporating%20an%20efficient%20mask-based%0Aevent%20prediction%20model%20to%20perform%20an%20Event%20Granger%20Test.%20It%20estimates%20causality%0Aby%20comparing%20the%20predicted%20result%20event%20when%20premise%20events%20are%20masked%20versus%0Aunmasked.%20Furthermore%2C%20we%20integrate%20causal%20inference%20techniques%20such%20as%0Afront-door%20adjustment%20and%20counterfactual%20inference%20to%20mitigate%20challenges%20in%0AMECD%20like%20causality%20confounding%20and%20illusory%20causality.%20Additionally%2C%20context%0Achain%20reasoning%20is%20introduced%20to%20conduct%20more%20robust%20and%20generalized%20reasoning.%0AExperiments%20validate%20the%20effectiveness%20of%20our%20framework%20in%20reasoning%20complete%0Acausal%20relations%2C%20outperforming%20GPT-4o%20and%20VideoChat2%20by%205.77%25%20and%202.70%25%2C%0Arespectively.%20Further%20experiments%20demonstrate%20that%20causal%20relation%20graphs%20can%0Aalso%20contribute%20to%20downstream%20video%20understanding%20tasks%20such%20as%20video%20question%0Aanswering%20and%20video%20event%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMECD%252B%253A%2520Unlocking%2520Event-Level%2520Causal%2520Graph%2520Discovery%2520for%2520Video%2520Reasoning%26entry.906535625%3DTieyuan%2520Chen%2520and%2520Huabin%2520Liu%2520and%2520Yi%2520Wang%2520and%2520Yihang%2520Chen%2520and%2520Tianyao%2520He%2520and%2520Chaofan%2520Gan%2520and%2520Huanyu%2520He%2520and%2520Weiyao%2520Lin%26entry.1292438233%3D%2520%2520Video%2520causal%2520reasoning%2520aims%2520to%2520achieve%2520a%2520high-level%2520understanding%2520of%2520videos%250Afrom%2520a%2520causal%2520perspective.%2520However%252C%2520it%2520exhibits%2520limitations%2520in%2520its%2520scope%252C%250Aprimarily%2520executed%2520in%2520a%2520question-answering%2520paradigm%2520and%2520focusing%2520on%2520brief%2520video%250Asegments%2520containing%2520isolated%2520events%2520and%2520basic%2520causal%2520relations%252C%2520lacking%250Acomprehensive%2520and%2520structured%2520causality%2520analysis%2520for%2520videos%2520with%2520multiple%250Ainterconnected%2520events.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520a%2520new%2520task%2520and%2520dataset%252C%250AMulti-Event%2520Causal%2520Discovery%2520%2528MECD%2529.%2520It%2520aims%2520to%2520uncover%2520the%2520causal%2520relations%250Abetween%2520events%2520distributed%2520chronologically%2520across%2520long%2520videos.%2520Given%2520visual%250Asegments%2520and%2520textual%2520descriptions%2520of%2520events%252C%2520MECD%2520identifies%2520the%2520causal%250Aassociations%2520between%2520these%2520events%2520to%2520derive%2520a%2520comprehensive%2520and%2520structured%250Aevent-level%2520video%2520causal%2520graph%2520explaining%2520why%2520and%2520how%2520the%2520result%2520event%250Aoccurred.%2520To%2520address%2520the%2520challenges%2520of%2520MECD%252C%2520we%2520devise%2520a%2520novel%2520framework%250Ainspired%2520by%2520the%2520Granger%2520Causality%2520method%252C%2520incorporating%2520an%2520efficient%2520mask-based%250Aevent%2520prediction%2520model%2520to%2520perform%2520an%2520Event%2520Granger%2520Test.%2520It%2520estimates%2520causality%250Aby%2520comparing%2520the%2520predicted%2520result%2520event%2520when%2520premise%2520events%2520are%2520masked%2520versus%250Aunmasked.%2520Furthermore%252C%2520we%2520integrate%2520causal%2520inference%2520techniques%2520such%2520as%250Afront-door%2520adjustment%2520and%2520counterfactual%2520inference%2520to%2520mitigate%2520challenges%2520in%250AMECD%2520like%2520causality%2520confounding%2520and%2520illusory%2520causality.%2520Additionally%252C%2520context%250Achain%2520reasoning%2520is%2520introduced%2520to%2520conduct%2520more%2520robust%2520and%2520generalized%2520reasoning.%250AExperiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520framework%2520in%2520reasoning%2520complete%250Acausal%2520relations%252C%2520outperforming%2520GPT-4o%2520and%2520VideoChat2%2520by%25205.77%2525%2520and%25202.70%2525%252C%250Arespectively.%2520Further%2520experiments%2520demonstrate%2520that%2520causal%2520relation%2520graphs%2520can%250Aalso%2520contribute%2520to%2520downstream%2520video%2520understanding%2520tasks%2520such%2520as%2520video%2520question%250Aanswering%2520and%2520video%2520event%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MECD%2B%3A%20Unlocking%20Event-Level%20Causal%20Graph%20Discovery%20for%20Video%20Reasoning&entry.906535625=Tieyuan%20Chen%20and%20Huabin%20Liu%20and%20Yi%20Wang%20and%20Yihang%20Chen%20and%20Tianyao%20He%20and%20Chaofan%20Gan%20and%20Huanyu%20He%20and%20Weiyao%20Lin&entry.1292438233=%20%20Video%20causal%20reasoning%20aims%20to%20achieve%20a%20high-level%20understanding%20of%20videos%0Afrom%20a%20causal%20perspective.%20However%2C%20it%20exhibits%20limitations%20in%20its%20scope%2C%0Aprimarily%20executed%20in%20a%20question-answering%20paradigm%20and%20focusing%20on%20brief%20video%0Asegments%20containing%20isolated%20events%20and%20basic%20causal%20relations%2C%20lacking%0Acomprehensive%20and%20structured%20causality%20analysis%20for%20videos%20with%20multiple%0Ainterconnected%20events.%20To%20fill%20this%20gap%2C%20we%20introduce%20a%20new%20task%20and%20dataset%2C%0AMulti-Event%20Causal%20Discovery%20%28MECD%29.%20It%20aims%20to%20uncover%20the%20causal%20relations%0Abetween%20events%20distributed%20chronologically%20across%20long%20videos.%20Given%20visual%0Asegments%20and%20textual%20descriptions%20of%20events%2C%20MECD%20identifies%20the%20causal%0Aassociations%20between%20these%20events%20to%20derive%20a%20comprehensive%20and%20structured%0Aevent-level%20video%20causal%20graph%20explaining%20why%20and%20how%20the%20result%20event%0Aoccurred.%20To%20address%20the%20challenges%20of%20MECD%2C%20we%20devise%20a%20novel%20framework%0Ainspired%20by%20the%20Granger%20Causality%20method%2C%20incorporating%20an%20efficient%20mask-based%0Aevent%20prediction%20model%20to%20perform%20an%20Event%20Granger%20Test.%20It%20estimates%20causality%0Aby%20comparing%20the%20predicted%20result%20event%20when%20premise%20events%20are%20masked%20versus%0Aunmasked.%20Furthermore%2C%20we%20integrate%20causal%20inference%20techniques%20such%20as%0Afront-door%20adjustment%20and%20counterfactual%20inference%20to%20mitigate%20challenges%20in%0AMECD%20like%20causality%20confounding%20and%20illusory%20causality.%20Additionally%2C%20context%0Achain%20reasoning%20is%20introduced%20to%20conduct%20more%20robust%20and%20generalized%20reasoning.%0AExperiments%20validate%20the%20effectiveness%20of%20our%20framework%20in%20reasoning%20complete%0Acausal%20relations%2C%20outperforming%20GPT-4o%20and%20VideoChat2%20by%205.77%25%20and%202.70%25%2C%0Arespectively.%20Further%20experiments%20demonstrate%20that%20causal%20relation%20graphs%20can%0Aalso%20contribute%20to%20downstream%20video%20understanding%20tasks%20such%20as%20video%20question%0Aanswering%20and%20video%20event%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07227v1&entry.124074799=Read"},
{"title": "TimeLogic: A Temporal Logic Benchmark for Video QA", "author": "Sirnam Swetha and Hilde Kuehne and Mubarak Shah", "abstract": "  Temporal logical understanding, a core facet of human cognition, plays a\npivotal role in capturing complex sequential events and their temporal\nrelationships within videos. This capability is particularly crucial in tasks\nlike Video Question Answering (VideoQA), where the goal is to process visual\ndata over time together with textual data to provide coherent answers. However,\ncurrent VideoQA benchmarks devote little focus to evaluating this critical\nskill due to the challenge of annotating temporal logic. Despite the\nadvancement of vision-language models, assessing their temporal logical\nreasoning powers remains a challenge, primarily due to the lack QA pairs that\ndemand formal, complex temporal reasoning. To bridge this gap, we introduce the\nTimeLogic QA (TLQA) framework to automatically generate the QA pairs,\nspecifically designed to evaluate the temporal logical understanding. To this\nend, TLQA leverages temporal annotations from existing video datasets together\nwith temporal operators derived from logic theory to construct questions that\ntest understanding of event sequences and their temporal relationships. TLQA\nframework is generic and scalable, capable of leveraging both, existing video\naction datasets with temporal action segmentation annotations, or video\ndatasets with temporal scene graph annotations, to automatically generate\ntemporal logical questions. We leverage 4 datasets, STAR, Breakfast, AGQA, and\nCrossTask, and generate two VideoQA dataset variants - small (TLQA-S) and large\n(TLQA-L) - containing 2k and 10k QA pairs for each category, resulting in 32k\nand 160k total pairs per dataset. We undertake a comprehensive evaluation of\nleading-edge VideoQA models, employing the TLQA to benchmark their temporal\nlogical understanding capabilities. We assess the VideoQA model's temporal\nreasoning performance on 16 categories of temporal logic with varying temporal\ncomplexity.\n", "link": "http://arxiv.org/abs/2501.07214v1", "date": "2025-01-13", "relevancy": 2.0952, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5297}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeLogic%3A%20A%20Temporal%20Logic%20Benchmark%20for%20Video%20QA&body=Title%3A%20TimeLogic%3A%20A%20Temporal%20Logic%20Benchmark%20for%20Video%20QA%0AAuthor%3A%20Sirnam%20Swetha%20and%20Hilde%20Kuehne%20and%20Mubarak%20Shah%0AAbstract%3A%20%20%20Temporal%20logical%20understanding%2C%20a%20core%20facet%20of%20human%20cognition%2C%20plays%20a%0Apivotal%20role%20in%20capturing%20complex%20sequential%20events%20and%20their%20temporal%0Arelationships%20within%20videos.%20This%20capability%20is%20particularly%20crucial%20in%20tasks%0Alike%20Video%20Question%20Answering%20%28VideoQA%29%2C%20where%20the%20goal%20is%20to%20process%20visual%0Adata%20over%20time%20together%20with%20textual%20data%20to%20provide%20coherent%20answers.%20However%2C%0Acurrent%20VideoQA%20benchmarks%20devote%20little%20focus%20to%20evaluating%20this%20critical%0Askill%20due%20to%20the%20challenge%20of%20annotating%20temporal%20logic.%20Despite%20the%0Aadvancement%20of%20vision-language%20models%2C%20assessing%20their%20temporal%20logical%0Areasoning%20powers%20remains%20a%20challenge%2C%20primarily%20due%20to%20the%20lack%20QA%20pairs%20that%0Ademand%20formal%2C%20complex%20temporal%20reasoning.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%0ATimeLogic%20QA%20%28TLQA%29%20framework%20to%20automatically%20generate%20the%20QA%20pairs%2C%0Aspecifically%20designed%20to%20evaluate%20the%20temporal%20logical%20understanding.%20To%20this%0Aend%2C%20TLQA%20leverages%20temporal%20annotations%20from%20existing%20video%20datasets%20together%0Awith%20temporal%20operators%20derived%20from%20logic%20theory%20to%20construct%20questions%20that%0Atest%20understanding%20of%20event%20sequences%20and%20their%20temporal%20relationships.%20TLQA%0Aframework%20is%20generic%20and%20scalable%2C%20capable%20of%20leveraging%20both%2C%20existing%20video%0Aaction%20datasets%20with%20temporal%20action%20segmentation%20annotations%2C%20or%20video%0Adatasets%20with%20temporal%20scene%20graph%20annotations%2C%20to%20automatically%20generate%0Atemporal%20logical%20questions.%20We%20leverage%204%20datasets%2C%20STAR%2C%20Breakfast%2C%20AGQA%2C%20and%0ACrossTask%2C%20and%20generate%20two%20VideoQA%20dataset%20variants%20-%20small%20%28TLQA-S%29%20and%20large%0A%28TLQA-L%29%20-%20containing%202k%20and%2010k%20QA%20pairs%20for%20each%20category%2C%20resulting%20in%2032k%0Aand%20160k%20total%20pairs%20per%20dataset.%20We%20undertake%20a%20comprehensive%20evaluation%20of%0Aleading-edge%20VideoQA%20models%2C%20employing%20the%20TLQA%20to%20benchmark%20their%20temporal%0Alogical%20understanding%20capabilities.%20We%20assess%20the%20VideoQA%20model%27s%20temporal%0Areasoning%20performance%20on%2016%20categories%20of%20temporal%20logic%20with%20varying%20temporal%0Acomplexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeLogic%253A%2520A%2520Temporal%2520Logic%2520Benchmark%2520for%2520Video%2520QA%26entry.906535625%3DSirnam%2520Swetha%2520and%2520Hilde%2520Kuehne%2520and%2520Mubarak%2520Shah%26entry.1292438233%3D%2520%2520Temporal%2520logical%2520understanding%252C%2520a%2520core%2520facet%2520of%2520human%2520cognition%252C%2520plays%2520a%250Apivotal%2520role%2520in%2520capturing%2520complex%2520sequential%2520events%2520and%2520their%2520temporal%250Arelationships%2520within%2520videos.%2520This%2520capability%2520is%2520particularly%2520crucial%2520in%2520tasks%250Alike%2520Video%2520Question%2520Answering%2520%2528VideoQA%2529%252C%2520where%2520the%2520goal%2520is%2520to%2520process%2520visual%250Adata%2520over%2520time%2520together%2520with%2520textual%2520data%2520to%2520provide%2520coherent%2520answers.%2520However%252C%250Acurrent%2520VideoQA%2520benchmarks%2520devote%2520little%2520focus%2520to%2520evaluating%2520this%2520critical%250Askill%2520due%2520to%2520the%2520challenge%2520of%2520annotating%2520temporal%2520logic.%2520Despite%2520the%250Aadvancement%2520of%2520vision-language%2520models%252C%2520assessing%2520their%2520temporal%2520logical%250Areasoning%2520powers%2520remains%2520a%2520challenge%252C%2520primarily%2520due%2520to%2520the%2520lack%2520QA%2520pairs%2520that%250Ademand%2520formal%252C%2520complex%2520temporal%2520reasoning.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520the%250ATimeLogic%2520QA%2520%2528TLQA%2529%2520framework%2520to%2520automatically%2520generate%2520the%2520QA%2520pairs%252C%250Aspecifically%2520designed%2520to%2520evaluate%2520the%2520temporal%2520logical%2520understanding.%2520To%2520this%250Aend%252C%2520TLQA%2520leverages%2520temporal%2520annotations%2520from%2520existing%2520video%2520datasets%2520together%250Awith%2520temporal%2520operators%2520derived%2520from%2520logic%2520theory%2520to%2520construct%2520questions%2520that%250Atest%2520understanding%2520of%2520event%2520sequences%2520and%2520their%2520temporal%2520relationships.%2520TLQA%250Aframework%2520is%2520generic%2520and%2520scalable%252C%2520capable%2520of%2520leveraging%2520both%252C%2520existing%2520video%250Aaction%2520datasets%2520with%2520temporal%2520action%2520segmentation%2520annotations%252C%2520or%2520video%250Adatasets%2520with%2520temporal%2520scene%2520graph%2520annotations%252C%2520to%2520automatically%2520generate%250Atemporal%2520logical%2520questions.%2520We%2520leverage%25204%2520datasets%252C%2520STAR%252C%2520Breakfast%252C%2520AGQA%252C%2520and%250ACrossTask%252C%2520and%2520generate%2520two%2520VideoQA%2520dataset%2520variants%2520-%2520small%2520%2528TLQA-S%2529%2520and%2520large%250A%2528TLQA-L%2529%2520-%2520containing%25202k%2520and%252010k%2520QA%2520pairs%2520for%2520each%2520category%252C%2520resulting%2520in%252032k%250Aand%2520160k%2520total%2520pairs%2520per%2520dataset.%2520We%2520undertake%2520a%2520comprehensive%2520evaluation%2520of%250Aleading-edge%2520VideoQA%2520models%252C%2520employing%2520the%2520TLQA%2520to%2520benchmark%2520their%2520temporal%250Alogical%2520understanding%2520capabilities.%2520We%2520assess%2520the%2520VideoQA%2520model%2527s%2520temporal%250Areasoning%2520performance%2520on%252016%2520categories%2520of%2520temporal%2520logic%2520with%2520varying%2520temporal%250Acomplexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeLogic%3A%20A%20Temporal%20Logic%20Benchmark%20for%20Video%20QA&entry.906535625=Sirnam%20Swetha%20and%20Hilde%20Kuehne%20and%20Mubarak%20Shah&entry.1292438233=%20%20Temporal%20logical%20understanding%2C%20a%20core%20facet%20of%20human%20cognition%2C%20plays%20a%0Apivotal%20role%20in%20capturing%20complex%20sequential%20events%20and%20their%20temporal%0Arelationships%20within%20videos.%20This%20capability%20is%20particularly%20crucial%20in%20tasks%0Alike%20Video%20Question%20Answering%20%28VideoQA%29%2C%20where%20the%20goal%20is%20to%20process%20visual%0Adata%20over%20time%20together%20with%20textual%20data%20to%20provide%20coherent%20answers.%20However%2C%0Acurrent%20VideoQA%20benchmarks%20devote%20little%20focus%20to%20evaluating%20this%20critical%0Askill%20due%20to%20the%20challenge%20of%20annotating%20temporal%20logic.%20Despite%20the%0Aadvancement%20of%20vision-language%20models%2C%20assessing%20their%20temporal%20logical%0Areasoning%20powers%20remains%20a%20challenge%2C%20primarily%20due%20to%20the%20lack%20QA%20pairs%20that%0Ademand%20formal%2C%20complex%20temporal%20reasoning.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%0ATimeLogic%20QA%20%28TLQA%29%20framework%20to%20automatically%20generate%20the%20QA%20pairs%2C%0Aspecifically%20designed%20to%20evaluate%20the%20temporal%20logical%20understanding.%20To%20this%0Aend%2C%20TLQA%20leverages%20temporal%20annotations%20from%20existing%20video%20datasets%20together%0Awith%20temporal%20operators%20derived%20from%20logic%20theory%20to%20construct%20questions%20that%0Atest%20understanding%20of%20event%20sequences%20and%20their%20temporal%20relationships.%20TLQA%0Aframework%20is%20generic%20and%20scalable%2C%20capable%20of%20leveraging%20both%2C%20existing%20video%0Aaction%20datasets%20with%20temporal%20action%20segmentation%20annotations%2C%20or%20video%0Adatasets%20with%20temporal%20scene%20graph%20annotations%2C%20to%20automatically%20generate%0Atemporal%20logical%20questions.%20We%20leverage%204%20datasets%2C%20STAR%2C%20Breakfast%2C%20AGQA%2C%20and%0ACrossTask%2C%20and%20generate%20two%20VideoQA%20dataset%20variants%20-%20small%20%28TLQA-S%29%20and%20large%0A%28TLQA-L%29%20-%20containing%202k%20and%2010k%20QA%20pairs%20for%20each%20category%2C%20resulting%20in%2032k%0Aand%20160k%20total%20pairs%20per%20dataset.%20We%20undertake%20a%20comprehensive%20evaluation%20of%0Aleading-edge%20VideoQA%20models%2C%20employing%20the%20TLQA%20to%20benchmark%20their%20temporal%0Alogical%20understanding%20capabilities.%20We%20assess%20the%20VideoQA%20model%27s%20temporal%0Areasoning%20performance%20on%2016%20categories%20of%20temporal%20logic%20with%20varying%20temporal%0Acomplexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07214v1&entry.124074799=Read"},
{"title": "Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning", "author": "Daniel Schug and Tyler J. Kovach and M. A. Wolfe and Jared Benson and Sanghyeok Park and J. P. Dodson and J. Corrigan and M. A. Eriksson and Justyna P. Zwolak", "abstract": "  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. This work demonstrates the feasibility and advantages of applying\nexplainable machine learning techniques to the analysis of quantum dot\nmeasurements, paving the way for further advances in automated and transparent\nQD device tuning. Many of the measurements acquired during the tuning process\ncome in the form of images that need to be properly analyzed to guide the\nsubsequent tuning steps. By design, features present in such images capture\ncertain behaviors or states of the measured QD devices. When considered\ncarefully, such features can aid the control and calibration of QD devices. An\nimportant example of such images are so-called $\\textit{triangle plots}$, which\nvisually represent current flow and reveal characteristics important for QD\ndevice calibration. While image-based classification tools, such as\nconvolutional neural networks (CNNs), can be used to verify whether a given\nmeasurement is $\\textit{good}$ and thus warrants the initiation of the next\nphase of tuning, they do not provide any insights into how the device should be\nadjusted in the case of $\\textit{bad}$ images. This is because CNNs sacrifice\nprediction and model intelligibility for high accuracy. To ameliorate this\ntrade-off, a recent study introduced an image vectorization approach that\nrelies on the Gabor wavelet transform (Schug $\\textit{et al.}$ 2024\n$\\textit{Proc. XAI4Sci: Explainable Machine Learning for Sciences Workshop\n(AAAI 2024) (Vancouver, Canada)}$ pp 1-6). Here we propose an alternative\nvectorization method that involves mathematical modeling of synthetic triangles\nto mimic the experimental data. Using explainable boosting machines, we show\nthat this new method offers superior explainability of model prediction without\nsacrificing accuracy.\n", "link": "http://arxiv.org/abs/2402.13699v5", "date": "2025-01-13", "relevancy": 2.0738, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automation%20of%20Quantum%20Dot%20Measurement%20Analysis%20via%20Explainable%20Machine%0A%20%20Learning&body=Title%3A%20Automation%20of%20Quantum%20Dot%20Measurement%20Analysis%20via%20Explainable%20Machine%0A%20%20Learning%0AAuthor%3A%20Daniel%20Schug%20and%20Tyler%20J.%20Kovach%20and%20M.%20A.%20Wolfe%20and%20Jared%20Benson%20and%20Sanghyeok%20Park%20and%20J.%20P.%20Dodson%20and%20J.%20Corrigan%20and%20M.%20A.%20Eriksson%20and%20Justyna%20P.%20Zwolak%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20quantum%20dot%20%28QD%29%20devices%20for%20quantum%20computing%20has%0Anecessitated%20more%20efficient%20and%20automated%20methods%20for%20device%20characterization%0Aand%20tuning.%20This%20work%20demonstrates%20the%20feasibility%20and%20advantages%20of%20applying%0Aexplainable%20machine%20learning%20techniques%20to%20the%20analysis%20of%20quantum%20dot%0Ameasurements%2C%20paving%20the%20way%20for%20further%20advances%20in%20automated%20and%20transparent%0AQD%20device%20tuning.%20Many%20of%20the%20measurements%20acquired%20during%20the%20tuning%20process%0Acome%20in%20the%20form%20of%20images%20that%20need%20to%20be%20properly%20analyzed%20to%20guide%20the%0Asubsequent%20tuning%20steps.%20By%20design%2C%20features%20present%20in%20such%20images%20capture%0Acertain%20behaviors%20or%20states%20of%20the%20measured%20QD%20devices.%20When%20considered%0Acarefully%2C%20such%20features%20can%20aid%20the%20control%20and%20calibration%20of%20QD%20devices.%20An%0Aimportant%20example%20of%20such%20images%20are%20so-called%20%24%5Ctextit%7Btriangle%20plots%7D%24%2C%20which%0Avisually%20represent%20current%20flow%20and%20reveal%20characteristics%20important%20for%20QD%0Adevice%20calibration.%20While%20image-based%20classification%20tools%2C%20such%20as%0Aconvolutional%20neural%20networks%20%28CNNs%29%2C%20can%20be%20used%20to%20verify%20whether%20a%20given%0Ameasurement%20is%20%24%5Ctextit%7Bgood%7D%24%20and%20thus%20warrants%20the%20initiation%20of%20the%20next%0Aphase%20of%20tuning%2C%20they%20do%20not%20provide%20any%20insights%20into%20how%20the%20device%20should%20be%0Aadjusted%20in%20the%20case%20of%20%24%5Ctextit%7Bbad%7D%24%20images.%20This%20is%20because%20CNNs%20sacrifice%0Aprediction%20and%20model%20intelligibility%20for%20high%20accuracy.%20To%20ameliorate%20this%0Atrade-off%2C%20a%20recent%20study%20introduced%20an%20image%20vectorization%20approach%20that%0Arelies%20on%20the%20Gabor%20wavelet%20transform%20%28Schug%20%24%5Ctextit%7Bet%20al.%7D%24%202024%0A%24%5Ctextit%7BProc.%20XAI4Sci%3A%20Explainable%20Machine%20Learning%20for%20Sciences%20Workshop%0A%28AAAI%202024%29%20%28Vancouver%2C%20Canada%29%7D%24%20pp%201-6%29.%20Here%20we%20propose%20an%20alternative%0Avectorization%20method%20that%20involves%20mathematical%20modeling%20of%20synthetic%20triangles%0Ato%20mimic%20the%20experimental%20data.%20Using%20explainable%20boosting%20machines%2C%20we%20show%0Athat%20this%20new%20method%20offers%20superior%20explainability%20of%20model%20prediction%20without%0Asacrificing%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13699v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomation%2520of%2520Quantum%2520Dot%2520Measurement%2520Analysis%2520via%2520Explainable%2520Machine%250A%2520%2520Learning%26entry.906535625%3DDaniel%2520Schug%2520and%2520Tyler%2520J.%2520Kovach%2520and%2520M.%2520A.%2520Wolfe%2520and%2520Jared%2520Benson%2520and%2520Sanghyeok%2520Park%2520and%2520J.%2520P.%2520Dodson%2520and%2520J.%2520Corrigan%2520and%2520M.%2520A.%2520Eriksson%2520and%2520Justyna%2520P.%2520Zwolak%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520quantum%2520dot%2520%2528QD%2529%2520devices%2520for%2520quantum%2520computing%2520has%250Anecessitated%2520more%2520efficient%2520and%2520automated%2520methods%2520for%2520device%2520characterization%250Aand%2520tuning.%2520This%2520work%2520demonstrates%2520the%2520feasibility%2520and%2520advantages%2520of%2520applying%250Aexplainable%2520machine%2520learning%2520techniques%2520to%2520the%2520analysis%2520of%2520quantum%2520dot%250Ameasurements%252C%2520paving%2520the%2520way%2520for%2520further%2520advances%2520in%2520automated%2520and%2520transparent%250AQD%2520device%2520tuning.%2520Many%2520of%2520the%2520measurements%2520acquired%2520during%2520the%2520tuning%2520process%250Acome%2520in%2520the%2520form%2520of%2520images%2520that%2520need%2520to%2520be%2520properly%2520analyzed%2520to%2520guide%2520the%250Asubsequent%2520tuning%2520steps.%2520By%2520design%252C%2520features%2520present%2520in%2520such%2520images%2520capture%250Acertain%2520behaviors%2520or%2520states%2520of%2520the%2520measured%2520QD%2520devices.%2520When%2520considered%250Acarefully%252C%2520such%2520features%2520can%2520aid%2520the%2520control%2520and%2520calibration%2520of%2520QD%2520devices.%2520An%250Aimportant%2520example%2520of%2520such%2520images%2520are%2520so-called%2520%2524%255Ctextit%257Btriangle%2520plots%257D%2524%252C%2520which%250Avisually%2520represent%2520current%2520flow%2520and%2520reveal%2520characteristics%2520important%2520for%2520QD%250Adevice%2520calibration.%2520While%2520image-based%2520classification%2520tools%252C%2520such%2520as%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520can%2520be%2520used%2520to%2520verify%2520whether%2520a%2520given%250Ameasurement%2520is%2520%2524%255Ctextit%257Bgood%257D%2524%2520and%2520thus%2520warrants%2520the%2520initiation%2520of%2520the%2520next%250Aphase%2520of%2520tuning%252C%2520they%2520do%2520not%2520provide%2520any%2520insights%2520into%2520how%2520the%2520device%2520should%2520be%250Aadjusted%2520in%2520the%2520case%2520of%2520%2524%255Ctextit%257Bbad%257D%2524%2520images.%2520This%2520is%2520because%2520CNNs%2520sacrifice%250Aprediction%2520and%2520model%2520intelligibility%2520for%2520high%2520accuracy.%2520To%2520ameliorate%2520this%250Atrade-off%252C%2520a%2520recent%2520study%2520introduced%2520an%2520image%2520vectorization%2520approach%2520that%250Arelies%2520on%2520the%2520Gabor%2520wavelet%2520transform%2520%2528Schug%2520%2524%255Ctextit%257Bet%2520al.%257D%2524%25202024%250A%2524%255Ctextit%257BProc.%2520XAI4Sci%253A%2520Explainable%2520Machine%2520Learning%2520for%2520Sciences%2520Workshop%250A%2528AAAI%25202024%2529%2520%2528Vancouver%252C%2520Canada%2529%257D%2524%2520pp%25201-6%2529.%2520Here%2520we%2520propose%2520an%2520alternative%250Avectorization%2520method%2520that%2520involves%2520mathematical%2520modeling%2520of%2520synthetic%2520triangles%250Ato%2520mimic%2520the%2520experimental%2520data.%2520Using%2520explainable%2520boosting%2520machines%252C%2520we%2520show%250Athat%2520this%2520new%2520method%2520offers%2520superior%2520explainability%2520of%2520model%2520prediction%2520without%250Asacrificing%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13699v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automation%20of%20Quantum%20Dot%20Measurement%20Analysis%20via%20Explainable%20Machine%0A%20%20Learning&entry.906535625=Daniel%20Schug%20and%20Tyler%20J.%20Kovach%20and%20M.%20A.%20Wolfe%20and%20Jared%20Benson%20and%20Sanghyeok%20Park%20and%20J.%20P.%20Dodson%20and%20J.%20Corrigan%20and%20M.%20A.%20Eriksson%20and%20Justyna%20P.%20Zwolak&entry.1292438233=%20%20The%20rapid%20development%20of%20quantum%20dot%20%28QD%29%20devices%20for%20quantum%20computing%20has%0Anecessitated%20more%20efficient%20and%20automated%20methods%20for%20device%20characterization%0Aand%20tuning.%20This%20work%20demonstrates%20the%20feasibility%20and%20advantages%20of%20applying%0Aexplainable%20machine%20learning%20techniques%20to%20the%20analysis%20of%20quantum%20dot%0Ameasurements%2C%20paving%20the%20way%20for%20further%20advances%20in%20automated%20and%20transparent%0AQD%20device%20tuning.%20Many%20of%20the%20measurements%20acquired%20during%20the%20tuning%20process%0Acome%20in%20the%20form%20of%20images%20that%20need%20to%20be%20properly%20analyzed%20to%20guide%20the%0Asubsequent%20tuning%20steps.%20By%20design%2C%20features%20present%20in%20such%20images%20capture%0Acertain%20behaviors%20or%20states%20of%20the%20measured%20QD%20devices.%20When%20considered%0Acarefully%2C%20such%20features%20can%20aid%20the%20control%20and%20calibration%20of%20QD%20devices.%20An%0Aimportant%20example%20of%20such%20images%20are%20so-called%20%24%5Ctextit%7Btriangle%20plots%7D%24%2C%20which%0Avisually%20represent%20current%20flow%20and%20reveal%20characteristics%20important%20for%20QD%0Adevice%20calibration.%20While%20image-based%20classification%20tools%2C%20such%20as%0Aconvolutional%20neural%20networks%20%28CNNs%29%2C%20can%20be%20used%20to%20verify%20whether%20a%20given%0Ameasurement%20is%20%24%5Ctextit%7Bgood%7D%24%20and%20thus%20warrants%20the%20initiation%20of%20the%20next%0Aphase%20of%20tuning%2C%20they%20do%20not%20provide%20any%20insights%20into%20how%20the%20device%20should%20be%0Aadjusted%20in%20the%20case%20of%20%24%5Ctextit%7Bbad%7D%24%20images.%20This%20is%20because%20CNNs%20sacrifice%0Aprediction%20and%20model%20intelligibility%20for%20high%20accuracy.%20To%20ameliorate%20this%0Atrade-off%2C%20a%20recent%20study%20introduced%20an%20image%20vectorization%20approach%20that%0Arelies%20on%20the%20Gabor%20wavelet%20transform%20%28Schug%20%24%5Ctextit%7Bet%20al.%7D%24%202024%0A%24%5Ctextit%7BProc.%20XAI4Sci%3A%20Explainable%20Machine%20Learning%20for%20Sciences%20Workshop%0A%28AAAI%202024%29%20%28Vancouver%2C%20Canada%29%7D%24%20pp%201-6%29.%20Here%20we%20propose%20an%20alternative%0Avectorization%20method%20that%20involves%20mathematical%20modeling%20of%20synthetic%20triangles%0Ato%20mimic%20the%20experimental%20data.%20Using%20explainable%20boosting%20machines%2C%20we%20show%0Athat%20this%20new%20method%20offers%20superior%20explainability%20of%20model%20prediction%20without%0Asacrificing%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13699v5&entry.124074799=Read"},
{"title": "FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image\n  Segmentation", "author": "Zhipeng Deng and Zhe Xu and Tsuyoshi Isshiki and Yefeng Zheng", "abstract": "  Medical image segmentation is challenging due to the diversity of medical\nimages and the lack of labeled data, which motivates recent developments in\nfederated semi-supervised learning (FSSL) to leverage a large amount of\nunlabeled data from multiple centers for model training without sharing raw\ndata. However, what remains under-explored in FSSL is the domain shift problem\nwhich may cause suboptimal model aggregation and low effectivity of the\nutilization of unlabeled data, eventually leading to unsatisfactory performance\nin unseen domains. In this paper, we explore this previously ignored scenario,\nnamely domain generalized federated semi-supervised learning (FedSemiDG), which\naims to learn a model in a distributed manner from multiple domains with\nlimited labeled data and abundant unlabeled data such that the model can\ngeneralize well to unseen domains. We present a novel framework, Federated\nGeneralization-Aware SemiSupervised Learning (FGASL), to address the challenges\nin FedSemiDG by effectively tackling critical issues at both global and local\nlevels. Globally, we introduce Generalization-Aware Aggregation (GAA),\nassigning adaptive weights to local models based on their generalization\nperformance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement\n(DR) strategy to combine global and domain-specific knowledge, generating more\nreliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA)\nenforces feature consistency under perturbations, promoting domain-invariant\nlearning. Extensive experiments on three medical segmentation tasks (cardiac\nMRI, spine MRI and bladder cancer MRI) demonstrate that our method\nsignificantly outperforms state-of-the-art FSSL and domain generalization\napproaches, achieving robust generalization on unseen domains.\n", "link": "http://arxiv.org/abs/2501.07378v1", "date": "2025-01-13", "relevancy": 2.0502, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5413}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5161}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedSemiDG%3A%20Domain%20Generalized%20Federated%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20FedSemiDG%3A%20Domain%20Generalized%20Federated%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Zhipeng%20Deng%20and%20Zhe%20Xu%20and%20Tsuyoshi%20Isshiki%20and%20Yefeng%20Zheng%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20challenging%20due%20to%20the%20diversity%20of%20medical%0Aimages%20and%20the%20lack%20of%20labeled%20data%2C%20which%20motivates%20recent%20developments%20in%0Afederated%20semi-supervised%20learning%20%28FSSL%29%20to%20leverage%20a%20large%20amount%20of%0Aunlabeled%20data%20from%20multiple%20centers%20for%20model%20training%20without%20sharing%20raw%0Adata.%20However%2C%20what%20remains%20under-explored%20in%20FSSL%20is%20the%20domain%20shift%20problem%0Awhich%20may%20cause%20suboptimal%20model%20aggregation%20and%20low%20effectivity%20of%20the%0Autilization%20of%20unlabeled%20data%2C%20eventually%20leading%20to%20unsatisfactory%20performance%0Ain%20unseen%20domains.%20In%20this%20paper%2C%20we%20explore%20this%20previously%20ignored%20scenario%2C%0Anamely%20domain%20generalized%20federated%20semi-supervised%20learning%20%28FedSemiDG%29%2C%20which%0Aaims%20to%20learn%20a%20model%20in%20a%20distributed%20manner%20from%20multiple%20domains%20with%0Alimited%20labeled%20data%20and%20abundant%20unlabeled%20data%20such%20that%20the%20model%20can%0Ageneralize%20well%20to%20unseen%20domains.%20We%20present%20a%20novel%20framework%2C%20Federated%0AGeneralization-Aware%20SemiSupervised%20Learning%20%28FGASL%29%2C%20to%20address%20the%20challenges%0Ain%20FedSemiDG%20by%20effectively%20tackling%20critical%20issues%20at%20both%20global%20and%20local%0Alevels.%20Globally%2C%20we%20introduce%20Generalization-Aware%20Aggregation%20%28GAA%29%2C%0Aassigning%20adaptive%20weights%20to%20local%20models%20based%20on%20their%20generalization%0Aperformance.%20Locally%2C%20we%20use%20a%20Dual-Teacher%20Adaptive%20Pseudo%20Label%20Refinement%0A%28DR%29%20strategy%20to%20combine%20global%20and%20domain-specific%20knowledge%2C%20generating%20more%0Areliable%20pseudo%20labels.%20Additionally%2C%20Perturbation-Invariant%20Alignment%20%28PIA%29%0Aenforces%20feature%20consistency%20under%20perturbations%2C%20promoting%20domain-invariant%0Alearning.%20Extensive%20experiments%20on%20three%20medical%20segmentation%20tasks%20%28cardiac%0AMRI%2C%20spine%20MRI%20and%20bladder%20cancer%20MRI%29%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20FSSL%20and%20domain%20generalization%0Aapproaches%2C%20achieving%20robust%20generalization%20on%20unseen%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedSemiDG%253A%2520Domain%2520Generalized%2520Federated%2520Semi-supervised%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DZhipeng%2520Deng%2520and%2520Zhe%2520Xu%2520and%2520Tsuyoshi%2520Isshiki%2520and%2520Yefeng%2520Zheng%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520challenging%2520due%2520to%2520the%2520diversity%2520of%2520medical%250Aimages%2520and%2520the%2520lack%2520of%2520labeled%2520data%252C%2520which%2520motivates%2520recent%2520developments%2520in%250Afederated%2520semi-supervised%2520learning%2520%2528FSSL%2529%2520to%2520leverage%2520a%2520large%2520amount%2520of%250Aunlabeled%2520data%2520from%2520multiple%2520centers%2520for%2520model%2520training%2520without%2520sharing%2520raw%250Adata.%2520However%252C%2520what%2520remains%2520under-explored%2520in%2520FSSL%2520is%2520the%2520domain%2520shift%2520problem%250Awhich%2520may%2520cause%2520suboptimal%2520model%2520aggregation%2520and%2520low%2520effectivity%2520of%2520the%250Autilization%2520of%2520unlabeled%2520data%252C%2520eventually%2520leading%2520to%2520unsatisfactory%2520performance%250Ain%2520unseen%2520domains.%2520In%2520this%2520paper%252C%2520we%2520explore%2520this%2520previously%2520ignored%2520scenario%252C%250Anamely%2520domain%2520generalized%2520federated%2520semi-supervised%2520learning%2520%2528FedSemiDG%2529%252C%2520which%250Aaims%2520to%2520learn%2520a%2520model%2520in%2520a%2520distributed%2520manner%2520from%2520multiple%2520domains%2520with%250Alimited%2520labeled%2520data%2520and%2520abundant%2520unlabeled%2520data%2520such%2520that%2520the%2520model%2520can%250Ageneralize%2520well%2520to%2520unseen%2520domains.%2520We%2520present%2520a%2520novel%2520framework%252C%2520Federated%250AGeneralization-Aware%2520SemiSupervised%2520Learning%2520%2528FGASL%2529%252C%2520to%2520address%2520the%2520challenges%250Ain%2520FedSemiDG%2520by%2520effectively%2520tackling%2520critical%2520issues%2520at%2520both%2520global%2520and%2520local%250Alevels.%2520Globally%252C%2520we%2520introduce%2520Generalization-Aware%2520Aggregation%2520%2528GAA%2529%252C%250Aassigning%2520adaptive%2520weights%2520to%2520local%2520models%2520based%2520on%2520their%2520generalization%250Aperformance.%2520Locally%252C%2520we%2520use%2520a%2520Dual-Teacher%2520Adaptive%2520Pseudo%2520Label%2520Refinement%250A%2528DR%2529%2520strategy%2520to%2520combine%2520global%2520and%2520domain-specific%2520knowledge%252C%2520generating%2520more%250Areliable%2520pseudo%2520labels.%2520Additionally%252C%2520Perturbation-Invariant%2520Alignment%2520%2528PIA%2529%250Aenforces%2520feature%2520consistency%2520under%2520perturbations%252C%2520promoting%2520domain-invariant%250Alearning.%2520Extensive%2520experiments%2520on%2520three%2520medical%2520segmentation%2520tasks%2520%2528cardiac%250AMRI%252C%2520spine%2520MRI%2520and%2520bladder%2520cancer%2520MRI%2529%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520state-of-the-art%2520FSSL%2520and%2520domain%2520generalization%250Aapproaches%252C%2520achieving%2520robust%2520generalization%2520on%2520unseen%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedSemiDG%3A%20Domain%20Generalized%20Federated%20Semi-supervised%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Zhipeng%20Deng%20and%20Zhe%20Xu%20and%20Tsuyoshi%20Isshiki%20and%20Yefeng%20Zheng&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20challenging%20due%20to%20the%20diversity%20of%20medical%0Aimages%20and%20the%20lack%20of%20labeled%20data%2C%20which%20motivates%20recent%20developments%20in%0Afederated%20semi-supervised%20learning%20%28FSSL%29%20to%20leverage%20a%20large%20amount%20of%0Aunlabeled%20data%20from%20multiple%20centers%20for%20model%20training%20without%20sharing%20raw%0Adata.%20However%2C%20what%20remains%20under-explored%20in%20FSSL%20is%20the%20domain%20shift%20problem%0Awhich%20may%20cause%20suboptimal%20model%20aggregation%20and%20low%20effectivity%20of%20the%0Autilization%20of%20unlabeled%20data%2C%20eventually%20leading%20to%20unsatisfactory%20performance%0Ain%20unseen%20domains.%20In%20this%20paper%2C%20we%20explore%20this%20previously%20ignored%20scenario%2C%0Anamely%20domain%20generalized%20federated%20semi-supervised%20learning%20%28FedSemiDG%29%2C%20which%0Aaims%20to%20learn%20a%20model%20in%20a%20distributed%20manner%20from%20multiple%20domains%20with%0Alimited%20labeled%20data%20and%20abundant%20unlabeled%20data%20such%20that%20the%20model%20can%0Ageneralize%20well%20to%20unseen%20domains.%20We%20present%20a%20novel%20framework%2C%20Federated%0AGeneralization-Aware%20SemiSupervised%20Learning%20%28FGASL%29%2C%20to%20address%20the%20challenges%0Ain%20FedSemiDG%20by%20effectively%20tackling%20critical%20issues%20at%20both%20global%20and%20local%0Alevels.%20Globally%2C%20we%20introduce%20Generalization-Aware%20Aggregation%20%28GAA%29%2C%0Aassigning%20adaptive%20weights%20to%20local%20models%20based%20on%20their%20generalization%0Aperformance.%20Locally%2C%20we%20use%20a%20Dual-Teacher%20Adaptive%20Pseudo%20Label%20Refinement%0A%28DR%29%20strategy%20to%20combine%20global%20and%20domain-specific%20knowledge%2C%20generating%20more%0Areliable%20pseudo%20labels.%20Additionally%2C%20Perturbation-Invariant%20Alignment%20%28PIA%29%0Aenforces%20feature%20consistency%20under%20perturbations%2C%20promoting%20domain-invariant%0Alearning.%20Extensive%20experiments%20on%20three%20medical%20segmentation%20tasks%20%28cardiac%0AMRI%2C%20spine%20MRI%20and%20bladder%20cancer%20MRI%29%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20FSSL%20and%20domain%20generalization%0Aapproaches%2C%20achieving%20robust%20generalization%20on%20unseen%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07378v1&entry.124074799=Read"},
{"title": "Data and System Perspectives of Sustainable Artificial Intelligence", "author": "Tao Xie and David Harel and Dezhi Ran and Zhenwen Li and Maoliang Li and Zhi Yang and Leye Wang and Xiang Chen and Ying Zhang and Wentao Zhang and Meng Li and Chen Zhang and Linyi Li and Assaf Marron", "abstract": "  Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.\n", "link": "http://arxiv.org/abs/2501.07487v1", "date": "2025-01-13", "relevancy": 2.0389, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.422}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4047}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20and%20System%20Perspectives%20of%20Sustainable%20Artificial%20Intelligence&body=Title%3A%20Data%20and%20System%20Perspectives%20of%20Sustainable%20Artificial%20Intelligence%0AAuthor%3A%20Tao%20Xie%20and%20David%20Harel%20and%20Dezhi%20Ran%20and%20Zhenwen%20Li%20and%20Maoliang%20Li%20and%20Zhi%20Yang%20and%20Leye%20Wang%20and%20Xiang%20Chen%20and%20Ying%20Zhang%20and%20Wentao%20Zhang%20and%20Meng%20Li%20and%20Chen%20Zhang%20and%20Linyi%20Li%20and%20Assaf%20Marron%0AAbstract%3A%20%20%20Sustainable%20AI%20is%20a%20subfield%20of%20AI%20for%20concerning%20developing%20and%20using%20AI%0Asystems%20in%20ways%20of%20aiming%20to%20reduce%20environmental%20impact%20and%20achieve%0Asustainability.%20Sustainable%20AI%20is%20increasingly%20important%20given%20that%20training%20of%0Aand%20inference%20with%20AI%20models%20such%20as%20large%20langrage%20models%20are%20consuming%20a%0Alarge%20amount%20of%20computing%20power.%20In%20this%20article%2C%20we%20discuss%20current%20issues%2C%0Aopportunities%20and%20example%20solutions%20for%20addressing%20these%20issues%2C%20and%20future%0Achallenges%20to%20tackle%2C%20from%20the%20data%20and%20system%20perspectives%2C%20related%20to%20data%0Aacquisition%2C%20data%20processing%2C%20and%20AI%20model%20training%20and%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520and%2520System%2520Perspectives%2520of%2520Sustainable%2520Artificial%2520Intelligence%26entry.906535625%3DTao%2520Xie%2520and%2520David%2520Harel%2520and%2520Dezhi%2520Ran%2520and%2520Zhenwen%2520Li%2520and%2520Maoliang%2520Li%2520and%2520Zhi%2520Yang%2520and%2520Leye%2520Wang%2520and%2520Xiang%2520Chen%2520and%2520Ying%2520Zhang%2520and%2520Wentao%2520Zhang%2520and%2520Meng%2520Li%2520and%2520Chen%2520Zhang%2520and%2520Linyi%2520Li%2520and%2520Assaf%2520Marron%26entry.1292438233%3D%2520%2520Sustainable%2520AI%2520is%2520a%2520subfield%2520of%2520AI%2520for%2520concerning%2520developing%2520and%2520using%2520AI%250Asystems%2520in%2520ways%2520of%2520aiming%2520to%2520reduce%2520environmental%2520impact%2520and%2520achieve%250Asustainability.%2520Sustainable%2520AI%2520is%2520increasingly%2520important%2520given%2520that%2520training%2520of%250Aand%2520inference%2520with%2520AI%2520models%2520such%2520as%2520large%2520langrage%2520models%2520are%2520consuming%2520a%250Alarge%2520amount%2520of%2520computing%2520power.%2520In%2520this%2520article%252C%2520we%2520discuss%2520current%2520issues%252C%250Aopportunities%2520and%2520example%2520solutions%2520for%2520addressing%2520these%2520issues%252C%2520and%2520future%250Achallenges%2520to%2520tackle%252C%2520from%2520the%2520data%2520and%2520system%2520perspectives%252C%2520related%2520to%2520data%250Aacquisition%252C%2520data%2520processing%252C%2520and%2520AI%2520model%2520training%2520and%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20and%20System%20Perspectives%20of%20Sustainable%20Artificial%20Intelligence&entry.906535625=Tao%20Xie%20and%20David%20Harel%20and%20Dezhi%20Ran%20and%20Zhenwen%20Li%20and%20Maoliang%20Li%20and%20Zhi%20Yang%20and%20Leye%20Wang%20and%20Xiang%20Chen%20and%20Ying%20Zhang%20and%20Wentao%20Zhang%20and%20Meng%20Li%20and%20Chen%20Zhang%20and%20Linyi%20Li%20and%20Assaf%20Marron&entry.1292438233=%20%20Sustainable%20AI%20is%20a%20subfield%20of%20AI%20for%20concerning%20developing%20and%20using%20AI%0Asystems%20in%20ways%20of%20aiming%20to%20reduce%20environmental%20impact%20and%20achieve%0Asustainability.%20Sustainable%20AI%20is%20increasingly%20important%20given%20that%20training%20of%0Aand%20inference%20with%20AI%20models%20such%20as%20large%20langrage%20models%20are%20consuming%20a%0Alarge%20amount%20of%20computing%20power.%20In%20this%20article%2C%20we%20discuss%20current%20issues%2C%0Aopportunities%20and%20example%20solutions%20for%20addressing%20these%20issues%2C%20and%20future%0Achallenges%20to%20tackle%2C%20from%20the%20data%20and%20system%20perspectives%2C%20related%20to%20data%0Aacquisition%2C%20data%20processing%2C%20and%20AI%20model%20training%20and%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07487v1&entry.124074799=Read"},
{"title": "TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language\n  Models", "author": "Thales Sales Almeida and Giovana Kerche Bon\u00e1s and Jo\u00e3o Guilherme Alves Santos and Hugo Abonizio and Rodrigo Nogueira", "abstract": "  In a rapidly evolving knowledge landscape and the increasing adoption of\nlarge language models, a need has emerged to keep these models continuously\nupdated with current events. While existing benchmarks evaluate general factual\nrecall, they often overlook two critical aspects: the ability of models to\nintegrate evolving knowledge through continual learning and the significant\nregional disparities in their performance. To address these gaps, we introduce\nthe Timely Events Benchmark (TiEBe), a dataset containing over 11,000\nquestion-answer pairs focused on globally and regionally significant events.\nTiEBe leverages structured retrospective data from Wikipedia, enabling\ncontinuous updates to assess LLMs' knowledge of evolving global affairs and\ntheir understanding of events across different regions. Our benchmark\ndemonstrates that LLMs exhibit substantial geographic disparities in factual\nrecall, emphasizing the need for more balanced global knowledge representation.\nFurthermore, TiEBe serves as a tool for evaluating continual learning\nstrategies, providing insights into models' ability to acquire new information\nwithout forgetting past knowledge.\n", "link": "http://arxiv.org/abs/2501.07482v1", "date": "2025-01-13", "relevancy": 2.0299, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiEBe%3A%20A%20Benchmark%20for%20Assessing%20the%20Current%20Knowledge%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20TiEBe%3A%20A%20Benchmark%20for%20Assessing%20the%20Current%20Knowledge%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Thales%20Sales%20Almeida%20and%20Giovana%20Kerche%20Bon%C3%A1s%20and%20Jo%C3%A3o%20Guilherme%20Alves%20Santos%20and%20Hugo%20Abonizio%20and%20Rodrigo%20Nogueira%0AAbstract%3A%20%20%20In%20a%20rapidly%20evolving%20knowledge%20landscape%20and%20the%20increasing%20adoption%20of%0Alarge%20language%20models%2C%20a%20need%20has%20emerged%20to%20keep%20these%20models%20continuously%0Aupdated%20with%20current%20events.%20While%20existing%20benchmarks%20evaluate%20general%20factual%0Arecall%2C%20they%20often%20overlook%20two%20critical%20aspects%3A%20the%20ability%20of%20models%20to%0Aintegrate%20evolving%20knowledge%20through%20continual%20learning%20and%20the%20significant%0Aregional%20disparities%20in%20their%20performance.%20To%20address%20these%20gaps%2C%20we%20introduce%0Athe%20Timely%20Events%20Benchmark%20%28TiEBe%29%2C%20a%20dataset%20containing%20over%2011%2C000%0Aquestion-answer%20pairs%20focused%20on%20globally%20and%20regionally%20significant%20events.%0ATiEBe%20leverages%20structured%20retrospective%20data%20from%20Wikipedia%2C%20enabling%0Acontinuous%20updates%20to%20assess%20LLMs%27%20knowledge%20of%20evolving%20global%20affairs%20and%0Atheir%20understanding%20of%20events%20across%20different%20regions.%20Our%20benchmark%0Ademonstrates%20that%20LLMs%20exhibit%20substantial%20geographic%20disparities%20in%20factual%0Arecall%2C%20emphasizing%20the%20need%20for%20more%20balanced%20global%20knowledge%20representation.%0AFurthermore%2C%20TiEBe%20serves%20as%20a%20tool%20for%20evaluating%20continual%20learning%0Astrategies%2C%20providing%20insights%20into%20models%27%20ability%20to%20acquire%20new%20information%0Awithout%20forgetting%20past%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiEBe%253A%2520A%2520Benchmark%2520for%2520Assessing%2520the%2520Current%2520Knowledge%2520of%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DThales%2520Sales%2520Almeida%2520and%2520Giovana%2520Kerche%2520Bon%25C3%25A1s%2520and%2520Jo%25C3%25A3o%2520Guilherme%2520Alves%2520Santos%2520and%2520Hugo%2520Abonizio%2520and%2520Rodrigo%2520Nogueira%26entry.1292438233%3D%2520%2520In%2520a%2520rapidly%2520evolving%2520knowledge%2520landscape%2520and%2520the%2520increasing%2520adoption%2520of%250Alarge%2520language%2520models%252C%2520a%2520need%2520has%2520emerged%2520to%2520keep%2520these%2520models%2520continuously%250Aupdated%2520with%2520current%2520events.%2520While%2520existing%2520benchmarks%2520evaluate%2520general%2520factual%250Arecall%252C%2520they%2520often%2520overlook%2520two%2520critical%2520aspects%253A%2520the%2520ability%2520of%2520models%2520to%250Aintegrate%2520evolving%2520knowledge%2520through%2520continual%2520learning%2520and%2520the%2520significant%250Aregional%2520disparities%2520in%2520their%2520performance.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%250Athe%2520Timely%2520Events%2520Benchmark%2520%2528TiEBe%2529%252C%2520a%2520dataset%2520containing%2520over%252011%252C000%250Aquestion-answer%2520pairs%2520focused%2520on%2520globally%2520and%2520regionally%2520significant%2520events.%250ATiEBe%2520leverages%2520structured%2520retrospective%2520data%2520from%2520Wikipedia%252C%2520enabling%250Acontinuous%2520updates%2520to%2520assess%2520LLMs%2527%2520knowledge%2520of%2520evolving%2520global%2520affairs%2520and%250Atheir%2520understanding%2520of%2520events%2520across%2520different%2520regions.%2520Our%2520benchmark%250Ademonstrates%2520that%2520LLMs%2520exhibit%2520substantial%2520geographic%2520disparities%2520in%2520factual%250Arecall%252C%2520emphasizing%2520the%2520need%2520for%2520more%2520balanced%2520global%2520knowledge%2520representation.%250AFurthermore%252C%2520TiEBe%2520serves%2520as%2520a%2520tool%2520for%2520evaluating%2520continual%2520learning%250Astrategies%252C%2520providing%2520insights%2520into%2520models%2527%2520ability%2520to%2520acquire%2520new%2520information%250Awithout%2520forgetting%2520past%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiEBe%3A%20A%20Benchmark%20for%20Assessing%20the%20Current%20Knowledge%20of%20Large%20Language%0A%20%20Models&entry.906535625=Thales%20Sales%20Almeida%20and%20Giovana%20Kerche%20Bon%C3%A1s%20and%20Jo%C3%A3o%20Guilherme%20Alves%20Santos%20and%20Hugo%20Abonizio%20and%20Rodrigo%20Nogueira&entry.1292438233=%20%20In%20a%20rapidly%20evolving%20knowledge%20landscape%20and%20the%20increasing%20adoption%20of%0Alarge%20language%20models%2C%20a%20need%20has%20emerged%20to%20keep%20these%20models%20continuously%0Aupdated%20with%20current%20events.%20While%20existing%20benchmarks%20evaluate%20general%20factual%0Arecall%2C%20they%20often%20overlook%20two%20critical%20aspects%3A%20the%20ability%20of%20models%20to%0Aintegrate%20evolving%20knowledge%20through%20continual%20learning%20and%20the%20significant%0Aregional%20disparities%20in%20their%20performance.%20To%20address%20these%20gaps%2C%20we%20introduce%0Athe%20Timely%20Events%20Benchmark%20%28TiEBe%29%2C%20a%20dataset%20containing%20over%2011%2C000%0Aquestion-answer%20pairs%20focused%20on%20globally%20and%20regionally%20significant%20events.%0ATiEBe%20leverages%20structured%20retrospective%20data%20from%20Wikipedia%2C%20enabling%0Acontinuous%20updates%20to%20assess%20LLMs%27%20knowledge%20of%20evolving%20global%20affairs%20and%0Atheir%20understanding%20of%20events%20across%20different%20regions.%20Our%20benchmark%0Ademonstrates%20that%20LLMs%20exhibit%20substantial%20geographic%20disparities%20in%20factual%0Arecall%2C%20emphasizing%20the%20need%20for%20more%20balanced%20global%20knowledge%20representation.%0AFurthermore%2C%20TiEBe%20serves%20as%20a%20tool%20for%20evaluating%20continual%20learning%0Astrategies%2C%20providing%20insights%20into%20models%27%20ability%20to%20acquire%20new%20information%0Awithout%20forgetting%20past%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07482v1&entry.124074799=Read"},
{"title": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering", "author": "Joris Postmus and Steven Abreu", "abstract": "  Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering.\n", "link": "http://arxiv.org/abs/2410.16314v3", "date": "2025-01-13", "relevancy": 2.0297, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20Large%20Language%20Models%20using%20Conceptors%3A%20Improving%0A%20%20Addition-Based%20Activation%20Engineering&body=Title%3A%20Steering%20Large%20Language%20Models%20using%20Conceptors%3A%20Improving%0A%20%20Addition-Based%20Activation%20Engineering%0AAuthor%3A%20Joris%20Postmus%20and%20Steven%20Abreu%0AAbstract%3A%20%20%20Large%20language%20models%20have%20transformed%20AI%2C%20yet%20reliably%20controlling%20their%0Aoutputs%20remains%20a%20challenge.%20This%20paper%20explores%20activation%20engineering%2C%20where%0Aoutputs%20of%20pre-trained%20LLMs%20are%20controlled%20by%20manipulating%20their%20activations%20at%0Ainference%20time.%20Unlike%20traditional%20methods%20using%20a%20single%20steering%20vector%2C%20we%0Aintroduce%20conceptors%20-%20mathematical%20constructs%20that%20represent%20sets%20of%0Aactivation%20vectors%20as%20ellipsoidal%20regions.%20Conceptors%20act%20as%20soft%20projection%0Amatrices%20and%20offer%20more%20precise%20control%20over%20complex%20activation%20patterns.%20Our%0Aexperiments%20demonstrate%20that%20conceptors%20outperform%20traditional%20methods%20across%0Amultiple%20steering%20tasks.%20We%20further%20use%20Boolean%20operations%20on%20conceptors%20for%0Acombined%20steering%20goals%20that%20empirically%20outperform%20additively%20combining%0Asteering%20vectors%20on%20a%20set%20of%20tasks.%20These%20results%20highlight%20conceptors%20as%20a%0Apromising%20tool%20for%20more%20effective%20steering%20of%20LLMs.%20Our%20code%20is%20available%20on%0Agithub.com/jorispos/conceptorsteering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16314v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520Large%2520Language%2520Models%2520using%2520Conceptors%253A%2520Improving%250A%2520%2520Addition-Based%2520Activation%2520Engineering%26entry.906535625%3DJoris%2520Postmus%2520and%2520Steven%2520Abreu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520transformed%2520AI%252C%2520yet%2520reliably%2520controlling%2520their%250Aoutputs%2520remains%2520a%2520challenge.%2520This%2520paper%2520explores%2520activation%2520engineering%252C%2520where%250Aoutputs%2520of%2520pre-trained%2520LLMs%2520are%2520controlled%2520by%2520manipulating%2520their%2520activations%2520at%250Ainference%2520time.%2520Unlike%2520traditional%2520methods%2520using%2520a%2520single%2520steering%2520vector%252C%2520we%250Aintroduce%2520conceptors%2520-%2520mathematical%2520constructs%2520that%2520represent%2520sets%2520of%250Aactivation%2520vectors%2520as%2520ellipsoidal%2520regions.%2520Conceptors%2520act%2520as%2520soft%2520projection%250Amatrices%2520and%2520offer%2520more%2520precise%2520control%2520over%2520complex%2520activation%2520patterns.%2520Our%250Aexperiments%2520demonstrate%2520that%2520conceptors%2520outperform%2520traditional%2520methods%2520across%250Amultiple%2520steering%2520tasks.%2520We%2520further%2520use%2520Boolean%2520operations%2520on%2520conceptors%2520for%250Acombined%2520steering%2520goals%2520that%2520empirically%2520outperform%2520additively%2520combining%250Asteering%2520vectors%2520on%2520a%2520set%2520of%2520tasks.%2520These%2520results%2520highlight%2520conceptors%2520as%2520a%250Apromising%2520tool%2520for%2520more%2520effective%2520steering%2520of%2520LLMs.%2520Our%2520code%2520is%2520available%2520on%250Agithub.com/jorispos/conceptorsteering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16314v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20Large%20Language%20Models%20using%20Conceptors%3A%20Improving%0A%20%20Addition-Based%20Activation%20Engineering&entry.906535625=Joris%20Postmus%20and%20Steven%20Abreu&entry.1292438233=%20%20Large%20language%20models%20have%20transformed%20AI%2C%20yet%20reliably%20controlling%20their%0Aoutputs%20remains%20a%20challenge.%20This%20paper%20explores%20activation%20engineering%2C%20where%0Aoutputs%20of%20pre-trained%20LLMs%20are%20controlled%20by%20manipulating%20their%20activations%20at%0Ainference%20time.%20Unlike%20traditional%20methods%20using%20a%20single%20steering%20vector%2C%20we%0Aintroduce%20conceptors%20-%20mathematical%20constructs%20that%20represent%20sets%20of%0Aactivation%20vectors%20as%20ellipsoidal%20regions.%20Conceptors%20act%20as%20soft%20projection%0Amatrices%20and%20offer%20more%20precise%20control%20over%20complex%20activation%20patterns.%20Our%0Aexperiments%20demonstrate%20that%20conceptors%20outperform%20traditional%20methods%20across%0Amultiple%20steering%20tasks.%20We%20further%20use%20Boolean%20operations%20on%20conceptors%20for%0Acombined%20steering%20goals%20that%20empirically%20outperform%20additively%20combining%0Asteering%20vectors%20on%20a%20set%20of%20tasks.%20These%20results%20highlight%20conceptors%20as%20a%0Apromising%20tool%20for%20more%20effective%20steering%20of%20LLMs.%20Our%20code%20is%20available%20on%0Agithub.com/jorispos/conceptorsteering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16314v3&entry.124074799=Read"},
{"title": "Geometric Scattering on Measure Spaces", "author": "Joyce Chew and Matthew Hirn and Smita Krishnaswamy and Deanna Needell and Michael Perlmutter and Holly Steach and Siddharth Viswanath and Hau-Tieng Wu", "abstract": "  The scattering transform is a multilayered, wavelet-based transform initially\nintroduced as a model of convolutional neural networks (CNNs) that has played a\nfoundational role in our understanding of these networks' stability and\ninvariance properties. Subsequently, there has been widespread interest in\nextending the success of CNNs to data sets with non-Euclidean structure, such\nas graphs and manifolds, leading to the emerging field of geometric deep\nlearning. In order to improve our understanding of the architectures used in\nthis new field, several papers have proposed generalizations of the scattering\ntransform for non-Euclidean data structures such as undirected graphs and\ncompact Riemannian manifolds without boundary.\n  In this paper, we introduce a general, unified model for geometric scattering\non measure spaces. Our proposed framework includes previous work on geometric\nscattering as special cases but also applies to more general settings such as\ndirected graphs, signed graphs, and manifolds with boundary. We propose a new\ncriterion that identifies to which groups a useful representation should be\ninvariant and show that this criterion is sufficient to guarantee that the\nscattering transform has desirable stability and invariance properties.\nAdditionally, we consider finite measure spaces that are obtained from randomly\nsampling an unknown manifold. We propose two methods for constructing a\ndata-driven graph on which the associated graph scattering transform\napproximates the scattering transform on the underlying manifold. Moreover, we\nuse a diffusion-maps based approach to prove quantitative estimates on the rate\nof convergence of one of these approximations as the number of sample points\ntends to infinity. Lastly, we showcase the utility of our method on spherical\nimages, directed graphs, and on high-dimensional single-cell data.\n", "link": "http://arxiv.org/abs/2208.08561v3", "date": "2025-01-13", "relevancy": 2.0161, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.517}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4976}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Scattering%20on%20Measure%20Spaces&body=Title%3A%20Geometric%20Scattering%20on%20Measure%20Spaces%0AAuthor%3A%20Joyce%20Chew%20and%20Matthew%20Hirn%20and%20Smita%20Krishnaswamy%20and%20Deanna%20Needell%20and%20Michael%20Perlmutter%20and%20Holly%20Steach%20and%20Siddharth%20Viswanath%20and%20Hau-Tieng%20Wu%0AAbstract%3A%20%20%20The%20scattering%20transform%20is%20a%20multilayered%2C%20wavelet-based%20transform%20initially%0Aintroduced%20as%20a%20model%20of%20convolutional%20neural%20networks%20%28CNNs%29%20that%20has%20played%20a%0Afoundational%20role%20in%20our%20understanding%20of%20these%20networks%27%20stability%20and%0Ainvariance%20properties.%20Subsequently%2C%20there%20has%20been%20widespread%20interest%20in%0Aextending%20the%20success%20of%20CNNs%20to%20data%20sets%20with%20non-Euclidean%20structure%2C%20such%0Aas%20graphs%20and%20manifolds%2C%20leading%20to%20the%20emerging%20field%20of%20geometric%20deep%0Alearning.%20In%20order%20to%20improve%20our%20understanding%20of%20the%20architectures%20used%20in%0Athis%20new%20field%2C%20several%20papers%20have%20proposed%20generalizations%20of%20the%20scattering%0Atransform%20for%20non-Euclidean%20data%20structures%20such%20as%20undirected%20graphs%20and%0Acompact%20Riemannian%20manifolds%20without%20boundary.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20general%2C%20unified%20model%20for%20geometric%20scattering%0Aon%20measure%20spaces.%20Our%20proposed%20framework%20includes%20previous%20work%20on%20geometric%0Ascattering%20as%20special%20cases%20but%20also%20applies%20to%20more%20general%20settings%20such%20as%0Adirected%20graphs%2C%20signed%20graphs%2C%20and%20manifolds%20with%20boundary.%20We%20propose%20a%20new%0Acriterion%20that%20identifies%20to%20which%20groups%20a%20useful%20representation%20should%20be%0Ainvariant%20and%20show%20that%20this%20criterion%20is%20sufficient%20to%20guarantee%20that%20the%0Ascattering%20transform%20has%20desirable%20stability%20and%20invariance%20properties.%0AAdditionally%2C%20we%20consider%20finite%20measure%20spaces%20that%20are%20obtained%20from%20randomly%0Asampling%20an%20unknown%20manifold.%20We%20propose%20two%20methods%20for%20constructing%20a%0Adata-driven%20graph%20on%20which%20the%20associated%20graph%20scattering%20transform%0Aapproximates%20the%20scattering%20transform%20on%20the%20underlying%20manifold.%20Moreover%2C%20we%0Ause%20a%20diffusion-maps%20based%20approach%20to%20prove%20quantitative%20estimates%20on%20the%20rate%0Aof%20convergence%20of%20one%20of%20these%20approximations%20as%20the%20number%20of%20sample%20points%0Atends%20to%20infinity.%20Lastly%2C%20we%20showcase%20the%20utility%20of%20our%20method%20on%20spherical%0Aimages%2C%20directed%20graphs%2C%20and%20on%20high-dimensional%20single-cell%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.08561v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Scattering%2520on%2520Measure%2520Spaces%26entry.906535625%3DJoyce%2520Chew%2520and%2520Matthew%2520Hirn%2520and%2520Smita%2520Krishnaswamy%2520and%2520Deanna%2520Needell%2520and%2520Michael%2520Perlmutter%2520and%2520Holly%2520Steach%2520and%2520Siddharth%2520Viswanath%2520and%2520Hau-Tieng%2520Wu%26entry.1292438233%3D%2520%2520The%2520scattering%2520transform%2520is%2520a%2520multilayered%252C%2520wavelet-based%2520transform%2520initially%250Aintroduced%2520as%2520a%2520model%2520of%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520that%2520has%2520played%2520a%250Afoundational%2520role%2520in%2520our%2520understanding%2520of%2520these%2520networks%2527%2520stability%2520and%250Ainvariance%2520properties.%2520Subsequently%252C%2520there%2520has%2520been%2520widespread%2520interest%2520in%250Aextending%2520the%2520success%2520of%2520CNNs%2520to%2520data%2520sets%2520with%2520non-Euclidean%2520structure%252C%2520such%250Aas%2520graphs%2520and%2520manifolds%252C%2520leading%2520to%2520the%2520emerging%2520field%2520of%2520geometric%2520deep%250Alearning.%2520In%2520order%2520to%2520improve%2520our%2520understanding%2520of%2520the%2520architectures%2520used%2520in%250Athis%2520new%2520field%252C%2520several%2520papers%2520have%2520proposed%2520generalizations%2520of%2520the%2520scattering%250Atransform%2520for%2520non-Euclidean%2520data%2520structures%2520such%2520as%2520undirected%2520graphs%2520and%250Acompact%2520Riemannian%2520manifolds%2520without%2520boundary.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520general%252C%2520unified%2520model%2520for%2520geometric%2520scattering%250Aon%2520measure%2520spaces.%2520Our%2520proposed%2520framework%2520includes%2520previous%2520work%2520on%2520geometric%250Ascattering%2520as%2520special%2520cases%2520but%2520also%2520applies%2520to%2520more%2520general%2520settings%2520such%2520as%250Adirected%2520graphs%252C%2520signed%2520graphs%252C%2520and%2520manifolds%2520with%2520boundary.%2520We%2520propose%2520a%2520new%250Acriterion%2520that%2520identifies%2520to%2520which%2520groups%2520a%2520useful%2520representation%2520should%2520be%250Ainvariant%2520and%2520show%2520that%2520this%2520criterion%2520is%2520sufficient%2520to%2520guarantee%2520that%2520the%250Ascattering%2520transform%2520has%2520desirable%2520stability%2520and%2520invariance%2520properties.%250AAdditionally%252C%2520we%2520consider%2520finite%2520measure%2520spaces%2520that%2520are%2520obtained%2520from%2520randomly%250Asampling%2520an%2520unknown%2520manifold.%2520We%2520propose%2520two%2520methods%2520for%2520constructing%2520a%250Adata-driven%2520graph%2520on%2520which%2520the%2520associated%2520graph%2520scattering%2520transform%250Aapproximates%2520the%2520scattering%2520transform%2520on%2520the%2520underlying%2520manifold.%2520Moreover%252C%2520we%250Ause%2520a%2520diffusion-maps%2520based%2520approach%2520to%2520prove%2520quantitative%2520estimates%2520on%2520the%2520rate%250Aof%2520convergence%2520of%2520one%2520of%2520these%2520approximations%2520as%2520the%2520number%2520of%2520sample%2520points%250Atends%2520to%2520infinity.%2520Lastly%252C%2520we%2520showcase%2520the%2520utility%2520of%2520our%2520method%2520on%2520spherical%250Aimages%252C%2520directed%2520graphs%252C%2520and%2520on%2520high-dimensional%2520single-cell%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.08561v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Scattering%20on%20Measure%20Spaces&entry.906535625=Joyce%20Chew%20and%20Matthew%20Hirn%20and%20Smita%20Krishnaswamy%20and%20Deanna%20Needell%20and%20Michael%20Perlmutter%20and%20Holly%20Steach%20and%20Siddharth%20Viswanath%20and%20Hau-Tieng%20Wu&entry.1292438233=%20%20The%20scattering%20transform%20is%20a%20multilayered%2C%20wavelet-based%20transform%20initially%0Aintroduced%20as%20a%20model%20of%20convolutional%20neural%20networks%20%28CNNs%29%20that%20has%20played%20a%0Afoundational%20role%20in%20our%20understanding%20of%20these%20networks%27%20stability%20and%0Ainvariance%20properties.%20Subsequently%2C%20there%20has%20been%20widespread%20interest%20in%0Aextending%20the%20success%20of%20CNNs%20to%20data%20sets%20with%20non-Euclidean%20structure%2C%20such%0Aas%20graphs%20and%20manifolds%2C%20leading%20to%20the%20emerging%20field%20of%20geometric%20deep%0Alearning.%20In%20order%20to%20improve%20our%20understanding%20of%20the%20architectures%20used%20in%0Athis%20new%20field%2C%20several%20papers%20have%20proposed%20generalizations%20of%20the%20scattering%0Atransform%20for%20non-Euclidean%20data%20structures%20such%20as%20undirected%20graphs%20and%0Acompact%20Riemannian%20manifolds%20without%20boundary.%0A%20%20In%20this%20paper%2C%20we%20introduce%20a%20general%2C%20unified%20model%20for%20geometric%20scattering%0Aon%20measure%20spaces.%20Our%20proposed%20framework%20includes%20previous%20work%20on%20geometric%0Ascattering%20as%20special%20cases%20but%20also%20applies%20to%20more%20general%20settings%20such%20as%0Adirected%20graphs%2C%20signed%20graphs%2C%20and%20manifolds%20with%20boundary.%20We%20propose%20a%20new%0Acriterion%20that%20identifies%20to%20which%20groups%20a%20useful%20representation%20should%20be%0Ainvariant%20and%20show%20that%20this%20criterion%20is%20sufficient%20to%20guarantee%20that%20the%0Ascattering%20transform%20has%20desirable%20stability%20and%20invariance%20properties.%0AAdditionally%2C%20we%20consider%20finite%20measure%20spaces%20that%20are%20obtained%20from%20randomly%0Asampling%20an%20unknown%20manifold.%20We%20propose%20two%20methods%20for%20constructing%20a%0Adata-driven%20graph%20on%20which%20the%20associated%20graph%20scattering%20transform%0Aapproximates%20the%20scattering%20transform%20on%20the%20underlying%20manifold.%20Moreover%2C%20we%0Ause%20a%20diffusion-maps%20based%20approach%20to%20prove%20quantitative%20estimates%20on%20the%20rate%0Aof%20convergence%20of%20one%20of%20these%20approximations%20as%20the%20number%20of%20sample%20points%0Atends%20to%20infinity.%20Lastly%2C%20we%20showcase%20the%20utility%20of%20our%20method%20on%20spherical%0Aimages%2C%20directed%20graphs%2C%20and%20on%20high-dimensional%20single-cell%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.08561v3&entry.124074799=Read"},
{"title": "Quilt-1M: One Million Image-Text Pairs for Histopathology", "author": "Wisdom Oluchi Ikezogwo and Mehmet Saygin Seyfioglu and Fatemeh Ghezloo and Dylan Stefan Chan Geva and Fatwir Sheikh Mohammed and Pavan Kumar Anand and Ranjay Krishna and Linda Shapiro", "abstract": "  Recent accelerations in multi-modal applications have been made possible with\nthe plethora of image and text data available online. However, the scarcity of\nanalogous data in the medical field, specifically in histopathology, has slowed\ncomparable progress. To enable similar representation learning for\nhistopathology, we turn to YouTube, an untapped resource of videos, offering\n$1,087$ hours of valuable educational histopathology videos from expert\nclinicians. From YouTube, we curate QUILT: a large-scale vision-language\ndataset consisting of $802, 144$ image and text pairs. QUILT was automatically\ncurated using a mixture of models, including large language models, handcrafted\nalgorithms, human knowledge databases, and automatic speech recognition. In\ncomparison, the most comprehensive datasets curated for histopathology amass\nonly around $200$K samples. We combine QUILT with datasets from other sources,\nincluding Twitter, research papers, and the internet in general, to create an\neven larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it\nas the largest vision-language histopathology dataset to date. We demonstrate\nthe value of QUILT-1M by fine-tuning a pre-trained CLIP model. Our model\noutperforms state-of-the-art models on both zero-shot and linear probing tasks\nfor classifying new histopathology images across $13$ diverse patch-level\ndatasets of $8$ different sub-pathologies and cross-modal retrieval tasks.\n", "link": "http://arxiv.org/abs/2306.11207v4", "date": "2025-01-13", "relevancy": 2.0141, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4923}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quilt-1M%3A%20One%20Million%20Image-Text%20Pairs%20for%20Histopathology&body=Title%3A%20Quilt-1M%3A%20One%20Million%20Image-Text%20Pairs%20for%20Histopathology%0AAuthor%3A%20Wisdom%20Oluchi%20Ikezogwo%20and%20Mehmet%20Saygin%20Seyfioglu%20and%20Fatemeh%20Ghezloo%20and%20Dylan%20Stefan%20Chan%20Geva%20and%20Fatwir%20Sheikh%20Mohammed%20and%20Pavan%20Kumar%20Anand%20and%20Ranjay%20Krishna%20and%20Linda%20Shapiro%0AAbstract%3A%20%20%20Recent%20accelerations%20in%20multi-modal%20applications%20have%20been%20made%20possible%20with%0Athe%20plethora%20of%20image%20and%20text%20data%20available%20online.%20However%2C%20the%20scarcity%20of%0Aanalogous%20data%20in%20the%20medical%20field%2C%20specifically%20in%20histopathology%2C%20has%20slowed%0Acomparable%20progress.%20To%20enable%20similar%20representation%20learning%20for%0Ahistopathology%2C%20we%20turn%20to%20YouTube%2C%20an%20untapped%20resource%20of%20videos%2C%20offering%0A%241%2C087%24%20hours%20of%20valuable%20educational%20histopathology%20videos%20from%20expert%0Aclinicians.%20From%20YouTube%2C%20we%20curate%20QUILT%3A%20a%20large-scale%20vision-language%0Adataset%20consisting%20of%20%24802%2C%20144%24%20image%20and%20text%20pairs.%20QUILT%20was%20automatically%0Acurated%20using%20a%20mixture%20of%20models%2C%20including%20large%20language%20models%2C%20handcrafted%0Aalgorithms%2C%20human%20knowledge%20databases%2C%20and%20automatic%20speech%20recognition.%20In%0Acomparison%2C%20the%20most%20comprehensive%20datasets%20curated%20for%20histopathology%20amass%0Aonly%20around%20%24200%24K%20samples.%20We%20combine%20QUILT%20with%20datasets%20from%20other%20sources%2C%0Aincluding%20Twitter%2C%20research%20papers%2C%20and%20the%20internet%20in%20general%2C%20to%20create%20an%0Aeven%20larger%20dataset%3A%20QUILT-1M%2C%20with%20%241%24M%20paired%20image-text%20samples%2C%20marking%20it%0Aas%20the%20largest%20vision-language%20histopathology%20dataset%20to%20date.%20We%20demonstrate%0Athe%20value%20of%20QUILT-1M%20by%20fine-tuning%20a%20pre-trained%20CLIP%20model.%20Our%20model%0Aoutperforms%20state-of-the-art%20models%20on%20both%20zero-shot%20and%20linear%20probing%20tasks%0Afor%20classifying%20new%20histopathology%20images%20across%20%2413%24%20diverse%20patch-level%0Adatasets%20of%20%248%24%20different%20sub-pathologies%20and%20cross-modal%20retrieval%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11207v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuilt-1M%253A%2520One%2520Million%2520Image-Text%2520Pairs%2520for%2520Histopathology%26entry.906535625%3DWisdom%2520Oluchi%2520Ikezogwo%2520and%2520Mehmet%2520Saygin%2520Seyfioglu%2520and%2520Fatemeh%2520Ghezloo%2520and%2520Dylan%2520Stefan%2520Chan%2520Geva%2520and%2520Fatwir%2520Sheikh%2520Mohammed%2520and%2520Pavan%2520Kumar%2520Anand%2520and%2520Ranjay%2520Krishna%2520and%2520Linda%2520Shapiro%26entry.1292438233%3D%2520%2520Recent%2520accelerations%2520in%2520multi-modal%2520applications%2520have%2520been%2520made%2520possible%2520with%250Athe%2520plethora%2520of%2520image%2520and%2520text%2520data%2520available%2520online.%2520However%252C%2520the%2520scarcity%2520of%250Aanalogous%2520data%2520in%2520the%2520medical%2520field%252C%2520specifically%2520in%2520histopathology%252C%2520has%2520slowed%250Acomparable%2520progress.%2520To%2520enable%2520similar%2520representation%2520learning%2520for%250Ahistopathology%252C%2520we%2520turn%2520to%2520YouTube%252C%2520an%2520untapped%2520resource%2520of%2520videos%252C%2520offering%250A%25241%252C087%2524%2520hours%2520of%2520valuable%2520educational%2520histopathology%2520videos%2520from%2520expert%250Aclinicians.%2520From%2520YouTube%252C%2520we%2520curate%2520QUILT%253A%2520a%2520large-scale%2520vision-language%250Adataset%2520consisting%2520of%2520%2524802%252C%2520144%2524%2520image%2520and%2520text%2520pairs.%2520QUILT%2520was%2520automatically%250Acurated%2520using%2520a%2520mixture%2520of%2520models%252C%2520including%2520large%2520language%2520models%252C%2520handcrafted%250Aalgorithms%252C%2520human%2520knowledge%2520databases%252C%2520and%2520automatic%2520speech%2520recognition.%2520In%250Acomparison%252C%2520the%2520most%2520comprehensive%2520datasets%2520curated%2520for%2520histopathology%2520amass%250Aonly%2520around%2520%2524200%2524K%2520samples.%2520We%2520combine%2520QUILT%2520with%2520datasets%2520from%2520other%2520sources%252C%250Aincluding%2520Twitter%252C%2520research%2520papers%252C%2520and%2520the%2520internet%2520in%2520general%252C%2520to%2520create%2520an%250Aeven%2520larger%2520dataset%253A%2520QUILT-1M%252C%2520with%2520%25241%2524M%2520paired%2520image-text%2520samples%252C%2520marking%2520it%250Aas%2520the%2520largest%2520vision-language%2520histopathology%2520dataset%2520to%2520date.%2520We%2520demonstrate%250Athe%2520value%2520of%2520QUILT-1M%2520by%2520fine-tuning%2520a%2520pre-trained%2520CLIP%2520model.%2520Our%2520model%250Aoutperforms%2520state-of-the-art%2520models%2520on%2520both%2520zero-shot%2520and%2520linear%2520probing%2520tasks%250Afor%2520classifying%2520new%2520histopathology%2520images%2520across%2520%252413%2524%2520diverse%2520patch-level%250Adatasets%2520of%2520%25248%2524%2520different%2520sub-pathologies%2520and%2520cross-modal%2520retrieval%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11207v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quilt-1M%3A%20One%20Million%20Image-Text%20Pairs%20for%20Histopathology&entry.906535625=Wisdom%20Oluchi%20Ikezogwo%20and%20Mehmet%20Saygin%20Seyfioglu%20and%20Fatemeh%20Ghezloo%20and%20Dylan%20Stefan%20Chan%20Geva%20and%20Fatwir%20Sheikh%20Mohammed%20and%20Pavan%20Kumar%20Anand%20and%20Ranjay%20Krishna%20and%20Linda%20Shapiro&entry.1292438233=%20%20Recent%20accelerations%20in%20multi-modal%20applications%20have%20been%20made%20possible%20with%0Athe%20plethora%20of%20image%20and%20text%20data%20available%20online.%20However%2C%20the%20scarcity%20of%0Aanalogous%20data%20in%20the%20medical%20field%2C%20specifically%20in%20histopathology%2C%20has%20slowed%0Acomparable%20progress.%20To%20enable%20similar%20representation%20learning%20for%0Ahistopathology%2C%20we%20turn%20to%20YouTube%2C%20an%20untapped%20resource%20of%20videos%2C%20offering%0A%241%2C087%24%20hours%20of%20valuable%20educational%20histopathology%20videos%20from%20expert%0Aclinicians.%20From%20YouTube%2C%20we%20curate%20QUILT%3A%20a%20large-scale%20vision-language%0Adataset%20consisting%20of%20%24802%2C%20144%24%20image%20and%20text%20pairs.%20QUILT%20was%20automatically%0Acurated%20using%20a%20mixture%20of%20models%2C%20including%20large%20language%20models%2C%20handcrafted%0Aalgorithms%2C%20human%20knowledge%20databases%2C%20and%20automatic%20speech%20recognition.%20In%0Acomparison%2C%20the%20most%20comprehensive%20datasets%20curated%20for%20histopathology%20amass%0Aonly%20around%20%24200%24K%20samples.%20We%20combine%20QUILT%20with%20datasets%20from%20other%20sources%2C%0Aincluding%20Twitter%2C%20research%20papers%2C%20and%20the%20internet%20in%20general%2C%20to%20create%20an%0Aeven%20larger%20dataset%3A%20QUILT-1M%2C%20with%20%241%24M%20paired%20image-text%20samples%2C%20marking%20it%0Aas%20the%20largest%20vision-language%20histopathology%20dataset%20to%20date.%20We%20demonstrate%0Athe%20value%20of%20QUILT-1M%20by%20fine-tuning%20a%20pre-trained%20CLIP%20model.%20Our%20model%0Aoutperforms%20state-of-the-art%20models%20on%20both%20zero-shot%20and%20linear%20probing%20tasks%0Afor%20classifying%20new%20histopathology%20images%20across%20%2413%24%20diverse%20patch-level%0Adatasets%20of%20%248%24%20different%20sub-pathologies%20and%20cross-modal%20retrieval%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11207v4&entry.124074799=Read"},
{"title": "Exploring energy minimization to model strain localization as a strong\n  discontinuity using Physics Informed Neural Networks", "author": "Omar Le\u00f3n and V\u00edctor Rivera and Angel V\u00e1zquez-Pati\u00f1o and Jacinto Ulloa and Esteban Samaniego", "abstract": "  We explore the possibilities of using energy minimization for the numerical\nmodeling of strain localization in solids as a sharp discontinuity in the\ndisplacement field. For this purpose, we consider (regularized) strong\ndiscontinuity kinematics in elastoplastic solids. The corresponding\nmathematical model is discretized using Artificial Neural Networks (ANNs),\naiming to predict both the magnitude and location of the displacement jump from\nenergy minimization, $\\textit{i.e.}$, within a variational setting. The\narchitecture takes care of the kinematics, while the loss function takes care\nof the variational statement of the boundary value problem. The main idea\nbehind this approach is to solve both the equilibrium problem and the location\nof the localization band by means of trainable parameters in the ANN. As a\nproof of concept, we show through both 1D and 2D numerical examples that the\ncomputational modeling of strain localization for elastoplastic solids using\nenergy minimization is feasible.\n", "link": "http://arxiv.org/abs/2409.13241v2", "date": "2025-01-13", "relevancy": 1.8288, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.46}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4556}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20energy%20minimization%20to%20model%20strain%20localization%20as%20a%20strong%0A%20%20discontinuity%20using%20Physics%20Informed%20Neural%20Networks&body=Title%3A%20Exploring%20energy%20minimization%20to%20model%20strain%20localization%20as%20a%20strong%0A%20%20discontinuity%20using%20Physics%20Informed%20Neural%20Networks%0AAuthor%3A%20Omar%20Le%C3%B3n%20and%20V%C3%ADctor%20Rivera%20and%20Angel%20V%C3%A1zquez-Pati%C3%B1o%20and%20Jacinto%20Ulloa%20and%20Esteban%20Samaniego%0AAbstract%3A%20%20%20We%20explore%20the%20possibilities%20of%20using%20energy%20minimization%20for%20the%20numerical%0Amodeling%20of%20strain%20localization%20in%20solids%20as%20a%20sharp%20discontinuity%20in%20the%0Adisplacement%20field.%20For%20this%20purpose%2C%20we%20consider%20%28regularized%29%20strong%0Adiscontinuity%20kinematics%20in%20elastoplastic%20solids.%20The%20corresponding%0Amathematical%20model%20is%20discretized%20using%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%0Aaiming%20to%20predict%20both%20the%20magnitude%20and%20location%20of%20the%20displacement%20jump%20from%0Aenergy%20minimization%2C%20%24%5Ctextit%7Bi.e.%7D%24%2C%20within%20a%20variational%20setting.%20The%0Aarchitecture%20takes%20care%20of%20the%20kinematics%2C%20while%20the%20loss%20function%20takes%20care%0Aof%20the%20variational%20statement%20of%20the%20boundary%20value%20problem.%20The%20main%20idea%0Abehind%20this%20approach%20is%20to%20solve%20both%20the%20equilibrium%20problem%20and%20the%20location%0Aof%20the%20localization%20band%20by%20means%20of%20trainable%20parameters%20in%20the%20ANN.%20As%20a%0Aproof%20of%20concept%2C%20we%20show%20through%20both%201D%20and%202D%20numerical%20examples%20that%20the%0Acomputational%20modeling%20of%20strain%20localization%20for%20elastoplastic%20solids%20using%0Aenergy%20minimization%20is%20feasible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13241v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520energy%2520minimization%2520to%2520model%2520strain%2520localization%2520as%2520a%2520strong%250A%2520%2520discontinuity%2520using%2520Physics%2520Informed%2520Neural%2520Networks%26entry.906535625%3DOmar%2520Le%25C3%25B3n%2520and%2520V%25C3%25ADctor%2520Rivera%2520and%2520Angel%2520V%25C3%25A1zquez-Pati%25C3%25B1o%2520and%2520Jacinto%2520Ulloa%2520and%2520Esteban%2520Samaniego%26entry.1292438233%3D%2520%2520We%2520explore%2520the%2520possibilities%2520of%2520using%2520energy%2520minimization%2520for%2520the%2520numerical%250Amodeling%2520of%2520strain%2520localization%2520in%2520solids%2520as%2520a%2520sharp%2520discontinuity%2520in%2520the%250Adisplacement%2520field.%2520For%2520this%2520purpose%252C%2520we%2520consider%2520%2528regularized%2529%2520strong%250Adiscontinuity%2520kinematics%2520in%2520elastoplastic%2520solids.%2520The%2520corresponding%250Amathematical%2520model%2520is%2520discretized%2520using%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529%252C%250Aaiming%2520to%2520predict%2520both%2520the%2520magnitude%2520and%2520location%2520of%2520the%2520displacement%2520jump%2520from%250Aenergy%2520minimization%252C%2520%2524%255Ctextit%257Bi.e.%257D%2524%252C%2520within%2520a%2520variational%2520setting.%2520The%250Aarchitecture%2520takes%2520care%2520of%2520the%2520kinematics%252C%2520while%2520the%2520loss%2520function%2520takes%2520care%250Aof%2520the%2520variational%2520statement%2520of%2520the%2520boundary%2520value%2520problem.%2520The%2520main%2520idea%250Abehind%2520this%2520approach%2520is%2520to%2520solve%2520both%2520the%2520equilibrium%2520problem%2520and%2520the%2520location%250Aof%2520the%2520localization%2520band%2520by%2520means%2520of%2520trainable%2520parameters%2520in%2520the%2520ANN.%2520As%2520a%250Aproof%2520of%2520concept%252C%2520we%2520show%2520through%2520both%25201D%2520and%25202D%2520numerical%2520examples%2520that%2520the%250Acomputational%2520modeling%2520of%2520strain%2520localization%2520for%2520elastoplastic%2520solids%2520using%250Aenergy%2520minimization%2520is%2520feasible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13241v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20energy%20minimization%20to%20model%20strain%20localization%20as%20a%20strong%0A%20%20discontinuity%20using%20Physics%20Informed%20Neural%20Networks&entry.906535625=Omar%20Le%C3%B3n%20and%20V%C3%ADctor%20Rivera%20and%20Angel%20V%C3%A1zquez-Pati%C3%B1o%20and%20Jacinto%20Ulloa%20and%20Esteban%20Samaniego&entry.1292438233=%20%20We%20explore%20the%20possibilities%20of%20using%20energy%20minimization%20for%20the%20numerical%0Amodeling%20of%20strain%20localization%20in%20solids%20as%20a%20sharp%20discontinuity%20in%20the%0Adisplacement%20field.%20For%20this%20purpose%2C%20we%20consider%20%28regularized%29%20strong%0Adiscontinuity%20kinematics%20in%20elastoplastic%20solids.%20The%20corresponding%0Amathematical%20model%20is%20discretized%20using%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%0Aaiming%20to%20predict%20both%20the%20magnitude%20and%20location%20of%20the%20displacement%20jump%20from%0Aenergy%20minimization%2C%20%24%5Ctextit%7Bi.e.%7D%24%2C%20within%20a%20variational%20setting.%20The%0Aarchitecture%20takes%20care%20of%20the%20kinematics%2C%20while%20the%20loss%20function%20takes%20care%0Aof%20the%20variational%20statement%20of%20the%20boundary%20value%20problem.%20The%20main%20idea%0Abehind%20this%20approach%20is%20to%20solve%20both%20the%20equilibrium%20problem%20and%20the%20location%0Aof%20the%20localization%20band%20by%20means%20of%20trainable%20parameters%20in%20the%20ANN.%20As%20a%0Aproof%20of%20concept%2C%20we%20show%20through%20both%201D%20and%202D%20numerical%20examples%20that%20the%0Acomputational%20modeling%20of%20strain%20localization%20for%20elastoplastic%20solids%20using%0Aenergy%20minimization%20is%20feasible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13241v2&entry.124074799=Read"},
{"title": "Anomalous Agreement: How to find the Ideal Number of Anomaly Classes in\n  Correlated, Multivariate Time Series Data", "author": "Ferdinand Rewicki and Joachim Denzler and Julia Niebling", "abstract": "  Detecting and classifying abnormal system states is critical for condition\nmonitoring, but supervised methods often fall short due to the rarity of\nanomalies and the lack of labeled data. Therefore, clustering is often used to\ngroup similar abnormal behavior. However, evaluating cluster quality without\nground truth is challenging, as existing measures such as the Silhouette Score\n(SSC) only evaluate the cohesion and separation of clusters and ignore possible\nprior knowledge about the data. To address this challenge, we introduce the\nSynchronized Anomaly Agreement Index (SAAI), which exploits the synchronicity\nof anomalies across multivariate time series to assess cluster quality. We\ndemonstrate the effectiveness of SAAI by showing that maximizing SAAI improves\naccuracy on the task of finding the true number of anomaly classes K in\ncorrelated time series by 0.23 compared to SSC and by 0.32 compared to X-Means.\nWe also show that clusters obtained by maximizing SAAI are easier to interpret\ncompared to SSC.\n", "link": "http://arxiv.org/abs/2501.07172v1", "date": "2025-01-13", "relevancy": 1.686, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4342}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4297}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomalous%20Agreement%3A%20How%20to%20find%20the%20Ideal%20Number%20of%20Anomaly%20Classes%20in%0A%20%20Correlated%2C%20Multivariate%20Time%20Series%20Data&body=Title%3A%20Anomalous%20Agreement%3A%20How%20to%20find%20the%20Ideal%20Number%20of%20Anomaly%20Classes%20in%0A%20%20Correlated%2C%20Multivariate%20Time%20Series%20Data%0AAuthor%3A%20Ferdinand%20Rewicki%20and%20Joachim%20Denzler%20and%20Julia%20Niebling%0AAbstract%3A%20%20%20Detecting%20and%20classifying%20abnormal%20system%20states%20is%20critical%20for%20condition%0Amonitoring%2C%20but%20supervised%20methods%20often%20fall%20short%20due%20to%20the%20rarity%20of%0Aanomalies%20and%20the%20lack%20of%20labeled%20data.%20Therefore%2C%20clustering%20is%20often%20used%20to%0Agroup%20similar%20abnormal%20behavior.%20However%2C%20evaluating%20cluster%20quality%20without%0Aground%20truth%20is%20challenging%2C%20as%20existing%20measures%20such%20as%20the%20Silhouette%20Score%0A%28SSC%29%20only%20evaluate%20the%20cohesion%20and%20separation%20of%20clusters%20and%20ignore%20possible%0Aprior%20knowledge%20about%20the%20data.%20To%20address%20this%20challenge%2C%20we%20introduce%20the%0ASynchronized%20Anomaly%20Agreement%20Index%20%28SAAI%29%2C%20which%20exploits%20the%20synchronicity%0Aof%20anomalies%20across%20multivariate%20time%20series%20to%20assess%20cluster%20quality.%20We%0Ademonstrate%20the%20effectiveness%20of%20SAAI%20by%20showing%20that%20maximizing%20SAAI%20improves%0Aaccuracy%20on%20the%20task%20of%20finding%20the%20true%20number%20of%20anomaly%20classes%20K%20in%0Acorrelated%20time%20series%20by%200.23%20compared%20to%20SSC%20and%20by%200.32%20compared%20to%20X-Means.%0AWe%20also%20show%20that%20clusters%20obtained%20by%20maximizing%20SAAI%20are%20easier%20to%20interpret%0Acompared%20to%20SSC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalous%2520Agreement%253A%2520How%2520to%2520find%2520the%2520Ideal%2520Number%2520of%2520Anomaly%2520Classes%2520in%250A%2520%2520Correlated%252C%2520Multivariate%2520Time%2520Series%2520Data%26entry.906535625%3DFerdinand%2520Rewicki%2520and%2520Joachim%2520Denzler%2520and%2520Julia%2520Niebling%26entry.1292438233%3D%2520%2520Detecting%2520and%2520classifying%2520abnormal%2520system%2520states%2520is%2520critical%2520for%2520condition%250Amonitoring%252C%2520but%2520supervised%2520methods%2520often%2520fall%2520short%2520due%2520to%2520the%2520rarity%2520of%250Aanomalies%2520and%2520the%2520lack%2520of%2520labeled%2520data.%2520Therefore%252C%2520clustering%2520is%2520often%2520used%2520to%250Agroup%2520similar%2520abnormal%2520behavior.%2520However%252C%2520evaluating%2520cluster%2520quality%2520without%250Aground%2520truth%2520is%2520challenging%252C%2520as%2520existing%2520measures%2520such%2520as%2520the%2520Silhouette%2520Score%250A%2528SSC%2529%2520only%2520evaluate%2520the%2520cohesion%2520and%2520separation%2520of%2520clusters%2520and%2520ignore%2520possible%250Aprior%2520knowledge%2520about%2520the%2520data.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520the%250ASynchronized%2520Anomaly%2520Agreement%2520Index%2520%2528SAAI%2529%252C%2520which%2520exploits%2520the%2520synchronicity%250Aof%2520anomalies%2520across%2520multivariate%2520time%2520series%2520to%2520assess%2520cluster%2520quality.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520SAAI%2520by%2520showing%2520that%2520maximizing%2520SAAI%2520improves%250Aaccuracy%2520on%2520the%2520task%2520of%2520finding%2520the%2520true%2520number%2520of%2520anomaly%2520classes%2520K%2520in%250Acorrelated%2520time%2520series%2520by%25200.23%2520compared%2520to%2520SSC%2520and%2520by%25200.32%2520compared%2520to%2520X-Means.%250AWe%2520also%2520show%2520that%2520clusters%2520obtained%2520by%2520maximizing%2520SAAI%2520are%2520easier%2520to%2520interpret%250Acompared%2520to%2520SSC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomalous%20Agreement%3A%20How%20to%20find%20the%20Ideal%20Number%20of%20Anomaly%20Classes%20in%0A%20%20Correlated%2C%20Multivariate%20Time%20Series%20Data&entry.906535625=Ferdinand%20Rewicki%20and%20Joachim%20Denzler%20and%20Julia%20Niebling&entry.1292438233=%20%20Detecting%20and%20classifying%20abnormal%20system%20states%20is%20critical%20for%20condition%0Amonitoring%2C%20but%20supervised%20methods%20often%20fall%20short%20due%20to%20the%20rarity%20of%0Aanomalies%20and%20the%20lack%20of%20labeled%20data.%20Therefore%2C%20clustering%20is%20often%20used%20to%0Agroup%20similar%20abnormal%20behavior.%20However%2C%20evaluating%20cluster%20quality%20without%0Aground%20truth%20is%20challenging%2C%20as%20existing%20measures%20such%20as%20the%20Silhouette%20Score%0A%28SSC%29%20only%20evaluate%20the%20cohesion%20and%20separation%20of%20clusters%20and%20ignore%20possible%0Aprior%20knowledge%20about%20the%20data.%20To%20address%20this%20challenge%2C%20we%20introduce%20the%0ASynchronized%20Anomaly%20Agreement%20Index%20%28SAAI%29%2C%20which%20exploits%20the%20synchronicity%0Aof%20anomalies%20across%20multivariate%20time%20series%20to%20assess%20cluster%20quality.%20We%0Ademonstrate%20the%20effectiveness%20of%20SAAI%20by%20showing%20that%20maximizing%20SAAI%20improves%0Aaccuracy%20on%20the%20task%20of%20finding%20the%20true%20number%20of%20anomaly%20classes%20K%20in%0Acorrelated%20time%20series%20by%200.23%20compared%20to%20SSC%20and%20by%200.32%20compared%20to%20X-Means.%0AWe%20also%20show%20that%20clusters%20obtained%20by%20maximizing%20SAAI%20are%20easier%20to%20interpret%0Acompared%20to%20SSC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07172v1&entry.124074799=Read"},
{"title": "E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack\n  Prediction", "author": "Saurabh Bodhe and Zhanguang Zhang and Atia Hamidizadeh and Shixiong Kai and Yingxue Zhang and Mingxuan Yuan", "abstract": "  Pre-routing slack prediction remains a critical area of research in\nElectronic Design Automation (EDA). Despite numerous machine learning-based\napproaches targeting this task, there is still a lack of a truly end-to-end\nframework that engineers can use to obtain TNS/WNS metrics from raw circuit\ndata at the placement stage. Existing works have demonstrated effectiveness in\nArrival Time (AT) prediction but lack a mechanism for Required Arrival Time\n(RAT) prediction, which is essential for slack prediction and obtaining TNS/WNS\nmetrics. In this work, we propose E2ESlack, an end-to-end graph-based framework\nfor pre-routing slack prediction. The framework includes a TimingParser that\nsupports DEF, SDF and LIB files for feature extraction and graph construction,\nan arrival time prediction model and a fast RAT estimation module. To the best\nof our knowledge, this is the first work capable of predicting path-level\nslacks at the pre-routing stage. We perform extensive experiments and\ndemonstrate that our proposed RAT estimation method outperforms the SOTA\nML-based prediction method and also pre-routing STA tool. Additionally, the\nproposed E2ESlack framework achieves TNS/WNS values comparable to post-routing\nSTA results while saving up to 23x runtime.\n", "link": "http://arxiv.org/abs/2501.07564v1", "date": "2025-01-13", "relevancy": 1.7295, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4427}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4296}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2ESlack%3A%20An%20End-to-End%20Graph-Based%20Framework%20for%20Pre-Routing%20Slack%0A%20%20Prediction&body=Title%3A%20E2ESlack%3A%20An%20End-to-End%20Graph-Based%20Framework%20for%20Pre-Routing%20Slack%0A%20%20Prediction%0AAuthor%3A%20Saurabh%20Bodhe%20and%20Zhanguang%20Zhang%20and%20Atia%20Hamidizadeh%20and%20Shixiong%20Kai%20and%20Yingxue%20Zhang%20and%20Mingxuan%20Yuan%0AAbstract%3A%20%20%20Pre-routing%20slack%20prediction%20remains%20a%20critical%20area%20of%20research%20in%0AElectronic%20Design%20Automation%20%28EDA%29.%20Despite%20numerous%20machine%20learning-based%0Aapproaches%20targeting%20this%20task%2C%20there%20is%20still%20a%20lack%20of%20a%20truly%20end-to-end%0Aframework%20that%20engineers%20can%20use%20to%20obtain%20TNS/WNS%20metrics%20from%20raw%20circuit%0Adata%20at%20the%20placement%20stage.%20Existing%20works%20have%20demonstrated%20effectiveness%20in%0AArrival%20Time%20%28AT%29%20prediction%20but%20lack%20a%20mechanism%20for%20Required%20Arrival%20Time%0A%28RAT%29%20prediction%2C%20which%20is%20essential%20for%20slack%20prediction%20and%20obtaining%20TNS/WNS%0Ametrics.%20In%20this%20work%2C%20we%20propose%20E2ESlack%2C%20an%20end-to-end%20graph-based%20framework%0Afor%20pre-routing%20slack%20prediction.%20The%20framework%20includes%20a%20TimingParser%20that%0Asupports%20DEF%2C%20SDF%20and%20LIB%20files%20for%20feature%20extraction%20and%20graph%20construction%2C%0Aan%20arrival%20time%20prediction%20model%20and%20a%20fast%20RAT%20estimation%20module.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20work%20capable%20of%20predicting%20path-level%0Aslacks%20at%20the%20pre-routing%20stage.%20We%20perform%20extensive%20experiments%20and%0Ademonstrate%20that%20our%20proposed%20RAT%20estimation%20method%20outperforms%20the%20SOTA%0AML-based%20prediction%20method%20and%20also%20pre-routing%20STA%20tool.%20Additionally%2C%20the%0Aproposed%20E2ESlack%20framework%20achieves%20TNS/WNS%20values%20comparable%20to%20post-routing%0ASTA%20results%20while%20saving%20up%20to%2023x%20runtime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2ESlack%253A%2520An%2520End-to-End%2520Graph-Based%2520Framework%2520for%2520Pre-Routing%2520Slack%250A%2520%2520Prediction%26entry.906535625%3DSaurabh%2520Bodhe%2520and%2520Zhanguang%2520Zhang%2520and%2520Atia%2520Hamidizadeh%2520and%2520Shixiong%2520Kai%2520and%2520Yingxue%2520Zhang%2520and%2520Mingxuan%2520Yuan%26entry.1292438233%3D%2520%2520Pre-routing%2520slack%2520prediction%2520remains%2520a%2520critical%2520area%2520of%2520research%2520in%250AElectronic%2520Design%2520Automation%2520%2528EDA%2529.%2520Despite%2520numerous%2520machine%2520learning-based%250Aapproaches%2520targeting%2520this%2520task%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520a%2520truly%2520end-to-end%250Aframework%2520that%2520engineers%2520can%2520use%2520to%2520obtain%2520TNS/WNS%2520metrics%2520from%2520raw%2520circuit%250Adata%2520at%2520the%2520placement%2520stage.%2520Existing%2520works%2520have%2520demonstrated%2520effectiveness%2520in%250AArrival%2520Time%2520%2528AT%2529%2520prediction%2520but%2520lack%2520a%2520mechanism%2520for%2520Required%2520Arrival%2520Time%250A%2528RAT%2529%2520prediction%252C%2520which%2520is%2520essential%2520for%2520slack%2520prediction%2520and%2520obtaining%2520TNS/WNS%250Ametrics.%2520In%2520this%2520work%252C%2520we%2520propose%2520E2ESlack%252C%2520an%2520end-to-end%2520graph-based%2520framework%250Afor%2520pre-routing%2520slack%2520prediction.%2520The%2520framework%2520includes%2520a%2520TimingParser%2520that%250Asupports%2520DEF%252C%2520SDF%2520and%2520LIB%2520files%2520for%2520feature%2520extraction%2520and%2520graph%2520construction%252C%250Aan%2520arrival%2520time%2520prediction%2520model%2520and%2520a%2520fast%2520RAT%2520estimation%2520module.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520capable%2520of%2520predicting%2520path-level%250Aslacks%2520at%2520the%2520pre-routing%2520stage.%2520We%2520perform%2520extensive%2520experiments%2520and%250Ademonstrate%2520that%2520our%2520proposed%2520RAT%2520estimation%2520method%2520outperforms%2520the%2520SOTA%250AML-based%2520prediction%2520method%2520and%2520also%2520pre-routing%2520STA%2520tool.%2520Additionally%252C%2520the%250Aproposed%2520E2ESlack%2520framework%2520achieves%2520TNS/WNS%2520values%2520comparable%2520to%2520post-routing%250ASTA%2520results%2520while%2520saving%2520up%2520to%252023x%2520runtime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2ESlack%3A%20An%20End-to-End%20Graph-Based%20Framework%20for%20Pre-Routing%20Slack%0A%20%20Prediction&entry.906535625=Saurabh%20Bodhe%20and%20Zhanguang%20Zhang%20and%20Atia%20Hamidizadeh%20and%20Shixiong%20Kai%20and%20Yingxue%20Zhang%20and%20Mingxuan%20Yuan&entry.1292438233=%20%20Pre-routing%20slack%20prediction%20remains%20a%20critical%20area%20of%20research%20in%0AElectronic%20Design%20Automation%20%28EDA%29.%20Despite%20numerous%20machine%20learning-based%0Aapproaches%20targeting%20this%20task%2C%20there%20is%20still%20a%20lack%20of%20a%20truly%20end-to-end%0Aframework%20that%20engineers%20can%20use%20to%20obtain%20TNS/WNS%20metrics%20from%20raw%20circuit%0Adata%20at%20the%20placement%20stage.%20Existing%20works%20have%20demonstrated%20effectiveness%20in%0AArrival%20Time%20%28AT%29%20prediction%20but%20lack%20a%20mechanism%20for%20Required%20Arrival%20Time%0A%28RAT%29%20prediction%2C%20which%20is%20essential%20for%20slack%20prediction%20and%20obtaining%20TNS/WNS%0Ametrics.%20In%20this%20work%2C%20we%20propose%20E2ESlack%2C%20an%20end-to-end%20graph-based%20framework%0Afor%20pre-routing%20slack%20prediction.%20The%20framework%20includes%20a%20TimingParser%20that%0Asupports%20DEF%2C%20SDF%20and%20LIB%20files%20for%20feature%20extraction%20and%20graph%20construction%2C%0Aan%20arrival%20time%20prediction%20model%20and%20a%20fast%20RAT%20estimation%20module.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20work%20capable%20of%20predicting%20path-level%0Aslacks%20at%20the%20pre-routing%20stage.%20We%20perform%20extensive%20experiments%20and%0Ademonstrate%20that%20our%20proposed%20RAT%20estimation%20method%20outperforms%20the%20SOTA%0AML-based%20prediction%20method%20and%20also%20pre-routing%20STA%20tool.%20Additionally%2C%20the%0Aproposed%20E2ESlack%20framework%20achieves%20TNS/WNS%20values%20comparable%20to%20post-routing%0ASTA%20results%20while%20saving%20up%20to%2023x%20runtime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07564v1&entry.124074799=Read"},
{"title": "RbRL2.0: Integrated Reward and Policy Learning for Rating-based\n  Reinforcement Learning", "author": "Mingkang Wu and Devin White and Vernon Lawhern and Nicholas R. Waytowich and Yongcan Cao", "abstract": "  Reinforcement learning (RL), a common tool in decision making, learns\npolicies from various experiences based on the associated cumulative\nreturn/rewards without treating them differently. On the contrary, humans often\nlearn to distinguish from different levels of performance and extract the\nunderlying trends towards improving their decision making for best performance.\nMotivated by this, this paper proposes a novel RL method that mimics humans'\ndecision making process by differentiating among collected experiences for\neffective policy learning. The main idea is to extract important directional\ninformation from experiences with different performance levels, named ratings,\nso that policies can be updated towards desired deviation from these\nexperiences with different ratings. Specifically, we propose a new policy loss\nfunction that penalizes distribution similarities between the current policy\nand failed experiences with different ratings, and assign different weights to\nthe penalty terms based on the rating classes. Meanwhile, reward learning from\nthese rated samples can be integrated with the new policy loss towards an\nintegrated reward and policy learning from rated samples. Optimizing the\nintegrated reward and policy loss function will lead to the discovery of\ndirections for policy improvement towards maximizing cumulative rewards and\npenalizing most from the lowest performance level while least from the highest\nperformance level. To evaluate the effectiveness of the proposed method, we\npresent results for experiments on a few typical environments that show\nimproved convergence and overall performance over the existing rating-based\nreinforcement learning method with only reward learning.\n", "link": "http://arxiv.org/abs/2501.07502v1", "date": "2025-01-13", "relevancy": 1.8484, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5217}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4581}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RbRL2.0%3A%20Integrated%20Reward%20and%20Policy%20Learning%20for%20Rating-based%0A%20%20Reinforcement%20Learning&body=Title%3A%20RbRL2.0%3A%20Integrated%20Reward%20and%20Policy%20Learning%20for%20Rating-based%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Mingkang%20Wu%20and%20Devin%20White%20and%20Vernon%20Lawhern%20and%20Nicholas%20R.%20Waytowich%20and%20Yongcan%20Cao%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%2C%20a%20common%20tool%20in%20decision%20making%2C%20learns%0Apolicies%20from%20various%20experiences%20based%20on%20the%20associated%20cumulative%0Areturn/rewards%20without%20treating%20them%20differently.%20On%20the%20contrary%2C%20humans%20often%0Alearn%20to%20distinguish%20from%20different%20levels%20of%20performance%20and%20extract%20the%0Aunderlying%20trends%20towards%20improving%20their%20decision%20making%20for%20best%20performance.%0AMotivated%20by%20this%2C%20this%20paper%20proposes%20a%20novel%20RL%20method%20that%20mimics%20humans%27%0Adecision%20making%20process%20by%20differentiating%20among%20collected%20experiences%20for%0Aeffective%20policy%20learning.%20The%20main%20idea%20is%20to%20extract%20important%20directional%0Ainformation%20from%20experiences%20with%20different%20performance%20levels%2C%20named%20ratings%2C%0Aso%20that%20policies%20can%20be%20updated%20towards%20desired%20deviation%20from%20these%0Aexperiences%20with%20different%20ratings.%20Specifically%2C%20we%20propose%20a%20new%20policy%20loss%0Afunction%20that%20penalizes%20distribution%20similarities%20between%20the%20current%20policy%0Aand%20failed%20experiences%20with%20different%20ratings%2C%20and%20assign%20different%20weights%20to%0Athe%20penalty%20terms%20based%20on%20the%20rating%20classes.%20Meanwhile%2C%20reward%20learning%20from%0Athese%20rated%20samples%20can%20be%20integrated%20with%20the%20new%20policy%20loss%20towards%20an%0Aintegrated%20reward%20and%20policy%20learning%20from%20rated%20samples.%20Optimizing%20the%0Aintegrated%20reward%20and%20policy%20loss%20function%20will%20lead%20to%20the%20discovery%20of%0Adirections%20for%20policy%20improvement%20towards%20maximizing%20cumulative%20rewards%20and%0Apenalizing%20most%20from%20the%20lowest%20performance%20level%20while%20least%20from%20the%20highest%0Aperformance%20level.%20To%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20we%0Apresent%20results%20for%20experiments%20on%20a%20few%20typical%20environments%20that%20show%0Aimproved%20convergence%20and%20overall%20performance%20over%20the%20existing%20rating-based%0Areinforcement%20learning%20method%20with%20only%20reward%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRbRL2.0%253A%2520Integrated%2520Reward%2520and%2520Policy%2520Learning%2520for%2520Rating-based%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DMingkang%2520Wu%2520and%2520Devin%2520White%2520and%2520Vernon%2520Lawhern%2520and%2520Nicholas%2520R.%2520Waytowich%2520and%2520Yongcan%2520Cao%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%252C%2520a%2520common%2520tool%2520in%2520decision%2520making%252C%2520learns%250Apolicies%2520from%2520various%2520experiences%2520based%2520on%2520the%2520associated%2520cumulative%250Areturn/rewards%2520without%2520treating%2520them%2520differently.%2520On%2520the%2520contrary%252C%2520humans%2520often%250Alearn%2520to%2520distinguish%2520from%2520different%2520levels%2520of%2520performance%2520and%2520extract%2520the%250Aunderlying%2520trends%2520towards%2520improving%2520their%2520decision%2520making%2520for%2520best%2520performance.%250AMotivated%2520by%2520this%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520RL%2520method%2520that%2520mimics%2520humans%2527%250Adecision%2520making%2520process%2520by%2520differentiating%2520among%2520collected%2520experiences%2520for%250Aeffective%2520policy%2520learning.%2520The%2520main%2520idea%2520is%2520to%2520extract%2520important%2520directional%250Ainformation%2520from%2520experiences%2520with%2520different%2520performance%2520levels%252C%2520named%2520ratings%252C%250Aso%2520that%2520policies%2520can%2520be%2520updated%2520towards%2520desired%2520deviation%2520from%2520these%250Aexperiences%2520with%2520different%2520ratings.%2520Specifically%252C%2520we%2520propose%2520a%2520new%2520policy%2520loss%250Afunction%2520that%2520penalizes%2520distribution%2520similarities%2520between%2520the%2520current%2520policy%250Aand%2520failed%2520experiences%2520with%2520different%2520ratings%252C%2520and%2520assign%2520different%2520weights%2520to%250Athe%2520penalty%2520terms%2520based%2520on%2520the%2520rating%2520classes.%2520Meanwhile%252C%2520reward%2520learning%2520from%250Athese%2520rated%2520samples%2520can%2520be%2520integrated%2520with%2520the%2520new%2520policy%2520loss%2520towards%2520an%250Aintegrated%2520reward%2520and%2520policy%2520learning%2520from%2520rated%2520samples.%2520Optimizing%2520the%250Aintegrated%2520reward%2520and%2520policy%2520loss%2520function%2520will%2520lead%2520to%2520the%2520discovery%2520of%250Adirections%2520for%2520policy%2520improvement%2520towards%2520maximizing%2520cumulative%2520rewards%2520and%250Apenalizing%2520most%2520from%2520the%2520lowest%2520performance%2520level%2520while%2520least%2520from%2520the%2520highest%250Aperformance%2520level.%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520we%250Apresent%2520results%2520for%2520experiments%2520on%2520a%2520few%2520typical%2520environments%2520that%2520show%250Aimproved%2520convergence%2520and%2520overall%2520performance%2520over%2520the%2520existing%2520rating-based%250Areinforcement%2520learning%2520method%2520with%2520only%2520reward%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RbRL2.0%3A%20Integrated%20Reward%20and%20Policy%20Learning%20for%20Rating-based%0A%20%20Reinforcement%20Learning&entry.906535625=Mingkang%20Wu%20and%20Devin%20White%20and%20Vernon%20Lawhern%20and%20Nicholas%20R.%20Waytowich%20and%20Yongcan%20Cao&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%2C%20a%20common%20tool%20in%20decision%20making%2C%20learns%0Apolicies%20from%20various%20experiences%20based%20on%20the%20associated%20cumulative%0Areturn/rewards%20without%20treating%20them%20differently.%20On%20the%20contrary%2C%20humans%20often%0Alearn%20to%20distinguish%20from%20different%20levels%20of%20performance%20and%20extract%20the%0Aunderlying%20trends%20towards%20improving%20their%20decision%20making%20for%20best%20performance.%0AMotivated%20by%20this%2C%20this%20paper%20proposes%20a%20novel%20RL%20method%20that%20mimics%20humans%27%0Adecision%20making%20process%20by%20differentiating%20among%20collected%20experiences%20for%0Aeffective%20policy%20learning.%20The%20main%20idea%20is%20to%20extract%20important%20directional%0Ainformation%20from%20experiences%20with%20different%20performance%20levels%2C%20named%20ratings%2C%0Aso%20that%20policies%20can%20be%20updated%20towards%20desired%20deviation%20from%20these%0Aexperiences%20with%20different%20ratings.%20Specifically%2C%20we%20propose%20a%20new%20policy%20loss%0Afunction%20that%20penalizes%20distribution%20similarities%20between%20the%20current%20policy%0Aand%20failed%20experiences%20with%20different%20ratings%2C%20and%20assign%20different%20weights%20to%0Athe%20penalty%20terms%20based%20on%20the%20rating%20classes.%20Meanwhile%2C%20reward%20learning%20from%0Athese%20rated%20samples%20can%20be%20integrated%20with%20the%20new%20policy%20loss%20towards%20an%0Aintegrated%20reward%20and%20policy%20learning%20from%20rated%20samples.%20Optimizing%20the%0Aintegrated%20reward%20and%20policy%20loss%20function%20will%20lead%20to%20the%20discovery%20of%0Adirections%20for%20policy%20improvement%20towards%20maximizing%20cumulative%20rewards%20and%0Apenalizing%20most%20from%20the%20lowest%20performance%20level%20while%20least%20from%20the%20highest%0Aperformance%20level.%20To%20evaluate%20the%20effectiveness%20of%20the%20proposed%20method%2C%20we%0Apresent%20results%20for%20experiments%20on%20a%20few%20typical%20environments%20that%20show%0Aimproved%20convergence%20and%20overall%20performance%20over%20the%20existing%20rating-based%0Areinforcement%20learning%20method%20with%20only%20reward%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07502v1&entry.124074799=Read"},
{"title": "A monthly sub-national Harmonized Food Insecurity Dataset for\n  comprehensive analysis and predictive modeling", "author": "M\u00e9lissande Machefer and Michele Ronco and Anne-Claire Thomas and Michael Assouline and Melanie Rabier and Christina Corbane and Felix Rembold", "abstract": "  Food security is a complex, multidimensional concept challenging to measure\ncomprehensively. Effective anticipation, monitoring, and mitigation of food\ncrises require timely and comprehensive global data. This paper introduces the\nHarmonized Food Insecurity Dataset (HFID), an open-source resource\nconsolidating four key data sources: the Integrated Food Security Phase\nClassification (IPC)/Cadre Harmonis\\'e (CH) phases, the Famine Early Warning\nSystems Network (FEWS NET) IPC-compatible phases, and the World Food Program's\n(WFP) Food Consumption Score (FCS) and reduced Coping Strategy Index (rCSI).\nUpdated monthly and using a common reference system for administrative units,\nthe HFID offers extensive spatial and temporal coverage. It serves as a vital\ntool for food security experts and humanitarian agencies, providing a unified\nresource for analyzing food security conditions and highlighting global data\ndisparities. The scientific community can also leverage the HFID to develop\ndata-driven predictive models, enhancing the capacity to forecast and prevent\nfuture food crises.\n", "link": "http://arxiv.org/abs/2501.06076v2", "date": "2025-01-13", "relevancy": 1.1637, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4037}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.3773}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20monthly%20sub-national%20Harmonized%20Food%20Insecurity%20Dataset%20for%0A%20%20comprehensive%20analysis%20and%20predictive%20modeling&body=Title%3A%20A%20monthly%20sub-national%20Harmonized%20Food%20Insecurity%20Dataset%20for%0A%20%20comprehensive%20analysis%20and%20predictive%20modeling%0AAuthor%3A%20M%C3%A9lissande%20Machefer%20and%20Michele%20Ronco%20and%20Anne-Claire%20Thomas%20and%20Michael%20Assouline%20and%20Melanie%20Rabier%20and%20Christina%20Corbane%20and%20Felix%20Rembold%0AAbstract%3A%20%20%20Food%20security%20is%20a%20complex%2C%20multidimensional%20concept%20challenging%20to%20measure%0Acomprehensively.%20Effective%20anticipation%2C%20monitoring%2C%20and%20mitigation%20of%20food%0Acrises%20require%20timely%20and%20comprehensive%20global%20data.%20This%20paper%20introduces%20the%0AHarmonized%20Food%20Insecurity%20Dataset%20%28HFID%29%2C%20an%20open-source%20resource%0Aconsolidating%20four%20key%20data%20sources%3A%20the%20Integrated%20Food%20Security%20Phase%0AClassification%20%28IPC%29/Cadre%20Harmonis%5C%27e%20%28CH%29%20phases%2C%20the%20Famine%20Early%20Warning%0ASystems%20Network%20%28FEWS%20NET%29%20IPC-compatible%20phases%2C%20and%20the%20World%20Food%20Program%27s%0A%28WFP%29%20Food%20Consumption%20Score%20%28FCS%29%20and%20reduced%20Coping%20Strategy%20Index%20%28rCSI%29.%0AUpdated%20monthly%20and%20using%20a%20common%20reference%20system%20for%20administrative%20units%2C%0Athe%20HFID%20offers%20extensive%20spatial%20and%20temporal%20coverage.%20It%20serves%20as%20a%20vital%0Atool%20for%20food%20security%20experts%20and%20humanitarian%20agencies%2C%20providing%20a%20unified%0Aresource%20for%20analyzing%20food%20security%20conditions%20and%20highlighting%20global%20data%0Adisparities.%20The%20scientific%20community%20can%20also%20leverage%20the%20HFID%20to%20develop%0Adata-driven%20predictive%20models%2C%20enhancing%20the%20capacity%20to%20forecast%20and%20prevent%0Afuture%20food%20crises.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06076v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520monthly%2520sub-national%2520Harmonized%2520Food%2520Insecurity%2520Dataset%2520for%250A%2520%2520comprehensive%2520analysis%2520and%2520predictive%2520modeling%26entry.906535625%3DM%25C3%25A9lissande%2520Machefer%2520and%2520Michele%2520Ronco%2520and%2520Anne-Claire%2520Thomas%2520and%2520Michael%2520Assouline%2520and%2520Melanie%2520Rabier%2520and%2520Christina%2520Corbane%2520and%2520Felix%2520Rembold%26entry.1292438233%3D%2520%2520Food%2520security%2520is%2520a%2520complex%252C%2520multidimensional%2520concept%2520challenging%2520to%2520measure%250Acomprehensively.%2520Effective%2520anticipation%252C%2520monitoring%252C%2520and%2520mitigation%2520of%2520food%250Acrises%2520require%2520timely%2520and%2520comprehensive%2520global%2520data.%2520This%2520paper%2520introduces%2520the%250AHarmonized%2520Food%2520Insecurity%2520Dataset%2520%2528HFID%2529%252C%2520an%2520open-source%2520resource%250Aconsolidating%2520four%2520key%2520data%2520sources%253A%2520the%2520Integrated%2520Food%2520Security%2520Phase%250AClassification%2520%2528IPC%2529/Cadre%2520Harmonis%255C%2527e%2520%2528CH%2529%2520phases%252C%2520the%2520Famine%2520Early%2520Warning%250ASystems%2520Network%2520%2528FEWS%2520NET%2529%2520IPC-compatible%2520phases%252C%2520and%2520the%2520World%2520Food%2520Program%2527s%250A%2528WFP%2529%2520Food%2520Consumption%2520Score%2520%2528FCS%2529%2520and%2520reduced%2520Coping%2520Strategy%2520Index%2520%2528rCSI%2529.%250AUpdated%2520monthly%2520and%2520using%2520a%2520common%2520reference%2520system%2520for%2520administrative%2520units%252C%250Athe%2520HFID%2520offers%2520extensive%2520spatial%2520and%2520temporal%2520coverage.%2520It%2520serves%2520as%2520a%2520vital%250Atool%2520for%2520food%2520security%2520experts%2520and%2520humanitarian%2520agencies%252C%2520providing%2520a%2520unified%250Aresource%2520for%2520analyzing%2520food%2520security%2520conditions%2520and%2520highlighting%2520global%2520data%250Adisparities.%2520The%2520scientific%2520community%2520can%2520also%2520leverage%2520the%2520HFID%2520to%2520develop%250Adata-driven%2520predictive%2520models%252C%2520enhancing%2520the%2520capacity%2520to%2520forecast%2520and%2520prevent%250Afuture%2520food%2520crises.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06076v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20monthly%20sub-national%20Harmonized%20Food%20Insecurity%20Dataset%20for%0A%20%20comprehensive%20analysis%20and%20predictive%20modeling&entry.906535625=M%C3%A9lissande%20Machefer%20and%20Michele%20Ronco%20and%20Anne-Claire%20Thomas%20and%20Michael%20Assouline%20and%20Melanie%20Rabier%20and%20Christina%20Corbane%20and%20Felix%20Rembold&entry.1292438233=%20%20Food%20security%20is%20a%20complex%2C%20multidimensional%20concept%20challenging%20to%20measure%0Acomprehensively.%20Effective%20anticipation%2C%20monitoring%2C%20and%20mitigation%20of%20food%0Acrises%20require%20timely%20and%20comprehensive%20global%20data.%20This%20paper%20introduces%20the%0AHarmonized%20Food%20Insecurity%20Dataset%20%28HFID%29%2C%20an%20open-source%20resource%0Aconsolidating%20four%20key%20data%20sources%3A%20the%20Integrated%20Food%20Security%20Phase%0AClassification%20%28IPC%29/Cadre%20Harmonis%5C%27e%20%28CH%29%20phases%2C%20the%20Famine%20Early%20Warning%0ASystems%20Network%20%28FEWS%20NET%29%20IPC-compatible%20phases%2C%20and%20the%20World%20Food%20Program%27s%0A%28WFP%29%20Food%20Consumption%20Score%20%28FCS%29%20and%20reduced%20Coping%20Strategy%20Index%20%28rCSI%29.%0AUpdated%20monthly%20and%20using%20a%20common%20reference%20system%20for%20administrative%20units%2C%0Athe%20HFID%20offers%20extensive%20spatial%20and%20temporal%20coverage.%20It%20serves%20as%20a%20vital%0Atool%20for%20food%20security%20experts%20and%20humanitarian%20agencies%2C%20providing%20a%20unified%0Aresource%20for%20analyzing%20food%20security%20conditions%20and%20highlighting%20global%20data%0Adisparities.%20The%20scientific%20community%20can%20also%20leverage%20the%20HFID%20to%20develop%0Adata-driven%20predictive%20models%2C%20enhancing%20the%20capacity%20to%20forecast%20and%20prevent%0Afuture%20food%20crises.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06076v2&entry.124074799=Read"},
{"title": "Lessons From Red Teaming 100 Generative AI Products", "author": "Blake Bullwinkel and Amanda Minnich and Shiven Chawla and Gary Lopez and Martin Pouliot and Whitney Maxwell and Joris de Gruyter and Katherine Pratt and Saphir Qi and Nina Chikanov and Roman Lutz and Raja Sekhar Rao Dheekonda and Bolor-Erdene Jagdagdorj and Eugenia Kim and Justin Song and Keegan Hines and Daniel Jones and Giorgio Severi and Richard Lundeen and Sam Vaughan and Victoria Westerhoff and Pete Bryan and Ram Shankar Siva Kumar and Yonatan Zunger and Chang Kawaguchi and Mark Russinovich", "abstract": "  In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider.\n", "link": "http://arxiv.org/abs/2501.07238v1", "date": "2025-01-13", "relevancy": 1.8199, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.485}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4454}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lessons%20From%20Red%20Teaming%20100%20Generative%20AI%20Products&body=Title%3A%20Lessons%20From%20Red%20Teaming%20100%20Generative%20AI%20Products%0AAuthor%3A%20Blake%20Bullwinkel%20and%20Amanda%20Minnich%20and%20Shiven%20Chawla%20and%20Gary%20Lopez%20and%20Martin%20Pouliot%20and%20Whitney%20Maxwell%20and%20Joris%20de%20Gruyter%20and%20Katherine%20Pratt%20and%20Saphir%20Qi%20and%20Nina%20Chikanov%20and%20Roman%20Lutz%20and%20Raja%20Sekhar%20Rao%20Dheekonda%20and%20Bolor-Erdene%20Jagdagdorj%20and%20Eugenia%20Kim%20and%20Justin%20Song%20and%20Keegan%20Hines%20and%20Daniel%20Jones%20and%20Giorgio%20Severi%20and%20Richard%20Lundeen%20and%20Sam%20Vaughan%20and%20Victoria%20Westerhoff%20and%20Pete%20Bryan%20and%20Ram%20Shankar%20Siva%20Kumar%20and%20Yonatan%20Zunger%20and%20Chang%20Kawaguchi%20and%20Mark%20Russinovich%0AAbstract%3A%20%20%20In%20recent%20years%2C%20AI%20red%20teaming%20has%20emerged%20as%20a%20practice%20for%20probing%20the%0Asafety%20and%20security%20of%20generative%20AI%20systems.%20Due%20to%20the%20nascency%20of%20the%20field%2C%0Athere%20are%20many%20open%20questions%20about%20how%20red%20teaming%20operations%20should%20be%0Aconducted.%20Based%20on%20our%20experience%20red%20teaming%20over%20100%20generative%20AI%20products%0Aat%20Microsoft%2C%20we%20present%20our%20internal%20threat%20model%20ontology%20and%20eight%20main%0Alessons%20we%20have%20learned%3A%0A%20%201.%20Understand%20what%20the%20system%20can%20do%20and%20where%20it%20is%20applied%0A%20%202.%20You%20don%27t%20have%20to%20compute%20gradients%20to%20break%20an%20AI%20system%0A%20%203.%20AI%20red%20teaming%20is%20not%20safety%20benchmarking%0A%20%204.%20Automation%20can%20help%20cover%20more%20of%20the%20risk%20landscape%0A%20%205.%20The%20human%20element%20of%20AI%20red%20teaming%20is%20crucial%0A%20%206.%20Responsible%20AI%20harms%20are%20pervasive%20but%20difficult%20to%20measure%0A%20%207.%20LLMs%20amplify%20existing%20security%20risks%20and%20introduce%20new%20ones%0A%20%208.%20The%20work%20of%20securing%20AI%20systems%20will%20never%20be%20complete%0A%20%20By%20sharing%20these%20insights%20alongside%20case%20studies%20from%20our%20operations%2C%20we%0Aoffer%20practical%20recommendations%20aimed%20at%20aligning%20red%20teaming%20efforts%20with%20real%0Aworld%20risks.%20We%20also%20highlight%20aspects%20of%20AI%20red%20teaming%20that%20we%20believe%20are%0Aoften%20misunderstood%20and%20discuss%20open%20questions%20for%20the%20field%20to%20consider.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLessons%2520From%2520Red%2520Teaming%2520100%2520Generative%2520AI%2520Products%26entry.906535625%3DBlake%2520Bullwinkel%2520and%2520Amanda%2520Minnich%2520and%2520Shiven%2520Chawla%2520and%2520Gary%2520Lopez%2520and%2520Martin%2520Pouliot%2520and%2520Whitney%2520Maxwell%2520and%2520Joris%2520de%2520Gruyter%2520and%2520Katherine%2520Pratt%2520and%2520Saphir%2520Qi%2520and%2520Nina%2520Chikanov%2520and%2520Roman%2520Lutz%2520and%2520Raja%2520Sekhar%2520Rao%2520Dheekonda%2520and%2520Bolor-Erdene%2520Jagdagdorj%2520and%2520Eugenia%2520Kim%2520and%2520Justin%2520Song%2520and%2520Keegan%2520Hines%2520and%2520Daniel%2520Jones%2520and%2520Giorgio%2520Severi%2520and%2520Richard%2520Lundeen%2520and%2520Sam%2520Vaughan%2520and%2520Victoria%2520Westerhoff%2520and%2520Pete%2520Bryan%2520and%2520Ram%2520Shankar%2520Siva%2520Kumar%2520and%2520Yonatan%2520Zunger%2520and%2520Chang%2520Kawaguchi%2520and%2520Mark%2520Russinovich%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520AI%2520red%2520teaming%2520has%2520emerged%2520as%2520a%2520practice%2520for%2520probing%2520the%250Asafety%2520and%2520security%2520of%2520generative%2520AI%2520systems.%2520Due%2520to%2520the%2520nascency%2520of%2520the%2520field%252C%250Athere%2520are%2520many%2520open%2520questions%2520about%2520how%2520red%2520teaming%2520operations%2520should%2520be%250Aconducted.%2520Based%2520on%2520our%2520experience%2520red%2520teaming%2520over%2520100%2520generative%2520AI%2520products%250Aat%2520Microsoft%252C%2520we%2520present%2520our%2520internal%2520threat%2520model%2520ontology%2520and%2520eight%2520main%250Alessons%2520we%2520have%2520learned%253A%250A%2520%25201.%2520Understand%2520what%2520the%2520system%2520can%2520do%2520and%2520where%2520it%2520is%2520applied%250A%2520%25202.%2520You%2520don%2527t%2520have%2520to%2520compute%2520gradients%2520to%2520break%2520an%2520AI%2520system%250A%2520%25203.%2520AI%2520red%2520teaming%2520is%2520not%2520safety%2520benchmarking%250A%2520%25204.%2520Automation%2520can%2520help%2520cover%2520more%2520of%2520the%2520risk%2520landscape%250A%2520%25205.%2520The%2520human%2520element%2520of%2520AI%2520red%2520teaming%2520is%2520crucial%250A%2520%25206.%2520Responsible%2520AI%2520harms%2520are%2520pervasive%2520but%2520difficult%2520to%2520measure%250A%2520%25207.%2520LLMs%2520amplify%2520existing%2520security%2520risks%2520and%2520introduce%2520new%2520ones%250A%2520%25208.%2520The%2520work%2520of%2520securing%2520AI%2520systems%2520will%2520never%2520be%2520complete%250A%2520%2520By%2520sharing%2520these%2520insights%2520alongside%2520case%2520studies%2520from%2520our%2520operations%252C%2520we%250Aoffer%2520practical%2520recommendations%2520aimed%2520at%2520aligning%2520red%2520teaming%2520efforts%2520with%2520real%250Aworld%2520risks.%2520We%2520also%2520highlight%2520aspects%2520of%2520AI%2520red%2520teaming%2520that%2520we%2520believe%2520are%250Aoften%2520misunderstood%2520and%2520discuss%2520open%2520questions%2520for%2520the%2520field%2520to%2520consider.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lessons%20From%20Red%20Teaming%20100%20Generative%20AI%20Products&entry.906535625=Blake%20Bullwinkel%20and%20Amanda%20Minnich%20and%20Shiven%20Chawla%20and%20Gary%20Lopez%20and%20Martin%20Pouliot%20and%20Whitney%20Maxwell%20and%20Joris%20de%20Gruyter%20and%20Katherine%20Pratt%20and%20Saphir%20Qi%20and%20Nina%20Chikanov%20and%20Roman%20Lutz%20and%20Raja%20Sekhar%20Rao%20Dheekonda%20and%20Bolor-Erdene%20Jagdagdorj%20and%20Eugenia%20Kim%20and%20Justin%20Song%20and%20Keegan%20Hines%20and%20Daniel%20Jones%20and%20Giorgio%20Severi%20and%20Richard%20Lundeen%20and%20Sam%20Vaughan%20and%20Victoria%20Westerhoff%20and%20Pete%20Bryan%20and%20Ram%20Shankar%20Siva%20Kumar%20and%20Yonatan%20Zunger%20and%20Chang%20Kawaguchi%20and%20Mark%20Russinovich&entry.1292438233=%20%20In%20recent%20years%2C%20AI%20red%20teaming%20has%20emerged%20as%20a%20practice%20for%20probing%20the%0Asafety%20and%20security%20of%20generative%20AI%20systems.%20Due%20to%20the%20nascency%20of%20the%20field%2C%0Athere%20are%20many%20open%20questions%20about%20how%20red%20teaming%20operations%20should%20be%0Aconducted.%20Based%20on%20our%20experience%20red%20teaming%20over%20100%20generative%20AI%20products%0Aat%20Microsoft%2C%20we%20present%20our%20internal%20threat%20model%20ontology%20and%20eight%20main%0Alessons%20we%20have%20learned%3A%0A%20%201.%20Understand%20what%20the%20system%20can%20do%20and%20where%20it%20is%20applied%0A%20%202.%20You%20don%27t%20have%20to%20compute%20gradients%20to%20break%20an%20AI%20system%0A%20%203.%20AI%20red%20teaming%20is%20not%20safety%20benchmarking%0A%20%204.%20Automation%20can%20help%20cover%20more%20of%20the%20risk%20landscape%0A%20%205.%20The%20human%20element%20of%20AI%20red%20teaming%20is%20crucial%0A%20%206.%20Responsible%20AI%20harms%20are%20pervasive%20but%20difficult%20to%20measure%0A%20%207.%20LLMs%20amplify%20existing%20security%20risks%20and%20introduce%20new%20ones%0A%20%208.%20The%20work%20of%20securing%20AI%20systems%20will%20never%20be%20complete%0A%20%20By%20sharing%20these%20insights%20alongside%20case%20studies%20from%20our%20operations%2C%20we%0Aoffer%20practical%20recommendations%20aimed%20at%20aligning%20red%20teaming%20efforts%20with%20real%0Aworld%20risks.%20We%20also%20highlight%20aspects%20of%20AI%20red%20teaming%20that%20we%20believe%20are%0Aoften%20misunderstood%20and%20discuss%20open%20questions%20for%20the%20field%20to%20consider.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07238v1&entry.124074799=Read"},
{"title": "Evaluating Robotic Approach Techniques for the Insertion of a Straight\n  Instrument into a Vitreoretinal Surgery Trocar", "author": "Ross Henry and Martin Huber and Anestis Mablekos-Alexiou and Carlo Seneci and Mohamed Abdelaziz and Hans Natalius and Lyndon da Cruz and Christos Bergeles", "abstract": "  Advances in vitreoretinal robotic surgery enable precise techniques for gene\ntherapies. This study evaluates three robotic approaches using the 7-DoF\nrobotic arm for docking a micro-precise tool to a trocar: fully co-manipulated,\nhybrid co-manipulated/teleoperated, and hybrid with camera assistance. The\nfully co-manipulated approach was the fastest but had a 42% success rate.\nHybrid methods showed higher success rates (91.6% and 100%) and completed tasks\nwithin 2 minutes. NASA Task Load Index (TLX) assessments indicated lower\nphysical demand and effort for hybrid approaches.\n", "link": "http://arxiv.org/abs/2501.07180v1", "date": "2025-01-13", "relevancy": 1.4162, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4875}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4855}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Robotic%20Approach%20Techniques%20for%20the%20Insertion%20of%20a%20Straight%0A%20%20Instrument%20into%20a%20Vitreoretinal%20Surgery%20Trocar&body=Title%3A%20Evaluating%20Robotic%20Approach%20Techniques%20for%20the%20Insertion%20of%20a%20Straight%0A%20%20Instrument%20into%20a%20Vitreoretinal%20Surgery%20Trocar%0AAuthor%3A%20Ross%20Henry%20and%20Martin%20Huber%20and%20Anestis%20Mablekos-Alexiou%20and%20Carlo%20Seneci%20and%20Mohamed%20Abdelaziz%20and%20Hans%20Natalius%20and%20Lyndon%20da%20Cruz%20and%20Christos%20Bergeles%0AAbstract%3A%20%20%20Advances%20in%20vitreoretinal%20robotic%20surgery%20enable%20precise%20techniques%20for%20gene%0Atherapies.%20This%20study%20evaluates%20three%20robotic%20approaches%20using%20the%207-DoF%0Arobotic%20arm%20for%20docking%20a%20micro-precise%20tool%20to%20a%20trocar%3A%20fully%20co-manipulated%2C%0Ahybrid%20co-manipulated/teleoperated%2C%20and%20hybrid%20with%20camera%20assistance.%20The%0Afully%20co-manipulated%20approach%20was%20the%20fastest%20but%20had%20a%2042%25%20success%20rate.%0AHybrid%20methods%20showed%20higher%20success%20rates%20%2891.6%25%20and%20100%25%29%20and%20completed%20tasks%0Awithin%202%20minutes.%20NASA%20Task%20Load%20Index%20%28TLX%29%20assessments%20indicated%20lower%0Aphysical%20demand%20and%20effort%20for%20hybrid%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Robotic%2520Approach%2520Techniques%2520for%2520the%2520Insertion%2520of%2520a%2520Straight%250A%2520%2520Instrument%2520into%2520a%2520Vitreoretinal%2520Surgery%2520Trocar%26entry.906535625%3DRoss%2520Henry%2520and%2520Martin%2520Huber%2520and%2520Anestis%2520Mablekos-Alexiou%2520and%2520Carlo%2520Seneci%2520and%2520Mohamed%2520Abdelaziz%2520and%2520Hans%2520Natalius%2520and%2520Lyndon%2520da%2520Cruz%2520and%2520Christos%2520Bergeles%26entry.1292438233%3D%2520%2520Advances%2520in%2520vitreoretinal%2520robotic%2520surgery%2520enable%2520precise%2520techniques%2520for%2520gene%250Atherapies.%2520This%2520study%2520evaluates%2520three%2520robotic%2520approaches%2520using%2520the%25207-DoF%250Arobotic%2520arm%2520for%2520docking%2520a%2520micro-precise%2520tool%2520to%2520a%2520trocar%253A%2520fully%2520co-manipulated%252C%250Ahybrid%2520co-manipulated/teleoperated%252C%2520and%2520hybrid%2520with%2520camera%2520assistance.%2520The%250Afully%2520co-manipulated%2520approach%2520was%2520the%2520fastest%2520but%2520had%2520a%252042%2525%2520success%2520rate.%250AHybrid%2520methods%2520showed%2520higher%2520success%2520rates%2520%252891.6%2525%2520and%2520100%2525%2529%2520and%2520completed%2520tasks%250Awithin%25202%2520minutes.%2520NASA%2520Task%2520Load%2520Index%2520%2528TLX%2529%2520assessments%2520indicated%2520lower%250Aphysical%2520demand%2520and%2520effort%2520for%2520hybrid%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Robotic%20Approach%20Techniques%20for%20the%20Insertion%20of%20a%20Straight%0A%20%20Instrument%20into%20a%20Vitreoretinal%20Surgery%20Trocar&entry.906535625=Ross%20Henry%20and%20Martin%20Huber%20and%20Anestis%20Mablekos-Alexiou%20and%20Carlo%20Seneci%20and%20Mohamed%20Abdelaziz%20and%20Hans%20Natalius%20and%20Lyndon%20da%20Cruz%20and%20Christos%20Bergeles&entry.1292438233=%20%20Advances%20in%20vitreoretinal%20robotic%20surgery%20enable%20precise%20techniques%20for%20gene%0Atherapies.%20This%20study%20evaluates%20three%20robotic%20approaches%20using%20the%207-DoF%0Arobotic%20arm%20for%20docking%20a%20micro-precise%20tool%20to%20a%20trocar%3A%20fully%20co-manipulated%2C%0Ahybrid%20co-manipulated/teleoperated%2C%20and%20hybrid%20with%20camera%20assistance.%20The%0Afully%20co-manipulated%20approach%20was%20the%20fastest%20but%20had%20a%2042%25%20success%20rate.%0AHybrid%20methods%20showed%20higher%20success%20rates%20%2891.6%25%20and%20100%25%29%20and%20completed%20tasks%0Awithin%202%20minutes.%20NASA%20Task%20Load%20Index%20%28TLX%29%20assessments%20indicated%20lower%0Aphysical%20demand%20and%20effort%20for%20hybrid%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07180v1&entry.124074799=Read"},
{"title": "PROTECT: Protein circadian time prediction using unsupervised learning", "author": "Aram Ansary Ogholbake and Qiang Cheng", "abstract": "  Circadian rhythms regulate the physiology and behavior of humans and animals.\nDespite advancements in understanding these rhythms and predicting circadian\nphases at the transcriptional level, predicting circadian phases from proteomic\ndata remains elusive. This challenge is largely due to the scarcity of time\nlabels in proteomic datasets, which are often characterized by small sample\nsizes, high dimensionality, and significant noise. Furthermore, existing\nmethods for predicting circadian phases from transcriptomic data typically rely\non prior knowledge of known rhythmic genes, making them unsuitable for\nproteomic datasets. To address this gap, we developed a novel computational\nmethod using unsupervised deep learning techniques to predict circadian sample\nphases from proteomic data without requiring time labels or prior knowledge of\nproteins or genes. Our model involves a two-stage training process optimized\nfor robust circadian phase prediction: an initial greedy one-layer-at-a-time\npre-training which generates informative initial parameters followed by\nfine-tuning. During fine-tuning, a specialized loss function guides the model\nto align protein expression levels with circadian patterns, enabling it to\naccurately capture the underlying rhythmic structure within the data. We tested\nour method on both time-labeled and unlabeled proteomic data. For labeled data,\nwe compared our predictions to the known time labels, achieving high accuracy,\nwhile for unlabeled human datasets, including postmortem brain regions and\nurine samples, we explored circadian disruptions. Notably, our analysis\nidentified disruptions in rhythmic proteins between Alzheimer's disease and\ncontrol subjects across these samples.\n", "link": "http://arxiv.org/abs/2501.07405v1", "date": "2025-01-13", "relevancy": 1.4947, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5257}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4657}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROTECT%3A%20Protein%20circadian%20time%20prediction%20using%20unsupervised%20learning&body=Title%3A%20PROTECT%3A%20Protein%20circadian%20time%20prediction%20using%20unsupervised%20learning%0AAuthor%3A%20Aram%20Ansary%20Ogholbake%20and%20Qiang%20Cheng%0AAbstract%3A%20%20%20Circadian%20rhythms%20regulate%20the%20physiology%20and%20behavior%20of%20humans%20and%20animals.%0ADespite%20advancements%20in%20understanding%20these%20rhythms%20and%20predicting%20circadian%0Aphases%20at%20the%20transcriptional%20level%2C%20predicting%20circadian%20phases%20from%20proteomic%0Adata%20remains%20elusive.%20This%20challenge%20is%20largely%20due%20to%20the%20scarcity%20of%20time%0Alabels%20in%20proteomic%20datasets%2C%20which%20are%20often%20characterized%20by%20small%20sample%0Asizes%2C%20high%20dimensionality%2C%20and%20significant%20noise.%20Furthermore%2C%20existing%0Amethods%20for%20predicting%20circadian%20phases%20from%20transcriptomic%20data%20typically%20rely%0Aon%20prior%20knowledge%20of%20known%20rhythmic%20genes%2C%20making%20them%20unsuitable%20for%0Aproteomic%20datasets.%20To%20address%20this%20gap%2C%20we%20developed%20a%20novel%20computational%0Amethod%20using%20unsupervised%20deep%20learning%20techniques%20to%20predict%20circadian%20sample%0Aphases%20from%20proteomic%20data%20without%20requiring%20time%20labels%20or%20prior%20knowledge%20of%0Aproteins%20or%20genes.%20Our%20model%20involves%20a%20two-stage%20training%20process%20optimized%0Afor%20robust%20circadian%20phase%20prediction%3A%20an%20initial%20greedy%20one-layer-at-a-time%0Apre-training%20which%20generates%20informative%20initial%20parameters%20followed%20by%0Afine-tuning.%20During%20fine-tuning%2C%20a%20specialized%20loss%20function%20guides%20the%20model%0Ato%20align%20protein%20expression%20levels%20with%20circadian%20patterns%2C%20enabling%20it%20to%0Aaccurately%20capture%20the%20underlying%20rhythmic%20structure%20within%20the%20data.%20We%20tested%0Aour%20method%20on%20both%20time-labeled%20and%20unlabeled%20proteomic%20data.%20For%20labeled%20data%2C%0Awe%20compared%20our%20predictions%20to%20the%20known%20time%20labels%2C%20achieving%20high%20accuracy%2C%0Awhile%20for%20unlabeled%20human%20datasets%2C%20including%20postmortem%20brain%20regions%20and%0Aurine%20samples%2C%20we%20explored%20circadian%20disruptions.%20Notably%2C%20our%20analysis%0Aidentified%20disruptions%20in%20rhythmic%20proteins%20between%20Alzheimer%27s%20disease%20and%0Acontrol%20subjects%20across%20these%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.07405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROTECT%253A%2520Protein%2520circadian%2520time%2520prediction%2520using%2520unsupervised%2520learning%26entry.906535625%3DAram%2520Ansary%2520Ogholbake%2520and%2520Qiang%2520Cheng%26entry.1292438233%3D%2520%2520Circadian%2520rhythms%2520regulate%2520the%2520physiology%2520and%2520behavior%2520of%2520humans%2520and%2520animals.%250ADespite%2520advancements%2520in%2520understanding%2520these%2520rhythms%2520and%2520predicting%2520circadian%250Aphases%2520at%2520the%2520transcriptional%2520level%252C%2520predicting%2520circadian%2520phases%2520from%2520proteomic%250Adata%2520remains%2520elusive.%2520This%2520challenge%2520is%2520largely%2520due%2520to%2520the%2520scarcity%2520of%2520time%250Alabels%2520in%2520proteomic%2520datasets%252C%2520which%2520are%2520often%2520characterized%2520by%2520small%2520sample%250Asizes%252C%2520high%2520dimensionality%252C%2520and%2520significant%2520noise.%2520Furthermore%252C%2520existing%250Amethods%2520for%2520predicting%2520circadian%2520phases%2520from%2520transcriptomic%2520data%2520typically%2520rely%250Aon%2520prior%2520knowledge%2520of%2520known%2520rhythmic%2520genes%252C%2520making%2520them%2520unsuitable%2520for%250Aproteomic%2520datasets.%2520To%2520address%2520this%2520gap%252C%2520we%2520developed%2520a%2520novel%2520computational%250Amethod%2520using%2520unsupervised%2520deep%2520learning%2520techniques%2520to%2520predict%2520circadian%2520sample%250Aphases%2520from%2520proteomic%2520data%2520without%2520requiring%2520time%2520labels%2520or%2520prior%2520knowledge%2520of%250Aproteins%2520or%2520genes.%2520Our%2520model%2520involves%2520a%2520two-stage%2520training%2520process%2520optimized%250Afor%2520robust%2520circadian%2520phase%2520prediction%253A%2520an%2520initial%2520greedy%2520one-layer-at-a-time%250Apre-training%2520which%2520generates%2520informative%2520initial%2520parameters%2520followed%2520by%250Afine-tuning.%2520During%2520fine-tuning%252C%2520a%2520specialized%2520loss%2520function%2520guides%2520the%2520model%250Ato%2520align%2520protein%2520expression%2520levels%2520with%2520circadian%2520patterns%252C%2520enabling%2520it%2520to%250Aaccurately%2520capture%2520the%2520underlying%2520rhythmic%2520structure%2520within%2520the%2520data.%2520We%2520tested%250Aour%2520method%2520on%2520both%2520time-labeled%2520and%2520unlabeled%2520proteomic%2520data.%2520For%2520labeled%2520data%252C%250Awe%2520compared%2520our%2520predictions%2520to%2520the%2520known%2520time%2520labels%252C%2520achieving%2520high%2520accuracy%252C%250Awhile%2520for%2520unlabeled%2520human%2520datasets%252C%2520including%2520postmortem%2520brain%2520regions%2520and%250Aurine%2520samples%252C%2520we%2520explored%2520circadian%2520disruptions.%2520Notably%252C%2520our%2520analysis%250Aidentified%2520disruptions%2520in%2520rhythmic%2520proteins%2520between%2520Alzheimer%2527s%2520disease%2520and%250Acontrol%2520subjects%2520across%2520these%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.07405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROTECT%3A%20Protein%20circadian%20time%20prediction%20using%20unsupervised%20learning&entry.906535625=Aram%20Ansary%20Ogholbake%20and%20Qiang%20Cheng&entry.1292438233=%20%20Circadian%20rhythms%20regulate%20the%20physiology%20and%20behavior%20of%20humans%20and%20animals.%0ADespite%20advancements%20in%20understanding%20these%20rhythms%20and%20predicting%20circadian%0Aphases%20at%20the%20transcriptional%20level%2C%20predicting%20circadian%20phases%20from%20proteomic%0Adata%20remains%20elusive.%20This%20challenge%20is%20largely%20due%20to%20the%20scarcity%20of%20time%0Alabels%20in%20proteomic%20datasets%2C%20which%20are%20often%20characterized%20by%20small%20sample%0Asizes%2C%20high%20dimensionality%2C%20and%20significant%20noise.%20Furthermore%2C%20existing%0Amethods%20for%20predicting%20circadian%20phases%20from%20transcriptomic%20data%20typically%20rely%0Aon%20prior%20knowledge%20of%20known%20rhythmic%20genes%2C%20making%20them%20unsuitable%20for%0Aproteomic%20datasets.%20To%20address%20this%20gap%2C%20we%20developed%20a%20novel%20computational%0Amethod%20using%20unsupervised%20deep%20learning%20techniques%20to%20predict%20circadian%20sample%0Aphases%20from%20proteomic%20data%20without%20requiring%20time%20labels%20or%20prior%20knowledge%20of%0Aproteins%20or%20genes.%20Our%20model%20involves%20a%20two-stage%20training%20process%20optimized%0Afor%20robust%20circadian%20phase%20prediction%3A%20an%20initial%20greedy%20one-layer-at-a-time%0Apre-training%20which%20generates%20informative%20initial%20parameters%20followed%20by%0Afine-tuning.%20During%20fine-tuning%2C%20a%20specialized%20loss%20function%20guides%20the%20model%0Ato%20align%20protein%20expression%20levels%20with%20circadian%20patterns%2C%20enabling%20it%20to%0Aaccurately%20capture%20the%20underlying%20rhythmic%20structure%20within%20the%20data.%20We%20tested%0Aour%20method%20on%20both%20time-labeled%20and%20unlabeled%20proteomic%20data.%20For%20labeled%20data%2C%0Awe%20compared%20our%20predictions%20to%20the%20known%20time%20labels%2C%20achieving%20high%20accuracy%2C%0Awhile%20for%20unlabeled%20human%20datasets%2C%20including%20postmortem%20brain%20regions%20and%0Aurine%20samples%2C%20we%20explored%20circadian%20disruptions.%20Notably%2C%20our%20analysis%0Aidentified%20disruptions%20in%20rhythmic%20proteins%20between%20Alzheimer%27s%20disease%20and%0Acontrol%20subjects%20across%20these%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.07405v1&entry.124074799=Read"},
{"title": "Imitating from auxiliary imperfect demonstrations via Adversarial\n  Density Weighted Regression", "author": "Ziqi Zhang and Zifeng Zhuang and Jingzehua Xu and Yiyuan Yang and Yubo Huang and Donglin Wang and Shuai Zhang", "abstract": "  We propose a novel one-step supervised imitation learning (IL) framework\ncalled Adversarial Density Regression (ADR). This IL framework aims to correct\nthe policy learned on unknown-quality to match the expert distribution by\nutilizing demonstrations, without relying on the Bellman operator.\nSpecifically, ADR addresses several limitations in previous IL algorithms:\nFirst, most IL algorithms are based on the Bellman operator, which inevitably\nsuffer from cumulative offsets from sub-optimal rewards during multi-step\nupdate processes. Additionally, off-policy training frameworks suffer from\nOut-of-Distribution (OOD) state-actions. Second, while conservative terms help\nsolve the OOD issue, balancing the conservative term is difficult. To address\nthese limitations, we fully integrate a one-step density-weighted Behavioral\nCloning (BC) objective for IL with auxiliary imperfect demonstration.\nTheoretically, we demonstrate that this adaptation can effectively correct the\ndistribution of policies trained on unknown-quality datasets to align with the\nexpert policy's distribution. Moreover, the difference between the empirical\nand the optimal value function is proportional to the upper bound of ADR's\nobjective, indicating that minimizing ADR's objective is akin to approaching\nthe optimal value. Experimentally, we validated the performance of ADR by\nconducting extensive evaluations. Specifically, ADR outperforms all of the\nselected IL algorithms on tasks from the Gym-Mujoco domain. Meanwhile, it\nachieves an 89.5% improvement over IQL when utilizing ground truth rewards on\ntasks from the Adroit and Kitchen domains. Our codebase will be released at:\nhttps://github.com/stevezhangzA/Adverserial_Density_Regression.\n", "link": "http://arxiv.org/abs/2405.20351v3", "date": "2025-01-13", "relevancy": 1.545, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5158}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imitating%20from%20auxiliary%20imperfect%20demonstrations%20via%20Adversarial%0A%20%20Density%20Weighted%20Regression&body=Title%3A%20Imitating%20from%20auxiliary%20imperfect%20demonstrations%20via%20Adversarial%0A%20%20Density%20Weighted%20Regression%0AAuthor%3A%20Ziqi%20Zhang%20and%20Zifeng%20Zhuang%20and%20Jingzehua%20Xu%20and%20Yiyuan%20Yang%20and%20Yubo%20Huang%20and%20Donglin%20Wang%20and%20Shuai%20Zhang%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20one-step%20supervised%20imitation%20learning%20%28IL%29%20framework%0Acalled%20Adversarial%20Density%20Regression%20%28ADR%29.%20This%20IL%20framework%20aims%20to%20correct%0Athe%20policy%20learned%20on%20unknown-quality%20to%20match%20the%20expert%20distribution%20by%0Autilizing%20demonstrations%2C%20without%20relying%20on%20the%20Bellman%20operator.%0ASpecifically%2C%20ADR%20addresses%20several%20limitations%20in%20previous%20IL%20algorithms%3A%0AFirst%2C%20most%20IL%20algorithms%20are%20based%20on%20the%20Bellman%20operator%2C%20which%20inevitably%0Asuffer%20from%20cumulative%20offsets%20from%20sub-optimal%20rewards%20during%20multi-step%0Aupdate%20processes.%20Additionally%2C%20off-policy%20training%20frameworks%20suffer%20from%0AOut-of-Distribution%20%28OOD%29%20state-actions.%20Second%2C%20while%20conservative%20terms%20help%0Asolve%20the%20OOD%20issue%2C%20balancing%20the%20conservative%20term%20is%20difficult.%20To%20address%0Athese%20limitations%2C%20we%20fully%20integrate%20a%20one-step%20density-weighted%20Behavioral%0ACloning%20%28BC%29%20objective%20for%20IL%20with%20auxiliary%20imperfect%20demonstration.%0ATheoretically%2C%20we%20demonstrate%20that%20this%20adaptation%20can%20effectively%20correct%20the%0Adistribution%20of%20policies%20trained%20on%20unknown-quality%20datasets%20to%20align%20with%20the%0Aexpert%20policy%27s%20distribution.%20Moreover%2C%20the%20difference%20between%20the%20empirical%0Aand%20the%20optimal%20value%20function%20is%20proportional%20to%20the%20upper%20bound%20of%20ADR%27s%0Aobjective%2C%20indicating%20that%20minimizing%20ADR%27s%20objective%20is%20akin%20to%20approaching%0Athe%20optimal%20value.%20Experimentally%2C%20we%20validated%20the%20performance%20of%20ADR%20by%0Aconducting%20extensive%20evaluations.%20Specifically%2C%20ADR%20outperforms%20all%20of%20the%0Aselected%20IL%20algorithms%20on%20tasks%20from%20the%20Gym-Mujoco%20domain.%20Meanwhile%2C%20it%0Aachieves%20an%2089.5%25%20improvement%20over%20IQL%20when%20utilizing%20ground%20truth%20rewards%20on%0Atasks%20from%20the%20Adroit%20and%20Kitchen%20domains.%20Our%20codebase%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/stevezhangzA/Adverserial_Density_Regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20351v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImitating%2520from%2520auxiliary%2520imperfect%2520demonstrations%2520via%2520Adversarial%250A%2520%2520Density%2520Weighted%2520Regression%26entry.906535625%3DZiqi%2520Zhang%2520and%2520Zifeng%2520Zhuang%2520and%2520Jingzehua%2520Xu%2520and%2520Yiyuan%2520Yang%2520and%2520Yubo%2520Huang%2520and%2520Donglin%2520Wang%2520and%2520Shuai%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520one-step%2520supervised%2520imitation%2520learning%2520%2528IL%2529%2520framework%250Acalled%2520Adversarial%2520Density%2520Regression%2520%2528ADR%2529.%2520This%2520IL%2520framework%2520aims%2520to%2520correct%250Athe%2520policy%2520learned%2520on%2520unknown-quality%2520to%2520match%2520the%2520expert%2520distribution%2520by%250Autilizing%2520demonstrations%252C%2520without%2520relying%2520on%2520the%2520Bellman%2520operator.%250ASpecifically%252C%2520ADR%2520addresses%2520several%2520limitations%2520in%2520previous%2520IL%2520algorithms%253A%250AFirst%252C%2520most%2520IL%2520algorithms%2520are%2520based%2520on%2520the%2520Bellman%2520operator%252C%2520which%2520inevitably%250Asuffer%2520from%2520cumulative%2520offsets%2520from%2520sub-optimal%2520rewards%2520during%2520multi-step%250Aupdate%2520processes.%2520Additionally%252C%2520off-policy%2520training%2520frameworks%2520suffer%2520from%250AOut-of-Distribution%2520%2528OOD%2529%2520state-actions.%2520Second%252C%2520while%2520conservative%2520terms%2520help%250Asolve%2520the%2520OOD%2520issue%252C%2520balancing%2520the%2520conservative%2520term%2520is%2520difficult.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520fully%2520integrate%2520a%2520one-step%2520density-weighted%2520Behavioral%250ACloning%2520%2528BC%2529%2520objective%2520for%2520IL%2520with%2520auxiliary%2520imperfect%2520demonstration.%250ATheoretically%252C%2520we%2520demonstrate%2520that%2520this%2520adaptation%2520can%2520effectively%2520correct%2520the%250Adistribution%2520of%2520policies%2520trained%2520on%2520unknown-quality%2520datasets%2520to%2520align%2520with%2520the%250Aexpert%2520policy%2527s%2520distribution.%2520Moreover%252C%2520the%2520difference%2520between%2520the%2520empirical%250Aand%2520the%2520optimal%2520value%2520function%2520is%2520proportional%2520to%2520the%2520upper%2520bound%2520of%2520ADR%2527s%250Aobjective%252C%2520indicating%2520that%2520minimizing%2520ADR%2527s%2520objective%2520is%2520akin%2520to%2520approaching%250Athe%2520optimal%2520value.%2520Experimentally%252C%2520we%2520validated%2520the%2520performance%2520of%2520ADR%2520by%250Aconducting%2520extensive%2520evaluations.%2520Specifically%252C%2520ADR%2520outperforms%2520all%2520of%2520the%250Aselected%2520IL%2520algorithms%2520on%2520tasks%2520from%2520the%2520Gym-Mujoco%2520domain.%2520Meanwhile%252C%2520it%250Aachieves%2520an%252089.5%2525%2520improvement%2520over%2520IQL%2520when%2520utilizing%2520ground%2520truth%2520rewards%2520on%250Atasks%2520from%2520the%2520Adroit%2520and%2520Kitchen%2520domains.%2520Our%2520codebase%2520will%2520be%2520released%2520at%253A%250Ahttps%253A//github.com/stevezhangzA/Adverserial_Density_Regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20351v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imitating%20from%20auxiliary%20imperfect%20demonstrations%20via%20Adversarial%0A%20%20Density%20Weighted%20Regression&entry.906535625=Ziqi%20Zhang%20and%20Zifeng%20Zhuang%20and%20Jingzehua%20Xu%20and%20Yiyuan%20Yang%20and%20Yubo%20Huang%20and%20Donglin%20Wang%20and%20Shuai%20Zhang&entry.1292438233=%20%20We%20propose%20a%20novel%20one-step%20supervised%20imitation%20learning%20%28IL%29%20framework%0Acalled%20Adversarial%20Density%20Regression%20%28ADR%29.%20This%20IL%20framework%20aims%20to%20correct%0Athe%20policy%20learned%20on%20unknown-quality%20to%20match%20the%20expert%20distribution%20by%0Autilizing%20demonstrations%2C%20without%20relying%20on%20the%20Bellman%20operator.%0ASpecifically%2C%20ADR%20addresses%20several%20limitations%20in%20previous%20IL%20algorithms%3A%0AFirst%2C%20most%20IL%20algorithms%20are%20based%20on%20the%20Bellman%20operator%2C%20which%20inevitably%0Asuffer%20from%20cumulative%20offsets%20from%20sub-optimal%20rewards%20during%20multi-step%0Aupdate%20processes.%20Additionally%2C%20off-policy%20training%20frameworks%20suffer%20from%0AOut-of-Distribution%20%28OOD%29%20state-actions.%20Second%2C%20while%20conservative%20terms%20help%0Asolve%20the%20OOD%20issue%2C%20balancing%20the%20conservative%20term%20is%20difficult.%20To%20address%0Athese%20limitations%2C%20we%20fully%20integrate%20a%20one-step%20density-weighted%20Behavioral%0ACloning%20%28BC%29%20objective%20for%20IL%20with%20auxiliary%20imperfect%20demonstration.%0ATheoretically%2C%20we%20demonstrate%20that%20this%20adaptation%20can%20effectively%20correct%20the%0Adistribution%20of%20policies%20trained%20on%20unknown-quality%20datasets%20to%20align%20with%20the%0Aexpert%20policy%27s%20distribution.%20Moreover%2C%20the%20difference%20between%20the%20empirical%0Aand%20the%20optimal%20value%20function%20is%20proportional%20to%20the%20upper%20bound%20of%20ADR%27s%0Aobjective%2C%20indicating%20that%20minimizing%20ADR%27s%20objective%20is%20akin%20to%20approaching%0Athe%20optimal%20value.%20Experimentally%2C%20we%20validated%20the%20performance%20of%20ADR%20by%0Aconducting%20extensive%20evaluations.%20Specifically%2C%20ADR%20outperforms%20all%20of%20the%0Aselected%20IL%20algorithms%20on%20tasks%20from%20the%20Gym-Mujoco%20domain.%20Meanwhile%2C%20it%0Aachieves%20an%2089.5%25%20improvement%20over%20IQL%20when%20utilizing%20ground%20truth%20rewards%20on%0Atasks%20from%20the%20Adroit%20and%20Kitchen%20domains.%20Our%20codebase%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/stevezhangzA/Adverserial_Density_Regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20351v3&entry.124074799=Read"},
{"title": "Mitigating Out-of-Entity Errors in Named Entity Recognition: A\n  Sentence-Level Strategy", "author": "Guochao Jiang and Ziqin Luo and Chengwei Hu and Zepeng Ding and Deqing Yang", "abstract": "  Many previous models of named entity recognition (NER) suffer from the\nproblem of Out-of-Entity (OOE), i.e., the tokens in the entity mentions of the\ntest samples have not appeared in the training samples, which hinders the\nachievement of satisfactory performance. To improve OOE-NER performance, in\nthis paper, we propose a new framework, namely S+NER, which fully leverages\nsentence-level information. Our S+NER achieves better OOE-NER performance\nmainly due to the following two particular designs. 1) It first exploits the\npre-trained language model's capability of understanding the target entity's\nsentence-level context with a template set. 2) Then, it refines the\nsentence-level representation based on the positive and negative templates,\nthrough a contrastive learning strategy and template pooling method, to obtain\nbetter NER results. Our extensive experiments on five benchmark datasets have\ndemonstrated that, our S+NER outperforms some state-of-the-art OOE-NER models.\n", "link": "http://arxiv.org/abs/2412.08434v2", "date": "2025-01-13", "relevancy": 1.8445, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4667}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Out-of-Entity%20Errors%20in%20Named%20Entity%20Recognition%3A%20A%0A%20%20Sentence-Level%20Strategy&body=Title%3A%20Mitigating%20Out-of-Entity%20Errors%20in%20Named%20Entity%20Recognition%3A%20A%0A%20%20Sentence-Level%20Strategy%0AAuthor%3A%20Guochao%20Jiang%20and%20Ziqin%20Luo%20and%20Chengwei%20Hu%20and%20Zepeng%20Ding%20and%20Deqing%20Yang%0AAbstract%3A%20%20%20Many%20previous%20models%20of%20named%20entity%20recognition%20%28NER%29%20suffer%20from%20the%0Aproblem%20of%20Out-of-Entity%20%28OOE%29%2C%20i.e.%2C%20the%20tokens%20in%20the%20entity%20mentions%20of%20the%0Atest%20samples%20have%20not%20appeared%20in%20the%20training%20samples%2C%20which%20hinders%20the%0Aachievement%20of%20satisfactory%20performance.%20To%20improve%20OOE-NER%20performance%2C%20in%0Athis%20paper%2C%20we%20propose%20a%20new%20framework%2C%20namely%20S%2BNER%2C%20which%20fully%20leverages%0Asentence-level%20information.%20Our%20S%2BNER%20achieves%20better%20OOE-NER%20performance%0Amainly%20due%20to%20the%20following%20two%20particular%20designs.%201%29%20It%20first%20exploits%20the%0Apre-trained%20language%20model%27s%20capability%20of%20understanding%20the%20target%20entity%27s%0Asentence-level%20context%20with%20a%20template%20set.%202%29%20Then%2C%20it%20refines%20the%0Asentence-level%20representation%20based%20on%20the%20positive%20and%20negative%20templates%2C%0Athrough%20a%20contrastive%20learning%20strategy%20and%20template%20pooling%20method%2C%20to%20obtain%0Abetter%20NER%20results.%20Our%20extensive%20experiments%20on%20five%20benchmark%20datasets%20have%0Ademonstrated%20that%2C%20our%20S%2BNER%20outperforms%20some%20state-of-the-art%20OOE-NER%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08434v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Out-of-Entity%2520Errors%2520in%2520Named%2520Entity%2520Recognition%253A%2520A%250A%2520%2520Sentence-Level%2520Strategy%26entry.906535625%3DGuochao%2520Jiang%2520and%2520Ziqin%2520Luo%2520and%2520Chengwei%2520Hu%2520and%2520Zepeng%2520Ding%2520and%2520Deqing%2520Yang%26entry.1292438233%3D%2520%2520Many%2520previous%2520models%2520of%2520named%2520entity%2520recognition%2520%2528NER%2529%2520suffer%2520from%2520the%250Aproblem%2520of%2520Out-of-Entity%2520%2528OOE%2529%252C%2520i.e.%252C%2520the%2520tokens%2520in%2520the%2520entity%2520mentions%2520of%2520the%250Atest%2520samples%2520have%2520not%2520appeared%2520in%2520the%2520training%2520samples%252C%2520which%2520hinders%2520the%250Aachievement%2520of%2520satisfactory%2520performance.%2520To%2520improve%2520OOE-NER%2520performance%252C%2520in%250Athis%2520paper%252C%2520we%2520propose%2520a%2520new%2520framework%252C%2520namely%2520S%252BNER%252C%2520which%2520fully%2520leverages%250Asentence-level%2520information.%2520Our%2520S%252BNER%2520achieves%2520better%2520OOE-NER%2520performance%250Amainly%2520due%2520to%2520the%2520following%2520two%2520particular%2520designs.%25201%2529%2520It%2520first%2520exploits%2520the%250Apre-trained%2520language%2520model%2527s%2520capability%2520of%2520understanding%2520the%2520target%2520entity%2527s%250Asentence-level%2520context%2520with%2520a%2520template%2520set.%25202%2529%2520Then%252C%2520it%2520refines%2520the%250Asentence-level%2520representation%2520based%2520on%2520the%2520positive%2520and%2520negative%2520templates%252C%250Athrough%2520a%2520contrastive%2520learning%2520strategy%2520and%2520template%2520pooling%2520method%252C%2520to%2520obtain%250Abetter%2520NER%2520results.%2520Our%2520extensive%2520experiments%2520on%2520five%2520benchmark%2520datasets%2520have%250Ademonstrated%2520that%252C%2520our%2520S%252BNER%2520outperforms%2520some%2520state-of-the-art%2520OOE-NER%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08434v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Out-of-Entity%20Errors%20in%20Named%20Entity%20Recognition%3A%20A%0A%20%20Sentence-Level%20Strategy&entry.906535625=Guochao%20Jiang%20and%20Ziqin%20Luo%20and%20Chengwei%20Hu%20and%20Zepeng%20Ding%20and%20Deqing%20Yang&entry.1292438233=%20%20Many%20previous%20models%20of%20named%20entity%20recognition%20%28NER%29%20suffer%20from%20the%0Aproblem%20of%20Out-of-Entity%20%28OOE%29%2C%20i.e.%2C%20the%20tokens%20in%20the%20entity%20mentions%20of%20the%0Atest%20samples%20have%20not%20appeared%20in%20the%20training%20samples%2C%20which%20hinders%20the%0Aachievement%20of%20satisfactory%20performance.%20To%20improve%20OOE-NER%20performance%2C%20in%0Athis%20paper%2C%20we%20propose%20a%20new%20framework%2C%20namely%20S%2BNER%2C%20which%20fully%20leverages%0Asentence-level%20information.%20Our%20S%2BNER%20achieves%20better%20OOE-NER%20performance%0Amainly%20due%20to%20the%20following%20two%20particular%20designs.%201%29%20It%20first%20exploits%20the%0Apre-trained%20language%20model%27s%20capability%20of%20understanding%20the%20target%20entity%27s%0Asentence-level%20context%20with%20a%20template%20set.%202%29%20Then%2C%20it%20refines%20the%0Asentence-level%20representation%20based%20on%20the%20positive%20and%20negative%20templates%2C%0Athrough%20a%20contrastive%20learning%20strategy%20and%20template%20pooling%20method%2C%20to%20obtain%0Abetter%20NER%20results.%20Our%20extensive%20experiments%20on%20five%20benchmark%20datasets%20have%0Ademonstrated%20that%2C%20our%20S%2BNER%20outperforms%20some%20state-of-the-art%20OOE-NER%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08434v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


