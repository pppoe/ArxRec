<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250928.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild", "author": "Deming Li and Kaiwen Jiang and Yutao Tang and Ravi Ramamoorthi and Rama Chellappa and Cheng Peng", "abstract": "  In-the-wild photo collections often contain limited volumes of imagery and\nexhibit multiple appearances, e.g., taken at different times of day or seasons,\nposing significant challenges to scene reconstruction and novel view synthesis.\nAlthough recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian\nSplatting (3DGS) have improved in these areas, they tend to oversmooth and are\nprone to overfitting. In this paper, we present MS-GS, a novel framework\ndesigned with Multi-appearance capabilities in Sparse-view scenarios using\n3DGS. To address the lack of support due to sparse initializations, our\napproach is built on the geometric priors elicited from monocular depth\nestimations. The key lies in extracting and utilizing local semantic regions\nwith a Structure-from-Motion (SfM) points anchored algorithm for reliable\nalignment and geometry cues. Then, to introduce multi-view constraints, we\npropose a series of geometry-guided supervision at virtual views in a\nfine-grained and coarse scheme to encourage 3D consistency and reduce\noverfitting. We also introduce a dataset and an in-the-wild experiment setting\nto set up more realistic benchmarks. We demonstrate that MS-GS achieves\nphotorealistic renderings under various challenging sparse-view and\nmulti-appearance conditions and outperforms existing approaches significantly\nacross different datasets.\n", "link": "http://arxiv.org/abs/2509.15548v3", "date": "2025-09-26", "relevancy": 3.5118, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7209}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7133}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-GS%3A%20Multi-Appearance%20Sparse-View%203D%20Gaussian%20Splatting%20in%20the%20Wild&body=Title%3A%20MS-GS%3A%20Multi-Appearance%20Sparse-View%203D%20Gaussian%20Splatting%20in%20the%20Wild%0AAuthor%3A%20Deming%20Li%20and%20Kaiwen%20Jiang%20and%20Yutao%20Tang%20and%20Ravi%20Ramamoorthi%20and%20Rama%20Chellappa%20and%20Cheng%20Peng%0AAbstract%3A%20%20%20In-the-wild%20photo%20collections%20often%20contain%20limited%20volumes%20of%20imagery%20and%0Aexhibit%20multiple%20appearances%2C%20e.g.%2C%20taken%20at%20different%20times%20of%20day%20or%20seasons%2C%0Aposing%20significant%20challenges%20to%20scene%20reconstruction%20and%20novel%20view%20synthesis.%0AAlthough%20recent%20adaptations%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20and%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20improved%20in%20these%20areas%2C%20they%20tend%20to%20oversmooth%20and%20are%0Aprone%20to%20overfitting.%20In%20this%20paper%2C%20we%20present%20MS-GS%2C%20a%20novel%20framework%0Adesigned%20with%20Multi-appearance%20capabilities%20in%20Sparse-view%20scenarios%20using%0A3DGS.%20To%20address%20the%20lack%20of%20support%20due%20to%20sparse%20initializations%2C%20our%0Aapproach%20is%20built%20on%20the%20geometric%20priors%20elicited%20from%20monocular%20depth%0Aestimations.%20The%20key%20lies%20in%20extracting%20and%20utilizing%20local%20semantic%20regions%0Awith%20a%20Structure-from-Motion%20%28SfM%29%20points%20anchored%20algorithm%20for%20reliable%0Aalignment%20and%20geometry%20cues.%20Then%2C%20to%20introduce%20multi-view%20constraints%2C%20we%0Apropose%20a%20series%20of%20geometry-guided%20supervision%20at%20virtual%20views%20in%20a%0Afine-grained%20and%20coarse%20scheme%20to%20encourage%203D%20consistency%20and%20reduce%0Aoverfitting.%20We%20also%20introduce%20a%20dataset%20and%20an%20in-the-wild%20experiment%20setting%0Ato%20set%20up%20more%20realistic%20benchmarks.%20We%20demonstrate%20that%20MS-GS%20achieves%0Aphotorealistic%20renderings%20under%20various%20challenging%20sparse-view%20and%0Amulti-appearance%20conditions%20and%20outperforms%20existing%20approaches%20significantly%0Aacross%20different%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.15548v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-GS%253A%2520Multi-Appearance%2520Sparse-View%25203D%2520Gaussian%2520Splatting%2520in%2520the%2520Wild%26entry.906535625%3DDeming%2520Li%2520and%2520Kaiwen%2520Jiang%2520and%2520Yutao%2520Tang%2520and%2520Ravi%2520Ramamoorthi%2520and%2520Rama%2520Chellappa%2520and%2520Cheng%2520Peng%26entry.1292438233%3D%2520%2520In-the-wild%2520photo%2520collections%2520often%2520contain%2520limited%2520volumes%2520of%2520imagery%2520and%250Aexhibit%2520multiple%2520appearances%252C%2520e.g.%252C%2520taken%2520at%2520different%2520times%2520of%2520day%2520or%2520seasons%252C%250Aposing%2520significant%2520challenges%2520to%2520scene%2520reconstruction%2520and%2520novel%2520view%2520synthesis.%250AAlthough%2520recent%2520adaptations%2520of%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520and%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520have%2520improved%2520in%2520these%2520areas%252C%2520they%2520tend%2520to%2520oversmooth%2520and%2520are%250Aprone%2520to%2520overfitting.%2520In%2520this%2520paper%252C%2520we%2520present%2520MS-GS%252C%2520a%2520novel%2520framework%250Adesigned%2520with%2520Multi-appearance%2520capabilities%2520in%2520Sparse-view%2520scenarios%2520using%250A3DGS.%2520To%2520address%2520the%2520lack%2520of%2520support%2520due%2520to%2520sparse%2520initializations%252C%2520our%250Aapproach%2520is%2520built%2520on%2520the%2520geometric%2520priors%2520elicited%2520from%2520monocular%2520depth%250Aestimations.%2520The%2520key%2520lies%2520in%2520extracting%2520and%2520utilizing%2520local%2520semantic%2520regions%250Awith%2520a%2520Structure-from-Motion%2520%2528SfM%2529%2520points%2520anchored%2520algorithm%2520for%2520reliable%250Aalignment%2520and%2520geometry%2520cues.%2520Then%252C%2520to%2520introduce%2520multi-view%2520constraints%252C%2520we%250Apropose%2520a%2520series%2520of%2520geometry-guided%2520supervision%2520at%2520virtual%2520views%2520in%2520a%250Afine-grained%2520and%2520coarse%2520scheme%2520to%2520encourage%25203D%2520consistency%2520and%2520reduce%250Aoverfitting.%2520We%2520also%2520introduce%2520a%2520dataset%2520and%2520an%2520in-the-wild%2520experiment%2520setting%250Ato%2520set%2520up%2520more%2520realistic%2520benchmarks.%2520We%2520demonstrate%2520that%2520MS-GS%2520achieves%250Aphotorealistic%2520renderings%2520under%2520various%2520challenging%2520sparse-view%2520and%250Amulti-appearance%2520conditions%2520and%2520outperforms%2520existing%2520approaches%2520significantly%250Aacross%2520different%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.15548v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-GS%3A%20Multi-Appearance%20Sparse-View%203D%20Gaussian%20Splatting%20in%20the%20Wild&entry.906535625=Deming%20Li%20and%20Kaiwen%20Jiang%20and%20Yutao%20Tang%20and%20Ravi%20Ramamoorthi%20and%20Rama%20Chellappa%20and%20Cheng%20Peng&entry.1292438233=%20%20In-the-wild%20photo%20collections%20often%20contain%20limited%20volumes%20of%20imagery%20and%0Aexhibit%20multiple%20appearances%2C%20e.g.%2C%20taken%20at%20different%20times%20of%20day%20or%20seasons%2C%0Aposing%20significant%20challenges%20to%20scene%20reconstruction%20and%20novel%20view%20synthesis.%0AAlthough%20recent%20adaptations%20of%20Neural%20Radiance%20Field%20%28NeRF%29%20and%203D%20Gaussian%0ASplatting%20%283DGS%29%20have%20improved%20in%20these%20areas%2C%20they%20tend%20to%20oversmooth%20and%20are%0Aprone%20to%20overfitting.%20In%20this%20paper%2C%20we%20present%20MS-GS%2C%20a%20novel%20framework%0Adesigned%20with%20Multi-appearance%20capabilities%20in%20Sparse-view%20scenarios%20using%0A3DGS.%20To%20address%20the%20lack%20of%20support%20due%20to%20sparse%20initializations%2C%20our%0Aapproach%20is%20built%20on%20the%20geometric%20priors%20elicited%20from%20monocular%20depth%0Aestimations.%20The%20key%20lies%20in%20extracting%20and%20utilizing%20local%20semantic%20regions%0Awith%20a%20Structure-from-Motion%20%28SfM%29%20points%20anchored%20algorithm%20for%20reliable%0Aalignment%20and%20geometry%20cues.%20Then%2C%20to%20introduce%20multi-view%20constraints%2C%20we%0Apropose%20a%20series%20of%20geometry-guided%20supervision%20at%20virtual%20views%20in%20a%0Afine-grained%20and%20coarse%20scheme%20to%20encourage%203D%20consistency%20and%20reduce%0Aoverfitting.%20We%20also%20introduce%20a%20dataset%20and%20an%20in-the-wild%20experiment%20setting%0Ato%20set%20up%20more%20realistic%20benchmarks.%20We%20demonstrate%20that%20MS-GS%20achieves%0Aphotorealistic%20renderings%20under%20various%20challenging%20sparse-view%20and%0Amulti-appearance%20conditions%20and%20outperforms%20existing%20approaches%20significantly%0Aacross%20different%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.15548v3&entry.124074799=Read"},
{"title": "GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material\n  Decomposition", "author": "Dinh Minh Nguyen and Malte Avenhaus and Thomas Lindemeier", "abstract": "  We propose a unified solution for mesh reconstruction and material\ndecomposition from multi-view images based on 3D Gaussian Splatting, referred\nto as GS-2M. Previous works handle these tasks separately and struggle to\nreconstruct highly reflective surfaces, often relying on priors from external\nmodels to enhance the decomposition results. Conversely, our method addresses\nthese two problems by jointly optimizing attributes relevant to the quality of\nrendered depth and normals, maintaining geometric details while being resilient\nto reflective surfaces. Although contemporary works effectively solve these\ntasks together, they often employ sophisticated neural components to learn\nscene properties, which hinders their performance at scale. To further\neliminate these neural components, we propose a novel roughness supervision\nstrategy based on multi-view photometric variation. When combined with a\ncarefully designed loss and optimization process, our unified framework\nproduces reconstruction results comparable to state-of-the-art methods,\ndelivering triangle meshes and their associated material components for\ndownstream tasks. We validate the effectiveness of our approach with widely\nused datasets from previous works and qualitative comparisons with\nstate-of-the-art surface reconstruction methods.\n", "link": "http://arxiv.org/abs/2509.22276v1", "date": "2025-09-26", "relevancy": 3.4841, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7036}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6981}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-2M%3A%20Gaussian%20Splatting%20for%20Joint%20Mesh%20Reconstruction%20and%20Material%0A%20%20Decomposition&body=Title%3A%20GS-2M%3A%20Gaussian%20Splatting%20for%20Joint%20Mesh%20Reconstruction%20and%20Material%0A%20%20Decomposition%0AAuthor%3A%20Dinh%20Minh%20Nguyen%20and%20Malte%20Avenhaus%20and%20Thomas%20Lindemeier%0AAbstract%3A%20%20%20We%20propose%20a%20unified%20solution%20for%20mesh%20reconstruction%20and%20material%0Adecomposition%20from%20multi-view%20images%20based%20on%203D%20Gaussian%20Splatting%2C%20referred%0Ato%20as%20GS-2M.%20Previous%20works%20handle%20these%20tasks%20separately%20and%20struggle%20to%0Areconstruct%20highly%20reflective%20surfaces%2C%20often%20relying%20on%20priors%20from%20external%0Amodels%20to%20enhance%20the%20decomposition%20results.%20Conversely%2C%20our%20method%20addresses%0Athese%20two%20problems%20by%20jointly%20optimizing%20attributes%20relevant%20to%20the%20quality%20of%0Arendered%20depth%20and%20normals%2C%20maintaining%20geometric%20details%20while%20being%20resilient%0Ato%20reflective%20surfaces.%20Although%20contemporary%20works%20effectively%20solve%20these%0Atasks%20together%2C%20they%20often%20employ%20sophisticated%20neural%20components%20to%20learn%0Ascene%20properties%2C%20which%20hinders%20their%20performance%20at%20scale.%20To%20further%0Aeliminate%20these%20neural%20components%2C%20we%20propose%20a%20novel%20roughness%20supervision%0Astrategy%20based%20on%20multi-view%20photometric%20variation.%20When%20combined%20with%20a%0Acarefully%20designed%20loss%20and%20optimization%20process%2C%20our%20unified%20framework%0Aproduces%20reconstruction%20results%20comparable%20to%20state-of-the-art%20methods%2C%0Adelivering%20triangle%20meshes%20and%20their%20associated%20material%20components%20for%0Adownstream%20tasks.%20We%20validate%20the%20effectiveness%20of%20our%20approach%20with%20widely%0Aused%20datasets%20from%20previous%20works%20and%20qualitative%20comparisons%20with%0Astate-of-the-art%20surface%20reconstruction%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-2M%253A%2520Gaussian%2520Splatting%2520for%2520Joint%2520Mesh%2520Reconstruction%2520and%2520Material%250A%2520%2520Decomposition%26entry.906535625%3DDinh%2520Minh%2520Nguyen%2520and%2520Malte%2520Avenhaus%2520and%2520Thomas%2520Lindemeier%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520unified%2520solution%2520for%2520mesh%2520reconstruction%2520and%2520material%250Adecomposition%2520from%2520multi-view%2520images%2520based%2520on%25203D%2520Gaussian%2520Splatting%252C%2520referred%250Ato%2520as%2520GS-2M.%2520Previous%2520works%2520handle%2520these%2520tasks%2520separately%2520and%2520struggle%2520to%250Areconstruct%2520highly%2520reflective%2520surfaces%252C%2520often%2520relying%2520on%2520priors%2520from%2520external%250Amodels%2520to%2520enhance%2520the%2520decomposition%2520results.%2520Conversely%252C%2520our%2520method%2520addresses%250Athese%2520two%2520problems%2520by%2520jointly%2520optimizing%2520attributes%2520relevant%2520to%2520the%2520quality%2520of%250Arendered%2520depth%2520and%2520normals%252C%2520maintaining%2520geometric%2520details%2520while%2520being%2520resilient%250Ato%2520reflective%2520surfaces.%2520Although%2520contemporary%2520works%2520effectively%2520solve%2520these%250Atasks%2520together%252C%2520they%2520often%2520employ%2520sophisticated%2520neural%2520components%2520to%2520learn%250Ascene%2520properties%252C%2520which%2520hinders%2520their%2520performance%2520at%2520scale.%2520To%2520further%250Aeliminate%2520these%2520neural%2520components%252C%2520we%2520propose%2520a%2520novel%2520roughness%2520supervision%250Astrategy%2520based%2520on%2520multi-view%2520photometric%2520variation.%2520When%2520combined%2520with%2520a%250Acarefully%2520designed%2520loss%2520and%2520optimization%2520process%252C%2520our%2520unified%2520framework%250Aproduces%2520reconstruction%2520results%2520comparable%2520to%2520state-of-the-art%2520methods%252C%250Adelivering%2520triangle%2520meshes%2520and%2520their%2520associated%2520material%2520components%2520for%250Adownstream%2520tasks.%2520We%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520with%2520widely%250Aused%2520datasets%2520from%2520previous%2520works%2520and%2520qualitative%2520comparisons%2520with%250Astate-of-the-art%2520surface%2520reconstruction%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-2M%3A%20Gaussian%20Splatting%20for%20Joint%20Mesh%20Reconstruction%20and%20Material%0A%20%20Decomposition&entry.906535625=Dinh%20Minh%20Nguyen%20and%20Malte%20Avenhaus%20and%20Thomas%20Lindemeier&entry.1292438233=%20%20We%20propose%20a%20unified%20solution%20for%20mesh%20reconstruction%20and%20material%0Adecomposition%20from%20multi-view%20images%20based%20on%203D%20Gaussian%20Splatting%2C%20referred%0Ato%20as%20GS-2M.%20Previous%20works%20handle%20these%20tasks%20separately%20and%20struggle%20to%0Areconstruct%20highly%20reflective%20surfaces%2C%20often%20relying%20on%20priors%20from%20external%0Amodels%20to%20enhance%20the%20decomposition%20results.%20Conversely%2C%20our%20method%20addresses%0Athese%20two%20problems%20by%20jointly%20optimizing%20attributes%20relevant%20to%20the%20quality%20of%0Arendered%20depth%20and%20normals%2C%20maintaining%20geometric%20details%20while%20being%20resilient%0Ato%20reflective%20surfaces.%20Although%20contemporary%20works%20effectively%20solve%20these%0Atasks%20together%2C%20they%20often%20employ%20sophisticated%20neural%20components%20to%20learn%0Ascene%20properties%2C%20which%20hinders%20their%20performance%20at%20scale.%20To%20further%0Aeliminate%20these%20neural%20components%2C%20we%20propose%20a%20novel%20roughness%20supervision%0Astrategy%20based%20on%20multi-view%20photometric%20variation.%20When%20combined%20with%20a%0Acarefully%20designed%20loss%20and%20optimization%20process%2C%20our%20unified%20framework%0Aproduces%20reconstruction%20results%20comparable%20to%20state-of-the-art%20methods%2C%0Adelivering%20triangle%20meshes%20and%20their%20associated%20material%20components%20for%0Adownstream%20tasks.%20We%20validate%20the%20effectiveness%20of%20our%20approach%20with%20widely%0Aused%20datasets%20from%20previous%20works%20and%20qualitative%20comparisons%20with%0Astate-of-the-art%20surface%20reconstruction%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22276v1&entry.124074799=Read"},
{"title": "Polysemous Language Gaussian Splatting via Matching-based Mask Lifting", "author": "Jiayu Ding and Xinpeng Liu and Zhiyi Pan and Shiqiang Long and Ge Li", "abstract": "  Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS)\nscenes is a critical challenge. However, mainstream methods suffer from three\nkey flaws: (i) their reliance on costly per-scene retraining prevents\nplug-and-play application; (ii) their restrictive monosemous design fails to\nrepresent complex, multi-concept semantics; and (iii) their vulnerability to\ncross-view semantic inconsistencies corrupts the final semantic representation.\nTo overcome these limitations, we introduce MUSplat, a training-free framework\nthat abandons feature optimization entirely. Leveraging a pre-trained 2D\nsegmentation model, our pipeline generates and lifts multi-granularity 2D masks\ninto 3D, where we estimate a foreground probability for each Gaussian point to\nform initial object groups. We then optimize the ambiguous boundaries of these\ninitial groups using semantic entropy and geometric opacity. Subsequently, by\ninterpreting the object's appearance across its most representative viewpoints,\na Vision-Language Model (VLM) distills robust textual features that reconciles\nvisual inconsistencies, enabling open-vocabulary querying via semantic\nmatching. By eliminating the costly per-scene training process, MUSplat reduces\nscene adaptation time from hours to mere minutes. On benchmark tasks for\nopen-vocabulary 3D object selection and semantic segmentation, MUSplat\noutperforms established training-based frameworks while simultaneously\naddressing their monosemous limitations.\n", "link": "http://arxiv.org/abs/2509.22225v1", "date": "2025-09-26", "relevancy": 3.308, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6926}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6595}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polysemous%20Language%20Gaussian%20Splatting%20via%20Matching-based%20Mask%20Lifting&body=Title%3A%20Polysemous%20Language%20Gaussian%20Splatting%20via%20Matching-based%20Mask%20Lifting%0AAuthor%3A%20Jiayu%20Ding%20and%20Xinpeng%20Liu%20and%20Zhiyi%20Pan%20and%20Shiqiang%20Long%20and%20Ge%20Li%0AAbstract%3A%20%20%20Lifting%202D%20open-vocabulary%20understanding%20into%203D%20Gaussian%20Splatting%20%283DGS%29%0Ascenes%20is%20a%20critical%20challenge.%20However%2C%20mainstream%20methods%20suffer%20from%20three%0Akey%20flaws%3A%20%28i%29%20their%20reliance%20on%20costly%20per-scene%20retraining%20prevents%0Aplug-and-play%20application%3B%20%28ii%29%20their%20restrictive%20monosemous%20design%20fails%20to%0Arepresent%20complex%2C%20multi-concept%20semantics%3B%20and%20%28iii%29%20their%20vulnerability%20to%0Across-view%20semantic%20inconsistencies%20corrupts%20the%20final%20semantic%20representation.%0ATo%20overcome%20these%20limitations%2C%20we%20introduce%20MUSplat%2C%20a%20training-free%20framework%0Athat%20abandons%20feature%20optimization%20entirely.%20Leveraging%20a%20pre-trained%202D%0Asegmentation%20model%2C%20our%20pipeline%20generates%20and%20lifts%20multi-granularity%202D%20masks%0Ainto%203D%2C%20where%20we%20estimate%20a%20foreground%20probability%20for%20each%20Gaussian%20point%20to%0Aform%20initial%20object%20groups.%20We%20then%20optimize%20the%20ambiguous%20boundaries%20of%20these%0Ainitial%20groups%20using%20semantic%20entropy%20and%20geometric%20opacity.%20Subsequently%2C%20by%0Ainterpreting%20the%20object%27s%20appearance%20across%20its%20most%20representative%20viewpoints%2C%0Aa%20Vision-Language%20Model%20%28VLM%29%20distills%20robust%20textual%20features%20that%20reconciles%0Avisual%20inconsistencies%2C%20enabling%20open-vocabulary%20querying%20via%20semantic%0Amatching.%20By%20eliminating%20the%20costly%20per-scene%20training%20process%2C%20MUSplat%20reduces%0Ascene%20adaptation%20time%20from%20hours%20to%20mere%20minutes.%20On%20benchmark%20tasks%20for%0Aopen-vocabulary%203D%20object%20selection%20and%20semantic%20segmentation%2C%20MUSplat%0Aoutperforms%20established%20training-based%20frameworks%20while%20simultaneously%0Aaddressing%20their%20monosemous%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolysemous%2520Language%2520Gaussian%2520Splatting%2520via%2520Matching-based%2520Mask%2520Lifting%26entry.906535625%3DJiayu%2520Ding%2520and%2520Xinpeng%2520Liu%2520and%2520Zhiyi%2520Pan%2520and%2520Shiqiang%2520Long%2520and%2520Ge%2520Li%26entry.1292438233%3D%2520%2520Lifting%25202D%2520open-vocabulary%2520understanding%2520into%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Ascenes%2520is%2520a%2520critical%2520challenge.%2520However%252C%2520mainstream%2520methods%2520suffer%2520from%2520three%250Akey%2520flaws%253A%2520%2528i%2529%2520their%2520reliance%2520on%2520costly%2520per-scene%2520retraining%2520prevents%250Aplug-and-play%2520application%253B%2520%2528ii%2529%2520their%2520restrictive%2520monosemous%2520design%2520fails%2520to%250Arepresent%2520complex%252C%2520multi-concept%2520semantics%253B%2520and%2520%2528iii%2529%2520their%2520vulnerability%2520to%250Across-view%2520semantic%2520inconsistencies%2520corrupts%2520the%2520final%2520semantic%2520representation.%250ATo%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520MUSplat%252C%2520a%2520training-free%2520framework%250Athat%2520abandons%2520feature%2520optimization%2520entirely.%2520Leveraging%2520a%2520pre-trained%25202D%250Asegmentation%2520model%252C%2520our%2520pipeline%2520generates%2520and%2520lifts%2520multi-granularity%25202D%2520masks%250Ainto%25203D%252C%2520where%2520we%2520estimate%2520a%2520foreground%2520probability%2520for%2520each%2520Gaussian%2520point%2520to%250Aform%2520initial%2520object%2520groups.%2520We%2520then%2520optimize%2520the%2520ambiguous%2520boundaries%2520of%2520these%250Ainitial%2520groups%2520using%2520semantic%2520entropy%2520and%2520geometric%2520opacity.%2520Subsequently%252C%2520by%250Ainterpreting%2520the%2520object%2527s%2520appearance%2520across%2520its%2520most%2520representative%2520viewpoints%252C%250Aa%2520Vision-Language%2520Model%2520%2528VLM%2529%2520distills%2520robust%2520textual%2520features%2520that%2520reconciles%250Avisual%2520inconsistencies%252C%2520enabling%2520open-vocabulary%2520querying%2520via%2520semantic%250Amatching.%2520By%2520eliminating%2520the%2520costly%2520per-scene%2520training%2520process%252C%2520MUSplat%2520reduces%250Ascene%2520adaptation%2520time%2520from%2520hours%2520to%2520mere%2520minutes.%2520On%2520benchmark%2520tasks%2520for%250Aopen-vocabulary%25203D%2520object%2520selection%2520and%2520semantic%2520segmentation%252C%2520MUSplat%250Aoutperforms%2520established%2520training-based%2520frameworks%2520while%2520simultaneously%250Aaddressing%2520their%2520monosemous%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polysemous%20Language%20Gaussian%20Splatting%20via%20Matching-based%20Mask%20Lifting&entry.906535625=Jiayu%20Ding%20and%20Xinpeng%20Liu%20and%20Zhiyi%20Pan%20and%20Shiqiang%20Long%20and%20Ge%20Li&entry.1292438233=%20%20Lifting%202D%20open-vocabulary%20understanding%20into%203D%20Gaussian%20Splatting%20%283DGS%29%0Ascenes%20is%20a%20critical%20challenge.%20However%2C%20mainstream%20methods%20suffer%20from%20three%0Akey%20flaws%3A%20%28i%29%20their%20reliance%20on%20costly%20per-scene%20retraining%20prevents%0Aplug-and-play%20application%3B%20%28ii%29%20their%20restrictive%20monosemous%20design%20fails%20to%0Arepresent%20complex%2C%20multi-concept%20semantics%3B%20and%20%28iii%29%20their%20vulnerability%20to%0Across-view%20semantic%20inconsistencies%20corrupts%20the%20final%20semantic%20representation.%0ATo%20overcome%20these%20limitations%2C%20we%20introduce%20MUSplat%2C%20a%20training-free%20framework%0Athat%20abandons%20feature%20optimization%20entirely.%20Leveraging%20a%20pre-trained%202D%0Asegmentation%20model%2C%20our%20pipeline%20generates%20and%20lifts%20multi-granularity%202D%20masks%0Ainto%203D%2C%20where%20we%20estimate%20a%20foreground%20probability%20for%20each%20Gaussian%20point%20to%0Aform%20initial%20object%20groups.%20We%20then%20optimize%20the%20ambiguous%20boundaries%20of%20these%0Ainitial%20groups%20using%20semantic%20entropy%20and%20geometric%20opacity.%20Subsequently%2C%20by%0Ainterpreting%20the%20object%27s%20appearance%20across%20its%20most%20representative%20viewpoints%2C%0Aa%20Vision-Language%20Model%20%28VLM%29%20distills%20robust%20textual%20features%20that%20reconciles%0Avisual%20inconsistencies%2C%20enabling%20open-vocabulary%20querying%20via%20semantic%0Amatching.%20By%20eliminating%20the%20costly%20per-scene%20training%20process%2C%20MUSplat%20reduces%0Ascene%20adaptation%20time%20from%20hours%20to%20mere%20minutes.%20On%20benchmark%20tasks%20for%0Aopen-vocabulary%203D%20object%20selection%20and%20semantic%20segmentation%2C%20MUSplat%0Aoutperforms%20established%20training-based%20frameworks%20while%20simultaneously%0Aaddressing%20their%20monosemous%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22225v1&entry.124074799=Read"},
{"title": "Semantic Consistent Language Gaussian Splatting for Point-Level\n  Open-vocabulary Querying", "author": "Hairong Yin and Huangying Zhan and Yi Xu and Raymond A. Yeh", "abstract": "  Open-vocabulary 3D scene understanding is crucial for robotics applications,\nsuch as natural language-driven manipulation, human-robot interaction, and\nautonomous navigation. Existing methods for querying 3D Gaussian Splatting\noften struggle with inconsistent 2D mask supervision and lack a robust 3D\npoint-level retrieval mechanism. In this work, (i) we present a novel\npoint-level querying framework that performs tracking on segmentation masks to\nestablish a semantically consistent ground-truth for distilling the language\nGaussians; (ii) we introduce a GT-anchored querying approach that first\nretrieves the distilled ground-truth and subsequently uses the ground-truth to\nquery the individual Gaussians. Extensive experiments on three benchmark\ndatasets demonstrate that the proposed method outperforms state-of-the-art\nperformance. Our method achieves an mIoU improvement of +4.14, +20.42, and +1.7\non the LERF, 3D-OVS, and Replica datasets. These results validate our framework\nas a promising step toward open-vocabulary understanding in real-world robotic\nsystems.\n", "link": "http://arxiv.org/abs/2503.21767v2", "date": "2025-09-26", "relevancy": 3.2976, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6821}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6645}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Consistent%20Language%20Gaussian%20Splatting%20for%20Point-Level%0A%20%20Open-vocabulary%20Querying&body=Title%3A%20Semantic%20Consistent%20Language%20Gaussian%20Splatting%20for%20Point-Level%0A%20%20Open-vocabulary%20Querying%0AAuthor%3A%20Hairong%20Yin%20and%20Huangying%20Zhan%20and%20Yi%20Xu%20and%20Raymond%20A.%20Yeh%0AAbstract%3A%20%20%20Open-vocabulary%203D%20scene%20understanding%20is%20crucial%20for%20robotics%20applications%2C%0Asuch%20as%20natural%20language-driven%20manipulation%2C%20human-robot%20interaction%2C%20and%0Aautonomous%20navigation.%20Existing%20methods%20for%20querying%203D%20Gaussian%20Splatting%0Aoften%20struggle%20with%20inconsistent%202D%20mask%20supervision%20and%20lack%20a%20robust%203D%0Apoint-level%20retrieval%20mechanism.%20In%20this%20work%2C%20%28i%29%20we%20present%20a%20novel%0Apoint-level%20querying%20framework%20that%20performs%20tracking%20on%20segmentation%20masks%20to%0Aestablish%20a%20semantically%20consistent%20ground-truth%20for%20distilling%20the%20language%0AGaussians%3B%20%28ii%29%20we%20introduce%20a%20GT-anchored%20querying%20approach%20that%20first%0Aretrieves%20the%20distilled%20ground-truth%20and%20subsequently%20uses%20the%20ground-truth%20to%0Aquery%20the%20individual%20Gaussians.%20Extensive%20experiments%20on%20three%20benchmark%0Adatasets%20demonstrate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%0Aperformance.%20Our%20method%20achieves%20an%20mIoU%20improvement%20of%20%2B4.14%2C%20%2B20.42%2C%20and%20%2B1.7%0Aon%20the%20LERF%2C%203D-OVS%2C%20and%20Replica%20datasets.%20These%20results%20validate%20our%20framework%0Aas%20a%20promising%20step%20toward%20open-vocabulary%20understanding%20in%20real-world%20robotic%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.21767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Consistent%2520Language%2520Gaussian%2520Splatting%2520for%2520Point-Level%250A%2520%2520Open-vocabulary%2520Querying%26entry.906535625%3DHairong%2520Yin%2520and%2520Huangying%2520Zhan%2520and%2520Yi%2520Xu%2520and%2520Raymond%2520A.%2520Yeh%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520scene%2520understanding%2520is%2520crucial%2520for%2520robotics%2520applications%252C%250Asuch%2520as%2520natural%2520language-driven%2520manipulation%252C%2520human-robot%2520interaction%252C%2520and%250Aautonomous%2520navigation.%2520Existing%2520methods%2520for%2520querying%25203D%2520Gaussian%2520Splatting%250Aoften%2520struggle%2520with%2520inconsistent%25202D%2520mask%2520supervision%2520and%2520lack%2520a%2520robust%25203D%250Apoint-level%2520retrieval%2520mechanism.%2520In%2520this%2520work%252C%2520%2528i%2529%2520we%2520present%2520a%2520novel%250Apoint-level%2520querying%2520framework%2520that%2520performs%2520tracking%2520on%2520segmentation%2520masks%2520to%250Aestablish%2520a%2520semantically%2520consistent%2520ground-truth%2520for%2520distilling%2520the%2520language%250AGaussians%253B%2520%2528ii%2529%2520we%2520introduce%2520a%2520GT-anchored%2520querying%2520approach%2520that%2520first%250Aretrieves%2520the%2520distilled%2520ground-truth%2520and%2520subsequently%2520uses%2520the%2520ground-truth%2520to%250Aquery%2520the%2520individual%2520Gaussians.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%250Adatasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520outperforms%2520state-of-the-art%250Aperformance.%2520Our%2520method%2520achieves%2520an%2520mIoU%2520improvement%2520of%2520%252B4.14%252C%2520%252B20.42%252C%2520and%2520%252B1.7%250Aon%2520the%2520LERF%252C%25203D-OVS%252C%2520and%2520Replica%2520datasets.%2520These%2520results%2520validate%2520our%2520framework%250Aas%2520a%2520promising%2520step%2520toward%2520open-vocabulary%2520understanding%2520in%2520real-world%2520robotic%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.21767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Consistent%20Language%20Gaussian%20Splatting%20for%20Point-Level%0A%20%20Open-vocabulary%20Querying&entry.906535625=Hairong%20Yin%20and%20Huangying%20Zhan%20and%20Yi%20Xu%20and%20Raymond%20A.%20Yeh&entry.1292438233=%20%20Open-vocabulary%203D%20scene%20understanding%20is%20crucial%20for%20robotics%20applications%2C%0Asuch%20as%20natural%20language-driven%20manipulation%2C%20human-robot%20interaction%2C%20and%0Aautonomous%20navigation.%20Existing%20methods%20for%20querying%203D%20Gaussian%20Splatting%0Aoften%20struggle%20with%20inconsistent%202D%20mask%20supervision%20and%20lack%20a%20robust%203D%0Apoint-level%20retrieval%20mechanism.%20In%20this%20work%2C%20%28i%29%20we%20present%20a%20novel%0Apoint-level%20querying%20framework%20that%20performs%20tracking%20on%20segmentation%20masks%20to%0Aestablish%20a%20semantically%20consistent%20ground-truth%20for%20distilling%20the%20language%0AGaussians%3B%20%28ii%29%20we%20introduce%20a%20GT-anchored%20querying%20approach%20that%20first%0Aretrieves%20the%20distilled%20ground-truth%20and%20subsequently%20uses%20the%20ground-truth%20to%0Aquery%20the%20individual%20Gaussians.%20Extensive%20experiments%20on%20three%20benchmark%0Adatasets%20demonstrate%20that%20the%20proposed%20method%20outperforms%20state-of-the-art%0Aperformance.%20Our%20method%20achieves%20an%20mIoU%20improvement%20of%20%2B4.14%2C%20%2B20.42%2C%20and%20%2B1.7%0Aon%20the%20LERF%2C%203D-OVS%2C%20and%20Replica%20datasets.%20These%20results%20validate%20our%20framework%0Aas%20a%20promising%20step%20toward%20open-vocabulary%20understanding%20in%20real-world%20robotic%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.21767v2&entry.124074799=Read"},
{"title": "Vision-Language Alignment from Compressed Image Representations using 2D\n  Gaussian Splatting", "author": "Yasmine Omri and Connor Ding and Tsachy Weissman and Thierry Tambe", "abstract": "  Modern vision language pipelines are driven by RGB vision encoders trained on\nmassive image text corpora. While these pipelines have enabled impressive zero\nshot capabilities and strong transfer across tasks, they still inherit two\nstructural inefficiencies from the pixel domain: (i) transmitting dense RGB\nimages from edge devices to the cloud is energy intensive and costly, and (ii)\npatch based tokenization explodes sequence length, stressing attention budgets\nand context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative\nvisual substrate for alignment: a compact, spatially adaptive representation\nthat parameterizes images by a set of colored anisotropic Gaussians. We develop\na scalable 2DGS pipeline with structured initialization, luminance aware\npruning, and batched CUDA kernels, achieving over 90x faster fitting and about\n97% GPU utilization compared to prior implementations. We further adapt\ncontrastive language image pretraining (CLIP) to 2DGS by reusing a frozen\nRGB-based transformer backbone with a lightweight splat aware input stem and a\nperceiver resampler, training only about 7% of the total parameters. On large\nDataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K\nperformance while compressing inputs 3 to 20x relative to pixels. While\naccuracy currently trails RGB encoders, our results establish 2DGS as a viable\nmultimodal substrate, pinpoint architectural bottlenecks, and open a path\ntoward representations that are both semantically powerful and transmission\nefficient for edge cloud learning.\n", "link": "http://arxiv.org/abs/2509.22615v1", "date": "2025-09-26", "relevancy": 3.2686, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6742}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6506}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Alignment%20from%20Compressed%20Image%20Representations%20using%202D%0A%20%20Gaussian%20Splatting&body=Title%3A%20Vision-Language%20Alignment%20from%20Compressed%20Image%20Representations%20using%202D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Yasmine%20Omri%20and%20Connor%20Ding%20and%20Tsachy%20Weissman%20and%20Thierry%20Tambe%0AAbstract%3A%20%20%20Modern%20vision%20language%20pipelines%20are%20driven%20by%20RGB%20vision%20encoders%20trained%20on%0Amassive%20image%20text%20corpora.%20While%20these%20pipelines%20have%20enabled%20impressive%20zero%0Ashot%20capabilities%20and%20strong%20transfer%20across%20tasks%2C%20they%20still%20inherit%20two%0Astructural%20inefficiencies%20from%20the%20pixel%20domain%3A%20%28i%29%20transmitting%20dense%20RGB%0Aimages%20from%20edge%20devices%20to%20the%20cloud%20is%20energy%20intensive%20and%20costly%2C%20and%20%28ii%29%0Apatch%20based%20tokenization%20explodes%20sequence%20length%2C%20stressing%20attention%20budgets%0Aand%20context%20limits.%20We%20explore%202D%20Gaussian%20Splatting%20%282DGS%29%20as%20an%20alternative%0Avisual%20substrate%20for%20alignment%3A%20a%20compact%2C%20spatially%20adaptive%20representation%0Athat%20parameterizes%20images%20by%20a%20set%20of%20colored%20anisotropic%20Gaussians.%20We%20develop%0Aa%20scalable%202DGS%20pipeline%20with%20structured%20initialization%2C%20luminance%20aware%0Apruning%2C%20and%20batched%20CUDA%20kernels%2C%20achieving%20over%2090x%20faster%20fitting%20and%20about%0A97%25%20GPU%20utilization%20compared%20to%20prior%20implementations.%20We%20further%20adapt%0Acontrastive%20language%20image%20pretraining%20%28CLIP%29%20to%202DGS%20by%20reusing%20a%20frozen%0ARGB-based%20transformer%20backbone%20with%20a%20lightweight%20splat%20aware%20input%20stem%20and%20a%0Aperceiver%20resampler%2C%20training%20only%20about%207%25%20of%20the%20total%20parameters.%20On%20large%0ADataComp%20subsets%2C%20GS%20encoders%20yield%20meaningful%20zero%20shot%20ImageNet-1K%0Aperformance%20while%20compressing%20inputs%203%20to%2020x%20relative%20to%20pixels.%20While%0Aaccuracy%20currently%20trails%20RGB%20encoders%2C%20our%20results%20establish%202DGS%20as%20a%20viable%0Amultimodal%20substrate%2C%20pinpoint%20architectural%20bottlenecks%2C%20and%20open%20a%20path%0Atoward%20representations%20that%20are%20both%20semantically%20powerful%20and%20transmission%0Aefficient%20for%20edge%20cloud%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Alignment%2520from%2520Compressed%2520Image%2520Representations%2520using%25202D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DYasmine%2520Omri%2520and%2520Connor%2520Ding%2520and%2520Tsachy%2520Weissman%2520and%2520Thierry%2520Tambe%26entry.1292438233%3D%2520%2520Modern%2520vision%2520language%2520pipelines%2520are%2520driven%2520by%2520RGB%2520vision%2520encoders%2520trained%2520on%250Amassive%2520image%2520text%2520corpora.%2520While%2520these%2520pipelines%2520have%2520enabled%2520impressive%2520zero%250Ashot%2520capabilities%2520and%2520strong%2520transfer%2520across%2520tasks%252C%2520they%2520still%2520inherit%2520two%250Astructural%2520inefficiencies%2520from%2520the%2520pixel%2520domain%253A%2520%2528i%2529%2520transmitting%2520dense%2520RGB%250Aimages%2520from%2520edge%2520devices%2520to%2520the%2520cloud%2520is%2520energy%2520intensive%2520and%2520costly%252C%2520and%2520%2528ii%2529%250Apatch%2520based%2520tokenization%2520explodes%2520sequence%2520length%252C%2520stressing%2520attention%2520budgets%250Aand%2520context%2520limits.%2520We%2520explore%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520as%2520an%2520alternative%250Avisual%2520substrate%2520for%2520alignment%253A%2520a%2520compact%252C%2520spatially%2520adaptive%2520representation%250Athat%2520parameterizes%2520images%2520by%2520a%2520set%2520of%2520colored%2520anisotropic%2520Gaussians.%2520We%2520develop%250Aa%2520scalable%25202DGS%2520pipeline%2520with%2520structured%2520initialization%252C%2520luminance%2520aware%250Apruning%252C%2520and%2520batched%2520CUDA%2520kernels%252C%2520achieving%2520over%252090x%2520faster%2520fitting%2520and%2520about%250A97%2525%2520GPU%2520utilization%2520compared%2520to%2520prior%2520implementations.%2520We%2520further%2520adapt%250Acontrastive%2520language%2520image%2520pretraining%2520%2528CLIP%2529%2520to%25202DGS%2520by%2520reusing%2520a%2520frozen%250ARGB-based%2520transformer%2520backbone%2520with%2520a%2520lightweight%2520splat%2520aware%2520input%2520stem%2520and%2520a%250Aperceiver%2520resampler%252C%2520training%2520only%2520about%25207%2525%2520of%2520the%2520total%2520parameters.%2520On%2520large%250ADataComp%2520subsets%252C%2520GS%2520encoders%2520yield%2520meaningful%2520zero%2520shot%2520ImageNet-1K%250Aperformance%2520while%2520compressing%2520inputs%25203%2520to%252020x%2520relative%2520to%2520pixels.%2520While%250Aaccuracy%2520currently%2520trails%2520RGB%2520encoders%252C%2520our%2520results%2520establish%25202DGS%2520as%2520a%2520viable%250Amultimodal%2520substrate%252C%2520pinpoint%2520architectural%2520bottlenecks%252C%2520and%2520open%2520a%2520path%250Atoward%2520representations%2520that%2520are%2520both%2520semantically%2520powerful%2520and%2520transmission%250Aefficient%2520for%2520edge%2520cloud%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Alignment%20from%20Compressed%20Image%20Representations%20using%202D%0A%20%20Gaussian%20Splatting&entry.906535625=Yasmine%20Omri%20and%20Connor%20Ding%20and%20Tsachy%20Weissman%20and%20Thierry%20Tambe&entry.1292438233=%20%20Modern%20vision%20language%20pipelines%20are%20driven%20by%20RGB%20vision%20encoders%20trained%20on%0Amassive%20image%20text%20corpora.%20While%20these%20pipelines%20have%20enabled%20impressive%20zero%0Ashot%20capabilities%20and%20strong%20transfer%20across%20tasks%2C%20they%20still%20inherit%20two%0Astructural%20inefficiencies%20from%20the%20pixel%20domain%3A%20%28i%29%20transmitting%20dense%20RGB%0Aimages%20from%20edge%20devices%20to%20the%20cloud%20is%20energy%20intensive%20and%20costly%2C%20and%20%28ii%29%0Apatch%20based%20tokenization%20explodes%20sequence%20length%2C%20stressing%20attention%20budgets%0Aand%20context%20limits.%20We%20explore%202D%20Gaussian%20Splatting%20%282DGS%29%20as%20an%20alternative%0Avisual%20substrate%20for%20alignment%3A%20a%20compact%2C%20spatially%20adaptive%20representation%0Athat%20parameterizes%20images%20by%20a%20set%20of%20colored%20anisotropic%20Gaussians.%20We%20develop%0Aa%20scalable%202DGS%20pipeline%20with%20structured%20initialization%2C%20luminance%20aware%0Apruning%2C%20and%20batched%20CUDA%20kernels%2C%20achieving%20over%2090x%20faster%20fitting%20and%20about%0A97%25%20GPU%20utilization%20compared%20to%20prior%20implementations.%20We%20further%20adapt%0Acontrastive%20language%20image%20pretraining%20%28CLIP%29%20to%202DGS%20by%20reusing%20a%20frozen%0ARGB-based%20transformer%20backbone%20with%20a%20lightweight%20splat%20aware%20input%20stem%20and%20a%0Aperceiver%20resampler%2C%20training%20only%20about%207%25%20of%20the%20total%20parameters.%20On%20large%0ADataComp%20subsets%2C%20GS%20encoders%20yield%20meaningful%20zero%20shot%20ImageNet-1K%0Aperformance%20while%20compressing%20inputs%203%20to%2020x%20relative%20to%20pixels.%20While%0Aaccuracy%20currently%20trails%20RGB%20encoders%2C%20our%20results%20establish%202DGS%20as%20a%20viable%0Amultimodal%20substrate%2C%20pinpoint%20architectural%20bottlenecks%2C%20and%20open%20a%20path%0Atoward%20representations%20that%20are%20both%20semantically%20powerful%20and%20transmission%0Aefficient%20for%20edge%20cloud%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22615v1&entry.124074799=Read"},
{"title": "Rigidity-Aware 3D Gaussian Deformation from a Single Image", "author": "Jinhyeok Kim and Jaehun Bang and Seunghyun Seo and Kyungdon Joo", "abstract": "  Reconstructing object deformation from a single image remains a significant\nchallenge in computer vision and graphics. Existing methods typically rely on\nmulti-view video to recover deformation, limiting their applicability under\nconstrained scenarios. To address this, we propose DeformSplat, a novel\nframework that effectively guides 3D Gaussian deformation from only a single\nimage. Our method introduces two main technical contributions. First, we\npresent Gaussian-to-Pixel Matching which bridges the domain gap between 3D\nGaussian representations and 2D pixel observations. This enables robust\ndeformation guidance from sparse visual cues. Second, we propose Rigid Part\nSegmentation consisting of initialization and refinement. This segmentation\nexplicitly identifies rigid regions, crucial for maintaining geometric\ncoherence during deformation. By combining these two techniques, our approach\ncan reconstruct consistent deformations from a single image. Extensive\nexperiments demonstrate that our approach significantly outperforms existing\nmethods and naturally extends to various applications,such as frame\ninterpolation and interactive object manipulation.\n", "link": "http://arxiv.org/abs/2509.22222v1", "date": "2025-09-26", "relevancy": 3.2516, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6734}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6559}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rigidity-Aware%203D%20Gaussian%20Deformation%20from%20a%20Single%20Image&body=Title%3A%20Rigidity-Aware%203D%20Gaussian%20Deformation%20from%20a%20Single%20Image%0AAuthor%3A%20Jinhyeok%20Kim%20and%20Jaehun%20Bang%20and%20Seunghyun%20Seo%20and%20Kyungdon%20Joo%0AAbstract%3A%20%20%20Reconstructing%20object%20deformation%20from%20a%20single%20image%20remains%20a%20significant%0Achallenge%20in%20computer%20vision%20and%20graphics.%20Existing%20methods%20typically%20rely%20on%0Amulti-view%20video%20to%20recover%20deformation%2C%20limiting%20their%20applicability%20under%0Aconstrained%20scenarios.%20To%20address%20this%2C%20we%20propose%20DeformSplat%2C%20a%20novel%0Aframework%20that%20effectively%20guides%203D%20Gaussian%20deformation%20from%20only%20a%20single%0Aimage.%20Our%20method%20introduces%20two%20main%20technical%20contributions.%20First%2C%20we%0Apresent%20Gaussian-to-Pixel%20Matching%20which%20bridges%20the%20domain%20gap%20between%203D%0AGaussian%20representations%20and%202D%20pixel%20observations.%20This%20enables%20robust%0Adeformation%20guidance%20from%20sparse%20visual%20cues.%20Second%2C%20we%20propose%20Rigid%20Part%0ASegmentation%20consisting%20of%20initialization%20and%20refinement.%20This%20segmentation%0Aexplicitly%20identifies%20rigid%20regions%2C%20crucial%20for%20maintaining%20geometric%0Acoherence%20during%20deformation.%20By%20combining%20these%20two%20techniques%2C%20our%20approach%0Acan%20reconstruct%20consistent%20deformations%20from%20a%20single%20image.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%0Amethods%20and%20naturally%20extends%20to%20various%20applications%2Csuch%20as%20frame%0Ainterpolation%20and%20interactive%20object%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRigidity-Aware%25203D%2520Gaussian%2520Deformation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DJinhyeok%2520Kim%2520and%2520Jaehun%2520Bang%2520and%2520Seunghyun%2520Seo%2520and%2520Kyungdon%2520Joo%26entry.1292438233%3D%2520%2520Reconstructing%2520object%2520deformation%2520from%2520a%2520single%2520image%2520remains%2520a%2520significant%250Achallenge%2520in%2520computer%2520vision%2520and%2520graphics.%2520Existing%2520methods%2520typically%2520rely%2520on%250Amulti-view%2520video%2520to%2520recover%2520deformation%252C%2520limiting%2520their%2520applicability%2520under%250Aconstrained%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520propose%2520DeformSplat%252C%2520a%2520novel%250Aframework%2520that%2520effectively%2520guides%25203D%2520Gaussian%2520deformation%2520from%2520only%2520a%2520single%250Aimage.%2520Our%2520method%2520introduces%2520two%2520main%2520technical%2520contributions.%2520First%252C%2520we%250Apresent%2520Gaussian-to-Pixel%2520Matching%2520which%2520bridges%2520the%2520domain%2520gap%2520between%25203D%250AGaussian%2520representations%2520and%25202D%2520pixel%2520observations.%2520This%2520enables%2520robust%250Adeformation%2520guidance%2520from%2520sparse%2520visual%2520cues.%2520Second%252C%2520we%2520propose%2520Rigid%2520Part%250ASegmentation%2520consisting%2520of%2520initialization%2520and%2520refinement.%2520This%2520segmentation%250Aexplicitly%2520identifies%2520rigid%2520regions%252C%2520crucial%2520for%2520maintaining%2520geometric%250Acoherence%2520during%2520deformation.%2520By%2520combining%2520these%2520two%2520techniques%252C%2520our%2520approach%250Acan%2520reconstruct%2520consistent%2520deformations%2520from%2520a%2520single%2520image.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%250Amethods%2520and%2520naturally%2520extends%2520to%2520various%2520applications%252Csuch%2520as%2520frame%250Ainterpolation%2520and%2520interactive%2520object%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rigidity-Aware%203D%20Gaussian%20Deformation%20from%20a%20Single%20Image&entry.906535625=Jinhyeok%20Kim%20and%20Jaehun%20Bang%20and%20Seunghyun%20Seo%20and%20Kyungdon%20Joo&entry.1292438233=%20%20Reconstructing%20object%20deformation%20from%20a%20single%20image%20remains%20a%20significant%0Achallenge%20in%20computer%20vision%20and%20graphics.%20Existing%20methods%20typically%20rely%20on%0Amulti-view%20video%20to%20recover%20deformation%2C%20limiting%20their%20applicability%20under%0Aconstrained%20scenarios.%20To%20address%20this%2C%20we%20propose%20DeformSplat%2C%20a%20novel%0Aframework%20that%20effectively%20guides%203D%20Gaussian%20deformation%20from%20only%20a%20single%0Aimage.%20Our%20method%20introduces%20two%20main%20technical%20contributions.%20First%2C%20we%0Apresent%20Gaussian-to-Pixel%20Matching%20which%20bridges%20the%20domain%20gap%20between%203D%0AGaussian%20representations%20and%202D%20pixel%20observations.%20This%20enables%20robust%0Adeformation%20guidance%20from%20sparse%20visual%20cues.%20Second%2C%20we%20propose%20Rigid%20Part%0ASegmentation%20consisting%20of%20initialization%20and%20refinement.%20This%20segmentation%0Aexplicitly%20identifies%20rigid%20regions%2C%20crucial%20for%20maintaining%20geometric%0Acoherence%20during%20deformation.%20By%20combining%20these%20two%20techniques%2C%20our%20approach%0Acan%20reconstruct%20consistent%20deformations%20from%20a%20single%20image.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20significantly%20outperforms%20existing%0Amethods%20and%20naturally%20extends%20to%20various%20applications%2Csuch%20as%20frame%0Ainterpolation%20and%20interactive%20object%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22222v1&entry.124074799=Read"},
{"title": "UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual\n  Understanding of City Scenes through Human Perspective", "author": "Jun He and Yi Lin and Zilong Huang and Jiacong Yin and Junyan Ye and Yuchuan Zhou and Weijia Li and Xiang Zhang", "abstract": "  Urban development impacts over half of the global population, making\nhuman-centered understanding of its structural and perceptual changes essential\nfor sustainable development. While Multimodal Large Language Models (MLLMs)\nhave shown remarkable capabilities across various domains, existing benchmarks\nthat explore their performance in urban environments remain limited, lacking\nsystematic exploration of temporal evolution and subjective perception of urban\nenvironment that aligns with human perception. To address these limitations, we\npropose UrbanFeel, a comprehensive benchmark designed to evaluate the\nperformance of MLLMs in urban development understanding and subjective\nenvironmental perception. UrbanFeel comprises 14.3K carefully constructed\nvisual questions spanning three cognitively progressive dimensions: Static\nScene Perception, Temporal Change Understanding, and Subjective Environmental\nPerception. We collect multi-temporal single-view and panoramic street-view\nimages from 11 representative cities worldwide, and generate high-quality\nquestion-answer pairs through a hybrid pipeline of spatial clustering,\nrule-based generation, model-assisted prompting, and manual annotation. Through\nextensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5\nPro achieves the best overall performance, with its accuracy approaching human\nexpert levels and narrowing the average gap to just 1.5\\%. Most models perform\nwell on tasks grounded in scene understanding. In particular, some models even\nsurpass human annotators in pixel-level change detection. However, performance\ndrops notably in tasks requiring temporal reasoning over urban development.\nAdditionally, in the subjective perception dimension, several models reach\nhuman-level or even higher consistency in evaluating dimension such as\nbeautiful and safety.\n", "link": "http://arxiv.org/abs/2509.22228v1", "date": "2025-09-26", "relevancy": 2.978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6083}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanFeel%3A%20A%20Comprehensive%20Benchmark%20for%20Temporal%20and%20Perceptual%0A%20%20Understanding%20of%20City%20Scenes%20through%20Human%20Perspective&body=Title%3A%20UrbanFeel%3A%20A%20Comprehensive%20Benchmark%20for%20Temporal%20and%20Perceptual%0A%20%20Understanding%20of%20City%20Scenes%20through%20Human%20Perspective%0AAuthor%3A%20Jun%20He%20and%20Yi%20Lin%20and%20Zilong%20Huang%20and%20Jiacong%20Yin%20and%20Junyan%20Ye%20and%20Yuchuan%20Zhou%20and%20Weijia%20Li%20and%20Xiang%20Zhang%0AAbstract%3A%20%20%20Urban%20development%20impacts%20over%20half%20of%20the%20global%20population%2C%20making%0Ahuman-centered%20understanding%20of%20its%20structural%20and%20perceptual%20changes%20essential%0Afor%20sustainable%20development.%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ahave%20shown%20remarkable%20capabilities%20across%20various%20domains%2C%20existing%20benchmarks%0Athat%20explore%20their%20performance%20in%20urban%20environments%20remain%20limited%2C%20lacking%0Asystematic%20exploration%20of%20temporal%20evolution%20and%20subjective%20perception%20of%20urban%0Aenvironment%20that%20aligns%20with%20human%20perception.%20To%20address%20these%20limitations%2C%20we%0Apropose%20UrbanFeel%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%0Aperformance%20of%20MLLMs%20in%20urban%20development%20understanding%20and%20subjective%0Aenvironmental%20perception.%20UrbanFeel%20comprises%2014.3K%20carefully%20constructed%0Avisual%20questions%20spanning%20three%20cognitively%20progressive%20dimensions%3A%20Static%0AScene%20Perception%2C%20Temporal%20Change%20Understanding%2C%20and%20Subjective%20Environmental%0APerception.%20We%20collect%20multi-temporal%20single-view%20and%20panoramic%20street-view%0Aimages%20from%2011%20representative%20cities%20worldwide%2C%20and%20generate%20high-quality%0Aquestion-answer%20pairs%20through%20a%20hybrid%20pipeline%20of%20spatial%20clustering%2C%0Arule-based%20generation%2C%20model-assisted%20prompting%2C%20and%20manual%20annotation.%20Through%0Aextensive%20evaluation%20of%2020%20state-of-the-art%20MLLMs%2C%20we%20observe%20that%20Gemini-2.5%0APro%20achieves%20the%20best%20overall%20performance%2C%20with%20its%20accuracy%20approaching%20human%0Aexpert%20levels%20and%20narrowing%20the%20average%20gap%20to%20just%201.5%5C%25.%20Most%20models%20perform%0Awell%20on%20tasks%20grounded%20in%20scene%20understanding.%20In%20particular%2C%20some%20models%20even%0Asurpass%20human%20annotators%20in%20pixel-level%20change%20detection.%20However%2C%20performance%0Adrops%20notably%20in%20tasks%20requiring%20temporal%20reasoning%20over%20urban%20development.%0AAdditionally%2C%20in%20the%20subjective%20perception%20dimension%2C%20several%20models%20reach%0Ahuman-level%20or%20even%20higher%20consistency%20in%20evaluating%20dimension%20such%20as%0Abeautiful%20and%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanFeel%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Temporal%2520and%2520Perceptual%250A%2520%2520Understanding%2520of%2520City%2520Scenes%2520through%2520Human%2520Perspective%26entry.906535625%3DJun%2520He%2520and%2520Yi%2520Lin%2520and%2520Zilong%2520Huang%2520and%2520Jiacong%2520Yin%2520and%2520Junyan%2520Ye%2520and%2520Yuchuan%2520Zhou%2520and%2520Weijia%2520Li%2520and%2520Xiang%2520Zhang%26entry.1292438233%3D%2520%2520Urban%2520development%2520impacts%2520over%2520half%2520of%2520the%2520global%2520population%252C%2520making%250Ahuman-centered%2520understanding%2520of%2520its%2520structural%2520and%2520perceptual%2520changes%2520essential%250Afor%2520sustainable%2520development.%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Ahave%2520shown%2520remarkable%2520capabilities%2520across%2520various%2520domains%252C%2520existing%2520benchmarks%250Athat%2520explore%2520their%2520performance%2520in%2520urban%2520environments%2520remain%2520limited%252C%2520lacking%250Asystematic%2520exploration%2520of%2520temporal%2520evolution%2520and%2520subjective%2520perception%2520of%2520urban%250Aenvironment%2520that%2520aligns%2520with%2520human%2520perception.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520UrbanFeel%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%250Aperformance%2520of%2520MLLMs%2520in%2520urban%2520development%2520understanding%2520and%2520subjective%250Aenvironmental%2520perception.%2520UrbanFeel%2520comprises%252014.3K%2520carefully%2520constructed%250Avisual%2520questions%2520spanning%2520three%2520cognitively%2520progressive%2520dimensions%253A%2520Static%250AScene%2520Perception%252C%2520Temporal%2520Change%2520Understanding%252C%2520and%2520Subjective%2520Environmental%250APerception.%2520We%2520collect%2520multi-temporal%2520single-view%2520and%2520panoramic%2520street-view%250Aimages%2520from%252011%2520representative%2520cities%2520worldwide%252C%2520and%2520generate%2520high-quality%250Aquestion-answer%2520pairs%2520through%2520a%2520hybrid%2520pipeline%2520of%2520spatial%2520clustering%252C%250Arule-based%2520generation%252C%2520model-assisted%2520prompting%252C%2520and%2520manual%2520annotation.%2520Through%250Aextensive%2520evaluation%2520of%252020%2520state-of-the-art%2520MLLMs%252C%2520we%2520observe%2520that%2520Gemini-2.5%250APro%2520achieves%2520the%2520best%2520overall%2520performance%252C%2520with%2520its%2520accuracy%2520approaching%2520human%250Aexpert%2520levels%2520and%2520narrowing%2520the%2520average%2520gap%2520to%2520just%25201.5%255C%2525.%2520Most%2520models%2520perform%250Awell%2520on%2520tasks%2520grounded%2520in%2520scene%2520understanding.%2520In%2520particular%252C%2520some%2520models%2520even%250Asurpass%2520human%2520annotators%2520in%2520pixel-level%2520change%2520detection.%2520However%252C%2520performance%250Adrops%2520notably%2520in%2520tasks%2520requiring%2520temporal%2520reasoning%2520over%2520urban%2520development.%250AAdditionally%252C%2520in%2520the%2520subjective%2520perception%2520dimension%252C%2520several%2520models%2520reach%250Ahuman-level%2520or%2520even%2520higher%2520consistency%2520in%2520evaluating%2520dimension%2520such%2520as%250Abeautiful%2520and%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanFeel%3A%20A%20Comprehensive%20Benchmark%20for%20Temporal%20and%20Perceptual%0A%20%20Understanding%20of%20City%20Scenes%20through%20Human%20Perspective&entry.906535625=Jun%20He%20and%20Yi%20Lin%20and%20Zilong%20Huang%20and%20Jiacong%20Yin%20and%20Junyan%20Ye%20and%20Yuchuan%20Zhou%20and%20Weijia%20Li%20and%20Xiang%20Zhang&entry.1292438233=%20%20Urban%20development%20impacts%20over%20half%20of%20the%20global%20population%2C%20making%0Ahuman-centered%20understanding%20of%20its%20structural%20and%20perceptual%20changes%20essential%0Afor%20sustainable%20development.%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Ahave%20shown%20remarkable%20capabilities%20across%20various%20domains%2C%20existing%20benchmarks%0Athat%20explore%20their%20performance%20in%20urban%20environments%20remain%20limited%2C%20lacking%0Asystematic%20exploration%20of%20temporal%20evolution%20and%20subjective%20perception%20of%20urban%0Aenvironment%20that%20aligns%20with%20human%20perception.%20To%20address%20these%20limitations%2C%20we%0Apropose%20UrbanFeel%2C%20a%20comprehensive%20benchmark%20designed%20to%20evaluate%20the%0Aperformance%20of%20MLLMs%20in%20urban%20development%20understanding%20and%20subjective%0Aenvironmental%20perception.%20UrbanFeel%20comprises%2014.3K%20carefully%20constructed%0Avisual%20questions%20spanning%20three%20cognitively%20progressive%20dimensions%3A%20Static%0AScene%20Perception%2C%20Temporal%20Change%20Understanding%2C%20and%20Subjective%20Environmental%0APerception.%20We%20collect%20multi-temporal%20single-view%20and%20panoramic%20street-view%0Aimages%20from%2011%20representative%20cities%20worldwide%2C%20and%20generate%20high-quality%0Aquestion-answer%20pairs%20through%20a%20hybrid%20pipeline%20of%20spatial%20clustering%2C%0Arule-based%20generation%2C%20model-assisted%20prompting%2C%20and%20manual%20annotation.%20Through%0Aextensive%20evaluation%20of%2020%20state-of-the-art%20MLLMs%2C%20we%20observe%20that%20Gemini-2.5%0APro%20achieves%20the%20best%20overall%20performance%2C%20with%20its%20accuracy%20approaching%20human%0Aexpert%20levels%20and%20narrowing%20the%20average%20gap%20to%20just%201.5%5C%25.%20Most%20models%20perform%0Awell%20on%20tasks%20grounded%20in%20scene%20understanding.%20In%20particular%2C%20some%20models%20even%0Asurpass%20human%20annotators%20in%20pixel-level%20change%20detection.%20However%2C%20performance%0Adrops%20notably%20in%20tasks%20requiring%20temporal%20reasoning%20over%20urban%20development.%0AAdditionally%2C%20in%20the%20subjective%20perception%20dimension%2C%20several%20models%20reach%0Ahuman-level%20or%20even%20higher%20consistency%20in%20evaluating%20dimension%20such%20as%0Abeautiful%20and%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22228v1&entry.124074799=Read"},
{"title": "Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical\n  Segmentation", "author": "Jinpeng Lu and Linghan Cai and Yinda Chen and Guo Tang and Songhan Jiang and Haoyuan Shi and Zhiwei Xiong", "abstract": "  Lightweight 3D medical image segmentation remains constrained by a\nfundamental \"efficiency / robustness conflict\", particularly when processing\ncomplex anatomical structures and heterogeneous modalities. In this paper, we\nstudy how to redesign the framework based on the characteristics of\nhigh-dimensional 3D images, and explore data synergy to overcome the fragile\nrepresentation of lightweight methods. Our approach, VeloxSeg, begins with a\ndeployable and extensible dual-stream CNN-Transformer architecture composed of\nPaired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided\nconvolution (JLC). For each 3D image, we invoke a \"glance-and-focus\" principle,\nwhere PWA rapidly retrieves multi-scale information, and JLC ensures robust\nlocal feature extraction with minimal parameters, significantly enhancing the\nmodel's ability to operate with low computational budget. Followed by an\nextension of the dual-stream architecture that incorporates modal interaction\ninto the multi-scale image-retrieval process, VeloxSeg efficiently models\nheterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer\n(SDKT) via Gram matrices injects the texture prior extracted by a\nself-supervised network into the segmentation network, yielding stronger\nrepresentations than baselines at no extra inference cost. Experimental results\non multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement,\nalongside increasing GPU throughput by 11x and CPU by 48x. Codes are available\nat https://github.com/JinPLu/VeloxSeg.\n", "link": "http://arxiv.org/abs/2509.22307v1", "date": "2025-09-26", "relevancy": 2.9105, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Johnson-Lindenstrauss%20Lemma%20Guided%20Network%20for%20Efficient%203D%20Medical%0A%20%20Segmentation&body=Title%3A%20Johnson-Lindenstrauss%20Lemma%20Guided%20Network%20for%20Efficient%203D%20Medical%0A%20%20Segmentation%0AAuthor%3A%20Jinpeng%20Lu%20and%20Linghan%20Cai%20and%20Yinda%20Chen%20and%20Guo%20Tang%20and%20Songhan%20Jiang%20and%20Haoyuan%20Shi%20and%20Zhiwei%20Xiong%0AAbstract%3A%20%20%20Lightweight%203D%20medical%20image%20segmentation%20remains%20constrained%20by%20a%0Afundamental%20%22efficiency%20/%20robustness%20conflict%22%2C%20particularly%20when%20processing%0Acomplex%20anatomical%20structures%20and%20heterogeneous%20modalities.%20In%20this%20paper%2C%20we%0Astudy%20how%20to%20redesign%20the%20framework%20based%20on%20the%20characteristics%20of%0Ahigh-dimensional%203D%20images%2C%20and%20explore%20data%20synergy%20to%20overcome%20the%20fragile%0Arepresentation%20of%20lightweight%20methods.%20Our%20approach%2C%20VeloxSeg%2C%20begins%20with%20a%0Adeployable%20and%20extensible%20dual-stream%20CNN-Transformer%20architecture%20composed%20of%0APaired%20Window%20Attention%20%28PWA%29%20and%20Johnson-Lindenstrauss%20lemma-guided%0Aconvolution%20%28JLC%29.%20For%20each%203D%20image%2C%20we%20invoke%20a%20%22glance-and-focus%22%20principle%2C%0Awhere%20PWA%20rapidly%20retrieves%20multi-scale%20information%2C%20and%20JLC%20ensures%20robust%0Alocal%20feature%20extraction%20with%20minimal%20parameters%2C%20significantly%20enhancing%20the%0Amodel%27s%20ability%20to%20operate%20with%20low%20computational%20budget.%20Followed%20by%20an%0Aextension%20of%20the%20dual-stream%20architecture%20that%20incorporates%20modal%20interaction%0Ainto%20the%20multi-scale%20image-retrieval%20process%2C%20VeloxSeg%20efficiently%20models%0Aheterogeneous%20modalities.%20Finally%2C%20Spatially%20Decoupled%20Knowledge%20Transfer%0A%28SDKT%29%20via%20Gram%20matrices%20injects%20the%20texture%20prior%20extracted%20by%20a%0Aself-supervised%20network%20into%20the%20segmentation%20network%2C%20yielding%20stronger%0Arepresentations%20than%20baselines%20at%20no%20extra%20inference%20cost.%20Experimental%20results%0Aon%20multimodal%20benchmarks%20show%20that%20VeloxSeg%20achieves%20a%2026%25%20Dice%20improvement%2C%0Aalongside%20increasing%20GPU%20throughput%20by%2011x%20and%20CPU%20by%2048x.%20Codes%20are%20available%0Aat%20https%3A//github.com/JinPLu/VeloxSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJohnson-Lindenstrauss%2520Lemma%2520Guided%2520Network%2520for%2520Efficient%25203D%2520Medical%250A%2520%2520Segmentation%26entry.906535625%3DJinpeng%2520Lu%2520and%2520Linghan%2520Cai%2520and%2520Yinda%2520Chen%2520and%2520Guo%2520Tang%2520and%2520Songhan%2520Jiang%2520and%2520Haoyuan%2520Shi%2520and%2520Zhiwei%2520Xiong%26entry.1292438233%3D%2520%2520Lightweight%25203D%2520medical%2520image%2520segmentation%2520remains%2520constrained%2520by%2520a%250Afundamental%2520%2522efficiency%2520/%2520robustness%2520conflict%2522%252C%2520particularly%2520when%2520processing%250Acomplex%2520anatomical%2520structures%2520and%2520heterogeneous%2520modalities.%2520In%2520this%2520paper%252C%2520we%250Astudy%2520how%2520to%2520redesign%2520the%2520framework%2520based%2520on%2520the%2520characteristics%2520of%250Ahigh-dimensional%25203D%2520images%252C%2520and%2520explore%2520data%2520synergy%2520to%2520overcome%2520the%2520fragile%250Arepresentation%2520of%2520lightweight%2520methods.%2520Our%2520approach%252C%2520VeloxSeg%252C%2520begins%2520with%2520a%250Adeployable%2520and%2520extensible%2520dual-stream%2520CNN-Transformer%2520architecture%2520composed%2520of%250APaired%2520Window%2520Attention%2520%2528PWA%2529%2520and%2520Johnson-Lindenstrauss%2520lemma-guided%250Aconvolution%2520%2528JLC%2529.%2520For%2520each%25203D%2520image%252C%2520we%2520invoke%2520a%2520%2522glance-and-focus%2522%2520principle%252C%250Awhere%2520PWA%2520rapidly%2520retrieves%2520multi-scale%2520information%252C%2520and%2520JLC%2520ensures%2520robust%250Alocal%2520feature%2520extraction%2520with%2520minimal%2520parameters%252C%2520significantly%2520enhancing%2520the%250Amodel%2527s%2520ability%2520to%2520operate%2520with%2520low%2520computational%2520budget.%2520Followed%2520by%2520an%250Aextension%2520of%2520the%2520dual-stream%2520architecture%2520that%2520incorporates%2520modal%2520interaction%250Ainto%2520the%2520multi-scale%2520image-retrieval%2520process%252C%2520VeloxSeg%2520efficiently%2520models%250Aheterogeneous%2520modalities.%2520Finally%252C%2520Spatially%2520Decoupled%2520Knowledge%2520Transfer%250A%2528SDKT%2529%2520via%2520Gram%2520matrices%2520injects%2520the%2520texture%2520prior%2520extracted%2520by%2520a%250Aself-supervised%2520network%2520into%2520the%2520segmentation%2520network%252C%2520yielding%2520stronger%250Arepresentations%2520than%2520baselines%2520at%2520no%2520extra%2520inference%2520cost.%2520Experimental%2520results%250Aon%2520multimodal%2520benchmarks%2520show%2520that%2520VeloxSeg%2520achieves%2520a%252026%2525%2520Dice%2520improvement%252C%250Aalongside%2520increasing%2520GPU%2520throughput%2520by%252011x%2520and%2520CPU%2520by%252048x.%2520Codes%2520are%2520available%250Aat%2520https%253A//github.com/JinPLu/VeloxSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Johnson-Lindenstrauss%20Lemma%20Guided%20Network%20for%20Efficient%203D%20Medical%0A%20%20Segmentation&entry.906535625=Jinpeng%20Lu%20and%20Linghan%20Cai%20and%20Yinda%20Chen%20and%20Guo%20Tang%20and%20Songhan%20Jiang%20and%20Haoyuan%20Shi%20and%20Zhiwei%20Xiong&entry.1292438233=%20%20Lightweight%203D%20medical%20image%20segmentation%20remains%20constrained%20by%20a%0Afundamental%20%22efficiency%20/%20robustness%20conflict%22%2C%20particularly%20when%20processing%0Acomplex%20anatomical%20structures%20and%20heterogeneous%20modalities.%20In%20this%20paper%2C%20we%0Astudy%20how%20to%20redesign%20the%20framework%20based%20on%20the%20characteristics%20of%0Ahigh-dimensional%203D%20images%2C%20and%20explore%20data%20synergy%20to%20overcome%20the%20fragile%0Arepresentation%20of%20lightweight%20methods.%20Our%20approach%2C%20VeloxSeg%2C%20begins%20with%20a%0Adeployable%20and%20extensible%20dual-stream%20CNN-Transformer%20architecture%20composed%20of%0APaired%20Window%20Attention%20%28PWA%29%20and%20Johnson-Lindenstrauss%20lemma-guided%0Aconvolution%20%28JLC%29.%20For%20each%203D%20image%2C%20we%20invoke%20a%20%22glance-and-focus%22%20principle%2C%0Awhere%20PWA%20rapidly%20retrieves%20multi-scale%20information%2C%20and%20JLC%20ensures%20robust%0Alocal%20feature%20extraction%20with%20minimal%20parameters%2C%20significantly%20enhancing%20the%0Amodel%27s%20ability%20to%20operate%20with%20low%20computational%20budget.%20Followed%20by%20an%0Aextension%20of%20the%20dual-stream%20architecture%20that%20incorporates%20modal%20interaction%0Ainto%20the%20multi-scale%20image-retrieval%20process%2C%20VeloxSeg%20efficiently%20models%0Aheterogeneous%20modalities.%20Finally%2C%20Spatially%20Decoupled%20Knowledge%20Transfer%0A%28SDKT%29%20via%20Gram%20matrices%20injects%20the%20texture%20prior%20extracted%20by%20a%0Aself-supervised%20network%20into%20the%20segmentation%20network%2C%20yielding%20stronger%0Arepresentations%20than%20baselines%20at%20no%20extra%20inference%20cost.%20Experimental%20results%0Aon%20multimodal%20benchmarks%20show%20that%20VeloxSeg%20achieves%20a%2026%25%20Dice%20improvement%2C%0Aalongside%20increasing%20GPU%20throughput%20by%2011x%20and%20CPU%20by%2048x.%20Codes%20are%20available%0Aat%20https%3A//github.com/JinPLu/VeloxSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22307v1&entry.124074799=Read"},
{"title": "RAU: Reference-based Anatomical Understanding with Vision Language\n  Models", "author": "Yiwei Li and Yikang Liu and Jiaqi Guo and Lin Zhao and Zheyuan Zhang and Xiao Chen and Boris Mailhe and Ankush Mukherjee and Terrence Chen and Shanhui Sun", "abstract": "  Anatomical understanding through deep learning is critical for automatic\nreport generation, intra-operative navigation, and organ localization in\nmedical imaging; however, its progress is constrained by the scarcity of\nexpert-labeled data. A promising remedy is to leverage an annotated reference\nimage to guide the interpretation of an unlabeled target. Although recent\nvision-language models (VLMs) exhibit non-trivial visual reasoning, their\nreference-based understanding and fine-grained localization remain limited. We\nintroduce RAU, a framework for reference-based anatomical understanding with\nVLMs. We first show that a VLM learns to identify anatomical regions through\nrelative spatial reasoning between reference and target images, trained on a\nmoderately sized dataset. We validate this capability through visual question\nanswering (VQA) and bounding box prediction. Next, we demonstrate that the\nVLM-derived spatial cues can be seamlessly integrated with the fine-grained\nsegmentation capability of SAM2, enabling localization and pixel-level\nsegmentation of small anatomical regions, such as vessel segments. Across two\nin-distribution and two out-of-distribution datasets, RAU consistently\noutperforms a SAM2 fine-tuning baseline using the same memory setup, yielding\nmore accurate segmentations and more reliable localization. More importantly,\nits strong generalization ability makes it scalable to out-of-distribution\ndatasets, a property crucial for medical image applications. To the best of our\nknowledge, RAU is the first to explore the capability of VLMs for\nreference-based identification, localization, and segmentation of anatomical\nstructures in medical images. Its promising performance highlights the\npotential of VLM-driven approaches for anatomical understanding in automated\nclinical workflows.\n", "link": "http://arxiv.org/abs/2509.22404v1", "date": "2025-09-26", "relevancy": 2.8737, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.58}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAU%3A%20Reference-based%20Anatomical%20Understanding%20with%20Vision%20Language%0A%20%20Models&body=Title%3A%20RAU%3A%20Reference-based%20Anatomical%20Understanding%20with%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Yiwei%20Li%20and%20Yikang%20Liu%20and%20Jiaqi%20Guo%20and%20Lin%20Zhao%20and%20Zheyuan%20Zhang%20and%20Xiao%20Chen%20and%20Boris%20Mailhe%20and%20Ankush%20Mukherjee%20and%20Terrence%20Chen%20and%20Shanhui%20Sun%0AAbstract%3A%20%20%20Anatomical%20understanding%20through%20deep%20learning%20is%20critical%20for%20automatic%0Areport%20generation%2C%20intra-operative%20navigation%2C%20and%20organ%20localization%20in%0Amedical%20imaging%3B%20however%2C%20its%20progress%20is%20constrained%20by%20the%20scarcity%20of%0Aexpert-labeled%20data.%20A%20promising%20remedy%20is%20to%20leverage%20an%20annotated%20reference%0Aimage%20to%20guide%20the%20interpretation%20of%20an%20unlabeled%20target.%20Although%20recent%0Avision-language%20models%20%28VLMs%29%20exhibit%20non-trivial%20visual%20reasoning%2C%20their%0Areference-based%20understanding%20and%20fine-grained%20localization%20remain%20limited.%20We%0Aintroduce%20RAU%2C%20a%20framework%20for%20reference-based%20anatomical%20understanding%20with%0AVLMs.%20We%20first%20show%20that%20a%20VLM%20learns%20to%20identify%20anatomical%20regions%20through%0Arelative%20spatial%20reasoning%20between%20reference%20and%20target%20images%2C%20trained%20on%20a%0Amoderately%20sized%20dataset.%20We%20validate%20this%20capability%20through%20visual%20question%0Aanswering%20%28VQA%29%20and%20bounding%20box%20prediction.%20Next%2C%20we%20demonstrate%20that%20the%0AVLM-derived%20spatial%20cues%20can%20be%20seamlessly%20integrated%20with%20the%20fine-grained%0Asegmentation%20capability%20of%20SAM2%2C%20enabling%20localization%20and%20pixel-level%0Asegmentation%20of%20small%20anatomical%20regions%2C%20such%20as%20vessel%20segments.%20Across%20two%0Ain-distribution%20and%20two%20out-of-distribution%20datasets%2C%20RAU%20consistently%0Aoutperforms%20a%20SAM2%20fine-tuning%20baseline%20using%20the%20same%20memory%20setup%2C%20yielding%0Amore%20accurate%20segmentations%20and%20more%20reliable%20localization.%20More%20importantly%2C%0Aits%20strong%20generalization%20ability%20makes%20it%20scalable%20to%20out-of-distribution%0Adatasets%2C%20a%20property%20crucial%20for%20medical%20image%20applications.%20To%20the%20best%20of%20our%0Aknowledge%2C%20RAU%20is%20the%20first%20to%20explore%20the%20capability%20of%20VLMs%20for%0Areference-based%20identification%2C%20localization%2C%20and%20segmentation%20of%20anatomical%0Astructures%20in%20medical%20images.%20Its%20promising%20performance%20highlights%20the%0Apotential%20of%20VLM-driven%20approaches%20for%20anatomical%20understanding%20in%20automated%0Aclinical%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22404v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAU%253A%2520Reference-based%2520Anatomical%2520Understanding%2520with%2520Vision%2520Language%250A%2520%2520Models%26entry.906535625%3DYiwei%2520Li%2520and%2520Yikang%2520Liu%2520and%2520Jiaqi%2520Guo%2520and%2520Lin%2520Zhao%2520and%2520Zheyuan%2520Zhang%2520and%2520Xiao%2520Chen%2520and%2520Boris%2520Mailhe%2520and%2520Ankush%2520Mukherjee%2520and%2520Terrence%2520Chen%2520and%2520Shanhui%2520Sun%26entry.1292438233%3D%2520%2520Anatomical%2520understanding%2520through%2520deep%2520learning%2520is%2520critical%2520for%2520automatic%250Areport%2520generation%252C%2520intra-operative%2520navigation%252C%2520and%2520organ%2520localization%2520in%250Amedical%2520imaging%253B%2520however%252C%2520its%2520progress%2520is%2520constrained%2520by%2520the%2520scarcity%2520of%250Aexpert-labeled%2520data.%2520A%2520promising%2520remedy%2520is%2520to%2520leverage%2520an%2520annotated%2520reference%250Aimage%2520to%2520guide%2520the%2520interpretation%2520of%2520an%2520unlabeled%2520target.%2520Although%2520recent%250Avision-language%2520models%2520%2528VLMs%2529%2520exhibit%2520non-trivial%2520visual%2520reasoning%252C%2520their%250Areference-based%2520understanding%2520and%2520fine-grained%2520localization%2520remain%2520limited.%2520We%250Aintroduce%2520RAU%252C%2520a%2520framework%2520for%2520reference-based%2520anatomical%2520understanding%2520with%250AVLMs.%2520We%2520first%2520show%2520that%2520a%2520VLM%2520learns%2520to%2520identify%2520anatomical%2520regions%2520through%250Arelative%2520spatial%2520reasoning%2520between%2520reference%2520and%2520target%2520images%252C%2520trained%2520on%2520a%250Amoderately%2520sized%2520dataset.%2520We%2520validate%2520this%2520capability%2520through%2520visual%2520question%250Aanswering%2520%2528VQA%2529%2520and%2520bounding%2520box%2520prediction.%2520Next%252C%2520we%2520demonstrate%2520that%2520the%250AVLM-derived%2520spatial%2520cues%2520can%2520be%2520seamlessly%2520integrated%2520with%2520the%2520fine-grained%250Asegmentation%2520capability%2520of%2520SAM2%252C%2520enabling%2520localization%2520and%2520pixel-level%250Asegmentation%2520of%2520small%2520anatomical%2520regions%252C%2520such%2520as%2520vessel%2520segments.%2520Across%2520two%250Ain-distribution%2520and%2520two%2520out-of-distribution%2520datasets%252C%2520RAU%2520consistently%250Aoutperforms%2520a%2520SAM2%2520fine-tuning%2520baseline%2520using%2520the%2520same%2520memory%2520setup%252C%2520yielding%250Amore%2520accurate%2520segmentations%2520and%2520more%2520reliable%2520localization.%2520More%2520importantly%252C%250Aits%2520strong%2520generalization%2520ability%2520makes%2520it%2520scalable%2520to%2520out-of-distribution%250Adatasets%252C%2520a%2520property%2520crucial%2520for%2520medical%2520image%2520applications.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520RAU%2520is%2520the%2520first%2520to%2520explore%2520the%2520capability%2520of%2520VLMs%2520for%250Areference-based%2520identification%252C%2520localization%252C%2520and%2520segmentation%2520of%2520anatomical%250Astructures%2520in%2520medical%2520images.%2520Its%2520promising%2520performance%2520highlights%2520the%250Apotential%2520of%2520VLM-driven%2520approaches%2520for%2520anatomical%2520understanding%2520in%2520automated%250Aclinical%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22404v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAU%3A%20Reference-based%20Anatomical%20Understanding%20with%20Vision%20Language%0A%20%20Models&entry.906535625=Yiwei%20Li%20and%20Yikang%20Liu%20and%20Jiaqi%20Guo%20and%20Lin%20Zhao%20and%20Zheyuan%20Zhang%20and%20Xiao%20Chen%20and%20Boris%20Mailhe%20and%20Ankush%20Mukherjee%20and%20Terrence%20Chen%20and%20Shanhui%20Sun&entry.1292438233=%20%20Anatomical%20understanding%20through%20deep%20learning%20is%20critical%20for%20automatic%0Areport%20generation%2C%20intra-operative%20navigation%2C%20and%20organ%20localization%20in%0Amedical%20imaging%3B%20however%2C%20its%20progress%20is%20constrained%20by%20the%20scarcity%20of%0Aexpert-labeled%20data.%20A%20promising%20remedy%20is%20to%20leverage%20an%20annotated%20reference%0Aimage%20to%20guide%20the%20interpretation%20of%20an%20unlabeled%20target.%20Although%20recent%0Avision-language%20models%20%28VLMs%29%20exhibit%20non-trivial%20visual%20reasoning%2C%20their%0Areference-based%20understanding%20and%20fine-grained%20localization%20remain%20limited.%20We%0Aintroduce%20RAU%2C%20a%20framework%20for%20reference-based%20anatomical%20understanding%20with%0AVLMs.%20We%20first%20show%20that%20a%20VLM%20learns%20to%20identify%20anatomical%20regions%20through%0Arelative%20spatial%20reasoning%20between%20reference%20and%20target%20images%2C%20trained%20on%20a%0Amoderately%20sized%20dataset.%20We%20validate%20this%20capability%20through%20visual%20question%0Aanswering%20%28VQA%29%20and%20bounding%20box%20prediction.%20Next%2C%20we%20demonstrate%20that%20the%0AVLM-derived%20spatial%20cues%20can%20be%20seamlessly%20integrated%20with%20the%20fine-grained%0Asegmentation%20capability%20of%20SAM2%2C%20enabling%20localization%20and%20pixel-level%0Asegmentation%20of%20small%20anatomical%20regions%2C%20such%20as%20vessel%20segments.%20Across%20two%0Ain-distribution%20and%20two%20out-of-distribution%20datasets%2C%20RAU%20consistently%0Aoutperforms%20a%20SAM2%20fine-tuning%20baseline%20using%20the%20same%20memory%20setup%2C%20yielding%0Amore%20accurate%20segmentations%20and%20more%20reliable%20localization.%20More%20importantly%2C%0Aits%20strong%20generalization%20ability%20makes%20it%20scalable%20to%20out-of-distribution%0Adatasets%2C%20a%20property%20crucial%20for%20medical%20image%20applications.%20To%20the%20best%20of%20our%0Aknowledge%2C%20RAU%20is%20the%20first%20to%20explore%20the%20capability%20of%20VLMs%20for%0Areference-based%20identification%2C%20localization%2C%20and%20segmentation%20of%20anatomical%0Astructures%20in%20medical%20images.%20Its%20promising%20performance%20highlights%20the%0Apotential%20of%20VLM-driven%20approaches%20for%20anatomical%20understanding%20in%20automated%0Aclinical%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22404v1&entry.124074799=Read"},
{"title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned\n  Aerial Navigation", "author": "Chih Yao Hu and Yang-Sen Lin and Yuna Lee and Chih-Hai Su and Jie-Ying Lee and Shr-Ruei Tsai and Chin-Yang Lin and Kuan-Wen Chen and Tsung-Wei Ke and Yu-Lun Liu", "abstract": "  We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev\n", "link": "http://arxiv.org/abs/2509.22653v1", "date": "2025-09-26", "relevancy": 2.8669, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%2C%20Point%2C%20Fly%3A%20A%20Learning-Free%20VLM%20Framework%20for%20Universal%20Unmanned%0A%20%20Aerial%20Navigation&body=Title%3A%20See%2C%20Point%2C%20Fly%3A%20A%20Learning-Free%20VLM%20Framework%20for%20Universal%20Unmanned%0A%20%20Aerial%20Navigation%0AAuthor%3A%20Chih%20Yao%20Hu%20and%20Yang-Sen%20Lin%20and%20Yuna%20Lee%20and%20Chih-Hai%20Su%20and%20Jie-Ying%20Lee%20and%20Shr-Ruei%20Tsai%20and%20Chin-Yang%20Lin%20and%20Kuan-Wen%20Chen%20and%20Tsung-Wei%20Ke%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20We%20present%20See%2C%20Point%2C%20Fly%20%28SPF%29%2C%20a%20training-free%20aerial%20vision-and-language%0Anavigation%20%28AVLN%29%20framework%20built%20atop%20vision-language%20models%20%28VLMs%29.%20SPF%20is%0Acapable%20of%20navigating%20to%20any%20goal%20based%20on%20any%20type%20of%20free-form%20instructions%0Ain%20any%20kind%20of%20environment.%20In%20contrast%20to%20existing%20VLM-based%20approaches%20that%0Atreat%20action%20prediction%20as%20a%20text%20generation%20task%2C%20our%20key%20insight%20is%20to%0Aconsider%20action%20prediction%20for%20AVLN%20as%20a%202D%20spatial%20grounding%20task.%20SPF%0Aharnesses%20VLMs%20to%20decompose%20vague%20language%20instructions%20into%20iterative%0Aannotation%20of%202D%20waypoints%20on%20the%20input%20image.%20Along%20with%20the%20predicted%0Atraveling%20distance%2C%20SPF%20transforms%20predicted%202D%20waypoints%20into%203D%20displacement%0Avectors%20as%20action%20commands%20for%20UAVs.%20Moreover%2C%20SPF%20also%20adaptively%20adjusts%20the%0Atraveling%20distance%20to%20facilitate%20more%20efficient%20navigation.%20Notably%2C%20SPF%0Aperforms%20navigation%20in%20a%20closed-loop%20control%20manner%2C%20enabling%20UAVs%20to%20follow%0Adynamic%20targets%20in%20dynamic%20environments.%20SPF%20sets%20a%20new%20state%20of%20the%20art%20in%20DRL%0Asimulation%20benchmark%2C%20outperforming%20the%20previous%20best%20method%20by%20an%20absolute%0Amargin%20of%2063%25.%20In%20extensive%20real-world%20evaluations%2C%20SPF%20outperforms%20strong%0Abaselines%20by%20a%20large%20margin.%20We%20also%20conduct%20comprehensive%20ablation%20studies%20to%0Ahighlight%20the%20effectiveness%20of%20our%20design%20choice.%20Lastly%2C%20SPF%20shows%20remarkable%0Ageneralization%20to%20different%20VLMs.%20Project%20page%3A%20https%3A//spf-web.pages.dev%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%252C%2520Point%252C%2520Fly%253A%2520A%2520Learning-Free%2520VLM%2520Framework%2520for%2520Universal%2520Unmanned%250A%2520%2520Aerial%2520Navigation%26entry.906535625%3DChih%2520Yao%2520Hu%2520and%2520Yang-Sen%2520Lin%2520and%2520Yuna%2520Lee%2520and%2520Chih-Hai%2520Su%2520and%2520Jie-Ying%2520Lee%2520and%2520Shr-Ruei%2520Tsai%2520and%2520Chin-Yang%2520Lin%2520and%2520Kuan-Wen%2520Chen%2520and%2520Tsung-Wei%2520Ke%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520See%252C%2520Point%252C%2520Fly%2520%2528SPF%2529%252C%2520a%2520training-free%2520aerial%2520vision-and-language%250Anavigation%2520%2528AVLN%2529%2520framework%2520built%2520atop%2520vision-language%2520models%2520%2528VLMs%2529.%2520SPF%2520is%250Acapable%2520of%2520navigating%2520to%2520any%2520goal%2520based%2520on%2520any%2520type%2520of%2520free-form%2520instructions%250Ain%2520any%2520kind%2520of%2520environment.%2520In%2520contrast%2520to%2520existing%2520VLM-based%2520approaches%2520that%250Atreat%2520action%2520prediction%2520as%2520a%2520text%2520generation%2520task%252C%2520our%2520key%2520insight%2520is%2520to%250Aconsider%2520action%2520prediction%2520for%2520AVLN%2520as%2520a%25202D%2520spatial%2520grounding%2520task.%2520SPF%250Aharnesses%2520VLMs%2520to%2520decompose%2520vague%2520language%2520instructions%2520into%2520iterative%250Aannotation%2520of%25202D%2520waypoints%2520on%2520the%2520input%2520image.%2520Along%2520with%2520the%2520predicted%250Atraveling%2520distance%252C%2520SPF%2520transforms%2520predicted%25202D%2520waypoints%2520into%25203D%2520displacement%250Avectors%2520as%2520action%2520commands%2520for%2520UAVs.%2520Moreover%252C%2520SPF%2520also%2520adaptively%2520adjusts%2520the%250Atraveling%2520distance%2520to%2520facilitate%2520more%2520efficient%2520navigation.%2520Notably%252C%2520SPF%250Aperforms%2520navigation%2520in%2520a%2520closed-loop%2520control%2520manner%252C%2520enabling%2520UAVs%2520to%2520follow%250Adynamic%2520targets%2520in%2520dynamic%2520environments.%2520SPF%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520in%2520DRL%250Asimulation%2520benchmark%252C%2520outperforming%2520the%2520previous%2520best%2520method%2520by%2520an%2520absolute%250Amargin%2520of%252063%2525.%2520In%2520extensive%2520real-world%2520evaluations%252C%2520SPF%2520outperforms%2520strong%250Abaselines%2520by%2520a%2520large%2520margin.%2520We%2520also%2520conduct%2520comprehensive%2520ablation%2520studies%2520to%250Ahighlight%2520the%2520effectiveness%2520of%2520our%2520design%2520choice.%2520Lastly%252C%2520SPF%2520shows%2520remarkable%250Ageneralization%2520to%2520different%2520VLMs.%2520Project%2520page%253A%2520https%253A//spf-web.pages.dev%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%2C%20Point%2C%20Fly%3A%20A%20Learning-Free%20VLM%20Framework%20for%20Universal%20Unmanned%0A%20%20Aerial%20Navigation&entry.906535625=Chih%20Yao%20Hu%20and%20Yang-Sen%20Lin%20and%20Yuna%20Lee%20and%20Chih-Hai%20Su%20and%20Jie-Ying%20Lee%20and%20Shr-Ruei%20Tsai%20and%20Chin-Yang%20Lin%20and%20Kuan-Wen%20Chen%20and%20Tsung-Wei%20Ke%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20We%20present%20See%2C%20Point%2C%20Fly%20%28SPF%29%2C%20a%20training-free%20aerial%20vision-and-language%0Anavigation%20%28AVLN%29%20framework%20built%20atop%20vision-language%20models%20%28VLMs%29.%20SPF%20is%0Acapable%20of%20navigating%20to%20any%20goal%20based%20on%20any%20type%20of%20free-form%20instructions%0Ain%20any%20kind%20of%20environment.%20In%20contrast%20to%20existing%20VLM-based%20approaches%20that%0Atreat%20action%20prediction%20as%20a%20text%20generation%20task%2C%20our%20key%20insight%20is%20to%0Aconsider%20action%20prediction%20for%20AVLN%20as%20a%202D%20spatial%20grounding%20task.%20SPF%0Aharnesses%20VLMs%20to%20decompose%20vague%20language%20instructions%20into%20iterative%0Aannotation%20of%202D%20waypoints%20on%20the%20input%20image.%20Along%20with%20the%20predicted%0Atraveling%20distance%2C%20SPF%20transforms%20predicted%202D%20waypoints%20into%203D%20displacement%0Avectors%20as%20action%20commands%20for%20UAVs.%20Moreover%2C%20SPF%20also%20adaptively%20adjusts%20the%0Atraveling%20distance%20to%20facilitate%20more%20efficient%20navigation.%20Notably%2C%20SPF%0Aperforms%20navigation%20in%20a%20closed-loop%20control%20manner%2C%20enabling%20UAVs%20to%20follow%0Adynamic%20targets%20in%20dynamic%20environments.%20SPF%20sets%20a%20new%20state%20of%20the%20art%20in%20DRL%0Asimulation%20benchmark%2C%20outperforming%20the%20previous%20best%20method%20by%20an%20absolute%0Amargin%20of%2063%25.%20In%20extensive%20real-world%20evaluations%2C%20SPF%20outperforms%20strong%0Abaselines%20by%20a%20large%20margin.%20We%20also%20conduct%20comprehensive%20ablation%20studies%20to%0Ahighlight%20the%20effectiveness%20of%20our%20design%20choice.%20Lastly%2C%20SPF%20shows%20remarkable%0Ageneralization%20to%20different%20VLMs.%20Project%20page%3A%20https%3A//spf-web.pages.dev%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22653v1&entry.124074799=Read"},
{"title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning", "author": "Long Xing and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Jianze Liang and Qidong Huang and Jiaqi Wang and Feng Wu and Dahua Lin", "abstract": "  Image captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-training Large Vision-Language Models\n(LVLMs). Current state-of-the-art captioning models are typically trained with\nSupervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduce\nCaptioning Reinforcement Learning (CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answering Multiple-Choice Questions based solely on\nthat caption. As the first study to apply RLVR to the subjective image\ncaptioning task, we demonstrate that CapRL significantly enhances multiple\nsettings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B\nresults in substantial gains across 12 benchmarks. Moreover, within the Prism\nFramework for caption quality evaluation, CapRL achieves performance comparable\nto Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.\nCode is available here: https://github.com/InternLM/CapRL.\n", "link": "http://arxiv.org/abs/2509.22647v1", "date": "2025-09-26", "relevancy": 2.8348, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CapRL%3A%20Stimulating%20Dense%20Image%20Caption%20Capabilities%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20CapRL%3A%20Stimulating%20Dense%20Image%20Caption%20Capabilities%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Long%20Xing%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Jianze%20Liang%20and%20Qidong%20Huang%20and%20Jiaqi%20Wang%20and%20Feng%20Wu%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Image%20captioning%20is%20a%20fundamental%20task%20that%20bridges%20the%20visual%20and%20linguistic%0Adomains%2C%20playing%20a%20critical%20role%20in%20pre-training%20Large%20Vision-Language%20Models%0A%28LVLMs%29.%20Current%20state-of-the-art%20captioning%20models%20are%20typically%20trained%20with%0ASupervised%20Fine-Tuning%20%28SFT%29%2C%20a%20paradigm%20that%20relies%20on%20expensive%2C%20non-scalable%0Adata%20annotated%20by%20humans%20or%20proprietary%20models.%20This%20approach%20often%20leads%20to%0Amodels%20that%20memorize%20specific%20ground-truth%20answers%2C%20limiting%20their%20generality%0Aand%20ability%20to%20generate%20diverse%2C%20creative%20descriptions.%20To%20overcome%20the%0Alimitation%20of%20SFT%2C%20we%20propose%20applying%20the%20Reinforcement%20Learning%20with%0AVerifiable%20Rewards%20%28RLVR%29%20paradigm%20to%20the%20open-ended%20task%20of%20image%20captioning.%0AA%20primary%20challenge%2C%20however%2C%20is%20designing%20an%20objective%20reward%20function%20for%20the%0Ainherently%20subjective%20nature%20of%20what%20constitutes%20a%20%22good%22%20caption.%20We%20introduce%0ACaptioning%20Reinforcement%20Learning%20%28CapRL%29%2C%20a%20novel%20training%20framework%20that%0Aredefines%20caption%20quality%20through%20its%20utility%3A%20a%20high-quality%20caption%20should%0Aenable%20a%20non-visual%20language%20model%20to%20accurately%20answer%20questions%20about%20the%0Acorresponding%20image.%20CapRL%20employs%20a%20decoupled%20two-stage%20pipeline%20where%20an%20LVLM%0Agenerates%20a%20caption%2C%20and%20the%20objective%20reward%20is%20derived%20from%20the%20accuracy%20of%20a%0Aseparate%2C%20vision-free%20LLM%20answering%20Multiple-Choice%20Questions%20based%20solely%20on%0Athat%20caption.%20As%20the%20first%20study%20to%20apply%20RLVR%20to%20the%20subjective%20image%0Acaptioning%20task%2C%20we%20demonstrate%20that%20CapRL%20significantly%20enhances%20multiple%0Asettings.%20Pretraining%20on%20the%20CapRL-5M%20caption%20dataset%20annotated%20by%20CapRL-3B%0Aresults%20in%20substantial%20gains%20across%2012%20benchmarks.%20Moreover%2C%20within%20the%20Prism%0AFramework%20for%20caption%20quality%20evaluation%2C%20CapRL%20achieves%20performance%20comparable%0Ato%20Qwen2.5-VL-72B%2C%20while%20exceeding%20the%20baseline%20by%20an%20average%20margin%20of%208.4%25.%0ACode%20is%20available%20here%3A%20https%3A//github.com/InternLM/CapRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapRL%253A%2520Stimulating%2520Dense%2520Image%2520Caption%2520Capabilities%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DLong%2520Xing%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Jianze%2520Liang%2520and%2520Qidong%2520Huang%2520and%2520Jiaqi%2520Wang%2520and%2520Feng%2520Wu%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520Image%2520captioning%2520is%2520a%2520fundamental%2520task%2520that%2520bridges%2520the%2520visual%2520and%2520linguistic%250Adomains%252C%2520playing%2520a%2520critical%2520role%2520in%2520pre-training%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529.%2520Current%2520state-of-the-art%2520captioning%2520models%2520are%2520typically%2520trained%2520with%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%252C%2520a%2520paradigm%2520that%2520relies%2520on%2520expensive%252C%2520non-scalable%250Adata%2520annotated%2520by%2520humans%2520or%2520proprietary%2520models.%2520This%2520approach%2520often%2520leads%2520to%250Amodels%2520that%2520memorize%2520specific%2520ground-truth%2520answers%252C%2520limiting%2520their%2520generality%250Aand%2520ability%2520to%2520generate%2520diverse%252C%2520creative%2520descriptions.%2520To%2520overcome%2520the%250Alimitation%2520of%2520SFT%252C%2520we%2520propose%2520applying%2520the%2520Reinforcement%2520Learning%2520with%250AVerifiable%2520Rewards%2520%2528RLVR%2529%2520paradigm%2520to%2520the%2520open-ended%2520task%2520of%2520image%2520captioning.%250AA%2520primary%2520challenge%252C%2520however%252C%2520is%2520designing%2520an%2520objective%2520reward%2520function%2520for%2520the%250Ainherently%2520subjective%2520nature%2520of%2520what%2520constitutes%2520a%2520%2522good%2522%2520caption.%2520We%2520introduce%250ACaptioning%2520Reinforcement%2520Learning%2520%2528CapRL%2529%252C%2520a%2520novel%2520training%2520framework%2520that%250Aredefines%2520caption%2520quality%2520through%2520its%2520utility%253A%2520a%2520high-quality%2520caption%2520should%250Aenable%2520a%2520non-visual%2520language%2520model%2520to%2520accurately%2520answer%2520questions%2520about%2520the%250Acorresponding%2520image.%2520CapRL%2520employs%2520a%2520decoupled%2520two-stage%2520pipeline%2520where%2520an%2520LVLM%250Agenerates%2520a%2520caption%252C%2520and%2520the%2520objective%2520reward%2520is%2520derived%2520from%2520the%2520accuracy%2520of%2520a%250Aseparate%252C%2520vision-free%2520LLM%2520answering%2520Multiple-Choice%2520Questions%2520based%2520solely%2520on%250Athat%2520caption.%2520As%2520the%2520first%2520study%2520to%2520apply%2520RLVR%2520to%2520the%2520subjective%2520image%250Acaptioning%2520task%252C%2520we%2520demonstrate%2520that%2520CapRL%2520significantly%2520enhances%2520multiple%250Asettings.%2520Pretraining%2520on%2520the%2520CapRL-5M%2520caption%2520dataset%2520annotated%2520by%2520CapRL-3B%250Aresults%2520in%2520substantial%2520gains%2520across%252012%2520benchmarks.%2520Moreover%252C%2520within%2520the%2520Prism%250AFramework%2520for%2520caption%2520quality%2520evaluation%252C%2520CapRL%2520achieves%2520performance%2520comparable%250Ato%2520Qwen2.5-VL-72B%252C%2520while%2520exceeding%2520the%2520baseline%2520by%2520an%2520average%2520margin%2520of%25208.4%2525.%250ACode%2520is%2520available%2520here%253A%2520https%253A//github.com/InternLM/CapRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CapRL%3A%20Stimulating%20Dense%20Image%20Caption%20Capabilities%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Long%20Xing%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Jianze%20Liang%20and%20Qidong%20Huang%20and%20Jiaqi%20Wang%20and%20Feng%20Wu%20and%20Dahua%20Lin&entry.1292438233=%20%20Image%20captioning%20is%20a%20fundamental%20task%20that%20bridges%20the%20visual%20and%20linguistic%0Adomains%2C%20playing%20a%20critical%20role%20in%20pre-training%20Large%20Vision-Language%20Models%0A%28LVLMs%29.%20Current%20state-of-the-art%20captioning%20models%20are%20typically%20trained%20with%0ASupervised%20Fine-Tuning%20%28SFT%29%2C%20a%20paradigm%20that%20relies%20on%20expensive%2C%20non-scalable%0Adata%20annotated%20by%20humans%20or%20proprietary%20models.%20This%20approach%20often%20leads%20to%0Amodels%20that%20memorize%20specific%20ground-truth%20answers%2C%20limiting%20their%20generality%0Aand%20ability%20to%20generate%20diverse%2C%20creative%20descriptions.%20To%20overcome%20the%0Alimitation%20of%20SFT%2C%20we%20propose%20applying%20the%20Reinforcement%20Learning%20with%0AVerifiable%20Rewards%20%28RLVR%29%20paradigm%20to%20the%20open-ended%20task%20of%20image%20captioning.%0AA%20primary%20challenge%2C%20however%2C%20is%20designing%20an%20objective%20reward%20function%20for%20the%0Ainherently%20subjective%20nature%20of%20what%20constitutes%20a%20%22good%22%20caption.%20We%20introduce%0ACaptioning%20Reinforcement%20Learning%20%28CapRL%29%2C%20a%20novel%20training%20framework%20that%0Aredefines%20caption%20quality%20through%20its%20utility%3A%20a%20high-quality%20caption%20should%0Aenable%20a%20non-visual%20language%20model%20to%20accurately%20answer%20questions%20about%20the%0Acorresponding%20image.%20CapRL%20employs%20a%20decoupled%20two-stage%20pipeline%20where%20an%20LVLM%0Agenerates%20a%20caption%2C%20and%20the%20objective%20reward%20is%20derived%20from%20the%20accuracy%20of%20a%0Aseparate%2C%20vision-free%20LLM%20answering%20Multiple-Choice%20Questions%20based%20solely%20on%0Athat%20caption.%20As%20the%20first%20study%20to%20apply%20RLVR%20to%20the%20subjective%20image%0Acaptioning%20task%2C%20we%20demonstrate%20that%20CapRL%20significantly%20enhances%20multiple%0Asettings.%20Pretraining%20on%20the%20CapRL-5M%20caption%20dataset%20annotated%20by%20CapRL-3B%0Aresults%20in%20substantial%20gains%20across%2012%20benchmarks.%20Moreover%2C%20within%20the%20Prism%0AFramework%20for%20caption%20quality%20evaluation%2C%20CapRL%20achieves%20performance%20comparable%0Ato%20Qwen2.5-VL-72B%2C%20while%20exceeding%20the%20baseline%20by%20an%20average%20margin%20of%208.4%25.%0ACode%20is%20available%20here%3A%20https%3A//github.com/InternLM/CapRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22647v1&entry.124074799=Read"},
{"title": "LEO-VL: Efficient Scene Representation for Scalable 3D Vision-Language\n  Learning", "author": "Jiangyong Huang and Xiaojian Ma and Xiongkun Linghu and Yue Fan and Junchao He and Wenxin Tan and Qing Li and Song-Chun Zhu and Yixin Chen and Baoxiong Jia and Siyuan Huang", "abstract": "  Developing vision-language models (VLMs) capable of understanding 3D scenes\nhas been a longstanding goal in the 3D-VL community. Despite recent progress,\n3D VLMs still fall short of their 2D counterparts in capability and robustness.\nA key bottleneck is that current scene representations struggle to balance\nperformance and efficiency: competitive performance comes at the cost of heavy\ntoken overhead, which in turn hampers the scalability of 3D-VL learning. To\naddress this, we propose the condensed feature grid (CFG), an efficient scene\nrepresentation featuring significantly reduced token overhead and strong\nperception capability. Building on CFG, we introduce LEO-VL, a 3D VLM trained\non 700k 3D-VL data spanning four real-world indoor domains and five tasks such\nas captioning and dialogue. To enhance the robustness of 3D VLM, we further\npropose SceneDPO for post-training, which involves contrasts across answers and\nscenes. LEO-VL achieves state-of-the-art performance on various 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Our extensive experiments\nhighlight the efficiency of our representation, the benefit of task and scene\ndiversity, consistent scaling effects, and the advantages of SceneDPO compared\nto SFT and GRPO. We hope our findings advance the efficiency, scalability, and\nrobustness of future 3D VLMs.\n", "link": "http://arxiv.org/abs/2506.09935v2", "date": "2025-09-26", "relevancy": 2.8327, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7312}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEO-VL%3A%20Efficient%20Scene%20Representation%20for%20Scalable%203D%20Vision-Language%0A%20%20Learning&body=Title%3A%20LEO-VL%3A%20Efficient%20Scene%20Representation%20for%20Scalable%203D%20Vision-Language%0A%20%20Learning%0AAuthor%3A%20Jiangyong%20Huang%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Yue%20Fan%20and%20Junchao%20He%20and%20Wenxin%20Tan%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Developing%20vision-language%20models%20%28VLMs%29%20capable%20of%20understanding%203D%20scenes%0Ahas%20been%20a%20longstanding%20goal%20in%20the%203D-VL%20community.%20Despite%20recent%20progress%2C%0A3D%20VLMs%20still%20fall%20short%20of%20their%202D%20counterparts%20in%20capability%20and%20robustness.%0AA%20key%20bottleneck%20is%20that%20current%20scene%20representations%20struggle%20to%20balance%0Aperformance%20and%20efficiency%3A%20competitive%20performance%20comes%20at%20the%20cost%20of%20heavy%0Atoken%20overhead%2C%20which%20in%20turn%20hampers%20the%20scalability%20of%203D-VL%20learning.%20To%0Aaddress%20this%2C%20we%20propose%20the%20condensed%20feature%20grid%20%28CFG%29%2C%20an%20efficient%20scene%0Arepresentation%20featuring%20significantly%20reduced%20token%20overhead%20and%20strong%0Aperception%20capability.%20Building%20on%20CFG%2C%20we%20introduce%20LEO-VL%2C%20a%203D%20VLM%20trained%0Aon%20700k%203D-VL%20data%20spanning%20four%20real-world%20indoor%20domains%20and%20five%20tasks%20such%0Aas%20captioning%20and%20dialogue.%20To%20enhance%20the%20robustness%20of%203D%20VLM%2C%20we%20further%0Apropose%20SceneDPO%20for%20post-training%2C%20which%20involves%20contrasts%20across%20answers%20and%0Ascenes.%20LEO-VL%20achieves%20state-of-the-art%20performance%20on%20various%203D%20QA%0Abenchmarks%2C%20including%20SQA3D%2C%20MSQA%2C%20and%20Beacon3D.%20Our%20extensive%20experiments%0Ahighlight%20the%20efficiency%20of%20our%20representation%2C%20the%20benefit%20of%20task%20and%20scene%0Adiversity%2C%20consistent%20scaling%20effects%2C%20and%20the%20advantages%20of%20SceneDPO%20compared%0Ato%20SFT%20and%20GRPO.%20We%20hope%20our%20findings%20advance%20the%20efficiency%2C%20scalability%2C%20and%0Arobustness%20of%20future%203D%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEO-VL%253A%2520Efficient%2520Scene%2520Representation%2520for%2520Scalable%25203D%2520Vision-Language%250A%2520%2520Learning%26entry.906535625%3DJiangyong%2520Huang%2520and%2520Xiaojian%2520Ma%2520and%2520Xiongkun%2520Linghu%2520and%2520Yue%2520Fan%2520and%2520Junchao%2520He%2520and%2520Wenxin%2520Tan%2520and%2520Qing%2520Li%2520and%2520Song-Chun%2520Zhu%2520and%2520Yixin%2520Chen%2520and%2520Baoxiong%2520Jia%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Developing%2520vision-language%2520models%2520%2528VLMs%2529%2520capable%2520of%2520understanding%25203D%2520scenes%250Ahas%2520been%2520a%2520longstanding%2520goal%2520in%2520the%25203D-VL%2520community.%2520Despite%2520recent%2520progress%252C%250A3D%2520VLMs%2520still%2520fall%2520short%2520of%2520their%25202D%2520counterparts%2520in%2520capability%2520and%2520robustness.%250AA%2520key%2520bottleneck%2520is%2520that%2520current%2520scene%2520representations%2520struggle%2520to%2520balance%250Aperformance%2520and%2520efficiency%253A%2520competitive%2520performance%2520comes%2520at%2520the%2520cost%2520of%2520heavy%250Atoken%2520overhead%252C%2520which%2520in%2520turn%2520hampers%2520the%2520scalability%2520of%25203D-VL%2520learning.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520the%2520condensed%2520feature%2520grid%2520%2528CFG%2529%252C%2520an%2520efficient%2520scene%250Arepresentation%2520featuring%2520significantly%2520reduced%2520token%2520overhead%2520and%2520strong%250Aperception%2520capability.%2520Building%2520on%2520CFG%252C%2520we%2520introduce%2520LEO-VL%252C%2520a%25203D%2520VLM%2520trained%250Aon%2520700k%25203D-VL%2520data%2520spanning%2520four%2520real-world%2520indoor%2520domains%2520and%2520five%2520tasks%2520such%250Aas%2520captioning%2520and%2520dialogue.%2520To%2520enhance%2520the%2520robustness%2520of%25203D%2520VLM%252C%2520we%2520further%250Apropose%2520SceneDPO%2520for%2520post-training%252C%2520which%2520involves%2520contrasts%2520across%2520answers%2520and%250Ascenes.%2520LEO-VL%2520achieves%2520state-of-the-art%2520performance%2520on%2520various%25203D%2520QA%250Abenchmarks%252C%2520including%2520SQA3D%252C%2520MSQA%252C%2520and%2520Beacon3D.%2520Our%2520extensive%2520experiments%250Ahighlight%2520the%2520efficiency%2520of%2520our%2520representation%252C%2520the%2520benefit%2520of%2520task%2520and%2520scene%250Adiversity%252C%2520consistent%2520scaling%2520effects%252C%2520and%2520the%2520advantages%2520of%2520SceneDPO%2520compared%250Ato%2520SFT%2520and%2520GRPO.%2520We%2520hope%2520our%2520findings%2520advance%2520the%2520efficiency%252C%2520scalability%252C%2520and%250Arobustness%2520of%2520future%25203D%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEO-VL%3A%20Efficient%20Scene%20Representation%20for%20Scalable%203D%20Vision-Language%0A%20%20Learning&entry.906535625=Jiangyong%20Huang%20and%20Xiaojian%20Ma%20and%20Xiongkun%20Linghu%20and%20Yue%20Fan%20and%20Junchao%20He%20and%20Wenxin%20Tan%20and%20Qing%20Li%20and%20Song-Chun%20Zhu%20and%20Yixin%20Chen%20and%20Baoxiong%20Jia%20and%20Siyuan%20Huang&entry.1292438233=%20%20Developing%20vision-language%20models%20%28VLMs%29%20capable%20of%20understanding%203D%20scenes%0Ahas%20been%20a%20longstanding%20goal%20in%20the%203D-VL%20community.%20Despite%20recent%20progress%2C%0A3D%20VLMs%20still%20fall%20short%20of%20their%202D%20counterparts%20in%20capability%20and%20robustness.%0AA%20key%20bottleneck%20is%20that%20current%20scene%20representations%20struggle%20to%20balance%0Aperformance%20and%20efficiency%3A%20competitive%20performance%20comes%20at%20the%20cost%20of%20heavy%0Atoken%20overhead%2C%20which%20in%20turn%20hampers%20the%20scalability%20of%203D-VL%20learning.%20To%0Aaddress%20this%2C%20we%20propose%20the%20condensed%20feature%20grid%20%28CFG%29%2C%20an%20efficient%20scene%0Arepresentation%20featuring%20significantly%20reduced%20token%20overhead%20and%20strong%0Aperception%20capability.%20Building%20on%20CFG%2C%20we%20introduce%20LEO-VL%2C%20a%203D%20VLM%20trained%0Aon%20700k%203D-VL%20data%20spanning%20four%20real-world%20indoor%20domains%20and%20five%20tasks%20such%0Aas%20captioning%20and%20dialogue.%20To%20enhance%20the%20robustness%20of%203D%20VLM%2C%20we%20further%0Apropose%20SceneDPO%20for%20post-training%2C%20which%20involves%20contrasts%20across%20answers%20and%0Ascenes.%20LEO-VL%20achieves%20state-of-the-art%20performance%20on%20various%203D%20QA%0Abenchmarks%2C%20including%20SQA3D%2C%20MSQA%2C%20and%20Beacon3D.%20Our%20extensive%20experiments%0Ahighlight%20the%20efficiency%20of%20our%20representation%2C%20the%20benefit%20of%20task%20and%20scene%0Adiversity%2C%20consistent%20scaling%20effects%2C%20and%20the%20advantages%20of%20SceneDPO%20compared%0Ato%20SFT%20and%20GRPO.%20We%20hope%20our%20findings%20advance%20the%20efficiency%2C%20scalability%2C%20and%0Arobustness%20of%20future%203D%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09935v2&entry.124074799=Read"},
{"title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long\n  Video Generation", "author": "X. Feng and H. Yu and M. Wu and S. Hu and J. Chen and C. Zhu and J. Wu and X. Chu and K. Huang", "abstract": "  With the rapid development of foundation video generation technologies, long\nvideo generation models have exhibited promising research potential thanks to\nexpanded content creation space. Recent studies reveal that the goal of long\nvideo generation tasks is not only to extend video duration but also to\naccurately express richer narrative content within longer videos. However, due\nto the lack of evaluation benchmarks specifically designed for long video\ngeneration models, the current assessment of these models primarily relies on\nbenchmarks with simple narrative prompts (e.g., VBench). To the best of our\nknowledge, our proposed NarrLV is the first benchmark to comprehensively\nevaluate the Narrative expression capabilities of Long Video generation models.\nInspired by film narrative theory, (i) we first introduce the basic narrative\nunit maintaining continuous visual presentation in videos as Temporal Narrative\nAtom (TNA), and use its count to quantitatively measure narrative richness.\nGuided by three key film narrative elements influencing TNA changes, we\nconstruct an automatic prompt generation pipeline capable of producing\nevaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based\non the three progressive levels of narrative content expression, we design an\neffective evaluation metric using the MLLM-based question generation and\nanswering framework. (iii) Finally, we conduct extensive evaluations on\nexisting long video generation models and the foundation generation models.\nExperimental results demonstrate that our metric aligns closely with human\njudgments. The derived evaluation outcomes reveal the detailed capability\nboundaries of current video generation models in narrative content expression.\n", "link": "http://arxiv.org/abs/2507.11245v3", "date": "2025-09-26", "relevancy": 2.8198, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6033}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NarrLV%3A%20Towards%20a%20Comprehensive%20Narrative-Centric%20Evaluation%20for%20Long%0A%20%20Video%20Generation&body=Title%3A%20NarrLV%3A%20Towards%20a%20Comprehensive%20Narrative-Centric%20Evaluation%20for%20Long%0A%20%20Video%20Generation%0AAuthor%3A%20X.%20Feng%20and%20H.%20Yu%20and%20M.%20Wu%20and%20S.%20Hu%20and%20J.%20Chen%20and%20C.%20Zhu%20and%20J.%20Wu%20and%20X.%20Chu%20and%20K.%20Huang%0AAbstract%3A%20%20%20With%20the%20rapid%20development%20of%20foundation%20video%20generation%20technologies%2C%20long%0Avideo%20generation%20models%20have%20exhibited%20promising%20research%20potential%20thanks%20to%0Aexpanded%20content%20creation%20space.%20Recent%20studies%20reveal%20that%20the%20goal%20of%20long%0Avideo%20generation%20tasks%20is%20not%20only%20to%20extend%20video%20duration%20but%20also%20to%0Aaccurately%20express%20richer%20narrative%20content%20within%20longer%20videos.%20However%2C%20due%0Ato%20the%20lack%20of%20evaluation%20benchmarks%20specifically%20designed%20for%20long%20video%0Ageneration%20models%2C%20the%20current%20assessment%20of%20these%20models%20primarily%20relies%20on%0Abenchmarks%20with%20simple%20narrative%20prompts%20%28e.g.%2C%20VBench%29.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20proposed%20NarrLV%20is%20the%20first%20benchmark%20to%20comprehensively%0Aevaluate%20the%20Narrative%20expression%20capabilities%20of%20Long%20Video%20generation%20models.%0AInspired%20by%20film%20narrative%20theory%2C%20%28i%29%20we%20first%20introduce%20the%20basic%20narrative%0Aunit%20maintaining%20continuous%20visual%20presentation%20in%20videos%20as%20Temporal%20Narrative%0AAtom%20%28TNA%29%2C%20and%20use%20its%20count%20to%20quantitatively%20measure%20narrative%20richness.%0AGuided%20by%20three%20key%20film%20narrative%20elements%20influencing%20TNA%20changes%2C%20we%0Aconstruct%20an%20automatic%20prompt%20generation%20pipeline%20capable%20of%20producing%0Aevaluation%20prompts%20with%20a%20flexibly%20expandable%20number%20of%20TNAs.%20%28ii%29%20Then%2C%20based%0Aon%20the%20three%20progressive%20levels%20of%20narrative%20content%20expression%2C%20we%20design%20an%0Aeffective%20evaluation%20metric%20using%20the%20MLLM-based%20question%20generation%20and%0Aanswering%20framework.%20%28iii%29%20Finally%2C%20we%20conduct%20extensive%20evaluations%20on%0Aexisting%20long%20video%20generation%20models%20and%20the%20foundation%20generation%20models.%0AExperimental%20results%20demonstrate%20that%20our%20metric%20aligns%20closely%20with%20human%0Ajudgments.%20The%20derived%20evaluation%20outcomes%20reveal%20the%20detailed%20capability%0Aboundaries%20of%20current%20video%20generation%20models%20in%20narrative%20content%20expression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11245v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNarrLV%253A%2520Towards%2520a%2520Comprehensive%2520Narrative-Centric%2520Evaluation%2520for%2520Long%250A%2520%2520Video%2520Generation%26entry.906535625%3DX.%2520Feng%2520and%2520H.%2520Yu%2520and%2520M.%2520Wu%2520and%2520S.%2520Hu%2520and%2520J.%2520Chen%2520and%2520C.%2520Zhu%2520and%2520J.%2520Wu%2520and%2520X.%2520Chu%2520and%2520K.%2520Huang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520development%2520of%2520foundation%2520video%2520generation%2520technologies%252C%2520long%250Avideo%2520generation%2520models%2520have%2520exhibited%2520promising%2520research%2520potential%2520thanks%2520to%250Aexpanded%2520content%2520creation%2520space.%2520Recent%2520studies%2520reveal%2520that%2520the%2520goal%2520of%2520long%250Avideo%2520generation%2520tasks%2520is%2520not%2520only%2520to%2520extend%2520video%2520duration%2520but%2520also%2520to%250Aaccurately%2520express%2520richer%2520narrative%2520content%2520within%2520longer%2520videos.%2520However%252C%2520due%250Ato%2520the%2520lack%2520of%2520evaluation%2520benchmarks%2520specifically%2520designed%2520for%2520long%2520video%250Ageneration%2520models%252C%2520the%2520current%2520assessment%2520of%2520these%2520models%2520primarily%2520relies%2520on%250Abenchmarks%2520with%2520simple%2520narrative%2520prompts%2520%2528e.g.%252C%2520VBench%2529.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520our%2520proposed%2520NarrLV%2520is%2520the%2520first%2520benchmark%2520to%2520comprehensively%250Aevaluate%2520the%2520Narrative%2520expression%2520capabilities%2520of%2520Long%2520Video%2520generation%2520models.%250AInspired%2520by%2520film%2520narrative%2520theory%252C%2520%2528i%2529%2520we%2520first%2520introduce%2520the%2520basic%2520narrative%250Aunit%2520maintaining%2520continuous%2520visual%2520presentation%2520in%2520videos%2520as%2520Temporal%2520Narrative%250AAtom%2520%2528TNA%2529%252C%2520and%2520use%2520its%2520count%2520to%2520quantitatively%2520measure%2520narrative%2520richness.%250AGuided%2520by%2520three%2520key%2520film%2520narrative%2520elements%2520influencing%2520TNA%2520changes%252C%2520we%250Aconstruct%2520an%2520automatic%2520prompt%2520generation%2520pipeline%2520capable%2520of%2520producing%250Aevaluation%2520prompts%2520with%2520a%2520flexibly%2520expandable%2520number%2520of%2520TNAs.%2520%2528ii%2529%2520Then%252C%2520based%250Aon%2520the%2520three%2520progressive%2520levels%2520of%2520narrative%2520content%2520expression%252C%2520we%2520design%2520an%250Aeffective%2520evaluation%2520metric%2520using%2520the%2520MLLM-based%2520question%2520generation%2520and%250Aanswering%2520framework.%2520%2528iii%2529%2520Finally%252C%2520we%2520conduct%2520extensive%2520evaluations%2520on%250Aexisting%2520long%2520video%2520generation%2520models%2520and%2520the%2520foundation%2520generation%2520models.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520metric%2520aligns%2520closely%2520with%2520human%250Ajudgments.%2520The%2520derived%2520evaluation%2520outcomes%2520reveal%2520the%2520detailed%2520capability%250Aboundaries%2520of%2520current%2520video%2520generation%2520models%2520in%2520narrative%2520content%2520expression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11245v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NarrLV%3A%20Towards%20a%20Comprehensive%20Narrative-Centric%20Evaluation%20for%20Long%0A%20%20Video%20Generation&entry.906535625=X.%20Feng%20and%20H.%20Yu%20and%20M.%20Wu%20and%20S.%20Hu%20and%20J.%20Chen%20and%20C.%20Zhu%20and%20J.%20Wu%20and%20X.%20Chu%20and%20K.%20Huang&entry.1292438233=%20%20With%20the%20rapid%20development%20of%20foundation%20video%20generation%20technologies%2C%20long%0Avideo%20generation%20models%20have%20exhibited%20promising%20research%20potential%20thanks%20to%0Aexpanded%20content%20creation%20space.%20Recent%20studies%20reveal%20that%20the%20goal%20of%20long%0Avideo%20generation%20tasks%20is%20not%20only%20to%20extend%20video%20duration%20but%20also%20to%0Aaccurately%20express%20richer%20narrative%20content%20within%20longer%20videos.%20However%2C%20due%0Ato%20the%20lack%20of%20evaluation%20benchmarks%20specifically%20designed%20for%20long%20video%0Ageneration%20models%2C%20the%20current%20assessment%20of%20these%20models%20primarily%20relies%20on%0Abenchmarks%20with%20simple%20narrative%20prompts%20%28e.g.%2C%20VBench%29.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20proposed%20NarrLV%20is%20the%20first%20benchmark%20to%20comprehensively%0Aevaluate%20the%20Narrative%20expression%20capabilities%20of%20Long%20Video%20generation%20models.%0AInspired%20by%20film%20narrative%20theory%2C%20%28i%29%20we%20first%20introduce%20the%20basic%20narrative%0Aunit%20maintaining%20continuous%20visual%20presentation%20in%20videos%20as%20Temporal%20Narrative%0AAtom%20%28TNA%29%2C%20and%20use%20its%20count%20to%20quantitatively%20measure%20narrative%20richness.%0AGuided%20by%20three%20key%20film%20narrative%20elements%20influencing%20TNA%20changes%2C%20we%0Aconstruct%20an%20automatic%20prompt%20generation%20pipeline%20capable%20of%20producing%0Aevaluation%20prompts%20with%20a%20flexibly%20expandable%20number%20of%20TNAs.%20%28ii%29%20Then%2C%20based%0Aon%20the%20three%20progressive%20levels%20of%20narrative%20content%20expression%2C%20we%20design%20an%0Aeffective%20evaluation%20metric%20using%20the%20MLLM-based%20question%20generation%20and%0Aanswering%20framework.%20%28iii%29%20Finally%2C%20we%20conduct%20extensive%20evaluations%20on%0Aexisting%20long%20video%20generation%20models%20and%20the%20foundation%20generation%20models.%0AExperimental%20results%20demonstrate%20that%20our%20metric%20aligns%20closely%20with%20human%0Ajudgments.%20The%20derived%20evaluation%20outcomes%20reveal%20the%20detailed%20capability%0Aboundaries%20of%20current%20video%20generation%20models%20in%20narrative%20content%20expression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11245v3&entry.124074799=Read"},
{"title": "Hierarchical Representation Matching for CLIP-based Class-Incremental\n  Learning", "author": "Zhen-Hao Wen and Yan Wang and Ji Feng and Han-Jia Ye and De-Chuan Zhan and Da-Wei Zhou", "abstract": "  Class-Incremental Learning (CIL) aims to endow models with the ability to\ncontinuously adapt to evolving data streams. Recent advances in pre-trained\nvision-language models (e.g., CLIP) provide a powerful foundation for this\ntask. However, existing approaches often rely on simplistic templates, such as\n\"a photo of a [CLASS]\", which overlook the hierarchical nature of visual\nconcepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained\ncues, while distinguishing \"cat\" from \"lion\" requires fine-grained details.\nSimilarly, the current feature mapping in CLIP relies solely on the\nrepresentation from the last layer, neglecting the hierarchical information\ncontained in earlier layers. In this work, we introduce HiErarchical\nRepresentation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages\nLLMs to recursively generate discriminative textual descriptors, thereby\naugmenting the semantic space with explicit hierarchical cues. These\ndescriptors are matched to different levels of the semantic hierarchy and\nadaptively routed based on task-specific requirements, enabling precise\ndiscrimination while alleviating catastrophic forgetting in incremental tasks.\nExtensive experiments on multiple benchmarks demonstrate that our method\nconsistently achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2509.22645v1", "date": "2025-09-26", "relevancy": 2.8116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5959}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Representation%20Matching%20for%20CLIP-based%20Class-Incremental%0A%20%20Learning&body=Title%3A%20Hierarchical%20Representation%20Matching%20for%20CLIP-based%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Zhen-Hao%20Wen%20and%20Yan%20Wang%20and%20Ji%20Feng%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%20and%20Da-Wei%20Zhou%0AAbstract%3A%20%20%20Class-Incremental%20Learning%20%28CIL%29%20aims%20to%20endow%20models%20with%20the%20ability%20to%0Acontinuously%20adapt%20to%20evolving%20data%20streams.%20Recent%20advances%20in%20pre-trained%0Avision-language%20models%20%28e.g.%2C%20CLIP%29%20provide%20a%20powerful%20foundation%20for%20this%0Atask.%20However%2C%20existing%20approaches%20often%20rely%20on%20simplistic%20templates%2C%20such%20as%0A%22a%20photo%20of%20a%20%5BCLASS%5D%22%2C%20which%20overlook%20the%20hierarchical%20nature%20of%20visual%0Aconcepts.%20For%20example%2C%20recognizing%20%22cat%22%20versus%20%22car%22%20depends%20on%20coarse-grained%0Acues%2C%20while%20distinguishing%20%22cat%22%20from%20%22lion%22%20requires%20fine-grained%20details.%0ASimilarly%2C%20the%20current%20feature%20mapping%20in%20CLIP%20relies%20solely%20on%20the%0Arepresentation%20from%20the%20last%20layer%2C%20neglecting%20the%20hierarchical%20information%0Acontained%20in%20earlier%20layers.%20In%20this%20work%2C%20we%20introduce%20HiErarchical%0ARepresentation%20MAtchiNg%20%28HERMAN%29%20for%20CLIP-based%20CIL.%20Our%20approach%20leverages%0ALLMs%20to%20recursively%20generate%20discriminative%20textual%20descriptors%2C%20thereby%0Aaugmenting%20the%20semantic%20space%20with%20explicit%20hierarchical%20cues.%20These%0Adescriptors%20are%20matched%20to%20different%20levels%20of%20the%20semantic%20hierarchy%20and%0Aadaptively%20routed%20based%20on%20task-specific%20requirements%2C%20enabling%20precise%0Adiscrimination%20while%20alleviating%20catastrophic%20forgetting%20in%20incremental%20tasks.%0AExtensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20our%20method%0Aconsistently%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Representation%2520Matching%2520for%2520CLIP-based%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DZhen-Hao%2520Wen%2520and%2520Yan%2520Wang%2520and%2520Ji%2520Feng%2520and%2520Han-Jia%2520Ye%2520and%2520De-Chuan%2520Zhan%2520and%2520Da-Wei%2520Zhou%26entry.1292438233%3D%2520%2520Class-Incremental%2520Learning%2520%2528CIL%2529%2520aims%2520to%2520endow%2520models%2520with%2520the%2520ability%2520to%250Acontinuously%2520adapt%2520to%2520evolving%2520data%2520streams.%2520Recent%2520advances%2520in%2520pre-trained%250Avision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520provide%2520a%2520powerful%2520foundation%2520for%2520this%250Atask.%2520However%252C%2520existing%2520approaches%2520often%2520rely%2520on%2520simplistic%2520templates%252C%2520such%2520as%250A%2522a%2520photo%2520of%2520a%2520%255BCLASS%255D%2522%252C%2520which%2520overlook%2520the%2520hierarchical%2520nature%2520of%2520visual%250Aconcepts.%2520For%2520example%252C%2520recognizing%2520%2522cat%2522%2520versus%2520%2522car%2522%2520depends%2520on%2520coarse-grained%250Acues%252C%2520while%2520distinguishing%2520%2522cat%2522%2520from%2520%2522lion%2522%2520requires%2520fine-grained%2520details.%250ASimilarly%252C%2520the%2520current%2520feature%2520mapping%2520in%2520CLIP%2520relies%2520solely%2520on%2520the%250Arepresentation%2520from%2520the%2520last%2520layer%252C%2520neglecting%2520the%2520hierarchical%2520information%250Acontained%2520in%2520earlier%2520layers.%2520In%2520this%2520work%252C%2520we%2520introduce%2520HiErarchical%250ARepresentation%2520MAtchiNg%2520%2528HERMAN%2529%2520for%2520CLIP-based%2520CIL.%2520Our%2520approach%2520leverages%250ALLMs%2520to%2520recursively%2520generate%2520discriminative%2520textual%2520descriptors%252C%2520thereby%250Aaugmenting%2520the%2520semantic%2520space%2520with%2520explicit%2520hierarchical%2520cues.%2520These%250Adescriptors%2520are%2520matched%2520to%2520different%2520levels%2520of%2520the%2520semantic%2520hierarchy%2520and%250Aadaptively%2520routed%2520based%2520on%2520task-specific%2520requirements%252C%2520enabling%2520precise%250Adiscrimination%2520while%2520alleviating%2520catastrophic%2520forgetting%2520in%2520incremental%2520tasks.%250AExtensive%2520experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520that%2520our%2520method%250Aconsistently%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Representation%20Matching%20for%20CLIP-based%20Class-Incremental%0A%20%20Learning&entry.906535625=Zhen-Hao%20Wen%20and%20Yan%20Wang%20and%20Ji%20Feng%20and%20Han-Jia%20Ye%20and%20De-Chuan%20Zhan%20and%20Da-Wei%20Zhou&entry.1292438233=%20%20Class-Incremental%20Learning%20%28CIL%29%20aims%20to%20endow%20models%20with%20the%20ability%20to%0Acontinuously%20adapt%20to%20evolving%20data%20streams.%20Recent%20advances%20in%20pre-trained%0Avision-language%20models%20%28e.g.%2C%20CLIP%29%20provide%20a%20powerful%20foundation%20for%20this%0Atask.%20However%2C%20existing%20approaches%20often%20rely%20on%20simplistic%20templates%2C%20such%20as%0A%22a%20photo%20of%20a%20%5BCLASS%5D%22%2C%20which%20overlook%20the%20hierarchical%20nature%20of%20visual%0Aconcepts.%20For%20example%2C%20recognizing%20%22cat%22%20versus%20%22car%22%20depends%20on%20coarse-grained%0Acues%2C%20while%20distinguishing%20%22cat%22%20from%20%22lion%22%20requires%20fine-grained%20details.%0ASimilarly%2C%20the%20current%20feature%20mapping%20in%20CLIP%20relies%20solely%20on%20the%0Arepresentation%20from%20the%20last%20layer%2C%20neglecting%20the%20hierarchical%20information%0Acontained%20in%20earlier%20layers.%20In%20this%20work%2C%20we%20introduce%20HiErarchical%0ARepresentation%20MAtchiNg%20%28HERMAN%29%20for%20CLIP-based%20CIL.%20Our%20approach%20leverages%0ALLMs%20to%20recursively%20generate%20discriminative%20textual%20descriptors%2C%20thereby%0Aaugmenting%20the%20semantic%20space%20with%20explicit%20hierarchical%20cues.%20These%0Adescriptors%20are%20matched%20to%20different%20levels%20of%20the%20semantic%20hierarchy%20and%0Aadaptively%20routed%20based%20on%20task-specific%20requirements%2C%20enabling%20precise%0Adiscrimination%20while%20alleviating%20catastrophic%20forgetting%20in%20incremental%20tasks.%0AExtensive%20experiments%20on%20multiple%20benchmarks%20demonstrate%20that%20our%20method%0Aconsistently%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22645v1&entry.124074799=Read"},
{"title": "PSTTS: A Plug-and-Play Token Selector for Efficient Event-based\n  Spatio-temporal Representation Learning", "author": "Xiangmo Zhao and Nan Yang and Yang Wang and Zhanwen Liu", "abstract": "  Mainstream event-based spatio-temporal representation learning methods\ntypically process event streams by converting them into sequences of event\nframes, achieving remarkable performance. However, they neglect the high\nspatial sparsity and inter-frame motion redundancy inherent in event frame\nsequences, leading to significant computational overhead. Existing token\nsparsification methods for RGB videos rely on unreliable intermediate token\nrepresentations and neglect the influence of event noise, making them\nineffective for direct application to event data. In this paper, we propose\nProgressive Spatio-Temporal Token Selection (PSTTS), a Plug-and-Play module for\nevent data without introducing any additional parameters. PSTTS exploits the\nspatio-temporal distribution characteristics embedded in raw event data to\neffectively identify and discard spatio-temporal redundant tokens, achieving an\noptimal trade-off between accuracy and efficiency. Specifically, PSTTS consists\nof two stages, Spatial Token Purification and Temporal Token Selection. Spatial\nToken Purification discards noise and non-event regions by assessing the\nspatio-temporal consistency of events within each event frame to prevent\ninterference with subsequent temporal redundancy evaluation. Temporal Token\nSelection evaluates the motion pattern similarity between adjacent event\nframes, precisely identifying and removing redundant temporal information. We\napply PSTTS to four representative backbones UniformerV2, VideoSwin, EVMamba,\nand ExACT on the HARDVS, DailyDVS-200, and SeACT datasets. Experimental results\ndemonstrate that PSTTS achieves significant efficiency improvements.\nSpecifically, PSTTS reduces FLOPs by 29-43.6% and increases FPS by 21.6-41.3%\non the DailyDVS-200 dataset, while maintaining task accuracy. Our code will be\navailable.\n", "link": "http://arxiv.org/abs/2509.22481v1", "date": "2025-09-26", "relevancy": 2.8095, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5789}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5572}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSTTS%3A%20A%20Plug-and-Play%20Token%20Selector%20for%20Efficient%20Event-based%0A%20%20Spatio-temporal%20Representation%20Learning&body=Title%3A%20PSTTS%3A%20A%20Plug-and-Play%20Token%20Selector%20for%20Efficient%20Event-based%0A%20%20Spatio-temporal%20Representation%20Learning%0AAuthor%3A%20Xiangmo%20Zhao%20and%20Nan%20Yang%20and%20Yang%20Wang%20and%20Zhanwen%20Liu%0AAbstract%3A%20%20%20Mainstream%20event-based%20spatio-temporal%20representation%20learning%20methods%0Atypically%20process%20event%20streams%20by%20converting%20them%20into%20sequences%20of%20event%0Aframes%2C%20achieving%20remarkable%20performance.%20However%2C%20they%20neglect%20the%20high%0Aspatial%20sparsity%20and%20inter-frame%20motion%20redundancy%20inherent%20in%20event%20frame%0Asequences%2C%20leading%20to%20significant%20computational%20overhead.%20Existing%20token%0Asparsification%20methods%20for%20RGB%20videos%20rely%20on%20unreliable%20intermediate%20token%0Arepresentations%20and%20neglect%20the%20influence%20of%20event%20noise%2C%20making%20them%0Aineffective%20for%20direct%20application%20to%20event%20data.%20In%20this%20paper%2C%20we%20propose%0AProgressive%20Spatio-Temporal%20Token%20Selection%20%28PSTTS%29%2C%20a%20Plug-and-Play%20module%20for%0Aevent%20data%20without%20introducing%20any%20additional%20parameters.%20PSTTS%20exploits%20the%0Aspatio-temporal%20distribution%20characteristics%20embedded%20in%20raw%20event%20data%20to%0Aeffectively%20identify%20and%20discard%20spatio-temporal%20redundant%20tokens%2C%20achieving%20an%0Aoptimal%20trade-off%20between%20accuracy%20and%20efficiency.%20Specifically%2C%20PSTTS%20consists%0Aof%20two%20stages%2C%20Spatial%20Token%20Purification%20and%20Temporal%20Token%20Selection.%20Spatial%0AToken%20Purification%20discards%20noise%20and%20non-event%20regions%20by%20assessing%20the%0Aspatio-temporal%20consistency%20of%20events%20within%20each%20event%20frame%20to%20prevent%0Ainterference%20with%20subsequent%20temporal%20redundancy%20evaluation.%20Temporal%20Token%0ASelection%20evaluates%20the%20motion%20pattern%20similarity%20between%20adjacent%20event%0Aframes%2C%20precisely%20identifying%20and%20removing%20redundant%20temporal%20information.%20We%0Aapply%20PSTTS%20to%20four%20representative%20backbones%20UniformerV2%2C%20VideoSwin%2C%20EVMamba%2C%0Aand%20ExACT%20on%20the%20HARDVS%2C%20DailyDVS-200%2C%20and%20SeACT%20datasets.%20Experimental%20results%0Ademonstrate%20that%20PSTTS%20achieves%20significant%20efficiency%20improvements.%0ASpecifically%2C%20PSTTS%20reduces%20FLOPs%20by%2029-43.6%25%20and%20increases%20FPS%20by%2021.6-41.3%25%0Aon%20the%20DailyDVS-200%20dataset%2C%20while%20maintaining%20task%20accuracy.%20Our%20code%20will%20be%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSTTS%253A%2520A%2520Plug-and-Play%2520Token%2520Selector%2520for%2520Efficient%2520Event-based%250A%2520%2520Spatio-temporal%2520Representation%2520Learning%26entry.906535625%3DXiangmo%2520Zhao%2520and%2520Nan%2520Yang%2520and%2520Yang%2520Wang%2520and%2520Zhanwen%2520Liu%26entry.1292438233%3D%2520%2520Mainstream%2520event-based%2520spatio-temporal%2520representation%2520learning%2520methods%250Atypically%2520process%2520event%2520streams%2520by%2520converting%2520them%2520into%2520sequences%2520of%2520event%250Aframes%252C%2520achieving%2520remarkable%2520performance.%2520However%252C%2520they%2520neglect%2520the%2520high%250Aspatial%2520sparsity%2520and%2520inter-frame%2520motion%2520redundancy%2520inherent%2520in%2520event%2520frame%250Asequences%252C%2520leading%2520to%2520significant%2520computational%2520overhead.%2520Existing%2520token%250Asparsification%2520methods%2520for%2520RGB%2520videos%2520rely%2520on%2520unreliable%2520intermediate%2520token%250Arepresentations%2520and%2520neglect%2520the%2520influence%2520of%2520event%2520noise%252C%2520making%2520them%250Aineffective%2520for%2520direct%2520application%2520to%2520event%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%250AProgressive%2520Spatio-Temporal%2520Token%2520Selection%2520%2528PSTTS%2529%252C%2520a%2520Plug-and-Play%2520module%2520for%250Aevent%2520data%2520without%2520introducing%2520any%2520additional%2520parameters.%2520PSTTS%2520exploits%2520the%250Aspatio-temporal%2520distribution%2520characteristics%2520embedded%2520in%2520raw%2520event%2520data%2520to%250Aeffectively%2520identify%2520and%2520discard%2520spatio-temporal%2520redundant%2520tokens%252C%2520achieving%2520an%250Aoptimal%2520trade-off%2520between%2520accuracy%2520and%2520efficiency.%2520Specifically%252C%2520PSTTS%2520consists%250Aof%2520two%2520stages%252C%2520Spatial%2520Token%2520Purification%2520and%2520Temporal%2520Token%2520Selection.%2520Spatial%250AToken%2520Purification%2520discards%2520noise%2520and%2520non-event%2520regions%2520by%2520assessing%2520the%250Aspatio-temporal%2520consistency%2520of%2520events%2520within%2520each%2520event%2520frame%2520to%2520prevent%250Ainterference%2520with%2520subsequent%2520temporal%2520redundancy%2520evaluation.%2520Temporal%2520Token%250ASelection%2520evaluates%2520the%2520motion%2520pattern%2520similarity%2520between%2520adjacent%2520event%250Aframes%252C%2520precisely%2520identifying%2520and%2520removing%2520redundant%2520temporal%2520information.%2520We%250Aapply%2520PSTTS%2520to%2520four%2520representative%2520backbones%2520UniformerV2%252C%2520VideoSwin%252C%2520EVMamba%252C%250Aand%2520ExACT%2520on%2520the%2520HARDVS%252C%2520DailyDVS-200%252C%2520and%2520SeACT%2520datasets.%2520Experimental%2520results%250Ademonstrate%2520that%2520PSTTS%2520achieves%2520significant%2520efficiency%2520improvements.%250ASpecifically%252C%2520PSTTS%2520reduces%2520FLOPs%2520by%252029-43.6%2525%2520and%2520increases%2520FPS%2520by%252021.6-41.3%2525%250Aon%2520the%2520DailyDVS-200%2520dataset%252C%2520while%2520maintaining%2520task%2520accuracy.%2520Our%2520code%2520will%2520be%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSTTS%3A%20A%20Plug-and-Play%20Token%20Selector%20for%20Efficient%20Event-based%0A%20%20Spatio-temporal%20Representation%20Learning&entry.906535625=Xiangmo%20Zhao%20and%20Nan%20Yang%20and%20Yang%20Wang%20and%20Zhanwen%20Liu&entry.1292438233=%20%20Mainstream%20event-based%20spatio-temporal%20representation%20learning%20methods%0Atypically%20process%20event%20streams%20by%20converting%20them%20into%20sequences%20of%20event%0Aframes%2C%20achieving%20remarkable%20performance.%20However%2C%20they%20neglect%20the%20high%0Aspatial%20sparsity%20and%20inter-frame%20motion%20redundancy%20inherent%20in%20event%20frame%0Asequences%2C%20leading%20to%20significant%20computational%20overhead.%20Existing%20token%0Asparsification%20methods%20for%20RGB%20videos%20rely%20on%20unreliable%20intermediate%20token%0Arepresentations%20and%20neglect%20the%20influence%20of%20event%20noise%2C%20making%20them%0Aineffective%20for%20direct%20application%20to%20event%20data.%20In%20this%20paper%2C%20we%20propose%0AProgressive%20Spatio-Temporal%20Token%20Selection%20%28PSTTS%29%2C%20a%20Plug-and-Play%20module%20for%0Aevent%20data%20without%20introducing%20any%20additional%20parameters.%20PSTTS%20exploits%20the%0Aspatio-temporal%20distribution%20characteristics%20embedded%20in%20raw%20event%20data%20to%0Aeffectively%20identify%20and%20discard%20spatio-temporal%20redundant%20tokens%2C%20achieving%20an%0Aoptimal%20trade-off%20between%20accuracy%20and%20efficiency.%20Specifically%2C%20PSTTS%20consists%0Aof%20two%20stages%2C%20Spatial%20Token%20Purification%20and%20Temporal%20Token%20Selection.%20Spatial%0AToken%20Purification%20discards%20noise%20and%20non-event%20regions%20by%20assessing%20the%0Aspatio-temporal%20consistency%20of%20events%20within%20each%20event%20frame%20to%20prevent%0Ainterference%20with%20subsequent%20temporal%20redundancy%20evaluation.%20Temporal%20Token%0ASelection%20evaluates%20the%20motion%20pattern%20similarity%20between%20adjacent%20event%0Aframes%2C%20precisely%20identifying%20and%20removing%20redundant%20temporal%20information.%20We%0Aapply%20PSTTS%20to%20four%20representative%20backbones%20UniformerV2%2C%20VideoSwin%2C%20EVMamba%2C%0Aand%20ExACT%20on%20the%20HARDVS%2C%20DailyDVS-200%2C%20and%20SeACT%20datasets.%20Experimental%20results%0Ademonstrate%20that%20PSTTS%20achieves%20significant%20efficiency%20improvements.%0ASpecifically%2C%20PSTTS%20reduces%20FLOPs%20by%2029-43.6%25%20and%20increases%20FPS%20by%2021.6-41.3%25%0Aon%20the%20DailyDVS-200%20dataset%2C%20while%20maintaining%20task%20accuracy.%20Our%20code%20will%20be%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22481v1&entry.124074799=Read"},
{"title": "Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM\n  Approach", "author": "Zijian Zhao and Dian Jin and Zijing Zhou", "abstract": "  Recently, Image-to-Music (I2M) generation has garnered significant attention,\nwith potential applications in fields such as gaming, advertising, and\nmulti-modal art creation. However, due to the ambiguous and subjective nature\nof I2M tasks, most end-to-end methods lack interpretability, leaving users\npuzzled about the generation results. Even methods based on emotion mapping\nface controversy, as emotion represents only a singular aspect of art.\nAdditionally, most learning-based methods require substantial computational\nresources and large datasets for training, hindering accessibility for common\nusers. To address these challenges, we propose the first Vision Language Model\n(VLM)-based I2M framework that offers high interpretability and low\ncomputational cost. Specifically, we utilize ABC notation to bridge the text\nand music modalities, enabling the VLM to generate music using natural\nlanguage. We then apply multi-modal Retrieval-Augmented Generation (RAG) and\nself-refinement techniques to allow the VLM to produce high-quality music\nwithout external training. Furthermore, we leverage the generated motivations\nin text and the attention maps from the VLM to provide explanations for the\ngenerated results in both text and image modalities. To validate our method, we\nconduct both human studies and machine evaluations, where our method\noutperforms others in terms of music quality and music-image consistency,\nindicating promising results. Our code is available at\nhttps://github.com/RS2002/Image2Music .\n", "link": "http://arxiv.org/abs/2509.22378v1", "date": "2025-09-26", "relevancy": 2.7956, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5823}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5626}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Effort%20Image-to-Music%20Generation%3A%20An%20Interpretable%20RAG-based%20VLM%0A%20%20Approach&body=Title%3A%20Zero-Effort%20Image-to-Music%20Generation%3A%20An%20Interpretable%20RAG-based%20VLM%0A%20%20Approach%0AAuthor%3A%20Zijian%20Zhao%20and%20Dian%20Jin%20and%20Zijing%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20Image-to-Music%20%28I2M%29%20generation%20has%20garnered%20significant%20attention%2C%0Awith%20potential%20applications%20in%20fields%20such%20as%20gaming%2C%20advertising%2C%20and%0Amulti-modal%20art%20creation.%20However%2C%20due%20to%20the%20ambiguous%20and%20subjective%20nature%0Aof%20I2M%20tasks%2C%20most%20end-to-end%20methods%20lack%20interpretability%2C%20leaving%20users%0Apuzzled%20about%20the%20generation%20results.%20Even%20methods%20based%20on%20emotion%20mapping%0Aface%20controversy%2C%20as%20emotion%20represents%20only%20a%20singular%20aspect%20of%20art.%0AAdditionally%2C%20most%20learning-based%20methods%20require%20substantial%20computational%0Aresources%20and%20large%20datasets%20for%20training%2C%20hindering%20accessibility%20for%20common%0Ausers.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20Vision%20Language%20Model%0A%28VLM%29-based%20I2M%20framework%20that%20offers%20high%20interpretability%20and%20low%0Acomputational%20cost.%20Specifically%2C%20we%20utilize%20ABC%20notation%20to%20bridge%20the%20text%0Aand%20music%20modalities%2C%20enabling%20the%20VLM%20to%20generate%20music%20using%20natural%0Alanguage.%20We%20then%20apply%20multi-modal%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%0Aself-refinement%20techniques%20to%20allow%20the%20VLM%20to%20produce%20high-quality%20music%0Awithout%20external%20training.%20Furthermore%2C%20we%20leverage%20the%20generated%20motivations%0Ain%20text%20and%20the%20attention%20maps%20from%20the%20VLM%20to%20provide%20explanations%20for%20the%0Agenerated%20results%20in%20both%20text%20and%20image%20modalities.%20To%20validate%20our%20method%2C%20we%0Aconduct%20both%20human%20studies%20and%20machine%20evaluations%2C%20where%20our%20method%0Aoutperforms%20others%20in%20terms%20of%20music%20quality%20and%20music-image%20consistency%2C%0Aindicating%20promising%20results.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/RS2002/Image2Music%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Effort%2520Image-to-Music%2520Generation%253A%2520An%2520Interpretable%2520RAG-based%2520VLM%250A%2520%2520Approach%26entry.906535625%3DZijian%2520Zhao%2520and%2520Dian%2520Jin%2520and%2520Zijing%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520Image-to-Music%2520%2528I2M%2529%2520generation%2520has%2520garnered%2520significant%2520attention%252C%250Awith%2520potential%2520applications%2520in%2520fields%2520such%2520as%2520gaming%252C%2520advertising%252C%2520and%250Amulti-modal%2520art%2520creation.%2520However%252C%2520due%2520to%2520the%2520ambiguous%2520and%2520subjective%2520nature%250Aof%2520I2M%2520tasks%252C%2520most%2520end-to-end%2520methods%2520lack%2520interpretability%252C%2520leaving%2520users%250Apuzzled%2520about%2520the%2520generation%2520results.%2520Even%2520methods%2520based%2520on%2520emotion%2520mapping%250Aface%2520controversy%252C%2520as%2520emotion%2520represents%2520only%2520a%2520singular%2520aspect%2520of%2520art.%250AAdditionally%252C%2520most%2520learning-based%2520methods%2520require%2520substantial%2520computational%250Aresources%2520and%2520large%2520datasets%2520for%2520training%252C%2520hindering%2520accessibility%2520for%2520common%250Ausers.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520first%2520Vision%2520Language%2520Model%250A%2528VLM%2529-based%2520I2M%2520framework%2520that%2520offers%2520high%2520interpretability%2520and%2520low%250Acomputational%2520cost.%2520Specifically%252C%2520we%2520utilize%2520ABC%2520notation%2520to%2520bridge%2520the%2520text%250Aand%2520music%2520modalities%252C%2520enabling%2520the%2520VLM%2520to%2520generate%2520music%2520using%2520natural%250Alanguage.%2520We%2520then%2520apply%2520multi-modal%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520and%250Aself-refinement%2520techniques%2520to%2520allow%2520the%2520VLM%2520to%2520produce%2520high-quality%2520music%250Awithout%2520external%2520training.%2520Furthermore%252C%2520we%2520leverage%2520the%2520generated%2520motivations%250Ain%2520text%2520and%2520the%2520attention%2520maps%2520from%2520the%2520VLM%2520to%2520provide%2520explanations%2520for%2520the%250Agenerated%2520results%2520in%2520both%2520text%2520and%2520image%2520modalities.%2520To%2520validate%2520our%2520method%252C%2520we%250Aconduct%2520both%2520human%2520studies%2520and%2520machine%2520evaluations%252C%2520where%2520our%2520method%250Aoutperforms%2520others%2520in%2520terms%2520of%2520music%2520quality%2520and%2520music-image%2520consistency%252C%250Aindicating%2520promising%2520results.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/RS2002/Image2Music%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Effort%20Image-to-Music%20Generation%3A%20An%20Interpretable%20RAG-based%20VLM%0A%20%20Approach&entry.906535625=Zijian%20Zhao%20and%20Dian%20Jin%20and%20Zijing%20Zhou&entry.1292438233=%20%20Recently%2C%20Image-to-Music%20%28I2M%29%20generation%20has%20garnered%20significant%20attention%2C%0Awith%20potential%20applications%20in%20fields%20such%20as%20gaming%2C%20advertising%2C%20and%0Amulti-modal%20art%20creation.%20However%2C%20due%20to%20the%20ambiguous%20and%20subjective%20nature%0Aof%20I2M%20tasks%2C%20most%20end-to-end%20methods%20lack%20interpretability%2C%20leaving%20users%0Apuzzled%20about%20the%20generation%20results.%20Even%20methods%20based%20on%20emotion%20mapping%0Aface%20controversy%2C%20as%20emotion%20represents%20only%20a%20singular%20aspect%20of%20art.%0AAdditionally%2C%20most%20learning-based%20methods%20require%20substantial%20computational%0Aresources%20and%20large%20datasets%20for%20training%2C%20hindering%20accessibility%20for%20common%0Ausers.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20Vision%20Language%20Model%0A%28VLM%29-based%20I2M%20framework%20that%20offers%20high%20interpretability%20and%20low%0Acomputational%20cost.%20Specifically%2C%20we%20utilize%20ABC%20notation%20to%20bridge%20the%20text%0Aand%20music%20modalities%2C%20enabling%20the%20VLM%20to%20generate%20music%20using%20natural%0Alanguage.%20We%20then%20apply%20multi-modal%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%0Aself-refinement%20techniques%20to%20allow%20the%20VLM%20to%20produce%20high-quality%20music%0Awithout%20external%20training.%20Furthermore%2C%20we%20leverage%20the%20generated%20motivations%0Ain%20text%20and%20the%20attention%20maps%20from%20the%20VLM%20to%20provide%20explanations%20for%20the%0Agenerated%20results%20in%20both%20text%20and%20image%20modalities.%20To%20validate%20our%20method%2C%20we%0Aconduct%20both%20human%20studies%20and%20machine%20evaluations%2C%20where%20our%20method%0Aoutperforms%20others%20in%20terms%20of%20music%20quality%20and%20music-image%20consistency%2C%0Aindicating%20promising%20results.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/RS2002/Image2Music%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22378v1&entry.124074799=Read"},
{"title": "Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding", "author": "Ziheng Chi and Yifan Hou and Chenxi Pang and Shaobo Cui and Mubashara Akhtar and Mrinmaya Sachan", "abstract": "  Diagrams convey symbolic information in a visual format rather than a linear\nstream of words, making them especially challenging for AI models to process.\nWhile recent evaluations suggest that vision-language models (VLMs) perform\nwell on diagram-related benchmarks, their reliance on knowledge, reasoning, or\nmodality shortcuts raises concerns about whether they genuinely understand and\nreason over diagrams. To address this gap, we introduce Chimera, a\ncomprehensive test suite comprising 7,500 high-quality diagrams sourced from\nWikipedia; each diagram is annotated with its symbolic content represented by\nsemantic triples along with multi-level questions designed to assess four\nfundamental aspects of diagram comprehension: entity recognition, relation\nunderstanding, knowledge grounding, and visual reasoning. We use Chimera to\nmeasure the presence of three types of shortcuts in visual question answering:\n(1) the visual-memorization shortcut, where VLMs rely on memorized visual\npatterns; (2) the knowledge-recall shortcut, where models leverage memorized\nfactual knowledge instead of interpreting the diagram; and (3) the Clever-Hans\nshortcut, where models exploit superficial language patterns or priors without\ntrue comprehension. We evaluate 15 open-source VLMs from 7 model families on\nChimera and find that their seemingly strong performance largely stems from\nshortcut behaviors: visual-memorization shortcuts have slight impact,\nknowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts\ncontribute significantly. These findings expose critical limitations in current\nVLMs and underscore the need for more robust evaluation protocols that\nbenchmark genuine comprehension of complex visual inputs (e.g., diagrams)\nrather than question-answering shortcuts.\n", "link": "http://arxiv.org/abs/2509.22437v1", "date": "2025-09-26", "relevancy": 2.7908, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chimera%3A%20Diagnosing%20Shortcut%20Learning%20in%20Visual-Language%20Understanding&body=Title%3A%20Chimera%3A%20Diagnosing%20Shortcut%20Learning%20in%20Visual-Language%20Understanding%0AAuthor%3A%20Ziheng%20Chi%20and%20Yifan%20Hou%20and%20Chenxi%20Pang%20and%20Shaobo%20Cui%20and%20Mubashara%20Akhtar%20and%20Mrinmaya%20Sachan%0AAbstract%3A%20%20%20Diagrams%20convey%20symbolic%20information%20in%20a%20visual%20format%20rather%20than%20a%20linear%0Astream%20of%20words%2C%20making%20them%20especially%20challenging%20for%20AI%20models%20to%20process.%0AWhile%20recent%20evaluations%20suggest%20that%20vision-language%20models%20%28VLMs%29%20perform%0Awell%20on%20diagram-related%20benchmarks%2C%20their%20reliance%20on%20knowledge%2C%20reasoning%2C%20or%0Amodality%20shortcuts%20raises%20concerns%20about%20whether%20they%20genuinely%20understand%20and%0Areason%20over%20diagrams.%20To%20address%20this%20gap%2C%20we%20introduce%20Chimera%2C%20a%0Acomprehensive%20test%20suite%20comprising%207%2C500%20high-quality%20diagrams%20sourced%20from%0AWikipedia%3B%20each%20diagram%20is%20annotated%20with%20its%20symbolic%20content%20represented%20by%0Asemantic%20triples%20along%20with%20multi-level%20questions%20designed%20to%20assess%20four%0Afundamental%20aspects%20of%20diagram%20comprehension%3A%20entity%20recognition%2C%20relation%0Aunderstanding%2C%20knowledge%20grounding%2C%20and%20visual%20reasoning.%20We%20use%20Chimera%20to%0Ameasure%20the%20presence%20of%20three%20types%20of%20shortcuts%20in%20visual%20question%20answering%3A%0A%281%29%20the%20visual-memorization%20shortcut%2C%20where%20VLMs%20rely%20on%20memorized%20visual%0Apatterns%3B%20%282%29%20the%20knowledge-recall%20shortcut%2C%20where%20models%20leverage%20memorized%0Afactual%20knowledge%20instead%20of%20interpreting%20the%20diagram%3B%20and%20%283%29%20the%20Clever-Hans%0Ashortcut%2C%20where%20models%20exploit%20superficial%20language%20patterns%20or%20priors%20without%0Atrue%20comprehension.%20We%20evaluate%2015%20open-source%20VLMs%20from%207%20model%20families%20on%0AChimera%20and%20find%20that%20their%20seemingly%20strong%20performance%20largely%20stems%20from%0Ashortcut%20behaviors%3A%20visual-memorization%20shortcuts%20have%20slight%20impact%2C%0Aknowledge-recall%20shortcuts%20play%20a%20moderate%20role%2C%20and%20Clever-Hans%20shortcuts%0Acontribute%20significantly.%20These%20findings%20expose%20critical%20limitations%20in%20current%0AVLMs%20and%20underscore%20the%20need%20for%20more%20robust%20evaluation%20protocols%20that%0Abenchmark%20genuine%20comprehension%20of%20complex%20visual%20inputs%20%28e.g.%2C%20diagrams%29%0Arather%20than%20question-answering%20shortcuts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChimera%253A%2520Diagnosing%2520Shortcut%2520Learning%2520in%2520Visual-Language%2520Understanding%26entry.906535625%3DZiheng%2520Chi%2520and%2520Yifan%2520Hou%2520and%2520Chenxi%2520Pang%2520and%2520Shaobo%2520Cui%2520and%2520Mubashara%2520Akhtar%2520and%2520Mrinmaya%2520Sachan%26entry.1292438233%3D%2520%2520Diagrams%2520convey%2520symbolic%2520information%2520in%2520a%2520visual%2520format%2520rather%2520than%2520a%2520linear%250Astream%2520of%2520words%252C%2520making%2520them%2520especially%2520challenging%2520for%2520AI%2520models%2520to%2520process.%250AWhile%2520recent%2520evaluations%2520suggest%2520that%2520vision-language%2520models%2520%2528VLMs%2529%2520perform%250Awell%2520on%2520diagram-related%2520benchmarks%252C%2520their%2520reliance%2520on%2520knowledge%252C%2520reasoning%252C%2520or%250Amodality%2520shortcuts%2520raises%2520concerns%2520about%2520whether%2520they%2520genuinely%2520understand%2520and%250Areason%2520over%2520diagrams.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520Chimera%252C%2520a%250Acomprehensive%2520test%2520suite%2520comprising%25207%252C500%2520high-quality%2520diagrams%2520sourced%2520from%250AWikipedia%253B%2520each%2520diagram%2520is%2520annotated%2520with%2520its%2520symbolic%2520content%2520represented%2520by%250Asemantic%2520triples%2520along%2520with%2520multi-level%2520questions%2520designed%2520to%2520assess%2520four%250Afundamental%2520aspects%2520of%2520diagram%2520comprehension%253A%2520entity%2520recognition%252C%2520relation%250Aunderstanding%252C%2520knowledge%2520grounding%252C%2520and%2520visual%2520reasoning.%2520We%2520use%2520Chimera%2520to%250Ameasure%2520the%2520presence%2520of%2520three%2520types%2520of%2520shortcuts%2520in%2520visual%2520question%2520answering%253A%250A%25281%2529%2520the%2520visual-memorization%2520shortcut%252C%2520where%2520VLMs%2520rely%2520on%2520memorized%2520visual%250Apatterns%253B%2520%25282%2529%2520the%2520knowledge-recall%2520shortcut%252C%2520where%2520models%2520leverage%2520memorized%250Afactual%2520knowledge%2520instead%2520of%2520interpreting%2520the%2520diagram%253B%2520and%2520%25283%2529%2520the%2520Clever-Hans%250Ashortcut%252C%2520where%2520models%2520exploit%2520superficial%2520language%2520patterns%2520or%2520priors%2520without%250Atrue%2520comprehension.%2520We%2520evaluate%252015%2520open-source%2520VLMs%2520from%25207%2520model%2520families%2520on%250AChimera%2520and%2520find%2520that%2520their%2520seemingly%2520strong%2520performance%2520largely%2520stems%2520from%250Ashortcut%2520behaviors%253A%2520visual-memorization%2520shortcuts%2520have%2520slight%2520impact%252C%250Aknowledge-recall%2520shortcuts%2520play%2520a%2520moderate%2520role%252C%2520and%2520Clever-Hans%2520shortcuts%250Acontribute%2520significantly.%2520These%2520findings%2520expose%2520critical%2520limitations%2520in%2520current%250AVLMs%2520and%2520underscore%2520the%2520need%2520for%2520more%2520robust%2520evaluation%2520protocols%2520that%250Abenchmark%2520genuine%2520comprehension%2520of%2520complex%2520visual%2520inputs%2520%2528e.g.%252C%2520diagrams%2529%250Arather%2520than%2520question-answering%2520shortcuts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chimera%3A%20Diagnosing%20Shortcut%20Learning%20in%20Visual-Language%20Understanding&entry.906535625=Ziheng%20Chi%20and%20Yifan%20Hou%20and%20Chenxi%20Pang%20and%20Shaobo%20Cui%20and%20Mubashara%20Akhtar%20and%20Mrinmaya%20Sachan&entry.1292438233=%20%20Diagrams%20convey%20symbolic%20information%20in%20a%20visual%20format%20rather%20than%20a%20linear%0Astream%20of%20words%2C%20making%20them%20especially%20challenging%20for%20AI%20models%20to%20process.%0AWhile%20recent%20evaluations%20suggest%20that%20vision-language%20models%20%28VLMs%29%20perform%0Awell%20on%20diagram-related%20benchmarks%2C%20their%20reliance%20on%20knowledge%2C%20reasoning%2C%20or%0Amodality%20shortcuts%20raises%20concerns%20about%20whether%20they%20genuinely%20understand%20and%0Areason%20over%20diagrams.%20To%20address%20this%20gap%2C%20we%20introduce%20Chimera%2C%20a%0Acomprehensive%20test%20suite%20comprising%207%2C500%20high-quality%20diagrams%20sourced%20from%0AWikipedia%3B%20each%20diagram%20is%20annotated%20with%20its%20symbolic%20content%20represented%20by%0Asemantic%20triples%20along%20with%20multi-level%20questions%20designed%20to%20assess%20four%0Afundamental%20aspects%20of%20diagram%20comprehension%3A%20entity%20recognition%2C%20relation%0Aunderstanding%2C%20knowledge%20grounding%2C%20and%20visual%20reasoning.%20We%20use%20Chimera%20to%0Ameasure%20the%20presence%20of%20three%20types%20of%20shortcuts%20in%20visual%20question%20answering%3A%0A%281%29%20the%20visual-memorization%20shortcut%2C%20where%20VLMs%20rely%20on%20memorized%20visual%0Apatterns%3B%20%282%29%20the%20knowledge-recall%20shortcut%2C%20where%20models%20leverage%20memorized%0Afactual%20knowledge%20instead%20of%20interpreting%20the%20diagram%3B%20and%20%283%29%20the%20Clever-Hans%0Ashortcut%2C%20where%20models%20exploit%20superficial%20language%20patterns%20or%20priors%20without%0Atrue%20comprehension.%20We%20evaluate%2015%20open-source%20VLMs%20from%207%20model%20families%20on%0AChimera%20and%20find%20that%20their%20seemingly%20strong%20performance%20largely%20stems%20from%0Ashortcut%20behaviors%3A%20visual-memorization%20shortcuts%20have%20slight%20impact%2C%0Aknowledge-recall%20shortcuts%20play%20a%20moderate%20role%2C%20and%20Clever-Hans%20shortcuts%0Acontribute%20significantly.%20These%20findings%20expose%20critical%20limitations%20in%20current%0AVLMs%20and%20underscore%20the%20need%20for%20more%20robust%20evaluation%20protocols%20that%0Abenchmark%20genuine%20comprehension%20of%20complex%20visual%20inputs%20%28e.g.%2C%20diagrams%29%0Arather%20than%20question-answering%20shortcuts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22437v1&entry.124074799=Read"},
{"title": "GLip: A Global-Local Integrated Progressive Framework for Robust Visual\n  Speech Recognition", "author": "Tianyue Wang and Shuang Yang and Shiguang Shan and Xilin Chen", "abstract": "  Visual speech recognition (VSR), also known as lip reading, is the task of\nrecognizing speech from silent video. Despite significant advancements in VSR\nover recent decades, most existing methods pay limited attention to real-world\nvisual challenges such as illumination variations, occlusions, blurring, and\npose changes. To address these challenges, we propose GLip, a Global-Local\nIntegrated Progressive framework designed for robust VSR. GLip is built upon\ntwo key insights: (i) learning an initial coarse alignment between visual\nfeatures across varying conditions and corresponding speech content facilitates\nthe subsequent learning of precise visual-to-speech mappings in challenging\nenvironments; (ii) under adverse conditions, certain local regions (e.g.,\nnon-occluded areas) often exhibit more discriminative cues for lip reading than\nglobal features. To this end, GLip introduces a dual-path feature extraction\narchitecture that integrates both global and local features within a two-stage\nprogressive learning framework. In the first stage, the model learns to align\nboth global and local visual features with corresponding acoustic speech units\nusing easily accessible audio-visual data, establishing a coarse yet\nsemantically robust foundation. In the second stage, we introduce a Contextual\nEnhancement Module (CEM) to dynamically integrate local features with relevant\nglobal context across both spatial and temporal dimensions, refining the coarse\nrepresentations into precise visual-speech mappings. Our framework uniquely\nexploits discriminative local regions through a progressive learning strategy,\ndemonstrating enhanced robustness against various visual challenges and\nconsistently outperforming existing methods on the LRS2 and LRS3 benchmarks. We\nfurther validate its effectiveness on a newly introduced challenging Mandarin\ndataset.\n", "link": "http://arxiv.org/abs/2509.16031v2", "date": "2025-09-26", "relevancy": 2.7902, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLip%3A%20A%20Global-Local%20Integrated%20Progressive%20Framework%20for%20Robust%20Visual%0A%20%20Speech%20Recognition&body=Title%3A%20GLip%3A%20A%20Global-Local%20Integrated%20Progressive%20Framework%20for%20Robust%20Visual%0A%20%20Speech%20Recognition%0AAuthor%3A%20Tianyue%20Wang%20and%20Shuang%20Yang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen%0AAbstract%3A%20%20%20Visual%20speech%20recognition%20%28VSR%29%2C%20also%20known%20as%20lip%20reading%2C%20is%20the%20task%20of%0Arecognizing%20speech%20from%20silent%20video.%20Despite%20significant%20advancements%20in%20VSR%0Aover%20recent%20decades%2C%20most%20existing%20methods%20pay%20limited%20attention%20to%20real-world%0Avisual%20challenges%20such%20as%20illumination%20variations%2C%20occlusions%2C%20blurring%2C%20and%0Apose%20changes.%20To%20address%20these%20challenges%2C%20we%20propose%20GLip%2C%20a%20Global-Local%0AIntegrated%20Progressive%20framework%20designed%20for%20robust%20VSR.%20GLip%20is%20built%20upon%0Atwo%20key%20insights%3A%20%28i%29%20learning%20an%20initial%20coarse%20alignment%20between%20visual%0Afeatures%20across%20varying%20conditions%20and%20corresponding%20speech%20content%20facilitates%0Athe%20subsequent%20learning%20of%20precise%20visual-to-speech%20mappings%20in%20challenging%0Aenvironments%3B%20%28ii%29%20under%20adverse%20conditions%2C%20certain%20local%20regions%20%28e.g.%2C%0Anon-occluded%20areas%29%20often%20exhibit%20more%20discriminative%20cues%20for%20lip%20reading%20than%0Aglobal%20features.%20To%20this%20end%2C%20GLip%20introduces%20a%20dual-path%20feature%20extraction%0Aarchitecture%20that%20integrates%20both%20global%20and%20local%20features%20within%20a%20two-stage%0Aprogressive%20learning%20framework.%20In%20the%20first%20stage%2C%20the%20model%20learns%20to%20align%0Aboth%20global%20and%20local%20visual%20features%20with%20corresponding%20acoustic%20speech%20units%0Ausing%20easily%20accessible%20audio-visual%20data%2C%20establishing%20a%20coarse%20yet%0Asemantically%20robust%20foundation.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20Contextual%0AEnhancement%20Module%20%28CEM%29%20to%20dynamically%20integrate%20local%20features%20with%20relevant%0Aglobal%20context%20across%20both%20spatial%20and%20temporal%20dimensions%2C%20refining%20the%20coarse%0Arepresentations%20into%20precise%20visual-speech%20mappings.%20Our%20framework%20uniquely%0Aexploits%20discriminative%20local%20regions%20through%20a%20progressive%20learning%20strategy%2C%0Ademonstrating%20enhanced%20robustness%20against%20various%20visual%20challenges%20and%0Aconsistently%20outperforming%20existing%20methods%20on%20the%20LRS2%20and%20LRS3%20benchmarks.%20We%0Afurther%20validate%20its%20effectiveness%20on%20a%20newly%20introduced%20challenging%20Mandarin%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLip%253A%2520A%2520Global-Local%2520Integrated%2520Progressive%2520Framework%2520for%2520Robust%2520Visual%250A%2520%2520Speech%2520Recognition%26entry.906535625%3DTianyue%2520Wang%2520and%2520Shuang%2520Yang%2520and%2520Shiguang%2520Shan%2520and%2520Xilin%2520Chen%26entry.1292438233%3D%2520%2520Visual%2520speech%2520recognition%2520%2528VSR%2529%252C%2520also%2520known%2520as%2520lip%2520reading%252C%2520is%2520the%2520task%2520of%250Arecognizing%2520speech%2520from%2520silent%2520video.%2520Despite%2520significant%2520advancements%2520in%2520VSR%250Aover%2520recent%2520decades%252C%2520most%2520existing%2520methods%2520pay%2520limited%2520attention%2520to%2520real-world%250Avisual%2520challenges%2520such%2520as%2520illumination%2520variations%252C%2520occlusions%252C%2520blurring%252C%2520and%250Apose%2520changes.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GLip%252C%2520a%2520Global-Local%250AIntegrated%2520Progressive%2520framework%2520designed%2520for%2520robust%2520VSR.%2520GLip%2520is%2520built%2520upon%250Atwo%2520key%2520insights%253A%2520%2528i%2529%2520learning%2520an%2520initial%2520coarse%2520alignment%2520between%2520visual%250Afeatures%2520across%2520varying%2520conditions%2520and%2520corresponding%2520speech%2520content%2520facilitates%250Athe%2520subsequent%2520learning%2520of%2520precise%2520visual-to-speech%2520mappings%2520in%2520challenging%250Aenvironments%253B%2520%2528ii%2529%2520under%2520adverse%2520conditions%252C%2520certain%2520local%2520regions%2520%2528e.g.%252C%250Anon-occluded%2520areas%2529%2520often%2520exhibit%2520more%2520discriminative%2520cues%2520for%2520lip%2520reading%2520than%250Aglobal%2520features.%2520To%2520this%2520end%252C%2520GLip%2520introduces%2520a%2520dual-path%2520feature%2520extraction%250Aarchitecture%2520that%2520integrates%2520both%2520global%2520and%2520local%2520features%2520within%2520a%2520two-stage%250Aprogressive%2520learning%2520framework.%2520In%2520the%2520first%2520stage%252C%2520the%2520model%2520learns%2520to%2520align%250Aboth%2520global%2520and%2520local%2520visual%2520features%2520with%2520corresponding%2520acoustic%2520speech%2520units%250Ausing%2520easily%2520accessible%2520audio-visual%2520data%252C%2520establishing%2520a%2520coarse%2520yet%250Asemantically%2520robust%2520foundation.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520a%2520Contextual%250AEnhancement%2520Module%2520%2528CEM%2529%2520to%2520dynamically%2520integrate%2520local%2520features%2520with%2520relevant%250Aglobal%2520context%2520across%2520both%2520spatial%2520and%2520temporal%2520dimensions%252C%2520refining%2520the%2520coarse%250Arepresentations%2520into%2520precise%2520visual-speech%2520mappings.%2520Our%2520framework%2520uniquely%250Aexploits%2520discriminative%2520local%2520regions%2520through%2520a%2520progressive%2520learning%2520strategy%252C%250Ademonstrating%2520enhanced%2520robustness%2520against%2520various%2520visual%2520challenges%2520and%250Aconsistently%2520outperforming%2520existing%2520methods%2520on%2520the%2520LRS2%2520and%2520LRS3%2520benchmarks.%2520We%250Afurther%2520validate%2520its%2520effectiveness%2520on%2520a%2520newly%2520introduced%2520challenging%2520Mandarin%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLip%3A%20A%20Global-Local%20Integrated%20Progressive%20Framework%20for%20Robust%20Visual%0A%20%20Speech%20Recognition&entry.906535625=Tianyue%20Wang%20and%20Shuang%20Yang%20and%20Shiguang%20Shan%20and%20Xilin%20Chen&entry.1292438233=%20%20Visual%20speech%20recognition%20%28VSR%29%2C%20also%20known%20as%20lip%20reading%2C%20is%20the%20task%20of%0Arecognizing%20speech%20from%20silent%20video.%20Despite%20significant%20advancements%20in%20VSR%0Aover%20recent%20decades%2C%20most%20existing%20methods%20pay%20limited%20attention%20to%20real-world%0Avisual%20challenges%20such%20as%20illumination%20variations%2C%20occlusions%2C%20blurring%2C%20and%0Apose%20changes.%20To%20address%20these%20challenges%2C%20we%20propose%20GLip%2C%20a%20Global-Local%0AIntegrated%20Progressive%20framework%20designed%20for%20robust%20VSR.%20GLip%20is%20built%20upon%0Atwo%20key%20insights%3A%20%28i%29%20learning%20an%20initial%20coarse%20alignment%20between%20visual%0Afeatures%20across%20varying%20conditions%20and%20corresponding%20speech%20content%20facilitates%0Athe%20subsequent%20learning%20of%20precise%20visual-to-speech%20mappings%20in%20challenging%0Aenvironments%3B%20%28ii%29%20under%20adverse%20conditions%2C%20certain%20local%20regions%20%28e.g.%2C%0Anon-occluded%20areas%29%20often%20exhibit%20more%20discriminative%20cues%20for%20lip%20reading%20than%0Aglobal%20features.%20To%20this%20end%2C%20GLip%20introduces%20a%20dual-path%20feature%20extraction%0Aarchitecture%20that%20integrates%20both%20global%20and%20local%20features%20within%20a%20two-stage%0Aprogressive%20learning%20framework.%20In%20the%20first%20stage%2C%20the%20model%20learns%20to%20align%0Aboth%20global%20and%20local%20visual%20features%20with%20corresponding%20acoustic%20speech%20units%0Ausing%20easily%20accessible%20audio-visual%20data%2C%20establishing%20a%20coarse%20yet%0Asemantically%20robust%20foundation.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20Contextual%0AEnhancement%20Module%20%28CEM%29%20to%20dynamically%20integrate%20local%20features%20with%20relevant%0Aglobal%20context%20across%20both%20spatial%20and%20temporal%20dimensions%2C%20refining%20the%20coarse%0Arepresentations%20into%20precise%20visual-speech%20mappings.%20Our%20framework%20uniquely%0Aexploits%20discriminative%20local%20regions%20through%20a%20progressive%20learning%20strategy%2C%0Ademonstrating%20enhanced%20robustness%20against%20various%20visual%20challenges%20and%0Aconsistently%20outperforming%20existing%20methods%20on%20the%20LRS2%20and%20LRS3%20benchmarks.%20We%0Afurther%20validate%20its%20effectiveness%20on%20a%20newly%20introduced%20challenging%20Mandarin%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16031v2&entry.124074799=Read"},
{"title": "Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised\n  Molecular Graph Pretraining", "author": "Boshra Ariguib and Mathias Niepert and Andrei Manolache", "abstract": "  High-quality molecular representations are essential for property prediction\nand molecular design, yet large labeled datasets remain scarce. While\nself-supervised pretraining on molecular graphs has shown promise, many\nexisting approaches either depend on hand-crafted augmentations or complex\ngenerative objectives, and often rely solely on 2D topology, leaving valuable\n3D structural information underutilized. To address this gap, we introduce\nC-FREE (Contrast-Free Representation learning on Ego-nets), a simple framework\nthat integrates 2D graphs with ensembles of 3D conformers. C-FREE learns\nmolecular representations by predicting subgraph embeddings from their\ncomplementary neighborhoods in the latent space, using fixed-radius ego-nets as\nmodeling units across different conformers. This design allows us to integrate\nboth geometric and topological information within a hybrid Graph Neural Network\n(GNN)-Transformer backbone, without negatives, positional encodings, or\nexpensive pre-processing. Pretraining on the GEOM dataset, which provides rich\n3D conformational diversity, C-FREE achieves state-of-the-art results on\nMoleculeNet, surpassing contrastive, generative, and other multimodal\nself-supervised methods. Fine-tuning across datasets with diverse sizes and\nmolecule types further demonstrates that pretraining transfers effectively to\nnew chemical domains, highlighting the importance of 3D-informed molecular\nrepresentations.\n", "link": "http://arxiv.org/abs/2509.22468v1", "date": "2025-09-26", "relevancy": 2.787, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5764}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5538}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Neighborhood%3A%20Contrast-Free%20Multimodal%20Self-Supervised%0A%20%20Molecular%20Graph%20Pretraining&body=Title%3A%20Learning%20the%20Neighborhood%3A%20Contrast-Free%20Multimodal%20Self-Supervised%0A%20%20Molecular%20Graph%20Pretraining%0AAuthor%3A%20Boshra%20Ariguib%20and%20Mathias%20Niepert%20and%20Andrei%20Manolache%0AAbstract%3A%20%20%20High-quality%20molecular%20representations%20are%20essential%20for%20property%20prediction%0Aand%20molecular%20design%2C%20yet%20large%20labeled%20datasets%20remain%20scarce.%20While%0Aself-supervised%20pretraining%20on%20molecular%20graphs%20has%20shown%20promise%2C%20many%0Aexisting%20approaches%20either%20depend%20on%20hand-crafted%20augmentations%20or%20complex%0Agenerative%20objectives%2C%20and%20often%20rely%20solely%20on%202D%20topology%2C%20leaving%20valuable%0A3D%20structural%20information%20underutilized.%20To%20address%20this%20gap%2C%20we%20introduce%0AC-FREE%20%28Contrast-Free%20Representation%20learning%20on%20Ego-nets%29%2C%20a%20simple%20framework%0Athat%20integrates%202D%20graphs%20with%20ensembles%20of%203D%20conformers.%20C-FREE%20learns%0Amolecular%20representations%20by%20predicting%20subgraph%20embeddings%20from%20their%0Acomplementary%20neighborhoods%20in%20the%20latent%20space%2C%20using%20fixed-radius%20ego-nets%20as%0Amodeling%20units%20across%20different%20conformers.%20This%20design%20allows%20us%20to%20integrate%0Aboth%20geometric%20and%20topological%20information%20within%20a%20hybrid%20Graph%20Neural%20Network%0A%28GNN%29-Transformer%20backbone%2C%20without%20negatives%2C%20positional%20encodings%2C%20or%0Aexpensive%20pre-processing.%20Pretraining%20on%20the%20GEOM%20dataset%2C%20which%20provides%20rich%0A3D%20conformational%20diversity%2C%20C-FREE%20achieves%20state-of-the-art%20results%20on%0AMoleculeNet%2C%20surpassing%20contrastive%2C%20generative%2C%20and%20other%20multimodal%0Aself-supervised%20methods.%20Fine-tuning%20across%20datasets%20with%20diverse%20sizes%20and%0Amolecule%20types%20further%20demonstrates%20that%20pretraining%20transfers%20effectively%20to%0Anew%20chemical%20domains%2C%20highlighting%20the%20importance%20of%203D-informed%20molecular%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520Neighborhood%253A%2520Contrast-Free%2520Multimodal%2520Self-Supervised%250A%2520%2520Molecular%2520Graph%2520Pretraining%26entry.906535625%3DBoshra%2520Ariguib%2520and%2520Mathias%2520Niepert%2520and%2520Andrei%2520Manolache%26entry.1292438233%3D%2520%2520High-quality%2520molecular%2520representations%2520are%2520essential%2520for%2520property%2520prediction%250Aand%2520molecular%2520design%252C%2520yet%2520large%2520labeled%2520datasets%2520remain%2520scarce.%2520While%250Aself-supervised%2520pretraining%2520on%2520molecular%2520graphs%2520has%2520shown%2520promise%252C%2520many%250Aexisting%2520approaches%2520either%2520depend%2520on%2520hand-crafted%2520augmentations%2520or%2520complex%250Agenerative%2520objectives%252C%2520and%2520often%2520rely%2520solely%2520on%25202D%2520topology%252C%2520leaving%2520valuable%250A3D%2520structural%2520information%2520underutilized.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AC-FREE%2520%2528Contrast-Free%2520Representation%2520learning%2520on%2520Ego-nets%2529%252C%2520a%2520simple%2520framework%250Athat%2520integrates%25202D%2520graphs%2520with%2520ensembles%2520of%25203D%2520conformers.%2520C-FREE%2520learns%250Amolecular%2520representations%2520by%2520predicting%2520subgraph%2520embeddings%2520from%2520their%250Acomplementary%2520neighborhoods%2520in%2520the%2520latent%2520space%252C%2520using%2520fixed-radius%2520ego-nets%2520as%250Amodeling%2520units%2520across%2520different%2520conformers.%2520This%2520design%2520allows%2520us%2520to%2520integrate%250Aboth%2520geometric%2520and%2520topological%2520information%2520within%2520a%2520hybrid%2520Graph%2520Neural%2520Network%250A%2528GNN%2529-Transformer%2520backbone%252C%2520without%2520negatives%252C%2520positional%2520encodings%252C%2520or%250Aexpensive%2520pre-processing.%2520Pretraining%2520on%2520the%2520GEOM%2520dataset%252C%2520which%2520provides%2520rich%250A3D%2520conformational%2520diversity%252C%2520C-FREE%2520achieves%2520state-of-the-art%2520results%2520on%250AMoleculeNet%252C%2520surpassing%2520contrastive%252C%2520generative%252C%2520and%2520other%2520multimodal%250Aself-supervised%2520methods.%2520Fine-tuning%2520across%2520datasets%2520with%2520diverse%2520sizes%2520and%250Amolecule%2520types%2520further%2520demonstrates%2520that%2520pretraining%2520transfers%2520effectively%2520to%250Anew%2520chemical%2520domains%252C%2520highlighting%2520the%2520importance%2520of%25203D-informed%2520molecular%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Neighborhood%3A%20Contrast-Free%20Multimodal%20Self-Supervised%0A%20%20Molecular%20Graph%20Pretraining&entry.906535625=Boshra%20Ariguib%20and%20Mathias%20Niepert%20and%20Andrei%20Manolache&entry.1292438233=%20%20High-quality%20molecular%20representations%20are%20essential%20for%20property%20prediction%0Aand%20molecular%20design%2C%20yet%20large%20labeled%20datasets%20remain%20scarce.%20While%0Aself-supervised%20pretraining%20on%20molecular%20graphs%20has%20shown%20promise%2C%20many%0Aexisting%20approaches%20either%20depend%20on%20hand-crafted%20augmentations%20or%20complex%0Agenerative%20objectives%2C%20and%20often%20rely%20solely%20on%202D%20topology%2C%20leaving%20valuable%0A3D%20structural%20information%20underutilized.%20To%20address%20this%20gap%2C%20we%20introduce%0AC-FREE%20%28Contrast-Free%20Representation%20learning%20on%20Ego-nets%29%2C%20a%20simple%20framework%0Athat%20integrates%202D%20graphs%20with%20ensembles%20of%203D%20conformers.%20C-FREE%20learns%0Amolecular%20representations%20by%20predicting%20subgraph%20embeddings%20from%20their%0Acomplementary%20neighborhoods%20in%20the%20latent%20space%2C%20using%20fixed-radius%20ego-nets%20as%0Amodeling%20units%20across%20different%20conformers.%20This%20design%20allows%20us%20to%20integrate%0Aboth%20geometric%20and%20topological%20information%20within%20a%20hybrid%20Graph%20Neural%20Network%0A%28GNN%29-Transformer%20backbone%2C%20without%20negatives%2C%20positional%20encodings%2C%20or%0Aexpensive%20pre-processing.%20Pretraining%20on%20the%20GEOM%20dataset%2C%20which%20provides%20rich%0A3D%20conformational%20diversity%2C%20C-FREE%20achieves%20state-of-the-art%20results%20on%0AMoleculeNet%2C%20surpassing%20contrastive%2C%20generative%2C%20and%20other%20multimodal%0Aself-supervised%20methods.%20Fine-tuning%20across%20datasets%20with%20diverse%20sizes%20and%0Amolecule%20types%20further%20demonstrates%20that%20pretraining%20transfers%20effectively%20to%0Anew%20chemical%20domains%2C%20highlighting%20the%20importance%20of%203D-informed%20molecular%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22468v1&entry.124074799=Read"},
{"title": "Context and Diversity Matter: The Emergence of In-Context Learning in\n  World Models", "author": "Fan Wang and Zhiyuan Chen and Yuxuan Zhong and Sunjian Zheng and Pengtao Shao and Bo Yu and Shaoshan Liu and Jianan Wang and Ning Ding and Yang Cao and Yu Kang", "abstract": "  The capability of predicting environmental dynamics underpins both biological\nneural systems and general embodied AI in adapting to their surroundings. Yet\nprevailing approaches rest on static world models that falter when confronted\nwith novel or rare configurations. We investigate in-context environment\nlearning (ICEL), shifting attention from zero-shot performance to the growth\nand asymptotic limits of the world model. Our contributions are three-fold: (1)\nwe formalize in-context learning of a world model and identify two core\nmechanisms: environment recognition and environment learning; (2) we derive\nerror upper-bounds for both mechanisms that expose how the mechanisms emerge;\nand (3) we empirically confirm that distinct ICL mechanisms exist in the world\nmodel, and we further investigate how data distribution and model architecture\naffect ICL in a manner consistent with theory. These findings demonstrate the\npotential of self-adapting world models and highlight the key factors behind\nthe emergence of ICEL, most notably the necessity of long context and diverse\nenvironments.\n", "link": "http://arxiv.org/abs/2509.22353v1", "date": "2025-09-26", "relevancy": 2.7867, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5655}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20and%20Diversity%20Matter%3A%20The%20Emergence%20of%20In-Context%20Learning%20in%0A%20%20World%20Models&body=Title%3A%20Context%20and%20Diversity%20Matter%3A%20The%20Emergence%20of%20In-Context%20Learning%20in%0A%20%20World%20Models%0AAuthor%3A%20Fan%20Wang%20and%20Zhiyuan%20Chen%20and%20Yuxuan%20Zhong%20and%20Sunjian%20Zheng%20and%20Pengtao%20Shao%20and%20Bo%20Yu%20and%20Shaoshan%20Liu%20and%20Jianan%20Wang%20and%20Ning%20Ding%20and%20Yang%20Cao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20The%20capability%20of%20predicting%20environmental%20dynamics%20underpins%20both%20biological%0Aneural%20systems%20and%20general%20embodied%20AI%20in%20adapting%20to%20their%20surroundings.%20Yet%0Aprevailing%20approaches%20rest%20on%20static%20world%20models%20that%20falter%20when%20confronted%0Awith%20novel%20or%20rare%20configurations.%20We%20investigate%20in-context%20environment%0Alearning%20%28ICEL%29%2C%20shifting%20attention%20from%20zero-shot%20performance%20to%20the%20growth%0Aand%20asymptotic%20limits%20of%20the%20world%20model.%20Our%20contributions%20are%20three-fold%3A%20%281%29%0Awe%20formalize%20in-context%20learning%20of%20a%20world%20model%20and%20identify%20two%20core%0Amechanisms%3A%20environment%20recognition%20and%20environment%20learning%3B%20%282%29%20we%20derive%0Aerror%20upper-bounds%20for%20both%20mechanisms%20that%20expose%20how%20the%20mechanisms%20emerge%3B%0Aand%20%283%29%20we%20empirically%20confirm%20that%20distinct%20ICL%20mechanisms%20exist%20in%20the%20world%0Amodel%2C%20and%20we%20further%20investigate%20how%20data%20distribution%20and%20model%20architecture%0Aaffect%20ICL%20in%20a%20manner%20consistent%20with%20theory.%20These%20findings%20demonstrate%20the%0Apotential%20of%20self-adapting%20world%20models%20and%20highlight%20the%20key%20factors%20behind%0Athe%20emergence%20of%20ICEL%2C%20most%20notably%20the%20necessity%20of%20long%20context%20and%20diverse%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520and%2520Diversity%2520Matter%253A%2520The%2520Emergence%2520of%2520In-Context%2520Learning%2520in%250A%2520%2520World%2520Models%26entry.906535625%3DFan%2520Wang%2520and%2520Zhiyuan%2520Chen%2520and%2520Yuxuan%2520Zhong%2520and%2520Sunjian%2520Zheng%2520and%2520Pengtao%2520Shao%2520and%2520Bo%2520Yu%2520and%2520Shaoshan%2520Liu%2520and%2520Jianan%2520Wang%2520and%2520Ning%2520Ding%2520and%2520Yang%2520Cao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520The%2520capability%2520of%2520predicting%2520environmental%2520dynamics%2520underpins%2520both%2520biological%250Aneural%2520systems%2520and%2520general%2520embodied%2520AI%2520in%2520adapting%2520to%2520their%2520surroundings.%2520Yet%250Aprevailing%2520approaches%2520rest%2520on%2520static%2520world%2520models%2520that%2520falter%2520when%2520confronted%250Awith%2520novel%2520or%2520rare%2520configurations.%2520We%2520investigate%2520in-context%2520environment%250Alearning%2520%2528ICEL%2529%252C%2520shifting%2520attention%2520from%2520zero-shot%2520performance%2520to%2520the%2520growth%250Aand%2520asymptotic%2520limits%2520of%2520the%2520world%2520model.%2520Our%2520contributions%2520are%2520three-fold%253A%2520%25281%2529%250Awe%2520formalize%2520in-context%2520learning%2520of%2520a%2520world%2520model%2520and%2520identify%2520two%2520core%250Amechanisms%253A%2520environment%2520recognition%2520and%2520environment%2520learning%253B%2520%25282%2529%2520we%2520derive%250Aerror%2520upper-bounds%2520for%2520both%2520mechanisms%2520that%2520expose%2520how%2520the%2520mechanisms%2520emerge%253B%250Aand%2520%25283%2529%2520we%2520empirically%2520confirm%2520that%2520distinct%2520ICL%2520mechanisms%2520exist%2520in%2520the%2520world%250Amodel%252C%2520and%2520we%2520further%2520investigate%2520how%2520data%2520distribution%2520and%2520model%2520architecture%250Aaffect%2520ICL%2520in%2520a%2520manner%2520consistent%2520with%2520theory.%2520These%2520findings%2520demonstrate%2520the%250Apotential%2520of%2520self-adapting%2520world%2520models%2520and%2520highlight%2520the%2520key%2520factors%2520behind%250Athe%2520emergence%2520of%2520ICEL%252C%2520most%2520notably%2520the%2520necessity%2520of%2520long%2520context%2520and%2520diverse%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20and%20Diversity%20Matter%3A%20The%20Emergence%20of%20In-Context%20Learning%20in%0A%20%20World%20Models&entry.906535625=Fan%20Wang%20and%20Zhiyuan%20Chen%20and%20Yuxuan%20Zhong%20and%20Sunjian%20Zheng%20and%20Pengtao%20Shao%20and%20Bo%20Yu%20and%20Shaoshan%20Liu%20and%20Jianan%20Wang%20and%20Ning%20Ding%20and%20Yang%20Cao%20and%20Yu%20Kang&entry.1292438233=%20%20The%20capability%20of%20predicting%20environmental%20dynamics%20underpins%20both%20biological%0Aneural%20systems%20and%20general%20embodied%20AI%20in%20adapting%20to%20their%20surroundings.%20Yet%0Aprevailing%20approaches%20rest%20on%20static%20world%20models%20that%20falter%20when%20confronted%0Awith%20novel%20or%20rare%20configurations.%20We%20investigate%20in-context%20environment%0Alearning%20%28ICEL%29%2C%20shifting%20attention%20from%20zero-shot%20performance%20to%20the%20growth%0Aand%20asymptotic%20limits%20of%20the%20world%20model.%20Our%20contributions%20are%20three-fold%3A%20%281%29%0Awe%20formalize%20in-context%20learning%20of%20a%20world%20model%20and%20identify%20two%20core%0Amechanisms%3A%20environment%20recognition%20and%20environment%20learning%3B%20%282%29%20we%20derive%0Aerror%20upper-bounds%20for%20both%20mechanisms%20that%20expose%20how%20the%20mechanisms%20emerge%3B%0Aand%20%283%29%20we%20empirically%20confirm%20that%20distinct%20ICL%20mechanisms%20exist%20in%20the%20world%0Amodel%2C%20and%20we%20further%20investigate%20how%20data%20distribution%20and%20model%20architecture%0Aaffect%20ICL%20in%20a%20manner%20consistent%20with%20theory.%20These%20findings%20demonstrate%20the%0Apotential%20of%20self-adapting%20world%20models%20and%20highlight%20the%20key%20factors%20behind%0Athe%20emergence%20of%20ICEL%2C%20most%20notably%20the%20necessity%20of%20long%20context%20and%20diverse%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22353v1&entry.124074799=Read"},
{"title": "Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling", "author": "Zhe Chen and Weiyun Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Erfei Cui and Jinguo Zhu and Shenglong Ye and Hao Tian and Zhaoyang Liu and Lixin Gu and Xuehui Wang and Qingyun Li and Yiming Ren and Zixuan Chen and Jiapeng Luo and Jiahao Wang and Tan Jiang and Bo Wang and Conghui He and Botian Shi and Xingcheng Zhang and Han Lv and Yi Wang and Wenqi Shao and Pei Chu and Zhongying Tu and Tong He and Zhiyong Wu and Huipeng Deng and Jiaye Ge and Kai Chen and Kaipeng Zhang and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang", "abstract": "  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n", "link": "http://arxiv.org/abs/2412.05271v5", "date": "2025-09-26", "relevancy": 2.7765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling&body=Title%3A%20Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling%0AAuthor%3A%20Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jinguo%20Zhu%20and%20Shenglong%20Ye%20and%20Hao%20Tian%20and%20Zhaoyang%20Liu%20and%20Lixin%20Gu%20and%20Xuehui%20Wang%20and%20Qingyun%20Li%20and%20Yiming%20Ren%20and%20Zixuan%20Chen%20and%20Jiapeng%20Luo%20and%20Jiahao%20Wang%20and%20Tan%20Jiang%20and%20Bo%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Xingcheng%20Zhang%20and%20Han%20Lv%20and%20Yi%20Wang%20and%20Wenqi%20Shao%20and%20Pei%20Chu%20and%20Zhongying%20Tu%20and%20Tong%20He%20and%20Zhiyong%20Wu%20and%20Huipeng%20Deng%20and%20Jiaye%20Ge%20and%20Kai%20Chen%20and%20Kaipeng%20Zhang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang%0AAbstract%3A%20%20%20We%20introduce%20InternVL%202.5%2C%20an%20advanced%20multimodal%20large%20language%20model%20%28MLLM%29%0Aseries%20that%20builds%20upon%20InternVL%202.0%2C%20maintaining%20its%20core%20model%20architecture%0Awhile%20introducing%20significant%20enhancements%20in%20training%20and%20testing%20strategies%0Aas%20well%20as%20data%20quality.%20In%20this%20work%2C%20we%20delve%20into%20the%20relationship%20between%0Amodel%20scaling%20and%20performance%2C%20systematically%20exploring%20the%20performance%20trends%0Ain%20vision%20encoders%2C%20language%20models%2C%20dataset%20sizes%2C%20and%20test-time%0Aconfigurations.%20Through%20extensive%20evaluations%20on%20a%20wide%20range%20of%20benchmarks%2C%0Aincluding%20multi-discipline%20reasoning%2C%20document%20understanding%2C%20multi-image%20/%0Avideo%20understanding%2C%20real-world%20comprehension%2C%20multimodal%20hallucination%0Adetection%2C%20visual%20grounding%2C%20multilingual%20capabilities%2C%20and%20pure%20language%0Aprocessing%2C%20InternVL%202.5%20exhibits%20competitive%20performance%2C%20rivaling%20leading%0Acommercial%20models%20such%20as%20GPT-4o%20and%20Claude-3.5-Sonnet.%20Notably%2C%20our%20model%20is%0Athe%20first%20open-source%20MLLMs%20to%20surpass%2070%25%20on%20the%20MMMU%20benchmark%2C%20achieving%20a%0A3.7-point%20improvement%20through%20Chain-of-Thought%20%28CoT%29%20reasoning%20and%20showcasing%0Astrong%20potential%20for%20test-time%20scaling.%20We%20hope%20this%20model%20contributes%20to%20the%0Aopen-source%20community%20by%20setting%20new%20standards%20for%20developing%20and%20applying%0Amultimodal%20AI%20systems.%20HuggingFace%20demo%20see%0Ahttps%3A//huggingface.co/spaces/OpenGVLab/InternVL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05271v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpanding%2520Performance%2520Boundaries%2520of%2520Open-Source%2520Multimodal%2520Models%2520with%250A%2520%2520Model%252C%2520Data%252C%2520and%2520Test-Time%2520Scaling%26entry.906535625%3DZhe%2520Chen%2520and%2520Weiyun%2520Wang%2520and%2520Yue%2520Cao%2520and%2520Yangzhou%2520Liu%2520and%2520Zhangwei%2520Gao%2520and%2520Erfei%2520Cui%2520and%2520Jinguo%2520Zhu%2520and%2520Shenglong%2520Ye%2520and%2520Hao%2520Tian%2520and%2520Zhaoyang%2520Liu%2520and%2520Lixin%2520Gu%2520and%2520Xuehui%2520Wang%2520and%2520Qingyun%2520Li%2520and%2520Yiming%2520Ren%2520and%2520Zixuan%2520Chen%2520and%2520Jiapeng%2520Luo%2520and%2520Jiahao%2520Wang%2520and%2520Tan%2520Jiang%2520and%2520Bo%2520Wang%2520and%2520Conghui%2520He%2520and%2520Botian%2520Shi%2520and%2520Xingcheng%2520Zhang%2520and%2520Han%2520Lv%2520and%2520Yi%2520Wang%2520and%2520Wenqi%2520Shao%2520and%2520Pei%2520Chu%2520and%2520Zhongying%2520Tu%2520and%2520Tong%2520He%2520and%2520Zhiyong%2520Wu%2520and%2520Huipeng%2520Deng%2520and%2520Jiaye%2520Ge%2520and%2520Kai%2520Chen%2520and%2520Kaipeng%2520Zhang%2520and%2520Limin%2520Wang%2520and%2520Min%2520Dou%2520and%2520Lewei%2520Lu%2520and%2520Xizhou%2520Zhu%2520and%2520Tong%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%2520and%2520Wenhai%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520InternVL%25202.5%252C%2520an%2520advanced%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%250Aseries%2520that%2520builds%2520upon%2520InternVL%25202.0%252C%2520maintaining%2520its%2520core%2520model%2520architecture%250Awhile%2520introducing%2520significant%2520enhancements%2520in%2520training%2520and%2520testing%2520strategies%250Aas%2520well%2520as%2520data%2520quality.%2520In%2520this%2520work%252C%2520we%2520delve%2520into%2520the%2520relationship%2520between%250Amodel%2520scaling%2520and%2520performance%252C%2520systematically%2520exploring%2520the%2520performance%2520trends%250Ain%2520vision%2520encoders%252C%2520language%2520models%252C%2520dataset%2520sizes%252C%2520and%2520test-time%250Aconfigurations.%2520Through%2520extensive%2520evaluations%2520on%2520a%2520wide%2520range%2520of%2520benchmarks%252C%250Aincluding%2520multi-discipline%2520reasoning%252C%2520document%2520understanding%252C%2520multi-image%2520/%250Avideo%2520understanding%252C%2520real-world%2520comprehension%252C%2520multimodal%2520hallucination%250Adetection%252C%2520visual%2520grounding%252C%2520multilingual%2520capabilities%252C%2520and%2520pure%2520language%250Aprocessing%252C%2520InternVL%25202.5%2520exhibits%2520competitive%2520performance%252C%2520rivaling%2520leading%250Acommercial%2520models%2520such%2520as%2520GPT-4o%2520and%2520Claude-3.5-Sonnet.%2520Notably%252C%2520our%2520model%2520is%250Athe%2520first%2520open-source%2520MLLMs%2520to%2520surpass%252070%2525%2520on%2520the%2520MMMU%2520benchmark%252C%2520achieving%2520a%250A3.7-point%2520improvement%2520through%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520and%2520showcasing%250Astrong%2520potential%2520for%2520test-time%2520scaling.%2520We%2520hope%2520this%2520model%2520contributes%2520to%2520the%250Aopen-source%2520community%2520by%2520setting%2520new%2520standards%2520for%2520developing%2520and%2520applying%250Amultimodal%2520AI%2520systems.%2520HuggingFace%2520demo%2520see%250Ahttps%253A//huggingface.co/spaces/OpenGVLab/InternVL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05271v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expanding%20Performance%20Boundaries%20of%20Open-Source%20Multimodal%20Models%20with%0A%20%20Model%2C%20Data%2C%20and%20Test-Time%20Scaling&entry.906535625=Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Yue%20Cao%20and%20Yangzhou%20Liu%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jinguo%20Zhu%20and%20Shenglong%20Ye%20and%20Hao%20Tian%20and%20Zhaoyang%20Liu%20and%20Lixin%20Gu%20and%20Xuehui%20Wang%20and%20Qingyun%20Li%20and%20Yiming%20Ren%20and%20Zixuan%20Chen%20and%20Jiapeng%20Luo%20and%20Jiahao%20Wang%20and%20Tan%20Jiang%20and%20Bo%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Xingcheng%20Zhang%20and%20Han%20Lv%20and%20Yi%20Wang%20and%20Wenqi%20Shao%20and%20Pei%20Chu%20and%20Zhongying%20Tu%20and%20Tong%20He%20and%20Zhiyong%20Wu%20and%20Huipeng%20Deng%20and%20Jiaye%20Ge%20and%20Kai%20Chen%20and%20Kaipeng%20Zhang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Lewei%20Lu%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%20and%20Wenhai%20Wang&entry.1292438233=%20%20We%20introduce%20InternVL%202.5%2C%20an%20advanced%20multimodal%20large%20language%20model%20%28MLLM%29%0Aseries%20that%20builds%20upon%20InternVL%202.0%2C%20maintaining%20its%20core%20model%20architecture%0Awhile%20introducing%20significant%20enhancements%20in%20training%20and%20testing%20strategies%0Aas%20well%20as%20data%20quality.%20In%20this%20work%2C%20we%20delve%20into%20the%20relationship%20between%0Amodel%20scaling%20and%20performance%2C%20systematically%20exploring%20the%20performance%20trends%0Ain%20vision%20encoders%2C%20language%20models%2C%20dataset%20sizes%2C%20and%20test-time%0Aconfigurations.%20Through%20extensive%20evaluations%20on%20a%20wide%20range%20of%20benchmarks%2C%0Aincluding%20multi-discipline%20reasoning%2C%20document%20understanding%2C%20multi-image%20/%0Avideo%20understanding%2C%20real-world%20comprehension%2C%20multimodal%20hallucination%0Adetection%2C%20visual%20grounding%2C%20multilingual%20capabilities%2C%20and%20pure%20language%0Aprocessing%2C%20InternVL%202.5%20exhibits%20competitive%20performance%2C%20rivaling%20leading%0Acommercial%20models%20such%20as%20GPT-4o%20and%20Claude-3.5-Sonnet.%20Notably%2C%20our%20model%20is%0Athe%20first%20open-source%20MLLMs%20to%20surpass%2070%25%20on%20the%20MMMU%20benchmark%2C%20achieving%20a%0A3.7-point%20improvement%20through%20Chain-of-Thought%20%28CoT%29%20reasoning%20and%20showcasing%0Astrong%20potential%20for%20test-time%20scaling.%20We%20hope%20this%20model%20contributes%20to%20the%0Aopen-source%20community%20by%20setting%20new%20standards%20for%20developing%20and%20applying%0Amultimodal%20AI%20systems.%20HuggingFace%20demo%20see%0Ahttps%3A//huggingface.co/spaces/OpenGVLab/InternVL%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05271v5&entry.124074799=Read"},
{"title": "Representing LLMs in Prompt Semantic Task Space", "author": "Idan Kashani and Avi Mendelson and Yaniv Nemcovsky", "abstract": "  Large language models (LLMs) achieve impressive results over various tasks,\nand ever-expanding public repositories contain an abundance of pre-trained\nmodels. Therefore, identifying the best-performing LLM for a given task is a\nsignificant challenge. Previous works have suggested learning LLM\nrepresentations to address this. However, these approaches present limited\nscalability and require costly retraining to encompass additional models and\ndatasets. Moreover, the produced representation utilizes distinct spaces that\ncannot be easily interpreted. This work presents an efficient, training-free\napproach to representing LLMs as linear operators within the prompts' semantic\ntask space, thus providing a highly interpretable representation of the models'\napplication. Our method utilizes closed-form computation of geometrical\nproperties and ensures exceptional scalability and real-time adaptability to\ndynamically expanding repositories. We demonstrate our approach on success\nprediction and model selection tasks, achieving competitive or state-of-the-art\nresults with notable performance in out-of-sample scenarios.\n", "link": "http://arxiv.org/abs/2509.22506v1", "date": "2025-09-26", "relevancy": 2.7524, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representing%20LLMs%20in%20Prompt%20Semantic%20Task%20Space&body=Title%3A%20Representing%20LLMs%20in%20Prompt%20Semantic%20Task%20Space%0AAuthor%3A%20Idan%20Kashani%20and%20Avi%20Mendelson%20and%20Yaniv%20Nemcovsky%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20achieve%20impressive%20results%20over%20various%20tasks%2C%0Aand%20ever-expanding%20public%20repositories%20contain%20an%20abundance%20of%20pre-trained%0Amodels.%20Therefore%2C%20identifying%20the%20best-performing%20LLM%20for%20a%20given%20task%20is%20a%0Asignificant%20challenge.%20Previous%20works%20have%20suggested%20learning%20LLM%0Arepresentations%20to%20address%20this.%20However%2C%20these%20approaches%20present%20limited%0Ascalability%20and%20require%20costly%20retraining%20to%20encompass%20additional%20models%20and%0Adatasets.%20Moreover%2C%20the%20produced%20representation%20utilizes%20distinct%20spaces%20that%0Acannot%20be%20easily%20interpreted.%20This%20work%20presents%20an%20efficient%2C%20training-free%0Aapproach%20to%20representing%20LLMs%20as%20linear%20operators%20within%20the%20prompts%27%20semantic%0Atask%20space%2C%20thus%20providing%20a%20highly%20interpretable%20representation%20of%20the%20models%27%0Aapplication.%20Our%20method%20utilizes%20closed-form%20computation%20of%20geometrical%0Aproperties%20and%20ensures%20exceptional%20scalability%20and%20real-time%20adaptability%20to%0Adynamically%20expanding%20repositories.%20We%20demonstrate%20our%20approach%20on%20success%0Aprediction%20and%20model%20selection%20tasks%2C%20achieving%20competitive%20or%20state-of-the-art%0Aresults%20with%20notable%20performance%20in%20out-of-sample%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresenting%2520LLMs%2520in%2520Prompt%2520Semantic%2520Task%2520Space%26entry.906535625%3DIdan%2520Kashani%2520and%2520Avi%2520Mendelson%2520and%2520Yaniv%2520Nemcovsky%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520achieve%2520impressive%2520results%2520over%2520various%2520tasks%252C%250Aand%2520ever-expanding%2520public%2520repositories%2520contain%2520an%2520abundance%2520of%2520pre-trained%250Amodels.%2520Therefore%252C%2520identifying%2520the%2520best-performing%2520LLM%2520for%2520a%2520given%2520task%2520is%2520a%250Asignificant%2520challenge.%2520Previous%2520works%2520have%2520suggested%2520learning%2520LLM%250Arepresentations%2520to%2520address%2520this.%2520However%252C%2520these%2520approaches%2520present%2520limited%250Ascalability%2520and%2520require%2520costly%2520retraining%2520to%2520encompass%2520additional%2520models%2520and%250Adatasets.%2520Moreover%252C%2520the%2520produced%2520representation%2520utilizes%2520distinct%2520spaces%2520that%250Acannot%2520be%2520easily%2520interpreted.%2520This%2520work%2520presents%2520an%2520efficient%252C%2520training-free%250Aapproach%2520to%2520representing%2520LLMs%2520as%2520linear%2520operators%2520within%2520the%2520prompts%2527%2520semantic%250Atask%2520space%252C%2520thus%2520providing%2520a%2520highly%2520interpretable%2520representation%2520of%2520the%2520models%2527%250Aapplication.%2520Our%2520method%2520utilizes%2520closed-form%2520computation%2520of%2520geometrical%250Aproperties%2520and%2520ensures%2520exceptional%2520scalability%2520and%2520real-time%2520adaptability%2520to%250Adynamically%2520expanding%2520repositories.%2520We%2520demonstrate%2520our%2520approach%2520on%2520success%250Aprediction%2520and%2520model%2520selection%2520tasks%252C%2520achieving%2520competitive%2520or%2520state-of-the-art%250Aresults%2520with%2520notable%2520performance%2520in%2520out-of-sample%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representing%20LLMs%20in%20Prompt%20Semantic%20Task%20Space&entry.906535625=Idan%20Kashani%20and%20Avi%20Mendelson%20and%20Yaniv%20Nemcovsky&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20achieve%20impressive%20results%20over%20various%20tasks%2C%0Aand%20ever-expanding%20public%20repositories%20contain%20an%20abundance%20of%20pre-trained%0Amodels.%20Therefore%2C%20identifying%20the%20best-performing%20LLM%20for%20a%20given%20task%20is%20a%0Asignificant%20challenge.%20Previous%20works%20have%20suggested%20learning%20LLM%0Arepresentations%20to%20address%20this.%20However%2C%20these%20approaches%20present%20limited%0Ascalability%20and%20require%20costly%20retraining%20to%20encompass%20additional%20models%20and%0Adatasets.%20Moreover%2C%20the%20produced%20representation%20utilizes%20distinct%20spaces%20that%0Acannot%20be%20easily%20interpreted.%20This%20work%20presents%20an%20efficient%2C%20training-free%0Aapproach%20to%20representing%20LLMs%20as%20linear%20operators%20within%20the%20prompts%27%20semantic%0Atask%20space%2C%20thus%20providing%20a%20highly%20interpretable%20representation%20of%20the%20models%27%0Aapplication.%20Our%20method%20utilizes%20closed-form%20computation%20of%20geometrical%0Aproperties%20and%20ensures%20exceptional%20scalability%20and%20real-time%20adaptability%20to%0Adynamically%20expanding%20repositories.%20We%20demonstrate%20our%20approach%20on%20success%0Aprediction%20and%20model%20selection%20tasks%2C%20achieving%20competitive%20or%20state-of-the-art%0Aresults%20with%20notable%20performance%20in%20out-of-sample%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22506v1&entry.124074799=Read"},
{"title": "Rule-Based Reinforcement Learning for Document Image Classification with\n  Vision Language Models", "author": "Michael Jungo and Andreas Fischer", "abstract": "  Rule-based reinforcement learning has been gaining popularity ever since\nDeepSeek-R1 has demonstrated its success through simple verifiable rewards. In\nthe domain of document analysis, reinforcement learning is not as prevalent,\neven though many downstream tasks may benefit from the emerging properties of\nreinforcement learning, particularly the enhanced reason capabilities. We study\nthe effects of rule-based reinforcement learning with the task of Document\nImage Classification which is one of the most commonly studied downstream tasks\nin document analysis. We find that reinforcement learning tends to have better\ngeneralisation capabilities to out-of-distritbution data, which we examine in\nthree different scenarios, namely out-of-distribution images, unseen classes\nand different modalities. Our code is available at\nhttps://github.com/jungomi/vision-finetune.\n", "link": "http://arxiv.org/abs/2509.22283v1", "date": "2025-09-26", "relevancy": 2.7363, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rule-Based%20Reinforcement%20Learning%20for%20Document%20Image%20Classification%20with%0A%20%20Vision%20Language%20Models&body=Title%3A%20Rule-Based%20Reinforcement%20Learning%20for%20Document%20Image%20Classification%20with%0A%20%20Vision%20Language%20Models%0AAuthor%3A%20Michael%20Jungo%20and%20Andreas%20Fischer%0AAbstract%3A%20%20%20Rule-based%20reinforcement%20learning%20has%20been%20gaining%20popularity%20ever%20since%0ADeepSeek-R1%20has%20demonstrated%20its%20success%20through%20simple%20verifiable%20rewards.%20In%0Athe%20domain%20of%20document%20analysis%2C%20reinforcement%20learning%20is%20not%20as%20prevalent%2C%0Aeven%20though%20many%20downstream%20tasks%20may%20benefit%20from%20the%20emerging%20properties%20of%0Areinforcement%20learning%2C%20particularly%20the%20enhanced%20reason%20capabilities.%20We%20study%0Athe%20effects%20of%20rule-based%20reinforcement%20learning%20with%20the%20task%20of%20Document%0AImage%20Classification%20which%20is%20one%20of%20the%20most%20commonly%20studied%20downstream%20tasks%0Ain%20document%20analysis.%20We%20find%20that%20reinforcement%20learning%20tends%20to%20have%20better%0Ageneralisation%20capabilities%20to%20out-of-distritbution%20data%2C%20which%20we%20examine%20in%0Athree%20different%20scenarios%2C%20namely%20out-of-distribution%20images%2C%20unseen%20classes%0Aand%20different%20modalities.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jungomi/vision-finetune.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRule-Based%2520Reinforcement%2520Learning%2520for%2520Document%2520Image%2520Classification%2520with%250A%2520%2520Vision%2520Language%2520Models%26entry.906535625%3DMichael%2520Jungo%2520and%2520Andreas%2520Fischer%26entry.1292438233%3D%2520%2520Rule-based%2520reinforcement%2520learning%2520has%2520been%2520gaining%2520popularity%2520ever%2520since%250ADeepSeek-R1%2520has%2520demonstrated%2520its%2520success%2520through%2520simple%2520verifiable%2520rewards.%2520In%250Athe%2520domain%2520of%2520document%2520analysis%252C%2520reinforcement%2520learning%2520is%2520not%2520as%2520prevalent%252C%250Aeven%2520though%2520many%2520downstream%2520tasks%2520may%2520benefit%2520from%2520the%2520emerging%2520properties%2520of%250Areinforcement%2520learning%252C%2520particularly%2520the%2520enhanced%2520reason%2520capabilities.%2520We%2520study%250Athe%2520effects%2520of%2520rule-based%2520reinforcement%2520learning%2520with%2520the%2520task%2520of%2520Document%250AImage%2520Classification%2520which%2520is%2520one%2520of%2520the%2520most%2520commonly%2520studied%2520downstream%2520tasks%250Ain%2520document%2520analysis.%2520We%2520find%2520that%2520reinforcement%2520learning%2520tends%2520to%2520have%2520better%250Ageneralisation%2520capabilities%2520to%2520out-of-distritbution%2520data%252C%2520which%2520we%2520examine%2520in%250Athree%2520different%2520scenarios%252C%2520namely%2520out-of-distribution%2520images%252C%2520unseen%2520classes%250Aand%2520different%2520modalities.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/jungomi/vision-finetune.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rule-Based%20Reinforcement%20Learning%20for%20Document%20Image%20Classification%20with%0A%20%20Vision%20Language%20Models&entry.906535625=Michael%20Jungo%20and%20Andreas%20Fischer&entry.1292438233=%20%20Rule-based%20reinforcement%20learning%20has%20been%20gaining%20popularity%20ever%20since%0ADeepSeek-R1%20has%20demonstrated%20its%20success%20through%20simple%20verifiable%20rewards.%20In%0Athe%20domain%20of%20document%20analysis%2C%20reinforcement%20learning%20is%20not%20as%20prevalent%2C%0Aeven%20though%20many%20downstream%20tasks%20may%20benefit%20from%20the%20emerging%20properties%20of%0Areinforcement%20learning%2C%20particularly%20the%20enhanced%20reason%20capabilities.%20We%20study%0Athe%20effects%20of%20rule-based%20reinforcement%20learning%20with%20the%20task%20of%20Document%0AImage%20Classification%20which%20is%20one%20of%20the%20most%20commonly%20studied%20downstream%20tasks%0Ain%20document%20analysis.%20We%20find%20that%20reinforcement%20learning%20tends%20to%20have%20better%0Ageneralisation%20capabilities%20to%20out-of-distritbution%20data%2C%20which%20we%20examine%20in%0Athree%20different%20scenarios%2C%20namely%20out-of-distribution%20images%2C%20unseen%20classes%0Aand%20different%20modalities.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/jungomi/vision-finetune.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22283v1&entry.124074799=Read"},
{"title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model", "author": "Bo Li and Guanzhi Deng and Ronghao Chen and Junrong Yue and Shuo Zhang and Qinghua Zhao and Linqi Song and Lijie Wen", "abstract": "  Understanding how Large Language Models (LLMs) perform complex reasoning and\ntheir failure mechanisms is a challenge in interpretability research. To\nprovide a measurable geometric analysis perspective, we define the concept of\nthe Reasoning Manifold, a latent low-dimensional geometric structure formed by\nthe internal representations corresponding to all correctly reasoned\ngenerations. This structure can be conceptualized as the embodiment of the\neffective thinking paths that the model has learned to successfully solve a\ngiven task. Based on this concept, we build REMA, a framework that explains the\norigins of failures by quantitatively comparing the spatial relationships of\ninternal model representations corresponding to both erroneous and correct\nreasoning samples. Specifically, REMA first quantifies the geometric deviation\nof each erroneous representation by calculating its k-nearest neighbors\ndistance to the approximated manifold formed by correct representations,\nthereby providing a unified failure signal. It then localizes the divergence\npoints where these deviations first become significant by tracking this\ndeviation metric across the model's layers and comparing it against a baseline\nof internal fluctuations from correct representations, thus identifying where\nthe reasoning chain begins to go off-track. Our extensive experiments on\ndiverse language and multimodal models and tasks demonstrate the\nlow-dimensional nature of the reasoning manifold and the high separability\nbetween erroneous and correct reasoning representations. The results also\nvalidate the effectiveness of the REMA framework in analyzing the origins of\nreasoning failures. This research connects abstract reasoning failures to\nmeasurable geometric deviations in representations, providing new avenues for\nin-depth understanding and diagnosis of the internal computational processes of\nblack-box models.\n", "link": "http://arxiv.org/abs/2509.22518v1", "date": "2025-09-26", "relevancy": 2.726, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REMA%3A%20A%20Unified%20Reasoning%20Manifold%20Framework%20for%20Interpreting%20Large%0A%20%20Language%20Model&body=Title%3A%20REMA%3A%20A%20Unified%20Reasoning%20Manifold%20Framework%20for%20Interpreting%20Large%0A%20%20Language%20Model%0AAuthor%3A%20Bo%20Li%20and%20Guanzhi%20Deng%20and%20Ronghao%20Chen%20and%20Junrong%20Yue%20and%20Shuo%20Zhang%20and%20Qinghua%20Zhao%20and%20Linqi%20Song%20and%20Lijie%20Wen%0AAbstract%3A%20%20%20Understanding%20how%20Large%20Language%20Models%20%28LLMs%29%20perform%20complex%20reasoning%20and%0Atheir%20failure%20mechanisms%20is%20a%20challenge%20in%20interpretability%20research.%20To%0Aprovide%20a%20measurable%20geometric%20analysis%20perspective%2C%20we%20define%20the%20concept%20of%0Athe%20Reasoning%20Manifold%2C%20a%20latent%20low-dimensional%20geometric%20structure%20formed%20by%0Athe%20internal%20representations%20corresponding%20to%20all%20correctly%20reasoned%0Agenerations.%20This%20structure%20can%20be%20conceptualized%20as%20the%20embodiment%20of%20the%0Aeffective%20thinking%20paths%20that%20the%20model%20has%20learned%20to%20successfully%20solve%20a%0Agiven%20task.%20Based%20on%20this%20concept%2C%20we%20build%20REMA%2C%20a%20framework%20that%20explains%20the%0Aorigins%20of%20failures%20by%20quantitatively%20comparing%20the%20spatial%20relationships%20of%0Ainternal%20model%20representations%20corresponding%20to%20both%20erroneous%20and%20correct%0Areasoning%20samples.%20Specifically%2C%20REMA%20first%20quantifies%20the%20geometric%20deviation%0Aof%20each%20erroneous%20representation%20by%20calculating%20its%20k-nearest%20neighbors%0Adistance%20to%20the%20approximated%20manifold%20formed%20by%20correct%20representations%2C%0Athereby%20providing%20a%20unified%20failure%20signal.%20It%20then%20localizes%20the%20divergence%0Apoints%20where%20these%20deviations%20first%20become%20significant%20by%20tracking%20this%0Adeviation%20metric%20across%20the%20model%27s%20layers%20and%20comparing%20it%20against%20a%20baseline%0Aof%20internal%20fluctuations%20from%20correct%20representations%2C%20thus%20identifying%20where%0Athe%20reasoning%20chain%20begins%20to%20go%20off-track.%20Our%20extensive%20experiments%20on%0Adiverse%20language%20and%20multimodal%20models%20and%20tasks%20demonstrate%20the%0Alow-dimensional%20nature%20of%20the%20reasoning%20manifold%20and%20the%20high%20separability%0Abetween%20erroneous%20and%20correct%20reasoning%20representations.%20The%20results%20also%0Avalidate%20the%20effectiveness%20of%20the%20REMA%20framework%20in%20analyzing%20the%20origins%20of%0Areasoning%20failures.%20This%20research%20connects%20abstract%20reasoning%20failures%20to%0Ameasurable%20geometric%20deviations%20in%20representations%2C%20providing%20new%20avenues%20for%0Ain-depth%20understanding%20and%20diagnosis%20of%20the%20internal%20computational%20processes%20of%0Ablack-box%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREMA%253A%2520A%2520Unified%2520Reasoning%2520Manifold%2520Framework%2520for%2520Interpreting%2520Large%250A%2520%2520Language%2520Model%26entry.906535625%3DBo%2520Li%2520and%2520Guanzhi%2520Deng%2520and%2520Ronghao%2520Chen%2520and%2520Junrong%2520Yue%2520and%2520Shuo%2520Zhang%2520and%2520Qinghua%2520Zhao%2520and%2520Linqi%2520Song%2520and%2520Lijie%2520Wen%26entry.1292438233%3D%2520%2520Understanding%2520how%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520perform%2520complex%2520reasoning%2520and%250Atheir%2520failure%2520mechanisms%2520is%2520a%2520challenge%2520in%2520interpretability%2520research.%2520To%250Aprovide%2520a%2520measurable%2520geometric%2520analysis%2520perspective%252C%2520we%2520define%2520the%2520concept%2520of%250Athe%2520Reasoning%2520Manifold%252C%2520a%2520latent%2520low-dimensional%2520geometric%2520structure%2520formed%2520by%250Athe%2520internal%2520representations%2520corresponding%2520to%2520all%2520correctly%2520reasoned%250Agenerations.%2520This%2520structure%2520can%2520be%2520conceptualized%2520as%2520the%2520embodiment%2520of%2520the%250Aeffective%2520thinking%2520paths%2520that%2520the%2520model%2520has%2520learned%2520to%2520successfully%2520solve%2520a%250Agiven%2520task.%2520Based%2520on%2520this%2520concept%252C%2520we%2520build%2520REMA%252C%2520a%2520framework%2520that%2520explains%2520the%250Aorigins%2520of%2520failures%2520by%2520quantitatively%2520comparing%2520the%2520spatial%2520relationships%2520of%250Ainternal%2520model%2520representations%2520corresponding%2520to%2520both%2520erroneous%2520and%2520correct%250Areasoning%2520samples.%2520Specifically%252C%2520REMA%2520first%2520quantifies%2520the%2520geometric%2520deviation%250Aof%2520each%2520erroneous%2520representation%2520by%2520calculating%2520its%2520k-nearest%2520neighbors%250Adistance%2520to%2520the%2520approximated%2520manifold%2520formed%2520by%2520correct%2520representations%252C%250Athereby%2520providing%2520a%2520unified%2520failure%2520signal.%2520It%2520then%2520localizes%2520the%2520divergence%250Apoints%2520where%2520these%2520deviations%2520first%2520become%2520significant%2520by%2520tracking%2520this%250Adeviation%2520metric%2520across%2520the%2520model%2527s%2520layers%2520and%2520comparing%2520it%2520against%2520a%2520baseline%250Aof%2520internal%2520fluctuations%2520from%2520correct%2520representations%252C%2520thus%2520identifying%2520where%250Athe%2520reasoning%2520chain%2520begins%2520to%2520go%2520off-track.%2520Our%2520extensive%2520experiments%2520on%250Adiverse%2520language%2520and%2520multimodal%2520models%2520and%2520tasks%2520demonstrate%2520the%250Alow-dimensional%2520nature%2520of%2520the%2520reasoning%2520manifold%2520and%2520the%2520high%2520separability%250Abetween%2520erroneous%2520and%2520correct%2520reasoning%2520representations.%2520The%2520results%2520also%250Avalidate%2520the%2520effectiveness%2520of%2520the%2520REMA%2520framework%2520in%2520analyzing%2520the%2520origins%2520of%250Areasoning%2520failures.%2520This%2520research%2520connects%2520abstract%2520reasoning%2520failures%2520to%250Ameasurable%2520geometric%2520deviations%2520in%2520representations%252C%2520providing%2520new%2520avenues%2520for%250Ain-depth%2520understanding%2520and%2520diagnosis%2520of%2520the%2520internal%2520computational%2520processes%2520of%250Ablack-box%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REMA%3A%20A%20Unified%20Reasoning%20Manifold%20Framework%20for%20Interpreting%20Large%0A%20%20Language%20Model&entry.906535625=Bo%20Li%20and%20Guanzhi%20Deng%20and%20Ronghao%20Chen%20and%20Junrong%20Yue%20and%20Shuo%20Zhang%20and%20Qinghua%20Zhao%20and%20Linqi%20Song%20and%20Lijie%20Wen&entry.1292438233=%20%20Understanding%20how%20Large%20Language%20Models%20%28LLMs%29%20perform%20complex%20reasoning%20and%0Atheir%20failure%20mechanisms%20is%20a%20challenge%20in%20interpretability%20research.%20To%0Aprovide%20a%20measurable%20geometric%20analysis%20perspective%2C%20we%20define%20the%20concept%20of%0Athe%20Reasoning%20Manifold%2C%20a%20latent%20low-dimensional%20geometric%20structure%20formed%20by%0Athe%20internal%20representations%20corresponding%20to%20all%20correctly%20reasoned%0Agenerations.%20This%20structure%20can%20be%20conceptualized%20as%20the%20embodiment%20of%20the%0Aeffective%20thinking%20paths%20that%20the%20model%20has%20learned%20to%20successfully%20solve%20a%0Agiven%20task.%20Based%20on%20this%20concept%2C%20we%20build%20REMA%2C%20a%20framework%20that%20explains%20the%0Aorigins%20of%20failures%20by%20quantitatively%20comparing%20the%20spatial%20relationships%20of%0Ainternal%20model%20representations%20corresponding%20to%20both%20erroneous%20and%20correct%0Areasoning%20samples.%20Specifically%2C%20REMA%20first%20quantifies%20the%20geometric%20deviation%0Aof%20each%20erroneous%20representation%20by%20calculating%20its%20k-nearest%20neighbors%0Adistance%20to%20the%20approximated%20manifold%20formed%20by%20correct%20representations%2C%0Athereby%20providing%20a%20unified%20failure%20signal.%20It%20then%20localizes%20the%20divergence%0Apoints%20where%20these%20deviations%20first%20become%20significant%20by%20tracking%20this%0Adeviation%20metric%20across%20the%20model%27s%20layers%20and%20comparing%20it%20against%20a%20baseline%0Aof%20internal%20fluctuations%20from%20correct%20representations%2C%20thus%20identifying%20where%0Athe%20reasoning%20chain%20begins%20to%20go%20off-track.%20Our%20extensive%20experiments%20on%0Adiverse%20language%20and%20multimodal%20models%20and%20tasks%20demonstrate%20the%0Alow-dimensional%20nature%20of%20the%20reasoning%20manifold%20and%20the%20high%20separability%0Abetween%20erroneous%20and%20correct%20reasoning%20representations.%20The%20results%20also%0Avalidate%20the%20effectiveness%20of%20the%20REMA%20framework%20in%20analyzing%20the%20origins%20of%0Areasoning%20failures.%20This%20research%20connects%20abstract%20reasoning%20failures%20to%0Ameasurable%20geometric%20deviations%20in%20representations%2C%20providing%20new%20avenues%20for%0Ain-depth%20understanding%20and%20diagnosis%20of%20the%20internal%20computational%20processes%20of%0Ablack-box%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22518v1&entry.124074799=Read"},
{"title": "Self-Supervised Point Cloud Completion based on Multi-View Augmentations\n  of Single Partial Point Cloud", "author": "Jingjing Lu and Huilong Pi and Yunchuan Qin and Zhuo Tang and Ruihui Li", "abstract": "  Point cloud completion aims to reconstruct complete shapes from partial\nobservations. Although current methods have achieved remarkable performance,\nthey still have some limitations: Supervised methods heavily rely on ground\ntruth, which limits their generalization to real-world datasets due to the\nsynthetic-to-real domain gap. Unsupervised methods require complete point\nclouds to compose unpaired training data, and weakly-supervised methods need\nmulti-view observations of the object. Existing self-supervised methods\nfrequently produce unsatisfactory predictions due to the limited capabilities\nof their self-supervised signals. To overcome these challenges, we propose a\nnovel self-supervised point cloud completion method. We design a set of novel\nself-supervised signals based on multi-view augmentations of the single partial\npoint cloud. Additionally, to enhance the model's learning ability, we first\nincorporate Mamba into self-supervised point cloud completion task, encouraging\nthe model to generate point clouds with better quality. Experiments on\nsynthetic and real-world datasets demonstrate that our method achieves\nstate-of-the-art results.\n", "link": "http://arxiv.org/abs/2509.22132v1", "date": "2025-09-26", "relevancy": 2.7241, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Point%20Cloud%20Completion%20based%20on%20Multi-View%20Augmentations%0A%20%20of%20Single%20Partial%20Point%20Cloud&body=Title%3A%20Self-Supervised%20Point%20Cloud%20Completion%20based%20on%20Multi-View%20Augmentations%0A%20%20of%20Single%20Partial%20Point%20Cloud%0AAuthor%3A%20Jingjing%20Lu%20and%20Huilong%20Pi%20and%20Yunchuan%20Qin%20and%20Zhuo%20Tang%20and%20Ruihui%20Li%0AAbstract%3A%20%20%20Point%20cloud%20completion%20aims%20to%20reconstruct%20complete%20shapes%20from%20partial%0Aobservations.%20Although%20current%20methods%20have%20achieved%20remarkable%20performance%2C%0Athey%20still%20have%20some%20limitations%3A%20Supervised%20methods%20heavily%20rely%20on%20ground%0Atruth%2C%20which%20limits%20their%20generalization%20to%20real-world%20datasets%20due%20to%20the%0Asynthetic-to-real%20domain%20gap.%20Unsupervised%20methods%20require%20complete%20point%0Aclouds%20to%20compose%20unpaired%20training%20data%2C%20and%20weakly-supervised%20methods%20need%0Amulti-view%20observations%20of%20the%20object.%20Existing%20self-supervised%20methods%0Afrequently%20produce%20unsatisfactory%20predictions%20due%20to%20the%20limited%20capabilities%0Aof%20their%20self-supervised%20signals.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%0Anovel%20self-supervised%20point%20cloud%20completion%20method.%20We%20design%20a%20set%20of%20novel%0Aself-supervised%20signals%20based%20on%20multi-view%20augmentations%20of%20the%20single%20partial%0Apoint%20cloud.%20Additionally%2C%20to%20enhance%20the%20model%27s%20learning%20ability%2C%20we%20first%0Aincorporate%20Mamba%20into%20self-supervised%20point%20cloud%20completion%20task%2C%20encouraging%0Athe%20model%20to%20generate%20point%20clouds%20with%20better%20quality.%20Experiments%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Point%2520Cloud%2520Completion%2520based%2520on%2520Multi-View%2520Augmentations%250A%2520%2520of%2520Single%2520Partial%2520Point%2520Cloud%26entry.906535625%3DJingjing%2520Lu%2520and%2520Huilong%2520Pi%2520and%2520Yunchuan%2520Qin%2520and%2520Zhuo%2520Tang%2520and%2520Ruihui%2520Li%26entry.1292438233%3D%2520%2520Point%2520cloud%2520completion%2520aims%2520to%2520reconstruct%2520complete%2520shapes%2520from%2520partial%250Aobservations.%2520Although%2520current%2520methods%2520have%2520achieved%2520remarkable%2520performance%252C%250Athey%2520still%2520have%2520some%2520limitations%253A%2520Supervised%2520methods%2520heavily%2520rely%2520on%2520ground%250Atruth%252C%2520which%2520limits%2520their%2520generalization%2520to%2520real-world%2520datasets%2520due%2520to%2520the%250Asynthetic-to-real%2520domain%2520gap.%2520Unsupervised%2520methods%2520require%2520complete%2520point%250Aclouds%2520to%2520compose%2520unpaired%2520training%2520data%252C%2520and%2520weakly-supervised%2520methods%2520need%250Amulti-view%2520observations%2520of%2520the%2520object.%2520Existing%2520self-supervised%2520methods%250Afrequently%2520produce%2520unsatisfactory%2520predictions%2520due%2520to%2520the%2520limited%2520capabilities%250Aof%2520their%2520self-supervised%2520signals.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520a%250Anovel%2520self-supervised%2520point%2520cloud%2520completion%2520method.%2520We%2520design%2520a%2520set%2520of%2520novel%250Aself-supervised%2520signals%2520based%2520on%2520multi-view%2520augmentations%2520of%2520the%2520single%2520partial%250Apoint%2520cloud.%2520Additionally%252C%2520to%2520enhance%2520the%2520model%2527s%2520learning%2520ability%252C%2520we%2520first%250Aincorporate%2520Mamba%2520into%2520self-supervised%2520point%2520cloud%2520completion%2520task%252C%2520encouraging%250Athe%2520model%2520to%2520generate%2520point%2520clouds%2520with%2520better%2520quality.%2520Experiments%2520on%250Asynthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Point%20Cloud%20Completion%20based%20on%20Multi-View%20Augmentations%0A%20%20of%20Single%20Partial%20Point%20Cloud&entry.906535625=Jingjing%20Lu%20and%20Huilong%20Pi%20and%20Yunchuan%20Qin%20and%20Zhuo%20Tang%20and%20Ruihui%20Li&entry.1292438233=%20%20Point%20cloud%20completion%20aims%20to%20reconstruct%20complete%20shapes%20from%20partial%0Aobservations.%20Although%20current%20methods%20have%20achieved%20remarkable%20performance%2C%0Athey%20still%20have%20some%20limitations%3A%20Supervised%20methods%20heavily%20rely%20on%20ground%0Atruth%2C%20which%20limits%20their%20generalization%20to%20real-world%20datasets%20due%20to%20the%0Asynthetic-to-real%20domain%20gap.%20Unsupervised%20methods%20require%20complete%20point%0Aclouds%20to%20compose%20unpaired%20training%20data%2C%20and%20weakly-supervised%20methods%20need%0Amulti-view%20observations%20of%20the%20object.%20Existing%20self-supervised%20methods%0Afrequently%20produce%20unsatisfactory%20predictions%20due%20to%20the%20limited%20capabilities%0Aof%20their%20self-supervised%20signals.%20To%20overcome%20these%20challenges%2C%20we%20propose%20a%0Anovel%20self-supervised%20point%20cloud%20completion%20method.%20We%20design%20a%20set%20of%20novel%0Aself-supervised%20signals%20based%20on%20multi-view%20augmentations%20of%20the%20single%20partial%0Apoint%20cloud.%20Additionally%2C%20to%20enhance%20the%20model%27s%20learning%20ability%2C%20we%20first%0Aincorporate%20Mamba%20into%20self-supervised%20point%20cloud%20completion%20task%2C%20encouraging%0Athe%20model%20to%20generate%20point%20clouds%20with%20better%20quality.%20Experiments%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22132v1&entry.124074799=Read"},
{"title": "Thinking in Many Modes: How Composite Reasoning Elevates Large Language\n  Model Performance with Limited Data", "author": "Zishan Ahmad and Saisubramaniam Gopalakrishnan", "abstract": "  Large Language Models (LLMs), despite their remarkable capabilities, rely on\nsingular, pre-dominant reasoning paradigms, hindering their performance on\nintricate problems that demand diverse cognitive strategies. To address this,\nwe introduce Composite Reasoning (CR), a novel reasoning approach empowering\nLLMs to dynamically explore and combine multiple reasoning styles like\ndeductive, inductive, and abductive for more nuanced problem-solving. Evaluated\non scientific and medical question-answering benchmarks, our approach\noutperforms existing baselines like Chain-of-Thought (CoT) and also surpasses\nthe accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while\ndemonstrating superior sample efficiency and adequate token usage. Notably, CR\nadaptively emphasizes domain-appropriate reasoning styles. It prioritizes\nabductive and deductive reasoning for medical question answering, but shifts to\ncausal, deductive, and inductive methods for scientific reasoning. Our findings\nhighlight that by cultivating internal reasoning style diversity, LLMs acquire\nmore robust, adaptive, and efficient problem-solving abilities.\n", "link": "http://arxiv.org/abs/2509.22224v1", "date": "2025-09-26", "relevancy": 2.7166, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5654}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20in%20Many%20Modes%3A%20How%20Composite%20Reasoning%20Elevates%20Large%20Language%0A%20%20Model%20Performance%20with%20Limited%20Data&body=Title%3A%20Thinking%20in%20Many%20Modes%3A%20How%20Composite%20Reasoning%20Elevates%20Large%20Language%0A%20%20Model%20Performance%20with%20Limited%20Data%0AAuthor%3A%20Zishan%20Ahmad%20and%20Saisubramaniam%20Gopalakrishnan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20despite%20their%20remarkable%20capabilities%2C%20rely%20on%0Asingular%2C%20pre-dominant%20reasoning%20paradigms%2C%20hindering%20their%20performance%20on%0Aintricate%20problems%20that%20demand%20diverse%20cognitive%20strategies.%20To%20address%20this%2C%0Awe%20introduce%20Composite%20Reasoning%20%28CR%29%2C%20a%20novel%20reasoning%20approach%20empowering%0ALLMs%20to%20dynamically%20explore%20and%20combine%20multiple%20reasoning%20styles%20like%0Adeductive%2C%20inductive%2C%20and%20abductive%20for%20more%20nuanced%20problem-solving.%20Evaluated%0Aon%20scientific%20and%20medical%20question-answering%20benchmarks%2C%20our%20approach%0Aoutperforms%20existing%20baselines%20like%20Chain-of-Thought%20%28CoT%29%20and%20also%20surpasses%0Athe%20accuracy%20of%20DeepSeek-R1%20style%20reasoning%20%28SR%29%20capabilities%2C%20while%0Ademonstrating%20superior%20sample%20efficiency%20and%20adequate%20token%20usage.%20Notably%2C%20CR%0Aadaptively%20emphasizes%20domain-appropriate%20reasoning%20styles.%20It%20prioritizes%0Aabductive%20and%20deductive%20reasoning%20for%20medical%20question%20answering%2C%20but%20shifts%20to%0Acausal%2C%20deductive%2C%20and%20inductive%20methods%20for%20scientific%20reasoning.%20Our%20findings%0Ahighlight%20that%20by%20cultivating%20internal%20reasoning%20style%20diversity%2C%20LLMs%20acquire%0Amore%20robust%2C%20adaptive%2C%20and%20efficient%20problem-solving%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520in%2520Many%2520Modes%253A%2520How%2520Composite%2520Reasoning%2520Elevates%2520Large%2520Language%250A%2520%2520Model%2520Performance%2520with%2520Limited%2520Data%26entry.906535625%3DZishan%2520Ahmad%2520and%2520Saisubramaniam%2520Gopalakrishnan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520despite%2520their%2520remarkable%2520capabilities%252C%2520rely%2520on%250Asingular%252C%2520pre-dominant%2520reasoning%2520paradigms%252C%2520hindering%2520their%2520performance%2520on%250Aintricate%2520problems%2520that%2520demand%2520diverse%2520cognitive%2520strategies.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520Composite%2520Reasoning%2520%2528CR%2529%252C%2520a%2520novel%2520reasoning%2520approach%2520empowering%250ALLMs%2520to%2520dynamically%2520explore%2520and%2520combine%2520multiple%2520reasoning%2520styles%2520like%250Adeductive%252C%2520inductive%252C%2520and%2520abductive%2520for%2520more%2520nuanced%2520problem-solving.%2520Evaluated%250Aon%2520scientific%2520and%2520medical%2520question-answering%2520benchmarks%252C%2520our%2520approach%250Aoutperforms%2520existing%2520baselines%2520like%2520Chain-of-Thought%2520%2528CoT%2529%2520and%2520also%2520surpasses%250Athe%2520accuracy%2520of%2520DeepSeek-R1%2520style%2520reasoning%2520%2528SR%2529%2520capabilities%252C%2520while%250Ademonstrating%2520superior%2520sample%2520efficiency%2520and%2520adequate%2520token%2520usage.%2520Notably%252C%2520CR%250Aadaptively%2520emphasizes%2520domain-appropriate%2520reasoning%2520styles.%2520It%2520prioritizes%250Aabductive%2520and%2520deductive%2520reasoning%2520for%2520medical%2520question%2520answering%252C%2520but%2520shifts%2520to%250Acausal%252C%2520deductive%252C%2520and%2520inductive%2520methods%2520for%2520scientific%2520reasoning.%2520Our%2520findings%250Ahighlight%2520that%2520by%2520cultivating%2520internal%2520reasoning%2520style%2520diversity%252C%2520LLMs%2520acquire%250Amore%2520robust%252C%2520adaptive%252C%2520and%2520efficient%2520problem-solving%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20in%20Many%20Modes%3A%20How%20Composite%20Reasoning%20Elevates%20Large%20Language%0A%20%20Model%20Performance%20with%20Limited%20Data&entry.906535625=Zishan%20Ahmad%20and%20Saisubramaniam%20Gopalakrishnan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20despite%20their%20remarkable%20capabilities%2C%20rely%20on%0Asingular%2C%20pre-dominant%20reasoning%20paradigms%2C%20hindering%20their%20performance%20on%0Aintricate%20problems%20that%20demand%20diverse%20cognitive%20strategies.%20To%20address%20this%2C%0Awe%20introduce%20Composite%20Reasoning%20%28CR%29%2C%20a%20novel%20reasoning%20approach%20empowering%0ALLMs%20to%20dynamically%20explore%20and%20combine%20multiple%20reasoning%20styles%20like%0Adeductive%2C%20inductive%2C%20and%20abductive%20for%20more%20nuanced%20problem-solving.%20Evaluated%0Aon%20scientific%20and%20medical%20question-answering%20benchmarks%2C%20our%20approach%0Aoutperforms%20existing%20baselines%20like%20Chain-of-Thought%20%28CoT%29%20and%20also%20surpasses%0Athe%20accuracy%20of%20DeepSeek-R1%20style%20reasoning%20%28SR%29%20capabilities%2C%20while%0Ademonstrating%20superior%20sample%20efficiency%20and%20adequate%20token%20usage.%20Notably%2C%20CR%0Aadaptively%20emphasizes%20domain-appropriate%20reasoning%20styles.%20It%20prioritizes%0Aabductive%20and%20deductive%20reasoning%20for%20medical%20question%20answering%2C%20but%20shifts%20to%0Acausal%2C%20deductive%2C%20and%20inductive%20methods%20for%20scientific%20reasoning.%20Our%20findings%0Ahighlight%20that%20by%20cultivating%20internal%20reasoning%20style%20diversity%2C%20LLMs%20acquire%0Amore%20robust%2C%20adaptive%2C%20and%20efficient%20problem-solving%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22224v1&entry.124074799=Read"},
{"title": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection", "author": "Mohammad Mahdi Hemmatyar and Mahdi Jafari and Mohammad Amin Yousefi and Mohammad Reza Nemati and Mobin Azadani and Hamid Reza Rastad and Amirmohammad Akbari", "abstract": "  Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios.\n", "link": "http://arxiv.org/abs/2509.22544v1", "date": "2025-09-26", "relevancy": 2.7048, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyCoVAD%3A%20A%20Hybrid%20SSL-LLM%20Model%20for%20Complex%20Video%20Anomaly%20Detection&body=Title%3A%20HyCoVAD%3A%20A%20Hybrid%20SSL-LLM%20Model%20for%20Complex%20Video%20Anomaly%20Detection%0AAuthor%3A%20Mohammad%20Mahdi%20Hemmatyar%20and%20Mahdi%20Jafari%20and%20Mohammad%20Amin%20Yousefi%20and%20Mohammad%20Reza%20Nemati%20and%20Mobin%20Azadani%20and%20Hamid%20Reza%20Rastad%20and%20Amirmohammad%20Akbari%0AAbstract%3A%20%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20crucial%20for%20intelligent%20surveillance%2C%20but%20a%0Asignificant%20challenge%20lies%20in%20identifying%20complex%20anomalies%2C%20which%20are%20events%0Adefined%20by%20intricate%20relationships%20and%20temporal%20dependencies%20among%20multiple%0Aentities%20rather%20than%20by%20isolated%20actions.%20While%20self-supervised%20learning%20%28SSL%29%0Amethods%20effectively%20model%20low-level%20spatiotemporal%20patterns%2C%20they%20often%0Astruggle%20to%20grasp%20the%20semantic%20meaning%20of%20these%20interactions.%20Conversely%2C%20large%0Alanguage%20models%20%28LLMs%29%20offer%20powerful%20contextual%20reasoning%20but%20are%0Acomputationally%20expensive%20for%20frame-by-frame%20analysis%20and%20lack%20fine-grained%0Aspatial%20localization.%20We%20introduce%20HyCoVAD%2C%20Hybrid%20Complex%20Video%20Anomaly%0ADetection%2C%20a%20hybrid%20SSL-LLM%20model%20that%20combines%20a%20multi-task%20SSL%20temporal%0Aanalyzer%20with%20LLM%20validator.%20The%20SSL%20module%20is%20built%20upon%20an%20nnFormer%20backbone%0Awhich%20is%20a%20transformer-based%20model%20for%20image%20segmentation.%20It%20is%20trained%20with%0Amultiple%20proxy%20tasks%2C%20learns%20from%20video%20frames%20to%20identify%20those%20suspected%20of%0Aanomaly.%20The%20selected%20frames%20are%20then%20forwarded%20to%20the%20LLM%2C%20which%20enriches%20the%0Aanalysis%20with%20semantic%20context%20by%20applying%20structured%2C%20rule-based%20reasoning%20to%0Avalidate%20the%20presence%20of%20anomalies.%20Experiments%20on%20the%20challenging%20ComplexVAD%0Adataset%20show%20that%20HyCoVAD%20achieves%20a%2072.5%25%20frame-level%20AUC%2C%20outperforming%0Aexisting%20baselines%20by%2012.5%25%20while%20reducing%20LLM%20computation.%20We%20release%20our%0Ainteraction%20anomaly%20taxonomy%2C%20adaptive%20thresholding%20protocol%2C%20and%20code%20to%0Afacilitate%20future%20research%20in%20complex%20VAD%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyCoVAD%253A%2520A%2520Hybrid%2520SSL-LLM%2520Model%2520for%2520Complex%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DMohammad%2520Mahdi%2520Hemmatyar%2520and%2520Mahdi%2520Jafari%2520and%2520Mohammad%2520Amin%2520Yousefi%2520and%2520Mohammad%2520Reza%2520Nemati%2520and%2520Mobin%2520Azadani%2520and%2520Hamid%2520Reza%2520Rastad%2520and%2520Amirmohammad%2520Akbari%26entry.1292438233%3D%2520%2520Video%2520anomaly%2520detection%2520%2528VAD%2529%2520is%2520crucial%2520for%2520intelligent%2520surveillance%252C%2520but%2520a%250Asignificant%2520challenge%2520lies%2520in%2520identifying%2520complex%2520anomalies%252C%2520which%2520are%2520events%250Adefined%2520by%2520intricate%2520relationships%2520and%2520temporal%2520dependencies%2520among%2520multiple%250Aentities%2520rather%2520than%2520by%2520isolated%2520actions.%2520While%2520self-supervised%2520learning%2520%2528SSL%2529%250Amethods%2520effectively%2520model%2520low-level%2520spatiotemporal%2520patterns%252C%2520they%2520often%250Astruggle%2520to%2520grasp%2520the%2520semantic%2520meaning%2520of%2520these%2520interactions.%2520Conversely%252C%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520offer%2520powerful%2520contextual%2520reasoning%2520but%2520are%250Acomputationally%2520expensive%2520for%2520frame-by-frame%2520analysis%2520and%2520lack%2520fine-grained%250Aspatial%2520localization.%2520We%2520introduce%2520HyCoVAD%252C%2520Hybrid%2520Complex%2520Video%2520Anomaly%250ADetection%252C%2520a%2520hybrid%2520SSL-LLM%2520model%2520that%2520combines%2520a%2520multi-task%2520SSL%2520temporal%250Aanalyzer%2520with%2520LLM%2520validator.%2520The%2520SSL%2520module%2520is%2520built%2520upon%2520an%2520nnFormer%2520backbone%250Awhich%2520is%2520a%2520transformer-based%2520model%2520for%2520image%2520segmentation.%2520It%2520is%2520trained%2520with%250Amultiple%2520proxy%2520tasks%252C%2520learns%2520from%2520video%2520frames%2520to%2520identify%2520those%2520suspected%2520of%250Aanomaly.%2520The%2520selected%2520frames%2520are%2520then%2520forwarded%2520to%2520the%2520LLM%252C%2520which%2520enriches%2520the%250Aanalysis%2520with%2520semantic%2520context%2520by%2520applying%2520structured%252C%2520rule-based%2520reasoning%2520to%250Avalidate%2520the%2520presence%2520of%2520anomalies.%2520Experiments%2520on%2520the%2520challenging%2520ComplexVAD%250Adataset%2520show%2520that%2520HyCoVAD%2520achieves%2520a%252072.5%2525%2520frame-level%2520AUC%252C%2520outperforming%250Aexisting%2520baselines%2520by%252012.5%2525%2520while%2520reducing%2520LLM%2520computation.%2520We%2520release%2520our%250Ainteraction%2520anomaly%2520taxonomy%252C%2520adaptive%2520thresholding%2520protocol%252C%2520and%2520code%2520to%250Afacilitate%2520future%2520research%2520in%2520complex%2520VAD%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyCoVAD%3A%20A%20Hybrid%20SSL-LLM%20Model%20for%20Complex%20Video%20Anomaly%20Detection&entry.906535625=Mohammad%20Mahdi%20Hemmatyar%20and%20Mahdi%20Jafari%20and%20Mohammad%20Amin%20Yousefi%20and%20Mohammad%20Reza%20Nemati%20and%20Mobin%20Azadani%20and%20Hamid%20Reza%20Rastad%20and%20Amirmohammad%20Akbari&entry.1292438233=%20%20Video%20anomaly%20detection%20%28VAD%29%20is%20crucial%20for%20intelligent%20surveillance%2C%20but%20a%0Asignificant%20challenge%20lies%20in%20identifying%20complex%20anomalies%2C%20which%20are%20events%0Adefined%20by%20intricate%20relationships%20and%20temporal%20dependencies%20among%20multiple%0Aentities%20rather%20than%20by%20isolated%20actions.%20While%20self-supervised%20learning%20%28SSL%29%0Amethods%20effectively%20model%20low-level%20spatiotemporal%20patterns%2C%20they%20often%0Astruggle%20to%20grasp%20the%20semantic%20meaning%20of%20these%20interactions.%20Conversely%2C%20large%0Alanguage%20models%20%28LLMs%29%20offer%20powerful%20contextual%20reasoning%20but%20are%0Acomputationally%20expensive%20for%20frame-by-frame%20analysis%20and%20lack%20fine-grained%0Aspatial%20localization.%20We%20introduce%20HyCoVAD%2C%20Hybrid%20Complex%20Video%20Anomaly%0ADetection%2C%20a%20hybrid%20SSL-LLM%20model%20that%20combines%20a%20multi-task%20SSL%20temporal%0Aanalyzer%20with%20LLM%20validator.%20The%20SSL%20module%20is%20built%20upon%20an%20nnFormer%20backbone%0Awhich%20is%20a%20transformer-based%20model%20for%20image%20segmentation.%20It%20is%20trained%20with%0Amultiple%20proxy%20tasks%2C%20learns%20from%20video%20frames%20to%20identify%20those%20suspected%20of%0Aanomaly.%20The%20selected%20frames%20are%20then%20forwarded%20to%20the%20LLM%2C%20which%20enriches%20the%0Aanalysis%20with%20semantic%20context%20by%20applying%20structured%2C%20rule-based%20reasoning%20to%0Avalidate%20the%20presence%20of%20anomalies.%20Experiments%20on%20the%20challenging%20ComplexVAD%0Adataset%20show%20that%20HyCoVAD%20achieves%20a%2072.5%25%20frame-level%20AUC%2C%20outperforming%0Aexisting%20baselines%20by%2012.5%25%20while%20reducing%20LLM%20computation.%20We%20release%20our%0Ainteraction%20anomaly%20taxonomy%2C%20adaptive%20thresholding%20protocol%2C%20and%20code%20to%0Afacilitate%20future%20research%20in%20complex%20VAD%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22544v1&entry.124074799=Read"},
{"title": "Tricks and Plug-ins for Gradient Boosting in Image Classification", "author": "Biyi Fang and Jean Utke and Truong Vo and Diego Klabjan", "abstract": "  Convolutional Neural Networks (CNNs) have achieved remarkable success across\na wide range of machine learning tasks by leveraging hierarchical feature\nlearning through deep architectures. However, the large number of layers and\nmillions of parameters often make CNNs computationally expensive to train,\nrequiring extensive time and manual tuning to discover optimal architectures.\nIn this paper, we introduce a novel framework for boosting CNN performance that\nintegrates dynamic feature selection with the principles of BoostCNN. Our\napproach incorporates two key strategies: subgrid selection and importance\nsampling, to guide training toward informative regions of the feature space. We\nfurther develop a family of algorithms that embed boosting weights directly\ninto the network training process using a least squares loss formulation. This\nintegration not only alleviates the burden of manual architecture design but\nalso enhances accuracy and efficiency. Experimental results across several\nfine-grained classification benchmarks demonstrate that our boosted CNN\nvariants consistently outperform conventional CNNs in both predictive\nperformance and training speed.\n", "link": "http://arxiv.org/abs/2507.22842v2", "date": "2025-09-26", "relevancy": 2.7031, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6326}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4954}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tricks%20and%20Plug-ins%20for%20Gradient%20Boosting%20in%20Image%20Classification&body=Title%3A%20Tricks%20and%20Plug-ins%20for%20Gradient%20Boosting%20in%20Image%20Classification%0AAuthor%3A%20Biyi%20Fang%20and%20Jean%20Utke%20and%20Truong%20Vo%20and%20Diego%20Klabjan%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20remarkable%20success%20across%0Aa%20wide%20range%20of%20machine%20learning%20tasks%20by%20leveraging%20hierarchical%20feature%0Alearning%20through%20deep%20architectures.%20However%2C%20the%20large%20number%20of%20layers%20and%0Amillions%20of%20parameters%20often%20make%20CNNs%20computationally%20expensive%20to%20train%2C%0Arequiring%20extensive%20time%20and%20manual%20tuning%20to%20discover%20optimal%20architectures.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20for%20boosting%20CNN%20performance%20that%0Aintegrates%20dynamic%20feature%20selection%20with%20the%20principles%20of%20BoostCNN.%20Our%0Aapproach%20incorporates%20two%20key%20strategies%3A%20subgrid%20selection%20and%20importance%0Asampling%2C%20to%20guide%20training%20toward%20informative%20regions%20of%20the%20feature%20space.%20We%0Afurther%20develop%20a%20family%20of%20algorithms%20that%20embed%20boosting%20weights%20directly%0Ainto%20the%20network%20training%20process%20using%20a%20least%20squares%20loss%20formulation.%20This%0Aintegration%20not%20only%20alleviates%20the%20burden%20of%20manual%20architecture%20design%20but%0Aalso%20enhances%20accuracy%20and%20efficiency.%20Experimental%20results%20across%20several%0Afine-grained%20classification%20benchmarks%20demonstrate%20that%20our%20boosted%20CNN%0Avariants%20consistently%20outperform%20conventional%20CNNs%20in%20both%20predictive%0Aperformance%20and%20training%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTricks%2520and%2520Plug-ins%2520for%2520Gradient%2520Boosting%2520in%2520Image%2520Classification%26entry.906535625%3DBiyi%2520Fang%2520and%2520Jean%2520Utke%2520and%2520Truong%2520Vo%2520and%2520Diego%2520Klabjan%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520achieved%2520remarkable%2520success%2520across%250Aa%2520wide%2520range%2520of%2520machine%2520learning%2520tasks%2520by%2520leveraging%2520hierarchical%2520feature%250Alearning%2520through%2520deep%2520architectures.%2520However%252C%2520the%2520large%2520number%2520of%2520layers%2520and%250Amillions%2520of%2520parameters%2520often%2520make%2520CNNs%2520computationally%2520expensive%2520to%2520train%252C%250Arequiring%2520extensive%2520time%2520and%2520manual%2520tuning%2520to%2520discover%2520optimal%2520architectures.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520for%2520boosting%2520CNN%2520performance%2520that%250Aintegrates%2520dynamic%2520feature%2520selection%2520with%2520the%2520principles%2520of%2520BoostCNN.%2520Our%250Aapproach%2520incorporates%2520two%2520key%2520strategies%253A%2520subgrid%2520selection%2520and%2520importance%250Asampling%252C%2520to%2520guide%2520training%2520toward%2520informative%2520regions%2520of%2520the%2520feature%2520space.%2520We%250Afurther%2520develop%2520a%2520family%2520of%2520algorithms%2520that%2520embed%2520boosting%2520weights%2520directly%250Ainto%2520the%2520network%2520training%2520process%2520using%2520a%2520least%2520squares%2520loss%2520formulation.%2520This%250Aintegration%2520not%2520only%2520alleviates%2520the%2520burden%2520of%2520manual%2520architecture%2520design%2520but%250Aalso%2520enhances%2520accuracy%2520and%2520efficiency.%2520Experimental%2520results%2520across%2520several%250Afine-grained%2520classification%2520benchmarks%2520demonstrate%2520that%2520our%2520boosted%2520CNN%250Avariants%2520consistently%2520outperform%2520conventional%2520CNNs%2520in%2520both%2520predictive%250Aperformance%2520and%2520training%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tricks%20and%20Plug-ins%20for%20Gradient%20Boosting%20in%20Image%20Classification&entry.906535625=Biyi%20Fang%20and%20Jean%20Utke%20and%20Truong%20Vo%20and%20Diego%20Klabjan&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20remarkable%20success%20across%0Aa%20wide%20range%20of%20machine%20learning%20tasks%20by%20leveraging%20hierarchical%20feature%0Alearning%20through%20deep%20architectures.%20However%2C%20the%20large%20number%20of%20layers%20and%0Amillions%20of%20parameters%20often%20make%20CNNs%20computationally%20expensive%20to%20train%2C%0Arequiring%20extensive%20time%20and%20manual%20tuning%20to%20discover%20optimal%20architectures.%0AIn%20this%20paper%2C%20we%20introduce%20a%20novel%20framework%20for%20boosting%20CNN%20performance%20that%0Aintegrates%20dynamic%20feature%20selection%20with%20the%20principles%20of%20BoostCNN.%20Our%0Aapproach%20incorporates%20two%20key%20strategies%3A%20subgrid%20selection%20and%20importance%0Asampling%2C%20to%20guide%20training%20toward%20informative%20regions%20of%20the%20feature%20space.%20We%0Afurther%20develop%20a%20family%20of%20algorithms%20that%20embed%20boosting%20weights%20directly%0Ainto%20the%20network%20training%20process%20using%20a%20least%20squares%20loss%20formulation.%20This%0Aintegration%20not%20only%20alleviates%20the%20burden%20of%20manual%20architecture%20design%20but%0Aalso%20enhances%20accuracy%20and%20efficiency.%20Experimental%20results%20across%20several%0Afine-grained%20classification%20benchmarks%20demonstrate%20that%20our%20boosted%20CNN%0Avariants%20consistently%20outperform%20conventional%20CNNs%20in%20both%20predictive%0Aperformance%20and%20training%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22842v2&entry.124074799=Read"},
{"title": "ECHO: Toward Contextual Seq2Seq Paradigms in Large EEG Models", "author": "Chenyu Liu and Yuqiu Deng and Tianyu Liu and Jinan Zhou and Xinliang Zhou and Ziyu Jia and Yi Ding", "abstract": "  Electroencephalography (EEG), with its broad range of applications,\nnecessitates models that can generalize effectively across various tasks and\ndatasets. Large EEG Models (LEMs) address this by pretraining encoder-centric\narchitectures on large-scale unlabeled data to extract universal\nrepresentations. While effective, these models lack decoders of comparable\ncapacity, limiting the full utilization of the learned features. To address\nthis issue, we introduce ECHO, a novel decoder-centric LEM paradigm that\nreformulates EEG modeling as sequence-to-sequence learning. ECHO captures\nlayered relationships among signals, labels, and tasks within sequence space,\nwhile incorporating discrete support samples to construct contextual cues. This\ndesign equips ECHO with in-context learning, enabling dynamic adaptation to\nheterogeneous tasks without parameter updates. Extensive experiments across\nmultiple datasets demonstrate that, even with basic model components, ECHO\nconsistently outperforms state-of-the-art single-task LEMs in multi-task\nsettings, showing superior generalization and adaptability.\n", "link": "http://arxiv.org/abs/2509.22556v1", "date": "2025-09-26", "relevancy": 2.6955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECHO%3A%20Toward%20Contextual%20Seq2Seq%20Paradigms%20in%20Large%20EEG%20Models&body=Title%3A%20ECHO%3A%20Toward%20Contextual%20Seq2Seq%20Paradigms%20in%20Large%20EEG%20Models%0AAuthor%3A%20Chenyu%20Liu%20and%20Yuqiu%20Deng%20and%20Tianyu%20Liu%20and%20Jinan%20Zhou%20and%20Xinliang%20Zhou%20and%20Ziyu%20Jia%20and%20Yi%20Ding%0AAbstract%3A%20%20%20Electroencephalography%20%28EEG%29%2C%20with%20its%20broad%20range%20of%20applications%2C%0Anecessitates%20models%20that%20can%20generalize%20effectively%20across%20various%20tasks%20and%0Adatasets.%20Large%20EEG%20Models%20%28LEMs%29%20address%20this%20by%20pretraining%20encoder-centric%0Aarchitectures%20on%20large-scale%20unlabeled%20data%20to%20extract%20universal%0Arepresentations.%20While%20effective%2C%20these%20models%20lack%20decoders%20of%20comparable%0Acapacity%2C%20limiting%20the%20full%20utilization%20of%20the%20learned%20features.%20To%20address%0Athis%20issue%2C%20we%20introduce%20ECHO%2C%20a%20novel%20decoder-centric%20LEM%20paradigm%20that%0Areformulates%20EEG%20modeling%20as%20sequence-to-sequence%20learning.%20ECHO%20captures%0Alayered%20relationships%20among%20signals%2C%20labels%2C%20and%20tasks%20within%20sequence%20space%2C%0Awhile%20incorporating%20discrete%20support%20samples%20to%20construct%20contextual%20cues.%20This%0Adesign%20equips%20ECHO%20with%20in-context%20learning%2C%20enabling%20dynamic%20adaptation%20to%0Aheterogeneous%20tasks%20without%20parameter%20updates.%20Extensive%20experiments%20across%0Amultiple%20datasets%20demonstrate%20that%2C%20even%20with%20basic%20model%20components%2C%20ECHO%0Aconsistently%20outperforms%20state-of-the-art%20single-task%20LEMs%20in%20multi-task%0Asettings%2C%20showing%20superior%20generalization%20and%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECHO%253A%2520Toward%2520Contextual%2520Seq2Seq%2520Paradigms%2520in%2520Large%2520EEG%2520Models%26entry.906535625%3DChenyu%2520Liu%2520and%2520Yuqiu%2520Deng%2520and%2520Tianyu%2520Liu%2520and%2520Jinan%2520Zhou%2520and%2520Xinliang%2520Zhou%2520and%2520Ziyu%2520Jia%2520and%2520Yi%2520Ding%26entry.1292438233%3D%2520%2520Electroencephalography%2520%2528EEG%2529%252C%2520with%2520its%2520broad%2520range%2520of%2520applications%252C%250Anecessitates%2520models%2520that%2520can%2520generalize%2520effectively%2520across%2520various%2520tasks%2520and%250Adatasets.%2520Large%2520EEG%2520Models%2520%2528LEMs%2529%2520address%2520this%2520by%2520pretraining%2520encoder-centric%250Aarchitectures%2520on%2520large-scale%2520unlabeled%2520data%2520to%2520extract%2520universal%250Arepresentations.%2520While%2520effective%252C%2520these%2520models%2520lack%2520decoders%2520of%2520comparable%250Acapacity%252C%2520limiting%2520the%2520full%2520utilization%2520of%2520the%2520learned%2520features.%2520To%2520address%250Athis%2520issue%252C%2520we%2520introduce%2520ECHO%252C%2520a%2520novel%2520decoder-centric%2520LEM%2520paradigm%2520that%250Areformulates%2520EEG%2520modeling%2520as%2520sequence-to-sequence%2520learning.%2520ECHO%2520captures%250Alayered%2520relationships%2520among%2520signals%252C%2520labels%252C%2520and%2520tasks%2520within%2520sequence%2520space%252C%250Awhile%2520incorporating%2520discrete%2520support%2520samples%2520to%2520construct%2520contextual%2520cues.%2520This%250Adesign%2520equips%2520ECHO%2520with%2520in-context%2520learning%252C%2520enabling%2520dynamic%2520adaptation%2520to%250Aheterogeneous%2520tasks%2520without%2520parameter%2520updates.%2520Extensive%2520experiments%2520across%250Amultiple%2520datasets%2520demonstrate%2520that%252C%2520even%2520with%2520basic%2520model%2520components%252C%2520ECHO%250Aconsistently%2520outperforms%2520state-of-the-art%2520single-task%2520LEMs%2520in%2520multi-task%250Asettings%252C%2520showing%2520superior%2520generalization%2520and%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECHO%3A%20Toward%20Contextual%20Seq2Seq%20Paradigms%20in%20Large%20EEG%20Models&entry.906535625=Chenyu%20Liu%20and%20Yuqiu%20Deng%20and%20Tianyu%20Liu%20and%20Jinan%20Zhou%20and%20Xinliang%20Zhou%20and%20Ziyu%20Jia%20and%20Yi%20Ding&entry.1292438233=%20%20Electroencephalography%20%28EEG%29%2C%20with%20its%20broad%20range%20of%20applications%2C%0Anecessitates%20models%20that%20can%20generalize%20effectively%20across%20various%20tasks%20and%0Adatasets.%20Large%20EEG%20Models%20%28LEMs%29%20address%20this%20by%20pretraining%20encoder-centric%0Aarchitectures%20on%20large-scale%20unlabeled%20data%20to%20extract%20universal%0Arepresentations.%20While%20effective%2C%20these%20models%20lack%20decoders%20of%20comparable%0Acapacity%2C%20limiting%20the%20full%20utilization%20of%20the%20learned%20features.%20To%20address%0Athis%20issue%2C%20we%20introduce%20ECHO%2C%20a%20novel%20decoder-centric%20LEM%20paradigm%20that%0Areformulates%20EEG%20modeling%20as%20sequence-to-sequence%20learning.%20ECHO%20captures%0Alayered%20relationships%20among%20signals%2C%20labels%2C%20and%20tasks%20within%20sequence%20space%2C%0Awhile%20incorporating%20discrete%20support%20samples%20to%20construct%20contextual%20cues.%20This%0Adesign%20equips%20ECHO%20with%20in-context%20learning%2C%20enabling%20dynamic%20adaptation%20to%0Aheterogeneous%20tasks%20without%20parameter%20updates.%20Extensive%20experiments%20across%0Amultiple%20datasets%20demonstrate%20that%2C%20even%20with%20basic%20model%20components%2C%20ECHO%0Aconsistently%20outperforms%20state-of-the-art%20single-task%20LEMs%20in%20multi-task%0Asettings%2C%20showing%20superior%20generalization%20and%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22556v1&entry.124074799=Read"},
{"title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models", "author": "Justus Westerhoff and Erblina Purelku and Jakob Hackstein and Jonas Loos and Leo Pinetzki and Erik Rodner and Lorenz Hufe", "abstract": "  Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. Existing datasets are limited in size and diversity,\nmaking it difficult to study such vulnerabilities. In this paper, we introduce\nSCAM, the largest and most diverse dataset of real-world typographic attack\nimages to date, containing 1162 images across hundreds of object categories and\nattack words. Through extensive benchmarking of Vision-Language Models on SCAM,\nwe demonstrate that typographic attacks significantly degrade performance, and\nidentify that training data and model architecture influence the susceptibility\nto these attacks. Our findings indicate that typographic attacks remain\neffective against state-of-the-art Large Vision-Language Models, especially\nthose employing vision encoders inherently vulnerable to such attacks. However,\nemploying larger Large Language Model backbones reduces this vulnerability\nwhile simultaneously enhancing typographic understanding. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. Finally, we publicly release the datasets\nintroduced in this paper, along with the code for evaluations under\nwww.bliss.berlin/research/scam.\n", "link": "http://arxiv.org/abs/2504.04893v6", "date": "2025-09-26", "relevancy": 2.694, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&body=Title%3A%20SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models%0AAuthor%3A%20Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Erik%20Rodner%20and%20Lorenz%20Hufe%0AAbstract%3A%20%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20Existing%20datasets%20are%20limited%20in%20size%20and%20diversity%2C%0Amaking%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%20introduce%0ASCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%20attack%0Aimages%20to%20date%2C%20containing%201162%20images%20across%20hundreds%20of%20object%20categories%20and%0Aattack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%20Models%20on%20SCAM%2C%0Awe%20demonstrate%20that%20typographic%20attacks%20significantly%20degrade%20performance%2C%20and%0Aidentify%20that%20training%20data%20and%20model%20architecture%20influence%20the%20susceptibility%0Ato%20these%20attacks.%20Our%20findings%20indicate%20that%20typographic%20attacks%20remain%0Aeffective%20against%20state-of-the-art%20Large%20Vision-Language%20Models%2C%20especially%0Athose%20employing%20vision%20encoders%20inherently%20vulnerable%20to%20such%20attacks.%20However%2C%0Aemploying%20larger%20Large%20Language%20Model%20backbones%20reduces%20this%20vulnerability%0Awhile%20simultaneously%20enhancing%20typographic%20understanding.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20Finally%2C%20we%20publicly%20release%20the%20datasets%0Aintroduced%20in%20this%20paper%2C%20along%20with%20the%20code%20for%20evaluations%20under%0Awww.bliss.berlin/research/scam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04893v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAM%253A%2520A%2520Real-World%2520Typographic%2520Robustness%2520Evaluation%2520for%2520Multimodal%250A%2520%2520Foundation%2520Models%26entry.906535625%3DJustus%2520Westerhoff%2520and%2520Erblina%2520Purelku%2520and%2520Jakob%2520Hackstein%2520and%2520Jonas%2520Loos%2520and%2520Leo%2520Pinetzki%2520and%2520Erik%2520Rodner%2520and%2520Lorenz%2520Hufe%26entry.1292438233%3D%2520%2520Typographic%2520attacks%2520exploit%2520the%2520interplay%2520between%2520text%2520and%2520visual%2520content%2520in%250Amultimodal%2520foundation%2520models%252C%2520causing%2520misclassifications%2520when%2520misleading%2520text%250Ais%2520embedded%2520within%2520images.%2520Existing%2520datasets%2520are%2520limited%2520in%2520size%2520and%2520diversity%252C%250Amaking%2520it%2520difficult%2520to%2520study%2520such%2520vulnerabilities.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ASCAM%252C%2520the%2520largest%2520and%2520most%2520diverse%2520dataset%2520of%2520real-world%2520typographic%2520attack%250Aimages%2520to%2520date%252C%2520containing%25201162%2520images%2520across%2520hundreds%2520of%2520object%2520categories%2520and%250Aattack%2520words.%2520Through%2520extensive%2520benchmarking%2520of%2520Vision-Language%2520Models%2520on%2520SCAM%252C%250Awe%2520demonstrate%2520that%2520typographic%2520attacks%2520significantly%2520degrade%2520performance%252C%2520and%250Aidentify%2520that%2520training%2520data%2520and%2520model%2520architecture%2520influence%2520the%2520susceptibility%250Ato%2520these%2520attacks.%2520Our%2520findings%2520indicate%2520that%2520typographic%2520attacks%2520remain%250Aeffective%2520against%2520state-of-the-art%2520Large%2520Vision-Language%2520Models%252C%2520especially%250Athose%2520employing%2520vision%2520encoders%2520inherently%2520vulnerable%2520to%2520such%2520attacks.%2520However%252C%250Aemploying%2520larger%2520Large%2520Language%2520Model%2520backbones%2520reduces%2520this%2520vulnerability%250Awhile%2520simultaneously%2520enhancing%2520typographic%2520understanding.%2520Additionally%252C%2520we%250Ademonstrate%2520that%2520synthetic%2520attacks%2520closely%2520resemble%2520real-world%2520%2528handwritten%2529%250Aattacks%252C%2520validating%2520their%2520use%2520in%2520research.%2520Our%2520work%2520provides%2520a%2520comprehensive%250Aresource%2520and%2520empirical%2520insights%2520to%2520facilitate%2520future%2520research%2520toward%2520robust%2520and%250Atrustworthy%2520multimodal%2520AI%2520systems.%2520Finally%252C%2520we%2520publicly%2520release%2520the%2520datasets%250Aintroduced%2520in%2520this%2520paper%252C%2520along%2520with%2520the%2520code%2520for%2520evaluations%2520under%250Awww.bliss.berlin/research/scam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04893v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAM%3A%20A%20Real-World%20Typographic%20Robustness%20Evaluation%20for%20Multimodal%0A%20%20Foundation%20Models&entry.906535625=Justus%20Westerhoff%20and%20Erblina%20Purelku%20and%20Jakob%20Hackstein%20and%20Jonas%20Loos%20and%20Leo%20Pinetzki%20and%20Erik%20Rodner%20and%20Lorenz%20Hufe&entry.1292438233=%20%20Typographic%20attacks%20exploit%20the%20interplay%20between%20text%20and%20visual%20content%20in%0Amultimodal%20foundation%20models%2C%20causing%20misclassifications%20when%20misleading%20text%0Ais%20embedded%20within%20images.%20Existing%20datasets%20are%20limited%20in%20size%20and%20diversity%2C%0Amaking%20it%20difficult%20to%20study%20such%20vulnerabilities.%20In%20this%20paper%2C%20we%20introduce%0ASCAM%2C%20the%20largest%20and%20most%20diverse%20dataset%20of%20real-world%20typographic%20attack%0Aimages%20to%20date%2C%20containing%201162%20images%20across%20hundreds%20of%20object%20categories%20and%0Aattack%20words.%20Through%20extensive%20benchmarking%20of%20Vision-Language%20Models%20on%20SCAM%2C%0Awe%20demonstrate%20that%20typographic%20attacks%20significantly%20degrade%20performance%2C%20and%0Aidentify%20that%20training%20data%20and%20model%20architecture%20influence%20the%20susceptibility%0Ato%20these%20attacks.%20Our%20findings%20indicate%20that%20typographic%20attacks%20remain%0Aeffective%20against%20state-of-the-art%20Large%20Vision-Language%20Models%2C%20especially%0Athose%20employing%20vision%20encoders%20inherently%20vulnerable%20to%20such%20attacks.%20However%2C%0Aemploying%20larger%20Large%20Language%20Model%20backbones%20reduces%20this%20vulnerability%0Awhile%20simultaneously%20enhancing%20typographic%20understanding.%20Additionally%2C%20we%0Ademonstrate%20that%20synthetic%20attacks%20closely%20resemble%20real-world%20%28handwritten%29%0Aattacks%2C%20validating%20their%20use%20in%20research.%20Our%20work%20provides%20a%20comprehensive%0Aresource%20and%20empirical%20insights%20to%20facilitate%20future%20research%20toward%20robust%20and%0Atrustworthy%20multimodal%20AI%20systems.%20Finally%2C%20we%20publicly%20release%20the%20datasets%0Aintroduced%20in%20this%20paper%2C%20along%20with%20the%20code%20for%20evaluations%20under%0Awww.bliss.berlin/research/scam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04893v6&entry.124074799=Read"},
{"title": "Self-Supervised Learning of Graph Representations for Network Intrusion\n  Detection", "author": "Lorenzo Guerra and Thomas Chapuis and Guillaume Duc and Pavlo Mozharovskyi and Van-Tam Nguyen", "abstract": "  Detecting intrusions in network traffic is a challenging task, particularly\nunder limited supervision and constantly evolving attack patterns. While recent\nworks have leveraged graph neural networks for network intrusion detection,\nthey often decouple representation learning from anomaly detection, limiting\nthe utility of the embeddings for identifying attacks. We propose GraphIDS, a\nself-supervised intrusion detection model that unifies these two stages by\nlearning local graph representations of normal communication patterns through a\nmasked autoencoder. An inductive graph neural network embeds each flow with its\nlocal topological context to capture typical network behavior, while a\nTransformer-based encoder-decoder reconstructs these embeddings, implicitly\nlearning global co-occurrence patterns via self-attention without requiring\nexplicit positional information. During inference, flows with unusually high\nreconstruction errors are flagged as potential intrusions. This end-to-end\nframework ensures that embeddings are directly optimized for the downstream\ntask, facilitating the recognition of malicious traffic. On diverse NetFlow\nbenchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score,\noutperforming baselines by 5-25 percentage points.\n", "link": "http://arxiv.org/abs/2509.16625v2", "date": "2025-09-26", "relevancy": 2.6885, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6069}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5058}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20of%20Graph%20Representations%20for%20Network%20Intrusion%0A%20%20Detection&body=Title%3A%20Self-Supervised%20Learning%20of%20Graph%20Representations%20for%20Network%20Intrusion%0A%20%20Detection%0AAuthor%3A%20Lorenzo%20Guerra%20and%20Thomas%20Chapuis%20and%20Guillaume%20Duc%20and%20Pavlo%20Mozharovskyi%20and%20Van-Tam%20Nguyen%0AAbstract%3A%20%20%20Detecting%20intrusions%20in%20network%20traffic%20is%20a%20challenging%20task%2C%20particularly%0Aunder%20limited%20supervision%20and%20constantly%20evolving%20attack%20patterns.%20While%20recent%0Aworks%20have%20leveraged%20graph%20neural%20networks%20for%20network%20intrusion%20detection%2C%0Athey%20often%20decouple%20representation%20learning%20from%20anomaly%20detection%2C%20limiting%0Athe%20utility%20of%20the%20embeddings%20for%20identifying%20attacks.%20We%20propose%20GraphIDS%2C%20a%0Aself-supervised%20intrusion%20detection%20model%20that%20unifies%20these%20two%20stages%20by%0Alearning%20local%20graph%20representations%20of%20normal%20communication%20patterns%20through%20a%0Amasked%20autoencoder.%20An%20inductive%20graph%20neural%20network%20embeds%20each%20flow%20with%20its%0Alocal%20topological%20context%20to%20capture%20typical%20network%20behavior%2C%20while%20a%0ATransformer-based%20encoder-decoder%20reconstructs%20these%20embeddings%2C%20implicitly%0Alearning%20global%20co-occurrence%20patterns%20via%20self-attention%20without%20requiring%0Aexplicit%20positional%20information.%20During%20inference%2C%20flows%20with%20unusually%20high%0Areconstruction%20errors%20are%20flagged%20as%20potential%20intrusions.%20This%20end-to-end%0Aframework%20ensures%20that%20embeddings%20are%20directly%20optimized%20for%20the%20downstream%0Atask%2C%20facilitating%20the%20recognition%20of%20malicious%20traffic.%20On%20diverse%20NetFlow%0Abenchmarks%2C%20GraphIDS%20achieves%20up%20to%2099.98%25%20PR-AUC%20and%2099.61%25%20macro%20F1-score%2C%0Aoutperforming%20baselines%20by%205-25%20percentage%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.16625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Learning%2520of%2520Graph%2520Representations%2520for%2520Network%2520Intrusion%250A%2520%2520Detection%26entry.906535625%3DLorenzo%2520Guerra%2520and%2520Thomas%2520Chapuis%2520and%2520Guillaume%2520Duc%2520and%2520Pavlo%2520Mozharovskyi%2520and%2520Van-Tam%2520Nguyen%26entry.1292438233%3D%2520%2520Detecting%2520intrusions%2520in%2520network%2520traffic%2520is%2520a%2520challenging%2520task%252C%2520particularly%250Aunder%2520limited%2520supervision%2520and%2520constantly%2520evolving%2520attack%2520patterns.%2520While%2520recent%250Aworks%2520have%2520leveraged%2520graph%2520neural%2520networks%2520for%2520network%2520intrusion%2520detection%252C%250Athey%2520often%2520decouple%2520representation%2520learning%2520from%2520anomaly%2520detection%252C%2520limiting%250Athe%2520utility%2520of%2520the%2520embeddings%2520for%2520identifying%2520attacks.%2520We%2520propose%2520GraphIDS%252C%2520a%250Aself-supervised%2520intrusion%2520detection%2520model%2520that%2520unifies%2520these%2520two%2520stages%2520by%250Alearning%2520local%2520graph%2520representations%2520of%2520normal%2520communication%2520patterns%2520through%2520a%250Amasked%2520autoencoder.%2520An%2520inductive%2520graph%2520neural%2520network%2520embeds%2520each%2520flow%2520with%2520its%250Alocal%2520topological%2520context%2520to%2520capture%2520typical%2520network%2520behavior%252C%2520while%2520a%250ATransformer-based%2520encoder-decoder%2520reconstructs%2520these%2520embeddings%252C%2520implicitly%250Alearning%2520global%2520co-occurrence%2520patterns%2520via%2520self-attention%2520without%2520requiring%250Aexplicit%2520positional%2520information.%2520During%2520inference%252C%2520flows%2520with%2520unusually%2520high%250Areconstruction%2520errors%2520are%2520flagged%2520as%2520potential%2520intrusions.%2520This%2520end-to-end%250Aframework%2520ensures%2520that%2520embeddings%2520are%2520directly%2520optimized%2520for%2520the%2520downstream%250Atask%252C%2520facilitating%2520the%2520recognition%2520of%2520malicious%2520traffic.%2520On%2520diverse%2520NetFlow%250Abenchmarks%252C%2520GraphIDS%2520achieves%2520up%2520to%252099.98%2525%2520PR-AUC%2520and%252099.61%2525%2520macro%2520F1-score%252C%250Aoutperforming%2520baselines%2520by%25205-25%2520percentage%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20of%20Graph%20Representations%20for%20Network%20Intrusion%0A%20%20Detection&entry.906535625=Lorenzo%20Guerra%20and%20Thomas%20Chapuis%20and%20Guillaume%20Duc%20and%20Pavlo%20Mozharovskyi%20and%20Van-Tam%20Nguyen&entry.1292438233=%20%20Detecting%20intrusions%20in%20network%20traffic%20is%20a%20challenging%20task%2C%20particularly%0Aunder%20limited%20supervision%20and%20constantly%20evolving%20attack%20patterns.%20While%20recent%0Aworks%20have%20leveraged%20graph%20neural%20networks%20for%20network%20intrusion%20detection%2C%0Athey%20often%20decouple%20representation%20learning%20from%20anomaly%20detection%2C%20limiting%0Athe%20utility%20of%20the%20embeddings%20for%20identifying%20attacks.%20We%20propose%20GraphIDS%2C%20a%0Aself-supervised%20intrusion%20detection%20model%20that%20unifies%20these%20two%20stages%20by%0Alearning%20local%20graph%20representations%20of%20normal%20communication%20patterns%20through%20a%0Amasked%20autoencoder.%20An%20inductive%20graph%20neural%20network%20embeds%20each%20flow%20with%20its%0Alocal%20topological%20context%20to%20capture%20typical%20network%20behavior%2C%20while%20a%0ATransformer-based%20encoder-decoder%20reconstructs%20these%20embeddings%2C%20implicitly%0Alearning%20global%20co-occurrence%20patterns%20via%20self-attention%20without%20requiring%0Aexplicit%20positional%20information.%20During%20inference%2C%20flows%20with%20unusually%20high%0Areconstruction%20errors%20are%20flagged%20as%20potential%20intrusions.%20This%20end-to-end%0Aframework%20ensures%20that%20embeddings%20are%20directly%20optimized%20for%20the%20downstream%0Atask%2C%20facilitating%20the%20recognition%20of%20malicious%20traffic.%20On%20diverse%20NetFlow%0Abenchmarks%2C%20GraphIDS%20achieves%20up%20to%2099.98%25%20PR-AUC%20and%2099.61%25%20macro%20F1-score%2C%0Aoutperforming%20baselines%20by%205-25%20percentage%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.16625v2&entry.124074799=Read"},
{"title": "Beyond Textual Context: Structural Graph Encoding with Adaptive Space\n  Alignment to alleviate the hallucination of LLMs", "author": "Yifang Zhang and Pengfei Duan and Yiwen Yang and Shengwu Xiong", "abstract": "  Currently, the main approach for Large Language Models (LLMs) to tackle the\nhallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs\ntypically treat KGs as plain text, extracting only semantic information and\nlimiting their use of the crucial structural aspects of KGs. Another challenge\nis the gap between the embedding spaces of KGs encoders and LLMs text\nembeddings, which hinders the effective integration of structured knowledge. To\novercome these obstacles, we put forward the SSKG-LLM, an innovative model\narchitecture that is designed to efficiently integrate both the Structural and\nSemantic information of KGs into the reasoning processes of LLMs. SSKG-LLM\nincorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph\nEncoding (KGE) module to preserve semantics while utilizing structure. Then,\nthe Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to\nunderstand KGs embeddings. We conduct extensive experiments and provide a\ndetailed analysis to explore how incorporating the structural information of\nKGs can enhance the factual reasoning abilities of LLMs. Our code are available\nat https://github.com/yfangZhang/SSKG-LLM.\n", "link": "http://arxiv.org/abs/2509.22251v1", "date": "2025-09-26", "relevancy": 2.6841, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5407}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Textual%20Context%3A%20Structural%20Graph%20Encoding%20with%20Adaptive%20Space%0A%20%20Alignment%20to%20alleviate%20the%20hallucination%20of%20LLMs&body=Title%3A%20Beyond%20Textual%20Context%3A%20Structural%20Graph%20Encoding%20with%20Adaptive%20Space%0A%20%20Alignment%20to%20alleviate%20the%20hallucination%20of%20LLMs%0AAuthor%3A%20Yifang%20Zhang%20and%20Pengfei%20Duan%20and%20Yiwen%20Yang%20and%20Shengwu%20Xiong%0AAbstract%3A%20%20%20Currently%2C%20the%20main%20approach%20for%20Large%20Language%20Models%20%28LLMs%29%20to%20tackle%20the%0Ahallucination%20issue%20is%20incorporating%20Knowledge%20Graphs%28KGs%29.However%2C%20LLMs%0Atypically%20treat%20KGs%20as%20plain%20text%2C%20extracting%20only%20semantic%20information%20and%0Alimiting%20their%20use%20of%20the%20crucial%20structural%20aspects%20of%20KGs.%20Another%20challenge%0Ais%20the%20gap%20between%20the%20embedding%20spaces%20of%20KGs%20encoders%20and%20LLMs%20text%0Aembeddings%2C%20which%20hinders%20the%20effective%20integration%20of%20structured%20knowledge.%20To%0Aovercome%20these%20obstacles%2C%20we%20put%20forward%20the%20SSKG-LLM%2C%20an%20innovative%20model%0Aarchitecture%20that%20is%20designed%20to%20efficiently%20integrate%20both%20the%20Structural%20and%0ASemantic%20information%20of%20KGs%20into%20the%20reasoning%20processes%20of%20LLMs.%20SSKG-LLM%0Aincorporates%20the%20Knowledge%20Graph%20Retrieval%20%28KGR%29%20module%20and%20the%20Knowledge%20Graph%0AEncoding%20%28KGE%29%20module%20to%20preserve%20semantics%20while%20utilizing%20structure.%20Then%2C%0Athe%20Knowledge%20Graph%20Adaptation%20%28KGA%29%20module%20is%20incorporated%20to%20enable%20LLMs%20to%0Aunderstand%20KGs%20embeddings.%20We%20conduct%20extensive%20experiments%20and%20provide%20a%0Adetailed%20analysis%20to%20explore%20how%20incorporating%20the%20structural%20information%20of%0AKGs%20can%20enhance%20the%20factual%20reasoning%20abilities%20of%20LLMs.%20Our%20code%20are%20available%0Aat%20https%3A//github.com/yfangZhang/SSKG-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Textual%2520Context%253A%2520Structural%2520Graph%2520Encoding%2520with%2520Adaptive%2520Space%250A%2520%2520Alignment%2520to%2520alleviate%2520the%2520hallucination%2520of%2520LLMs%26entry.906535625%3DYifang%2520Zhang%2520and%2520Pengfei%2520Duan%2520and%2520Yiwen%2520Yang%2520and%2520Shengwu%2520Xiong%26entry.1292438233%3D%2520%2520Currently%252C%2520the%2520main%2520approach%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520tackle%2520the%250Ahallucination%2520issue%2520is%2520incorporating%2520Knowledge%2520Graphs%2528KGs%2529.However%252C%2520LLMs%250Atypically%2520treat%2520KGs%2520as%2520plain%2520text%252C%2520extracting%2520only%2520semantic%2520information%2520and%250Alimiting%2520their%2520use%2520of%2520the%2520crucial%2520structural%2520aspects%2520of%2520KGs.%2520Another%2520challenge%250Ais%2520the%2520gap%2520between%2520the%2520embedding%2520spaces%2520of%2520KGs%2520encoders%2520and%2520LLMs%2520text%250Aembeddings%252C%2520which%2520hinders%2520the%2520effective%2520integration%2520of%2520structured%2520knowledge.%2520To%250Aovercome%2520these%2520obstacles%252C%2520we%2520put%2520forward%2520the%2520SSKG-LLM%252C%2520an%2520innovative%2520model%250Aarchitecture%2520that%2520is%2520designed%2520to%2520efficiently%2520integrate%2520both%2520the%2520Structural%2520and%250ASemantic%2520information%2520of%2520KGs%2520into%2520the%2520reasoning%2520processes%2520of%2520LLMs.%2520SSKG-LLM%250Aincorporates%2520the%2520Knowledge%2520Graph%2520Retrieval%2520%2528KGR%2529%2520module%2520and%2520the%2520Knowledge%2520Graph%250AEncoding%2520%2528KGE%2529%2520module%2520to%2520preserve%2520semantics%2520while%2520utilizing%2520structure.%2520Then%252C%250Athe%2520Knowledge%2520Graph%2520Adaptation%2520%2528KGA%2529%2520module%2520is%2520incorporated%2520to%2520enable%2520LLMs%2520to%250Aunderstand%2520KGs%2520embeddings.%2520We%2520conduct%2520extensive%2520experiments%2520and%2520provide%2520a%250Adetailed%2520analysis%2520to%2520explore%2520how%2520incorporating%2520the%2520structural%2520information%2520of%250AKGs%2520can%2520enhance%2520the%2520factual%2520reasoning%2520abilities%2520of%2520LLMs.%2520Our%2520code%2520are%2520available%250Aat%2520https%253A//github.com/yfangZhang/SSKG-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Textual%20Context%3A%20Structural%20Graph%20Encoding%20with%20Adaptive%20Space%0A%20%20Alignment%20to%20alleviate%20the%20hallucination%20of%20LLMs&entry.906535625=Yifang%20Zhang%20and%20Pengfei%20Duan%20and%20Yiwen%20Yang%20and%20Shengwu%20Xiong&entry.1292438233=%20%20Currently%2C%20the%20main%20approach%20for%20Large%20Language%20Models%20%28LLMs%29%20to%20tackle%20the%0Ahallucination%20issue%20is%20incorporating%20Knowledge%20Graphs%28KGs%29.However%2C%20LLMs%0Atypically%20treat%20KGs%20as%20plain%20text%2C%20extracting%20only%20semantic%20information%20and%0Alimiting%20their%20use%20of%20the%20crucial%20structural%20aspects%20of%20KGs.%20Another%20challenge%0Ais%20the%20gap%20between%20the%20embedding%20spaces%20of%20KGs%20encoders%20and%20LLMs%20text%0Aembeddings%2C%20which%20hinders%20the%20effective%20integration%20of%20structured%20knowledge.%20To%0Aovercome%20these%20obstacles%2C%20we%20put%20forward%20the%20SSKG-LLM%2C%20an%20innovative%20model%0Aarchitecture%20that%20is%20designed%20to%20efficiently%20integrate%20both%20the%20Structural%20and%0ASemantic%20information%20of%20KGs%20into%20the%20reasoning%20processes%20of%20LLMs.%20SSKG-LLM%0Aincorporates%20the%20Knowledge%20Graph%20Retrieval%20%28KGR%29%20module%20and%20the%20Knowledge%20Graph%0AEncoding%20%28KGE%29%20module%20to%20preserve%20semantics%20while%20utilizing%20structure.%20Then%2C%0Athe%20Knowledge%20Graph%20Adaptation%20%28KGA%29%20module%20is%20incorporated%20to%20enable%20LLMs%20to%0Aunderstand%20KGs%20embeddings.%20We%20conduct%20extensive%20experiments%20and%20provide%20a%0Adetailed%20analysis%20to%20explore%20how%20incorporating%20the%20structural%20information%20of%0AKGs%20can%20enhance%20the%20factual%20reasoning%20abilities%20of%20LLMs.%20Our%20code%20are%20available%0Aat%20https%3A//github.com/yfangZhang/SSKG-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22251v1&entry.124074799=Read"},
{"title": "Explaining multimodal LLMs via intra-modal token interactions", "author": "Jiawei Liang and Ruoyu Chen and Xianghao Jiao and Siyuan Liang and Shiming Liu and Qunli Zhang and Zheng Hu and Xiaochun Cao", "abstract": "  Multimodal Large Language Models (MLLMs) have achieved remarkable success\nacross diverse vision-language tasks, yet their internal decision-making\nmechanisms remain insufficiently understood. Existing interpretability research\nhas primarily focused on cross-modal attribution, identifying which image\nregions the model attends to during output generation. However, these\napproaches often overlook intra-modal dependencies. In the visual modality,\nattributing importance to isolated image patches ignores spatial context due to\nlimited receptive fields, resulting in fragmented and noisy explanations. In\nthe textual modality, reliance on preceding tokens introduces spurious\nactivations. Failing to effectively mitigate these interference compromises\nattribution fidelity. To address these limitations, we propose enhancing\ninterpretability by leveraging intra-modal interaction. For the visual branch,\nwe introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which\naggregates attributions over multi-scale inputs to dynamically adjust receptive\nfields, producing more holistic and spatially coherent visual explanations. For\nthe textual branch, we propose \\textit{Activation Ranking Correlation} (ARC),\nwhich measures the relevance of contextual tokens to the current token via\nalignment of their top-$k$ prediction rankings. ARC leverages this relevance to\nsuppress spurious activations from irrelevant contexts while preserving\nsemantically coherent ones. Extensive experiments across state-of-the-art MLLMs\nand benchmark datasets demonstrate that our approach consistently outperforms\nexisting interpretability methods, yielding more faithful and fine-grained\nexplanations of model behavior.\n", "link": "http://arxiv.org/abs/2509.22415v1", "date": "2025-09-26", "relevancy": 2.6683, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5473}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20multimodal%20LLMs%20via%20intra-modal%20token%20interactions&body=Title%3A%20Explaining%20multimodal%20LLMs%20via%20intra-modal%20token%20interactions%0AAuthor%3A%20Jiawei%20Liang%20and%20Ruoyu%20Chen%20and%20Xianghao%20Jiao%20and%20Siyuan%20Liang%20and%20Shiming%20Liu%20and%20Qunli%20Zhang%20and%20Zheng%20Hu%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%0Aacross%20diverse%20vision-language%20tasks%2C%20yet%20their%20internal%20decision-making%0Amechanisms%20remain%20insufficiently%20understood.%20Existing%20interpretability%20research%0Ahas%20primarily%20focused%20on%20cross-modal%20attribution%2C%20identifying%20which%20image%0Aregions%20the%20model%20attends%20to%20during%20output%20generation.%20However%2C%20these%0Aapproaches%20often%20overlook%20intra-modal%20dependencies.%20In%20the%20visual%20modality%2C%0Aattributing%20importance%20to%20isolated%20image%20patches%20ignores%20spatial%20context%20due%20to%0Alimited%20receptive%20fields%2C%20resulting%20in%20fragmented%20and%20noisy%20explanations.%20In%0Athe%20textual%20modality%2C%20reliance%20on%20preceding%20tokens%20introduces%20spurious%0Aactivations.%20Failing%20to%20effectively%20mitigate%20these%20interference%20compromises%0Aattribution%20fidelity.%20To%20address%20these%20limitations%2C%20we%20propose%20enhancing%0Ainterpretability%20by%20leveraging%20intra-modal%20interaction.%20For%20the%20visual%20branch%2C%0Awe%20introduce%20%5Ctextit%7BMulti-Scale%20Explanation%20Aggregation%7D%20%28MSEA%29%2C%20which%0Aaggregates%20attributions%20over%20multi-scale%20inputs%20to%20dynamically%20adjust%20receptive%0Afields%2C%20producing%20more%20holistic%20and%20spatially%20coherent%20visual%20explanations.%20For%0Athe%20textual%20branch%2C%20we%20propose%20%5Ctextit%7BActivation%20Ranking%20Correlation%7D%20%28ARC%29%2C%0Awhich%20measures%20the%20relevance%20of%20contextual%20tokens%20to%20the%20current%20token%20via%0Aalignment%20of%20their%20top-%24k%24%20prediction%20rankings.%20ARC%20leverages%20this%20relevance%20to%0Asuppress%20spurious%20activations%20from%20irrelevant%20contexts%20while%20preserving%0Asemantically%20coherent%20ones.%20Extensive%20experiments%20across%20state-of-the-art%20MLLMs%0Aand%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Aexisting%20interpretability%20methods%2C%20yielding%20more%20faithful%20and%20fine-grained%0Aexplanations%20of%20model%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520multimodal%2520LLMs%2520via%2520intra-modal%2520token%2520interactions%26entry.906535625%3DJiawei%2520Liang%2520and%2520Ruoyu%2520Chen%2520and%2520Xianghao%2520Jiao%2520and%2520Siyuan%2520Liang%2520and%2520Shiming%2520Liu%2520and%2520Qunli%2520Zhang%2520and%2520Zheng%2520Hu%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520remarkable%2520success%250Aacross%2520diverse%2520vision-language%2520tasks%252C%2520yet%2520their%2520internal%2520decision-making%250Amechanisms%2520remain%2520insufficiently%2520understood.%2520Existing%2520interpretability%2520research%250Ahas%2520primarily%2520focused%2520on%2520cross-modal%2520attribution%252C%2520identifying%2520which%2520image%250Aregions%2520the%2520model%2520attends%2520to%2520during%2520output%2520generation.%2520However%252C%2520these%250Aapproaches%2520often%2520overlook%2520intra-modal%2520dependencies.%2520In%2520the%2520visual%2520modality%252C%250Aattributing%2520importance%2520to%2520isolated%2520image%2520patches%2520ignores%2520spatial%2520context%2520due%2520to%250Alimited%2520receptive%2520fields%252C%2520resulting%2520in%2520fragmented%2520and%2520noisy%2520explanations.%2520In%250Athe%2520textual%2520modality%252C%2520reliance%2520on%2520preceding%2520tokens%2520introduces%2520spurious%250Aactivations.%2520Failing%2520to%2520effectively%2520mitigate%2520these%2520interference%2520compromises%250Aattribution%2520fidelity.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520enhancing%250Ainterpretability%2520by%2520leveraging%2520intra-modal%2520interaction.%2520For%2520the%2520visual%2520branch%252C%250Awe%2520introduce%2520%255Ctextit%257BMulti-Scale%2520Explanation%2520Aggregation%257D%2520%2528MSEA%2529%252C%2520which%250Aaggregates%2520attributions%2520over%2520multi-scale%2520inputs%2520to%2520dynamically%2520adjust%2520receptive%250Afields%252C%2520producing%2520more%2520holistic%2520and%2520spatially%2520coherent%2520visual%2520explanations.%2520For%250Athe%2520textual%2520branch%252C%2520we%2520propose%2520%255Ctextit%257BActivation%2520Ranking%2520Correlation%257D%2520%2528ARC%2529%252C%250Awhich%2520measures%2520the%2520relevance%2520of%2520contextual%2520tokens%2520to%2520the%2520current%2520token%2520via%250Aalignment%2520of%2520their%2520top-%2524k%2524%2520prediction%2520rankings.%2520ARC%2520leverages%2520this%2520relevance%2520to%250Asuppress%2520spurious%2520activations%2520from%2520irrelevant%2520contexts%2520while%2520preserving%250Asemantically%2520coherent%2520ones.%2520Extensive%2520experiments%2520across%2520state-of-the-art%2520MLLMs%250Aand%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520outperforms%250Aexisting%2520interpretability%2520methods%252C%2520yielding%2520more%2520faithful%2520and%2520fine-grained%250Aexplanations%2520of%2520model%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20multimodal%20LLMs%20via%20intra-modal%20token%20interactions&entry.906535625=Jiawei%20Liang%20and%20Ruoyu%20Chen%20and%20Xianghao%20Jiao%20and%20Siyuan%20Liang%20and%20Shiming%20Liu%20and%20Qunli%20Zhang%20and%20Zheng%20Hu%20and%20Xiaochun%20Cao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20remarkable%20success%0Aacross%20diverse%20vision-language%20tasks%2C%20yet%20their%20internal%20decision-making%0Amechanisms%20remain%20insufficiently%20understood.%20Existing%20interpretability%20research%0Ahas%20primarily%20focused%20on%20cross-modal%20attribution%2C%20identifying%20which%20image%0Aregions%20the%20model%20attends%20to%20during%20output%20generation.%20However%2C%20these%0Aapproaches%20often%20overlook%20intra-modal%20dependencies.%20In%20the%20visual%20modality%2C%0Aattributing%20importance%20to%20isolated%20image%20patches%20ignores%20spatial%20context%20due%20to%0Alimited%20receptive%20fields%2C%20resulting%20in%20fragmented%20and%20noisy%20explanations.%20In%0Athe%20textual%20modality%2C%20reliance%20on%20preceding%20tokens%20introduces%20spurious%0Aactivations.%20Failing%20to%20effectively%20mitigate%20these%20interference%20compromises%0Aattribution%20fidelity.%20To%20address%20these%20limitations%2C%20we%20propose%20enhancing%0Ainterpretability%20by%20leveraging%20intra-modal%20interaction.%20For%20the%20visual%20branch%2C%0Awe%20introduce%20%5Ctextit%7BMulti-Scale%20Explanation%20Aggregation%7D%20%28MSEA%29%2C%20which%0Aaggregates%20attributions%20over%20multi-scale%20inputs%20to%20dynamically%20adjust%20receptive%0Afields%2C%20producing%20more%20holistic%20and%20spatially%20coherent%20visual%20explanations.%20For%0Athe%20textual%20branch%2C%20we%20propose%20%5Ctextit%7BActivation%20Ranking%20Correlation%7D%20%28ARC%29%2C%0Awhich%20measures%20the%20relevance%20of%20contextual%20tokens%20to%20the%20current%20token%20via%0Aalignment%20of%20their%20top-%24k%24%20prediction%20rankings.%20ARC%20leverages%20this%20relevance%20to%0Asuppress%20spurious%20activations%20from%20irrelevant%20contexts%20while%20preserving%0Asemantically%20coherent%20ones.%20Extensive%20experiments%20across%20state-of-the-art%20MLLMs%0Aand%20benchmark%20datasets%20demonstrate%20that%20our%20approach%20consistently%20outperforms%0Aexisting%20interpretability%20methods%2C%20yielding%20more%20faithful%20and%20fine-grained%0Aexplanations%20of%20model%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22415v1&entry.124074799=Read"},
{"title": "VoiceAssistant-Eval: Benchmarking AI Assistants across Listening,\n  Speaking, and Viewing", "author": "Ke Wang and Houxing Ren and Zimu Lu and Mingjie Zhan and Hongsheng Li", "abstract": "  The growing capabilities of large language models and multimodal systems have\nspurred interest in voice-first AI assistants, yet existing benchmarks are\ninadequate for evaluating the full range of these systems' capabilities. We\nintroduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI\nassistants across listening, speaking, and viewing. VoiceAssistant-Eval\ncomprises 10,497 curated examples spanning 13 task categories. These tasks\ninclude natural sounds, music, and spoken dialogue for listening; multi-turn\ndialogue, role-play imitation, and various scenarios for speaking; and highly\nheterogeneous images for viewing. To demonstrate its utility, we evaluate 21\nopen-source models and GPT-4o-Audio, measuring the quality of the response\ncontent and speech, as well as their consistency. The results reveal three key\nfindings: (1) proprietary models do not universally outperform open-source\nmodels; (2) most models excel at speaking tasks but lag in audio understanding;\nand (3) well-designed smaller models can rival much larger ones. Notably, the\nmid-sized Step-Audio-2-mini (7B) achieves more than double the listening\naccuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal\n(audio plus visual) input and role-play voice imitation tasks are difficult for\ncurrent models, and significant gaps persist in robustness and safety\nalignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous\nframework for evaluating and guiding the development of next-generation AI\nassistants. Code and data will be released at\nhttps://mathllm.github.io/VoiceAssistantEval/ .\n", "link": "http://arxiv.org/abs/2509.22651v1", "date": "2025-09-26", "relevancy": 2.6622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5418}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoiceAssistant-Eval%3A%20Benchmarking%20AI%20Assistants%20across%20Listening%2C%0A%20%20Speaking%2C%20and%20Viewing&body=Title%3A%20VoiceAssistant-Eval%3A%20Benchmarking%20AI%20Assistants%20across%20Listening%2C%0A%20%20Speaking%2C%20and%20Viewing%0AAuthor%3A%20Ke%20Wang%20and%20Houxing%20Ren%20and%20Zimu%20Lu%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20The%20growing%20capabilities%20of%20large%20language%20models%20and%20multimodal%20systems%20have%0Aspurred%20interest%20in%20voice-first%20AI%20assistants%2C%20yet%20existing%20benchmarks%20are%0Ainadequate%20for%20evaluating%20the%20full%20range%20of%20these%20systems%27%20capabilities.%20We%0Aintroduce%20VoiceAssistant-Eval%2C%20a%20comprehensive%20benchmark%20designed%20to%20assess%20AI%0Aassistants%20across%20listening%2C%20speaking%2C%20and%20viewing.%20VoiceAssistant-Eval%0Acomprises%2010%2C497%20curated%20examples%20spanning%2013%20task%20categories.%20These%20tasks%0Ainclude%20natural%20sounds%2C%20music%2C%20and%20spoken%20dialogue%20for%20listening%3B%20multi-turn%0Adialogue%2C%20role-play%20imitation%2C%20and%20various%20scenarios%20for%20speaking%3B%20and%20highly%0Aheterogeneous%20images%20for%20viewing.%20To%20demonstrate%20its%20utility%2C%20we%20evaluate%2021%0Aopen-source%20models%20and%20GPT-4o-Audio%2C%20measuring%20the%20quality%20of%20the%20response%0Acontent%20and%20speech%2C%20as%20well%20as%20their%20consistency.%20The%20results%20reveal%20three%20key%0Afindings%3A%20%281%29%20proprietary%20models%20do%20not%20universally%20outperform%20open-source%0Amodels%3B%20%282%29%20most%20models%20excel%20at%20speaking%20tasks%20but%20lag%20in%20audio%20understanding%3B%0Aand%20%283%29%20well-designed%20smaller%20models%20can%20rival%20much%20larger%20ones.%20Notably%2C%20the%0Amid-sized%20Step-Audio-2-mini%20%287B%29%20achieves%20more%20than%20double%20the%20listening%0Aaccuracy%20of%20LLaMA-Omni2-32B-Bilingual.%20However%2C%20challenges%20remain%3A%20multimodal%0A%28audio%20plus%20visual%29%20input%20and%20role-play%20voice%20imitation%20tasks%20are%20difficult%20for%0Acurrent%20models%2C%20and%20significant%20gaps%20persist%20in%20robustness%20and%20safety%0Aalignment.%20VoiceAssistant-Eval%20identifies%20these%20gaps%20and%20establishes%20a%20rigorous%0Aframework%20for%20evaluating%20and%20guiding%20the%20development%20of%20next-generation%20AI%0Aassistants.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//mathllm.github.io/VoiceAssistantEval/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoiceAssistant-Eval%253A%2520Benchmarking%2520AI%2520Assistants%2520across%2520Listening%252C%250A%2520%2520Speaking%252C%2520and%2520Viewing%26entry.906535625%3DKe%2520Wang%2520and%2520Houxing%2520Ren%2520and%2520Zimu%2520Lu%2520and%2520Mingjie%2520Zhan%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520The%2520growing%2520capabilities%2520of%2520large%2520language%2520models%2520and%2520multimodal%2520systems%2520have%250Aspurred%2520interest%2520in%2520voice-first%2520AI%2520assistants%252C%2520yet%2520existing%2520benchmarks%2520are%250Ainadequate%2520for%2520evaluating%2520the%2520full%2520range%2520of%2520these%2520systems%2527%2520capabilities.%2520We%250Aintroduce%2520VoiceAssistant-Eval%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520assess%2520AI%250Aassistants%2520across%2520listening%252C%2520speaking%252C%2520and%2520viewing.%2520VoiceAssistant-Eval%250Acomprises%252010%252C497%2520curated%2520examples%2520spanning%252013%2520task%2520categories.%2520These%2520tasks%250Ainclude%2520natural%2520sounds%252C%2520music%252C%2520and%2520spoken%2520dialogue%2520for%2520listening%253B%2520multi-turn%250Adialogue%252C%2520role-play%2520imitation%252C%2520and%2520various%2520scenarios%2520for%2520speaking%253B%2520and%2520highly%250Aheterogeneous%2520images%2520for%2520viewing.%2520To%2520demonstrate%2520its%2520utility%252C%2520we%2520evaluate%252021%250Aopen-source%2520models%2520and%2520GPT-4o-Audio%252C%2520measuring%2520the%2520quality%2520of%2520the%2520response%250Acontent%2520and%2520speech%252C%2520as%2520well%2520as%2520their%2520consistency.%2520The%2520results%2520reveal%2520three%2520key%250Afindings%253A%2520%25281%2529%2520proprietary%2520models%2520do%2520not%2520universally%2520outperform%2520open-source%250Amodels%253B%2520%25282%2529%2520most%2520models%2520excel%2520at%2520speaking%2520tasks%2520but%2520lag%2520in%2520audio%2520understanding%253B%250Aand%2520%25283%2529%2520well-designed%2520smaller%2520models%2520can%2520rival%2520much%2520larger%2520ones.%2520Notably%252C%2520the%250Amid-sized%2520Step-Audio-2-mini%2520%25287B%2529%2520achieves%2520more%2520than%2520double%2520the%2520listening%250Aaccuracy%2520of%2520LLaMA-Omni2-32B-Bilingual.%2520However%252C%2520challenges%2520remain%253A%2520multimodal%250A%2528audio%2520plus%2520visual%2529%2520input%2520and%2520role-play%2520voice%2520imitation%2520tasks%2520are%2520difficult%2520for%250Acurrent%2520models%252C%2520and%2520significant%2520gaps%2520persist%2520in%2520robustness%2520and%2520safety%250Aalignment.%2520VoiceAssistant-Eval%2520identifies%2520these%2520gaps%2520and%2520establishes%2520a%2520rigorous%250Aframework%2520for%2520evaluating%2520and%2520guiding%2520the%2520development%2520of%2520next-generation%2520AI%250Aassistants.%2520Code%2520and%2520data%2520will%2520be%2520released%2520at%250Ahttps%253A//mathllm.github.io/VoiceAssistantEval/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoiceAssistant-Eval%3A%20Benchmarking%20AI%20Assistants%20across%20Listening%2C%0A%20%20Speaking%2C%20and%20Viewing&entry.906535625=Ke%20Wang%20and%20Houxing%20Ren%20and%20Zimu%20Lu%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li&entry.1292438233=%20%20The%20growing%20capabilities%20of%20large%20language%20models%20and%20multimodal%20systems%20have%0Aspurred%20interest%20in%20voice-first%20AI%20assistants%2C%20yet%20existing%20benchmarks%20are%0Ainadequate%20for%20evaluating%20the%20full%20range%20of%20these%20systems%27%20capabilities.%20We%0Aintroduce%20VoiceAssistant-Eval%2C%20a%20comprehensive%20benchmark%20designed%20to%20assess%20AI%0Aassistants%20across%20listening%2C%20speaking%2C%20and%20viewing.%20VoiceAssistant-Eval%0Acomprises%2010%2C497%20curated%20examples%20spanning%2013%20task%20categories.%20These%20tasks%0Ainclude%20natural%20sounds%2C%20music%2C%20and%20spoken%20dialogue%20for%20listening%3B%20multi-turn%0Adialogue%2C%20role-play%20imitation%2C%20and%20various%20scenarios%20for%20speaking%3B%20and%20highly%0Aheterogeneous%20images%20for%20viewing.%20To%20demonstrate%20its%20utility%2C%20we%20evaluate%2021%0Aopen-source%20models%20and%20GPT-4o-Audio%2C%20measuring%20the%20quality%20of%20the%20response%0Acontent%20and%20speech%2C%20as%20well%20as%20their%20consistency.%20The%20results%20reveal%20three%20key%0Afindings%3A%20%281%29%20proprietary%20models%20do%20not%20universally%20outperform%20open-source%0Amodels%3B%20%282%29%20most%20models%20excel%20at%20speaking%20tasks%20but%20lag%20in%20audio%20understanding%3B%0Aand%20%283%29%20well-designed%20smaller%20models%20can%20rival%20much%20larger%20ones.%20Notably%2C%20the%0Amid-sized%20Step-Audio-2-mini%20%287B%29%20achieves%20more%20than%20double%20the%20listening%0Aaccuracy%20of%20LLaMA-Omni2-32B-Bilingual.%20However%2C%20challenges%20remain%3A%20multimodal%0A%28audio%20plus%20visual%29%20input%20and%20role-play%20voice%20imitation%20tasks%20are%20difficult%20for%0Acurrent%20models%2C%20and%20significant%20gaps%20persist%20in%20robustness%20and%20safety%0Aalignment.%20VoiceAssistant-Eval%20identifies%20these%20gaps%20and%20establishes%20a%20rigorous%0Aframework%20for%20evaluating%20and%20guiding%20the%20development%20of%20next-generation%20AI%0Aassistants.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//mathllm.github.io/VoiceAssistantEval/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22651v1&entry.124074799=Read"},
{"title": "Category Discovery: An Open-World Perspective", "author": "Zhenqi He and Yuanpei Liu and Kai Han", "abstract": "  Category discovery (CD) is an emerging open-world learning task, which aims\nat automatically categorizing unlabelled data containing instances from unseen\nclasses, given some labelled data from seen classes. This task has attracted\nsignificant attention over the years and leads to a rich body of literature\ntrying to address the problem from different perspectives. In this survey, we\nprovide a comprehensive review of the literature, and offer detailed analysis\nand in-depth discussion on different methods. Firstly, we introduce a taxonomy\nfor the literature by considering two base settings, namely novel category\ndiscovery (NCD) and generalized category discovery (GCD), and several derived\nsettings that are designed to address the extra challenges in different\nreal-world application scenarios, including continual category discovery,\nskewed data distribution, federated category discovery, etc. Secondly, for each\nsetting, we offer a detailed analysis of the methods encompassing three\nfundamental components, representation learning, label assignment, and\nestimation of class number. Thirdly, we benchmark all the methods and distill\nkey insights showing that large-scale pretrained backbones, hierarchical and\nauxiliary cues, and curriculum-style training are all beneficial for category\ndiscovery, while challenges remain in the design of label assignment, the\nestimation of class numbers, and scaling to complex multi-object\nscenarios.Finally, we discuss the key insights from the literature so far and\npoint out promising future research directions. We compile a living survey of\nthe category discovery literature at\n\\href{https://github.com/Visual-AI/Category-Discovery}{https://github.com/Visual-AI/Category-Discovery}.\n", "link": "http://arxiv.org/abs/2509.22542v1", "date": "2025-09-26", "relevancy": 2.6509, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5387}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category%20Discovery%3A%20An%20Open-World%20Perspective&body=Title%3A%20Category%20Discovery%3A%20An%20Open-World%20Perspective%0AAuthor%3A%20Zhenqi%20He%20and%20Yuanpei%20Liu%20and%20Kai%20Han%0AAbstract%3A%20%20%20Category%20discovery%20%28CD%29%20is%20an%20emerging%20open-world%20learning%20task%2C%20which%20aims%0Aat%20automatically%20categorizing%20unlabelled%20data%20containing%20instances%20from%20unseen%0Aclasses%2C%20given%20some%20labelled%20data%20from%20seen%20classes.%20This%20task%20has%20attracted%0Asignificant%20attention%20over%20the%20years%20and%20leads%20to%20a%20rich%20body%20of%20literature%0Atrying%20to%20address%20the%20problem%20from%20different%20perspectives.%20In%20this%20survey%2C%20we%0Aprovide%20a%20comprehensive%20review%20of%20the%20literature%2C%20and%20offer%20detailed%20analysis%0Aand%20in-depth%20discussion%20on%20different%20methods.%20Firstly%2C%20we%20introduce%20a%20taxonomy%0Afor%20the%20literature%20by%20considering%20two%20base%20settings%2C%20namely%20novel%20category%0Adiscovery%20%28NCD%29%20and%20generalized%20category%20discovery%20%28GCD%29%2C%20and%20several%20derived%0Asettings%20that%20are%20designed%20to%20address%20the%20extra%20challenges%20in%20different%0Areal-world%20application%20scenarios%2C%20including%20continual%20category%20discovery%2C%0Askewed%20data%20distribution%2C%20federated%20category%20discovery%2C%20etc.%20Secondly%2C%20for%20each%0Asetting%2C%20we%20offer%20a%20detailed%20analysis%20of%20the%20methods%20encompassing%20three%0Afundamental%20components%2C%20representation%20learning%2C%20label%20assignment%2C%20and%0Aestimation%20of%20class%20number.%20Thirdly%2C%20we%20benchmark%20all%20the%20methods%20and%20distill%0Akey%20insights%20showing%20that%20large-scale%20pretrained%20backbones%2C%20hierarchical%20and%0Aauxiliary%20cues%2C%20and%20curriculum-style%20training%20are%20all%20beneficial%20for%20category%0Adiscovery%2C%20while%20challenges%20remain%20in%20the%20design%20of%20label%20assignment%2C%20the%0Aestimation%20of%20class%20numbers%2C%20and%20scaling%20to%20complex%20multi-object%0Ascenarios.Finally%2C%20we%20discuss%20the%20key%20insights%20from%20the%20literature%20so%20far%20and%0Apoint%20out%20promising%20future%20research%20directions.%20We%20compile%20a%20living%20survey%20of%0Athe%20category%20discovery%20literature%20at%0A%5Chref%7Bhttps%3A//github.com/Visual-AI/Category-Discovery%7D%7Bhttps%3A//github.com/Visual-AI/Category-Discovery%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory%2520Discovery%253A%2520An%2520Open-World%2520Perspective%26entry.906535625%3DZhenqi%2520He%2520and%2520Yuanpei%2520Liu%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520Category%2520discovery%2520%2528CD%2529%2520is%2520an%2520emerging%2520open-world%2520learning%2520task%252C%2520which%2520aims%250Aat%2520automatically%2520categorizing%2520unlabelled%2520data%2520containing%2520instances%2520from%2520unseen%250Aclasses%252C%2520given%2520some%2520labelled%2520data%2520from%2520seen%2520classes.%2520This%2520task%2520has%2520attracted%250Asignificant%2520attention%2520over%2520the%2520years%2520and%2520leads%2520to%2520a%2520rich%2520body%2520of%2520literature%250Atrying%2520to%2520address%2520the%2520problem%2520from%2520different%2520perspectives.%2520In%2520this%2520survey%252C%2520we%250Aprovide%2520a%2520comprehensive%2520review%2520of%2520the%2520literature%252C%2520and%2520offer%2520detailed%2520analysis%250Aand%2520in-depth%2520discussion%2520on%2520different%2520methods.%2520Firstly%252C%2520we%2520introduce%2520a%2520taxonomy%250Afor%2520the%2520literature%2520by%2520considering%2520two%2520base%2520settings%252C%2520namely%2520novel%2520category%250Adiscovery%2520%2528NCD%2529%2520and%2520generalized%2520category%2520discovery%2520%2528GCD%2529%252C%2520and%2520several%2520derived%250Asettings%2520that%2520are%2520designed%2520to%2520address%2520the%2520extra%2520challenges%2520in%2520different%250Areal-world%2520application%2520scenarios%252C%2520including%2520continual%2520category%2520discovery%252C%250Askewed%2520data%2520distribution%252C%2520federated%2520category%2520discovery%252C%2520etc.%2520Secondly%252C%2520for%2520each%250Asetting%252C%2520we%2520offer%2520a%2520detailed%2520analysis%2520of%2520the%2520methods%2520encompassing%2520three%250Afundamental%2520components%252C%2520representation%2520learning%252C%2520label%2520assignment%252C%2520and%250Aestimation%2520of%2520class%2520number.%2520Thirdly%252C%2520we%2520benchmark%2520all%2520the%2520methods%2520and%2520distill%250Akey%2520insights%2520showing%2520that%2520large-scale%2520pretrained%2520backbones%252C%2520hierarchical%2520and%250Aauxiliary%2520cues%252C%2520and%2520curriculum-style%2520training%2520are%2520all%2520beneficial%2520for%2520category%250Adiscovery%252C%2520while%2520challenges%2520remain%2520in%2520the%2520design%2520of%2520label%2520assignment%252C%2520the%250Aestimation%2520of%2520class%2520numbers%252C%2520and%2520scaling%2520to%2520complex%2520multi-object%250Ascenarios.Finally%252C%2520we%2520discuss%2520the%2520key%2520insights%2520from%2520the%2520literature%2520so%2520far%2520and%250Apoint%2520out%2520promising%2520future%2520research%2520directions.%2520We%2520compile%2520a%2520living%2520survey%2520of%250Athe%2520category%2520discovery%2520literature%2520at%250A%255Chref%257Bhttps%253A//github.com/Visual-AI/Category-Discovery%257D%257Bhttps%253A//github.com/Visual-AI/Category-Discovery%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category%20Discovery%3A%20An%20Open-World%20Perspective&entry.906535625=Zhenqi%20He%20and%20Yuanpei%20Liu%20and%20Kai%20Han&entry.1292438233=%20%20Category%20discovery%20%28CD%29%20is%20an%20emerging%20open-world%20learning%20task%2C%20which%20aims%0Aat%20automatically%20categorizing%20unlabelled%20data%20containing%20instances%20from%20unseen%0Aclasses%2C%20given%20some%20labelled%20data%20from%20seen%20classes.%20This%20task%20has%20attracted%0Asignificant%20attention%20over%20the%20years%20and%20leads%20to%20a%20rich%20body%20of%20literature%0Atrying%20to%20address%20the%20problem%20from%20different%20perspectives.%20In%20this%20survey%2C%20we%0Aprovide%20a%20comprehensive%20review%20of%20the%20literature%2C%20and%20offer%20detailed%20analysis%0Aand%20in-depth%20discussion%20on%20different%20methods.%20Firstly%2C%20we%20introduce%20a%20taxonomy%0Afor%20the%20literature%20by%20considering%20two%20base%20settings%2C%20namely%20novel%20category%0Adiscovery%20%28NCD%29%20and%20generalized%20category%20discovery%20%28GCD%29%2C%20and%20several%20derived%0Asettings%20that%20are%20designed%20to%20address%20the%20extra%20challenges%20in%20different%0Areal-world%20application%20scenarios%2C%20including%20continual%20category%20discovery%2C%0Askewed%20data%20distribution%2C%20federated%20category%20discovery%2C%20etc.%20Secondly%2C%20for%20each%0Asetting%2C%20we%20offer%20a%20detailed%20analysis%20of%20the%20methods%20encompassing%20three%0Afundamental%20components%2C%20representation%20learning%2C%20label%20assignment%2C%20and%0Aestimation%20of%20class%20number.%20Thirdly%2C%20we%20benchmark%20all%20the%20methods%20and%20distill%0Akey%20insights%20showing%20that%20large-scale%20pretrained%20backbones%2C%20hierarchical%20and%0Aauxiliary%20cues%2C%20and%20curriculum-style%20training%20are%20all%20beneficial%20for%20category%0Adiscovery%2C%20while%20challenges%20remain%20in%20the%20design%20of%20label%20assignment%2C%20the%0Aestimation%20of%20class%20numbers%2C%20and%20scaling%20to%20complex%20multi-object%0Ascenarios.Finally%2C%20we%20discuss%20the%20key%20insights%20from%20the%20literature%20so%20far%20and%0Apoint%20out%20promising%20future%20research%20directions.%20We%20compile%20a%20living%20survey%20of%0Athe%20category%20discovery%20literature%20at%0A%5Chref%7Bhttps%3A//github.com/Visual-AI/Category-Discovery%7D%7Bhttps%3A//github.com/Visual-AI/Category-Discovery%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22542v1&entry.124074799=Read"},
{"title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic\n  Forgetting", "author": "Asher J. Hancock and Xindi Wu and Lihan Zha and Olga Russakovsky and Anirudha Majumdar", "abstract": "  Fine-tuning vision-language models (VLMs) on robot teleoperation data to\ncreate vision-language-action (VLA) models is a promising paradigm for training\ngeneralist policies, but it suffers from a fundamental tradeoff: learning to\nproduce actions often diminishes the VLM's foundational reasoning and\nmultimodal understanding, hindering generalization to novel scenarios,\ninstruction following, and semantic understanding. We argue that this\ncatastrophic forgetting is due to a distribution mismatch between the VLM's\ninternet-scale pretraining corpus and the robotics fine-tuning data. Inspired\nby this observation, we introduce VLM2VLA: a VLA training paradigm that first\nresolves this mismatch at the data level by representing low-level actions with\nnatural language. This alignment makes it possible to train VLAs solely with\nLow-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and\naverting catastrophic forgetting. As a result, the VLM can be fine-tuned on\nrobot teleoperation data without fundamentally altering the underlying\narchitecture and without expensive co-training on internet-scale VLM datasets.\nThrough extensive Visual Question Answering (VQA) studies and over 800\nreal-world robotics experiments, we demonstrate that VLM2VLA preserves the\nVLM's core capabilities, enabling zero-shot generalization to novel tasks that\nrequire open-world semantic reasoning and multilingual instruction following.\n", "link": "http://arxiv.org/abs/2509.22195v1", "date": "2025-09-26", "relevancy": 2.5962, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Actions%20as%20Language%3A%20Fine-Tuning%20VLMs%20into%20VLAs%20Without%20Catastrophic%0A%20%20Forgetting&body=Title%3A%20Actions%20as%20Language%3A%20Fine-Tuning%20VLMs%20into%20VLAs%20Without%20Catastrophic%0A%20%20Forgetting%0AAuthor%3A%20Asher%20J.%20Hancock%20and%20Xindi%20Wu%20and%20Lihan%20Zha%20and%20Olga%20Russakovsky%20and%20Anirudha%20Majumdar%0AAbstract%3A%20%20%20Fine-tuning%20vision-language%20models%20%28VLMs%29%20on%20robot%20teleoperation%20data%20to%0Acreate%20vision-language-action%20%28VLA%29%20models%20is%20a%20promising%20paradigm%20for%20training%0Ageneralist%20policies%2C%20but%20it%20suffers%20from%20a%20fundamental%20tradeoff%3A%20learning%20to%0Aproduce%20actions%20often%20diminishes%20the%20VLM%27s%20foundational%20reasoning%20and%0Amultimodal%20understanding%2C%20hindering%20generalization%20to%20novel%20scenarios%2C%0Ainstruction%20following%2C%20and%20semantic%20understanding.%20We%20argue%20that%20this%0Acatastrophic%20forgetting%20is%20due%20to%20a%20distribution%20mismatch%20between%20the%20VLM%27s%0Ainternet-scale%20pretraining%20corpus%20and%20the%20robotics%20fine-tuning%20data.%20Inspired%0Aby%20this%20observation%2C%20we%20introduce%20VLM2VLA%3A%20a%20VLA%20training%20paradigm%20that%20first%0Aresolves%20this%20mismatch%20at%20the%20data%20level%20by%20representing%20low-level%20actions%20with%0Anatural%20language.%20This%20alignment%20makes%20it%20possible%20to%20train%20VLAs%20solely%20with%0ALow-Rank%20Adaptation%20%28LoRA%29%2C%20thereby%20minimally%20modifying%20the%20VLM%20backbone%20and%0Aaverting%20catastrophic%20forgetting.%20As%20a%20result%2C%20the%20VLM%20can%20be%20fine-tuned%20on%0Arobot%20teleoperation%20data%20without%20fundamentally%20altering%20the%20underlying%0Aarchitecture%20and%20without%20expensive%20co-training%20on%20internet-scale%20VLM%20datasets.%0AThrough%20extensive%20Visual%20Question%20Answering%20%28VQA%29%20studies%20and%20over%20800%0Areal-world%20robotics%20experiments%2C%20we%20demonstrate%20that%20VLM2VLA%20preserves%20the%0AVLM%27s%20core%20capabilities%2C%20enabling%20zero-shot%20generalization%20to%20novel%20tasks%20that%0Arequire%20open-world%20semantic%20reasoning%20and%20multilingual%20instruction%20following.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActions%2520as%2520Language%253A%2520Fine-Tuning%2520VLMs%2520into%2520VLAs%2520Without%2520Catastrophic%250A%2520%2520Forgetting%26entry.906535625%3DAsher%2520J.%2520Hancock%2520and%2520Xindi%2520Wu%2520and%2520Lihan%2520Zha%2520and%2520Olga%2520Russakovsky%2520and%2520Anirudha%2520Majumdar%26entry.1292438233%3D%2520%2520Fine-tuning%2520vision-language%2520models%2520%2528VLMs%2529%2520on%2520robot%2520teleoperation%2520data%2520to%250Acreate%2520vision-language-action%2520%2528VLA%2529%2520models%2520is%2520a%2520promising%2520paradigm%2520for%2520training%250Ageneralist%2520policies%252C%2520but%2520it%2520suffers%2520from%2520a%2520fundamental%2520tradeoff%253A%2520learning%2520to%250Aproduce%2520actions%2520often%2520diminishes%2520the%2520VLM%2527s%2520foundational%2520reasoning%2520and%250Amultimodal%2520understanding%252C%2520hindering%2520generalization%2520to%2520novel%2520scenarios%252C%250Ainstruction%2520following%252C%2520and%2520semantic%2520understanding.%2520We%2520argue%2520that%2520this%250Acatastrophic%2520forgetting%2520is%2520due%2520to%2520a%2520distribution%2520mismatch%2520between%2520the%2520VLM%2527s%250Ainternet-scale%2520pretraining%2520corpus%2520and%2520the%2520robotics%2520fine-tuning%2520data.%2520Inspired%250Aby%2520this%2520observation%252C%2520we%2520introduce%2520VLM2VLA%253A%2520a%2520VLA%2520training%2520paradigm%2520that%2520first%250Aresolves%2520this%2520mismatch%2520at%2520the%2520data%2520level%2520by%2520representing%2520low-level%2520actions%2520with%250Anatural%2520language.%2520This%2520alignment%2520makes%2520it%2520possible%2520to%2520train%2520VLAs%2520solely%2520with%250ALow-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520thereby%2520minimally%2520modifying%2520the%2520VLM%2520backbone%2520and%250Aaverting%2520catastrophic%2520forgetting.%2520As%2520a%2520result%252C%2520the%2520VLM%2520can%2520be%2520fine-tuned%2520on%250Arobot%2520teleoperation%2520data%2520without%2520fundamentally%2520altering%2520the%2520underlying%250Aarchitecture%2520and%2520without%2520expensive%2520co-training%2520on%2520internet-scale%2520VLM%2520datasets.%250AThrough%2520extensive%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520studies%2520and%2520over%2520800%250Areal-world%2520robotics%2520experiments%252C%2520we%2520demonstrate%2520that%2520VLM2VLA%2520preserves%2520the%250AVLM%2527s%2520core%2520capabilities%252C%2520enabling%2520zero-shot%2520generalization%2520to%2520novel%2520tasks%2520that%250Arequire%2520open-world%2520semantic%2520reasoning%2520and%2520multilingual%2520instruction%2520following.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Actions%20as%20Language%3A%20Fine-Tuning%20VLMs%20into%20VLAs%20Without%20Catastrophic%0A%20%20Forgetting&entry.906535625=Asher%20J.%20Hancock%20and%20Xindi%20Wu%20and%20Lihan%20Zha%20and%20Olga%20Russakovsky%20and%20Anirudha%20Majumdar&entry.1292438233=%20%20Fine-tuning%20vision-language%20models%20%28VLMs%29%20on%20robot%20teleoperation%20data%20to%0Acreate%20vision-language-action%20%28VLA%29%20models%20is%20a%20promising%20paradigm%20for%20training%0Ageneralist%20policies%2C%20but%20it%20suffers%20from%20a%20fundamental%20tradeoff%3A%20learning%20to%0Aproduce%20actions%20often%20diminishes%20the%20VLM%27s%20foundational%20reasoning%20and%0Amultimodal%20understanding%2C%20hindering%20generalization%20to%20novel%20scenarios%2C%0Ainstruction%20following%2C%20and%20semantic%20understanding.%20We%20argue%20that%20this%0Acatastrophic%20forgetting%20is%20due%20to%20a%20distribution%20mismatch%20between%20the%20VLM%27s%0Ainternet-scale%20pretraining%20corpus%20and%20the%20robotics%20fine-tuning%20data.%20Inspired%0Aby%20this%20observation%2C%20we%20introduce%20VLM2VLA%3A%20a%20VLA%20training%20paradigm%20that%20first%0Aresolves%20this%20mismatch%20at%20the%20data%20level%20by%20representing%20low-level%20actions%20with%0Anatural%20language.%20This%20alignment%20makes%20it%20possible%20to%20train%20VLAs%20solely%20with%0ALow-Rank%20Adaptation%20%28LoRA%29%2C%20thereby%20minimally%20modifying%20the%20VLM%20backbone%20and%0Aaverting%20catastrophic%20forgetting.%20As%20a%20result%2C%20the%20VLM%20can%20be%20fine-tuned%20on%0Arobot%20teleoperation%20data%20without%20fundamentally%20altering%20the%20underlying%0Aarchitecture%20and%20without%20expensive%20co-training%20on%20internet-scale%20VLM%20datasets.%0AThrough%20extensive%20Visual%20Question%20Answering%20%28VQA%29%20studies%20and%20over%20800%0Areal-world%20robotics%20experiments%2C%20we%20demonstrate%20that%20VLM2VLA%20preserves%20the%0AVLM%27s%20core%20capabilities%2C%20enabling%20zero-shot%20generalization%20to%20novel%20tasks%20that%0Arequire%20open-world%20semantic%20reasoning%20and%20multilingual%20instruction%20following.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22195v1&entry.124074799=Read"},
{"title": "HierLight-YOLO: A Hierarchical and Lightweight Object Detection Network\n  for UAV Photography", "author": "Defan Chen and Yaohua Hu and Luchan Zhang", "abstract": "  The real-time detection of small objects in complex scenes, such as the\nunmanned aerial vehicle (UAV) photography captured by drones, has dual\nchallenges of detecting small targets (<32 pixels) and maintaining real-time\nefficiency on resource-constrained platforms. While YOLO-series detectors have\nachieved remarkable success in real-time large object detection, they suffer\nfrom significantly higher false negative rates for drone-based detection where\nsmall objects dominate, compared to large object scenarios. This paper proposes\nHierLight-YOLO, a hierarchical feature fusion and lightweight model that\nenhances the real-time detection of small objects, based on the YOLOv8\narchitecture. We propose the Hierarchical Extended Path Aggregation Network\n(HEPAN), a multi-scale feature fusion method through hierarchical cross-level\nconnections, enhancing the small object detection accuracy. HierLight-YOLO\nincludes two innovative lightweight modules: Inverted Residual Depthwise\nConvolution Block (IRDCB) and Lightweight Downsample (LDown) module, which\nsignificantly reduce the model's parameters and computational complexity\nwithout sacrificing detection capabilities. Small object detection head is\ndesigned to further enhance spatial resolution and feature fusion to tackle the\ntiny object (4 pixels) detection. Comparison experiments and ablation studies\non the VisDrone2019 benchmark demonstrate state-of-the-art performance of\nHierLight-YOLO.\n", "link": "http://arxiv.org/abs/2509.22365v1", "date": "2025-09-26", "relevancy": 2.596, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5261}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5168}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HierLight-YOLO%3A%20A%20Hierarchical%20and%20Lightweight%20Object%20Detection%20Network%0A%20%20for%20UAV%20Photography&body=Title%3A%20HierLight-YOLO%3A%20A%20Hierarchical%20and%20Lightweight%20Object%20Detection%20Network%0A%20%20for%20UAV%20Photography%0AAuthor%3A%20Defan%20Chen%20and%20Yaohua%20Hu%20and%20Luchan%20Zhang%0AAbstract%3A%20%20%20The%20real-time%20detection%20of%20small%20objects%20in%20complex%20scenes%2C%20such%20as%20the%0Aunmanned%20aerial%20vehicle%20%28UAV%29%20photography%20captured%20by%20drones%2C%20has%20dual%0Achallenges%20of%20detecting%20small%20targets%20%28%3C32%20pixels%29%20and%20maintaining%20real-time%0Aefficiency%20on%20resource-constrained%20platforms.%20While%20YOLO-series%20detectors%20have%0Aachieved%20remarkable%20success%20in%20real-time%20large%20object%20detection%2C%20they%20suffer%0Afrom%20significantly%20higher%20false%20negative%20rates%20for%20drone-based%20detection%20where%0Asmall%20objects%20dominate%2C%20compared%20to%20large%20object%20scenarios.%20This%20paper%20proposes%0AHierLight-YOLO%2C%20a%20hierarchical%20feature%20fusion%20and%20lightweight%20model%20that%0Aenhances%20the%20real-time%20detection%20of%20small%20objects%2C%20based%20on%20the%20YOLOv8%0Aarchitecture.%20We%20propose%20the%20Hierarchical%20Extended%20Path%20Aggregation%20Network%0A%28HEPAN%29%2C%20a%20multi-scale%20feature%20fusion%20method%20through%20hierarchical%20cross-level%0Aconnections%2C%20enhancing%20the%20small%20object%20detection%20accuracy.%20HierLight-YOLO%0Aincludes%20two%20innovative%20lightweight%20modules%3A%20Inverted%20Residual%20Depthwise%0AConvolution%20Block%20%28IRDCB%29%20and%20Lightweight%20Downsample%20%28LDown%29%20module%2C%20which%0Asignificantly%20reduce%20the%20model%27s%20parameters%20and%20computational%20complexity%0Awithout%20sacrificing%20detection%20capabilities.%20Small%20object%20detection%20head%20is%0Adesigned%20to%20further%20enhance%20spatial%20resolution%20and%20feature%20fusion%20to%20tackle%20the%0Atiny%20object%20%284%20pixels%29%20detection.%20Comparison%20experiments%20and%20ablation%20studies%0Aon%20the%20VisDrone2019%20benchmark%20demonstrate%20state-of-the-art%20performance%20of%0AHierLight-YOLO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierLight-YOLO%253A%2520A%2520Hierarchical%2520and%2520Lightweight%2520Object%2520Detection%2520Network%250A%2520%2520for%2520UAV%2520Photography%26entry.906535625%3DDefan%2520Chen%2520and%2520Yaohua%2520Hu%2520and%2520Luchan%2520Zhang%26entry.1292438233%3D%2520%2520The%2520real-time%2520detection%2520of%2520small%2520objects%2520in%2520complex%2520scenes%252C%2520such%2520as%2520the%250Aunmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520photography%2520captured%2520by%2520drones%252C%2520has%2520dual%250Achallenges%2520of%2520detecting%2520small%2520targets%2520%2528%253C32%2520pixels%2529%2520and%2520maintaining%2520real-time%250Aefficiency%2520on%2520resource-constrained%2520platforms.%2520While%2520YOLO-series%2520detectors%2520have%250Aachieved%2520remarkable%2520success%2520in%2520real-time%2520large%2520object%2520detection%252C%2520they%2520suffer%250Afrom%2520significantly%2520higher%2520false%2520negative%2520rates%2520for%2520drone-based%2520detection%2520where%250Asmall%2520objects%2520dominate%252C%2520compared%2520to%2520large%2520object%2520scenarios.%2520This%2520paper%2520proposes%250AHierLight-YOLO%252C%2520a%2520hierarchical%2520feature%2520fusion%2520and%2520lightweight%2520model%2520that%250Aenhances%2520the%2520real-time%2520detection%2520of%2520small%2520objects%252C%2520based%2520on%2520the%2520YOLOv8%250Aarchitecture.%2520We%2520propose%2520the%2520Hierarchical%2520Extended%2520Path%2520Aggregation%2520Network%250A%2528HEPAN%2529%252C%2520a%2520multi-scale%2520feature%2520fusion%2520method%2520through%2520hierarchical%2520cross-level%250Aconnections%252C%2520enhancing%2520the%2520small%2520object%2520detection%2520accuracy.%2520HierLight-YOLO%250Aincludes%2520two%2520innovative%2520lightweight%2520modules%253A%2520Inverted%2520Residual%2520Depthwise%250AConvolution%2520Block%2520%2528IRDCB%2529%2520and%2520Lightweight%2520Downsample%2520%2528LDown%2529%2520module%252C%2520which%250Asignificantly%2520reduce%2520the%2520model%2527s%2520parameters%2520and%2520computational%2520complexity%250Awithout%2520sacrificing%2520detection%2520capabilities.%2520Small%2520object%2520detection%2520head%2520is%250Adesigned%2520to%2520further%2520enhance%2520spatial%2520resolution%2520and%2520feature%2520fusion%2520to%2520tackle%2520the%250Atiny%2520object%2520%25284%2520pixels%2529%2520detection.%2520Comparison%2520experiments%2520and%2520ablation%2520studies%250Aon%2520the%2520VisDrone2019%2520benchmark%2520demonstrate%2520state-of-the-art%2520performance%2520of%250AHierLight-YOLO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HierLight-YOLO%3A%20A%20Hierarchical%20and%20Lightweight%20Object%20Detection%20Network%0A%20%20for%20UAV%20Photography&entry.906535625=Defan%20Chen%20and%20Yaohua%20Hu%20and%20Luchan%20Zhang&entry.1292438233=%20%20The%20real-time%20detection%20of%20small%20objects%20in%20complex%20scenes%2C%20such%20as%20the%0Aunmanned%20aerial%20vehicle%20%28UAV%29%20photography%20captured%20by%20drones%2C%20has%20dual%0Achallenges%20of%20detecting%20small%20targets%20%28%3C32%20pixels%29%20and%20maintaining%20real-time%0Aefficiency%20on%20resource-constrained%20platforms.%20While%20YOLO-series%20detectors%20have%0Aachieved%20remarkable%20success%20in%20real-time%20large%20object%20detection%2C%20they%20suffer%0Afrom%20significantly%20higher%20false%20negative%20rates%20for%20drone-based%20detection%20where%0Asmall%20objects%20dominate%2C%20compared%20to%20large%20object%20scenarios.%20This%20paper%20proposes%0AHierLight-YOLO%2C%20a%20hierarchical%20feature%20fusion%20and%20lightweight%20model%20that%0Aenhances%20the%20real-time%20detection%20of%20small%20objects%2C%20based%20on%20the%20YOLOv8%0Aarchitecture.%20We%20propose%20the%20Hierarchical%20Extended%20Path%20Aggregation%20Network%0A%28HEPAN%29%2C%20a%20multi-scale%20feature%20fusion%20method%20through%20hierarchical%20cross-level%0Aconnections%2C%20enhancing%20the%20small%20object%20detection%20accuracy.%20HierLight-YOLO%0Aincludes%20two%20innovative%20lightweight%20modules%3A%20Inverted%20Residual%20Depthwise%0AConvolution%20Block%20%28IRDCB%29%20and%20Lightweight%20Downsample%20%28LDown%29%20module%2C%20which%0Asignificantly%20reduce%20the%20model%27s%20parameters%20and%20computational%20complexity%0Awithout%20sacrificing%20detection%20capabilities.%20Small%20object%20detection%20head%20is%0Adesigned%20to%20further%20enhance%20spatial%20resolution%20and%20feature%20fusion%20to%20tackle%20the%0Atiny%20object%20%284%20pixels%29%20detection.%20Comparison%20experiments%20and%20ablation%20studies%0Aon%20the%20VisDrone2019%20benchmark%20demonstrate%20state-of-the-art%20performance%20of%0AHierLight-YOLO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22365v1&entry.124074799=Read"},
{"title": "Multi-View Hypercomplex Learning for Breast Cancer Screening", "author": "Eleonora Lopez and Eleonora Grassucci and Danilo Comminiello", "abstract": "  Radiologists interpret mammography exams by jointly analyzing all four views,\nas correlations among them are crucial for accurate diagnosis. Recent methods\nemploy dedicated fusion blocks to capture such dependencies, but these are\noften hindered by view dominance, training instability, and computational\noverhead. To address these challenges, we introduce multi-view hypercomplex\nlearning, a novel learning paradigm for multi-view breast cancer classification\nbased on parameterized hypercomplex neural networks (PHNNs). Thanks to\nhypercomplex algebra, our models intrinsically capture both intra- and\ninter-view relations. We propose PHResNets for two-view exams and two\ncomplementary four-view architectures: PHYBOnet, optimized for efficiency, and\nPHYSEnet, optimized for accuracy. Extensive experiments demonstrate that our\napproach consistently outperforms state-of-the-art multi-view models, while\nalso generalizing across radiographic modalities and tasks such as disease\nclassification from chest X-rays and multimodal brain tumor segmentation. Full\ncode and pretrained models are available at https://github.com/ispamm/PHBreast.\n", "link": "http://arxiv.org/abs/2204.05798v4", "date": "2025-09-26", "relevancy": 2.5808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5034}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Hypercomplex%20Learning%20for%20Breast%20Cancer%20Screening&body=Title%3A%20Multi-View%20Hypercomplex%20Learning%20for%20Breast%20Cancer%20Screening%0AAuthor%3A%20Eleonora%20Lopez%20and%20Eleonora%20Grassucci%20and%20Danilo%20Comminiello%0AAbstract%3A%20%20%20Radiologists%20interpret%20mammography%20exams%20by%20jointly%20analyzing%20all%20four%20views%2C%0Aas%20correlations%20among%20them%20are%20crucial%20for%20accurate%20diagnosis.%20Recent%20methods%0Aemploy%20dedicated%20fusion%20blocks%20to%20capture%20such%20dependencies%2C%20but%20these%20are%0Aoften%20hindered%20by%20view%20dominance%2C%20training%20instability%2C%20and%20computational%0Aoverhead.%20To%20address%20these%20challenges%2C%20we%20introduce%20multi-view%20hypercomplex%0Alearning%2C%20a%20novel%20learning%20paradigm%20for%20multi-view%20breast%20cancer%20classification%0Abased%20on%20parameterized%20hypercomplex%20neural%20networks%20%28PHNNs%29.%20Thanks%20to%0Ahypercomplex%20algebra%2C%20our%20models%20intrinsically%20capture%20both%20intra-%20and%0Ainter-view%20relations.%20We%20propose%20PHResNets%20for%20two-view%20exams%20and%20two%0Acomplementary%20four-view%20architectures%3A%20PHYBOnet%2C%20optimized%20for%20efficiency%2C%20and%0APHYSEnet%2C%20optimized%20for%20accuracy.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20consistently%20outperforms%20state-of-the-art%20multi-view%20models%2C%20while%0Aalso%20generalizing%20across%20radiographic%20modalities%20and%20tasks%20such%20as%20disease%0Aclassification%20from%20chest%20X-rays%20and%20multimodal%20brain%20tumor%20segmentation.%20Full%0Acode%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/ispamm/PHBreast.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.05798v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Hypercomplex%2520Learning%2520for%2520Breast%2520Cancer%2520Screening%26entry.906535625%3DEleonora%2520Lopez%2520and%2520Eleonora%2520Grassucci%2520and%2520Danilo%2520Comminiello%26entry.1292438233%3D%2520%2520Radiologists%2520interpret%2520mammography%2520exams%2520by%2520jointly%2520analyzing%2520all%2520four%2520views%252C%250Aas%2520correlations%2520among%2520them%2520are%2520crucial%2520for%2520accurate%2520diagnosis.%2520Recent%2520methods%250Aemploy%2520dedicated%2520fusion%2520blocks%2520to%2520capture%2520such%2520dependencies%252C%2520but%2520these%2520are%250Aoften%2520hindered%2520by%2520view%2520dominance%252C%2520training%2520instability%252C%2520and%2520computational%250Aoverhead.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520multi-view%2520hypercomplex%250Alearning%252C%2520a%2520novel%2520learning%2520paradigm%2520for%2520multi-view%2520breast%2520cancer%2520classification%250Abased%2520on%2520parameterized%2520hypercomplex%2520neural%2520networks%2520%2528PHNNs%2529.%2520Thanks%2520to%250Ahypercomplex%2520algebra%252C%2520our%2520models%2520intrinsically%2520capture%2520both%2520intra-%2520and%250Ainter-view%2520relations.%2520We%2520propose%2520PHResNets%2520for%2520two-view%2520exams%2520and%2520two%250Acomplementary%2520four-view%2520architectures%253A%2520PHYBOnet%252C%2520optimized%2520for%2520efficiency%252C%2520and%250APHYSEnet%252C%2520optimized%2520for%2520accuracy.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520consistently%2520outperforms%2520state-of-the-art%2520multi-view%2520models%252C%2520while%250Aalso%2520generalizing%2520across%2520radiographic%2520modalities%2520and%2520tasks%2520such%2520as%2520disease%250Aclassification%2520from%2520chest%2520X-rays%2520and%2520multimodal%2520brain%2520tumor%2520segmentation.%2520Full%250Acode%2520and%2520pretrained%2520models%2520are%2520available%2520at%2520https%253A//github.com/ispamm/PHBreast.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.05798v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Hypercomplex%20Learning%20for%20Breast%20Cancer%20Screening&entry.906535625=Eleonora%20Lopez%20and%20Eleonora%20Grassucci%20and%20Danilo%20Comminiello&entry.1292438233=%20%20Radiologists%20interpret%20mammography%20exams%20by%20jointly%20analyzing%20all%20four%20views%2C%0Aas%20correlations%20among%20them%20are%20crucial%20for%20accurate%20diagnosis.%20Recent%20methods%0Aemploy%20dedicated%20fusion%20blocks%20to%20capture%20such%20dependencies%2C%20but%20these%20are%0Aoften%20hindered%20by%20view%20dominance%2C%20training%20instability%2C%20and%20computational%0Aoverhead.%20To%20address%20these%20challenges%2C%20we%20introduce%20multi-view%20hypercomplex%0Alearning%2C%20a%20novel%20learning%20paradigm%20for%20multi-view%20breast%20cancer%20classification%0Abased%20on%20parameterized%20hypercomplex%20neural%20networks%20%28PHNNs%29.%20Thanks%20to%0Ahypercomplex%20algebra%2C%20our%20models%20intrinsically%20capture%20both%20intra-%20and%0Ainter-view%20relations.%20We%20propose%20PHResNets%20for%20two-view%20exams%20and%20two%0Acomplementary%20four-view%20architectures%3A%20PHYBOnet%2C%20optimized%20for%20efficiency%2C%20and%0APHYSEnet%2C%20optimized%20for%20accuracy.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20consistently%20outperforms%20state-of-the-art%20multi-view%20models%2C%20while%0Aalso%20generalizing%20across%20radiographic%20modalities%20and%20tasks%20such%20as%20disease%0Aclassification%20from%20chest%20X-rays%20and%20multimodal%20brain%20tumor%20segmentation.%20Full%0Acode%20and%20pretrained%20models%20are%20available%20at%20https%3A//github.com/ispamm/PHBreast.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.05798v4&entry.124074799=Read"},
{"title": "Generative Logic: A New Computer Architecture for Deterministic\n  Reasoning and Knowledge Generation", "author": "Nikolai Sergeev", "abstract": "  We present Generative Logic (GL), a deterministic architecture that starts\nfrom user-supplied axiomatic definitions (and, optionally, a list of simple\nfacts for counterexample (CE) construction), written in a minimalist\nMathematical Programming Language (MPL), and systematically explores their\ndeductive neighborhood. Definitions are compiled into a distributed grid of\nsimple Logic Blocks (LBs) that exchange messages; whenever the premises of an\ninference rule unify, a new fact is emitted with full provenance to its\nsources, yielding replayable, auditable proof graphs. A prototype software\nimplementation instantiates the workflow on first-order Peano arithmetic.\nStarting only from the Peano axioms, GL enumerates conjectures, applies\nnormalization, type, and CE filter, and automatically reconstructs\nmachine-checkable proofs of foundational arithmetic laws, including\nassociativity and commutativity of addition, associativity and commutativity of\nmultiplication, and distributivity. On commodity hardware, the prover phase\nrequires approximately 7 seconds; a complete run finishes in about 5 minutes.\nGenerated proofs export to navigable HTML so that every inference step can be\ninspected independently. We outline a hardware-software co-design path toward\nmassively parallel realizations and describe prospective integration with\nprobabilistic models (e.g., large language models) for auto-formalization and\nconjecture seeding. The Python, C++, and MPL code to reproduce the Peano\nexperiments, along with the full proof graphs in HTML as well as\nmachine-readable text format, are available in the project's GitHub repository\nat github.com/Generative-Logic/GL commit 56c9233 and are permanently archived\nat doi:10.5281/zenodo.17206386.\n", "link": "http://arxiv.org/abs/2508.00017v2", "date": "2025-09-26", "relevancy": 2.5792, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5298}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5249}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Logic%3A%20A%20New%20Computer%20Architecture%20for%20Deterministic%0A%20%20Reasoning%20and%20Knowledge%20Generation&body=Title%3A%20Generative%20Logic%3A%20A%20New%20Computer%20Architecture%20for%20Deterministic%0A%20%20Reasoning%20and%20Knowledge%20Generation%0AAuthor%3A%20Nikolai%20Sergeev%0AAbstract%3A%20%20%20We%20present%20Generative%20Logic%20%28GL%29%2C%20a%20deterministic%20architecture%20that%20starts%0Afrom%20user-supplied%20axiomatic%20definitions%20%28and%2C%20optionally%2C%20a%20list%20of%20simple%0Afacts%20for%20counterexample%20%28CE%29%20construction%29%2C%20written%20in%20a%20minimalist%0AMathematical%20Programming%20Language%20%28MPL%29%2C%20and%20systematically%20explores%20their%0Adeductive%20neighborhood.%20Definitions%20are%20compiled%20into%20a%20distributed%20grid%20of%0Asimple%20Logic%20Blocks%20%28LBs%29%20that%20exchange%20messages%3B%20whenever%20the%20premises%20of%20an%0Ainference%20rule%20unify%2C%20a%20new%20fact%20is%20emitted%20with%20full%20provenance%20to%20its%0Asources%2C%20yielding%20replayable%2C%20auditable%20proof%20graphs.%20A%20prototype%20software%0Aimplementation%20instantiates%20the%20workflow%20on%20first-order%20Peano%20arithmetic.%0AStarting%20only%20from%20the%20Peano%20axioms%2C%20GL%20enumerates%20conjectures%2C%20applies%0Anormalization%2C%20type%2C%20and%20CE%20filter%2C%20and%20automatically%20reconstructs%0Amachine-checkable%20proofs%20of%20foundational%20arithmetic%20laws%2C%20including%0Aassociativity%20and%20commutativity%20of%20addition%2C%20associativity%20and%20commutativity%20of%0Amultiplication%2C%20and%20distributivity.%20On%20commodity%20hardware%2C%20the%20prover%20phase%0Arequires%20approximately%207%20seconds%3B%20a%20complete%20run%20finishes%20in%20about%205%20minutes.%0AGenerated%20proofs%20export%20to%20navigable%20HTML%20so%20that%20every%20inference%20step%20can%20be%0Ainspected%20independently.%20We%20outline%20a%20hardware-software%20co-design%20path%20toward%0Amassively%20parallel%20realizations%20and%20describe%20prospective%20integration%20with%0Aprobabilistic%20models%20%28e.g.%2C%20large%20language%20models%29%20for%20auto-formalization%20and%0Aconjecture%20seeding.%20The%20Python%2C%20C%2B%2B%2C%20and%20MPL%20code%20to%20reproduce%20the%20Peano%0Aexperiments%2C%20along%20with%20the%20full%20proof%20graphs%20in%20HTML%20as%20well%20as%0Amachine-readable%20text%20format%2C%20are%20available%20in%20the%20project%27s%20GitHub%20repository%0Aat%20github.com/Generative-Logic/GL%20commit%2056c9233%20and%20are%20permanently%20archived%0Aat%20doi%3A10.5281/zenodo.17206386.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00017v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Logic%253A%2520A%2520New%2520Computer%2520Architecture%2520for%2520Deterministic%250A%2520%2520Reasoning%2520and%2520Knowledge%2520Generation%26entry.906535625%3DNikolai%2520Sergeev%26entry.1292438233%3D%2520%2520We%2520present%2520Generative%2520Logic%2520%2528GL%2529%252C%2520a%2520deterministic%2520architecture%2520that%2520starts%250Afrom%2520user-supplied%2520axiomatic%2520definitions%2520%2528and%252C%2520optionally%252C%2520a%2520list%2520of%2520simple%250Afacts%2520for%2520counterexample%2520%2528CE%2529%2520construction%2529%252C%2520written%2520in%2520a%2520minimalist%250AMathematical%2520Programming%2520Language%2520%2528MPL%2529%252C%2520and%2520systematically%2520explores%2520their%250Adeductive%2520neighborhood.%2520Definitions%2520are%2520compiled%2520into%2520a%2520distributed%2520grid%2520of%250Asimple%2520Logic%2520Blocks%2520%2528LBs%2529%2520that%2520exchange%2520messages%253B%2520whenever%2520the%2520premises%2520of%2520an%250Ainference%2520rule%2520unify%252C%2520a%2520new%2520fact%2520is%2520emitted%2520with%2520full%2520provenance%2520to%2520its%250Asources%252C%2520yielding%2520replayable%252C%2520auditable%2520proof%2520graphs.%2520A%2520prototype%2520software%250Aimplementation%2520instantiates%2520the%2520workflow%2520on%2520first-order%2520Peano%2520arithmetic.%250AStarting%2520only%2520from%2520the%2520Peano%2520axioms%252C%2520GL%2520enumerates%2520conjectures%252C%2520applies%250Anormalization%252C%2520type%252C%2520and%2520CE%2520filter%252C%2520and%2520automatically%2520reconstructs%250Amachine-checkable%2520proofs%2520of%2520foundational%2520arithmetic%2520laws%252C%2520including%250Aassociativity%2520and%2520commutativity%2520of%2520addition%252C%2520associativity%2520and%2520commutativity%2520of%250Amultiplication%252C%2520and%2520distributivity.%2520On%2520commodity%2520hardware%252C%2520the%2520prover%2520phase%250Arequires%2520approximately%25207%2520seconds%253B%2520a%2520complete%2520run%2520finishes%2520in%2520about%25205%2520minutes.%250AGenerated%2520proofs%2520export%2520to%2520navigable%2520HTML%2520so%2520that%2520every%2520inference%2520step%2520can%2520be%250Ainspected%2520independently.%2520We%2520outline%2520a%2520hardware-software%2520co-design%2520path%2520toward%250Amassively%2520parallel%2520realizations%2520and%2520describe%2520prospective%2520integration%2520with%250Aprobabilistic%2520models%2520%2528e.g.%252C%2520large%2520language%2520models%2529%2520for%2520auto-formalization%2520and%250Aconjecture%2520seeding.%2520The%2520Python%252C%2520C%252B%252B%252C%2520and%2520MPL%2520code%2520to%2520reproduce%2520the%2520Peano%250Aexperiments%252C%2520along%2520with%2520the%2520full%2520proof%2520graphs%2520in%2520HTML%2520as%2520well%2520as%250Amachine-readable%2520text%2520format%252C%2520are%2520available%2520in%2520the%2520project%2527s%2520GitHub%2520repository%250Aat%2520github.com/Generative-Logic/GL%2520commit%252056c9233%2520and%2520are%2520permanently%2520archived%250Aat%2520doi%253A10.5281/zenodo.17206386.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00017v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Logic%3A%20A%20New%20Computer%20Architecture%20for%20Deterministic%0A%20%20Reasoning%20and%20Knowledge%20Generation&entry.906535625=Nikolai%20Sergeev&entry.1292438233=%20%20We%20present%20Generative%20Logic%20%28GL%29%2C%20a%20deterministic%20architecture%20that%20starts%0Afrom%20user-supplied%20axiomatic%20definitions%20%28and%2C%20optionally%2C%20a%20list%20of%20simple%0Afacts%20for%20counterexample%20%28CE%29%20construction%29%2C%20written%20in%20a%20minimalist%0AMathematical%20Programming%20Language%20%28MPL%29%2C%20and%20systematically%20explores%20their%0Adeductive%20neighborhood.%20Definitions%20are%20compiled%20into%20a%20distributed%20grid%20of%0Asimple%20Logic%20Blocks%20%28LBs%29%20that%20exchange%20messages%3B%20whenever%20the%20premises%20of%20an%0Ainference%20rule%20unify%2C%20a%20new%20fact%20is%20emitted%20with%20full%20provenance%20to%20its%0Asources%2C%20yielding%20replayable%2C%20auditable%20proof%20graphs.%20A%20prototype%20software%0Aimplementation%20instantiates%20the%20workflow%20on%20first-order%20Peano%20arithmetic.%0AStarting%20only%20from%20the%20Peano%20axioms%2C%20GL%20enumerates%20conjectures%2C%20applies%0Anormalization%2C%20type%2C%20and%20CE%20filter%2C%20and%20automatically%20reconstructs%0Amachine-checkable%20proofs%20of%20foundational%20arithmetic%20laws%2C%20including%0Aassociativity%20and%20commutativity%20of%20addition%2C%20associativity%20and%20commutativity%20of%0Amultiplication%2C%20and%20distributivity.%20On%20commodity%20hardware%2C%20the%20prover%20phase%0Arequires%20approximately%207%20seconds%3B%20a%20complete%20run%20finishes%20in%20about%205%20minutes.%0AGenerated%20proofs%20export%20to%20navigable%20HTML%20so%20that%20every%20inference%20step%20can%20be%0Ainspected%20independently.%20We%20outline%20a%20hardware-software%20co-design%20path%20toward%0Amassively%20parallel%20realizations%20and%20describe%20prospective%20integration%20with%0Aprobabilistic%20models%20%28e.g.%2C%20large%20language%20models%29%20for%20auto-formalization%20and%0Aconjecture%20seeding.%20The%20Python%2C%20C%2B%2B%2C%20and%20MPL%20code%20to%20reproduce%20the%20Peano%0Aexperiments%2C%20along%20with%20the%20full%20proof%20graphs%20in%20HTML%20as%20well%20as%0Amachine-readable%20text%20format%2C%20are%20available%20in%20the%20project%27s%20GitHub%20repository%0Aat%20github.com/Generative-Logic/GL%20commit%2056c9233%20and%20are%20permanently%20archived%0Aat%20doi%3A10.5281/zenodo.17206386.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00017v2&entry.124074799=Read"},
{"title": "Question-Driven Analysis and Synthesis: Building Interpretable Thematic\n  Trees with LLMs for Text Clustering and Controllable Generation", "author": "Tiago Fernandes Tavares", "abstract": "  Unsupervised analysis of text corpora is challenging, especially in\ndata-scarce domains where traditional topic models struggle. While these models\noffer a solution, they typically describe clusters with lists of keywords that\nrequire significant manual effort to interpret and often lack semantic\ncoherence. To address this critical interpretability gap, we introduce\nRecursive Thematic Partitioning (RTP), a novel framework that leverages Large\nLanguage Models (LLMs) to interactively build a binary tree. Each node in the\ntree is a natural language question that semantically partitions the data,\nresulting in a fully interpretable taxonomy where the logic of each cluster is\nexplicit. Our experiments demonstrate that RTP's question-driven hierarchy is\nmore interpretable than the keyword-based topics from a strong baseline like\nBERTopic. Furthermore, we establish the quantitative utility of these clusters\nby showing they serve as powerful features in downstream classification tasks,\nparticularly when the data's underlying themes correlate with the task labels.\nRTP introduces a new paradigm for data exploration, shifting the focus from\nstatistical pattern discovery to knowledge-driven thematic analysis.\nFurthermore, we demonstrate that the thematic paths from the RTP tree can serve\nas structured, controllable prompts for generative models. This transforms our\nanalytical framework into a powerful tool for synthesis, enabling the\nconsistent imitation of specific characteristics discovered in the source\ncorpus.\n", "link": "http://arxiv.org/abs/2509.22211v1", "date": "2025-09-26", "relevancy": 2.5739, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Question-Driven%20Analysis%20and%20Synthesis%3A%20Building%20Interpretable%20Thematic%0A%20%20Trees%20with%20LLMs%20for%20Text%20Clustering%20and%20Controllable%20Generation&body=Title%3A%20Question-Driven%20Analysis%20and%20Synthesis%3A%20Building%20Interpretable%20Thematic%0A%20%20Trees%20with%20LLMs%20for%20Text%20Clustering%20and%20Controllable%20Generation%0AAuthor%3A%20Tiago%20Fernandes%20Tavares%0AAbstract%3A%20%20%20Unsupervised%20analysis%20of%20text%20corpora%20is%20challenging%2C%20especially%20in%0Adata-scarce%20domains%20where%20traditional%20topic%20models%20struggle.%20While%20these%20models%0Aoffer%20a%20solution%2C%20they%20typically%20describe%20clusters%20with%20lists%20of%20keywords%20that%0Arequire%20significant%20manual%20effort%20to%20interpret%20and%20often%20lack%20semantic%0Acoherence.%20To%20address%20this%20critical%20interpretability%20gap%2C%20we%20introduce%0ARecursive%20Thematic%20Partitioning%20%28RTP%29%2C%20a%20novel%20framework%20that%20leverages%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20interactively%20build%20a%20binary%20tree.%20Each%20node%20in%20the%0Atree%20is%20a%20natural%20language%20question%20that%20semantically%20partitions%20the%20data%2C%0Aresulting%20in%20a%20fully%20interpretable%20taxonomy%20where%20the%20logic%20of%20each%20cluster%20is%0Aexplicit.%20Our%20experiments%20demonstrate%20that%20RTP%27s%20question-driven%20hierarchy%20is%0Amore%20interpretable%20than%20the%20keyword-based%20topics%20from%20a%20strong%20baseline%20like%0ABERTopic.%20Furthermore%2C%20we%20establish%20the%20quantitative%20utility%20of%20these%20clusters%0Aby%20showing%20they%20serve%20as%20powerful%20features%20in%20downstream%20classification%20tasks%2C%0Aparticularly%20when%20the%20data%27s%20underlying%20themes%20correlate%20with%20the%20task%20labels.%0ARTP%20introduces%20a%20new%20paradigm%20for%20data%20exploration%2C%20shifting%20the%20focus%20from%0Astatistical%20pattern%20discovery%20to%20knowledge-driven%20thematic%20analysis.%0AFurthermore%2C%20we%20demonstrate%20that%20the%20thematic%20paths%20from%20the%20RTP%20tree%20can%20serve%0Aas%20structured%2C%20controllable%20prompts%20for%20generative%20models.%20This%20transforms%20our%0Aanalytical%20framework%20into%20a%20powerful%20tool%20for%20synthesis%2C%20enabling%20the%0Aconsistent%20imitation%20of%20specific%20characteristics%20discovered%20in%20the%20source%0Acorpus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestion-Driven%2520Analysis%2520and%2520Synthesis%253A%2520Building%2520Interpretable%2520Thematic%250A%2520%2520Trees%2520with%2520LLMs%2520for%2520Text%2520Clustering%2520and%2520Controllable%2520Generation%26entry.906535625%3DTiago%2520Fernandes%2520Tavares%26entry.1292438233%3D%2520%2520Unsupervised%2520analysis%2520of%2520text%2520corpora%2520is%2520challenging%252C%2520especially%2520in%250Adata-scarce%2520domains%2520where%2520traditional%2520topic%2520models%2520struggle.%2520While%2520these%2520models%250Aoffer%2520a%2520solution%252C%2520they%2520typically%2520describe%2520clusters%2520with%2520lists%2520of%2520keywords%2520that%250Arequire%2520significant%2520manual%2520effort%2520to%2520interpret%2520and%2520often%2520lack%2520semantic%250Acoherence.%2520To%2520address%2520this%2520critical%2520interpretability%2520gap%252C%2520we%2520introduce%250ARecursive%2520Thematic%2520Partitioning%2520%2528RTP%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520interactively%2520build%2520a%2520binary%2520tree.%2520Each%2520node%2520in%2520the%250Atree%2520is%2520a%2520natural%2520language%2520question%2520that%2520semantically%2520partitions%2520the%2520data%252C%250Aresulting%2520in%2520a%2520fully%2520interpretable%2520taxonomy%2520where%2520the%2520logic%2520of%2520each%2520cluster%2520is%250Aexplicit.%2520Our%2520experiments%2520demonstrate%2520that%2520RTP%2527s%2520question-driven%2520hierarchy%2520is%250Amore%2520interpretable%2520than%2520the%2520keyword-based%2520topics%2520from%2520a%2520strong%2520baseline%2520like%250ABERTopic.%2520Furthermore%252C%2520we%2520establish%2520the%2520quantitative%2520utility%2520of%2520these%2520clusters%250Aby%2520showing%2520they%2520serve%2520as%2520powerful%2520features%2520in%2520downstream%2520classification%2520tasks%252C%250Aparticularly%2520when%2520the%2520data%2527s%2520underlying%2520themes%2520correlate%2520with%2520the%2520task%2520labels.%250ARTP%2520introduces%2520a%2520new%2520paradigm%2520for%2520data%2520exploration%252C%2520shifting%2520the%2520focus%2520from%250Astatistical%2520pattern%2520discovery%2520to%2520knowledge-driven%2520thematic%2520analysis.%250AFurthermore%252C%2520we%2520demonstrate%2520that%2520the%2520thematic%2520paths%2520from%2520the%2520RTP%2520tree%2520can%2520serve%250Aas%2520structured%252C%2520controllable%2520prompts%2520for%2520generative%2520models.%2520This%2520transforms%2520our%250Aanalytical%2520framework%2520into%2520a%2520powerful%2520tool%2520for%2520synthesis%252C%2520enabling%2520the%250Aconsistent%2520imitation%2520of%2520specific%2520characteristics%2520discovered%2520in%2520the%2520source%250Acorpus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Question-Driven%20Analysis%20and%20Synthesis%3A%20Building%20Interpretable%20Thematic%0A%20%20Trees%20with%20LLMs%20for%20Text%20Clustering%20and%20Controllable%20Generation&entry.906535625=Tiago%20Fernandes%20Tavares&entry.1292438233=%20%20Unsupervised%20analysis%20of%20text%20corpora%20is%20challenging%2C%20especially%20in%0Adata-scarce%20domains%20where%20traditional%20topic%20models%20struggle.%20While%20these%20models%0Aoffer%20a%20solution%2C%20they%20typically%20describe%20clusters%20with%20lists%20of%20keywords%20that%0Arequire%20significant%20manual%20effort%20to%20interpret%20and%20often%20lack%20semantic%0Acoherence.%20To%20address%20this%20critical%20interpretability%20gap%2C%20we%20introduce%0ARecursive%20Thematic%20Partitioning%20%28RTP%29%2C%20a%20novel%20framework%20that%20leverages%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20interactively%20build%20a%20binary%20tree.%20Each%20node%20in%20the%0Atree%20is%20a%20natural%20language%20question%20that%20semantically%20partitions%20the%20data%2C%0Aresulting%20in%20a%20fully%20interpretable%20taxonomy%20where%20the%20logic%20of%20each%20cluster%20is%0Aexplicit.%20Our%20experiments%20demonstrate%20that%20RTP%27s%20question-driven%20hierarchy%20is%0Amore%20interpretable%20than%20the%20keyword-based%20topics%20from%20a%20strong%20baseline%20like%0ABERTopic.%20Furthermore%2C%20we%20establish%20the%20quantitative%20utility%20of%20these%20clusters%0Aby%20showing%20they%20serve%20as%20powerful%20features%20in%20downstream%20classification%20tasks%2C%0Aparticularly%20when%20the%20data%27s%20underlying%20themes%20correlate%20with%20the%20task%20labels.%0ARTP%20introduces%20a%20new%20paradigm%20for%20data%20exploration%2C%20shifting%20the%20focus%20from%0Astatistical%20pattern%20discovery%20to%20knowledge-driven%20thematic%20analysis.%0AFurthermore%2C%20we%20demonstrate%20that%20the%20thematic%20paths%20from%20the%20RTP%20tree%20can%20serve%0Aas%20structured%2C%20controllable%20prompts%20for%20generative%20models.%20This%20transforms%20our%0Aanalytical%20framework%20into%20a%20powerful%20tool%20for%20synthesis%2C%20enabling%20the%0Aconsistent%20imitation%20of%20specific%20characteristics%20discovered%20in%20the%20source%0Acorpus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22211v1&entry.124074799=Read"},
{"title": "SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image\n  Fusion", "author": "Zixian Zhao and Xingchen Zhang", "abstract": "  Visible and infrared image fusion (VIF) has gained significant attention in\nrecent years due to its wide application in tasks such as scene segmentation\nand object detection. VIF methods can be broadly classified into traditional\nVIF methods and application-oriented VIF methods. Traditional methods focus\nsolely on improving the quality of fused images, while application-oriented VIF\nmethods additionally consider the performance of downstream tasks on fused\nimages by introducing task-specific loss terms during training. However,\ncompared to traditional methods, application-oriented VIF methods require\ndatasets labeled for downstream tasks (e.g., semantic segmentation or object\ndetection), making data acquisition labor-intensive and time-consuming. To\naddress this issue, we propose a self-supervised training framework for\nsegmentation-oriented VIF methods (SSVIF). Leveraging the consistency between\nfeature-level fusion-based segmentation and pixel-level fusion-based\nsegmentation, we introduce a novel self-supervised task-cross-segmentation\nconsistency-that enables the fusion model to learn high-level semantic features\nwithout the supervision of segmentation labels. Additionally, we design a\ntwo-stage training strategy and a dynamic weight adjustment method for\neffective joint learning within our self-supervised framework. Extensive\nexperiments on public datasets demonstrate the effectiveness of our proposed\nSSVIF. Remarkably, although trained only on unlabeled visible-infrared image\npairs, our SSVIF outperforms traditional VIF methods and rivals supervised\nsegmentation-oriented ones. Our code will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2509.22450v1", "date": "2025-09-26", "relevancy": 2.56, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5159}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5134}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSVIF%3A%20Self-Supervised%20Segmentation-Oriented%20Visible%20and%20Infrared%20Image%0A%20%20Fusion&body=Title%3A%20SSVIF%3A%20Self-Supervised%20Segmentation-Oriented%20Visible%20and%20Infrared%20Image%0A%20%20Fusion%0AAuthor%3A%20Zixian%20Zhao%20and%20Xingchen%20Zhang%0AAbstract%3A%20%20%20Visible%20and%20infrared%20image%20fusion%20%28VIF%29%20has%20gained%20significant%20attention%20in%0Arecent%20years%20due%20to%20its%20wide%20application%20in%20tasks%20such%20as%20scene%20segmentation%0Aand%20object%20detection.%20VIF%20methods%20can%20be%20broadly%20classified%20into%20traditional%0AVIF%20methods%20and%20application-oriented%20VIF%20methods.%20Traditional%20methods%20focus%0Asolely%20on%20improving%20the%20quality%20of%20fused%20images%2C%20while%20application-oriented%20VIF%0Amethods%20additionally%20consider%20the%20performance%20of%20downstream%20tasks%20on%20fused%0Aimages%20by%20introducing%20task-specific%20loss%20terms%20during%20training.%20However%2C%0Acompared%20to%20traditional%20methods%2C%20application-oriented%20VIF%20methods%20require%0Adatasets%20labeled%20for%20downstream%20tasks%20%28e.g.%2C%20semantic%20segmentation%20or%20object%0Adetection%29%2C%20making%20data%20acquisition%20labor-intensive%20and%20time-consuming.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20self-supervised%20training%20framework%20for%0Asegmentation-oriented%20VIF%20methods%20%28SSVIF%29.%20Leveraging%20the%20consistency%20between%0Afeature-level%20fusion-based%20segmentation%20and%20pixel-level%20fusion-based%0Asegmentation%2C%20we%20introduce%20a%20novel%20self-supervised%20task-cross-segmentation%0Aconsistency-that%20enables%20the%20fusion%20model%20to%20learn%20high-level%20semantic%20features%0Awithout%20the%20supervision%20of%20segmentation%20labels.%20Additionally%2C%20we%20design%20a%0Atwo-stage%20training%20strategy%20and%20a%20dynamic%20weight%20adjustment%20method%20for%0Aeffective%20joint%20learning%20within%20our%20self-supervised%20framework.%20Extensive%0Aexperiments%20on%20public%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0ASSVIF.%20Remarkably%2C%20although%20trained%20only%20on%20unlabeled%20visible-infrared%20image%0Apairs%2C%20our%20SSVIF%20outperforms%20traditional%20VIF%20methods%20and%20rivals%20supervised%0Asegmentation-oriented%20ones.%20Our%20code%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSVIF%253A%2520Self-Supervised%2520Segmentation-Oriented%2520Visible%2520and%2520Infrared%2520Image%250A%2520%2520Fusion%26entry.906535625%3DZixian%2520Zhao%2520and%2520Xingchen%2520Zhang%26entry.1292438233%3D%2520%2520Visible%2520and%2520infrared%2520image%2520fusion%2520%2528VIF%2529%2520has%2520gained%2520significant%2520attention%2520in%250Arecent%2520years%2520due%2520to%2520its%2520wide%2520application%2520in%2520tasks%2520such%2520as%2520scene%2520segmentation%250Aand%2520object%2520detection.%2520VIF%2520methods%2520can%2520be%2520broadly%2520classified%2520into%2520traditional%250AVIF%2520methods%2520and%2520application-oriented%2520VIF%2520methods.%2520Traditional%2520methods%2520focus%250Asolely%2520on%2520improving%2520the%2520quality%2520of%2520fused%2520images%252C%2520while%2520application-oriented%2520VIF%250Amethods%2520additionally%2520consider%2520the%2520performance%2520of%2520downstream%2520tasks%2520on%2520fused%250Aimages%2520by%2520introducing%2520task-specific%2520loss%2520terms%2520during%2520training.%2520However%252C%250Acompared%2520to%2520traditional%2520methods%252C%2520application-oriented%2520VIF%2520methods%2520require%250Adatasets%2520labeled%2520for%2520downstream%2520tasks%2520%2528e.g.%252C%2520semantic%2520segmentation%2520or%2520object%250Adetection%2529%252C%2520making%2520data%2520acquisition%2520labor-intensive%2520and%2520time-consuming.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520self-supervised%2520training%2520framework%2520for%250Asegmentation-oriented%2520VIF%2520methods%2520%2528SSVIF%2529.%2520Leveraging%2520the%2520consistency%2520between%250Afeature-level%2520fusion-based%2520segmentation%2520and%2520pixel-level%2520fusion-based%250Asegmentation%252C%2520we%2520introduce%2520a%2520novel%2520self-supervised%2520task-cross-segmentation%250Aconsistency-that%2520enables%2520the%2520fusion%2520model%2520to%2520learn%2520high-level%2520semantic%2520features%250Awithout%2520the%2520supervision%2520of%2520segmentation%2520labels.%2520Additionally%252C%2520we%2520design%2520a%250Atwo-stage%2520training%2520strategy%2520and%2520a%2520dynamic%2520weight%2520adjustment%2520method%2520for%250Aeffective%2520joint%2520learning%2520within%2520our%2520self-supervised%2520framework.%2520Extensive%250Aexperiments%2520on%2520public%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250ASSVIF.%2520Remarkably%252C%2520although%2520trained%2520only%2520on%2520unlabeled%2520visible-infrared%2520image%250Apairs%252C%2520our%2520SSVIF%2520outperforms%2520traditional%2520VIF%2520methods%2520and%2520rivals%2520supervised%250Asegmentation-oriented%2520ones.%2520Our%2520code%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSVIF%3A%20Self-Supervised%20Segmentation-Oriented%20Visible%20and%20Infrared%20Image%0A%20%20Fusion&entry.906535625=Zixian%20Zhao%20and%20Xingchen%20Zhang&entry.1292438233=%20%20Visible%20and%20infrared%20image%20fusion%20%28VIF%29%20has%20gained%20significant%20attention%20in%0Arecent%20years%20due%20to%20its%20wide%20application%20in%20tasks%20such%20as%20scene%20segmentation%0Aand%20object%20detection.%20VIF%20methods%20can%20be%20broadly%20classified%20into%20traditional%0AVIF%20methods%20and%20application-oriented%20VIF%20methods.%20Traditional%20methods%20focus%0Asolely%20on%20improving%20the%20quality%20of%20fused%20images%2C%20while%20application-oriented%20VIF%0Amethods%20additionally%20consider%20the%20performance%20of%20downstream%20tasks%20on%20fused%0Aimages%20by%20introducing%20task-specific%20loss%20terms%20during%20training.%20However%2C%0Acompared%20to%20traditional%20methods%2C%20application-oriented%20VIF%20methods%20require%0Adatasets%20labeled%20for%20downstream%20tasks%20%28e.g.%2C%20semantic%20segmentation%20or%20object%0Adetection%29%2C%20making%20data%20acquisition%20labor-intensive%20and%20time-consuming.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20self-supervised%20training%20framework%20for%0Asegmentation-oriented%20VIF%20methods%20%28SSVIF%29.%20Leveraging%20the%20consistency%20between%0Afeature-level%20fusion-based%20segmentation%20and%20pixel-level%20fusion-based%0Asegmentation%2C%20we%20introduce%20a%20novel%20self-supervised%20task-cross-segmentation%0Aconsistency-that%20enables%20the%20fusion%20model%20to%20learn%20high-level%20semantic%20features%0Awithout%20the%20supervision%20of%20segmentation%20labels.%20Additionally%2C%20we%20design%20a%0Atwo-stage%20training%20strategy%20and%20a%20dynamic%20weight%20adjustment%20method%20for%0Aeffective%20joint%20learning%20within%20our%20self-supervised%20framework.%20Extensive%0Aexperiments%20on%20public%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0ASSVIF.%20Remarkably%2C%20although%20trained%20only%20on%20unlabeled%20visible-infrared%20image%0Apairs%2C%20our%20SSVIF%20outperforms%20traditional%20VIF%20methods%20and%20rivals%20supervised%0Asegmentation-oriented%20ones.%20Our%20code%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22450v1&entry.124074799=Read"},
{"title": "Pre-Training Representations of Binary Code Using Contrastive Learning", "author": "Yifan Zhang and Chen Huang and Yueke Zhang and Huajie Shao and Kevin Leach and Yu Huang", "abstract": "  Binary code analysis and comprehension is critical to applications in reverse\nengineering and computer security tasks where source code is not available.\nUnfortunately, unlike source code, binary code lacks semantics and is more\ndifficult for human engineers to understand and analyze. In this paper, we\npresent ContraBin, a contrastive learning technique that integrates source code\nand comment information along with binaries to create an embedding capable of\naiding binary analysis and comprehension tasks. Specifically, we present three\ncomponents in ContraBin: (1) a primary contrastive learning method for initial\npre-training, (2) a simplex interpolation method to integrate source code,\ncomments, and binary code, and (3) an intermediate representation learning\nalgorithm to train a binary code embedding. We further analyze the impact of\nhuman-written and synthetic comments on binary code comprehension tasks,\nrevealing a significant performance disparity. While synthetic comments provide\nsubstantial benefits, human-written comments are found to introduce noise, even\nresulting in performance drops compared to using no comments. These findings\nreshape the narrative around the role of comment types in binary code analysis.\nWe evaluate the effectiveness of ContraBin through four indicative downstream\ntasks related to binary code: algorithmic functionality classification,\nfunction name recovery, code summarization, and reverse engineering. The\nresults show that ContraBin considerably improves performance on all four\ntasks, measured by accuracy, mean of average precision, and BLEU scores as\nappropriate. ContraBin is the first language representation model to\nincorporate source code, binary code, and comments into contrastive code\nrepresentation learning and is intended to contribute to the field of binary\ncode analysis. The dataset used in this study is available for further\nresearch.\n", "link": "http://arxiv.org/abs/2210.05102v5", "date": "2025-09-26", "relevancy": 2.5503, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5178}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Training%20Representations%20of%20Binary%20Code%20Using%20Contrastive%20Learning&body=Title%3A%20Pre-Training%20Representations%20of%20Binary%20Code%20Using%20Contrastive%20Learning%0AAuthor%3A%20Yifan%20Zhang%20and%20Chen%20Huang%20and%20Yueke%20Zhang%20and%20Huajie%20Shao%20and%20Kevin%20Leach%20and%20Yu%20Huang%0AAbstract%3A%20%20%20Binary%20code%20analysis%20and%20comprehension%20is%20critical%20to%20applications%20in%20reverse%0Aengineering%20and%20computer%20security%20tasks%20where%20source%20code%20is%20not%20available.%0AUnfortunately%2C%20unlike%20source%20code%2C%20binary%20code%20lacks%20semantics%20and%20is%20more%0Adifficult%20for%20human%20engineers%20to%20understand%20and%20analyze.%20In%20this%20paper%2C%20we%0Apresent%20ContraBin%2C%20a%20contrastive%20learning%20technique%20that%20integrates%20source%20code%0Aand%20comment%20information%20along%20with%20binaries%20to%20create%20an%20embedding%20capable%20of%0Aaiding%20binary%20analysis%20and%20comprehension%20tasks.%20Specifically%2C%20we%20present%20three%0Acomponents%20in%20ContraBin%3A%20%281%29%20a%20primary%20contrastive%20learning%20method%20for%20initial%0Apre-training%2C%20%282%29%20a%20simplex%20interpolation%20method%20to%20integrate%20source%20code%2C%0Acomments%2C%20and%20binary%20code%2C%20and%20%283%29%20an%20intermediate%20representation%20learning%0Aalgorithm%20to%20train%20a%20binary%20code%20embedding.%20We%20further%20analyze%20the%20impact%20of%0Ahuman-written%20and%20synthetic%20comments%20on%20binary%20code%20comprehension%20tasks%2C%0Arevealing%20a%20significant%20performance%20disparity.%20While%20synthetic%20comments%20provide%0Asubstantial%20benefits%2C%20human-written%20comments%20are%20found%20to%20introduce%20noise%2C%20even%0Aresulting%20in%20performance%20drops%20compared%20to%20using%20no%20comments.%20These%20findings%0Areshape%20the%20narrative%20around%20the%20role%20of%20comment%20types%20in%20binary%20code%20analysis.%0AWe%20evaluate%20the%20effectiveness%20of%20ContraBin%20through%20four%20indicative%20downstream%0Atasks%20related%20to%20binary%20code%3A%20algorithmic%20functionality%20classification%2C%0Afunction%20name%20recovery%2C%20code%20summarization%2C%20and%20reverse%20engineering.%20The%0Aresults%20show%20that%20ContraBin%20considerably%20improves%20performance%20on%20all%20four%0Atasks%2C%20measured%20by%20accuracy%2C%20mean%20of%20average%20precision%2C%20and%20BLEU%20scores%20as%0Aappropriate.%20ContraBin%20is%20the%20first%20language%20representation%20model%20to%0Aincorporate%20source%20code%2C%20binary%20code%2C%20and%20comments%20into%20contrastive%20code%0Arepresentation%20learning%20and%20is%20intended%20to%20contribute%20to%20the%20field%20of%20binary%0Acode%20analysis.%20The%20dataset%20used%20in%20this%20study%20is%20available%20for%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.05102v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Training%2520Representations%2520of%2520Binary%2520Code%2520Using%2520Contrastive%2520Learning%26entry.906535625%3DYifan%2520Zhang%2520and%2520Chen%2520Huang%2520and%2520Yueke%2520Zhang%2520and%2520Huajie%2520Shao%2520and%2520Kevin%2520Leach%2520and%2520Yu%2520Huang%26entry.1292438233%3D%2520%2520Binary%2520code%2520analysis%2520and%2520comprehension%2520is%2520critical%2520to%2520applications%2520in%2520reverse%250Aengineering%2520and%2520computer%2520security%2520tasks%2520where%2520source%2520code%2520is%2520not%2520available.%250AUnfortunately%252C%2520unlike%2520source%2520code%252C%2520binary%2520code%2520lacks%2520semantics%2520and%2520is%2520more%250Adifficult%2520for%2520human%2520engineers%2520to%2520understand%2520and%2520analyze.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520ContraBin%252C%2520a%2520contrastive%2520learning%2520technique%2520that%2520integrates%2520source%2520code%250Aand%2520comment%2520information%2520along%2520with%2520binaries%2520to%2520create%2520an%2520embedding%2520capable%2520of%250Aaiding%2520binary%2520analysis%2520and%2520comprehension%2520tasks.%2520Specifically%252C%2520we%2520present%2520three%250Acomponents%2520in%2520ContraBin%253A%2520%25281%2529%2520a%2520primary%2520contrastive%2520learning%2520method%2520for%2520initial%250Apre-training%252C%2520%25282%2529%2520a%2520simplex%2520interpolation%2520method%2520to%2520integrate%2520source%2520code%252C%250Acomments%252C%2520and%2520binary%2520code%252C%2520and%2520%25283%2529%2520an%2520intermediate%2520representation%2520learning%250Aalgorithm%2520to%2520train%2520a%2520binary%2520code%2520embedding.%2520We%2520further%2520analyze%2520the%2520impact%2520of%250Ahuman-written%2520and%2520synthetic%2520comments%2520on%2520binary%2520code%2520comprehension%2520tasks%252C%250Arevealing%2520a%2520significant%2520performance%2520disparity.%2520While%2520synthetic%2520comments%2520provide%250Asubstantial%2520benefits%252C%2520human-written%2520comments%2520are%2520found%2520to%2520introduce%2520noise%252C%2520even%250Aresulting%2520in%2520performance%2520drops%2520compared%2520to%2520using%2520no%2520comments.%2520These%2520findings%250Areshape%2520the%2520narrative%2520around%2520the%2520role%2520of%2520comment%2520types%2520in%2520binary%2520code%2520analysis.%250AWe%2520evaluate%2520the%2520effectiveness%2520of%2520ContraBin%2520through%2520four%2520indicative%2520downstream%250Atasks%2520related%2520to%2520binary%2520code%253A%2520algorithmic%2520functionality%2520classification%252C%250Afunction%2520name%2520recovery%252C%2520code%2520summarization%252C%2520and%2520reverse%2520engineering.%2520The%250Aresults%2520show%2520that%2520ContraBin%2520considerably%2520improves%2520performance%2520on%2520all%2520four%250Atasks%252C%2520measured%2520by%2520accuracy%252C%2520mean%2520of%2520average%2520precision%252C%2520and%2520BLEU%2520scores%2520as%250Aappropriate.%2520ContraBin%2520is%2520the%2520first%2520language%2520representation%2520model%2520to%250Aincorporate%2520source%2520code%252C%2520binary%2520code%252C%2520and%2520comments%2520into%2520contrastive%2520code%250Arepresentation%2520learning%2520and%2520is%2520intended%2520to%2520contribute%2520to%2520the%2520field%2520of%2520binary%250Acode%2520analysis.%2520The%2520dataset%2520used%2520in%2520this%2520study%2520is%2520available%2520for%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.05102v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Training%20Representations%20of%20Binary%20Code%20Using%20Contrastive%20Learning&entry.906535625=Yifan%20Zhang%20and%20Chen%20Huang%20and%20Yueke%20Zhang%20and%20Huajie%20Shao%20and%20Kevin%20Leach%20and%20Yu%20Huang&entry.1292438233=%20%20Binary%20code%20analysis%20and%20comprehension%20is%20critical%20to%20applications%20in%20reverse%0Aengineering%20and%20computer%20security%20tasks%20where%20source%20code%20is%20not%20available.%0AUnfortunately%2C%20unlike%20source%20code%2C%20binary%20code%20lacks%20semantics%20and%20is%20more%0Adifficult%20for%20human%20engineers%20to%20understand%20and%20analyze.%20In%20this%20paper%2C%20we%0Apresent%20ContraBin%2C%20a%20contrastive%20learning%20technique%20that%20integrates%20source%20code%0Aand%20comment%20information%20along%20with%20binaries%20to%20create%20an%20embedding%20capable%20of%0Aaiding%20binary%20analysis%20and%20comprehension%20tasks.%20Specifically%2C%20we%20present%20three%0Acomponents%20in%20ContraBin%3A%20%281%29%20a%20primary%20contrastive%20learning%20method%20for%20initial%0Apre-training%2C%20%282%29%20a%20simplex%20interpolation%20method%20to%20integrate%20source%20code%2C%0Acomments%2C%20and%20binary%20code%2C%20and%20%283%29%20an%20intermediate%20representation%20learning%0Aalgorithm%20to%20train%20a%20binary%20code%20embedding.%20We%20further%20analyze%20the%20impact%20of%0Ahuman-written%20and%20synthetic%20comments%20on%20binary%20code%20comprehension%20tasks%2C%0Arevealing%20a%20significant%20performance%20disparity.%20While%20synthetic%20comments%20provide%0Asubstantial%20benefits%2C%20human-written%20comments%20are%20found%20to%20introduce%20noise%2C%20even%0Aresulting%20in%20performance%20drops%20compared%20to%20using%20no%20comments.%20These%20findings%0Areshape%20the%20narrative%20around%20the%20role%20of%20comment%20types%20in%20binary%20code%20analysis.%0AWe%20evaluate%20the%20effectiveness%20of%20ContraBin%20through%20four%20indicative%20downstream%0Atasks%20related%20to%20binary%20code%3A%20algorithmic%20functionality%20classification%2C%0Afunction%20name%20recovery%2C%20code%20summarization%2C%20and%20reverse%20engineering.%20The%0Aresults%20show%20that%20ContraBin%20considerably%20improves%20performance%20on%20all%20four%0Atasks%2C%20measured%20by%20accuracy%2C%20mean%20of%20average%20precision%2C%20and%20BLEU%20scores%20as%0Aappropriate.%20ContraBin%20is%20the%20first%20language%20representation%20model%20to%0Aincorporate%20source%20code%2C%20binary%20code%2C%20and%20comments%20into%20contrastive%20code%0Arepresentation%20learning%20and%20is%20intended%20to%20contribute%20to%20the%20field%20of%20binary%0Acode%20analysis.%20The%20dataset%20used%20in%20this%20study%20is%20available%20for%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.05102v5&entry.124074799=Read"},
{"title": "Mamba-Driven Topology Fusion for Monocular 3D Human Pose Estimation", "author": "Zenghao Zheng and Lianping Yang and Jinshan Pan and Hegui Zhu", "abstract": "  Transformer-based methods for 3D human pose estimation face significant\ncomputational challenges due to the quadratic growth of self-attention\nmechanism complexity with sequence length. Recently, the Mamba model has\nsubstantially reduced computational overhead and demonstrated outstanding\nperformance in modeling long sequences by leveraging state space model (SSM).\nHowever, the ability of SSM to process sequential data is not suitable for 3D\njoint sequences with topological structures, and the causal convolution\nstructure in Mamba also lacks insight into local joint relationships. To\naddress these issues, we propose the Mamba-Driven Topology Fusion framework in\nthis paper. Specifically, the proposed Bone Aware Module infers the direction\nand length of bone vectors in the spherical coordinate system, providing\neffective topological guidance for the Mamba model in processing joint\nsequences. Furthermore, we enhance the convolutional structure within the Mamba\nmodel by integrating forward and backward graph convolutional network, enabling\nit to better capture local joint dependencies. Finally, we design a\nSpatiotemporal Refinement Module to model both temporal and spatial\nrelationships within the sequence. Through the incorporation of skeletal\ntopology, our approach effectively alleviates Mamba's limitations in capturing\nhuman structural relationships. We conduct extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results\nshow that the proposed method greatly reduces computational cost while\nachieving higher accuracy. Ablation studies further demonstrate the\neffectiveness of each proposed module. The code and models will be released.\n", "link": "http://arxiv.org/abs/2505.20611v2", "date": "2025-09-26", "relevancy": 2.5073, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6429}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6352}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba-Driven%20Topology%20Fusion%20for%20Monocular%203D%20Human%20Pose%20Estimation&body=Title%3A%20Mamba-Driven%20Topology%20Fusion%20for%20Monocular%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Zenghao%20Zheng%20and%20Lianping%20Yang%20and%20Jinshan%20Pan%20and%20Hegui%20Zhu%0AAbstract%3A%20%20%20Transformer-based%20methods%20for%203D%20human%20pose%20estimation%20face%20significant%0Acomputational%20challenges%20due%20to%20the%20quadratic%20growth%20of%20self-attention%0Amechanism%20complexity%20with%20sequence%20length.%20Recently%2C%20the%20Mamba%20model%20has%0Asubstantially%20reduced%20computational%20overhead%20and%20demonstrated%20outstanding%0Aperformance%20in%20modeling%20long%20sequences%20by%20leveraging%20state%20space%20model%20%28SSM%29.%0AHowever%2C%20the%20ability%20of%20SSM%20to%20process%20sequential%20data%20is%20not%20suitable%20for%203D%0Ajoint%20sequences%20with%20topological%20structures%2C%20and%20the%20causal%20convolution%0Astructure%20in%20Mamba%20also%20lacks%20insight%20into%20local%20joint%20relationships.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Mamba-Driven%20Topology%20Fusion%20framework%20in%0Athis%20paper.%20Specifically%2C%20the%20proposed%20Bone%20Aware%20Module%20infers%20the%20direction%0Aand%20length%20of%20bone%20vectors%20in%20the%20spherical%20coordinate%20system%2C%20providing%0Aeffective%20topological%20guidance%20for%20the%20Mamba%20model%20in%20processing%20joint%0Asequences.%20Furthermore%2C%20we%20enhance%20the%20convolutional%20structure%20within%20the%20Mamba%0Amodel%20by%20integrating%20forward%20and%20backward%20graph%20convolutional%20network%2C%20enabling%0Ait%20to%20better%20capture%20local%20joint%20dependencies.%20Finally%2C%20we%20design%20a%0ASpatiotemporal%20Refinement%20Module%20to%20model%20both%20temporal%20and%20spatial%0Arelationships%20within%20the%20sequence.%20Through%20the%20incorporation%20of%20skeletal%0Atopology%2C%20our%20approach%20effectively%20alleviates%20Mamba%27s%20limitations%20in%20capturing%0Ahuman%20structural%20relationships.%20We%20conduct%20extensive%20experiments%20on%20the%0AHuman3.6M%20and%20MPI-INF-3DHP%20datasets%20for%20testing%20and%20comparison%2C%20and%20the%20results%0Ashow%20that%20the%20proposed%20method%20greatly%20reduces%20computational%20cost%20while%0Aachieving%20higher%20accuracy.%20Ablation%20studies%20further%20demonstrate%20the%0Aeffectiveness%20of%20each%20proposed%20module.%20The%20code%20and%20models%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20611v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba-Driven%2520Topology%2520Fusion%2520for%2520Monocular%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DZenghao%2520Zheng%2520and%2520Lianping%2520Yang%2520and%2520Jinshan%2520Pan%2520and%2520Hegui%2520Zhu%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520for%25203D%2520human%2520pose%2520estimation%2520face%2520significant%250Acomputational%2520challenges%2520due%2520to%2520the%2520quadratic%2520growth%2520of%2520self-attention%250Amechanism%2520complexity%2520with%2520sequence%2520length.%2520Recently%252C%2520the%2520Mamba%2520model%2520has%250Asubstantially%2520reduced%2520computational%2520overhead%2520and%2520demonstrated%2520outstanding%250Aperformance%2520in%2520modeling%2520long%2520sequences%2520by%2520leveraging%2520state%2520space%2520model%2520%2528SSM%2529.%250AHowever%252C%2520the%2520ability%2520of%2520SSM%2520to%2520process%2520sequential%2520data%2520is%2520not%2520suitable%2520for%25203D%250Ajoint%2520sequences%2520with%2520topological%2520structures%252C%2520and%2520the%2520causal%2520convolution%250Astructure%2520in%2520Mamba%2520also%2520lacks%2520insight%2520into%2520local%2520joint%2520relationships.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520propose%2520the%2520Mamba-Driven%2520Topology%2520Fusion%2520framework%2520in%250Athis%2520paper.%2520Specifically%252C%2520the%2520proposed%2520Bone%2520Aware%2520Module%2520infers%2520the%2520direction%250Aand%2520length%2520of%2520bone%2520vectors%2520in%2520the%2520spherical%2520coordinate%2520system%252C%2520providing%250Aeffective%2520topological%2520guidance%2520for%2520the%2520Mamba%2520model%2520in%2520processing%2520joint%250Asequences.%2520Furthermore%252C%2520we%2520enhance%2520the%2520convolutional%2520structure%2520within%2520the%2520Mamba%250Amodel%2520by%2520integrating%2520forward%2520and%2520backward%2520graph%2520convolutional%2520network%252C%2520enabling%250Ait%2520to%2520better%2520capture%2520local%2520joint%2520dependencies.%2520Finally%252C%2520we%2520design%2520a%250ASpatiotemporal%2520Refinement%2520Module%2520to%2520model%2520both%2520temporal%2520and%2520spatial%250Arelationships%2520within%2520the%2520sequence.%2520Through%2520the%2520incorporation%2520of%2520skeletal%250Atopology%252C%2520our%2520approach%2520effectively%2520alleviates%2520Mamba%2527s%2520limitations%2520in%2520capturing%250Ahuman%2520structural%2520relationships.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520the%250AHuman3.6M%2520and%2520MPI-INF-3DHP%2520datasets%2520for%2520testing%2520and%2520comparison%252C%2520and%2520the%2520results%250Ashow%2520that%2520the%2520proposed%2520method%2520greatly%2520reduces%2520computational%2520cost%2520while%250Aachieving%2520higher%2520accuracy.%2520Ablation%2520studies%2520further%2520demonstrate%2520the%250Aeffectiveness%2520of%2520each%2520proposed%2520module.%2520The%2520code%2520and%2520models%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20611v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba-Driven%20Topology%20Fusion%20for%20Monocular%203D%20Human%20Pose%20Estimation&entry.906535625=Zenghao%20Zheng%20and%20Lianping%20Yang%20and%20Jinshan%20Pan%20and%20Hegui%20Zhu&entry.1292438233=%20%20Transformer-based%20methods%20for%203D%20human%20pose%20estimation%20face%20significant%0Acomputational%20challenges%20due%20to%20the%20quadratic%20growth%20of%20self-attention%0Amechanism%20complexity%20with%20sequence%20length.%20Recently%2C%20the%20Mamba%20model%20has%0Asubstantially%20reduced%20computational%20overhead%20and%20demonstrated%20outstanding%0Aperformance%20in%20modeling%20long%20sequences%20by%20leveraging%20state%20space%20model%20%28SSM%29.%0AHowever%2C%20the%20ability%20of%20SSM%20to%20process%20sequential%20data%20is%20not%20suitable%20for%203D%0Ajoint%20sequences%20with%20topological%20structures%2C%20and%20the%20causal%20convolution%0Astructure%20in%20Mamba%20also%20lacks%20insight%20into%20local%20joint%20relationships.%20To%0Aaddress%20these%20issues%2C%20we%20propose%20the%20Mamba-Driven%20Topology%20Fusion%20framework%20in%0Athis%20paper.%20Specifically%2C%20the%20proposed%20Bone%20Aware%20Module%20infers%20the%20direction%0Aand%20length%20of%20bone%20vectors%20in%20the%20spherical%20coordinate%20system%2C%20providing%0Aeffective%20topological%20guidance%20for%20the%20Mamba%20model%20in%20processing%20joint%0Asequences.%20Furthermore%2C%20we%20enhance%20the%20convolutional%20structure%20within%20the%20Mamba%0Amodel%20by%20integrating%20forward%20and%20backward%20graph%20convolutional%20network%2C%20enabling%0Ait%20to%20better%20capture%20local%20joint%20dependencies.%20Finally%2C%20we%20design%20a%0ASpatiotemporal%20Refinement%20Module%20to%20model%20both%20temporal%20and%20spatial%0Arelationships%20within%20the%20sequence.%20Through%20the%20incorporation%20of%20skeletal%0Atopology%2C%20our%20approach%20effectively%20alleviates%20Mamba%27s%20limitations%20in%20capturing%0Ahuman%20structural%20relationships.%20We%20conduct%20extensive%20experiments%20on%20the%0AHuman3.6M%20and%20MPI-INF-3DHP%20datasets%20for%20testing%20and%20comparison%2C%20and%20the%20results%0Ashow%20that%20the%20proposed%20method%20greatly%20reduces%20computational%20cost%20while%0Aachieving%20higher%20accuracy.%20Ablation%20studies%20further%20demonstrate%20the%0Aeffectiveness%20of%20each%20proposed%20module.%20The%20code%20and%20models%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20611v2&entry.124074799=Read"},
{"title": "CHRONOBERG: Capturing Language Evolution and Temporal Awareness in\n  Foundation Models", "author": "Niharika Hegde and Subarnaduti Paul and Lars Joel-Frey and Manuel Brack and Kristian Kersting and Martin Mundt and Patrick Schramowski", "abstract": "  Large language models (LLMs) excel at operating at scale by leveraging social\nmedia and various data crawled from the web. Whereas existing corpora are\ndiverse, their frequent lack of long-term temporal structure may however limit\nan LLM's ability to contextualize semantic and normative evolution of language\nand to capture diachronic variation. To support analysis and training for the\nlatter, we introduce CHRONOBERG, a temporally structured corpus of English book\ntexts spanning 250 years, curated from Project Gutenberg and enriched with a\nvariety of temporal annotations. First, the edited nature of books enables us\nto quantify lexical semantic change through time-sensitive\nValence-Arousal-Dominance (VAD) analysis and to construct historically\ncalibrated affective lexicons to support temporally grounded interpretation.\nWith the lexicons at hand, we demonstrate a need for modern LLM-based tools to\nbetter situate their detection of discriminatory language and contextualization\nof sentiment across various time-periods. In fact, we show how language models\ntrained sequentially on CHRONOBERG struggle to encode diachronic shifts in\nmeaning, emphasizing the need for temporally aware training and evaluation\npipelines, and positioning CHRONOBERG as a scalable resource for the study of\nlinguistic change and temporal generalization. Disclaimer: This paper includes\nlanguage and display of samples that could be offensive to readers. Open\nAccess: Chronoberg is available publicly on HuggingFace at (\nhttps://huggingface.co/datasets/spaul25/Chronoberg). Code is available at\n(https://github.com/paulsubarna/Chronoberg).\n", "link": "http://arxiv.org/abs/2509.22360v1", "date": "2025-09-26", "relevancy": 2.4921, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5075}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHRONOBERG%3A%20Capturing%20Language%20Evolution%20and%20Temporal%20Awareness%20in%0A%20%20Foundation%20Models&body=Title%3A%20CHRONOBERG%3A%20Capturing%20Language%20Evolution%20and%20Temporal%20Awareness%20in%0A%20%20Foundation%20Models%0AAuthor%3A%20Niharika%20Hegde%20and%20Subarnaduti%20Paul%20and%20Lars%20Joel-Frey%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Martin%20Mundt%20and%20Patrick%20Schramowski%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20operating%20at%20scale%20by%20leveraging%20social%0Amedia%20and%20various%20data%20crawled%20from%20the%20web.%20Whereas%20existing%20corpora%20are%0Adiverse%2C%20their%20frequent%20lack%20of%20long-term%20temporal%20structure%20may%20however%20limit%0Aan%20LLM%27s%20ability%20to%20contextualize%20semantic%20and%20normative%20evolution%20of%20language%0Aand%20to%20capture%20diachronic%20variation.%20To%20support%20analysis%20and%20training%20for%20the%0Alatter%2C%20we%20introduce%20CHRONOBERG%2C%20a%20temporally%20structured%20corpus%20of%20English%20book%0Atexts%20spanning%20250%20years%2C%20curated%20from%20Project%20Gutenberg%20and%20enriched%20with%20a%0Avariety%20of%20temporal%20annotations.%20First%2C%20the%20edited%20nature%20of%20books%20enables%20us%0Ato%20quantify%20lexical%20semantic%20change%20through%20time-sensitive%0AValence-Arousal-Dominance%20%28VAD%29%20analysis%20and%20to%20construct%20historically%0Acalibrated%20affective%20lexicons%20to%20support%20temporally%20grounded%20interpretation.%0AWith%20the%20lexicons%20at%20hand%2C%20we%20demonstrate%20a%20need%20for%20modern%20LLM-based%20tools%20to%0Abetter%20situate%20their%20detection%20of%20discriminatory%20language%20and%20contextualization%0Aof%20sentiment%20across%20various%20time-periods.%20In%20fact%2C%20we%20show%20how%20language%20models%0Atrained%20sequentially%20on%20CHRONOBERG%20struggle%20to%20encode%20diachronic%20shifts%20in%0Ameaning%2C%20emphasizing%20the%20need%20for%20temporally%20aware%20training%20and%20evaluation%0Apipelines%2C%20and%20positioning%20CHRONOBERG%20as%20a%20scalable%20resource%20for%20the%20study%20of%0Alinguistic%20change%20and%20temporal%20generalization.%20Disclaimer%3A%20This%20paper%20includes%0Alanguage%20and%20display%20of%20samples%20that%20could%20be%20offensive%20to%20readers.%20Open%0AAccess%3A%20Chronoberg%20is%20available%20publicly%20on%20HuggingFace%20at%20%28%0Ahttps%3A//huggingface.co/datasets/spaul25/Chronoberg%29.%20Code%20is%20available%20at%0A%28https%3A//github.com/paulsubarna/Chronoberg%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHRONOBERG%253A%2520Capturing%2520Language%2520Evolution%2520and%2520Temporal%2520Awareness%2520in%250A%2520%2520Foundation%2520Models%26entry.906535625%3DNiharika%2520Hegde%2520and%2520Subarnaduti%2520Paul%2520and%2520Lars%2520Joel-Frey%2520and%2520Manuel%2520Brack%2520and%2520Kristian%2520Kersting%2520and%2520Martin%2520Mundt%2520and%2520Patrick%2520Schramowski%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520operating%2520at%2520scale%2520by%2520leveraging%2520social%250Amedia%2520and%2520various%2520data%2520crawled%2520from%2520the%2520web.%2520Whereas%2520existing%2520corpora%2520are%250Adiverse%252C%2520their%2520frequent%2520lack%2520of%2520long-term%2520temporal%2520structure%2520may%2520however%2520limit%250Aan%2520LLM%2527s%2520ability%2520to%2520contextualize%2520semantic%2520and%2520normative%2520evolution%2520of%2520language%250Aand%2520to%2520capture%2520diachronic%2520variation.%2520To%2520support%2520analysis%2520and%2520training%2520for%2520the%250Alatter%252C%2520we%2520introduce%2520CHRONOBERG%252C%2520a%2520temporally%2520structured%2520corpus%2520of%2520English%2520book%250Atexts%2520spanning%2520250%2520years%252C%2520curated%2520from%2520Project%2520Gutenberg%2520and%2520enriched%2520with%2520a%250Avariety%2520of%2520temporal%2520annotations.%2520First%252C%2520the%2520edited%2520nature%2520of%2520books%2520enables%2520us%250Ato%2520quantify%2520lexical%2520semantic%2520change%2520through%2520time-sensitive%250AValence-Arousal-Dominance%2520%2528VAD%2529%2520analysis%2520and%2520to%2520construct%2520historically%250Acalibrated%2520affective%2520lexicons%2520to%2520support%2520temporally%2520grounded%2520interpretation.%250AWith%2520the%2520lexicons%2520at%2520hand%252C%2520we%2520demonstrate%2520a%2520need%2520for%2520modern%2520LLM-based%2520tools%2520to%250Abetter%2520situate%2520their%2520detection%2520of%2520discriminatory%2520language%2520and%2520contextualization%250Aof%2520sentiment%2520across%2520various%2520time-periods.%2520In%2520fact%252C%2520we%2520show%2520how%2520language%2520models%250Atrained%2520sequentially%2520on%2520CHRONOBERG%2520struggle%2520to%2520encode%2520diachronic%2520shifts%2520in%250Ameaning%252C%2520emphasizing%2520the%2520need%2520for%2520temporally%2520aware%2520training%2520and%2520evaluation%250Apipelines%252C%2520and%2520positioning%2520CHRONOBERG%2520as%2520a%2520scalable%2520resource%2520for%2520the%2520study%2520of%250Alinguistic%2520change%2520and%2520temporal%2520generalization.%2520Disclaimer%253A%2520This%2520paper%2520includes%250Alanguage%2520and%2520display%2520of%2520samples%2520that%2520could%2520be%2520offensive%2520to%2520readers.%2520Open%250AAccess%253A%2520Chronoberg%2520is%2520available%2520publicly%2520on%2520HuggingFace%2520at%2520%2528%250Ahttps%253A//huggingface.co/datasets/spaul25/Chronoberg%2529.%2520Code%2520is%2520available%2520at%250A%2528https%253A//github.com/paulsubarna/Chronoberg%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHRONOBERG%3A%20Capturing%20Language%20Evolution%20and%20Temporal%20Awareness%20in%0A%20%20Foundation%20Models&entry.906535625=Niharika%20Hegde%20and%20Subarnaduti%20Paul%20and%20Lars%20Joel-Frey%20and%20Manuel%20Brack%20and%20Kristian%20Kersting%20and%20Martin%20Mundt%20and%20Patrick%20Schramowski&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20at%20operating%20at%20scale%20by%20leveraging%20social%0Amedia%20and%20various%20data%20crawled%20from%20the%20web.%20Whereas%20existing%20corpora%20are%0Adiverse%2C%20their%20frequent%20lack%20of%20long-term%20temporal%20structure%20may%20however%20limit%0Aan%20LLM%27s%20ability%20to%20contextualize%20semantic%20and%20normative%20evolution%20of%20language%0Aand%20to%20capture%20diachronic%20variation.%20To%20support%20analysis%20and%20training%20for%20the%0Alatter%2C%20we%20introduce%20CHRONOBERG%2C%20a%20temporally%20structured%20corpus%20of%20English%20book%0Atexts%20spanning%20250%20years%2C%20curated%20from%20Project%20Gutenberg%20and%20enriched%20with%20a%0Avariety%20of%20temporal%20annotations.%20First%2C%20the%20edited%20nature%20of%20books%20enables%20us%0Ato%20quantify%20lexical%20semantic%20change%20through%20time-sensitive%0AValence-Arousal-Dominance%20%28VAD%29%20analysis%20and%20to%20construct%20historically%0Acalibrated%20affective%20lexicons%20to%20support%20temporally%20grounded%20interpretation.%0AWith%20the%20lexicons%20at%20hand%2C%20we%20demonstrate%20a%20need%20for%20modern%20LLM-based%20tools%20to%0Abetter%20situate%20their%20detection%20of%20discriminatory%20language%20and%20contextualization%0Aof%20sentiment%20across%20various%20time-periods.%20In%20fact%2C%20we%20show%20how%20language%20models%0Atrained%20sequentially%20on%20CHRONOBERG%20struggle%20to%20encode%20diachronic%20shifts%20in%0Ameaning%2C%20emphasizing%20the%20need%20for%20temporally%20aware%20training%20and%20evaluation%0Apipelines%2C%20and%20positioning%20CHRONOBERG%20as%20a%20scalable%20resource%20for%20the%20study%20of%0Alinguistic%20change%20and%20temporal%20generalization.%20Disclaimer%3A%20This%20paper%20includes%0Alanguage%20and%20display%20of%20samples%20that%20could%20be%20offensive%20to%20readers.%20Open%0AAccess%3A%20Chronoberg%20is%20available%20publicly%20on%20HuggingFace%20at%20%28%0Ahttps%3A//huggingface.co/datasets/spaul25/Chronoberg%29.%20Code%20is%20available%20at%0A%28https%3A//github.com/paulsubarna/Chronoberg%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22360v1&entry.124074799=Read"},
{"title": "Chain or tree? Re-evaluating complex reasoning from the perspective of a\n  matrix of thought", "author": "Fengxiao Tang and Yufeng Li and Zongzong Wu and Ming Zhao", "abstract": "  Large Language Models (LLMs) face significant accuracy degradation due to\ninsufficient reasoning ability when dealing with complex and abstract tasks.\nThought structures such as Chain of Thought (CoT) and Tree of Thought (ToT)\nfocus on enhancing the reasoning capability of LLMs. However, they suffer from\ninherent drawbacks such as redundancy within the same layer of the tree\nstructure and the singularity of the paths in the chain structure. Some studies\nhave utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and\nToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of\nthe thought structures still persist. Furthermore, when dealing with\nmulti-entity and multi-hop information, the retrieved verification knowledge\noften contains large amounts of fragmented, superficial, or even erroneous\ndata, misleading the reasoning process of LLMs. To address these issues, we\npropose the Matrix of Thought (MoT), a novel and efficient thought structure\nfor LLMs. MoT explores problems in both horizontal and vertical dimensions\nthrough a \"column-cell communication\" mechanism, enabling LLMs to actively\nengage in multi-strategy and deep thinking while reducing redundancy in the\nthought nodes within the column cells, thereby enhancing the reasoning\ncapability of LLMs. Additionally, through a fact-correction mechanism, it\nleverages the knowledge graph triples retrieved by RAG and the original text to\nconstruct knowledge units and correct erroneous answers. To validate the\neffectiveness of this method, we conducted extensive experiments in three\ntasks: 24-point game, question answering evaluation, and proposition\nwriting.The results demonstrate that our framework outperforms state-of-the-art\nmethods, with reasoning time only 14.4\\% of that of the baseline method,\nproving its efficiency and accuracy. The code for framework is available at\nhttps://github.com/lyfiter/mtqa.\n", "link": "http://arxiv.org/abs/2509.03918v2", "date": "2025-09-26", "relevancy": 2.4896, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain%20or%20tree%3F%20Re-evaluating%20complex%20reasoning%20from%20the%20perspective%20of%20a%0A%20%20matrix%20of%20thought&body=Title%3A%20Chain%20or%20tree%3F%20Re-evaluating%20complex%20reasoning%20from%20the%20perspective%20of%20a%0A%20%20matrix%20of%20thought%0AAuthor%3A%20Fengxiao%20Tang%20and%20Yufeng%20Li%20and%20Zongzong%20Wu%20and%20Ming%20Zhao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20accuracy%20degradation%20due%20to%0Ainsufficient%20reasoning%20ability%20when%20dealing%20with%20complex%20and%20abstract%20tasks.%0AThought%20structures%20such%20as%20Chain%20of%20Thought%20%28CoT%29%20and%20Tree%20of%20Thought%20%28ToT%29%0Afocus%20on%20enhancing%20the%20reasoning%20capability%20of%20LLMs.%20However%2C%20they%20suffer%20from%0Ainherent%20drawbacks%20such%20as%20redundancy%20within%20the%20same%20layer%20of%20the%20tree%0Astructure%20and%20the%20singularity%20of%20the%20paths%20in%20the%20chain%20structure.%20Some%20studies%0Ahave%20utilized%20Retrieval-Augmented%20Generation%20%28RAG%29%20methods%20to%20enhance%20CoT%20and%0AToT%20in%20mitigating%20hallucinations%20in%20LLMs%2C%20yet%20the%20fundamental%20shortcomings%20of%0Athe%20thought%20structures%20still%20persist.%20Furthermore%2C%20when%20dealing%20with%0Amulti-entity%20and%20multi-hop%20information%2C%20the%20retrieved%20verification%20knowledge%0Aoften%20contains%20large%20amounts%20of%20fragmented%2C%20superficial%2C%20or%20even%20erroneous%0Adata%2C%20misleading%20the%20reasoning%20process%20of%20LLMs.%20To%20address%20these%20issues%2C%20we%0Apropose%20the%20Matrix%20of%20Thought%20%28MoT%29%2C%20a%20novel%20and%20efficient%20thought%20structure%0Afor%20LLMs.%20MoT%20explores%20problems%20in%20both%20horizontal%20and%20vertical%20dimensions%0Athrough%20a%20%22column-cell%20communication%22%20mechanism%2C%20enabling%20LLMs%20to%20actively%0Aengage%20in%20multi-strategy%20and%20deep%20thinking%20while%20reducing%20redundancy%20in%20the%0Athought%20nodes%20within%20the%20column%20cells%2C%20thereby%20enhancing%20the%20reasoning%0Acapability%20of%20LLMs.%20Additionally%2C%20through%20a%20fact-correction%20mechanism%2C%20it%0Aleverages%20the%20knowledge%20graph%20triples%20retrieved%20by%20RAG%20and%20the%20original%20text%20to%0Aconstruct%20knowledge%20units%20and%20correct%20erroneous%20answers.%20To%20validate%20the%0Aeffectiveness%20of%20this%20method%2C%20we%20conducted%20extensive%20experiments%20in%20three%0Atasks%3A%2024-point%20game%2C%20question%20answering%20evaluation%2C%20and%20proposition%0Awriting.The%20results%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%0Amethods%2C%20with%20reasoning%20time%20only%2014.4%5C%25%20of%20that%20of%20the%20baseline%20method%2C%0Aproving%20its%20efficiency%20and%20accuracy.%20The%20code%20for%20framework%20is%20available%20at%0Ahttps%3A//github.com/lyfiter/mtqa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain%2520or%2520tree%253F%2520Re-evaluating%2520complex%2520reasoning%2520from%2520the%2520perspective%2520of%2520a%250A%2520%2520matrix%2520of%2520thought%26entry.906535625%3DFengxiao%2520Tang%2520and%2520Yufeng%2520Li%2520and%2520Zongzong%2520Wu%2520and%2520Ming%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520significant%2520accuracy%2520degradation%2520due%2520to%250Ainsufficient%2520reasoning%2520ability%2520when%2520dealing%2520with%2520complex%2520and%2520abstract%2520tasks.%250AThought%2520structures%2520such%2520as%2520Chain%2520of%2520Thought%2520%2528CoT%2529%2520and%2520Tree%2520of%2520Thought%2520%2528ToT%2529%250Afocus%2520on%2520enhancing%2520the%2520reasoning%2520capability%2520of%2520LLMs.%2520However%252C%2520they%2520suffer%2520from%250Ainherent%2520drawbacks%2520such%2520as%2520redundancy%2520within%2520the%2520same%2520layer%2520of%2520the%2520tree%250Astructure%2520and%2520the%2520singularity%2520of%2520the%2520paths%2520in%2520the%2520chain%2520structure.%2520Some%2520studies%250Ahave%2520utilized%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520methods%2520to%2520enhance%2520CoT%2520and%250AToT%2520in%2520mitigating%2520hallucinations%2520in%2520LLMs%252C%2520yet%2520the%2520fundamental%2520shortcomings%2520of%250Athe%2520thought%2520structures%2520still%2520persist.%2520Furthermore%252C%2520when%2520dealing%2520with%250Amulti-entity%2520and%2520multi-hop%2520information%252C%2520the%2520retrieved%2520verification%2520knowledge%250Aoften%2520contains%2520large%2520amounts%2520of%2520fragmented%252C%2520superficial%252C%2520or%2520even%2520erroneous%250Adata%252C%2520misleading%2520the%2520reasoning%2520process%2520of%2520LLMs.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520the%2520Matrix%2520of%2520Thought%2520%2528MoT%2529%252C%2520a%2520novel%2520and%2520efficient%2520thought%2520structure%250Afor%2520LLMs.%2520MoT%2520explores%2520problems%2520in%2520both%2520horizontal%2520and%2520vertical%2520dimensions%250Athrough%2520a%2520%2522column-cell%2520communication%2522%2520mechanism%252C%2520enabling%2520LLMs%2520to%2520actively%250Aengage%2520in%2520multi-strategy%2520and%2520deep%2520thinking%2520while%2520reducing%2520redundancy%2520in%2520the%250Athought%2520nodes%2520within%2520the%2520column%2520cells%252C%2520thereby%2520enhancing%2520the%2520reasoning%250Acapability%2520of%2520LLMs.%2520Additionally%252C%2520through%2520a%2520fact-correction%2520mechanism%252C%2520it%250Aleverages%2520the%2520knowledge%2520graph%2520triples%2520retrieved%2520by%2520RAG%2520and%2520the%2520original%2520text%2520to%250Aconstruct%2520knowledge%2520units%2520and%2520correct%2520erroneous%2520answers.%2520To%2520validate%2520the%250Aeffectiveness%2520of%2520this%2520method%252C%2520we%2520conducted%2520extensive%2520experiments%2520in%2520three%250Atasks%253A%252024-point%2520game%252C%2520question%2520answering%2520evaluation%252C%2520and%2520proposition%250Awriting.The%2520results%2520demonstrate%2520that%2520our%2520framework%2520outperforms%2520state-of-the-art%250Amethods%252C%2520with%2520reasoning%2520time%2520only%252014.4%255C%2525%2520of%2520that%2520of%2520the%2520baseline%2520method%252C%250Aproving%2520its%2520efficiency%2520and%2520accuracy.%2520The%2520code%2520for%2520framework%2520is%2520available%2520at%250Ahttps%253A//github.com/lyfiter/mtqa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain%20or%20tree%3F%20Re-evaluating%20complex%20reasoning%20from%20the%20perspective%20of%20a%0A%20%20matrix%20of%20thought&entry.906535625=Fengxiao%20Tang%20and%20Yufeng%20Li%20and%20Zongzong%20Wu%20and%20Ming%20Zhao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20accuracy%20degradation%20due%20to%0Ainsufficient%20reasoning%20ability%20when%20dealing%20with%20complex%20and%20abstract%20tasks.%0AThought%20structures%20such%20as%20Chain%20of%20Thought%20%28CoT%29%20and%20Tree%20of%20Thought%20%28ToT%29%0Afocus%20on%20enhancing%20the%20reasoning%20capability%20of%20LLMs.%20However%2C%20they%20suffer%20from%0Ainherent%20drawbacks%20such%20as%20redundancy%20within%20the%20same%20layer%20of%20the%20tree%0Astructure%20and%20the%20singularity%20of%20the%20paths%20in%20the%20chain%20structure.%20Some%20studies%0Ahave%20utilized%20Retrieval-Augmented%20Generation%20%28RAG%29%20methods%20to%20enhance%20CoT%20and%0AToT%20in%20mitigating%20hallucinations%20in%20LLMs%2C%20yet%20the%20fundamental%20shortcomings%20of%0Athe%20thought%20structures%20still%20persist.%20Furthermore%2C%20when%20dealing%20with%0Amulti-entity%20and%20multi-hop%20information%2C%20the%20retrieved%20verification%20knowledge%0Aoften%20contains%20large%20amounts%20of%20fragmented%2C%20superficial%2C%20or%20even%20erroneous%0Adata%2C%20misleading%20the%20reasoning%20process%20of%20LLMs.%20To%20address%20these%20issues%2C%20we%0Apropose%20the%20Matrix%20of%20Thought%20%28MoT%29%2C%20a%20novel%20and%20efficient%20thought%20structure%0Afor%20LLMs.%20MoT%20explores%20problems%20in%20both%20horizontal%20and%20vertical%20dimensions%0Athrough%20a%20%22column-cell%20communication%22%20mechanism%2C%20enabling%20LLMs%20to%20actively%0Aengage%20in%20multi-strategy%20and%20deep%20thinking%20while%20reducing%20redundancy%20in%20the%0Athought%20nodes%20within%20the%20column%20cells%2C%20thereby%20enhancing%20the%20reasoning%0Acapability%20of%20LLMs.%20Additionally%2C%20through%20a%20fact-correction%20mechanism%2C%20it%0Aleverages%20the%20knowledge%20graph%20triples%20retrieved%20by%20RAG%20and%20the%20original%20text%20to%0Aconstruct%20knowledge%20units%20and%20correct%20erroneous%20answers.%20To%20validate%20the%0Aeffectiveness%20of%20this%20method%2C%20we%20conducted%20extensive%20experiments%20in%20three%0Atasks%3A%2024-point%20game%2C%20question%20answering%20evaluation%2C%20and%20proposition%0Awriting.The%20results%20demonstrate%20that%20our%20framework%20outperforms%20state-of-the-art%0Amethods%2C%20with%20reasoning%20time%20only%2014.4%5C%25%20of%20that%20of%20the%20baseline%20method%2C%0Aproving%20its%20efficiency%20and%20accuracy.%20The%20code%20for%20framework%20is%20available%20at%0Ahttps%3A//github.com/lyfiter/mtqa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03918v2&entry.124074799=Read"},
{"title": "Caterpillar GNN: Replacing Message Passing with Efficient Aggregation", "author": "Marek \u010cern\u00fd", "abstract": "  Message-passing graph neural networks (MPGNNs) dominate modern graph\nlearning. Typical efforts enhance MPGNN's expressive power by enriching the\nadjacency-based aggregation. In contrast, we introduce an efficient aggregation\nover walk incidence-based matrices that are constructed to deliberately trade\noff some expressivity for stronger and more structured inductive bias. Our\napproach allows for seamless scaling between classical message-passing and\nsimpler methods based on walks. We rigorously characterize the expressive power\nat each intermediate step using homomorphism counts over a hierarchy of\ngeneralized caterpillar graphs. Based on this foundation, we propose\nCaterpillar GNNs, whose robust graph-level aggregation successfully tackles a\nbenchmark specifically designed to challenge MPGNNs. Moreover, we demonstrate\nthat, on real-world datasets, Caterpillar GNNs achieve comparable predictive\nperformance while significantly reducing the number of nodes in the hidden\nlayers of the computational graph.\n", "link": "http://arxiv.org/abs/2506.06784v2", "date": "2025-09-26", "relevancy": 2.4753, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5061}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.497}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Caterpillar%20GNN%3A%20Replacing%20Message%20Passing%20with%20Efficient%20Aggregation&body=Title%3A%20Caterpillar%20GNN%3A%20Replacing%20Message%20Passing%20with%20Efficient%20Aggregation%0AAuthor%3A%20Marek%20%C4%8Cern%C3%BD%0AAbstract%3A%20%20%20Message-passing%20graph%20neural%20networks%20%28MPGNNs%29%20dominate%20modern%20graph%0Alearning.%20Typical%20efforts%20enhance%20MPGNN%27s%20expressive%20power%20by%20enriching%20the%0Aadjacency-based%20aggregation.%20In%20contrast%2C%20we%20introduce%20an%20efficient%20aggregation%0Aover%20walk%20incidence-based%20matrices%20that%20are%20constructed%20to%20deliberately%20trade%0Aoff%20some%20expressivity%20for%20stronger%20and%20more%20structured%20inductive%20bias.%20Our%0Aapproach%20allows%20for%20seamless%20scaling%20between%20classical%20message-passing%20and%0Asimpler%20methods%20based%20on%20walks.%20We%20rigorously%20characterize%20the%20expressive%20power%0Aat%20each%20intermediate%20step%20using%20homomorphism%20counts%20over%20a%20hierarchy%20of%0Ageneralized%20caterpillar%20graphs.%20Based%20on%20this%20foundation%2C%20we%20propose%0ACaterpillar%20GNNs%2C%20whose%20robust%20graph-level%20aggregation%20successfully%20tackles%20a%0Abenchmark%20specifically%20designed%20to%20challenge%20MPGNNs.%20Moreover%2C%20we%20demonstrate%0Athat%2C%20on%20real-world%20datasets%2C%20Caterpillar%20GNNs%20achieve%20comparable%20predictive%0Aperformance%20while%20significantly%20reducing%20the%20number%20of%20nodes%20in%20the%20hidden%0Alayers%20of%20the%20computational%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaterpillar%2520GNN%253A%2520Replacing%2520Message%2520Passing%2520with%2520Efficient%2520Aggregation%26entry.906535625%3DMarek%2520%25C4%258Cern%25C3%25BD%26entry.1292438233%3D%2520%2520Message-passing%2520graph%2520neural%2520networks%2520%2528MPGNNs%2529%2520dominate%2520modern%2520graph%250Alearning.%2520Typical%2520efforts%2520enhance%2520MPGNN%2527s%2520expressive%2520power%2520by%2520enriching%2520the%250Aadjacency-based%2520aggregation.%2520In%2520contrast%252C%2520we%2520introduce%2520an%2520efficient%2520aggregation%250Aover%2520walk%2520incidence-based%2520matrices%2520that%2520are%2520constructed%2520to%2520deliberately%2520trade%250Aoff%2520some%2520expressivity%2520for%2520stronger%2520and%2520more%2520structured%2520inductive%2520bias.%2520Our%250Aapproach%2520allows%2520for%2520seamless%2520scaling%2520between%2520classical%2520message-passing%2520and%250Asimpler%2520methods%2520based%2520on%2520walks.%2520We%2520rigorously%2520characterize%2520the%2520expressive%2520power%250Aat%2520each%2520intermediate%2520step%2520using%2520homomorphism%2520counts%2520over%2520a%2520hierarchy%2520of%250Ageneralized%2520caterpillar%2520graphs.%2520Based%2520on%2520this%2520foundation%252C%2520we%2520propose%250ACaterpillar%2520GNNs%252C%2520whose%2520robust%2520graph-level%2520aggregation%2520successfully%2520tackles%2520a%250Abenchmark%2520specifically%2520designed%2520to%2520challenge%2520MPGNNs.%2520Moreover%252C%2520we%2520demonstrate%250Athat%252C%2520on%2520real-world%2520datasets%252C%2520Caterpillar%2520GNNs%2520achieve%2520comparable%2520predictive%250Aperformance%2520while%2520significantly%2520reducing%2520the%2520number%2520of%2520nodes%2520in%2520the%2520hidden%250Alayers%2520of%2520the%2520computational%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Caterpillar%20GNN%3A%20Replacing%20Message%20Passing%20with%20Efficient%20Aggregation&entry.906535625=Marek%20%C4%8Cern%C3%BD&entry.1292438233=%20%20Message-passing%20graph%20neural%20networks%20%28MPGNNs%29%20dominate%20modern%20graph%0Alearning.%20Typical%20efforts%20enhance%20MPGNN%27s%20expressive%20power%20by%20enriching%20the%0Aadjacency-based%20aggregation.%20In%20contrast%2C%20we%20introduce%20an%20efficient%20aggregation%0Aover%20walk%20incidence-based%20matrices%20that%20are%20constructed%20to%20deliberately%20trade%0Aoff%20some%20expressivity%20for%20stronger%20and%20more%20structured%20inductive%20bias.%20Our%0Aapproach%20allows%20for%20seamless%20scaling%20between%20classical%20message-passing%20and%0Asimpler%20methods%20based%20on%20walks.%20We%20rigorously%20characterize%20the%20expressive%20power%0Aat%20each%20intermediate%20step%20using%20homomorphism%20counts%20over%20a%20hierarchy%20of%0Ageneralized%20caterpillar%20graphs.%20Based%20on%20this%20foundation%2C%20we%20propose%0ACaterpillar%20GNNs%2C%20whose%20robust%20graph-level%20aggregation%20successfully%20tackles%20a%0Abenchmark%20specifically%20designed%20to%20challenge%20MPGNNs.%20Moreover%2C%20we%20demonstrate%0Athat%2C%20on%20real-world%20datasets%2C%20Caterpillar%20GNNs%20achieve%20comparable%20predictive%0Aperformance%20while%20significantly%20reducing%20the%20number%20of%20nodes%20in%20the%20hidden%0Alayers%20of%20the%20computational%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06784v2&entry.124074799=Read"},
{"title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and\n  Generation", "author": "Zhen Li and Duan Li and Yukai Guo and Xinyuan Guo and Bowen Li and Lanxi Xiao and Shenyu Qiao and Jiashu Chen and Zijian Wu and Hui Zhang and Xinhuan Shu and Shixia Liu", "abstract": "  Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 440\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs.\n", "link": "http://arxiv.org/abs/2505.18668v4", "date": "2025-09-26", "relevancy": 2.4729, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4997}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4973}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartGalaxy%3A%20A%20Dataset%20for%20Infographic%20Chart%20Understanding%20and%0A%20%20Generation&body=Title%3A%20ChartGalaxy%3A%20A%20Dataset%20for%20Infographic%20Chart%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Zhen%20Li%20and%20Duan%20Li%20and%20Yukai%20Guo%20and%20Xinyuan%20Guo%20and%20Bowen%20Li%20and%20Lanxi%20Xiao%20and%20Shenyu%20Qiao%20and%20Jiashu%20Chen%20and%20Zijian%20Wu%20and%20Hui%20Zhang%20and%20Xinhuan%20Shu%20and%20Shixia%20Liu%0AAbstract%3A%20%20%20Infographic%20charts%20are%20a%20powerful%20medium%20for%20communicating%20abstract%20data%20by%0Acombining%20visual%20elements%20%28e.g.%2C%20charts%2C%20images%29%20with%20textual%20information.%0AHowever%2C%20their%20visual%20and%20structural%20richness%20poses%20challenges%20for%20large%0Avision-language%20models%20%28LVLMs%29%2C%20which%20are%20typically%20trained%20on%20plain%20charts.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20ChartGalaxy%2C%20a%20million-scale%20dataset%20designed%20to%0Aadvance%20the%20understanding%20and%20generation%20of%20infographic%20charts.%20The%20dataset%20is%0Aconstructed%20through%20an%20inductive%20process%20that%20identifies%2075%20chart%20types%2C%20440%0Achart%20variations%2C%20and%2068%20layout%20templates%20from%20real%20infographic%20charts%20and%20uses%0Athem%20to%20create%20synthetic%20ones%20programmatically.%20We%20showcase%20the%20utility%20of%20this%0Adataset%20through%3A%201%29%20improving%20infographic%20chart%20understanding%20via%20fine-tuning%2C%0A2%29%20benchmarking%20code%20generation%20for%20infographic%20charts%2C%20and%203%29%20enabling%0Aexample-based%20infographic%20chart%20generation.%20By%20capturing%20the%20visual%20and%0Astructural%20complexity%20of%20real%20design%2C%20ChartGalaxy%20provides%20a%20useful%20resource%0Afor%20enhancing%20multimodal%20reasoning%20and%20generation%20in%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18668v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartGalaxy%253A%2520A%2520Dataset%2520for%2520Infographic%2520Chart%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DZhen%2520Li%2520and%2520Duan%2520Li%2520and%2520Yukai%2520Guo%2520and%2520Xinyuan%2520Guo%2520and%2520Bowen%2520Li%2520and%2520Lanxi%2520Xiao%2520and%2520Shenyu%2520Qiao%2520and%2520Jiashu%2520Chen%2520and%2520Zijian%2520Wu%2520and%2520Hui%2520Zhang%2520and%2520Xinhuan%2520Shu%2520and%2520Shixia%2520Liu%26entry.1292438233%3D%2520%2520Infographic%2520charts%2520are%2520a%2520powerful%2520medium%2520for%2520communicating%2520abstract%2520data%2520by%250Acombining%2520visual%2520elements%2520%2528e.g.%252C%2520charts%252C%2520images%2529%2520with%2520textual%2520information.%250AHowever%252C%2520their%2520visual%2520and%2520structural%2520richness%2520poses%2520challenges%2520for%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%252C%2520which%2520are%2520typically%2520trained%2520on%2520plain%2520charts.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520introduce%2520ChartGalaxy%252C%2520a%2520million-scale%2520dataset%2520designed%2520to%250Aadvance%2520the%2520understanding%2520and%2520generation%2520of%2520infographic%2520charts.%2520The%2520dataset%2520is%250Aconstructed%2520through%2520an%2520inductive%2520process%2520that%2520identifies%252075%2520chart%2520types%252C%2520440%250Achart%2520variations%252C%2520and%252068%2520layout%2520templates%2520from%2520real%2520infographic%2520charts%2520and%2520uses%250Athem%2520to%2520create%2520synthetic%2520ones%2520programmatically.%2520We%2520showcase%2520the%2520utility%2520of%2520this%250Adataset%2520through%253A%25201%2529%2520improving%2520infographic%2520chart%2520understanding%2520via%2520fine-tuning%252C%250A2%2529%2520benchmarking%2520code%2520generation%2520for%2520infographic%2520charts%252C%2520and%25203%2529%2520enabling%250Aexample-based%2520infographic%2520chart%2520generation.%2520By%2520capturing%2520the%2520visual%2520and%250Astructural%2520complexity%2520of%2520real%2520design%252C%2520ChartGalaxy%2520provides%2520a%2520useful%2520resource%250Afor%2520enhancing%2520multimodal%2520reasoning%2520and%2520generation%2520in%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18668v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartGalaxy%3A%20A%20Dataset%20for%20Infographic%20Chart%20Understanding%20and%0A%20%20Generation&entry.906535625=Zhen%20Li%20and%20Duan%20Li%20and%20Yukai%20Guo%20and%20Xinyuan%20Guo%20and%20Bowen%20Li%20and%20Lanxi%20Xiao%20and%20Shenyu%20Qiao%20and%20Jiashu%20Chen%20and%20Zijian%20Wu%20and%20Hui%20Zhang%20and%20Xinhuan%20Shu%20and%20Shixia%20Liu&entry.1292438233=%20%20Infographic%20charts%20are%20a%20powerful%20medium%20for%20communicating%20abstract%20data%20by%0Acombining%20visual%20elements%20%28e.g.%2C%20charts%2C%20images%29%20with%20textual%20information.%0AHowever%2C%20their%20visual%20and%20structural%20richness%20poses%20challenges%20for%20large%0Avision-language%20models%20%28LVLMs%29%2C%20which%20are%20typically%20trained%20on%20plain%20charts.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20ChartGalaxy%2C%20a%20million-scale%20dataset%20designed%20to%0Aadvance%20the%20understanding%20and%20generation%20of%20infographic%20charts.%20The%20dataset%20is%0Aconstructed%20through%20an%20inductive%20process%20that%20identifies%2075%20chart%20types%2C%20440%0Achart%20variations%2C%20and%2068%20layout%20templates%20from%20real%20infographic%20charts%20and%20uses%0Athem%20to%20create%20synthetic%20ones%20programmatically.%20We%20showcase%20the%20utility%20of%20this%0Adataset%20through%3A%201%29%20improving%20infographic%20chart%20understanding%20via%20fine-tuning%2C%0A2%29%20benchmarking%20code%20generation%20for%20infographic%20charts%2C%20and%203%29%20enabling%0Aexample-based%20infographic%20chart%20generation.%20By%20capturing%20the%20visual%20and%0Astructural%20complexity%20of%20real%20design%2C%20ChartGalaxy%20provides%20a%20useful%20resource%0Afor%20enhancing%20multimodal%20reasoning%20and%20generation%20in%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18668v4&entry.124074799=Read"},
{"title": "Advancing Natural Language Formalization to First Order Logic with\n  Fine-tuned LLMs", "author": "Felix Vossel and Till Mossakowski and Bj\u00f6rn Gehrke", "abstract": "  Automating the translation of natural language to first-order logic (FOL) is\ncrucial for knowledge representation and formal methods, yet remains\nchallenging. We present a systematic evaluation of fine-tuned LLMs for this\ntask, comparing architectures (encoder-decoder vs. decoder-only) and training\nstrategies. Using the MALLS and Willow datasets, we explore techniques like\nvocabulary extension, predicate conditioning, and multilingual training,\nintroducing metrics for exact match, logical equivalence, and predicate\nalignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate\nlists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT\nreasoning ability as well as symbolic systems like ccg2lambda. Key findings\nshow: (1) predicate availability boosts performance by 15-20%, (2) T5 models\nsurpass larger decoder-only LLMs, and (3) models generalize to unseen logical\narguments (FOLIO dataset) without specific training. While structural logic\ntranslation proves robust, predicate extraction emerges as the main bottleneck.\n", "link": "http://arxiv.org/abs/2509.22338v1", "date": "2025-09-26", "relevancy": 2.4553, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Natural%20Language%20Formalization%20to%20First%20Order%20Logic%20with%0A%20%20Fine-tuned%20LLMs&body=Title%3A%20Advancing%20Natural%20Language%20Formalization%20to%20First%20Order%20Logic%20with%0A%20%20Fine-tuned%20LLMs%0AAuthor%3A%20Felix%20Vossel%20and%20Till%20Mossakowski%20and%20Bj%C3%B6rn%20Gehrke%0AAbstract%3A%20%20%20Automating%20the%20translation%20of%20natural%20language%20to%20first-order%20logic%20%28FOL%29%20is%0Acrucial%20for%20knowledge%20representation%20and%20formal%20methods%2C%20yet%20remains%0Achallenging.%20We%20present%20a%20systematic%20evaluation%20of%20fine-tuned%20LLMs%20for%20this%0Atask%2C%20comparing%20architectures%20%28encoder-decoder%20vs.%20decoder-only%29%20and%20training%0Astrategies.%20Using%20the%20MALLS%20and%20Willow%20datasets%2C%20we%20explore%20techniques%20like%0Avocabulary%20extension%2C%20predicate%20conditioning%2C%20and%20multilingual%20training%2C%0Aintroducing%20metrics%20for%20exact%20match%2C%20logical%20equivalence%2C%20and%20predicate%0Aalignment.%20Our%20fine-tuned%20Flan-T5-XXL%20achieves%2070%25%20accuracy%20with%20predicate%0Alists%2C%20outperforming%20GPT-4o%20and%20even%20the%20DeepSeek-R1-0528%20model%20with%20CoT%0Areasoning%20ability%20as%20well%20as%20symbolic%20systems%20like%20ccg2lambda.%20Key%20findings%0Ashow%3A%20%281%29%20predicate%20availability%20boosts%20performance%20by%2015-20%25%2C%20%282%29%20T5%20models%0Asurpass%20larger%20decoder-only%20LLMs%2C%20and%20%283%29%20models%20generalize%20to%20unseen%20logical%0Aarguments%20%28FOLIO%20dataset%29%20without%20specific%20training.%20While%20structural%20logic%0Atranslation%20proves%20robust%2C%20predicate%20extraction%20emerges%20as%20the%20main%20bottleneck.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Natural%2520Language%2520Formalization%2520to%2520First%2520Order%2520Logic%2520with%250A%2520%2520Fine-tuned%2520LLMs%26entry.906535625%3DFelix%2520Vossel%2520and%2520Till%2520Mossakowski%2520and%2520Bj%25C3%25B6rn%2520Gehrke%26entry.1292438233%3D%2520%2520Automating%2520the%2520translation%2520of%2520natural%2520language%2520to%2520first-order%2520logic%2520%2528FOL%2529%2520is%250Acrucial%2520for%2520knowledge%2520representation%2520and%2520formal%2520methods%252C%2520yet%2520remains%250Achallenging.%2520We%2520present%2520a%2520systematic%2520evaluation%2520of%2520fine-tuned%2520LLMs%2520for%2520this%250Atask%252C%2520comparing%2520architectures%2520%2528encoder-decoder%2520vs.%2520decoder-only%2529%2520and%2520training%250Astrategies.%2520Using%2520the%2520MALLS%2520and%2520Willow%2520datasets%252C%2520we%2520explore%2520techniques%2520like%250Avocabulary%2520extension%252C%2520predicate%2520conditioning%252C%2520and%2520multilingual%2520training%252C%250Aintroducing%2520metrics%2520for%2520exact%2520match%252C%2520logical%2520equivalence%252C%2520and%2520predicate%250Aalignment.%2520Our%2520fine-tuned%2520Flan-T5-XXL%2520achieves%252070%2525%2520accuracy%2520with%2520predicate%250Alists%252C%2520outperforming%2520GPT-4o%2520and%2520even%2520the%2520DeepSeek-R1-0528%2520model%2520with%2520CoT%250Areasoning%2520ability%2520as%2520well%2520as%2520symbolic%2520systems%2520like%2520ccg2lambda.%2520Key%2520findings%250Ashow%253A%2520%25281%2529%2520predicate%2520availability%2520boosts%2520performance%2520by%252015-20%2525%252C%2520%25282%2529%2520T5%2520models%250Asurpass%2520larger%2520decoder-only%2520LLMs%252C%2520and%2520%25283%2529%2520models%2520generalize%2520to%2520unseen%2520logical%250Aarguments%2520%2528FOLIO%2520dataset%2529%2520without%2520specific%2520training.%2520While%2520structural%2520logic%250Atranslation%2520proves%2520robust%252C%2520predicate%2520extraction%2520emerges%2520as%2520the%2520main%2520bottleneck.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Natural%20Language%20Formalization%20to%20First%20Order%20Logic%20with%0A%20%20Fine-tuned%20LLMs&entry.906535625=Felix%20Vossel%20and%20Till%20Mossakowski%20and%20Bj%C3%B6rn%20Gehrke&entry.1292438233=%20%20Automating%20the%20translation%20of%20natural%20language%20to%20first-order%20logic%20%28FOL%29%20is%0Acrucial%20for%20knowledge%20representation%20and%20formal%20methods%2C%20yet%20remains%0Achallenging.%20We%20present%20a%20systematic%20evaluation%20of%20fine-tuned%20LLMs%20for%20this%0Atask%2C%20comparing%20architectures%20%28encoder-decoder%20vs.%20decoder-only%29%20and%20training%0Astrategies.%20Using%20the%20MALLS%20and%20Willow%20datasets%2C%20we%20explore%20techniques%20like%0Avocabulary%20extension%2C%20predicate%20conditioning%2C%20and%20multilingual%20training%2C%0Aintroducing%20metrics%20for%20exact%20match%2C%20logical%20equivalence%2C%20and%20predicate%0Aalignment.%20Our%20fine-tuned%20Flan-T5-XXL%20achieves%2070%25%20accuracy%20with%20predicate%0Alists%2C%20outperforming%20GPT-4o%20and%20even%20the%20DeepSeek-R1-0528%20model%20with%20CoT%0Areasoning%20ability%20as%20well%20as%20symbolic%20systems%20like%20ccg2lambda.%20Key%20findings%0Ashow%3A%20%281%29%20predicate%20availability%20boosts%20performance%20by%2015-20%25%2C%20%282%29%20T5%20models%0Asurpass%20larger%20decoder-only%20LLMs%2C%20and%20%283%29%20models%20generalize%20to%20unseen%20logical%0Aarguments%20%28FOLIO%20dataset%29%20without%20specific%20training.%20While%20structural%20logic%0Atranslation%20proves%20robust%2C%20predicate%20extraction%20emerges%20as%20the%20main%20bottleneck.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22338v1&entry.124074799=Read"},
{"title": "Integrating Background Knowledge in Medical Semantic Segmentation with\n  Logic Tensor Networks", "author": "Luca Bergamin and Giovanna Maria Dimitri and Fabio Aiolli", "abstract": "  Semantic segmentation is a fundamental task in medical image analysis, aiding\nmedical decision-making by helping radiologists distinguish objects in an\nimage. Research in this field has been driven by deep learning applications,\nwhich have the potential to scale these systems even in the presence of noise\nand artifacts. However, these systems are not yet perfected. We argue that\nperformance can be improved by incorporating common medical knowledge into the\nsegmentation model's loss function. To this end, we introduce Logic Tensor\nNetworks (LTNs) to encode medical background knowledge using first-order logic\n(FOL) rules. The encoded rules span from constraints on the shape of the\nproduced segmentation, to relationships between different segmented areas. We\napply LTNs in an end-to-end framework with a SwinUNETR for semantic\nsegmentation. We evaluate our method on the task of segmenting the hippocampus\nin brain MRI scans. Our experiments show that LTNs improve the baseline\nsegmentation performance, especially when training data is scarce. Despite\nbeing in its preliminary stages, we argue that neurosymbolic methods are\ngeneral enough to be adapted and applied to other medical semantic segmentation\ntasks.\n", "link": "http://arxiv.org/abs/2509.22399v1", "date": "2025-09-26", "relevancy": 2.451, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Background%20Knowledge%20in%20Medical%20Semantic%20Segmentation%20with%0A%20%20Logic%20Tensor%20Networks&body=Title%3A%20Integrating%20Background%20Knowledge%20in%20Medical%20Semantic%20Segmentation%20with%0A%20%20Logic%20Tensor%20Networks%0AAuthor%3A%20Luca%20Bergamin%20and%20Giovanna%20Maria%20Dimitri%20and%20Fabio%20Aiolli%0AAbstract%3A%20%20%20Semantic%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20image%20analysis%2C%20aiding%0Amedical%20decision-making%20by%20helping%20radiologists%20distinguish%20objects%20in%20an%0Aimage.%20Research%20in%20this%20field%20has%20been%20driven%20by%20deep%20learning%20applications%2C%0Awhich%20have%20the%20potential%20to%20scale%20these%20systems%20even%20in%20the%20presence%20of%20noise%0Aand%20artifacts.%20However%2C%20these%20systems%20are%20not%20yet%20perfected.%20We%20argue%20that%0Aperformance%20can%20be%20improved%20by%20incorporating%20common%20medical%20knowledge%20into%20the%0Asegmentation%20model%27s%20loss%20function.%20To%20this%20end%2C%20we%20introduce%20Logic%20Tensor%0ANetworks%20%28LTNs%29%20to%20encode%20medical%20background%20knowledge%20using%20first-order%20logic%0A%28FOL%29%20rules.%20The%20encoded%20rules%20span%20from%20constraints%20on%20the%20shape%20of%20the%0Aproduced%20segmentation%2C%20to%20relationships%20between%20different%20segmented%20areas.%20We%0Aapply%20LTNs%20in%20an%20end-to-end%20framework%20with%20a%20SwinUNETR%20for%20semantic%0Asegmentation.%20We%20evaluate%20our%20method%20on%20the%20task%20of%20segmenting%20the%20hippocampus%0Ain%20brain%20MRI%20scans.%20Our%20experiments%20show%20that%20LTNs%20improve%20the%20baseline%0Asegmentation%20performance%2C%20especially%20when%20training%20data%20is%20scarce.%20Despite%0Abeing%20in%20its%20preliminary%20stages%2C%20we%20argue%20that%20neurosymbolic%20methods%20are%0Ageneral%20enough%20to%20be%20adapted%20and%20applied%20to%20other%20medical%20semantic%20segmentation%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Background%2520Knowledge%2520in%2520Medical%2520Semantic%2520Segmentation%2520with%250A%2520%2520Logic%2520Tensor%2520Networks%26entry.906535625%3DLuca%2520Bergamin%2520and%2520Giovanna%2520Maria%2520Dimitri%2520and%2520Fabio%2520Aiolli%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520is%2520a%2520fundamental%2520task%2520in%2520medical%2520image%2520analysis%252C%2520aiding%250Amedical%2520decision-making%2520by%2520helping%2520radiologists%2520distinguish%2520objects%2520in%2520an%250Aimage.%2520Research%2520in%2520this%2520field%2520has%2520been%2520driven%2520by%2520deep%2520learning%2520applications%252C%250Awhich%2520have%2520the%2520potential%2520to%2520scale%2520these%2520systems%2520even%2520in%2520the%2520presence%2520of%2520noise%250Aand%2520artifacts.%2520However%252C%2520these%2520systems%2520are%2520not%2520yet%2520perfected.%2520We%2520argue%2520that%250Aperformance%2520can%2520be%2520improved%2520by%2520incorporating%2520common%2520medical%2520knowledge%2520into%2520the%250Asegmentation%2520model%2527s%2520loss%2520function.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Logic%2520Tensor%250ANetworks%2520%2528LTNs%2529%2520to%2520encode%2520medical%2520background%2520knowledge%2520using%2520first-order%2520logic%250A%2528FOL%2529%2520rules.%2520The%2520encoded%2520rules%2520span%2520from%2520constraints%2520on%2520the%2520shape%2520of%2520the%250Aproduced%2520segmentation%252C%2520to%2520relationships%2520between%2520different%2520segmented%2520areas.%2520We%250Aapply%2520LTNs%2520in%2520an%2520end-to-end%2520framework%2520with%2520a%2520SwinUNETR%2520for%2520semantic%250Asegmentation.%2520We%2520evaluate%2520our%2520method%2520on%2520the%2520task%2520of%2520segmenting%2520the%2520hippocampus%250Ain%2520brain%2520MRI%2520scans.%2520Our%2520experiments%2520show%2520that%2520LTNs%2520improve%2520the%2520baseline%250Asegmentation%2520performance%252C%2520especially%2520when%2520training%2520data%2520is%2520scarce.%2520Despite%250Abeing%2520in%2520its%2520preliminary%2520stages%252C%2520we%2520argue%2520that%2520neurosymbolic%2520methods%2520are%250Ageneral%2520enough%2520to%2520be%2520adapted%2520and%2520applied%2520to%2520other%2520medical%2520semantic%2520segmentation%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Background%20Knowledge%20in%20Medical%20Semantic%20Segmentation%20with%0A%20%20Logic%20Tensor%20Networks&entry.906535625=Luca%20Bergamin%20and%20Giovanna%20Maria%20Dimitri%20and%20Fabio%20Aiolli&entry.1292438233=%20%20Semantic%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20image%20analysis%2C%20aiding%0Amedical%20decision-making%20by%20helping%20radiologists%20distinguish%20objects%20in%20an%0Aimage.%20Research%20in%20this%20field%20has%20been%20driven%20by%20deep%20learning%20applications%2C%0Awhich%20have%20the%20potential%20to%20scale%20these%20systems%20even%20in%20the%20presence%20of%20noise%0Aand%20artifacts.%20However%2C%20these%20systems%20are%20not%20yet%20perfected.%20We%20argue%20that%0Aperformance%20can%20be%20improved%20by%20incorporating%20common%20medical%20knowledge%20into%20the%0Asegmentation%20model%27s%20loss%20function.%20To%20this%20end%2C%20we%20introduce%20Logic%20Tensor%0ANetworks%20%28LTNs%29%20to%20encode%20medical%20background%20knowledge%20using%20first-order%20logic%0A%28FOL%29%20rules.%20The%20encoded%20rules%20span%20from%20constraints%20on%20the%20shape%20of%20the%0Aproduced%20segmentation%2C%20to%20relationships%20between%20different%20segmented%20areas.%20We%0Aapply%20LTNs%20in%20an%20end-to-end%20framework%20with%20a%20SwinUNETR%20for%20semantic%0Asegmentation.%20We%20evaluate%20our%20method%20on%20the%20task%20of%20segmenting%20the%20hippocampus%0Ain%20brain%20MRI%20scans.%20Our%20experiments%20show%20that%20LTNs%20improve%20the%20baseline%0Asegmentation%20performance%2C%20especially%20when%20training%20data%20is%20scarce.%20Despite%0Abeing%20in%20its%20preliminary%20stages%2C%20we%20argue%20that%20neurosymbolic%20methods%20are%0Ageneral%20enough%20to%20be%20adapted%20and%20applied%20to%20other%20medical%20semantic%20segmentation%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22399v1&entry.124074799=Read"},
{"title": "The Lie of the Average: How Class Incremental Learning Evaluation\n  Deceives You?", "author": "Guannan Lai and Da-Wei Zhou and Xin Yang and Han-Jia Ye", "abstract": "  Class Incremental Learning (CIL) requires models to continuously learn new\nclasses without forgetting previously learned ones, while maintaining stable\nperformance across all possible class sequences. In real-world settings, the\norder in which classes arrive is diverse and unpredictable, and model\nperformance can vary substantially across different sequences. Yet mainstream\nevaluation protocols calculate mean and variance from only a small set of\nrandomly sampled sequences. Our theoretical analysis and empirical results\ndemonstrate that this sampling strategy fails to capture the full performance\nrange, resulting in biased mean estimates and a severe underestimation of the\ntrue variance in the performance distribution. We therefore contend that a\nrobust CIL evaluation protocol should accurately characterize and estimate the\nentire performance distribution. To this end, we introduce the concept of\nextreme sequences and provide theoretical justification for their crucial role\nin the reliable evaluation of CIL. Moreover, we observe a consistent positive\ncorrelation between inter-task similarity and model performance, a relation\nthat can be leveraged to guide the search for extreme sequences. Building on\nthese insights, we propose EDGE (Extreme case-based Distribution and\nGeneralization Evaluation), an evaluation protocol that adaptively identifies\nand samples extreme class sequences using inter-task similarity, offering a\ncloser approximation of the ground-truth performance distribution. Extensive\nexperiments demonstrate that EDGE effectively captures performance extremes and\nyields more accurate estimates of distributional boundaries, providing\nactionable insights for model selection and robustness checking. Our code is\navailable at https://github.com/AIGNLAI/EDGE.\n", "link": "http://arxiv.org/abs/2509.22580v1", "date": "2025-09-26", "relevancy": 2.4412, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4947}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4869}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Lie%20of%20the%20Average%3A%20How%20Class%20Incremental%20Learning%20Evaluation%0A%20%20Deceives%20You%3F&body=Title%3A%20The%20Lie%20of%20the%20Average%3A%20How%20Class%20Incremental%20Learning%20Evaluation%0A%20%20Deceives%20You%3F%0AAuthor%3A%20Guannan%20Lai%20and%20Da-Wei%20Zhou%20and%20Xin%20Yang%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Class%20Incremental%20Learning%20%28CIL%29%20requires%20models%20to%20continuously%20learn%20new%0Aclasses%20without%20forgetting%20previously%20learned%20ones%2C%20while%20maintaining%20stable%0Aperformance%20across%20all%20possible%20class%20sequences.%20In%20real-world%20settings%2C%20the%0Aorder%20in%20which%20classes%20arrive%20is%20diverse%20and%20unpredictable%2C%20and%20model%0Aperformance%20can%20vary%20substantially%20across%20different%20sequences.%20Yet%20mainstream%0Aevaluation%20protocols%20calculate%20mean%20and%20variance%20from%20only%20a%20small%20set%20of%0Arandomly%20sampled%20sequences.%20Our%20theoretical%20analysis%20and%20empirical%20results%0Ademonstrate%20that%20this%20sampling%20strategy%20fails%20to%20capture%20the%20full%20performance%0Arange%2C%20resulting%20in%20biased%20mean%20estimates%20and%20a%20severe%20underestimation%20of%20the%0Atrue%20variance%20in%20the%20performance%20distribution.%20We%20therefore%20contend%20that%20a%0Arobust%20CIL%20evaluation%20protocol%20should%20accurately%20characterize%20and%20estimate%20the%0Aentire%20performance%20distribution.%20To%20this%20end%2C%20we%20introduce%20the%20concept%20of%0Aextreme%20sequences%20and%20provide%20theoretical%20justification%20for%20their%20crucial%20role%0Ain%20the%20reliable%20evaluation%20of%20CIL.%20Moreover%2C%20we%20observe%20a%20consistent%20positive%0Acorrelation%20between%20inter-task%20similarity%20and%20model%20performance%2C%20a%20relation%0Athat%20can%20be%20leveraged%20to%20guide%20the%20search%20for%20extreme%20sequences.%20Building%20on%0Athese%20insights%2C%20we%20propose%20EDGE%20%28Extreme%20case-based%20Distribution%20and%0AGeneralization%20Evaluation%29%2C%20an%20evaluation%20protocol%20that%20adaptively%20identifies%0Aand%20samples%20extreme%20class%20sequences%20using%20inter-task%20similarity%2C%20offering%20a%0Acloser%20approximation%20of%20the%20ground-truth%20performance%20distribution.%20Extensive%0Aexperiments%20demonstrate%20that%20EDGE%20effectively%20captures%20performance%20extremes%20and%0Ayields%20more%20accurate%20estimates%20of%20distributional%20boundaries%2C%20providing%0Aactionable%20insights%20for%20model%20selection%20and%20robustness%20checking.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AIGNLAI/EDGE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Lie%2520of%2520the%2520Average%253A%2520How%2520Class%2520Incremental%2520Learning%2520Evaluation%250A%2520%2520Deceives%2520You%253F%26entry.906535625%3DGuannan%2520Lai%2520and%2520Da-Wei%2520Zhou%2520and%2520Xin%2520Yang%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520Class%2520Incremental%2520Learning%2520%2528CIL%2529%2520requires%2520models%2520to%2520continuously%2520learn%2520new%250Aclasses%2520without%2520forgetting%2520previously%2520learned%2520ones%252C%2520while%2520maintaining%2520stable%250Aperformance%2520across%2520all%2520possible%2520class%2520sequences.%2520In%2520real-world%2520settings%252C%2520the%250Aorder%2520in%2520which%2520classes%2520arrive%2520is%2520diverse%2520and%2520unpredictable%252C%2520and%2520model%250Aperformance%2520can%2520vary%2520substantially%2520across%2520different%2520sequences.%2520Yet%2520mainstream%250Aevaluation%2520protocols%2520calculate%2520mean%2520and%2520variance%2520from%2520only%2520a%2520small%2520set%2520of%250Arandomly%2520sampled%2520sequences.%2520Our%2520theoretical%2520analysis%2520and%2520empirical%2520results%250Ademonstrate%2520that%2520this%2520sampling%2520strategy%2520fails%2520to%2520capture%2520the%2520full%2520performance%250Arange%252C%2520resulting%2520in%2520biased%2520mean%2520estimates%2520and%2520a%2520severe%2520underestimation%2520of%2520the%250Atrue%2520variance%2520in%2520the%2520performance%2520distribution.%2520We%2520therefore%2520contend%2520that%2520a%250Arobust%2520CIL%2520evaluation%2520protocol%2520should%2520accurately%2520characterize%2520and%2520estimate%2520the%250Aentire%2520performance%2520distribution.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520concept%2520of%250Aextreme%2520sequences%2520and%2520provide%2520theoretical%2520justification%2520for%2520their%2520crucial%2520role%250Ain%2520the%2520reliable%2520evaluation%2520of%2520CIL.%2520Moreover%252C%2520we%2520observe%2520a%2520consistent%2520positive%250Acorrelation%2520between%2520inter-task%2520similarity%2520and%2520model%2520performance%252C%2520a%2520relation%250Athat%2520can%2520be%2520leveraged%2520to%2520guide%2520the%2520search%2520for%2520extreme%2520sequences.%2520Building%2520on%250Athese%2520insights%252C%2520we%2520propose%2520EDGE%2520%2528Extreme%2520case-based%2520Distribution%2520and%250AGeneralization%2520Evaluation%2529%252C%2520an%2520evaluation%2520protocol%2520that%2520adaptively%2520identifies%250Aand%2520samples%2520extreme%2520class%2520sequences%2520using%2520inter-task%2520similarity%252C%2520offering%2520a%250Acloser%2520approximation%2520of%2520the%2520ground-truth%2520performance%2520distribution.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520EDGE%2520effectively%2520captures%2520performance%2520extremes%2520and%250Ayields%2520more%2520accurate%2520estimates%2520of%2520distributional%2520boundaries%252C%2520providing%250Aactionable%2520insights%2520for%2520model%2520selection%2520and%2520robustness%2520checking.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/AIGNLAI/EDGE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Lie%20of%20the%20Average%3A%20How%20Class%20Incremental%20Learning%20Evaluation%0A%20%20Deceives%20You%3F&entry.906535625=Guannan%20Lai%20and%20Da-Wei%20Zhou%20and%20Xin%20Yang%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Class%20Incremental%20Learning%20%28CIL%29%20requires%20models%20to%20continuously%20learn%20new%0Aclasses%20without%20forgetting%20previously%20learned%20ones%2C%20while%20maintaining%20stable%0Aperformance%20across%20all%20possible%20class%20sequences.%20In%20real-world%20settings%2C%20the%0Aorder%20in%20which%20classes%20arrive%20is%20diverse%20and%20unpredictable%2C%20and%20model%0Aperformance%20can%20vary%20substantially%20across%20different%20sequences.%20Yet%20mainstream%0Aevaluation%20protocols%20calculate%20mean%20and%20variance%20from%20only%20a%20small%20set%20of%0Arandomly%20sampled%20sequences.%20Our%20theoretical%20analysis%20and%20empirical%20results%0Ademonstrate%20that%20this%20sampling%20strategy%20fails%20to%20capture%20the%20full%20performance%0Arange%2C%20resulting%20in%20biased%20mean%20estimates%20and%20a%20severe%20underestimation%20of%20the%0Atrue%20variance%20in%20the%20performance%20distribution.%20We%20therefore%20contend%20that%20a%0Arobust%20CIL%20evaluation%20protocol%20should%20accurately%20characterize%20and%20estimate%20the%0Aentire%20performance%20distribution.%20To%20this%20end%2C%20we%20introduce%20the%20concept%20of%0Aextreme%20sequences%20and%20provide%20theoretical%20justification%20for%20their%20crucial%20role%0Ain%20the%20reliable%20evaluation%20of%20CIL.%20Moreover%2C%20we%20observe%20a%20consistent%20positive%0Acorrelation%20between%20inter-task%20similarity%20and%20model%20performance%2C%20a%20relation%0Athat%20can%20be%20leveraged%20to%20guide%20the%20search%20for%20extreme%20sequences.%20Building%20on%0Athese%20insights%2C%20we%20propose%20EDGE%20%28Extreme%20case-based%20Distribution%20and%0AGeneralization%20Evaluation%29%2C%20an%20evaluation%20protocol%20that%20adaptively%20identifies%0Aand%20samples%20extreme%20class%20sequences%20using%20inter-task%20similarity%2C%20offering%20a%0Acloser%20approximation%20of%20the%20ground-truth%20performance%20distribution.%20Extensive%0Aexperiments%20demonstrate%20that%20EDGE%20effectively%20captures%20performance%20extremes%20and%0Ayields%20more%20accurate%20estimates%20of%20distributional%20boundaries%2C%20providing%0Aactionable%20insights%20for%20model%20selection%20and%20robustness%20checking.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AIGNLAI/EDGE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22580v1&entry.124074799=Read"},
{"title": "Learnable Kernel Density Estimation for Graphs", "author": "Xudong Wang and Ziheng Sun and Chris Ding and Jicong Fan", "abstract": "  This work proposes a framework LGKDE that learns kernel density estimation\nfor graphs. The key challenge in graph density estimation lies in effectively\ncapturing both structural patterns and semantic variations while maintaining\ntheoretical guarantees. Combining graph kernels and kernel density estimation\n(KDE) is a standard approach to graph density estimation, but has\nunsatisfactory performance due to the handcrafted and fixed features of\nkernels. Our method LGKDE leverages graph neural networks to represent each\ngraph as a discrete distribution and utilizes maximum mean discrepancy to learn\nthe graph metric for multi-scale KDE, where all parameters are learned by\nmaximizing the density of graphs relative to the density of their well-designed\nperturbed counterparts. The perturbations are conducted on both node features\nand graph spectra, which helps better characterize the boundary of normal\ndensity regions. Theoretically, we establish consistency and convergence\nguarantees for LGKDE, including bounds on the mean integrated squared error,\nrobustness, and generalization. We validate LGKDE by demonstrating its\neffectiveness in recovering the underlying density of synthetic graph\ndistributions and applying it to graph anomaly detection across diverse\nbenchmark datasets. Extensive empirical evaluation shows that LGKDE\ndemonstrates superior performance compared to state-of-the-art baselines on\nmost benchmark datasets.\n", "link": "http://arxiv.org/abs/2505.21285v3", "date": "2025-09-26", "relevancy": 2.441, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5177}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4767}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Kernel%20Density%20Estimation%20for%20Graphs&body=Title%3A%20Learnable%20Kernel%20Density%20Estimation%20for%20Graphs%0AAuthor%3A%20Xudong%20Wang%20and%20Ziheng%20Sun%20and%20Chris%20Ding%20and%20Jicong%20Fan%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20framework%20LGKDE%20that%20learns%20kernel%20density%20estimation%0Afor%20graphs.%20The%20key%20challenge%20in%20graph%20density%20estimation%20lies%20in%20effectively%0Acapturing%20both%20structural%20patterns%20and%20semantic%20variations%20while%20maintaining%0Atheoretical%20guarantees.%20Combining%20graph%20kernels%20and%20kernel%20density%20estimation%0A%28KDE%29%20is%20a%20standard%20approach%20to%20graph%20density%20estimation%2C%20but%20has%0Aunsatisfactory%20performance%20due%20to%20the%20handcrafted%20and%20fixed%20features%20of%0Akernels.%20Our%20method%20LGKDE%20leverages%20graph%20neural%20networks%20to%20represent%20each%0Agraph%20as%20a%20discrete%20distribution%20and%20utilizes%20maximum%20mean%20discrepancy%20to%20learn%0Athe%20graph%20metric%20for%20multi-scale%20KDE%2C%20where%20all%20parameters%20are%20learned%20by%0Amaximizing%20the%20density%20of%20graphs%20relative%20to%20the%20density%20of%20their%20well-designed%0Aperturbed%20counterparts.%20The%20perturbations%20are%20conducted%20on%20both%20node%20features%0Aand%20graph%20spectra%2C%20which%20helps%20better%20characterize%20the%20boundary%20of%20normal%0Adensity%20regions.%20Theoretically%2C%20we%20establish%20consistency%20and%20convergence%0Aguarantees%20for%20LGKDE%2C%20including%20bounds%20on%20the%20mean%20integrated%20squared%20error%2C%0Arobustness%2C%20and%20generalization.%20We%20validate%20LGKDE%20by%20demonstrating%20its%0Aeffectiveness%20in%20recovering%20the%20underlying%20density%20of%20synthetic%20graph%0Adistributions%20and%20applying%20it%20to%20graph%20anomaly%20detection%20across%20diverse%0Abenchmark%20datasets.%20Extensive%20empirical%20evaluation%20shows%20that%20LGKDE%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Amost%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21285v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Kernel%2520Density%2520Estimation%2520for%2520Graphs%26entry.906535625%3DXudong%2520Wang%2520and%2520Ziheng%2520Sun%2520and%2520Chris%2520Ding%2520and%2520Jicong%2520Fan%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520framework%2520LGKDE%2520that%2520learns%2520kernel%2520density%2520estimation%250Afor%2520graphs.%2520The%2520key%2520challenge%2520in%2520graph%2520density%2520estimation%2520lies%2520in%2520effectively%250Acapturing%2520both%2520structural%2520patterns%2520and%2520semantic%2520variations%2520while%2520maintaining%250Atheoretical%2520guarantees.%2520Combining%2520graph%2520kernels%2520and%2520kernel%2520density%2520estimation%250A%2528KDE%2529%2520is%2520a%2520standard%2520approach%2520to%2520graph%2520density%2520estimation%252C%2520but%2520has%250Aunsatisfactory%2520performance%2520due%2520to%2520the%2520handcrafted%2520and%2520fixed%2520features%2520of%250Akernels.%2520Our%2520method%2520LGKDE%2520leverages%2520graph%2520neural%2520networks%2520to%2520represent%2520each%250Agraph%2520as%2520a%2520discrete%2520distribution%2520and%2520utilizes%2520maximum%2520mean%2520discrepancy%2520to%2520learn%250Athe%2520graph%2520metric%2520for%2520multi-scale%2520KDE%252C%2520where%2520all%2520parameters%2520are%2520learned%2520by%250Amaximizing%2520the%2520density%2520of%2520graphs%2520relative%2520to%2520the%2520density%2520of%2520their%2520well-designed%250Aperturbed%2520counterparts.%2520The%2520perturbations%2520are%2520conducted%2520on%2520both%2520node%2520features%250Aand%2520graph%2520spectra%252C%2520which%2520helps%2520better%2520characterize%2520the%2520boundary%2520of%2520normal%250Adensity%2520regions.%2520Theoretically%252C%2520we%2520establish%2520consistency%2520and%2520convergence%250Aguarantees%2520for%2520LGKDE%252C%2520including%2520bounds%2520on%2520the%2520mean%2520integrated%2520squared%2520error%252C%250Arobustness%252C%2520and%2520generalization.%2520We%2520validate%2520LGKDE%2520by%2520demonstrating%2520its%250Aeffectiveness%2520in%2520recovering%2520the%2520underlying%2520density%2520of%2520synthetic%2520graph%250Adistributions%2520and%2520applying%2520it%2520to%2520graph%2520anomaly%2520detection%2520across%2520diverse%250Abenchmark%2520datasets.%2520Extensive%2520empirical%2520evaluation%2520shows%2520that%2520LGKDE%250Ademonstrates%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520baselines%2520on%250Amost%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21285v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Kernel%20Density%20Estimation%20for%20Graphs&entry.906535625=Xudong%20Wang%20and%20Ziheng%20Sun%20and%20Chris%20Ding%20and%20Jicong%20Fan&entry.1292438233=%20%20This%20work%20proposes%20a%20framework%20LGKDE%20that%20learns%20kernel%20density%20estimation%0Afor%20graphs.%20The%20key%20challenge%20in%20graph%20density%20estimation%20lies%20in%20effectively%0Acapturing%20both%20structural%20patterns%20and%20semantic%20variations%20while%20maintaining%0Atheoretical%20guarantees.%20Combining%20graph%20kernels%20and%20kernel%20density%20estimation%0A%28KDE%29%20is%20a%20standard%20approach%20to%20graph%20density%20estimation%2C%20but%20has%0Aunsatisfactory%20performance%20due%20to%20the%20handcrafted%20and%20fixed%20features%20of%0Akernels.%20Our%20method%20LGKDE%20leverages%20graph%20neural%20networks%20to%20represent%20each%0Agraph%20as%20a%20discrete%20distribution%20and%20utilizes%20maximum%20mean%20discrepancy%20to%20learn%0Athe%20graph%20metric%20for%20multi-scale%20KDE%2C%20where%20all%20parameters%20are%20learned%20by%0Amaximizing%20the%20density%20of%20graphs%20relative%20to%20the%20density%20of%20their%20well-designed%0Aperturbed%20counterparts.%20The%20perturbations%20are%20conducted%20on%20both%20node%20features%0Aand%20graph%20spectra%2C%20which%20helps%20better%20characterize%20the%20boundary%20of%20normal%0Adensity%20regions.%20Theoretically%2C%20we%20establish%20consistency%20and%20convergence%0Aguarantees%20for%20LGKDE%2C%20including%20bounds%20on%20the%20mean%20integrated%20squared%20error%2C%0Arobustness%2C%20and%20generalization.%20We%20validate%20LGKDE%20by%20demonstrating%20its%0Aeffectiveness%20in%20recovering%20the%20underlying%20density%20of%20synthetic%20graph%0Adistributions%20and%20applying%20it%20to%20graph%20anomaly%20detection%20across%20diverse%0Abenchmark%20datasets.%20Extensive%20empirical%20evaluation%20shows%20that%20LGKDE%0Ademonstrates%20superior%20performance%20compared%20to%20state-of-the-art%20baselines%20on%0Amost%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21285v3&entry.124074799=Read"},
{"title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual\n  Reasoning", "author": "Jaedong Hwang and Kumar Tanmay and Seok-Jin Lee and Ayush Agrawal and Hamid Palangi and Kumar Ayush and Ila Fiete and Paul Pu Liang", "abstract": "  Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual question answering, and code generation, yet their ability\nto reason on these tasks in different languages remains underdeveloped.\nEspecially for low-resource languages such as Swahili or Thai, LLMs can often\nmisinterpret prompts or default to reasoning in English. This implicit bias\ntoward high-resource languages undermines factual accuracy, interpretability,\nand trust. We propose M2A, a novel method that combines multi-scale\nmultilingual alignment with language-consistency rewards on machine-translated\nquestions, training models to reason directly and accurately in the target\nlanguage. Furthermore, existing multilingual benchmarks only evaluate on final\nanswers, overlooking whether reasoning occurs in the intended language. To\nclose this gap, we introduce GeoFact-X, a geography-based multilingual factual\nreasoning benchmark together with reasoning traces in five languages: English,\nHindi, Japanese, Swahili, and Thai. Our results show that M2A significantly\nenhances multilingual reasoning fidelity in both mathematical and factual\nreasoning tasks, highlighting that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/M2A_GeoFact-X\n", "link": "http://arxiv.org/abs/2507.05418v2", "date": "2025-09-26", "relevancy": 2.4389, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20Globally%2C%20Speak%20Locally%3A%20Bridging%20the%20Gaps%20in%20Multilingual%0A%20%20Reasoning&body=Title%3A%20Learn%20Globally%2C%20Speak%20Locally%3A%20Bridging%20the%20Gaps%20in%20Multilingual%0A%20%20Reasoning%0AAuthor%3A%20Jaedong%20Hwang%20and%20Kumar%20Tanmay%20and%20Seok-Jin%20Lee%20and%20Ayush%20Agrawal%20and%20Hamid%20Palangi%20and%20Kumar%20Ayush%20and%20Ila%20Fiete%20and%20Paul%20Pu%20Liang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20strong%20performance%20in%20domains%20like%0Amathematics%2C%20factual%20question%20answering%2C%20and%20code%20generation%2C%20yet%20their%20ability%0Ato%20reason%20on%20these%20tasks%20in%20different%20languages%20remains%20underdeveloped.%0AEspecially%20for%20low-resource%20languages%20such%20as%20Swahili%20or%20Thai%2C%20LLMs%20can%20often%0Amisinterpret%20prompts%20or%20default%20to%20reasoning%20in%20English.%20This%20implicit%20bias%0Atoward%20high-resource%20languages%20undermines%20factual%20accuracy%2C%20interpretability%2C%0Aand%20trust.%20We%20propose%20M2A%2C%20a%20novel%20method%20that%20combines%20multi-scale%0Amultilingual%20alignment%20with%20language-consistency%20rewards%20on%20machine-translated%0Aquestions%2C%20training%20models%20to%20reason%20directly%20and%20accurately%20in%20the%20target%0Alanguage.%20Furthermore%2C%20existing%20multilingual%20benchmarks%20only%20evaluate%20on%20final%0Aanswers%2C%20overlooking%20whether%20reasoning%20occurs%20in%20the%20intended%20language.%20To%0Aclose%20this%20gap%2C%20we%20introduce%20GeoFact-X%2C%20a%20geography-based%20multilingual%20factual%0Areasoning%20benchmark%20together%20with%20reasoning%20traces%20in%20five%20languages%3A%20English%2C%0AHindi%2C%20Japanese%2C%20Swahili%2C%20and%20Thai.%20Our%20results%20show%20that%20M2A%20significantly%0Aenhances%20multilingual%20reasoning%20fidelity%20in%20both%20mathematical%20and%20factual%0Areasoning%20tasks%2C%20highlighting%20that%20reasoning-aware%20multilingual%20reinforcement%0Alearning%20is%20crucial%20for%20robust%20cross-lingual%20generalization.%0Ahttps%3A//jd730.github.io/projects/M2A_GeoFact-X%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05418v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520Globally%252C%2520Speak%2520Locally%253A%2520Bridging%2520the%2520Gaps%2520in%2520Multilingual%250A%2520%2520Reasoning%26entry.906535625%3DJaedong%2520Hwang%2520and%2520Kumar%2520Tanmay%2520and%2520Seok-Jin%2520Lee%2520and%2520Ayush%2520Agrawal%2520and%2520Hamid%2520Palangi%2520and%2520Kumar%2520Ayush%2520and%2520Ila%2520Fiete%2520and%2520Paul%2520Pu%2520Liang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520strong%2520performance%2520in%2520domains%2520like%250Amathematics%252C%2520factual%2520question%2520answering%252C%2520and%2520code%2520generation%252C%2520yet%2520their%2520ability%250Ato%2520reason%2520on%2520these%2520tasks%2520in%2520different%2520languages%2520remains%2520underdeveloped.%250AEspecially%2520for%2520low-resource%2520languages%2520such%2520as%2520Swahili%2520or%2520Thai%252C%2520LLMs%2520can%2520often%250Amisinterpret%2520prompts%2520or%2520default%2520to%2520reasoning%2520in%2520English.%2520This%2520implicit%2520bias%250Atoward%2520high-resource%2520languages%2520undermines%2520factual%2520accuracy%252C%2520interpretability%252C%250Aand%2520trust.%2520We%2520propose%2520M2A%252C%2520a%2520novel%2520method%2520that%2520combines%2520multi-scale%250Amultilingual%2520alignment%2520with%2520language-consistency%2520rewards%2520on%2520machine-translated%250Aquestions%252C%2520training%2520models%2520to%2520reason%2520directly%2520and%2520accurately%2520in%2520the%2520target%250Alanguage.%2520Furthermore%252C%2520existing%2520multilingual%2520benchmarks%2520only%2520evaluate%2520on%2520final%250Aanswers%252C%2520overlooking%2520whether%2520reasoning%2520occurs%2520in%2520the%2520intended%2520language.%2520To%250Aclose%2520this%2520gap%252C%2520we%2520introduce%2520GeoFact-X%252C%2520a%2520geography-based%2520multilingual%2520factual%250Areasoning%2520benchmark%2520together%2520with%2520reasoning%2520traces%2520in%2520five%2520languages%253A%2520English%252C%250AHindi%252C%2520Japanese%252C%2520Swahili%252C%2520and%2520Thai.%2520Our%2520results%2520show%2520that%2520M2A%2520significantly%250Aenhances%2520multilingual%2520reasoning%2520fidelity%2520in%2520both%2520mathematical%2520and%2520factual%250Areasoning%2520tasks%252C%2520highlighting%2520that%2520reasoning-aware%2520multilingual%2520reinforcement%250Alearning%2520is%2520crucial%2520for%2520robust%2520cross-lingual%2520generalization.%250Ahttps%253A//jd730.github.io/projects/M2A_GeoFact-X%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05418v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20Globally%2C%20Speak%20Locally%3A%20Bridging%20the%20Gaps%20in%20Multilingual%0A%20%20Reasoning&entry.906535625=Jaedong%20Hwang%20and%20Kumar%20Tanmay%20and%20Seok-Jin%20Lee%20and%20Ayush%20Agrawal%20and%20Hamid%20Palangi%20and%20Kumar%20Ayush%20and%20Ila%20Fiete%20and%20Paul%20Pu%20Liang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20strong%20performance%20in%20domains%20like%0Amathematics%2C%20factual%20question%20answering%2C%20and%20code%20generation%2C%20yet%20their%20ability%0Ato%20reason%20on%20these%20tasks%20in%20different%20languages%20remains%20underdeveloped.%0AEspecially%20for%20low-resource%20languages%20such%20as%20Swahili%20or%20Thai%2C%20LLMs%20can%20often%0Amisinterpret%20prompts%20or%20default%20to%20reasoning%20in%20English.%20This%20implicit%20bias%0Atoward%20high-resource%20languages%20undermines%20factual%20accuracy%2C%20interpretability%2C%0Aand%20trust.%20We%20propose%20M2A%2C%20a%20novel%20method%20that%20combines%20multi-scale%0Amultilingual%20alignment%20with%20language-consistency%20rewards%20on%20machine-translated%0Aquestions%2C%20training%20models%20to%20reason%20directly%20and%20accurately%20in%20the%20target%0Alanguage.%20Furthermore%2C%20existing%20multilingual%20benchmarks%20only%20evaluate%20on%20final%0Aanswers%2C%20overlooking%20whether%20reasoning%20occurs%20in%20the%20intended%20language.%20To%0Aclose%20this%20gap%2C%20we%20introduce%20GeoFact-X%2C%20a%20geography-based%20multilingual%20factual%0Areasoning%20benchmark%20together%20with%20reasoning%20traces%20in%20five%20languages%3A%20English%2C%0AHindi%2C%20Japanese%2C%20Swahili%2C%20and%20Thai.%20Our%20results%20show%20that%20M2A%20significantly%0Aenhances%20multilingual%20reasoning%20fidelity%20in%20both%20mathematical%20and%20factual%0Areasoning%20tasks%2C%20highlighting%20that%20reasoning-aware%20multilingual%20reinforcement%0Alearning%20is%20crucial%20for%20robust%20cross-lingual%20generalization.%0Ahttps%3A//jd730.github.io/projects/M2A_GeoFact-X%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05418v2&entry.124074799=Read"},
{"title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in\n  Open Scenes", "author": "Katrina Ashton and Chahyon Ku and Shrey Shah and Wen Jiang and Kostas Daniilidis and Bernadette Bucher", "abstract": "  Language-specified mobile manipulation tasks in novel environments\nsimultaneously face challenges interacting with a scene which is only partially\nobserved, grounding semantic information from language instructions to the\npartially observed scene, and actively updating knowledge of the scene with new\nobservations. To address these challenges, we propose HELIOS, a hierarchical\nscene representation and associated search objective to perform language\nspecified pick and place mobile manipulation tasks. We construct 2D maps\ncontaining the relevant semantic and occupancy information for navigation while\nsimultaneously actively constructing 3D Gaussian representations of\ntask-relevant objects. We fuse observations across this multi-layered\nrepresentation while explicitly modeling the multi-view consistency of the\ndetections of each object. In order to efficiently search for the target\nobject, we formulate an objective function balancing exploration of unobserved\nor uncertain regions with exploitation of scene semantic information. We\nevaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and\nplace benchmark in which perception is challenging due to large and complex\nscenes with comparatively small target objects. HELIOS achieves\nstate-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also\ntransfer to the real world without requiring additional data, as we illustrate\nby demonstrating it in a real world office environment on a Spot robot.\n", "link": "http://arxiv.org/abs/2509.22498v1", "date": "2025-09-26", "relevancy": 2.4332, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HELIOS%3A%20Hierarchical%20Exploration%20for%20Language-grounded%20Interaction%20in%0A%20%20Open%20Scenes&body=Title%3A%20HELIOS%3A%20Hierarchical%20Exploration%20for%20Language-grounded%20Interaction%20in%0A%20%20Open%20Scenes%0AAuthor%3A%20Katrina%20Ashton%20and%20Chahyon%20Ku%20and%20Shrey%20Shah%20and%20Wen%20Jiang%20and%20Kostas%20Daniilidis%20and%20Bernadette%20Bucher%0AAbstract%3A%20%20%20Language-specified%20mobile%20manipulation%20tasks%20in%20novel%20environments%0Asimultaneously%20face%20challenges%20interacting%20with%20a%20scene%20which%20is%20only%20partially%0Aobserved%2C%20grounding%20semantic%20information%20from%20language%20instructions%20to%20the%0Apartially%20observed%20scene%2C%20and%20actively%20updating%20knowledge%20of%20the%20scene%20with%20new%0Aobservations.%20To%20address%20these%20challenges%2C%20we%20propose%20HELIOS%2C%20a%20hierarchical%0Ascene%20representation%20and%20associated%20search%20objective%20to%20perform%20language%0Aspecified%20pick%20and%20place%20mobile%20manipulation%20tasks.%20We%20construct%202D%20maps%0Acontaining%20the%20relevant%20semantic%20and%20occupancy%20information%20for%20navigation%20while%0Asimultaneously%20actively%20constructing%203D%20Gaussian%20representations%20of%0Atask-relevant%20objects.%20We%20fuse%20observations%20across%20this%20multi-layered%0Arepresentation%20while%20explicitly%20modeling%20the%20multi-view%20consistency%20of%20the%0Adetections%20of%20each%20object.%20In%20order%20to%20efficiently%20search%20for%20the%20target%0Aobject%2C%20we%20formulate%20an%20objective%20function%20balancing%20exploration%20of%20unobserved%0Aor%20uncertain%20regions%20with%20exploitation%20of%20scene%20semantic%20information.%20We%0Aevaluate%20HELIOS%20on%20the%20OVMM%20benchmark%20in%20the%20Habitat%20simulator%2C%20a%20pick%20and%0Aplace%20benchmark%20in%20which%20perception%20is%20challenging%20due%20to%20large%20and%20complex%0Ascenes%20with%20comparatively%20small%20target%20objects.%20HELIOS%20achieves%0Astate-of-the-art%20results%20on%20OVMM.%20As%20our%20approach%20is%20zero-shot%2C%20HELIOS%20can%20also%0Atransfer%20to%20the%20real%20world%20without%20requiring%20additional%20data%2C%20as%20we%20illustrate%0Aby%20demonstrating%20it%20in%20a%20real%20world%20office%20environment%20on%20a%20Spot%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHELIOS%253A%2520Hierarchical%2520Exploration%2520for%2520Language-grounded%2520Interaction%2520in%250A%2520%2520Open%2520Scenes%26entry.906535625%3DKatrina%2520Ashton%2520and%2520Chahyon%2520Ku%2520and%2520Shrey%2520Shah%2520and%2520Wen%2520Jiang%2520and%2520Kostas%2520Daniilidis%2520and%2520Bernadette%2520Bucher%26entry.1292438233%3D%2520%2520Language-specified%2520mobile%2520manipulation%2520tasks%2520in%2520novel%2520environments%250Asimultaneously%2520face%2520challenges%2520interacting%2520with%2520a%2520scene%2520which%2520is%2520only%2520partially%250Aobserved%252C%2520grounding%2520semantic%2520information%2520from%2520language%2520instructions%2520to%2520the%250Apartially%2520observed%2520scene%252C%2520and%2520actively%2520updating%2520knowledge%2520of%2520the%2520scene%2520with%2520new%250Aobservations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520HELIOS%252C%2520a%2520hierarchical%250Ascene%2520representation%2520and%2520associated%2520search%2520objective%2520to%2520perform%2520language%250Aspecified%2520pick%2520and%2520place%2520mobile%2520manipulation%2520tasks.%2520We%2520construct%25202D%2520maps%250Acontaining%2520the%2520relevant%2520semantic%2520and%2520occupancy%2520information%2520for%2520navigation%2520while%250Asimultaneously%2520actively%2520constructing%25203D%2520Gaussian%2520representations%2520of%250Atask-relevant%2520objects.%2520We%2520fuse%2520observations%2520across%2520this%2520multi-layered%250Arepresentation%2520while%2520explicitly%2520modeling%2520the%2520multi-view%2520consistency%2520of%2520the%250Adetections%2520of%2520each%2520object.%2520In%2520order%2520to%2520efficiently%2520search%2520for%2520the%2520target%250Aobject%252C%2520we%2520formulate%2520an%2520objective%2520function%2520balancing%2520exploration%2520of%2520unobserved%250Aor%2520uncertain%2520regions%2520with%2520exploitation%2520of%2520scene%2520semantic%2520information.%2520We%250Aevaluate%2520HELIOS%2520on%2520the%2520OVMM%2520benchmark%2520in%2520the%2520Habitat%2520simulator%252C%2520a%2520pick%2520and%250Aplace%2520benchmark%2520in%2520which%2520perception%2520is%2520challenging%2520due%2520to%2520large%2520and%2520complex%250Ascenes%2520with%2520comparatively%2520small%2520target%2520objects.%2520HELIOS%2520achieves%250Astate-of-the-art%2520results%2520on%2520OVMM.%2520As%2520our%2520approach%2520is%2520zero-shot%252C%2520HELIOS%2520can%2520also%250Atransfer%2520to%2520the%2520real%2520world%2520without%2520requiring%2520additional%2520data%252C%2520as%2520we%2520illustrate%250Aby%2520demonstrating%2520it%2520in%2520a%2520real%2520world%2520office%2520environment%2520on%2520a%2520Spot%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HELIOS%3A%20Hierarchical%20Exploration%20for%20Language-grounded%20Interaction%20in%0A%20%20Open%20Scenes&entry.906535625=Katrina%20Ashton%20and%20Chahyon%20Ku%20and%20Shrey%20Shah%20and%20Wen%20Jiang%20and%20Kostas%20Daniilidis%20and%20Bernadette%20Bucher&entry.1292438233=%20%20Language-specified%20mobile%20manipulation%20tasks%20in%20novel%20environments%0Asimultaneously%20face%20challenges%20interacting%20with%20a%20scene%20which%20is%20only%20partially%0Aobserved%2C%20grounding%20semantic%20information%20from%20language%20instructions%20to%20the%0Apartially%20observed%20scene%2C%20and%20actively%20updating%20knowledge%20of%20the%20scene%20with%20new%0Aobservations.%20To%20address%20these%20challenges%2C%20we%20propose%20HELIOS%2C%20a%20hierarchical%0Ascene%20representation%20and%20associated%20search%20objective%20to%20perform%20language%0Aspecified%20pick%20and%20place%20mobile%20manipulation%20tasks.%20We%20construct%202D%20maps%0Acontaining%20the%20relevant%20semantic%20and%20occupancy%20information%20for%20navigation%20while%0Asimultaneously%20actively%20constructing%203D%20Gaussian%20representations%20of%0Atask-relevant%20objects.%20We%20fuse%20observations%20across%20this%20multi-layered%0Arepresentation%20while%20explicitly%20modeling%20the%20multi-view%20consistency%20of%20the%0Adetections%20of%20each%20object.%20In%20order%20to%20efficiently%20search%20for%20the%20target%0Aobject%2C%20we%20formulate%20an%20objective%20function%20balancing%20exploration%20of%20unobserved%0Aor%20uncertain%20regions%20with%20exploitation%20of%20scene%20semantic%20information.%20We%0Aevaluate%20HELIOS%20on%20the%20OVMM%20benchmark%20in%20the%20Habitat%20simulator%2C%20a%20pick%20and%0Aplace%20benchmark%20in%20which%20perception%20is%20challenging%20due%20to%20large%20and%20complex%0Ascenes%20with%20comparatively%20small%20target%20objects.%20HELIOS%20achieves%0Astate-of-the-art%20results%20on%20OVMM.%20As%20our%20approach%20is%20zero-shot%2C%20HELIOS%20can%20also%0Atransfer%20to%20the%20real%20world%20without%20requiring%20additional%20data%2C%20as%20we%20illustrate%0Aby%20demonstrating%20it%20in%20a%20real%20world%20office%20environment%20on%20a%20Spot%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22498v1&entry.124074799=Read"},
{"title": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models", "author": "Wenjun Wang and Shuo Cai and Congkai Xie and Mingfa Feng and Yiming Zhang and Zhen Li and Kejing Yang and Ming Li and Jiannong Cao and Yuan Xie and Hongxia Yang", "abstract": "  The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training.\n", "link": "http://arxiv.org/abs/2509.22536v1", "date": "2025-09-26", "relevancy": 2.4254, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4872}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4872}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiR2%3A%20A%20Comprehensive%20FP8%20Training%20Recipe%20for%20Reasoning-Enhanced%0A%20%20Language%20Models&body=Title%3A%20InfiR2%3A%20A%20Comprehensive%20FP8%20Training%20Recipe%20for%20Reasoning-Enhanced%0A%20%20Language%20Models%0AAuthor%3A%20Wenjun%20Wang%20and%20Shuo%20Cai%20and%20Congkai%20Xie%20and%20Mingfa%20Feng%20and%20Yiming%20Zhang%20and%20Zhen%20Li%20and%20Kejing%20Yang%20and%20Ming%20Li%20and%20Jiannong%20Cao%20and%20Yuan%20Xie%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20The%20immense%20computational%20cost%20of%20training%20Large%20Language%20Models%20%28LLMs%29%0Apresents%20a%20major%20barrier%20to%20innovation.%20While%20FP8%20training%20offers%20a%20promising%0Asolution%20with%20significant%20theoretical%20efficiency%20gains%2C%20its%20widespread%20adoption%0Ahas%20been%20hindered%20by%20the%20lack%20of%20a%20comprehensive%2C%20open-source%20training%20recipe.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20an%20end-to-end%20FP8%20training%20recipe%20that%0Aseamlessly%20integrates%20continual%20pre-training%20and%20supervised%20fine-tuning.%20Our%0Amethodology%20employs%20a%20fine-grained%2C%20hybrid-granularity%20quantization%20strategy%20to%0Amaintain%20numerical%20fidelity%20while%20maximizing%20computational%20efficiency.%20Through%0Aextensive%20experiments%2C%20including%20the%20continue%20pre-training%20of%20models%20on%20a%0A160B-token%20corpus%2C%20we%20demonstrate%20that%20our%20recipe%20is%20not%20only%20remarkably%20stable%0Abut%20also%20essentially%20lossless%2C%20achieving%20performance%20on%20par%20with%20the%20BF16%0Abaseline%20across%20a%20suite%20of%20reasoning%20benchmarks.%20Crucially%2C%20this%20is%20achieved%0Awith%20substantial%20efficiency%20improvements%2C%20including%20up%20to%20a%2022%25%20reduction%20in%0Atraining%20time%2C%20a%2014%25%20decrease%20in%20peak%20memory%20usage%2C%20and%20a%2019%25%20increase%20in%0Athroughput.%20Our%20results%20establish%20FP8%20as%20a%20practical%20and%20robust%20alternative%20to%0ABF16%2C%20and%20we%20will%20release%20the%20accompanying%20code%20to%20further%20democratize%0Alarge-scale%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiR2%253A%2520A%2520Comprehensive%2520FP8%2520Training%2520Recipe%2520for%2520Reasoning-Enhanced%250A%2520%2520Language%2520Models%26entry.906535625%3DWenjun%2520Wang%2520and%2520Shuo%2520Cai%2520and%2520Congkai%2520Xie%2520and%2520Mingfa%2520Feng%2520and%2520Yiming%2520Zhang%2520and%2520Zhen%2520Li%2520and%2520Kejing%2520Yang%2520and%2520Ming%2520Li%2520and%2520Jiannong%2520Cao%2520and%2520Yuan%2520Xie%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520The%2520immense%2520computational%2520cost%2520of%2520training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Apresents%2520a%2520major%2520barrier%2520to%2520innovation.%2520While%2520FP8%2520training%2520offers%2520a%2520promising%250Asolution%2520with%2520significant%2520theoretical%2520efficiency%2520gains%252C%2520its%2520widespread%2520adoption%250Ahas%2520been%2520hindered%2520by%2520the%2520lack%2520of%2520a%2520comprehensive%252C%2520open-source%2520training%2520recipe.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520an%2520end-to-end%2520FP8%2520training%2520recipe%2520that%250Aseamlessly%2520integrates%2520continual%2520pre-training%2520and%2520supervised%2520fine-tuning.%2520Our%250Amethodology%2520employs%2520a%2520fine-grained%252C%2520hybrid-granularity%2520quantization%2520strategy%2520to%250Amaintain%2520numerical%2520fidelity%2520while%2520maximizing%2520computational%2520efficiency.%2520Through%250Aextensive%2520experiments%252C%2520including%2520the%2520continue%2520pre-training%2520of%2520models%2520on%2520a%250A160B-token%2520corpus%252C%2520we%2520demonstrate%2520that%2520our%2520recipe%2520is%2520not%2520only%2520remarkably%2520stable%250Abut%2520also%2520essentially%2520lossless%252C%2520achieving%2520performance%2520on%2520par%2520with%2520the%2520BF16%250Abaseline%2520across%2520a%2520suite%2520of%2520reasoning%2520benchmarks.%2520Crucially%252C%2520this%2520is%2520achieved%250Awith%2520substantial%2520efficiency%2520improvements%252C%2520including%2520up%2520to%2520a%252022%2525%2520reduction%2520in%250Atraining%2520time%252C%2520a%252014%2525%2520decrease%2520in%2520peak%2520memory%2520usage%252C%2520and%2520a%252019%2525%2520increase%2520in%250Athroughput.%2520Our%2520results%2520establish%2520FP8%2520as%2520a%2520practical%2520and%2520robust%2520alternative%2520to%250ABF16%252C%2520and%2520we%2520will%2520release%2520the%2520accompanying%2520code%2520to%2520further%2520democratize%250Alarge-scale%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiR2%3A%20A%20Comprehensive%20FP8%20Training%20Recipe%20for%20Reasoning-Enhanced%0A%20%20Language%20Models&entry.906535625=Wenjun%20Wang%20and%20Shuo%20Cai%20and%20Congkai%20Xie%20and%20Mingfa%20Feng%20and%20Yiming%20Zhang%20and%20Zhen%20Li%20and%20Kejing%20Yang%20and%20Ming%20Li%20and%20Jiannong%20Cao%20and%20Yuan%20Xie%20and%20Hongxia%20Yang&entry.1292438233=%20%20The%20immense%20computational%20cost%20of%20training%20Large%20Language%20Models%20%28LLMs%29%0Apresents%20a%20major%20barrier%20to%20innovation.%20While%20FP8%20training%20offers%20a%20promising%0Asolution%20with%20significant%20theoretical%20efficiency%20gains%2C%20its%20widespread%20adoption%0Ahas%20been%20hindered%20by%20the%20lack%20of%20a%20comprehensive%2C%20open-source%20training%20recipe.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20an%20end-to-end%20FP8%20training%20recipe%20that%0Aseamlessly%20integrates%20continual%20pre-training%20and%20supervised%20fine-tuning.%20Our%0Amethodology%20employs%20a%20fine-grained%2C%20hybrid-granularity%20quantization%20strategy%20to%0Amaintain%20numerical%20fidelity%20while%20maximizing%20computational%20efficiency.%20Through%0Aextensive%20experiments%2C%20including%20the%20continue%20pre-training%20of%20models%20on%20a%0A160B-token%20corpus%2C%20we%20demonstrate%20that%20our%20recipe%20is%20not%20only%20remarkably%20stable%0Abut%20also%20essentially%20lossless%2C%20achieving%20performance%20on%20par%20with%20the%20BF16%0Abaseline%20across%20a%20suite%20of%20reasoning%20benchmarks.%20Crucially%2C%20this%20is%20achieved%0Awith%20substantial%20efficiency%20improvements%2C%20including%20up%20to%20a%2022%25%20reduction%20in%0Atraining%20time%2C%20a%2014%25%20decrease%20in%20peak%20memory%20usage%2C%20and%20a%2019%25%20increase%20in%0Athroughput.%20Our%20results%20establish%20FP8%20as%20a%20practical%20and%20robust%20alternative%20to%0ABF16%2C%20and%20we%20will%20release%20the%20accompanying%20code%20to%20further%20democratize%0Alarge-scale%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22536v1&entry.124074799=Read"},
{"title": "SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking\n  Neural Networks", "author": "Jini Yang and Beomseok Oh and Seungryong Kim and Sunok Kim", "abstract": "  Spiking neural networks (SNNs) have recently been attracting significant\nattention for their biological plausibility and energy efficiency, but\nsemi-supervised learning (SSL) methods for SNN-based models remain\nunderexplored compared to those for artificial neural networks (ANNs). In this\npaper, we introduce SpikeMatch, the first SSL framework for SNNs that leverages\nthe temporal dynamics through the leakage factor of SNNs for diverse\npseudo-labeling within a co-training framework. By utilizing agreement among\nmultiple predictions from a single SNN, SpikeMatch generates reliable\npseudo-labels from weakly-augmented unlabeled samples to train on\nstrongly-augmented ones, effectively mitigating confirmation bias by capturing\ndiscriminative features with limited labels. Experiments show that SpikeMatch\noutperforms existing SSL methods adapted to SNN backbones across various\nstandard benchmarks.\n", "link": "http://arxiv.org/abs/2509.22581v1", "date": "2025-09-26", "relevancy": 2.4232, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5041}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeMatch%3A%20Semi-Supervised%20Learning%20with%20Temporal%20Dynamics%20of%20Spiking%0A%20%20Neural%20Networks&body=Title%3A%20SpikeMatch%3A%20Semi-Supervised%20Learning%20with%20Temporal%20Dynamics%20of%20Spiking%0A%20%20Neural%20Networks%0AAuthor%3A%20Jini%20Yang%20and%20Beomseok%20Oh%20and%20Seungryong%20Kim%20and%20Sunok%20Kim%0AAbstract%3A%20%20%20Spiking%20neural%20networks%20%28SNNs%29%20have%20recently%20been%20attracting%20significant%0Aattention%20for%20their%20biological%20plausibility%20and%20energy%20efficiency%2C%20but%0Asemi-supervised%20learning%20%28SSL%29%20methods%20for%20SNN-based%20models%20remain%0Aunderexplored%20compared%20to%20those%20for%20artificial%20neural%20networks%20%28ANNs%29.%20In%20this%0Apaper%2C%20we%20introduce%20SpikeMatch%2C%20the%20first%20SSL%20framework%20for%20SNNs%20that%20leverages%0Athe%20temporal%20dynamics%20through%20the%20leakage%20factor%20of%20SNNs%20for%20diverse%0Apseudo-labeling%20within%20a%20co-training%20framework.%20By%20utilizing%20agreement%20among%0Amultiple%20predictions%20from%20a%20single%20SNN%2C%20SpikeMatch%20generates%20reliable%0Apseudo-labels%20from%20weakly-augmented%20unlabeled%20samples%20to%20train%20on%0Astrongly-augmented%20ones%2C%20effectively%20mitigating%20confirmation%20bias%20by%20capturing%0Adiscriminative%20features%20with%20limited%20labels.%20Experiments%20show%20that%20SpikeMatch%0Aoutperforms%20existing%20SSL%20methods%20adapted%20to%20SNN%20backbones%20across%20various%0Astandard%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeMatch%253A%2520Semi-Supervised%2520Learning%2520with%2520Temporal%2520Dynamics%2520of%2520Spiking%250A%2520%2520Neural%2520Networks%26entry.906535625%3DJini%2520Yang%2520and%2520Beomseok%2520Oh%2520and%2520Seungryong%2520Kim%2520and%2520Sunok%2520Kim%26entry.1292438233%3D%2520%2520Spiking%2520neural%2520networks%2520%2528SNNs%2529%2520have%2520recently%2520been%2520attracting%2520significant%250Aattention%2520for%2520their%2520biological%2520plausibility%2520and%2520energy%2520efficiency%252C%2520but%250Asemi-supervised%2520learning%2520%2528SSL%2529%2520methods%2520for%2520SNN-based%2520models%2520remain%250Aunderexplored%2520compared%2520to%2520those%2520for%2520artificial%2520neural%2520networks%2520%2528ANNs%2529.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520SpikeMatch%252C%2520the%2520first%2520SSL%2520framework%2520for%2520SNNs%2520that%2520leverages%250Athe%2520temporal%2520dynamics%2520through%2520the%2520leakage%2520factor%2520of%2520SNNs%2520for%2520diverse%250Apseudo-labeling%2520within%2520a%2520co-training%2520framework.%2520By%2520utilizing%2520agreement%2520among%250Amultiple%2520predictions%2520from%2520a%2520single%2520SNN%252C%2520SpikeMatch%2520generates%2520reliable%250Apseudo-labels%2520from%2520weakly-augmented%2520unlabeled%2520samples%2520to%2520train%2520on%250Astrongly-augmented%2520ones%252C%2520effectively%2520mitigating%2520confirmation%2520bias%2520by%2520capturing%250Adiscriminative%2520features%2520with%2520limited%2520labels.%2520Experiments%2520show%2520that%2520SpikeMatch%250Aoutperforms%2520existing%2520SSL%2520methods%2520adapted%2520to%2520SNN%2520backbones%2520across%2520various%250Astandard%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeMatch%3A%20Semi-Supervised%20Learning%20with%20Temporal%20Dynamics%20of%20Spiking%0A%20%20Neural%20Networks&entry.906535625=Jini%20Yang%20and%20Beomseok%20Oh%20and%20Seungryong%20Kim%20and%20Sunok%20Kim&entry.1292438233=%20%20Spiking%20neural%20networks%20%28SNNs%29%20have%20recently%20been%20attracting%20significant%0Aattention%20for%20their%20biological%20plausibility%20and%20energy%20efficiency%2C%20but%0Asemi-supervised%20learning%20%28SSL%29%20methods%20for%20SNN-based%20models%20remain%0Aunderexplored%20compared%20to%20those%20for%20artificial%20neural%20networks%20%28ANNs%29.%20In%20this%0Apaper%2C%20we%20introduce%20SpikeMatch%2C%20the%20first%20SSL%20framework%20for%20SNNs%20that%20leverages%0Athe%20temporal%20dynamics%20through%20the%20leakage%20factor%20of%20SNNs%20for%20diverse%0Apseudo-labeling%20within%20a%20co-training%20framework.%20By%20utilizing%20agreement%20among%0Amultiple%20predictions%20from%20a%20single%20SNN%2C%20SpikeMatch%20generates%20reliable%0Apseudo-labels%20from%20weakly-augmented%20unlabeled%20samples%20to%20train%20on%0Astrongly-augmented%20ones%2C%20effectively%20mitigating%20confirmation%20bias%20by%20capturing%0Adiscriminative%20features%20with%20limited%20labels.%20Experiments%20show%20that%20SpikeMatch%0Aoutperforms%20existing%20SSL%20methods%20adapted%20to%20SNN%20backbones%20across%20various%0Astandard%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22581v1&entry.124074799=Read"},
{"title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual\n  Transfer", "author": "Zhehao Dong and Xiaofeng Wang and Zheng Zhu and Yirui Wang and Yang Wang and Yukun Zhou and Boyuan Wang and Chaojun Ni and Runqi Ouyang and Wenkang Qin and Xinze Chen and Yun Ye and Guan Huang", "abstract": "  Vision-language-action (VLA) models increasingly rely on diverse training\ndata to achieve robust generalization. However, collecting large-scale\nreal-world robot manipulation data across varied object appearances and\nenvironmental conditions remains prohibitively time-consuming and expensive. To\novercome this bottleneck, we propose Embodied Manipulation Media Adaptation\n(EMMA), a VLA policy enhancement framework that integrates a generative data\nengine with an effective training pipeline. We introduce DreamTransfer, a\ndiffusion Transformer-based framework for generating multi-view consistent,\ngeometrically grounded embodied manipulation videos. DreamTransfer enables\ntext-controlled visual editing of robot videos, transforming foreground,\nbackground, and lighting conditions without compromising 3D structure or\ngeometrical plausibility. Furthermore, we explore hybrid training with real and\ngenerated data, and introduce AdaMix, a hard-sample-aware training strategy\nthat dynamically reweights training batches to focus optimization on\nperceptually or kinematically challenging samples. Extensive experiments show\nthat videos generated by DreamTransfer significantly outperform prior video\ngeneration methods in multi-view consistency, geometric fidelity, and\ntext-conditioning accuracy. Crucially, VLAs trained with generated data enable\nrobots to generalize to unseen object categories and novel visual domains using\nonly demonstrations from a single appearance. In real-world robotic\nmanipulation tasks with zero-shot visual domains, our approach achieves over a\n200% relative performance gain compared to training on real data alone, and\nfurther improves by 13% with AdaMix, demonstrating its effectiveness in\nboosting policy generalization.\n", "link": "http://arxiv.org/abs/2509.22407v1", "date": "2025-09-26", "relevancy": 2.4159, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6157}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5978}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.59}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20Generalizing%20Real-World%20Robot%20Manipulation%20via%20Generative%20Visual%0A%20%20Transfer&body=Title%3A%20EMMA%3A%20Generalizing%20Real-World%20Robot%20Manipulation%20via%20Generative%20Visual%0A%20%20Transfer%0AAuthor%3A%20Zhehao%20Dong%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Yirui%20Wang%20and%20Yang%20Wang%20and%20Yukun%20Zhou%20and%20Boyuan%20Wang%20and%20Chaojun%20Ni%20and%20Runqi%20Ouyang%20and%20Wenkang%20Qin%20and%20Xinze%20Chen%20and%20Yun%20Ye%20and%20Guan%20Huang%0AAbstract%3A%20%20%20Vision-language-action%20%28VLA%29%20models%20increasingly%20rely%20on%20diverse%20training%0Adata%20to%20achieve%20robust%20generalization.%20However%2C%20collecting%20large-scale%0Areal-world%20robot%20manipulation%20data%20across%20varied%20object%20appearances%20and%0Aenvironmental%20conditions%20remains%20prohibitively%20time-consuming%20and%20expensive.%20To%0Aovercome%20this%20bottleneck%2C%20we%20propose%20Embodied%20Manipulation%20Media%20Adaptation%0A%28EMMA%29%2C%20a%20VLA%20policy%20enhancement%20framework%20that%20integrates%20a%20generative%20data%0Aengine%20with%20an%20effective%20training%20pipeline.%20We%20introduce%20DreamTransfer%2C%20a%0Adiffusion%20Transformer-based%20framework%20for%20generating%20multi-view%20consistent%2C%0Ageometrically%20grounded%20embodied%20manipulation%20videos.%20DreamTransfer%20enables%0Atext-controlled%20visual%20editing%20of%20robot%20videos%2C%20transforming%20foreground%2C%0Abackground%2C%20and%20lighting%20conditions%20without%20compromising%203D%20structure%20or%0Ageometrical%20plausibility.%20Furthermore%2C%20we%20explore%20hybrid%20training%20with%20real%20and%0Agenerated%20data%2C%20and%20introduce%20AdaMix%2C%20a%20hard-sample-aware%20training%20strategy%0Athat%20dynamically%20reweights%20training%20batches%20to%20focus%20optimization%20on%0Aperceptually%20or%20kinematically%20challenging%20samples.%20Extensive%20experiments%20show%0Athat%20videos%20generated%20by%20DreamTransfer%20significantly%20outperform%20prior%20video%0Ageneration%20methods%20in%20multi-view%20consistency%2C%20geometric%20fidelity%2C%20and%0Atext-conditioning%20accuracy.%20Crucially%2C%20VLAs%20trained%20with%20generated%20data%20enable%0Arobots%20to%20generalize%20to%20unseen%20object%20categories%20and%20novel%20visual%20domains%20using%0Aonly%20demonstrations%20from%20a%20single%20appearance.%20In%20real-world%20robotic%0Amanipulation%20tasks%20with%20zero-shot%20visual%20domains%2C%20our%20approach%20achieves%20over%20a%0A200%25%20relative%20performance%20gain%20compared%20to%20training%20on%20real%20data%20alone%2C%20and%0Afurther%20improves%20by%2013%25%20with%20AdaMix%2C%20demonstrating%20its%20effectiveness%20in%0Aboosting%20policy%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520Generalizing%2520Real-World%2520Robot%2520Manipulation%2520via%2520Generative%2520Visual%250A%2520%2520Transfer%26entry.906535625%3DZhehao%2520Dong%2520and%2520Xiaofeng%2520Wang%2520and%2520Zheng%2520Zhu%2520and%2520Yirui%2520Wang%2520and%2520Yang%2520Wang%2520and%2520Yukun%2520Zhou%2520and%2520Boyuan%2520Wang%2520and%2520Chaojun%2520Ni%2520and%2520Runqi%2520Ouyang%2520and%2520Wenkang%2520Qin%2520and%2520Xinze%2520Chen%2520and%2520Yun%2520Ye%2520and%2520Guan%2520Huang%26entry.1292438233%3D%2520%2520Vision-language-action%2520%2528VLA%2529%2520models%2520increasingly%2520rely%2520on%2520diverse%2520training%250Adata%2520to%2520achieve%2520robust%2520generalization.%2520However%252C%2520collecting%2520large-scale%250Areal-world%2520robot%2520manipulation%2520data%2520across%2520varied%2520object%2520appearances%2520and%250Aenvironmental%2520conditions%2520remains%2520prohibitively%2520time-consuming%2520and%2520expensive.%2520To%250Aovercome%2520this%2520bottleneck%252C%2520we%2520propose%2520Embodied%2520Manipulation%2520Media%2520Adaptation%250A%2528EMMA%2529%252C%2520a%2520VLA%2520policy%2520enhancement%2520framework%2520that%2520integrates%2520a%2520generative%2520data%250Aengine%2520with%2520an%2520effective%2520training%2520pipeline.%2520We%2520introduce%2520DreamTransfer%252C%2520a%250Adiffusion%2520Transformer-based%2520framework%2520for%2520generating%2520multi-view%2520consistent%252C%250Ageometrically%2520grounded%2520embodied%2520manipulation%2520videos.%2520DreamTransfer%2520enables%250Atext-controlled%2520visual%2520editing%2520of%2520robot%2520videos%252C%2520transforming%2520foreground%252C%250Abackground%252C%2520and%2520lighting%2520conditions%2520without%2520compromising%25203D%2520structure%2520or%250Ageometrical%2520plausibility.%2520Furthermore%252C%2520we%2520explore%2520hybrid%2520training%2520with%2520real%2520and%250Agenerated%2520data%252C%2520and%2520introduce%2520AdaMix%252C%2520a%2520hard-sample-aware%2520training%2520strategy%250Athat%2520dynamically%2520reweights%2520training%2520batches%2520to%2520focus%2520optimization%2520on%250Aperceptually%2520or%2520kinematically%2520challenging%2520samples.%2520Extensive%2520experiments%2520show%250Athat%2520videos%2520generated%2520by%2520DreamTransfer%2520significantly%2520outperform%2520prior%2520video%250Ageneration%2520methods%2520in%2520multi-view%2520consistency%252C%2520geometric%2520fidelity%252C%2520and%250Atext-conditioning%2520accuracy.%2520Crucially%252C%2520VLAs%2520trained%2520with%2520generated%2520data%2520enable%250Arobots%2520to%2520generalize%2520to%2520unseen%2520object%2520categories%2520and%2520novel%2520visual%2520domains%2520using%250Aonly%2520demonstrations%2520from%2520a%2520single%2520appearance.%2520In%2520real-world%2520robotic%250Amanipulation%2520tasks%2520with%2520zero-shot%2520visual%2520domains%252C%2520our%2520approach%2520achieves%2520over%2520a%250A200%2525%2520relative%2520performance%2520gain%2520compared%2520to%2520training%2520on%2520real%2520data%2520alone%252C%2520and%250Afurther%2520improves%2520by%252013%2525%2520with%2520AdaMix%252C%2520demonstrating%2520its%2520effectiveness%2520in%250Aboosting%2520policy%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20Generalizing%20Real-World%20Robot%20Manipulation%20via%20Generative%20Visual%0A%20%20Transfer&entry.906535625=Zhehao%20Dong%20and%20Xiaofeng%20Wang%20and%20Zheng%20Zhu%20and%20Yirui%20Wang%20and%20Yang%20Wang%20and%20Yukun%20Zhou%20and%20Boyuan%20Wang%20and%20Chaojun%20Ni%20and%20Runqi%20Ouyang%20and%20Wenkang%20Qin%20and%20Xinze%20Chen%20and%20Yun%20Ye%20and%20Guan%20Huang&entry.1292438233=%20%20Vision-language-action%20%28VLA%29%20models%20increasingly%20rely%20on%20diverse%20training%0Adata%20to%20achieve%20robust%20generalization.%20However%2C%20collecting%20large-scale%0Areal-world%20robot%20manipulation%20data%20across%20varied%20object%20appearances%20and%0Aenvironmental%20conditions%20remains%20prohibitively%20time-consuming%20and%20expensive.%20To%0Aovercome%20this%20bottleneck%2C%20we%20propose%20Embodied%20Manipulation%20Media%20Adaptation%0A%28EMMA%29%2C%20a%20VLA%20policy%20enhancement%20framework%20that%20integrates%20a%20generative%20data%0Aengine%20with%20an%20effective%20training%20pipeline.%20We%20introduce%20DreamTransfer%2C%20a%0Adiffusion%20Transformer-based%20framework%20for%20generating%20multi-view%20consistent%2C%0Ageometrically%20grounded%20embodied%20manipulation%20videos.%20DreamTransfer%20enables%0Atext-controlled%20visual%20editing%20of%20robot%20videos%2C%20transforming%20foreground%2C%0Abackground%2C%20and%20lighting%20conditions%20without%20compromising%203D%20structure%20or%0Ageometrical%20plausibility.%20Furthermore%2C%20we%20explore%20hybrid%20training%20with%20real%20and%0Agenerated%20data%2C%20and%20introduce%20AdaMix%2C%20a%20hard-sample-aware%20training%20strategy%0Athat%20dynamically%20reweights%20training%20batches%20to%20focus%20optimization%20on%0Aperceptually%20or%20kinematically%20challenging%20samples.%20Extensive%20experiments%20show%0Athat%20videos%20generated%20by%20DreamTransfer%20significantly%20outperform%20prior%20video%0Ageneration%20methods%20in%20multi-view%20consistency%2C%20geometric%20fidelity%2C%20and%0Atext-conditioning%20accuracy.%20Crucially%2C%20VLAs%20trained%20with%20generated%20data%20enable%0Arobots%20to%20generalize%20to%20unseen%20object%20categories%20and%20novel%20visual%20domains%20using%0Aonly%20demonstrations%20from%20a%20single%20appearance.%20In%20real-world%20robotic%0Amanipulation%20tasks%20with%20zero-shot%20visual%20domains%2C%20our%20approach%20achieves%20over%20a%0A200%25%20relative%20performance%20gain%20compared%20to%20training%20on%20real%20data%20alone%2C%20and%0Afurther%20improves%20by%2013%25%20with%20AdaMix%2C%20demonstrating%20its%20effectiveness%20in%0Aboosting%20policy%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22407v1&entry.124074799=Read"},
{"title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation", "author": "Shuang Zeng and Dekang Qi and Xinyuan Chang and Feng Xiong and Shichao Xie and Xiaolong Wu and Shiyi Liang and Mu Xu and Xing Wei", "abstract": "  Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.\n", "link": "http://arxiv.org/abs/2509.22548v1", "date": "2025-09-26", "relevancy": 2.4139, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6082}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JanusVLN%3A%20Decoupling%20Semantics%20and%20Spatiality%20with%20Dual%20Implicit%20Memory%0A%20%20for%20Vision-Language%20Navigation&body=Title%3A%20JanusVLN%3A%20Decoupling%20Semantics%20and%20Spatiality%20with%20Dual%20Implicit%20Memory%0A%20%20for%20Vision-Language%20Navigation%0AAuthor%3A%20Shuang%20Zeng%20and%20Dekang%20Qi%20and%20Xinyuan%20Chang%20and%20Feng%20Xiong%20and%20Shichao%20Xie%20and%20Xiaolong%20Wu%20and%20Shiyi%20Liang%20and%20Mu%20Xu%20and%20Xing%20Wei%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20requires%20an%20embodied%20agent%20to%20navigate%20through%0Aunseen%20environments%2C%20guided%20by%20natural%20language%20instructions%20and%20a%20continuous%0Avideo%20stream.%20Recent%20advances%20in%20VLN%20have%20been%20driven%20by%20the%20powerful%20semantic%0Aunderstanding%20of%20Multimodal%20Large%20Language%20Models.%20However%2C%20these%20methods%0Atypically%20rely%20on%20explicit%20semantic%20memory%2C%20such%20as%20building%20textual%20cognitive%0Amaps%20or%20storing%20historical%20visual%20frames.%20This%20type%20of%20method%20suffers%20from%0Aspatial%20information%20loss%2C%20computational%20redundancy%2C%20and%20memory%20bloat%2C%20which%0Aimpede%20efficient%20navigation.%20Inspired%20by%20the%20implicit%20scene%20representation%20in%0Ahuman%20navigation%2C%20analogous%20to%20the%20left%20brain%27s%20semantic%20understanding%20and%20the%0Aright%20brain%27s%20spatial%20cognition%2C%20we%20propose%20JanusVLN%2C%20a%20novel%20VLN%20framework%0Afeaturing%20a%20dual%20implicit%20neural%20memory%20that%20models%20spatial-geometric%20and%0Avisual-semantic%20memory%20as%20separate%2C%20compact%2C%20and%20fixed-size%20neural%0Arepresentations.%20This%20framework%20first%20extends%20the%20MLLM%20to%20incorporate%203D%20prior%0Aknowledge%20from%20the%20spatial-geometric%20encoder%2C%20thereby%20enhancing%20the%20spatial%0Areasoning%20capabilities%20of%20models%20based%20solely%20on%20RGB%20input.%20Then%2C%20the%0Ahistorical%20key-value%20caches%20from%20the%20spatial-geometric%20and%20visual-semantic%0Aencoders%20are%20constructed%20into%20a%20dual%20implicit%20memory.%20By%20retaining%20only%20the%20KVs%0Aof%20tokens%20in%20the%20initial%20and%20sliding%20window%2C%20redundant%20computation%20is%20avoided%2C%0Aenabling%20efficient%20incremental%20updates.%20Extensive%20experiments%20demonstrate%20that%0AJanusVLN%20outperforms%20over%2020%20recent%20methods%20to%20achieve%20SOTA%20performance.%20For%0Aexample%2C%20the%20success%20rate%20improves%20by%2010.5-35.5%20compared%20to%20methods%20using%0Amultiple%20data%20types%20as%20input%20and%20by%203.6-10.8%20compared%20to%20methods%20using%20more%20RGB%0Atraining%20data.%20This%20indicates%20that%20the%20proposed%20dual%20implicit%20neural%20memory%2C%20as%0Aa%20novel%20paradigm%2C%20explores%20promising%20new%20directions%20for%20future%20VLN%20research.%0AOurs%20project%20page%3A%20https%3A//miv-xjtu.github.io/JanusVLN.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJanusVLN%253A%2520Decoupling%2520Semantics%2520and%2520Spatiality%2520with%2520Dual%2520Implicit%2520Memory%250A%2520%2520for%2520Vision-Language%2520Navigation%26entry.906535625%3DShuang%2520Zeng%2520and%2520Dekang%2520Qi%2520and%2520Xinyuan%2520Chang%2520and%2520Feng%2520Xiong%2520and%2520Shichao%2520Xie%2520and%2520Xiaolong%2520Wu%2520and%2520Shiyi%2520Liang%2520and%2520Mu%2520Xu%2520and%2520Xing%2520Wei%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520requires%2520an%2520embodied%2520agent%2520to%2520navigate%2520through%250Aunseen%2520environments%252C%2520guided%2520by%2520natural%2520language%2520instructions%2520and%2520a%2520continuous%250Avideo%2520stream.%2520Recent%2520advances%2520in%2520VLN%2520have%2520been%2520driven%2520by%2520the%2520powerful%2520semantic%250Aunderstanding%2520of%2520Multimodal%2520Large%2520Language%2520Models.%2520However%252C%2520these%2520methods%250Atypically%2520rely%2520on%2520explicit%2520semantic%2520memory%252C%2520such%2520as%2520building%2520textual%2520cognitive%250Amaps%2520or%2520storing%2520historical%2520visual%2520frames.%2520This%2520type%2520of%2520method%2520suffers%2520from%250Aspatial%2520information%2520loss%252C%2520computational%2520redundancy%252C%2520and%2520memory%2520bloat%252C%2520which%250Aimpede%2520efficient%2520navigation.%2520Inspired%2520by%2520the%2520implicit%2520scene%2520representation%2520in%250Ahuman%2520navigation%252C%2520analogous%2520to%2520the%2520left%2520brain%2527s%2520semantic%2520understanding%2520and%2520the%250Aright%2520brain%2527s%2520spatial%2520cognition%252C%2520we%2520propose%2520JanusVLN%252C%2520a%2520novel%2520VLN%2520framework%250Afeaturing%2520a%2520dual%2520implicit%2520neural%2520memory%2520that%2520models%2520spatial-geometric%2520and%250Avisual-semantic%2520memory%2520as%2520separate%252C%2520compact%252C%2520and%2520fixed-size%2520neural%250Arepresentations.%2520This%2520framework%2520first%2520extends%2520the%2520MLLM%2520to%2520incorporate%25203D%2520prior%250Aknowledge%2520from%2520the%2520spatial-geometric%2520encoder%252C%2520thereby%2520enhancing%2520the%2520spatial%250Areasoning%2520capabilities%2520of%2520models%2520based%2520solely%2520on%2520RGB%2520input.%2520Then%252C%2520the%250Ahistorical%2520key-value%2520caches%2520from%2520the%2520spatial-geometric%2520and%2520visual-semantic%250Aencoders%2520are%2520constructed%2520into%2520a%2520dual%2520implicit%2520memory.%2520By%2520retaining%2520only%2520the%2520KVs%250Aof%2520tokens%2520in%2520the%2520initial%2520and%2520sliding%2520window%252C%2520redundant%2520computation%2520is%2520avoided%252C%250Aenabling%2520efficient%2520incremental%2520updates.%2520Extensive%2520experiments%2520demonstrate%2520that%250AJanusVLN%2520outperforms%2520over%252020%2520recent%2520methods%2520to%2520achieve%2520SOTA%2520performance.%2520For%250Aexample%252C%2520the%2520success%2520rate%2520improves%2520by%252010.5-35.5%2520compared%2520to%2520methods%2520using%250Amultiple%2520data%2520types%2520as%2520input%2520and%2520by%25203.6-10.8%2520compared%2520to%2520methods%2520using%2520more%2520RGB%250Atraining%2520data.%2520This%2520indicates%2520that%2520the%2520proposed%2520dual%2520implicit%2520neural%2520memory%252C%2520as%250Aa%2520novel%2520paradigm%252C%2520explores%2520promising%2520new%2520directions%2520for%2520future%2520VLN%2520research.%250AOurs%2520project%2520page%253A%2520https%253A//miv-xjtu.github.io/JanusVLN.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JanusVLN%3A%20Decoupling%20Semantics%20and%20Spatiality%20with%20Dual%20Implicit%20Memory%0A%20%20for%20Vision-Language%20Navigation&entry.906535625=Shuang%20Zeng%20and%20Dekang%20Qi%20and%20Xinyuan%20Chang%20and%20Feng%20Xiong%20and%20Shichao%20Xie%20and%20Xiaolong%20Wu%20and%20Shiyi%20Liang%20and%20Mu%20Xu%20and%20Xing%20Wei&entry.1292438233=%20%20Vision-and-Language%20Navigation%20requires%20an%20embodied%20agent%20to%20navigate%20through%0Aunseen%20environments%2C%20guided%20by%20natural%20language%20instructions%20and%20a%20continuous%0Avideo%20stream.%20Recent%20advances%20in%20VLN%20have%20been%20driven%20by%20the%20powerful%20semantic%0Aunderstanding%20of%20Multimodal%20Large%20Language%20Models.%20However%2C%20these%20methods%0Atypically%20rely%20on%20explicit%20semantic%20memory%2C%20such%20as%20building%20textual%20cognitive%0Amaps%20or%20storing%20historical%20visual%20frames.%20This%20type%20of%20method%20suffers%20from%0Aspatial%20information%20loss%2C%20computational%20redundancy%2C%20and%20memory%20bloat%2C%20which%0Aimpede%20efficient%20navigation.%20Inspired%20by%20the%20implicit%20scene%20representation%20in%0Ahuman%20navigation%2C%20analogous%20to%20the%20left%20brain%27s%20semantic%20understanding%20and%20the%0Aright%20brain%27s%20spatial%20cognition%2C%20we%20propose%20JanusVLN%2C%20a%20novel%20VLN%20framework%0Afeaturing%20a%20dual%20implicit%20neural%20memory%20that%20models%20spatial-geometric%20and%0Avisual-semantic%20memory%20as%20separate%2C%20compact%2C%20and%20fixed-size%20neural%0Arepresentations.%20This%20framework%20first%20extends%20the%20MLLM%20to%20incorporate%203D%20prior%0Aknowledge%20from%20the%20spatial-geometric%20encoder%2C%20thereby%20enhancing%20the%20spatial%0Areasoning%20capabilities%20of%20models%20based%20solely%20on%20RGB%20input.%20Then%2C%20the%0Ahistorical%20key-value%20caches%20from%20the%20spatial-geometric%20and%20visual-semantic%0Aencoders%20are%20constructed%20into%20a%20dual%20implicit%20memory.%20By%20retaining%20only%20the%20KVs%0Aof%20tokens%20in%20the%20initial%20and%20sliding%20window%2C%20redundant%20computation%20is%20avoided%2C%0Aenabling%20efficient%20incremental%20updates.%20Extensive%20experiments%20demonstrate%20that%0AJanusVLN%20outperforms%20over%2020%20recent%20methods%20to%20achieve%20SOTA%20performance.%20For%0Aexample%2C%20the%20success%20rate%20improves%20by%2010.5-35.5%20compared%20to%20methods%20using%0Amultiple%20data%20types%20as%20input%20and%20by%203.6-10.8%20compared%20to%20methods%20using%20more%20RGB%0Atraining%20data.%20This%20indicates%20that%20the%20proposed%20dual%20implicit%20neural%20memory%2C%20as%0Aa%20novel%20paradigm%2C%20explores%20promising%20new%20directions%20for%20future%20VLN%20research.%0AOurs%20project%20page%3A%20https%3A//miv-xjtu.github.io/JanusVLN.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22548v1&entry.124074799=Read"},
{"title": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning?\n  A Systematic Study", "author": "Ziyang Cheng and Zhixun Li and Yuhan Li and Yixin Song and Kangyi Zhao and Dawei Cheng and Jia Li and Hong Cheng and Jeffrey Xu Yu", "abstract": "  Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL.\n", "link": "http://arxiv.org/abs/2505.18697v2", "date": "2025-09-26", "relevancy": 2.4118, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Alleviate%20Catastrophic%20Forgetting%20in%20Graph%20Continual%20Learning%3F%0A%20%20A%20Systematic%20Study&body=Title%3A%20Can%20LLMs%20Alleviate%20Catastrophic%20Forgetting%20in%20Graph%20Continual%20Learning%3F%0A%20%20A%20Systematic%20Study%0AAuthor%3A%20Ziyang%20Cheng%20and%20Zhixun%20Li%20and%20Yuhan%20Li%20and%20Yixin%20Song%20and%20Kangyi%20Zhao%20and%20Dawei%20Cheng%20and%20Jia%20Li%20and%20Hong%20Cheng%20and%20Jeffrey%20Xu%20Yu%0AAbstract%3A%20%20%20Nowadays%2C%20real-world%20data%2C%20including%20graph-structure%20data%2C%20often%20arrives%20in%20a%0Astreaming%20manner%2C%20which%20means%20that%20learning%20systems%20need%20to%20continuously%0Aacquire%20new%20knowledge%20without%20forgetting%20previously%20learned%20information.%0AAlthough%20substantial%20existing%20works%20attempt%20to%20address%20catastrophic%20forgetting%0Ain%20graph%20machine%20learning%2C%20they%20are%20all%20based%20on%20training%20from%20scratch%20with%0Astreaming%20data.%20With%20the%20rise%20of%20pretrained%20models%2C%20an%20increasing%20number%20of%0Astudies%20have%20leveraged%20their%20strong%20generalization%20ability%20for%20continual%0Alearning.%20Therefore%2C%20in%20this%20work%2C%20we%20attempt%20to%20answer%20whether%20large%20language%0Amodels%20%28LLMs%29%20can%20mitigate%20catastrophic%20forgetting%20in%20Graph%20Continual%20Learning%0A%28GCL%29.%20We%20first%20point%20out%20that%20current%20experimental%20setups%20for%20GCL%20have%0Asignificant%20flaws%2C%20as%20the%20evaluation%20stage%20may%20lead%20to%20task%20ID%20leakage.%20Then%2C%0Awe%20evaluate%20the%20performance%20of%20LLMs%20in%20more%20realistic%20scenarios%20and%20find%20that%0Aeven%20minor%20modifications%20can%20lead%20to%20outstanding%20results.%20Finally%2C%20based%20on%0Aextensive%20experiments%2C%20we%20propose%20a%20simple-yet-effective%20method%2C%20Simple%20Graph%0AContinual%20Learning%20%28SimGCL%29%2C%20that%20surpasses%20the%20previous%20state-of-the-art%0AGNN-based%20baseline%20by%20around%2020%25%20under%20the%20rehearsal-free%20constraint.%20To%0Afacilitate%20reproducibility%2C%20we%20have%20developed%20an%20easy-to-use%20benchmark%20LLM4GCL%0Afor%20training%20and%20evaluating%20existing%20GCL%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZhixunLEE/LLM4GCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Alleviate%2520Catastrophic%2520Forgetting%2520in%2520Graph%2520Continual%2520Learning%253F%250A%2520%2520A%2520Systematic%2520Study%26entry.906535625%3DZiyang%2520Cheng%2520and%2520Zhixun%2520Li%2520and%2520Yuhan%2520Li%2520and%2520Yixin%2520Song%2520and%2520Kangyi%2520Zhao%2520and%2520Dawei%2520Cheng%2520and%2520Jia%2520Li%2520and%2520Hong%2520Cheng%2520and%2520Jeffrey%2520Xu%2520Yu%26entry.1292438233%3D%2520%2520Nowadays%252C%2520real-world%2520data%252C%2520including%2520graph-structure%2520data%252C%2520often%2520arrives%2520in%2520a%250Astreaming%2520manner%252C%2520which%2520means%2520that%2520learning%2520systems%2520need%2520to%2520continuously%250Aacquire%2520new%2520knowledge%2520without%2520forgetting%2520previously%2520learned%2520information.%250AAlthough%2520substantial%2520existing%2520works%2520attempt%2520to%2520address%2520catastrophic%2520forgetting%250Ain%2520graph%2520machine%2520learning%252C%2520they%2520are%2520all%2520based%2520on%2520training%2520from%2520scratch%2520with%250Astreaming%2520data.%2520With%2520the%2520rise%2520of%2520pretrained%2520models%252C%2520an%2520increasing%2520number%2520of%250Astudies%2520have%2520leveraged%2520their%2520strong%2520generalization%2520ability%2520for%2520continual%250Alearning.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520attempt%2520to%2520answer%2520whether%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520can%2520mitigate%2520catastrophic%2520forgetting%2520in%2520Graph%2520Continual%2520Learning%250A%2528GCL%2529.%2520We%2520first%2520point%2520out%2520that%2520current%2520experimental%2520setups%2520for%2520GCL%2520have%250Asignificant%2520flaws%252C%2520as%2520the%2520evaluation%2520stage%2520may%2520lead%2520to%2520task%2520ID%2520leakage.%2520Then%252C%250Awe%2520evaluate%2520the%2520performance%2520of%2520LLMs%2520in%2520more%2520realistic%2520scenarios%2520and%2520find%2520that%250Aeven%2520minor%2520modifications%2520can%2520lead%2520to%2520outstanding%2520results.%2520Finally%252C%2520based%2520on%250Aextensive%2520experiments%252C%2520we%2520propose%2520a%2520simple-yet-effective%2520method%252C%2520Simple%2520Graph%250AContinual%2520Learning%2520%2528SimGCL%2529%252C%2520that%2520surpasses%2520the%2520previous%2520state-of-the-art%250AGNN-based%2520baseline%2520by%2520around%252020%2525%2520under%2520the%2520rehearsal-free%2520constraint.%2520To%250Afacilitate%2520reproducibility%252C%2520we%2520have%2520developed%2520an%2520easy-to-use%2520benchmark%2520LLM4GCL%250Afor%2520training%2520and%2520evaluating%2520existing%2520GCL%2520methods.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ZhixunLEE/LLM4GCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Alleviate%20Catastrophic%20Forgetting%20in%20Graph%20Continual%20Learning%3F%0A%20%20A%20Systematic%20Study&entry.906535625=Ziyang%20Cheng%20and%20Zhixun%20Li%20and%20Yuhan%20Li%20and%20Yixin%20Song%20and%20Kangyi%20Zhao%20and%20Dawei%20Cheng%20and%20Jia%20Li%20and%20Hong%20Cheng%20and%20Jeffrey%20Xu%20Yu&entry.1292438233=%20%20Nowadays%2C%20real-world%20data%2C%20including%20graph-structure%20data%2C%20often%20arrives%20in%20a%0Astreaming%20manner%2C%20which%20means%20that%20learning%20systems%20need%20to%20continuously%0Aacquire%20new%20knowledge%20without%20forgetting%20previously%20learned%20information.%0AAlthough%20substantial%20existing%20works%20attempt%20to%20address%20catastrophic%20forgetting%0Ain%20graph%20machine%20learning%2C%20they%20are%20all%20based%20on%20training%20from%20scratch%20with%0Astreaming%20data.%20With%20the%20rise%20of%20pretrained%20models%2C%20an%20increasing%20number%20of%0Astudies%20have%20leveraged%20their%20strong%20generalization%20ability%20for%20continual%0Alearning.%20Therefore%2C%20in%20this%20work%2C%20we%20attempt%20to%20answer%20whether%20large%20language%0Amodels%20%28LLMs%29%20can%20mitigate%20catastrophic%20forgetting%20in%20Graph%20Continual%20Learning%0A%28GCL%29.%20We%20first%20point%20out%20that%20current%20experimental%20setups%20for%20GCL%20have%0Asignificant%20flaws%2C%20as%20the%20evaluation%20stage%20may%20lead%20to%20task%20ID%20leakage.%20Then%2C%0Awe%20evaluate%20the%20performance%20of%20LLMs%20in%20more%20realistic%20scenarios%20and%20find%20that%0Aeven%20minor%20modifications%20can%20lead%20to%20outstanding%20results.%20Finally%2C%20based%20on%0Aextensive%20experiments%2C%20we%20propose%20a%20simple-yet-effective%20method%2C%20Simple%20Graph%0AContinual%20Learning%20%28SimGCL%29%2C%20that%20surpasses%20the%20previous%20state-of-the-art%0AGNN-based%20baseline%20by%20around%2020%25%20under%20the%20rehearsal-free%20constraint.%20To%0Afacilitate%20reproducibility%2C%20we%20have%20developed%20an%20easy-to-use%20benchmark%20LLM4GCL%0Afor%20training%20and%20evaluating%20existing%20GCL%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZhixunLEE/LLM4GCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18697v2&entry.124074799=Read"},
{"title": "Fused Partial Gromov-Wasserstein for Structured Objects", "author": "Yikun Bai and Shuang Wang and Huy Tran and Hengrong Du and Juexin Wang and Soheil Kolouri", "abstract": "  Structured data, such as graphs, is vital in machine learning due to its\ncapacity to capture complex relationships and interactions. In recent years,\nthe Fused Gromov-Wasserstein (FGW) distance has attracted growing interest\nbecause it enables the comparison of structured data by jointly accounting for\nfeature similarity and geometric structure. However, as a variant of optimal\ntransport (OT), classical FGW assumes an equal mass constraint on the compared\ndata. In this work, we relax this mass constraint and propose the Fused Partial\nGromov-Wasserstein (FPGW) framework, which extends FGW to accommodate\nunbalanced data. Theoretically, we establish the relationship between FPGW and\nFGW and prove the metric properties of FPGW. Numerically, we introduce\nFrank-Wolfe solvers and Sinkhorn solvers for the proposed FPGW framework.\nFinally, we evaluate the FPGW distance through graph matching, graph\nclassification and graph clustering experiments, demonstrating its robust\nperformance.\n", "link": "http://arxiv.org/abs/2502.09934v2", "date": "2025-09-26", "relevancy": 2.4116, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.495}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4836}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fused%20Partial%20Gromov-Wasserstein%20for%20Structured%20Objects&body=Title%3A%20Fused%20Partial%20Gromov-Wasserstein%20for%20Structured%20Objects%0AAuthor%3A%20Yikun%20Bai%20and%20Shuang%20Wang%20and%20Huy%20Tran%20and%20Hengrong%20Du%20and%20Juexin%20Wang%20and%20Soheil%20Kolouri%0AAbstract%3A%20%20%20Structured%20data%2C%20such%20as%20graphs%2C%20is%20vital%20in%20machine%20learning%20due%20to%20its%0Acapacity%20to%20capture%20complex%20relationships%20and%20interactions.%20In%20recent%20years%2C%0Athe%20Fused%20Gromov-Wasserstein%20%28FGW%29%20distance%20has%20attracted%20growing%20interest%0Abecause%20it%20enables%20the%20comparison%20of%20structured%20data%20by%20jointly%20accounting%20for%0Afeature%20similarity%20and%20geometric%20structure.%20However%2C%20as%20a%20variant%20of%20optimal%0Atransport%20%28OT%29%2C%20classical%20FGW%20assumes%20an%20equal%20mass%20constraint%20on%20the%20compared%0Adata.%20In%20this%20work%2C%20we%20relax%20this%20mass%20constraint%20and%20propose%20the%20Fused%20Partial%0AGromov-Wasserstein%20%28FPGW%29%20framework%2C%20which%20extends%20FGW%20to%20accommodate%0Aunbalanced%20data.%20Theoretically%2C%20we%20establish%20the%20relationship%20between%20FPGW%20and%0AFGW%20and%20prove%20the%20metric%20properties%20of%20FPGW.%20Numerically%2C%20we%20introduce%0AFrank-Wolfe%20solvers%20and%20Sinkhorn%20solvers%20for%20the%20proposed%20FPGW%20framework.%0AFinally%2C%20we%20evaluate%20the%20FPGW%20distance%20through%20graph%20matching%2C%20graph%0Aclassification%20and%20graph%20clustering%20experiments%2C%20demonstrating%20its%20robust%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09934v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFused%2520Partial%2520Gromov-Wasserstein%2520for%2520Structured%2520Objects%26entry.906535625%3DYikun%2520Bai%2520and%2520Shuang%2520Wang%2520and%2520Huy%2520Tran%2520and%2520Hengrong%2520Du%2520and%2520Juexin%2520Wang%2520and%2520Soheil%2520Kolouri%26entry.1292438233%3D%2520%2520Structured%2520data%252C%2520such%2520as%2520graphs%252C%2520is%2520vital%2520in%2520machine%2520learning%2520due%2520to%2520its%250Acapacity%2520to%2520capture%2520complex%2520relationships%2520and%2520interactions.%2520In%2520recent%2520years%252C%250Athe%2520Fused%2520Gromov-Wasserstein%2520%2528FGW%2529%2520distance%2520has%2520attracted%2520growing%2520interest%250Abecause%2520it%2520enables%2520the%2520comparison%2520of%2520structured%2520data%2520by%2520jointly%2520accounting%2520for%250Afeature%2520similarity%2520and%2520geometric%2520structure.%2520However%252C%2520as%2520a%2520variant%2520of%2520optimal%250Atransport%2520%2528OT%2529%252C%2520classical%2520FGW%2520assumes%2520an%2520equal%2520mass%2520constraint%2520on%2520the%2520compared%250Adata.%2520In%2520this%2520work%252C%2520we%2520relax%2520this%2520mass%2520constraint%2520and%2520propose%2520the%2520Fused%2520Partial%250AGromov-Wasserstein%2520%2528FPGW%2529%2520framework%252C%2520which%2520extends%2520FGW%2520to%2520accommodate%250Aunbalanced%2520data.%2520Theoretically%252C%2520we%2520establish%2520the%2520relationship%2520between%2520FPGW%2520and%250AFGW%2520and%2520prove%2520the%2520metric%2520properties%2520of%2520FPGW.%2520Numerically%252C%2520we%2520introduce%250AFrank-Wolfe%2520solvers%2520and%2520Sinkhorn%2520solvers%2520for%2520the%2520proposed%2520FPGW%2520framework.%250AFinally%252C%2520we%2520evaluate%2520the%2520FPGW%2520distance%2520through%2520graph%2520matching%252C%2520graph%250Aclassification%2520and%2520graph%2520clustering%2520experiments%252C%2520demonstrating%2520its%2520robust%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09934v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fused%20Partial%20Gromov-Wasserstein%20for%20Structured%20Objects&entry.906535625=Yikun%20Bai%20and%20Shuang%20Wang%20and%20Huy%20Tran%20and%20Hengrong%20Du%20and%20Juexin%20Wang%20and%20Soheil%20Kolouri&entry.1292438233=%20%20Structured%20data%2C%20such%20as%20graphs%2C%20is%20vital%20in%20machine%20learning%20due%20to%20its%0Acapacity%20to%20capture%20complex%20relationships%20and%20interactions.%20In%20recent%20years%2C%0Athe%20Fused%20Gromov-Wasserstein%20%28FGW%29%20distance%20has%20attracted%20growing%20interest%0Abecause%20it%20enables%20the%20comparison%20of%20structured%20data%20by%20jointly%20accounting%20for%0Afeature%20similarity%20and%20geometric%20structure.%20However%2C%20as%20a%20variant%20of%20optimal%0Atransport%20%28OT%29%2C%20classical%20FGW%20assumes%20an%20equal%20mass%20constraint%20on%20the%20compared%0Adata.%20In%20this%20work%2C%20we%20relax%20this%20mass%20constraint%20and%20propose%20the%20Fused%20Partial%0AGromov-Wasserstein%20%28FPGW%29%20framework%2C%20which%20extends%20FGW%20to%20accommodate%0Aunbalanced%20data.%20Theoretically%2C%20we%20establish%20the%20relationship%20between%20FPGW%20and%0AFGW%20and%20prove%20the%20metric%20properties%20of%20FPGW.%20Numerically%2C%20we%20introduce%0AFrank-Wolfe%20solvers%20and%20Sinkhorn%20solvers%20for%20the%20proposed%20FPGW%20framework.%0AFinally%2C%20we%20evaluate%20the%20FPGW%20distance%20through%20graph%20matching%2C%20graph%0Aclassification%20and%20graph%20clustering%20experiments%2C%20demonstrating%20its%20robust%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09934v2&entry.124074799=Read"},
{"title": "HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space", "author": "Ke Li and Zheng Yang and Zhongbin Zhou and Feng Xue and Zhonglin Jiang and Wenxiao Wang", "abstract": "  Mixture-of-Experts (MoE) architectures in large language models (LLMs)\ndeliver exceptional performance and reduced inference costs compared to dense\nLLMs. However, their large parameter counts result in prohibitive memory\nrequirements, limiting practical deployment. While existing pruning methods\nprimarily focus on expert-level pruning, this coarse granularity often leads to\nsubstantial accuracy degradation. In this work, we introduce HEAPr, a novel\npruning algorithm that decomposes experts into smaller, indivisible atomic\nexperts, enabling more precise and flexible atomic expert pruning. To measure\nthe importance of each atomic expert, we leverage second-order information\nbased on principles similar to Optimal Brain Surgeon (OBS) theory. To address\nthe computational and storage challenges posed by second-order information,\nHEAPr exploits the inherent properties of atomic experts to transform the\nsecond-order information from expert parameters into that of atomic expert\nparameters, and further simplifies it to the second-order information of atomic\nexpert outputs. This approach reduces the space complexity from $O(d^4)$, where\nd is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward\npasses and one backward pass on a small calibration set to compute the\nimportance of atomic experts. Extensive experiments on MoE models, including\nDeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing\nexpert-level pruning methods across a wide range of compression ratios and\nbenchmarks. Specifically, HEAPr achieves nearly lossless compression at\ncompression ratios of 20% ~ 25% in most models, while also reducing FLOPs\nnearly by 20%. The code can be found at\n\\href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.\n", "link": "http://arxiv.org/abs/2509.22299v1", "date": "2025-09-26", "relevancy": 2.4034, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEAPr%3A%20Hessian-based%20Efficient%20Atomic%20Expert%20Pruning%20in%20Output%20Space&body=Title%3A%20HEAPr%3A%20Hessian-based%20Efficient%20Atomic%20Expert%20Pruning%20in%20Output%20Space%0AAuthor%3A%20Ke%20Li%20and%20Zheng%20Yang%20and%20Zhongbin%20Zhou%20and%20Feng%20Xue%20and%20Zhonglin%20Jiang%20and%20Wenxiao%20Wang%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20architectures%20in%20large%20language%20models%20%28LLMs%29%0Adeliver%20exceptional%20performance%20and%20reduced%20inference%20costs%20compared%20to%20dense%0ALLMs.%20However%2C%20their%20large%20parameter%20counts%20result%20in%20prohibitive%20memory%0Arequirements%2C%20limiting%20practical%20deployment.%20While%20existing%20pruning%20methods%0Aprimarily%20focus%20on%20expert-level%20pruning%2C%20this%20coarse%20granularity%20often%20leads%20to%0Asubstantial%20accuracy%20degradation.%20In%20this%20work%2C%20we%20introduce%20HEAPr%2C%20a%20novel%0Apruning%20algorithm%20that%20decomposes%20experts%20into%20smaller%2C%20indivisible%20atomic%0Aexperts%2C%20enabling%20more%20precise%20and%20flexible%20atomic%20expert%20pruning.%20To%20measure%0Athe%20importance%20of%20each%20atomic%20expert%2C%20we%20leverage%20second-order%20information%0Abased%20on%20principles%20similar%20to%20Optimal%20Brain%20Surgeon%20%28OBS%29%20theory.%20To%20address%0Athe%20computational%20and%20storage%20challenges%20posed%20by%20second-order%20information%2C%0AHEAPr%20exploits%20the%20inherent%20properties%20of%20atomic%20experts%20to%20transform%20the%0Asecond-order%20information%20from%20expert%20parameters%20into%20that%20of%20atomic%20expert%0Aparameters%2C%20and%20further%20simplifies%20it%20to%20the%20second-order%20information%20of%20atomic%0Aexpert%20outputs.%20This%20approach%20reduces%20the%20space%20complexity%20from%20%24O%28d%5E4%29%24%2C%20where%0Ad%20is%20the%20model%27s%20dimensionality%2C%20to%20%24O%28d%5E2%29%24.%20HEAPr%20requires%20only%20two%20forward%0Apasses%20and%20one%20backward%20pass%20on%20a%20small%20calibration%20set%20to%20compute%20the%0Aimportance%20of%20atomic%20experts.%20Extensive%20experiments%20on%20MoE%20models%2C%20including%0ADeepSeek%20MoE%20and%20Qwen%20MoE%20family%2C%20demonstrate%20that%20HEAPr%20outperforms%20existing%0Aexpert-level%20pruning%20methods%20across%20a%20wide%20range%20of%20compression%20ratios%20and%0Abenchmarks.%20Specifically%2C%20HEAPr%20achieves%20nearly%20lossless%20compression%20at%0Acompression%20ratios%20of%2020%25%20~%2025%25%20in%20most%20models%2C%20while%20also%20reducing%20FLOPs%0Anearly%20by%2020%25.%20The%20code%20can%20be%20found%20at%0A%5Chref%7Bhttps%3A//github.com/LLIKKE/HEAPr%7D%7Bhttps%3A//github.com/LLIKKE/HEAPr%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEAPr%253A%2520Hessian-based%2520Efficient%2520Atomic%2520Expert%2520Pruning%2520in%2520Output%2520Space%26entry.906535625%3DKe%2520Li%2520and%2520Zheng%2520Yang%2520and%2520Zhongbin%2520Zhou%2520and%2520Feng%2520Xue%2520and%2520Zhonglin%2520Jiang%2520and%2520Wenxiao%2520Wang%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%250Adeliver%2520exceptional%2520performance%2520and%2520reduced%2520inference%2520costs%2520compared%2520to%2520dense%250ALLMs.%2520However%252C%2520their%2520large%2520parameter%2520counts%2520result%2520in%2520prohibitive%2520memory%250Arequirements%252C%2520limiting%2520practical%2520deployment.%2520While%2520existing%2520pruning%2520methods%250Aprimarily%2520focus%2520on%2520expert-level%2520pruning%252C%2520this%2520coarse%2520granularity%2520often%2520leads%2520to%250Asubstantial%2520accuracy%2520degradation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520HEAPr%252C%2520a%2520novel%250Apruning%2520algorithm%2520that%2520decomposes%2520experts%2520into%2520smaller%252C%2520indivisible%2520atomic%250Aexperts%252C%2520enabling%2520more%2520precise%2520and%2520flexible%2520atomic%2520expert%2520pruning.%2520To%2520measure%250Athe%2520importance%2520of%2520each%2520atomic%2520expert%252C%2520we%2520leverage%2520second-order%2520information%250Abased%2520on%2520principles%2520similar%2520to%2520Optimal%2520Brain%2520Surgeon%2520%2528OBS%2529%2520theory.%2520To%2520address%250Athe%2520computational%2520and%2520storage%2520challenges%2520posed%2520by%2520second-order%2520information%252C%250AHEAPr%2520exploits%2520the%2520inherent%2520properties%2520of%2520atomic%2520experts%2520to%2520transform%2520the%250Asecond-order%2520information%2520from%2520expert%2520parameters%2520into%2520that%2520of%2520atomic%2520expert%250Aparameters%252C%2520and%2520further%2520simplifies%2520it%2520to%2520the%2520second-order%2520information%2520of%2520atomic%250Aexpert%2520outputs.%2520This%2520approach%2520reduces%2520the%2520space%2520complexity%2520from%2520%2524O%2528d%255E4%2529%2524%252C%2520where%250Ad%2520is%2520the%2520model%2527s%2520dimensionality%252C%2520to%2520%2524O%2528d%255E2%2529%2524.%2520HEAPr%2520requires%2520only%2520two%2520forward%250Apasses%2520and%2520one%2520backward%2520pass%2520on%2520a%2520small%2520calibration%2520set%2520to%2520compute%2520the%250Aimportance%2520of%2520atomic%2520experts.%2520Extensive%2520experiments%2520on%2520MoE%2520models%252C%2520including%250ADeepSeek%2520MoE%2520and%2520Qwen%2520MoE%2520family%252C%2520demonstrate%2520that%2520HEAPr%2520outperforms%2520existing%250Aexpert-level%2520pruning%2520methods%2520across%2520a%2520wide%2520range%2520of%2520compression%2520ratios%2520and%250Abenchmarks.%2520Specifically%252C%2520HEAPr%2520achieves%2520nearly%2520lossless%2520compression%2520at%250Acompression%2520ratios%2520of%252020%2525%2520~%252025%2525%2520in%2520most%2520models%252C%2520while%2520also%2520reducing%2520FLOPs%250Anearly%2520by%252020%2525.%2520The%2520code%2520can%2520be%2520found%2520at%250A%255Chref%257Bhttps%253A//github.com/LLIKKE/HEAPr%257D%257Bhttps%253A//github.com/LLIKKE/HEAPr%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEAPr%3A%20Hessian-based%20Efficient%20Atomic%20Expert%20Pruning%20in%20Output%20Space&entry.906535625=Ke%20Li%20and%20Zheng%20Yang%20and%20Zhongbin%20Zhou%20and%20Feng%20Xue%20and%20Zhonglin%20Jiang%20and%20Wenxiao%20Wang&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20architectures%20in%20large%20language%20models%20%28LLMs%29%0Adeliver%20exceptional%20performance%20and%20reduced%20inference%20costs%20compared%20to%20dense%0ALLMs.%20However%2C%20their%20large%20parameter%20counts%20result%20in%20prohibitive%20memory%0Arequirements%2C%20limiting%20practical%20deployment.%20While%20existing%20pruning%20methods%0Aprimarily%20focus%20on%20expert-level%20pruning%2C%20this%20coarse%20granularity%20often%20leads%20to%0Asubstantial%20accuracy%20degradation.%20In%20this%20work%2C%20we%20introduce%20HEAPr%2C%20a%20novel%0Apruning%20algorithm%20that%20decomposes%20experts%20into%20smaller%2C%20indivisible%20atomic%0Aexperts%2C%20enabling%20more%20precise%20and%20flexible%20atomic%20expert%20pruning.%20To%20measure%0Athe%20importance%20of%20each%20atomic%20expert%2C%20we%20leverage%20second-order%20information%0Abased%20on%20principles%20similar%20to%20Optimal%20Brain%20Surgeon%20%28OBS%29%20theory.%20To%20address%0Athe%20computational%20and%20storage%20challenges%20posed%20by%20second-order%20information%2C%0AHEAPr%20exploits%20the%20inherent%20properties%20of%20atomic%20experts%20to%20transform%20the%0Asecond-order%20information%20from%20expert%20parameters%20into%20that%20of%20atomic%20expert%0Aparameters%2C%20and%20further%20simplifies%20it%20to%20the%20second-order%20information%20of%20atomic%0Aexpert%20outputs.%20This%20approach%20reduces%20the%20space%20complexity%20from%20%24O%28d%5E4%29%24%2C%20where%0Ad%20is%20the%20model%27s%20dimensionality%2C%20to%20%24O%28d%5E2%29%24.%20HEAPr%20requires%20only%20two%20forward%0Apasses%20and%20one%20backward%20pass%20on%20a%20small%20calibration%20set%20to%20compute%20the%0Aimportance%20of%20atomic%20experts.%20Extensive%20experiments%20on%20MoE%20models%2C%20including%0ADeepSeek%20MoE%20and%20Qwen%20MoE%20family%2C%20demonstrate%20that%20HEAPr%20outperforms%20existing%0Aexpert-level%20pruning%20methods%20across%20a%20wide%20range%20of%20compression%20ratios%20and%0Abenchmarks.%20Specifically%2C%20HEAPr%20achieves%20nearly%20lossless%20compression%20at%0Acompression%20ratios%20of%2020%25%20~%2025%25%20in%20most%20models%2C%20while%20also%20reducing%20FLOPs%0Anearly%20by%2020%25.%20The%20code%20can%20be%20found%20at%0A%5Chref%7Bhttps%3A//github.com/LLIKKE/HEAPr%7D%7Bhttps%3A//github.com/LLIKKE/HEAPr%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22299v1&entry.124074799=Read"},
{"title": "The Flood Complex: Large-Scale Persistent Homology on Millions of Points", "author": "Florian Graf and Paolo Pellizzoni and Martin Uray and Stefan Huber and Roland Kwitt", "abstract": "  We consider the problem of computing persistent homology (PH) for large-scale\nEuclidean point cloud data, aimed at downstream machine learning tasks, where\nthe exponential growth of the most widely-used Vietoris-Rips complex imposes\nserious computational limitations. Although more scalable alternatives such as\nthe Alpha complex or sparse Rips approximations exist, they often still result\nin a prohibitively large number of simplices. This poses challenges in the\ncomplex construction and in the subsequent PH computation, prohibiting their\nuse on large-scale point clouds. To mitigate these issues, we introduce the\nFlood complex, inspired by the advantages of the Alpha and Witness complex\nconstructions. Informally, at a given filtration value $r\\geq 0$, the Flood\ncomplex contains all simplices from a Delaunay triangulation of a small subset\nof the point cloud $X$ that are fully covered by balls of radius $r$ emanating\nfrom $X$, a process we call flooding. Our construction allows for efficient PH\ncomputation, possesses several desirable theoretical properties, and is\namenable to GPU parallelization. Scaling experiments on 3D point cloud data\nshow that we can compute PH of up to dimension 2 on several millions of points.\nImportantly, when evaluating object classification performance on real-world\nand synthetic data, we provide evidence that this scaling capability is needed,\nespecially if objects are geometrically or topologically complex, yielding\nperformance superior to other PH-based methods and neural networks for point\ncloud data.\n", "link": "http://arxiv.org/abs/2509.22432v1", "date": "2025-09-26", "relevancy": 2.4014, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5057}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4791}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Flood%20Complex%3A%20Large-Scale%20Persistent%20Homology%20on%20Millions%20of%20Points&body=Title%3A%20The%20Flood%20Complex%3A%20Large-Scale%20Persistent%20Homology%20on%20Millions%20of%20Points%0AAuthor%3A%20Florian%20Graf%20and%20Paolo%20Pellizzoni%20and%20Martin%20Uray%20and%20Stefan%20Huber%20and%20Roland%20Kwitt%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20computing%20persistent%20homology%20%28PH%29%20for%20large-scale%0AEuclidean%20point%20cloud%20data%2C%20aimed%20at%20downstream%20machine%20learning%20tasks%2C%20where%0Athe%20exponential%20growth%20of%20the%20most%20widely-used%20Vietoris-Rips%20complex%20imposes%0Aserious%20computational%20limitations.%20Although%20more%20scalable%20alternatives%20such%20as%0Athe%20Alpha%20complex%20or%20sparse%20Rips%20approximations%20exist%2C%20they%20often%20still%20result%0Ain%20a%20prohibitively%20large%20number%20of%20simplices.%20This%20poses%20challenges%20in%20the%0Acomplex%20construction%20and%20in%20the%20subsequent%20PH%20computation%2C%20prohibiting%20their%0Ause%20on%20large-scale%20point%20clouds.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20the%0AFlood%20complex%2C%20inspired%20by%20the%20advantages%20of%20the%20Alpha%20and%20Witness%20complex%0Aconstructions.%20Informally%2C%20at%20a%20given%20filtration%20value%20%24r%5Cgeq%200%24%2C%20the%20Flood%0Acomplex%20contains%20all%20simplices%20from%20a%20Delaunay%20triangulation%20of%20a%20small%20subset%0Aof%20the%20point%20cloud%20%24X%24%20that%20are%20fully%20covered%20by%20balls%20of%20radius%20%24r%24%20emanating%0Afrom%20%24X%24%2C%20a%20process%20we%20call%20flooding.%20Our%20construction%20allows%20for%20efficient%20PH%0Acomputation%2C%20possesses%20several%20desirable%20theoretical%20properties%2C%20and%20is%0Aamenable%20to%20GPU%20parallelization.%20Scaling%20experiments%20on%203D%20point%20cloud%20data%0Ashow%20that%20we%20can%20compute%20PH%20of%20up%20to%20dimension%202%20on%20several%20millions%20of%20points.%0AImportantly%2C%20when%20evaluating%20object%20classification%20performance%20on%20real-world%0Aand%20synthetic%20data%2C%20we%20provide%20evidence%20that%20this%20scaling%20capability%20is%20needed%2C%0Aespecially%20if%20objects%20are%20geometrically%20or%20topologically%20complex%2C%20yielding%0Aperformance%20superior%20to%20other%20PH-based%20methods%20and%20neural%20networks%20for%20point%0Acloud%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Flood%2520Complex%253A%2520Large-Scale%2520Persistent%2520Homology%2520on%2520Millions%2520of%2520Points%26entry.906535625%3DFlorian%2520Graf%2520and%2520Paolo%2520Pellizzoni%2520and%2520Martin%2520Uray%2520and%2520Stefan%2520Huber%2520and%2520Roland%2520Kwitt%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520computing%2520persistent%2520homology%2520%2528PH%2529%2520for%2520large-scale%250AEuclidean%2520point%2520cloud%2520data%252C%2520aimed%2520at%2520downstream%2520machine%2520learning%2520tasks%252C%2520where%250Athe%2520exponential%2520growth%2520of%2520the%2520most%2520widely-used%2520Vietoris-Rips%2520complex%2520imposes%250Aserious%2520computational%2520limitations.%2520Although%2520more%2520scalable%2520alternatives%2520such%2520as%250Athe%2520Alpha%2520complex%2520or%2520sparse%2520Rips%2520approximations%2520exist%252C%2520they%2520often%2520still%2520result%250Ain%2520a%2520prohibitively%2520large%2520number%2520of%2520simplices.%2520This%2520poses%2520challenges%2520in%2520the%250Acomplex%2520construction%2520and%2520in%2520the%2520subsequent%2520PH%2520computation%252C%2520prohibiting%2520their%250Ause%2520on%2520large-scale%2520point%2520clouds.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520introduce%2520the%250AFlood%2520complex%252C%2520inspired%2520by%2520the%2520advantages%2520of%2520the%2520Alpha%2520and%2520Witness%2520complex%250Aconstructions.%2520Informally%252C%2520at%2520a%2520given%2520filtration%2520value%2520%2524r%255Cgeq%25200%2524%252C%2520the%2520Flood%250Acomplex%2520contains%2520all%2520simplices%2520from%2520a%2520Delaunay%2520triangulation%2520of%2520a%2520small%2520subset%250Aof%2520the%2520point%2520cloud%2520%2524X%2524%2520that%2520are%2520fully%2520covered%2520by%2520balls%2520of%2520radius%2520%2524r%2524%2520emanating%250Afrom%2520%2524X%2524%252C%2520a%2520process%2520we%2520call%2520flooding.%2520Our%2520construction%2520allows%2520for%2520efficient%2520PH%250Acomputation%252C%2520possesses%2520several%2520desirable%2520theoretical%2520properties%252C%2520and%2520is%250Aamenable%2520to%2520GPU%2520parallelization.%2520Scaling%2520experiments%2520on%25203D%2520point%2520cloud%2520data%250Ashow%2520that%2520we%2520can%2520compute%2520PH%2520of%2520up%2520to%2520dimension%25202%2520on%2520several%2520millions%2520of%2520points.%250AImportantly%252C%2520when%2520evaluating%2520object%2520classification%2520performance%2520on%2520real-world%250Aand%2520synthetic%2520data%252C%2520we%2520provide%2520evidence%2520that%2520this%2520scaling%2520capability%2520is%2520needed%252C%250Aespecially%2520if%2520objects%2520are%2520geometrically%2520or%2520topologically%2520complex%252C%2520yielding%250Aperformance%2520superior%2520to%2520other%2520PH-based%2520methods%2520and%2520neural%2520networks%2520for%2520point%250Acloud%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Flood%20Complex%3A%20Large-Scale%20Persistent%20Homology%20on%20Millions%20of%20Points&entry.906535625=Florian%20Graf%20and%20Paolo%20Pellizzoni%20and%20Martin%20Uray%20and%20Stefan%20Huber%20and%20Roland%20Kwitt&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20computing%20persistent%20homology%20%28PH%29%20for%20large-scale%0AEuclidean%20point%20cloud%20data%2C%20aimed%20at%20downstream%20machine%20learning%20tasks%2C%20where%0Athe%20exponential%20growth%20of%20the%20most%20widely-used%20Vietoris-Rips%20complex%20imposes%0Aserious%20computational%20limitations.%20Although%20more%20scalable%20alternatives%20such%20as%0Athe%20Alpha%20complex%20or%20sparse%20Rips%20approximations%20exist%2C%20they%20often%20still%20result%0Ain%20a%20prohibitively%20large%20number%20of%20simplices.%20This%20poses%20challenges%20in%20the%0Acomplex%20construction%20and%20in%20the%20subsequent%20PH%20computation%2C%20prohibiting%20their%0Ause%20on%20large-scale%20point%20clouds.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20the%0AFlood%20complex%2C%20inspired%20by%20the%20advantages%20of%20the%20Alpha%20and%20Witness%20complex%0Aconstructions.%20Informally%2C%20at%20a%20given%20filtration%20value%20%24r%5Cgeq%200%24%2C%20the%20Flood%0Acomplex%20contains%20all%20simplices%20from%20a%20Delaunay%20triangulation%20of%20a%20small%20subset%0Aof%20the%20point%20cloud%20%24X%24%20that%20are%20fully%20covered%20by%20balls%20of%20radius%20%24r%24%20emanating%0Afrom%20%24X%24%2C%20a%20process%20we%20call%20flooding.%20Our%20construction%20allows%20for%20efficient%20PH%0Acomputation%2C%20possesses%20several%20desirable%20theoretical%20properties%2C%20and%20is%0Aamenable%20to%20GPU%20parallelization.%20Scaling%20experiments%20on%203D%20point%20cloud%20data%0Ashow%20that%20we%20can%20compute%20PH%20of%20up%20to%20dimension%202%20on%20several%20millions%20of%20points.%0AImportantly%2C%20when%20evaluating%20object%20classification%20performance%20on%20real-world%0Aand%20synthetic%20data%2C%20we%20provide%20evidence%20that%20this%20scaling%20capability%20is%20needed%2C%0Aespecially%20if%20objects%20are%20geometrically%20or%20topologically%20complex%2C%20yielding%0Aperformance%20superior%20to%20other%20PH-based%20methods%20and%20neural%20networks%20for%20point%0Acloud%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22432v1&entry.124074799=Read"},
{"title": "DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text\n  Instructions", "author": "Quanzhou Li and Zhonghua Wu and Jingbo Wang and Chen Change Loy and Bo Dai", "abstract": "  Learning to generate dual-hand grasps that respect object semantics is\nessential for robust hand-object interaction but remains largely underexplored\ndue to dataset scarcity. Existing grasp datasets predominantly focus on\nsingle-hand interactions and contain only limited semantic part annotations. To\naddress these challenges, we introduce a pipeline, SymOpt, that constructs a\nlarge-scale dual-hand grasp dataset by leveraging existing single-hand datasets\nand exploiting object and hand symmetries. Building on this, we propose a\ntext-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand\nAffordance-aware Grasps for unseen objects. Our approach incorporates a novel\ndual-hand affordance representation and follows a two-stage design, which\nenables effective learning from a small set of segmented training objects while\nscaling to a much larger pool of unsegmented data. Extensive experiments\ndemonstrate that our method produces diverse and semantically consistent\ngrasps, outperforming strong baselines in both grasp quality and generalization\nto unseen objects. The project page is at\nhttps://quanzhou-li.github.io/DHAGrasp/.\n", "link": "http://arxiv.org/abs/2509.22175v1", "date": "2025-09-26", "relevancy": 2.3574, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6819}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5358}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DHAGrasp%3A%20Synthesizing%20Affordance-Aware%20Dual-Hand%20Grasps%20with%20Text%0A%20%20Instructions&body=Title%3A%20DHAGrasp%3A%20Synthesizing%20Affordance-Aware%20Dual-Hand%20Grasps%20with%20Text%0A%20%20Instructions%0AAuthor%3A%20Quanzhou%20Li%20and%20Zhonghua%20Wu%20and%20Jingbo%20Wang%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Learning%20to%20generate%20dual-hand%20grasps%20that%20respect%20object%20semantics%20is%0Aessential%20for%20robust%20hand-object%20interaction%20but%20remains%20largely%20underexplored%0Adue%20to%20dataset%20scarcity.%20Existing%20grasp%20datasets%20predominantly%20focus%20on%0Asingle-hand%20interactions%20and%20contain%20only%20limited%20semantic%20part%20annotations.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20pipeline%2C%20SymOpt%2C%20that%20constructs%20a%0Alarge-scale%20dual-hand%20grasp%20dataset%20by%20leveraging%20existing%20single-hand%20datasets%0Aand%20exploiting%20object%20and%20hand%20symmetries.%20Building%20on%20this%2C%20we%20propose%20a%0Atext-guided%20dual-hand%20grasp%20generator%2C%20DHAGrasp%2C%20that%20synthesizes%20Dual-Hand%0AAffordance-aware%20Grasps%20for%20unseen%20objects.%20Our%20approach%20incorporates%20a%20novel%0Adual-hand%20affordance%20representation%20and%20follows%20a%20two-stage%20design%2C%20which%0Aenables%20effective%20learning%20from%20a%20small%20set%20of%20segmented%20training%20objects%20while%0Ascaling%20to%20a%20much%20larger%20pool%20of%20unsegmented%20data.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20produces%20diverse%20and%20semantically%20consistent%0Agrasps%2C%20outperforming%20strong%20baselines%20in%20both%20grasp%20quality%20and%20generalization%0Ato%20unseen%20objects.%20The%20project%20page%20is%20at%0Ahttps%3A//quanzhou-li.github.io/DHAGrasp/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDHAGrasp%253A%2520Synthesizing%2520Affordance-Aware%2520Dual-Hand%2520Grasps%2520with%2520Text%250A%2520%2520Instructions%26entry.906535625%3DQuanzhou%2520Li%2520and%2520Zhonghua%2520Wu%2520and%2520Jingbo%2520Wang%2520and%2520Chen%2520Change%2520Loy%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520Learning%2520to%2520generate%2520dual-hand%2520grasps%2520that%2520respect%2520object%2520semantics%2520is%250Aessential%2520for%2520robust%2520hand-object%2520interaction%2520but%2520remains%2520largely%2520underexplored%250Adue%2520to%2520dataset%2520scarcity.%2520Existing%2520grasp%2520datasets%2520predominantly%2520focus%2520on%250Asingle-hand%2520interactions%2520and%2520contain%2520only%2520limited%2520semantic%2520part%2520annotations.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520pipeline%252C%2520SymOpt%252C%2520that%2520constructs%2520a%250Alarge-scale%2520dual-hand%2520grasp%2520dataset%2520by%2520leveraging%2520existing%2520single-hand%2520datasets%250Aand%2520exploiting%2520object%2520and%2520hand%2520symmetries.%2520Building%2520on%2520this%252C%2520we%2520propose%2520a%250Atext-guided%2520dual-hand%2520grasp%2520generator%252C%2520DHAGrasp%252C%2520that%2520synthesizes%2520Dual-Hand%250AAffordance-aware%2520Grasps%2520for%2520unseen%2520objects.%2520Our%2520approach%2520incorporates%2520a%2520novel%250Adual-hand%2520affordance%2520representation%2520and%2520follows%2520a%2520two-stage%2520design%252C%2520which%250Aenables%2520effective%2520learning%2520from%2520a%2520small%2520set%2520of%2520segmented%2520training%2520objects%2520while%250Ascaling%2520to%2520a%2520much%2520larger%2520pool%2520of%2520unsegmented%2520data.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520produces%2520diverse%2520and%2520semantically%2520consistent%250Agrasps%252C%2520outperforming%2520strong%2520baselines%2520in%2520both%2520grasp%2520quality%2520and%2520generalization%250Ato%2520unseen%2520objects.%2520The%2520project%2520page%2520is%2520at%250Ahttps%253A//quanzhou-li.github.io/DHAGrasp/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DHAGrasp%3A%20Synthesizing%20Affordance-Aware%20Dual-Hand%20Grasps%20with%20Text%0A%20%20Instructions&entry.906535625=Quanzhou%20Li%20and%20Zhonghua%20Wu%20and%20Jingbo%20Wang%20and%20Chen%20Change%20Loy%20and%20Bo%20Dai&entry.1292438233=%20%20Learning%20to%20generate%20dual-hand%20grasps%20that%20respect%20object%20semantics%20is%0Aessential%20for%20robust%20hand-object%20interaction%20but%20remains%20largely%20underexplored%0Adue%20to%20dataset%20scarcity.%20Existing%20grasp%20datasets%20predominantly%20focus%20on%0Asingle-hand%20interactions%20and%20contain%20only%20limited%20semantic%20part%20annotations.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20pipeline%2C%20SymOpt%2C%20that%20constructs%20a%0Alarge-scale%20dual-hand%20grasp%20dataset%20by%20leveraging%20existing%20single-hand%20datasets%0Aand%20exploiting%20object%20and%20hand%20symmetries.%20Building%20on%20this%2C%20we%20propose%20a%0Atext-guided%20dual-hand%20grasp%20generator%2C%20DHAGrasp%2C%20that%20synthesizes%20Dual-Hand%0AAffordance-aware%20Grasps%20for%20unseen%20objects.%20Our%20approach%20incorporates%20a%20novel%0Adual-hand%20affordance%20representation%20and%20follows%20a%20two-stage%20design%2C%20which%0Aenables%20effective%20learning%20from%20a%20small%20set%20of%20segmented%20training%20objects%20while%0Ascaling%20to%20a%20much%20larger%20pool%20of%20unsegmented%20data.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20produces%20diverse%20and%20semantically%20consistent%0Agrasps%2C%20outperforming%20strong%20baselines%20in%20both%20grasp%20quality%20and%20generalization%0Ato%20unseen%20objects.%20The%20project%20page%20is%20at%0Ahttps%3A//quanzhou-li.github.io/DHAGrasp/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22175v1&entry.124074799=Read"},
{"title": "WeightLoRA: Keep Only Necessary Adapters", "author": "Andrey Veprikov and Vladimir Solodkin and Alexander Zyl and Andrey Savchenko and Aleksandr Beznosikov", "abstract": "  The widespread utilization of language models in modern applications is\ninconceivable without Parameter-Efficient Fine-Tuning techniques, such as\nlow-rank adaptation ($\\texttt{LoRA}$), which adds trainable adapters to\nselected layers. Although $\\texttt{LoRA}$ may obtain accurate solutions, it\nrequires significant memory to train large models and intuition on which layers\nto add adapters. In this paper, we propose a novel method,\n$\\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the\nmost critical $\\texttt{LoRA}$ heads throughout the optimization process. As a\nresult, we can significantly reduce the number of trainable parameters while\nmaintaining the capability to obtain consistent or even superior metric values.\nWe conduct experiments for a series of competitive benchmarks and DeBERTa,\nBART, and Llama models, comparing our method with different adaptive\napproaches. The experimental results demonstrate the efficacy of\n$\\texttt{WeightLoRA}$ and the superior performance of $\\texttt{WeightLoRA+}$ in\nalmost all cases.\n", "link": "http://arxiv.org/abs/2506.02724v2", "date": "2025-09-26", "relevancy": 2.3559, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4784}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4723}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters&body=Title%3A%20WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters%0AAuthor%3A%20Andrey%20Veprikov%20and%20Vladimir%20Solodkin%20and%20Alexander%20Zyl%20and%20Andrey%20Savchenko%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20The%20widespread%20utilization%20of%20language%20models%20in%20modern%20applications%20is%0Ainconceivable%20without%20Parameter-Efficient%20Fine-Tuning%20techniques%2C%20such%20as%0Alow-rank%20adaptation%20%28%24%5Ctexttt%7BLoRA%7D%24%29%2C%20which%20adds%20trainable%20adapters%20to%0Aselected%20layers.%20Although%20%24%5Ctexttt%7BLoRA%7D%24%20may%20obtain%20accurate%20solutions%2C%20it%0Arequires%20significant%20memory%20to%20train%20large%20models%20and%20intuition%20on%20which%20layers%0Ato%20add%20adapters.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0A%24%5Ctexttt%7BWeightLoRA%7D%24%2C%20which%20overcomes%20this%20issue%20by%20adaptive%20selection%20of%20the%0Amost%20critical%20%24%5Ctexttt%7BLoRA%7D%24%20heads%20throughout%20the%20optimization%20process.%20As%20a%0Aresult%2C%20we%20can%20significantly%20reduce%20the%20number%20of%20trainable%20parameters%20while%0Amaintaining%20the%20capability%20to%20obtain%20consistent%20or%20even%20superior%20metric%20values.%0AWe%20conduct%20experiments%20for%20a%20series%20of%20competitive%20benchmarks%20and%20DeBERTa%2C%0ABART%2C%20and%20Llama%20models%2C%20comparing%20our%20method%20with%20different%20adaptive%0Aapproaches.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%0A%24%5Ctexttt%7BWeightLoRA%7D%24%20and%20the%20superior%20performance%20of%20%24%5Ctexttt%7BWeightLoRA%2B%7D%24%20in%0Aalmost%20all%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeightLoRA%253A%2520Keep%2520Only%2520Necessary%2520Adapters%26entry.906535625%3DAndrey%2520Veprikov%2520and%2520Vladimir%2520Solodkin%2520and%2520Alexander%2520Zyl%2520and%2520Andrey%2520Savchenko%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520The%2520widespread%2520utilization%2520of%2520language%2520models%2520in%2520modern%2520applications%2520is%250Ainconceivable%2520without%2520Parameter-Efficient%2520Fine-Tuning%2520techniques%252C%2520such%2520as%250Alow-rank%2520adaptation%2520%2528%2524%255Ctexttt%257BLoRA%257D%2524%2529%252C%2520which%2520adds%2520trainable%2520adapters%2520to%250Aselected%2520layers.%2520Although%2520%2524%255Ctexttt%257BLoRA%257D%2524%2520may%2520obtain%2520accurate%2520solutions%252C%2520it%250Arequires%2520significant%2520memory%2520to%2520train%2520large%2520models%2520and%2520intuition%2520on%2520which%2520layers%250Ato%2520add%2520adapters.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250A%2524%255Ctexttt%257BWeightLoRA%257D%2524%252C%2520which%2520overcomes%2520this%2520issue%2520by%2520adaptive%2520selection%2520of%2520the%250Amost%2520critical%2520%2524%255Ctexttt%257BLoRA%257D%2524%2520heads%2520throughout%2520the%2520optimization%2520process.%2520As%2520a%250Aresult%252C%2520we%2520can%2520significantly%2520reduce%2520the%2520number%2520of%2520trainable%2520parameters%2520while%250Amaintaining%2520the%2520capability%2520to%2520obtain%2520consistent%2520or%2520even%2520superior%2520metric%2520values.%250AWe%2520conduct%2520experiments%2520for%2520a%2520series%2520of%2520competitive%2520benchmarks%2520and%2520DeBERTa%252C%250ABART%252C%2520and%2520Llama%2520models%252C%2520comparing%2520our%2520method%2520with%2520different%2520adaptive%250Aapproaches.%2520The%2520experimental%2520results%2520demonstrate%2520the%2520efficacy%2520of%250A%2524%255Ctexttt%257BWeightLoRA%257D%2524%2520and%2520the%2520superior%2520performance%2520of%2520%2524%255Ctexttt%257BWeightLoRA%252B%257D%2524%2520in%250Aalmost%2520all%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters&entry.906535625=Andrey%20Veprikov%20and%20Vladimir%20Solodkin%20and%20Alexander%20Zyl%20and%20Andrey%20Savchenko%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20The%20widespread%20utilization%20of%20language%20models%20in%20modern%20applications%20is%0Ainconceivable%20without%20Parameter-Efficient%20Fine-Tuning%20techniques%2C%20such%20as%0Alow-rank%20adaptation%20%28%24%5Ctexttt%7BLoRA%7D%24%29%2C%20which%20adds%20trainable%20adapters%20to%0Aselected%20layers.%20Although%20%24%5Ctexttt%7BLoRA%7D%24%20may%20obtain%20accurate%20solutions%2C%20it%0Arequires%20significant%20memory%20to%20train%20large%20models%20and%20intuition%20on%20which%20layers%0Ato%20add%20adapters.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0A%24%5Ctexttt%7BWeightLoRA%7D%24%2C%20which%20overcomes%20this%20issue%20by%20adaptive%20selection%20of%20the%0Amost%20critical%20%24%5Ctexttt%7BLoRA%7D%24%20heads%20throughout%20the%20optimization%20process.%20As%20a%0Aresult%2C%20we%20can%20significantly%20reduce%20the%20number%20of%20trainable%20parameters%20while%0Amaintaining%20the%20capability%20to%20obtain%20consistent%20or%20even%20superior%20metric%20values.%0AWe%20conduct%20experiments%20for%20a%20series%20of%20competitive%20benchmarks%20and%20DeBERTa%2C%0ABART%2C%20and%20Llama%20models%2C%20comparing%20our%20method%20with%20different%20adaptive%0Aapproaches.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%0A%24%5Ctexttt%7BWeightLoRA%7D%24%20and%20the%20superior%20performance%20of%20%24%5Ctexttt%7BWeightLoRA%2B%7D%24%20in%0Aalmost%20all%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02724v2&entry.124074799=Read"},
{"title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs", "author": "Xingyu Fu and Siyi Liu and Yinuo Xu and Pan Lu and Guangqiuse Hu and Tianbo Yang and Taran Anantasagar and Christopher Shen and Yikai Mao and Yuanzhe Liu and Keyush Shah and Chung Un Lee and Yejin Choi and James Zou and Dan Roth and Chris Callison-Burch", "abstract": "  Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detect deepfake traces within a generated video, i.e.,\nspatiotemporal grounded visual artifacts that reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirst fine-grained, spatially- and temporally- aware benchmark that annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories of\ndeepfake traces that lead humans to identify a video as AI-generated, and train\nmultimodal language models (LMs) as reward models to mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier than fine-grained deepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), to spatial grounding, to temporal labeling (hardest).\nBy foregrounding human-perceived deepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration.\n", "link": "http://arxiv.org/abs/2509.22646v1", "date": "2025-09-26", "relevancy": 2.342, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6121}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5762}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Human-Perceived%20Fakeness%20in%20AI-Generated%20Videos%20via%20Multimodal%0A%20%20LLMs&body=Title%3A%20Learning%20Human-Perceived%20Fakeness%20in%20AI-Generated%20Videos%20via%20Multimodal%0A%20%20LLMs%0AAuthor%3A%20Xingyu%20Fu%20and%20Siyi%20Liu%20and%20Yinuo%20Xu%20and%20Pan%20Lu%20and%20Guangqiuse%20Hu%20and%20Tianbo%20Yang%20and%20Taran%20Anantasagar%20and%20Christopher%20Shen%20and%20Yikai%20Mao%20and%20Yuanzhe%20Liu%20and%20Keyush%20Shah%20and%20Chung%20Un%20Lee%20and%20Yejin%20Choi%20and%20James%20Zou%20and%20Dan%20Roth%20and%20Chris%20Callison-Burch%0AAbstract%3A%20%20%20Can%20humans%20identify%20AI-generated%20%28fake%29%20videos%20and%20provide%20grounded%20reasons%3F%0AWhile%20video%20generation%20models%20have%20advanced%20rapidly%2C%20a%20critical%20dimension%20--%0Awhether%20humans%20can%20detect%20deepfake%20traces%20within%20a%20generated%20video%2C%20i.e.%2C%0Aspatiotemporal%20grounded%20visual%20artifacts%20that%20reveal%20a%20video%20as%20machine%0Agenerated%20--%20has%20been%20largely%20overlooked.%20We%20introduce%20DeeptraceReward%2C%20the%0Afirst%20fine-grained%2C%20spatially-%20and%20temporally-%20aware%20benchmark%20that%20annotates%0Ahuman-perceived%20fake%20traces%20for%20video%20generation%20reward.%20The%20dataset%20comprises%0A4.3K%20detailed%20annotations%20across%203.3K%20high-quality%20generated%20videos.%20Each%0Aannotation%20provides%20a%20natural-language%20explanation%2C%20pinpoints%20a%20bounding-box%0Aregion%20containing%20the%20perceived%20trace%2C%20and%20marks%20precise%20onset%20and%20offset%0Atimestamps.%20We%20consolidate%20these%20annotations%20into%209%20major%20categories%20of%0Adeepfake%20traces%20that%20lead%20humans%20to%20identify%20a%20video%20as%20AI-generated%2C%20and%20train%0Amultimodal%20language%20models%20%28LMs%29%20as%20reward%20models%20to%20mimic%20human%20judgments%20and%0Alocalizations.%20On%20DeeptraceReward%2C%20our%207B%20reward%20model%20outperforms%20GPT-5%20by%0A34.7%25%20on%20average%20across%20fake%20clue%20identification%2C%20grounding%2C%20and%20explanation.%0AInterestingly%2C%20we%20observe%20a%20consistent%20difficulty%20gradient%3A%20binary%20fake%20v.s.%0Areal%20classification%20is%20substantially%20easier%20than%20fine-grained%20deepfake%20trace%0Adetection%3B%20within%20the%20latter%2C%20performance%20degrades%20from%20natural%20language%0Aexplanations%20%28easiest%29%2C%20to%20spatial%20grounding%2C%20to%20temporal%20labeling%20%28hardest%29.%0ABy%20foregrounding%20human-perceived%20deepfake%20traces%2C%20DeeptraceReward%20provides%20a%0Arigorous%20testbed%20and%20training%20signal%20for%20socially%20aware%20and%20trustworthy%20video%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Human-Perceived%2520Fakeness%2520in%2520AI-Generated%2520Videos%2520via%2520Multimodal%250A%2520%2520LLMs%26entry.906535625%3DXingyu%2520Fu%2520and%2520Siyi%2520Liu%2520and%2520Yinuo%2520Xu%2520and%2520Pan%2520Lu%2520and%2520Guangqiuse%2520Hu%2520and%2520Tianbo%2520Yang%2520and%2520Taran%2520Anantasagar%2520and%2520Christopher%2520Shen%2520and%2520Yikai%2520Mao%2520and%2520Yuanzhe%2520Liu%2520and%2520Keyush%2520Shah%2520and%2520Chung%2520Un%2520Lee%2520and%2520Yejin%2520Choi%2520and%2520James%2520Zou%2520and%2520Dan%2520Roth%2520and%2520Chris%2520Callison-Burch%26entry.1292438233%3D%2520%2520Can%2520humans%2520identify%2520AI-generated%2520%2528fake%2529%2520videos%2520and%2520provide%2520grounded%2520reasons%253F%250AWhile%2520video%2520generation%2520models%2520have%2520advanced%2520rapidly%252C%2520a%2520critical%2520dimension%2520--%250Awhether%2520humans%2520can%2520detect%2520deepfake%2520traces%2520within%2520a%2520generated%2520video%252C%2520i.e.%252C%250Aspatiotemporal%2520grounded%2520visual%2520artifacts%2520that%2520reveal%2520a%2520video%2520as%2520machine%250Agenerated%2520--%2520has%2520been%2520largely%2520overlooked.%2520We%2520introduce%2520DeeptraceReward%252C%2520the%250Afirst%2520fine-grained%252C%2520spatially-%2520and%2520temporally-%2520aware%2520benchmark%2520that%2520annotates%250Ahuman-perceived%2520fake%2520traces%2520for%2520video%2520generation%2520reward.%2520The%2520dataset%2520comprises%250A4.3K%2520detailed%2520annotations%2520across%25203.3K%2520high-quality%2520generated%2520videos.%2520Each%250Aannotation%2520provides%2520a%2520natural-language%2520explanation%252C%2520pinpoints%2520a%2520bounding-box%250Aregion%2520containing%2520the%2520perceived%2520trace%252C%2520and%2520marks%2520precise%2520onset%2520and%2520offset%250Atimestamps.%2520We%2520consolidate%2520these%2520annotations%2520into%25209%2520major%2520categories%2520of%250Adeepfake%2520traces%2520that%2520lead%2520humans%2520to%2520identify%2520a%2520video%2520as%2520AI-generated%252C%2520and%2520train%250Amultimodal%2520language%2520models%2520%2528LMs%2529%2520as%2520reward%2520models%2520to%2520mimic%2520human%2520judgments%2520and%250Alocalizations.%2520On%2520DeeptraceReward%252C%2520our%25207B%2520reward%2520model%2520outperforms%2520GPT-5%2520by%250A34.7%2525%2520on%2520average%2520across%2520fake%2520clue%2520identification%252C%2520grounding%252C%2520and%2520explanation.%250AInterestingly%252C%2520we%2520observe%2520a%2520consistent%2520difficulty%2520gradient%253A%2520binary%2520fake%2520v.s.%250Areal%2520classification%2520is%2520substantially%2520easier%2520than%2520fine-grained%2520deepfake%2520trace%250Adetection%253B%2520within%2520the%2520latter%252C%2520performance%2520degrades%2520from%2520natural%2520language%250Aexplanations%2520%2528easiest%2529%252C%2520to%2520spatial%2520grounding%252C%2520to%2520temporal%2520labeling%2520%2528hardest%2529.%250ABy%2520foregrounding%2520human-perceived%2520deepfake%2520traces%252C%2520DeeptraceReward%2520provides%2520a%250Arigorous%2520testbed%2520and%2520training%2520signal%2520for%2520socially%2520aware%2520and%2520trustworthy%2520video%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Human-Perceived%20Fakeness%20in%20AI-Generated%20Videos%20via%20Multimodal%0A%20%20LLMs&entry.906535625=Xingyu%20Fu%20and%20Siyi%20Liu%20and%20Yinuo%20Xu%20and%20Pan%20Lu%20and%20Guangqiuse%20Hu%20and%20Tianbo%20Yang%20and%20Taran%20Anantasagar%20and%20Christopher%20Shen%20and%20Yikai%20Mao%20and%20Yuanzhe%20Liu%20and%20Keyush%20Shah%20and%20Chung%20Un%20Lee%20and%20Yejin%20Choi%20and%20James%20Zou%20and%20Dan%20Roth%20and%20Chris%20Callison-Burch&entry.1292438233=%20%20Can%20humans%20identify%20AI-generated%20%28fake%29%20videos%20and%20provide%20grounded%20reasons%3F%0AWhile%20video%20generation%20models%20have%20advanced%20rapidly%2C%20a%20critical%20dimension%20--%0Awhether%20humans%20can%20detect%20deepfake%20traces%20within%20a%20generated%20video%2C%20i.e.%2C%0Aspatiotemporal%20grounded%20visual%20artifacts%20that%20reveal%20a%20video%20as%20machine%0Agenerated%20--%20has%20been%20largely%20overlooked.%20We%20introduce%20DeeptraceReward%2C%20the%0Afirst%20fine-grained%2C%20spatially-%20and%20temporally-%20aware%20benchmark%20that%20annotates%0Ahuman-perceived%20fake%20traces%20for%20video%20generation%20reward.%20The%20dataset%20comprises%0A4.3K%20detailed%20annotations%20across%203.3K%20high-quality%20generated%20videos.%20Each%0Aannotation%20provides%20a%20natural-language%20explanation%2C%20pinpoints%20a%20bounding-box%0Aregion%20containing%20the%20perceived%20trace%2C%20and%20marks%20precise%20onset%20and%20offset%0Atimestamps.%20We%20consolidate%20these%20annotations%20into%209%20major%20categories%20of%0Adeepfake%20traces%20that%20lead%20humans%20to%20identify%20a%20video%20as%20AI-generated%2C%20and%20train%0Amultimodal%20language%20models%20%28LMs%29%20as%20reward%20models%20to%20mimic%20human%20judgments%20and%0Alocalizations.%20On%20DeeptraceReward%2C%20our%207B%20reward%20model%20outperforms%20GPT-5%20by%0A34.7%25%20on%20average%20across%20fake%20clue%20identification%2C%20grounding%2C%20and%20explanation.%0AInterestingly%2C%20we%20observe%20a%20consistent%20difficulty%20gradient%3A%20binary%20fake%20v.s.%0Areal%20classification%20is%20substantially%20easier%20than%20fine-grained%20deepfake%20trace%0Adetection%3B%20within%20the%20latter%2C%20performance%20degrades%20from%20natural%20language%0Aexplanations%20%28easiest%29%2C%20to%20spatial%20grounding%2C%20to%20temporal%20labeling%20%28hardest%29.%0ABy%20foregrounding%20human-perceived%20deepfake%20traces%2C%20DeeptraceReward%20provides%20a%0Arigorous%20testbed%20and%20training%20signal%20for%20socially%20aware%20and%20trustworthy%20video%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22646v1&entry.124074799=Read"},
{"title": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing", "author": "Syed Mahbubul Huq and Daniel Brito and Daniel Sikar and Rajesh Mojumder", "abstract": "  This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks.\n", "link": "http://arxiv.org/abs/2509.22255v1", "date": "2025-09-26", "relevancy": 2.3366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20LLMs%20for%20Combinatorial%20Optimization%3A%20One-Phase%20and%20Two-Phase%0A%20%20Heuristics%20for%202D%20Bin-Packing&body=Title%3A%20Evaluating%20LLMs%20for%20Combinatorial%20Optimization%3A%20One-Phase%20and%20Two-Phase%0A%20%20Heuristics%20for%202D%20Bin-Packing%0AAuthor%3A%20Syed%20Mahbubul%20Huq%20and%20Daniel%20Brito%20and%20Daniel%20Sikar%20and%20Rajesh%20Mojumder%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20evaluation%20framework%20for%20assessing%20Large%20Language%0AModels%27%20%28LLMs%29%20capabilities%20in%20combinatorial%20optimization%2C%20specifically%0Aaddressing%20the%202D%20bin-packing%20problem.%20We%20introduce%20a%20systematic%20methodology%0Athat%20combines%20LLMs%20with%20evolutionary%20algorithms%20to%20generate%20and%20refine%0Aheuristic%20solutions%20iteratively.%20Through%20comprehensive%20experiments%20comparing%0ALLM%20generated%20heuristics%20against%20traditional%20approaches%20%28Finite%20First-Fit%20and%0AHybrid%20First-Fit%29%2C%20we%20demonstrate%20that%20LLMs%20can%20produce%20more%20efficient%0Asolutions%20while%20requiring%20fewer%20computational%20resources.%20Our%20evaluation%20reveals%0Athat%20GPT-4o%20achieves%20optimal%20solutions%20within%20two%20iterations%2C%20reducing%20average%0Abin%20usage%20from%2016%20to%2015%20bins%20while%20improving%20space%20utilization%20from%200.76-0.78%0Ato%200.83.%20This%20work%20contributes%20to%20understanding%20LLM%20evaluation%20in%20specialized%0Adomains%20and%20establishes%20benchmarks%20for%20assessing%20LLM%20performance%20in%0Acombinatorial%20optimization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520LLMs%2520for%2520Combinatorial%2520Optimization%253A%2520One-Phase%2520and%2520Two-Phase%250A%2520%2520Heuristics%2520for%25202D%2520Bin-Packing%26entry.906535625%3DSyed%2520Mahbubul%2520Huq%2520and%2520Daniel%2520Brito%2520and%2520Daniel%2520Sikar%2520and%2520Rajesh%2520Mojumder%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520evaluation%2520framework%2520for%2520assessing%2520Large%2520Language%250AModels%2527%2520%2528LLMs%2529%2520capabilities%2520in%2520combinatorial%2520optimization%252C%2520specifically%250Aaddressing%2520the%25202D%2520bin-packing%2520problem.%2520We%2520introduce%2520a%2520systematic%2520methodology%250Athat%2520combines%2520LLMs%2520with%2520evolutionary%2520algorithms%2520to%2520generate%2520and%2520refine%250Aheuristic%2520solutions%2520iteratively.%2520Through%2520comprehensive%2520experiments%2520comparing%250ALLM%2520generated%2520heuristics%2520against%2520traditional%2520approaches%2520%2528Finite%2520First-Fit%2520and%250AHybrid%2520First-Fit%2529%252C%2520we%2520demonstrate%2520that%2520LLMs%2520can%2520produce%2520more%2520efficient%250Asolutions%2520while%2520requiring%2520fewer%2520computational%2520resources.%2520Our%2520evaluation%2520reveals%250Athat%2520GPT-4o%2520achieves%2520optimal%2520solutions%2520within%2520two%2520iterations%252C%2520reducing%2520average%250Abin%2520usage%2520from%252016%2520to%252015%2520bins%2520while%2520improving%2520space%2520utilization%2520from%25200.76-0.78%250Ato%25200.83.%2520This%2520work%2520contributes%2520to%2520understanding%2520LLM%2520evaluation%2520in%2520specialized%250Adomains%2520and%2520establishes%2520benchmarks%2520for%2520assessing%2520LLM%2520performance%2520in%250Acombinatorial%2520optimization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20LLMs%20for%20Combinatorial%20Optimization%3A%20One-Phase%20and%20Two-Phase%0A%20%20Heuristics%20for%202D%20Bin-Packing&entry.906535625=Syed%20Mahbubul%20Huq%20and%20Daniel%20Brito%20and%20Daniel%20Sikar%20and%20Rajesh%20Mojumder&entry.1292438233=%20%20This%20paper%20presents%20an%20evaluation%20framework%20for%20assessing%20Large%20Language%0AModels%27%20%28LLMs%29%20capabilities%20in%20combinatorial%20optimization%2C%20specifically%0Aaddressing%20the%202D%20bin-packing%20problem.%20We%20introduce%20a%20systematic%20methodology%0Athat%20combines%20LLMs%20with%20evolutionary%20algorithms%20to%20generate%20and%20refine%0Aheuristic%20solutions%20iteratively.%20Through%20comprehensive%20experiments%20comparing%0ALLM%20generated%20heuristics%20against%20traditional%20approaches%20%28Finite%20First-Fit%20and%0AHybrid%20First-Fit%29%2C%20we%20demonstrate%20that%20LLMs%20can%20produce%20more%20efficient%0Asolutions%20while%20requiring%20fewer%20computational%20resources.%20Our%20evaluation%20reveals%0Athat%20GPT-4o%20achieves%20optimal%20solutions%20within%20two%20iterations%2C%20reducing%20average%0Abin%20usage%20from%2016%20to%2015%20bins%20while%20improving%20space%20utilization%20from%200.76-0.78%0Ato%200.83.%20This%20work%20contributes%20to%20understanding%20LLM%20evaluation%20in%20specialized%0Adomains%20and%20establishes%20benchmarks%20for%20assessing%20LLM%20performance%20in%0Acombinatorial%20optimization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22255v1&entry.124074799=Read"},
{"title": "LongLive: Real-time Interactive Long Video Generation", "author": "Shuai Yang and Wei Huang and Ruihang Chu and Yicheng Xiao and Yuyang Zhao and Xianbang Wang and Muyang Li and Enze Xie and Yingcong Chen and Yao Lu and Song Han and Yukang Chen", "abstract": "  We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.\n", "link": "http://arxiv.org/abs/2509.22622v1", "date": "2025-09-26", "relevancy": 2.332, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5913}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5891}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongLive%3A%20Real-time%20Interactive%20Long%20Video%20Generation&body=Title%3A%20LongLive%3A%20Real-time%20Interactive%20Long%20Video%20Generation%0AAuthor%3A%20Shuai%20Yang%20and%20Wei%20Huang%20and%20Ruihang%20Chu%20and%20Yicheng%20Xiao%20and%20Yuyang%20Zhao%20and%20Xianbang%20Wang%20and%20Muyang%20Li%20and%20Enze%20Xie%20and%20Yingcong%20Chen%20and%20Yao%20Lu%20and%20Song%20Han%20and%20Yukang%20Chen%0AAbstract%3A%20%20%20We%20present%20LongLive%2C%20a%20frame-level%20autoregressive%20%28AR%29%20framework%20for%0Areal-time%20and%20interactive%20long%20video%20generation.%20Long%20video%20generation%20presents%0Achallenges%20in%20both%20efficiency%20and%20quality.%20Diffusion%20and%20Diffusion-Forcing%0Amodels%20can%20produce%20high-quality%20videos%20but%20suffer%20from%20low%20efficiency%20due%20to%0Abidirectional%20attention.%20Causal%20attention%20AR%20models%20support%20KV%20caching%20for%0Afaster%20inference%2C%20but%20often%20degrade%20in%20quality%20on%20long%20videos%20due%20to%20memory%0Achallenges%20during%20long-video%20training.%20In%20addition%2C%20beyond%20static%20prompt-based%0Ageneration%2C%20interactive%20capabilities%2C%20such%20as%20streaming%20prompt%20inputs%2C%20are%0Acritical%20for%20dynamic%20content%20creation%2C%20enabling%20users%20to%20guide%20narratives%20in%0Areal%20time.%20This%20interactive%20requirement%20significantly%20increases%20complexity%2C%0Aespecially%20in%20ensuring%20visual%20consistency%20and%20semantic%20coherence%20during%20prompt%0Atransitions.%20To%20address%20these%20challenges%2C%20LongLive%20adopts%20a%20causal%2C%20frame-level%0AAR%20design%20that%20integrates%20a%20KV-recache%20mechanism%20that%20refreshes%20cached%20states%0Awith%20new%20prompts%20for%20smooth%2C%20adherent%20switches%3B%20streaming%20long%20tuning%20to%20enable%0Along%20video%20training%20and%20to%20align%20training%20and%20inference%20%28train-long-test-long%29%3B%0Aand%20short%20window%20attention%20paired%20with%20a%20frame-level%20attention%20sink%2C%20shorten%20as%0Aframe%20sink%2C%20preserving%20long-range%20consistency%20while%20enabling%20faster%20generation.%0AWith%20these%20key%20designs%2C%20LongLive%20fine-tunes%20a%201.3B-parameter%20short-clip%20model%0Ato%20minute-long%20generation%20in%20just%2032%20GPU-days.%20At%20inference%2C%20LongLive%20sustains%0A20.7%20FPS%20on%20a%20single%20NVIDIA%20H100%2C%20achieves%20strong%20performance%20on%20VBench%20in%20both%0Ashort%20and%20long%20videos.%20LongLive%20supports%20up%20to%20240-second%20videos%20on%20a%20single%0AH100%20GPU.%20LongLive%20further%20supports%20INT8-quantized%20inference%20with%20only%20marginal%0Aquality%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongLive%253A%2520Real-time%2520Interactive%2520Long%2520Video%2520Generation%26entry.906535625%3DShuai%2520Yang%2520and%2520Wei%2520Huang%2520and%2520Ruihang%2520Chu%2520and%2520Yicheng%2520Xiao%2520and%2520Yuyang%2520Zhao%2520and%2520Xianbang%2520Wang%2520and%2520Muyang%2520Li%2520and%2520Enze%2520Xie%2520and%2520Yingcong%2520Chen%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%2520and%2520Yukang%2520Chen%26entry.1292438233%3D%2520%2520We%2520present%2520LongLive%252C%2520a%2520frame-level%2520autoregressive%2520%2528AR%2529%2520framework%2520for%250Areal-time%2520and%2520interactive%2520long%2520video%2520generation.%2520Long%2520video%2520generation%2520presents%250Achallenges%2520in%2520both%2520efficiency%2520and%2520quality.%2520Diffusion%2520and%2520Diffusion-Forcing%250Amodels%2520can%2520produce%2520high-quality%2520videos%2520but%2520suffer%2520from%2520low%2520efficiency%2520due%2520to%250Abidirectional%2520attention.%2520Causal%2520attention%2520AR%2520models%2520support%2520KV%2520caching%2520for%250Afaster%2520inference%252C%2520but%2520often%2520degrade%2520in%2520quality%2520on%2520long%2520videos%2520due%2520to%2520memory%250Achallenges%2520during%2520long-video%2520training.%2520In%2520addition%252C%2520beyond%2520static%2520prompt-based%250Ageneration%252C%2520interactive%2520capabilities%252C%2520such%2520as%2520streaming%2520prompt%2520inputs%252C%2520are%250Acritical%2520for%2520dynamic%2520content%2520creation%252C%2520enabling%2520users%2520to%2520guide%2520narratives%2520in%250Areal%2520time.%2520This%2520interactive%2520requirement%2520significantly%2520increases%2520complexity%252C%250Aespecially%2520in%2520ensuring%2520visual%2520consistency%2520and%2520semantic%2520coherence%2520during%2520prompt%250Atransitions.%2520To%2520address%2520these%2520challenges%252C%2520LongLive%2520adopts%2520a%2520causal%252C%2520frame-level%250AAR%2520design%2520that%2520integrates%2520a%2520KV-recache%2520mechanism%2520that%2520refreshes%2520cached%2520states%250Awith%2520new%2520prompts%2520for%2520smooth%252C%2520adherent%2520switches%253B%2520streaming%2520long%2520tuning%2520to%2520enable%250Along%2520video%2520training%2520and%2520to%2520align%2520training%2520and%2520inference%2520%2528train-long-test-long%2529%253B%250Aand%2520short%2520window%2520attention%2520paired%2520with%2520a%2520frame-level%2520attention%2520sink%252C%2520shorten%2520as%250Aframe%2520sink%252C%2520preserving%2520long-range%2520consistency%2520while%2520enabling%2520faster%2520generation.%250AWith%2520these%2520key%2520designs%252C%2520LongLive%2520fine-tunes%2520a%25201.3B-parameter%2520short-clip%2520model%250Ato%2520minute-long%2520generation%2520in%2520just%252032%2520GPU-days.%2520At%2520inference%252C%2520LongLive%2520sustains%250A20.7%2520FPS%2520on%2520a%2520single%2520NVIDIA%2520H100%252C%2520achieves%2520strong%2520performance%2520on%2520VBench%2520in%2520both%250Ashort%2520and%2520long%2520videos.%2520LongLive%2520supports%2520up%2520to%2520240-second%2520videos%2520on%2520a%2520single%250AH100%2520GPU.%2520LongLive%2520further%2520supports%2520INT8-quantized%2520inference%2520with%2520only%2520marginal%250Aquality%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongLive%3A%20Real-time%20Interactive%20Long%20Video%20Generation&entry.906535625=Shuai%20Yang%20and%20Wei%20Huang%20and%20Ruihang%20Chu%20and%20Yicheng%20Xiao%20and%20Yuyang%20Zhao%20and%20Xianbang%20Wang%20and%20Muyang%20Li%20and%20Enze%20Xie%20and%20Yingcong%20Chen%20and%20Yao%20Lu%20and%20Song%20Han%20and%20Yukang%20Chen&entry.1292438233=%20%20We%20present%20LongLive%2C%20a%20frame-level%20autoregressive%20%28AR%29%20framework%20for%0Areal-time%20and%20interactive%20long%20video%20generation.%20Long%20video%20generation%20presents%0Achallenges%20in%20both%20efficiency%20and%20quality.%20Diffusion%20and%20Diffusion-Forcing%0Amodels%20can%20produce%20high-quality%20videos%20but%20suffer%20from%20low%20efficiency%20due%20to%0Abidirectional%20attention.%20Causal%20attention%20AR%20models%20support%20KV%20caching%20for%0Afaster%20inference%2C%20but%20often%20degrade%20in%20quality%20on%20long%20videos%20due%20to%20memory%0Achallenges%20during%20long-video%20training.%20In%20addition%2C%20beyond%20static%20prompt-based%0Ageneration%2C%20interactive%20capabilities%2C%20such%20as%20streaming%20prompt%20inputs%2C%20are%0Acritical%20for%20dynamic%20content%20creation%2C%20enabling%20users%20to%20guide%20narratives%20in%0Areal%20time.%20This%20interactive%20requirement%20significantly%20increases%20complexity%2C%0Aespecially%20in%20ensuring%20visual%20consistency%20and%20semantic%20coherence%20during%20prompt%0Atransitions.%20To%20address%20these%20challenges%2C%20LongLive%20adopts%20a%20causal%2C%20frame-level%0AAR%20design%20that%20integrates%20a%20KV-recache%20mechanism%20that%20refreshes%20cached%20states%0Awith%20new%20prompts%20for%20smooth%2C%20adherent%20switches%3B%20streaming%20long%20tuning%20to%20enable%0Along%20video%20training%20and%20to%20align%20training%20and%20inference%20%28train-long-test-long%29%3B%0Aand%20short%20window%20attention%20paired%20with%20a%20frame-level%20attention%20sink%2C%20shorten%20as%0Aframe%20sink%2C%20preserving%20long-range%20consistency%20while%20enabling%20faster%20generation.%0AWith%20these%20key%20designs%2C%20LongLive%20fine-tunes%20a%201.3B-parameter%20short-clip%20model%0Ato%20minute-long%20generation%20in%20just%2032%20GPU-days.%20At%20inference%2C%20LongLive%20sustains%0A20.7%20FPS%20on%20a%20single%20NVIDIA%20H100%2C%20achieves%20strong%20performance%20on%20VBench%20in%20both%0Ashort%20and%20long%20videos.%20LongLive%20supports%20up%20to%20240-second%20videos%20on%20a%20single%0AH100%20GPU.%20LongLive%20further%20supports%20INT8-quantized%20inference%20with%20only%20marginal%0Aquality%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22622v1&entry.124074799=Read"},
{"title": "Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on\n  the Unit Hypersphere", "author": "Li Ju and Max Andersson and Stina Fredriksson and Edward Gl\u00f6ckner and Andreas Hellander and Ekta Vats and Prashant Singh", "abstract": "  Vision-language models (VLMs) as foundation models have significantly\nenhanced performance across a wide range of visual and textual tasks, without\nrequiring large-scale training from scratch for downstream tasks. However,\nthese deterministic VLMs fail to capture the inherent ambiguity and uncertainty\nin natural language and visual data. Recent probabilistic post-hoc adaptation\nmethods address this by mapping deterministic embeddings onto probability\ndistributions; however, existing approaches do not account for the asymmetric\nuncertainty structure of the modalities, and the constraint that meaningful\ndeterministic embeddings reside on a unit hypersphere, potentially leading to\nsuboptimal performance. In this paper, we address the asymmetric uncertainty\nstructure inherent in textual and visual data, and propose AsymVLM to build\nprobabilistic embeddings from pre-trained VLMs on the unit hypersphere,\nenabling uncertainty quantification. We validate the effectiveness of the\nprobabilistic embeddings on established benchmarks, and present comprehensive\nablation studies demonstrating the inherent nature of asymmetry in the\nuncertainty structure of textual and visual data.\n", "link": "http://arxiv.org/abs/2505.11029v2", "date": "2025-09-26", "relevancy": 2.3273, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5991}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20the%20Asymmetric%20Uncertainty%20Structure%20of%20Pre-trained%20VLMs%20on%0A%20%20the%20Unit%20Hypersphere&body=Title%3A%20Exploiting%20the%20Asymmetric%20Uncertainty%20Structure%20of%20Pre-trained%20VLMs%20on%0A%20%20the%20Unit%20Hypersphere%0AAuthor%3A%20Li%20Ju%20and%20Max%20Andersson%20and%20Stina%20Fredriksson%20and%20Edward%20Gl%C3%B6ckner%20and%20Andreas%20Hellander%20and%20Ekta%20Vats%20and%20Prashant%20Singh%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20as%20foundation%20models%20have%20significantly%0Aenhanced%20performance%20across%20a%20wide%20range%20of%20visual%20and%20textual%20tasks%2C%20without%0Arequiring%20large-scale%20training%20from%20scratch%20for%20downstream%20tasks.%20However%2C%0Athese%20deterministic%20VLMs%20fail%20to%20capture%20the%20inherent%20ambiguity%20and%20uncertainty%0Ain%20natural%20language%20and%20visual%20data.%20Recent%20probabilistic%20post-hoc%20adaptation%0Amethods%20address%20this%20by%20mapping%20deterministic%20embeddings%20onto%20probability%0Adistributions%3B%20however%2C%20existing%20approaches%20do%20not%20account%20for%20the%20asymmetric%0Auncertainty%20structure%20of%20the%20modalities%2C%20and%20the%20constraint%20that%20meaningful%0Adeterministic%20embeddings%20reside%20on%20a%20unit%20hypersphere%2C%20potentially%20leading%20to%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20address%20the%20asymmetric%20uncertainty%0Astructure%20inherent%20in%20textual%20and%20visual%20data%2C%20and%20propose%20AsymVLM%20to%20build%0Aprobabilistic%20embeddings%20from%20pre-trained%20VLMs%20on%20the%20unit%20hypersphere%2C%0Aenabling%20uncertainty%20quantification.%20We%20validate%20the%20effectiveness%20of%20the%0Aprobabilistic%20embeddings%20on%20established%20benchmarks%2C%20and%20present%20comprehensive%0Aablation%20studies%20demonstrating%20the%20inherent%20nature%20of%20asymmetry%20in%20the%0Auncertainty%20structure%20of%20textual%20and%20visual%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520the%2520Asymmetric%2520Uncertainty%2520Structure%2520of%2520Pre-trained%2520VLMs%2520on%250A%2520%2520the%2520Unit%2520Hypersphere%26entry.906535625%3DLi%2520Ju%2520and%2520Max%2520Andersson%2520and%2520Stina%2520Fredriksson%2520and%2520Edward%2520Gl%25C3%25B6ckner%2520and%2520Andreas%2520Hellander%2520and%2520Ekta%2520Vats%2520and%2520Prashant%2520Singh%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520as%2520foundation%2520models%2520have%2520significantly%250Aenhanced%2520performance%2520across%2520a%2520wide%2520range%2520of%2520visual%2520and%2520textual%2520tasks%252C%2520without%250Arequiring%2520large-scale%2520training%2520from%2520scratch%2520for%2520downstream%2520tasks.%2520However%252C%250Athese%2520deterministic%2520VLMs%2520fail%2520to%2520capture%2520the%2520inherent%2520ambiguity%2520and%2520uncertainty%250Ain%2520natural%2520language%2520and%2520visual%2520data.%2520Recent%2520probabilistic%2520post-hoc%2520adaptation%250Amethods%2520address%2520this%2520by%2520mapping%2520deterministic%2520embeddings%2520onto%2520probability%250Adistributions%253B%2520however%252C%2520existing%2520approaches%2520do%2520not%2520account%2520for%2520the%2520asymmetric%250Auncertainty%2520structure%2520of%2520the%2520modalities%252C%2520and%2520the%2520constraint%2520that%2520meaningful%250Adeterministic%2520embeddings%2520reside%2520on%2520a%2520unit%2520hypersphere%252C%2520potentially%2520leading%2520to%250Asuboptimal%2520performance.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520asymmetric%2520uncertainty%250Astructure%2520inherent%2520in%2520textual%2520and%2520visual%2520data%252C%2520and%2520propose%2520AsymVLM%2520to%2520build%250Aprobabilistic%2520embeddings%2520from%2520pre-trained%2520VLMs%2520on%2520the%2520unit%2520hypersphere%252C%250Aenabling%2520uncertainty%2520quantification.%2520We%2520validate%2520the%2520effectiveness%2520of%2520the%250Aprobabilistic%2520embeddings%2520on%2520established%2520benchmarks%252C%2520and%2520present%2520comprehensive%250Aablation%2520studies%2520demonstrating%2520the%2520inherent%2520nature%2520of%2520asymmetry%2520in%2520the%250Auncertainty%2520structure%2520of%2520textual%2520and%2520visual%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20the%20Asymmetric%20Uncertainty%20Structure%20of%20Pre-trained%20VLMs%20on%0A%20%20the%20Unit%20Hypersphere&entry.906535625=Li%20Ju%20and%20Max%20Andersson%20and%20Stina%20Fredriksson%20and%20Edward%20Gl%C3%B6ckner%20and%20Andreas%20Hellander%20and%20Ekta%20Vats%20and%20Prashant%20Singh&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20as%20foundation%20models%20have%20significantly%0Aenhanced%20performance%20across%20a%20wide%20range%20of%20visual%20and%20textual%20tasks%2C%20without%0Arequiring%20large-scale%20training%20from%20scratch%20for%20downstream%20tasks.%20However%2C%0Athese%20deterministic%20VLMs%20fail%20to%20capture%20the%20inherent%20ambiguity%20and%20uncertainty%0Ain%20natural%20language%20and%20visual%20data.%20Recent%20probabilistic%20post-hoc%20adaptation%0Amethods%20address%20this%20by%20mapping%20deterministic%20embeddings%20onto%20probability%0Adistributions%3B%20however%2C%20existing%20approaches%20do%20not%20account%20for%20the%20asymmetric%0Auncertainty%20structure%20of%20the%20modalities%2C%20and%20the%20constraint%20that%20meaningful%0Adeterministic%20embeddings%20reside%20on%20a%20unit%20hypersphere%2C%20potentially%20leading%20to%0Asuboptimal%20performance.%20In%20this%20paper%2C%20we%20address%20the%20asymmetric%20uncertainty%0Astructure%20inherent%20in%20textual%20and%20visual%20data%2C%20and%20propose%20AsymVLM%20to%20build%0Aprobabilistic%20embeddings%20from%20pre-trained%20VLMs%20on%20the%20unit%20hypersphere%2C%0Aenabling%20uncertainty%20quantification.%20We%20validate%20the%20effectiveness%20of%20the%0Aprobabilistic%20embeddings%20on%20established%20benchmarks%2C%20and%20present%20comprehensive%0Aablation%20studies%20demonstrating%20the%20inherent%20nature%20of%20asymmetry%20in%20the%0Auncertainty%20structure%20of%20textual%20and%20visual%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11029v2&entry.124074799=Read"},
{"title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning", "author": "Aayush Mishra and Daniel Khashabi and Anqi Liu", "abstract": "  Supervised Fine-Tuning (SFT) is used to specialize model behavior by training\nweights to produce intended target responses for queries. In contrast,\nIn-Context Learning (ICL) adapts models during inference with instructions or\ndemonstrations in the prompt. ICL can offer better generalizability and more\ncalibrated responses compared to SFT in data scarce settings, at the cost of\nmore inference compute. In this work, we ask the question: Can ICL's internal\ncomputations be used to improve the qualities of SFT? We first show that ICL\nand SFT produce distinct activation patterns, indicating that the two methods\nachieve adaptation through different functional mechanisms. Motivated by this\nobservation and to use ICL's rich functionality, we introduce ICL Activation\nAlignment (IA2), a self-distillation technique which aims to replicate ICL's\nactivation patterns in SFT models and incentivizes ICL-like internal reasoning.\nPerforming IA2 as a priming step before SFT significantly improves the accuracy\nand calibration of model outputs, as shown by our extensive empirical results\non 12 popular benchmarks and 2 model families. This finding is not only\npractically useful, but also offers a conceptual window into the inner\nmechanics of model adaptation.\n", "link": "http://arxiv.org/abs/2509.22621v1", "date": "2025-09-26", "relevancy": 2.3272, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4814}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.468}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IA2%3A%20Alignment%20with%20ICL%20Activations%20Improves%20Supervised%20Fine-Tuning&body=Title%3A%20IA2%3A%20Alignment%20with%20ICL%20Activations%20Improves%20Supervised%20Fine-Tuning%0AAuthor%3A%20Aayush%20Mishra%20and%20Daniel%20Khashabi%20and%20Anqi%20Liu%0AAbstract%3A%20%20%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20used%20to%20specialize%20model%20behavior%20by%20training%0Aweights%20to%20produce%20intended%20target%20responses%20for%20queries.%20In%20contrast%2C%0AIn-Context%20Learning%20%28ICL%29%20adapts%20models%20during%20inference%20with%20instructions%20or%0Ademonstrations%20in%20the%20prompt.%20ICL%20can%20offer%20better%20generalizability%20and%20more%0Acalibrated%20responses%20compared%20to%20SFT%20in%20data%20scarce%20settings%2C%20at%20the%20cost%20of%0Amore%20inference%20compute.%20In%20this%20work%2C%20we%20ask%20the%20question%3A%20Can%20ICL%27s%20internal%0Acomputations%20be%20used%20to%20improve%20the%20qualities%20of%20SFT%3F%20We%20first%20show%20that%20ICL%0Aand%20SFT%20produce%20distinct%20activation%20patterns%2C%20indicating%20that%20the%20two%20methods%0Aachieve%20adaptation%20through%20different%20functional%20mechanisms.%20Motivated%20by%20this%0Aobservation%20and%20to%20use%20ICL%27s%20rich%20functionality%2C%20we%20introduce%20ICL%20Activation%0AAlignment%20%28IA2%29%2C%20a%20self-distillation%20technique%20which%20aims%20to%20replicate%20ICL%27s%0Aactivation%20patterns%20in%20SFT%20models%20and%20incentivizes%20ICL-like%20internal%20reasoning.%0APerforming%20IA2%20as%20a%20priming%20step%20before%20SFT%20significantly%20improves%20the%20accuracy%0Aand%20calibration%20of%20model%20outputs%2C%20as%20shown%20by%20our%20extensive%20empirical%20results%0Aon%2012%20popular%20benchmarks%20and%202%20model%20families.%20This%20finding%20is%20not%20only%0Apractically%20useful%2C%20but%20also%20offers%20a%20conceptual%20window%20into%20the%20inner%0Amechanics%20of%20model%20adaptation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIA2%253A%2520Alignment%2520with%2520ICL%2520Activations%2520Improves%2520Supervised%2520Fine-Tuning%26entry.906535625%3DAayush%2520Mishra%2520and%2520Daniel%2520Khashabi%2520and%2520Anqi%2520Liu%26entry.1292438233%3D%2520%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520is%2520used%2520to%2520specialize%2520model%2520behavior%2520by%2520training%250Aweights%2520to%2520produce%2520intended%2520target%2520responses%2520for%2520queries.%2520In%2520contrast%252C%250AIn-Context%2520Learning%2520%2528ICL%2529%2520adapts%2520models%2520during%2520inference%2520with%2520instructions%2520or%250Ademonstrations%2520in%2520the%2520prompt.%2520ICL%2520can%2520offer%2520better%2520generalizability%2520and%2520more%250Acalibrated%2520responses%2520compared%2520to%2520SFT%2520in%2520data%2520scarce%2520settings%252C%2520at%2520the%2520cost%2520of%250Amore%2520inference%2520compute.%2520In%2520this%2520work%252C%2520we%2520ask%2520the%2520question%253A%2520Can%2520ICL%2527s%2520internal%250Acomputations%2520be%2520used%2520to%2520improve%2520the%2520qualities%2520of%2520SFT%253F%2520We%2520first%2520show%2520that%2520ICL%250Aand%2520SFT%2520produce%2520distinct%2520activation%2520patterns%252C%2520indicating%2520that%2520the%2520two%2520methods%250Aachieve%2520adaptation%2520through%2520different%2520functional%2520mechanisms.%2520Motivated%2520by%2520this%250Aobservation%2520and%2520to%2520use%2520ICL%2527s%2520rich%2520functionality%252C%2520we%2520introduce%2520ICL%2520Activation%250AAlignment%2520%2528IA2%2529%252C%2520a%2520self-distillation%2520technique%2520which%2520aims%2520to%2520replicate%2520ICL%2527s%250Aactivation%2520patterns%2520in%2520SFT%2520models%2520and%2520incentivizes%2520ICL-like%2520internal%2520reasoning.%250APerforming%2520IA2%2520as%2520a%2520priming%2520step%2520before%2520SFT%2520significantly%2520improves%2520the%2520accuracy%250Aand%2520calibration%2520of%2520model%2520outputs%252C%2520as%2520shown%2520by%2520our%2520extensive%2520empirical%2520results%250Aon%252012%2520popular%2520benchmarks%2520and%25202%2520model%2520families.%2520This%2520finding%2520is%2520not%2520only%250Apractically%2520useful%252C%2520but%2520also%2520offers%2520a%2520conceptual%2520window%2520into%2520the%2520inner%250Amechanics%2520of%2520model%2520adaptation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IA2%3A%20Alignment%20with%20ICL%20Activations%20Improves%20Supervised%20Fine-Tuning&entry.906535625=Aayush%20Mishra%20and%20Daniel%20Khashabi%20and%20Anqi%20Liu&entry.1292438233=%20%20Supervised%20Fine-Tuning%20%28SFT%29%20is%20used%20to%20specialize%20model%20behavior%20by%20training%0Aweights%20to%20produce%20intended%20target%20responses%20for%20queries.%20In%20contrast%2C%0AIn-Context%20Learning%20%28ICL%29%20adapts%20models%20during%20inference%20with%20instructions%20or%0Ademonstrations%20in%20the%20prompt.%20ICL%20can%20offer%20better%20generalizability%20and%20more%0Acalibrated%20responses%20compared%20to%20SFT%20in%20data%20scarce%20settings%2C%20at%20the%20cost%20of%0Amore%20inference%20compute.%20In%20this%20work%2C%20we%20ask%20the%20question%3A%20Can%20ICL%27s%20internal%0Acomputations%20be%20used%20to%20improve%20the%20qualities%20of%20SFT%3F%20We%20first%20show%20that%20ICL%0Aand%20SFT%20produce%20distinct%20activation%20patterns%2C%20indicating%20that%20the%20two%20methods%0Aachieve%20adaptation%20through%20different%20functional%20mechanisms.%20Motivated%20by%20this%0Aobservation%20and%20to%20use%20ICL%27s%20rich%20functionality%2C%20we%20introduce%20ICL%20Activation%0AAlignment%20%28IA2%29%2C%20a%20self-distillation%20technique%20which%20aims%20to%20replicate%20ICL%27s%0Aactivation%20patterns%20in%20SFT%20models%20and%20incentivizes%20ICL-like%20internal%20reasoning.%0APerforming%20IA2%20as%20a%20priming%20step%20before%20SFT%20significantly%20improves%20the%20accuracy%0Aand%20calibration%20of%20model%20outputs%2C%20as%20shown%20by%20our%20extensive%20empirical%20results%0Aon%2012%20popular%20benchmarks%20and%202%20model%20families.%20This%20finding%20is%20not%20only%0Apractically%20useful%2C%20but%20also%20offers%20a%20conceptual%20window%20into%20the%20inner%0Amechanics%20of%20model%20adaptation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22621v1&entry.124074799=Read"},
{"title": "pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for\n  Vision-Language Models", "author": "Sajjad Ghiasvand and Mahnoosh Alizadeh and Ramtin Pedarsani", "abstract": "  Vision-Language Models (VLMs) like CLIP have demonstrated remarkable\ngeneralization in zero- and few-shot settings, but adapting them efficiently to\ndecentralized, heterogeneous data remains a challenge. While prompt tuning has\nemerged as a popular parameter-efficient approach in personalized federated\nlearning, existing methods often sacrifice generalization in favor of\npersonalization, struggling particularly on unseen classes or domains. In this\nwork, we propose pFedMMA, the first personalized federated learning framework\nthat leverages multi-modal adapters for vision-language tasks. Each adapter\ncontains modality-specific up- and down-projection layers alongside a globally\nshared projection that aligns cross-modal features. Our optimization strategy\nallows clients to locally adapt to personalized data distributions while\ncollaboratively training the shared projection to improve global\ngeneralization. This design is also communication-efficient, as only the shared\ncomponent is exchanged during communication rounds. Through extensive\nexperiments across eleven datasets, including domain- and label-shift\nscenarios, we show that pFedMMA achieves state-of-the-art trade-offs between\npersonalization and generalization, outperforming recent federated prompt\ntuning methods.\n", "link": "http://arxiv.org/abs/2507.05394v2", "date": "2025-09-26", "relevancy": 2.3246, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6396}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5464}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20pFedMMA%3A%20Personalized%20Federated%20Fine-Tuning%20with%20Multi-Modal%20Adapter%20for%0A%20%20Vision-Language%20Models&body=Title%3A%20pFedMMA%3A%20Personalized%20Federated%20Fine-Tuning%20with%20Multi-Modal%20Adapter%20for%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Sajjad%20Ghiasvand%20and%20Mahnoosh%20Alizadeh%20and%20Ramtin%20Pedarsani%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20have%20demonstrated%20remarkable%0Ageneralization%20in%20zero-%20and%20few-shot%20settings%2C%20but%20adapting%20them%20efficiently%20to%0Adecentralized%2C%20heterogeneous%20data%20remains%20a%20challenge.%20While%20prompt%20tuning%20has%0Aemerged%20as%20a%20popular%20parameter-efficient%20approach%20in%20personalized%20federated%0Alearning%2C%20existing%20methods%20often%20sacrifice%20generalization%20in%20favor%20of%0Apersonalization%2C%20struggling%20particularly%20on%20unseen%20classes%20or%20domains.%20In%20this%0Awork%2C%20we%20propose%20pFedMMA%2C%20the%20first%20personalized%20federated%20learning%20framework%0Athat%20leverages%20multi-modal%20adapters%20for%20vision-language%20tasks.%20Each%20adapter%0Acontains%20modality-specific%20up-%20and%20down-projection%20layers%20alongside%20a%20globally%0Ashared%20projection%20that%20aligns%20cross-modal%20features.%20Our%20optimization%20strategy%0Aallows%20clients%20to%20locally%20adapt%20to%20personalized%20data%20distributions%20while%0Acollaboratively%20training%20the%20shared%20projection%20to%20improve%20global%0Ageneralization.%20This%20design%20is%20also%20communication-efficient%2C%20as%20only%20the%20shared%0Acomponent%20is%20exchanged%20during%20communication%20rounds.%20Through%20extensive%0Aexperiments%20across%20eleven%20datasets%2C%20including%20domain-%20and%20label-shift%0Ascenarios%2C%20we%20show%20that%20pFedMMA%20achieves%20state-of-the-art%20trade-offs%20between%0Apersonalization%20and%20generalization%2C%20outperforming%20recent%20federated%20prompt%0Atuning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DpFedMMA%253A%2520Personalized%2520Federated%2520Fine-Tuning%2520with%2520Multi-Modal%2520Adapter%2520for%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DSajjad%2520Ghiasvand%2520and%2520Mahnoosh%2520Alizadeh%2520and%2520Ramtin%2520Pedarsani%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520like%2520CLIP%2520have%2520demonstrated%2520remarkable%250Ageneralization%2520in%2520zero-%2520and%2520few-shot%2520settings%252C%2520but%2520adapting%2520them%2520efficiently%2520to%250Adecentralized%252C%2520heterogeneous%2520data%2520remains%2520a%2520challenge.%2520While%2520prompt%2520tuning%2520has%250Aemerged%2520as%2520a%2520popular%2520parameter-efficient%2520approach%2520in%2520personalized%2520federated%250Alearning%252C%2520existing%2520methods%2520often%2520sacrifice%2520generalization%2520in%2520favor%2520of%250Apersonalization%252C%2520struggling%2520particularly%2520on%2520unseen%2520classes%2520or%2520domains.%2520In%2520this%250Awork%252C%2520we%2520propose%2520pFedMMA%252C%2520the%2520first%2520personalized%2520federated%2520learning%2520framework%250Athat%2520leverages%2520multi-modal%2520adapters%2520for%2520vision-language%2520tasks.%2520Each%2520adapter%250Acontains%2520modality-specific%2520up-%2520and%2520down-projection%2520layers%2520alongside%2520a%2520globally%250Ashared%2520projection%2520that%2520aligns%2520cross-modal%2520features.%2520Our%2520optimization%2520strategy%250Aallows%2520clients%2520to%2520locally%2520adapt%2520to%2520personalized%2520data%2520distributions%2520while%250Acollaboratively%2520training%2520the%2520shared%2520projection%2520to%2520improve%2520global%250Ageneralization.%2520This%2520design%2520is%2520also%2520communication-efficient%252C%2520as%2520only%2520the%2520shared%250Acomponent%2520is%2520exchanged%2520during%2520communication%2520rounds.%2520Through%2520extensive%250Aexperiments%2520across%2520eleven%2520datasets%252C%2520including%2520domain-%2520and%2520label-shift%250Ascenarios%252C%2520we%2520show%2520that%2520pFedMMA%2520achieves%2520state-of-the-art%2520trade-offs%2520between%250Apersonalization%2520and%2520generalization%252C%2520outperforming%2520recent%2520federated%2520prompt%250Atuning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=pFedMMA%3A%20Personalized%20Federated%20Fine-Tuning%20with%20Multi-Modal%20Adapter%20for%0A%20%20Vision-Language%20Models&entry.906535625=Sajjad%20Ghiasvand%20and%20Mahnoosh%20Alizadeh%20and%20Ramtin%20Pedarsani&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20like%20CLIP%20have%20demonstrated%20remarkable%0Ageneralization%20in%20zero-%20and%20few-shot%20settings%2C%20but%20adapting%20them%20efficiently%20to%0Adecentralized%2C%20heterogeneous%20data%20remains%20a%20challenge.%20While%20prompt%20tuning%20has%0Aemerged%20as%20a%20popular%20parameter-efficient%20approach%20in%20personalized%20federated%0Alearning%2C%20existing%20methods%20often%20sacrifice%20generalization%20in%20favor%20of%0Apersonalization%2C%20struggling%20particularly%20on%20unseen%20classes%20or%20domains.%20In%20this%0Awork%2C%20we%20propose%20pFedMMA%2C%20the%20first%20personalized%20federated%20learning%20framework%0Athat%20leverages%20multi-modal%20adapters%20for%20vision-language%20tasks.%20Each%20adapter%0Acontains%20modality-specific%20up-%20and%20down-projection%20layers%20alongside%20a%20globally%0Ashared%20projection%20that%20aligns%20cross-modal%20features.%20Our%20optimization%20strategy%0Aallows%20clients%20to%20locally%20adapt%20to%20personalized%20data%20distributions%20while%0Acollaboratively%20training%20the%20shared%20projection%20to%20improve%20global%0Ageneralization.%20This%20design%20is%20also%20communication-efficient%2C%20as%20only%20the%20shared%0Acomponent%20is%20exchanged%20during%20communication%20rounds.%20Through%20extensive%0Aexperiments%20across%20eleven%20datasets%2C%20including%20domain-%20and%20label-shift%0Ascenarios%2C%20we%20show%20that%20pFedMMA%20achieves%20state-of-the-art%20trade-offs%20between%0Apersonalization%20and%20generalization%2C%20outperforming%20recent%20federated%20prompt%0Atuning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05394v2&entry.124074799=Read"},
{"title": "NIFTY: a Non-Local Image Flow Matching for Texture Synthesis", "author": "Pierrick Chatillon and Julien Rabin and David Tschumperl\u00e9", "abstract": "  This paper addresses the problem of exemplar-based texture synthesis. We\nintroduce NIFTY, a hybrid framework that combines recent insights on diffusion\nmodels trained with convolutional neural networks, and classical patch-based\ntexture optimization techniques. NIFTY is a non-parametric flow-matching model\nbuilt on non-local patch matching, which avoids the need for neural network\ntraining while alleviating common shortcomings of patch-based methods, such as\npoor initialization or visual artifacts. Experimental results demonstrate the\neffectiveness of the proposed approach compared to representative methods from\nthe literature. Code is available at https://github.com/PierrickCh/Nifty.git\n", "link": "http://arxiv.org/abs/2509.22318v1", "date": "2025-09-26", "relevancy": 2.3245, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5984}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5697}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NIFTY%3A%20a%20Non-Local%20Image%20Flow%20Matching%20for%20Texture%20Synthesis&body=Title%3A%20NIFTY%3A%20a%20Non-Local%20Image%20Flow%20Matching%20for%20Texture%20Synthesis%0AAuthor%3A%20Pierrick%20Chatillon%20and%20Julien%20Rabin%20and%20David%20Tschumperl%C3%A9%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20problem%20of%20exemplar-based%20texture%20synthesis.%20We%0Aintroduce%20NIFTY%2C%20a%20hybrid%20framework%20that%20combines%20recent%20insights%20on%20diffusion%0Amodels%20trained%20with%20convolutional%20neural%20networks%2C%20and%20classical%20patch-based%0Atexture%20optimization%20techniques.%20NIFTY%20is%20a%20non-parametric%20flow-matching%20model%0Abuilt%20on%20non-local%20patch%20matching%2C%20which%20avoids%20the%20need%20for%20neural%20network%0Atraining%20while%20alleviating%20common%20shortcomings%20of%20patch-based%20methods%2C%20such%20as%0Apoor%20initialization%20or%20visual%20artifacts.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20compared%20to%20representative%20methods%20from%0Athe%20literature.%20Code%20is%20available%20at%20https%3A//github.com/PierrickCh/Nifty.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNIFTY%253A%2520a%2520Non-Local%2520Image%2520Flow%2520Matching%2520for%2520Texture%2520Synthesis%26entry.906535625%3DPierrick%2520Chatillon%2520and%2520Julien%2520Rabin%2520and%2520David%2520Tschumperl%25C3%25A9%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520problem%2520of%2520exemplar-based%2520texture%2520synthesis.%2520We%250Aintroduce%2520NIFTY%252C%2520a%2520hybrid%2520framework%2520that%2520combines%2520recent%2520insights%2520on%2520diffusion%250Amodels%2520trained%2520with%2520convolutional%2520neural%2520networks%252C%2520and%2520classical%2520patch-based%250Atexture%2520optimization%2520techniques.%2520NIFTY%2520is%2520a%2520non-parametric%2520flow-matching%2520model%250Abuilt%2520on%2520non-local%2520patch%2520matching%252C%2520which%2520avoids%2520the%2520need%2520for%2520neural%2520network%250Atraining%2520while%2520alleviating%2520common%2520shortcomings%2520of%2520patch-based%2520methods%252C%2520such%2520as%250Apoor%2520initialization%2520or%2520visual%2520artifacts.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520approach%2520compared%2520to%2520representative%2520methods%2520from%250Athe%2520literature.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/PierrickCh/Nifty.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NIFTY%3A%20a%20Non-Local%20Image%20Flow%20Matching%20for%20Texture%20Synthesis&entry.906535625=Pierrick%20Chatillon%20and%20Julien%20Rabin%20and%20David%20Tschumperl%C3%A9&entry.1292438233=%20%20This%20paper%20addresses%20the%20problem%20of%20exemplar-based%20texture%20synthesis.%20We%0Aintroduce%20NIFTY%2C%20a%20hybrid%20framework%20that%20combines%20recent%20insights%20on%20diffusion%0Amodels%20trained%20with%20convolutional%20neural%20networks%2C%20and%20classical%20patch-based%0Atexture%20optimization%20techniques.%20NIFTY%20is%20a%20non-parametric%20flow-matching%20model%0Abuilt%20on%20non-local%20patch%20matching%2C%20which%20avoids%20the%20need%20for%20neural%20network%0Atraining%20while%20alleviating%20common%20shortcomings%20of%20patch-based%20methods%2C%20such%20as%0Apoor%20initialization%20or%20visual%20artifacts.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20compared%20to%20representative%20methods%20from%0Athe%20literature.%20Code%20is%20available%20at%20https%3A//github.com/PierrickCh/Nifty.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22318v1&entry.124074799=Read"},
{"title": "JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory\n  Generation", "author": "Guillem Capellera and Luis Ferraz and Antonio Rubio and Alexandre Alahi and Antonio Agudo", "abstract": "  Generative models often treat continuous data and discrete events as separate\nprocesses, creating a gap in modeling complex systems where they interact\nsynchronously. To bridge this gap, we introduce JointDiff, a novel diffusion\nframework designed to unify these two processes by simultaneously generating\ncontinuous spatio-temporal data and synchronous discrete events. We demonstrate\nits efficacy in the sports domain by simultaneously modeling multi-agent\ntrajectories and key possession events. This joint modeling is validated with\nnon-controllable generation and two novel controllable generation scenarios:\nweak-possessor-guidance, which offers flexible semantic control over game\ndynamics through a simple list of intended ball possessors, and text-guidance,\nwhich enables fine-grained, language-driven generation. To enable the\nconditioning with these guidance signals, we introduce CrossGuid, an effective\nconditioning operation for multi-agent domains. We also share a new unified\nsports benchmark enhanced with textual descriptions for soccer and football\ndatasets. JointDiff achieves state-of-the-art performance, demonstrating that\njoint modeling is crucial for building realistic and controllable generative\nmodels for interactive systems.\n", "link": "http://arxiv.org/abs/2509.22522v1", "date": "2025-09-26", "relevancy": 2.3097, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5868}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5799}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%0A%20%20Generation&body=Title%3A%20JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%0A%20%20Generation%0AAuthor%3A%20Guillem%20Capellera%20and%20Luis%20Ferraz%20and%20Antonio%20Rubio%20and%20Alexandre%20Alahi%20and%20Antonio%20Agudo%0AAbstract%3A%20%20%20Generative%20models%20often%20treat%20continuous%20data%20and%20discrete%20events%20as%20separate%0Aprocesses%2C%20creating%20a%20gap%20in%20modeling%20complex%20systems%20where%20they%20interact%0Asynchronously.%20To%20bridge%20this%20gap%2C%20we%20introduce%20JointDiff%2C%20a%20novel%20diffusion%0Aframework%20designed%20to%20unify%20these%20two%20processes%20by%20simultaneously%20generating%0Acontinuous%20spatio-temporal%20data%20and%20synchronous%20discrete%20events.%20We%20demonstrate%0Aits%20efficacy%20in%20the%20sports%20domain%20by%20simultaneously%20modeling%20multi-agent%0Atrajectories%20and%20key%20possession%20events.%20This%20joint%20modeling%20is%20validated%20with%0Anon-controllable%20generation%20and%20two%20novel%20controllable%20generation%20scenarios%3A%0Aweak-possessor-guidance%2C%20which%20offers%20flexible%20semantic%20control%20over%20game%0Adynamics%20through%20a%20simple%20list%20of%20intended%20ball%20possessors%2C%20and%20text-guidance%2C%0Awhich%20enables%20fine-grained%2C%20language-driven%20generation.%20To%20enable%20the%0Aconditioning%20with%20these%20guidance%20signals%2C%20we%20introduce%20CrossGuid%2C%20an%20effective%0Aconditioning%20operation%20for%20multi-agent%20domains.%20We%20also%20share%20a%20new%20unified%0Asports%20benchmark%20enhanced%20with%20textual%20descriptions%20for%20soccer%20and%20football%0Adatasets.%20JointDiff%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20that%0Ajoint%20modeling%20is%20crucial%20for%20building%20realistic%20and%20controllable%20generative%0Amodels%20for%20interactive%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointDiff%253A%2520Bridging%2520Continuous%2520and%2520Discrete%2520in%2520Multi-Agent%2520Trajectory%250A%2520%2520Generation%26entry.906535625%3DGuillem%2520Capellera%2520and%2520Luis%2520Ferraz%2520and%2520Antonio%2520Rubio%2520and%2520Alexandre%2520Alahi%2520and%2520Antonio%2520Agudo%26entry.1292438233%3D%2520%2520Generative%2520models%2520often%2520treat%2520continuous%2520data%2520and%2520discrete%2520events%2520as%2520separate%250Aprocesses%252C%2520creating%2520a%2520gap%2520in%2520modeling%2520complex%2520systems%2520where%2520they%2520interact%250Asynchronously.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520JointDiff%252C%2520a%2520novel%2520diffusion%250Aframework%2520designed%2520to%2520unify%2520these%2520two%2520processes%2520by%2520simultaneously%2520generating%250Acontinuous%2520spatio-temporal%2520data%2520and%2520synchronous%2520discrete%2520events.%2520We%2520demonstrate%250Aits%2520efficacy%2520in%2520the%2520sports%2520domain%2520by%2520simultaneously%2520modeling%2520multi-agent%250Atrajectories%2520and%2520key%2520possession%2520events.%2520This%2520joint%2520modeling%2520is%2520validated%2520with%250Anon-controllable%2520generation%2520and%2520two%2520novel%2520controllable%2520generation%2520scenarios%253A%250Aweak-possessor-guidance%252C%2520which%2520offers%2520flexible%2520semantic%2520control%2520over%2520game%250Adynamics%2520through%2520a%2520simple%2520list%2520of%2520intended%2520ball%2520possessors%252C%2520and%2520text-guidance%252C%250Awhich%2520enables%2520fine-grained%252C%2520language-driven%2520generation.%2520To%2520enable%2520the%250Aconditioning%2520with%2520these%2520guidance%2520signals%252C%2520we%2520introduce%2520CrossGuid%252C%2520an%2520effective%250Aconditioning%2520operation%2520for%2520multi-agent%2520domains.%2520We%2520also%2520share%2520a%2520new%2520unified%250Asports%2520benchmark%2520enhanced%2520with%2520textual%2520descriptions%2520for%2520soccer%2520and%2520football%250Adatasets.%2520JointDiff%2520achieves%2520state-of-the-art%2520performance%252C%2520demonstrating%2520that%250Ajoint%2520modeling%2520is%2520crucial%2520for%2520building%2520realistic%2520and%2520controllable%2520generative%250Amodels%2520for%2520interactive%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointDiff%3A%20Bridging%20Continuous%20and%20Discrete%20in%20Multi-Agent%20Trajectory%0A%20%20Generation&entry.906535625=Guillem%20Capellera%20and%20Luis%20Ferraz%20and%20Antonio%20Rubio%20and%20Alexandre%20Alahi%20and%20Antonio%20Agudo&entry.1292438233=%20%20Generative%20models%20often%20treat%20continuous%20data%20and%20discrete%20events%20as%20separate%0Aprocesses%2C%20creating%20a%20gap%20in%20modeling%20complex%20systems%20where%20they%20interact%0Asynchronously.%20To%20bridge%20this%20gap%2C%20we%20introduce%20JointDiff%2C%20a%20novel%20diffusion%0Aframework%20designed%20to%20unify%20these%20two%20processes%20by%20simultaneously%20generating%0Acontinuous%20spatio-temporal%20data%20and%20synchronous%20discrete%20events.%20We%20demonstrate%0Aits%20efficacy%20in%20the%20sports%20domain%20by%20simultaneously%20modeling%20multi-agent%0Atrajectories%20and%20key%20possession%20events.%20This%20joint%20modeling%20is%20validated%20with%0Anon-controllable%20generation%20and%20two%20novel%20controllable%20generation%20scenarios%3A%0Aweak-possessor-guidance%2C%20which%20offers%20flexible%20semantic%20control%20over%20game%0Adynamics%20through%20a%20simple%20list%20of%20intended%20ball%20possessors%2C%20and%20text-guidance%2C%0Awhich%20enables%20fine-grained%2C%20language-driven%20generation.%20To%20enable%20the%0Aconditioning%20with%20these%20guidance%20signals%2C%20we%20introduce%20CrossGuid%2C%20an%20effective%0Aconditioning%20operation%20for%20multi-agent%20domains.%20We%20also%20share%20a%20new%20unified%0Asports%20benchmark%20enhanced%20with%20textual%20descriptions%20for%20soccer%20and%20football%0Adatasets.%20JointDiff%20achieves%20state-of-the-art%20performance%2C%20demonstrating%20that%0Ajoint%20modeling%20is%20crucial%20for%20building%20realistic%20and%20controllable%20generative%0Amodels%20for%20interactive%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22522v1&entry.124074799=Read"},
{"title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level\n  Feedback and Step-Level Reinforcement Learning", "author": "Zimu Lu and Houxing Ren and Yunqiao Yang and Ke Wang and Zhuofan Zong and Junting Pan and Mingjie Zhan and Hongsheng Li", "abstract": "  Agent systems powered by large language models (LLMs) have demonstrated\nimpressive performance on repository-level code-generation tasks. However, for\ntasks such as website codebase generation, which depend heavily on visual\neffects and user-interaction feedback, current code agents rely only on simple\ncode execution for feedback and verification. This approach fails to capture\nthe actual quality of the generated code. In this paper, we propose\nWebGen-Agent, a novel website-generation agent that leverages comprehensive and\nmulti-level visual feedback to iteratively generate and refine the website\ncodebase. Detailed and expressive text descriptions and suggestions regarding\nthe screenshots and GUI-agent testing of the websites are generated by a visual\nlanguage model (VLM), together with scores that quantify their quality. The\nscreenshot and GUI-agent scores are further integrated with a backtracking and\nselect-best mechanism, enhancing the performance of the agent. Utilizing the\naccurate visual scores inherent in the WebGen-Agent workflow, we further\nintroduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve\nthe ability of LLMs to act as the reasoning engine of WebGen-Agent. By using\nthe screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we\nprovide a dense and reliable process supervision signal, which effectively\nimproves the model's website-generation ability. On the WebGen-Bench dataset,\nWebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%\nand its appearance score from 3.0 to 3.9, outperforming the previous\nstate-of-the-art agent system. Additionally, our Step-GRPO training approach\nincreases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and\nraises the appearance score from 3.4 to 3.7.\n", "link": "http://arxiv.org/abs/2509.22644v1", "date": "2025-09-26", "relevancy": 2.3039, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6165}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5515}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WebGen-Agent%3A%20Enhancing%20Interactive%20Website%20Generation%20with%20Multi-Level%0A%20%20Feedback%20and%20Step-Level%20Reinforcement%20Learning&body=Title%3A%20WebGen-Agent%3A%20Enhancing%20Interactive%20Website%20Generation%20with%20Multi-Level%0A%20%20Feedback%20and%20Step-Level%20Reinforcement%20Learning%0AAuthor%3A%20Zimu%20Lu%20and%20Houxing%20Ren%20and%20Yunqiao%20Yang%20and%20Ke%20Wang%20and%20Zhuofan%20Zong%20and%20Junting%20Pan%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li%0AAbstract%3A%20%20%20Agent%20systems%20powered%20by%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aimpressive%20performance%20on%20repository-level%20code-generation%20tasks.%20However%2C%20for%0Atasks%20such%20as%20website%20codebase%20generation%2C%20which%20depend%20heavily%20on%20visual%0Aeffects%20and%20user-interaction%20feedback%2C%20current%20code%20agents%20rely%20only%20on%20simple%0Acode%20execution%20for%20feedback%20and%20verification.%20This%20approach%20fails%20to%20capture%0Athe%20actual%20quality%20of%20the%20generated%20code.%20In%20this%20paper%2C%20we%20propose%0AWebGen-Agent%2C%20a%20novel%20website-generation%20agent%20that%20leverages%20comprehensive%20and%0Amulti-level%20visual%20feedback%20to%20iteratively%20generate%20and%20refine%20the%20website%0Acodebase.%20Detailed%20and%20expressive%20text%20descriptions%20and%20suggestions%20regarding%0Athe%20screenshots%20and%20GUI-agent%20testing%20of%20the%20websites%20are%20generated%20by%20a%20visual%0Alanguage%20model%20%28VLM%29%2C%20together%20with%20scores%20that%20quantify%20their%20quality.%20The%0Ascreenshot%20and%20GUI-agent%20scores%20are%20further%20integrated%20with%20a%20backtracking%20and%0Aselect-best%20mechanism%2C%20enhancing%20the%20performance%20of%20the%20agent.%20Utilizing%20the%0Aaccurate%20visual%20scores%20inherent%20in%20the%20WebGen-Agent%20workflow%2C%20we%20further%0Aintroduce%20%5Ctextit%7BStep-GRPO%20with%20Screenshot%20and%20GUI-agent%20Feedback%7D%20to%20improve%0Athe%20ability%20of%20LLMs%20to%20act%20as%20the%20reasoning%20engine%20of%20WebGen-Agent.%20By%20using%0Athe%20screenshot%20and%20GUI-agent%20scores%20at%20each%20step%20as%20the%20reward%20in%20Step-GRPO%2C%20we%0Aprovide%20a%20dense%20and%20reliable%20process%20supervision%20signal%2C%20which%20effectively%0Aimproves%20the%20model%27s%20website-generation%20ability.%20On%20the%20WebGen-Bench%20dataset%2C%0AWebGen-Agent%20increases%20the%20accuracy%20of%20Claude-3.5-Sonnet%20from%2026.4%25%20to%2051.9%25%0Aand%20its%20appearance%20score%20from%203.0%20to%203.9%2C%20outperforming%20the%20previous%0Astate-of-the-art%20agent%20system.%20Additionally%2C%20our%20Step-GRPO%20training%20approach%0Aincreases%20the%20accuracy%20of%20Qwen2.5-Coder-7B-Instruct%20from%2038.9%25%20to%2045.4%25%20and%0Araises%20the%20appearance%20score%20from%203.4%20to%203.7.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22644v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWebGen-Agent%253A%2520Enhancing%2520Interactive%2520Website%2520Generation%2520with%2520Multi-Level%250A%2520%2520Feedback%2520and%2520Step-Level%2520Reinforcement%2520Learning%26entry.906535625%3DZimu%2520Lu%2520and%2520Houxing%2520Ren%2520and%2520Yunqiao%2520Yang%2520and%2520Ke%2520Wang%2520and%2520Zhuofan%2520Zong%2520and%2520Junting%2520Pan%2520and%2520Mingjie%2520Zhan%2520and%2520Hongsheng%2520Li%26entry.1292438233%3D%2520%2520Agent%2520systems%2520powered%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%250Aimpressive%2520performance%2520on%2520repository-level%2520code-generation%2520tasks.%2520However%252C%2520for%250Atasks%2520such%2520as%2520website%2520codebase%2520generation%252C%2520which%2520depend%2520heavily%2520on%2520visual%250Aeffects%2520and%2520user-interaction%2520feedback%252C%2520current%2520code%2520agents%2520rely%2520only%2520on%2520simple%250Acode%2520execution%2520for%2520feedback%2520and%2520verification.%2520This%2520approach%2520fails%2520to%2520capture%250Athe%2520actual%2520quality%2520of%2520the%2520generated%2520code.%2520In%2520this%2520paper%252C%2520we%2520propose%250AWebGen-Agent%252C%2520a%2520novel%2520website-generation%2520agent%2520that%2520leverages%2520comprehensive%2520and%250Amulti-level%2520visual%2520feedback%2520to%2520iteratively%2520generate%2520and%2520refine%2520the%2520website%250Acodebase.%2520Detailed%2520and%2520expressive%2520text%2520descriptions%2520and%2520suggestions%2520regarding%250Athe%2520screenshots%2520and%2520GUI-agent%2520testing%2520of%2520the%2520websites%2520are%2520generated%2520by%2520a%2520visual%250Alanguage%2520model%2520%2528VLM%2529%252C%2520together%2520with%2520scores%2520that%2520quantify%2520their%2520quality.%2520The%250Ascreenshot%2520and%2520GUI-agent%2520scores%2520are%2520further%2520integrated%2520with%2520a%2520backtracking%2520and%250Aselect-best%2520mechanism%252C%2520enhancing%2520the%2520performance%2520of%2520the%2520agent.%2520Utilizing%2520the%250Aaccurate%2520visual%2520scores%2520inherent%2520in%2520the%2520WebGen-Agent%2520workflow%252C%2520we%2520further%250Aintroduce%2520%255Ctextit%257BStep-GRPO%2520with%2520Screenshot%2520and%2520GUI-agent%2520Feedback%257D%2520to%2520improve%250Athe%2520ability%2520of%2520LLMs%2520to%2520act%2520as%2520the%2520reasoning%2520engine%2520of%2520WebGen-Agent.%2520By%2520using%250Athe%2520screenshot%2520and%2520GUI-agent%2520scores%2520at%2520each%2520step%2520as%2520the%2520reward%2520in%2520Step-GRPO%252C%2520we%250Aprovide%2520a%2520dense%2520and%2520reliable%2520process%2520supervision%2520signal%252C%2520which%2520effectively%250Aimproves%2520the%2520model%2527s%2520website-generation%2520ability.%2520On%2520the%2520WebGen-Bench%2520dataset%252C%250AWebGen-Agent%2520increases%2520the%2520accuracy%2520of%2520Claude-3.5-Sonnet%2520from%252026.4%2525%2520to%252051.9%2525%250Aand%2520its%2520appearance%2520score%2520from%25203.0%2520to%25203.9%252C%2520outperforming%2520the%2520previous%250Astate-of-the-art%2520agent%2520system.%2520Additionally%252C%2520our%2520Step-GRPO%2520training%2520approach%250Aincreases%2520the%2520accuracy%2520of%2520Qwen2.5-Coder-7B-Instruct%2520from%252038.9%2525%2520to%252045.4%2525%2520and%250Araises%2520the%2520appearance%2520score%2520from%25203.4%2520to%25203.7.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22644v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WebGen-Agent%3A%20Enhancing%20Interactive%20Website%20Generation%20with%20Multi-Level%0A%20%20Feedback%20and%20Step-Level%20Reinforcement%20Learning&entry.906535625=Zimu%20Lu%20and%20Houxing%20Ren%20and%20Yunqiao%20Yang%20and%20Ke%20Wang%20and%20Zhuofan%20Zong%20and%20Junting%20Pan%20and%20Mingjie%20Zhan%20and%20Hongsheng%20Li&entry.1292438233=%20%20Agent%20systems%20powered%20by%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Aimpressive%20performance%20on%20repository-level%20code-generation%20tasks.%20However%2C%20for%0Atasks%20such%20as%20website%20codebase%20generation%2C%20which%20depend%20heavily%20on%20visual%0Aeffects%20and%20user-interaction%20feedback%2C%20current%20code%20agents%20rely%20only%20on%20simple%0Acode%20execution%20for%20feedback%20and%20verification.%20This%20approach%20fails%20to%20capture%0Athe%20actual%20quality%20of%20the%20generated%20code.%20In%20this%20paper%2C%20we%20propose%0AWebGen-Agent%2C%20a%20novel%20website-generation%20agent%20that%20leverages%20comprehensive%20and%0Amulti-level%20visual%20feedback%20to%20iteratively%20generate%20and%20refine%20the%20website%0Acodebase.%20Detailed%20and%20expressive%20text%20descriptions%20and%20suggestions%20regarding%0Athe%20screenshots%20and%20GUI-agent%20testing%20of%20the%20websites%20are%20generated%20by%20a%20visual%0Alanguage%20model%20%28VLM%29%2C%20together%20with%20scores%20that%20quantify%20their%20quality.%20The%0Ascreenshot%20and%20GUI-agent%20scores%20are%20further%20integrated%20with%20a%20backtracking%20and%0Aselect-best%20mechanism%2C%20enhancing%20the%20performance%20of%20the%20agent.%20Utilizing%20the%0Aaccurate%20visual%20scores%20inherent%20in%20the%20WebGen-Agent%20workflow%2C%20we%20further%0Aintroduce%20%5Ctextit%7BStep-GRPO%20with%20Screenshot%20and%20GUI-agent%20Feedback%7D%20to%20improve%0Athe%20ability%20of%20LLMs%20to%20act%20as%20the%20reasoning%20engine%20of%20WebGen-Agent.%20By%20using%0Athe%20screenshot%20and%20GUI-agent%20scores%20at%20each%20step%20as%20the%20reward%20in%20Step-GRPO%2C%20we%0Aprovide%20a%20dense%20and%20reliable%20process%20supervision%20signal%2C%20which%20effectively%0Aimproves%20the%20model%27s%20website-generation%20ability.%20On%20the%20WebGen-Bench%20dataset%2C%0AWebGen-Agent%20increases%20the%20accuracy%20of%20Claude-3.5-Sonnet%20from%2026.4%25%20to%2051.9%25%0Aand%20its%20appearance%20score%20from%203.0%20to%203.9%2C%20outperforming%20the%20previous%0Astate-of-the-art%20agent%20system.%20Additionally%2C%20our%20Step-GRPO%20training%20approach%0Aincreases%20the%20accuracy%20of%20Qwen2.5-Coder-7B-Instruct%20from%2038.9%25%20to%2045.4%25%20and%0Araises%20the%20appearance%20score%20from%203.4%20to%203.7.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22644v1&entry.124074799=Read"},
{"title": "DriveAgent-R1: Advancing VLM-based Autonomous Driving with Active\n  Perception and Hybrid Thinking", "author": "Weicheng Zheng and Xiaofei Mao and Nanfei Ye and Pengxiang Li and Kun Zhan and Xianpeng Lang and Hang Zhao", "abstract": "  The advent of Vision-Language Models (VLMs) has significantly advanced\nend-to-end autonomous driving, demonstrating powerful reasoning abilities for\nhigh-level behavior planning tasks. However, existing methods are often\nconstrained by a passive perception paradigm, relying solely on text-based\nreasoning. This passivity restricts the model's capacity to actively seek\ncrucial visual evidence when faced with uncertainty. To address this, we\nintroduce DriveAgent-R1, the first autonomous driving agent capable of active\nperception for planning. In complex scenarios, DriveAgent-R1 proactively\ninvokes tools to perform visual reasoning, firmly grounding its decisions in\nvisual evidence, thereby enhancing both interpretability and reliability.\nFurthermore, we propose a hybrid thinking framework, inspired by human driver\ncognitive patterns, allowing the agent to adaptively switch between efficient\ntext-only reasoning and robust tool-augmented visual reasoning based on scene\ncomplexity. This capability is cultivated through a three-stage progressive\ntraining strategy, featuring a core Cascaded Reinforcement Learning (Cascaded\nRL) phase. Extensive experiments on the Drive-Internal dataset, which is rich\nin long-tail scenarios, and the public nuScenes dataset show that, with only 3B\nparameters, DriveAgent-R1 achieves competitive performance comparable to top\nclosed model systems such as GPT-5 and to human driving proficiency while\nremaining deployment-friendly, offering a proven path toward building more\nintelligent autonomous driving systems.\n", "link": "http://arxiv.org/abs/2507.20879v2", "date": "2025-09-26", "relevancy": 2.2979, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveAgent-R1%3A%20Advancing%20VLM-based%20Autonomous%20Driving%20with%20Active%0A%20%20Perception%20and%20Hybrid%20Thinking&body=Title%3A%20DriveAgent-R1%3A%20Advancing%20VLM-based%20Autonomous%20Driving%20with%20Active%0A%20%20Perception%20and%20Hybrid%20Thinking%0AAuthor%3A%20Weicheng%20Zheng%20and%20Xiaofei%20Mao%20and%20Nanfei%20Ye%20and%20Pengxiang%20Li%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20The%20advent%20of%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%20advanced%0Aend-to-end%20autonomous%20driving%2C%20demonstrating%20powerful%20reasoning%20abilities%20for%0Ahigh-level%20behavior%20planning%20tasks.%20However%2C%20existing%20methods%20are%20often%0Aconstrained%20by%20a%20passive%20perception%20paradigm%2C%20relying%20solely%20on%20text-based%0Areasoning.%20This%20passivity%20restricts%20the%20model%27s%20capacity%20to%20actively%20seek%0Acrucial%20visual%20evidence%20when%20faced%20with%20uncertainty.%20To%20address%20this%2C%20we%0Aintroduce%20DriveAgent-R1%2C%20the%20first%20autonomous%20driving%20agent%20capable%20of%20active%0Aperception%20for%20planning.%20In%20complex%20scenarios%2C%20DriveAgent-R1%20proactively%0Ainvokes%20tools%20to%20perform%20visual%20reasoning%2C%20firmly%20grounding%20its%20decisions%20in%0Avisual%20evidence%2C%20thereby%20enhancing%20both%20interpretability%20and%20reliability.%0AFurthermore%2C%20we%20propose%20a%20hybrid%20thinking%20framework%2C%20inspired%20by%20human%20driver%0Acognitive%20patterns%2C%20allowing%20the%20agent%20to%20adaptively%20switch%20between%20efficient%0Atext-only%20reasoning%20and%20robust%20tool-augmented%20visual%20reasoning%20based%20on%20scene%0Acomplexity.%20This%20capability%20is%20cultivated%20through%20a%20three-stage%20progressive%0Atraining%20strategy%2C%20featuring%20a%20core%20Cascaded%20Reinforcement%20Learning%20%28Cascaded%0ARL%29%20phase.%20Extensive%20experiments%20on%20the%20Drive-Internal%20dataset%2C%20which%20is%20rich%0Ain%20long-tail%20scenarios%2C%20and%20the%20public%20nuScenes%20dataset%20show%20that%2C%20with%20only%203B%0Aparameters%2C%20DriveAgent-R1%20achieves%20competitive%20performance%20comparable%20to%20top%0Aclosed%20model%20systems%20such%20as%20GPT-5%20and%20to%20human%20driving%20proficiency%20while%0Aremaining%20deployment-friendly%2C%20offering%20a%20proven%20path%20toward%20building%20more%0Aintelligent%20autonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveAgent-R1%253A%2520Advancing%2520VLM-based%2520Autonomous%2520Driving%2520with%2520Active%250A%2520%2520Perception%2520and%2520Hybrid%2520Thinking%26entry.906535625%3DWeicheng%2520Zheng%2520and%2520Xiaofei%2520Mao%2520and%2520Nanfei%2520Ye%2520and%2520Pengxiang%2520Li%2520and%2520Kun%2520Zhan%2520and%2520Xianpeng%2520Lang%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520significantly%2520advanced%250Aend-to-end%2520autonomous%2520driving%252C%2520demonstrating%2520powerful%2520reasoning%2520abilities%2520for%250Ahigh-level%2520behavior%2520planning%2520tasks.%2520However%252C%2520existing%2520methods%2520are%2520often%250Aconstrained%2520by%2520a%2520passive%2520perception%2520paradigm%252C%2520relying%2520solely%2520on%2520text-based%250Areasoning.%2520This%2520passivity%2520restricts%2520the%2520model%2527s%2520capacity%2520to%2520actively%2520seek%250Acrucial%2520visual%2520evidence%2520when%2520faced%2520with%2520uncertainty.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520DriveAgent-R1%252C%2520the%2520first%2520autonomous%2520driving%2520agent%2520capable%2520of%2520active%250Aperception%2520for%2520planning.%2520In%2520complex%2520scenarios%252C%2520DriveAgent-R1%2520proactively%250Ainvokes%2520tools%2520to%2520perform%2520visual%2520reasoning%252C%2520firmly%2520grounding%2520its%2520decisions%2520in%250Avisual%2520evidence%252C%2520thereby%2520enhancing%2520both%2520interpretability%2520and%2520reliability.%250AFurthermore%252C%2520we%2520propose%2520a%2520hybrid%2520thinking%2520framework%252C%2520inspired%2520by%2520human%2520driver%250Acognitive%2520patterns%252C%2520allowing%2520the%2520agent%2520to%2520adaptively%2520switch%2520between%2520efficient%250Atext-only%2520reasoning%2520and%2520robust%2520tool-augmented%2520visual%2520reasoning%2520based%2520on%2520scene%250Acomplexity.%2520This%2520capability%2520is%2520cultivated%2520through%2520a%2520three-stage%2520progressive%250Atraining%2520strategy%252C%2520featuring%2520a%2520core%2520Cascaded%2520Reinforcement%2520Learning%2520%2528Cascaded%250ARL%2529%2520phase.%2520Extensive%2520experiments%2520on%2520the%2520Drive-Internal%2520dataset%252C%2520which%2520is%2520rich%250Ain%2520long-tail%2520scenarios%252C%2520and%2520the%2520public%2520nuScenes%2520dataset%2520show%2520that%252C%2520with%2520only%25203B%250Aparameters%252C%2520DriveAgent-R1%2520achieves%2520competitive%2520performance%2520comparable%2520to%2520top%250Aclosed%2520model%2520systems%2520such%2520as%2520GPT-5%2520and%2520to%2520human%2520driving%2520proficiency%2520while%250Aremaining%2520deployment-friendly%252C%2520offering%2520a%2520proven%2520path%2520toward%2520building%2520more%250Aintelligent%2520autonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveAgent-R1%3A%20Advancing%20VLM-based%20Autonomous%20Driving%20with%20Active%0A%20%20Perception%20and%20Hybrid%20Thinking&entry.906535625=Weicheng%20Zheng%20and%20Xiaofei%20Mao%20and%20Nanfei%20Ye%20and%20Pengxiang%20Li%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao&entry.1292438233=%20%20The%20advent%20of%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%20advanced%0Aend-to-end%20autonomous%20driving%2C%20demonstrating%20powerful%20reasoning%20abilities%20for%0Ahigh-level%20behavior%20planning%20tasks.%20However%2C%20existing%20methods%20are%20often%0Aconstrained%20by%20a%20passive%20perception%20paradigm%2C%20relying%20solely%20on%20text-based%0Areasoning.%20This%20passivity%20restricts%20the%20model%27s%20capacity%20to%20actively%20seek%0Acrucial%20visual%20evidence%20when%20faced%20with%20uncertainty.%20To%20address%20this%2C%20we%0Aintroduce%20DriveAgent-R1%2C%20the%20first%20autonomous%20driving%20agent%20capable%20of%20active%0Aperception%20for%20planning.%20In%20complex%20scenarios%2C%20DriveAgent-R1%20proactively%0Ainvokes%20tools%20to%20perform%20visual%20reasoning%2C%20firmly%20grounding%20its%20decisions%20in%0Avisual%20evidence%2C%20thereby%20enhancing%20both%20interpretability%20and%20reliability.%0AFurthermore%2C%20we%20propose%20a%20hybrid%20thinking%20framework%2C%20inspired%20by%20human%20driver%0Acognitive%20patterns%2C%20allowing%20the%20agent%20to%20adaptively%20switch%20between%20efficient%0Atext-only%20reasoning%20and%20robust%20tool-augmented%20visual%20reasoning%20based%20on%20scene%0Acomplexity.%20This%20capability%20is%20cultivated%20through%20a%20three-stage%20progressive%0Atraining%20strategy%2C%20featuring%20a%20core%20Cascaded%20Reinforcement%20Learning%20%28Cascaded%0ARL%29%20phase.%20Extensive%20experiments%20on%20the%20Drive-Internal%20dataset%2C%20which%20is%20rich%0Ain%20long-tail%20scenarios%2C%20and%20the%20public%20nuScenes%20dataset%20show%20that%2C%20with%20only%203B%0Aparameters%2C%20DriveAgent-R1%20achieves%20competitive%20performance%20comparable%20to%20top%0Aclosed%20model%20systems%20such%20as%20GPT-5%20and%20to%20human%20driving%20proficiency%20while%0Aremaining%20deployment-friendly%2C%20offering%20a%20proven%20path%20toward%20building%20more%0Aintelligent%20autonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20879v2&entry.124074799=Read"},
{"title": "Color Names in Vision-Language Models", "author": "Alexandra Gomez-Villa and Pablo Hern\u00e1ndez-C\u00e1mara and Muhammad Atif Butt and Valero Laparra and Jesus Malo and Javier Vazquez-Corral", "abstract": "  Color serves as a fundamental dimension of human visual perception and a\nprimary means of communicating about objects and scenes. As vision-language\nmodels (VLMs) become increasingly prevalent, understanding whether they name\ncolors like humans is crucial for effective human-AI interaction. We present\nthe first systematic evaluation of color naming capabilities across VLMs,\nreplicating classic color naming methodologies using 957 color samples across\nfive representative models. Our results show that while VLMs achieve high\naccuracy on prototypical colors from classical studies, performance drops\nsignificantly on expanded, non-prototypical color sets. We identify 21 common\ncolor terms that consistently emerge across all models, revealing two distinct\napproaches: constrained models using predominantly basic terms versus expansive\nmodels employing systematic lightness modifiers. Cross-linguistic analysis\nacross nine languages demonstrates severe training imbalances favoring English\nand Chinese, with hue serving as the primary driver of color naming decisions.\nFinally, ablation studies reveal that language model architecture significantly\ninfluences color naming independent of visual processing capabilities.\n", "link": "http://arxiv.org/abs/2509.22524v1", "date": "2025-09-26", "relevancy": 2.2907, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Color%20Names%20in%20Vision-Language%20Models&body=Title%3A%20Color%20Names%20in%20Vision-Language%20Models%0AAuthor%3A%20Alexandra%20Gomez-Villa%20and%20Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Muhammad%20Atif%20Butt%20and%20Valero%20Laparra%20and%20Jesus%20Malo%20and%20Javier%20Vazquez-Corral%0AAbstract%3A%20%20%20Color%20serves%20as%20a%20fundamental%20dimension%20of%20human%20visual%20perception%20and%20a%0Aprimary%20means%20of%20communicating%20about%20objects%20and%20scenes.%20As%20vision-language%0Amodels%20%28VLMs%29%20become%20increasingly%20prevalent%2C%20understanding%20whether%20they%20name%0Acolors%20like%20humans%20is%20crucial%20for%20effective%20human-AI%20interaction.%20We%20present%0Athe%20first%20systematic%20evaluation%20of%20color%20naming%20capabilities%20across%20VLMs%2C%0Areplicating%20classic%20color%20naming%20methodologies%20using%20957%20color%20samples%20across%0Afive%20representative%20models.%20Our%20results%20show%20that%20while%20VLMs%20achieve%20high%0Aaccuracy%20on%20prototypical%20colors%20from%20classical%20studies%2C%20performance%20drops%0Asignificantly%20on%20expanded%2C%20non-prototypical%20color%20sets.%20We%20identify%2021%20common%0Acolor%20terms%20that%20consistently%20emerge%20across%20all%20models%2C%20revealing%20two%20distinct%0Aapproaches%3A%20constrained%20models%20using%20predominantly%20basic%20terms%20versus%20expansive%0Amodels%20employing%20systematic%20lightness%20modifiers.%20Cross-linguistic%20analysis%0Aacross%20nine%20languages%20demonstrates%20severe%20training%20imbalances%20favoring%20English%0Aand%20Chinese%2C%20with%20hue%20serving%20as%20the%20primary%20driver%20of%20color%20naming%20decisions.%0AFinally%2C%20ablation%20studies%20reveal%20that%20language%20model%20architecture%20significantly%0Ainfluences%20color%20naming%20independent%20of%20visual%20processing%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColor%2520Names%2520in%2520Vision-Language%2520Models%26entry.906535625%3DAlexandra%2520Gomez-Villa%2520and%2520Pablo%2520Hern%25C3%25A1ndez-C%25C3%25A1mara%2520and%2520Muhammad%2520Atif%2520Butt%2520and%2520Valero%2520Laparra%2520and%2520Jesus%2520Malo%2520and%2520Javier%2520Vazquez-Corral%26entry.1292438233%3D%2520%2520Color%2520serves%2520as%2520a%2520fundamental%2520dimension%2520of%2520human%2520visual%2520perception%2520and%2520a%250Aprimary%2520means%2520of%2520communicating%2520about%2520objects%2520and%2520scenes.%2520As%2520vision-language%250Amodels%2520%2528VLMs%2529%2520become%2520increasingly%2520prevalent%252C%2520understanding%2520whether%2520they%2520name%250Acolors%2520like%2520humans%2520is%2520crucial%2520for%2520effective%2520human-AI%2520interaction.%2520We%2520present%250Athe%2520first%2520systematic%2520evaluation%2520of%2520color%2520naming%2520capabilities%2520across%2520VLMs%252C%250Areplicating%2520classic%2520color%2520naming%2520methodologies%2520using%2520957%2520color%2520samples%2520across%250Afive%2520representative%2520models.%2520Our%2520results%2520show%2520that%2520while%2520VLMs%2520achieve%2520high%250Aaccuracy%2520on%2520prototypical%2520colors%2520from%2520classical%2520studies%252C%2520performance%2520drops%250Asignificantly%2520on%2520expanded%252C%2520non-prototypical%2520color%2520sets.%2520We%2520identify%252021%2520common%250Acolor%2520terms%2520that%2520consistently%2520emerge%2520across%2520all%2520models%252C%2520revealing%2520two%2520distinct%250Aapproaches%253A%2520constrained%2520models%2520using%2520predominantly%2520basic%2520terms%2520versus%2520expansive%250Amodels%2520employing%2520systematic%2520lightness%2520modifiers.%2520Cross-linguistic%2520analysis%250Aacross%2520nine%2520languages%2520demonstrates%2520severe%2520training%2520imbalances%2520favoring%2520English%250Aand%2520Chinese%252C%2520with%2520hue%2520serving%2520as%2520the%2520primary%2520driver%2520of%2520color%2520naming%2520decisions.%250AFinally%252C%2520ablation%2520studies%2520reveal%2520that%2520language%2520model%2520architecture%2520significantly%250Ainfluences%2520color%2520naming%2520independent%2520of%2520visual%2520processing%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Color%20Names%20in%20Vision-Language%20Models&entry.906535625=Alexandra%20Gomez-Villa%20and%20Pablo%20Hern%C3%A1ndez-C%C3%A1mara%20and%20Muhammad%20Atif%20Butt%20and%20Valero%20Laparra%20and%20Jesus%20Malo%20and%20Javier%20Vazquez-Corral&entry.1292438233=%20%20Color%20serves%20as%20a%20fundamental%20dimension%20of%20human%20visual%20perception%20and%20a%0Aprimary%20means%20of%20communicating%20about%20objects%20and%20scenes.%20As%20vision-language%0Amodels%20%28VLMs%29%20become%20increasingly%20prevalent%2C%20understanding%20whether%20they%20name%0Acolors%20like%20humans%20is%20crucial%20for%20effective%20human-AI%20interaction.%20We%20present%0Athe%20first%20systematic%20evaluation%20of%20color%20naming%20capabilities%20across%20VLMs%2C%0Areplicating%20classic%20color%20naming%20methodologies%20using%20957%20color%20samples%20across%0Afive%20representative%20models.%20Our%20results%20show%20that%20while%20VLMs%20achieve%20high%0Aaccuracy%20on%20prototypical%20colors%20from%20classical%20studies%2C%20performance%20drops%0Asignificantly%20on%20expanded%2C%20non-prototypical%20color%20sets.%20We%20identify%2021%20common%0Acolor%20terms%20that%20consistently%20emerge%20across%20all%20models%2C%20revealing%20two%20distinct%0Aapproaches%3A%20constrained%20models%20using%20predominantly%20basic%20terms%20versus%20expansive%0Amodels%20employing%20systematic%20lightness%20modifiers.%20Cross-linguistic%20analysis%0Aacross%20nine%20languages%20demonstrates%20severe%20training%20imbalances%20favoring%20English%0Aand%20Chinese%2C%20with%20hue%20serving%20as%20the%20primary%20driver%20of%20color%20naming%20decisions.%0AFinally%2C%20ablation%20studies%20reveal%20that%20language%20model%20architecture%20significantly%0Ainfluences%20color%20naming%20independent%20of%20visual%20processing%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22524v1&entry.124074799=Read"},
{"title": "Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded\n  GeoSpatial Chain-of-Thought for Vision-Language Models", "author": "Jiaqi Liu and Lang Sun and Ronghao Fu and Bo Yang", "abstract": "  Vision-Language Models (VLMs) in remote sensing often fail at complex\nanalytical tasks, a limitation stemming from their end-to-end training paradigm\nthat bypasses crucial reasoning steps and leads to unverifiable outputs. To\naddress this limitation, we introduce the Perceptually-Grounded Geospatial\nChain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as\na verifiable, multi-step process. We instill this analytical process through a\ntwo-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale\ndataset of structured Geo-CoT rationales. This strategy first employs\nsupervised fine-tuning (SFT) to instill the foundational cognitive\narchitecture, then leverages Group Reward Policy Optimization (GRPO) to refine\nthe model's reasoning policy towards factual correctness. The resulting model,\nRSThinker, outputs both a final answer and its justifying, verifiable\nanalytical trace. This capability yields dominant performance, significantly\noutperforming state-of-the-art models across a comprehensive range of tasks.\nThe public release of our Geo-CoT380k dataset and RSThinker model upon\npublication serves as a concrete pathway from opaque perception towards\nstructured, verifiable reasoning for Earth Observation.\n", "link": "http://arxiv.org/abs/2509.22221v1", "date": "2025-09-26", "relevancy": 2.2854, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5779}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Faithful%20Reasoning%20in%20Remote%20Sensing%3A%20A%20Perceptually-Grounded%0A%20%20GeoSpatial%20Chain-of-Thought%20for%20Vision-Language%20Models&body=Title%3A%20Towards%20Faithful%20Reasoning%20in%20Remote%20Sensing%3A%20A%20Perceptually-Grounded%0A%20%20GeoSpatial%20Chain-of-Thought%20for%20Vision-Language%20Models%0AAuthor%3A%20Jiaqi%20Liu%20and%20Lang%20Sun%20and%20Ronghao%20Fu%20and%20Bo%20Yang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20in%20remote%20sensing%20often%20fail%20at%20complex%0Aanalytical%20tasks%2C%20a%20limitation%20stemming%20from%20their%20end-to-end%20training%20paradigm%0Athat%20bypasses%20crucial%20reasoning%20steps%20and%20leads%20to%20unverifiable%20outputs.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20the%20Perceptually-Grounded%20Geospatial%0AChain-of-Thought%20%28Geo-CoT%29%2C%20a%20framework%20that%20models%20remote%20sensing%20analysis%20as%0Aa%20verifiable%2C%20multi-step%20process.%20We%20instill%20this%20analytical%20process%20through%20a%0Atwo-stage%20alignment%20strategy%2C%20leveraging%20Geo-CoT380k%2C%20the%20first%20large-scale%0Adataset%20of%20structured%20Geo-CoT%20rationales.%20This%20strategy%20first%20employs%0Asupervised%20fine-tuning%20%28SFT%29%20to%20instill%20the%20foundational%20cognitive%0Aarchitecture%2C%20then%20leverages%20Group%20Reward%20Policy%20Optimization%20%28GRPO%29%20to%20refine%0Athe%20model%27s%20reasoning%20policy%20towards%20factual%20correctness.%20The%20resulting%20model%2C%0ARSThinker%2C%20outputs%20both%20a%20final%20answer%20and%20its%20justifying%2C%20verifiable%0Aanalytical%20trace.%20This%20capability%20yields%20dominant%20performance%2C%20significantly%0Aoutperforming%20state-of-the-art%20models%20across%20a%20comprehensive%20range%20of%20tasks.%0AThe%20public%20release%20of%20our%20Geo-CoT380k%20dataset%20and%20RSThinker%20model%20upon%0Apublication%20serves%20as%20a%20concrete%20pathway%20from%20opaque%20perception%20towards%0Astructured%2C%20verifiable%20reasoning%20for%20Earth%20Observation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Faithful%2520Reasoning%2520in%2520Remote%2520Sensing%253A%2520A%2520Perceptually-Grounded%250A%2520%2520GeoSpatial%2520Chain-of-Thought%2520for%2520Vision-Language%2520Models%26entry.906535625%3DJiaqi%2520Liu%2520and%2520Lang%2520Sun%2520and%2520Ronghao%2520Fu%2520and%2520Bo%2520Yang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520in%2520remote%2520sensing%2520often%2520fail%2520at%2520complex%250Aanalytical%2520tasks%252C%2520a%2520limitation%2520stemming%2520from%2520their%2520end-to-end%2520training%2520paradigm%250Athat%2520bypasses%2520crucial%2520reasoning%2520steps%2520and%2520leads%2520to%2520unverifiable%2520outputs.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520Perceptually-Grounded%2520Geospatial%250AChain-of-Thought%2520%2528Geo-CoT%2529%252C%2520a%2520framework%2520that%2520models%2520remote%2520sensing%2520analysis%2520as%250Aa%2520verifiable%252C%2520multi-step%2520process.%2520We%2520instill%2520this%2520analytical%2520process%2520through%2520a%250Atwo-stage%2520alignment%2520strategy%252C%2520leveraging%2520Geo-CoT380k%252C%2520the%2520first%2520large-scale%250Adataset%2520of%2520structured%2520Geo-CoT%2520rationales.%2520This%2520strategy%2520first%2520employs%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520instill%2520the%2520foundational%2520cognitive%250Aarchitecture%252C%2520then%2520leverages%2520Group%2520Reward%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520refine%250Athe%2520model%2527s%2520reasoning%2520policy%2520towards%2520factual%2520correctness.%2520The%2520resulting%2520model%252C%250ARSThinker%252C%2520outputs%2520both%2520a%2520final%2520answer%2520and%2520its%2520justifying%252C%2520verifiable%250Aanalytical%2520trace.%2520This%2520capability%2520yields%2520dominant%2520performance%252C%2520significantly%250Aoutperforming%2520state-of-the-art%2520models%2520across%2520a%2520comprehensive%2520range%2520of%2520tasks.%250AThe%2520public%2520release%2520of%2520our%2520Geo-CoT380k%2520dataset%2520and%2520RSThinker%2520model%2520upon%250Apublication%2520serves%2520as%2520a%2520concrete%2520pathway%2520from%2520opaque%2520perception%2520towards%250Astructured%252C%2520verifiable%2520reasoning%2520for%2520Earth%2520Observation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Faithful%20Reasoning%20in%20Remote%20Sensing%3A%20A%20Perceptually-Grounded%0A%20%20GeoSpatial%20Chain-of-Thought%20for%20Vision-Language%20Models&entry.906535625=Jiaqi%20Liu%20and%20Lang%20Sun%20and%20Ronghao%20Fu%20and%20Bo%20Yang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20in%20remote%20sensing%20often%20fail%20at%20complex%0Aanalytical%20tasks%2C%20a%20limitation%20stemming%20from%20their%20end-to-end%20training%20paradigm%0Athat%20bypasses%20crucial%20reasoning%20steps%20and%20leads%20to%20unverifiable%20outputs.%20To%0Aaddress%20this%20limitation%2C%20we%20introduce%20the%20Perceptually-Grounded%20Geospatial%0AChain-of-Thought%20%28Geo-CoT%29%2C%20a%20framework%20that%20models%20remote%20sensing%20analysis%20as%0Aa%20verifiable%2C%20multi-step%20process.%20We%20instill%20this%20analytical%20process%20through%20a%0Atwo-stage%20alignment%20strategy%2C%20leveraging%20Geo-CoT380k%2C%20the%20first%20large-scale%0Adataset%20of%20structured%20Geo-CoT%20rationales.%20This%20strategy%20first%20employs%0Asupervised%20fine-tuning%20%28SFT%29%20to%20instill%20the%20foundational%20cognitive%0Aarchitecture%2C%20then%20leverages%20Group%20Reward%20Policy%20Optimization%20%28GRPO%29%20to%20refine%0Athe%20model%27s%20reasoning%20policy%20towards%20factual%20correctness.%20The%20resulting%20model%2C%0ARSThinker%2C%20outputs%20both%20a%20final%20answer%20and%20its%20justifying%2C%20verifiable%0Aanalytical%20trace.%20This%20capability%20yields%20dominant%20performance%2C%20significantly%0Aoutperforming%20state-of-the-art%20models%20across%20a%20comprehensive%20range%20of%20tasks.%0AThe%20public%20release%20of%20our%20Geo-CoT380k%20dataset%20and%20RSThinker%20model%20upon%0Apublication%20serves%20as%20a%20concrete%20pathway%20from%20opaque%20perception%20towards%0Astructured%2C%20verifiable%20reasoning%20for%20Earth%20Observation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22221v1&entry.124074799=Read"},
{"title": "EgoDemoGen: Novel Egocentric Demonstration Generation Enables\n  Viewpoint-Robust Manipulation", "author": "Yuan Xu and Jiabing Yang and Xiaofeng Wang and Yixiang Chen and Zheng Zhu and Bowen Fang and Guan Huang and Xinze Chen and Yun Ye and Qiang Zhang and Peiyan Li and Xiangnan Wu and Kai Wang and Bing Zhan and Shuo Lu and Jing Liu and Nianfeng Liu and Yan Huang and Liang Wang", "abstract": "  Imitation learning based policies perform well in robotic manipulation, but\nthey often degrade under *egocentric viewpoint shifts* when trained from a\nsingle egocentric viewpoint. To address this issue, we present **EgoDemoGen**,\na framework that generates *paired* novel egocentric demonstrations by\nretargeting actions in the novel egocentric frame and synthesizing the\ncorresponding egocentric observation videos with proposed generative video\nrepair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint\nreprojected scene video and a robot-only video rendered from the retargeted\njoint actions. EgoViewTransfer is finetuned from a pretrained video generation\nmodel using self-supervised double reprojection strategy. We evaluate\nEgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After\ntraining with a mixture of EgoDemoGen-generated novel egocentric demonstrations\nand original standard egocentric demonstrations, policy success rate improves\n**absolutely** by **+17.0%** for standard egocentric viewpoint and by\n**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,\nthe **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,\nperformance continues to improve as the proportion of EgoDemoGen-generated\ndemonstrations increases, with diminishing returns. These results demonstrate\nthat EgoDemoGen provides a practical route to egocentric viewpoint-robust\nrobotic manipulation.\n", "link": "http://arxiv.org/abs/2509.22578v1", "date": "2025-09-26", "relevancy": 2.2838, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.574}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5737}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoDemoGen%3A%20Novel%20Egocentric%20Demonstration%20Generation%20Enables%0A%20%20Viewpoint-Robust%20Manipulation&body=Title%3A%20EgoDemoGen%3A%20Novel%20Egocentric%20Demonstration%20Generation%20Enables%0A%20%20Viewpoint-Robust%20Manipulation%0AAuthor%3A%20Yuan%20Xu%20and%20Jiabing%20Yang%20and%20Xiaofeng%20Wang%20and%20Yixiang%20Chen%20and%20Zheng%20Zhu%20and%20Bowen%20Fang%20and%20Guan%20Huang%20and%20Xinze%20Chen%20and%20Yun%20Ye%20and%20Qiang%20Zhang%20and%20Peiyan%20Li%20and%20Xiangnan%20Wu%20and%20Kai%20Wang%20and%20Bing%20Zhan%20and%20Shuo%20Lu%20and%20Jing%20Liu%20and%20Nianfeng%20Liu%20and%20Yan%20Huang%20and%20Liang%20Wang%0AAbstract%3A%20%20%20Imitation%20learning%20based%20policies%20perform%20well%20in%20robotic%20manipulation%2C%20but%0Athey%20often%20degrade%20under%20%2Aegocentric%20viewpoint%20shifts%2A%20when%20trained%20from%20a%0Asingle%20egocentric%20viewpoint.%20To%20address%20this%20issue%2C%20we%20present%20%2A%2AEgoDemoGen%2A%2A%2C%0Aa%20framework%20that%20generates%20%2Apaired%2A%20novel%20egocentric%20demonstrations%20by%0Aretargeting%20actions%20in%20the%20novel%20egocentric%20frame%20and%20synthesizing%20the%0Acorresponding%20egocentric%20observation%20videos%20with%20proposed%20generative%20video%0Arepair%20model%20%2A%2AEgoViewTransfer%2A%2A%2C%20which%20is%20conditioned%20by%20a%20novel-viewpoint%0Areprojected%20scene%20video%20and%20a%20robot-only%20video%20rendered%20from%20the%20retargeted%0Ajoint%20actions.%20EgoViewTransfer%20is%20finetuned%20from%20a%20pretrained%20video%20generation%0Amodel%20using%20self-supervised%20double%20reprojection%20strategy.%20We%20evaluate%0AEgoDemoGen%20on%20both%20simulation%20%28RoboTwin2.0%29%20and%20real-world%20robot.%20After%0Atraining%20with%20a%20mixture%20of%20EgoDemoGen-generated%20novel%20egocentric%20demonstrations%0Aand%20original%20standard%20egocentric%20demonstrations%2C%20policy%20success%20rate%20improves%0A%2A%2Aabsolutely%2A%2A%20by%20%2A%2A%2B17.0%25%2A%2A%20for%20standard%20egocentric%20viewpoint%20and%20by%0A%2A%2A%2B17.7%25%2A%2A%20for%20novel%20egocentric%20viewpoints%20in%20simulation.%20On%20real-world%20robot%2C%0Athe%20%2A%2Aabsolute%2A%2A%20improvements%20are%20%2A%2A%2B18.3%25%2A%2A%20and%20%2A%2A%2B25.8%25%2A%2A.%20Moreover%2C%0Aperformance%20continues%20to%20improve%20as%20the%20proportion%20of%20EgoDemoGen-generated%0Ademonstrations%20increases%2C%20with%20diminishing%20returns.%20These%20results%20demonstrate%0Athat%20EgoDemoGen%20provides%20a%20practical%20route%20to%20egocentric%20viewpoint-robust%0Arobotic%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoDemoGen%253A%2520Novel%2520Egocentric%2520Demonstration%2520Generation%2520Enables%250A%2520%2520Viewpoint-Robust%2520Manipulation%26entry.906535625%3DYuan%2520Xu%2520and%2520Jiabing%2520Yang%2520and%2520Xiaofeng%2520Wang%2520and%2520Yixiang%2520Chen%2520and%2520Zheng%2520Zhu%2520and%2520Bowen%2520Fang%2520and%2520Guan%2520Huang%2520and%2520Xinze%2520Chen%2520and%2520Yun%2520Ye%2520and%2520Qiang%2520Zhang%2520and%2520Peiyan%2520Li%2520and%2520Xiangnan%2520Wu%2520and%2520Kai%2520Wang%2520and%2520Bing%2520Zhan%2520and%2520Shuo%2520Lu%2520and%2520Jing%2520Liu%2520and%2520Nianfeng%2520Liu%2520and%2520Yan%2520Huang%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520based%2520policies%2520perform%2520well%2520in%2520robotic%2520manipulation%252C%2520but%250Athey%2520often%2520degrade%2520under%2520%252Aegocentric%2520viewpoint%2520shifts%252A%2520when%2520trained%2520from%2520a%250Asingle%2520egocentric%2520viewpoint.%2520To%2520address%2520this%2520issue%252C%2520we%2520present%2520%252A%252AEgoDemoGen%252A%252A%252C%250Aa%2520framework%2520that%2520generates%2520%252Apaired%252A%2520novel%2520egocentric%2520demonstrations%2520by%250Aretargeting%2520actions%2520in%2520the%2520novel%2520egocentric%2520frame%2520and%2520synthesizing%2520the%250Acorresponding%2520egocentric%2520observation%2520videos%2520with%2520proposed%2520generative%2520video%250Arepair%2520model%2520%252A%252AEgoViewTransfer%252A%252A%252C%2520which%2520is%2520conditioned%2520by%2520a%2520novel-viewpoint%250Areprojected%2520scene%2520video%2520and%2520a%2520robot-only%2520video%2520rendered%2520from%2520the%2520retargeted%250Ajoint%2520actions.%2520EgoViewTransfer%2520is%2520finetuned%2520from%2520a%2520pretrained%2520video%2520generation%250Amodel%2520using%2520self-supervised%2520double%2520reprojection%2520strategy.%2520We%2520evaluate%250AEgoDemoGen%2520on%2520both%2520simulation%2520%2528RoboTwin2.0%2529%2520and%2520real-world%2520robot.%2520After%250Atraining%2520with%2520a%2520mixture%2520of%2520EgoDemoGen-generated%2520novel%2520egocentric%2520demonstrations%250Aand%2520original%2520standard%2520egocentric%2520demonstrations%252C%2520policy%2520success%2520rate%2520improves%250A%252A%252Aabsolutely%252A%252A%2520by%2520%252A%252A%252B17.0%2525%252A%252A%2520for%2520standard%2520egocentric%2520viewpoint%2520and%2520by%250A%252A%252A%252B17.7%2525%252A%252A%2520for%2520novel%2520egocentric%2520viewpoints%2520in%2520simulation.%2520On%2520real-world%2520robot%252C%250Athe%2520%252A%252Aabsolute%252A%252A%2520improvements%2520are%2520%252A%252A%252B18.3%2525%252A%252A%2520and%2520%252A%252A%252B25.8%2525%252A%252A.%2520Moreover%252C%250Aperformance%2520continues%2520to%2520improve%2520as%2520the%2520proportion%2520of%2520EgoDemoGen-generated%250Ademonstrations%2520increases%252C%2520with%2520diminishing%2520returns.%2520These%2520results%2520demonstrate%250Athat%2520EgoDemoGen%2520provides%2520a%2520practical%2520route%2520to%2520egocentric%2520viewpoint-robust%250Arobotic%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoDemoGen%3A%20Novel%20Egocentric%20Demonstration%20Generation%20Enables%0A%20%20Viewpoint-Robust%20Manipulation&entry.906535625=Yuan%20Xu%20and%20Jiabing%20Yang%20and%20Xiaofeng%20Wang%20and%20Yixiang%20Chen%20and%20Zheng%20Zhu%20and%20Bowen%20Fang%20and%20Guan%20Huang%20and%20Xinze%20Chen%20and%20Yun%20Ye%20and%20Qiang%20Zhang%20and%20Peiyan%20Li%20and%20Xiangnan%20Wu%20and%20Kai%20Wang%20and%20Bing%20Zhan%20and%20Shuo%20Lu%20and%20Jing%20Liu%20and%20Nianfeng%20Liu%20and%20Yan%20Huang%20and%20Liang%20Wang&entry.1292438233=%20%20Imitation%20learning%20based%20policies%20perform%20well%20in%20robotic%20manipulation%2C%20but%0Athey%20often%20degrade%20under%20%2Aegocentric%20viewpoint%20shifts%2A%20when%20trained%20from%20a%0Asingle%20egocentric%20viewpoint.%20To%20address%20this%20issue%2C%20we%20present%20%2A%2AEgoDemoGen%2A%2A%2C%0Aa%20framework%20that%20generates%20%2Apaired%2A%20novel%20egocentric%20demonstrations%20by%0Aretargeting%20actions%20in%20the%20novel%20egocentric%20frame%20and%20synthesizing%20the%0Acorresponding%20egocentric%20observation%20videos%20with%20proposed%20generative%20video%0Arepair%20model%20%2A%2AEgoViewTransfer%2A%2A%2C%20which%20is%20conditioned%20by%20a%20novel-viewpoint%0Areprojected%20scene%20video%20and%20a%20robot-only%20video%20rendered%20from%20the%20retargeted%0Ajoint%20actions.%20EgoViewTransfer%20is%20finetuned%20from%20a%20pretrained%20video%20generation%0Amodel%20using%20self-supervised%20double%20reprojection%20strategy.%20We%20evaluate%0AEgoDemoGen%20on%20both%20simulation%20%28RoboTwin2.0%29%20and%20real-world%20robot.%20After%0Atraining%20with%20a%20mixture%20of%20EgoDemoGen-generated%20novel%20egocentric%20demonstrations%0Aand%20original%20standard%20egocentric%20demonstrations%2C%20policy%20success%20rate%20improves%0A%2A%2Aabsolutely%2A%2A%20by%20%2A%2A%2B17.0%25%2A%2A%20for%20standard%20egocentric%20viewpoint%20and%20by%0A%2A%2A%2B17.7%25%2A%2A%20for%20novel%20egocentric%20viewpoints%20in%20simulation.%20On%20real-world%20robot%2C%0Athe%20%2A%2Aabsolute%2A%2A%20improvements%20are%20%2A%2A%2B18.3%25%2A%2A%20and%20%2A%2A%2B25.8%25%2A%2A.%20Moreover%2C%0Aperformance%20continues%20to%20improve%20as%20the%20proportion%20of%20EgoDemoGen-generated%0Ademonstrations%20increases%2C%20with%20diminishing%20returns.%20These%20results%20demonstrate%0Athat%20EgoDemoGen%20provides%20a%20practical%20route%20to%20egocentric%20viewpoint-robust%0Arobotic%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22578v1&entry.124074799=Read"},
{"title": "DoDo-Code: an Efficient Levenshtein Distance Embedding-based Code for\n  4-ary IDS Channel", "author": "Alan J. X. Guo and Sihan Sun and Xiang Wei and Mengyi Wei and Xin Chen", "abstract": "  With the emergence of new storage and communication methods, the insertion,\ndeletion, and substitution (IDS) channel has attracted considerable attention.\nHowever, many topics on the IDS channel and the associated Levenshtein distance\nremain open, making the invention of a novel IDS-correcting code a hard task.\nFurthermore, current studies on single-IDS-correcting code misalign with the\nrequirements of applications which necessitates the correcting of multiple\nerrors. Compromise solutions have involved shortening codewords to reduce the\nchance of multiple errors. However, the code rates of existing codes are poor\nat short lengths, diminishing the overall storage density. In this study, a\nnovel method is introduced for designing high-code-rate single-IDS-correcting\ncodewords through deep Levenshtein distance embedding. A deep learning model is\nutilized to project the sequences into embedding vectors that preserve the\nLevenshtein distances between the original sequences. This embedding space\nserves as a proxy for the complex Levenshtein domain, within which algorithms\nfor codeword search and segment correcting is developed. While the concept\nunderpinning this approach is straightforward, it bypasses the mathematical\nchallenges typically encountered in code design. The proposed method results in\na code rate that outperforms existing combinatorial solutions, particularly for\ndesigning short-length codewords.\n", "link": "http://arxiv.org/abs/2312.12717v2", "date": "2025-09-26", "relevancy": 2.2798, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoDo-Code%3A%20an%20Efficient%20Levenshtein%20Distance%20Embedding-based%20Code%20for%0A%20%204-ary%20IDS%20Channel&body=Title%3A%20DoDo-Code%3A%20an%20Efficient%20Levenshtein%20Distance%20Embedding-based%20Code%20for%0A%20%204-ary%20IDS%20Channel%0AAuthor%3A%20Alan%20J.%20X.%20Guo%20and%20Sihan%20Sun%20and%20Xiang%20Wei%20and%20Mengyi%20Wei%20and%20Xin%20Chen%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20new%20storage%20and%20communication%20methods%2C%20the%20insertion%2C%0Adeletion%2C%20and%20substitution%20%28IDS%29%20channel%20has%20attracted%20considerable%20attention.%0AHowever%2C%20many%20topics%20on%20the%20IDS%20channel%20and%20the%20associated%20Levenshtein%20distance%0Aremain%20open%2C%20making%20the%20invention%20of%20a%20novel%20IDS-correcting%20code%20a%20hard%20task.%0AFurthermore%2C%20current%20studies%20on%20single-IDS-correcting%20code%20misalign%20with%20the%0Arequirements%20of%20applications%20which%20necessitates%20the%20correcting%20of%20multiple%0Aerrors.%20Compromise%20solutions%20have%20involved%20shortening%20codewords%20to%20reduce%20the%0Achance%20of%20multiple%20errors.%20However%2C%20the%20code%20rates%20of%20existing%20codes%20are%20poor%0Aat%20short%20lengths%2C%20diminishing%20the%20overall%20storage%20density.%20In%20this%20study%2C%20a%0Anovel%20method%20is%20introduced%20for%20designing%20high-code-rate%20single-IDS-correcting%0Acodewords%20through%20deep%20Levenshtein%20distance%20embedding.%20A%20deep%20learning%20model%20is%0Autilized%20to%20project%20the%20sequences%20into%20embedding%20vectors%20that%20preserve%20the%0ALevenshtein%20distances%20between%20the%20original%20sequences.%20This%20embedding%20space%0Aserves%20as%20a%20proxy%20for%20the%20complex%20Levenshtein%20domain%2C%20within%20which%20algorithms%0Afor%20codeword%20search%20and%20segment%20correcting%20is%20developed.%20While%20the%20concept%0Aunderpinning%20this%20approach%20is%20straightforward%2C%20it%20bypasses%20the%20mathematical%0Achallenges%20typically%20encountered%20in%20code%20design.%20The%20proposed%20method%20results%20in%0Aa%20code%20rate%20that%20outperforms%20existing%20combinatorial%20solutions%2C%20particularly%20for%0Adesigning%20short-length%20codewords.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.12717v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoDo-Code%253A%2520an%2520Efficient%2520Levenshtein%2520Distance%2520Embedding-based%2520Code%2520for%250A%2520%25204-ary%2520IDS%2520Channel%26entry.906535625%3DAlan%2520J.%2520X.%2520Guo%2520and%2520Sihan%2520Sun%2520and%2520Xiang%2520Wei%2520and%2520Mengyi%2520Wei%2520and%2520Xin%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520new%2520storage%2520and%2520communication%2520methods%252C%2520the%2520insertion%252C%250Adeletion%252C%2520and%2520substitution%2520%2528IDS%2529%2520channel%2520has%2520attracted%2520considerable%2520attention.%250AHowever%252C%2520many%2520topics%2520on%2520the%2520IDS%2520channel%2520and%2520the%2520associated%2520Levenshtein%2520distance%250Aremain%2520open%252C%2520making%2520the%2520invention%2520of%2520a%2520novel%2520IDS-correcting%2520code%2520a%2520hard%2520task.%250AFurthermore%252C%2520current%2520studies%2520on%2520single-IDS-correcting%2520code%2520misalign%2520with%2520the%250Arequirements%2520of%2520applications%2520which%2520necessitates%2520the%2520correcting%2520of%2520multiple%250Aerrors.%2520Compromise%2520solutions%2520have%2520involved%2520shortening%2520codewords%2520to%2520reduce%2520the%250Achance%2520of%2520multiple%2520errors.%2520However%252C%2520the%2520code%2520rates%2520of%2520existing%2520codes%2520are%2520poor%250Aat%2520short%2520lengths%252C%2520diminishing%2520the%2520overall%2520storage%2520density.%2520In%2520this%2520study%252C%2520a%250Anovel%2520method%2520is%2520introduced%2520for%2520designing%2520high-code-rate%2520single-IDS-correcting%250Acodewords%2520through%2520deep%2520Levenshtein%2520distance%2520embedding.%2520A%2520deep%2520learning%2520model%2520is%250Autilized%2520to%2520project%2520the%2520sequences%2520into%2520embedding%2520vectors%2520that%2520preserve%2520the%250ALevenshtein%2520distances%2520between%2520the%2520original%2520sequences.%2520This%2520embedding%2520space%250Aserves%2520as%2520a%2520proxy%2520for%2520the%2520complex%2520Levenshtein%2520domain%252C%2520within%2520which%2520algorithms%250Afor%2520codeword%2520search%2520and%2520segment%2520correcting%2520is%2520developed.%2520While%2520the%2520concept%250Aunderpinning%2520this%2520approach%2520is%2520straightforward%252C%2520it%2520bypasses%2520the%2520mathematical%250Achallenges%2520typically%2520encountered%2520in%2520code%2520design.%2520The%2520proposed%2520method%2520results%2520in%250Aa%2520code%2520rate%2520that%2520outperforms%2520existing%2520combinatorial%2520solutions%252C%2520particularly%2520for%250Adesigning%2520short-length%2520codewords.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.12717v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoDo-Code%3A%20an%20Efficient%20Levenshtein%20Distance%20Embedding-based%20Code%20for%0A%20%204-ary%20IDS%20Channel&entry.906535625=Alan%20J.%20X.%20Guo%20and%20Sihan%20Sun%20and%20Xiang%20Wei%20and%20Mengyi%20Wei%20and%20Xin%20Chen&entry.1292438233=%20%20With%20the%20emergence%20of%20new%20storage%20and%20communication%20methods%2C%20the%20insertion%2C%0Adeletion%2C%20and%20substitution%20%28IDS%29%20channel%20has%20attracted%20considerable%20attention.%0AHowever%2C%20many%20topics%20on%20the%20IDS%20channel%20and%20the%20associated%20Levenshtein%20distance%0Aremain%20open%2C%20making%20the%20invention%20of%20a%20novel%20IDS-correcting%20code%20a%20hard%20task.%0AFurthermore%2C%20current%20studies%20on%20single-IDS-correcting%20code%20misalign%20with%20the%0Arequirements%20of%20applications%20which%20necessitates%20the%20correcting%20of%20multiple%0Aerrors.%20Compromise%20solutions%20have%20involved%20shortening%20codewords%20to%20reduce%20the%0Achance%20of%20multiple%20errors.%20However%2C%20the%20code%20rates%20of%20existing%20codes%20are%20poor%0Aat%20short%20lengths%2C%20diminishing%20the%20overall%20storage%20density.%20In%20this%20study%2C%20a%0Anovel%20method%20is%20introduced%20for%20designing%20high-code-rate%20single-IDS-correcting%0Acodewords%20through%20deep%20Levenshtein%20distance%20embedding.%20A%20deep%20learning%20model%20is%0Autilized%20to%20project%20the%20sequences%20into%20embedding%20vectors%20that%20preserve%20the%0ALevenshtein%20distances%20between%20the%20original%20sequences.%20This%20embedding%20space%0Aserves%20as%20a%20proxy%20for%20the%20complex%20Levenshtein%20domain%2C%20within%20which%20algorithms%0Afor%20codeword%20search%20and%20segment%20correcting%20is%20developed.%20While%20the%20concept%0Aunderpinning%20this%20approach%20is%20straightforward%2C%20it%20bypasses%20the%20mathematical%0Achallenges%20typically%20encountered%20in%20code%20design.%20The%20proposed%20method%20results%20in%0Aa%20code%20rate%20that%20outperforms%20existing%20combinatorial%20solutions%2C%20particularly%20for%0Adesigning%20short-length%20codewords.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.12717v2&entry.124074799=Read"},
{"title": "MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for\n  Multimodal Large Language Models", "author": "Xiaolong Wang and Zhaolu Kang and Wangyuxuan Zhai and Xinyue Lou and Yunghwei Lai and Ziyue Wang and Yawen Wang and Kaiyu Huang and Yile Wang and Peng Li and Yang Liu", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances across numerous vision-language tasks. MLLMs have shown promising\ncapability in aligning visual and textual modalities, allowing them to process\nimage-text pairs with clear and explicit meanings. However, resolving the\ninherent ambiguities present in real-world language and visual contexts remains\na challenge. Existing multimodal benchmarks typically overlook linguistic and\nvisual ambiguities, relying mainly on unimodal context for disambiguation and\nthus failing to exploit the mutual clarification potential between modalities.\nTo bridge this gap, we introduce MUCAR, a novel and challenging benchmark\ndesigned explicitly for evaluating multimodal ambiguity resolution across\nmultilingual and cross-modal scenarios. MUCAR includes first a multilingual\ndataset where ambiguous textual expressions are uniquely resolved by\ncorresponding visual contexts, and second a dual-ambiguity dataset that\nsystematically pairs ambiguous images with ambiguous textual contexts, with\neach combination carefully constructed to yield a single, clear interpretation\nthrough mutual disambiguation. Extensive evaluations involving 19\nstate-of-the-art multimodal models--encompassing both open-source and\nproprietary architectures--reveal substantial gaps compared to human-level\nperformance, highlighting the need for future research into more sophisticated\ncross-modal ambiguity comprehension methods, further pushing the boundaries of\nmultimodal reasoning.\n", "link": "http://arxiv.org/abs/2506.17046v2", "date": "2025-09-26", "relevancy": 2.2631, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUCAR%3A%20Benchmarking%20Multilingual%20Cross-Modal%20Ambiguity%20Resolution%20for%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20MUCAR%3A%20Benchmarking%20Multilingual%20Cross-Modal%20Ambiguity%20Resolution%20for%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Xiaolong%20Wang%20and%20Zhaolu%20Kang%20and%20Wangyuxuan%20Zhai%20and%20Xinyue%20Lou%20and%20Yunghwei%20Lai%20and%20Ziyue%20Wang%20and%20Yawen%20Wang%20and%20Kaiyu%20Huang%20and%20Yile%20Wang%20and%20Peng%20Li%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20across%20numerous%20vision-language%20tasks.%20MLLMs%20have%20shown%20promising%0Acapability%20in%20aligning%20visual%20and%20textual%20modalities%2C%20allowing%20them%20to%20process%0Aimage-text%20pairs%20with%20clear%20and%20explicit%20meanings.%20However%2C%20resolving%20the%0Ainherent%20ambiguities%20present%20in%20real-world%20language%20and%20visual%20contexts%20remains%0Aa%20challenge.%20Existing%20multimodal%20benchmarks%20typically%20overlook%20linguistic%20and%0Avisual%20ambiguities%2C%20relying%20mainly%20on%20unimodal%20context%20for%20disambiguation%20and%0Athus%20failing%20to%20exploit%20the%20mutual%20clarification%20potential%20between%20modalities.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20MUCAR%2C%20a%20novel%20and%20challenging%20benchmark%0Adesigned%20explicitly%20for%20evaluating%20multimodal%20ambiguity%20resolution%20across%0Amultilingual%20and%20cross-modal%20scenarios.%20MUCAR%20includes%20first%20a%20multilingual%0Adataset%20where%20ambiguous%20textual%20expressions%20are%20uniquely%20resolved%20by%0Acorresponding%20visual%20contexts%2C%20and%20second%20a%20dual-ambiguity%20dataset%20that%0Asystematically%20pairs%20ambiguous%20images%20with%20ambiguous%20textual%20contexts%2C%20with%0Aeach%20combination%20carefully%20constructed%20to%20yield%20a%20single%2C%20clear%20interpretation%0Athrough%20mutual%20disambiguation.%20Extensive%20evaluations%20involving%2019%0Astate-of-the-art%20multimodal%20models--encompassing%20both%20open-source%20and%0Aproprietary%20architectures--reveal%20substantial%20gaps%20compared%20to%20human-level%0Aperformance%2C%20highlighting%20the%20need%20for%20future%20research%20into%20more%20sophisticated%0Across-modal%20ambiguity%20comprehension%20methods%2C%20further%20pushing%20the%20boundaries%20of%0Amultimodal%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.17046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUCAR%253A%2520Benchmarking%2520Multilingual%2520Cross-Modal%2520Ambiguity%2520Resolution%2520for%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DXiaolong%2520Wang%2520and%2520Zhaolu%2520Kang%2520and%2520Wangyuxuan%2520Zhai%2520and%2520Xinyue%2520Lou%2520and%2520Yunghwei%2520Lai%2520and%2520Ziyue%2520Wang%2520and%2520Yawen%2520Wang%2520and%2520Kaiyu%2520Huang%2520and%2520Yile%2520Wang%2520and%2520Peng%2520Li%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%250Aadvances%2520across%2520numerous%2520vision-language%2520tasks.%2520MLLMs%2520have%2520shown%2520promising%250Acapability%2520in%2520aligning%2520visual%2520and%2520textual%2520modalities%252C%2520allowing%2520them%2520to%2520process%250Aimage-text%2520pairs%2520with%2520clear%2520and%2520explicit%2520meanings.%2520However%252C%2520resolving%2520the%250Ainherent%2520ambiguities%2520present%2520in%2520real-world%2520language%2520and%2520visual%2520contexts%2520remains%250Aa%2520challenge.%2520Existing%2520multimodal%2520benchmarks%2520typically%2520overlook%2520linguistic%2520and%250Avisual%2520ambiguities%252C%2520relying%2520mainly%2520on%2520unimodal%2520context%2520for%2520disambiguation%2520and%250Athus%2520failing%2520to%2520exploit%2520the%2520mutual%2520clarification%2520potential%2520between%2520modalities.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520MUCAR%252C%2520a%2520novel%2520and%2520challenging%2520benchmark%250Adesigned%2520explicitly%2520for%2520evaluating%2520multimodal%2520ambiguity%2520resolution%2520across%250Amultilingual%2520and%2520cross-modal%2520scenarios.%2520MUCAR%2520includes%2520first%2520a%2520multilingual%250Adataset%2520where%2520ambiguous%2520textual%2520expressions%2520are%2520uniquely%2520resolved%2520by%250Acorresponding%2520visual%2520contexts%252C%2520and%2520second%2520a%2520dual-ambiguity%2520dataset%2520that%250Asystematically%2520pairs%2520ambiguous%2520images%2520with%2520ambiguous%2520textual%2520contexts%252C%2520with%250Aeach%2520combination%2520carefully%2520constructed%2520to%2520yield%2520a%2520single%252C%2520clear%2520interpretation%250Athrough%2520mutual%2520disambiguation.%2520Extensive%2520evaluations%2520involving%252019%250Astate-of-the-art%2520multimodal%2520models--encompassing%2520both%2520open-source%2520and%250Aproprietary%2520architectures--reveal%2520substantial%2520gaps%2520compared%2520to%2520human-level%250Aperformance%252C%2520highlighting%2520the%2520need%2520for%2520future%2520research%2520into%2520more%2520sophisticated%250Across-modal%2520ambiguity%2520comprehension%2520methods%252C%2520further%2520pushing%2520the%2520boundaries%2520of%250Amultimodal%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.17046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUCAR%3A%20Benchmarking%20Multilingual%20Cross-Modal%20Ambiguity%20Resolution%20for%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Xiaolong%20Wang%20and%20Zhaolu%20Kang%20and%20Wangyuxuan%20Zhai%20and%20Xinyue%20Lou%20and%20Yunghwei%20Lai%20and%20Ziyue%20Wang%20and%20Yawen%20Wang%20and%20Kaiyu%20Huang%20and%20Yile%20Wang%20and%20Peng%20Li%20and%20Yang%20Liu&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aadvances%20across%20numerous%20vision-language%20tasks.%20MLLMs%20have%20shown%20promising%0Acapability%20in%20aligning%20visual%20and%20textual%20modalities%2C%20allowing%20them%20to%20process%0Aimage-text%20pairs%20with%20clear%20and%20explicit%20meanings.%20However%2C%20resolving%20the%0Ainherent%20ambiguities%20present%20in%20real-world%20language%20and%20visual%20contexts%20remains%0Aa%20challenge.%20Existing%20multimodal%20benchmarks%20typically%20overlook%20linguistic%20and%0Avisual%20ambiguities%2C%20relying%20mainly%20on%20unimodal%20context%20for%20disambiguation%20and%0Athus%20failing%20to%20exploit%20the%20mutual%20clarification%20potential%20between%20modalities.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20MUCAR%2C%20a%20novel%20and%20challenging%20benchmark%0Adesigned%20explicitly%20for%20evaluating%20multimodal%20ambiguity%20resolution%20across%0Amultilingual%20and%20cross-modal%20scenarios.%20MUCAR%20includes%20first%20a%20multilingual%0Adataset%20where%20ambiguous%20textual%20expressions%20are%20uniquely%20resolved%20by%0Acorresponding%20visual%20contexts%2C%20and%20second%20a%20dual-ambiguity%20dataset%20that%0Asystematically%20pairs%20ambiguous%20images%20with%20ambiguous%20textual%20contexts%2C%20with%0Aeach%20combination%20carefully%20constructed%20to%20yield%20a%20single%2C%20clear%20interpretation%0Athrough%20mutual%20disambiguation.%20Extensive%20evaluations%20involving%2019%0Astate-of-the-art%20multimodal%20models--encompassing%20both%20open-source%20and%0Aproprietary%20architectures--reveal%20substantial%20gaps%20compared%20to%20human-level%0Aperformance%2C%20highlighting%20the%20need%20for%20future%20research%20into%20more%20sophisticated%0Across-modal%20ambiguity%20comprehension%20methods%2C%20further%20pushing%20the%20boundaries%20of%0Amultimodal%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.17046v2&entry.124074799=Read"},
{"title": "Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding", "author": "S M A Sharif and Abdur Rehman and Fayaz Ali Dharejo and Radu Timofte and Rizwan Ali Naqvi", "abstract": "  Real-world images often suffer from spatially diverse degradations such as\nhaze, rain, snow, and low-light, significantly impacting visual quality and\ndownstream vision tasks. Existing all-in-one restoration (AIR) approaches\neither depend on external text prompts or embed hand-crafted architectural\npriors (e.g., frequency heuristics); both impose discrete, brittle assumptions\nthat weaken generalization to unseen or mixed degradations. To address this\nlimitation, we propose to reframe AIR as learned latent prior inference, where\ndegradation-aware representations are automatically inferred from the input\nwithout explicit task cues. Based on latent priors, we formulate AIR as a\nstructured reasoning paradigm: (1) which features to route (adaptive feature\nselection), (2) where to restore (spatial localization), and (3) what to\nrestore (degradation semantics). We design a lightweight decoding module that\nefficiently leverages these latent encoded cues for spatially-adaptive\nrestoration. Extensive experiments across six common degradation tasks, five\ncompound settings, and previously unseen degradations demonstrate that our\nmethod outperforms state-of-the-art (SOTA) approaches, achieving an average\nPSNR improvement of 1.68 dB while being three times more efficient.\n", "link": "http://arxiv.org/abs/2509.17792v2", "date": "2025-09-26", "relevancy": 2.26, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5849}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5678}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding&body=Title%3A%20Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding%0AAuthor%3A%20S%20M%20A%20Sharif%20and%20Abdur%20Rehman%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte%20and%20Rizwan%20Ali%20Naqvi%0AAbstract%3A%20%20%20Real-world%20images%20often%20suffer%20from%20spatially%20diverse%20degradations%20such%20as%0Ahaze%2C%20rain%2C%20snow%2C%20and%20low-light%2C%20significantly%20impacting%20visual%20quality%20and%0Adownstream%20vision%20tasks.%20Existing%20all-in-one%20restoration%20%28AIR%29%20approaches%0Aeither%20depend%20on%20external%20text%20prompts%20or%20embed%20hand-crafted%20architectural%0Apriors%20%28e.g.%2C%20frequency%20heuristics%29%3B%20both%20impose%20discrete%2C%20brittle%20assumptions%0Athat%20weaken%20generalization%20to%20unseen%20or%20mixed%20degradations.%20To%20address%20this%0Alimitation%2C%20we%20propose%20to%20reframe%20AIR%20as%20learned%20latent%20prior%20inference%2C%20where%0Adegradation-aware%20representations%20are%20automatically%20inferred%20from%20the%20input%0Awithout%20explicit%20task%20cues.%20Based%20on%20latent%20priors%2C%20we%20formulate%20AIR%20as%20a%0Astructured%20reasoning%20paradigm%3A%20%281%29%20which%20features%20to%20route%20%28adaptive%20feature%0Aselection%29%2C%20%282%29%20where%20to%20restore%20%28spatial%20localization%29%2C%20and%20%283%29%20what%20to%0Arestore%20%28degradation%20semantics%29.%20We%20design%20a%20lightweight%20decoding%20module%20that%0Aefficiently%20leverages%20these%20latent%20encoded%20cues%20for%20spatially-adaptive%0Arestoration.%20Extensive%20experiments%20across%20six%20common%20degradation%20tasks%2C%20five%0Acompound%20settings%2C%20and%20previously%20unseen%20degradations%20demonstrate%20that%20our%0Amethod%20outperforms%20state-of-the-art%20%28SOTA%29%20approaches%2C%20achieving%20an%20average%0APSNR%20improvement%20of%201.68%20dB%20while%20being%20three%20times%20more%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.17792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDegradation-Aware%2520All-in-One%2520Image%2520Restoration%2520via%2520Latent%2520Prior%2520Encoding%26entry.906535625%3DS%2520M%2520A%2520Sharif%2520and%2520Abdur%2520Rehman%2520and%2520Fayaz%2520Ali%2520Dharejo%2520and%2520Radu%2520Timofte%2520and%2520Rizwan%2520Ali%2520Naqvi%26entry.1292438233%3D%2520%2520Real-world%2520images%2520often%2520suffer%2520from%2520spatially%2520diverse%2520degradations%2520such%2520as%250Ahaze%252C%2520rain%252C%2520snow%252C%2520and%2520low-light%252C%2520significantly%2520impacting%2520visual%2520quality%2520and%250Adownstream%2520vision%2520tasks.%2520Existing%2520all-in-one%2520restoration%2520%2528AIR%2529%2520approaches%250Aeither%2520depend%2520on%2520external%2520text%2520prompts%2520or%2520embed%2520hand-crafted%2520architectural%250Apriors%2520%2528e.g.%252C%2520frequency%2520heuristics%2529%253B%2520both%2520impose%2520discrete%252C%2520brittle%2520assumptions%250Athat%2520weaken%2520generalization%2520to%2520unseen%2520or%2520mixed%2520degradations.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520to%2520reframe%2520AIR%2520as%2520learned%2520latent%2520prior%2520inference%252C%2520where%250Adegradation-aware%2520representations%2520are%2520automatically%2520inferred%2520from%2520the%2520input%250Awithout%2520explicit%2520task%2520cues.%2520Based%2520on%2520latent%2520priors%252C%2520we%2520formulate%2520AIR%2520as%2520a%250Astructured%2520reasoning%2520paradigm%253A%2520%25281%2529%2520which%2520features%2520to%2520route%2520%2528adaptive%2520feature%250Aselection%2529%252C%2520%25282%2529%2520where%2520to%2520restore%2520%2528spatial%2520localization%2529%252C%2520and%2520%25283%2529%2520what%2520to%250Arestore%2520%2528degradation%2520semantics%2529.%2520We%2520design%2520a%2520lightweight%2520decoding%2520module%2520that%250Aefficiently%2520leverages%2520these%2520latent%2520encoded%2520cues%2520for%2520spatially-adaptive%250Arestoration.%2520Extensive%2520experiments%2520across%2520six%2520common%2520degradation%2520tasks%252C%2520five%250Acompound%2520settings%252C%2520and%2520previously%2520unseen%2520degradations%2520demonstrate%2520that%2520our%250Amethod%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520approaches%252C%2520achieving%2520an%2520average%250APSNR%2520improvement%2520of%25201.68%2520dB%2520while%2520being%2520three%2520times%2520more%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Degradation-Aware%20All-in-One%20Image%20Restoration%20via%20Latent%20Prior%20Encoding&entry.906535625=S%20M%20A%20Sharif%20and%20Abdur%20Rehman%20and%20Fayaz%20Ali%20Dharejo%20and%20Radu%20Timofte%20and%20Rizwan%20Ali%20Naqvi&entry.1292438233=%20%20Real-world%20images%20often%20suffer%20from%20spatially%20diverse%20degradations%20such%20as%0Ahaze%2C%20rain%2C%20snow%2C%20and%20low-light%2C%20significantly%20impacting%20visual%20quality%20and%0Adownstream%20vision%20tasks.%20Existing%20all-in-one%20restoration%20%28AIR%29%20approaches%0Aeither%20depend%20on%20external%20text%20prompts%20or%20embed%20hand-crafted%20architectural%0Apriors%20%28e.g.%2C%20frequency%20heuristics%29%3B%20both%20impose%20discrete%2C%20brittle%20assumptions%0Athat%20weaken%20generalization%20to%20unseen%20or%20mixed%20degradations.%20To%20address%20this%0Alimitation%2C%20we%20propose%20to%20reframe%20AIR%20as%20learned%20latent%20prior%20inference%2C%20where%0Adegradation-aware%20representations%20are%20automatically%20inferred%20from%20the%20input%0Awithout%20explicit%20task%20cues.%20Based%20on%20latent%20priors%2C%20we%20formulate%20AIR%20as%20a%0Astructured%20reasoning%20paradigm%3A%20%281%29%20which%20features%20to%20route%20%28adaptive%20feature%0Aselection%29%2C%20%282%29%20where%20to%20restore%20%28spatial%20localization%29%2C%20and%20%283%29%20what%20to%0Arestore%20%28degradation%20semantics%29.%20We%20design%20a%20lightweight%20decoding%20module%20that%0Aefficiently%20leverages%20these%20latent%20encoded%20cues%20for%20spatially-adaptive%0Arestoration.%20Extensive%20experiments%20across%20six%20common%20degradation%20tasks%2C%20five%0Acompound%20settings%2C%20and%20previously%20unseen%20degradations%20demonstrate%20that%20our%0Amethod%20outperforms%20state-of-the-art%20%28SOTA%29%20approaches%2C%20achieving%20an%20average%0APSNR%20improvement%20of%201.68%20dB%20while%20being%20three%20times%20more%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.17792v2&entry.124074799=Read"},
{"title": "Jailbreaking on Text-to-Video Models via Scene Splitting Strategy", "author": "Wonjun Lee and Haon Park and Doehyeon Lee and Bumsub Ham and Suhyun Kim", "abstract": "  Along with the rapid advancement of numerous Text-to-Video (T2V) models,\ngrowing concerns have emerged regarding their safety risks. While recent\nstudies have explored vulnerabilities in models like LLMs, VLMs, and\nText-to-Image (T2I) models through jailbreak attacks, T2V models remain largely\nunexplored, leaving a significant safety gap. To address this gap, we introduce\nSceneSplit, a novel black-box jailbreak method that works by fragmenting a\nharmful narrative into multiple scenes, each individually benign. This approach\nmanipulates the generative output space, the abstract set of all potential\nvideo outputs for a given prompt, using the combination of scenes as a powerful\nconstraint to guide the final outcome. While each scene individually\ncorresponds to a wide and safe space where most outcomes are benign, their\nsequential combination collectively restricts this space, narrowing it to an\nunsafe region and significantly increasing the likelihood of generating a\nharmful video. This core mechanism is further enhanced through iterative scene\nmanipulation, which bypasses the safety filter within this constrained unsafe\nregion. Additionally, a strategy library that reuses successful attack patterns\nfurther improves the attack's overall effectiveness and robustness. To validate\nour method, we evaluate SceneSplit across 11 safety categories on T2V models.\nOur results show that it achieves a high average Attack Success Rate (ASR) of\n77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly\noutperforming the existing baseline. Through this work, we demonstrate that\ncurrent T2V safety mechanisms are vulnerable to attacks that exploit narrative\nstructure, providing new insights for understanding and improving the safety of\nT2V models.\n", "link": "http://arxiv.org/abs/2509.22292v1", "date": "2025-09-26", "relevancy": 2.2462, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5913}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jailbreaking%20on%20Text-to-Video%20Models%20via%20Scene%20Splitting%20Strategy&body=Title%3A%20Jailbreaking%20on%20Text-to-Video%20Models%20via%20Scene%20Splitting%20Strategy%0AAuthor%3A%20Wonjun%20Lee%20and%20Haon%20Park%20and%20Doehyeon%20Lee%20and%20Bumsub%20Ham%20and%20Suhyun%20Kim%0AAbstract%3A%20%20%20Along%20with%20the%20rapid%20advancement%20of%20numerous%20Text-to-Video%20%28T2V%29%20models%2C%0Agrowing%20concerns%20have%20emerged%20regarding%20their%20safety%20risks.%20While%20recent%0Astudies%20have%20explored%20vulnerabilities%20in%20models%20like%20LLMs%2C%20VLMs%2C%20and%0AText-to-Image%20%28T2I%29%20models%20through%20jailbreak%20attacks%2C%20T2V%20models%20remain%20largely%0Aunexplored%2C%20leaving%20a%20significant%20safety%20gap.%20To%20address%20this%20gap%2C%20we%20introduce%0ASceneSplit%2C%20a%20novel%20black-box%20jailbreak%20method%20that%20works%20by%20fragmenting%20a%0Aharmful%20narrative%20into%20multiple%20scenes%2C%20each%20individually%20benign.%20This%20approach%0Amanipulates%20the%20generative%20output%20space%2C%20the%20abstract%20set%20of%20all%20potential%0Avideo%20outputs%20for%20a%20given%20prompt%2C%20using%20the%20combination%20of%20scenes%20as%20a%20powerful%0Aconstraint%20to%20guide%20the%20final%20outcome.%20While%20each%20scene%20individually%0Acorresponds%20to%20a%20wide%20and%20safe%20space%20where%20most%20outcomes%20are%20benign%2C%20their%0Asequential%20combination%20collectively%20restricts%20this%20space%2C%20narrowing%20it%20to%20an%0Aunsafe%20region%20and%20significantly%20increasing%20the%20likelihood%20of%20generating%20a%0Aharmful%20video.%20This%20core%20mechanism%20is%20further%20enhanced%20through%20iterative%20scene%0Amanipulation%2C%20which%20bypasses%20the%20safety%20filter%20within%20this%20constrained%20unsafe%0Aregion.%20Additionally%2C%20a%20strategy%20library%20that%20reuses%20successful%20attack%20patterns%0Afurther%20improves%20the%20attack%27s%20overall%20effectiveness%20and%20robustness.%20To%20validate%0Aour%20method%2C%20we%20evaluate%20SceneSplit%20across%2011%20safety%20categories%20on%20T2V%20models.%0AOur%20results%20show%20that%20it%20achieves%20a%20high%20average%20Attack%20Success%20Rate%20%28ASR%29%20of%0A77.2%25%20on%20Luma%20Ray2%2C%2084.1%25%20on%20Hailuo%2C%20and%2078.2%25%20on%20Veo2%2C%20significantly%0Aoutperforming%20the%20existing%20baseline.%20Through%20this%20work%2C%20we%20demonstrate%20that%0Acurrent%20T2V%20safety%20mechanisms%20are%20vulnerable%20to%20attacks%20that%20exploit%20narrative%0Astructure%2C%20providing%20new%20insights%20for%20understanding%20and%20improving%20the%20safety%20of%0AT2V%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJailbreaking%2520on%2520Text-to-Video%2520Models%2520via%2520Scene%2520Splitting%2520Strategy%26entry.906535625%3DWonjun%2520Lee%2520and%2520Haon%2520Park%2520and%2520Doehyeon%2520Lee%2520and%2520Bumsub%2520Ham%2520and%2520Suhyun%2520Kim%26entry.1292438233%3D%2520%2520Along%2520with%2520the%2520rapid%2520advancement%2520of%2520numerous%2520Text-to-Video%2520%2528T2V%2529%2520models%252C%250Agrowing%2520concerns%2520have%2520emerged%2520regarding%2520their%2520safety%2520risks.%2520While%2520recent%250Astudies%2520have%2520explored%2520vulnerabilities%2520in%2520models%2520like%2520LLMs%252C%2520VLMs%252C%2520and%250AText-to-Image%2520%2528T2I%2529%2520models%2520through%2520jailbreak%2520attacks%252C%2520T2V%2520models%2520remain%2520largely%250Aunexplored%252C%2520leaving%2520a%2520significant%2520safety%2520gap.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ASceneSplit%252C%2520a%2520novel%2520black-box%2520jailbreak%2520method%2520that%2520works%2520by%2520fragmenting%2520a%250Aharmful%2520narrative%2520into%2520multiple%2520scenes%252C%2520each%2520individually%2520benign.%2520This%2520approach%250Amanipulates%2520the%2520generative%2520output%2520space%252C%2520the%2520abstract%2520set%2520of%2520all%2520potential%250Avideo%2520outputs%2520for%2520a%2520given%2520prompt%252C%2520using%2520the%2520combination%2520of%2520scenes%2520as%2520a%2520powerful%250Aconstraint%2520to%2520guide%2520the%2520final%2520outcome.%2520While%2520each%2520scene%2520individually%250Acorresponds%2520to%2520a%2520wide%2520and%2520safe%2520space%2520where%2520most%2520outcomes%2520are%2520benign%252C%2520their%250Asequential%2520combination%2520collectively%2520restricts%2520this%2520space%252C%2520narrowing%2520it%2520to%2520an%250Aunsafe%2520region%2520and%2520significantly%2520increasing%2520the%2520likelihood%2520of%2520generating%2520a%250Aharmful%2520video.%2520This%2520core%2520mechanism%2520is%2520further%2520enhanced%2520through%2520iterative%2520scene%250Amanipulation%252C%2520which%2520bypasses%2520the%2520safety%2520filter%2520within%2520this%2520constrained%2520unsafe%250Aregion.%2520Additionally%252C%2520a%2520strategy%2520library%2520that%2520reuses%2520successful%2520attack%2520patterns%250Afurther%2520improves%2520the%2520attack%2527s%2520overall%2520effectiveness%2520and%2520robustness.%2520To%2520validate%250Aour%2520method%252C%2520we%2520evaluate%2520SceneSplit%2520across%252011%2520safety%2520categories%2520on%2520T2V%2520models.%250AOur%2520results%2520show%2520that%2520it%2520achieves%2520a%2520high%2520average%2520Attack%2520Success%2520Rate%2520%2528ASR%2529%2520of%250A77.2%2525%2520on%2520Luma%2520Ray2%252C%252084.1%2525%2520on%2520Hailuo%252C%2520and%252078.2%2525%2520on%2520Veo2%252C%2520significantly%250Aoutperforming%2520the%2520existing%2520baseline.%2520Through%2520this%2520work%252C%2520we%2520demonstrate%2520that%250Acurrent%2520T2V%2520safety%2520mechanisms%2520are%2520vulnerable%2520to%2520attacks%2520that%2520exploit%2520narrative%250Astructure%252C%2520providing%2520new%2520insights%2520for%2520understanding%2520and%2520improving%2520the%2520safety%2520of%250AT2V%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jailbreaking%20on%20Text-to-Video%20Models%20via%20Scene%20Splitting%20Strategy&entry.906535625=Wonjun%20Lee%20and%20Haon%20Park%20and%20Doehyeon%20Lee%20and%20Bumsub%20Ham%20and%20Suhyun%20Kim&entry.1292438233=%20%20Along%20with%20the%20rapid%20advancement%20of%20numerous%20Text-to-Video%20%28T2V%29%20models%2C%0Agrowing%20concerns%20have%20emerged%20regarding%20their%20safety%20risks.%20While%20recent%0Astudies%20have%20explored%20vulnerabilities%20in%20models%20like%20LLMs%2C%20VLMs%2C%20and%0AText-to-Image%20%28T2I%29%20models%20through%20jailbreak%20attacks%2C%20T2V%20models%20remain%20largely%0Aunexplored%2C%20leaving%20a%20significant%20safety%20gap.%20To%20address%20this%20gap%2C%20we%20introduce%0ASceneSplit%2C%20a%20novel%20black-box%20jailbreak%20method%20that%20works%20by%20fragmenting%20a%0Aharmful%20narrative%20into%20multiple%20scenes%2C%20each%20individually%20benign.%20This%20approach%0Amanipulates%20the%20generative%20output%20space%2C%20the%20abstract%20set%20of%20all%20potential%0Avideo%20outputs%20for%20a%20given%20prompt%2C%20using%20the%20combination%20of%20scenes%20as%20a%20powerful%0Aconstraint%20to%20guide%20the%20final%20outcome.%20While%20each%20scene%20individually%0Acorresponds%20to%20a%20wide%20and%20safe%20space%20where%20most%20outcomes%20are%20benign%2C%20their%0Asequential%20combination%20collectively%20restricts%20this%20space%2C%20narrowing%20it%20to%20an%0Aunsafe%20region%20and%20significantly%20increasing%20the%20likelihood%20of%20generating%20a%0Aharmful%20video.%20This%20core%20mechanism%20is%20further%20enhanced%20through%20iterative%20scene%0Amanipulation%2C%20which%20bypasses%20the%20safety%20filter%20within%20this%20constrained%20unsafe%0Aregion.%20Additionally%2C%20a%20strategy%20library%20that%20reuses%20successful%20attack%20patterns%0Afurther%20improves%20the%20attack%27s%20overall%20effectiveness%20and%20robustness.%20To%20validate%0Aour%20method%2C%20we%20evaluate%20SceneSplit%20across%2011%20safety%20categories%20on%20T2V%20models.%0AOur%20results%20show%20that%20it%20achieves%20a%20high%20average%20Attack%20Success%20Rate%20%28ASR%29%20of%0A77.2%25%20on%20Luma%20Ray2%2C%2084.1%25%20on%20Hailuo%2C%20and%2078.2%25%20on%20Veo2%2C%20significantly%0Aoutperforming%20the%20existing%20baseline.%20Through%20this%20work%2C%20we%20demonstrate%20that%0Acurrent%20T2V%20safety%20mechanisms%20are%20vulnerable%20to%20attacks%20that%20exploit%20narrative%0Astructure%2C%20providing%20new%20insights%20for%20understanding%20and%20improving%20the%20safety%20of%0AT2V%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22292v1&entry.124074799=Read"},
{"title": "InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with\n  Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning", "author": "Guanghao Zhu and Zhitian Hou and Zeyu Liu and Zhijie Sang and Congkai Xie and Hongxia Yang", "abstract": "  Multimodal large language models (MLLMs) have shown remarkable potential in\nvarious domains, yet their application in the medical field is hindered by\nseveral challenges. General-purpose MLLMs often lack the specialized knowledge\nrequired for medical tasks, leading to uncertain or hallucinatory responses.\nKnowledge distillation from advanced models struggles to capture\ndomain-specific expertise in radiology and pharmacology. Additionally, the\ncomputational cost of continual pretraining with large-scale medical data poses\nsignificant efficiency challenges. To address these issues, we propose\nInfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs\ndesigned to deliver state-of-the-art performance in medical applications. We\ncombined high-quality general-purpose and medical multimodal data and proposed\na novel five-dimensional quality assessment framework to curate high-quality\nmultimodal medical datasets. We employ low-to-high image resolution and\nmultimodal sequence packing to enhance training efficiency, enabling the\nintegration of extensive medical data. Furthermore, a three-stage supervised\nfine-tuning process ensures effective knowledge extraction for complex medical\ntasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B\noutperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B\nand MedGemma-27B-IT, demonstrating superior performance in medical visual\nquestion answering and diagnostic tasks. By addressing key challenges in data\nquality, training efficiency, and domain-specific knowledge extraction, our\nwork paves the way for more reliable and effective AI-driven solutions in\nhealthcare. InfiMed-Foundation-4B model is available at\n\\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.\n", "link": "http://arxiv.org/abs/2509.22261v1", "date": "2025-09-26", "relevancy": 2.2403, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiMed-Foundation%3A%20Pioneering%20Advanced%20Multimodal%20Medical%20Models%20with%0A%20%20Compute-Efficient%20Pre-Training%20and%20Multi-Stage%20Fine-Tuning&body=Title%3A%20InfiMed-Foundation%3A%20Pioneering%20Advanced%20Multimodal%20Medical%20Models%20with%0A%20%20Compute-Efficient%20Pre-Training%20and%20Multi-Stage%20Fine-Tuning%0AAuthor%3A%20Guanghao%20Zhu%20and%20Zhitian%20Hou%20and%20Zeyu%20Liu%20and%20Zhijie%20Sang%20and%20Congkai%20Xie%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20remarkable%20potential%20in%0Avarious%20domains%2C%20yet%20their%20application%20in%20the%20medical%20field%20is%20hindered%20by%0Aseveral%20challenges.%20General-purpose%20MLLMs%20often%20lack%20the%20specialized%20knowledge%0Arequired%20for%20medical%20tasks%2C%20leading%20to%20uncertain%20or%20hallucinatory%20responses.%0AKnowledge%20distillation%20from%20advanced%20models%20struggles%20to%20capture%0Adomain-specific%20expertise%20in%20radiology%20and%20pharmacology.%20Additionally%2C%20the%0Acomputational%20cost%20of%20continual%20pretraining%20with%20large-scale%20medical%20data%20poses%0Asignificant%20efficiency%20challenges.%20To%20address%20these%20issues%2C%20we%20propose%0AInfiMed-Foundation-1.7B%20and%20InfiMed-Foundation-4B%2C%20two%20medical-specific%20MLLMs%0Adesigned%20to%20deliver%20state-of-the-art%20performance%20in%20medical%20applications.%20We%0Acombined%20high-quality%20general-purpose%20and%20medical%20multimodal%20data%20and%20proposed%0Aa%20novel%20five-dimensional%20quality%20assessment%20framework%20to%20curate%20high-quality%0Amultimodal%20medical%20datasets.%20We%20employ%20low-to-high%20image%20resolution%20and%0Amultimodal%20sequence%20packing%20to%20enhance%20training%20efficiency%2C%20enabling%20the%0Aintegration%20of%20extensive%20medical%20data.%20Furthermore%2C%20a%20three-stage%20supervised%0Afine-tuning%20process%20ensures%20effective%20knowledge%20extraction%20for%20complex%20medical%0Atasks.%20Evaluated%20on%20the%20MedEvalKit%20framework%2C%20InfiMed-Foundation-1.7B%0Aoutperforms%20Qwen2.5VL-3B%2C%20while%20InfiMed-Foundation-4B%20surpasses%20HuatuoGPT-V-7B%0Aand%20MedGemma-27B-IT%2C%20demonstrating%20superior%20performance%20in%20medical%20visual%0Aquestion%20answering%20and%20diagnostic%20tasks.%20By%20addressing%20key%20challenges%20in%20data%0Aquality%2C%20training%20efficiency%2C%20and%20domain-specific%20knowledge%20extraction%2C%20our%0Awork%20paves%20the%20way%20for%20more%20reliable%20and%20effective%20AI-driven%20solutions%20in%0Ahealthcare.%20InfiMed-Foundation-4B%20model%20is%20available%20at%0A%5Chref%7Bhttps%3A//huggingface.co/InfiX-ai/InfiMed-Foundation-4B%7D%7BInfiMed-Foundation-4B%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22261v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiMed-Foundation%253A%2520Pioneering%2520Advanced%2520Multimodal%2520Medical%2520Models%2520with%250A%2520%2520Compute-Efficient%2520Pre-Training%2520and%2520Multi-Stage%2520Fine-Tuning%26entry.906535625%3DGuanghao%2520Zhu%2520and%2520Zhitian%2520Hou%2520and%2520Zeyu%2520Liu%2520and%2520Zhijie%2520Sang%2520and%2520Congkai%2520Xie%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520remarkable%2520potential%2520in%250Avarious%2520domains%252C%2520yet%2520their%2520application%2520in%2520the%2520medical%2520field%2520is%2520hindered%2520by%250Aseveral%2520challenges.%2520General-purpose%2520MLLMs%2520often%2520lack%2520the%2520specialized%2520knowledge%250Arequired%2520for%2520medical%2520tasks%252C%2520leading%2520to%2520uncertain%2520or%2520hallucinatory%2520responses.%250AKnowledge%2520distillation%2520from%2520advanced%2520models%2520struggles%2520to%2520capture%250Adomain-specific%2520expertise%2520in%2520radiology%2520and%2520pharmacology.%2520Additionally%252C%2520the%250Acomputational%2520cost%2520of%2520continual%2520pretraining%2520with%2520large-scale%2520medical%2520data%2520poses%250Asignificant%2520efficiency%2520challenges.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%250AInfiMed-Foundation-1.7B%2520and%2520InfiMed-Foundation-4B%252C%2520two%2520medical-specific%2520MLLMs%250Adesigned%2520to%2520deliver%2520state-of-the-art%2520performance%2520in%2520medical%2520applications.%2520We%250Acombined%2520high-quality%2520general-purpose%2520and%2520medical%2520multimodal%2520data%2520and%2520proposed%250Aa%2520novel%2520five-dimensional%2520quality%2520assessment%2520framework%2520to%2520curate%2520high-quality%250Amultimodal%2520medical%2520datasets.%2520We%2520employ%2520low-to-high%2520image%2520resolution%2520and%250Amultimodal%2520sequence%2520packing%2520to%2520enhance%2520training%2520efficiency%252C%2520enabling%2520the%250Aintegration%2520of%2520extensive%2520medical%2520data.%2520Furthermore%252C%2520a%2520three-stage%2520supervised%250Afine-tuning%2520process%2520ensures%2520effective%2520knowledge%2520extraction%2520for%2520complex%2520medical%250Atasks.%2520Evaluated%2520on%2520the%2520MedEvalKit%2520framework%252C%2520InfiMed-Foundation-1.7B%250Aoutperforms%2520Qwen2.5VL-3B%252C%2520while%2520InfiMed-Foundation-4B%2520surpasses%2520HuatuoGPT-V-7B%250Aand%2520MedGemma-27B-IT%252C%2520demonstrating%2520superior%2520performance%2520in%2520medical%2520visual%250Aquestion%2520answering%2520and%2520diagnostic%2520tasks.%2520By%2520addressing%2520key%2520challenges%2520in%2520data%250Aquality%252C%2520training%2520efficiency%252C%2520and%2520domain-specific%2520knowledge%2520extraction%252C%2520our%250Awork%2520paves%2520the%2520way%2520for%2520more%2520reliable%2520and%2520effective%2520AI-driven%2520solutions%2520in%250Ahealthcare.%2520InfiMed-Foundation-4B%2520model%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//huggingface.co/InfiX-ai/InfiMed-Foundation-4B%257D%257BInfiMed-Foundation-4B%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22261v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiMed-Foundation%3A%20Pioneering%20Advanced%20Multimodal%20Medical%20Models%20with%0A%20%20Compute-Efficient%20Pre-Training%20and%20Multi-Stage%20Fine-Tuning&entry.906535625=Guanghao%20Zhu%20and%20Zhitian%20Hou%20and%20Zeyu%20Liu%20and%20Zhijie%20Sang%20and%20Congkai%20Xie%20and%20Hongxia%20Yang&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%20remarkable%20potential%20in%0Avarious%20domains%2C%20yet%20their%20application%20in%20the%20medical%20field%20is%20hindered%20by%0Aseveral%20challenges.%20General-purpose%20MLLMs%20often%20lack%20the%20specialized%20knowledge%0Arequired%20for%20medical%20tasks%2C%20leading%20to%20uncertain%20or%20hallucinatory%20responses.%0AKnowledge%20distillation%20from%20advanced%20models%20struggles%20to%20capture%0Adomain-specific%20expertise%20in%20radiology%20and%20pharmacology.%20Additionally%2C%20the%0Acomputational%20cost%20of%20continual%20pretraining%20with%20large-scale%20medical%20data%20poses%0Asignificant%20efficiency%20challenges.%20To%20address%20these%20issues%2C%20we%20propose%0AInfiMed-Foundation-1.7B%20and%20InfiMed-Foundation-4B%2C%20two%20medical-specific%20MLLMs%0Adesigned%20to%20deliver%20state-of-the-art%20performance%20in%20medical%20applications.%20We%0Acombined%20high-quality%20general-purpose%20and%20medical%20multimodal%20data%20and%20proposed%0Aa%20novel%20five-dimensional%20quality%20assessment%20framework%20to%20curate%20high-quality%0Amultimodal%20medical%20datasets.%20We%20employ%20low-to-high%20image%20resolution%20and%0Amultimodal%20sequence%20packing%20to%20enhance%20training%20efficiency%2C%20enabling%20the%0Aintegration%20of%20extensive%20medical%20data.%20Furthermore%2C%20a%20three-stage%20supervised%0Afine-tuning%20process%20ensures%20effective%20knowledge%20extraction%20for%20complex%20medical%0Atasks.%20Evaluated%20on%20the%20MedEvalKit%20framework%2C%20InfiMed-Foundation-1.7B%0Aoutperforms%20Qwen2.5VL-3B%2C%20while%20InfiMed-Foundation-4B%20surpasses%20HuatuoGPT-V-7B%0Aand%20MedGemma-27B-IT%2C%20demonstrating%20superior%20performance%20in%20medical%20visual%0Aquestion%20answering%20and%20diagnostic%20tasks.%20By%20addressing%20key%20challenges%20in%20data%0Aquality%2C%20training%20efficiency%2C%20and%20domain-specific%20knowledge%20extraction%2C%20our%0Awork%20paves%20the%20way%20for%20more%20reliable%20and%20effective%20AI-driven%20solutions%20in%0Ahealthcare.%20InfiMed-Foundation-4B%20model%20is%20available%20at%0A%5Chref%7Bhttps%3A//huggingface.co/InfiX-ai/InfiMed-Foundation-4B%7D%7BInfiMed-Foundation-4B%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22261v1&entry.124074799=Read"},
{"title": "CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach", "author": "Alexandre Lopes and Roberto Souza and Helio Pedrini", "abstract": "  Depth Estimation plays a crucial role in recent applications in robotics,\nautonomous vehicles, and augmented reality. These scenarios commonly operate\nunder constraints imposed by computational power. Stereo image pairs offer an\neffective solution for depth estimation since it only needs to estimate the\ndisparity of pixels in image pairs to determine the depth in a known rectified\nsystem. Due to the difficulty in acquiring reliable ground-truth depth data\nacross diverse scenarios, self-supervised techniques emerge as a solution,\nparticularly when large unlabeled datasets are available. We propose a novel\nself-supervised convolutional approach that outperforms existing\nstate-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers\n(ViTs) while balancing computational cost. The proposed CCNeXt architecture\nemploys a modern CNN feature extractor with a novel windowed epipolar\ncross-attention module in the encoder, complemented by a comprehensive redesign\nof the depth estimation decoder. Our experiments demonstrate that CCNeXt\nachieves competitive metrics on the KITTI Eigen Split test data while being\n10.18$\\times$ faster than the current best model and achieves state-of-the-art\nresults in all metrics in the KITTI Eigen Split Improved Ground Truth and\nDriving Stereo datasets when compared to recently proposed techniques. To\nensure complete reproducibility, our project is accessible at\n\\href{https://github.com/alelopes/CCNext}{\\texttt{https://github.com/alelopes/CCNext}}.\n", "link": "http://arxiv.org/abs/2509.22627v1", "date": "2025-09-26", "relevancy": 2.2335, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.598}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CCNeXt%3A%20An%20Effective%20Self-Supervised%20Stereo%20Depth%20Estimation%20Approach&body=Title%3A%20CCNeXt%3A%20An%20Effective%20Self-Supervised%20Stereo%20Depth%20Estimation%20Approach%0AAuthor%3A%20Alexandre%20Lopes%20and%20Roberto%20Souza%20and%20Helio%20Pedrini%0AAbstract%3A%20%20%20Depth%20Estimation%20plays%20a%20crucial%20role%20in%20recent%20applications%20in%20robotics%2C%0Aautonomous%20vehicles%2C%20and%20augmented%20reality.%20These%20scenarios%20commonly%20operate%0Aunder%20constraints%20imposed%20by%20computational%20power.%20Stereo%20image%20pairs%20offer%20an%0Aeffective%20solution%20for%20depth%20estimation%20since%20it%20only%20needs%20to%20estimate%20the%0Adisparity%20of%20pixels%20in%20image%20pairs%20to%20determine%20the%20depth%20in%20a%20known%20rectified%0Asystem.%20Due%20to%20the%20difficulty%20in%20acquiring%20reliable%20ground-truth%20depth%20data%0Aacross%20diverse%20scenarios%2C%20self-supervised%20techniques%20emerge%20as%20a%20solution%2C%0Aparticularly%20when%20large%20unlabeled%20datasets%20are%20available.%20We%20propose%20a%20novel%0Aself-supervised%20convolutional%20approach%20that%20outperforms%20existing%0Astate-of-the-art%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%0A%28ViTs%29%20while%20balancing%20computational%20cost.%20The%20proposed%20CCNeXt%20architecture%0Aemploys%20a%20modern%20CNN%20feature%20extractor%20with%20a%20novel%20windowed%20epipolar%0Across-attention%20module%20in%20the%20encoder%2C%20complemented%20by%20a%20comprehensive%20redesign%0Aof%20the%20depth%20estimation%20decoder.%20Our%20experiments%20demonstrate%20that%20CCNeXt%0Aachieves%20competitive%20metrics%20on%20the%20KITTI%20Eigen%20Split%20test%20data%20while%20being%0A10.18%24%5Ctimes%24%20faster%20than%20the%20current%20best%20model%20and%20achieves%20state-of-the-art%0Aresults%20in%20all%20metrics%20in%20the%20KITTI%20Eigen%20Split%20Improved%20Ground%20Truth%20and%0ADriving%20Stereo%20datasets%20when%20compared%20to%20recently%20proposed%20techniques.%20To%0Aensure%20complete%20reproducibility%2C%20our%20project%20is%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/alelopes/CCNext%7D%7B%5Ctexttt%7Bhttps%3A//github.com/alelopes/CCNext%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCCNeXt%253A%2520An%2520Effective%2520Self-Supervised%2520Stereo%2520Depth%2520Estimation%2520Approach%26entry.906535625%3DAlexandre%2520Lopes%2520and%2520Roberto%2520Souza%2520and%2520Helio%2520Pedrini%26entry.1292438233%3D%2520%2520Depth%2520Estimation%2520plays%2520a%2520crucial%2520role%2520in%2520recent%2520applications%2520in%2520robotics%252C%250Aautonomous%2520vehicles%252C%2520and%2520augmented%2520reality.%2520These%2520scenarios%2520commonly%2520operate%250Aunder%2520constraints%2520imposed%2520by%2520computational%2520power.%2520Stereo%2520image%2520pairs%2520offer%2520an%250Aeffective%2520solution%2520for%2520depth%2520estimation%2520since%2520it%2520only%2520needs%2520to%2520estimate%2520the%250Adisparity%2520of%2520pixels%2520in%2520image%2520pairs%2520to%2520determine%2520the%2520depth%2520in%2520a%2520known%2520rectified%250Asystem.%2520Due%2520to%2520the%2520difficulty%2520in%2520acquiring%2520reliable%2520ground-truth%2520depth%2520data%250Aacross%2520diverse%2520scenarios%252C%2520self-supervised%2520techniques%2520emerge%2520as%2520a%2520solution%252C%250Aparticularly%2520when%2520large%2520unlabeled%2520datasets%2520are%2520available.%2520We%2520propose%2520a%2520novel%250Aself-supervised%2520convolutional%2520approach%2520that%2520outperforms%2520existing%250Astate-of-the-art%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Vision%2520Transformers%250A%2528ViTs%2529%2520while%2520balancing%2520computational%2520cost.%2520The%2520proposed%2520CCNeXt%2520architecture%250Aemploys%2520a%2520modern%2520CNN%2520feature%2520extractor%2520with%2520a%2520novel%2520windowed%2520epipolar%250Across-attention%2520module%2520in%2520the%2520encoder%252C%2520complemented%2520by%2520a%2520comprehensive%2520redesign%250Aof%2520the%2520depth%2520estimation%2520decoder.%2520Our%2520experiments%2520demonstrate%2520that%2520CCNeXt%250Aachieves%2520competitive%2520metrics%2520on%2520the%2520KITTI%2520Eigen%2520Split%2520test%2520data%2520while%2520being%250A10.18%2524%255Ctimes%2524%2520faster%2520than%2520the%2520current%2520best%2520model%2520and%2520achieves%2520state-of-the-art%250Aresults%2520in%2520all%2520metrics%2520in%2520the%2520KITTI%2520Eigen%2520Split%2520Improved%2520Ground%2520Truth%2520and%250ADriving%2520Stereo%2520datasets%2520when%2520compared%2520to%2520recently%2520proposed%2520techniques.%2520To%250Aensure%2520complete%2520reproducibility%252C%2520our%2520project%2520is%2520accessible%2520at%250A%255Chref%257Bhttps%253A//github.com/alelopes/CCNext%257D%257B%255Ctexttt%257Bhttps%253A//github.com/alelopes/CCNext%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CCNeXt%3A%20An%20Effective%20Self-Supervised%20Stereo%20Depth%20Estimation%20Approach&entry.906535625=Alexandre%20Lopes%20and%20Roberto%20Souza%20and%20Helio%20Pedrini&entry.1292438233=%20%20Depth%20Estimation%20plays%20a%20crucial%20role%20in%20recent%20applications%20in%20robotics%2C%0Aautonomous%20vehicles%2C%20and%20augmented%20reality.%20These%20scenarios%20commonly%20operate%0Aunder%20constraints%20imposed%20by%20computational%20power.%20Stereo%20image%20pairs%20offer%20an%0Aeffective%20solution%20for%20depth%20estimation%20since%20it%20only%20needs%20to%20estimate%20the%0Adisparity%20of%20pixels%20in%20image%20pairs%20to%20determine%20the%20depth%20in%20a%20known%20rectified%0Asystem.%20Due%20to%20the%20difficulty%20in%20acquiring%20reliable%20ground-truth%20depth%20data%0Aacross%20diverse%20scenarios%2C%20self-supervised%20techniques%20emerge%20as%20a%20solution%2C%0Aparticularly%20when%20large%20unlabeled%20datasets%20are%20available.%20We%20propose%20a%20novel%0Aself-supervised%20convolutional%20approach%20that%20outperforms%20existing%0Astate-of-the-art%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%0A%28ViTs%29%20while%20balancing%20computational%20cost.%20The%20proposed%20CCNeXt%20architecture%0Aemploys%20a%20modern%20CNN%20feature%20extractor%20with%20a%20novel%20windowed%20epipolar%0Across-attention%20module%20in%20the%20encoder%2C%20complemented%20by%20a%20comprehensive%20redesign%0Aof%20the%20depth%20estimation%20decoder.%20Our%20experiments%20demonstrate%20that%20CCNeXt%0Aachieves%20competitive%20metrics%20on%20the%20KITTI%20Eigen%20Split%20test%20data%20while%20being%0A10.18%24%5Ctimes%24%20faster%20than%20the%20current%20best%20model%20and%20achieves%20state-of-the-art%0Aresults%20in%20all%20metrics%20in%20the%20KITTI%20Eigen%20Split%20Improved%20Ground%20Truth%20and%0ADriving%20Stereo%20datasets%20when%20compared%20to%20recently%20proposed%20techniques.%20To%0Aensure%20complete%20reproducibility%2C%20our%20project%20is%20accessible%20at%0A%5Chref%7Bhttps%3A//github.com/alelopes/CCNext%7D%7B%5Ctexttt%7Bhttps%3A//github.com/alelopes/CCNext%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22627v1&entry.124074799=Read"},
{"title": "VizGen: Data Exploration and Visualization from Natural Language via a\n  Multi-Agent AI Architecture", "author": "Sandaru Fernando and Imasha Jayarathne and Sithumini Abeysekara and Shanuja Sithamparanthan and Thushari Silva and Deshan Jayawardana", "abstract": "  Data visualization is essential for interpreting complex datasets, yet\ntraditional tools often require technical expertise, limiting accessibility.\nVizGen is an AI-assisted graph generation system that empowers users to create\nmeaningful visualizations using natural language. Leveraging advanced NLP and\nLLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries\ninto SQL and recommends suitable graph types. Built on a multi-agent\narchitecture, VizGen handles SQL generation, graph creation, customization, and\ninsight extraction. Beyond visualization, it analyzes data for patterns,\nanomalies, and correlations, and enhances user understanding by providing\nexplanations enriched with contextual information gathered from the internet.\nThe system supports real-time interaction with SQL databases and allows\nconversational graph refinement, making data analysis intuitive and accessible.\nVizGen democratizes data visualization by bridging the gap between technical\ncomplexity and user-friendly design.\n", "link": "http://arxiv.org/abs/2509.22218v1", "date": "2025-09-26", "relevancy": 2.2312, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.584}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5441}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VizGen%3A%20Data%20Exploration%20and%20Visualization%20from%20Natural%20Language%20via%20a%0A%20%20Multi-Agent%20AI%20Architecture&body=Title%3A%20VizGen%3A%20Data%20Exploration%20and%20Visualization%20from%20Natural%20Language%20via%20a%0A%20%20Multi-Agent%20AI%20Architecture%0AAuthor%3A%20Sandaru%20Fernando%20and%20Imasha%20Jayarathne%20and%20Sithumini%20Abeysekara%20and%20Shanuja%20Sithamparanthan%20and%20Thushari%20Silva%20and%20Deshan%20Jayawardana%0AAbstract%3A%20%20%20Data%20visualization%20is%20essential%20for%20interpreting%20complex%20datasets%2C%20yet%0Atraditional%20tools%20often%20require%20technical%20expertise%2C%20limiting%20accessibility.%0AVizGen%20is%20an%20AI-assisted%20graph%20generation%20system%20that%20empowers%20users%20to%20create%0Ameaningful%20visualizations%20using%20natural%20language.%20Leveraging%20advanced%20NLP%20and%0ALLMs%20like%20Claude%203.7%20Sonnet%20and%20Gemini%202.0%20Flash%2C%20it%20translates%20user%20queries%0Ainto%20SQL%20and%20recommends%20suitable%20graph%20types.%20Built%20on%20a%20multi-agent%0Aarchitecture%2C%20VizGen%20handles%20SQL%20generation%2C%20graph%20creation%2C%20customization%2C%20and%0Ainsight%20extraction.%20Beyond%20visualization%2C%20it%20analyzes%20data%20for%20patterns%2C%0Aanomalies%2C%20and%20correlations%2C%20and%20enhances%20user%20understanding%20by%20providing%0Aexplanations%20enriched%20with%20contextual%20information%20gathered%20from%20the%20internet.%0AThe%20system%20supports%20real-time%20interaction%20with%20SQL%20databases%20and%20allows%0Aconversational%20graph%20refinement%2C%20making%20data%20analysis%20intuitive%20and%20accessible.%0AVizGen%20democratizes%20data%20visualization%20by%20bridging%20the%20gap%20between%20technical%0Acomplexity%20and%20user-friendly%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22218v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVizGen%253A%2520Data%2520Exploration%2520and%2520Visualization%2520from%2520Natural%2520Language%2520via%2520a%250A%2520%2520Multi-Agent%2520AI%2520Architecture%26entry.906535625%3DSandaru%2520Fernando%2520and%2520Imasha%2520Jayarathne%2520and%2520Sithumini%2520Abeysekara%2520and%2520Shanuja%2520Sithamparanthan%2520and%2520Thushari%2520Silva%2520and%2520Deshan%2520Jayawardana%26entry.1292438233%3D%2520%2520Data%2520visualization%2520is%2520essential%2520for%2520interpreting%2520complex%2520datasets%252C%2520yet%250Atraditional%2520tools%2520often%2520require%2520technical%2520expertise%252C%2520limiting%2520accessibility.%250AVizGen%2520is%2520an%2520AI-assisted%2520graph%2520generation%2520system%2520that%2520empowers%2520users%2520to%2520create%250Ameaningful%2520visualizations%2520using%2520natural%2520language.%2520Leveraging%2520advanced%2520NLP%2520and%250ALLMs%2520like%2520Claude%25203.7%2520Sonnet%2520and%2520Gemini%25202.0%2520Flash%252C%2520it%2520translates%2520user%2520queries%250Ainto%2520SQL%2520and%2520recommends%2520suitable%2520graph%2520types.%2520Built%2520on%2520a%2520multi-agent%250Aarchitecture%252C%2520VizGen%2520handles%2520SQL%2520generation%252C%2520graph%2520creation%252C%2520customization%252C%2520and%250Ainsight%2520extraction.%2520Beyond%2520visualization%252C%2520it%2520analyzes%2520data%2520for%2520patterns%252C%250Aanomalies%252C%2520and%2520correlations%252C%2520and%2520enhances%2520user%2520understanding%2520by%2520providing%250Aexplanations%2520enriched%2520with%2520contextual%2520information%2520gathered%2520from%2520the%2520internet.%250AThe%2520system%2520supports%2520real-time%2520interaction%2520with%2520SQL%2520databases%2520and%2520allows%250Aconversational%2520graph%2520refinement%252C%2520making%2520data%2520analysis%2520intuitive%2520and%2520accessible.%250AVizGen%2520democratizes%2520data%2520visualization%2520by%2520bridging%2520the%2520gap%2520between%2520technical%250Acomplexity%2520and%2520user-friendly%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22218v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VizGen%3A%20Data%20Exploration%20and%20Visualization%20from%20Natural%20Language%20via%20a%0A%20%20Multi-Agent%20AI%20Architecture&entry.906535625=Sandaru%20Fernando%20and%20Imasha%20Jayarathne%20and%20Sithumini%20Abeysekara%20and%20Shanuja%20Sithamparanthan%20and%20Thushari%20Silva%20and%20Deshan%20Jayawardana&entry.1292438233=%20%20Data%20visualization%20is%20essential%20for%20interpreting%20complex%20datasets%2C%20yet%0Atraditional%20tools%20often%20require%20technical%20expertise%2C%20limiting%20accessibility.%0AVizGen%20is%20an%20AI-assisted%20graph%20generation%20system%20that%20empowers%20users%20to%20create%0Ameaningful%20visualizations%20using%20natural%20language.%20Leveraging%20advanced%20NLP%20and%0ALLMs%20like%20Claude%203.7%20Sonnet%20and%20Gemini%202.0%20Flash%2C%20it%20translates%20user%20queries%0Ainto%20SQL%20and%20recommends%20suitable%20graph%20types.%20Built%20on%20a%20multi-agent%0Aarchitecture%2C%20VizGen%20handles%20SQL%20generation%2C%20graph%20creation%2C%20customization%2C%20and%0Ainsight%20extraction.%20Beyond%20visualization%2C%20it%20analyzes%20data%20for%20patterns%2C%0Aanomalies%2C%20and%20correlations%2C%20and%20enhances%20user%20understanding%20by%20providing%0Aexplanations%20enriched%20with%20contextual%20information%20gathered%20from%20the%20internet.%0AThe%20system%20supports%20real-time%20interaction%20with%20SQL%20databases%20and%20allows%0Aconversational%20graph%20refinement%2C%20making%20data%20analysis%20intuitive%20and%20accessible.%0AVizGen%20democratizes%20data%20visualization%20by%20bridging%20the%20gap%20between%20technical%0Acomplexity%20and%20user-friendly%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22218v1&entry.124074799=Read"},
{"title": "CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual\n  Comprehension and Symbolic Reasoning in Engineering Design Process", "author": "Arman Akbari and Jian Gao and Yifei Zou and Mei Yang and Jinru Duan and Dmitrii Torbunov and Yanzhi Wang and Yihui Ren and Xuan Zhang", "abstract": "  Engineering design operates through hierarchical abstraction from system\nspecifications to component implementations, requiring visual understanding\ncoupled with mathematical reasoning at each level. While Multi-modal Large\nLanguage Models (MLLMs) excel at natural image tasks, their ability to extract\nmathematical models from technical diagrams remains unexplored. We present\n\\textbf{CircuitSense}, a comprehensive benchmark evaluating circuit\nunderstanding across this hierarchy through 8,006+ problems spanning\ncomponent-level schematics to system-level block diagrams. Our benchmark\nuniquely examines the complete engineering workflow: Perception, Analysis, and\nDesign, with a particular emphasis on the critical but underexplored capability\nof deriving symbolic equations from visual inputs. We introduce a hierarchical\nsynthetic generation pipeline consisting of a grid-based schematic generator\nand a block diagram generator with auto-derived symbolic equation labels.\nComprehensive evaluation of six state-of-the-art MLLMs, including both\nclosed-source and open-source models, reveals fundamental limitations in\nvisual-to-mathematical reasoning. Closed-source models achieve over 85\\%\naccuracy on perception tasks involving component recognition and topology\nidentification, yet their performance on symbolic derivation and analytical\nreasoning falls below 19\\%, exposing a critical gap between visual parsing and\nsymbolic reasoning. Models with stronger symbolic reasoning capabilities\nconsistently achieve higher design task accuracy, confirming the fundamental\nrole of mathematical understanding in circuit synthesis and establishing\nsymbolic reasoning as the key metric for engineering competence.\n", "link": "http://arxiv.org/abs/2509.22339v1", "date": "2025-09-26", "relevancy": 2.2304, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CircuitSense%3A%20A%20Hierarchical%20Circuit%20System%20Benchmark%20Bridging%20Visual%0A%20%20Comprehension%20and%20Symbolic%20Reasoning%20in%20Engineering%20Design%20Process&body=Title%3A%20CircuitSense%3A%20A%20Hierarchical%20Circuit%20System%20Benchmark%20Bridging%20Visual%0A%20%20Comprehension%20and%20Symbolic%20Reasoning%20in%20Engineering%20Design%20Process%0AAuthor%3A%20Arman%20Akbari%20and%20Jian%20Gao%20and%20Yifei%20Zou%20and%20Mei%20Yang%20and%20Jinru%20Duan%20and%20Dmitrii%20Torbunov%20and%20Yanzhi%20Wang%20and%20Yihui%20Ren%20and%20Xuan%20Zhang%0AAbstract%3A%20%20%20Engineering%20design%20operates%20through%20hierarchical%20abstraction%20from%20system%0Aspecifications%20to%20component%20implementations%2C%20requiring%20visual%20understanding%0Acoupled%20with%20mathematical%20reasoning%20at%20each%20level.%20While%20Multi-modal%20Large%0ALanguage%20Models%20%28MLLMs%29%20excel%20at%20natural%20image%20tasks%2C%20their%20ability%20to%20extract%0Amathematical%20models%20from%20technical%20diagrams%20remains%20unexplored.%20We%20present%0A%5Ctextbf%7BCircuitSense%7D%2C%20a%20comprehensive%20benchmark%20evaluating%20circuit%0Aunderstanding%20across%20this%20hierarchy%20through%208%2C006%2B%20problems%20spanning%0Acomponent-level%20schematics%20to%20system-level%20block%20diagrams.%20Our%20benchmark%0Auniquely%20examines%20the%20complete%20engineering%20workflow%3A%20Perception%2C%20Analysis%2C%20and%0ADesign%2C%20with%20a%20particular%20emphasis%20on%20the%20critical%20but%20underexplored%20capability%0Aof%20deriving%20symbolic%20equations%20from%20visual%20inputs.%20We%20introduce%20a%20hierarchical%0Asynthetic%20generation%20pipeline%20consisting%20of%20a%20grid-based%20schematic%20generator%0Aand%20a%20block%20diagram%20generator%20with%20auto-derived%20symbolic%20equation%20labels.%0AComprehensive%20evaluation%20of%20six%20state-of-the-art%20MLLMs%2C%20including%20both%0Aclosed-source%20and%20open-source%20models%2C%20reveals%20fundamental%20limitations%20in%0Avisual-to-mathematical%20reasoning.%20Closed-source%20models%20achieve%20over%2085%5C%25%0Aaccuracy%20on%20perception%20tasks%20involving%20component%20recognition%20and%20topology%0Aidentification%2C%20yet%20their%20performance%20on%20symbolic%20derivation%20and%20analytical%0Areasoning%20falls%20below%2019%5C%25%2C%20exposing%20a%20critical%20gap%20between%20visual%20parsing%20and%0Asymbolic%20reasoning.%20Models%20with%20stronger%20symbolic%20reasoning%20capabilities%0Aconsistently%20achieve%20higher%20design%20task%20accuracy%2C%20confirming%20the%20fundamental%0Arole%20of%20mathematical%20understanding%20in%20circuit%20synthesis%20and%20establishing%0Asymbolic%20reasoning%20as%20the%20key%20metric%20for%20engineering%20competence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCircuitSense%253A%2520A%2520Hierarchical%2520Circuit%2520System%2520Benchmark%2520Bridging%2520Visual%250A%2520%2520Comprehension%2520and%2520Symbolic%2520Reasoning%2520in%2520Engineering%2520Design%2520Process%26entry.906535625%3DArman%2520Akbari%2520and%2520Jian%2520Gao%2520and%2520Yifei%2520Zou%2520and%2520Mei%2520Yang%2520and%2520Jinru%2520Duan%2520and%2520Dmitrii%2520Torbunov%2520and%2520Yanzhi%2520Wang%2520and%2520Yihui%2520Ren%2520and%2520Xuan%2520Zhang%26entry.1292438233%3D%2520%2520Engineering%2520design%2520operates%2520through%2520hierarchical%2520abstraction%2520from%2520system%250Aspecifications%2520to%2520component%2520implementations%252C%2520requiring%2520visual%2520understanding%250Acoupled%2520with%2520mathematical%2520reasoning%2520at%2520each%2520level.%2520While%2520Multi-modal%2520Large%250ALanguage%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520natural%2520image%2520tasks%252C%2520their%2520ability%2520to%2520extract%250Amathematical%2520models%2520from%2520technical%2520diagrams%2520remains%2520unexplored.%2520We%2520present%250A%255Ctextbf%257BCircuitSense%257D%252C%2520a%2520comprehensive%2520benchmark%2520evaluating%2520circuit%250Aunderstanding%2520across%2520this%2520hierarchy%2520through%25208%252C006%252B%2520problems%2520spanning%250Acomponent-level%2520schematics%2520to%2520system-level%2520block%2520diagrams.%2520Our%2520benchmark%250Auniquely%2520examines%2520the%2520complete%2520engineering%2520workflow%253A%2520Perception%252C%2520Analysis%252C%2520and%250ADesign%252C%2520with%2520a%2520particular%2520emphasis%2520on%2520the%2520critical%2520but%2520underexplored%2520capability%250Aof%2520deriving%2520symbolic%2520equations%2520from%2520visual%2520inputs.%2520We%2520introduce%2520a%2520hierarchical%250Asynthetic%2520generation%2520pipeline%2520consisting%2520of%2520a%2520grid-based%2520schematic%2520generator%250Aand%2520a%2520block%2520diagram%2520generator%2520with%2520auto-derived%2520symbolic%2520equation%2520labels.%250AComprehensive%2520evaluation%2520of%2520six%2520state-of-the-art%2520MLLMs%252C%2520including%2520both%250Aclosed-source%2520and%2520open-source%2520models%252C%2520reveals%2520fundamental%2520limitations%2520in%250Avisual-to-mathematical%2520reasoning.%2520Closed-source%2520models%2520achieve%2520over%252085%255C%2525%250Aaccuracy%2520on%2520perception%2520tasks%2520involving%2520component%2520recognition%2520and%2520topology%250Aidentification%252C%2520yet%2520their%2520performance%2520on%2520symbolic%2520derivation%2520and%2520analytical%250Areasoning%2520falls%2520below%252019%255C%2525%252C%2520exposing%2520a%2520critical%2520gap%2520between%2520visual%2520parsing%2520and%250Asymbolic%2520reasoning.%2520Models%2520with%2520stronger%2520symbolic%2520reasoning%2520capabilities%250Aconsistently%2520achieve%2520higher%2520design%2520task%2520accuracy%252C%2520confirming%2520the%2520fundamental%250Arole%2520of%2520mathematical%2520understanding%2520in%2520circuit%2520synthesis%2520and%2520establishing%250Asymbolic%2520reasoning%2520as%2520the%2520key%2520metric%2520for%2520engineering%2520competence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CircuitSense%3A%20A%20Hierarchical%20Circuit%20System%20Benchmark%20Bridging%20Visual%0A%20%20Comprehension%20and%20Symbolic%20Reasoning%20in%20Engineering%20Design%20Process&entry.906535625=Arman%20Akbari%20and%20Jian%20Gao%20and%20Yifei%20Zou%20and%20Mei%20Yang%20and%20Jinru%20Duan%20and%20Dmitrii%20Torbunov%20and%20Yanzhi%20Wang%20and%20Yihui%20Ren%20and%20Xuan%20Zhang&entry.1292438233=%20%20Engineering%20design%20operates%20through%20hierarchical%20abstraction%20from%20system%0Aspecifications%20to%20component%20implementations%2C%20requiring%20visual%20understanding%0Acoupled%20with%20mathematical%20reasoning%20at%20each%20level.%20While%20Multi-modal%20Large%0ALanguage%20Models%20%28MLLMs%29%20excel%20at%20natural%20image%20tasks%2C%20their%20ability%20to%20extract%0Amathematical%20models%20from%20technical%20diagrams%20remains%20unexplored.%20We%20present%0A%5Ctextbf%7BCircuitSense%7D%2C%20a%20comprehensive%20benchmark%20evaluating%20circuit%0Aunderstanding%20across%20this%20hierarchy%20through%208%2C006%2B%20problems%20spanning%0Acomponent-level%20schematics%20to%20system-level%20block%20diagrams.%20Our%20benchmark%0Auniquely%20examines%20the%20complete%20engineering%20workflow%3A%20Perception%2C%20Analysis%2C%20and%0ADesign%2C%20with%20a%20particular%20emphasis%20on%20the%20critical%20but%20underexplored%20capability%0Aof%20deriving%20symbolic%20equations%20from%20visual%20inputs.%20We%20introduce%20a%20hierarchical%0Asynthetic%20generation%20pipeline%20consisting%20of%20a%20grid-based%20schematic%20generator%0Aand%20a%20block%20diagram%20generator%20with%20auto-derived%20symbolic%20equation%20labels.%0AComprehensive%20evaluation%20of%20six%20state-of-the-art%20MLLMs%2C%20including%20both%0Aclosed-source%20and%20open-source%20models%2C%20reveals%20fundamental%20limitations%20in%0Avisual-to-mathematical%20reasoning.%20Closed-source%20models%20achieve%20over%2085%5C%25%0Aaccuracy%20on%20perception%20tasks%20involving%20component%20recognition%20and%20topology%0Aidentification%2C%20yet%20their%20performance%20on%20symbolic%20derivation%20and%20analytical%0Areasoning%20falls%20below%2019%5C%25%2C%20exposing%20a%20critical%20gap%20between%20visual%20parsing%20and%0Asymbolic%20reasoning.%20Models%20with%20stronger%20symbolic%20reasoning%20capabilities%0Aconsistently%20achieve%20higher%20design%20task%20accuracy%2C%20confirming%20the%20fundamental%0Arole%20of%20mathematical%20understanding%20in%20circuit%20synthesis%20and%20establishing%0Asymbolic%20reasoning%20as%20the%20key%20metric%20for%20engineering%20competence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22339v1&entry.124074799=Read"},
{"title": "MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark", "author": "Hui Li and Changhao Jiang and Hongyu Wang and Ming Zhang and Jiajun Sun and Zhixiong Yang and Yifei Cao and Shihan Dou and Xiaoran Fan and Baoyu Fan and Tao Ji and Tao Gui and Qi Zhang and Xuanjing Huang", "abstract": "  The ability to reason from audio, including speech, paralinguistic cues,\nenvironmental sounds, and music, is essential for AI agents to interact\neffectively in real-world scenarios. Existing benchmarks mainly focus on static\nor single-scene settings and do not fully capture scenarios where multiple\nspeakers, unfolding events, and heterogeneous audio sources interact. To\naddress these challenges, we introduce MDAR, a benchmark for evaluating models\non complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR\ncomprises 3,000 carefully curated question-answer pairs linked to diverse audio\nclips, covering five categories of complex reasoning and spanning three\nquestion types. We benchmark 26 state-of-the-art audio language models on MDAR\nand observe that they exhibit limitations in complex reasoning tasks. On\nsingle-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,\nwhereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio\nsubstantially outperforms Qwen2.5-Omni on the more challenging multiple-choice\nand open-ended tasks. Across all three question types, no model achieves 80%\nperformance. These findings underscore the unique challenges posed by MDAR and\nits value as a benchmark for advancing audio reasoning research.Code and\nbenchmark can be found at https://github.com/luckyerr/MDAR.\n", "link": "http://arxiv.org/abs/2509.22461v1", "date": "2025-09-26", "relevancy": 2.2285, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5681}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MDAR%3A%20A%20Multi-scene%20Dynamic%20Audio%20Reasoning%20Benchmark&body=Title%3A%20MDAR%3A%20A%20Multi-scene%20Dynamic%20Audio%20Reasoning%20Benchmark%0AAuthor%3A%20Hui%20Li%20and%20Changhao%20Jiang%20and%20Hongyu%20Wang%20and%20Ming%20Zhang%20and%20Jiajun%20Sun%20and%20Zhixiong%20Yang%20and%20Yifei%20Cao%20and%20Shihan%20Dou%20and%20Xiaoran%20Fan%20and%20Baoyu%20Fan%20and%20Tao%20Ji%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%0AAbstract%3A%20%20%20The%20ability%20to%20reason%20from%20audio%2C%20including%20speech%2C%20paralinguistic%20cues%2C%0Aenvironmental%20sounds%2C%20and%20music%2C%20is%20essential%20for%20AI%20agents%20to%20interact%0Aeffectively%20in%20real-world%20scenarios.%20Existing%20benchmarks%20mainly%20focus%20on%20static%0Aor%20single-scene%20settings%20and%20do%20not%20fully%20capture%20scenarios%20where%20multiple%0Aspeakers%2C%20unfolding%20events%2C%20and%20heterogeneous%20audio%20sources%20interact.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20MDAR%2C%20a%20benchmark%20for%20evaluating%20models%0Aon%20complex%2C%20multi-scene%2C%20and%20dynamically%20evolving%20audio%20reasoning%20tasks.%20MDAR%0Acomprises%203%2C000%20carefully%20curated%20question-answer%20pairs%20linked%20to%20diverse%20audio%0Aclips%2C%20covering%20five%20categories%20of%20complex%20reasoning%20and%20spanning%20three%0Aquestion%20types.%20We%20benchmark%2026%20state-of-the-art%20audio%20language%20models%20on%20MDAR%0Aand%20observe%20that%20they%20exhibit%20limitations%20in%20complex%20reasoning%20tasks.%20On%0Asingle-choice%20questions%2C%20Qwen2.5-Omni%20%28open-source%29%20achieves%2076.67%25%20accuracy%2C%0Awhereas%20GPT-4o%20Audio%20%28closed-source%29%20reaches%2068.47%25%3B%20however%2C%20GPT-4o%20Audio%0Asubstantially%20outperforms%20Qwen2.5-Omni%20on%20the%20more%20challenging%20multiple-choice%0Aand%20open-ended%20tasks.%20Across%20all%20three%20question%20types%2C%20no%20model%20achieves%2080%25%0Aperformance.%20These%20findings%20underscore%20the%20unique%20challenges%20posed%20by%20MDAR%20and%0Aits%20value%20as%20a%20benchmark%20for%20advancing%20audio%20reasoning%20research.Code%20and%0Abenchmark%20can%20be%20found%20at%20https%3A//github.com/luckyerr/MDAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMDAR%253A%2520A%2520Multi-scene%2520Dynamic%2520Audio%2520Reasoning%2520Benchmark%26entry.906535625%3DHui%2520Li%2520and%2520Changhao%2520Jiang%2520and%2520Hongyu%2520Wang%2520and%2520Ming%2520Zhang%2520and%2520Jiajun%2520Sun%2520and%2520Zhixiong%2520Yang%2520and%2520Yifei%2520Cao%2520and%2520Shihan%2520Dou%2520and%2520Xiaoran%2520Fan%2520and%2520Baoyu%2520Fan%2520and%2520Tao%2520Ji%2520and%2520Tao%2520Gui%2520and%2520Qi%2520Zhang%2520and%2520Xuanjing%2520Huang%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520reason%2520from%2520audio%252C%2520including%2520speech%252C%2520paralinguistic%2520cues%252C%250Aenvironmental%2520sounds%252C%2520and%2520music%252C%2520is%2520essential%2520for%2520AI%2520agents%2520to%2520interact%250Aeffectively%2520in%2520real-world%2520scenarios.%2520Existing%2520benchmarks%2520mainly%2520focus%2520on%2520static%250Aor%2520single-scene%2520settings%2520and%2520do%2520not%2520fully%2520capture%2520scenarios%2520where%2520multiple%250Aspeakers%252C%2520unfolding%2520events%252C%2520and%2520heterogeneous%2520audio%2520sources%2520interact.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520MDAR%252C%2520a%2520benchmark%2520for%2520evaluating%2520models%250Aon%2520complex%252C%2520multi-scene%252C%2520and%2520dynamically%2520evolving%2520audio%2520reasoning%2520tasks.%2520MDAR%250Acomprises%25203%252C000%2520carefully%2520curated%2520question-answer%2520pairs%2520linked%2520to%2520diverse%2520audio%250Aclips%252C%2520covering%2520five%2520categories%2520of%2520complex%2520reasoning%2520and%2520spanning%2520three%250Aquestion%2520types.%2520We%2520benchmark%252026%2520state-of-the-art%2520audio%2520language%2520models%2520on%2520MDAR%250Aand%2520observe%2520that%2520they%2520exhibit%2520limitations%2520in%2520complex%2520reasoning%2520tasks.%2520On%250Asingle-choice%2520questions%252C%2520Qwen2.5-Omni%2520%2528open-source%2529%2520achieves%252076.67%2525%2520accuracy%252C%250Awhereas%2520GPT-4o%2520Audio%2520%2528closed-source%2529%2520reaches%252068.47%2525%253B%2520however%252C%2520GPT-4o%2520Audio%250Asubstantially%2520outperforms%2520Qwen2.5-Omni%2520on%2520the%2520more%2520challenging%2520multiple-choice%250Aand%2520open-ended%2520tasks.%2520Across%2520all%2520three%2520question%2520types%252C%2520no%2520model%2520achieves%252080%2525%250Aperformance.%2520These%2520findings%2520underscore%2520the%2520unique%2520challenges%2520posed%2520by%2520MDAR%2520and%250Aits%2520value%2520as%2520a%2520benchmark%2520for%2520advancing%2520audio%2520reasoning%2520research.Code%2520and%250Abenchmark%2520can%2520be%2520found%2520at%2520https%253A//github.com/luckyerr/MDAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MDAR%3A%20A%20Multi-scene%20Dynamic%20Audio%20Reasoning%20Benchmark&entry.906535625=Hui%20Li%20and%20Changhao%20Jiang%20and%20Hongyu%20Wang%20and%20Ming%20Zhang%20and%20Jiajun%20Sun%20and%20Zhixiong%20Yang%20and%20Yifei%20Cao%20and%20Shihan%20Dou%20and%20Xiaoran%20Fan%20and%20Baoyu%20Fan%20and%20Tao%20Ji%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang&entry.1292438233=%20%20The%20ability%20to%20reason%20from%20audio%2C%20including%20speech%2C%20paralinguistic%20cues%2C%0Aenvironmental%20sounds%2C%20and%20music%2C%20is%20essential%20for%20AI%20agents%20to%20interact%0Aeffectively%20in%20real-world%20scenarios.%20Existing%20benchmarks%20mainly%20focus%20on%20static%0Aor%20single-scene%20settings%20and%20do%20not%20fully%20capture%20scenarios%20where%20multiple%0Aspeakers%2C%20unfolding%20events%2C%20and%20heterogeneous%20audio%20sources%20interact.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20MDAR%2C%20a%20benchmark%20for%20evaluating%20models%0Aon%20complex%2C%20multi-scene%2C%20and%20dynamically%20evolving%20audio%20reasoning%20tasks.%20MDAR%0Acomprises%203%2C000%20carefully%20curated%20question-answer%20pairs%20linked%20to%20diverse%20audio%0Aclips%2C%20covering%20five%20categories%20of%20complex%20reasoning%20and%20spanning%20three%0Aquestion%20types.%20We%20benchmark%2026%20state-of-the-art%20audio%20language%20models%20on%20MDAR%0Aand%20observe%20that%20they%20exhibit%20limitations%20in%20complex%20reasoning%20tasks.%20On%0Asingle-choice%20questions%2C%20Qwen2.5-Omni%20%28open-source%29%20achieves%2076.67%25%20accuracy%2C%0Awhereas%20GPT-4o%20Audio%20%28closed-source%29%20reaches%2068.47%25%3B%20however%2C%20GPT-4o%20Audio%0Asubstantially%20outperforms%20Qwen2.5-Omni%20on%20the%20more%20challenging%20multiple-choice%0Aand%20open-ended%20tasks.%20Across%20all%20three%20question%20types%2C%20no%20model%20achieves%2080%25%0Aperformance.%20These%20findings%20underscore%20the%20unique%20challenges%20posed%20by%20MDAR%20and%0Aits%20value%20as%20a%20benchmark%20for%20advancing%20audio%20reasoning%20research.Code%20and%0Abenchmark%20can%20be%20found%20at%20https%3A//github.com/luckyerr/MDAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22461v1&entry.124074799=Read"},
{"title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for\n  Autonomous Underwater Navigation", "author": "Zhangyuan Wang and Yunpeng Zhu and Yuqi Yan and Xiaoyuan Tian and Xinhao Shao and Meixuan Li and Weikun Li and Guangsheng Su and Weicheng Cui and Dixia Fan", "abstract": "  This paper presents UnderwaterVLA, a novel framework for autonomous\nunderwater navigation that integrates multimodal foundation models with\nembodied intelligence systems. Underwater operations remain difficult due to\nhydrodynamic disturbances, limited communication bandwidth, and degraded\nsensing in turbid waters. To address these challenges, we introduce three\ninnovations. First, a dual-brain architecture decouples high-level mission\nreasoning from low-level reactive control, enabling robust operation under\ncommunication and computational constraints. Second, we apply\nVision-Language-Action(VLA) models to underwater robotics for the first time,\nincorporating structured chain-of-thought reasoning for interpretable\ndecision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)\nscheme compensates for fluid effects in real time without costly task-specific\ntraining. Experimental results in field tests show that UnderwaterVLA reduces\nnavigation errors in degraded visual conditions while maintaining higher task\ncompletion by 19% to 27% over baseline. By minimizing reliance on\nunderwater-specific training data and improving adaptability across\nenvironments, UnderwaterVLA provides a scalable and cost-effective path toward\nthe next generation of intelligent AUVs.\n", "link": "http://arxiv.org/abs/2509.22441v1", "date": "2025-09-26", "relevancy": 2.2253, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5689}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5512}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnderwaterVLA%3A%20Dual-brain%20Vision-Language-Action%20architecture%20for%0A%20%20Autonomous%20Underwater%20Navigation&body=Title%3A%20UnderwaterVLA%3A%20Dual-brain%20Vision-Language-Action%20architecture%20for%0A%20%20Autonomous%20Underwater%20Navigation%0AAuthor%3A%20Zhangyuan%20Wang%20and%20Yunpeng%20Zhu%20and%20Yuqi%20Yan%20and%20Xiaoyuan%20Tian%20and%20Xinhao%20Shao%20and%20Meixuan%20Li%20and%20Weikun%20Li%20and%20Guangsheng%20Su%20and%20Weicheng%20Cui%20and%20Dixia%20Fan%0AAbstract%3A%20%20%20This%20paper%20presents%20UnderwaterVLA%2C%20a%20novel%20framework%20for%20autonomous%0Aunderwater%20navigation%20that%20integrates%20multimodal%20foundation%20models%20with%0Aembodied%20intelligence%20systems.%20Underwater%20operations%20remain%20difficult%20due%20to%0Ahydrodynamic%20disturbances%2C%20limited%20communication%20bandwidth%2C%20and%20degraded%0Asensing%20in%20turbid%20waters.%20To%20address%20these%20challenges%2C%20we%20introduce%20three%0Ainnovations.%20First%2C%20a%20dual-brain%20architecture%20decouples%20high-level%20mission%0Areasoning%20from%20low-level%20reactive%20control%2C%20enabling%20robust%20operation%20under%0Acommunication%20and%20computational%20constraints.%20Second%2C%20we%20apply%0AVision-Language-Action%28VLA%29%20models%20to%20underwater%20robotics%20for%20the%20first%20time%2C%0Aincorporating%20structured%20chain-of-thought%20reasoning%20for%20interpretable%0Adecision-making.%20Third%2C%20a%20hydrodynamics-informed%20Model%20Predictive%20Control%28MPC%29%0Ascheme%20compensates%20for%20fluid%20effects%20in%20real%20time%20without%20costly%20task-specific%0Atraining.%20Experimental%20results%20in%20field%20tests%20show%20that%20UnderwaterVLA%20reduces%0Anavigation%20errors%20in%20degraded%20visual%20conditions%20while%20maintaining%20higher%20task%0Acompletion%20by%2019%25%20to%2027%25%20over%20baseline.%20By%20minimizing%20reliance%20on%0Aunderwater-specific%20training%20data%20and%20improving%20adaptability%20across%0Aenvironments%2C%20UnderwaterVLA%20provides%20a%20scalable%20and%20cost-effective%20path%20toward%0Athe%20next%20generation%20of%20intelligent%20AUVs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderwaterVLA%253A%2520Dual-brain%2520Vision-Language-Action%2520architecture%2520for%250A%2520%2520Autonomous%2520Underwater%2520Navigation%26entry.906535625%3DZhangyuan%2520Wang%2520and%2520Yunpeng%2520Zhu%2520and%2520Yuqi%2520Yan%2520and%2520Xiaoyuan%2520Tian%2520and%2520Xinhao%2520Shao%2520and%2520Meixuan%2520Li%2520and%2520Weikun%2520Li%2520and%2520Guangsheng%2520Su%2520and%2520Weicheng%2520Cui%2520and%2520Dixia%2520Fan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520UnderwaterVLA%252C%2520a%2520novel%2520framework%2520for%2520autonomous%250Aunderwater%2520navigation%2520that%2520integrates%2520multimodal%2520foundation%2520models%2520with%250Aembodied%2520intelligence%2520systems.%2520Underwater%2520operations%2520remain%2520difficult%2520due%2520to%250Ahydrodynamic%2520disturbances%252C%2520limited%2520communication%2520bandwidth%252C%2520and%2520degraded%250Asensing%2520in%2520turbid%2520waters.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520three%250Ainnovations.%2520First%252C%2520a%2520dual-brain%2520architecture%2520decouples%2520high-level%2520mission%250Areasoning%2520from%2520low-level%2520reactive%2520control%252C%2520enabling%2520robust%2520operation%2520under%250Acommunication%2520and%2520computational%2520constraints.%2520Second%252C%2520we%2520apply%250AVision-Language-Action%2528VLA%2529%2520models%2520to%2520underwater%2520robotics%2520for%2520the%2520first%2520time%252C%250Aincorporating%2520structured%2520chain-of-thought%2520reasoning%2520for%2520interpretable%250Adecision-making.%2520Third%252C%2520a%2520hydrodynamics-informed%2520Model%2520Predictive%2520Control%2528MPC%2529%250Ascheme%2520compensates%2520for%2520fluid%2520effects%2520in%2520real%2520time%2520without%2520costly%2520task-specific%250Atraining.%2520Experimental%2520results%2520in%2520field%2520tests%2520show%2520that%2520UnderwaterVLA%2520reduces%250Anavigation%2520errors%2520in%2520degraded%2520visual%2520conditions%2520while%2520maintaining%2520higher%2520task%250Acompletion%2520by%252019%2525%2520to%252027%2525%2520over%2520baseline.%2520By%2520minimizing%2520reliance%2520on%250Aunderwater-specific%2520training%2520data%2520and%2520improving%2520adaptability%2520across%250Aenvironments%252C%2520UnderwaterVLA%2520provides%2520a%2520scalable%2520and%2520cost-effective%2520path%2520toward%250Athe%2520next%2520generation%2520of%2520intelligent%2520AUVs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnderwaterVLA%3A%20Dual-brain%20Vision-Language-Action%20architecture%20for%0A%20%20Autonomous%20Underwater%20Navigation&entry.906535625=Zhangyuan%20Wang%20and%20Yunpeng%20Zhu%20and%20Yuqi%20Yan%20and%20Xiaoyuan%20Tian%20and%20Xinhao%20Shao%20and%20Meixuan%20Li%20and%20Weikun%20Li%20and%20Guangsheng%20Su%20and%20Weicheng%20Cui%20and%20Dixia%20Fan&entry.1292438233=%20%20This%20paper%20presents%20UnderwaterVLA%2C%20a%20novel%20framework%20for%20autonomous%0Aunderwater%20navigation%20that%20integrates%20multimodal%20foundation%20models%20with%0Aembodied%20intelligence%20systems.%20Underwater%20operations%20remain%20difficult%20due%20to%0Ahydrodynamic%20disturbances%2C%20limited%20communication%20bandwidth%2C%20and%20degraded%0Asensing%20in%20turbid%20waters.%20To%20address%20these%20challenges%2C%20we%20introduce%20three%0Ainnovations.%20First%2C%20a%20dual-brain%20architecture%20decouples%20high-level%20mission%0Areasoning%20from%20low-level%20reactive%20control%2C%20enabling%20robust%20operation%20under%0Acommunication%20and%20computational%20constraints.%20Second%2C%20we%20apply%0AVision-Language-Action%28VLA%29%20models%20to%20underwater%20robotics%20for%20the%20first%20time%2C%0Aincorporating%20structured%20chain-of-thought%20reasoning%20for%20interpretable%0Adecision-making.%20Third%2C%20a%20hydrodynamics-informed%20Model%20Predictive%20Control%28MPC%29%0Ascheme%20compensates%20for%20fluid%20effects%20in%20real%20time%20without%20costly%20task-specific%0Atraining.%20Experimental%20results%20in%20field%20tests%20show%20that%20UnderwaterVLA%20reduces%0Anavigation%20errors%20in%20degraded%20visual%20conditions%20while%20maintaining%20higher%20task%0Acompletion%20by%2019%25%20to%2027%25%20over%20baseline.%20By%20minimizing%20reliance%20on%0Aunderwater-specific%20training%20data%20and%20improving%20adaptability%20across%0Aenvironments%2C%20UnderwaterVLA%20provides%20a%20scalable%20and%20cost-effective%20path%20toward%0Athe%20next%20generation%20of%20intelligent%20AUVs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22441v1&entry.124074799=Read"},
{"title": "Do Data Valuations Make Good Data Prices?", "author": "Dongyang Fan and Tyler J. Rotello and Sai Praneeth Karimireddy", "abstract": "  As large language models increasingly rely on external data sources,\ncompensating data contributors has become a central concern. But how should\nthese payments be devised? We revisit data valuations from a\n$\\textit{market-design perspective}$ where payments serve to compensate data\nowners for the $\\textit{private}$ heterogeneous costs they incur for collecting\nand sharing data. We show that popular valuation methods-such as Leave-One-Out\nand Data Shapley-make for poor payments. They fail to ensure truthful reporting\nof the costs, leading to $\\textit{inefficient market}$ outcomes. To address\nthis, we adapt well-established payment rules from mechanism design, namely\nMyerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We show\nthat Myerson payment is the minimal truthful mechanism, optimal from the\nbuyer's perspective. Additionally, we identify a condition under which both\ndata buyers and sellers are utility-satisfied, and the market achieves\nefficiency. Our findings highlight the importance of incorporating incentive\ncompatibility into data valuation design, paving the way for more robust and\nefficient data markets. Our data market framework is readily applicable to\nreal-world scenarios. We illustrate this with simulations of contributor\ncompensation in an LLM based retrieval-augmented generation (RAG) marketplace\ntasked with challenging medical question answering.\n", "link": "http://arxiv.org/abs/2504.05563v2", "date": "2025-09-26", "relevancy": 2.225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Data%20Valuations%20Make%20Good%20Data%20Prices%3F&body=Title%3A%20Do%20Data%20Valuations%20Make%20Good%20Data%20Prices%3F%0AAuthor%3A%20Dongyang%20Fan%20and%20Tyler%20J.%20Rotello%20and%20Sai%20Praneeth%20Karimireddy%0AAbstract%3A%20%20%20As%20large%20language%20models%20increasingly%20rely%20on%20external%20data%20sources%2C%0Acompensating%20data%20contributors%20has%20become%20a%20central%20concern.%20But%20how%20should%0Athese%20payments%20be%20devised%3F%20We%20revisit%20data%20valuations%20from%20a%0A%24%5Ctextit%7Bmarket-design%20perspective%7D%24%20where%20payments%20serve%20to%20compensate%20data%0Aowners%20for%20the%20%24%5Ctextit%7Bprivate%7D%24%20heterogeneous%20costs%20they%20incur%20for%20collecting%0Aand%20sharing%20data.%20We%20show%20that%20popular%20valuation%20methods-such%20as%20Leave-One-Out%0Aand%20Data%20Shapley-make%20for%20poor%20payments.%20They%20fail%20to%20ensure%20truthful%20reporting%0Aof%20the%20costs%2C%20leading%20to%20%24%5Ctextit%7Binefficient%20market%7D%24%20outcomes.%20To%20address%0Athis%2C%20we%20adapt%20well-established%20payment%20rules%20from%20mechanism%20design%2C%20namely%0AMyerson%20and%20Vickrey-Clarke-Groves%20%28VCG%29%2C%20to%20the%20data%20market%20setting.%20We%20show%0Athat%20Myerson%20payment%20is%20the%20minimal%20truthful%20mechanism%2C%20optimal%20from%20the%0Abuyer%27s%20perspective.%20Additionally%2C%20we%20identify%20a%20condition%20under%20which%20both%0Adata%20buyers%20and%20sellers%20are%20utility-satisfied%2C%20and%20the%20market%20achieves%0Aefficiency.%20Our%20findings%20highlight%20the%20importance%20of%20incorporating%20incentive%0Acompatibility%20into%20data%20valuation%20design%2C%20paving%20the%20way%20for%20more%20robust%20and%0Aefficient%20data%20markets.%20Our%20data%20market%20framework%20is%20readily%20applicable%20to%0Areal-world%20scenarios.%20We%20illustrate%20this%20with%20simulations%20of%20contributor%0Acompensation%20in%20an%20LLM%20based%20retrieval-augmented%20generation%20%28RAG%29%20marketplace%0Atasked%20with%20challenging%20medical%20question%20answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.05563v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Data%2520Valuations%2520Make%2520Good%2520Data%2520Prices%253F%26entry.906535625%3DDongyang%2520Fan%2520and%2520Tyler%2520J.%2520Rotello%2520and%2520Sai%2520Praneeth%2520Karimireddy%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520increasingly%2520rely%2520on%2520external%2520data%2520sources%252C%250Acompensating%2520data%2520contributors%2520has%2520become%2520a%2520central%2520concern.%2520But%2520how%2520should%250Athese%2520payments%2520be%2520devised%253F%2520We%2520revisit%2520data%2520valuations%2520from%2520a%250A%2524%255Ctextit%257Bmarket-design%2520perspective%257D%2524%2520where%2520payments%2520serve%2520to%2520compensate%2520data%250Aowners%2520for%2520the%2520%2524%255Ctextit%257Bprivate%257D%2524%2520heterogeneous%2520costs%2520they%2520incur%2520for%2520collecting%250Aand%2520sharing%2520data.%2520We%2520show%2520that%2520popular%2520valuation%2520methods-such%2520as%2520Leave-One-Out%250Aand%2520Data%2520Shapley-make%2520for%2520poor%2520payments.%2520They%2520fail%2520to%2520ensure%2520truthful%2520reporting%250Aof%2520the%2520costs%252C%2520leading%2520to%2520%2524%255Ctextit%257Binefficient%2520market%257D%2524%2520outcomes.%2520To%2520address%250Athis%252C%2520we%2520adapt%2520well-established%2520payment%2520rules%2520from%2520mechanism%2520design%252C%2520namely%250AMyerson%2520and%2520Vickrey-Clarke-Groves%2520%2528VCG%2529%252C%2520to%2520the%2520data%2520market%2520setting.%2520We%2520show%250Athat%2520Myerson%2520payment%2520is%2520the%2520minimal%2520truthful%2520mechanism%252C%2520optimal%2520from%2520the%250Abuyer%2527s%2520perspective.%2520Additionally%252C%2520we%2520identify%2520a%2520condition%2520under%2520which%2520both%250Adata%2520buyers%2520and%2520sellers%2520are%2520utility-satisfied%252C%2520and%2520the%2520market%2520achieves%250Aefficiency.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520incorporating%2520incentive%250Acompatibility%2520into%2520data%2520valuation%2520design%252C%2520paving%2520the%2520way%2520for%2520more%2520robust%2520and%250Aefficient%2520data%2520markets.%2520Our%2520data%2520market%2520framework%2520is%2520readily%2520applicable%2520to%250Areal-world%2520scenarios.%2520We%2520illustrate%2520this%2520with%2520simulations%2520of%2520contributor%250Acompensation%2520in%2520an%2520LLM%2520based%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520marketplace%250Atasked%2520with%2520challenging%2520medical%2520question%2520answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.05563v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Data%20Valuations%20Make%20Good%20Data%20Prices%3F&entry.906535625=Dongyang%20Fan%20and%20Tyler%20J.%20Rotello%20and%20Sai%20Praneeth%20Karimireddy&entry.1292438233=%20%20As%20large%20language%20models%20increasingly%20rely%20on%20external%20data%20sources%2C%0Acompensating%20data%20contributors%20has%20become%20a%20central%20concern.%20But%20how%20should%0Athese%20payments%20be%20devised%3F%20We%20revisit%20data%20valuations%20from%20a%0A%24%5Ctextit%7Bmarket-design%20perspective%7D%24%20where%20payments%20serve%20to%20compensate%20data%0Aowners%20for%20the%20%24%5Ctextit%7Bprivate%7D%24%20heterogeneous%20costs%20they%20incur%20for%20collecting%0Aand%20sharing%20data.%20We%20show%20that%20popular%20valuation%20methods-such%20as%20Leave-One-Out%0Aand%20Data%20Shapley-make%20for%20poor%20payments.%20They%20fail%20to%20ensure%20truthful%20reporting%0Aof%20the%20costs%2C%20leading%20to%20%24%5Ctextit%7Binefficient%20market%7D%24%20outcomes.%20To%20address%0Athis%2C%20we%20adapt%20well-established%20payment%20rules%20from%20mechanism%20design%2C%20namely%0AMyerson%20and%20Vickrey-Clarke-Groves%20%28VCG%29%2C%20to%20the%20data%20market%20setting.%20We%20show%0Athat%20Myerson%20payment%20is%20the%20minimal%20truthful%20mechanism%2C%20optimal%20from%20the%0Abuyer%27s%20perspective.%20Additionally%2C%20we%20identify%20a%20condition%20under%20which%20both%0Adata%20buyers%20and%20sellers%20are%20utility-satisfied%2C%20and%20the%20market%20achieves%0Aefficiency.%20Our%20findings%20highlight%20the%20importance%20of%20incorporating%20incentive%0Acompatibility%20into%20data%20valuation%20design%2C%20paving%20the%20way%20for%20more%20robust%20and%0Aefficient%20data%20markets.%20Our%20data%20market%20framework%20is%20readily%20applicable%20to%0Areal-world%20scenarios.%20We%20illustrate%20this%20with%20simulations%20of%20contributor%0Acompensation%20in%20an%20LLM%20based%20retrieval-augmented%20generation%20%28RAG%29%20marketplace%0Atasked%20with%20challenging%20medical%20question%20answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.05563v2&entry.124074799=Read"},
{"title": "Multimodal Recurrent Ensembles for Predicting Brain Responses to\n  Naturalistic Movies (Algonauts 2025)", "author": "Semih Eren and Deniz Kucukahmetler and Nico Scherf", "abstract": "  Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.\n", "link": "http://arxiv.org/abs/2507.17897v3", "date": "2025-09-26", "relevancy": 2.2212, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29&body=Title%3A%20Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29%0AAuthor%3A%20Semih%20Eren%20and%20Deniz%20Kucukahmetler%20and%20Nico%20Scherf%0AAbstract%3A%20%20%20Accurately%20predicting%20distributed%20cortical%20responses%20to%20naturalistic%20stimuli%0Arequires%20models%20that%20integrate%20visual%2C%20auditory%20and%20semantic%20information%20over%0Atime.%20We%20present%20a%20hierarchical%20multimodal%20recurrent%20ensemble%20that%20maps%0Apretrained%20video%2C%20audio%2C%20and%20language%20embeddings%20to%20fMRI%20time%20series%20recorded%0Awhile%20four%20subjects%20watched%20almost%2080%20hours%20of%20movies%20provided%20by%20the%20Algonauts%0A2025%20challenge.%20Modality-specific%20bidirectional%20RNNs%20encode%20temporal%20dynamics%3B%0Atheir%20hidden%20states%20are%20fused%20and%20passed%20to%20a%20second%20recurrent%20layer%2C%20and%0Alightweight%20subject-specific%20heads%20output%20responses%20for%201000%20cortical%20parcels.%0ATraining%20relies%20on%20a%20composite%20MSE-correlation%20loss%20and%20a%20curriculum%20that%0Agradually%20shifts%20emphasis%20from%20early%20sensory%20to%20late%20association%20regions.%0AAveraging%20100%20model%20variants%20further%20boosts%20robustness.%20The%20resulting%20system%0Aranked%20third%20on%20the%20competition%20leaderboard%2C%20achieving%20an%20overall%20Pearson%20r%20%3D%0A0.2094%20and%20the%20highest%20single-parcel%20peak%20score%20%28mean%20r%20%3D%200.63%29%20among%20all%0Aparticipants%2C%20with%20particularly%20strong%20gains%20for%20the%20most%20challenging%20subject%0A%28Subject%205%29.%20The%20approach%20establishes%20a%20simple%2C%20extensible%20baseline%20for%20future%0Amultimodal%20brain-encoding%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17897v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Recurrent%2520Ensembles%2520for%2520Predicting%2520Brain%2520Responses%2520to%250A%2520%2520Naturalistic%2520Movies%2520%2528Algonauts%25202025%2529%26entry.906535625%3DSemih%2520Eren%2520and%2520Deniz%2520Kucukahmetler%2520and%2520Nico%2520Scherf%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520distributed%2520cortical%2520responses%2520to%2520naturalistic%2520stimuli%250Arequires%2520models%2520that%2520integrate%2520visual%252C%2520auditory%2520and%2520semantic%2520information%2520over%250Atime.%2520We%2520present%2520a%2520hierarchical%2520multimodal%2520recurrent%2520ensemble%2520that%2520maps%250Apretrained%2520video%252C%2520audio%252C%2520and%2520language%2520embeddings%2520to%2520fMRI%2520time%2520series%2520recorded%250Awhile%2520four%2520subjects%2520watched%2520almost%252080%2520hours%2520of%2520movies%2520provided%2520by%2520the%2520Algonauts%250A2025%2520challenge.%2520Modality-specific%2520bidirectional%2520RNNs%2520encode%2520temporal%2520dynamics%253B%250Atheir%2520hidden%2520states%2520are%2520fused%2520and%2520passed%2520to%2520a%2520second%2520recurrent%2520layer%252C%2520and%250Alightweight%2520subject-specific%2520heads%2520output%2520responses%2520for%25201000%2520cortical%2520parcels.%250ATraining%2520relies%2520on%2520a%2520composite%2520MSE-correlation%2520loss%2520and%2520a%2520curriculum%2520that%250Agradually%2520shifts%2520emphasis%2520from%2520early%2520sensory%2520to%2520late%2520association%2520regions.%250AAveraging%2520100%2520model%2520variants%2520further%2520boosts%2520robustness.%2520The%2520resulting%2520system%250Aranked%2520third%2520on%2520the%2520competition%2520leaderboard%252C%2520achieving%2520an%2520overall%2520Pearson%2520r%2520%253D%250A0.2094%2520and%2520the%2520highest%2520single-parcel%2520peak%2520score%2520%2528mean%2520r%2520%253D%25200.63%2529%2520among%2520all%250Aparticipants%252C%2520with%2520particularly%2520strong%2520gains%2520for%2520the%2520most%2520challenging%2520subject%250A%2528Subject%25205%2529.%2520The%2520approach%2520establishes%2520a%2520simple%252C%2520extensible%2520baseline%2520for%2520future%250Amultimodal%2520brain-encoding%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17897v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Recurrent%20Ensembles%20for%20Predicting%20Brain%20Responses%20to%0A%20%20Naturalistic%20Movies%20%28Algonauts%202025%29&entry.906535625=Semih%20Eren%20and%20Deniz%20Kucukahmetler%20and%20Nico%20Scherf&entry.1292438233=%20%20Accurately%20predicting%20distributed%20cortical%20responses%20to%20naturalistic%20stimuli%0Arequires%20models%20that%20integrate%20visual%2C%20auditory%20and%20semantic%20information%20over%0Atime.%20We%20present%20a%20hierarchical%20multimodal%20recurrent%20ensemble%20that%20maps%0Apretrained%20video%2C%20audio%2C%20and%20language%20embeddings%20to%20fMRI%20time%20series%20recorded%0Awhile%20four%20subjects%20watched%20almost%2080%20hours%20of%20movies%20provided%20by%20the%20Algonauts%0A2025%20challenge.%20Modality-specific%20bidirectional%20RNNs%20encode%20temporal%20dynamics%3B%0Atheir%20hidden%20states%20are%20fused%20and%20passed%20to%20a%20second%20recurrent%20layer%2C%20and%0Alightweight%20subject-specific%20heads%20output%20responses%20for%201000%20cortical%20parcels.%0ATraining%20relies%20on%20a%20composite%20MSE-correlation%20loss%20and%20a%20curriculum%20that%0Agradually%20shifts%20emphasis%20from%20early%20sensory%20to%20late%20association%20regions.%0AAveraging%20100%20model%20variants%20further%20boosts%20robustness.%20The%20resulting%20system%0Aranked%20third%20on%20the%20competition%20leaderboard%2C%20achieving%20an%20overall%20Pearson%20r%20%3D%0A0.2094%20and%20the%20highest%20single-parcel%20peak%20score%20%28mean%20r%20%3D%200.63%29%20among%20all%0Aparticipants%2C%20with%20particularly%20strong%20gains%20for%20the%20most%20challenging%20subject%0A%28Subject%205%29.%20The%20approach%20establishes%20a%20simple%2C%20extensible%20baseline%20for%20future%0Amultimodal%20brain-encoding%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17897v3&entry.124074799=Read"},
{"title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI\n  Collaboration", "author": "Qi Mao and Tinghan Yang and Jiahao Li and Bin Li and Libiao Jin and Yan Lu", "abstract": "  The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI\nagents is transforming human-AI collaboration into bidirectional, multimodal\ninteraction. However, existing codecs remain optimized for unimodal, one-way\ncommunication, resulting in repeated degradation under conventional\ncompress-transmit-reconstruct pipelines. To address this limitation, we propose\nUniMIC, a Unified token-based Multimodal Interactive Coding framework that\nbridges edge devices and cloud AI agents. Instead of transmitting raw pixels or\nplain text, UniMIC employs compact tokenized representations as the\ncommunication medium, enabling efficient low-bitrate transmission while\nmaintaining compatibility with LMMs. To further enhance compression,\nlightweight Transformer-based entropy models with scenario-specific\ndesigns-generic, masked, and text-conditioned-effectively minimize inter-token\nredundancy. Extensive experiments on text-to-image generation, text-guided\ninpainting, outpainting, and visual question answering show that UniMIC\nachieves substantial bitrate savings and remains robust even at ultra-low\nbitrates (<0.05bpp), without compromising downstream task performance. These\nresults establish UniMIC as a practical and forward-looking paradigm for\nnext-generation multimodal interactive communication.\n", "link": "http://arxiv.org/abs/2509.22570v1", "date": "2025-09-26", "relevancy": 2.212, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.562}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniMIC%3A%20Token-Based%20Multimodal%20Interactive%20Coding%20for%20Human-AI%0A%20%20Collaboration&body=Title%3A%20UniMIC%3A%20Token-Based%20Multimodal%20Interactive%20Coding%20for%20Human-AI%0A%20%20Collaboration%0AAuthor%3A%20Qi%20Mao%20and%20Tinghan%20Yang%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Libiao%20Jin%20and%20Yan%20Lu%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20and%20cloud-based%20AI%0Aagents%20is%20transforming%20human-AI%20collaboration%20into%20bidirectional%2C%20multimodal%0Ainteraction.%20However%2C%20existing%20codecs%20remain%20optimized%20for%20unimodal%2C%20one-way%0Acommunication%2C%20resulting%20in%20repeated%20degradation%20under%20conventional%0Acompress-transmit-reconstruct%20pipelines.%20To%20address%20this%20limitation%2C%20we%20propose%0AUniMIC%2C%20a%20Unified%20token-based%20Multimodal%20Interactive%20Coding%20framework%20that%0Abridges%20edge%20devices%20and%20cloud%20AI%20agents.%20Instead%20of%20transmitting%20raw%20pixels%20or%0Aplain%20text%2C%20UniMIC%20employs%20compact%20tokenized%20representations%20as%20the%0Acommunication%20medium%2C%20enabling%20efficient%20low-bitrate%20transmission%20while%0Amaintaining%20compatibility%20with%20LMMs.%20To%20further%20enhance%20compression%2C%0Alightweight%20Transformer-based%20entropy%20models%20with%20scenario-specific%0Adesigns-generic%2C%20masked%2C%20and%20text-conditioned-effectively%20minimize%20inter-token%0Aredundancy.%20Extensive%20experiments%20on%20text-to-image%20generation%2C%20text-guided%0Ainpainting%2C%20outpainting%2C%20and%20visual%20question%20answering%20show%20that%20UniMIC%0Aachieves%20substantial%20bitrate%20savings%20and%20remains%20robust%20even%20at%20ultra-low%0Abitrates%20%28%3C0.05bpp%29%2C%20without%20compromising%20downstream%20task%20performance.%20These%0Aresults%20establish%20UniMIC%20as%20a%20practical%20and%20forward-looking%20paradigm%20for%0Anext-generation%20multimodal%20interactive%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22570v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniMIC%253A%2520Token-Based%2520Multimodal%2520Interactive%2520Coding%2520for%2520Human-AI%250A%2520%2520Collaboration%26entry.906535625%3DQi%2520Mao%2520and%2520Tinghan%2520Yang%2520and%2520Jiahao%2520Li%2520and%2520Bin%2520Li%2520and%2520Libiao%2520Jin%2520and%2520Yan%2520Lu%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520and%2520cloud-based%2520AI%250Aagents%2520is%2520transforming%2520human-AI%2520collaboration%2520into%2520bidirectional%252C%2520multimodal%250Ainteraction.%2520However%252C%2520existing%2520codecs%2520remain%2520optimized%2520for%2520unimodal%252C%2520one-way%250Acommunication%252C%2520resulting%2520in%2520repeated%2520degradation%2520under%2520conventional%250Acompress-transmit-reconstruct%2520pipelines.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250AUniMIC%252C%2520a%2520Unified%2520token-based%2520Multimodal%2520Interactive%2520Coding%2520framework%2520that%250Abridges%2520edge%2520devices%2520and%2520cloud%2520AI%2520agents.%2520Instead%2520of%2520transmitting%2520raw%2520pixels%2520or%250Aplain%2520text%252C%2520UniMIC%2520employs%2520compact%2520tokenized%2520representations%2520as%2520the%250Acommunication%2520medium%252C%2520enabling%2520efficient%2520low-bitrate%2520transmission%2520while%250Amaintaining%2520compatibility%2520with%2520LMMs.%2520To%2520further%2520enhance%2520compression%252C%250Alightweight%2520Transformer-based%2520entropy%2520models%2520with%2520scenario-specific%250Adesigns-generic%252C%2520masked%252C%2520and%2520text-conditioned-effectively%2520minimize%2520inter-token%250Aredundancy.%2520Extensive%2520experiments%2520on%2520text-to-image%2520generation%252C%2520text-guided%250Ainpainting%252C%2520outpainting%252C%2520and%2520visual%2520question%2520answering%2520show%2520that%2520UniMIC%250Aachieves%2520substantial%2520bitrate%2520savings%2520and%2520remains%2520robust%2520even%2520at%2520ultra-low%250Abitrates%2520%2528%253C0.05bpp%2529%252C%2520without%2520compromising%2520downstream%2520task%2520performance.%2520These%250Aresults%2520establish%2520UniMIC%2520as%2520a%2520practical%2520and%2520forward-looking%2520paradigm%2520for%250Anext-generation%2520multimodal%2520interactive%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22570v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniMIC%3A%20Token-Based%20Multimodal%20Interactive%20Coding%20for%20Human-AI%0A%20%20Collaboration&entry.906535625=Qi%20Mao%20and%20Tinghan%20Yang%20and%20Jiahao%20Li%20and%20Bin%20Li%20and%20Libiao%20Jin%20and%20Yan%20Lu&entry.1292438233=%20%20The%20rapid%20progress%20of%20Large%20Multimodal%20Models%20%28LMMs%29%20and%20cloud-based%20AI%0Aagents%20is%20transforming%20human-AI%20collaboration%20into%20bidirectional%2C%20multimodal%0Ainteraction.%20However%2C%20existing%20codecs%20remain%20optimized%20for%20unimodal%2C%20one-way%0Acommunication%2C%20resulting%20in%20repeated%20degradation%20under%20conventional%0Acompress-transmit-reconstruct%20pipelines.%20To%20address%20this%20limitation%2C%20we%20propose%0AUniMIC%2C%20a%20Unified%20token-based%20Multimodal%20Interactive%20Coding%20framework%20that%0Abridges%20edge%20devices%20and%20cloud%20AI%20agents.%20Instead%20of%20transmitting%20raw%20pixels%20or%0Aplain%20text%2C%20UniMIC%20employs%20compact%20tokenized%20representations%20as%20the%0Acommunication%20medium%2C%20enabling%20efficient%20low-bitrate%20transmission%20while%0Amaintaining%20compatibility%20with%20LMMs.%20To%20further%20enhance%20compression%2C%0Alightweight%20Transformer-based%20entropy%20models%20with%20scenario-specific%0Adesigns-generic%2C%20masked%2C%20and%20text-conditioned-effectively%20minimize%20inter-token%0Aredundancy.%20Extensive%20experiments%20on%20text-to-image%20generation%2C%20text-guided%0Ainpainting%2C%20outpainting%2C%20and%20visual%20question%20answering%20show%20that%20UniMIC%0Aachieves%20substantial%20bitrate%20savings%20and%20remains%20robust%20even%20at%20ultra-low%0Abitrates%20%28%3C0.05bpp%29%2C%20without%20compromising%20downstream%20task%20performance.%20These%0Aresults%20establish%20UniMIC%20as%20a%20practical%20and%20forward-looking%20paradigm%20for%0Anext-generation%20multimodal%20interactive%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22570v1&entry.124074799=Read"},
{"title": "Multi-Agent Path Finding via Offline RL and LLM Collaboration", "author": "Merve Atasever and Matthew Hong and Mihir Nitin Kulkarni and Qingpei Li and Jyotirmoy V. Deshmukh", "abstract": "  Multi-Agent Path Finding (MAPF) poses a significant and challenging problem\ncritical for applications in robotics and logistics, particularly due to its\ncombinatorial complexity and the partial observability inherent in realistic\nenvironments. Decentralized reinforcement learning methods commonly encounter\ntwo substantial difficulties: first, they often yield self-centered behaviors\namong agents, resulting in frequent collisions, and second, their reliance on\ncomplex communication modules leads to prolonged training times, sometimes\nspanning weeks. To address these challenges, we propose an efficient\ndecentralized planning framework based on the Decision Transformer (DT),\nuniquely leveraging offline reinforcement learning to substantially reduce\ntraining durations from weeks to mere hours. Crucially, our approach\neffectively handles long-horizon credit assignment and significantly improves\nperformance in scenarios with sparse and delayed rewards. Furthermore, to\novercome adaptability limitations inherent in standard RL methods under dynamic\nenvironmental changes, we integrate a large language model (GPT-4o) to\ndynamically guide agent policies. Extensive experiments in both static and\ndynamically changing environments demonstrate that our DT-based approach,\naugmented briefly by GPT-4o, significantly enhances adaptability and\nperformance.\n", "link": "http://arxiv.org/abs/2509.22130v1", "date": "2025-09-26", "relevancy": 2.2069, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5637}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5537}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Path%20Finding%20via%20Offline%20RL%20and%20LLM%20Collaboration&body=Title%3A%20Multi-Agent%20Path%20Finding%20via%20Offline%20RL%20and%20LLM%20Collaboration%0AAuthor%3A%20Merve%20Atasever%20and%20Matthew%20Hong%20and%20Mihir%20Nitin%20Kulkarni%20and%20Qingpei%20Li%20and%20Jyotirmoy%20V.%20Deshmukh%0AAbstract%3A%20%20%20Multi-Agent%20Path%20Finding%20%28MAPF%29%20poses%20a%20significant%20and%20challenging%20problem%0Acritical%20for%20applications%20in%20robotics%20and%20logistics%2C%20particularly%20due%20to%20its%0Acombinatorial%20complexity%20and%20the%20partial%20observability%20inherent%20in%20realistic%0Aenvironments.%20Decentralized%20reinforcement%20learning%20methods%20commonly%20encounter%0Atwo%20substantial%20difficulties%3A%20first%2C%20they%20often%20yield%20self-centered%20behaviors%0Aamong%20agents%2C%20resulting%20in%20frequent%20collisions%2C%20and%20second%2C%20their%20reliance%20on%0Acomplex%20communication%20modules%20leads%20to%20prolonged%20training%20times%2C%20sometimes%0Aspanning%20weeks.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20efficient%0Adecentralized%20planning%20framework%20based%20on%20the%20Decision%20Transformer%20%28DT%29%2C%0Auniquely%20leveraging%20offline%20reinforcement%20learning%20to%20substantially%20reduce%0Atraining%20durations%20from%20weeks%20to%20mere%20hours.%20Crucially%2C%20our%20approach%0Aeffectively%20handles%20long-horizon%20credit%20assignment%20and%20significantly%20improves%0Aperformance%20in%20scenarios%20with%20sparse%20and%20delayed%20rewards.%20Furthermore%2C%20to%0Aovercome%20adaptability%20limitations%20inherent%20in%20standard%20RL%20methods%20under%20dynamic%0Aenvironmental%20changes%2C%20we%20integrate%20a%20large%20language%20model%20%28GPT-4o%29%20to%0Adynamically%20guide%20agent%20policies.%20Extensive%20experiments%20in%20both%20static%20and%0Adynamically%20changing%20environments%20demonstrate%20that%20our%20DT-based%20approach%2C%0Aaugmented%20briefly%20by%20GPT-4o%2C%20significantly%20enhances%20adaptability%20and%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Path%2520Finding%2520via%2520Offline%2520RL%2520and%2520LLM%2520Collaboration%26entry.906535625%3DMerve%2520Atasever%2520and%2520Matthew%2520Hong%2520and%2520Mihir%2520Nitin%2520Kulkarni%2520and%2520Qingpei%2520Li%2520and%2520Jyotirmoy%2520V.%2520Deshmukh%26entry.1292438233%3D%2520%2520Multi-Agent%2520Path%2520Finding%2520%2528MAPF%2529%2520poses%2520a%2520significant%2520and%2520challenging%2520problem%250Acritical%2520for%2520applications%2520in%2520robotics%2520and%2520logistics%252C%2520particularly%2520due%2520to%2520its%250Acombinatorial%2520complexity%2520and%2520the%2520partial%2520observability%2520inherent%2520in%2520realistic%250Aenvironments.%2520Decentralized%2520reinforcement%2520learning%2520methods%2520commonly%2520encounter%250Atwo%2520substantial%2520difficulties%253A%2520first%252C%2520they%2520often%2520yield%2520self-centered%2520behaviors%250Aamong%2520agents%252C%2520resulting%2520in%2520frequent%2520collisions%252C%2520and%2520second%252C%2520their%2520reliance%2520on%250Acomplex%2520communication%2520modules%2520leads%2520to%2520prolonged%2520training%2520times%252C%2520sometimes%250Aspanning%2520weeks.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520efficient%250Adecentralized%2520planning%2520framework%2520based%2520on%2520the%2520Decision%2520Transformer%2520%2528DT%2529%252C%250Auniquely%2520leveraging%2520offline%2520reinforcement%2520learning%2520to%2520substantially%2520reduce%250Atraining%2520durations%2520from%2520weeks%2520to%2520mere%2520hours.%2520Crucially%252C%2520our%2520approach%250Aeffectively%2520handles%2520long-horizon%2520credit%2520assignment%2520and%2520significantly%2520improves%250Aperformance%2520in%2520scenarios%2520with%2520sparse%2520and%2520delayed%2520rewards.%2520Furthermore%252C%2520to%250Aovercome%2520adaptability%2520limitations%2520inherent%2520in%2520standard%2520RL%2520methods%2520under%2520dynamic%250Aenvironmental%2520changes%252C%2520we%2520integrate%2520a%2520large%2520language%2520model%2520%2528GPT-4o%2529%2520to%250Adynamically%2520guide%2520agent%2520policies.%2520Extensive%2520experiments%2520in%2520both%2520static%2520and%250Adynamically%2520changing%2520environments%2520demonstrate%2520that%2520our%2520DT-based%2520approach%252C%250Aaugmented%2520briefly%2520by%2520GPT-4o%252C%2520significantly%2520enhances%2520adaptability%2520and%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Path%20Finding%20via%20Offline%20RL%20and%20LLM%20Collaboration&entry.906535625=Merve%20Atasever%20and%20Matthew%20Hong%20and%20Mihir%20Nitin%20Kulkarni%20and%20Qingpei%20Li%20and%20Jyotirmoy%20V.%20Deshmukh&entry.1292438233=%20%20Multi-Agent%20Path%20Finding%20%28MAPF%29%20poses%20a%20significant%20and%20challenging%20problem%0Acritical%20for%20applications%20in%20robotics%20and%20logistics%2C%20particularly%20due%20to%20its%0Acombinatorial%20complexity%20and%20the%20partial%20observability%20inherent%20in%20realistic%0Aenvironments.%20Decentralized%20reinforcement%20learning%20methods%20commonly%20encounter%0Atwo%20substantial%20difficulties%3A%20first%2C%20they%20often%20yield%20self-centered%20behaviors%0Aamong%20agents%2C%20resulting%20in%20frequent%20collisions%2C%20and%20second%2C%20their%20reliance%20on%0Acomplex%20communication%20modules%20leads%20to%20prolonged%20training%20times%2C%20sometimes%0Aspanning%20weeks.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20efficient%0Adecentralized%20planning%20framework%20based%20on%20the%20Decision%20Transformer%20%28DT%29%2C%0Auniquely%20leveraging%20offline%20reinforcement%20learning%20to%20substantially%20reduce%0Atraining%20durations%20from%20weeks%20to%20mere%20hours.%20Crucially%2C%20our%20approach%0Aeffectively%20handles%20long-horizon%20credit%20assignment%20and%20significantly%20improves%0Aperformance%20in%20scenarios%20with%20sparse%20and%20delayed%20rewards.%20Furthermore%2C%20to%0Aovercome%20adaptability%20limitations%20inherent%20in%20standard%20RL%20methods%20under%20dynamic%0Aenvironmental%20changes%2C%20we%20integrate%20a%20large%20language%20model%20%28GPT-4o%29%20to%0Adynamically%20guide%20agent%20policies.%20Extensive%20experiments%20in%20both%20static%20and%0Adynamically%20changing%20environments%20demonstrate%20that%20our%20DT-based%20approach%2C%0Aaugmented%20briefly%20by%20GPT-4o%2C%20significantly%20enhances%20adaptability%20and%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22130v1&entry.124074799=Read"},
{"title": "Lightweight error mitigation strategies for post-training N:M activation\n  sparsity in LLMs", "author": "Shirin Alanova and Kristina Kazistova and Ekaterina Galaeva and Alina Kostromina and Vladimir Smirnov and Redko Dmitry and Alexey Dontsov and Maxim Zhelnin and Evgeny Burnaev and Egor Shvetsov", "abstract": "  The demand for efficient large language model (LLM) inference has intensified\nthe focus on sparsification techniques. While semi-structured (N:M) pruning is\nwell-established for weights, its application to activation pruning remains\nunderexplored despite its potential for dynamic, input-adaptive compression and\nreductions in I/O overhead. This work presents a comprehensive analysis of\nmethods for post-training N:M activation pruning in LLMs. Across multiple LLMs,\nwe demonstrate that pruning activations enables superior preservation of\ngenerative capabilities compared to weight pruning at equivalent sparsity\nlevels. We evaluate lightweight, plug-and-play error mitigation techniques and\npruning criteria, establishing strong hardware-friendly baselines that require\nminimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's\nstandard 2:4, showing that the 16:32 pattern achieves performance nearly on par\nwith unstructured sparsity. However, considering the trade-off between\nflexibility and hardware implementation complexity, we focus on the 8:16\npattern as a superior candidate. Our findings provide both effective practical\nmethods for activation pruning and a motivation for future hardware to support\nmore flexible sparsity patterns. Our code is available\nhttps://anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md .\n", "link": "http://arxiv.org/abs/2509.22166v1", "date": "2025-09-26", "relevancy": 1.8399, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4601}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20error%20mitigation%20strategies%20for%20post-training%20N%3AM%20activation%0A%20%20sparsity%20in%20LLMs&body=Title%3A%20Lightweight%20error%20mitigation%20strategies%20for%20post-training%20N%3AM%20activation%0A%20%20sparsity%20in%20LLMs%0AAuthor%3A%20Shirin%20Alanova%20and%20Kristina%20Kazistova%20and%20Ekaterina%20Galaeva%20and%20Alina%20Kostromina%20and%20Vladimir%20Smirnov%20and%20Redko%20Dmitry%20and%20Alexey%20Dontsov%20and%20Maxim%20Zhelnin%20and%20Evgeny%20Burnaev%20and%20Egor%20Shvetsov%0AAbstract%3A%20%20%20The%20demand%20for%20efficient%20large%20language%20model%20%28LLM%29%20inference%20has%20intensified%0Athe%20focus%20on%20sparsification%20techniques.%20While%20semi-structured%20%28N%3AM%29%20pruning%20is%0Awell-established%20for%20weights%2C%20its%20application%20to%20activation%20pruning%20remains%0Aunderexplored%20despite%20its%20potential%20for%20dynamic%2C%20input-adaptive%20compression%20and%0Areductions%20in%20I/O%20overhead.%20This%20work%20presents%20a%20comprehensive%20analysis%20of%0Amethods%20for%20post-training%20N%3AM%20activation%20pruning%20in%20LLMs.%20Across%20multiple%20LLMs%2C%0Awe%20demonstrate%20that%20pruning%20activations%20enables%20superior%20preservation%20of%0Agenerative%20capabilities%20compared%20to%20weight%20pruning%20at%20equivalent%20sparsity%0Alevels.%20We%20evaluate%20lightweight%2C%20plug-and-play%20error%20mitigation%20techniques%20and%0Apruning%20criteria%2C%20establishing%20strong%20hardware-friendly%20baselines%20that%20require%0Aminimal%20calibration.%20Furthermore%2C%20we%20explore%20sparsity%20patterns%20beyond%20NVIDIA%27s%0Astandard%202%3A4%2C%20showing%20that%20the%2016%3A32%20pattern%20achieves%20performance%20nearly%20on%20par%0Awith%20unstructured%20sparsity.%20However%2C%20considering%20the%20trade-off%20between%0Aflexibility%20and%20hardware%20implementation%20complexity%2C%20we%20focus%20on%20the%208%3A16%0Apattern%20as%20a%20superior%20candidate.%20Our%20findings%20provide%20both%20effective%20practical%0Amethods%20for%20activation%20pruning%20and%20a%20motivation%20for%20future%20hardware%20to%20support%0Amore%20flexible%20sparsity%20patterns.%20Our%20code%20is%20available%0Ahttps%3A//anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520error%2520mitigation%2520strategies%2520for%2520post-training%2520N%253AM%2520activation%250A%2520%2520sparsity%2520in%2520LLMs%26entry.906535625%3DShirin%2520Alanova%2520and%2520Kristina%2520Kazistova%2520and%2520Ekaterina%2520Galaeva%2520and%2520Alina%2520Kostromina%2520and%2520Vladimir%2520Smirnov%2520and%2520Redko%2520Dmitry%2520and%2520Alexey%2520Dontsov%2520and%2520Maxim%2520Zhelnin%2520and%2520Evgeny%2520Burnaev%2520and%2520Egor%2520Shvetsov%26entry.1292438233%3D%2520%2520The%2520demand%2520for%2520efficient%2520large%2520language%2520model%2520%2528LLM%2529%2520inference%2520has%2520intensified%250Athe%2520focus%2520on%2520sparsification%2520techniques.%2520While%2520semi-structured%2520%2528N%253AM%2529%2520pruning%2520is%250Awell-established%2520for%2520weights%252C%2520its%2520application%2520to%2520activation%2520pruning%2520remains%250Aunderexplored%2520despite%2520its%2520potential%2520for%2520dynamic%252C%2520input-adaptive%2520compression%2520and%250Areductions%2520in%2520I/O%2520overhead.%2520This%2520work%2520presents%2520a%2520comprehensive%2520analysis%2520of%250Amethods%2520for%2520post-training%2520N%253AM%2520activation%2520pruning%2520in%2520LLMs.%2520Across%2520multiple%2520LLMs%252C%250Awe%2520demonstrate%2520that%2520pruning%2520activations%2520enables%2520superior%2520preservation%2520of%250Agenerative%2520capabilities%2520compared%2520to%2520weight%2520pruning%2520at%2520equivalent%2520sparsity%250Alevels.%2520We%2520evaluate%2520lightweight%252C%2520plug-and-play%2520error%2520mitigation%2520techniques%2520and%250Apruning%2520criteria%252C%2520establishing%2520strong%2520hardware-friendly%2520baselines%2520that%2520require%250Aminimal%2520calibration.%2520Furthermore%252C%2520we%2520explore%2520sparsity%2520patterns%2520beyond%2520NVIDIA%2527s%250Astandard%25202%253A4%252C%2520showing%2520that%2520the%252016%253A32%2520pattern%2520achieves%2520performance%2520nearly%2520on%2520par%250Awith%2520unstructured%2520sparsity.%2520However%252C%2520considering%2520the%2520trade-off%2520between%250Aflexibility%2520and%2520hardware%2520implementation%2520complexity%252C%2520we%2520focus%2520on%2520the%25208%253A16%250Apattern%2520as%2520a%2520superior%2520candidate.%2520Our%2520findings%2520provide%2520both%2520effective%2520practical%250Amethods%2520for%2520activation%2520pruning%2520and%2520a%2520motivation%2520for%2520future%2520hardware%2520to%2520support%250Amore%2520flexible%2520sparsity%2520patterns.%2520Our%2520code%2520is%2520available%250Ahttps%253A//anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20error%20mitigation%20strategies%20for%20post-training%20N%3AM%20activation%0A%20%20sparsity%20in%20LLMs&entry.906535625=Shirin%20Alanova%20and%20Kristina%20Kazistova%20and%20Ekaterina%20Galaeva%20and%20Alina%20Kostromina%20and%20Vladimir%20Smirnov%20and%20Redko%20Dmitry%20and%20Alexey%20Dontsov%20and%20Maxim%20Zhelnin%20and%20Evgeny%20Burnaev%20and%20Egor%20Shvetsov&entry.1292438233=%20%20The%20demand%20for%20efficient%20large%20language%20model%20%28LLM%29%20inference%20has%20intensified%0Athe%20focus%20on%20sparsification%20techniques.%20While%20semi-structured%20%28N%3AM%29%20pruning%20is%0Awell-established%20for%20weights%2C%20its%20application%20to%20activation%20pruning%20remains%0Aunderexplored%20despite%20its%20potential%20for%20dynamic%2C%20input-adaptive%20compression%20and%0Areductions%20in%20I/O%20overhead.%20This%20work%20presents%20a%20comprehensive%20analysis%20of%0Amethods%20for%20post-training%20N%3AM%20activation%20pruning%20in%20LLMs.%20Across%20multiple%20LLMs%2C%0Awe%20demonstrate%20that%20pruning%20activations%20enables%20superior%20preservation%20of%0Agenerative%20capabilities%20compared%20to%20weight%20pruning%20at%20equivalent%20sparsity%0Alevels.%20We%20evaluate%20lightweight%2C%20plug-and-play%20error%20mitigation%20techniques%20and%0Apruning%20criteria%2C%20establishing%20strong%20hardware-friendly%20baselines%20that%20require%0Aminimal%20calibration.%20Furthermore%2C%20we%20explore%20sparsity%20patterns%20beyond%20NVIDIA%27s%0Astandard%202%3A4%2C%20showing%20that%20the%2016%3A32%20pattern%20achieves%20performance%20nearly%20on%20par%0Awith%20unstructured%20sparsity.%20However%2C%20considering%20the%20trade-off%20between%0Aflexibility%20and%20hardware%20implementation%20complexity%2C%20we%20focus%20on%20the%208%3A16%0Apattern%20as%20a%20superior%20candidate.%20Our%20findings%20provide%20both%20effective%20practical%0Amethods%20for%20activation%20pruning%20and%20a%20motivation%20for%20future%20hardware%20to%20support%0Amore%20flexible%20sparsity%20patterns.%20Our%20code%20is%20available%0Ahttps%3A//anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22166v1&entry.124074799=Read"},
{"title": "Nearly Tight Regret Bounds for Profit Maximization in Bilateral Trade", "author": "Simone Di Gregorio and Paul D\u00fctting and Federico Fusco and Chris Schwiegelshohn", "abstract": "  Bilateral trade models the task of intermediating between two strategic\nagents, a seller and a buyer, willing to trade a good for which they hold\nprivate valuations. We study this problem from the perspective of a broker, in\na regret minimization framework. At each time step, a new seller and buyer\narrive, and the broker has to propose a mechanism that is incentive-compatible\nand individually rational, with the goal of maximizing profit.\n  We propose a learning algorithm that guarantees a nearly tight\n$\\tilde{O}(\\sqrt{T})$ regret in the stochastic setting when seller and buyer\nvaluations are drawn i.i.d. from a fixed and possibly correlated unknown\ndistribution. We further show that it is impossible to achieve sublinear regret\nin the non-stationary scenario where valuations are generated upfront by an\nadversary. Our ambitious benchmark for these results is the best\nincentive-compatible and individually rational mechanism. This separates us\nfrom previous works on efficiency maximization in bilateral trade, where the\nbenchmark is a single number: the best fixed price in hindsight.\n  A particular challenge we face is that uniform convergence for all\nmechanisms' profits is impossible. We overcome this difficulty via a careful\nchaining analysis that proves convergence for a provably near-optimal mechanism\nat (essentially) optimal rate. We further showcase the broader applicability of\nour techniques by providing nearly optimal results for the joint ads problem.\n", "link": "http://arxiv.org/abs/2509.22563v1", "date": "2025-09-26", "relevancy": 1.6647, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4414}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4112}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nearly%20Tight%20Regret%20Bounds%20for%20Profit%20Maximization%20in%20Bilateral%20Trade&body=Title%3A%20Nearly%20Tight%20Regret%20Bounds%20for%20Profit%20Maximization%20in%20Bilateral%20Trade%0AAuthor%3A%20Simone%20Di%20Gregorio%20and%20Paul%20D%C3%BCtting%20and%20Federico%20Fusco%20and%20Chris%20Schwiegelshohn%0AAbstract%3A%20%20%20Bilateral%20trade%20models%20the%20task%20of%20intermediating%20between%20two%20strategic%0Aagents%2C%20a%20seller%20and%20a%20buyer%2C%20willing%20to%20trade%20a%20good%20for%20which%20they%20hold%0Aprivate%20valuations.%20We%20study%20this%20problem%20from%20the%20perspective%20of%20a%20broker%2C%20in%0Aa%20regret%20minimization%20framework.%20At%20each%20time%20step%2C%20a%20new%20seller%20and%20buyer%0Aarrive%2C%20and%20the%20broker%20has%20to%20propose%20a%20mechanism%20that%20is%20incentive-compatible%0Aand%20individually%20rational%2C%20with%20the%20goal%20of%20maximizing%20profit.%0A%20%20We%20propose%20a%20learning%20algorithm%20that%20guarantees%20a%20nearly%20tight%0A%24%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret%20in%20the%20stochastic%20setting%20when%20seller%20and%20buyer%0Avaluations%20are%20drawn%20i.i.d.%20from%20a%20fixed%20and%20possibly%20correlated%20unknown%0Adistribution.%20We%20further%20show%20that%20it%20is%20impossible%20to%20achieve%20sublinear%20regret%0Ain%20the%20non-stationary%20scenario%20where%20valuations%20are%20generated%20upfront%20by%20an%0Aadversary.%20Our%20ambitious%20benchmark%20for%20these%20results%20is%20the%20best%0Aincentive-compatible%20and%20individually%20rational%20mechanism.%20This%20separates%20us%0Afrom%20previous%20works%20on%20efficiency%20maximization%20in%20bilateral%20trade%2C%20where%20the%0Abenchmark%20is%20a%20single%20number%3A%20the%20best%20fixed%20price%20in%20hindsight.%0A%20%20A%20particular%20challenge%20we%20face%20is%20that%20uniform%20convergence%20for%20all%0Amechanisms%27%20profits%20is%20impossible.%20We%20overcome%20this%20difficulty%20via%20a%20careful%0Achaining%20analysis%20that%20proves%20convergence%20for%20a%20provably%20near-optimal%20mechanism%0Aat%20%28essentially%29%20optimal%20rate.%20We%20further%20showcase%20the%20broader%20applicability%20of%0Aour%20techniques%20by%20providing%20nearly%20optimal%20results%20for%20the%20joint%20ads%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22563v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNearly%2520Tight%2520Regret%2520Bounds%2520for%2520Profit%2520Maximization%2520in%2520Bilateral%2520Trade%26entry.906535625%3DSimone%2520Di%2520Gregorio%2520and%2520Paul%2520D%25C3%25BCtting%2520and%2520Federico%2520Fusco%2520and%2520Chris%2520Schwiegelshohn%26entry.1292438233%3D%2520%2520Bilateral%2520trade%2520models%2520the%2520task%2520of%2520intermediating%2520between%2520two%2520strategic%250Aagents%252C%2520a%2520seller%2520and%2520a%2520buyer%252C%2520willing%2520to%2520trade%2520a%2520good%2520for%2520which%2520they%2520hold%250Aprivate%2520valuations.%2520We%2520study%2520this%2520problem%2520from%2520the%2520perspective%2520of%2520a%2520broker%252C%2520in%250Aa%2520regret%2520minimization%2520framework.%2520At%2520each%2520time%2520step%252C%2520a%2520new%2520seller%2520and%2520buyer%250Aarrive%252C%2520and%2520the%2520broker%2520has%2520to%2520propose%2520a%2520mechanism%2520that%2520is%2520incentive-compatible%250Aand%2520individually%2520rational%252C%2520with%2520the%2520goal%2520of%2520maximizing%2520profit.%250A%2520%2520We%2520propose%2520a%2520learning%2520algorithm%2520that%2520guarantees%2520a%2520nearly%2520tight%250A%2524%255Ctilde%257BO%257D%2528%255Csqrt%257BT%257D%2529%2524%2520regret%2520in%2520the%2520stochastic%2520setting%2520when%2520seller%2520and%2520buyer%250Avaluations%2520are%2520drawn%2520i.i.d.%2520from%2520a%2520fixed%2520and%2520possibly%2520correlated%2520unknown%250Adistribution.%2520We%2520further%2520show%2520that%2520it%2520is%2520impossible%2520to%2520achieve%2520sublinear%2520regret%250Ain%2520the%2520non-stationary%2520scenario%2520where%2520valuations%2520are%2520generated%2520upfront%2520by%2520an%250Aadversary.%2520Our%2520ambitious%2520benchmark%2520for%2520these%2520results%2520is%2520the%2520best%250Aincentive-compatible%2520and%2520individually%2520rational%2520mechanism.%2520This%2520separates%2520us%250Afrom%2520previous%2520works%2520on%2520efficiency%2520maximization%2520in%2520bilateral%2520trade%252C%2520where%2520the%250Abenchmark%2520is%2520a%2520single%2520number%253A%2520the%2520best%2520fixed%2520price%2520in%2520hindsight.%250A%2520%2520A%2520particular%2520challenge%2520we%2520face%2520is%2520that%2520uniform%2520convergence%2520for%2520all%250Amechanisms%2527%2520profits%2520is%2520impossible.%2520We%2520overcome%2520this%2520difficulty%2520via%2520a%2520careful%250Achaining%2520analysis%2520that%2520proves%2520convergence%2520for%2520a%2520provably%2520near-optimal%2520mechanism%250Aat%2520%2528essentially%2529%2520optimal%2520rate.%2520We%2520further%2520showcase%2520the%2520broader%2520applicability%2520of%250Aour%2520techniques%2520by%2520providing%2520nearly%2520optimal%2520results%2520for%2520the%2520joint%2520ads%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22563v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearly%20Tight%20Regret%20Bounds%20for%20Profit%20Maximization%20in%20Bilateral%20Trade&entry.906535625=Simone%20Di%20Gregorio%20and%20Paul%20D%C3%BCtting%20and%20Federico%20Fusco%20and%20Chris%20Schwiegelshohn&entry.1292438233=%20%20Bilateral%20trade%20models%20the%20task%20of%20intermediating%20between%20two%20strategic%0Aagents%2C%20a%20seller%20and%20a%20buyer%2C%20willing%20to%20trade%20a%20good%20for%20which%20they%20hold%0Aprivate%20valuations.%20We%20study%20this%20problem%20from%20the%20perspective%20of%20a%20broker%2C%20in%0Aa%20regret%20minimization%20framework.%20At%20each%20time%20step%2C%20a%20new%20seller%20and%20buyer%0Aarrive%2C%20and%20the%20broker%20has%20to%20propose%20a%20mechanism%20that%20is%20incentive-compatible%0Aand%20individually%20rational%2C%20with%20the%20goal%20of%20maximizing%20profit.%0A%20%20We%20propose%20a%20learning%20algorithm%20that%20guarantees%20a%20nearly%20tight%0A%24%5Ctilde%7BO%7D%28%5Csqrt%7BT%7D%29%24%20regret%20in%20the%20stochastic%20setting%20when%20seller%20and%20buyer%0Avaluations%20are%20drawn%20i.i.d.%20from%20a%20fixed%20and%20possibly%20correlated%20unknown%0Adistribution.%20We%20further%20show%20that%20it%20is%20impossible%20to%20achieve%20sublinear%20regret%0Ain%20the%20non-stationary%20scenario%20where%20valuations%20are%20generated%20upfront%20by%20an%0Aadversary.%20Our%20ambitious%20benchmark%20for%20these%20results%20is%20the%20best%0Aincentive-compatible%20and%20individually%20rational%20mechanism.%20This%20separates%20us%0Afrom%20previous%20works%20on%20efficiency%20maximization%20in%20bilateral%20trade%2C%20where%20the%0Abenchmark%20is%20a%20single%20number%3A%20the%20best%20fixed%20price%20in%20hindsight.%0A%20%20A%20particular%20challenge%20we%20face%20is%20that%20uniform%20convergence%20for%20all%0Amechanisms%27%20profits%20is%20impossible.%20We%20overcome%20this%20difficulty%20via%20a%20careful%0Achaining%20analysis%20that%20proves%20convergence%20for%20a%20provably%20near-optimal%20mechanism%0Aat%20%28essentially%29%20optimal%20rate.%20We%20further%20showcase%20the%20broader%20applicability%20of%0Aour%20techniques%20by%20providing%20nearly%20optimal%20results%20for%20the%20joint%20ads%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22563v1&entry.124074799=Read"},
{"title": "SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival\n  Analysis", "author": "Marie Brockschmidt and Maresa Schr\u00f6der and Stefan Feuerriegel", "abstract": "  Survival analysis is a cornerstone of clinical research by modeling\ntime-to-event outcomes such as metastasis, disease relapse, or patient death.\nUnlike standard tabular data, survival data often come with incomplete event\ninformation due to dropout, or loss to follow-up. This poses unique challenges\nfor synthetic data generation, where it is crucial for clinical research to\nfaithfully reproduce both the event-time distribution and the censoring\nmechanism. In this paper, we propose SurvDiff, an end-to-end diffusion model\nspecifically designed for generating synthetic data in survival analysis.\nSurvDiff is tailored to capture the data-generating mechanism by jointly\ngenerating mixed-type covariates, event times, and right-censoring, guided by a\nsurvival-tailored loss function. The loss encodes the time-to-event structure\nand directly optimizes for downstream survival tasks, which ensures that\nSurvDiff (i) reproduces realistic event-time distributions and (ii) preserves\nthe censoring mechanism. Across multiple datasets, we show that \\survdiff\nconsistently outperforms state-of-the-art generative baselines in both\ndistributional fidelity and downstream evaluation metrics across multiple\nmedical datasets. To the best of our knowledge, SurvDiff is the first diffusion\nmodel explicitly designed for generating synthetic survival data.\n", "link": "http://arxiv.org/abs/2509.22352v1", "date": "2025-09-26", "relevancy": 2.0524, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5326}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5071}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurvDiff%3A%20A%20Diffusion%20Model%20for%20Generating%20Synthetic%20Data%20in%20Survival%0A%20%20Analysis&body=Title%3A%20SurvDiff%3A%20A%20Diffusion%20Model%20for%20Generating%20Synthetic%20Data%20in%20Survival%0A%20%20Analysis%0AAuthor%3A%20Marie%20Brockschmidt%20and%20Maresa%20Schr%C3%B6der%20and%20Stefan%20Feuerriegel%0AAbstract%3A%20%20%20Survival%20analysis%20is%20a%20cornerstone%20of%20clinical%20research%20by%20modeling%0Atime-to-event%20outcomes%20such%20as%20metastasis%2C%20disease%20relapse%2C%20or%20patient%20death.%0AUnlike%20standard%20tabular%20data%2C%20survival%20data%20often%20come%20with%20incomplete%20event%0Ainformation%20due%20to%20dropout%2C%20or%20loss%20to%20follow-up.%20This%20poses%20unique%20challenges%0Afor%20synthetic%20data%20generation%2C%20where%20it%20is%20crucial%20for%20clinical%20research%20to%0Afaithfully%20reproduce%20both%20the%20event-time%20distribution%20and%20the%20censoring%0Amechanism.%20In%20this%20paper%2C%20we%20propose%20SurvDiff%2C%20an%20end-to-end%20diffusion%20model%0Aspecifically%20designed%20for%20generating%20synthetic%20data%20in%20survival%20analysis.%0ASurvDiff%20is%20tailored%20to%20capture%20the%20data-generating%20mechanism%20by%20jointly%0Agenerating%20mixed-type%20covariates%2C%20event%20times%2C%20and%20right-censoring%2C%20guided%20by%20a%0Asurvival-tailored%20loss%20function.%20The%20loss%20encodes%20the%20time-to-event%20structure%0Aand%20directly%20optimizes%20for%20downstream%20survival%20tasks%2C%20which%20ensures%20that%0ASurvDiff%20%28i%29%20reproduces%20realistic%20event-time%20distributions%20and%20%28ii%29%20preserves%0Athe%20censoring%20mechanism.%20Across%20multiple%20datasets%2C%20we%20show%20that%20%5Csurvdiff%0Aconsistently%20outperforms%20state-of-the-art%20generative%20baselines%20in%20both%0Adistributional%20fidelity%20and%20downstream%20evaluation%20metrics%20across%20multiple%0Amedical%20datasets.%20To%20the%20best%20of%20our%20knowledge%2C%20SurvDiff%20is%20the%20first%20diffusion%0Amodel%20explicitly%20designed%20for%20generating%20synthetic%20survival%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurvDiff%253A%2520A%2520Diffusion%2520Model%2520for%2520Generating%2520Synthetic%2520Data%2520in%2520Survival%250A%2520%2520Analysis%26entry.906535625%3DMarie%2520Brockschmidt%2520and%2520Maresa%2520Schr%25C3%25B6der%2520and%2520Stefan%2520Feuerriegel%26entry.1292438233%3D%2520%2520Survival%2520analysis%2520is%2520a%2520cornerstone%2520of%2520clinical%2520research%2520by%2520modeling%250Atime-to-event%2520outcomes%2520such%2520as%2520metastasis%252C%2520disease%2520relapse%252C%2520or%2520patient%2520death.%250AUnlike%2520standard%2520tabular%2520data%252C%2520survival%2520data%2520often%2520come%2520with%2520incomplete%2520event%250Ainformation%2520due%2520to%2520dropout%252C%2520or%2520loss%2520to%2520follow-up.%2520This%2520poses%2520unique%2520challenges%250Afor%2520synthetic%2520data%2520generation%252C%2520where%2520it%2520is%2520crucial%2520for%2520clinical%2520research%2520to%250Afaithfully%2520reproduce%2520both%2520the%2520event-time%2520distribution%2520and%2520the%2520censoring%250Amechanism.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SurvDiff%252C%2520an%2520end-to-end%2520diffusion%2520model%250Aspecifically%2520designed%2520for%2520generating%2520synthetic%2520data%2520in%2520survival%2520analysis.%250ASurvDiff%2520is%2520tailored%2520to%2520capture%2520the%2520data-generating%2520mechanism%2520by%2520jointly%250Agenerating%2520mixed-type%2520covariates%252C%2520event%2520times%252C%2520and%2520right-censoring%252C%2520guided%2520by%2520a%250Asurvival-tailored%2520loss%2520function.%2520The%2520loss%2520encodes%2520the%2520time-to-event%2520structure%250Aand%2520directly%2520optimizes%2520for%2520downstream%2520survival%2520tasks%252C%2520which%2520ensures%2520that%250ASurvDiff%2520%2528i%2529%2520reproduces%2520realistic%2520event-time%2520distributions%2520and%2520%2528ii%2529%2520preserves%250Athe%2520censoring%2520mechanism.%2520Across%2520multiple%2520datasets%252C%2520we%2520show%2520that%2520%255Csurvdiff%250Aconsistently%2520outperforms%2520state-of-the-art%2520generative%2520baselines%2520in%2520both%250Adistributional%2520fidelity%2520and%2520downstream%2520evaluation%2520metrics%2520across%2520multiple%250Amedical%2520datasets.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520SurvDiff%2520is%2520the%2520first%2520diffusion%250Amodel%2520explicitly%2520designed%2520for%2520generating%2520synthetic%2520survival%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurvDiff%3A%20A%20Diffusion%20Model%20for%20Generating%20Synthetic%20Data%20in%20Survival%0A%20%20Analysis&entry.906535625=Marie%20Brockschmidt%20and%20Maresa%20Schr%C3%B6der%20and%20Stefan%20Feuerriegel&entry.1292438233=%20%20Survival%20analysis%20is%20a%20cornerstone%20of%20clinical%20research%20by%20modeling%0Atime-to-event%20outcomes%20such%20as%20metastasis%2C%20disease%20relapse%2C%20or%20patient%20death.%0AUnlike%20standard%20tabular%20data%2C%20survival%20data%20often%20come%20with%20incomplete%20event%0Ainformation%20due%20to%20dropout%2C%20or%20loss%20to%20follow-up.%20This%20poses%20unique%20challenges%0Afor%20synthetic%20data%20generation%2C%20where%20it%20is%20crucial%20for%20clinical%20research%20to%0Afaithfully%20reproduce%20both%20the%20event-time%20distribution%20and%20the%20censoring%0Amechanism.%20In%20this%20paper%2C%20we%20propose%20SurvDiff%2C%20an%20end-to-end%20diffusion%20model%0Aspecifically%20designed%20for%20generating%20synthetic%20data%20in%20survival%20analysis.%0ASurvDiff%20is%20tailored%20to%20capture%20the%20data-generating%20mechanism%20by%20jointly%0Agenerating%20mixed-type%20covariates%2C%20event%20times%2C%20and%20right-censoring%2C%20guided%20by%20a%0Asurvival-tailored%20loss%20function.%20The%20loss%20encodes%20the%20time-to-event%20structure%0Aand%20directly%20optimizes%20for%20downstream%20survival%20tasks%2C%20which%20ensures%20that%0ASurvDiff%20%28i%29%20reproduces%20realistic%20event-time%20distributions%20and%20%28ii%29%20preserves%0Athe%20censoring%20mechanism.%20Across%20multiple%20datasets%2C%20we%20show%20that%20%5Csurvdiff%0Aconsistently%20outperforms%20state-of-the-art%20generative%20baselines%20in%20both%0Adistributional%20fidelity%20and%20downstream%20evaluation%20metrics%20across%20multiple%0Amedical%20datasets.%20To%20the%20best%20of%20our%20knowledge%2C%20SurvDiff%20is%20the%20first%20diffusion%0Amodel%20explicitly%20designed%20for%20generating%20synthetic%20survival%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22352v1&entry.124074799=Read"},
{"title": "The need for and feasibility of alternative ground robots to traverse\n  sandy and rocky extraterrestrial terrain", "author": "Chen Li and Kevin Lewis", "abstract": "  Robotic spacecraft have helped expand our reach for many planetary\nexploration missions. Most ground mobile planetary exploration robots use\nwheeled or modified wheeled platforms. Although extraordinarily successful at\ncompleting intended mission goals, because of the limitations of wheeled\nlocomotion, they have been largely limited to benign, solid terrain and avoided\nextreme terrain with loose soil/sand and large rocks. Unfortunately, such\nchallenging terrain is often scientifically interesting for planetary geology.\nAlthough many animals traverse such terrain at ease, robots have not matched\ntheir performance and robustness. This is in major part due to a lack of\nfundamental understanding of how effective locomotion can be generated from\ncontrolled interaction with complex terrain on the same level of flight\naerodynamics and underwater vehicle hydrodynamics. Early fundamental\nunderstanding of legged and limbless locomotor-ground interaction has already\nenabled stable and efficient bio-inspired robot locomotion on relatively flat\nground with small obstacles. Recent progress in the new field of terradynamics\nof locomotor-terrain interaction begins to reveal the principles of\nbio-inspired locomotion on loose soil/sand and over large obstacles.\nMulti-legged and limbless platforms using terradynamics insights hold the\npromise for serving as robust alternative platforms for traversing extreme\nextraterrestrial terrain and expanding our reach in planetary exploration.\n", "link": "http://arxiv.org/abs/2201.11984v3", "date": "2025-09-26", "relevancy": 1.4835, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5344}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5088}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20need%20for%20and%20feasibility%20of%20alternative%20ground%20robots%20to%20traverse%0A%20%20sandy%20and%20rocky%20extraterrestrial%20terrain&body=Title%3A%20The%20need%20for%20and%20feasibility%20of%20alternative%20ground%20robots%20to%20traverse%0A%20%20sandy%20and%20rocky%20extraterrestrial%20terrain%0AAuthor%3A%20Chen%20Li%20and%20Kevin%20Lewis%0AAbstract%3A%20%20%20Robotic%20spacecraft%20have%20helped%20expand%20our%20reach%20for%20many%20planetary%0Aexploration%20missions.%20Most%20ground%20mobile%20planetary%20exploration%20robots%20use%0Awheeled%20or%20modified%20wheeled%20platforms.%20Although%20extraordinarily%20successful%20at%0Acompleting%20intended%20mission%20goals%2C%20because%20of%20the%20limitations%20of%20wheeled%0Alocomotion%2C%20they%20have%20been%20largely%20limited%20to%20benign%2C%20solid%20terrain%20and%20avoided%0Aextreme%20terrain%20with%20loose%20soil/sand%20and%20large%20rocks.%20Unfortunately%2C%20such%0Achallenging%20terrain%20is%20often%20scientifically%20interesting%20for%20planetary%20geology.%0AAlthough%20many%20animals%20traverse%20such%20terrain%20at%20ease%2C%20robots%20have%20not%20matched%0Atheir%20performance%20and%20robustness.%20This%20is%20in%20major%20part%20due%20to%20a%20lack%20of%0Afundamental%20understanding%20of%20how%20effective%20locomotion%20can%20be%20generated%20from%0Acontrolled%20interaction%20with%20complex%20terrain%20on%20the%20same%20level%20of%20flight%0Aaerodynamics%20and%20underwater%20vehicle%20hydrodynamics.%20Early%20fundamental%0Aunderstanding%20of%20legged%20and%20limbless%20locomotor-ground%20interaction%20has%20already%0Aenabled%20stable%20and%20efficient%20bio-inspired%20robot%20locomotion%20on%20relatively%20flat%0Aground%20with%20small%20obstacles.%20Recent%20progress%20in%20the%20new%20field%20of%20terradynamics%0Aof%20locomotor-terrain%20interaction%20begins%20to%20reveal%20the%20principles%20of%0Abio-inspired%20locomotion%20on%20loose%20soil/sand%20and%20over%20large%20obstacles.%0AMulti-legged%20and%20limbless%20platforms%20using%20terradynamics%20insights%20hold%20the%0Apromise%20for%20serving%20as%20robust%20alternative%20platforms%20for%20traversing%20extreme%0Aextraterrestrial%20terrain%20and%20expanding%20our%20reach%20in%20planetary%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.11984v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520need%2520for%2520and%2520feasibility%2520of%2520alternative%2520ground%2520robots%2520to%2520traverse%250A%2520%2520sandy%2520and%2520rocky%2520extraterrestrial%2520terrain%26entry.906535625%3DChen%2520Li%2520and%2520Kevin%2520Lewis%26entry.1292438233%3D%2520%2520Robotic%2520spacecraft%2520have%2520helped%2520expand%2520our%2520reach%2520for%2520many%2520planetary%250Aexploration%2520missions.%2520Most%2520ground%2520mobile%2520planetary%2520exploration%2520robots%2520use%250Awheeled%2520or%2520modified%2520wheeled%2520platforms.%2520Although%2520extraordinarily%2520successful%2520at%250Acompleting%2520intended%2520mission%2520goals%252C%2520because%2520of%2520the%2520limitations%2520of%2520wheeled%250Alocomotion%252C%2520they%2520have%2520been%2520largely%2520limited%2520to%2520benign%252C%2520solid%2520terrain%2520and%2520avoided%250Aextreme%2520terrain%2520with%2520loose%2520soil/sand%2520and%2520large%2520rocks.%2520Unfortunately%252C%2520such%250Achallenging%2520terrain%2520is%2520often%2520scientifically%2520interesting%2520for%2520planetary%2520geology.%250AAlthough%2520many%2520animals%2520traverse%2520such%2520terrain%2520at%2520ease%252C%2520robots%2520have%2520not%2520matched%250Atheir%2520performance%2520and%2520robustness.%2520This%2520is%2520in%2520major%2520part%2520due%2520to%2520a%2520lack%2520of%250Afundamental%2520understanding%2520of%2520how%2520effective%2520locomotion%2520can%2520be%2520generated%2520from%250Acontrolled%2520interaction%2520with%2520complex%2520terrain%2520on%2520the%2520same%2520level%2520of%2520flight%250Aaerodynamics%2520and%2520underwater%2520vehicle%2520hydrodynamics.%2520Early%2520fundamental%250Aunderstanding%2520of%2520legged%2520and%2520limbless%2520locomotor-ground%2520interaction%2520has%2520already%250Aenabled%2520stable%2520and%2520efficient%2520bio-inspired%2520robot%2520locomotion%2520on%2520relatively%2520flat%250Aground%2520with%2520small%2520obstacles.%2520Recent%2520progress%2520in%2520the%2520new%2520field%2520of%2520terradynamics%250Aof%2520locomotor-terrain%2520interaction%2520begins%2520to%2520reveal%2520the%2520principles%2520of%250Abio-inspired%2520locomotion%2520on%2520loose%2520soil/sand%2520and%2520over%2520large%2520obstacles.%250AMulti-legged%2520and%2520limbless%2520platforms%2520using%2520terradynamics%2520insights%2520hold%2520the%250Apromise%2520for%2520serving%2520as%2520robust%2520alternative%2520platforms%2520for%2520traversing%2520extreme%250Aextraterrestrial%2520terrain%2520and%2520expanding%2520our%2520reach%2520in%2520planetary%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.11984v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20need%20for%20and%20feasibility%20of%20alternative%20ground%20robots%20to%20traverse%0A%20%20sandy%20and%20rocky%20extraterrestrial%20terrain&entry.906535625=Chen%20Li%20and%20Kevin%20Lewis&entry.1292438233=%20%20Robotic%20spacecraft%20have%20helped%20expand%20our%20reach%20for%20many%20planetary%0Aexploration%20missions.%20Most%20ground%20mobile%20planetary%20exploration%20robots%20use%0Awheeled%20or%20modified%20wheeled%20platforms.%20Although%20extraordinarily%20successful%20at%0Acompleting%20intended%20mission%20goals%2C%20because%20of%20the%20limitations%20of%20wheeled%0Alocomotion%2C%20they%20have%20been%20largely%20limited%20to%20benign%2C%20solid%20terrain%20and%20avoided%0Aextreme%20terrain%20with%20loose%20soil/sand%20and%20large%20rocks.%20Unfortunately%2C%20such%0Achallenging%20terrain%20is%20often%20scientifically%20interesting%20for%20planetary%20geology.%0AAlthough%20many%20animals%20traverse%20such%20terrain%20at%20ease%2C%20robots%20have%20not%20matched%0Atheir%20performance%20and%20robustness.%20This%20is%20in%20major%20part%20due%20to%20a%20lack%20of%0Afundamental%20understanding%20of%20how%20effective%20locomotion%20can%20be%20generated%20from%0Acontrolled%20interaction%20with%20complex%20terrain%20on%20the%20same%20level%20of%20flight%0Aaerodynamics%20and%20underwater%20vehicle%20hydrodynamics.%20Early%20fundamental%0Aunderstanding%20of%20legged%20and%20limbless%20locomotor-ground%20interaction%20has%20already%0Aenabled%20stable%20and%20efficient%20bio-inspired%20robot%20locomotion%20on%20relatively%20flat%0Aground%20with%20small%20obstacles.%20Recent%20progress%20in%20the%20new%20field%20of%20terradynamics%0Aof%20locomotor-terrain%20interaction%20begins%20to%20reveal%20the%20principles%20of%0Abio-inspired%20locomotion%20on%20loose%20soil/sand%20and%20over%20large%20obstacles.%0AMulti-legged%20and%20limbless%20platforms%20using%20terradynamics%20insights%20hold%20the%0Apromise%20for%20serving%20as%20robust%20alternative%20platforms%20for%20traversing%20extreme%0Aextraterrestrial%20terrain%20and%20expanding%20our%20reach%20in%20planetary%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.11984v3&entry.124074799=Read"},
{"title": "Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an\n  Internet of Robotic Things Paradigm", "author": "Joseph Hunt and Koyo Fujii and Aly Magassouba and Praminda Caleb-Solly", "abstract": "  Hospital patient falls remain a critical and costly challenge worldwide.\nWhile conventional fall prevention systems typically rely on post-fall\ndetection or reactive alerts, they also often suffer from high false positive\nrates and fail to address the underlying patient needs that lead to bed-exit\nattempts. This paper presents a novel system architecture that leverages the\nInternet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction\nfor proactive and personalized patient assistance. The system integrates a\nprivacy-preserving thermal sensing model capable of real-time bed-exit\nprediction, with two coordinated robotic agents that respond dynamically based\non predicted intent and patient input. This orchestrated response could not\nonly reduce fall risk but also attend to the patient's underlying motivations\nfor movement, such as thirst, discomfort, or the need for assistance, before a\nhazardous situation arises. Our contributions with this pilot study are\nthree-fold: (1) a modular IoRT-based framework enabling distributed sensing,\nprediction, and multi-robot coordination; (2) a demonstration of low-resolution\nthermal sensing for accurate, privacy-preserving preemptive bed-exit detection;\nand (3) results from a user study and systematic error analysis that inform the\ndesign of situationally aware, multi-agent interactions in hospital settings.\nThe findings highlight how interactive and connected robotic systems can move\nbeyond passive monitoring to deliver timely, meaningful assistance, empowering\nsafer, more responsive care environments.\n", "link": "http://arxiv.org/abs/2509.22296v1", "date": "2025-09-26", "relevancy": 1.7013, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5781}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5579}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Detection%20--%20Orchestrating%20Human-Robot-Robot%20Assistance%20via%20an%0A%20%20Internet%20of%20Robotic%20Things%20Paradigm&body=Title%3A%20Beyond%20Detection%20--%20Orchestrating%20Human-Robot-Robot%20Assistance%20via%20an%0A%20%20Internet%20of%20Robotic%20Things%20Paradigm%0AAuthor%3A%20Joseph%20Hunt%20and%20Koyo%20Fujii%20and%20Aly%20Magassouba%20and%20Praminda%20Caleb-Solly%0AAbstract%3A%20%20%20Hospital%20patient%20falls%20remain%20a%20critical%20and%20costly%20challenge%20worldwide.%0AWhile%20conventional%20fall%20prevention%20systems%20typically%20rely%20on%20post-fall%0Adetection%20or%20reactive%20alerts%2C%20they%20also%20often%20suffer%20from%20high%20false%20positive%0Arates%20and%20fail%20to%20address%20the%20underlying%20patient%20needs%20that%20lead%20to%20bed-exit%0Aattempts.%20This%20paper%20presents%20a%20novel%20system%20architecture%20that%20leverages%20the%0AInternet%20of%20Robotic%20Things%20%28IoRT%29%20to%20orchestrate%20human-robot-robot%20interaction%0Afor%20proactive%20and%20personalized%20patient%20assistance.%20The%20system%20integrates%20a%0Aprivacy-preserving%20thermal%20sensing%20model%20capable%20of%20real-time%20bed-exit%0Aprediction%2C%20with%20two%20coordinated%20robotic%20agents%20that%20respond%20dynamically%20based%0Aon%20predicted%20intent%20and%20patient%20input.%20This%20orchestrated%20response%20could%20not%0Aonly%20reduce%20fall%20risk%20but%20also%20attend%20to%20the%20patient%27s%20underlying%20motivations%0Afor%20movement%2C%20such%20as%20thirst%2C%20discomfort%2C%20or%20the%20need%20for%20assistance%2C%20before%20a%0Ahazardous%20situation%20arises.%20Our%20contributions%20with%20this%20pilot%20study%20are%0Athree-fold%3A%20%281%29%20a%20modular%20IoRT-based%20framework%20enabling%20distributed%20sensing%2C%0Aprediction%2C%20and%20multi-robot%20coordination%3B%20%282%29%20a%20demonstration%20of%20low-resolution%0Athermal%20sensing%20for%20accurate%2C%20privacy-preserving%20preemptive%20bed-exit%20detection%3B%0Aand%20%283%29%20results%20from%20a%20user%20study%20and%20systematic%20error%20analysis%20that%20inform%20the%0Adesign%20of%20situationally%20aware%2C%20multi-agent%20interactions%20in%20hospital%20settings.%0AThe%20findings%20highlight%20how%20interactive%20and%20connected%20robotic%20systems%20can%20move%0Abeyond%20passive%20monitoring%20to%20deliver%20timely%2C%20meaningful%20assistance%2C%20empowering%0Asafer%2C%20more%20responsive%20care%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22296v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Detection%2520--%2520Orchestrating%2520Human-Robot-Robot%2520Assistance%2520via%2520an%250A%2520%2520Internet%2520of%2520Robotic%2520Things%2520Paradigm%26entry.906535625%3DJoseph%2520Hunt%2520and%2520Koyo%2520Fujii%2520and%2520Aly%2520Magassouba%2520and%2520Praminda%2520Caleb-Solly%26entry.1292438233%3D%2520%2520Hospital%2520patient%2520falls%2520remain%2520a%2520critical%2520and%2520costly%2520challenge%2520worldwide.%250AWhile%2520conventional%2520fall%2520prevention%2520systems%2520typically%2520rely%2520on%2520post-fall%250Adetection%2520or%2520reactive%2520alerts%252C%2520they%2520also%2520often%2520suffer%2520from%2520high%2520false%2520positive%250Arates%2520and%2520fail%2520to%2520address%2520the%2520underlying%2520patient%2520needs%2520that%2520lead%2520to%2520bed-exit%250Aattempts.%2520This%2520paper%2520presents%2520a%2520novel%2520system%2520architecture%2520that%2520leverages%2520the%250AInternet%2520of%2520Robotic%2520Things%2520%2528IoRT%2529%2520to%2520orchestrate%2520human-robot-robot%2520interaction%250Afor%2520proactive%2520and%2520personalized%2520patient%2520assistance.%2520The%2520system%2520integrates%2520a%250Aprivacy-preserving%2520thermal%2520sensing%2520model%2520capable%2520of%2520real-time%2520bed-exit%250Aprediction%252C%2520with%2520two%2520coordinated%2520robotic%2520agents%2520that%2520respond%2520dynamically%2520based%250Aon%2520predicted%2520intent%2520and%2520patient%2520input.%2520This%2520orchestrated%2520response%2520could%2520not%250Aonly%2520reduce%2520fall%2520risk%2520but%2520also%2520attend%2520to%2520the%2520patient%2527s%2520underlying%2520motivations%250Afor%2520movement%252C%2520such%2520as%2520thirst%252C%2520discomfort%252C%2520or%2520the%2520need%2520for%2520assistance%252C%2520before%2520a%250Ahazardous%2520situation%2520arises.%2520Our%2520contributions%2520with%2520this%2520pilot%2520study%2520are%250Athree-fold%253A%2520%25281%2529%2520a%2520modular%2520IoRT-based%2520framework%2520enabling%2520distributed%2520sensing%252C%250Aprediction%252C%2520and%2520multi-robot%2520coordination%253B%2520%25282%2529%2520a%2520demonstration%2520of%2520low-resolution%250Athermal%2520sensing%2520for%2520accurate%252C%2520privacy-preserving%2520preemptive%2520bed-exit%2520detection%253B%250Aand%2520%25283%2529%2520results%2520from%2520a%2520user%2520study%2520and%2520systematic%2520error%2520analysis%2520that%2520inform%2520the%250Adesign%2520of%2520situationally%2520aware%252C%2520multi-agent%2520interactions%2520in%2520hospital%2520settings.%250AThe%2520findings%2520highlight%2520how%2520interactive%2520and%2520connected%2520robotic%2520systems%2520can%2520move%250Abeyond%2520passive%2520monitoring%2520to%2520deliver%2520timely%252C%2520meaningful%2520assistance%252C%2520empowering%250Asafer%252C%2520more%2520responsive%2520care%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22296v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Detection%20--%20Orchestrating%20Human-Robot-Robot%20Assistance%20via%20an%0A%20%20Internet%20of%20Robotic%20Things%20Paradigm&entry.906535625=Joseph%20Hunt%20and%20Koyo%20Fujii%20and%20Aly%20Magassouba%20and%20Praminda%20Caleb-Solly&entry.1292438233=%20%20Hospital%20patient%20falls%20remain%20a%20critical%20and%20costly%20challenge%20worldwide.%0AWhile%20conventional%20fall%20prevention%20systems%20typically%20rely%20on%20post-fall%0Adetection%20or%20reactive%20alerts%2C%20they%20also%20often%20suffer%20from%20high%20false%20positive%0Arates%20and%20fail%20to%20address%20the%20underlying%20patient%20needs%20that%20lead%20to%20bed-exit%0Aattempts.%20This%20paper%20presents%20a%20novel%20system%20architecture%20that%20leverages%20the%0AInternet%20of%20Robotic%20Things%20%28IoRT%29%20to%20orchestrate%20human-robot-robot%20interaction%0Afor%20proactive%20and%20personalized%20patient%20assistance.%20The%20system%20integrates%20a%0Aprivacy-preserving%20thermal%20sensing%20model%20capable%20of%20real-time%20bed-exit%0Aprediction%2C%20with%20two%20coordinated%20robotic%20agents%20that%20respond%20dynamically%20based%0Aon%20predicted%20intent%20and%20patient%20input.%20This%20orchestrated%20response%20could%20not%0Aonly%20reduce%20fall%20risk%20but%20also%20attend%20to%20the%20patient%27s%20underlying%20motivations%0Afor%20movement%2C%20such%20as%20thirst%2C%20discomfort%2C%20or%20the%20need%20for%20assistance%2C%20before%20a%0Ahazardous%20situation%20arises.%20Our%20contributions%20with%20this%20pilot%20study%20are%0Athree-fold%3A%20%281%29%20a%20modular%20IoRT-based%20framework%20enabling%20distributed%20sensing%2C%0Aprediction%2C%20and%20multi-robot%20coordination%3B%20%282%29%20a%20demonstration%20of%20low-resolution%0Athermal%20sensing%20for%20accurate%2C%20privacy-preserving%20preemptive%20bed-exit%20detection%3B%0Aand%20%283%29%20results%20from%20a%20user%20study%20and%20systematic%20error%20analysis%20that%20inform%20the%0Adesign%20of%20situationally%20aware%2C%20multi-agent%20interactions%20in%20hospital%20settings.%0AThe%20findings%20highlight%20how%20interactive%20and%20connected%20robotic%20systems%20can%20move%0Abeyond%20passive%20monitoring%20to%20deliver%20timely%2C%20meaningful%20assistance%2C%20empowering%0Asafer%2C%20more%20responsive%20care%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22296v1&entry.124074799=Read"},
{"title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from\n  Multiple Perspectives", "author": "Ayoung Lee and Ryan Sungmo Kwon and Peter Railton and Lu Wang", "abstract": "  Navigating dilemmas involving conflicting values is challenging even for\nhumans in high-stakes domains, let alone for AI, yet prior work has been\nlimited to everyday scenarios. To close this gap, we introduce CLASH (Character\nperspective-based LLM Assessments in Situations with High-stakes), a\nmeticulously curated dataset consisting of 345 high-impact dilemmas along with\n3,795 individual perspectives of diverse values. CLASH enables the study of\ncritical yet underexplored aspects of value-based decision-making processes,\nincluding understanding of decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in the perspectives of\ncharacters. By benchmarking 14 non-thinking and thinking models, we uncover\nseveral key findings. (1) Even strong proprietary models, such as GPT-5 and\nClaude-4-Sonnet, struggle with ambivalent decisions, achieving only 24.06 and\n51.01 accuracy. (2) Although LLMs reasonably predict psychological discomfort,\nthey do not adequately comprehend perspectives involving value shifts. (3)\nCognitive behaviors that are effective in the math-solving and game strategy\ndomains do not transfer to value reasoning. Instead, new failure patterns\nemerge, including early commitment and overcommitment. (4) The steerability of\nLLMs towards a given value is significantly correlated with their value\npreferences. (5) Finally, LLMs exhibit greater steerability when reasoning from\na third-party perspective, although certain values (e.g., safety) benefit\nuniquely from first-person framing.\n", "link": "http://arxiv.org/abs/2504.10823v3", "date": "2025-09-26", "relevancy": 2.0359, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5153}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLASH%3A%20Evaluating%20Language%20Models%20on%20Judging%20High-Stakes%20Dilemmas%20from%0A%20%20Multiple%20Perspectives&body=Title%3A%20CLASH%3A%20Evaluating%20Language%20Models%20on%20Judging%20High-Stakes%20Dilemmas%20from%0A%20%20Multiple%20Perspectives%0AAuthor%3A%20Ayoung%20Lee%20and%20Ryan%20Sungmo%20Kwon%20and%20Peter%20Railton%20and%20Lu%20Wang%0AAbstract%3A%20%20%20Navigating%20dilemmas%20involving%20conflicting%20values%20is%20challenging%20even%20for%0Ahumans%20in%20high-stakes%20domains%2C%20let%20alone%20for%20AI%2C%20yet%20prior%20work%20has%20been%0Alimited%20to%20everyday%20scenarios.%20To%20close%20this%20gap%2C%20we%20introduce%20CLASH%20%28Character%0Aperspective-based%20LLM%20Assessments%20in%20Situations%20with%20High-stakes%29%2C%20a%0Ameticulously%20curated%20dataset%20consisting%20of%20345%20high-impact%20dilemmas%20along%20with%0A3%2C795%20individual%20perspectives%20of%20diverse%20values.%20CLASH%20enables%20the%20study%20of%0Acritical%20yet%20underexplored%20aspects%20of%20value-based%20decision-making%20processes%2C%0Aincluding%20understanding%20of%20decision%20ambivalence%20and%20psychological%20discomfort%20as%0Awell%20as%20capturing%20the%20temporal%20shifts%20of%20values%20in%20the%20perspectives%20of%0Acharacters.%20By%20benchmarking%2014%20non-thinking%20and%20thinking%20models%2C%20we%20uncover%0Aseveral%20key%20findings.%20%281%29%20Even%20strong%20proprietary%20models%2C%20such%20as%20GPT-5%20and%0AClaude-4-Sonnet%2C%20struggle%20with%20ambivalent%20decisions%2C%20achieving%20only%2024.06%20and%0A51.01%20accuracy.%20%282%29%20Although%20LLMs%20reasonably%20predict%20psychological%20discomfort%2C%0Athey%20do%20not%20adequately%20comprehend%20perspectives%20involving%20value%20shifts.%20%283%29%0ACognitive%20behaviors%20that%20are%20effective%20in%20the%20math-solving%20and%20game%20strategy%0Adomains%20do%20not%20transfer%20to%20value%20reasoning.%20Instead%2C%20new%20failure%20patterns%0Aemerge%2C%20including%20early%20commitment%20and%20overcommitment.%20%284%29%20The%20steerability%20of%0ALLMs%20towards%20a%20given%20value%20is%20significantly%20correlated%20with%20their%20value%0Apreferences.%20%285%29%20Finally%2C%20LLMs%20exhibit%20greater%20steerability%20when%20reasoning%20from%0Aa%20third-party%20perspective%2C%20although%20certain%20values%20%28e.g.%2C%20safety%29%20benefit%0Auniquely%20from%20first-person%20framing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.10823v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLASH%253A%2520Evaluating%2520Language%2520Models%2520on%2520Judging%2520High-Stakes%2520Dilemmas%2520from%250A%2520%2520Multiple%2520Perspectives%26entry.906535625%3DAyoung%2520Lee%2520and%2520Ryan%2520Sungmo%2520Kwon%2520and%2520Peter%2520Railton%2520and%2520Lu%2520Wang%26entry.1292438233%3D%2520%2520Navigating%2520dilemmas%2520involving%2520conflicting%2520values%2520is%2520challenging%2520even%2520for%250Ahumans%2520in%2520high-stakes%2520domains%252C%2520let%2520alone%2520for%2520AI%252C%2520yet%2520prior%2520work%2520has%2520been%250Alimited%2520to%2520everyday%2520scenarios.%2520To%2520close%2520this%2520gap%252C%2520we%2520introduce%2520CLASH%2520%2528Character%250Aperspective-based%2520LLM%2520Assessments%2520in%2520Situations%2520with%2520High-stakes%2529%252C%2520a%250Ameticulously%2520curated%2520dataset%2520consisting%2520of%2520345%2520high-impact%2520dilemmas%2520along%2520with%250A3%252C795%2520individual%2520perspectives%2520of%2520diverse%2520values.%2520CLASH%2520enables%2520the%2520study%2520of%250Acritical%2520yet%2520underexplored%2520aspects%2520of%2520value-based%2520decision-making%2520processes%252C%250Aincluding%2520understanding%2520of%2520decision%2520ambivalence%2520and%2520psychological%2520discomfort%2520as%250Awell%2520as%2520capturing%2520the%2520temporal%2520shifts%2520of%2520values%2520in%2520the%2520perspectives%2520of%250Acharacters.%2520By%2520benchmarking%252014%2520non-thinking%2520and%2520thinking%2520models%252C%2520we%2520uncover%250Aseveral%2520key%2520findings.%2520%25281%2529%2520Even%2520strong%2520proprietary%2520models%252C%2520such%2520as%2520GPT-5%2520and%250AClaude-4-Sonnet%252C%2520struggle%2520with%2520ambivalent%2520decisions%252C%2520achieving%2520only%252024.06%2520and%250A51.01%2520accuracy.%2520%25282%2529%2520Although%2520LLMs%2520reasonably%2520predict%2520psychological%2520discomfort%252C%250Athey%2520do%2520not%2520adequately%2520comprehend%2520perspectives%2520involving%2520value%2520shifts.%2520%25283%2529%250ACognitive%2520behaviors%2520that%2520are%2520effective%2520in%2520the%2520math-solving%2520and%2520game%2520strategy%250Adomains%2520do%2520not%2520transfer%2520to%2520value%2520reasoning.%2520Instead%252C%2520new%2520failure%2520patterns%250Aemerge%252C%2520including%2520early%2520commitment%2520and%2520overcommitment.%2520%25284%2529%2520The%2520steerability%2520of%250ALLMs%2520towards%2520a%2520given%2520value%2520is%2520significantly%2520correlated%2520with%2520their%2520value%250Apreferences.%2520%25285%2529%2520Finally%252C%2520LLMs%2520exhibit%2520greater%2520steerability%2520when%2520reasoning%2520from%250Aa%2520third-party%2520perspective%252C%2520although%2520certain%2520values%2520%2528e.g.%252C%2520safety%2529%2520benefit%250Auniquely%2520from%2520first-person%2520framing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.10823v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLASH%3A%20Evaluating%20Language%20Models%20on%20Judging%20High-Stakes%20Dilemmas%20from%0A%20%20Multiple%20Perspectives&entry.906535625=Ayoung%20Lee%20and%20Ryan%20Sungmo%20Kwon%20and%20Peter%20Railton%20and%20Lu%20Wang&entry.1292438233=%20%20Navigating%20dilemmas%20involving%20conflicting%20values%20is%20challenging%20even%20for%0Ahumans%20in%20high-stakes%20domains%2C%20let%20alone%20for%20AI%2C%20yet%20prior%20work%20has%20been%0Alimited%20to%20everyday%20scenarios.%20To%20close%20this%20gap%2C%20we%20introduce%20CLASH%20%28Character%0Aperspective-based%20LLM%20Assessments%20in%20Situations%20with%20High-stakes%29%2C%20a%0Ameticulously%20curated%20dataset%20consisting%20of%20345%20high-impact%20dilemmas%20along%20with%0A3%2C795%20individual%20perspectives%20of%20diverse%20values.%20CLASH%20enables%20the%20study%20of%0Acritical%20yet%20underexplored%20aspects%20of%20value-based%20decision-making%20processes%2C%0Aincluding%20understanding%20of%20decision%20ambivalence%20and%20psychological%20discomfort%20as%0Awell%20as%20capturing%20the%20temporal%20shifts%20of%20values%20in%20the%20perspectives%20of%0Acharacters.%20By%20benchmarking%2014%20non-thinking%20and%20thinking%20models%2C%20we%20uncover%0Aseveral%20key%20findings.%20%281%29%20Even%20strong%20proprietary%20models%2C%20such%20as%20GPT-5%20and%0AClaude-4-Sonnet%2C%20struggle%20with%20ambivalent%20decisions%2C%20achieving%20only%2024.06%20and%0A51.01%20accuracy.%20%282%29%20Although%20LLMs%20reasonably%20predict%20psychological%20discomfort%2C%0Athey%20do%20not%20adequately%20comprehend%20perspectives%20involving%20value%20shifts.%20%283%29%0ACognitive%20behaviors%20that%20are%20effective%20in%20the%20math-solving%20and%20game%20strategy%0Adomains%20do%20not%20transfer%20to%20value%20reasoning.%20Instead%2C%20new%20failure%20patterns%0Aemerge%2C%20including%20early%20commitment%20and%20overcommitment.%20%284%29%20The%20steerability%20of%0ALLMs%20towards%20a%20given%20value%20is%20significantly%20correlated%20with%20their%20value%0Apreferences.%20%285%29%20Finally%2C%20LLMs%20exhibit%20greater%20steerability%20when%20reasoning%20from%0Aa%20third-party%20perspective%2C%20although%20certain%20values%20%28e.g.%2C%20safety%29%20benefit%0Auniquely%20from%20first-person%20framing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.10823v3&entry.124074799=Read"},
{"title": "GraphSCENE: On-Demand Critical Scenario Generation for Autonomous\n  Vehicles in Simulation", "author": "Efimia Panagiotaki and Georgi Pramatarov and Lars Kunze and Daniele De Martini", "abstract": "  Testing and validating Autonomous Vehicle (AV) performance in safety-critical\nand diverse scenarios is crucial before real-world deployment. However,\nmanually creating such scenarios in simulation remains a significant and\ntime-consuming challenge. This work introduces a novel method that generates\ndynamic temporal scene graphs corresponding to diverse traffic scenarios,\non-demand, tailored to user-defined preferences, such as AV actions, sets of\ndynamic agents, and criticality levels. A temporal Graph Neural Network (GNN)\nmodel learns to predict relationships between ego-vehicle, agents, and static\nstructures, guided by real-world spatiotemporal interaction patterns and\nconstrained by an ontology that restricts predictions to semantically valid\nlinks. Our model consistently outperforms the baselines in accurately\ngenerating links corresponding to the requested scenarios. We render the\npredicted scenarios in simulation to further demonstrate their effectiveness as\ntesting environments for AV agents.\n", "link": "http://arxiv.org/abs/2410.13514v3", "date": "2025-09-26", "relevancy": 1.1375, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6023}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5696}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphSCENE%3A%20On-Demand%20Critical%20Scenario%20Generation%20for%20Autonomous%0A%20%20Vehicles%20in%20Simulation&body=Title%3A%20GraphSCENE%3A%20On-Demand%20Critical%20Scenario%20Generation%20for%20Autonomous%0A%20%20Vehicles%20in%20Simulation%0AAuthor%3A%20Efimia%20Panagiotaki%20and%20Georgi%20Pramatarov%20and%20Lars%20Kunze%20and%20Daniele%20De%20Martini%0AAbstract%3A%20%20%20Testing%20and%20validating%20Autonomous%20Vehicle%20%28AV%29%20performance%20in%20safety-critical%0Aand%20diverse%20scenarios%20is%20crucial%20before%20real-world%20deployment.%20However%2C%0Amanually%20creating%20such%20scenarios%20in%20simulation%20remains%20a%20significant%20and%0Atime-consuming%20challenge.%20This%20work%20introduces%20a%20novel%20method%20that%20generates%0Adynamic%20temporal%20scene%20graphs%20corresponding%20to%20diverse%20traffic%20scenarios%2C%0Aon-demand%2C%20tailored%20to%20user-defined%20preferences%2C%20such%20as%20AV%20actions%2C%20sets%20of%0Adynamic%20agents%2C%20and%20criticality%20levels.%20A%20temporal%20Graph%20Neural%20Network%20%28GNN%29%0Amodel%20learns%20to%20predict%20relationships%20between%20ego-vehicle%2C%20agents%2C%20and%20static%0Astructures%2C%20guided%20by%20real-world%20spatiotemporal%20interaction%20patterns%20and%0Aconstrained%20by%20an%20ontology%20that%20restricts%20predictions%20to%20semantically%20valid%0Alinks.%20Our%20model%20consistently%20outperforms%20the%20baselines%20in%20accurately%0Agenerating%20links%20corresponding%20to%20the%20requested%20scenarios.%20We%20render%20the%0Apredicted%20scenarios%20in%20simulation%20to%20further%20demonstrate%20their%20effectiveness%20as%0Atesting%20environments%20for%20AV%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13514v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphSCENE%253A%2520On-Demand%2520Critical%2520Scenario%2520Generation%2520for%2520Autonomous%250A%2520%2520Vehicles%2520in%2520Simulation%26entry.906535625%3DEfimia%2520Panagiotaki%2520and%2520Georgi%2520Pramatarov%2520and%2520Lars%2520Kunze%2520and%2520Daniele%2520De%2520Martini%26entry.1292438233%3D%2520%2520Testing%2520and%2520validating%2520Autonomous%2520Vehicle%2520%2528AV%2529%2520performance%2520in%2520safety-critical%250Aand%2520diverse%2520scenarios%2520is%2520crucial%2520before%2520real-world%2520deployment.%2520However%252C%250Amanually%2520creating%2520such%2520scenarios%2520in%2520simulation%2520remains%2520a%2520significant%2520and%250Atime-consuming%2520challenge.%2520This%2520work%2520introduces%2520a%2520novel%2520method%2520that%2520generates%250Adynamic%2520temporal%2520scene%2520graphs%2520corresponding%2520to%2520diverse%2520traffic%2520scenarios%252C%250Aon-demand%252C%2520tailored%2520to%2520user-defined%2520preferences%252C%2520such%2520as%2520AV%2520actions%252C%2520sets%2520of%250Adynamic%2520agents%252C%2520and%2520criticality%2520levels.%2520A%2520temporal%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%250Amodel%2520learns%2520to%2520predict%2520relationships%2520between%2520ego-vehicle%252C%2520agents%252C%2520and%2520static%250Astructures%252C%2520guided%2520by%2520real-world%2520spatiotemporal%2520interaction%2520patterns%2520and%250Aconstrained%2520by%2520an%2520ontology%2520that%2520restricts%2520predictions%2520to%2520semantically%2520valid%250Alinks.%2520Our%2520model%2520consistently%2520outperforms%2520the%2520baselines%2520in%2520accurately%250Agenerating%2520links%2520corresponding%2520to%2520the%2520requested%2520scenarios.%2520We%2520render%2520the%250Apredicted%2520scenarios%2520in%2520simulation%2520to%2520further%2520demonstrate%2520their%2520effectiveness%2520as%250Atesting%2520environments%2520for%2520AV%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13514v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphSCENE%3A%20On-Demand%20Critical%20Scenario%20Generation%20for%20Autonomous%0A%20%20Vehicles%20in%20Simulation&entry.906535625=Efimia%20Panagiotaki%20and%20Georgi%20Pramatarov%20and%20Lars%20Kunze%20and%20Daniele%20De%20Martini&entry.1292438233=%20%20Testing%20and%20validating%20Autonomous%20Vehicle%20%28AV%29%20performance%20in%20safety-critical%0Aand%20diverse%20scenarios%20is%20crucial%20before%20real-world%20deployment.%20However%2C%0Amanually%20creating%20such%20scenarios%20in%20simulation%20remains%20a%20significant%20and%0Atime-consuming%20challenge.%20This%20work%20introduces%20a%20novel%20method%20that%20generates%0Adynamic%20temporal%20scene%20graphs%20corresponding%20to%20diverse%20traffic%20scenarios%2C%0Aon-demand%2C%20tailored%20to%20user-defined%20preferences%2C%20such%20as%20AV%20actions%2C%20sets%20of%0Adynamic%20agents%2C%20and%20criticality%20levels.%20A%20temporal%20Graph%20Neural%20Network%20%28GNN%29%0Amodel%20learns%20to%20predict%20relationships%20between%20ego-vehicle%2C%20agents%2C%20and%20static%0Astructures%2C%20guided%20by%20real-world%20spatiotemporal%20interaction%20patterns%20and%0Aconstrained%20by%20an%20ontology%20that%20restricts%20predictions%20to%20semantically%20valid%0Alinks.%20Our%20model%20consistently%20outperforms%20the%20baselines%20in%20accurately%0Agenerating%20links%20corresponding%20to%20the%20requested%20scenarios.%20We%20render%20the%0Apredicted%20scenarios%20in%20simulation%20to%20further%20demonstrate%20their%20effectiveness%20as%0Atesting%20environments%20for%20AV%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13514v3&entry.124074799=Read"},
{"title": "A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving\n  Systems", "author": "Stefano Carlo Lambertenghi and Mirena Flores Valdez and Andrea Stocco", "abstract": "  Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)\ndevelopment, offering safe and scalable evaluation across diverse driving\nscenarios. However, discrepancies between simulated and real-world behavior,\nknown as the reality gap, challenge the transferability of test results to\ndeployed systems. In this paper, we present a comprehensive empirical study\ncomparing four representative testing modalities: Software-in-the-Loop (SiL),\nVehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.\nUsing a small-scale physical vehicle equipped with real sensors (camera and\nLiDAR) and its digital twin, we implement each setup and evaluate two ADS\narchitectures (modular and end-to-end) across diverse indoor driving scenarios\ninvolving real obstacles, road topologies, and indoor environments. We\nsystematically assess the impact of each testing modality along three\ndimensions of the reality gap: actuation, perception, and behavioral fidelity.\nOur results show that while SiL and ViL setups simplify critical aspects of\nreal-world dynamics and sensing, MR testing improves perceptual realism without\ncompromising safety or control. Importantly, we identify the conditions under\nwhich failures do not transfer across testing modalities and isolate the\nunderlying dimensions of the gap responsible for these discrepancies. Our\nfindings offer actionable insights into the respective strengths and\nlimitations of each modality and outline a path toward more robust and\ntransferable validation of autonomous driving systems.\n", "link": "http://arxiv.org/abs/2509.22379v1", "date": "2025-09-26", "relevancy": 2.1408, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5803}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Modality%20Evaluation%20of%20the%20Reality%20Gap%20in%20Autonomous%20Driving%0A%20%20Systems&body=Title%3A%20A%20Multi-Modality%20Evaluation%20of%20the%20Reality%20Gap%20in%20Autonomous%20Driving%0A%20%20Systems%0AAuthor%3A%20Stefano%20Carlo%20Lambertenghi%20and%20Mirena%20Flores%20Valdez%20and%20Andrea%20Stocco%0AAbstract%3A%20%20%20Simulation-based%20testing%20is%20a%20cornerstone%20of%20Autonomous%20Driving%20System%20%28ADS%29%0Adevelopment%2C%20offering%20safe%20and%20scalable%20evaluation%20across%20diverse%20driving%0Ascenarios.%20However%2C%20discrepancies%20between%20simulated%20and%20real-world%20behavior%2C%0Aknown%20as%20the%20reality%20gap%2C%20challenge%20the%20transferability%20of%20test%20results%20to%0Adeployed%20systems.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20empirical%20study%0Acomparing%20four%20representative%20testing%20modalities%3A%20Software-in-the-Loop%20%28SiL%29%2C%0AVehicle-in-the-Loop%20%28ViL%29%2C%20Mixed-Reality%20%28MR%29%2C%20and%20full%20real-world%20testing.%0AUsing%20a%20small-scale%20physical%20vehicle%20equipped%20with%20real%20sensors%20%28camera%20and%0ALiDAR%29%20and%20its%20digital%20twin%2C%20we%20implement%20each%20setup%20and%20evaluate%20two%20ADS%0Aarchitectures%20%28modular%20and%20end-to-end%29%20across%20diverse%20indoor%20driving%20scenarios%0Ainvolving%20real%20obstacles%2C%20road%20topologies%2C%20and%20indoor%20environments.%20We%0Asystematically%20assess%20the%20impact%20of%20each%20testing%20modality%20along%20three%0Adimensions%20of%20the%20reality%20gap%3A%20actuation%2C%20perception%2C%20and%20behavioral%20fidelity.%0AOur%20results%20show%20that%20while%20SiL%20and%20ViL%20setups%20simplify%20critical%20aspects%20of%0Areal-world%20dynamics%20and%20sensing%2C%20MR%20testing%20improves%20perceptual%20realism%20without%0Acompromising%20safety%20or%20control.%20Importantly%2C%20we%20identify%20the%20conditions%20under%0Awhich%20failures%20do%20not%20transfer%20across%20testing%20modalities%20and%20isolate%20the%0Aunderlying%20dimensions%20of%20the%20gap%20responsible%20for%20these%20discrepancies.%20Our%0Afindings%20offer%20actionable%20insights%20into%20the%20respective%20strengths%20and%0Alimitations%20of%20each%20modality%20and%20outline%20a%20path%20toward%20more%20robust%20and%0Atransferable%20validation%20of%20autonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Modality%2520Evaluation%2520of%2520the%2520Reality%2520Gap%2520in%2520Autonomous%2520Driving%250A%2520%2520Systems%26entry.906535625%3DStefano%2520Carlo%2520Lambertenghi%2520and%2520Mirena%2520Flores%2520Valdez%2520and%2520Andrea%2520Stocco%26entry.1292438233%3D%2520%2520Simulation-based%2520testing%2520is%2520a%2520cornerstone%2520of%2520Autonomous%2520Driving%2520System%2520%2528ADS%2529%250Adevelopment%252C%2520offering%2520safe%2520and%2520scalable%2520evaluation%2520across%2520diverse%2520driving%250Ascenarios.%2520However%252C%2520discrepancies%2520between%2520simulated%2520and%2520real-world%2520behavior%252C%250Aknown%2520as%2520the%2520reality%2520gap%252C%2520challenge%2520the%2520transferability%2520of%2520test%2520results%2520to%250Adeployed%2520systems.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520empirical%2520study%250Acomparing%2520four%2520representative%2520testing%2520modalities%253A%2520Software-in-the-Loop%2520%2528SiL%2529%252C%250AVehicle-in-the-Loop%2520%2528ViL%2529%252C%2520Mixed-Reality%2520%2528MR%2529%252C%2520and%2520full%2520real-world%2520testing.%250AUsing%2520a%2520small-scale%2520physical%2520vehicle%2520equipped%2520with%2520real%2520sensors%2520%2528camera%2520and%250ALiDAR%2529%2520and%2520its%2520digital%2520twin%252C%2520we%2520implement%2520each%2520setup%2520and%2520evaluate%2520two%2520ADS%250Aarchitectures%2520%2528modular%2520and%2520end-to-end%2529%2520across%2520diverse%2520indoor%2520driving%2520scenarios%250Ainvolving%2520real%2520obstacles%252C%2520road%2520topologies%252C%2520and%2520indoor%2520environments.%2520We%250Asystematically%2520assess%2520the%2520impact%2520of%2520each%2520testing%2520modality%2520along%2520three%250Adimensions%2520of%2520the%2520reality%2520gap%253A%2520actuation%252C%2520perception%252C%2520and%2520behavioral%2520fidelity.%250AOur%2520results%2520show%2520that%2520while%2520SiL%2520and%2520ViL%2520setups%2520simplify%2520critical%2520aspects%2520of%250Areal-world%2520dynamics%2520and%2520sensing%252C%2520MR%2520testing%2520improves%2520perceptual%2520realism%2520without%250Acompromising%2520safety%2520or%2520control.%2520Importantly%252C%2520we%2520identify%2520the%2520conditions%2520under%250Awhich%2520failures%2520do%2520not%2520transfer%2520across%2520testing%2520modalities%2520and%2520isolate%2520the%250Aunderlying%2520dimensions%2520of%2520the%2520gap%2520responsible%2520for%2520these%2520discrepancies.%2520Our%250Afindings%2520offer%2520actionable%2520insights%2520into%2520the%2520respective%2520strengths%2520and%250Alimitations%2520of%2520each%2520modality%2520and%2520outline%2520a%2520path%2520toward%2520more%2520robust%2520and%250Atransferable%2520validation%2520of%2520autonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Modality%20Evaluation%20of%20the%20Reality%20Gap%20in%20Autonomous%20Driving%0A%20%20Systems&entry.906535625=Stefano%20Carlo%20Lambertenghi%20and%20Mirena%20Flores%20Valdez%20and%20Andrea%20Stocco&entry.1292438233=%20%20Simulation-based%20testing%20is%20a%20cornerstone%20of%20Autonomous%20Driving%20System%20%28ADS%29%0Adevelopment%2C%20offering%20safe%20and%20scalable%20evaluation%20across%20diverse%20driving%0Ascenarios.%20However%2C%20discrepancies%20between%20simulated%20and%20real-world%20behavior%2C%0Aknown%20as%20the%20reality%20gap%2C%20challenge%20the%20transferability%20of%20test%20results%20to%0Adeployed%20systems.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20empirical%20study%0Acomparing%20four%20representative%20testing%20modalities%3A%20Software-in-the-Loop%20%28SiL%29%2C%0AVehicle-in-the-Loop%20%28ViL%29%2C%20Mixed-Reality%20%28MR%29%2C%20and%20full%20real-world%20testing.%0AUsing%20a%20small-scale%20physical%20vehicle%20equipped%20with%20real%20sensors%20%28camera%20and%0ALiDAR%29%20and%20its%20digital%20twin%2C%20we%20implement%20each%20setup%20and%20evaluate%20two%20ADS%0Aarchitectures%20%28modular%20and%20end-to-end%29%20across%20diverse%20indoor%20driving%20scenarios%0Ainvolving%20real%20obstacles%2C%20road%20topologies%2C%20and%20indoor%20environments.%20We%0Asystematically%20assess%20the%20impact%20of%20each%20testing%20modality%20along%20three%0Adimensions%20of%20the%20reality%20gap%3A%20actuation%2C%20perception%2C%20and%20behavioral%20fidelity.%0AOur%20results%20show%20that%20while%20SiL%20and%20ViL%20setups%20simplify%20critical%20aspects%20of%0Areal-world%20dynamics%20and%20sensing%2C%20MR%20testing%20improves%20perceptual%20realism%20without%0Acompromising%20safety%20or%20control.%20Importantly%2C%20we%20identify%20the%20conditions%20under%0Awhich%20failures%20do%20not%20transfer%20across%20testing%20modalities%20and%20isolate%20the%0Aunderlying%20dimensions%20of%20the%20gap%20responsible%20for%20these%20discrepancies.%20Our%0Afindings%20offer%20actionable%20insights%20into%20the%20respective%20strengths%20and%0Alimitations%20of%20each%20modality%20and%20outline%20a%20path%20toward%20more%20robust%20and%0Atransferable%20validation%20of%20autonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22379v1&entry.124074799=Read"},
{"title": "WoW: Towards a World omniscient World model Through Embodied Interaction", "author": "Xiaowei Chi and Peidong Jia and Chun-Kai Fan and Xiaozhu Ju and Weishi Mi and Kevin Zhang and Zhiyuan Qin and Wanxin Tian and Kuangzhi Ge and Hao Li and Zezhong Qian and Anthony Chen and Qiang Zhou and Yueru Jia and Jiaming Liu and Yong Dai and Qingpo Wuwu and Chengyu Bai and Yu-Kai Wang and Ying Li and Lizhang Chen and Yong Bao and Zhiyuan Jiang and Jiacheng Zhu and Kai Tang and Ruichuan An and Yulin Luo and Qiuxuan Feng and Siyuan Zhou and Chi-min Chan and Chengkai Hou and Wei Xue and Sirui Han and Yike Guo and Shanghang Zhang and Jian Tang", "abstract": "  Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.\n", "link": "http://arxiv.org/abs/2509.22642v1", "date": "2025-09-26", "relevancy": 1.6936, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6087}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5608}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WoW%3A%20Towards%20a%20World%20omniscient%20World%20model%20Through%20Embodied%20Interaction&body=Title%3A%20WoW%3A%20Towards%20a%20World%20omniscient%20World%20model%20Through%20Embodied%20Interaction%0AAuthor%3A%20Xiaowei%20Chi%20and%20Peidong%20Jia%20and%20Chun-Kai%20Fan%20and%20Xiaozhu%20Ju%20and%20Weishi%20Mi%20and%20Kevin%20Zhang%20and%20Zhiyuan%20Qin%20and%20Wanxin%20Tian%20and%20Kuangzhi%20Ge%20and%20Hao%20Li%20and%20Zezhong%20Qian%20and%20Anthony%20Chen%20and%20Qiang%20Zhou%20and%20Yueru%20Jia%20and%20Jiaming%20Liu%20and%20Yong%20Dai%20and%20Qingpo%20Wuwu%20and%20Chengyu%20Bai%20and%20Yu-Kai%20Wang%20and%20Ying%20Li%20and%20Lizhang%20Chen%20and%20Yong%20Bao%20and%20Zhiyuan%20Jiang%20and%20Jiacheng%20Zhu%20and%20Kai%20Tang%20and%20Ruichuan%20An%20and%20Yulin%20Luo%20and%20Qiuxuan%20Feng%20and%20Siyuan%20Zhou%20and%20Chi-min%20Chan%20and%20Chengkai%20Hou%20and%20Wei%20Xue%20and%20Sirui%20Han%20and%20Yike%20Guo%20and%20Shanghang%20Zhang%20and%20Jian%20Tang%0AAbstract%3A%20%20%20Humans%20develop%20an%20understanding%20of%20intuitive%20physics%20through%20active%0Ainteraction%20with%20the%20world.%20This%20approach%20is%20in%20stark%20contrast%20to%20current%20video%0Amodels%2C%20such%20as%20Sora%2C%20which%20rely%20on%20passive%20observation%20and%20therefore%20struggle%0Awith%20grasping%20physical%20causality.%20This%20observation%20leads%20to%20our%20central%0Ahypothesis%3A%20authentic%20physical%20intuition%20of%20the%20world%20model%20must%20be%20grounded%20in%0Aextensive%2C%20causally%20rich%20interactions%20with%20the%20real%20world.%20To%20test%20this%0Ahypothesis%2C%20we%20present%20WoW%2C%20a%2014-billion-parameter%20generative%20world%20model%0Atrained%20on%202%20million%20robot%20interaction%20trajectories.%20Our%20findings%20reveal%20that%0Athe%20model%27s%20understanding%20of%20physics%20is%20a%20probabilistic%20distribution%20of%0Aplausible%20outcomes%2C%20leading%20to%20stochastic%20instabilities%20and%20physical%0Ahallucinations.%20Furthermore%2C%20we%20demonstrate%20that%20this%20emergent%20capability%20can%0Abe%20actively%20constrained%20toward%20physical%20realism%20by%20SOPHIA%2C%20where%0Avision-language%20model%20agents%20evaluate%20the%20DiT-generated%20output%20and%20guide%20its%0Arefinement%20by%20iteratively%20evolving%20the%20language%20instructions.%20In%20addition%2C%20a%0Aco-trained%20Inverse%20Dynamics%20Model%20translates%20these%20refined%20plans%20into%0Aexecutable%20robotic%20actions%2C%20thus%20closing%20the%20imagination-to-action%20loop.%20We%0Aestablish%20WoWBench%2C%20a%20new%20benchmark%20focused%20on%20physical%20consistency%20and%20causal%0Areasoning%20in%20video%2C%20where%20WoW%20achieves%20state-of-the-art%20performance%20in%20both%0Ahuman%20and%20autonomous%20evaluation%2C%20demonstrating%20strong%20ability%20in%20physical%0Acausality%2C%20collision%20dynamics%2C%20and%20object%20permanence.%20Our%20work%20provides%0Asystematic%20evidence%20that%20large-scale%2C%20real-world%20interaction%20is%20a%20cornerstone%0Afor%20developing%20physical%20intuition%20in%20AI.%20Models%2C%20data%2C%20and%20benchmarks%20will%20be%0Aopen-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWoW%253A%2520Towards%2520a%2520World%2520omniscient%2520World%2520model%2520Through%2520Embodied%2520Interaction%26entry.906535625%3DXiaowei%2520Chi%2520and%2520Peidong%2520Jia%2520and%2520Chun-Kai%2520Fan%2520and%2520Xiaozhu%2520Ju%2520and%2520Weishi%2520Mi%2520and%2520Kevin%2520Zhang%2520and%2520Zhiyuan%2520Qin%2520and%2520Wanxin%2520Tian%2520and%2520Kuangzhi%2520Ge%2520and%2520Hao%2520Li%2520and%2520Zezhong%2520Qian%2520and%2520Anthony%2520Chen%2520and%2520Qiang%2520Zhou%2520and%2520Yueru%2520Jia%2520and%2520Jiaming%2520Liu%2520and%2520Yong%2520Dai%2520and%2520Qingpo%2520Wuwu%2520and%2520Chengyu%2520Bai%2520and%2520Yu-Kai%2520Wang%2520and%2520Ying%2520Li%2520and%2520Lizhang%2520Chen%2520and%2520Yong%2520Bao%2520and%2520Zhiyuan%2520Jiang%2520and%2520Jiacheng%2520Zhu%2520and%2520Kai%2520Tang%2520and%2520Ruichuan%2520An%2520and%2520Yulin%2520Luo%2520and%2520Qiuxuan%2520Feng%2520and%2520Siyuan%2520Zhou%2520and%2520Chi-min%2520Chan%2520and%2520Chengkai%2520Hou%2520and%2520Wei%2520Xue%2520and%2520Sirui%2520Han%2520and%2520Yike%2520Guo%2520and%2520Shanghang%2520Zhang%2520and%2520Jian%2520Tang%26entry.1292438233%3D%2520%2520Humans%2520develop%2520an%2520understanding%2520of%2520intuitive%2520physics%2520through%2520active%250Ainteraction%2520with%2520the%2520world.%2520This%2520approach%2520is%2520in%2520stark%2520contrast%2520to%2520current%2520video%250Amodels%252C%2520such%2520as%2520Sora%252C%2520which%2520rely%2520on%2520passive%2520observation%2520and%2520therefore%2520struggle%250Awith%2520grasping%2520physical%2520causality.%2520This%2520observation%2520leads%2520to%2520our%2520central%250Ahypothesis%253A%2520authentic%2520physical%2520intuition%2520of%2520the%2520world%2520model%2520must%2520be%2520grounded%2520in%250Aextensive%252C%2520causally%2520rich%2520interactions%2520with%2520the%2520real%2520world.%2520To%2520test%2520this%250Ahypothesis%252C%2520we%2520present%2520WoW%252C%2520a%252014-billion-parameter%2520generative%2520world%2520model%250Atrained%2520on%25202%2520million%2520robot%2520interaction%2520trajectories.%2520Our%2520findings%2520reveal%2520that%250Athe%2520model%2527s%2520understanding%2520of%2520physics%2520is%2520a%2520probabilistic%2520distribution%2520of%250Aplausible%2520outcomes%252C%2520leading%2520to%2520stochastic%2520instabilities%2520and%2520physical%250Ahallucinations.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520this%2520emergent%2520capability%2520can%250Abe%2520actively%2520constrained%2520toward%2520physical%2520realism%2520by%2520SOPHIA%252C%2520where%250Avision-language%2520model%2520agents%2520evaluate%2520the%2520DiT-generated%2520output%2520and%2520guide%2520its%250Arefinement%2520by%2520iteratively%2520evolving%2520the%2520language%2520instructions.%2520In%2520addition%252C%2520a%250Aco-trained%2520Inverse%2520Dynamics%2520Model%2520translates%2520these%2520refined%2520plans%2520into%250Aexecutable%2520robotic%2520actions%252C%2520thus%2520closing%2520the%2520imagination-to-action%2520loop.%2520We%250Aestablish%2520WoWBench%252C%2520a%2520new%2520benchmark%2520focused%2520on%2520physical%2520consistency%2520and%2520causal%250Areasoning%2520in%2520video%252C%2520where%2520WoW%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%250Ahuman%2520and%2520autonomous%2520evaluation%252C%2520demonstrating%2520strong%2520ability%2520in%2520physical%250Acausality%252C%2520collision%2520dynamics%252C%2520and%2520object%2520permanence.%2520Our%2520work%2520provides%250Asystematic%2520evidence%2520that%2520large-scale%252C%2520real-world%2520interaction%2520is%2520a%2520cornerstone%250Afor%2520developing%2520physical%2520intuition%2520in%2520AI.%2520Models%252C%2520data%252C%2520and%2520benchmarks%2520will%2520be%250Aopen-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WoW%3A%20Towards%20a%20World%20omniscient%20World%20model%20Through%20Embodied%20Interaction&entry.906535625=Xiaowei%20Chi%20and%20Peidong%20Jia%20and%20Chun-Kai%20Fan%20and%20Xiaozhu%20Ju%20and%20Weishi%20Mi%20and%20Kevin%20Zhang%20and%20Zhiyuan%20Qin%20and%20Wanxin%20Tian%20and%20Kuangzhi%20Ge%20and%20Hao%20Li%20and%20Zezhong%20Qian%20and%20Anthony%20Chen%20and%20Qiang%20Zhou%20and%20Yueru%20Jia%20and%20Jiaming%20Liu%20and%20Yong%20Dai%20and%20Qingpo%20Wuwu%20and%20Chengyu%20Bai%20and%20Yu-Kai%20Wang%20and%20Ying%20Li%20and%20Lizhang%20Chen%20and%20Yong%20Bao%20and%20Zhiyuan%20Jiang%20and%20Jiacheng%20Zhu%20and%20Kai%20Tang%20and%20Ruichuan%20An%20and%20Yulin%20Luo%20and%20Qiuxuan%20Feng%20and%20Siyuan%20Zhou%20and%20Chi-min%20Chan%20and%20Chengkai%20Hou%20and%20Wei%20Xue%20and%20Sirui%20Han%20and%20Yike%20Guo%20and%20Shanghang%20Zhang%20and%20Jian%20Tang&entry.1292438233=%20%20Humans%20develop%20an%20understanding%20of%20intuitive%20physics%20through%20active%0Ainteraction%20with%20the%20world.%20This%20approach%20is%20in%20stark%20contrast%20to%20current%20video%0Amodels%2C%20such%20as%20Sora%2C%20which%20rely%20on%20passive%20observation%20and%20therefore%20struggle%0Awith%20grasping%20physical%20causality.%20This%20observation%20leads%20to%20our%20central%0Ahypothesis%3A%20authentic%20physical%20intuition%20of%20the%20world%20model%20must%20be%20grounded%20in%0Aextensive%2C%20causally%20rich%20interactions%20with%20the%20real%20world.%20To%20test%20this%0Ahypothesis%2C%20we%20present%20WoW%2C%20a%2014-billion-parameter%20generative%20world%20model%0Atrained%20on%202%20million%20robot%20interaction%20trajectories.%20Our%20findings%20reveal%20that%0Athe%20model%27s%20understanding%20of%20physics%20is%20a%20probabilistic%20distribution%20of%0Aplausible%20outcomes%2C%20leading%20to%20stochastic%20instabilities%20and%20physical%0Ahallucinations.%20Furthermore%2C%20we%20demonstrate%20that%20this%20emergent%20capability%20can%0Abe%20actively%20constrained%20toward%20physical%20realism%20by%20SOPHIA%2C%20where%0Avision-language%20model%20agents%20evaluate%20the%20DiT-generated%20output%20and%20guide%20its%0Arefinement%20by%20iteratively%20evolving%20the%20language%20instructions.%20In%20addition%2C%20a%0Aco-trained%20Inverse%20Dynamics%20Model%20translates%20these%20refined%20plans%20into%0Aexecutable%20robotic%20actions%2C%20thus%20closing%20the%20imagination-to-action%20loop.%20We%0Aestablish%20WoWBench%2C%20a%20new%20benchmark%20focused%20on%20physical%20consistency%20and%20causal%0Areasoning%20in%20video%2C%20where%20WoW%20achieves%20state-of-the-art%20performance%20in%20both%0Ahuman%20and%20autonomous%20evaluation%2C%20demonstrating%20strong%20ability%20in%20physical%0Acausality%2C%20collision%20dynamics%2C%20and%20object%20permanence.%20Our%20work%20provides%0Asystematic%20evidence%20that%20large-scale%2C%20real-world%20interaction%20is%20a%20cornerstone%0Afor%20developing%20physical%20intuition%20in%20AI.%20Models%2C%20data%2C%20and%20benchmarks%20will%20be%0Aopen-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22642v1&entry.124074799=Read"},
{"title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and\n  Capabilities in Personal Service Robotics", "author": "Margherita Martorana and Francesca Urgese and Ilaria Tiddi and Stefan Schlobach", "abstract": "  Personal service robots are increasingly used in domestic settings to assist\nolder adults and people requiring support. Effective operation involves not\nonly physical interaction but also the ability to interpret dynamic\nenvironments, understand tasks, and choose appropriate actions based on\ncontext. This requires integrating both hardware components (e.g. sensors,\nactuators) and software systems capable of reasoning about tasks, environments,\nand robot capabilities. Frameworks such as the Robot Operating System (ROS)\nprovide open-source tools that help connect low-level hardware with\nhigher-level functionalities. However, real-world deployments remain tightly\ncoupled to specific platforms. As a result, solutions are often isolated and\nhard-coded, limiting interoperability, reusability, and knowledge sharing.\nOntologies and knowledge graphs offer a structured way to represent tasks,\nenvironments, and robot capabilities. Existing ontologies, such as the\nSocio-physical Model of Activities (SOMA) and the Descriptive Ontology for\nLinguistic and Cognitive Engineering (DOLCE), provide models for activities,\nspatial relationships, and reasoning structures. However, they often focus on\nspecific domains and do not fully capture the connection between environment,\naction, robot capabilities, and system-level integration. In this work, we\npropose the Ontology for roBOts and acTions (OntoBOT), which extends existing\nontologies to provide a unified representation of tasks, actions, environments,\nand capabilities. Our contributions are twofold: (1) we unify these aspects\ninto a cohesive ontology to support formal reasoning about task execution, and\n(2) we demonstrate its generalizability by evaluating competency questions\nacross four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how\nOntoBOT enables context-aware reasoning, task-oriented execution, and knowledge\nsharing in service robotics.\n", "link": "http://arxiv.org/abs/2509.22434v1", "date": "2025-09-26", "relevancy": 1.6832, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6347}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5653}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Ontology%20for%20Unified%20Modeling%20of%20Tasks%2C%20Actions%2C%20Environments%2C%20and%0A%20%20Capabilities%20in%20Personal%20Service%20Robotics&body=Title%3A%20An%20Ontology%20for%20Unified%20Modeling%20of%20Tasks%2C%20Actions%2C%20Environments%2C%20and%0A%20%20Capabilities%20in%20Personal%20Service%20Robotics%0AAuthor%3A%20Margherita%20Martorana%20and%20Francesca%20Urgese%20and%20Ilaria%20Tiddi%20and%20Stefan%20Schlobach%0AAbstract%3A%20%20%20Personal%20service%20robots%20are%20increasingly%20used%20in%20domestic%20settings%20to%20assist%0Aolder%20adults%20and%20people%20requiring%20support.%20Effective%20operation%20involves%20not%0Aonly%20physical%20interaction%20but%20also%20the%20ability%20to%20interpret%20dynamic%0Aenvironments%2C%20understand%20tasks%2C%20and%20choose%20appropriate%20actions%20based%20on%0Acontext.%20This%20requires%20integrating%20both%20hardware%20components%20%28e.g.%20sensors%2C%0Aactuators%29%20and%20software%20systems%20capable%20of%20reasoning%20about%20tasks%2C%20environments%2C%0Aand%20robot%20capabilities.%20Frameworks%20such%20as%20the%20Robot%20Operating%20System%20%28ROS%29%0Aprovide%20open-source%20tools%20that%20help%20connect%20low-level%20hardware%20with%0Ahigher-level%20functionalities.%20However%2C%20real-world%20deployments%20remain%20tightly%0Acoupled%20to%20specific%20platforms.%20As%20a%20result%2C%20solutions%20are%20often%20isolated%20and%0Ahard-coded%2C%20limiting%20interoperability%2C%20reusability%2C%20and%20knowledge%20sharing.%0AOntologies%20and%20knowledge%20graphs%20offer%20a%20structured%20way%20to%20represent%20tasks%2C%0Aenvironments%2C%20and%20robot%20capabilities.%20Existing%20ontologies%2C%20such%20as%20the%0ASocio-physical%20Model%20of%20Activities%20%28SOMA%29%20and%20the%20Descriptive%20Ontology%20for%0ALinguistic%20and%20Cognitive%20Engineering%20%28DOLCE%29%2C%20provide%20models%20for%20activities%2C%0Aspatial%20relationships%2C%20and%20reasoning%20structures.%20However%2C%20they%20often%20focus%20on%0Aspecific%20domains%20and%20do%20not%20fully%20capture%20the%20connection%20between%20environment%2C%0Aaction%2C%20robot%20capabilities%2C%20and%20system-level%20integration.%20In%20this%20work%2C%20we%0Apropose%20the%20Ontology%20for%20roBOts%20and%20acTions%20%28OntoBOT%29%2C%20which%20extends%20existing%0Aontologies%20to%20provide%20a%20unified%20representation%20of%20tasks%2C%20actions%2C%20environments%2C%0Aand%20capabilities.%20Our%20contributions%20are%20twofold%3A%20%281%29%20we%20unify%20these%20aspects%0Ainto%20a%20cohesive%20ontology%20to%20support%20formal%20reasoning%20about%20task%20execution%2C%20and%0A%282%29%20we%20demonstrate%20its%20generalizability%20by%20evaluating%20competency%20questions%0Aacross%20four%20embodied%20agents%20-%20TIAGo%2C%20HSR%2C%20UR3%2C%20and%20Stretch%20-%20showing%20how%0AOntoBOT%20enables%20context-aware%20reasoning%2C%20task-oriented%20execution%2C%20and%20knowledge%0Asharing%20in%20service%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.22434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Ontology%2520for%2520Unified%2520Modeling%2520of%2520Tasks%252C%2520Actions%252C%2520Environments%252C%2520and%250A%2520%2520Capabilities%2520in%2520Personal%2520Service%2520Robotics%26entry.906535625%3DMargherita%2520Martorana%2520and%2520Francesca%2520Urgese%2520and%2520Ilaria%2520Tiddi%2520and%2520Stefan%2520Schlobach%26entry.1292438233%3D%2520%2520Personal%2520service%2520robots%2520are%2520increasingly%2520used%2520in%2520domestic%2520settings%2520to%2520assist%250Aolder%2520adults%2520and%2520people%2520requiring%2520support.%2520Effective%2520operation%2520involves%2520not%250Aonly%2520physical%2520interaction%2520but%2520also%2520the%2520ability%2520to%2520interpret%2520dynamic%250Aenvironments%252C%2520understand%2520tasks%252C%2520and%2520choose%2520appropriate%2520actions%2520based%2520on%250Acontext.%2520This%2520requires%2520integrating%2520both%2520hardware%2520components%2520%2528e.g.%2520sensors%252C%250Aactuators%2529%2520and%2520software%2520systems%2520capable%2520of%2520reasoning%2520about%2520tasks%252C%2520environments%252C%250Aand%2520robot%2520capabilities.%2520Frameworks%2520such%2520as%2520the%2520Robot%2520Operating%2520System%2520%2528ROS%2529%250Aprovide%2520open-source%2520tools%2520that%2520help%2520connect%2520low-level%2520hardware%2520with%250Ahigher-level%2520functionalities.%2520However%252C%2520real-world%2520deployments%2520remain%2520tightly%250Acoupled%2520to%2520specific%2520platforms.%2520As%2520a%2520result%252C%2520solutions%2520are%2520often%2520isolated%2520and%250Ahard-coded%252C%2520limiting%2520interoperability%252C%2520reusability%252C%2520and%2520knowledge%2520sharing.%250AOntologies%2520and%2520knowledge%2520graphs%2520offer%2520a%2520structured%2520way%2520to%2520represent%2520tasks%252C%250Aenvironments%252C%2520and%2520robot%2520capabilities.%2520Existing%2520ontologies%252C%2520such%2520as%2520the%250ASocio-physical%2520Model%2520of%2520Activities%2520%2528SOMA%2529%2520and%2520the%2520Descriptive%2520Ontology%2520for%250ALinguistic%2520and%2520Cognitive%2520Engineering%2520%2528DOLCE%2529%252C%2520provide%2520models%2520for%2520activities%252C%250Aspatial%2520relationships%252C%2520and%2520reasoning%2520structures.%2520However%252C%2520they%2520often%2520focus%2520on%250Aspecific%2520domains%2520and%2520do%2520not%2520fully%2520capture%2520the%2520connection%2520between%2520environment%252C%250Aaction%252C%2520robot%2520capabilities%252C%2520and%2520system-level%2520integration.%2520In%2520this%2520work%252C%2520we%250Apropose%2520the%2520Ontology%2520for%2520roBOts%2520and%2520acTions%2520%2528OntoBOT%2529%252C%2520which%2520extends%2520existing%250Aontologies%2520to%2520provide%2520a%2520unified%2520representation%2520of%2520tasks%252C%2520actions%252C%2520environments%252C%250Aand%2520capabilities.%2520Our%2520contributions%2520are%2520twofold%253A%2520%25281%2529%2520we%2520unify%2520these%2520aspects%250Ainto%2520a%2520cohesive%2520ontology%2520to%2520support%2520formal%2520reasoning%2520about%2520task%2520execution%252C%2520and%250A%25282%2529%2520we%2520demonstrate%2520its%2520generalizability%2520by%2520evaluating%2520competency%2520questions%250Aacross%2520four%2520embodied%2520agents%2520-%2520TIAGo%252C%2520HSR%252C%2520UR3%252C%2520and%2520Stretch%2520-%2520showing%2520how%250AOntoBOT%2520enables%2520context-aware%2520reasoning%252C%2520task-oriented%2520execution%252C%2520and%2520knowledge%250Asharing%2520in%2520service%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Ontology%20for%20Unified%20Modeling%20of%20Tasks%2C%20Actions%2C%20Environments%2C%20and%0A%20%20Capabilities%20in%20Personal%20Service%20Robotics&entry.906535625=Margherita%20Martorana%20and%20Francesca%20Urgese%20and%20Ilaria%20Tiddi%20and%20Stefan%20Schlobach&entry.1292438233=%20%20Personal%20service%20robots%20are%20increasingly%20used%20in%20domestic%20settings%20to%20assist%0Aolder%20adults%20and%20people%20requiring%20support.%20Effective%20operation%20involves%20not%0Aonly%20physical%20interaction%20but%20also%20the%20ability%20to%20interpret%20dynamic%0Aenvironments%2C%20understand%20tasks%2C%20and%20choose%20appropriate%20actions%20based%20on%0Acontext.%20This%20requires%20integrating%20both%20hardware%20components%20%28e.g.%20sensors%2C%0Aactuators%29%20and%20software%20systems%20capable%20of%20reasoning%20about%20tasks%2C%20environments%2C%0Aand%20robot%20capabilities.%20Frameworks%20such%20as%20the%20Robot%20Operating%20System%20%28ROS%29%0Aprovide%20open-source%20tools%20that%20help%20connect%20low-level%20hardware%20with%0Ahigher-level%20functionalities.%20However%2C%20real-world%20deployments%20remain%20tightly%0Acoupled%20to%20specific%20platforms.%20As%20a%20result%2C%20solutions%20are%20often%20isolated%20and%0Ahard-coded%2C%20limiting%20interoperability%2C%20reusability%2C%20and%20knowledge%20sharing.%0AOntologies%20and%20knowledge%20graphs%20offer%20a%20structured%20way%20to%20represent%20tasks%2C%0Aenvironments%2C%20and%20robot%20capabilities.%20Existing%20ontologies%2C%20such%20as%20the%0ASocio-physical%20Model%20of%20Activities%20%28SOMA%29%20and%20the%20Descriptive%20Ontology%20for%0ALinguistic%20and%20Cognitive%20Engineering%20%28DOLCE%29%2C%20provide%20models%20for%20activities%2C%0Aspatial%20relationships%2C%20and%20reasoning%20structures.%20However%2C%20they%20often%20focus%20on%0Aspecific%20domains%20and%20do%20not%20fully%20capture%20the%20connection%20between%20environment%2C%0Aaction%2C%20robot%20capabilities%2C%20and%20system-level%20integration.%20In%20this%20work%2C%20we%0Apropose%20the%20Ontology%20for%20roBOts%20and%20acTions%20%28OntoBOT%29%2C%20which%20extends%20existing%0Aontologies%20to%20provide%20a%20unified%20representation%20of%20tasks%2C%20actions%2C%20environments%2C%0Aand%20capabilities.%20Our%20contributions%20are%20twofold%3A%20%281%29%20we%20unify%20these%20aspects%0Ainto%20a%20cohesive%20ontology%20to%20support%20formal%20reasoning%20about%20task%20execution%2C%20and%0A%282%29%20we%20demonstrate%20its%20generalizability%20by%20evaluating%20competency%20questions%0Aacross%20four%20embodied%20agents%20-%20TIAGo%2C%20HSR%2C%20UR3%2C%20and%20Stretch%20-%20showing%20how%0AOntoBOT%20enables%20context-aware%20reasoning%2C%20task-oriented%20execution%2C%20and%20knowledge%0Asharing%20in%20service%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.22434v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


