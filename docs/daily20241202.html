<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241128.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Splashing: Direct Volumetric Rendering Underwater", "author": "Nir Mualem and Roy Amoyal and Oren Freifeld and Derya Akkaynak", "abstract": "  In underwater images, most useful features are occluded by water. The extent\nof the occlusion depends on imaging geometry and can vary even across a\nsequence of burst images. As a result, 3D reconstruction methods robust on\nin-air scenes, like Neural Radiance Field methods (NeRFs) or 3D Gaussian\nSplatting (3DGS), fail on underwater scenes. While a recent underwater\nadaptation of NeRFs achieved state-of-the-art results, it is impractically\nslow: reconstruction takes hours and its rendering rate, in frames per second\n(FPS), is less than 1. Here, we present a new method that takes only a few\nminutes for reconstruction and renders novel underwater scenes at 140 FPS.\nNamed Gaussian Splashing, our method unifies the strengths and speed of 3DGS\nwith an image formation model for capturing scattering, introducing innovations\nin the rendering and depth estimation procedures and in the 3DGS loss function.\nDespite the complexities of underwater adaptation, our method produces images\nat unparalleled speeds with superior details. Moreover, it reveals distant\nscene details with far greater clarity than other methods, dramatically\nimproving reconstructed and rendered images. We demonstrate results on existing\ndatasets and a new dataset we have collected.\n  Additional visual results are available at:\nhttps://bgu-cs-vil.github.io/gaussiansplashingUW.github.io/ .\n", "link": "http://arxiv.org/abs/2411.19588v1", "date": "2024-11-29", "relevancy": 3.3992, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7029}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6771}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splashing%3A%20Direct%20Volumetric%20Rendering%20Underwater&body=Title%3A%20Gaussian%20Splashing%3A%20Direct%20Volumetric%20Rendering%20Underwater%0AAuthor%3A%20Nir%20Mualem%20and%20Roy%20Amoyal%20and%20Oren%20Freifeld%20and%20Derya%20Akkaynak%0AAbstract%3A%20%20%20In%20underwater%20images%2C%20most%20useful%20features%20are%20occluded%20by%20water.%20The%20extent%0Aof%20the%20occlusion%20depends%20on%20imaging%20geometry%20and%20can%20vary%20even%20across%20a%0Asequence%20of%20burst%20images.%20As%20a%20result%2C%203D%20reconstruction%20methods%20robust%20on%0Ain-air%20scenes%2C%20like%20Neural%20Radiance%20Field%20methods%20%28NeRFs%29%20or%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20fail%20on%20underwater%20scenes.%20While%20a%20recent%20underwater%0Aadaptation%20of%20NeRFs%20achieved%20state-of-the-art%20results%2C%20it%20is%20impractically%0Aslow%3A%20reconstruction%20takes%20hours%20and%20its%20rendering%20rate%2C%20in%20frames%20per%20second%0A%28FPS%29%2C%20is%20less%20than%201.%20Here%2C%20we%20present%20a%20new%20method%20that%20takes%20only%20a%20few%0Aminutes%20for%20reconstruction%20and%20renders%20novel%20underwater%20scenes%20at%20140%20FPS.%0ANamed%20Gaussian%20Splashing%2C%20our%20method%20unifies%20the%20strengths%20and%20speed%20of%203DGS%0Awith%20an%20image%20formation%20model%20for%20capturing%20scattering%2C%20introducing%20innovations%0Ain%20the%20rendering%20and%20depth%20estimation%20procedures%20and%20in%20the%203DGS%20loss%20function.%0ADespite%20the%20complexities%20of%20underwater%20adaptation%2C%20our%20method%20produces%20images%0Aat%20unparalleled%20speeds%20with%20superior%20details.%20Moreover%2C%20it%20reveals%20distant%0Ascene%20details%20with%20far%20greater%20clarity%20than%20other%20methods%2C%20dramatically%0Aimproving%20reconstructed%20and%20rendered%20images.%20We%20demonstrate%20results%20on%20existing%0Adatasets%20and%20a%20new%20dataset%20we%20have%20collected.%0A%20%20Additional%20visual%20results%20are%20available%20at%3A%0Ahttps%3A//bgu-cs-vil.github.io/gaussiansplashingUW.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splashing%253A%2520Direct%2520Volumetric%2520Rendering%2520Underwater%26entry.906535625%3DNir%2520Mualem%2520and%2520Roy%2520Amoyal%2520and%2520Oren%2520Freifeld%2520and%2520Derya%2520Akkaynak%26entry.1292438233%3D%2520%2520In%2520underwater%2520images%252C%2520most%2520useful%2520features%2520are%2520occluded%2520by%2520water.%2520The%2520extent%250Aof%2520the%2520occlusion%2520depends%2520on%2520imaging%2520geometry%2520and%2520can%2520vary%2520even%2520across%2520a%250Asequence%2520of%2520burst%2520images.%2520As%2520a%2520result%252C%25203D%2520reconstruction%2520methods%2520robust%2520on%250Ain-air%2520scenes%252C%2520like%2520Neural%2520Radiance%2520Field%2520methods%2520%2528NeRFs%2529%2520or%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%252C%2520fail%2520on%2520underwater%2520scenes.%2520While%2520a%2520recent%2520underwater%250Aadaptation%2520of%2520NeRFs%2520achieved%2520state-of-the-art%2520results%252C%2520it%2520is%2520impractically%250Aslow%253A%2520reconstruction%2520takes%2520hours%2520and%2520its%2520rendering%2520rate%252C%2520in%2520frames%2520per%2520second%250A%2528FPS%2529%252C%2520is%2520less%2520than%25201.%2520Here%252C%2520we%2520present%2520a%2520new%2520method%2520that%2520takes%2520only%2520a%2520few%250Aminutes%2520for%2520reconstruction%2520and%2520renders%2520novel%2520underwater%2520scenes%2520at%2520140%2520FPS.%250ANamed%2520Gaussian%2520Splashing%252C%2520our%2520method%2520unifies%2520the%2520strengths%2520and%2520speed%2520of%25203DGS%250Awith%2520an%2520image%2520formation%2520model%2520for%2520capturing%2520scattering%252C%2520introducing%2520innovations%250Ain%2520the%2520rendering%2520and%2520depth%2520estimation%2520procedures%2520and%2520in%2520the%25203DGS%2520loss%2520function.%250ADespite%2520the%2520complexities%2520of%2520underwater%2520adaptation%252C%2520our%2520method%2520produces%2520images%250Aat%2520unparalleled%2520speeds%2520with%2520superior%2520details.%2520Moreover%252C%2520it%2520reveals%2520distant%250Ascene%2520details%2520with%2520far%2520greater%2520clarity%2520than%2520other%2520methods%252C%2520dramatically%250Aimproving%2520reconstructed%2520and%2520rendered%2520images.%2520We%2520demonstrate%2520results%2520on%2520existing%250Adatasets%2520and%2520a%2520new%2520dataset%2520we%2520have%2520collected.%250A%2520%2520Additional%2520visual%2520results%2520are%2520available%2520at%253A%250Ahttps%253A//bgu-cs-vil.github.io/gaussiansplashingUW.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splashing%3A%20Direct%20Volumetric%20Rendering%20Underwater&entry.906535625=Nir%20Mualem%20and%20Roy%20Amoyal%20and%20Oren%20Freifeld%20and%20Derya%20Akkaynak&entry.1292438233=%20%20In%20underwater%20images%2C%20most%20useful%20features%20are%20occluded%20by%20water.%20The%20extent%0Aof%20the%20occlusion%20depends%20on%20imaging%20geometry%20and%20can%20vary%20even%20across%20a%0Asequence%20of%20burst%20images.%20As%20a%20result%2C%203D%20reconstruction%20methods%20robust%20on%0Ain-air%20scenes%2C%20like%20Neural%20Radiance%20Field%20methods%20%28NeRFs%29%20or%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20fail%20on%20underwater%20scenes.%20While%20a%20recent%20underwater%0Aadaptation%20of%20NeRFs%20achieved%20state-of-the-art%20results%2C%20it%20is%20impractically%0Aslow%3A%20reconstruction%20takes%20hours%20and%20its%20rendering%20rate%2C%20in%20frames%20per%20second%0A%28FPS%29%2C%20is%20less%20than%201.%20Here%2C%20we%20present%20a%20new%20method%20that%20takes%20only%20a%20few%0Aminutes%20for%20reconstruction%20and%20renders%20novel%20underwater%20scenes%20at%20140%20FPS.%0ANamed%20Gaussian%20Splashing%2C%20our%20method%20unifies%20the%20strengths%20and%20speed%20of%203DGS%0Awith%20an%20image%20formation%20model%20for%20capturing%20scattering%2C%20introducing%20innovations%0Ain%20the%20rendering%20and%20depth%20estimation%20procedures%20and%20in%20the%203DGS%20loss%20function.%0ADespite%20the%20complexities%20of%20underwater%20adaptation%2C%20our%20method%20produces%20images%0Aat%20unparalleled%20speeds%20with%20superior%20details.%20Moreover%2C%20it%20reveals%20distant%0Ascene%20details%20with%20far%20greater%20clarity%20than%20other%20methods%2C%20dramatically%0Aimproving%20reconstructed%20and%20rendered%20images.%20We%20demonstrate%20results%20on%20existing%0Adatasets%20and%20a%20new%20dataset%20we%20have%20collected.%0A%20%20Additional%20visual%20results%20are%20available%20at%3A%0Ahttps%3A//bgu-cs-vil.github.io/gaussiansplashingUW.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19588v1&entry.124074799=Read"},
{"title": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting", "author": "Christian Homeyer and Leon Begiristain and Christoph Schn\u00f6rr", "abstract": "  Recent progress in scene synthesis makes standalone SLAM systems purely based\non optimizing hyperprimitives with a Rendering objective possible. However, the\ntracking performance still lacks behind traditional and end-to-end SLAM\nsystems. An optimal trade-off between robustness, speed and accuracy has not\nyet been reached, especially for monocular video. In this paper, we introduce a\nSLAM system based on an end-to-end Tracker and extend it with a Renderer based\non recent 3D Gaussian Splatting techniques. Our framework \\textbf{DroidSplat}\nachieves both SotA tracking and rendering results on common SLAM benchmarks. We\nimplemented multiple building blocks of modern SLAM systems to run in parallel,\nallowing for fast inference on common consumer GPU's. Recent progress in\nmonocular depth prediction and camera calibration allows our system to achieve\nstrong results even on in-the-wild data without known camera intrinsics. Code\nwill be available at \\url{https://github.com/ChenHoy/DROID-Splat}.\n", "link": "http://arxiv.org/abs/2411.17660v2", "date": "2024-11-29", "relevancy": 3.3318, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.749}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6465}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DROID-Splat%3A%20Combining%20end-to-end%20SLAM%20with%203D%20Gaussian%20Splatting&body=Title%3A%20DROID-Splat%3A%20Combining%20end-to-end%20SLAM%20with%203D%20Gaussian%20Splatting%0AAuthor%3A%20Christian%20Homeyer%20and%20Leon%20Begiristain%20and%20Christoph%20Schn%C3%B6rr%0AAbstract%3A%20%20%20Recent%20progress%20in%20scene%20synthesis%20makes%20standalone%20SLAM%20systems%20purely%20based%0Aon%20optimizing%20hyperprimitives%20with%20a%20Rendering%20objective%20possible.%20However%2C%20the%0Atracking%20performance%20still%20lacks%20behind%20traditional%20and%20end-to-end%20SLAM%0Asystems.%20An%20optimal%20trade-off%20between%20robustness%2C%20speed%20and%20accuracy%20has%20not%0Ayet%20been%20reached%2C%20especially%20for%20monocular%20video.%20In%20this%20paper%2C%20we%20introduce%20a%0ASLAM%20system%20based%20on%20an%20end-to-end%20Tracker%20and%20extend%20it%20with%20a%20Renderer%20based%0Aon%20recent%203D%20Gaussian%20Splatting%20techniques.%20Our%20framework%20%5Ctextbf%7BDroidSplat%7D%0Aachieves%20both%20SotA%20tracking%20and%20rendering%20results%20on%20common%20SLAM%20benchmarks.%20We%0Aimplemented%20multiple%20building%20blocks%20of%20modern%20SLAM%20systems%20to%20run%20in%20parallel%2C%0Aallowing%20for%20fast%20inference%20on%20common%20consumer%20GPU%27s.%20Recent%20progress%20in%0Amonocular%20depth%20prediction%20and%20camera%20calibration%20allows%20our%20system%20to%20achieve%0Astrong%20results%20even%20on%20in-the-wild%20data%20without%20known%20camera%20intrinsics.%20Code%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/ChenHoy/DROID-Splat%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDROID-Splat%253A%2520Combining%2520end-to-end%2520SLAM%2520with%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DChristian%2520Homeyer%2520and%2520Leon%2520Begiristain%2520and%2520Christoph%2520Schn%25C3%25B6rr%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520scene%2520synthesis%2520makes%2520standalone%2520SLAM%2520systems%2520purely%2520based%250Aon%2520optimizing%2520hyperprimitives%2520with%2520a%2520Rendering%2520objective%2520possible.%2520However%252C%2520the%250Atracking%2520performance%2520still%2520lacks%2520behind%2520traditional%2520and%2520end-to-end%2520SLAM%250Asystems.%2520An%2520optimal%2520trade-off%2520between%2520robustness%252C%2520speed%2520and%2520accuracy%2520has%2520not%250Ayet%2520been%2520reached%252C%2520especially%2520for%2520monocular%2520video.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250ASLAM%2520system%2520based%2520on%2520an%2520end-to-end%2520Tracker%2520and%2520extend%2520it%2520with%2520a%2520Renderer%2520based%250Aon%2520recent%25203D%2520Gaussian%2520Splatting%2520techniques.%2520Our%2520framework%2520%255Ctextbf%257BDroidSplat%257D%250Aachieves%2520both%2520SotA%2520tracking%2520and%2520rendering%2520results%2520on%2520common%2520SLAM%2520benchmarks.%2520We%250Aimplemented%2520multiple%2520building%2520blocks%2520of%2520modern%2520SLAM%2520systems%2520to%2520run%2520in%2520parallel%252C%250Aallowing%2520for%2520fast%2520inference%2520on%2520common%2520consumer%2520GPU%2527s.%2520Recent%2520progress%2520in%250Amonocular%2520depth%2520prediction%2520and%2520camera%2520calibration%2520allows%2520our%2520system%2520to%2520achieve%250Astrong%2520results%2520even%2520on%2520in-the-wild%2520data%2520without%2520known%2520camera%2520intrinsics.%2520Code%250Awill%2520be%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/ChenHoy/DROID-Splat%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DROID-Splat%3A%20Combining%20end-to-end%20SLAM%20with%203D%20Gaussian%20Splatting&entry.906535625=Christian%20Homeyer%20and%20Leon%20Begiristain%20and%20Christoph%20Schn%C3%B6rr&entry.1292438233=%20%20Recent%20progress%20in%20scene%20synthesis%20makes%20standalone%20SLAM%20systems%20purely%20based%0Aon%20optimizing%20hyperprimitives%20with%20a%20Rendering%20objective%20possible.%20However%2C%20the%0Atracking%20performance%20still%20lacks%20behind%20traditional%20and%20end-to-end%20SLAM%0Asystems.%20An%20optimal%20trade-off%20between%20robustness%2C%20speed%20and%20accuracy%20has%20not%0Ayet%20been%20reached%2C%20especially%20for%20monocular%20video.%20In%20this%20paper%2C%20we%20introduce%20a%0ASLAM%20system%20based%20on%20an%20end-to-end%20Tracker%20and%20extend%20it%20with%20a%20Renderer%20based%0Aon%20recent%203D%20Gaussian%20Splatting%20techniques.%20Our%20framework%20%5Ctextbf%7BDroidSplat%7D%0Aachieves%20both%20SotA%20tracking%20and%20rendering%20results%20on%20common%20SLAM%20benchmarks.%20We%0Aimplemented%20multiple%20building%20blocks%20of%20modern%20SLAM%20systems%20to%20run%20in%20parallel%2C%0Aallowing%20for%20fast%20inference%20on%20common%20consumer%20GPU%27s.%20Recent%20progress%20in%0Amonocular%20depth%20prediction%20and%20camera%20calibration%20allows%20our%20system%20to%20achieve%0Astrong%20results%20even%20on%20in-the-wild%20data%20without%20known%20camera%20intrinsics.%20Code%0Awill%20be%20available%20at%20%5Curl%7Bhttps%3A//github.com/ChenHoy/DROID-Splat%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17660v2&entry.124074799=Read"},
{"title": "DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering", "author": "Yihao Wang and Marcus Klasson and Matias Turkulainen and Shuzhe Wang and Juho Kannala and Arno Solin", "abstract": "  Gaussian splatting enables fast novel view synthesis in static 3D\nenvironments. However, reconstructing real-world environments remains\nchallenging as distractors or occluders break the multi-view consistency\nassumption required for accurate 3D reconstruction. Most existing methods rely\non external semantic information from pre-trained models, introducing\nadditional computational overhead as pre-processing steps or during\noptimization. In this work, we propose a novel method, DeSplat, that directly\nseparates distractors and static scene elements purely based on volume\nrendering of Gaussian primitives. We initialize Gaussians within each camera\nview for reconstructing the view-specific distractors to separately model the\nstatic 3D scene and distractors in the alpha compositing stages. DeSplat yields\nan explicit scene separation of static elements and distractors, achieving\ncomparable results to prior distractor-free approaches without sacrificing\nrendering speed. We demonstrate DeSplat's effectiveness on three benchmark data\nsets for distractor-free novel view synthesis. See the project website at\nhttps://aaltoml.github.io/desplat/.\n", "link": "http://arxiv.org/abs/2411.19756v1", "date": "2024-11-29", "relevancy": 3.2628, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6823}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6433}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeSplat%3A%20Decomposed%20Gaussian%20Splatting%20for%20Distractor-Free%20Rendering&body=Title%3A%20DeSplat%3A%20Decomposed%20Gaussian%20Splatting%20for%20Distractor-Free%20Rendering%0AAuthor%3A%20Yihao%20Wang%20and%20Marcus%20Klasson%20and%20Matias%20Turkulainen%20and%20Shuzhe%20Wang%20and%20Juho%20Kannala%20and%20Arno%20Solin%0AAbstract%3A%20%20%20Gaussian%20splatting%20enables%20fast%20novel%20view%20synthesis%20in%20static%203D%0Aenvironments.%20However%2C%20reconstructing%20real-world%20environments%20remains%0Achallenging%20as%20distractors%20or%20occluders%20break%20the%20multi-view%20consistency%0Aassumption%20required%20for%20accurate%203D%20reconstruction.%20Most%20existing%20methods%20rely%0Aon%20external%20semantic%20information%20from%20pre-trained%20models%2C%20introducing%0Aadditional%20computational%20overhead%20as%20pre-processing%20steps%20or%20during%0Aoptimization.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%2C%20DeSplat%2C%20that%20directly%0Aseparates%20distractors%20and%20static%20scene%20elements%20purely%20based%20on%20volume%0Arendering%20of%20Gaussian%20primitives.%20We%20initialize%20Gaussians%20within%20each%20camera%0Aview%20for%20reconstructing%20the%20view-specific%20distractors%20to%20separately%20model%20the%0Astatic%203D%20scene%20and%20distractors%20in%20the%20alpha%20compositing%20stages.%20DeSplat%20yields%0Aan%20explicit%20scene%20separation%20of%20static%20elements%20and%20distractors%2C%20achieving%0Acomparable%20results%20to%20prior%20distractor-free%20approaches%20without%20sacrificing%0Arendering%20speed.%20We%20demonstrate%20DeSplat%27s%20effectiveness%20on%20three%20benchmark%20data%0Asets%20for%20distractor-free%20novel%20view%20synthesis.%20See%20the%20project%20website%20at%0Ahttps%3A//aaltoml.github.io/desplat/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeSplat%253A%2520Decomposed%2520Gaussian%2520Splatting%2520for%2520Distractor-Free%2520Rendering%26entry.906535625%3DYihao%2520Wang%2520and%2520Marcus%2520Klasson%2520and%2520Matias%2520Turkulainen%2520and%2520Shuzhe%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Arno%2520Solin%26entry.1292438233%3D%2520%2520Gaussian%2520splatting%2520enables%2520fast%2520novel%2520view%2520synthesis%2520in%2520static%25203D%250Aenvironments.%2520However%252C%2520reconstructing%2520real-world%2520environments%2520remains%250Achallenging%2520as%2520distractors%2520or%2520occluders%2520break%2520the%2520multi-view%2520consistency%250Aassumption%2520required%2520for%2520accurate%25203D%2520reconstruction.%2520Most%2520existing%2520methods%2520rely%250Aon%2520external%2520semantic%2520information%2520from%2520pre-trained%2520models%252C%2520introducing%250Aadditional%2520computational%2520overhead%2520as%2520pre-processing%2520steps%2520or%2520during%250Aoptimization.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520DeSplat%252C%2520that%2520directly%250Aseparates%2520distractors%2520and%2520static%2520scene%2520elements%2520purely%2520based%2520on%2520volume%250Arendering%2520of%2520Gaussian%2520primitives.%2520We%2520initialize%2520Gaussians%2520within%2520each%2520camera%250Aview%2520for%2520reconstructing%2520the%2520view-specific%2520distractors%2520to%2520separately%2520model%2520the%250Astatic%25203D%2520scene%2520and%2520distractors%2520in%2520the%2520alpha%2520compositing%2520stages.%2520DeSplat%2520yields%250Aan%2520explicit%2520scene%2520separation%2520of%2520static%2520elements%2520and%2520distractors%252C%2520achieving%250Acomparable%2520results%2520to%2520prior%2520distractor-free%2520approaches%2520without%2520sacrificing%250Arendering%2520speed.%2520We%2520demonstrate%2520DeSplat%2527s%2520effectiveness%2520on%2520three%2520benchmark%2520data%250Asets%2520for%2520distractor-free%2520novel%2520view%2520synthesis.%2520See%2520the%2520project%2520website%2520at%250Ahttps%253A//aaltoml.github.io/desplat/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeSplat%3A%20Decomposed%20Gaussian%20Splatting%20for%20Distractor-Free%20Rendering&entry.906535625=Yihao%20Wang%20and%20Marcus%20Klasson%20and%20Matias%20Turkulainen%20and%20Shuzhe%20Wang%20and%20Juho%20Kannala%20and%20Arno%20Solin&entry.1292438233=%20%20Gaussian%20splatting%20enables%20fast%20novel%20view%20synthesis%20in%20static%203D%0Aenvironments.%20However%2C%20reconstructing%20real-world%20environments%20remains%0Achallenging%20as%20distractors%20or%20occluders%20break%20the%20multi-view%20consistency%0Aassumption%20required%20for%20accurate%203D%20reconstruction.%20Most%20existing%20methods%20rely%0Aon%20external%20semantic%20information%20from%20pre-trained%20models%2C%20introducing%0Aadditional%20computational%20overhead%20as%20pre-processing%20steps%20or%20during%0Aoptimization.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%2C%20DeSplat%2C%20that%20directly%0Aseparates%20distractors%20and%20static%20scene%20elements%20purely%20based%20on%20volume%0Arendering%20of%20Gaussian%20primitives.%20We%20initialize%20Gaussians%20within%20each%20camera%0Aview%20for%20reconstructing%20the%20view-specific%20distractors%20to%20separately%20model%20the%0Astatic%203D%20scene%20and%20distractors%20in%20the%20alpha%20compositing%20stages.%20DeSplat%20yields%0Aan%20explicit%20scene%20separation%20of%20static%20elements%20and%20distractors%2C%20achieving%0Acomparable%20results%20to%20prior%20distractor-free%20approaches%20without%20sacrificing%0Arendering%20speed.%20We%20demonstrate%20DeSplat%27s%20effectiveness%20on%20three%20benchmark%20data%0Asets%20for%20distractor-free%20novel%20view%20synthesis.%20See%20the%20project%20website%20at%0Ahttps%3A//aaltoml.github.io/desplat/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19756v1&entry.124074799=Read"},
{"title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion\n  Scaffolds", "author": "Jiahui Lei and Yijia Weng and Adam Harley and Leonidas Guibas and Kostas Daniilidis", "abstract": "  We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction system\ndesigned to reconstruct and synthesize novel views of dynamic scenes from\nmonocular videos captured casually in the wild. To address such a challenging\nand ill-posed inverse problem, we leverage prior knowledge from foundational\nvision models and lift the video data to a novel Motion Scaffold (MoSca)\nrepresentation, which compactly and smoothly encodes the underlying\nmotions/deformations. The scene geometry and appearance are then disentangled\nfrom the deformation field and are encoded by globally fusing the Gaussians\nanchored onto the MoSca and optimized via Gaussian Splatting. Additionally,\ncamera focal length and poses can be solved using bundle adjustment without the\nneed of any other pose estimation tools. Experiments demonstrate\nstate-of-the-art performance on dynamic rendering benchmarks and its\neffectiveness on real videos.\n", "link": "http://arxiv.org/abs/2405.17421v2", "date": "2024-11-29", "relevancy": 3.2385, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7041}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6231}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoSca%3A%20Dynamic%20Gaussian%20Fusion%20from%20Casual%20Videos%20via%204D%20Motion%0A%20%20Scaffolds&body=Title%3A%20MoSca%3A%20Dynamic%20Gaussian%20Fusion%20from%20Casual%20Videos%20via%204D%20Motion%0A%20%20Scaffolds%0AAuthor%3A%20Jiahui%20Lei%20and%20Yijia%20Weng%20and%20Adam%20Harley%20and%20Leonidas%20Guibas%20and%20Kostas%20Daniilidis%0AAbstract%3A%20%20%20We%20introduce%204D%20Motion%20Scaffolds%20%28MoSca%29%2C%20a%20modern%204D%20reconstruction%20system%0Adesigned%20to%20reconstruct%20and%20synthesize%20novel%20views%20of%20dynamic%20scenes%20from%0Amonocular%20videos%20captured%20casually%20in%20the%20wild.%20To%20address%20such%20a%20challenging%0Aand%20ill-posed%20inverse%20problem%2C%20we%20leverage%20prior%20knowledge%20from%20foundational%0Avision%20models%20and%20lift%20the%20video%20data%20to%20a%20novel%20Motion%20Scaffold%20%28MoSca%29%0Arepresentation%2C%20which%20compactly%20and%20smoothly%20encodes%20the%20underlying%0Amotions/deformations.%20The%20scene%20geometry%20and%20appearance%20are%20then%20disentangled%0Afrom%20the%20deformation%20field%20and%20are%20encoded%20by%20globally%20fusing%20the%20Gaussians%0Aanchored%20onto%20the%20MoSca%20and%20optimized%20via%20Gaussian%20Splatting.%20Additionally%2C%0Acamera%20focal%20length%20and%20poses%20can%20be%20solved%20using%20bundle%20adjustment%20without%20the%0Aneed%20of%20any%20other%20pose%20estimation%20tools.%20Experiments%20demonstrate%0Astate-of-the-art%20performance%20on%20dynamic%20rendering%20benchmarks%20and%20its%0Aeffectiveness%20on%20real%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17421v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoSca%253A%2520Dynamic%2520Gaussian%2520Fusion%2520from%2520Casual%2520Videos%2520via%25204D%2520Motion%250A%2520%2520Scaffolds%26entry.906535625%3DJiahui%2520Lei%2520and%2520Yijia%2520Weng%2520and%2520Adam%2520Harley%2520and%2520Leonidas%2520Guibas%2520and%2520Kostas%2520Daniilidis%26entry.1292438233%3D%2520%2520We%2520introduce%25204D%2520Motion%2520Scaffolds%2520%2528MoSca%2529%252C%2520a%2520modern%25204D%2520reconstruction%2520system%250Adesigned%2520to%2520reconstruct%2520and%2520synthesize%2520novel%2520views%2520of%2520dynamic%2520scenes%2520from%250Amonocular%2520videos%2520captured%2520casually%2520in%2520the%2520wild.%2520To%2520address%2520such%2520a%2520challenging%250Aand%2520ill-posed%2520inverse%2520problem%252C%2520we%2520leverage%2520prior%2520knowledge%2520from%2520foundational%250Avision%2520models%2520and%2520lift%2520the%2520video%2520data%2520to%2520a%2520novel%2520Motion%2520Scaffold%2520%2528MoSca%2529%250Arepresentation%252C%2520which%2520compactly%2520and%2520smoothly%2520encodes%2520the%2520underlying%250Amotions/deformations.%2520The%2520scene%2520geometry%2520and%2520appearance%2520are%2520then%2520disentangled%250Afrom%2520the%2520deformation%2520field%2520and%2520are%2520encoded%2520by%2520globally%2520fusing%2520the%2520Gaussians%250Aanchored%2520onto%2520the%2520MoSca%2520and%2520optimized%2520via%2520Gaussian%2520Splatting.%2520Additionally%252C%250Acamera%2520focal%2520length%2520and%2520poses%2520can%2520be%2520solved%2520using%2520bundle%2520adjustment%2520without%2520the%250Aneed%2520of%2520any%2520other%2520pose%2520estimation%2520tools.%2520Experiments%2520demonstrate%250Astate-of-the-art%2520performance%2520on%2520dynamic%2520rendering%2520benchmarks%2520and%2520its%250Aeffectiveness%2520on%2520real%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17421v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoSca%3A%20Dynamic%20Gaussian%20Fusion%20from%20Casual%20Videos%20via%204D%20Motion%0A%20%20Scaffolds&entry.906535625=Jiahui%20Lei%20and%20Yijia%20Weng%20and%20Adam%20Harley%20and%20Leonidas%20Guibas%20and%20Kostas%20Daniilidis&entry.1292438233=%20%20We%20introduce%204D%20Motion%20Scaffolds%20%28MoSca%29%2C%20a%20modern%204D%20reconstruction%20system%0Adesigned%20to%20reconstruct%20and%20synthesize%20novel%20views%20of%20dynamic%20scenes%20from%0Amonocular%20videos%20captured%20casually%20in%20the%20wild.%20To%20address%20such%20a%20challenging%0Aand%20ill-posed%20inverse%20problem%2C%20we%20leverage%20prior%20knowledge%20from%20foundational%0Avision%20models%20and%20lift%20the%20video%20data%20to%20a%20novel%20Motion%20Scaffold%20%28MoSca%29%0Arepresentation%2C%20which%20compactly%20and%20smoothly%20encodes%20the%20underlying%0Amotions/deformations.%20The%20scene%20geometry%20and%20appearance%20are%20then%20disentangled%0Afrom%20the%20deformation%20field%20and%20are%20encoded%20by%20globally%20fusing%20the%20Gaussians%0Aanchored%20onto%20the%20MoSca%20and%20optimized%20via%20Gaussian%20Splatting.%20Additionally%2C%0Acamera%20focal%20length%20and%20poses%20can%20be%20solved%20using%20bundle%20adjustment%20without%20the%0Aneed%20of%20any%20other%20pose%20estimation%20tools.%20Experiments%20demonstrate%0Astate-of-the-art%20performance%20on%20dynamic%20rendering%20benchmarks%20and%20its%0Aeffectiveness%20on%20real%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17421v2&entry.124074799=Read"},
{"title": "TexGaussian: Generating High-quality PBR Material via Octree-based 3D\n  Gaussian Splatting", "author": "Bojun Xiong and Jialun Liu and Jiakui Hu and Chenming Wu and Jinbo Wu and Xing Liu and Chen Zhao and Errui Ding and Zhouhui Lian", "abstract": "  Physically Based Rendering (PBR) materials play a crucial role in modern\ngraphics, enabling photorealistic rendering across diverse environment maps.\nDeveloping an effective and efficient algorithm that is capable of\nautomatically generating high-quality PBR materials rather than RGB texture for\n3D meshes can significantly streamline the 3D content creation. Most existing\nmethods leverage pre-trained 2D diffusion models for multi-view image\nsynthesis, which often leads to severe inconsistency between the generated\ntextures and input 3D meshes. This paper presents TexGaussian, a novel method\nthat uses octant-aligned 3D Gaussian Splatting for rapid PBR material\ngeneration. Specifically, we place each 3D Gaussian on the finest leaf node of\nthe octree built from the input 3D mesh to render the multiview images not only\nfor the albedo map but also for roughness and metallic. Moreover, our model is\ntrained in a regression manner instead of diffusion denoising, capable of\ngenerating the PBR material for a 3D mesh in a single feed-forward process.\nExtensive experiments on publicly available benchmarks demonstrate that our\nmethod synthesizes more visually pleasing PBR materials and runs faster than\nprevious methods in both unconditional and text-conditional scenarios, which\nexhibit better consistency with the given geometry. Our code and trained models\nare available at https://3d-aigc.github.io/TexGaussian.\n", "link": "http://arxiv.org/abs/2411.19654v1", "date": "2024-11-29", "relevancy": 3.2077, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6474}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6474}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexGaussian%3A%20Generating%20High-quality%20PBR%20Material%20via%20Octree-based%203D%0A%20%20Gaussian%20Splatting&body=Title%3A%20TexGaussian%3A%20Generating%20High-quality%20PBR%20Material%20via%20Octree-based%203D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Bojun%20Xiong%20and%20Jialun%20Liu%20and%20Jiakui%20Hu%20and%20Chenming%20Wu%20and%20Jinbo%20Wu%20and%20Xing%20Liu%20and%20Chen%20Zhao%20and%20Errui%20Ding%20and%20Zhouhui%20Lian%0AAbstract%3A%20%20%20Physically%20Based%20Rendering%20%28PBR%29%20materials%20play%20a%20crucial%20role%20in%20modern%0Agraphics%2C%20enabling%20photorealistic%20rendering%20across%20diverse%20environment%20maps.%0ADeveloping%20an%20effective%20and%20efficient%20algorithm%20that%20is%20capable%20of%0Aautomatically%20generating%20high-quality%20PBR%20materials%20rather%20than%20RGB%20texture%20for%0A3D%20meshes%20can%20significantly%20streamline%20the%203D%20content%20creation.%20Most%20existing%0Amethods%20leverage%20pre-trained%202D%20diffusion%20models%20for%20multi-view%20image%0Asynthesis%2C%20which%20often%20leads%20to%20severe%20inconsistency%20between%20the%20generated%0Atextures%20and%20input%203D%20meshes.%20This%20paper%20presents%20TexGaussian%2C%20a%20novel%20method%0Athat%20uses%20octant-aligned%203D%20Gaussian%20Splatting%20for%20rapid%20PBR%20material%0Ageneration.%20Specifically%2C%20we%20place%20each%203D%20Gaussian%20on%20the%20finest%20leaf%20node%20of%0Athe%20octree%20built%20from%20the%20input%203D%20mesh%20to%20render%20the%20multiview%20images%20not%20only%0Afor%20the%20albedo%20map%20but%20also%20for%20roughness%20and%20metallic.%20Moreover%2C%20our%20model%20is%0Atrained%20in%20a%20regression%20manner%20instead%20of%20diffusion%20denoising%2C%20capable%20of%0Agenerating%20the%20PBR%20material%20for%20a%203D%20mesh%20in%20a%20single%20feed-forward%20process.%0AExtensive%20experiments%20on%20publicly%20available%20benchmarks%20demonstrate%20that%20our%0Amethod%20synthesizes%20more%20visually%20pleasing%20PBR%20materials%20and%20runs%20faster%20than%0Aprevious%20methods%20in%20both%20unconditional%20and%20text-conditional%20scenarios%2C%20which%0Aexhibit%20better%20consistency%20with%20the%20given%20geometry.%20Our%20code%20and%20trained%20models%0Aare%20available%20at%20https%3A//3d-aigc.github.io/TexGaussian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexGaussian%253A%2520Generating%2520High-quality%2520PBR%2520Material%2520via%2520Octree-based%25203D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DBojun%2520Xiong%2520and%2520Jialun%2520Liu%2520and%2520Jiakui%2520Hu%2520and%2520Chenming%2520Wu%2520and%2520Jinbo%2520Wu%2520and%2520Xing%2520Liu%2520and%2520Chen%2520Zhao%2520and%2520Errui%2520Ding%2520and%2520Zhouhui%2520Lian%26entry.1292438233%3D%2520%2520Physically%2520Based%2520Rendering%2520%2528PBR%2529%2520materials%2520play%2520a%2520crucial%2520role%2520in%2520modern%250Agraphics%252C%2520enabling%2520photorealistic%2520rendering%2520across%2520diverse%2520environment%2520maps.%250ADeveloping%2520an%2520effective%2520and%2520efficient%2520algorithm%2520that%2520is%2520capable%2520of%250Aautomatically%2520generating%2520high-quality%2520PBR%2520materials%2520rather%2520than%2520RGB%2520texture%2520for%250A3D%2520meshes%2520can%2520significantly%2520streamline%2520the%25203D%2520content%2520creation.%2520Most%2520existing%250Amethods%2520leverage%2520pre-trained%25202D%2520diffusion%2520models%2520for%2520multi-view%2520image%250Asynthesis%252C%2520which%2520often%2520leads%2520to%2520severe%2520inconsistency%2520between%2520the%2520generated%250Atextures%2520and%2520input%25203D%2520meshes.%2520This%2520paper%2520presents%2520TexGaussian%252C%2520a%2520novel%2520method%250Athat%2520uses%2520octant-aligned%25203D%2520Gaussian%2520Splatting%2520for%2520rapid%2520PBR%2520material%250Ageneration.%2520Specifically%252C%2520we%2520place%2520each%25203D%2520Gaussian%2520on%2520the%2520finest%2520leaf%2520node%2520of%250Athe%2520octree%2520built%2520from%2520the%2520input%25203D%2520mesh%2520to%2520render%2520the%2520multiview%2520images%2520not%2520only%250Afor%2520the%2520albedo%2520map%2520but%2520also%2520for%2520roughness%2520and%2520metallic.%2520Moreover%252C%2520our%2520model%2520is%250Atrained%2520in%2520a%2520regression%2520manner%2520instead%2520of%2520diffusion%2520denoising%252C%2520capable%2520of%250Agenerating%2520the%2520PBR%2520material%2520for%2520a%25203D%2520mesh%2520in%2520a%2520single%2520feed-forward%2520process.%250AExtensive%2520experiments%2520on%2520publicly%2520available%2520benchmarks%2520demonstrate%2520that%2520our%250Amethod%2520synthesizes%2520more%2520visually%2520pleasing%2520PBR%2520materials%2520and%2520runs%2520faster%2520than%250Aprevious%2520methods%2520in%2520both%2520unconditional%2520and%2520text-conditional%2520scenarios%252C%2520which%250Aexhibit%2520better%2520consistency%2520with%2520the%2520given%2520geometry.%2520Our%2520code%2520and%2520trained%2520models%250Aare%2520available%2520at%2520https%253A//3d-aigc.github.io/TexGaussian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexGaussian%3A%20Generating%20High-quality%20PBR%20Material%20via%20Octree-based%203D%0A%20%20Gaussian%20Splatting&entry.906535625=Bojun%20Xiong%20and%20Jialun%20Liu%20and%20Jiakui%20Hu%20and%20Chenming%20Wu%20and%20Jinbo%20Wu%20and%20Xing%20Liu%20and%20Chen%20Zhao%20and%20Errui%20Ding%20and%20Zhouhui%20Lian&entry.1292438233=%20%20Physically%20Based%20Rendering%20%28PBR%29%20materials%20play%20a%20crucial%20role%20in%20modern%0Agraphics%2C%20enabling%20photorealistic%20rendering%20across%20diverse%20environment%20maps.%0ADeveloping%20an%20effective%20and%20efficient%20algorithm%20that%20is%20capable%20of%0Aautomatically%20generating%20high-quality%20PBR%20materials%20rather%20than%20RGB%20texture%20for%0A3D%20meshes%20can%20significantly%20streamline%20the%203D%20content%20creation.%20Most%20existing%0Amethods%20leverage%20pre-trained%202D%20diffusion%20models%20for%20multi-view%20image%0Asynthesis%2C%20which%20often%20leads%20to%20severe%20inconsistency%20between%20the%20generated%0Atextures%20and%20input%203D%20meshes.%20This%20paper%20presents%20TexGaussian%2C%20a%20novel%20method%0Athat%20uses%20octant-aligned%203D%20Gaussian%20Splatting%20for%20rapid%20PBR%20material%0Ageneration.%20Specifically%2C%20we%20place%20each%203D%20Gaussian%20on%20the%20finest%20leaf%20node%20of%0Athe%20octree%20built%20from%20the%20input%203D%20mesh%20to%20render%20the%20multiview%20images%20not%20only%0Afor%20the%20albedo%20map%20but%20also%20for%20roughness%20and%20metallic.%20Moreover%2C%20our%20model%20is%0Atrained%20in%20a%20regression%20manner%20instead%20of%20diffusion%20denoising%2C%20capable%20of%0Agenerating%20the%20PBR%20material%20for%20a%203D%20mesh%20in%20a%20single%20feed-forward%20process.%0AExtensive%20experiments%20on%20publicly%20available%20benchmarks%20demonstrate%20that%20our%0Amethod%20synthesizes%20more%20visually%20pleasing%20PBR%20materials%20and%20runs%20faster%20than%0Aprevious%20methods%20in%20both%20unconditional%20and%20text-conditional%20scenarios%2C%20which%0Aexhibit%20better%20consistency%20with%20the%20given%20geometry.%20Our%20code%20and%20trained%20models%0Aare%20available%20at%20https%3A//3d-aigc.github.io/TexGaussian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19654v1&entry.124074799=Read"},
{"title": "AlphaTablets: A Generic Plane Representation for 3D Planar\n  Reconstruction from Monocular Videos", "author": "Yuze He and Wang Zhao and Shaohui Liu and Yubin Hu and Yushi Bai and Yu-Hui Wen and Yong-Jin Liu", "abstract": "  We introduce AlphaTablets, a novel and generic representation of 3D planes\nthat features continuous 3D surface and precise boundary delineation. By\nrepresenting 3D planes as rectangles with alpha channels, AlphaTablets combine\nthe advantages of current 2D and 3D plane representations, enabling accurate,\nconsistent and flexible modeling of 3D planes. We derive differentiable\nrasterization on top of AlphaTablets to efficiently render 3D planes into\nimages, and propose a novel bottom-up pipeline for 3D planar reconstruction\nfrom monocular videos. Starting with 2D superpixels and geometric cues from\npre-trained models, we initialize 3D planes as AlphaTablets and optimize them\nvia differentiable rendering. An effective merging scheme is introduced to\nfacilitate the growth and refinement of AlphaTablets. Through iterative\noptimization and merging, we reconstruct complete and accurate 3D planes with\nsolid surfaces and clear boundaries. Extensive experiments on the ScanNet\ndataset demonstrate state-of-the-art performance in 3D planar reconstruction,\nunderscoring the great potential of AlphaTablets as a generic 3D plane\nrepresentation for various applications. Project page is available at:\nhttps://hyzcluster.github.io/alphatablets\n", "link": "http://arxiv.org/abs/2411.19950v1", "date": "2024-11-29", "relevancy": 3.092, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6195}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6178}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaTablets%3A%20A%20Generic%20Plane%20Representation%20for%203D%20Planar%0A%20%20Reconstruction%20from%20Monocular%20Videos&body=Title%3A%20AlphaTablets%3A%20A%20Generic%20Plane%20Representation%20for%203D%20Planar%0A%20%20Reconstruction%20from%20Monocular%20Videos%0AAuthor%3A%20Yuze%20He%20and%20Wang%20Zhao%20and%20Shaohui%20Liu%20and%20Yubin%20Hu%20and%20Yushi%20Bai%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu%0AAbstract%3A%20%20%20We%20introduce%20AlphaTablets%2C%20a%20novel%20and%20generic%20representation%20of%203D%20planes%0Athat%20features%20continuous%203D%20surface%20and%20precise%20boundary%20delineation.%20By%0Arepresenting%203D%20planes%20as%20rectangles%20with%20alpha%20channels%2C%20AlphaTablets%20combine%0Athe%20advantages%20of%20current%202D%20and%203D%20plane%20representations%2C%20enabling%20accurate%2C%0Aconsistent%20and%20flexible%20modeling%20of%203D%20planes.%20We%20derive%20differentiable%0Arasterization%20on%20top%20of%20AlphaTablets%20to%20efficiently%20render%203D%20planes%20into%0Aimages%2C%20and%20propose%20a%20novel%20bottom-up%20pipeline%20for%203D%20planar%20reconstruction%0Afrom%20monocular%20videos.%20Starting%20with%202D%20superpixels%20and%20geometric%20cues%20from%0Apre-trained%20models%2C%20we%20initialize%203D%20planes%20as%20AlphaTablets%20and%20optimize%20them%0Avia%20differentiable%20rendering.%20An%20effective%20merging%20scheme%20is%20introduced%20to%0Afacilitate%20the%20growth%20and%20refinement%20of%20AlphaTablets.%20Through%20iterative%0Aoptimization%20and%20merging%2C%20we%20reconstruct%20complete%20and%20accurate%203D%20planes%20with%0Asolid%20surfaces%20and%20clear%20boundaries.%20Extensive%20experiments%20on%20the%20ScanNet%0Adataset%20demonstrate%20state-of-the-art%20performance%20in%203D%20planar%20reconstruction%2C%0Aunderscoring%20the%20great%20potential%20of%20AlphaTablets%20as%20a%20generic%203D%20plane%0Arepresentation%20for%20various%20applications.%20Project%20page%20is%20available%20at%3A%0Ahttps%3A//hyzcluster.github.io/alphatablets%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaTablets%253A%2520A%2520Generic%2520Plane%2520Representation%2520for%25203D%2520Planar%250A%2520%2520Reconstruction%2520from%2520Monocular%2520Videos%26entry.906535625%3DYuze%2520He%2520and%2520Wang%2520Zhao%2520and%2520Shaohui%2520Liu%2520and%2520Yubin%2520Hu%2520and%2520Yushi%2520Bai%2520and%2520Yu-Hui%2520Wen%2520and%2520Yong-Jin%2520Liu%26entry.1292438233%3D%2520%2520We%2520introduce%2520AlphaTablets%252C%2520a%2520novel%2520and%2520generic%2520representation%2520of%25203D%2520planes%250Athat%2520features%2520continuous%25203D%2520surface%2520and%2520precise%2520boundary%2520delineation.%2520By%250Arepresenting%25203D%2520planes%2520as%2520rectangles%2520with%2520alpha%2520channels%252C%2520AlphaTablets%2520combine%250Athe%2520advantages%2520of%2520current%25202D%2520and%25203D%2520plane%2520representations%252C%2520enabling%2520accurate%252C%250Aconsistent%2520and%2520flexible%2520modeling%2520of%25203D%2520planes.%2520We%2520derive%2520differentiable%250Arasterization%2520on%2520top%2520of%2520AlphaTablets%2520to%2520efficiently%2520render%25203D%2520planes%2520into%250Aimages%252C%2520and%2520propose%2520a%2520novel%2520bottom-up%2520pipeline%2520for%25203D%2520planar%2520reconstruction%250Afrom%2520monocular%2520videos.%2520Starting%2520with%25202D%2520superpixels%2520and%2520geometric%2520cues%2520from%250Apre-trained%2520models%252C%2520we%2520initialize%25203D%2520planes%2520as%2520AlphaTablets%2520and%2520optimize%2520them%250Avia%2520differentiable%2520rendering.%2520An%2520effective%2520merging%2520scheme%2520is%2520introduced%2520to%250Afacilitate%2520the%2520growth%2520and%2520refinement%2520of%2520AlphaTablets.%2520Through%2520iterative%250Aoptimization%2520and%2520merging%252C%2520we%2520reconstruct%2520complete%2520and%2520accurate%25203D%2520planes%2520with%250Asolid%2520surfaces%2520and%2520clear%2520boundaries.%2520Extensive%2520experiments%2520on%2520the%2520ScanNet%250Adataset%2520demonstrate%2520state-of-the-art%2520performance%2520in%25203D%2520planar%2520reconstruction%252C%250Aunderscoring%2520the%2520great%2520potential%2520of%2520AlphaTablets%2520as%2520a%2520generic%25203D%2520plane%250Arepresentation%2520for%2520various%2520applications.%2520Project%2520page%2520is%2520available%2520at%253A%250Ahttps%253A//hyzcluster.github.io/alphatablets%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaTablets%3A%20A%20Generic%20Plane%20Representation%20for%203D%20Planar%0A%20%20Reconstruction%20from%20Monocular%20Videos&entry.906535625=Yuze%20He%20and%20Wang%20Zhao%20and%20Shaohui%20Liu%20and%20Yubin%20Hu%20and%20Yushi%20Bai%20and%20Yu-Hui%20Wen%20and%20Yong-Jin%20Liu&entry.1292438233=%20%20We%20introduce%20AlphaTablets%2C%20a%20novel%20and%20generic%20representation%20of%203D%20planes%0Athat%20features%20continuous%203D%20surface%20and%20precise%20boundary%20delineation.%20By%0Arepresenting%203D%20planes%20as%20rectangles%20with%20alpha%20channels%2C%20AlphaTablets%20combine%0Athe%20advantages%20of%20current%202D%20and%203D%20plane%20representations%2C%20enabling%20accurate%2C%0Aconsistent%20and%20flexible%20modeling%20of%203D%20planes.%20We%20derive%20differentiable%0Arasterization%20on%20top%20of%20AlphaTablets%20to%20efficiently%20render%203D%20planes%20into%0Aimages%2C%20and%20propose%20a%20novel%20bottom-up%20pipeline%20for%203D%20planar%20reconstruction%0Afrom%20monocular%20videos.%20Starting%20with%202D%20superpixels%20and%20geometric%20cues%20from%0Apre-trained%20models%2C%20we%20initialize%203D%20planes%20as%20AlphaTablets%20and%20optimize%20them%0Avia%20differentiable%20rendering.%20An%20effective%20merging%20scheme%20is%20introduced%20to%0Afacilitate%20the%20growth%20and%20refinement%20of%20AlphaTablets.%20Through%20iterative%0Aoptimization%20and%20merging%2C%20we%20reconstruct%20complete%20and%20accurate%203D%20planes%20with%0Asolid%20surfaces%20and%20clear%20boundaries.%20Extensive%20experiments%20on%20the%20ScanNet%0Adataset%20demonstrate%20state-of-the-art%20performance%20in%203D%20planar%20reconstruction%2C%0Aunderscoring%20the%20great%20potential%20of%20AlphaTablets%20as%20a%20generic%203D%20plane%0Arepresentation%20for%20various%20applications.%20Project%20page%20is%20available%20at%3A%0Ahttps%3A//hyzcluster.github.io/alphatablets%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19950v1&entry.124074799=Read"},
{"title": "Tortho-Gaussian: Splatting True Digital Orthophoto Maps", "author": "Xin Wang and Wendi Zhang and Hong Xie and Haibin Ai and Qiangqiang Yuan and Zongqian Zhan", "abstract": "  True Digital Orthophoto Maps (TDOMs) are essential products for digital twins\nand Geographic Information Systems (GIS). Traditionally, TDOM generation\ninvolves a complex set of traditional photogrammetric process, which may\ndeteriorate due to various challenges, including inaccurate Digital Surface\nModel (DSM), degenerated occlusion detections, and visual artifacts in weak\ntexture regions and reflective surfaces, etc. To address these challenges, we\nintroduce TOrtho-Gaussian, a novel method inspired by 3D Gaussian Splatting\n(3DGS) that generates TDOMs through orthogonal splatting of optimized\nanisotropic Gaussian kernel. More specifically, we first simplify the\northophoto generation by orthographically splatting the Gaussian kernels onto\n2D image planes, formulating a geometrically elegant solution that avoids the\nneed for explicit DSM and occlusion detection. Second, to produce TDOM of\nlarge-scale area, a divide-and-conquer strategy is adopted to optimize memory\nusage and time efficiency of training and rendering for 3DGS. Lastly, we design\na fully anisotropic Gaussian kernel that adapts to the varying characteristics\nof different regions, particularly improving the rendering quality of\nreflective surfaces and slender structures. Extensive experimental evaluations\ndemonstrate that our method outperforms existing commercial software in several\naspects, including the accuracy of building boundaries, the visual quality of\nlow-texture regions and building facades. These results underscore the\npotential of our approach for large-scale urban scene reconstruction, offering\na robust alternative for enhancing TDOM quality and scalability.\n", "link": "http://arxiv.org/abs/2411.19594v1", "date": "2024-11-29", "relevancy": 3.0579, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6529}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6143}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tortho-Gaussian%3A%20Splatting%20True%20Digital%20Orthophoto%20Maps&body=Title%3A%20Tortho-Gaussian%3A%20Splatting%20True%20Digital%20Orthophoto%20Maps%0AAuthor%3A%20Xin%20Wang%20and%20Wendi%20Zhang%20and%20Hong%20Xie%20and%20Haibin%20Ai%20and%20Qiangqiang%20Yuan%20and%20Zongqian%20Zhan%0AAbstract%3A%20%20%20True%20Digital%20Orthophoto%20Maps%20%28TDOMs%29%20are%20essential%20products%20for%20digital%20twins%0Aand%20Geographic%20Information%20Systems%20%28GIS%29.%20Traditionally%2C%20TDOM%20generation%0Ainvolves%20a%20complex%20set%20of%20traditional%20photogrammetric%20process%2C%20which%20may%0Adeteriorate%20due%20to%20various%20challenges%2C%20including%20inaccurate%20Digital%20Surface%0AModel%20%28DSM%29%2C%20degenerated%20occlusion%20detections%2C%20and%20visual%20artifacts%20in%20weak%0Atexture%20regions%20and%20reflective%20surfaces%2C%20etc.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20TOrtho-Gaussian%2C%20a%20novel%20method%20inspired%20by%203D%20Gaussian%20Splatting%0A%283DGS%29%20that%20generates%20TDOMs%20through%20orthogonal%20splatting%20of%20optimized%0Aanisotropic%20Gaussian%20kernel.%20More%20specifically%2C%20we%20first%20simplify%20the%0Aorthophoto%20generation%20by%20orthographically%20splatting%20the%20Gaussian%20kernels%20onto%0A2D%20image%20planes%2C%20formulating%20a%20geometrically%20elegant%20solution%20that%20avoids%20the%0Aneed%20for%20explicit%20DSM%20and%20occlusion%20detection.%20Second%2C%20to%20produce%20TDOM%20of%0Alarge-scale%20area%2C%20a%20divide-and-conquer%20strategy%20is%20adopted%20to%20optimize%20memory%0Ausage%20and%20time%20efficiency%20of%20training%20and%20rendering%20for%203DGS.%20Lastly%2C%20we%20design%0Aa%20fully%20anisotropic%20Gaussian%20kernel%20that%20adapts%20to%20the%20varying%20characteristics%0Aof%20different%20regions%2C%20particularly%20improving%20the%20rendering%20quality%20of%0Areflective%20surfaces%20and%20slender%20structures.%20Extensive%20experimental%20evaluations%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20commercial%20software%20in%20several%0Aaspects%2C%20including%20the%20accuracy%20of%20building%20boundaries%2C%20the%20visual%20quality%20of%0Alow-texture%20regions%20and%20building%20facades.%20These%20results%20underscore%20the%0Apotential%20of%20our%20approach%20for%20large-scale%20urban%20scene%20reconstruction%2C%20offering%0Aa%20robust%20alternative%20for%20enhancing%20TDOM%20quality%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTortho-Gaussian%253A%2520Splatting%2520True%2520Digital%2520Orthophoto%2520Maps%26entry.906535625%3DXin%2520Wang%2520and%2520Wendi%2520Zhang%2520and%2520Hong%2520Xie%2520and%2520Haibin%2520Ai%2520and%2520Qiangqiang%2520Yuan%2520and%2520Zongqian%2520Zhan%26entry.1292438233%3D%2520%2520True%2520Digital%2520Orthophoto%2520Maps%2520%2528TDOMs%2529%2520are%2520essential%2520products%2520for%2520digital%2520twins%250Aand%2520Geographic%2520Information%2520Systems%2520%2528GIS%2529.%2520Traditionally%252C%2520TDOM%2520generation%250Ainvolves%2520a%2520complex%2520set%2520of%2520traditional%2520photogrammetric%2520process%252C%2520which%2520may%250Adeteriorate%2520due%2520to%2520various%2520challenges%252C%2520including%2520inaccurate%2520Digital%2520Surface%250AModel%2520%2528DSM%2529%252C%2520degenerated%2520occlusion%2520detections%252C%2520and%2520visual%2520artifacts%2520in%2520weak%250Atexture%2520regions%2520and%2520reflective%2520surfaces%252C%2520etc.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520TOrtho-Gaussian%252C%2520a%2520novel%2520method%2520inspired%2520by%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520that%2520generates%2520TDOMs%2520through%2520orthogonal%2520splatting%2520of%2520optimized%250Aanisotropic%2520Gaussian%2520kernel.%2520More%2520specifically%252C%2520we%2520first%2520simplify%2520the%250Aorthophoto%2520generation%2520by%2520orthographically%2520splatting%2520the%2520Gaussian%2520kernels%2520onto%250A2D%2520image%2520planes%252C%2520formulating%2520a%2520geometrically%2520elegant%2520solution%2520that%2520avoids%2520the%250Aneed%2520for%2520explicit%2520DSM%2520and%2520occlusion%2520detection.%2520Second%252C%2520to%2520produce%2520TDOM%2520of%250Alarge-scale%2520area%252C%2520a%2520divide-and-conquer%2520strategy%2520is%2520adopted%2520to%2520optimize%2520memory%250Ausage%2520and%2520time%2520efficiency%2520of%2520training%2520and%2520rendering%2520for%25203DGS.%2520Lastly%252C%2520we%2520design%250Aa%2520fully%2520anisotropic%2520Gaussian%2520kernel%2520that%2520adapts%2520to%2520the%2520varying%2520characteristics%250Aof%2520different%2520regions%252C%2520particularly%2520improving%2520the%2520rendering%2520quality%2520of%250Areflective%2520surfaces%2520and%2520slender%2520structures.%2520Extensive%2520experimental%2520evaluations%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520commercial%2520software%2520in%2520several%250Aaspects%252C%2520including%2520the%2520accuracy%2520of%2520building%2520boundaries%252C%2520the%2520visual%2520quality%2520of%250Alow-texture%2520regions%2520and%2520building%2520facades.%2520These%2520results%2520underscore%2520the%250Apotential%2520of%2520our%2520approach%2520for%2520large-scale%2520urban%2520scene%2520reconstruction%252C%2520offering%250Aa%2520robust%2520alternative%2520for%2520enhancing%2520TDOM%2520quality%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tortho-Gaussian%3A%20Splatting%20True%20Digital%20Orthophoto%20Maps&entry.906535625=Xin%20Wang%20and%20Wendi%20Zhang%20and%20Hong%20Xie%20and%20Haibin%20Ai%20and%20Qiangqiang%20Yuan%20and%20Zongqian%20Zhan&entry.1292438233=%20%20True%20Digital%20Orthophoto%20Maps%20%28TDOMs%29%20are%20essential%20products%20for%20digital%20twins%0Aand%20Geographic%20Information%20Systems%20%28GIS%29.%20Traditionally%2C%20TDOM%20generation%0Ainvolves%20a%20complex%20set%20of%20traditional%20photogrammetric%20process%2C%20which%20may%0Adeteriorate%20due%20to%20various%20challenges%2C%20including%20inaccurate%20Digital%20Surface%0AModel%20%28DSM%29%2C%20degenerated%20occlusion%20detections%2C%20and%20visual%20artifacts%20in%20weak%0Atexture%20regions%20and%20reflective%20surfaces%2C%20etc.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20TOrtho-Gaussian%2C%20a%20novel%20method%20inspired%20by%203D%20Gaussian%20Splatting%0A%283DGS%29%20that%20generates%20TDOMs%20through%20orthogonal%20splatting%20of%20optimized%0Aanisotropic%20Gaussian%20kernel.%20More%20specifically%2C%20we%20first%20simplify%20the%0Aorthophoto%20generation%20by%20orthographically%20splatting%20the%20Gaussian%20kernels%20onto%0A2D%20image%20planes%2C%20formulating%20a%20geometrically%20elegant%20solution%20that%20avoids%20the%0Aneed%20for%20explicit%20DSM%20and%20occlusion%20detection.%20Second%2C%20to%20produce%20TDOM%20of%0Alarge-scale%20area%2C%20a%20divide-and-conquer%20strategy%20is%20adopted%20to%20optimize%20memory%0Ausage%20and%20time%20efficiency%20of%20training%20and%20rendering%20for%203DGS.%20Lastly%2C%20we%20design%0Aa%20fully%20anisotropic%20Gaussian%20kernel%20that%20adapts%20to%20the%20varying%20characteristics%0Aof%20different%20regions%2C%20particularly%20improving%20the%20rendering%20quality%20of%0Areflective%20surfaces%20and%20slender%20structures.%20Extensive%20experimental%20evaluations%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20commercial%20software%20in%20several%0Aaspects%2C%20including%20the%20accuracy%20of%20building%20boundaries%2C%20the%20visual%20quality%20of%0Alow-texture%20regions%20and%20building%20facades.%20These%20results%20underscore%20the%0Apotential%20of%20our%20approach%20for%20large-scale%20urban%20scene%20reconstruction%2C%20offering%0Aa%20robust%20alternative%20for%20enhancing%20TDOM%20quality%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19594v1&entry.124074799=Read"},
{"title": "PerLA: Perceptive 3D Language Assistant", "author": "Guofeng Mei and Wei Lin and Luigi Riz and Yujiao Wu and Fabio Poiesi and Yiming Wang", "abstract": "  Enabling Large Language Models (LLMs) to understand the 3D physical world is\nan emerging yet challenging research direction. Current strategies for\nprocessing point clouds typically downsample the scene or divide it into\nsmaller parts for separate analysis. However, both approaches risk losing key\nlocal details or global contextual information. In this paper, we introduce\nPerLA, a 3D language assistant designed to be more perceptive to both details\nand context, making visual representations more informative for the LLM. PerLA\ncaptures high-resolution (local) details in parallel from different point cloud\nareas and integrates them with (global) context obtained from a\nlower-resolution whole point cloud. We present a novel algorithm that preserves\npoint cloud locality through the Hilbert curve and effectively aggregates\nlocal-to-global information via cross-attention and a graph neural network.\nLastly, we introduce a novel loss for local representation consensus to promote\ntraining stability. PerLA outperforms state-of-the-art 3D language assistants,\nwith gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on\nScanRefer and +3.88 on Nr3D for dense\ncaptioning.\\url{https://gfmei.github.io/PerLA/}\n", "link": "http://arxiv.org/abs/2411.19774v1", "date": "2024-11-29", "relevancy": 3.0351, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6138}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6138}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PerLA%3A%20Perceptive%203D%20Language%20Assistant&body=Title%3A%20PerLA%3A%20Perceptive%203D%20Language%20Assistant%0AAuthor%3A%20Guofeng%20Mei%20and%20Wei%20Lin%20and%20Luigi%20Riz%20and%20Yujiao%20Wu%20and%20Fabio%20Poiesi%20and%20Yiming%20Wang%0AAbstract%3A%20%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20understand%20the%203D%20physical%20world%20is%0Aan%20emerging%20yet%20challenging%20research%20direction.%20Current%20strategies%20for%0Aprocessing%20point%20clouds%20typically%20downsample%20the%20scene%20or%20divide%20it%20into%0Asmaller%20parts%20for%20separate%20analysis.%20However%2C%20both%20approaches%20risk%20losing%20key%0Alocal%20details%20or%20global%20contextual%20information.%20In%20this%20paper%2C%20we%20introduce%0APerLA%2C%20a%203D%20language%20assistant%20designed%20to%20be%20more%20perceptive%20to%20both%20details%0Aand%20context%2C%20making%20visual%20representations%20more%20informative%20for%20the%20LLM.%20PerLA%0Acaptures%20high-resolution%20%28local%29%20details%20in%20parallel%20from%20different%20point%20cloud%0Aareas%20and%20integrates%20them%20with%20%28global%29%20context%20obtained%20from%20a%0Alower-resolution%20whole%20point%20cloud.%20We%20present%20a%20novel%20algorithm%20that%20preserves%0Apoint%20cloud%20locality%20through%20the%20Hilbert%20curve%20and%20effectively%20aggregates%0Alocal-to-global%20information%20via%20cross-attention%20and%20a%20graph%20neural%20network.%0ALastly%2C%20we%20introduce%20a%20novel%20loss%20for%20local%20representation%20consensus%20to%20promote%0Atraining%20stability.%20PerLA%20outperforms%20state-of-the-art%203D%20language%20assistants%2C%0Awith%20gains%20of%20up%20to%20%2B1.34%20CiDEr%20on%20ScanQA%20for%20question%20answering%2C%20and%20%2B4.22%20on%0AScanRefer%20and%20%2B3.88%20on%20Nr3D%20for%20dense%0Acaptioning.%5Curl%7Bhttps%3A//gfmei.github.io/PerLA/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerLA%253A%2520Perceptive%25203D%2520Language%2520Assistant%26entry.906535625%3DGuofeng%2520Mei%2520and%2520Wei%2520Lin%2520and%2520Luigi%2520Riz%2520and%2520Yujiao%2520Wu%2520and%2520Fabio%2520Poiesi%2520and%2520Yiming%2520Wang%26entry.1292438233%3D%2520%2520Enabling%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520understand%2520the%25203D%2520physical%2520world%2520is%250Aan%2520emerging%2520yet%2520challenging%2520research%2520direction.%2520Current%2520strategies%2520for%250Aprocessing%2520point%2520clouds%2520typically%2520downsample%2520the%2520scene%2520or%2520divide%2520it%2520into%250Asmaller%2520parts%2520for%2520separate%2520analysis.%2520However%252C%2520both%2520approaches%2520risk%2520losing%2520key%250Alocal%2520details%2520or%2520global%2520contextual%2520information.%2520In%2520this%2520paper%252C%2520we%2520introduce%250APerLA%252C%2520a%25203D%2520language%2520assistant%2520designed%2520to%2520be%2520more%2520perceptive%2520to%2520both%2520details%250Aand%2520context%252C%2520making%2520visual%2520representations%2520more%2520informative%2520for%2520the%2520LLM.%2520PerLA%250Acaptures%2520high-resolution%2520%2528local%2529%2520details%2520in%2520parallel%2520from%2520different%2520point%2520cloud%250Aareas%2520and%2520integrates%2520them%2520with%2520%2528global%2529%2520context%2520obtained%2520from%2520a%250Alower-resolution%2520whole%2520point%2520cloud.%2520We%2520present%2520a%2520novel%2520algorithm%2520that%2520preserves%250Apoint%2520cloud%2520locality%2520through%2520the%2520Hilbert%2520curve%2520and%2520effectively%2520aggregates%250Alocal-to-global%2520information%2520via%2520cross-attention%2520and%2520a%2520graph%2520neural%2520network.%250ALastly%252C%2520we%2520introduce%2520a%2520novel%2520loss%2520for%2520local%2520representation%2520consensus%2520to%2520promote%250Atraining%2520stability.%2520PerLA%2520outperforms%2520state-of-the-art%25203D%2520language%2520assistants%252C%250Awith%2520gains%2520of%2520up%2520to%2520%252B1.34%2520CiDEr%2520on%2520ScanQA%2520for%2520question%2520answering%252C%2520and%2520%252B4.22%2520on%250AScanRefer%2520and%2520%252B3.88%2520on%2520Nr3D%2520for%2520dense%250Acaptioning.%255Curl%257Bhttps%253A//gfmei.github.io/PerLA/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PerLA%3A%20Perceptive%203D%20Language%20Assistant&entry.906535625=Guofeng%20Mei%20and%20Wei%20Lin%20and%20Luigi%20Riz%20and%20Yujiao%20Wu%20and%20Fabio%20Poiesi%20and%20Yiming%20Wang&entry.1292438233=%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20understand%20the%203D%20physical%20world%20is%0Aan%20emerging%20yet%20challenging%20research%20direction.%20Current%20strategies%20for%0Aprocessing%20point%20clouds%20typically%20downsample%20the%20scene%20or%20divide%20it%20into%0Asmaller%20parts%20for%20separate%20analysis.%20However%2C%20both%20approaches%20risk%20losing%20key%0Alocal%20details%20or%20global%20contextual%20information.%20In%20this%20paper%2C%20we%20introduce%0APerLA%2C%20a%203D%20language%20assistant%20designed%20to%20be%20more%20perceptive%20to%20both%20details%0Aand%20context%2C%20making%20visual%20representations%20more%20informative%20for%20the%20LLM.%20PerLA%0Acaptures%20high-resolution%20%28local%29%20details%20in%20parallel%20from%20different%20point%20cloud%0Aareas%20and%20integrates%20them%20with%20%28global%29%20context%20obtained%20from%20a%0Alower-resolution%20whole%20point%20cloud.%20We%20present%20a%20novel%20algorithm%20that%20preserves%0Apoint%20cloud%20locality%20through%20the%20Hilbert%20curve%20and%20effectively%20aggregates%0Alocal-to-global%20information%20via%20cross-attention%20and%20a%20graph%20neural%20network.%0ALastly%2C%20we%20introduce%20a%20novel%20loss%20for%20local%20representation%20consensus%20to%20promote%0Atraining%20stability.%20PerLA%20outperforms%20state-of-the-art%203D%20language%20assistants%2C%0Awith%20gains%20of%20up%20to%20%2B1.34%20CiDEr%20on%20ScanQA%20for%20question%20answering%2C%20and%20%2B4.22%20on%0AScanRefer%20and%20%2B3.88%20on%20Nr3D%20for%20dense%0Acaptioning.%5Curl%7Bhttps%3A//gfmei.github.io/PerLA/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19774v1&entry.124074799=Read"},
{"title": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive\n  Tokens", "author": "Chi Su and Xiaoxuan Ma and Jiajun Su and Yizhou Wang", "abstract": "  We propose a one-stage framework for real-time multi-person 3D human mesh\nestimation from a single RGB image. While current one-stage methods, which\nfollow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with\nhigh-resolution inputs, we observe that this particularly benefits the\nestimation of individuals in smaller scales of the image (e.g., those far from\nthe camera), but at the cost of significantly increased computation overhead.\nTo address this, we introduce scale-adaptive tokens that are dynamically\nadjusted based on the relative scale of each individual in the image within the\nDETR framework. Specifically, individuals in smaller scales are processed at\nhigher resolutions, larger ones at lower resolutions, and background regions\nare further distilled. These scale-adaptive tokens more efficiently encode the\nimage features, facilitating subsequent decoding to regress the human mesh,\nwhile allowing the model to allocate computational resources more effectively\nand focus on more challenging cases. Experiments show that our method preserves\nthe accuracy benefits of high-resolution processing while substantially\nreducing computational cost, achieving real-time inference with performance\ncomparable to SOTA methods.\n", "link": "http://arxiv.org/abs/2411.19824v1", "date": "2024-11-29", "relevancy": 3.0302, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6215}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6048}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAT-HMR%3A%20Real-Time%20Multi-Person%203D%20Mesh%20Estimation%20via%20Scale-Adaptive%0A%20%20Tokens&body=Title%3A%20SAT-HMR%3A%20Real-Time%20Multi-Person%203D%20Mesh%20Estimation%20via%20Scale-Adaptive%0A%20%20Tokens%0AAuthor%3A%20Chi%20Su%20and%20Xiaoxuan%20Ma%20and%20Jiajun%20Su%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20We%20propose%20a%20one-stage%20framework%20for%20real-time%20multi-person%203D%20human%20mesh%0Aestimation%20from%20a%20single%20RGB%20image.%20While%20current%20one-stage%20methods%2C%20which%0Afollow%20a%20DETR-style%20pipeline%2C%20achieve%20state-of-the-art%20%28SOTA%29%20performance%20with%0Ahigh-resolution%20inputs%2C%20we%20observe%20that%20this%20particularly%20benefits%20the%0Aestimation%20of%20individuals%20in%20smaller%20scales%20of%20the%20image%20%28e.g.%2C%20those%20far%20from%0Athe%20camera%29%2C%20but%20at%20the%20cost%20of%20significantly%20increased%20computation%20overhead.%0ATo%20address%20this%2C%20we%20introduce%20scale-adaptive%20tokens%20that%20are%20dynamically%0Aadjusted%20based%20on%20the%20relative%20scale%20of%20each%20individual%20in%20the%20image%20within%20the%0ADETR%20framework.%20Specifically%2C%20individuals%20in%20smaller%20scales%20are%20processed%20at%0Ahigher%20resolutions%2C%20larger%20ones%20at%20lower%20resolutions%2C%20and%20background%20regions%0Aare%20further%20distilled.%20These%20scale-adaptive%20tokens%20more%20efficiently%20encode%20the%0Aimage%20features%2C%20facilitating%20subsequent%20decoding%20to%20regress%20the%20human%20mesh%2C%0Awhile%20allowing%20the%20model%20to%20allocate%20computational%20resources%20more%20effectively%0Aand%20focus%20on%20more%20challenging%20cases.%20Experiments%20show%20that%20our%20method%20preserves%0Athe%20accuracy%20benefits%20of%20high-resolution%20processing%20while%20substantially%0Areducing%20computational%20cost%2C%20achieving%20real-time%20inference%20with%20performance%0Acomparable%20to%20SOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAT-HMR%253A%2520Real-Time%2520Multi-Person%25203D%2520Mesh%2520Estimation%2520via%2520Scale-Adaptive%250A%2520%2520Tokens%26entry.906535625%3DChi%2520Su%2520and%2520Xiaoxuan%2520Ma%2520and%2520Jiajun%2520Su%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520one-stage%2520framework%2520for%2520real-time%2520multi-person%25203D%2520human%2520mesh%250Aestimation%2520from%2520a%2520single%2520RGB%2520image.%2520While%2520current%2520one-stage%2520methods%252C%2520which%250Afollow%2520a%2520DETR-style%2520pipeline%252C%2520achieve%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520with%250Ahigh-resolution%2520inputs%252C%2520we%2520observe%2520that%2520this%2520particularly%2520benefits%2520the%250Aestimation%2520of%2520individuals%2520in%2520smaller%2520scales%2520of%2520the%2520image%2520%2528e.g.%252C%2520those%2520far%2520from%250Athe%2520camera%2529%252C%2520but%2520at%2520the%2520cost%2520of%2520significantly%2520increased%2520computation%2520overhead.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520scale-adaptive%2520tokens%2520that%2520are%2520dynamically%250Aadjusted%2520based%2520on%2520the%2520relative%2520scale%2520of%2520each%2520individual%2520in%2520the%2520image%2520within%2520the%250ADETR%2520framework.%2520Specifically%252C%2520individuals%2520in%2520smaller%2520scales%2520are%2520processed%2520at%250Ahigher%2520resolutions%252C%2520larger%2520ones%2520at%2520lower%2520resolutions%252C%2520and%2520background%2520regions%250Aare%2520further%2520distilled.%2520These%2520scale-adaptive%2520tokens%2520more%2520efficiently%2520encode%2520the%250Aimage%2520features%252C%2520facilitating%2520subsequent%2520decoding%2520to%2520regress%2520the%2520human%2520mesh%252C%250Awhile%2520allowing%2520the%2520model%2520to%2520allocate%2520computational%2520resources%2520more%2520effectively%250Aand%2520focus%2520on%2520more%2520challenging%2520cases.%2520Experiments%2520show%2520that%2520our%2520method%2520preserves%250Athe%2520accuracy%2520benefits%2520of%2520high-resolution%2520processing%2520while%2520substantially%250Areducing%2520computational%2520cost%252C%2520achieving%2520real-time%2520inference%2520with%2520performance%250Acomparable%2520to%2520SOTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAT-HMR%3A%20Real-Time%20Multi-Person%203D%20Mesh%20Estimation%20via%20Scale-Adaptive%0A%20%20Tokens&entry.906535625=Chi%20Su%20and%20Xiaoxuan%20Ma%20and%20Jiajun%20Su%20and%20Yizhou%20Wang&entry.1292438233=%20%20We%20propose%20a%20one-stage%20framework%20for%20real-time%20multi-person%203D%20human%20mesh%0Aestimation%20from%20a%20single%20RGB%20image.%20While%20current%20one-stage%20methods%2C%20which%0Afollow%20a%20DETR-style%20pipeline%2C%20achieve%20state-of-the-art%20%28SOTA%29%20performance%20with%0Ahigh-resolution%20inputs%2C%20we%20observe%20that%20this%20particularly%20benefits%20the%0Aestimation%20of%20individuals%20in%20smaller%20scales%20of%20the%20image%20%28e.g.%2C%20those%20far%20from%0Athe%20camera%29%2C%20but%20at%20the%20cost%20of%20significantly%20increased%20computation%20overhead.%0ATo%20address%20this%2C%20we%20introduce%20scale-adaptive%20tokens%20that%20are%20dynamically%0Aadjusted%20based%20on%20the%20relative%20scale%20of%20each%20individual%20in%20the%20image%20within%20the%0ADETR%20framework.%20Specifically%2C%20individuals%20in%20smaller%20scales%20are%20processed%20at%0Ahigher%20resolutions%2C%20larger%20ones%20at%20lower%20resolutions%2C%20and%20background%20regions%0Aare%20further%20distilled.%20These%20scale-adaptive%20tokens%20more%20efficiently%20encode%20the%0Aimage%20features%2C%20facilitating%20subsequent%20decoding%20to%20regress%20the%20human%20mesh%2C%0Awhile%20allowing%20the%20model%20to%20allocate%20computational%20resources%20more%20effectively%0Aand%20focus%20on%20more%20challenging%20cases.%20Experiments%20show%20that%20our%20method%20preserves%0Athe%20accuracy%20benefits%20of%20high-resolution%20processing%20while%20substantially%0Areducing%20computational%20cost%2C%20achieving%20real-time%20inference%20with%20performance%0Acomparable%20to%20SOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19824v1&entry.124074799=Read"},
{"title": "GuardSplat: Robust and Efficient Watermarking for 3D Gaussian Splatting", "author": "Zixuan Chen and Guangcong Wang and Jiahao Zhu and Jianhuang Lai and Xiaohua Xie", "abstract": "  3D Gaussian Splatting (3DGS) has recently created impressive assets for\nvarious applications. However, the copyright of these assets is not well\nprotected as existing watermarking methods are not suited for 3DGS considering\nsecurity, capacity, and invisibility. Besides, these methods often require\nhours or even days for optimization, limiting the application scenarios. In\nthis paper, we propose GuardSplat, an innovative and efficient framework that\neffectively protects the copyright of 3DGS assets. Specifically, 1) We first\npropose a CLIP-guided Message Decoupling Optimization module for training the\nmessage decoder, leveraging CLIP's aligning capability and rich representations\nto achieve a high extraction accuracy with minimal optimization costs,\npresenting exceptional capability and efficiency. 2) Then, we propose a\nSpherical-harmonic-aware (SH-aware) Message Embedding module tailored for 3DGS,\nwhich employs a set of SH offsets to seamlessly embed the message into the SH\nfeatures of each 3D Gaussian while maintaining the original 3D structure. It\nenables the 3DGS assets to be watermarked with minimal fidelity trade-offs and\nprevents malicious users from removing the messages from the model files,\nmeeting the demands for invisibility and security. 3) We further propose an\nAnti-distortion Message Extraction module to improve robustness against various\nvisual distortions. Extensive experiments demonstrate that GuardSplat\noutperforms the state-of-the-art methods and achieves fast optimization speed.\n", "link": "http://arxiv.org/abs/2411.19895v1", "date": "2024-11-29", "relevancy": 3.0229, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6158}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6118}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GuardSplat%3A%20Robust%20and%20Efficient%20Watermarking%20for%203D%20Gaussian%20Splatting&body=Title%3A%20GuardSplat%3A%20Robust%20and%20Efficient%20Watermarking%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zixuan%20Chen%20and%20Guangcong%20Wang%20and%20Jiahao%20Zhu%20and%20Jianhuang%20Lai%20and%20Xiaohua%20Xie%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20created%20impressive%20assets%20for%0Avarious%20applications.%20However%2C%20the%20copyright%20of%20these%20assets%20is%20not%20well%0Aprotected%20as%20existing%20watermarking%20methods%20are%20not%20suited%20for%203DGS%20considering%0Asecurity%2C%20capacity%2C%20and%20invisibility.%20Besides%2C%20these%20methods%20often%20require%0Ahours%20or%20even%20days%20for%20optimization%2C%20limiting%20the%20application%20scenarios.%20In%0Athis%20paper%2C%20we%20propose%20GuardSplat%2C%20an%20innovative%20and%20efficient%20framework%20that%0Aeffectively%20protects%20the%20copyright%20of%203DGS%20assets.%20Specifically%2C%201%29%20We%20first%0Apropose%20a%20CLIP-guided%20Message%20Decoupling%20Optimization%20module%20for%20training%20the%0Amessage%20decoder%2C%20leveraging%20CLIP%27s%20aligning%20capability%20and%20rich%20representations%0Ato%20achieve%20a%20high%20extraction%20accuracy%20with%20minimal%20optimization%20costs%2C%0Apresenting%20exceptional%20capability%20and%20efficiency.%202%29%20Then%2C%20we%20propose%20a%0ASpherical-harmonic-aware%20%28SH-aware%29%20Message%20Embedding%20module%20tailored%20for%203DGS%2C%0Awhich%20employs%20a%20set%20of%20SH%20offsets%20to%20seamlessly%20embed%20the%20message%20into%20the%20SH%0Afeatures%20of%20each%203D%20Gaussian%20while%20maintaining%20the%20original%203D%20structure.%20It%0Aenables%20the%203DGS%20assets%20to%20be%20watermarked%20with%20minimal%20fidelity%20trade-offs%20and%0Aprevents%20malicious%20users%20from%20removing%20the%20messages%20from%20the%20model%20files%2C%0Ameeting%20the%20demands%20for%20invisibility%20and%20security.%203%29%20We%20further%20propose%20an%0AAnti-distortion%20Message%20Extraction%20module%20to%20improve%20robustness%20against%20various%0Avisual%20distortions.%20Extensive%20experiments%20demonstrate%20that%20GuardSplat%0Aoutperforms%20the%20state-of-the-art%20methods%20and%20achieves%20fast%20optimization%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuardSplat%253A%2520Robust%2520and%2520Efficient%2520Watermarking%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZixuan%2520Chen%2520and%2520Guangcong%2520Wang%2520and%2520Jiahao%2520Zhu%2520and%2520Jianhuang%2520Lai%2520and%2520Xiaohua%2520Xie%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520created%2520impressive%2520assets%2520for%250Avarious%2520applications.%2520However%252C%2520the%2520copyright%2520of%2520these%2520assets%2520is%2520not%2520well%250Aprotected%2520as%2520existing%2520watermarking%2520methods%2520are%2520not%2520suited%2520for%25203DGS%2520considering%250Asecurity%252C%2520capacity%252C%2520and%2520invisibility.%2520Besides%252C%2520these%2520methods%2520often%2520require%250Ahours%2520or%2520even%2520days%2520for%2520optimization%252C%2520limiting%2520the%2520application%2520scenarios.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520GuardSplat%252C%2520an%2520innovative%2520and%2520efficient%2520framework%2520that%250Aeffectively%2520protects%2520the%2520copyright%2520of%25203DGS%2520assets.%2520Specifically%252C%25201%2529%2520We%2520first%250Apropose%2520a%2520CLIP-guided%2520Message%2520Decoupling%2520Optimization%2520module%2520for%2520training%2520the%250Amessage%2520decoder%252C%2520leveraging%2520CLIP%2527s%2520aligning%2520capability%2520and%2520rich%2520representations%250Ato%2520achieve%2520a%2520high%2520extraction%2520accuracy%2520with%2520minimal%2520optimization%2520costs%252C%250Apresenting%2520exceptional%2520capability%2520and%2520efficiency.%25202%2529%2520Then%252C%2520we%2520propose%2520a%250ASpherical-harmonic-aware%2520%2528SH-aware%2529%2520Message%2520Embedding%2520module%2520tailored%2520for%25203DGS%252C%250Awhich%2520employs%2520a%2520set%2520of%2520SH%2520offsets%2520to%2520seamlessly%2520embed%2520the%2520message%2520into%2520the%2520SH%250Afeatures%2520of%2520each%25203D%2520Gaussian%2520while%2520maintaining%2520the%2520original%25203D%2520structure.%2520It%250Aenables%2520the%25203DGS%2520assets%2520to%2520be%2520watermarked%2520with%2520minimal%2520fidelity%2520trade-offs%2520and%250Aprevents%2520malicious%2520users%2520from%2520removing%2520the%2520messages%2520from%2520the%2520model%2520files%252C%250Ameeting%2520the%2520demands%2520for%2520invisibility%2520and%2520security.%25203%2529%2520We%2520further%2520propose%2520an%250AAnti-distortion%2520Message%2520Extraction%2520module%2520to%2520improve%2520robustness%2520against%2520various%250Avisual%2520distortions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GuardSplat%250Aoutperforms%2520the%2520state-of-the-art%2520methods%2520and%2520achieves%2520fast%2520optimization%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GuardSplat%3A%20Robust%20and%20Efficient%20Watermarking%20for%203D%20Gaussian%20Splatting&entry.906535625=Zixuan%20Chen%20and%20Guangcong%20Wang%20and%20Jiahao%20Zhu%20and%20Jianhuang%20Lai%20and%20Xiaohua%20Xie&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20created%20impressive%20assets%20for%0Avarious%20applications.%20However%2C%20the%20copyright%20of%20these%20assets%20is%20not%20well%0Aprotected%20as%20existing%20watermarking%20methods%20are%20not%20suited%20for%203DGS%20considering%0Asecurity%2C%20capacity%2C%20and%20invisibility.%20Besides%2C%20these%20methods%20often%20require%0Ahours%20or%20even%20days%20for%20optimization%2C%20limiting%20the%20application%20scenarios.%20In%0Athis%20paper%2C%20we%20propose%20GuardSplat%2C%20an%20innovative%20and%20efficient%20framework%20that%0Aeffectively%20protects%20the%20copyright%20of%203DGS%20assets.%20Specifically%2C%201%29%20We%20first%0Apropose%20a%20CLIP-guided%20Message%20Decoupling%20Optimization%20module%20for%20training%20the%0Amessage%20decoder%2C%20leveraging%20CLIP%27s%20aligning%20capability%20and%20rich%20representations%0Ato%20achieve%20a%20high%20extraction%20accuracy%20with%20minimal%20optimization%20costs%2C%0Apresenting%20exceptional%20capability%20and%20efficiency.%202%29%20Then%2C%20we%20propose%20a%0ASpherical-harmonic-aware%20%28SH-aware%29%20Message%20Embedding%20module%20tailored%20for%203DGS%2C%0Awhich%20employs%20a%20set%20of%20SH%20offsets%20to%20seamlessly%20embed%20the%20message%20into%20the%20SH%0Afeatures%20of%20each%203D%20Gaussian%20while%20maintaining%20the%20original%203D%20structure.%20It%0Aenables%20the%203DGS%20assets%20to%20be%20watermarked%20with%20minimal%20fidelity%20trade-offs%20and%0Aprevents%20malicious%20users%20from%20removing%20the%20messages%20from%20the%20model%20files%2C%0Ameeting%20the%20demands%20for%20invisibility%20and%20security.%203%29%20We%20further%20propose%20an%0AAnti-distortion%20Message%20Extraction%20module%20to%20improve%20robustness%20against%20various%0Avisual%20distortions.%20Extensive%20experiments%20demonstrate%20that%20GuardSplat%0Aoutperforms%20the%20state-of-the-art%20methods%20and%20achieves%20fast%20optimization%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19895v1&entry.124074799=Read"},
{"title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos", "author": "Tiantian Geng and Jinrui Zhang and Qingni Wang and Teng Wang and Jinming Duan and Feng Zheng", "abstract": "  Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.\n", "link": "http://arxiv.org/abs/2411.19772v1", "date": "2024-11-29", "relevancy": 2.9376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongVALE%3A%20Vision-Audio-Language-Event%20Benchmark%20Towards%20Time-Aware%0A%20%20Omni-Modal%20Perception%20of%20Long%20Videos&body=Title%3A%20LongVALE%3A%20Vision-Audio-Language-Event%20Benchmark%20Towards%20Time-Aware%0A%20%20Omni-Modal%20Perception%20of%20Long%20Videos%0AAuthor%3A%20Tiantian%20Geng%20and%20Jinrui%20Zhang%20and%20Qingni%20Wang%20and%20Teng%20Wang%20and%20Jinming%20Duan%20and%20Feng%20Zheng%0AAbstract%3A%20%20%20Despite%20impressive%20advancements%20in%20video%20understanding%2C%20most%20efforts%20remain%0Alimited%20to%20coarse-grained%20or%20visual-only%20video%20tasks.%20However%2C%20real-world%0Avideos%20encompass%20omni-modal%20information%20%28vision%2C%20audio%2C%20and%20speech%29%20with%20a%0Aseries%20of%20events%20forming%20a%20cohesive%20storyline.%20The%20lack%20of%20multi-modal%20video%0Adata%20with%20fine-grained%20event%20annotations%20and%20the%20high%20cost%20of%20manual%20labeling%0Aare%20major%20obstacles%20to%20comprehensive%20omni-modality%20video%20perception.%20To%20address%0Athis%20gap%2C%20we%20propose%20an%20automatic%20pipeline%20consisting%20of%20high-quality%0Amulti-modal%20video%20filtering%2C%20semantically%20coherent%20omni-modal%20event%20boundary%0Adetection%2C%20and%20cross-modal%20correlation-aware%20event%20captioning.%20In%20this%20way%2C%20we%0Apresent%20LongVALE%2C%20the%20first-ever%20Vision-Audio-Language%20Event%20understanding%0Abenchmark%20comprising%20105K%20omni-modal%20events%20with%20precise%20temporal%20boundaries%0Aand%20detailed%20relation-aware%20captions%20within%208.4K%20high-quality%20long%20videos.%0AFurther%2C%20we%20build%20a%20baseline%20that%20leverages%20LongVALE%20to%20enable%20video%20large%0Alanguage%20models%20%28LLMs%29%20for%20omni-modality%20fine-grained%20temporal%20video%0Aunderstanding%20for%20the%20first%20time.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20and%20great%20potential%20of%20LongVALE%20in%20advancing%20comprehensive%0Amulti-modal%20video%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongVALE%253A%2520Vision-Audio-Language-Event%2520Benchmark%2520Towards%2520Time-Aware%250A%2520%2520Omni-Modal%2520Perception%2520of%2520Long%2520Videos%26entry.906535625%3DTiantian%2520Geng%2520and%2520Jinrui%2520Zhang%2520and%2520Qingni%2520Wang%2520and%2520Teng%2520Wang%2520and%2520Jinming%2520Duan%2520and%2520Feng%2520Zheng%26entry.1292438233%3D%2520%2520Despite%2520impressive%2520advancements%2520in%2520video%2520understanding%252C%2520most%2520efforts%2520remain%250Alimited%2520to%2520coarse-grained%2520or%2520visual-only%2520video%2520tasks.%2520However%252C%2520real-world%250Avideos%2520encompass%2520omni-modal%2520information%2520%2528vision%252C%2520audio%252C%2520and%2520speech%2529%2520with%2520a%250Aseries%2520of%2520events%2520forming%2520a%2520cohesive%2520storyline.%2520The%2520lack%2520of%2520multi-modal%2520video%250Adata%2520with%2520fine-grained%2520event%2520annotations%2520and%2520the%2520high%2520cost%2520of%2520manual%2520labeling%250Aare%2520major%2520obstacles%2520to%2520comprehensive%2520omni-modality%2520video%2520perception.%2520To%2520address%250Athis%2520gap%252C%2520we%2520propose%2520an%2520automatic%2520pipeline%2520consisting%2520of%2520high-quality%250Amulti-modal%2520video%2520filtering%252C%2520semantically%2520coherent%2520omni-modal%2520event%2520boundary%250Adetection%252C%2520and%2520cross-modal%2520correlation-aware%2520event%2520captioning.%2520In%2520this%2520way%252C%2520we%250Apresent%2520LongVALE%252C%2520the%2520first-ever%2520Vision-Audio-Language%2520Event%2520understanding%250Abenchmark%2520comprising%2520105K%2520omni-modal%2520events%2520with%2520precise%2520temporal%2520boundaries%250Aand%2520detailed%2520relation-aware%2520captions%2520within%25208.4K%2520high-quality%2520long%2520videos.%250AFurther%252C%2520we%2520build%2520a%2520baseline%2520that%2520leverages%2520LongVALE%2520to%2520enable%2520video%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520for%2520omni-modality%2520fine-grained%2520temporal%2520video%250Aunderstanding%2520for%2520the%2520first%2520time.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520and%2520great%2520potential%2520of%2520LongVALE%2520in%2520advancing%2520comprehensive%250Amulti-modal%2520video%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongVALE%3A%20Vision-Audio-Language-Event%20Benchmark%20Towards%20Time-Aware%0A%20%20Omni-Modal%20Perception%20of%20Long%20Videos&entry.906535625=Tiantian%20Geng%20and%20Jinrui%20Zhang%20and%20Qingni%20Wang%20and%20Teng%20Wang%20and%20Jinming%20Duan%20and%20Feng%20Zheng&entry.1292438233=%20%20Despite%20impressive%20advancements%20in%20video%20understanding%2C%20most%20efforts%20remain%0Alimited%20to%20coarse-grained%20or%20visual-only%20video%20tasks.%20However%2C%20real-world%0Avideos%20encompass%20omni-modal%20information%20%28vision%2C%20audio%2C%20and%20speech%29%20with%20a%0Aseries%20of%20events%20forming%20a%20cohesive%20storyline.%20The%20lack%20of%20multi-modal%20video%0Adata%20with%20fine-grained%20event%20annotations%20and%20the%20high%20cost%20of%20manual%20labeling%0Aare%20major%20obstacles%20to%20comprehensive%20omni-modality%20video%20perception.%20To%20address%0Athis%20gap%2C%20we%20propose%20an%20automatic%20pipeline%20consisting%20of%20high-quality%0Amulti-modal%20video%20filtering%2C%20semantically%20coherent%20omni-modal%20event%20boundary%0Adetection%2C%20and%20cross-modal%20correlation-aware%20event%20captioning.%20In%20this%20way%2C%20we%0Apresent%20LongVALE%2C%20the%20first-ever%20Vision-Audio-Language%20Event%20understanding%0Abenchmark%20comprising%20105K%20omni-modal%20events%20with%20precise%20temporal%20boundaries%0Aand%20detailed%20relation-aware%20captions%20within%208.4K%20high-quality%20long%20videos.%0AFurther%2C%20we%20build%20a%20baseline%20that%20leverages%20LongVALE%20to%20enable%20video%20large%0Alanguage%20models%20%28LLMs%29%20for%20omni-modality%20fine-grained%20temporal%20video%0Aunderstanding%20for%20the%20first%20time.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20and%20great%20potential%20of%20LongVALE%20in%20advancing%20comprehensive%0Amulti-modal%20video%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19772v1&entry.124074799=Read"},
{"title": "SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection", "author": "Philipp Wolters and Johannes Gilg and Torben Teepe and Fabian Herzog and Felix Fent and Gerhard Rigoll", "abstract": "  In this work, we present SpaRC, a novel Sparse fusion transformer for 3D\nperception that integrates multi-view image semantics with Radar and Camera\npoint features. The fusion of radar and camera modalities has emerged as an\nefficient perception paradigm for autonomous driving systems. While\nconventional approaches utilize dense Bird's Eye View (BEV)-based architectures\nfor depth estimation, contemporary query-based transformers excel in\ncamera-only detection through object-centric methodology. However, these\nquery-based approaches exhibit limitations in false positive detections and\nlocalization precision due to implicit depth modeling. We address these\nchallenges through three key contributions: (1) sparse frustum fusion (SFF) for\ncross-modal feature alignment, (2) range-adaptive radar aggregation (RAR) for\nprecise object localization, and (3) local self-attention (LSA) for focused\nquery aggregation. In contrast to existing methods requiring computationally\nintensive BEV-grid rendering, SpaRC operates directly on encoded point\nfeatures, yielding substantial improvements in efficiency and accuracy.\nEmpirical evaluations on the nuScenes and TruckScenes benchmarks demonstrate\nthat SpaRC significantly outperforms existing dense BEV-based and sparse\nquery-based detectors. Our method achieves state-of-the-art performance metrics\nof 67.1 NDS and 63.1 AMOTA. The code and pretrained models are available at\nhttps://github.com/phi-wol/sparc.\n", "link": "http://arxiv.org/abs/2411.19860v1", "date": "2024-11-29", "relevancy": 2.9127, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaRC%3A%20Sparse%20Radar-Camera%20Fusion%20for%203D%20Object%20Detection&body=Title%3A%20SpaRC%3A%20Sparse%20Radar-Camera%20Fusion%20for%203D%20Object%20Detection%0AAuthor%3A%20Philipp%20Wolters%20and%20Johannes%20Gilg%20and%20Torben%20Teepe%20and%20Fabian%20Herzog%20and%20Felix%20Fent%20and%20Gerhard%20Rigoll%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20SpaRC%2C%20a%20novel%20Sparse%20fusion%20transformer%20for%203D%0Aperception%20that%20integrates%20multi-view%20image%20semantics%20with%20Radar%20and%20Camera%0Apoint%20features.%20The%20fusion%20of%20radar%20and%20camera%20modalities%20has%20emerged%20as%20an%0Aefficient%20perception%20paradigm%20for%20autonomous%20driving%20systems.%20While%0Aconventional%20approaches%20utilize%20dense%20Bird%27s%20Eye%20View%20%28BEV%29-based%20architectures%0Afor%20depth%20estimation%2C%20contemporary%20query-based%20transformers%20excel%20in%0Acamera-only%20detection%20through%20object-centric%20methodology.%20However%2C%20these%0Aquery-based%20approaches%20exhibit%20limitations%20in%20false%20positive%20detections%20and%0Alocalization%20precision%20due%20to%20implicit%20depth%20modeling.%20We%20address%20these%0Achallenges%20through%20three%20key%20contributions%3A%20%281%29%20sparse%20frustum%20fusion%20%28SFF%29%20for%0Across-modal%20feature%20alignment%2C%20%282%29%20range-adaptive%20radar%20aggregation%20%28RAR%29%20for%0Aprecise%20object%20localization%2C%20and%20%283%29%20local%20self-attention%20%28LSA%29%20for%20focused%0Aquery%20aggregation.%20In%20contrast%20to%20existing%20methods%20requiring%20computationally%0Aintensive%20BEV-grid%20rendering%2C%20SpaRC%20operates%20directly%20on%20encoded%20point%0Afeatures%2C%20yielding%20substantial%20improvements%20in%20efficiency%20and%20accuracy.%0AEmpirical%20evaluations%20on%20the%20nuScenes%20and%20TruckScenes%20benchmarks%20demonstrate%0Athat%20SpaRC%20significantly%20outperforms%20existing%20dense%20BEV-based%20and%20sparse%0Aquery-based%20detectors.%20Our%20method%20achieves%20state-of-the-art%20performance%20metrics%0Aof%2067.1%20NDS%20and%2063.1%20AMOTA.%20The%20code%20and%20pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/phi-wol/sparc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaRC%253A%2520Sparse%2520Radar-Camera%2520Fusion%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DPhilipp%2520Wolters%2520and%2520Johannes%2520Gilg%2520and%2520Torben%2520Teepe%2520and%2520Fabian%2520Herzog%2520and%2520Felix%2520Fent%2520and%2520Gerhard%2520Rigoll%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520SpaRC%252C%2520a%2520novel%2520Sparse%2520fusion%2520transformer%2520for%25203D%250Aperception%2520that%2520integrates%2520multi-view%2520image%2520semantics%2520with%2520Radar%2520and%2520Camera%250Apoint%2520features.%2520The%2520fusion%2520of%2520radar%2520and%2520camera%2520modalities%2520has%2520emerged%2520as%2520an%250Aefficient%2520perception%2520paradigm%2520for%2520autonomous%2520driving%2520systems.%2520While%250Aconventional%2520approaches%2520utilize%2520dense%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529-based%2520architectures%250Afor%2520depth%2520estimation%252C%2520contemporary%2520query-based%2520transformers%2520excel%2520in%250Acamera-only%2520detection%2520through%2520object-centric%2520methodology.%2520However%252C%2520these%250Aquery-based%2520approaches%2520exhibit%2520limitations%2520in%2520false%2520positive%2520detections%2520and%250Alocalization%2520precision%2520due%2520to%2520implicit%2520depth%2520modeling.%2520We%2520address%2520these%250Achallenges%2520through%2520three%2520key%2520contributions%253A%2520%25281%2529%2520sparse%2520frustum%2520fusion%2520%2528SFF%2529%2520for%250Across-modal%2520feature%2520alignment%252C%2520%25282%2529%2520range-adaptive%2520radar%2520aggregation%2520%2528RAR%2529%2520for%250Aprecise%2520object%2520localization%252C%2520and%2520%25283%2529%2520local%2520self-attention%2520%2528LSA%2529%2520for%2520focused%250Aquery%2520aggregation.%2520In%2520contrast%2520to%2520existing%2520methods%2520requiring%2520computationally%250Aintensive%2520BEV-grid%2520rendering%252C%2520SpaRC%2520operates%2520directly%2520on%2520encoded%2520point%250Afeatures%252C%2520yielding%2520substantial%2520improvements%2520in%2520efficiency%2520and%2520accuracy.%250AEmpirical%2520evaluations%2520on%2520the%2520nuScenes%2520and%2520TruckScenes%2520benchmarks%2520demonstrate%250Athat%2520SpaRC%2520significantly%2520outperforms%2520existing%2520dense%2520BEV-based%2520and%2520sparse%250Aquery-based%2520detectors.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520metrics%250Aof%252067.1%2520NDS%2520and%252063.1%2520AMOTA.%2520The%2520code%2520and%2520pretrained%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/phi-wol/sparc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaRC%3A%20Sparse%20Radar-Camera%20Fusion%20for%203D%20Object%20Detection&entry.906535625=Philipp%20Wolters%20and%20Johannes%20Gilg%20and%20Torben%20Teepe%20and%20Fabian%20Herzog%20and%20Felix%20Fent%20and%20Gerhard%20Rigoll&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20SpaRC%2C%20a%20novel%20Sparse%20fusion%20transformer%20for%203D%0Aperception%20that%20integrates%20multi-view%20image%20semantics%20with%20Radar%20and%20Camera%0Apoint%20features.%20The%20fusion%20of%20radar%20and%20camera%20modalities%20has%20emerged%20as%20an%0Aefficient%20perception%20paradigm%20for%20autonomous%20driving%20systems.%20While%0Aconventional%20approaches%20utilize%20dense%20Bird%27s%20Eye%20View%20%28BEV%29-based%20architectures%0Afor%20depth%20estimation%2C%20contemporary%20query-based%20transformers%20excel%20in%0Acamera-only%20detection%20through%20object-centric%20methodology.%20However%2C%20these%0Aquery-based%20approaches%20exhibit%20limitations%20in%20false%20positive%20detections%20and%0Alocalization%20precision%20due%20to%20implicit%20depth%20modeling.%20We%20address%20these%0Achallenges%20through%20three%20key%20contributions%3A%20%281%29%20sparse%20frustum%20fusion%20%28SFF%29%20for%0Across-modal%20feature%20alignment%2C%20%282%29%20range-adaptive%20radar%20aggregation%20%28RAR%29%20for%0Aprecise%20object%20localization%2C%20and%20%283%29%20local%20self-attention%20%28LSA%29%20for%20focused%0Aquery%20aggregation.%20In%20contrast%20to%20existing%20methods%20requiring%20computationally%0Aintensive%20BEV-grid%20rendering%2C%20SpaRC%20operates%20directly%20on%20encoded%20point%0Afeatures%2C%20yielding%20substantial%20improvements%20in%20efficiency%20and%20accuracy.%0AEmpirical%20evaluations%20on%20the%20nuScenes%20and%20TruckScenes%20benchmarks%20demonstrate%0Athat%20SpaRC%20significantly%20outperforms%20existing%20dense%20BEV-based%20and%20sparse%0Aquery-based%20detectors.%20Our%20method%20achieves%20state-of-the-art%20performance%20metrics%0Aof%2067.1%20NDS%20and%2063.1%20AMOTA.%20The%20code%20and%20pretrained%20models%20are%20available%20at%0Ahttps%3A//github.com/phi-wol/sparc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19860v1&entry.124074799=Read"},
{"title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings", "author": "Qiong Wu and Wenhao Lin and Weihao Ye and Yiyi Zhou and Xiaoshuai Sun and Rongrong Ji", "abstract": "  The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is anonymously\nreleased at https://github.com/DoubtedSteam/DyVTE.\n", "link": "http://arxiv.org/abs/2411.19628v1", "date": "2024-11-29", "relevancy": 2.8963, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5801}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%20Visual-Token%0A%20%20Exit%20and%20the%20Empirical%20Findings&body=Title%3A%20Accelerating%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%20Visual-Token%0A%20%20Exit%20and%20the%20Empirical%20Findings%0AAuthor%3A%20Qiong%20Wu%20and%20Wenhao%20Lin%20and%20Weihao%20Ye%20and%20Yiyi%20Zhou%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20The%20excessive%20use%20of%20visual%20tokens%20in%20existing%20Multimoal%20Large%20Language%0AModels%20%28MLLMs%29%20often%20exhibits%20obvious%20redundancy%20and%20brings%20in%20prohibitively%0Aexpensive%20computation.%20To%20gain%20insights%20into%20this%20problem%2C%20we%20first%20conduct%0Aextensive%20empirical%20studies%20on%20the%20attention%20behaviors%20of%20MLLMs%2C%20and%20summarize%0Athree%20main%20inference%20stages%20in%20MLLMs%3A%20%28i%29%20Early%20fusion%20between%20tokens%20is%20first%0Aaccomplished%20quickly.%20%28ii%29%20Intra-modality%20modeling%20then%20comes%20to%20play.%20%28iii%29%0AMultimodal%20reasoning%7D%20resumes%20and%20lasts%20until%20the%20end%20of%20inference.%20In%0Aparticular%2C%20we%20reveal%20that%20visual%20tokens%20will%20stop%20contributing%20to%20reasoning%0Awhen%20the%20text%20tokens%20receive%20enough%20image%20information%2C%20yielding%20obvious%20visual%0Aredundancy.%20Based%20on%20these%20generalized%20observations%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20method%20to%20improve%20the%20efficiency%20of%20MLLMs%2C%20termed%20dynamic%0Avisual-token%20exit%20%28DyVTE%29.%20DyVTE%20uses%20lightweight%20hyper-networks%20to%20perceive%0Athe%20text%20token%20status%20and%20decide%20the%20removal%20of%20all%20visual%20tokens%20after%20a%0Acertain%20layer%2C%20thereby%20addressing%20the%20observed%20visual%20redundancy.%20To%20validate%0AVTE%2C%20we%20apply%20it%20to%20a%20set%20of%20MLLMs%2C%20including%20LLaVA%2C%20VILA%2C%20Eagle%20and%20InternVL%2C%0Aand%20conduct%20extensive%20experiments%20on%20a%20bunch%20of%20benchmarks.%20The%20experiment%0Aresults%20not%20only%20show%20the%20effectiveness%20of%20our%20VTE%20in%20improving%20MLLMs%27%0Aefficiency%2C%20but%20also%20yield%20the%20general%20modeling%20patterns%20of%20MLLMs%2C%20well%0Afacilitating%20the%20in-depth%20understanding%20of%20MLLMs.%20Our%20code%20is%20anonymously%0Areleased%20at%20https%3A//github.com/DoubtedSteam/DyVTE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Dynamic%2520Visual-Token%250A%2520%2520Exit%2520and%2520the%2520Empirical%2520Findings%26entry.906535625%3DQiong%2520Wu%2520and%2520Wenhao%2520Lin%2520and%2520Weihao%2520Ye%2520and%2520Yiyi%2520Zhou%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520The%2520excessive%2520use%2520of%2520visual%2520tokens%2520in%2520existing%2520Multimoal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520often%2520exhibits%2520obvious%2520redundancy%2520and%2520brings%2520in%2520prohibitively%250Aexpensive%2520computation.%2520To%2520gain%2520insights%2520into%2520this%2520problem%252C%2520we%2520first%2520conduct%250Aextensive%2520empirical%2520studies%2520on%2520the%2520attention%2520behaviors%2520of%2520MLLMs%252C%2520and%2520summarize%250Athree%2520main%2520inference%2520stages%2520in%2520MLLMs%253A%2520%2528i%2529%2520Early%2520fusion%2520between%2520tokens%2520is%2520first%250Aaccomplished%2520quickly.%2520%2528ii%2529%2520Intra-modality%2520modeling%2520then%2520comes%2520to%2520play.%2520%2528iii%2529%250AMultimodal%2520reasoning%257D%2520resumes%2520and%2520lasts%2520until%2520the%2520end%2520of%2520inference.%2520In%250Aparticular%252C%2520we%2520reveal%2520that%2520visual%2520tokens%2520will%2520stop%2520contributing%2520to%2520reasoning%250Awhen%2520the%2520text%2520tokens%2520receive%2520enough%2520image%2520information%252C%2520yielding%2520obvious%2520visual%250Aredundancy.%2520Based%2520on%2520these%2520generalized%2520observations%252C%2520we%2520propose%2520a%2520simple%2520yet%250Aeffective%2520method%2520to%2520improve%2520the%2520efficiency%2520of%2520MLLMs%252C%2520termed%2520dynamic%250Avisual-token%2520exit%2520%2528DyVTE%2529.%2520DyVTE%2520uses%2520lightweight%2520hyper-networks%2520to%2520perceive%250Athe%2520text%2520token%2520status%2520and%2520decide%2520the%2520removal%2520of%2520all%2520visual%2520tokens%2520after%2520a%250Acertain%2520layer%252C%2520thereby%2520addressing%2520the%2520observed%2520visual%2520redundancy.%2520To%2520validate%250AVTE%252C%2520we%2520apply%2520it%2520to%2520a%2520set%2520of%2520MLLMs%252C%2520including%2520LLaVA%252C%2520VILA%252C%2520Eagle%2520and%2520InternVL%252C%250Aand%2520conduct%2520extensive%2520experiments%2520on%2520a%2520bunch%2520of%2520benchmarks.%2520The%2520experiment%250Aresults%2520not%2520only%2520show%2520the%2520effectiveness%2520of%2520our%2520VTE%2520in%2520improving%2520MLLMs%2527%250Aefficiency%252C%2520but%2520also%2520yield%2520the%2520general%2520modeling%2520patterns%2520of%2520MLLMs%252C%2520well%250Afacilitating%2520the%2520in-depth%2520understanding%2520of%2520MLLMs.%2520Our%2520code%2520is%2520anonymously%250Areleased%2520at%2520https%253A//github.com/DoubtedSteam/DyVTE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Multimodal%20Large%20Language%20Models%20via%20Dynamic%20Visual-Token%0A%20%20Exit%20and%20the%20Empirical%20Findings&entry.906535625=Qiong%20Wu%20and%20Wenhao%20Lin%20and%20Weihao%20Ye%20and%20Yiyi%20Zhou%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20The%20excessive%20use%20of%20visual%20tokens%20in%20existing%20Multimoal%20Large%20Language%0AModels%20%28MLLMs%29%20often%20exhibits%20obvious%20redundancy%20and%20brings%20in%20prohibitively%0Aexpensive%20computation.%20To%20gain%20insights%20into%20this%20problem%2C%20we%20first%20conduct%0Aextensive%20empirical%20studies%20on%20the%20attention%20behaviors%20of%20MLLMs%2C%20and%20summarize%0Athree%20main%20inference%20stages%20in%20MLLMs%3A%20%28i%29%20Early%20fusion%20between%20tokens%20is%20first%0Aaccomplished%20quickly.%20%28ii%29%20Intra-modality%20modeling%20then%20comes%20to%20play.%20%28iii%29%0AMultimodal%20reasoning%7D%20resumes%20and%20lasts%20until%20the%20end%20of%20inference.%20In%0Aparticular%2C%20we%20reveal%20that%20visual%20tokens%20will%20stop%20contributing%20to%20reasoning%0Awhen%20the%20text%20tokens%20receive%20enough%20image%20information%2C%20yielding%20obvious%20visual%0Aredundancy.%20Based%20on%20these%20generalized%20observations%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20method%20to%20improve%20the%20efficiency%20of%20MLLMs%2C%20termed%20dynamic%0Avisual-token%20exit%20%28DyVTE%29.%20DyVTE%20uses%20lightweight%20hyper-networks%20to%20perceive%0Athe%20text%20token%20status%20and%20decide%20the%20removal%20of%20all%20visual%20tokens%20after%20a%0Acertain%20layer%2C%20thereby%20addressing%20the%20observed%20visual%20redundancy.%20To%20validate%0AVTE%2C%20we%20apply%20it%20to%20a%20set%20of%20MLLMs%2C%20including%20LLaVA%2C%20VILA%2C%20Eagle%20and%20InternVL%2C%0Aand%20conduct%20extensive%20experiments%20on%20a%20bunch%20of%20benchmarks.%20The%20experiment%0Aresults%20not%20only%20show%20the%20effectiveness%20of%20our%20VTE%20in%20improving%20MLLMs%27%0Aefficiency%2C%20but%20also%20yield%20the%20general%20modeling%20patterns%20of%20MLLMs%2C%20well%0Afacilitating%20the%20in-depth%20understanding%20of%20MLLMs.%20Our%20code%20is%20anonymously%0Areleased%20at%20https%3A//github.com/DoubtedSteam/DyVTE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19628v1&entry.124074799=Read"},
{"title": "$C^{3}$-NeRF: Modeling Multiple Scenes via Conditional-cum-Continual\n  Neural Radiance Fields", "author": "Prajwal Singh and Ashish Tiwari and Gautam Vashishtha and Shanmuganathan Raman", "abstract": "  Neural radiance fields (NeRF) have exhibited highly photorealistic rendering\nof novel views through per-scene optimization over a single 3D scene. With the\ngrowing popularity of NeRF and its variants, they have become ubiquitous and\nhave been identified as efficient 3D resources. However, they are still far\nfrom being scalable since a separate model needs to be stored for each scene,\nand the training time increases linearly with every newly added scene.\nSurprisingly, the idea of encoding multiple 3D scenes into a single NeRF model\nis heavily under-explored. In this work, we propose a novel\nconditional-cum-continual framework, called $C^{3}$-NeRF, to accommodate\nmultiple scenes into the parameters of a single neural radiance field. Unlike\nconventional approaches that leverage feature extractors and pre-trained priors\nfor scene conditioning, we use simple pseudo-scene labels to model multiple\nscenes in NeRF. Interestingly, we observe the framework is also inherently\ncontinual (via generative replay) with minimal, if not no, forgetting of the\npreviously learned scenes. Consequently, the proposed framework adapts to\nmultiple new scenes without necessarily accessing the old data. Through\nextensive qualitative and quantitative evaluation using synthetic and real\ndatasets, we demonstrate the inherent capacity of the NeRF model to accommodate\nmultiple scenes with high-quality novel-view renderings without adding\nadditional parameters. We provide implementation details and dynamic\nvisualizations of our results in the supplementary file.\n", "link": "http://arxiv.org/abs/2411.19903v1", "date": "2024-11-29", "relevancy": 2.863, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5757}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24C%5E%7B3%7D%24-NeRF%3A%20Modeling%20Multiple%20Scenes%20via%20Conditional-cum-Continual%0A%20%20Neural%20Radiance%20Fields&body=Title%3A%20%24C%5E%7B3%7D%24-NeRF%3A%20Modeling%20Multiple%20Scenes%20via%20Conditional-cum-Continual%0A%20%20Neural%20Radiance%20Fields%0AAuthor%3A%20Prajwal%20Singh%20and%20Ashish%20Tiwari%20and%20Gautam%20Vashishtha%20and%20Shanmuganathan%20Raman%0AAbstract%3A%20%20%20Neural%20radiance%20fields%20%28NeRF%29%20have%20exhibited%20highly%20photorealistic%20rendering%0Aof%20novel%20views%20through%20per-scene%20optimization%20over%20a%20single%203D%20scene.%20With%20the%0Agrowing%20popularity%20of%20NeRF%20and%20its%20variants%2C%20they%20have%20become%20ubiquitous%20and%0Ahave%20been%20identified%20as%20efficient%203D%20resources.%20However%2C%20they%20are%20still%20far%0Afrom%20being%20scalable%20since%20a%20separate%20model%20needs%20to%20be%20stored%20for%20each%20scene%2C%0Aand%20the%20training%20time%20increases%20linearly%20with%20every%20newly%20added%20scene.%0ASurprisingly%2C%20the%20idea%20of%20encoding%20multiple%203D%20scenes%20into%20a%20single%20NeRF%20model%0Ais%20heavily%20under-explored.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aconditional-cum-continual%20framework%2C%20called%20%24C%5E%7B3%7D%24-NeRF%2C%20to%20accommodate%0Amultiple%20scenes%20into%20the%20parameters%20of%20a%20single%20neural%20radiance%20field.%20Unlike%0Aconventional%20approaches%20that%20leverage%20feature%20extractors%20and%20pre-trained%20priors%0Afor%20scene%20conditioning%2C%20we%20use%20simple%20pseudo-scene%20labels%20to%20model%20multiple%0Ascenes%20in%20NeRF.%20Interestingly%2C%20we%20observe%20the%20framework%20is%20also%20inherently%0Acontinual%20%28via%20generative%20replay%29%20with%20minimal%2C%20if%20not%20no%2C%20forgetting%20of%20the%0Apreviously%20learned%20scenes.%20Consequently%2C%20the%20proposed%20framework%20adapts%20to%0Amultiple%20new%20scenes%20without%20necessarily%20accessing%20the%20old%20data.%20Through%0Aextensive%20qualitative%20and%20quantitative%20evaluation%20using%20synthetic%20and%20real%0Adatasets%2C%20we%20demonstrate%20the%20inherent%20capacity%20of%20the%20NeRF%20model%20to%20accommodate%0Amultiple%20scenes%20with%20high-quality%20novel-view%20renderings%20without%20adding%0Aadditional%20parameters.%20We%20provide%20implementation%20details%20and%20dynamic%0Avisualizations%20of%20our%20results%20in%20the%20supplementary%20file.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524C%255E%257B3%257D%2524-NeRF%253A%2520Modeling%2520Multiple%2520Scenes%2520via%2520Conditional-cum-Continual%250A%2520%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DPrajwal%2520Singh%2520and%2520Ashish%2520Tiwari%2520and%2520Gautam%2520Vashishtha%2520and%2520Shanmuganathan%2520Raman%26entry.1292438233%3D%2520%2520Neural%2520radiance%2520fields%2520%2528NeRF%2529%2520have%2520exhibited%2520highly%2520photorealistic%2520rendering%250Aof%2520novel%2520views%2520through%2520per-scene%2520optimization%2520over%2520a%2520single%25203D%2520scene.%2520With%2520the%250Agrowing%2520popularity%2520of%2520NeRF%2520and%2520its%2520variants%252C%2520they%2520have%2520become%2520ubiquitous%2520and%250Ahave%2520been%2520identified%2520as%2520efficient%25203D%2520resources.%2520However%252C%2520they%2520are%2520still%2520far%250Afrom%2520being%2520scalable%2520since%2520a%2520separate%2520model%2520needs%2520to%2520be%2520stored%2520for%2520each%2520scene%252C%250Aand%2520the%2520training%2520time%2520increases%2520linearly%2520with%2520every%2520newly%2520added%2520scene.%250ASurprisingly%252C%2520the%2520idea%2520of%2520encoding%2520multiple%25203D%2520scenes%2520into%2520a%2520single%2520NeRF%2520model%250Ais%2520heavily%2520under-explored.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Aconditional-cum-continual%2520framework%252C%2520called%2520%2524C%255E%257B3%257D%2524-NeRF%252C%2520to%2520accommodate%250Amultiple%2520scenes%2520into%2520the%2520parameters%2520of%2520a%2520single%2520neural%2520radiance%2520field.%2520Unlike%250Aconventional%2520approaches%2520that%2520leverage%2520feature%2520extractors%2520and%2520pre-trained%2520priors%250Afor%2520scene%2520conditioning%252C%2520we%2520use%2520simple%2520pseudo-scene%2520labels%2520to%2520model%2520multiple%250Ascenes%2520in%2520NeRF.%2520Interestingly%252C%2520we%2520observe%2520the%2520framework%2520is%2520also%2520inherently%250Acontinual%2520%2528via%2520generative%2520replay%2529%2520with%2520minimal%252C%2520if%2520not%2520no%252C%2520forgetting%2520of%2520the%250Apreviously%2520learned%2520scenes.%2520Consequently%252C%2520the%2520proposed%2520framework%2520adapts%2520to%250Amultiple%2520new%2520scenes%2520without%2520necessarily%2520accessing%2520the%2520old%2520data.%2520Through%250Aextensive%2520qualitative%2520and%2520quantitative%2520evaluation%2520using%2520synthetic%2520and%2520real%250Adatasets%252C%2520we%2520demonstrate%2520the%2520inherent%2520capacity%2520of%2520the%2520NeRF%2520model%2520to%2520accommodate%250Amultiple%2520scenes%2520with%2520high-quality%2520novel-view%2520renderings%2520without%2520adding%250Aadditional%2520parameters.%2520We%2520provide%2520implementation%2520details%2520and%2520dynamic%250Avisualizations%2520of%2520our%2520results%2520in%2520the%2520supplementary%2520file.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24C%5E%7B3%7D%24-NeRF%3A%20Modeling%20Multiple%20Scenes%20via%20Conditional-cum-Continual%0A%20%20Neural%20Radiance%20Fields&entry.906535625=Prajwal%20Singh%20and%20Ashish%20Tiwari%20and%20Gautam%20Vashishtha%20and%20Shanmuganathan%20Raman&entry.1292438233=%20%20Neural%20radiance%20fields%20%28NeRF%29%20have%20exhibited%20highly%20photorealistic%20rendering%0Aof%20novel%20views%20through%20per-scene%20optimization%20over%20a%20single%203D%20scene.%20With%20the%0Agrowing%20popularity%20of%20NeRF%20and%20its%20variants%2C%20they%20have%20become%20ubiquitous%20and%0Ahave%20been%20identified%20as%20efficient%203D%20resources.%20However%2C%20they%20are%20still%20far%0Afrom%20being%20scalable%20since%20a%20separate%20model%20needs%20to%20be%20stored%20for%20each%20scene%2C%0Aand%20the%20training%20time%20increases%20linearly%20with%20every%20newly%20added%20scene.%0ASurprisingly%2C%20the%20idea%20of%20encoding%20multiple%203D%20scenes%20into%20a%20single%20NeRF%20model%0Ais%20heavily%20under-explored.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aconditional-cum-continual%20framework%2C%20called%20%24C%5E%7B3%7D%24-NeRF%2C%20to%20accommodate%0Amultiple%20scenes%20into%20the%20parameters%20of%20a%20single%20neural%20radiance%20field.%20Unlike%0Aconventional%20approaches%20that%20leverage%20feature%20extractors%20and%20pre-trained%20priors%0Afor%20scene%20conditioning%2C%20we%20use%20simple%20pseudo-scene%20labels%20to%20model%20multiple%0Ascenes%20in%20NeRF.%20Interestingly%2C%20we%20observe%20the%20framework%20is%20also%20inherently%0Acontinual%20%28via%20generative%20replay%29%20with%20minimal%2C%20if%20not%20no%2C%20forgetting%20of%20the%0Apreviously%20learned%20scenes.%20Consequently%2C%20the%20proposed%20framework%20adapts%20to%0Amultiple%20new%20scenes%20without%20necessarily%20accessing%20the%20old%20data.%20Through%0Aextensive%20qualitative%20and%20quantitative%20evaluation%20using%20synthetic%20and%20real%0Adatasets%2C%20we%20demonstrate%20the%20inherent%20capacity%20of%20the%20NeRF%20model%20to%20accommodate%0Amultiple%20scenes%20with%20high-quality%20novel-view%20renderings%20without%20adding%0Aadditional%20parameters.%20We%20provide%20implementation%20details%20and%20dynamic%0Avisualizations%20of%20our%20results%20in%20the%20supplementary%20file.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19903v1&entry.124074799=Read"},
{"title": "LDA-AQU: Adaptive Query-guided Upsampling via Local Deformable Attention", "author": "Zewen Du and Zhenjiang Hu and Guiyu Zhao and Ying Jin and Hongbin Ma", "abstract": "  Feature upsampling is an essential operation in constructing deep\nconvolutional neural networks. However, existing upsamplers either lack\nspecific feature guidance or necessitate the utilization of high-resolution\nfeature maps, resulting in a loss of performance and flexibility. In this\npaper, we find that the local self-attention naturally has the feature guidance\ncapability, and its computational paradigm aligns closely with the essence of\nfeature upsampling (\\ie feature reassembly of neighboring points). Therefore,\nwe introduce local self-attention into the upsampling task and demonstrate that\nthe majority of existing upsamplers can be regarded as special cases of\nupsamplers based on local self-attention. Considering the potential semantic\ngap between upsampled points and their neighboring points, we further introduce\nthe deformation mechanism into the upsampler based on local self-attention,\nthereby proposing LDA-AQU. As a novel dynamic kernel-based upsampler, LDA-AQU\nutilizes the feature of queries to guide the model in adaptively adjusting the\nposition and aggregation weight of neighboring points, thereby meeting the\nupsampling requirements across various complex scenarios. In addition, LDA-AQU\nis lightweight and can be easily integrated into various model architectures.\nWe evaluate the effectiveness of LDA-AQU across four dense prediction tasks:\nobject detection, instance segmentation, panoptic segmentation, and semantic\nsegmentation. LDA-AQU consistently outperforms previous state-of-the-art\nupsamplers, achieving performance enhancements of 1.7 AP, 1.5 AP, 2.0 PQ, and\n2.5 mIoU compared to the baseline models in the aforementioned four tasks,\nrespectively. Code is available at \\url{https://github.com/duzw9311/LDA-AQU}.\n", "link": "http://arxiv.org/abs/2411.19585v1", "date": "2024-11-29", "relevancy": 2.8555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5861}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5776}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LDA-AQU%3A%20Adaptive%20Query-guided%20Upsampling%20via%20Local%20Deformable%20Attention&body=Title%3A%20LDA-AQU%3A%20Adaptive%20Query-guided%20Upsampling%20via%20Local%20Deformable%20Attention%0AAuthor%3A%20Zewen%20Du%20and%20Zhenjiang%20Hu%20and%20Guiyu%20Zhao%20and%20Ying%20Jin%20and%20Hongbin%20Ma%0AAbstract%3A%20%20%20Feature%20upsampling%20is%20an%20essential%20operation%20in%20constructing%20deep%0Aconvolutional%20neural%20networks.%20However%2C%20existing%20upsamplers%20either%20lack%0Aspecific%20feature%20guidance%20or%20necessitate%20the%20utilization%20of%20high-resolution%0Afeature%20maps%2C%20resulting%20in%20a%20loss%20of%20performance%20and%20flexibility.%20In%20this%0Apaper%2C%20we%20find%20that%20the%20local%20self-attention%20naturally%20has%20the%20feature%20guidance%0Acapability%2C%20and%20its%20computational%20paradigm%20aligns%20closely%20with%20the%20essence%20of%0Afeature%20upsampling%20%28%5Cie%20feature%20reassembly%20of%20neighboring%20points%29.%20Therefore%2C%0Awe%20introduce%20local%20self-attention%20into%20the%20upsampling%20task%20and%20demonstrate%20that%0Athe%20majority%20of%20existing%20upsamplers%20can%20be%20regarded%20as%20special%20cases%20of%0Aupsamplers%20based%20on%20local%20self-attention.%20Considering%20the%20potential%20semantic%0Agap%20between%20upsampled%20points%20and%20their%20neighboring%20points%2C%20we%20further%20introduce%0Athe%20deformation%20mechanism%20into%20the%20upsampler%20based%20on%20local%20self-attention%2C%0Athereby%20proposing%20LDA-AQU.%20As%20a%20novel%20dynamic%20kernel-based%20upsampler%2C%20LDA-AQU%0Autilizes%20the%20feature%20of%20queries%20to%20guide%20the%20model%20in%20adaptively%20adjusting%20the%0Aposition%20and%20aggregation%20weight%20of%20neighboring%20points%2C%20thereby%20meeting%20the%0Aupsampling%20requirements%20across%20various%20complex%20scenarios.%20In%20addition%2C%20LDA-AQU%0Ais%20lightweight%20and%20can%20be%20easily%20integrated%20into%20various%20model%20architectures.%0AWe%20evaluate%20the%20effectiveness%20of%20LDA-AQU%20across%20four%20dense%20prediction%20tasks%3A%0Aobject%20detection%2C%20instance%20segmentation%2C%20panoptic%20segmentation%2C%20and%20semantic%0Asegmentation.%20LDA-AQU%20consistently%20outperforms%20previous%20state-of-the-art%0Aupsamplers%2C%20achieving%20performance%20enhancements%20of%201.7%20AP%2C%201.5%20AP%2C%202.0%20PQ%2C%20and%0A2.5%20mIoU%20compared%20to%20the%20baseline%20models%20in%20the%20aforementioned%20four%20tasks%2C%0Arespectively.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/duzw9311/LDA-AQU%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLDA-AQU%253A%2520Adaptive%2520Query-guided%2520Upsampling%2520via%2520Local%2520Deformable%2520Attention%26entry.906535625%3DZewen%2520Du%2520and%2520Zhenjiang%2520Hu%2520and%2520Guiyu%2520Zhao%2520and%2520Ying%2520Jin%2520and%2520Hongbin%2520Ma%26entry.1292438233%3D%2520%2520Feature%2520upsampling%2520is%2520an%2520essential%2520operation%2520in%2520constructing%2520deep%250Aconvolutional%2520neural%2520networks.%2520However%252C%2520existing%2520upsamplers%2520either%2520lack%250Aspecific%2520feature%2520guidance%2520or%2520necessitate%2520the%2520utilization%2520of%2520high-resolution%250Afeature%2520maps%252C%2520resulting%2520in%2520a%2520loss%2520of%2520performance%2520and%2520flexibility.%2520In%2520this%250Apaper%252C%2520we%2520find%2520that%2520the%2520local%2520self-attention%2520naturally%2520has%2520the%2520feature%2520guidance%250Acapability%252C%2520and%2520its%2520computational%2520paradigm%2520aligns%2520closely%2520with%2520the%2520essence%2520of%250Afeature%2520upsampling%2520%2528%255Cie%2520feature%2520reassembly%2520of%2520neighboring%2520points%2529.%2520Therefore%252C%250Awe%2520introduce%2520local%2520self-attention%2520into%2520the%2520upsampling%2520task%2520and%2520demonstrate%2520that%250Athe%2520majority%2520of%2520existing%2520upsamplers%2520can%2520be%2520regarded%2520as%2520special%2520cases%2520of%250Aupsamplers%2520based%2520on%2520local%2520self-attention.%2520Considering%2520the%2520potential%2520semantic%250Agap%2520between%2520upsampled%2520points%2520and%2520their%2520neighboring%2520points%252C%2520we%2520further%2520introduce%250Athe%2520deformation%2520mechanism%2520into%2520the%2520upsampler%2520based%2520on%2520local%2520self-attention%252C%250Athereby%2520proposing%2520LDA-AQU.%2520As%2520a%2520novel%2520dynamic%2520kernel-based%2520upsampler%252C%2520LDA-AQU%250Autilizes%2520the%2520feature%2520of%2520queries%2520to%2520guide%2520the%2520model%2520in%2520adaptively%2520adjusting%2520the%250Aposition%2520and%2520aggregation%2520weight%2520of%2520neighboring%2520points%252C%2520thereby%2520meeting%2520the%250Aupsampling%2520requirements%2520across%2520various%2520complex%2520scenarios.%2520In%2520addition%252C%2520LDA-AQU%250Ais%2520lightweight%2520and%2520can%2520be%2520easily%2520integrated%2520into%2520various%2520model%2520architectures.%250AWe%2520evaluate%2520the%2520effectiveness%2520of%2520LDA-AQU%2520across%2520four%2520dense%2520prediction%2520tasks%253A%250Aobject%2520detection%252C%2520instance%2520segmentation%252C%2520panoptic%2520segmentation%252C%2520and%2520semantic%250Asegmentation.%2520LDA-AQU%2520consistently%2520outperforms%2520previous%2520state-of-the-art%250Aupsamplers%252C%2520achieving%2520performance%2520enhancements%2520of%25201.7%2520AP%252C%25201.5%2520AP%252C%25202.0%2520PQ%252C%2520and%250A2.5%2520mIoU%2520compared%2520to%2520the%2520baseline%2520models%2520in%2520the%2520aforementioned%2520four%2520tasks%252C%250Arespectively.%2520Code%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/duzw9311/LDA-AQU%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LDA-AQU%3A%20Adaptive%20Query-guided%20Upsampling%20via%20Local%20Deformable%20Attention&entry.906535625=Zewen%20Du%20and%20Zhenjiang%20Hu%20and%20Guiyu%20Zhao%20and%20Ying%20Jin%20and%20Hongbin%20Ma&entry.1292438233=%20%20Feature%20upsampling%20is%20an%20essential%20operation%20in%20constructing%20deep%0Aconvolutional%20neural%20networks.%20However%2C%20existing%20upsamplers%20either%20lack%0Aspecific%20feature%20guidance%20or%20necessitate%20the%20utilization%20of%20high-resolution%0Afeature%20maps%2C%20resulting%20in%20a%20loss%20of%20performance%20and%20flexibility.%20In%20this%0Apaper%2C%20we%20find%20that%20the%20local%20self-attention%20naturally%20has%20the%20feature%20guidance%0Acapability%2C%20and%20its%20computational%20paradigm%20aligns%20closely%20with%20the%20essence%20of%0Afeature%20upsampling%20%28%5Cie%20feature%20reassembly%20of%20neighboring%20points%29.%20Therefore%2C%0Awe%20introduce%20local%20self-attention%20into%20the%20upsampling%20task%20and%20demonstrate%20that%0Athe%20majority%20of%20existing%20upsamplers%20can%20be%20regarded%20as%20special%20cases%20of%0Aupsamplers%20based%20on%20local%20self-attention.%20Considering%20the%20potential%20semantic%0Agap%20between%20upsampled%20points%20and%20their%20neighboring%20points%2C%20we%20further%20introduce%0Athe%20deformation%20mechanism%20into%20the%20upsampler%20based%20on%20local%20self-attention%2C%0Athereby%20proposing%20LDA-AQU.%20As%20a%20novel%20dynamic%20kernel-based%20upsampler%2C%20LDA-AQU%0Autilizes%20the%20feature%20of%20queries%20to%20guide%20the%20model%20in%20adaptively%20adjusting%20the%0Aposition%20and%20aggregation%20weight%20of%20neighboring%20points%2C%20thereby%20meeting%20the%0Aupsampling%20requirements%20across%20various%20complex%20scenarios.%20In%20addition%2C%20LDA-AQU%0Ais%20lightweight%20and%20can%20be%20easily%20integrated%20into%20various%20model%20architectures.%0AWe%20evaluate%20the%20effectiveness%20of%20LDA-AQU%20across%20four%20dense%20prediction%20tasks%3A%0Aobject%20detection%2C%20instance%20segmentation%2C%20panoptic%20segmentation%2C%20and%20semantic%0Asegmentation.%20LDA-AQU%20consistently%20outperforms%20previous%20state-of-the-art%0Aupsamplers%2C%20achieving%20performance%20enhancements%20of%201.7%20AP%2C%201.5%20AP%2C%202.0%20PQ%2C%20and%0A2.5%20mIoU%20compared%20to%20the%20baseline%20models%20in%20the%20aforementioned%20four%20tasks%2C%0Arespectively.%20Code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/duzw9311/LDA-AQU%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19585v1&entry.124074799=Read"},
{"title": "LaVIDE: A Language-Vision Discriminator for Detecting Changes in\n  Satellite Image with Map References", "author": "Shuguo Jiang and Fang Xu and Sen Jia and Gui-Song Xia", "abstract": "  Change detection, which typically relies on the comparison of bi-temporal\nimages, is significantly hindered when only a single image is available.\nComparing a single image with an existing map, such as OpenStreetMap, which is\ncontinuously updated through crowd-sourcing, offers a viable solution to this\nchallenge. Unlike images that carry low-level visual details of ground objects,\nmaps convey high-level categorical information. This discrepancy in abstraction\nlevels complicates the alignment and comparison of the two data types. In this\npaper, we propose a \\textbf{La}nguage-\\textbf{VI}sion \\textbf{D}iscriminator\nfor d\\textbf{E}tecting changes in satellite image with map references, namely\n\\ours{}, which leverages language to bridge the information gap between maps\nand images. Specifically, \\ours{} formulates change detection as the problem of\n``{\\textit Does the pixel belong to [class]?}'', aligning maps and images\nwithin the feature space of the language-vision model to associate high-level\nmap categories with low-level image details. Moreover, we build a\nmixture-of-experts discriminative module, which compares linguistic features\nfrom maps with visual features from images across various semantic\nperspectives, achieving comprehensive semantic comparison for change detection.\nExtensive evaluation on four benchmark datasets demonstrates that \\ours{} can\neffectively detect changes in satellite image with map references,\noutperforming state-of-the-art change detection algorithms, e.g., with gains of\nabout $13.8$\\% on the DynamicEarthNet dataset and $4.3$\\% on the SECOND\ndataset.\n", "link": "http://arxiv.org/abs/2411.19758v1", "date": "2024-11-29", "relevancy": 2.7859, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaVIDE%3A%20A%20Language-Vision%20Discriminator%20for%20Detecting%20Changes%20in%0A%20%20Satellite%20Image%20with%20Map%20References&body=Title%3A%20LaVIDE%3A%20A%20Language-Vision%20Discriminator%20for%20Detecting%20Changes%20in%0A%20%20Satellite%20Image%20with%20Map%20References%0AAuthor%3A%20Shuguo%20Jiang%20and%20Fang%20Xu%20and%20Sen%20Jia%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20Change%20detection%2C%20which%20typically%20relies%20on%20the%20comparison%20of%20bi-temporal%0Aimages%2C%20is%20significantly%20hindered%20when%20only%20a%20single%20image%20is%20available.%0AComparing%20a%20single%20image%20with%20an%20existing%20map%2C%20such%20as%20OpenStreetMap%2C%20which%20is%0Acontinuously%20updated%20through%20crowd-sourcing%2C%20offers%20a%20viable%20solution%20to%20this%0Achallenge.%20Unlike%20images%20that%20carry%20low-level%20visual%20details%20of%20ground%20objects%2C%0Amaps%20convey%20high-level%20categorical%20information.%20This%20discrepancy%20in%20abstraction%0Alevels%20complicates%20the%20alignment%20and%20comparison%20of%20the%20two%20data%20types.%20In%20this%0Apaper%2C%20we%20propose%20a%20%5Ctextbf%7BLa%7Dnguage-%5Ctextbf%7BVI%7Dsion%20%5Ctextbf%7BD%7Discriminator%0Afor%20d%5Ctextbf%7BE%7Dtecting%20changes%20in%20satellite%20image%20with%20map%20references%2C%20namely%0A%5Cours%7B%7D%2C%20which%20leverages%20language%20to%20bridge%20the%20information%20gap%20between%20maps%0Aand%20images.%20Specifically%2C%20%5Cours%7B%7D%20formulates%20change%20detection%20as%20the%20problem%20of%0A%60%60%7B%5Ctextit%20Does%20the%20pixel%20belong%20to%20%5Bclass%5D%3F%7D%27%27%2C%20aligning%20maps%20and%20images%0Awithin%20the%20feature%20space%20of%20the%20language-vision%20model%20to%20associate%20high-level%0Amap%20categories%20with%20low-level%20image%20details.%20Moreover%2C%20we%20build%20a%0Amixture-of-experts%20discriminative%20module%2C%20which%20compares%20linguistic%20features%0Afrom%20maps%20with%20visual%20features%20from%20images%20across%20various%20semantic%0Aperspectives%2C%20achieving%20comprehensive%20semantic%20comparison%20for%20change%20detection.%0AExtensive%20evaluation%20on%20four%20benchmark%20datasets%20demonstrates%20that%20%5Cours%7B%7D%20can%0Aeffectively%20detect%20changes%20in%20satellite%20image%20with%20map%20references%2C%0Aoutperforming%20state-of-the-art%20change%20detection%20algorithms%2C%20e.g.%2C%20with%20gains%20of%0Aabout%20%2413.8%24%5C%25%20on%20the%20DynamicEarthNet%20dataset%20and%20%244.3%24%5C%25%20on%20the%20SECOND%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaVIDE%253A%2520A%2520Language-Vision%2520Discriminator%2520for%2520Detecting%2520Changes%2520in%250A%2520%2520Satellite%2520Image%2520with%2520Map%2520References%26entry.906535625%3DShuguo%2520Jiang%2520and%2520Fang%2520Xu%2520and%2520Sen%2520Jia%2520and%2520Gui-Song%2520Xia%26entry.1292438233%3D%2520%2520Change%2520detection%252C%2520which%2520typically%2520relies%2520on%2520the%2520comparison%2520of%2520bi-temporal%250Aimages%252C%2520is%2520significantly%2520hindered%2520when%2520only%2520a%2520single%2520image%2520is%2520available.%250AComparing%2520a%2520single%2520image%2520with%2520an%2520existing%2520map%252C%2520such%2520as%2520OpenStreetMap%252C%2520which%2520is%250Acontinuously%2520updated%2520through%2520crowd-sourcing%252C%2520offers%2520a%2520viable%2520solution%2520to%2520this%250Achallenge.%2520Unlike%2520images%2520that%2520carry%2520low-level%2520visual%2520details%2520of%2520ground%2520objects%252C%250Amaps%2520convey%2520high-level%2520categorical%2520information.%2520This%2520discrepancy%2520in%2520abstraction%250Alevels%2520complicates%2520the%2520alignment%2520and%2520comparison%2520of%2520the%2520two%2520data%2520types.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520%255Ctextbf%257BLa%257Dnguage-%255Ctextbf%257BVI%257Dsion%2520%255Ctextbf%257BD%257Discriminator%250Afor%2520d%255Ctextbf%257BE%257Dtecting%2520changes%2520in%2520satellite%2520image%2520with%2520map%2520references%252C%2520namely%250A%255Cours%257B%257D%252C%2520which%2520leverages%2520language%2520to%2520bridge%2520the%2520information%2520gap%2520between%2520maps%250Aand%2520images.%2520Specifically%252C%2520%255Cours%257B%257D%2520formulates%2520change%2520detection%2520as%2520the%2520problem%2520of%250A%2560%2560%257B%255Ctextit%2520Does%2520the%2520pixel%2520belong%2520to%2520%255Bclass%255D%253F%257D%2527%2527%252C%2520aligning%2520maps%2520and%2520images%250Awithin%2520the%2520feature%2520space%2520of%2520the%2520language-vision%2520model%2520to%2520associate%2520high-level%250Amap%2520categories%2520with%2520low-level%2520image%2520details.%2520Moreover%252C%2520we%2520build%2520a%250Amixture-of-experts%2520discriminative%2520module%252C%2520which%2520compares%2520linguistic%2520features%250Afrom%2520maps%2520with%2520visual%2520features%2520from%2520images%2520across%2520various%2520semantic%250Aperspectives%252C%2520achieving%2520comprehensive%2520semantic%2520comparison%2520for%2520change%2520detection.%250AExtensive%2520evaluation%2520on%2520four%2520benchmark%2520datasets%2520demonstrates%2520that%2520%255Cours%257B%257D%2520can%250Aeffectively%2520detect%2520changes%2520in%2520satellite%2520image%2520with%2520map%2520references%252C%250Aoutperforming%2520state-of-the-art%2520change%2520detection%2520algorithms%252C%2520e.g.%252C%2520with%2520gains%2520of%250Aabout%2520%252413.8%2524%255C%2525%2520on%2520the%2520DynamicEarthNet%2520dataset%2520and%2520%25244.3%2524%255C%2525%2520on%2520the%2520SECOND%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaVIDE%3A%20A%20Language-Vision%20Discriminator%20for%20Detecting%20Changes%20in%0A%20%20Satellite%20Image%20with%20Map%20References&entry.906535625=Shuguo%20Jiang%20and%20Fang%20Xu%20and%20Sen%20Jia%20and%20Gui-Song%20Xia&entry.1292438233=%20%20Change%20detection%2C%20which%20typically%20relies%20on%20the%20comparison%20of%20bi-temporal%0Aimages%2C%20is%20significantly%20hindered%20when%20only%20a%20single%20image%20is%20available.%0AComparing%20a%20single%20image%20with%20an%20existing%20map%2C%20such%20as%20OpenStreetMap%2C%20which%20is%0Acontinuously%20updated%20through%20crowd-sourcing%2C%20offers%20a%20viable%20solution%20to%20this%0Achallenge.%20Unlike%20images%20that%20carry%20low-level%20visual%20details%20of%20ground%20objects%2C%0Amaps%20convey%20high-level%20categorical%20information.%20This%20discrepancy%20in%20abstraction%0Alevels%20complicates%20the%20alignment%20and%20comparison%20of%20the%20two%20data%20types.%20In%20this%0Apaper%2C%20we%20propose%20a%20%5Ctextbf%7BLa%7Dnguage-%5Ctextbf%7BVI%7Dsion%20%5Ctextbf%7BD%7Discriminator%0Afor%20d%5Ctextbf%7BE%7Dtecting%20changes%20in%20satellite%20image%20with%20map%20references%2C%20namely%0A%5Cours%7B%7D%2C%20which%20leverages%20language%20to%20bridge%20the%20information%20gap%20between%20maps%0Aand%20images.%20Specifically%2C%20%5Cours%7B%7D%20formulates%20change%20detection%20as%20the%20problem%20of%0A%60%60%7B%5Ctextit%20Does%20the%20pixel%20belong%20to%20%5Bclass%5D%3F%7D%27%27%2C%20aligning%20maps%20and%20images%0Awithin%20the%20feature%20space%20of%20the%20language-vision%20model%20to%20associate%20high-level%0Amap%20categories%20with%20low-level%20image%20details.%20Moreover%2C%20we%20build%20a%0Amixture-of-experts%20discriminative%20module%2C%20which%20compares%20linguistic%20features%0Afrom%20maps%20with%20visual%20features%20from%20images%20across%20various%20semantic%0Aperspectives%2C%20achieving%20comprehensive%20semantic%20comparison%20for%20change%20detection.%0AExtensive%20evaluation%20on%20four%20benchmark%20datasets%20demonstrates%20that%20%5Cours%7B%7D%20can%0Aeffectively%20detect%20changes%20in%20satellite%20image%20with%20map%20references%2C%0Aoutperforming%20state-of-the-art%20change%20detection%20algorithms%2C%20e.g.%2C%20with%20gains%20of%0Aabout%20%2413.8%24%5C%25%20on%20the%20DynamicEarthNet%20dataset%20and%20%244.3%24%5C%25%20on%20the%20SECOND%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19758v1&entry.124074799=Read"},
{"title": "Multimodal Whole Slide Foundation Model for Pathology", "author": "Tong Ding and Sophia J. Wagner and Andrew H. Song and Richard J. Chen and Ming Y. Lu and Andrew Zhang and Anurag J. Vaidya and Guillaume Jaume and Muhammad Shaban and Ahrong Kim and Drew F. K. Williamson and Bowen Chen and Cristina Almagro-Perez and Paul Doucet and Sharifa Sahai and Chengkuan Chen and Daisuke Komura and Akihiro Kawabe and Shumpei Ishikawa and Georg Gerber and Tingying Peng and Long Phi Le and Faisal Mahmood", "abstract": "  The field of computational pathology has been transformed with recent\nadvances in foundation models that encode histopathology region-of-interests\n(ROIs) into versatile and transferable feature representations via\nself-supervised learning (SSL). However, translating these advancements to\naddress complex clinical challenges at the patient and slide level remains\nconstrained by limited clinical data in disease-specific cohorts, especially\nfor rare clinical conditions. We propose TITAN, a multimodal whole slide\nfoundation model pretrained using 335,645 WSIs via visual self-supervised\nlearning and vision-language alignment with corresponding pathology reports and\n423,122 synthetic captions generated from a multimodal generative AI copilot\nfor pathology. Without any finetuning or requiring clinical labels, TITAN can\nextract general-purpose slide representations and generate pathology reports\nthat generalize to resource-limited clinical scenarios such as rare disease\nretrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and\nfind that TITAN outperforms both ROI and slide foundation models across machine\nlearning settings such as linear probing, few-shot and zero-shot\nclassification, rare cancer retrieval and cross-modal retrieval, and pathology\nreport generation.\n", "link": "http://arxiv.org/abs/2411.19666v1", "date": "2024-11-29", "relevancy": 2.7645, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5655}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Whole%20Slide%20Foundation%20Model%20for%20Pathology&body=Title%3A%20Multimodal%20Whole%20Slide%20Foundation%20Model%20for%20Pathology%0AAuthor%3A%20Tong%20Ding%20and%20Sophia%20J.%20Wagner%20and%20Andrew%20H.%20Song%20and%20Richard%20J.%20Chen%20and%20Ming%20Y.%20Lu%20and%20Andrew%20Zhang%20and%20Anurag%20J.%20Vaidya%20and%20Guillaume%20Jaume%20and%20Muhammad%20Shaban%20and%20Ahrong%20Kim%20and%20Drew%20F.%20K.%20Williamson%20and%20Bowen%20Chen%20and%20Cristina%20Almagro-Perez%20and%20Paul%20Doucet%20and%20Sharifa%20Sahai%20and%20Chengkuan%20Chen%20and%20Daisuke%20Komura%20and%20Akihiro%20Kawabe%20and%20Shumpei%20Ishikawa%20and%20Georg%20Gerber%20and%20Tingying%20Peng%20and%20Long%20Phi%20Le%20and%20Faisal%20Mahmood%0AAbstract%3A%20%20%20The%20field%20of%20computational%20pathology%20has%20been%20transformed%20with%20recent%0Aadvances%20in%20foundation%20models%20that%20encode%20histopathology%20region-of-interests%0A%28ROIs%29%20into%20versatile%20and%20transferable%20feature%20representations%20via%0Aself-supervised%20learning%20%28SSL%29.%20However%2C%20translating%20these%20advancements%20to%0Aaddress%20complex%20clinical%20challenges%20at%20the%20patient%20and%20slide%20level%20remains%0Aconstrained%20by%20limited%20clinical%20data%20in%20disease-specific%20cohorts%2C%20especially%0Afor%20rare%20clinical%20conditions.%20We%20propose%20TITAN%2C%20a%20multimodal%20whole%20slide%0Afoundation%20model%20pretrained%20using%20335%2C645%20WSIs%20via%20visual%20self-supervised%0Alearning%20and%20vision-language%20alignment%20with%20corresponding%20pathology%20reports%20and%0A423%2C122%20synthetic%20captions%20generated%20from%20a%20multimodal%20generative%20AI%20copilot%0Afor%20pathology.%20Without%20any%20finetuning%20or%20requiring%20clinical%20labels%2C%20TITAN%20can%0Aextract%20general-purpose%20slide%20representations%20and%20generate%20pathology%20reports%0Athat%20generalize%20to%20resource-limited%20clinical%20scenarios%20such%20as%20rare%20disease%0Aretrieval%20and%20cancer%20prognosis.%20We%20evaluate%20TITAN%20on%20diverse%20clinical%20tasks%20and%0Afind%20that%20TITAN%20outperforms%20both%20ROI%20and%20slide%20foundation%20models%20across%20machine%0Alearning%20settings%20such%20as%20linear%20probing%2C%20few-shot%20and%20zero-shot%0Aclassification%2C%20rare%20cancer%20retrieval%20and%20cross-modal%20retrieval%2C%20and%20pathology%0Areport%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Whole%2520Slide%2520Foundation%2520Model%2520for%2520Pathology%26entry.906535625%3DTong%2520Ding%2520and%2520Sophia%2520J.%2520Wagner%2520and%2520Andrew%2520H.%2520Song%2520and%2520Richard%2520J.%2520Chen%2520and%2520Ming%2520Y.%2520Lu%2520and%2520Andrew%2520Zhang%2520and%2520Anurag%2520J.%2520Vaidya%2520and%2520Guillaume%2520Jaume%2520and%2520Muhammad%2520Shaban%2520and%2520Ahrong%2520Kim%2520and%2520Drew%2520F.%2520K.%2520Williamson%2520and%2520Bowen%2520Chen%2520and%2520Cristina%2520Almagro-Perez%2520and%2520Paul%2520Doucet%2520and%2520Sharifa%2520Sahai%2520and%2520Chengkuan%2520Chen%2520and%2520Daisuke%2520Komura%2520and%2520Akihiro%2520Kawabe%2520and%2520Shumpei%2520Ishikawa%2520and%2520Georg%2520Gerber%2520and%2520Tingying%2520Peng%2520and%2520Long%2520Phi%2520Le%2520and%2520Faisal%2520Mahmood%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520computational%2520pathology%2520has%2520been%2520transformed%2520with%2520recent%250Aadvances%2520in%2520foundation%2520models%2520that%2520encode%2520histopathology%2520region-of-interests%250A%2528ROIs%2529%2520into%2520versatile%2520and%2520transferable%2520feature%2520representations%2520via%250Aself-supervised%2520learning%2520%2528SSL%2529.%2520However%252C%2520translating%2520these%2520advancements%2520to%250Aaddress%2520complex%2520clinical%2520challenges%2520at%2520the%2520patient%2520and%2520slide%2520level%2520remains%250Aconstrained%2520by%2520limited%2520clinical%2520data%2520in%2520disease-specific%2520cohorts%252C%2520especially%250Afor%2520rare%2520clinical%2520conditions.%2520We%2520propose%2520TITAN%252C%2520a%2520multimodal%2520whole%2520slide%250Afoundation%2520model%2520pretrained%2520using%2520335%252C645%2520WSIs%2520via%2520visual%2520self-supervised%250Alearning%2520and%2520vision-language%2520alignment%2520with%2520corresponding%2520pathology%2520reports%2520and%250A423%252C122%2520synthetic%2520captions%2520generated%2520from%2520a%2520multimodal%2520generative%2520AI%2520copilot%250Afor%2520pathology.%2520Without%2520any%2520finetuning%2520or%2520requiring%2520clinical%2520labels%252C%2520TITAN%2520can%250Aextract%2520general-purpose%2520slide%2520representations%2520and%2520generate%2520pathology%2520reports%250Athat%2520generalize%2520to%2520resource-limited%2520clinical%2520scenarios%2520such%2520as%2520rare%2520disease%250Aretrieval%2520and%2520cancer%2520prognosis.%2520We%2520evaluate%2520TITAN%2520on%2520diverse%2520clinical%2520tasks%2520and%250Afind%2520that%2520TITAN%2520outperforms%2520both%2520ROI%2520and%2520slide%2520foundation%2520models%2520across%2520machine%250Alearning%2520settings%2520such%2520as%2520linear%2520probing%252C%2520few-shot%2520and%2520zero-shot%250Aclassification%252C%2520rare%2520cancer%2520retrieval%2520and%2520cross-modal%2520retrieval%252C%2520and%2520pathology%250Areport%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Whole%20Slide%20Foundation%20Model%20for%20Pathology&entry.906535625=Tong%20Ding%20and%20Sophia%20J.%20Wagner%20and%20Andrew%20H.%20Song%20and%20Richard%20J.%20Chen%20and%20Ming%20Y.%20Lu%20and%20Andrew%20Zhang%20and%20Anurag%20J.%20Vaidya%20and%20Guillaume%20Jaume%20and%20Muhammad%20Shaban%20and%20Ahrong%20Kim%20and%20Drew%20F.%20K.%20Williamson%20and%20Bowen%20Chen%20and%20Cristina%20Almagro-Perez%20and%20Paul%20Doucet%20and%20Sharifa%20Sahai%20and%20Chengkuan%20Chen%20and%20Daisuke%20Komura%20and%20Akihiro%20Kawabe%20and%20Shumpei%20Ishikawa%20and%20Georg%20Gerber%20and%20Tingying%20Peng%20and%20Long%20Phi%20Le%20and%20Faisal%20Mahmood&entry.1292438233=%20%20The%20field%20of%20computational%20pathology%20has%20been%20transformed%20with%20recent%0Aadvances%20in%20foundation%20models%20that%20encode%20histopathology%20region-of-interests%0A%28ROIs%29%20into%20versatile%20and%20transferable%20feature%20representations%20via%0Aself-supervised%20learning%20%28SSL%29.%20However%2C%20translating%20these%20advancements%20to%0Aaddress%20complex%20clinical%20challenges%20at%20the%20patient%20and%20slide%20level%20remains%0Aconstrained%20by%20limited%20clinical%20data%20in%20disease-specific%20cohorts%2C%20especially%0Afor%20rare%20clinical%20conditions.%20We%20propose%20TITAN%2C%20a%20multimodal%20whole%20slide%0Afoundation%20model%20pretrained%20using%20335%2C645%20WSIs%20via%20visual%20self-supervised%0Alearning%20and%20vision-language%20alignment%20with%20corresponding%20pathology%20reports%20and%0A423%2C122%20synthetic%20captions%20generated%20from%20a%20multimodal%20generative%20AI%20copilot%0Afor%20pathology.%20Without%20any%20finetuning%20or%20requiring%20clinical%20labels%2C%20TITAN%20can%0Aextract%20general-purpose%20slide%20representations%20and%20generate%20pathology%20reports%0Athat%20generalize%20to%20resource-limited%20clinical%20scenarios%20such%20as%20rare%20disease%0Aretrieval%20and%20cancer%20prognosis.%20We%20evaluate%20TITAN%20on%20diverse%20clinical%20tasks%20and%0Afind%20that%20TITAN%20outperforms%20both%20ROI%20and%20slide%20foundation%20models%20across%20machine%0Alearning%20settings%20such%20as%20linear%20probing%2C%20few-shot%20and%20zero-shot%0Aclassification%2C%20rare%20cancer%20retrieval%20and%20cross-modal%20retrieval%2C%20and%20pathology%0Areport%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19666v1&entry.124074799=Read"},
{"title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's\n  Reasoning Capability", "author": "Zicheng Lin and Tian Liang and Jiahao Xu and Xing Wang and Ruilin Luo and Chufan Shi and Siheng Li and Yujiu Yang and Zhaopeng Tu", "abstract": "  Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO.\n", "link": "http://arxiv.org/abs/2411.19943v1", "date": "2024-11-29", "relevancy": 2.7548, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critical%20Tokens%20Matter%3A%20Token-Level%20Contrastive%20Estimation%20Enhence%20LLM%27s%0A%20%20Reasoning%20Capability&body=Title%3A%20Critical%20Tokens%20Matter%3A%20Token-Level%20Contrastive%20Estimation%20Enhence%20LLM%27s%0A%20%20Reasoning%20Capability%0AAuthor%3A%20Zicheng%20Lin%20and%20Tian%20Liang%20and%20Jiahao%20Xu%20and%20Xing%20Wang%20and%20Ruilin%20Luo%20and%20Chufan%20Shi%20and%20Siheng%20Li%20and%20Yujiu%20Yang%20and%20Zhaopeng%20Tu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20remarkable%20performance%20on%0Areasoning%20tasks.%20They%20utilize%20autoregressive%20token%20generation%20to%20construct%0Areasoning%20trajectories%2C%20enabling%20the%20development%20of%20a%20coherent%20chain%20of%0Athought.%20In%20this%20work%2C%20we%20explore%20the%20impact%20of%20individual%20tokens%20on%20the%20final%0Aoutcomes%20of%20reasoning%20tasks.%20We%20identify%20the%20existence%20of%20%60%60critical%20tokens%27%27%0Athat%20lead%20to%20incorrect%20reasoning%20trajectories%20in%20LLMs.%20Specifically%2C%20we%20find%0Athat%20LLMs%20tend%20to%20produce%20positive%20outcomes%20when%20forced%20to%20decode%20other%20tokens%0Ainstead%20of%20critical%20tokens.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20novel%0Aapproach%20-%20cDPO%20-%20designed%20to%20automatically%20recognize%20and%20conduct%20token-level%0Arewards%20for%20the%20critical%20tokens%20during%20the%20alignment%20process.%20Specifically%2C%20we%0Adevelop%20a%20contrastive%20estimation%20approach%20to%20automatically%20identify%20critical%0Atokens.%20It%20is%20achieved%20by%20comparing%20the%20generation%20likelihood%20of%20positive%20and%0Anegative%20models.%20To%20achieve%20this%2C%20we%20separately%20fine-tune%20the%20positive%20and%0Anegative%20models%20on%20various%20reasoning%20trajectories%2C%20consequently%2C%20they%20are%0Acapable%20of%20identifying%20identify%20critical%20tokens%20within%20incorrect%20trajectories%0Athat%20contribute%20to%20erroneous%20outcomes.%20Moreover%2C%20to%20further%20align%20the%20model%0Awith%20the%20critical%20token%20information%20during%20the%20alignment%20process%2C%20we%20extend%20the%0Aconventional%20DPO%20algorithms%20to%20token-level%20DPO%20and%20utilize%20the%20differential%0Alikelihood%20from%20the%20aforementioned%20positive%20and%20negative%20model%20as%20important%0Aweight%20for%20token-level%20DPO%20learning.Experimental%20results%20on%20GSM8K%20and%20MATH500%0Abenchmarks%20with%20two-widely%20used%20models%20Llama-3%20%288B%20and%2070B%29%20and%20deepseek-math%0A%287B%29%20demonstrate%20the%20effectiveness%20of%20the%20propsoed%20approach%20cDPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19943v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritical%2520Tokens%2520Matter%253A%2520Token-Level%2520Contrastive%2520Estimation%2520Enhence%2520LLM%2527s%250A%2520%2520Reasoning%2520Capability%26entry.906535625%3DZicheng%2520Lin%2520and%2520Tian%2520Liang%2520and%2520Jiahao%2520Xu%2520and%2520Xing%2520Wang%2520and%2520Ruilin%2520Luo%2520and%2520Chufan%2520Shi%2520and%2520Siheng%2520Li%2520and%2520Yujiu%2520Yang%2520and%2520Zhaopeng%2520Tu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520remarkable%2520performance%2520on%250Areasoning%2520tasks.%2520They%2520utilize%2520autoregressive%2520token%2520generation%2520to%2520construct%250Areasoning%2520trajectories%252C%2520enabling%2520the%2520development%2520of%2520a%2520coherent%2520chain%2520of%250Athought.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520impact%2520of%2520individual%2520tokens%2520on%2520the%2520final%250Aoutcomes%2520of%2520reasoning%2520tasks.%2520We%2520identify%2520the%2520existence%2520of%2520%2560%2560critical%2520tokens%2527%2527%250Athat%2520lead%2520to%2520incorrect%2520reasoning%2520trajectories%2520in%2520LLMs.%2520Specifically%252C%2520we%2520find%250Athat%2520LLMs%2520tend%2520to%2520produce%2520positive%2520outcomes%2520when%2520forced%2520to%2520decode%2520other%2520tokens%250Ainstead%2520of%2520critical%2520tokens.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520-%2520cDPO%2520-%2520designed%2520to%2520automatically%2520recognize%2520and%2520conduct%2520token-level%250Arewards%2520for%2520the%2520critical%2520tokens%2520during%2520the%2520alignment%2520process.%2520Specifically%252C%2520we%250Adevelop%2520a%2520contrastive%2520estimation%2520approach%2520to%2520automatically%2520identify%2520critical%250Atokens.%2520It%2520is%2520achieved%2520by%2520comparing%2520the%2520generation%2520likelihood%2520of%2520positive%2520and%250Anegative%2520models.%2520To%2520achieve%2520this%252C%2520we%2520separately%2520fine-tune%2520the%2520positive%2520and%250Anegative%2520models%2520on%2520various%2520reasoning%2520trajectories%252C%2520consequently%252C%2520they%2520are%250Acapable%2520of%2520identifying%2520identify%2520critical%2520tokens%2520within%2520incorrect%2520trajectories%250Athat%2520contribute%2520to%2520erroneous%2520outcomes.%2520Moreover%252C%2520to%2520further%2520align%2520the%2520model%250Awith%2520the%2520critical%2520token%2520information%2520during%2520the%2520alignment%2520process%252C%2520we%2520extend%2520the%250Aconventional%2520DPO%2520algorithms%2520to%2520token-level%2520DPO%2520and%2520utilize%2520the%2520differential%250Alikelihood%2520from%2520the%2520aforementioned%2520positive%2520and%2520negative%2520model%2520as%2520important%250Aweight%2520for%2520token-level%2520DPO%2520learning.Experimental%2520results%2520on%2520GSM8K%2520and%2520MATH500%250Abenchmarks%2520with%2520two-widely%2520used%2520models%2520Llama-3%2520%25288B%2520and%252070B%2529%2520and%2520deepseek-math%250A%25287B%2529%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520propsoed%2520approach%2520cDPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19943v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critical%20Tokens%20Matter%3A%20Token-Level%20Contrastive%20Estimation%20Enhence%20LLM%27s%0A%20%20Reasoning%20Capability&entry.906535625=Zicheng%20Lin%20and%20Tian%20Liang%20and%20Jiahao%20Xu%20and%20Xing%20Wang%20and%20Ruilin%20Luo%20and%20Chufan%20Shi%20and%20Siheng%20Li%20and%20Yujiu%20Yang%20and%20Zhaopeng%20Tu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20remarkable%20performance%20on%0Areasoning%20tasks.%20They%20utilize%20autoregressive%20token%20generation%20to%20construct%0Areasoning%20trajectories%2C%20enabling%20the%20development%20of%20a%20coherent%20chain%20of%0Athought.%20In%20this%20work%2C%20we%20explore%20the%20impact%20of%20individual%20tokens%20on%20the%20final%0Aoutcomes%20of%20reasoning%20tasks.%20We%20identify%20the%20existence%20of%20%60%60critical%20tokens%27%27%0Athat%20lead%20to%20incorrect%20reasoning%20trajectories%20in%20LLMs.%20Specifically%2C%20we%20find%0Athat%20LLMs%20tend%20to%20produce%20positive%20outcomes%20when%20forced%20to%20decode%20other%20tokens%0Ainstead%20of%20critical%20tokens.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20novel%0Aapproach%20-%20cDPO%20-%20designed%20to%20automatically%20recognize%20and%20conduct%20token-level%0Arewards%20for%20the%20critical%20tokens%20during%20the%20alignment%20process.%20Specifically%2C%20we%0Adevelop%20a%20contrastive%20estimation%20approach%20to%20automatically%20identify%20critical%0Atokens.%20It%20is%20achieved%20by%20comparing%20the%20generation%20likelihood%20of%20positive%20and%0Anegative%20models.%20To%20achieve%20this%2C%20we%20separately%20fine-tune%20the%20positive%20and%0Anegative%20models%20on%20various%20reasoning%20trajectories%2C%20consequently%2C%20they%20are%0Acapable%20of%20identifying%20identify%20critical%20tokens%20within%20incorrect%20trajectories%0Athat%20contribute%20to%20erroneous%20outcomes.%20Moreover%2C%20to%20further%20align%20the%20model%0Awith%20the%20critical%20token%20information%20during%20the%20alignment%20process%2C%20we%20extend%20the%0Aconventional%20DPO%20algorithms%20to%20token-level%20DPO%20and%20utilize%20the%20differential%0Alikelihood%20from%20the%20aforementioned%20positive%20and%20negative%20model%20as%20important%0Aweight%20for%20token-level%20DPO%20learning.Experimental%20results%20on%20GSM8K%20and%20MATH500%0Abenchmarks%20with%20two-widely%20used%20models%20Llama-3%20%288B%20and%2070B%29%20and%20deepseek-math%0A%287B%29%20demonstrate%20the%20effectiveness%20of%20the%20propsoed%20approach%20cDPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19943v1&entry.124074799=Read"},
{"title": "On Domain-Specific Post-Training for Multimodal Large Language Models", "author": "Daixuan Cheng and Shaohan Huang and Ziyu Zhu and Xintong Zhang and Wayne Xin Zhao and Zhongzhi Luan and Bo Dai and Zhenliang Zhang", "abstract": "  Recent years have witnessed the rapid development of general multimodal large\nlanguage models (MLLMs). However, adapting general MLLMs to specific domains,\nsuch as scientific fields and industrial applications, remains less explored.\nThis paper systematically investigates domain adaptation of MLLMs through\npost-training, focusing on data synthesis, training pipelines, and task\nevaluation. (1) Data Synthesis: Using open-source models, we develop a visual\ninstruction synthesizer that effectively generates diverse visual instruction\ntasks from domain-specific image-caption pairs. Our synthetic tasks surpass\nthose generated by manual rules, GPT-4, and GPT-4V in enhancing the\ndomain-specific performance of MLLMs. (2) Training Pipeline: While the\ntwo-stage training--initially on image-caption pairs followed by visual\ninstruction tasks--is commonly adopted for developing general MLLMs, we apply a\nsingle-stage training pipeline to enhance task diversity for domain-specific\npost-training. (3) Task Evaluation: We conduct experiments in two domains,\nbiomedicine and food, by post-training MLLMs of different sources and scales\n(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM\nperformance on various domain-specific tasks. To support further research in\nMLLM domain adaptation, we will open-source our implementations.\n", "link": "http://arxiv.org/abs/2411.19930v1", "date": "2024-11-29", "relevancy": 2.7267, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Domain-Specific%20Post-Training%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20On%20Domain-Specific%20Post-Training%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Daixuan%20Cheng%20and%20Shaohan%20Huang%20and%20Ziyu%20Zhu%20and%20Xintong%20Zhang%20and%20Wayne%20Xin%20Zhao%20and%20Zhongzhi%20Luan%20and%20Bo%20Dai%20and%20Zhenliang%20Zhang%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20the%20rapid%20development%20of%20general%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20However%2C%20adapting%20general%20MLLMs%20to%20specific%20domains%2C%0Asuch%20as%20scientific%20fields%20and%20industrial%20applications%2C%20remains%20less%20explored.%0AThis%20paper%20systematically%20investigates%20domain%20adaptation%20of%20MLLMs%20through%0Apost-training%2C%20focusing%20on%20data%20synthesis%2C%20training%20pipelines%2C%20and%20task%0Aevaluation.%20%281%29%20Data%20Synthesis%3A%20Using%20open-source%20models%2C%20we%20develop%20a%20visual%0Ainstruction%20synthesizer%20that%20effectively%20generates%20diverse%20visual%20instruction%0Atasks%20from%20domain-specific%20image-caption%20pairs.%20Our%20synthetic%20tasks%20surpass%0Athose%20generated%20by%20manual%20rules%2C%20GPT-4%2C%20and%20GPT-4V%20in%20enhancing%20the%0Adomain-specific%20performance%20of%20MLLMs.%20%282%29%20Training%20Pipeline%3A%20While%20the%0Atwo-stage%20training--initially%20on%20image-caption%20pairs%20followed%20by%20visual%0Ainstruction%20tasks--is%20commonly%20adopted%20for%20developing%20general%20MLLMs%2C%20we%20apply%20a%0Asingle-stage%20training%20pipeline%20to%20enhance%20task%20diversity%20for%20domain-specific%0Apost-training.%20%283%29%20Task%20Evaluation%3A%20We%20conduct%20experiments%20in%20two%20domains%2C%0Abiomedicine%20and%20food%2C%20by%20post-training%20MLLMs%20of%20different%20sources%20and%20scales%0A%28e.g.%2C%20Qwen2-VL-2B%2C%20LLaVA-v1.6-8B%2C%20Llama-3.2-11B%29%2C%20and%20then%20evaluating%20MLLM%0Aperformance%20on%20various%20domain-specific%20tasks.%20To%20support%20further%20research%20in%0AMLLM%20domain%20adaptation%2C%20we%20will%20open-source%20our%20implementations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Domain-Specific%2520Post-Training%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DDaixuan%2520Cheng%2520and%2520Shaohan%2520Huang%2520and%2520Ziyu%2520Zhu%2520and%2520Xintong%2520Zhang%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Zhongzhi%2520Luan%2520and%2520Bo%2520Dai%2520and%2520Zhenliang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520the%2520rapid%2520development%2520of%2520general%2520multimodal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529.%2520However%252C%2520adapting%2520general%2520MLLMs%2520to%2520specific%2520domains%252C%250Asuch%2520as%2520scientific%2520fields%2520and%2520industrial%2520applications%252C%2520remains%2520less%2520explored.%250AThis%2520paper%2520systematically%2520investigates%2520domain%2520adaptation%2520of%2520MLLMs%2520through%250Apost-training%252C%2520focusing%2520on%2520data%2520synthesis%252C%2520training%2520pipelines%252C%2520and%2520task%250Aevaluation.%2520%25281%2529%2520Data%2520Synthesis%253A%2520Using%2520open-source%2520models%252C%2520we%2520develop%2520a%2520visual%250Ainstruction%2520synthesizer%2520that%2520effectively%2520generates%2520diverse%2520visual%2520instruction%250Atasks%2520from%2520domain-specific%2520image-caption%2520pairs.%2520Our%2520synthetic%2520tasks%2520surpass%250Athose%2520generated%2520by%2520manual%2520rules%252C%2520GPT-4%252C%2520and%2520GPT-4V%2520in%2520enhancing%2520the%250Adomain-specific%2520performance%2520of%2520MLLMs.%2520%25282%2529%2520Training%2520Pipeline%253A%2520While%2520the%250Atwo-stage%2520training--initially%2520on%2520image-caption%2520pairs%2520followed%2520by%2520visual%250Ainstruction%2520tasks--is%2520commonly%2520adopted%2520for%2520developing%2520general%2520MLLMs%252C%2520we%2520apply%2520a%250Asingle-stage%2520training%2520pipeline%2520to%2520enhance%2520task%2520diversity%2520for%2520domain-specific%250Apost-training.%2520%25283%2529%2520Task%2520Evaluation%253A%2520We%2520conduct%2520experiments%2520in%2520two%2520domains%252C%250Abiomedicine%2520and%2520food%252C%2520by%2520post-training%2520MLLMs%2520of%2520different%2520sources%2520and%2520scales%250A%2528e.g.%252C%2520Qwen2-VL-2B%252C%2520LLaVA-v1.6-8B%252C%2520Llama-3.2-11B%2529%252C%2520and%2520then%2520evaluating%2520MLLM%250Aperformance%2520on%2520various%2520domain-specific%2520tasks.%2520To%2520support%2520further%2520research%2520in%250AMLLM%2520domain%2520adaptation%252C%2520we%2520will%2520open-source%2520our%2520implementations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Domain-Specific%20Post-Training%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Daixuan%20Cheng%20and%20Shaohan%20Huang%20and%20Ziyu%20Zhu%20and%20Xintong%20Zhang%20and%20Wayne%20Xin%20Zhao%20and%20Zhongzhi%20Luan%20and%20Bo%20Dai%20and%20Zhenliang%20Zhang&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20the%20rapid%20development%20of%20general%20multimodal%20large%0Alanguage%20models%20%28MLLMs%29.%20However%2C%20adapting%20general%20MLLMs%20to%20specific%20domains%2C%0Asuch%20as%20scientific%20fields%20and%20industrial%20applications%2C%20remains%20less%20explored.%0AThis%20paper%20systematically%20investigates%20domain%20adaptation%20of%20MLLMs%20through%0Apost-training%2C%20focusing%20on%20data%20synthesis%2C%20training%20pipelines%2C%20and%20task%0Aevaluation.%20%281%29%20Data%20Synthesis%3A%20Using%20open-source%20models%2C%20we%20develop%20a%20visual%0Ainstruction%20synthesizer%20that%20effectively%20generates%20diverse%20visual%20instruction%0Atasks%20from%20domain-specific%20image-caption%20pairs.%20Our%20synthetic%20tasks%20surpass%0Athose%20generated%20by%20manual%20rules%2C%20GPT-4%2C%20and%20GPT-4V%20in%20enhancing%20the%0Adomain-specific%20performance%20of%20MLLMs.%20%282%29%20Training%20Pipeline%3A%20While%20the%0Atwo-stage%20training--initially%20on%20image-caption%20pairs%20followed%20by%20visual%0Ainstruction%20tasks--is%20commonly%20adopted%20for%20developing%20general%20MLLMs%2C%20we%20apply%20a%0Asingle-stage%20training%20pipeline%20to%20enhance%20task%20diversity%20for%20domain-specific%0Apost-training.%20%283%29%20Task%20Evaluation%3A%20We%20conduct%20experiments%20in%20two%20domains%2C%0Abiomedicine%20and%20food%2C%20by%20post-training%20MLLMs%20of%20different%20sources%20and%20scales%0A%28e.g.%2C%20Qwen2-VL-2B%2C%20LLaVA-v1.6-8B%2C%20Llama-3.2-11B%29%2C%20and%20then%20evaluating%20MLLM%0Aperformance%20on%20various%20domain-specific%20tasks.%20To%20support%20further%20research%20in%0AMLLM%20domain%20adaptation%2C%20we%20will%20open-source%20our%20implementations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19930v1&entry.124074799=Read"},
{"title": "Explaining the Impact of Training on Vision Models via Activation\n  Clustering", "author": "Ahc\u00e8ne Boubekki and Samuel G. Fadel and Sebastian Mair", "abstract": "  Recent developments in the field of explainable artificial intelligence (XAI)\nfor vision models investigate the information extracted by their feature\nencoder. We contribute to this effort and propose Neuro-Activated Vision\nExplanations (NAVE), which extracts the information captured by the encoder by\nclustering the feature activations of the frozen network to be explained. The\nmethod does not aim to explain the model's prediction but to answer questions\nsuch as which parts of the image are processed similarly or which information\nis kept in deeper layers. Experimentally, we leverage NAVE to show that the\ntraining dataset and the level of supervision affect which concepts are\ncaptured. In addition, our method reveals the impact of registers on vision\ntransformers (ViT) and the information saturation caused by the watermark\nClever Hans effect in the training set.\n", "link": "http://arxiv.org/abs/2411.19700v1", "date": "2024-11-29", "relevancy": 2.7045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5708}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20the%20Impact%20of%20Training%20on%20Vision%20Models%20via%20Activation%0A%20%20Clustering&body=Title%3A%20Explaining%20the%20Impact%20of%20Training%20on%20Vision%20Models%20via%20Activation%0A%20%20Clustering%0AAuthor%3A%20Ahc%C3%A8ne%20Boubekki%20and%20Samuel%20G.%20Fadel%20and%20Sebastian%20Mair%0AAbstract%3A%20%20%20Recent%20developments%20in%20the%20field%20of%20explainable%20artificial%20intelligence%20%28XAI%29%0Afor%20vision%20models%20investigate%20the%20information%20extracted%20by%20their%20feature%0Aencoder.%20We%20contribute%20to%20this%20effort%20and%20propose%20Neuro-Activated%20Vision%0AExplanations%20%28NAVE%29%2C%20which%20extracts%20the%20information%20captured%20by%20the%20encoder%20by%0Aclustering%20the%20feature%20activations%20of%20the%20frozen%20network%20to%20be%20explained.%20The%0Amethod%20does%20not%20aim%20to%20explain%20the%20model%27s%20prediction%20but%20to%20answer%20questions%0Asuch%20as%20which%20parts%20of%20the%20image%20are%20processed%20similarly%20or%20which%20information%0Ais%20kept%20in%20deeper%20layers.%20Experimentally%2C%20we%20leverage%20NAVE%20to%20show%20that%20the%0Atraining%20dataset%20and%20the%20level%20of%20supervision%20affect%20which%20concepts%20are%0Acaptured.%20In%20addition%2C%20our%20method%20reveals%20the%20impact%20of%20registers%20on%20vision%0Atransformers%20%28ViT%29%20and%20the%20information%20saturation%20caused%20by%20the%20watermark%0AClever%20Hans%20effect%20in%20the%20training%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520the%2520Impact%2520of%2520Training%2520on%2520Vision%2520Models%2520via%2520Activation%250A%2520%2520Clustering%26entry.906535625%3DAhc%25C3%25A8ne%2520Boubekki%2520and%2520Samuel%2520G.%2520Fadel%2520and%2520Sebastian%2520Mair%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520the%2520field%2520of%2520explainable%2520artificial%2520intelligence%2520%2528XAI%2529%250Afor%2520vision%2520models%2520investigate%2520the%2520information%2520extracted%2520by%2520their%2520feature%250Aencoder.%2520We%2520contribute%2520to%2520this%2520effort%2520and%2520propose%2520Neuro-Activated%2520Vision%250AExplanations%2520%2528NAVE%2529%252C%2520which%2520extracts%2520the%2520information%2520captured%2520by%2520the%2520encoder%2520by%250Aclustering%2520the%2520feature%2520activations%2520of%2520the%2520frozen%2520network%2520to%2520be%2520explained.%2520The%250Amethod%2520does%2520not%2520aim%2520to%2520explain%2520the%2520model%2527s%2520prediction%2520but%2520to%2520answer%2520questions%250Asuch%2520as%2520which%2520parts%2520of%2520the%2520image%2520are%2520processed%2520similarly%2520or%2520which%2520information%250Ais%2520kept%2520in%2520deeper%2520layers.%2520Experimentally%252C%2520we%2520leverage%2520NAVE%2520to%2520show%2520that%2520the%250Atraining%2520dataset%2520and%2520the%2520level%2520of%2520supervision%2520affect%2520which%2520concepts%2520are%250Acaptured.%2520In%2520addition%252C%2520our%2520method%2520reveals%2520the%2520impact%2520of%2520registers%2520on%2520vision%250Atransformers%2520%2528ViT%2529%2520and%2520the%2520information%2520saturation%2520caused%2520by%2520the%2520watermark%250AClever%2520Hans%2520effect%2520in%2520the%2520training%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20the%20Impact%20of%20Training%20on%20Vision%20Models%20via%20Activation%0A%20%20Clustering&entry.906535625=Ahc%C3%A8ne%20Boubekki%20and%20Samuel%20G.%20Fadel%20and%20Sebastian%20Mair&entry.1292438233=%20%20Recent%20developments%20in%20the%20field%20of%20explainable%20artificial%20intelligence%20%28XAI%29%0Afor%20vision%20models%20investigate%20the%20information%20extracted%20by%20their%20feature%0Aencoder.%20We%20contribute%20to%20this%20effort%20and%20propose%20Neuro-Activated%20Vision%0AExplanations%20%28NAVE%29%2C%20which%20extracts%20the%20information%20captured%20by%20the%20encoder%20by%0Aclustering%20the%20feature%20activations%20of%20the%20frozen%20network%20to%20be%20explained.%20The%0Amethod%20does%20not%20aim%20to%20explain%20the%20model%27s%20prediction%20but%20to%20answer%20questions%0Asuch%20as%20which%20parts%20of%20the%20image%20are%20processed%20similarly%20or%20which%20information%0Ais%20kept%20in%20deeper%20layers.%20Experimentally%2C%20we%20leverage%20NAVE%20to%20show%20that%20the%0Atraining%20dataset%20and%20the%20level%20of%20supervision%20affect%20which%20concepts%20are%0Acaptured.%20In%20addition%2C%20our%20method%20reveals%20the%20impact%20of%20registers%20on%20vision%0Atransformers%20%28ViT%29%20and%20the%20information%20saturation%20caused%20by%20the%20watermark%0AClever%20Hans%20effect%20in%20the%20training%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19700v1&entry.124074799=Read"},
{"title": "ThermoNeRF: Joint RGB and Thermal Novel View Synthesis for Building\n  Facades using Multimodal Neural Radiance Fields", "author": "Mariam Hassan and Florent Forest and Olga Fink and Malcolm Mielle", "abstract": "  Thermal scene reconstruction holds great potential for various applications,\nsuch as analyzing building energy consumption and performing non-destructive\ninfrastructure testing. However, existing methods typically require dense scene\nmeasurements and often rely on RGB images for 3D geometry reconstruction,\nprojecting thermal information post-reconstruction. This can lead to\ninconsistencies between the reconstructed geometry and temperature data and\ntheir actual values. To address this challenge, we propose ThermoNeRF, a novel\nmultimodal approach based on Neural Radiance Fields that jointly renders new\nRGB and thermal views of a scene, and ThermoScenes, a dataset of paired\nRGB+thermal images comprising 8 scenes of building facades and 8 scenes of\neveryday objects. To address the lack of texture in thermal images, ThermoNeRF\nuses paired RGB and thermal images to learn scene density, while separate\nnetworks estimate color and temperature data. Unlike comparable studies, our\nfocus is on temperature reconstruction and experimental results demonstrate\nthat ThermoNeRF achieves an average mean absolute error of 1.13C and 0.41C for\ntemperature estimation in buildings and other scenes, respectively,\nrepresenting an improvement of over 50% compared to using concatenated\nRGB+thermal data as input to a standard NeRF. Code and dataset are available\nonline.\n", "link": "http://arxiv.org/abs/2403.12154v2", "date": "2024-11-29", "relevancy": 2.6999, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5474}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5363}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThermoNeRF%3A%20Joint%20RGB%20and%20Thermal%20Novel%20View%20Synthesis%20for%20Building%0A%20%20Facades%20using%20Multimodal%20Neural%20Radiance%20Fields&body=Title%3A%20ThermoNeRF%3A%20Joint%20RGB%20and%20Thermal%20Novel%20View%20Synthesis%20for%20Building%0A%20%20Facades%20using%20Multimodal%20Neural%20Radiance%20Fields%0AAuthor%3A%20Mariam%20Hassan%20and%20Florent%20Forest%20and%20Olga%20Fink%20and%20Malcolm%20Mielle%0AAbstract%3A%20%20%20Thermal%20scene%20reconstruction%20holds%20great%20potential%20for%20various%20applications%2C%0Asuch%20as%20analyzing%20building%20energy%20consumption%20and%20performing%20non-destructive%0Ainfrastructure%20testing.%20However%2C%20existing%20methods%20typically%20require%20dense%20scene%0Ameasurements%20and%20often%20rely%20on%20RGB%20images%20for%203D%20geometry%20reconstruction%2C%0Aprojecting%20thermal%20information%20post-reconstruction.%20This%20can%20lead%20to%0Ainconsistencies%20between%20the%20reconstructed%20geometry%20and%20temperature%20data%20and%0Atheir%20actual%20values.%20To%20address%20this%20challenge%2C%20we%20propose%20ThermoNeRF%2C%20a%20novel%0Amultimodal%20approach%20based%20on%20Neural%20Radiance%20Fields%20that%20jointly%20renders%20new%0ARGB%20and%20thermal%20views%20of%20a%20scene%2C%20and%20ThermoScenes%2C%20a%20dataset%20of%20paired%0ARGB%2Bthermal%20images%20comprising%208%20scenes%20of%20building%20facades%20and%208%20scenes%20of%0Aeveryday%20objects.%20To%20address%20the%20lack%20of%20texture%20in%20thermal%20images%2C%20ThermoNeRF%0Auses%20paired%20RGB%20and%20thermal%20images%20to%20learn%20scene%20density%2C%20while%20separate%0Anetworks%20estimate%20color%20and%20temperature%20data.%20Unlike%20comparable%20studies%2C%20our%0Afocus%20is%20on%20temperature%20reconstruction%20and%20experimental%20results%20demonstrate%0Athat%20ThermoNeRF%20achieves%20an%20average%20mean%20absolute%20error%20of%201.13C%20and%200.41C%20for%0Atemperature%20estimation%20in%20buildings%20and%20other%20scenes%2C%20respectively%2C%0Arepresenting%20an%20improvement%20of%20over%2050%25%20compared%20to%20using%20concatenated%0ARGB%2Bthermal%20data%20as%20input%20to%20a%20standard%20NeRF.%20Code%20and%20dataset%20are%20available%0Aonline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermoNeRF%253A%2520Joint%2520RGB%2520and%2520Thermal%2520Novel%2520View%2520Synthesis%2520for%2520Building%250A%2520%2520Facades%2520using%2520Multimodal%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DMariam%2520Hassan%2520and%2520Florent%2520Forest%2520and%2520Olga%2520Fink%2520and%2520Malcolm%2520Mielle%26entry.1292438233%3D%2520%2520Thermal%2520scene%2520reconstruction%2520holds%2520great%2520potential%2520for%2520various%2520applications%252C%250Asuch%2520as%2520analyzing%2520building%2520energy%2520consumption%2520and%2520performing%2520non-destructive%250Ainfrastructure%2520testing.%2520However%252C%2520existing%2520methods%2520typically%2520require%2520dense%2520scene%250Ameasurements%2520and%2520often%2520rely%2520on%2520RGB%2520images%2520for%25203D%2520geometry%2520reconstruction%252C%250Aprojecting%2520thermal%2520information%2520post-reconstruction.%2520This%2520can%2520lead%2520to%250Ainconsistencies%2520between%2520the%2520reconstructed%2520geometry%2520and%2520temperature%2520data%2520and%250Atheir%2520actual%2520values.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520ThermoNeRF%252C%2520a%2520novel%250Amultimodal%2520approach%2520based%2520on%2520Neural%2520Radiance%2520Fields%2520that%2520jointly%2520renders%2520new%250ARGB%2520and%2520thermal%2520views%2520of%2520a%2520scene%252C%2520and%2520ThermoScenes%252C%2520a%2520dataset%2520of%2520paired%250ARGB%252Bthermal%2520images%2520comprising%25208%2520scenes%2520of%2520building%2520facades%2520and%25208%2520scenes%2520of%250Aeveryday%2520objects.%2520To%2520address%2520the%2520lack%2520of%2520texture%2520in%2520thermal%2520images%252C%2520ThermoNeRF%250Auses%2520paired%2520RGB%2520and%2520thermal%2520images%2520to%2520learn%2520scene%2520density%252C%2520while%2520separate%250Anetworks%2520estimate%2520color%2520and%2520temperature%2520data.%2520Unlike%2520comparable%2520studies%252C%2520our%250Afocus%2520is%2520on%2520temperature%2520reconstruction%2520and%2520experimental%2520results%2520demonstrate%250Athat%2520ThermoNeRF%2520achieves%2520an%2520average%2520mean%2520absolute%2520error%2520of%25201.13C%2520and%25200.41C%2520for%250Atemperature%2520estimation%2520in%2520buildings%2520and%2520other%2520scenes%252C%2520respectively%252C%250Arepresenting%2520an%2520improvement%2520of%2520over%252050%2525%2520compared%2520to%2520using%2520concatenated%250ARGB%252Bthermal%2520data%2520as%2520input%2520to%2520a%2520standard%2520NeRF.%2520Code%2520and%2520dataset%2520are%2520available%250Aonline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThermoNeRF%3A%20Joint%20RGB%20and%20Thermal%20Novel%20View%20Synthesis%20for%20Building%0A%20%20Facades%20using%20Multimodal%20Neural%20Radiance%20Fields&entry.906535625=Mariam%20Hassan%20and%20Florent%20Forest%20and%20Olga%20Fink%20and%20Malcolm%20Mielle&entry.1292438233=%20%20Thermal%20scene%20reconstruction%20holds%20great%20potential%20for%20various%20applications%2C%0Asuch%20as%20analyzing%20building%20energy%20consumption%20and%20performing%20non-destructive%0Ainfrastructure%20testing.%20However%2C%20existing%20methods%20typically%20require%20dense%20scene%0Ameasurements%20and%20often%20rely%20on%20RGB%20images%20for%203D%20geometry%20reconstruction%2C%0Aprojecting%20thermal%20information%20post-reconstruction.%20This%20can%20lead%20to%0Ainconsistencies%20between%20the%20reconstructed%20geometry%20and%20temperature%20data%20and%0Atheir%20actual%20values.%20To%20address%20this%20challenge%2C%20we%20propose%20ThermoNeRF%2C%20a%20novel%0Amultimodal%20approach%20based%20on%20Neural%20Radiance%20Fields%20that%20jointly%20renders%20new%0ARGB%20and%20thermal%20views%20of%20a%20scene%2C%20and%20ThermoScenes%2C%20a%20dataset%20of%20paired%0ARGB%2Bthermal%20images%20comprising%208%20scenes%20of%20building%20facades%20and%208%20scenes%20of%0Aeveryday%20objects.%20To%20address%20the%20lack%20of%20texture%20in%20thermal%20images%2C%20ThermoNeRF%0Auses%20paired%20RGB%20and%20thermal%20images%20to%20learn%20scene%20density%2C%20while%20separate%0Anetworks%20estimate%20color%20and%20temperature%20data.%20Unlike%20comparable%20studies%2C%20our%0Afocus%20is%20on%20temperature%20reconstruction%20and%20experimental%20results%20demonstrate%0Athat%20ThermoNeRF%20achieves%20an%20average%20mean%20absolute%20error%20of%201.13C%20and%200.41C%20for%0Atemperature%20estimation%20in%20buildings%20and%20other%20scenes%2C%20respectively%2C%0Arepresenting%20an%20improvement%20of%20over%2050%25%20compared%20to%20using%20concatenated%0ARGB%2Bthermal%20data%20as%20input%20to%20a%20standard%20NeRF.%20Code%20and%20dataset%20are%20available%0Aonline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12154v2&entry.124074799=Read"},
{"title": "A Survey on Multimodal Large Language Models", "author": "Shukang Yin and Chaoyou Fu and Sirui Zhao and Ke Li and Xing Sun and Tong Xu and Enhong Chen", "abstract": "  Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\n", "link": "http://arxiv.org/abs/2306.13549v4", "date": "2024-11-29", "relevancy": 2.6638, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5301}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Multimodal%20Large%20Language%20Models&body=Title%3A%20A%20Survey%20on%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Xing%20Sun%20and%20Tong%20Xu%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Recently%2C%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20represented%20by%20GPT-4V%20has%0Abeen%20a%20new%20rising%20research%20hotspot%2C%20which%20uses%20powerful%20Large%20Language%20Models%0A%28LLMs%29%20as%20a%20brain%20to%20perform%20multimodal%20tasks.%20The%20surprising%20emergent%0Acapabilities%20of%20MLLM%2C%20such%20as%20writing%20stories%20based%20on%20images%20and%20OCR-free%20math%0Areasoning%2C%20are%20rare%20in%20traditional%20multimodal%20methods%2C%20suggesting%20a%20potential%0Apath%20to%20artificial%20general%20intelligence.%20To%20this%20end%2C%20both%20academia%20and%0Aindustry%20have%20endeavored%20to%20develop%20MLLMs%20that%20can%20compete%20with%20or%20even%20better%0Athan%20GPT-4V%2C%20pushing%20the%20limit%20of%20research%20at%20a%20surprising%20speed.%20In%20this%0Apaper%2C%20we%20aim%20to%20trace%20and%20summarize%20the%20recent%20progress%20of%20MLLMs.%20First%20of%0Aall%2C%20we%20present%20the%20basic%20formulation%20of%20MLLM%20and%20delineate%20its%20related%0Aconcepts%2C%20including%20architecture%2C%20training%20strategy%20and%20data%2C%20as%20well%20as%0Aevaluation.%20Then%2C%20we%20introduce%20research%20topics%20about%20how%20MLLMs%20can%20be%20extended%0Ato%20support%20more%20granularity%2C%20modalities%2C%20languages%2C%20and%20scenarios.%20We%20continue%0Awith%20multimodal%20hallucination%20and%20extended%20techniques%2C%20including%20Multimodal%20ICL%0A%28M-ICL%29%2C%20Multimodal%20CoT%20%28M-CoT%29%2C%20and%20LLM-Aided%20Visual%20Reasoning%20%28LAVR%29.%20To%0Aconclude%20the%20paper%2C%20we%20discuss%20existing%20challenges%20and%20point%20out%20promising%0Aresearch%20directions.%20In%20light%20of%20the%20fact%20that%20the%20era%20of%20MLLM%20has%20only%20just%0Abegun%2C%20we%20will%20keep%20updating%20this%20survey%20and%20hope%20it%20can%20inspire%20more%20research.%0AAn%20associated%20GitHub%20link%20collecting%20the%20latest%20papers%20is%20available%20at%0Ahttps%3A//github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13549v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DShukang%2520Yin%2520and%2520Chaoyou%2520Fu%2520and%2520Sirui%2520Zhao%2520and%2520Ke%2520Li%2520and%2520Xing%2520Sun%2520and%2520Tong%2520Xu%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520represented%2520by%2520GPT-4V%2520has%250Abeen%2520a%2520new%2520rising%2520research%2520hotspot%252C%2520which%2520uses%2520powerful%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520as%2520a%2520brain%2520to%2520perform%2520multimodal%2520tasks.%2520The%2520surprising%2520emergent%250Acapabilities%2520of%2520MLLM%252C%2520such%2520as%2520writing%2520stories%2520based%2520on%2520images%2520and%2520OCR-free%2520math%250Areasoning%252C%2520are%2520rare%2520in%2520traditional%2520multimodal%2520methods%252C%2520suggesting%2520a%2520potential%250Apath%2520to%2520artificial%2520general%2520intelligence.%2520To%2520this%2520end%252C%2520both%2520academia%2520and%250Aindustry%2520have%2520endeavored%2520to%2520develop%2520MLLMs%2520that%2520can%2520compete%2520with%2520or%2520even%2520better%250Athan%2520GPT-4V%252C%2520pushing%2520the%2520limit%2520of%2520research%2520at%2520a%2520surprising%2520speed.%2520In%2520this%250Apaper%252C%2520we%2520aim%2520to%2520trace%2520and%2520summarize%2520the%2520recent%2520progress%2520of%2520MLLMs.%2520First%2520of%250Aall%252C%2520we%2520present%2520the%2520basic%2520formulation%2520of%2520MLLM%2520and%2520delineate%2520its%2520related%250Aconcepts%252C%2520including%2520architecture%252C%2520training%2520strategy%2520and%2520data%252C%2520as%2520well%2520as%250Aevaluation.%2520Then%252C%2520we%2520introduce%2520research%2520topics%2520about%2520how%2520MLLMs%2520can%2520be%2520extended%250Ato%2520support%2520more%2520granularity%252C%2520modalities%252C%2520languages%252C%2520and%2520scenarios.%2520We%2520continue%250Awith%2520multimodal%2520hallucination%2520and%2520extended%2520techniques%252C%2520including%2520Multimodal%2520ICL%250A%2528M-ICL%2529%252C%2520Multimodal%2520CoT%2520%2528M-CoT%2529%252C%2520and%2520LLM-Aided%2520Visual%2520Reasoning%2520%2528LAVR%2529.%2520To%250Aconclude%2520the%2520paper%252C%2520we%2520discuss%2520existing%2520challenges%2520and%2520point%2520out%2520promising%250Aresearch%2520directions.%2520In%2520light%2520of%2520the%2520fact%2520that%2520the%2520era%2520of%2520MLLM%2520has%2520only%2520just%250Abegun%252C%2520we%2520will%2520keep%2520updating%2520this%2520survey%2520and%2520hope%2520it%2520can%2520inspire%2520more%2520research.%250AAn%2520associated%2520GitHub%2520link%2520collecting%2520the%2520latest%2520papers%2520is%2520available%2520at%250Ahttps%253A//github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13549v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Multimodal%20Large%20Language%20Models&entry.906535625=Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Xing%20Sun%20and%20Tong%20Xu%20and%20Enhong%20Chen&entry.1292438233=%20%20Recently%2C%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20represented%20by%20GPT-4V%20has%0Abeen%20a%20new%20rising%20research%20hotspot%2C%20which%20uses%20powerful%20Large%20Language%20Models%0A%28LLMs%29%20as%20a%20brain%20to%20perform%20multimodal%20tasks.%20The%20surprising%20emergent%0Acapabilities%20of%20MLLM%2C%20such%20as%20writing%20stories%20based%20on%20images%20and%20OCR-free%20math%0Areasoning%2C%20are%20rare%20in%20traditional%20multimodal%20methods%2C%20suggesting%20a%20potential%0Apath%20to%20artificial%20general%20intelligence.%20To%20this%20end%2C%20both%20academia%20and%0Aindustry%20have%20endeavored%20to%20develop%20MLLMs%20that%20can%20compete%20with%20or%20even%20better%0Athan%20GPT-4V%2C%20pushing%20the%20limit%20of%20research%20at%20a%20surprising%20speed.%20In%20this%0Apaper%2C%20we%20aim%20to%20trace%20and%20summarize%20the%20recent%20progress%20of%20MLLMs.%20First%20of%0Aall%2C%20we%20present%20the%20basic%20formulation%20of%20MLLM%20and%20delineate%20its%20related%0Aconcepts%2C%20including%20architecture%2C%20training%20strategy%20and%20data%2C%20as%20well%20as%0Aevaluation.%20Then%2C%20we%20introduce%20research%20topics%20about%20how%20MLLMs%20can%20be%20extended%0Ato%20support%20more%20granularity%2C%20modalities%2C%20languages%2C%20and%20scenarios.%20We%20continue%0Awith%20multimodal%20hallucination%20and%20extended%20techniques%2C%20including%20Multimodal%20ICL%0A%28M-ICL%29%2C%20Multimodal%20CoT%20%28M-CoT%29%2C%20and%20LLM-Aided%20Visual%20Reasoning%20%28LAVR%29.%20To%0Aconclude%20the%20paper%2C%20we%20discuss%20existing%20challenges%20and%20point%20out%20promising%0Aresearch%20directions.%20In%20light%20of%20the%20fact%20that%20the%20era%20of%20MLLM%20has%20only%20just%0Abegun%2C%20we%20will%20keep%20updating%20this%20survey%20and%20hope%20it%20can%20inspire%20more%20research.%0AAn%20associated%20GitHub%20link%20collecting%20the%20latest%20papers%20is%20available%20at%0Ahttps%3A//github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13549v4&entry.124074799=Read"},
{"title": "Simultaneous Image-to-Zero and Zero-to-Noise: Diffusion Models with\n  Analytical Image Attenuation", "author": "Yuhang Huang and Zheng Qin and Xinwang Liu and Kai Xu", "abstract": "  Recent studies have demonstrated that the forward diffusion process is\ncrucial for the effectiveness of diffusion models in terms of generative\nquality and sampling efficiency. We propose incorporating an analytical image\nattenuation process into the forward diffusion process for high-quality\n(un)conditioned image generation with significantly fewer denoising steps\ncompared to the vanilla diffusion model requiring thousands of steps. In a\nnutshell, our method represents the forward image-to-noise mapping as\nsimultaneous \\textit{image-to-zero} mapping and \\textit{zero-to-noise} mapping.\nUnder this framework, we mathematically derive 1) the training objectives and\n2) for the reverse time the sampling formula based on an analytical attenuation\nfunction which models image to zero mapping. The former enables our method to\nlearn noise and image components simultaneously which simplifies learning.\nImportantly, because of the latter's analyticity in the \\textit{zero-to-image}\nsampling function, we can avoid the ordinary differential equation-based\naccelerators and instead naturally perform sampling with an arbitrary step\nsize. We have conducted extensive experiments on unconditioned image\ngeneration, \\textit{e.g.}, CIFAR-10 and CelebA-HQ-256, and image-conditioned\ndownstream tasks such as super-resolution, saliency detection, edge detection,\nand image inpainting. The proposed diffusion models achieve competitive\ngenerative quality with much fewer denoising steps compared to the state of the\nart, thus greatly accelerating the generation speed. In particular, to generate\nimages of comparable quality, our models require only one-twentieth of the\ndenoising steps compared to the baseline denoising diffusion probabilistic\nmodels. Moreover, we achieve state-of-the-art performances on the\nimage-conditioned tasks using only no more than 10 steps.\n", "link": "http://arxiv.org/abs/2306.13720v9", "date": "2024-11-29", "relevancy": 2.662, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.746}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6624}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Image-to-Zero%20and%20Zero-to-Noise%3A%20Diffusion%20Models%20with%0A%20%20Analytical%20Image%20Attenuation&body=Title%3A%20Simultaneous%20Image-to-Zero%20and%20Zero-to-Noise%3A%20Diffusion%20Models%20with%0A%20%20Analytical%20Image%20Attenuation%0AAuthor%3A%20Yuhang%20Huang%20and%20Zheng%20Qin%20and%20Xinwang%20Liu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20Recent%20studies%20have%20demonstrated%20that%20the%20forward%20diffusion%20process%20is%0Acrucial%20for%20the%20effectiveness%20of%20diffusion%20models%20in%20terms%20of%20generative%0Aquality%20and%20sampling%20efficiency.%20We%20propose%20incorporating%20an%20analytical%20image%0Aattenuation%20process%20into%20the%20forward%20diffusion%20process%20for%20high-quality%0A%28un%29conditioned%20image%20generation%20with%20significantly%20fewer%20denoising%20steps%0Acompared%20to%20the%20vanilla%20diffusion%20model%20requiring%20thousands%20of%20steps.%20In%20a%0Anutshell%2C%20our%20method%20represents%20the%20forward%20image-to-noise%20mapping%20as%0Asimultaneous%20%5Ctextit%7Bimage-to-zero%7D%20mapping%20and%20%5Ctextit%7Bzero-to-noise%7D%20mapping.%0AUnder%20this%20framework%2C%20we%20mathematically%20derive%201%29%20the%20training%20objectives%20and%0A2%29%20for%20the%20reverse%20time%20the%20sampling%20formula%20based%20on%20an%20analytical%20attenuation%0Afunction%20which%20models%20image%20to%20zero%20mapping.%20The%20former%20enables%20our%20method%20to%0Alearn%20noise%20and%20image%20components%20simultaneously%20which%20simplifies%20learning.%0AImportantly%2C%20because%20of%20the%20latter%27s%20analyticity%20in%20the%20%5Ctextit%7Bzero-to-image%7D%0Asampling%20function%2C%20we%20can%20avoid%20the%20ordinary%20differential%20equation-based%0Aaccelerators%20and%20instead%20naturally%20perform%20sampling%20with%20an%20arbitrary%20step%0Asize.%20We%20have%20conducted%20extensive%20experiments%20on%20unconditioned%20image%0Ageneration%2C%20%5Ctextit%7Be.g.%7D%2C%20CIFAR-10%20and%20CelebA-HQ-256%2C%20and%20image-conditioned%0Adownstream%20tasks%20such%20as%20super-resolution%2C%20saliency%20detection%2C%20edge%20detection%2C%0Aand%20image%20inpainting.%20The%20proposed%20diffusion%20models%20achieve%20competitive%0Agenerative%20quality%20with%20much%20fewer%20denoising%20steps%20compared%20to%20the%20state%20of%20the%0Aart%2C%20thus%20greatly%20accelerating%20the%20generation%20speed.%20In%20particular%2C%20to%20generate%0Aimages%20of%20comparable%20quality%2C%20our%20models%20require%20only%20one-twentieth%20of%20the%0Adenoising%20steps%20compared%20to%20the%20baseline%20denoising%20diffusion%20probabilistic%0Amodels.%20Moreover%2C%20we%20achieve%20state-of-the-art%20performances%20on%20the%0Aimage-conditioned%20tasks%20using%20only%20no%20more%20than%2010%20steps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13720v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimultaneous%2520Image-to-Zero%2520and%2520Zero-to-Noise%253A%2520Diffusion%2520Models%2520with%250A%2520%2520Analytical%2520Image%2520Attenuation%26entry.906535625%3DYuhang%2520Huang%2520and%2520Zheng%2520Qin%2520and%2520Xinwang%2520Liu%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520demonstrated%2520that%2520the%2520forward%2520diffusion%2520process%2520is%250Acrucial%2520for%2520the%2520effectiveness%2520of%2520diffusion%2520models%2520in%2520terms%2520of%2520generative%250Aquality%2520and%2520sampling%2520efficiency.%2520We%2520propose%2520incorporating%2520an%2520analytical%2520image%250Aattenuation%2520process%2520into%2520the%2520forward%2520diffusion%2520process%2520for%2520high-quality%250A%2528un%2529conditioned%2520image%2520generation%2520with%2520significantly%2520fewer%2520denoising%2520steps%250Acompared%2520to%2520the%2520vanilla%2520diffusion%2520model%2520requiring%2520thousands%2520of%2520steps.%2520In%2520a%250Anutshell%252C%2520our%2520method%2520represents%2520the%2520forward%2520image-to-noise%2520mapping%2520as%250Asimultaneous%2520%255Ctextit%257Bimage-to-zero%257D%2520mapping%2520and%2520%255Ctextit%257Bzero-to-noise%257D%2520mapping.%250AUnder%2520this%2520framework%252C%2520we%2520mathematically%2520derive%25201%2529%2520the%2520training%2520objectives%2520and%250A2%2529%2520for%2520the%2520reverse%2520time%2520the%2520sampling%2520formula%2520based%2520on%2520an%2520analytical%2520attenuation%250Afunction%2520which%2520models%2520image%2520to%2520zero%2520mapping.%2520The%2520former%2520enables%2520our%2520method%2520to%250Alearn%2520noise%2520and%2520image%2520components%2520simultaneously%2520which%2520simplifies%2520learning.%250AImportantly%252C%2520because%2520of%2520the%2520latter%2527s%2520analyticity%2520in%2520the%2520%255Ctextit%257Bzero-to-image%257D%250Asampling%2520function%252C%2520we%2520can%2520avoid%2520the%2520ordinary%2520differential%2520equation-based%250Aaccelerators%2520and%2520instead%2520naturally%2520perform%2520sampling%2520with%2520an%2520arbitrary%2520step%250Asize.%2520We%2520have%2520conducted%2520extensive%2520experiments%2520on%2520unconditioned%2520image%250Ageneration%252C%2520%255Ctextit%257Be.g.%257D%252C%2520CIFAR-10%2520and%2520CelebA-HQ-256%252C%2520and%2520image-conditioned%250Adownstream%2520tasks%2520such%2520as%2520super-resolution%252C%2520saliency%2520detection%252C%2520edge%2520detection%252C%250Aand%2520image%2520inpainting.%2520The%2520proposed%2520diffusion%2520models%2520achieve%2520competitive%250Agenerative%2520quality%2520with%2520much%2520fewer%2520denoising%2520steps%2520compared%2520to%2520the%2520state%2520of%2520the%250Aart%252C%2520thus%2520greatly%2520accelerating%2520the%2520generation%2520speed.%2520In%2520particular%252C%2520to%2520generate%250Aimages%2520of%2520comparable%2520quality%252C%2520our%2520models%2520require%2520only%2520one-twentieth%2520of%2520the%250Adenoising%2520steps%2520compared%2520to%2520the%2520baseline%2520denoising%2520diffusion%2520probabilistic%250Amodels.%2520Moreover%252C%2520we%2520achieve%2520state-of-the-art%2520performances%2520on%2520the%250Aimage-conditioned%2520tasks%2520using%2520only%2520no%2520more%2520than%252010%2520steps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13720v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Image-to-Zero%20and%20Zero-to-Noise%3A%20Diffusion%20Models%20with%0A%20%20Analytical%20Image%20Attenuation&entry.906535625=Yuhang%20Huang%20and%20Zheng%20Qin%20and%20Xinwang%20Liu%20and%20Kai%20Xu&entry.1292438233=%20%20Recent%20studies%20have%20demonstrated%20that%20the%20forward%20diffusion%20process%20is%0Acrucial%20for%20the%20effectiveness%20of%20diffusion%20models%20in%20terms%20of%20generative%0Aquality%20and%20sampling%20efficiency.%20We%20propose%20incorporating%20an%20analytical%20image%0Aattenuation%20process%20into%20the%20forward%20diffusion%20process%20for%20high-quality%0A%28un%29conditioned%20image%20generation%20with%20significantly%20fewer%20denoising%20steps%0Acompared%20to%20the%20vanilla%20diffusion%20model%20requiring%20thousands%20of%20steps.%20In%20a%0Anutshell%2C%20our%20method%20represents%20the%20forward%20image-to-noise%20mapping%20as%0Asimultaneous%20%5Ctextit%7Bimage-to-zero%7D%20mapping%20and%20%5Ctextit%7Bzero-to-noise%7D%20mapping.%0AUnder%20this%20framework%2C%20we%20mathematically%20derive%201%29%20the%20training%20objectives%20and%0A2%29%20for%20the%20reverse%20time%20the%20sampling%20formula%20based%20on%20an%20analytical%20attenuation%0Afunction%20which%20models%20image%20to%20zero%20mapping.%20The%20former%20enables%20our%20method%20to%0Alearn%20noise%20and%20image%20components%20simultaneously%20which%20simplifies%20learning.%0AImportantly%2C%20because%20of%20the%20latter%27s%20analyticity%20in%20the%20%5Ctextit%7Bzero-to-image%7D%0Asampling%20function%2C%20we%20can%20avoid%20the%20ordinary%20differential%20equation-based%0Aaccelerators%20and%20instead%20naturally%20perform%20sampling%20with%20an%20arbitrary%20step%0Asize.%20We%20have%20conducted%20extensive%20experiments%20on%20unconditioned%20image%0Ageneration%2C%20%5Ctextit%7Be.g.%7D%2C%20CIFAR-10%20and%20CelebA-HQ-256%2C%20and%20image-conditioned%0Adownstream%20tasks%20such%20as%20super-resolution%2C%20saliency%20detection%2C%20edge%20detection%2C%0Aand%20image%20inpainting.%20The%20proposed%20diffusion%20models%20achieve%20competitive%0Agenerative%20quality%20with%20much%20fewer%20denoising%20steps%20compared%20to%20the%20state%20of%20the%0Aart%2C%20thus%20greatly%20accelerating%20the%20generation%20speed.%20In%20particular%2C%20to%20generate%0Aimages%20of%20comparable%20quality%2C%20our%20models%20require%20only%20one-twentieth%20of%20the%0Adenoising%20steps%20compared%20to%20the%20baseline%20denoising%20diffusion%20probabilistic%0Amodels.%20Moreover%2C%20we%20achieve%20state-of-the-art%20performances%20on%20the%0Aimage-conditioned%20tasks%20using%20only%20no%20more%20than%2010%20steps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13720v9&entry.124074799=Read"},
{"title": "GalLoP: Learning Global and Local Prompts for Vision-Language Models", "author": "Marc Lafon and Elias Ramzi and Cl\u00e9ment Rambour and Nicolas Audebert and Nicolas Thome", "abstract": "  Prompt learning has been widely adopted to efficiently adapt vision-language\nmodels (VLMs), e.g. CLIP, for few-shot image classification. Despite their\nsuccess, most prompt learning methods trade-off between classification accuracy\nand robustness, e.g. in domain generalization or out-of-distribution (OOD)\ndetection. In this work, we introduce Global-Local Prompts (GalLoP), a new\nprompt learning method that learns multiple diverse prompts leveraging both\nglobal and local visual features. The training of the local prompts relies on\nlocal features with an enhanced vision-text alignment. To focus only on\npertinent features, this local alignment is coupled with a sparsity strategy in\nthe selection of the local features. We enforce diversity on the set of prompts\nusing a new ``prompt dropout'' technique and a multiscale strategy on the local\nprompts. GalLoP outperforms previous prompt learning methods on accuracy on\neleven datasets in different few shots settings and with various backbones.\nFurthermore, GalLoP shows strong robustness performances in both domain\ngeneralization and OOD detection, even outperforming dedicated OOD detection\nmethods. Code and instructions to reproduce our results:\nhttps://github.com/MarcLafon/gallop.\n", "link": "http://arxiv.org/abs/2407.01400v2", "date": "2024-11-29", "relevancy": 2.6429, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GalLoP%3A%20Learning%20Global%20and%20Local%20Prompts%20for%20Vision-Language%20Models&body=Title%3A%20GalLoP%3A%20Learning%20Global%20and%20Local%20Prompts%20for%20Vision-Language%20Models%0AAuthor%3A%20Marc%20Lafon%20and%20Elias%20Ramzi%20and%20Cl%C3%A9ment%20Rambour%20and%20Nicolas%20Audebert%20and%20Nicolas%20Thome%0AAbstract%3A%20%20%20Prompt%20learning%20has%20been%20widely%20adopted%20to%20efficiently%20adapt%20vision-language%0Amodels%20%28VLMs%29%2C%20e.g.%20CLIP%2C%20for%20few-shot%20image%20classification.%20Despite%20their%0Asuccess%2C%20most%20prompt%20learning%20methods%20trade-off%20between%20classification%20accuracy%0Aand%20robustness%2C%20e.g.%20in%20domain%20generalization%20or%20out-of-distribution%20%28OOD%29%0Adetection.%20In%20this%20work%2C%20we%20introduce%20Global-Local%20Prompts%20%28GalLoP%29%2C%20a%20new%0Aprompt%20learning%20method%20that%20learns%20multiple%20diverse%20prompts%20leveraging%20both%0Aglobal%20and%20local%20visual%20features.%20The%20training%20of%20the%20local%20prompts%20relies%20on%0Alocal%20features%20with%20an%20enhanced%20vision-text%20alignment.%20To%20focus%20only%20on%0Apertinent%20features%2C%20this%20local%20alignment%20is%20coupled%20with%20a%20sparsity%20strategy%20in%0Athe%20selection%20of%20the%20local%20features.%20We%20enforce%20diversity%20on%20the%20set%20of%20prompts%0Ausing%20a%20new%20%60%60prompt%20dropout%27%27%20technique%20and%20a%20multiscale%20strategy%20on%20the%20local%0Aprompts.%20GalLoP%20outperforms%20previous%20prompt%20learning%20methods%20on%20accuracy%20on%0Aeleven%20datasets%20in%20different%20few%20shots%20settings%20and%20with%20various%20backbones.%0AFurthermore%2C%20GalLoP%20shows%20strong%20robustness%20performances%20in%20both%20domain%0Ageneralization%20and%20OOD%20detection%2C%20even%20outperforming%20dedicated%20OOD%20detection%0Amethods.%20Code%20and%20instructions%20to%20reproduce%20our%20results%3A%0Ahttps%3A//github.com/MarcLafon/gallop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGalLoP%253A%2520Learning%2520Global%2520and%2520Local%2520Prompts%2520for%2520Vision-Language%2520Models%26entry.906535625%3DMarc%2520Lafon%2520and%2520Elias%2520Ramzi%2520and%2520Cl%25C3%25A9ment%2520Rambour%2520and%2520Nicolas%2520Audebert%2520and%2520Nicolas%2520Thome%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520has%2520been%2520widely%2520adopted%2520to%2520efficiently%2520adapt%2520vision-language%250Amodels%2520%2528VLMs%2529%252C%2520e.g.%2520CLIP%252C%2520for%2520few-shot%2520image%2520classification.%2520Despite%2520their%250Asuccess%252C%2520most%2520prompt%2520learning%2520methods%2520trade-off%2520between%2520classification%2520accuracy%250Aand%2520robustness%252C%2520e.g.%2520in%2520domain%2520generalization%2520or%2520out-of-distribution%2520%2528OOD%2529%250Adetection.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Global-Local%2520Prompts%2520%2528GalLoP%2529%252C%2520a%2520new%250Aprompt%2520learning%2520method%2520that%2520learns%2520multiple%2520diverse%2520prompts%2520leveraging%2520both%250Aglobal%2520and%2520local%2520visual%2520features.%2520The%2520training%2520of%2520the%2520local%2520prompts%2520relies%2520on%250Alocal%2520features%2520with%2520an%2520enhanced%2520vision-text%2520alignment.%2520To%2520focus%2520only%2520on%250Apertinent%2520features%252C%2520this%2520local%2520alignment%2520is%2520coupled%2520with%2520a%2520sparsity%2520strategy%2520in%250Athe%2520selection%2520of%2520the%2520local%2520features.%2520We%2520enforce%2520diversity%2520on%2520the%2520set%2520of%2520prompts%250Ausing%2520a%2520new%2520%2560%2560prompt%2520dropout%2527%2527%2520technique%2520and%2520a%2520multiscale%2520strategy%2520on%2520the%2520local%250Aprompts.%2520GalLoP%2520outperforms%2520previous%2520prompt%2520learning%2520methods%2520on%2520accuracy%2520on%250Aeleven%2520datasets%2520in%2520different%2520few%2520shots%2520settings%2520and%2520with%2520various%2520backbones.%250AFurthermore%252C%2520GalLoP%2520shows%2520strong%2520robustness%2520performances%2520in%2520both%2520domain%250Ageneralization%2520and%2520OOD%2520detection%252C%2520even%2520outperforming%2520dedicated%2520OOD%2520detection%250Amethods.%2520Code%2520and%2520instructions%2520to%2520reproduce%2520our%2520results%253A%250Ahttps%253A//github.com/MarcLafon/gallop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GalLoP%3A%20Learning%20Global%20and%20Local%20Prompts%20for%20Vision-Language%20Models&entry.906535625=Marc%20Lafon%20and%20Elias%20Ramzi%20and%20Cl%C3%A9ment%20Rambour%20and%20Nicolas%20Audebert%20and%20Nicolas%20Thome&entry.1292438233=%20%20Prompt%20learning%20has%20been%20widely%20adopted%20to%20efficiently%20adapt%20vision-language%0Amodels%20%28VLMs%29%2C%20e.g.%20CLIP%2C%20for%20few-shot%20image%20classification.%20Despite%20their%0Asuccess%2C%20most%20prompt%20learning%20methods%20trade-off%20between%20classification%20accuracy%0Aand%20robustness%2C%20e.g.%20in%20domain%20generalization%20or%20out-of-distribution%20%28OOD%29%0Adetection.%20In%20this%20work%2C%20we%20introduce%20Global-Local%20Prompts%20%28GalLoP%29%2C%20a%20new%0Aprompt%20learning%20method%20that%20learns%20multiple%20diverse%20prompts%20leveraging%20both%0Aglobal%20and%20local%20visual%20features.%20The%20training%20of%20the%20local%20prompts%20relies%20on%0Alocal%20features%20with%20an%20enhanced%20vision-text%20alignment.%20To%20focus%20only%20on%0Apertinent%20features%2C%20this%20local%20alignment%20is%20coupled%20with%20a%20sparsity%20strategy%20in%0Athe%20selection%20of%20the%20local%20features.%20We%20enforce%20diversity%20on%20the%20set%20of%20prompts%0Ausing%20a%20new%20%60%60prompt%20dropout%27%27%20technique%20and%20a%20multiscale%20strategy%20on%20the%20local%0Aprompts.%20GalLoP%20outperforms%20previous%20prompt%20learning%20methods%20on%20accuracy%20on%0Aeleven%20datasets%20in%20different%20few%20shots%20settings%20and%20with%20various%20backbones.%0AFurthermore%2C%20GalLoP%20shows%20strong%20robustness%20performances%20in%20both%20domain%0Ageneralization%20and%20OOD%20detection%2C%20even%20outperforming%20dedicated%20OOD%20detection%0Amethods.%20Code%20and%20instructions%20to%20reproduce%20our%20results%3A%0Ahttps%3A//github.com/MarcLafon/gallop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01400v2&entry.124074799=Read"},
{"title": "Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and\n  Editing", "author": "Wenyi Mo and Tianyu Zhang and Yalong Bai and Bing Su and Ji-Rong Wen", "abstract": "  Text-guided image generation and editing using diffusion models have achieved\nremarkable advancements. Among these, tuning-free methods have gained attention\nfor their ability to perform edits without extensive model adjustments,\noffering simplicity and efficiency. However, existing tuning-free approaches\noften struggle with balancing fidelity and editing precision. Reconstruction\nerrors in DDIM Inversion are partly attributed to the cross-attention mechanism\nin U-Net, which introduces misalignments during the inversion and\nreconstruction process. To address this, we analyze reconstruction from a\nstructural perspective and propose a novel approach that replaces traditional\ncross-attention with uniform attention maps, significantly enhancing image\nreconstruction fidelity. Our method effectively minimizes distortions caused by\nvarying text conditions during noise prediction. To complement this\nimprovement, we introduce an adaptive mask-guided editing technique that\nintegrates seamlessly with our reconstruction approach, ensuring consistency\nand accuracy in editing tasks. Experimental results demonstrate that our\napproach not only excels in achieving high-fidelity image reconstruction but\nalso performs robustly in real image composition and editing scenarios. This\nstudy underscores the potential of uniform attention maps to enhance the\nfidelity and versatility of diffusion-based image processing methods. Code is\navailable at https://github.com/Mowenyii/Uniform-Attention-Maps.\n", "link": "http://arxiv.org/abs/2411.19652v1", "date": "2024-11-29", "relevancy": 2.6164, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6537}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uniform%20Attention%20Maps%3A%20Boosting%20Image%20Fidelity%20in%20Reconstruction%20and%0A%20%20Editing&body=Title%3A%20Uniform%20Attention%20Maps%3A%20Boosting%20Image%20Fidelity%20in%20Reconstruction%20and%0A%20%20Editing%0AAuthor%3A%20Wenyi%20Mo%20and%20Tianyu%20Zhang%20and%20Yalong%20Bai%20and%20Bing%20Su%20and%20Ji-Rong%20Wen%0AAbstract%3A%20%20%20Text-guided%20image%20generation%20and%20editing%20using%20diffusion%20models%20have%20achieved%0Aremarkable%20advancements.%20Among%20these%2C%20tuning-free%20methods%20have%20gained%20attention%0Afor%20their%20ability%20to%20perform%20edits%20without%20extensive%20model%20adjustments%2C%0Aoffering%20simplicity%20and%20efficiency.%20However%2C%20existing%20tuning-free%20approaches%0Aoften%20struggle%20with%20balancing%20fidelity%20and%20editing%20precision.%20Reconstruction%0Aerrors%20in%20DDIM%20Inversion%20are%20partly%20attributed%20to%20the%20cross-attention%20mechanism%0Ain%20U-Net%2C%20which%20introduces%20misalignments%20during%20the%20inversion%20and%0Areconstruction%20process.%20To%20address%20this%2C%20we%20analyze%20reconstruction%20from%20a%0Astructural%20perspective%20and%20propose%20a%20novel%20approach%20that%20replaces%20traditional%0Across-attention%20with%20uniform%20attention%20maps%2C%20significantly%20enhancing%20image%0Areconstruction%20fidelity.%20Our%20method%20effectively%20minimizes%20distortions%20caused%20by%0Avarying%20text%20conditions%20during%20noise%20prediction.%20To%20complement%20this%0Aimprovement%2C%20we%20introduce%20an%20adaptive%20mask-guided%20editing%20technique%20that%0Aintegrates%20seamlessly%20with%20our%20reconstruction%20approach%2C%20ensuring%20consistency%0Aand%20accuracy%20in%20editing%20tasks.%20Experimental%20results%20demonstrate%20that%20our%0Aapproach%20not%20only%20excels%20in%20achieving%20high-fidelity%20image%20reconstruction%20but%0Aalso%20performs%20robustly%20in%20real%20image%20composition%20and%20editing%20scenarios.%20This%0Astudy%20underscores%20the%20potential%20of%20uniform%20attention%20maps%20to%20enhance%20the%0Afidelity%20and%20versatility%20of%20diffusion-based%20image%20processing%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Mowenyii/Uniform-Attention-Maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniform%2520Attention%2520Maps%253A%2520Boosting%2520Image%2520Fidelity%2520in%2520Reconstruction%2520and%250A%2520%2520Editing%26entry.906535625%3DWenyi%2520Mo%2520and%2520Tianyu%2520Zhang%2520and%2520Yalong%2520Bai%2520and%2520Bing%2520Su%2520and%2520Ji-Rong%2520Wen%26entry.1292438233%3D%2520%2520Text-guided%2520image%2520generation%2520and%2520editing%2520using%2520diffusion%2520models%2520have%2520achieved%250Aremarkable%2520advancements.%2520Among%2520these%252C%2520tuning-free%2520methods%2520have%2520gained%2520attention%250Afor%2520their%2520ability%2520to%2520perform%2520edits%2520without%2520extensive%2520model%2520adjustments%252C%250Aoffering%2520simplicity%2520and%2520efficiency.%2520However%252C%2520existing%2520tuning-free%2520approaches%250Aoften%2520struggle%2520with%2520balancing%2520fidelity%2520and%2520editing%2520precision.%2520Reconstruction%250Aerrors%2520in%2520DDIM%2520Inversion%2520are%2520partly%2520attributed%2520to%2520the%2520cross-attention%2520mechanism%250Ain%2520U-Net%252C%2520which%2520introduces%2520misalignments%2520during%2520the%2520inversion%2520and%250Areconstruction%2520process.%2520To%2520address%2520this%252C%2520we%2520analyze%2520reconstruction%2520from%2520a%250Astructural%2520perspective%2520and%2520propose%2520a%2520novel%2520approach%2520that%2520replaces%2520traditional%250Across-attention%2520with%2520uniform%2520attention%2520maps%252C%2520significantly%2520enhancing%2520image%250Areconstruction%2520fidelity.%2520Our%2520method%2520effectively%2520minimizes%2520distortions%2520caused%2520by%250Avarying%2520text%2520conditions%2520during%2520noise%2520prediction.%2520To%2520complement%2520this%250Aimprovement%252C%2520we%2520introduce%2520an%2520adaptive%2520mask-guided%2520editing%2520technique%2520that%250Aintegrates%2520seamlessly%2520with%2520our%2520reconstruction%2520approach%252C%2520ensuring%2520consistency%250Aand%2520accuracy%2520in%2520editing%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Aapproach%2520not%2520only%2520excels%2520in%2520achieving%2520high-fidelity%2520image%2520reconstruction%2520but%250Aalso%2520performs%2520robustly%2520in%2520real%2520image%2520composition%2520and%2520editing%2520scenarios.%2520This%250Astudy%2520underscores%2520the%2520potential%2520of%2520uniform%2520attention%2520maps%2520to%2520enhance%2520the%250Afidelity%2520and%2520versatility%2520of%2520diffusion-based%2520image%2520processing%2520methods.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/Mowenyii/Uniform-Attention-Maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uniform%20Attention%20Maps%3A%20Boosting%20Image%20Fidelity%20in%20Reconstruction%20and%0A%20%20Editing&entry.906535625=Wenyi%20Mo%20and%20Tianyu%20Zhang%20and%20Yalong%20Bai%20and%20Bing%20Su%20and%20Ji-Rong%20Wen&entry.1292438233=%20%20Text-guided%20image%20generation%20and%20editing%20using%20diffusion%20models%20have%20achieved%0Aremarkable%20advancements.%20Among%20these%2C%20tuning-free%20methods%20have%20gained%20attention%0Afor%20their%20ability%20to%20perform%20edits%20without%20extensive%20model%20adjustments%2C%0Aoffering%20simplicity%20and%20efficiency.%20However%2C%20existing%20tuning-free%20approaches%0Aoften%20struggle%20with%20balancing%20fidelity%20and%20editing%20precision.%20Reconstruction%0Aerrors%20in%20DDIM%20Inversion%20are%20partly%20attributed%20to%20the%20cross-attention%20mechanism%0Ain%20U-Net%2C%20which%20introduces%20misalignments%20during%20the%20inversion%20and%0Areconstruction%20process.%20To%20address%20this%2C%20we%20analyze%20reconstruction%20from%20a%0Astructural%20perspective%20and%20propose%20a%20novel%20approach%20that%20replaces%20traditional%0Across-attention%20with%20uniform%20attention%20maps%2C%20significantly%20enhancing%20image%0Areconstruction%20fidelity.%20Our%20method%20effectively%20minimizes%20distortions%20caused%20by%0Avarying%20text%20conditions%20during%20noise%20prediction.%20To%20complement%20this%0Aimprovement%2C%20we%20introduce%20an%20adaptive%20mask-guided%20editing%20technique%20that%0Aintegrates%20seamlessly%20with%20our%20reconstruction%20approach%2C%20ensuring%20consistency%0Aand%20accuracy%20in%20editing%20tasks.%20Experimental%20results%20demonstrate%20that%20our%0Aapproach%20not%20only%20excels%20in%20achieving%20high-fidelity%20image%20reconstruction%20but%0Aalso%20performs%20robustly%20in%20real%20image%20composition%20and%20editing%20scenarios.%20This%0Astudy%20underscores%20the%20potential%20of%20uniform%20attention%20maps%20to%20enhance%20the%0Afidelity%20and%20versatility%20of%20diffusion-based%20image%20processing%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Mowenyii/Uniform-Attention-Maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19652v1&entry.124074799=Read"},
{"title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference\n  A!acks leveraging internal LLM states", "author": "Luis Ibanez-Lissen and Lorena Gonzalez-Manzano and Jose Maria de Fuentes and Nicolas Anciaux and Joaquin Garcia-Alfaro", "abstract": "  Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.\n", "link": "http://arxiv.org/abs/2411.19876v1", "date": "2024-11-29", "relevancy": 2.6139, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5265}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUMIA%3A%20Linear%20probing%20for%20Unimodal%20and%20MultiModal%20Membership%20Inference%0A%20%20A%21acks%20leveraging%20internal%20LLM%20states&body=Title%3A%20LUMIA%3A%20Linear%20probing%20for%20Unimodal%20and%20MultiModal%20Membership%20Inference%0A%20%20A%21acks%20leveraging%20internal%20LLM%20states%0AAuthor%3A%20Luis%20Ibanez-Lissen%20and%20Lorena%20Gonzalez-Manzano%20and%20Jose%20Maria%20de%20Fuentes%20and%20Nicolas%20Anciaux%20and%20Joaquin%20Garcia-Alfaro%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20a%20variety%20of%0Aapplications%2C%20but%20concerns%20around%20membership%20inference%20have%20grown%20in%20parallel.%0APrevious%20efforts%20focus%20on%20black-to-grey-box%20models%2C%20thus%20neglecting%20the%0Apotential%20benefit%20from%20internal%20LLM%20information.%20To%20address%20this%2C%20we%20propose%0Athe%20use%20of%20Linear%20Probes%20%28LPs%29%20as%20a%20method%20to%20detect%20Membership%20Inference%0AAttacks%20%28MIAs%29%20by%20examining%20internal%20activations%20of%20LLMs.%20Our%20approach%2C%20dubbed%0ALUMIA%2C%20applies%20LPs%20layer-by-layer%20to%20get%20fine-grained%20data%20on%20the%20model%20inner%0Aworkings.%20We%20test%20this%20method%20across%20several%20model%20architectures%2C%20sizes%20and%0Adatasets%2C%20including%20unimodal%20and%20multimodal%20tasks.%20In%20unimodal%20MIA%2C%20LUMIA%0Aachieves%20an%20average%20gain%20of%2015.71%20%25%20in%20Area%20Under%20the%20Curve%20%28AUC%29%20over%20previous%0Atechniques.%20Remarkably%2C%20LUMIA%20reaches%20AUC%3E60%25%20in%2065.33%25%20of%20cases%20--%20an%0Aincrement%20of%2046.80%25%20against%20the%20state%20of%20the%20art.%20Furthermore%2C%20our%20approach%0Areveals%20key%20insights%2C%20such%20as%20the%20model%20layers%20where%20MIAs%20are%20most%20detectable.%0AIn%20multimodal%20models%2C%20LPs%20indicate%20that%20visual%20inputs%20can%20significantly%0Acontribute%20to%20detect%20MIAs%20--%20AUC%3E60%25%20is%20reached%20in%2085.90%25%20of%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUMIA%253A%2520Linear%2520probing%2520for%2520Unimodal%2520and%2520MultiModal%2520Membership%2520Inference%250A%2520%2520A%2521acks%2520leveraging%2520internal%2520LLM%2520states%26entry.906535625%3DLuis%2520Ibanez-Lissen%2520and%2520Lorena%2520Gonzalez-Manzano%2520and%2520Jose%2520Maria%2520de%2520Fuentes%2520and%2520Nicolas%2520Anciaux%2520and%2520Joaquin%2520Garcia-Alfaro%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520in%2520a%2520variety%2520of%250Aapplications%252C%2520but%2520concerns%2520around%2520membership%2520inference%2520have%2520grown%2520in%2520parallel.%250APrevious%2520efforts%2520focus%2520on%2520black-to-grey-box%2520models%252C%2520thus%2520neglecting%2520the%250Apotential%2520benefit%2520from%2520internal%2520LLM%2520information.%2520To%2520address%2520this%252C%2520we%2520propose%250Athe%2520use%2520of%2520Linear%2520Probes%2520%2528LPs%2529%2520as%2520a%2520method%2520to%2520detect%2520Membership%2520Inference%250AAttacks%2520%2528MIAs%2529%2520by%2520examining%2520internal%2520activations%2520of%2520LLMs.%2520Our%2520approach%252C%2520dubbed%250ALUMIA%252C%2520applies%2520LPs%2520layer-by-layer%2520to%2520get%2520fine-grained%2520data%2520on%2520the%2520model%2520inner%250Aworkings.%2520We%2520test%2520this%2520method%2520across%2520several%2520model%2520architectures%252C%2520sizes%2520and%250Adatasets%252C%2520including%2520unimodal%2520and%2520multimodal%2520tasks.%2520In%2520unimodal%2520MIA%252C%2520LUMIA%250Aachieves%2520an%2520average%2520gain%2520of%252015.71%2520%2525%2520in%2520Area%2520Under%2520the%2520Curve%2520%2528AUC%2529%2520over%2520previous%250Atechniques.%2520Remarkably%252C%2520LUMIA%2520reaches%2520AUC%253E60%2525%2520in%252065.33%2525%2520of%2520cases%2520--%2520an%250Aincrement%2520of%252046.80%2525%2520against%2520the%2520state%2520of%2520the%2520art.%2520Furthermore%252C%2520our%2520approach%250Areveals%2520key%2520insights%252C%2520such%2520as%2520the%2520model%2520layers%2520where%2520MIAs%2520are%2520most%2520detectable.%250AIn%2520multimodal%2520models%252C%2520LPs%2520indicate%2520that%2520visual%2520inputs%2520can%2520significantly%250Acontribute%2520to%2520detect%2520MIAs%2520--%2520AUC%253E60%2525%2520is%2520reached%2520in%252085.90%2525%2520of%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUMIA%3A%20Linear%20probing%20for%20Unimodal%20and%20MultiModal%20Membership%20Inference%0A%20%20A%21acks%20leveraging%20internal%20LLM%20states&entry.906535625=Luis%20Ibanez-Lissen%20and%20Lorena%20Gonzalez-Manzano%20and%20Jose%20Maria%20de%20Fuentes%20and%20Nicolas%20Anciaux%20and%20Joaquin%20Garcia-Alfaro&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20in%20a%20variety%20of%0Aapplications%2C%20but%20concerns%20around%20membership%20inference%20have%20grown%20in%20parallel.%0APrevious%20efforts%20focus%20on%20black-to-grey-box%20models%2C%20thus%20neglecting%20the%0Apotential%20benefit%20from%20internal%20LLM%20information.%20To%20address%20this%2C%20we%20propose%0Athe%20use%20of%20Linear%20Probes%20%28LPs%29%20as%20a%20method%20to%20detect%20Membership%20Inference%0AAttacks%20%28MIAs%29%20by%20examining%20internal%20activations%20of%20LLMs.%20Our%20approach%2C%20dubbed%0ALUMIA%2C%20applies%20LPs%20layer-by-layer%20to%20get%20fine-grained%20data%20on%20the%20model%20inner%0Aworkings.%20We%20test%20this%20method%20across%20several%20model%20architectures%2C%20sizes%20and%0Adatasets%2C%20including%20unimodal%20and%20multimodal%20tasks.%20In%20unimodal%20MIA%2C%20LUMIA%0Aachieves%20an%20average%20gain%20of%2015.71%20%25%20in%20Area%20Under%20the%20Curve%20%28AUC%29%20over%20previous%0Atechniques.%20Remarkably%2C%20LUMIA%20reaches%20AUC%3E60%25%20in%2065.33%25%20of%20cases%20--%20an%0Aincrement%20of%2046.80%25%20against%20the%20state%20of%20the%20art.%20Furthermore%2C%20our%20approach%0Areveals%20key%20insights%2C%20such%20as%20the%20model%20layers%20where%20MIAs%20are%20most%20detectable.%0AIn%20multimodal%20models%2C%20LPs%20indicate%20that%20visual%20inputs%20can%20significantly%0Acontribute%20to%20detect%20MIAs%20--%20AUC%3E60%25%20is%20reached%20in%2085.90%25%20of%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19876v1&entry.124074799=Read"},
{"title": "VideoDirector: Precise Video Editing via Text-to-Video Models", "author": "Yukun Wang and Longguang Wang and Zhiyuan Ma and Qibin Hu and Kai Xu and Yulan Guo", "abstract": "  Despite the typical inversion-then-editing paradigm using text-to-image (T2I)\nmodels has demonstrated promising results, directly extending it to\ntext-to-video (T2V) models still suffers severe artifacts such as color\nflickering and content distortion. Consequently, current video editing methods\nprimarily rely on T2I models, which inherently lack temporal-coherence\ngenerative ability, often resulting in inferior editing results. In this paper,\nwe attribute the failure of the typical editing paradigm to: 1) Tightly\nSpatial-temporal Coupling. The vanilla pivotal-based inversion strategy\nstruggles to disentangle spatial-temporal information in the video diffusion\nmodel; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention\ncontrol is deficient in preserving the unedited content. To address these\nlimitations, we propose a spatial-temporal decoupled guidance (STDG) and\nmulti-frame null-text optimization strategy to provide pivotal temporal cues\nfor more precise pivotal inversion. Furthermore, we introduce a self-attention\ncontrol strategy to maintain higher fidelity for precise partial content\nediting. Experimental results demonstrate that our method (termed\nVideoDirector) effectively harnesses the powerful temporal generation\ncapabilities of T2V models, producing edited videos with state-of-the-art\nperformance in accuracy, motion smoothness, realism, and fidelity to unedited\ncontent.\n", "link": "http://arxiv.org/abs/2411.17592v2", "date": "2024-11-29", "relevancy": 2.6125, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7081}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6676}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoDirector%3A%20Precise%20Video%20Editing%20via%20Text-to-Video%20Models&body=Title%3A%20VideoDirector%3A%20Precise%20Video%20Editing%20via%20Text-to-Video%20Models%0AAuthor%3A%20Yukun%20Wang%20and%20Longguang%20Wang%20and%20Zhiyuan%20Ma%20and%20Qibin%20Hu%20and%20Kai%20Xu%20and%20Yulan%20Guo%0AAbstract%3A%20%20%20Despite%20the%20typical%20inversion-then-editing%20paradigm%20using%20text-to-image%20%28T2I%29%0Amodels%20has%20demonstrated%20promising%20results%2C%20directly%20extending%20it%20to%0Atext-to-video%20%28T2V%29%20models%20still%20suffers%20severe%20artifacts%20such%20as%20color%0Aflickering%20and%20content%20distortion.%20Consequently%2C%20current%20video%20editing%20methods%0Aprimarily%20rely%20on%20T2I%20models%2C%20which%20inherently%20lack%20temporal-coherence%0Agenerative%20ability%2C%20often%20resulting%20in%20inferior%20editing%20results.%20In%20this%20paper%2C%0Awe%20attribute%20the%20failure%20of%20the%20typical%20editing%20paradigm%20to%3A%201%29%20Tightly%0ASpatial-temporal%20Coupling.%20The%20vanilla%20pivotal-based%20inversion%20strategy%0Astruggles%20to%20disentangle%20spatial-temporal%20information%20in%20the%20video%20diffusion%0Amodel%3B%202%29%20Complicated%20Spatial-temporal%20Layout.%20The%20vanilla%20cross-attention%0Acontrol%20is%20deficient%20in%20preserving%20the%20unedited%20content.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20spatial-temporal%20decoupled%20guidance%20%28STDG%29%20and%0Amulti-frame%20null-text%20optimization%20strategy%20to%20provide%20pivotal%20temporal%20cues%0Afor%20more%20precise%20pivotal%20inversion.%20Furthermore%2C%20we%20introduce%20a%20self-attention%0Acontrol%20strategy%20to%20maintain%20higher%20fidelity%20for%20precise%20partial%20content%0Aediting.%20Experimental%20results%20demonstrate%20that%20our%20method%20%28termed%0AVideoDirector%29%20effectively%20harnesses%20the%20powerful%20temporal%20generation%0Acapabilities%20of%20T2V%20models%2C%20producing%20edited%20videos%20with%20state-of-the-art%0Aperformance%20in%20accuracy%2C%20motion%20smoothness%2C%20realism%2C%20and%20fidelity%20to%20unedited%0Acontent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoDirector%253A%2520Precise%2520Video%2520Editing%2520via%2520Text-to-Video%2520Models%26entry.906535625%3DYukun%2520Wang%2520and%2520Longguang%2520Wang%2520and%2520Zhiyuan%2520Ma%2520and%2520Qibin%2520Hu%2520and%2520Kai%2520Xu%2520and%2520Yulan%2520Guo%26entry.1292438233%3D%2520%2520Despite%2520the%2520typical%2520inversion-then-editing%2520paradigm%2520using%2520text-to-image%2520%2528T2I%2529%250Amodels%2520has%2520demonstrated%2520promising%2520results%252C%2520directly%2520extending%2520it%2520to%250Atext-to-video%2520%2528T2V%2529%2520models%2520still%2520suffers%2520severe%2520artifacts%2520such%2520as%2520color%250Aflickering%2520and%2520content%2520distortion.%2520Consequently%252C%2520current%2520video%2520editing%2520methods%250Aprimarily%2520rely%2520on%2520T2I%2520models%252C%2520which%2520inherently%2520lack%2520temporal-coherence%250Agenerative%2520ability%252C%2520often%2520resulting%2520in%2520inferior%2520editing%2520results.%2520In%2520this%2520paper%252C%250Awe%2520attribute%2520the%2520failure%2520of%2520the%2520typical%2520editing%2520paradigm%2520to%253A%25201%2529%2520Tightly%250ASpatial-temporal%2520Coupling.%2520The%2520vanilla%2520pivotal-based%2520inversion%2520strategy%250Astruggles%2520to%2520disentangle%2520spatial-temporal%2520information%2520in%2520the%2520video%2520diffusion%250Amodel%253B%25202%2529%2520Complicated%2520Spatial-temporal%2520Layout.%2520The%2520vanilla%2520cross-attention%250Acontrol%2520is%2520deficient%2520in%2520preserving%2520the%2520unedited%2520content.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520spatial-temporal%2520decoupled%2520guidance%2520%2528STDG%2529%2520and%250Amulti-frame%2520null-text%2520optimization%2520strategy%2520to%2520provide%2520pivotal%2520temporal%2520cues%250Afor%2520more%2520precise%2520pivotal%2520inversion.%2520Furthermore%252C%2520we%2520introduce%2520a%2520self-attention%250Acontrol%2520strategy%2520to%2520maintain%2520higher%2520fidelity%2520for%2520precise%2520partial%2520content%250Aediting.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520%2528termed%250AVideoDirector%2529%2520effectively%2520harnesses%2520the%2520powerful%2520temporal%2520generation%250Acapabilities%2520of%2520T2V%2520models%252C%2520producing%2520edited%2520videos%2520with%2520state-of-the-art%250Aperformance%2520in%2520accuracy%252C%2520motion%2520smoothness%252C%2520realism%252C%2520and%2520fidelity%2520to%2520unedited%250Acontent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoDirector%3A%20Precise%20Video%20Editing%20via%20Text-to-Video%20Models&entry.906535625=Yukun%20Wang%20and%20Longguang%20Wang%20and%20Zhiyuan%20Ma%20and%20Qibin%20Hu%20and%20Kai%20Xu%20and%20Yulan%20Guo&entry.1292438233=%20%20Despite%20the%20typical%20inversion-then-editing%20paradigm%20using%20text-to-image%20%28T2I%29%0Amodels%20has%20demonstrated%20promising%20results%2C%20directly%20extending%20it%20to%0Atext-to-video%20%28T2V%29%20models%20still%20suffers%20severe%20artifacts%20such%20as%20color%0Aflickering%20and%20content%20distortion.%20Consequently%2C%20current%20video%20editing%20methods%0Aprimarily%20rely%20on%20T2I%20models%2C%20which%20inherently%20lack%20temporal-coherence%0Agenerative%20ability%2C%20often%20resulting%20in%20inferior%20editing%20results.%20In%20this%20paper%2C%0Awe%20attribute%20the%20failure%20of%20the%20typical%20editing%20paradigm%20to%3A%201%29%20Tightly%0ASpatial-temporal%20Coupling.%20The%20vanilla%20pivotal-based%20inversion%20strategy%0Astruggles%20to%20disentangle%20spatial-temporal%20information%20in%20the%20video%20diffusion%0Amodel%3B%202%29%20Complicated%20Spatial-temporal%20Layout.%20The%20vanilla%20cross-attention%0Acontrol%20is%20deficient%20in%20preserving%20the%20unedited%20content.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20spatial-temporal%20decoupled%20guidance%20%28STDG%29%20and%0Amulti-frame%20null-text%20optimization%20strategy%20to%20provide%20pivotal%20temporal%20cues%0Afor%20more%20precise%20pivotal%20inversion.%20Furthermore%2C%20we%20introduce%20a%20self-attention%0Acontrol%20strategy%20to%20maintain%20higher%20fidelity%20for%20precise%20partial%20content%0Aediting.%20Experimental%20results%20demonstrate%20that%20our%20method%20%28termed%0AVideoDirector%29%20effectively%20harnesses%20the%20powerful%20temporal%20generation%0Acapabilities%20of%20T2V%20models%2C%20producing%20edited%20videos%20with%20state-of-the-art%0Aperformance%20in%20accuracy%2C%20motion%20smoothness%2C%20realism%2C%20and%20fidelity%20to%20unedited%0Acontent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17592v2&entry.124074799=Read"},
{"title": "Beyond adaptive gradient: Fast-Controlled Minibatch Algorithm for\n  large-scale optimization", "author": "Corrado Coppola and Lorenzo Papa and Irene Amerini and Laura Palagi", "abstract": "  Adaptive gradient methods have been increasingly adopted by deep learning\ncommunity due to their fast convergence and reduced sensitivity to\nhyper-parameters. However, these methods come with limitations, such as\nincreased memory requirements for elements like moving averages and a poorly\nunderstood convergence theory. To overcome these challenges, we introduce\nF-CMA, a Fast-Controlled Mini-batch Algorithm with a random reshuffling method\nfeaturing a sufficient decrease condition and a line-search procedure to ensure\nloss reduction per epoch, along with its deterministic proof of global\nconvergence to a stationary point. To evaluate the F-CMA, we integrate it into\nconventional training protocols for classification tasks involving both\nconvolutional neural networks and vision transformer models, allowing for a\ndirect comparison with popular optimizers. Computational tests show significant\nimprovements, including a decrease in the overall training time by up to 68%,\nan increase in per-epoch efficiency by up to 20%, and in model accuracy by up\nto 5%.\n", "link": "http://arxiv.org/abs/2411.15795v2", "date": "2024-11-29", "relevancy": 2.5968, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20adaptive%20gradient%3A%20Fast-Controlled%20Minibatch%20Algorithm%20for%0A%20%20large-scale%20optimization&body=Title%3A%20Beyond%20adaptive%20gradient%3A%20Fast-Controlled%20Minibatch%20Algorithm%20for%0A%20%20large-scale%20optimization%0AAuthor%3A%20Corrado%20Coppola%20and%20Lorenzo%20Papa%20and%20Irene%20Amerini%20and%20Laura%20Palagi%0AAbstract%3A%20%20%20Adaptive%20gradient%20methods%20have%20been%20increasingly%20adopted%20by%20deep%20learning%0Acommunity%20due%20to%20their%20fast%20convergence%20and%20reduced%20sensitivity%20to%0Ahyper-parameters.%20However%2C%20these%20methods%20come%20with%20limitations%2C%20such%20as%0Aincreased%20memory%20requirements%20for%20elements%20like%20moving%20averages%20and%20a%20poorly%0Aunderstood%20convergence%20theory.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0AF-CMA%2C%20a%20Fast-Controlled%20Mini-batch%20Algorithm%20with%20a%20random%20reshuffling%20method%0Afeaturing%20a%20sufficient%20decrease%20condition%20and%20a%20line-search%20procedure%20to%20ensure%0Aloss%20reduction%20per%20epoch%2C%20along%20with%20its%20deterministic%20proof%20of%20global%0Aconvergence%20to%20a%20stationary%20point.%20To%20evaluate%20the%20F-CMA%2C%20we%20integrate%20it%20into%0Aconventional%20training%20protocols%20for%20classification%20tasks%20involving%20both%0Aconvolutional%20neural%20networks%20and%20vision%20transformer%20models%2C%20allowing%20for%20a%0Adirect%20comparison%20with%20popular%20optimizers.%20Computational%20tests%20show%20significant%0Aimprovements%2C%20including%20a%20decrease%20in%20the%20overall%20training%20time%20by%20up%20to%2068%25%2C%0Aan%20increase%20in%20per-epoch%20efficiency%20by%20up%20to%2020%25%2C%20and%20in%20model%20accuracy%20by%20up%0Ato%205%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520adaptive%2520gradient%253A%2520Fast-Controlled%2520Minibatch%2520Algorithm%2520for%250A%2520%2520large-scale%2520optimization%26entry.906535625%3DCorrado%2520Coppola%2520and%2520Lorenzo%2520Papa%2520and%2520Irene%2520Amerini%2520and%2520Laura%2520Palagi%26entry.1292438233%3D%2520%2520Adaptive%2520gradient%2520methods%2520have%2520been%2520increasingly%2520adopted%2520by%2520deep%2520learning%250Acommunity%2520due%2520to%2520their%2520fast%2520convergence%2520and%2520reduced%2520sensitivity%2520to%250Ahyper-parameters.%2520However%252C%2520these%2520methods%2520come%2520with%2520limitations%252C%2520such%2520as%250Aincreased%2520memory%2520requirements%2520for%2520elements%2520like%2520moving%2520averages%2520and%2520a%2520poorly%250Aunderstood%2520convergence%2520theory.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%250AF-CMA%252C%2520a%2520Fast-Controlled%2520Mini-batch%2520Algorithm%2520with%2520a%2520random%2520reshuffling%2520method%250Afeaturing%2520a%2520sufficient%2520decrease%2520condition%2520and%2520a%2520line-search%2520procedure%2520to%2520ensure%250Aloss%2520reduction%2520per%2520epoch%252C%2520along%2520with%2520its%2520deterministic%2520proof%2520of%2520global%250Aconvergence%2520to%2520a%2520stationary%2520point.%2520To%2520evaluate%2520the%2520F-CMA%252C%2520we%2520integrate%2520it%2520into%250Aconventional%2520training%2520protocols%2520for%2520classification%2520tasks%2520involving%2520both%250Aconvolutional%2520neural%2520networks%2520and%2520vision%2520transformer%2520models%252C%2520allowing%2520for%2520a%250Adirect%2520comparison%2520with%2520popular%2520optimizers.%2520Computational%2520tests%2520show%2520significant%250Aimprovements%252C%2520including%2520a%2520decrease%2520in%2520the%2520overall%2520training%2520time%2520by%2520up%2520to%252068%2525%252C%250Aan%2520increase%2520in%2520per-epoch%2520efficiency%2520by%2520up%2520to%252020%2525%252C%2520and%2520in%2520model%2520accuracy%2520by%2520up%250Ato%25205%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20adaptive%20gradient%3A%20Fast-Controlled%20Minibatch%20Algorithm%20for%0A%20%20large-scale%20optimization&entry.906535625=Corrado%20Coppola%20and%20Lorenzo%20Papa%20and%20Irene%20Amerini%20and%20Laura%20Palagi&entry.1292438233=%20%20Adaptive%20gradient%20methods%20have%20been%20increasingly%20adopted%20by%20deep%20learning%0Acommunity%20due%20to%20their%20fast%20convergence%20and%20reduced%20sensitivity%20to%0Ahyper-parameters.%20However%2C%20these%20methods%20come%20with%20limitations%2C%20such%20as%0Aincreased%20memory%20requirements%20for%20elements%20like%20moving%20averages%20and%20a%20poorly%0Aunderstood%20convergence%20theory.%20To%20overcome%20these%20challenges%2C%20we%20introduce%0AF-CMA%2C%20a%20Fast-Controlled%20Mini-batch%20Algorithm%20with%20a%20random%20reshuffling%20method%0Afeaturing%20a%20sufficient%20decrease%20condition%20and%20a%20line-search%20procedure%20to%20ensure%0Aloss%20reduction%20per%20epoch%2C%20along%20with%20its%20deterministic%20proof%20of%20global%0Aconvergence%20to%20a%20stationary%20point.%20To%20evaluate%20the%20F-CMA%2C%20we%20integrate%20it%20into%0Aconventional%20training%20protocols%20for%20classification%20tasks%20involving%20both%0Aconvolutional%20neural%20networks%20and%20vision%20transformer%20models%2C%20allowing%20for%20a%0Adirect%20comparison%20with%20popular%20optimizers.%20Computational%20tests%20show%20significant%0Aimprovements%2C%20including%20a%20decrease%20in%20the%20overall%20training%20time%20by%20up%20to%2068%25%2C%0Aan%20increase%20in%20per-epoch%20efficiency%20by%20up%20to%2020%25%2C%20and%20in%20model%20accuracy%20by%20up%0Ato%205%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15795v2&entry.124074799=Read"},
{"title": "T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs", "author": "Shukang Yin and Chaoyou Fu and Sirui Zhao and Yunhang Shen and Chunjiang Ge and Yan Yang and Zuwei Long and Yuhan Dai and Tong Xu and Xing Sun and Ran He and Caifeng Shan and Enhong Chen", "abstract": "  The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.\n", "link": "http://arxiv.org/abs/2411.19951v1", "date": "2024-11-29", "relevancy": 2.5912, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6488}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6477}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2Vid%3A%20Translating%20Long%20Text%20into%20Multi-Image%20is%20the%20Catalyst%20for%0A%20%20Video-LLMs&body=Title%3A%20T2Vid%3A%20Translating%20Long%20Text%20into%20Multi-Image%20is%20the%20Catalyst%20for%0A%20%20Video-LLMs%0AAuthor%3A%20Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Yunhang%20Shen%20and%20Chunjiang%20Ge%20and%20Yan%20Yang%20and%20Zuwei%20Long%20and%20Yuhan%20Dai%20and%20Tong%20Xu%20and%20Xing%20Sun%20and%20Ran%20He%20and%20Caifeng%20Shan%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20The%20success%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20in%20the%20image%20domain%0Ahas%20garnered%20wide%20attention%20from%20the%20research%20community.%20Drawing%20on%20previous%0Asuccessful%20experiences%2C%20researchers%20have%20recently%20explored%20extending%20the%0Asuccess%20to%20the%20video%20understanding%20realms.%20Apart%20from%20training%20from%20scratch%2C%20an%0Aefficient%20way%20is%20to%20utilize%20the%20pre-trained%20image-LLMs%2C%20leading%20to%20two%0Amainstream%20approaches%2C%20i.e.%20zero-shot%20inference%20and%20further%20fine-tuning%20with%0Avideo%20data.%20In%20this%20work%2C%20our%20study%20of%20these%20approaches%20harvests%20an%20effective%0Adata%20augmentation%20method.%20We%20first%20make%20a%20deeper%20inspection%20of%20the%20zero-shot%0Ainference%20way%20and%20identify%20two%20limitations%2C%20i.e.%20limited%20generalization%20and%0Alack%20of%20temporal%20understanding%20capabilities.%20Thus%2C%20we%20further%20investigate%20the%0Afine-tuning%20approach%20and%20find%20a%20low%20learning%20efficiency%20when%20simply%20using%20all%0Athe%20video%20data%20samples%2C%20which%20can%20be%20attributed%20to%20a%20lack%20of%20instruction%0Adiversity.%20Aiming%20at%20this%20issue%2C%20we%20develop%20a%20method%20called%20T2Vid%20to%20synthesize%0Avideo-like%20samples%20to%20enrich%20the%20instruction%20diversity%20in%20the%20training%20corpus.%0AIntegrating%20these%20data%20enables%20a%20simple%20and%20efficient%20training%20scheme%2C%20which%0Aachieves%20performance%20comparable%20to%20or%20even%20superior%20to%20using%20full%20video%0Adatasets%20by%20training%20with%20just%2015%25%20the%20sample%20size.%20Meanwhile%2C%20we%20find%20that%20the%0Aproposed%20scheme%20can%20boost%20the%20performance%20of%20long%20video%20understanding%20without%0Atraining%20with%20long%20video%20samples.%20We%20hope%20our%20study%20will%20spark%20more%20thinking%0Aabout%20using%20MLLMs%20for%20video%20understanding%20and%20curation%20of%20high-quality%20data.%0AThe%20code%20is%20released%20at%20https%3A//github.com/xjtupanda/T2Vid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2Vid%253A%2520Translating%2520Long%2520Text%2520into%2520Multi-Image%2520is%2520the%2520Catalyst%2520for%250A%2520%2520Video-LLMs%26entry.906535625%3DShukang%2520Yin%2520and%2520Chaoyou%2520Fu%2520and%2520Sirui%2520Zhao%2520and%2520Yunhang%2520Shen%2520and%2520Chunjiang%2520Ge%2520and%2520Yan%2520Yang%2520and%2520Zuwei%2520Long%2520and%2520Yuhan%2520Dai%2520and%2520Tong%2520Xu%2520and%2520Xing%2520Sun%2520and%2520Ran%2520He%2520and%2520Caifeng%2520Shan%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520in%2520the%2520image%2520domain%250Ahas%2520garnered%2520wide%2520attention%2520from%2520the%2520research%2520community.%2520Drawing%2520on%2520previous%250Asuccessful%2520experiences%252C%2520researchers%2520have%2520recently%2520explored%2520extending%2520the%250Asuccess%2520to%2520the%2520video%2520understanding%2520realms.%2520Apart%2520from%2520training%2520from%2520scratch%252C%2520an%250Aefficient%2520way%2520is%2520to%2520utilize%2520the%2520pre-trained%2520image-LLMs%252C%2520leading%2520to%2520two%250Amainstream%2520approaches%252C%2520i.e.%2520zero-shot%2520inference%2520and%2520further%2520fine-tuning%2520with%250Avideo%2520data.%2520In%2520this%2520work%252C%2520our%2520study%2520of%2520these%2520approaches%2520harvests%2520an%2520effective%250Adata%2520augmentation%2520method.%2520We%2520first%2520make%2520a%2520deeper%2520inspection%2520of%2520the%2520zero-shot%250Ainference%2520way%2520and%2520identify%2520two%2520limitations%252C%2520i.e.%2520limited%2520generalization%2520and%250Alack%2520of%2520temporal%2520understanding%2520capabilities.%2520Thus%252C%2520we%2520further%2520investigate%2520the%250Afine-tuning%2520approach%2520and%2520find%2520a%2520low%2520learning%2520efficiency%2520when%2520simply%2520using%2520all%250Athe%2520video%2520data%2520samples%252C%2520which%2520can%2520be%2520attributed%2520to%2520a%2520lack%2520of%2520instruction%250Adiversity.%2520Aiming%2520at%2520this%2520issue%252C%2520we%2520develop%2520a%2520method%2520called%2520T2Vid%2520to%2520synthesize%250Avideo-like%2520samples%2520to%2520enrich%2520the%2520instruction%2520diversity%2520in%2520the%2520training%2520corpus.%250AIntegrating%2520these%2520data%2520enables%2520a%2520simple%2520and%2520efficient%2520training%2520scheme%252C%2520which%250Aachieves%2520performance%2520comparable%2520to%2520or%2520even%2520superior%2520to%2520using%2520full%2520video%250Adatasets%2520by%2520training%2520with%2520just%252015%2525%2520the%2520sample%2520size.%2520Meanwhile%252C%2520we%2520find%2520that%2520the%250Aproposed%2520scheme%2520can%2520boost%2520the%2520performance%2520of%2520long%2520video%2520understanding%2520without%250Atraining%2520with%2520long%2520video%2520samples.%2520We%2520hope%2520our%2520study%2520will%2520spark%2520more%2520thinking%250Aabout%2520using%2520MLLMs%2520for%2520video%2520understanding%2520and%2520curation%2520of%2520high-quality%2520data.%250AThe%2520code%2520is%2520released%2520at%2520https%253A//github.com/xjtupanda/T2Vid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2Vid%3A%20Translating%20Long%20Text%20into%20Multi-Image%20is%20the%20Catalyst%20for%0A%20%20Video-LLMs&entry.906535625=Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Yunhang%20Shen%20and%20Chunjiang%20Ge%20and%20Yan%20Yang%20and%20Zuwei%20Long%20and%20Yuhan%20Dai%20and%20Tong%20Xu%20and%20Xing%20Sun%20and%20Ran%20He%20and%20Caifeng%20Shan%20and%20Enhong%20Chen&entry.1292438233=%20%20The%20success%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20in%20the%20image%20domain%0Ahas%20garnered%20wide%20attention%20from%20the%20research%20community.%20Drawing%20on%20previous%0Asuccessful%20experiences%2C%20researchers%20have%20recently%20explored%20extending%20the%0Asuccess%20to%20the%20video%20understanding%20realms.%20Apart%20from%20training%20from%20scratch%2C%20an%0Aefficient%20way%20is%20to%20utilize%20the%20pre-trained%20image-LLMs%2C%20leading%20to%20two%0Amainstream%20approaches%2C%20i.e.%20zero-shot%20inference%20and%20further%20fine-tuning%20with%0Avideo%20data.%20In%20this%20work%2C%20our%20study%20of%20these%20approaches%20harvests%20an%20effective%0Adata%20augmentation%20method.%20We%20first%20make%20a%20deeper%20inspection%20of%20the%20zero-shot%0Ainference%20way%20and%20identify%20two%20limitations%2C%20i.e.%20limited%20generalization%20and%0Alack%20of%20temporal%20understanding%20capabilities.%20Thus%2C%20we%20further%20investigate%20the%0Afine-tuning%20approach%20and%20find%20a%20low%20learning%20efficiency%20when%20simply%20using%20all%0Athe%20video%20data%20samples%2C%20which%20can%20be%20attributed%20to%20a%20lack%20of%20instruction%0Adiversity.%20Aiming%20at%20this%20issue%2C%20we%20develop%20a%20method%20called%20T2Vid%20to%20synthesize%0Avideo-like%20samples%20to%20enrich%20the%20instruction%20diversity%20in%20the%20training%20corpus.%0AIntegrating%20these%20data%20enables%20a%20simple%20and%20efficient%20training%20scheme%2C%20which%0Aachieves%20performance%20comparable%20to%20or%20even%20superior%20to%20using%20full%20video%0Adatasets%20by%20training%20with%20just%2015%25%20the%20sample%20size.%20Meanwhile%2C%20we%20find%20that%20the%0Aproposed%20scheme%20can%20boost%20the%20performance%20of%20long%20video%20understanding%20without%0Atraining%20with%20long%20video%20samples.%20We%20hope%20our%20study%20will%20spark%20more%20thinking%0Aabout%20using%20MLLMs%20for%20video%20understanding%20and%20curation%20of%20high-quality%20data.%0AThe%20code%20is%20released%20at%20https%3A//github.com/xjtupanda/T2Vid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19951v1&entry.124074799=Read"},
{"title": "Free-form Generation Enhances Challenging Clothed Human Modeling", "author": "Hang Ye and Xiaoxuan Ma and Hai Ci and Wentao Zhu and Yizhou Wang", "abstract": "  Achieving realistic animated human avatars requires accurate modeling of\npose-dependent clothing deformations. Existing learning-based methods heavily\nrely on the Linear Blend Skinning (LBS) of minimally-clothed human models like\nSMPL to model deformation. However, these methods struggle to handle loose\nclothing, such as long dresses, where the canonicalization process becomes\nill-defined when the clothing is far from the body, leading to disjointed and\nfragmented results. To overcome this limitation, we propose a novel hybrid\nframework to model challenging clothed humans. Our core idea is to use\ndedicated strategies to model different regions, depending on whether they are\nclose to or distant from the body. Specifically, we segment the human body into\nthree categories: unclothed, deformed, and generated. We simply replicate\nunclothed regions that require no deformation. For deformed regions close to\nthe body, we leverage LBS to handle the deformation. As for the generated\nregions, which correspond to loose clothing areas, we introduce a novel\nfree-form, part-aware generator to model them, as they are less affected by\nmovements. This free-form generation paradigm brings enhanced flexibility and\nexpressiveness to our hybrid framework, enabling it to capture the intricate\ngeometric details of challenging loose clothing, such as skirts and dresses.\nExperimental results on the benchmark dataset featuring loose clothing\ndemonstrate that our method achieves state-of-the-art performance with superior\nvisual fidelity and realism, particularly in the most challenging cases.\n", "link": "http://arxiv.org/abs/2411.19942v1", "date": "2024-11-29", "relevancy": 2.5535, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6509}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6335}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free-form%20Generation%20Enhances%20Challenging%20Clothed%20Human%20Modeling&body=Title%3A%20Free-form%20Generation%20Enhances%20Challenging%20Clothed%20Human%20Modeling%0AAuthor%3A%20Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Hai%20Ci%20and%20Wentao%20Zhu%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20Achieving%20realistic%20animated%20human%20avatars%20requires%20accurate%20modeling%20of%0Apose-dependent%20clothing%20deformations.%20Existing%20learning-based%20methods%20heavily%0Arely%20on%20the%20Linear%20Blend%20Skinning%20%28LBS%29%20of%20minimally-clothed%20human%20models%20like%0ASMPL%20to%20model%20deformation.%20However%2C%20these%20methods%20struggle%20to%20handle%20loose%0Aclothing%2C%20such%20as%20long%20dresses%2C%20where%20the%20canonicalization%20process%20becomes%0Aill-defined%20when%20the%20clothing%20is%20far%20from%20the%20body%2C%20leading%20to%20disjointed%20and%0Afragmented%20results.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20hybrid%0Aframework%20to%20model%20challenging%20clothed%20humans.%20Our%20core%20idea%20is%20to%20use%0Adedicated%20strategies%20to%20model%20different%20regions%2C%20depending%20on%20whether%20they%20are%0Aclose%20to%20or%20distant%20from%20the%20body.%20Specifically%2C%20we%20segment%20the%20human%20body%20into%0Athree%20categories%3A%20unclothed%2C%20deformed%2C%20and%20generated.%20We%20simply%20replicate%0Aunclothed%20regions%20that%20require%20no%20deformation.%20For%20deformed%20regions%20close%20to%0Athe%20body%2C%20we%20leverage%20LBS%20to%20handle%20the%20deformation.%20As%20for%20the%20generated%0Aregions%2C%20which%20correspond%20to%20loose%20clothing%20areas%2C%20we%20introduce%20a%20novel%0Afree-form%2C%20part-aware%20generator%20to%20model%20them%2C%20as%20they%20are%20less%20affected%20by%0Amovements.%20This%20free-form%20generation%20paradigm%20brings%20enhanced%20flexibility%20and%0Aexpressiveness%20to%20our%20hybrid%20framework%2C%20enabling%20it%20to%20capture%20the%20intricate%0Ageometric%20details%20of%20challenging%20loose%20clothing%2C%20such%20as%20skirts%20and%20dresses.%0AExperimental%20results%20on%20the%20benchmark%20dataset%20featuring%20loose%20clothing%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20with%20superior%0Avisual%20fidelity%20and%20realism%2C%20particularly%20in%20the%20most%20challenging%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree-form%2520Generation%2520Enhances%2520Challenging%2520Clothed%2520Human%2520Modeling%26entry.906535625%3DHang%2520Ye%2520and%2520Xiaoxuan%2520Ma%2520and%2520Hai%2520Ci%2520and%2520Wentao%2520Zhu%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520Achieving%2520realistic%2520animated%2520human%2520avatars%2520requires%2520accurate%2520modeling%2520of%250Apose-dependent%2520clothing%2520deformations.%2520Existing%2520learning-based%2520methods%2520heavily%250Arely%2520on%2520the%2520Linear%2520Blend%2520Skinning%2520%2528LBS%2529%2520of%2520minimally-clothed%2520human%2520models%2520like%250ASMPL%2520to%2520model%2520deformation.%2520However%252C%2520these%2520methods%2520struggle%2520to%2520handle%2520loose%250Aclothing%252C%2520such%2520as%2520long%2520dresses%252C%2520where%2520the%2520canonicalization%2520process%2520becomes%250Aill-defined%2520when%2520the%2520clothing%2520is%2520far%2520from%2520the%2520body%252C%2520leading%2520to%2520disjointed%2520and%250Afragmented%2520results.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520hybrid%250Aframework%2520to%2520model%2520challenging%2520clothed%2520humans.%2520Our%2520core%2520idea%2520is%2520to%2520use%250Adedicated%2520strategies%2520to%2520model%2520different%2520regions%252C%2520depending%2520on%2520whether%2520they%2520are%250Aclose%2520to%2520or%2520distant%2520from%2520the%2520body.%2520Specifically%252C%2520we%2520segment%2520the%2520human%2520body%2520into%250Athree%2520categories%253A%2520unclothed%252C%2520deformed%252C%2520and%2520generated.%2520We%2520simply%2520replicate%250Aunclothed%2520regions%2520that%2520require%2520no%2520deformation.%2520For%2520deformed%2520regions%2520close%2520to%250Athe%2520body%252C%2520we%2520leverage%2520LBS%2520to%2520handle%2520the%2520deformation.%2520As%2520for%2520the%2520generated%250Aregions%252C%2520which%2520correspond%2520to%2520loose%2520clothing%2520areas%252C%2520we%2520introduce%2520a%2520novel%250Afree-form%252C%2520part-aware%2520generator%2520to%2520model%2520them%252C%2520as%2520they%2520are%2520less%2520affected%2520by%250Amovements.%2520This%2520free-form%2520generation%2520paradigm%2520brings%2520enhanced%2520flexibility%2520and%250Aexpressiveness%2520to%2520our%2520hybrid%2520framework%252C%2520enabling%2520it%2520to%2520capture%2520the%2520intricate%250Ageometric%2520details%2520of%2520challenging%2520loose%2520clothing%252C%2520such%2520as%2520skirts%2520and%2520dresses.%250AExperimental%2520results%2520on%2520the%2520benchmark%2520dataset%2520featuring%2520loose%2520clothing%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520with%2520superior%250Avisual%2520fidelity%2520and%2520realism%252C%2520particularly%2520in%2520the%2520most%2520challenging%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free-form%20Generation%20Enhances%20Challenging%20Clothed%20Human%20Modeling&entry.906535625=Hang%20Ye%20and%20Xiaoxuan%20Ma%20and%20Hai%20Ci%20and%20Wentao%20Zhu%20and%20Yizhou%20Wang&entry.1292438233=%20%20Achieving%20realistic%20animated%20human%20avatars%20requires%20accurate%20modeling%20of%0Apose-dependent%20clothing%20deformations.%20Existing%20learning-based%20methods%20heavily%0Arely%20on%20the%20Linear%20Blend%20Skinning%20%28LBS%29%20of%20minimally-clothed%20human%20models%20like%0ASMPL%20to%20model%20deformation.%20However%2C%20these%20methods%20struggle%20to%20handle%20loose%0Aclothing%2C%20such%20as%20long%20dresses%2C%20where%20the%20canonicalization%20process%20becomes%0Aill-defined%20when%20the%20clothing%20is%20far%20from%20the%20body%2C%20leading%20to%20disjointed%20and%0Afragmented%20results.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20hybrid%0Aframework%20to%20model%20challenging%20clothed%20humans.%20Our%20core%20idea%20is%20to%20use%0Adedicated%20strategies%20to%20model%20different%20regions%2C%20depending%20on%20whether%20they%20are%0Aclose%20to%20or%20distant%20from%20the%20body.%20Specifically%2C%20we%20segment%20the%20human%20body%20into%0Athree%20categories%3A%20unclothed%2C%20deformed%2C%20and%20generated.%20We%20simply%20replicate%0Aunclothed%20regions%20that%20require%20no%20deformation.%20For%20deformed%20regions%20close%20to%0Athe%20body%2C%20we%20leverage%20LBS%20to%20handle%20the%20deformation.%20As%20for%20the%20generated%0Aregions%2C%20which%20correspond%20to%20loose%20clothing%20areas%2C%20we%20introduce%20a%20novel%0Afree-form%2C%20part-aware%20generator%20to%20model%20them%2C%20as%20they%20are%20less%20affected%20by%0Amovements.%20This%20free-form%20generation%20paradigm%20brings%20enhanced%20flexibility%20and%0Aexpressiveness%20to%20our%20hybrid%20framework%2C%20enabling%20it%20to%20capture%20the%20intricate%0Ageometric%20details%20of%20challenging%20loose%20clothing%2C%20such%20as%20skirts%20and%20dresses.%0AExperimental%20results%20on%20the%20benchmark%20dataset%20featuring%20loose%20clothing%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20with%20superior%0Avisual%20fidelity%20and%20realism%2C%20particularly%20in%20the%20most%20challenging%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19942v1&entry.124074799=Read"},
{"title": "Convergence Analysis for Deep Sparse Coding via Convolutional Neural\n  Networks", "author": "Jianfei Li and Han Feng and Ding-Xuan Zhou", "abstract": "  In this work, we explore intersections between sparse coding and deep\nlearning to enhance our understanding of feature extraction capabilities in\nadvanced neural network architectures. We begin by introducing a novel class of\nDeep Sparse Coding (DSC) models and establish thorough theoretical analysis of\ntheir uniqueness and stability properties. By applying iterative algorithms to\nthese DSC models, we derive convergence rates for convolutional neural networks\n(CNNs) in their ability to extract sparse features. This provides a strong\ntheoretical foundation for the use of CNNs in sparse feature learning tasks. We\nadditionally extend the convergence analysis to more general neural network\narchitectures, including those with diverse activation functions, as well as\nself-attention and transformer-based models. This broadens the applicability of\nour findings to a wide range of deep learning methods for deep sparse feature\nextraction. Inspired by the strong connection between sparse coding and CNNs,\nwe also explore training strategies to encourage neural networks to learn more\nsparse features. Through numerical experiments, we demonstrate the\neffectiveness of these approaches, providing valuable insights for the design\nof efficient and interpretable deep learning models.\n", "link": "http://arxiv.org/abs/2408.05540v2", "date": "2024-11-29", "relevancy": 2.541, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergence%20Analysis%20for%20Deep%20Sparse%20Coding%20via%20Convolutional%20Neural%0A%20%20Networks&body=Title%3A%20Convergence%20Analysis%20for%20Deep%20Sparse%20Coding%20via%20Convolutional%20Neural%0A%20%20Networks%0AAuthor%3A%20Jianfei%20Li%20and%20Han%20Feng%20and%20Ding-Xuan%20Zhou%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20explore%20intersections%20between%20sparse%20coding%20and%20deep%0Alearning%20to%20enhance%20our%20understanding%20of%20feature%20extraction%20capabilities%20in%0Aadvanced%20neural%20network%20architectures.%20We%20begin%20by%20introducing%20a%20novel%20class%20of%0ADeep%20Sparse%20Coding%20%28DSC%29%20models%20and%20establish%20thorough%20theoretical%20analysis%20of%0Atheir%20uniqueness%20and%20stability%20properties.%20By%20applying%20iterative%20algorithms%20to%0Athese%20DSC%20models%2C%20we%20derive%20convergence%20rates%20for%20convolutional%20neural%20networks%0A%28CNNs%29%20in%20their%20ability%20to%20extract%20sparse%20features.%20This%20provides%20a%20strong%0Atheoretical%20foundation%20for%20the%20use%20of%20CNNs%20in%20sparse%20feature%20learning%20tasks.%20We%0Aadditionally%20extend%20the%20convergence%20analysis%20to%20more%20general%20neural%20network%0Aarchitectures%2C%20including%20those%20with%20diverse%20activation%20functions%2C%20as%20well%20as%0Aself-attention%20and%20transformer-based%20models.%20This%20broadens%20the%20applicability%20of%0Aour%20findings%20to%20a%20wide%20range%20of%20deep%20learning%20methods%20for%20deep%20sparse%20feature%0Aextraction.%20Inspired%20by%20the%20strong%20connection%20between%20sparse%20coding%20and%20CNNs%2C%0Awe%20also%20explore%20training%20strategies%20to%20encourage%20neural%20networks%20to%20learn%20more%0Asparse%20features.%20Through%20numerical%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20these%20approaches%2C%20providing%20valuable%20insights%20for%20the%20design%0Aof%20efficient%20and%20interpretable%20deep%20learning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergence%2520Analysis%2520for%2520Deep%2520Sparse%2520Coding%2520via%2520Convolutional%2520Neural%250A%2520%2520Networks%26entry.906535625%3DJianfei%2520Li%2520and%2520Han%2520Feng%2520and%2520Ding-Xuan%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520intersections%2520between%2520sparse%2520coding%2520and%2520deep%250Alearning%2520to%2520enhance%2520our%2520understanding%2520of%2520feature%2520extraction%2520capabilities%2520in%250Aadvanced%2520neural%2520network%2520architectures.%2520We%2520begin%2520by%2520introducing%2520a%2520novel%2520class%2520of%250ADeep%2520Sparse%2520Coding%2520%2528DSC%2529%2520models%2520and%2520establish%2520thorough%2520theoretical%2520analysis%2520of%250Atheir%2520uniqueness%2520and%2520stability%2520properties.%2520By%2520applying%2520iterative%2520algorithms%2520to%250Athese%2520DSC%2520models%252C%2520we%2520derive%2520convergence%2520rates%2520for%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%2520in%2520their%2520ability%2520to%2520extract%2520sparse%2520features.%2520This%2520provides%2520a%2520strong%250Atheoretical%2520foundation%2520for%2520the%2520use%2520of%2520CNNs%2520in%2520sparse%2520feature%2520learning%2520tasks.%2520We%250Aadditionally%2520extend%2520the%2520convergence%2520analysis%2520to%2520more%2520general%2520neural%2520network%250Aarchitectures%252C%2520including%2520those%2520with%2520diverse%2520activation%2520functions%252C%2520as%2520well%2520as%250Aself-attention%2520and%2520transformer-based%2520models.%2520This%2520broadens%2520the%2520applicability%2520of%250Aour%2520findings%2520to%2520a%2520wide%2520range%2520of%2520deep%2520learning%2520methods%2520for%2520deep%2520sparse%2520feature%250Aextraction.%2520Inspired%2520by%2520the%2520strong%2520connection%2520between%2520sparse%2520coding%2520and%2520CNNs%252C%250Awe%2520also%2520explore%2520training%2520strategies%2520to%2520encourage%2520neural%2520networks%2520to%2520learn%2520more%250Asparse%2520features.%2520Through%2520numerical%2520experiments%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520these%2520approaches%252C%2520providing%2520valuable%2520insights%2520for%2520the%2520design%250Aof%2520efficient%2520and%2520interpretable%2520deep%2520learning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Analysis%20for%20Deep%20Sparse%20Coding%20via%20Convolutional%20Neural%0A%20%20Networks&entry.906535625=Jianfei%20Li%20and%20Han%20Feng%20and%20Ding-Xuan%20Zhou&entry.1292438233=%20%20In%20this%20work%2C%20we%20explore%20intersections%20between%20sparse%20coding%20and%20deep%0Alearning%20to%20enhance%20our%20understanding%20of%20feature%20extraction%20capabilities%20in%0Aadvanced%20neural%20network%20architectures.%20We%20begin%20by%20introducing%20a%20novel%20class%20of%0ADeep%20Sparse%20Coding%20%28DSC%29%20models%20and%20establish%20thorough%20theoretical%20analysis%20of%0Atheir%20uniqueness%20and%20stability%20properties.%20By%20applying%20iterative%20algorithms%20to%0Athese%20DSC%20models%2C%20we%20derive%20convergence%20rates%20for%20convolutional%20neural%20networks%0A%28CNNs%29%20in%20their%20ability%20to%20extract%20sparse%20features.%20This%20provides%20a%20strong%0Atheoretical%20foundation%20for%20the%20use%20of%20CNNs%20in%20sparse%20feature%20learning%20tasks.%20We%0Aadditionally%20extend%20the%20convergence%20analysis%20to%20more%20general%20neural%20network%0Aarchitectures%2C%20including%20those%20with%20diverse%20activation%20functions%2C%20as%20well%20as%0Aself-attention%20and%20transformer-based%20models.%20This%20broadens%20the%20applicability%20of%0Aour%20findings%20to%20a%20wide%20range%20of%20deep%20learning%20methods%20for%20deep%20sparse%20feature%0Aextraction.%20Inspired%20by%20the%20strong%20connection%20between%20sparse%20coding%20and%20CNNs%2C%0Awe%20also%20explore%20training%20strategies%20to%20encourage%20neural%20networks%20to%20learn%20more%0Asparse%20features.%20Through%20numerical%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20these%20approaches%2C%20providing%20valuable%20insights%20for%20the%20design%0Aof%20efficient%20and%20interpretable%20deep%20learning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05540v2&entry.124074799=Read"},
{"title": "Reanimating Images using Neural Representations of Dynamic Stimuli", "author": "Jacob Yeung and Andrew F. Luo and Gabriel Sarch and Margaret M. Henderson and Deva Ramanan and Michael J. Tarr", "abstract": "  While computer vision models have made incredible strides in static image\nrecognition, they still do not match human performance in tasks that require\nthe understanding of complex, dynamic motion. This is notably true for\nreal-world scenarios where embodied agents face complex and motion-rich\nenvironments. Our approach leverages state-of-the-art video diffusion models to\ndecouple static image representation from motion generation, enabling us to\nutilize fMRI brain activity for a deeper understanding of human responses to\ndynamic visual stimuli. Conversely, we also demonstrate that information about\nthe brain's representation of motion can enhance the prediction of optical flow\nin artificial systems. Our novel approach leads to four main findings: (1)\nVisual motion, represented as fine-grained, object-level resolution optical\nflow, can be decoded from brain activity generated by participants viewing\nvideo stimuli; (2) Video encoders outperform image-based models in predicting\nvideo-driven brain activity; (3) Brain-decoded motion signals enable realistic\nvideo reanimation based only on the initial frame of the video; and (4) We\nextend prior work to achieve full video decoding from video-driven brain\nactivity. This framework advances our understanding of how the brain represents\nspatial and temporal information in dynamic visual scenes. Our findings\ndemonstrate the potential of combining brain imaging with video diffusion\nmodels for developing more robust and biologically-inspired computer vision\nsystems. We show additional decoding and encoding examples on this site:\nhttps://sites.google.com/view/neural-dynamics/home.\n", "link": "http://arxiv.org/abs/2406.02659v2", "date": "2024-11-29", "relevancy": 2.4818, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6367}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6213}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reanimating%20Images%20using%20Neural%20Representations%20of%20Dynamic%20Stimuli&body=Title%3A%20Reanimating%20Images%20using%20Neural%20Representations%20of%20Dynamic%20Stimuli%0AAuthor%3A%20Jacob%20Yeung%20and%20Andrew%20F.%20Luo%20and%20Gabriel%20Sarch%20and%20Margaret%20M.%20Henderson%20and%20Deva%20Ramanan%20and%20Michael%20J.%20Tarr%0AAbstract%3A%20%20%20While%20computer%20vision%20models%20have%20made%20incredible%20strides%20in%20static%20image%0Arecognition%2C%20they%20still%20do%20not%20match%20human%20performance%20in%20tasks%20that%20require%0Athe%20understanding%20of%20complex%2C%20dynamic%20motion.%20This%20is%20notably%20true%20for%0Areal-world%20scenarios%20where%20embodied%20agents%20face%20complex%20and%20motion-rich%0Aenvironments.%20Our%20approach%20leverages%20state-of-the-art%20video%20diffusion%20models%20to%0Adecouple%20static%20image%20representation%20from%20motion%20generation%2C%20enabling%20us%20to%0Autilize%20fMRI%20brain%20activity%20for%20a%20deeper%20understanding%20of%20human%20responses%20to%0Adynamic%20visual%20stimuli.%20Conversely%2C%20we%20also%20demonstrate%20that%20information%20about%0Athe%20brain%27s%20representation%20of%20motion%20can%20enhance%20the%20prediction%20of%20optical%20flow%0Ain%20artificial%20systems.%20Our%20novel%20approach%20leads%20to%20four%20main%20findings%3A%20%281%29%0AVisual%20motion%2C%20represented%20as%20fine-grained%2C%20object-level%20resolution%20optical%0Aflow%2C%20can%20be%20decoded%20from%20brain%20activity%20generated%20by%20participants%20viewing%0Avideo%20stimuli%3B%20%282%29%20Video%20encoders%20outperform%20image-based%20models%20in%20predicting%0Avideo-driven%20brain%20activity%3B%20%283%29%20Brain-decoded%20motion%20signals%20enable%20realistic%0Avideo%20reanimation%20based%20only%20on%20the%20initial%20frame%20of%20the%20video%3B%20and%20%284%29%20We%0Aextend%20prior%20work%20to%20achieve%20full%20video%20decoding%20from%20video-driven%20brain%0Aactivity.%20This%20framework%20advances%20our%20understanding%20of%20how%20the%20brain%20represents%0Aspatial%20and%20temporal%20information%20in%20dynamic%20visual%20scenes.%20Our%20findings%0Ademonstrate%20the%20potential%20of%20combining%20brain%20imaging%20with%20video%20diffusion%0Amodels%20for%20developing%20more%20robust%20and%20biologically-inspired%20computer%20vision%0Asystems.%20We%20show%20additional%20decoding%20and%20encoding%20examples%20on%20this%20site%3A%0Ahttps%3A//sites.google.com/view/neural-dynamics/home.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02659v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReanimating%2520Images%2520using%2520Neural%2520Representations%2520of%2520Dynamic%2520Stimuli%26entry.906535625%3DJacob%2520Yeung%2520and%2520Andrew%2520F.%2520Luo%2520and%2520Gabriel%2520Sarch%2520and%2520Margaret%2520M.%2520Henderson%2520and%2520Deva%2520Ramanan%2520and%2520Michael%2520J.%2520Tarr%26entry.1292438233%3D%2520%2520While%2520computer%2520vision%2520models%2520have%2520made%2520incredible%2520strides%2520in%2520static%2520image%250Arecognition%252C%2520they%2520still%2520do%2520not%2520match%2520human%2520performance%2520in%2520tasks%2520that%2520require%250Athe%2520understanding%2520of%2520complex%252C%2520dynamic%2520motion.%2520This%2520is%2520notably%2520true%2520for%250Areal-world%2520scenarios%2520where%2520embodied%2520agents%2520face%2520complex%2520and%2520motion-rich%250Aenvironments.%2520Our%2520approach%2520leverages%2520state-of-the-art%2520video%2520diffusion%2520models%2520to%250Adecouple%2520static%2520image%2520representation%2520from%2520motion%2520generation%252C%2520enabling%2520us%2520to%250Autilize%2520fMRI%2520brain%2520activity%2520for%2520a%2520deeper%2520understanding%2520of%2520human%2520responses%2520to%250Adynamic%2520visual%2520stimuli.%2520Conversely%252C%2520we%2520also%2520demonstrate%2520that%2520information%2520about%250Athe%2520brain%2527s%2520representation%2520of%2520motion%2520can%2520enhance%2520the%2520prediction%2520of%2520optical%2520flow%250Ain%2520artificial%2520systems.%2520Our%2520novel%2520approach%2520leads%2520to%2520four%2520main%2520findings%253A%2520%25281%2529%250AVisual%2520motion%252C%2520represented%2520as%2520fine-grained%252C%2520object-level%2520resolution%2520optical%250Aflow%252C%2520can%2520be%2520decoded%2520from%2520brain%2520activity%2520generated%2520by%2520participants%2520viewing%250Avideo%2520stimuli%253B%2520%25282%2529%2520Video%2520encoders%2520outperform%2520image-based%2520models%2520in%2520predicting%250Avideo-driven%2520brain%2520activity%253B%2520%25283%2529%2520Brain-decoded%2520motion%2520signals%2520enable%2520realistic%250Avideo%2520reanimation%2520based%2520only%2520on%2520the%2520initial%2520frame%2520of%2520the%2520video%253B%2520and%2520%25284%2529%2520We%250Aextend%2520prior%2520work%2520to%2520achieve%2520full%2520video%2520decoding%2520from%2520video-driven%2520brain%250Aactivity.%2520This%2520framework%2520advances%2520our%2520understanding%2520of%2520how%2520the%2520brain%2520represents%250Aspatial%2520and%2520temporal%2520information%2520in%2520dynamic%2520visual%2520scenes.%2520Our%2520findings%250Ademonstrate%2520the%2520potential%2520of%2520combining%2520brain%2520imaging%2520with%2520video%2520diffusion%250Amodels%2520for%2520developing%2520more%2520robust%2520and%2520biologically-inspired%2520computer%2520vision%250Asystems.%2520We%2520show%2520additional%2520decoding%2520and%2520encoding%2520examples%2520on%2520this%2520site%253A%250Ahttps%253A//sites.google.com/view/neural-dynamics/home.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02659v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reanimating%20Images%20using%20Neural%20Representations%20of%20Dynamic%20Stimuli&entry.906535625=Jacob%20Yeung%20and%20Andrew%20F.%20Luo%20and%20Gabriel%20Sarch%20and%20Margaret%20M.%20Henderson%20and%20Deva%20Ramanan%20and%20Michael%20J.%20Tarr&entry.1292438233=%20%20While%20computer%20vision%20models%20have%20made%20incredible%20strides%20in%20static%20image%0Arecognition%2C%20they%20still%20do%20not%20match%20human%20performance%20in%20tasks%20that%20require%0Athe%20understanding%20of%20complex%2C%20dynamic%20motion.%20This%20is%20notably%20true%20for%0Areal-world%20scenarios%20where%20embodied%20agents%20face%20complex%20and%20motion-rich%0Aenvironments.%20Our%20approach%20leverages%20state-of-the-art%20video%20diffusion%20models%20to%0Adecouple%20static%20image%20representation%20from%20motion%20generation%2C%20enabling%20us%20to%0Autilize%20fMRI%20brain%20activity%20for%20a%20deeper%20understanding%20of%20human%20responses%20to%0Adynamic%20visual%20stimuli.%20Conversely%2C%20we%20also%20demonstrate%20that%20information%20about%0Athe%20brain%27s%20representation%20of%20motion%20can%20enhance%20the%20prediction%20of%20optical%20flow%0Ain%20artificial%20systems.%20Our%20novel%20approach%20leads%20to%20four%20main%20findings%3A%20%281%29%0AVisual%20motion%2C%20represented%20as%20fine-grained%2C%20object-level%20resolution%20optical%0Aflow%2C%20can%20be%20decoded%20from%20brain%20activity%20generated%20by%20participants%20viewing%0Avideo%20stimuli%3B%20%282%29%20Video%20encoders%20outperform%20image-based%20models%20in%20predicting%0Avideo-driven%20brain%20activity%3B%20%283%29%20Brain-decoded%20motion%20signals%20enable%20realistic%0Avideo%20reanimation%20based%20only%20on%20the%20initial%20frame%20of%20the%20video%3B%20and%20%284%29%20We%0Aextend%20prior%20work%20to%20achieve%20full%20video%20decoding%20from%20video-driven%20brain%0Aactivity.%20This%20framework%20advances%20our%20understanding%20of%20how%20the%20brain%20represents%0Aspatial%20and%20temporal%20information%20in%20dynamic%20visual%20scenes.%20Our%20findings%0Ademonstrate%20the%20potential%20of%20combining%20brain%20imaging%20with%20video%20diffusion%0Amodels%20for%20developing%20more%20robust%20and%20biologically-inspired%20computer%20vision%0Asystems.%20We%20show%20additional%20decoding%20and%20encoding%20examples%20on%20this%20site%3A%0Ahttps%3A//sites.google.com/view/neural-dynamics/home.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02659v2&entry.124074799=Read"},
{"title": "Towards Class-wise Robustness Analysis", "author": "Tejaswini Medi and Julia Grabinski and Margret Keuper", "abstract": "  While being very successful in solving many downstream tasks, the application\nof deep neural networks is limited in real-life scenarios because of their\nsusceptibility to domain shifts such as common corruptions, and adversarial\nattacks. The existence of adversarial examples and data corruption\nsignificantly reduces the performance of deep classification models.\nResearchers have made strides in developing robust neural architectures to\nbolster decisions of deep classifiers. However, most of these works rely on\neffective adversarial training methods, and predominantly focus on overall\nmodel robustness, disregarding class-wise differences in robustness, which are\ncritical. Exploiting weakly robust classes is a potential avenue for attackers\nto fool the image recognition models. Therefore, this study investigates\nclass-to-class biases across adversarially trained robust classification models\nto understand their latent space structures and analyze their strong and weak\nclass-wise properties. We further assess the robustness of classes against\ncommon corruptions and adversarial attacks, recognizing that class\nvulnerability extends beyond the number of correct classifications for a\nspecific class. We find that the number of false positives of classes as\nspecific target classes significantly impacts their vulnerability to attacks.\nThrough our analysis on the Class False Positive Score, we assess a fair\nevaluation of how susceptible each class is to misclassification.\n", "link": "http://arxiv.org/abs/2411.19853v1", "date": "2024-11-29", "relevancy": 2.4737, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4991}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4944}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Class-wise%20Robustness%20Analysis&body=Title%3A%20Towards%20Class-wise%20Robustness%20Analysis%0AAuthor%3A%20Tejaswini%20Medi%20and%20Julia%20Grabinski%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20While%20being%20very%20successful%20in%20solving%20many%20downstream%20tasks%2C%20the%20application%0Aof%20deep%20neural%20networks%20is%20limited%20in%20real-life%20scenarios%20because%20of%20their%0Asusceptibility%20to%20domain%20shifts%20such%20as%20common%20corruptions%2C%20and%20adversarial%0Aattacks.%20The%20existence%20of%20adversarial%20examples%20and%20data%20corruption%0Asignificantly%20reduces%20the%20performance%20of%20deep%20classification%20models.%0AResearchers%20have%20made%20strides%20in%20developing%20robust%20neural%20architectures%20to%0Abolster%20decisions%20of%20deep%20classifiers.%20However%2C%20most%20of%20these%20works%20rely%20on%0Aeffective%20adversarial%20training%20methods%2C%20and%20predominantly%20focus%20on%20overall%0Amodel%20robustness%2C%20disregarding%20class-wise%20differences%20in%20robustness%2C%20which%20are%0Acritical.%20Exploiting%20weakly%20robust%20classes%20is%20a%20potential%20avenue%20for%20attackers%0Ato%20fool%20the%20image%20recognition%20models.%20Therefore%2C%20this%20study%20investigates%0Aclass-to-class%20biases%20across%20adversarially%20trained%20robust%20classification%20models%0Ato%20understand%20their%20latent%20space%20structures%20and%20analyze%20their%20strong%20and%20weak%0Aclass-wise%20properties.%20We%20further%20assess%20the%20robustness%20of%20classes%20against%0Acommon%20corruptions%20and%20adversarial%20attacks%2C%20recognizing%20that%20class%0Avulnerability%20extends%20beyond%20the%20number%20of%20correct%20classifications%20for%20a%0Aspecific%20class.%20We%20find%20that%20the%20number%20of%20false%20positives%20of%20classes%20as%0Aspecific%20target%20classes%20significantly%20impacts%20their%20vulnerability%20to%20attacks.%0AThrough%20our%20analysis%20on%20the%20Class%20False%20Positive%20Score%2C%20we%20assess%20a%20fair%0Aevaluation%20of%20how%20susceptible%20each%20class%20is%20to%20misclassification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Class-wise%2520Robustness%2520Analysis%26entry.906535625%3DTejaswini%2520Medi%2520and%2520Julia%2520Grabinski%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520While%2520being%2520very%2520successful%2520in%2520solving%2520many%2520downstream%2520tasks%252C%2520the%2520application%250Aof%2520deep%2520neural%2520networks%2520is%2520limited%2520in%2520real-life%2520scenarios%2520because%2520of%2520their%250Asusceptibility%2520to%2520domain%2520shifts%2520such%2520as%2520common%2520corruptions%252C%2520and%2520adversarial%250Aattacks.%2520The%2520existence%2520of%2520adversarial%2520examples%2520and%2520data%2520corruption%250Asignificantly%2520reduces%2520the%2520performance%2520of%2520deep%2520classification%2520models.%250AResearchers%2520have%2520made%2520strides%2520in%2520developing%2520robust%2520neural%2520architectures%2520to%250Abolster%2520decisions%2520of%2520deep%2520classifiers.%2520However%252C%2520most%2520of%2520these%2520works%2520rely%2520on%250Aeffective%2520adversarial%2520training%2520methods%252C%2520and%2520predominantly%2520focus%2520on%2520overall%250Amodel%2520robustness%252C%2520disregarding%2520class-wise%2520differences%2520in%2520robustness%252C%2520which%2520are%250Acritical.%2520Exploiting%2520weakly%2520robust%2520classes%2520is%2520a%2520potential%2520avenue%2520for%2520attackers%250Ato%2520fool%2520the%2520image%2520recognition%2520models.%2520Therefore%252C%2520this%2520study%2520investigates%250Aclass-to-class%2520biases%2520across%2520adversarially%2520trained%2520robust%2520classification%2520models%250Ato%2520understand%2520their%2520latent%2520space%2520structures%2520and%2520analyze%2520their%2520strong%2520and%2520weak%250Aclass-wise%2520properties.%2520We%2520further%2520assess%2520the%2520robustness%2520of%2520classes%2520against%250Acommon%2520corruptions%2520and%2520adversarial%2520attacks%252C%2520recognizing%2520that%2520class%250Avulnerability%2520extends%2520beyond%2520the%2520number%2520of%2520correct%2520classifications%2520for%2520a%250Aspecific%2520class.%2520We%2520find%2520that%2520the%2520number%2520of%2520false%2520positives%2520of%2520classes%2520as%250Aspecific%2520target%2520classes%2520significantly%2520impacts%2520their%2520vulnerability%2520to%2520attacks.%250AThrough%2520our%2520analysis%2520on%2520the%2520Class%2520False%2520Positive%2520Score%252C%2520we%2520assess%2520a%2520fair%250Aevaluation%2520of%2520how%2520susceptible%2520each%2520class%2520is%2520to%2520misclassification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Class-wise%20Robustness%20Analysis&entry.906535625=Tejaswini%20Medi%20and%20Julia%20Grabinski%20and%20Margret%20Keuper&entry.1292438233=%20%20While%20being%20very%20successful%20in%20solving%20many%20downstream%20tasks%2C%20the%20application%0Aof%20deep%20neural%20networks%20is%20limited%20in%20real-life%20scenarios%20because%20of%20their%0Asusceptibility%20to%20domain%20shifts%20such%20as%20common%20corruptions%2C%20and%20adversarial%0Aattacks.%20The%20existence%20of%20adversarial%20examples%20and%20data%20corruption%0Asignificantly%20reduces%20the%20performance%20of%20deep%20classification%20models.%0AResearchers%20have%20made%20strides%20in%20developing%20robust%20neural%20architectures%20to%0Abolster%20decisions%20of%20deep%20classifiers.%20However%2C%20most%20of%20these%20works%20rely%20on%0Aeffective%20adversarial%20training%20methods%2C%20and%20predominantly%20focus%20on%20overall%0Amodel%20robustness%2C%20disregarding%20class-wise%20differences%20in%20robustness%2C%20which%20are%0Acritical.%20Exploiting%20weakly%20robust%20classes%20is%20a%20potential%20avenue%20for%20attackers%0Ato%20fool%20the%20image%20recognition%20models.%20Therefore%2C%20this%20study%20investigates%0Aclass-to-class%20biases%20across%20adversarially%20trained%20robust%20classification%20models%0Ato%20understand%20their%20latent%20space%20structures%20and%20analyze%20their%20strong%20and%20weak%0Aclass-wise%20properties.%20We%20further%20assess%20the%20robustness%20of%20classes%20against%0Acommon%20corruptions%20and%20adversarial%20attacks%2C%20recognizing%20that%20class%0Avulnerability%20extends%20beyond%20the%20number%20of%20correct%20classifications%20for%20a%0Aspecific%20class.%20We%20find%20that%20the%20number%20of%20false%20positives%20of%20classes%20as%0Aspecific%20target%20classes%20significantly%20impacts%20their%20vulnerability%20to%20attacks.%0AThrough%20our%20analysis%20on%20the%20Class%20False%20Positive%20Score%2C%20we%20assess%20a%20fair%0Aevaluation%20of%20how%20susceptible%20each%20class%20is%20to%20misclassification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19853v1&entry.124074799=Read"},
{"title": "Enhanced anomaly detection in well log data through the application of\n  ensemble GANs", "author": "Abdulrahman Al-Fakih and A. Koeshidayatullah and Tapan Mukerji and SanLinn I. Kaka", "abstract": "  Although generative adversarial networks (GANs) have shown significant\nsuccess in modeling data distributions for image datasets, their application to\nstructured or tabular data, such as well logs, remains relatively\nunderexplored. This study extends the ensemble GANs (EGANs) framework to\ncapture the distribution of well log data and detect anomalies that fall\noutside of these distributions. The proposed approach compares the performance\nof traditional methods, such as Gaussian mixture models (GMMs), with EGANs in\ndetecting anomalies outside the expected data distributions. For the gamma ray\n(GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76,\noutperforming GMM's precision of 0.38 and F1 score of 0.54. Similarly, for\ntravel time (DT), EGANs achieved a precision of 0.70 and F1 score of 0.79,\nsurpassing GMM 0.56 and 0.71. In the neutron porosity (NPHI) dataset, EGANs\nrecorded a precision of 0.53 and F1 score of 0.68, outshining GMM 0.47 and\n0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52\nand an F1 score of 0.67, slightly outperforming GMM, which yielded a precision\nof 0.50 and an F1 score of 0.65. This work's novelty lies in applying EGANs for\nwell log data analysis, showcasing their ability to learn data patterns and\nidentify anomalies that deviate from them. This approach offers more reliable\nanomaly detection compared to traditional methods like GMM. The findings\nhighlight the potential of EGANs in enhancing anomaly detection for well log\ndata, delivering significant implications for optimizing drilling strategies\nand reservoir management through more accurate, data-driven insights into\nsubsurface characterization.\n", "link": "http://arxiv.org/abs/2411.19875v1", "date": "2024-11-29", "relevancy": 2.4533, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5018}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4865}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20anomaly%20detection%20in%20well%20log%20data%20through%20the%20application%20of%0A%20%20ensemble%20GANs&body=Title%3A%20Enhanced%20anomaly%20detection%20in%20well%20log%20data%20through%20the%20application%20of%0A%20%20ensemble%20GANs%0AAuthor%3A%20Abdulrahman%20Al-Fakih%20and%20A.%20Koeshidayatullah%20and%20Tapan%20Mukerji%20and%20SanLinn%20I.%20Kaka%0AAbstract%3A%20%20%20Although%20generative%20adversarial%20networks%20%28GANs%29%20have%20shown%20significant%0Asuccess%20in%20modeling%20data%20distributions%20for%20image%20datasets%2C%20their%20application%20to%0Astructured%20or%20tabular%20data%2C%20such%20as%20well%20logs%2C%20remains%20relatively%0Aunderexplored.%20This%20study%20extends%20the%20ensemble%20GANs%20%28EGANs%29%20framework%20to%0Acapture%20the%20distribution%20of%20well%20log%20data%20and%20detect%20anomalies%20that%20fall%0Aoutside%20of%20these%20distributions.%20The%20proposed%20approach%20compares%20the%20performance%0Aof%20traditional%20methods%2C%20such%20as%20Gaussian%20mixture%20models%20%28GMMs%29%2C%20with%20EGANs%20in%0Adetecting%20anomalies%20outside%20the%20expected%20data%20distributions.%20For%20the%20gamma%20ray%0A%28GR%29%20dataset%2C%20EGANs%20achieved%20a%20precision%20of%200.62%20and%20F1%20score%20of%200.76%2C%0Aoutperforming%20GMM%27s%20precision%20of%200.38%20and%20F1%20score%20of%200.54.%20Similarly%2C%20for%0Atravel%20time%20%28DT%29%2C%20EGANs%20achieved%20a%20precision%20of%200.70%20and%20F1%20score%20of%200.79%2C%0Asurpassing%20GMM%200.56%20and%200.71.%20In%20the%20neutron%20porosity%20%28NPHI%29%20dataset%2C%20EGANs%0Arecorded%20a%20precision%20of%200.53%20and%20F1%20score%20of%200.68%2C%20outshining%20GMM%200.47%20and%0A0.61.%20For%20the%20bulk%20density%20%28RHOB%29%20dataset%2C%20EGANs%20achieved%20a%20precision%20of%200.52%0Aand%20an%20F1%20score%20of%200.67%2C%20slightly%20outperforming%20GMM%2C%20which%20yielded%20a%20precision%0Aof%200.50%20and%20an%20F1%20score%20of%200.65.%20This%20work%27s%20novelty%20lies%20in%20applying%20EGANs%20for%0Awell%20log%20data%20analysis%2C%20showcasing%20their%20ability%20to%20learn%20data%20patterns%20and%0Aidentify%20anomalies%20that%20deviate%20from%20them.%20This%20approach%20offers%20more%20reliable%0Aanomaly%20detection%20compared%20to%20traditional%20methods%20like%20GMM.%20The%20findings%0Ahighlight%20the%20potential%20of%20EGANs%20in%20enhancing%20anomaly%20detection%20for%20well%20log%0Adata%2C%20delivering%20significant%20implications%20for%20optimizing%20drilling%20strategies%0Aand%20reservoir%20management%20through%20more%20accurate%2C%20data-driven%20insights%20into%0Asubsurface%20characterization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520anomaly%2520detection%2520in%2520well%2520log%2520data%2520through%2520the%2520application%2520of%250A%2520%2520ensemble%2520GANs%26entry.906535625%3DAbdulrahman%2520Al-Fakih%2520and%2520A.%2520Koeshidayatullah%2520and%2520Tapan%2520Mukerji%2520and%2520SanLinn%2520I.%2520Kaka%26entry.1292438233%3D%2520%2520Although%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%2520have%2520shown%2520significant%250Asuccess%2520in%2520modeling%2520data%2520distributions%2520for%2520image%2520datasets%252C%2520their%2520application%2520to%250Astructured%2520or%2520tabular%2520data%252C%2520such%2520as%2520well%2520logs%252C%2520remains%2520relatively%250Aunderexplored.%2520This%2520study%2520extends%2520the%2520ensemble%2520GANs%2520%2528EGANs%2529%2520framework%2520to%250Acapture%2520the%2520distribution%2520of%2520well%2520log%2520data%2520and%2520detect%2520anomalies%2520that%2520fall%250Aoutside%2520of%2520these%2520distributions.%2520The%2520proposed%2520approach%2520compares%2520the%2520performance%250Aof%2520traditional%2520methods%252C%2520such%2520as%2520Gaussian%2520mixture%2520models%2520%2528GMMs%2529%252C%2520with%2520EGANs%2520in%250Adetecting%2520anomalies%2520outside%2520the%2520expected%2520data%2520distributions.%2520For%2520the%2520gamma%2520ray%250A%2528GR%2529%2520dataset%252C%2520EGANs%2520achieved%2520a%2520precision%2520of%25200.62%2520and%2520F1%2520score%2520of%25200.76%252C%250Aoutperforming%2520GMM%2527s%2520precision%2520of%25200.38%2520and%2520F1%2520score%2520of%25200.54.%2520Similarly%252C%2520for%250Atravel%2520time%2520%2528DT%2529%252C%2520EGANs%2520achieved%2520a%2520precision%2520of%25200.70%2520and%2520F1%2520score%2520of%25200.79%252C%250Asurpassing%2520GMM%25200.56%2520and%25200.71.%2520In%2520the%2520neutron%2520porosity%2520%2528NPHI%2529%2520dataset%252C%2520EGANs%250Arecorded%2520a%2520precision%2520of%25200.53%2520and%2520F1%2520score%2520of%25200.68%252C%2520outshining%2520GMM%25200.47%2520and%250A0.61.%2520For%2520the%2520bulk%2520density%2520%2528RHOB%2529%2520dataset%252C%2520EGANs%2520achieved%2520a%2520precision%2520of%25200.52%250Aand%2520an%2520F1%2520score%2520of%25200.67%252C%2520slightly%2520outperforming%2520GMM%252C%2520which%2520yielded%2520a%2520precision%250Aof%25200.50%2520and%2520an%2520F1%2520score%2520of%25200.65.%2520This%2520work%2527s%2520novelty%2520lies%2520in%2520applying%2520EGANs%2520for%250Awell%2520log%2520data%2520analysis%252C%2520showcasing%2520their%2520ability%2520to%2520learn%2520data%2520patterns%2520and%250Aidentify%2520anomalies%2520that%2520deviate%2520from%2520them.%2520This%2520approach%2520offers%2520more%2520reliable%250Aanomaly%2520detection%2520compared%2520to%2520traditional%2520methods%2520like%2520GMM.%2520The%2520findings%250Ahighlight%2520the%2520potential%2520of%2520EGANs%2520in%2520enhancing%2520anomaly%2520detection%2520for%2520well%2520log%250Adata%252C%2520delivering%2520significant%2520implications%2520for%2520optimizing%2520drilling%2520strategies%250Aand%2520reservoir%2520management%2520through%2520more%2520accurate%252C%2520data-driven%2520insights%2520into%250Asubsurface%2520characterization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20anomaly%20detection%20in%20well%20log%20data%20through%20the%20application%20of%0A%20%20ensemble%20GANs&entry.906535625=Abdulrahman%20Al-Fakih%20and%20A.%20Koeshidayatullah%20and%20Tapan%20Mukerji%20and%20SanLinn%20I.%20Kaka&entry.1292438233=%20%20Although%20generative%20adversarial%20networks%20%28GANs%29%20have%20shown%20significant%0Asuccess%20in%20modeling%20data%20distributions%20for%20image%20datasets%2C%20their%20application%20to%0Astructured%20or%20tabular%20data%2C%20such%20as%20well%20logs%2C%20remains%20relatively%0Aunderexplored.%20This%20study%20extends%20the%20ensemble%20GANs%20%28EGANs%29%20framework%20to%0Acapture%20the%20distribution%20of%20well%20log%20data%20and%20detect%20anomalies%20that%20fall%0Aoutside%20of%20these%20distributions.%20The%20proposed%20approach%20compares%20the%20performance%0Aof%20traditional%20methods%2C%20such%20as%20Gaussian%20mixture%20models%20%28GMMs%29%2C%20with%20EGANs%20in%0Adetecting%20anomalies%20outside%20the%20expected%20data%20distributions.%20For%20the%20gamma%20ray%0A%28GR%29%20dataset%2C%20EGANs%20achieved%20a%20precision%20of%200.62%20and%20F1%20score%20of%200.76%2C%0Aoutperforming%20GMM%27s%20precision%20of%200.38%20and%20F1%20score%20of%200.54.%20Similarly%2C%20for%0Atravel%20time%20%28DT%29%2C%20EGANs%20achieved%20a%20precision%20of%200.70%20and%20F1%20score%20of%200.79%2C%0Asurpassing%20GMM%200.56%20and%200.71.%20In%20the%20neutron%20porosity%20%28NPHI%29%20dataset%2C%20EGANs%0Arecorded%20a%20precision%20of%200.53%20and%20F1%20score%20of%200.68%2C%20outshining%20GMM%200.47%20and%0A0.61.%20For%20the%20bulk%20density%20%28RHOB%29%20dataset%2C%20EGANs%20achieved%20a%20precision%20of%200.52%0Aand%20an%20F1%20score%20of%200.67%2C%20slightly%20outperforming%20GMM%2C%20which%20yielded%20a%20precision%0Aof%200.50%20and%20an%20F1%20score%20of%200.65.%20This%20work%27s%20novelty%20lies%20in%20applying%20EGANs%20for%0Awell%20log%20data%20analysis%2C%20showcasing%20their%20ability%20to%20learn%20data%20patterns%20and%0Aidentify%20anomalies%20that%20deviate%20from%20them.%20This%20approach%20offers%20more%20reliable%0Aanomaly%20detection%20compared%20to%20traditional%20methods%20like%20GMM.%20The%20findings%0Ahighlight%20the%20potential%20of%20EGANs%20in%20enhancing%20anomaly%20detection%20for%20well%20log%0Adata%2C%20delivering%20significant%20implications%20for%20optimizing%20drilling%20strategies%0Aand%20reservoir%20management%20through%20more%20accurate%2C%20data-driven%20insights%20into%0Asubsurface%20characterization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19875v1&entry.124074799=Read"},
{"title": "Quantifying the synthetic and real domain gap in aerial scene\n  understanding", "author": "Alina Marcu", "abstract": "  Quantifying the gap between synthetic and real-world imagery is essential for\nimproving both transformer-based models - that rely on large volumes of data -\nand datasets, especially in underexplored domains like aerial scene\nunderstanding where the potential impact is significant. This paper introduces\na novel methodology for scene complexity assessment using Multi-Model Consensus\nMetric (MMCM) and depth-based structural metrics, enabling a robust evaluation\nof perceptual and structural disparities between domains. Our experimental\nanalysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes)\ndatasets, demonstrates that real-world scenes generally exhibit higher\nconsensus among state-of-the-art vision transformers, while synthetic scenes\nshow greater variability and challenge model adaptability. The results\nunderline the inherent complexities and domain gaps, emphasizing the need for\nenhanced simulation fidelity and model generalization. This work provides\ncritical insights into the interplay between domain characteristics and model\nperformance, offering a pathway for improved domain adaptation strategies in\naerial scene understanding.\n", "link": "http://arxiv.org/abs/2411.19913v1", "date": "2024-11-29", "relevancy": 2.438, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6216}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20the%20synthetic%20and%20real%20domain%20gap%20in%20aerial%20scene%0A%20%20understanding&body=Title%3A%20Quantifying%20the%20synthetic%20and%20real%20domain%20gap%20in%20aerial%20scene%0A%20%20understanding%0AAuthor%3A%20Alina%20Marcu%0AAbstract%3A%20%20%20Quantifying%20the%20gap%20between%20synthetic%20and%20real-world%20imagery%20is%20essential%20for%0Aimproving%20both%20transformer-based%20models%20-%20that%20rely%20on%20large%20volumes%20of%20data%20-%0Aand%20datasets%2C%20especially%20in%20underexplored%20domains%20like%20aerial%20scene%0Aunderstanding%20where%20the%20potential%20impact%20is%20significant.%20This%20paper%20introduces%0Aa%20novel%20methodology%20for%20scene%20complexity%20assessment%20using%20Multi-Model%20Consensus%0AMetric%20%28MMCM%29%20and%20depth-based%20structural%20metrics%2C%20enabling%20a%20robust%20evaluation%0Aof%20perceptual%20and%20structural%20disparities%20between%20domains.%20Our%20experimental%0Aanalysis%2C%20utilizing%20real-world%20%28Dronescapes%29%20and%20synthetic%20%28Skyscenes%29%0Adatasets%2C%20demonstrates%20that%20real-world%20scenes%20generally%20exhibit%20higher%0Aconsensus%20among%20state-of-the-art%20vision%20transformers%2C%20while%20synthetic%20scenes%0Ashow%20greater%20variability%20and%20challenge%20model%20adaptability.%20The%20results%0Aunderline%20the%20inherent%20complexities%20and%20domain%20gaps%2C%20emphasizing%20the%20need%20for%0Aenhanced%20simulation%20fidelity%20and%20model%20generalization.%20This%20work%20provides%0Acritical%20insights%20into%20the%20interplay%20between%20domain%20characteristics%20and%20model%0Aperformance%2C%20offering%20a%20pathway%20for%20improved%20domain%20adaptation%20strategies%20in%0Aaerial%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520the%2520synthetic%2520and%2520real%2520domain%2520gap%2520in%2520aerial%2520scene%250A%2520%2520understanding%26entry.906535625%3DAlina%2520Marcu%26entry.1292438233%3D%2520%2520Quantifying%2520the%2520gap%2520between%2520synthetic%2520and%2520real-world%2520imagery%2520is%2520essential%2520for%250Aimproving%2520both%2520transformer-based%2520models%2520-%2520that%2520rely%2520on%2520large%2520volumes%2520of%2520data%2520-%250Aand%2520datasets%252C%2520especially%2520in%2520underexplored%2520domains%2520like%2520aerial%2520scene%250Aunderstanding%2520where%2520the%2520potential%2520impact%2520is%2520significant.%2520This%2520paper%2520introduces%250Aa%2520novel%2520methodology%2520for%2520scene%2520complexity%2520assessment%2520using%2520Multi-Model%2520Consensus%250AMetric%2520%2528MMCM%2529%2520and%2520depth-based%2520structural%2520metrics%252C%2520enabling%2520a%2520robust%2520evaluation%250Aof%2520perceptual%2520and%2520structural%2520disparities%2520between%2520domains.%2520Our%2520experimental%250Aanalysis%252C%2520utilizing%2520real-world%2520%2528Dronescapes%2529%2520and%2520synthetic%2520%2528Skyscenes%2529%250Adatasets%252C%2520demonstrates%2520that%2520real-world%2520scenes%2520generally%2520exhibit%2520higher%250Aconsensus%2520among%2520state-of-the-art%2520vision%2520transformers%252C%2520while%2520synthetic%2520scenes%250Ashow%2520greater%2520variability%2520and%2520challenge%2520model%2520adaptability.%2520The%2520results%250Aunderline%2520the%2520inherent%2520complexities%2520and%2520domain%2520gaps%252C%2520emphasizing%2520the%2520need%2520for%250Aenhanced%2520simulation%2520fidelity%2520and%2520model%2520generalization.%2520This%2520work%2520provides%250Acritical%2520insights%2520into%2520the%2520interplay%2520between%2520domain%2520characteristics%2520and%2520model%250Aperformance%252C%2520offering%2520a%2520pathway%2520for%2520improved%2520domain%2520adaptation%2520strategies%2520in%250Aaerial%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20the%20synthetic%20and%20real%20domain%20gap%20in%20aerial%20scene%0A%20%20understanding&entry.906535625=Alina%20Marcu&entry.1292438233=%20%20Quantifying%20the%20gap%20between%20synthetic%20and%20real-world%20imagery%20is%20essential%20for%0Aimproving%20both%20transformer-based%20models%20-%20that%20rely%20on%20large%20volumes%20of%20data%20-%0Aand%20datasets%2C%20especially%20in%20underexplored%20domains%20like%20aerial%20scene%0Aunderstanding%20where%20the%20potential%20impact%20is%20significant.%20This%20paper%20introduces%0Aa%20novel%20methodology%20for%20scene%20complexity%20assessment%20using%20Multi-Model%20Consensus%0AMetric%20%28MMCM%29%20and%20depth-based%20structural%20metrics%2C%20enabling%20a%20robust%20evaluation%0Aof%20perceptual%20and%20structural%20disparities%20between%20domains.%20Our%20experimental%0Aanalysis%2C%20utilizing%20real-world%20%28Dronescapes%29%20and%20synthetic%20%28Skyscenes%29%0Adatasets%2C%20demonstrates%20that%20real-world%20scenes%20generally%20exhibit%20higher%0Aconsensus%20among%20state-of-the-art%20vision%20transformers%2C%20while%20synthetic%20scenes%0Ashow%20greater%20variability%20and%20challenge%20model%20adaptability.%20The%20results%0Aunderline%20the%20inherent%20complexities%20and%20domain%20gaps%2C%20emphasizing%20the%20need%20for%0Aenhanced%20simulation%20fidelity%20and%20model%20generalization.%20This%20work%20provides%0Acritical%20insights%20into%20the%20interplay%20between%20domain%20characteristics%20and%20model%0Aperformance%2C%20offering%20a%20pathway%20for%20improved%20domain%20adaptation%20strategies%20in%0Aaerial%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19913v1&entry.124074799=Read"},
{"title": "A Visual-inertial Localization Algorithm using Opportunistic Visual\n  Beacons and Dead-Reckoning for GNSS-Denied Large-scale Applications", "author": "Liqiang Zhang Ye Tian Dongyan Wei", "abstract": "  With the development of smart cities, the demand for continuous pedestrian\nnavigation in large-scale urban environments has significantly increased. While\nglobal navigation satellite systems (GNSS) provide low-cost and reliable\npositioning services, they are often hindered in complex urban canyon\nenvironments. Thus, exploring opportunistic signals for positioning in urban\nareas has become a key solution. Augmented reality (AR) allows pedestrians to\nacquire real-time visual information. Accordingly, we propose a low-cost\nvisual-inertial positioning solution. This method comprises a lightweight\nmulti-scale group convolution (MSGC)-based visual place recognition (VPR)\nneural network, a pedestrian dead reckoning (PDR) algorithm, and a\nvisual/inertial fusion approach based on a Kalman filter with gross error\nsuppression. The VPR serves as a conditional observation to the Kalman filter,\neffectively correcting the errors accumulated through the PDR method. This\nenables the entire algorithm to ensure the reliability of long-term positioning\nin GNSS-denied areas. Extensive experimental results demonstrate that our\nmethod maintains stable positioning during large-scale movements. Compared to\nthe lightweight MobileNetV3-based VPR method, our proposed VPR solution\nimproves Recall@1 by at least 3\\% on two public datasets while reducing the\nnumber of parameters by 63.37\\%. It also achieves performance that is\ncomparable to the VGG16-based method. The VPR-PDR algorithm improves\nlocalization accuracy by more than 40\\% compared to the original PDR.\n", "link": "http://arxiv.org/abs/2411.19845v1", "date": "2024-11-29", "relevancy": 2.4197, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6264}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Visual-inertial%20Localization%20Algorithm%20using%20Opportunistic%20Visual%0A%20%20Beacons%20and%20Dead-Reckoning%20for%20GNSS-Denied%20Large-scale%20Applications&body=Title%3A%20A%20Visual-inertial%20Localization%20Algorithm%20using%20Opportunistic%20Visual%0A%20%20Beacons%20and%20Dead-Reckoning%20for%20GNSS-Denied%20Large-scale%20Applications%0AAuthor%3A%20Liqiang%20Zhang%20Ye%20Tian%20Dongyan%20Wei%0AAbstract%3A%20%20%20With%20the%20development%20of%20smart%20cities%2C%20the%20demand%20for%20continuous%20pedestrian%0Anavigation%20in%20large-scale%20urban%20environments%20has%20significantly%20increased.%20While%0Aglobal%20navigation%20satellite%20systems%20%28GNSS%29%20provide%20low-cost%20and%20reliable%0Apositioning%20services%2C%20they%20are%20often%20hindered%20in%20complex%20urban%20canyon%0Aenvironments.%20Thus%2C%20exploring%20opportunistic%20signals%20for%20positioning%20in%20urban%0Aareas%20has%20become%20a%20key%20solution.%20Augmented%20reality%20%28AR%29%20allows%20pedestrians%20to%0Aacquire%20real-time%20visual%20information.%20Accordingly%2C%20we%20propose%20a%20low-cost%0Avisual-inertial%20positioning%20solution.%20This%20method%20comprises%20a%20lightweight%0Amulti-scale%20group%20convolution%20%28MSGC%29-based%20visual%20place%20recognition%20%28VPR%29%0Aneural%20network%2C%20a%20pedestrian%20dead%20reckoning%20%28PDR%29%20algorithm%2C%20and%20a%0Avisual/inertial%20fusion%20approach%20based%20on%20a%20Kalman%20filter%20with%20gross%20error%0Asuppression.%20The%20VPR%20serves%20as%20a%20conditional%20observation%20to%20the%20Kalman%20filter%2C%0Aeffectively%20correcting%20the%20errors%20accumulated%20through%20the%20PDR%20method.%20This%0Aenables%20the%20entire%20algorithm%20to%20ensure%20the%20reliability%20of%20long-term%20positioning%0Ain%20GNSS-denied%20areas.%20Extensive%20experimental%20results%20demonstrate%20that%20our%0Amethod%20maintains%20stable%20positioning%20during%20large-scale%20movements.%20Compared%20to%0Athe%20lightweight%20MobileNetV3-based%20VPR%20method%2C%20our%20proposed%20VPR%20solution%0Aimproves%20Recall%401%20by%20at%20least%203%5C%25%20on%20two%20public%20datasets%20while%20reducing%20the%0Anumber%20of%20parameters%20by%2063.37%5C%25.%20It%20also%20achieves%20performance%20that%20is%0Acomparable%20to%20the%20VGG16-based%20method.%20The%20VPR-PDR%20algorithm%20improves%0Alocalization%20accuracy%20by%20more%20than%2040%5C%25%20compared%20to%20the%20original%20PDR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Visual-inertial%2520Localization%2520Algorithm%2520using%2520Opportunistic%2520Visual%250A%2520%2520Beacons%2520and%2520Dead-Reckoning%2520for%2520GNSS-Denied%2520Large-scale%2520Applications%26entry.906535625%3DLiqiang%2520Zhang%2520Ye%2520Tian%2520Dongyan%2520Wei%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520smart%2520cities%252C%2520the%2520demand%2520for%2520continuous%2520pedestrian%250Anavigation%2520in%2520large-scale%2520urban%2520environments%2520has%2520significantly%2520increased.%2520While%250Aglobal%2520navigation%2520satellite%2520systems%2520%2528GNSS%2529%2520provide%2520low-cost%2520and%2520reliable%250Apositioning%2520services%252C%2520they%2520are%2520often%2520hindered%2520in%2520complex%2520urban%2520canyon%250Aenvironments.%2520Thus%252C%2520exploring%2520opportunistic%2520signals%2520for%2520positioning%2520in%2520urban%250Aareas%2520has%2520become%2520a%2520key%2520solution.%2520Augmented%2520reality%2520%2528AR%2529%2520allows%2520pedestrians%2520to%250Aacquire%2520real-time%2520visual%2520information.%2520Accordingly%252C%2520we%2520propose%2520a%2520low-cost%250Avisual-inertial%2520positioning%2520solution.%2520This%2520method%2520comprises%2520a%2520lightweight%250Amulti-scale%2520group%2520convolution%2520%2528MSGC%2529-based%2520visual%2520place%2520recognition%2520%2528VPR%2529%250Aneural%2520network%252C%2520a%2520pedestrian%2520dead%2520reckoning%2520%2528PDR%2529%2520algorithm%252C%2520and%2520a%250Avisual/inertial%2520fusion%2520approach%2520based%2520on%2520a%2520Kalman%2520filter%2520with%2520gross%2520error%250Asuppression.%2520The%2520VPR%2520serves%2520as%2520a%2520conditional%2520observation%2520to%2520the%2520Kalman%2520filter%252C%250Aeffectively%2520correcting%2520the%2520errors%2520accumulated%2520through%2520the%2520PDR%2520method.%2520This%250Aenables%2520the%2520entire%2520algorithm%2520to%2520ensure%2520the%2520reliability%2520of%2520long-term%2520positioning%250Ain%2520GNSS-denied%2520areas.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520maintains%2520stable%2520positioning%2520during%2520large-scale%2520movements.%2520Compared%2520to%250Athe%2520lightweight%2520MobileNetV3-based%2520VPR%2520method%252C%2520our%2520proposed%2520VPR%2520solution%250Aimproves%2520Recall%25401%2520by%2520at%2520least%25203%255C%2525%2520on%2520two%2520public%2520datasets%2520while%2520reducing%2520the%250Anumber%2520of%2520parameters%2520by%252063.37%255C%2525.%2520It%2520also%2520achieves%2520performance%2520that%2520is%250Acomparable%2520to%2520the%2520VGG16-based%2520method.%2520The%2520VPR-PDR%2520algorithm%2520improves%250Alocalization%2520accuracy%2520by%2520more%2520than%252040%255C%2525%2520compared%2520to%2520the%2520original%2520PDR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Visual-inertial%20Localization%20Algorithm%20using%20Opportunistic%20Visual%0A%20%20Beacons%20and%20Dead-Reckoning%20for%20GNSS-Denied%20Large-scale%20Applications&entry.906535625=Liqiang%20Zhang%20Ye%20Tian%20Dongyan%20Wei&entry.1292438233=%20%20With%20the%20development%20of%20smart%20cities%2C%20the%20demand%20for%20continuous%20pedestrian%0Anavigation%20in%20large-scale%20urban%20environments%20has%20significantly%20increased.%20While%0Aglobal%20navigation%20satellite%20systems%20%28GNSS%29%20provide%20low-cost%20and%20reliable%0Apositioning%20services%2C%20they%20are%20often%20hindered%20in%20complex%20urban%20canyon%0Aenvironments.%20Thus%2C%20exploring%20opportunistic%20signals%20for%20positioning%20in%20urban%0Aareas%20has%20become%20a%20key%20solution.%20Augmented%20reality%20%28AR%29%20allows%20pedestrians%20to%0Aacquire%20real-time%20visual%20information.%20Accordingly%2C%20we%20propose%20a%20low-cost%0Avisual-inertial%20positioning%20solution.%20This%20method%20comprises%20a%20lightweight%0Amulti-scale%20group%20convolution%20%28MSGC%29-based%20visual%20place%20recognition%20%28VPR%29%0Aneural%20network%2C%20a%20pedestrian%20dead%20reckoning%20%28PDR%29%20algorithm%2C%20and%20a%0Avisual/inertial%20fusion%20approach%20based%20on%20a%20Kalman%20filter%20with%20gross%20error%0Asuppression.%20The%20VPR%20serves%20as%20a%20conditional%20observation%20to%20the%20Kalman%20filter%2C%0Aeffectively%20correcting%20the%20errors%20accumulated%20through%20the%20PDR%20method.%20This%0Aenables%20the%20entire%20algorithm%20to%20ensure%20the%20reliability%20of%20long-term%20positioning%0Ain%20GNSS-denied%20areas.%20Extensive%20experimental%20results%20demonstrate%20that%20our%0Amethod%20maintains%20stable%20positioning%20during%20large-scale%20movements.%20Compared%20to%0Athe%20lightweight%20MobileNetV3-based%20VPR%20method%2C%20our%20proposed%20VPR%20solution%0Aimproves%20Recall%401%20by%20at%20least%203%5C%25%20on%20two%20public%20datasets%20while%20reducing%20the%0Anumber%20of%20parameters%20by%2063.37%5C%25.%20It%20also%20achieves%20performance%20that%20is%0Acomparable%20to%20the%20VGG16-based%20method.%20The%20VPR-PDR%20algorithm%20improves%0Alocalization%20accuracy%20by%20more%20than%2040%5C%25%20compared%20to%20the%20original%20PDR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19845v1&entry.124074799=Read"},
{"title": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D\n  Object Affordance Grounding", "author": "Yawen Shao and Wei Zhai and Yuhang Yang and Hongchen Luo and Yang Cao and Zheng-Jun Zha", "abstract": "  Open-Vocabulary 3D object affordance grounding aims to anticipate ``action\npossibilities'' regions on 3D objects with arbitrary instructions, which is\ncrucial for robots to generically perceive real scenarios and respond to\noperational changes. Existing methods focus on combining images or languages\nthat depict interactions with 3D geometries to introduce external interaction\npriors. However, they are still vulnerable to a limited semantic space by\nfailing to leverage implied invariant geometries and potential interaction\nintentions. Normally, humans address complex tasks through multi-step reasoning\nand respond to diverse situations by leveraging associative and analogical\nthinking. In light of this, we propose GREAT (GeometRy-intEntion collAboraTive\ninference) for Open-Vocabulary 3D Object Affordance Grounding, a novel\nframework that mines the object invariant geometry attributes and performs\nanalogically reason in potential interaction scenarios to form affordance\nknowledge, fully combining the knowledge with both geometries and visual\ncontents to ground 3D object affordance. Besides, we introduce the Point Image\nAffordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at\npresent to support the task. Extensive experiments demonstrate the\neffectiveness and superiority of GREAT. Code and dataset are available at\nproject.\n", "link": "http://arxiv.org/abs/2411.19626v1", "date": "2024-11-29", "relevancy": 2.3842, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5982}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5982}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GREAT%3A%20Geometry-Intention%20Collaborative%20Inference%20for%20Open-Vocabulary%203D%0A%20%20Object%20Affordance%20Grounding&body=Title%3A%20GREAT%3A%20Geometry-Intention%20Collaborative%20Inference%20for%20Open-Vocabulary%203D%0A%20%20Object%20Affordance%20Grounding%0AAuthor%3A%20Yawen%20Shao%20and%20Wei%20Zhai%20and%20Yuhang%20Yang%20and%20Hongchen%20Luo%20and%20Yang%20Cao%20and%20Zheng-Jun%20Zha%0AAbstract%3A%20%20%20Open-Vocabulary%203D%20object%20affordance%20grounding%20aims%20to%20anticipate%20%60%60action%0Apossibilities%27%27%20regions%20on%203D%20objects%20with%20arbitrary%20instructions%2C%20which%20is%0Acrucial%20for%20robots%20to%20generically%20perceive%20real%20scenarios%20and%20respond%20to%0Aoperational%20changes.%20Existing%20methods%20focus%20on%20combining%20images%20or%20languages%0Athat%20depict%20interactions%20with%203D%20geometries%20to%20introduce%20external%20interaction%0Apriors.%20However%2C%20they%20are%20still%20vulnerable%20to%20a%20limited%20semantic%20space%20by%0Afailing%20to%20leverage%20implied%20invariant%20geometries%20and%20potential%20interaction%0Aintentions.%20Normally%2C%20humans%20address%20complex%20tasks%20through%20multi-step%20reasoning%0Aand%20respond%20to%20diverse%20situations%20by%20leveraging%20associative%20and%20analogical%0Athinking.%20In%20light%20of%20this%2C%20we%20propose%20GREAT%20%28GeometRy-intEntion%20collAboraTive%0Ainference%29%20for%20Open-Vocabulary%203D%20Object%20Affordance%20Grounding%2C%20a%20novel%0Aframework%20that%20mines%20the%20object%20invariant%20geometry%20attributes%20and%20performs%0Aanalogically%20reason%20in%20potential%20interaction%20scenarios%20to%20form%20affordance%0Aknowledge%2C%20fully%20combining%20the%20knowledge%20with%20both%20geometries%20and%20visual%0Acontents%20to%20ground%203D%20object%20affordance.%20Besides%2C%20we%20introduce%20the%20Point%20Image%0AAffordance%20Dataset%20v2%20%28PIADv2%29%2C%20the%20largest%203D%20object%20affordance%20dataset%20at%0Apresent%20to%20support%20the%20task.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20GREAT.%20Code%20and%20dataset%20are%20available%20at%0Aproject.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGREAT%253A%2520Geometry-Intention%2520Collaborative%2520Inference%2520for%2520Open-Vocabulary%25203D%250A%2520%2520Object%2520Affordance%2520Grounding%26entry.906535625%3DYawen%2520Shao%2520and%2520Wei%2520Zhai%2520and%2520Yuhang%2520Yang%2520and%2520Hongchen%2520Luo%2520and%2520Yang%2520Cao%2520and%2520Zheng-Jun%2520Zha%26entry.1292438233%3D%2520%2520Open-Vocabulary%25203D%2520object%2520affordance%2520grounding%2520aims%2520to%2520anticipate%2520%2560%2560action%250Apossibilities%2527%2527%2520regions%2520on%25203D%2520objects%2520with%2520arbitrary%2520instructions%252C%2520which%2520is%250Acrucial%2520for%2520robots%2520to%2520generically%2520perceive%2520real%2520scenarios%2520and%2520respond%2520to%250Aoperational%2520changes.%2520Existing%2520methods%2520focus%2520on%2520combining%2520images%2520or%2520languages%250Athat%2520depict%2520interactions%2520with%25203D%2520geometries%2520to%2520introduce%2520external%2520interaction%250Apriors.%2520However%252C%2520they%2520are%2520still%2520vulnerable%2520to%2520a%2520limited%2520semantic%2520space%2520by%250Afailing%2520to%2520leverage%2520implied%2520invariant%2520geometries%2520and%2520potential%2520interaction%250Aintentions.%2520Normally%252C%2520humans%2520address%2520complex%2520tasks%2520through%2520multi-step%2520reasoning%250Aand%2520respond%2520to%2520diverse%2520situations%2520by%2520leveraging%2520associative%2520and%2520analogical%250Athinking.%2520In%2520light%2520of%2520this%252C%2520we%2520propose%2520GREAT%2520%2528GeometRy-intEntion%2520collAboraTive%250Ainference%2529%2520for%2520Open-Vocabulary%25203D%2520Object%2520Affordance%2520Grounding%252C%2520a%2520novel%250Aframework%2520that%2520mines%2520the%2520object%2520invariant%2520geometry%2520attributes%2520and%2520performs%250Aanalogically%2520reason%2520in%2520potential%2520interaction%2520scenarios%2520to%2520form%2520affordance%250Aknowledge%252C%2520fully%2520combining%2520the%2520knowledge%2520with%2520both%2520geometries%2520and%2520visual%250Acontents%2520to%2520ground%25203D%2520object%2520affordance.%2520Besides%252C%2520we%2520introduce%2520the%2520Point%2520Image%250AAffordance%2520Dataset%2520v2%2520%2528PIADv2%2529%252C%2520the%2520largest%25203D%2520object%2520affordance%2520dataset%2520at%250Apresent%2520to%2520support%2520the%2520task.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520and%2520superiority%2520of%2520GREAT.%2520Code%2520and%2520dataset%2520are%2520available%2520at%250Aproject.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GREAT%3A%20Geometry-Intention%20Collaborative%20Inference%20for%20Open-Vocabulary%203D%0A%20%20Object%20Affordance%20Grounding&entry.906535625=Yawen%20Shao%20and%20Wei%20Zhai%20and%20Yuhang%20Yang%20and%20Hongchen%20Luo%20and%20Yang%20Cao%20and%20Zheng-Jun%20Zha&entry.1292438233=%20%20Open-Vocabulary%203D%20object%20affordance%20grounding%20aims%20to%20anticipate%20%60%60action%0Apossibilities%27%27%20regions%20on%203D%20objects%20with%20arbitrary%20instructions%2C%20which%20is%0Acrucial%20for%20robots%20to%20generically%20perceive%20real%20scenarios%20and%20respond%20to%0Aoperational%20changes.%20Existing%20methods%20focus%20on%20combining%20images%20or%20languages%0Athat%20depict%20interactions%20with%203D%20geometries%20to%20introduce%20external%20interaction%0Apriors.%20However%2C%20they%20are%20still%20vulnerable%20to%20a%20limited%20semantic%20space%20by%0Afailing%20to%20leverage%20implied%20invariant%20geometries%20and%20potential%20interaction%0Aintentions.%20Normally%2C%20humans%20address%20complex%20tasks%20through%20multi-step%20reasoning%0Aand%20respond%20to%20diverse%20situations%20by%20leveraging%20associative%20and%20analogical%0Athinking.%20In%20light%20of%20this%2C%20we%20propose%20GREAT%20%28GeometRy-intEntion%20collAboraTive%0Ainference%29%20for%20Open-Vocabulary%203D%20Object%20Affordance%20Grounding%2C%20a%20novel%0Aframework%20that%20mines%20the%20object%20invariant%20geometry%20attributes%20and%20performs%0Aanalogically%20reason%20in%20potential%20interaction%20scenarios%20to%20form%20affordance%0Aknowledge%2C%20fully%20combining%20the%20knowledge%20with%20both%20geometries%20and%20visual%0Acontents%20to%20ground%203D%20object%20affordance.%20Besides%2C%20we%20introduce%20the%20Point%20Image%0AAffordance%20Dataset%20v2%20%28PIADv2%29%2C%20the%20largest%203D%20object%20affordance%20dataset%20at%0Apresent%20to%20support%20the%20task.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20and%20superiority%20of%20GREAT.%20Code%20and%20dataset%20are%20available%20at%0Aproject.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19626v1&entry.124074799=Read"},
{"title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text", "author": "Michael Tschannen and Andr\u00e9 Susano Pinto and Alexander Kolesnikov", "abstract": "  Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.\n", "link": "http://arxiv.org/abs/2411.19722v1", "date": "2024-11-29", "relevancy": 2.3596, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6799}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5816}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JetFormer%3A%20An%20Autoregressive%20Generative%20Model%20of%20Raw%20Images%20and%20Text&body=Title%3A%20JetFormer%3A%20An%20Autoregressive%20Generative%20Model%20of%20Raw%20Images%20and%20Text%0AAuthor%3A%20Michael%20Tschannen%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov%0AAbstract%3A%20%20%20Removing%20modeling%20constraints%20and%20unifying%20architectures%20across%20domains%20has%0Abeen%20a%20key%20driver%20of%20the%20recent%20progress%20in%20training%20large%20multimodal%20models.%0AHowever%2C%20most%20of%20these%20models%20still%20rely%20on%20many%20separately%20trained%20components%0Asuch%20as%20modality-specific%20encoders%20and%20decoders.%20In%20this%20work%2C%20we%20further%0Astreamline%20joint%20generative%20modeling%20of%20images%20and%20text.%20We%20propose%20an%0Aautoregressive%20decoder-only%20transformer%20-%20JetFormer%20-%20which%20is%20trained%20to%0Adirectly%20maximize%20the%20likelihood%20of%20raw%20data%2C%20without%20relying%20on%20any%20separately%0Apretrained%20components%2C%20and%20can%20understand%20and%20generate%20both%20text%20and%20images.%0ASpecifically%2C%20we%20leverage%20a%20normalizing%20flow%20model%20to%20obtain%20a%20soft-token%20image%0Arepresentation%20that%20is%20jointly%20trained%20with%20an%20autoregressive%20multimodal%0Atransformer.%20The%20normalizing%20flow%20model%20serves%20as%20both%20an%20image%20encoder%20for%0Aperception%20tasks%20and%20an%20image%20decoder%20for%20image%20generation%20tasks%20during%0Ainference.%20JetFormer%20achieves%20text-to-image%20generation%20quality%20competitive%20with%0Arecent%20VQ-VAE-%20and%20VAE-based%20baselines.%20These%20baselines%20rely%20on%20pretrained%0Aimage%20autoencoders%2C%20which%20are%20trained%20with%20a%20complex%20mixture%20of%20losses%2C%0Aincluding%20perceptual%20ones.%20At%20the%20same%20time%2C%20JetFormer%20demonstrates%20robust%0Aimage%20understanding%20capabilities.%20To%20the%20best%20of%20our%20knowledge%2C%20JetFormer%20is%0Athe%20first%20model%20that%20is%20capable%20of%20generating%20high-fidelity%20images%20and%0Aproducing%20strong%20log-likelihood%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJetFormer%253A%2520An%2520Autoregressive%2520Generative%2520Model%2520of%2520Raw%2520Images%2520and%2520Text%26entry.906535625%3DMichael%2520Tschannen%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Alexander%2520Kolesnikov%26entry.1292438233%3D%2520%2520Removing%2520modeling%2520constraints%2520and%2520unifying%2520architectures%2520across%2520domains%2520has%250Abeen%2520a%2520key%2520driver%2520of%2520the%2520recent%2520progress%2520in%2520training%2520large%2520multimodal%2520models.%250AHowever%252C%2520most%2520of%2520these%2520models%2520still%2520rely%2520on%2520many%2520separately%2520trained%2520components%250Asuch%2520as%2520modality-specific%2520encoders%2520and%2520decoders.%2520In%2520this%2520work%252C%2520we%2520further%250Astreamline%2520joint%2520generative%2520modeling%2520of%2520images%2520and%2520text.%2520We%2520propose%2520an%250Aautoregressive%2520decoder-only%2520transformer%2520-%2520JetFormer%2520-%2520which%2520is%2520trained%2520to%250Adirectly%2520maximize%2520the%2520likelihood%2520of%2520raw%2520data%252C%2520without%2520relying%2520on%2520any%2520separately%250Apretrained%2520components%252C%2520and%2520can%2520understand%2520and%2520generate%2520both%2520text%2520and%2520images.%250ASpecifically%252C%2520we%2520leverage%2520a%2520normalizing%2520flow%2520model%2520to%2520obtain%2520a%2520soft-token%2520image%250Arepresentation%2520that%2520is%2520jointly%2520trained%2520with%2520an%2520autoregressive%2520multimodal%250Atransformer.%2520The%2520normalizing%2520flow%2520model%2520serves%2520as%2520both%2520an%2520image%2520encoder%2520for%250Aperception%2520tasks%2520and%2520an%2520image%2520decoder%2520for%2520image%2520generation%2520tasks%2520during%250Ainference.%2520JetFormer%2520achieves%2520text-to-image%2520generation%2520quality%2520competitive%2520with%250Arecent%2520VQ-VAE-%2520and%2520VAE-based%2520baselines.%2520These%2520baselines%2520rely%2520on%2520pretrained%250Aimage%2520autoencoders%252C%2520which%2520are%2520trained%2520with%2520a%2520complex%2520mixture%2520of%2520losses%252C%250Aincluding%2520perceptual%2520ones.%2520At%2520the%2520same%2520time%252C%2520JetFormer%2520demonstrates%2520robust%250Aimage%2520understanding%2520capabilities.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520JetFormer%2520is%250Athe%2520first%2520model%2520that%2520is%2520capable%2520of%2520generating%2520high-fidelity%2520images%2520and%250Aproducing%2520strong%2520log-likelihood%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JetFormer%3A%20An%20Autoregressive%20Generative%20Model%20of%20Raw%20Images%20and%20Text&entry.906535625=Michael%20Tschannen%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov&entry.1292438233=%20%20Removing%20modeling%20constraints%20and%20unifying%20architectures%20across%20domains%20has%0Abeen%20a%20key%20driver%20of%20the%20recent%20progress%20in%20training%20large%20multimodal%20models.%0AHowever%2C%20most%20of%20these%20models%20still%20rely%20on%20many%20separately%20trained%20components%0Asuch%20as%20modality-specific%20encoders%20and%20decoders.%20In%20this%20work%2C%20we%20further%0Astreamline%20joint%20generative%20modeling%20of%20images%20and%20text.%20We%20propose%20an%0Aautoregressive%20decoder-only%20transformer%20-%20JetFormer%20-%20which%20is%20trained%20to%0Adirectly%20maximize%20the%20likelihood%20of%20raw%20data%2C%20without%20relying%20on%20any%20separately%0Apretrained%20components%2C%20and%20can%20understand%20and%20generate%20both%20text%20and%20images.%0ASpecifically%2C%20we%20leverage%20a%20normalizing%20flow%20model%20to%20obtain%20a%20soft-token%20image%0Arepresentation%20that%20is%20jointly%20trained%20with%20an%20autoregressive%20multimodal%0Atransformer.%20The%20normalizing%20flow%20model%20serves%20as%20both%20an%20image%20encoder%20for%0Aperception%20tasks%20and%20an%20image%20decoder%20for%20image%20generation%20tasks%20during%0Ainference.%20JetFormer%20achieves%20text-to-image%20generation%20quality%20competitive%20with%0Arecent%20VQ-VAE-%20and%20VAE-based%20baselines.%20These%20baselines%20rely%20on%20pretrained%0Aimage%20autoencoders%2C%20which%20are%20trained%20with%20a%20complex%20mixture%20of%20losses%2C%0Aincluding%20perceptual%20ones.%20At%20the%20same%20time%2C%20JetFormer%20demonstrates%20robust%0Aimage%20understanding%20capabilities.%20To%20the%20best%20of%20our%20knowledge%2C%20JetFormer%20is%0Athe%20first%20model%20that%20is%20capable%20of%20generating%20high-fidelity%20images%20and%0Aproducing%20strong%20log-likelihood%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19722v1&entry.124074799=Read"},
{"title": "Gaussian multi-target filtering with target dynamics driven by a\n  stochastic differential equation", "author": "\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez and Simo S\u00e4rkk\u00e4", "abstract": "  This paper proposes multi-target filtering algorithms in which target\ndynamics are given in continuous time and measurements are obtained at discrete\ntime instants. In particular, targets appear according to a Poisson point\nprocess (PPP) in time with a given Gaussian spatial distribution, targets move\naccording to a general time-invariant linear stochastic differential equation,\nand the life span of each target is modelled with an exponential distribution.\nFor this multi-target dynamic model, we derive the distribution of the set of\nnew born targets and calculate closed-form expressions for the best fitting\nmean and covariance of each target at its time of birth by minimising the\nKullback-Leibler divergence via moment matching. This yields a novel Gaussian\ncontinuous-discrete Poisson multi-Bernoulli mixture (PMBM) filter, and its\napproximations based on Poisson multi-Bernoulli and probability hypothesis\ndensity filtering. These continuous-discrete multi-target filters are also\nextended to target dynamics driven by nonlinear stochastic differential\nequations.\n", "link": "http://arxiv.org/abs/2411.19814v1", "date": "2024-11-29", "relevancy": 2.3458, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5035}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4611}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20multi-target%20filtering%20with%20target%20dynamics%20driven%20by%20a%0A%20%20stochastic%20differential%20equation&body=Title%3A%20Gaussian%20multi-target%20filtering%20with%20target%20dynamics%20driven%20by%20a%0A%20%20stochastic%20differential%20equation%0AAuthor%3A%20%C3%81ngel%20F.%20Garc%C3%ADa-Fern%C3%A1ndez%20and%20Simo%20S%C3%A4rkk%C3%A4%0AAbstract%3A%20%20%20This%20paper%20proposes%20multi-target%20filtering%20algorithms%20in%20which%20target%0Adynamics%20are%20given%20in%20continuous%20time%20and%20measurements%20are%20obtained%20at%20discrete%0Atime%20instants.%20In%20particular%2C%20targets%20appear%20according%20to%20a%20Poisson%20point%0Aprocess%20%28PPP%29%20in%20time%20with%20a%20given%20Gaussian%20spatial%20distribution%2C%20targets%20move%0Aaccording%20to%20a%20general%20time-invariant%20linear%20stochastic%20differential%20equation%2C%0Aand%20the%20life%20span%20of%20each%20target%20is%20modelled%20with%20an%20exponential%20distribution.%0AFor%20this%20multi-target%20dynamic%20model%2C%20we%20derive%20the%20distribution%20of%20the%20set%20of%0Anew%20born%20targets%20and%20calculate%20closed-form%20expressions%20for%20the%20best%20fitting%0Amean%20and%20covariance%20of%20each%20target%20at%20its%20time%20of%20birth%20by%20minimising%20the%0AKullback-Leibler%20divergence%20via%20moment%20matching.%20This%20yields%20a%20novel%20Gaussian%0Acontinuous-discrete%20Poisson%20multi-Bernoulli%20mixture%20%28PMBM%29%20filter%2C%20and%20its%0Aapproximations%20based%20on%20Poisson%20multi-Bernoulli%20and%20probability%20hypothesis%0Adensity%20filtering.%20These%20continuous-discrete%20multi-target%20filters%20are%20also%0Aextended%20to%20target%20dynamics%20driven%20by%20nonlinear%20stochastic%20differential%0Aequations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520multi-target%2520filtering%2520with%2520target%2520dynamics%2520driven%2520by%2520a%250A%2520%2520stochastic%2520differential%2520equation%26entry.906535625%3D%25C3%2581ngel%2520F.%2520Garc%25C3%25ADa-Fern%25C3%25A1ndez%2520and%2520Simo%2520S%25C3%25A4rkk%25C3%25A4%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520multi-target%2520filtering%2520algorithms%2520in%2520which%2520target%250Adynamics%2520are%2520given%2520in%2520continuous%2520time%2520and%2520measurements%2520are%2520obtained%2520at%2520discrete%250Atime%2520instants.%2520In%2520particular%252C%2520targets%2520appear%2520according%2520to%2520a%2520Poisson%2520point%250Aprocess%2520%2528PPP%2529%2520in%2520time%2520with%2520a%2520given%2520Gaussian%2520spatial%2520distribution%252C%2520targets%2520move%250Aaccording%2520to%2520a%2520general%2520time-invariant%2520linear%2520stochastic%2520differential%2520equation%252C%250Aand%2520the%2520life%2520span%2520of%2520each%2520target%2520is%2520modelled%2520with%2520an%2520exponential%2520distribution.%250AFor%2520this%2520multi-target%2520dynamic%2520model%252C%2520we%2520derive%2520the%2520distribution%2520of%2520the%2520set%2520of%250Anew%2520born%2520targets%2520and%2520calculate%2520closed-form%2520expressions%2520for%2520the%2520best%2520fitting%250Amean%2520and%2520covariance%2520of%2520each%2520target%2520at%2520its%2520time%2520of%2520birth%2520by%2520minimising%2520the%250AKullback-Leibler%2520divergence%2520via%2520moment%2520matching.%2520This%2520yields%2520a%2520novel%2520Gaussian%250Acontinuous-discrete%2520Poisson%2520multi-Bernoulli%2520mixture%2520%2528PMBM%2529%2520filter%252C%2520and%2520its%250Aapproximations%2520based%2520on%2520Poisson%2520multi-Bernoulli%2520and%2520probability%2520hypothesis%250Adensity%2520filtering.%2520These%2520continuous-discrete%2520multi-target%2520filters%2520are%2520also%250Aextended%2520to%2520target%2520dynamics%2520driven%2520by%2520nonlinear%2520stochastic%2520differential%250Aequations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20multi-target%20filtering%20with%20target%20dynamics%20driven%20by%20a%0A%20%20stochastic%20differential%20equation&entry.906535625=%C3%81ngel%20F.%20Garc%C3%ADa-Fern%C3%A1ndez%20and%20Simo%20S%C3%A4rkk%C3%A4&entry.1292438233=%20%20This%20paper%20proposes%20multi-target%20filtering%20algorithms%20in%20which%20target%0Adynamics%20are%20given%20in%20continuous%20time%20and%20measurements%20are%20obtained%20at%20discrete%0Atime%20instants.%20In%20particular%2C%20targets%20appear%20according%20to%20a%20Poisson%20point%0Aprocess%20%28PPP%29%20in%20time%20with%20a%20given%20Gaussian%20spatial%20distribution%2C%20targets%20move%0Aaccording%20to%20a%20general%20time-invariant%20linear%20stochastic%20differential%20equation%2C%0Aand%20the%20life%20span%20of%20each%20target%20is%20modelled%20with%20an%20exponential%20distribution.%0AFor%20this%20multi-target%20dynamic%20model%2C%20we%20derive%20the%20distribution%20of%20the%20set%20of%0Anew%20born%20targets%20and%20calculate%20closed-form%20expressions%20for%20the%20best%20fitting%0Amean%20and%20covariance%20of%20each%20target%20at%20its%20time%20of%20birth%20by%20minimising%20the%0AKullback-Leibler%20divergence%20via%20moment%20matching.%20This%20yields%20a%20novel%20Gaussian%0Acontinuous-discrete%20Poisson%20multi-Bernoulli%20mixture%20%28PMBM%29%20filter%2C%20and%20its%0Aapproximations%20based%20on%20Poisson%20multi-Bernoulli%20and%20probability%20hypothesis%0Adensity%20filtering.%20These%20continuous-discrete%20multi-target%20filters%20are%20also%0Aextended%20to%20target%20dynamics%20driven%20by%20nonlinear%20stochastic%20differential%0Aequations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19814v1&entry.124074799=Read"},
{"title": "CLIPArTT: Adaptation of CLIP to New Domains at Test Time", "author": "Gustavo Adolfo Vargas Hakim and David Osowiechi and Mehrdad Noori and Milad Cheraghalikhani and Ali Bahri and Moslem Yazdanpanah and Ismail Ben Ayed and Christian Desrosiers", "abstract": "  Pre-trained vision-language models (VLMs), exemplified by CLIP, demonstrate\nremarkable adaptability across zero-shot classification tasks without\nadditional training. However, their performance diminishes in the presence of\ndomain shifts. In this study, we introduce CLIP Adaptation duRing Test-Time\n(CLIPArTT), a fully test-time adaptation (TTA) approach for CLIP, which\ninvolves automatic text prompts construction during inference for their use as\ntext supervision. Our method employs a unique, minimally invasive text prompt\ntuning process, wherein multiple predicted classes are aggregated into a single\nnew text prompt, used as \\emph{pseudo label} to re-classify inputs in a\ntransductive manner. Additionally, we pioneer the standardization of TTA\nbenchmarks (e.g., TENT) in the realm of VLMs. Our findings demonstrate that,\nwithout requiring additional transformations nor new trainable modules,\nCLIPArTT enhances performance dynamically across non-corrupted datasets such as\nCIFAR-100, corrupted datasets like CIFAR-100-C and ImageNet-C, alongside\nsynthetic datasets such as VisDA-C. This research underscores the potential for\nimproving VLMs' adaptability through novel test-time strategies, offering\ninsights for robust performance across varied datasets and environments. The\ncode can be found at: https://github.com/dosowiechi/CLIPArTT.git\n", "link": "http://arxiv.org/abs/2405.00754v2", "date": "2024-11-29", "relevancy": 2.345, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6501}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5768}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIPArTT%3A%20Adaptation%20of%20CLIP%20to%20New%20Domains%20at%20Test%20Time&body=Title%3A%20CLIPArTT%3A%20Adaptation%20of%20CLIP%20to%20New%20Domains%20at%20Test%20Time%0AAuthor%3A%20Gustavo%20Adolfo%20Vargas%20Hakim%20and%20David%20Osowiechi%20and%20Mehrdad%20Noori%20and%20Milad%20Cheraghalikhani%20and%20Ali%20Bahri%20and%20Moslem%20Yazdanpanah%20and%20Ismail%20Ben%20Ayed%20and%20Christian%20Desrosiers%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%2C%20exemplified%20by%20CLIP%2C%20demonstrate%0Aremarkable%20adaptability%20across%20zero-shot%20classification%20tasks%20without%0Aadditional%20training.%20However%2C%20their%20performance%20diminishes%20in%20the%20presence%20of%0Adomain%20shifts.%20In%20this%20study%2C%20we%20introduce%20CLIP%20Adaptation%20duRing%20Test-Time%0A%28CLIPArTT%29%2C%20a%20fully%20test-time%20adaptation%20%28TTA%29%20approach%20for%20CLIP%2C%20which%0Ainvolves%20automatic%20text%20prompts%20construction%20during%20inference%20for%20their%20use%20as%0Atext%20supervision.%20Our%20method%20employs%20a%20unique%2C%20minimally%20invasive%20text%20prompt%0Atuning%20process%2C%20wherein%20multiple%20predicted%20classes%20are%20aggregated%20into%20a%20single%0Anew%20text%20prompt%2C%20used%20as%20%5Cemph%7Bpseudo%20label%7D%20to%20re-classify%20inputs%20in%20a%0Atransductive%20manner.%20Additionally%2C%20we%20pioneer%20the%20standardization%20of%20TTA%0Abenchmarks%20%28e.g.%2C%20TENT%29%20in%20the%20realm%20of%20VLMs.%20Our%20findings%20demonstrate%20that%2C%0Awithout%20requiring%20additional%20transformations%20nor%20new%20trainable%20modules%2C%0ACLIPArTT%20enhances%20performance%20dynamically%20across%20non-corrupted%20datasets%20such%20as%0ACIFAR-100%2C%20corrupted%20datasets%20like%20CIFAR-100-C%20and%20ImageNet-C%2C%20alongside%0Asynthetic%20datasets%20such%20as%20VisDA-C.%20This%20research%20underscores%20the%20potential%20for%0Aimproving%20VLMs%27%20adaptability%20through%20novel%20test-time%20strategies%2C%20offering%0Ainsights%20for%20robust%20performance%20across%20varied%20datasets%20and%20environments.%20The%0Acode%20can%20be%20found%20at%3A%20https%3A//github.com/dosowiechi/CLIPArTT.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIPArTT%253A%2520Adaptation%2520of%2520CLIP%2520to%2520New%2520Domains%2520at%2520Test%2520Time%26entry.906535625%3DGustavo%2520Adolfo%2520Vargas%2520Hakim%2520and%2520David%2520Osowiechi%2520and%2520Mehrdad%2520Noori%2520and%2520Milad%2520Cheraghalikhani%2520and%2520Ali%2520Bahri%2520and%2520Moslem%2520Yazdanpanah%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Christian%2520Desrosiers%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520exemplified%2520by%2520CLIP%252C%2520demonstrate%250Aremarkable%2520adaptability%2520across%2520zero-shot%2520classification%2520tasks%2520without%250Aadditional%2520training.%2520However%252C%2520their%2520performance%2520diminishes%2520in%2520the%2520presence%2520of%250Adomain%2520shifts.%2520In%2520this%2520study%252C%2520we%2520introduce%2520CLIP%2520Adaptation%2520duRing%2520Test-Time%250A%2528CLIPArTT%2529%252C%2520a%2520fully%2520test-time%2520adaptation%2520%2528TTA%2529%2520approach%2520for%2520CLIP%252C%2520which%250Ainvolves%2520automatic%2520text%2520prompts%2520construction%2520during%2520inference%2520for%2520their%2520use%2520as%250Atext%2520supervision.%2520Our%2520method%2520employs%2520a%2520unique%252C%2520minimally%2520invasive%2520text%2520prompt%250Atuning%2520process%252C%2520wherein%2520multiple%2520predicted%2520classes%2520are%2520aggregated%2520into%2520a%2520single%250Anew%2520text%2520prompt%252C%2520used%2520as%2520%255Cemph%257Bpseudo%2520label%257D%2520to%2520re-classify%2520inputs%2520in%2520a%250Atransductive%2520manner.%2520Additionally%252C%2520we%2520pioneer%2520the%2520standardization%2520of%2520TTA%250Abenchmarks%2520%2528e.g.%252C%2520TENT%2529%2520in%2520the%2520realm%2520of%2520VLMs.%2520Our%2520findings%2520demonstrate%2520that%252C%250Awithout%2520requiring%2520additional%2520transformations%2520nor%2520new%2520trainable%2520modules%252C%250ACLIPArTT%2520enhances%2520performance%2520dynamically%2520across%2520non-corrupted%2520datasets%2520such%2520as%250ACIFAR-100%252C%2520corrupted%2520datasets%2520like%2520CIFAR-100-C%2520and%2520ImageNet-C%252C%2520alongside%250Asynthetic%2520datasets%2520such%2520as%2520VisDA-C.%2520This%2520research%2520underscores%2520the%2520potential%2520for%250Aimproving%2520VLMs%2527%2520adaptability%2520through%2520novel%2520test-time%2520strategies%252C%2520offering%250Ainsights%2520for%2520robust%2520performance%2520across%2520varied%2520datasets%2520and%2520environments.%2520The%250Acode%2520can%2520be%2520found%2520at%253A%2520https%253A//github.com/dosowiechi/CLIPArTT.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIPArTT%3A%20Adaptation%20of%20CLIP%20to%20New%20Domains%20at%20Test%20Time&entry.906535625=Gustavo%20Adolfo%20Vargas%20Hakim%20and%20David%20Osowiechi%20and%20Mehrdad%20Noori%20and%20Milad%20Cheraghalikhani%20and%20Ali%20Bahri%20and%20Moslem%20Yazdanpanah%20and%20Ismail%20Ben%20Ayed%20and%20Christian%20Desrosiers&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%2C%20exemplified%20by%20CLIP%2C%20demonstrate%0Aremarkable%20adaptability%20across%20zero-shot%20classification%20tasks%20without%0Aadditional%20training.%20However%2C%20their%20performance%20diminishes%20in%20the%20presence%20of%0Adomain%20shifts.%20In%20this%20study%2C%20we%20introduce%20CLIP%20Adaptation%20duRing%20Test-Time%0A%28CLIPArTT%29%2C%20a%20fully%20test-time%20adaptation%20%28TTA%29%20approach%20for%20CLIP%2C%20which%0Ainvolves%20automatic%20text%20prompts%20construction%20during%20inference%20for%20their%20use%20as%0Atext%20supervision.%20Our%20method%20employs%20a%20unique%2C%20minimally%20invasive%20text%20prompt%0Atuning%20process%2C%20wherein%20multiple%20predicted%20classes%20are%20aggregated%20into%20a%20single%0Anew%20text%20prompt%2C%20used%20as%20%5Cemph%7Bpseudo%20label%7D%20to%20re-classify%20inputs%20in%20a%0Atransductive%20manner.%20Additionally%2C%20we%20pioneer%20the%20standardization%20of%20TTA%0Abenchmarks%20%28e.g.%2C%20TENT%29%20in%20the%20realm%20of%20VLMs.%20Our%20findings%20demonstrate%20that%2C%0Awithout%20requiring%20additional%20transformations%20nor%20new%20trainable%20modules%2C%0ACLIPArTT%20enhances%20performance%20dynamically%20across%20non-corrupted%20datasets%20such%20as%0ACIFAR-100%2C%20corrupted%20datasets%20like%20CIFAR-100-C%20and%20ImageNet-C%2C%20alongside%0Asynthetic%20datasets%20such%20as%20VisDA-C.%20This%20research%20underscores%20the%20potential%20for%0Aimproving%20VLMs%27%20adaptability%20through%20novel%20test-time%20strategies%2C%20offering%0Ainsights%20for%20robust%20performance%20across%20varied%20datasets%20and%20environments.%20The%0Acode%20can%20be%20found%20at%3A%20https%3A//github.com/dosowiechi/CLIPArTT.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00754v2&entry.124074799=Read"},
{"title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing\n  Cognition and Action in Robotic Manipulation", "author": "Qixiu Li and Yaobo Liang and Zeyu Wang and Lin Luo and Xi Chen and Mozheng Liao and Fangyun Wei and Yu Deng and Sicheng Xu and Yizhong Zhang and Xiaofan Wang and Bei Liu and Jianlong Fu and Jianmin Bao and Dong Chen and Yuanchun Shi and Jiaolong Yang and Baining Guo", "abstract": "  The advancement of large Vision-Language-Action (VLA) models has\nsignificantly improved robotic manipulation in terms of language-guided task\nexecution and generalization to unseen scenarios. While existing VLAs adapted\nfrom pretrained large Vision-Language-Models (VLM) have demonstrated promising\ngeneralizability, their task performance is still unsatisfactory as indicated\nby the low tasks success rates in different environments. In this paper, we\npresent a new advanced VLA architecture derived from VLM. Unlike previous works\nthat directly repurpose VLM for action prediction by simple action\nquantization, we propose a omponentized VLA architecture that has a specialized\naction module conditioned on VLM output. We systematically study the design of\nthe action module and demonstrates the strong performance enhancement with\ndiffusion action transformers for action sequence modeling, as well as their\nfavorable scaling behaviors. We also conduct comprehensive experiments and\nablation studies to evaluate the efficacy of our models with varied designs.\nThe evaluation on 5 robot embodiments in simulation and real work shows that\nour model not only significantly surpasses existing VLAs in task performance\nand but also exhibits remarkable adaptation to new robots and generalization to\nunseen objects and backgrounds. It exceeds the average success rates of OpenVLA\nwhich has similar model size (7B) with ours by over 35% in simulated evaluation\nand 55% in real robot experiments. It also outperforms the large RT-2-X model\n(55B) by 18% absolute success rates in simulation. Code and models can be found\non our project page (https://cogact.github.io/).\n", "link": "http://arxiv.org/abs/2411.19650v1", "date": "2024-11-29", "relevancy": 2.3305, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogACT%3A%20A%20Foundational%20Vision-Language-Action%20Model%20for%20Synergizing%0A%20%20Cognition%20and%20Action%20in%20Robotic%20Manipulation&body=Title%3A%20CogACT%3A%20A%20Foundational%20Vision-Language-Action%20Model%20for%20Synergizing%0A%20%20Cognition%20and%20Action%20in%20Robotic%20Manipulation%0AAuthor%3A%20Qixiu%20Li%20and%20Yaobo%20Liang%20and%20Zeyu%20Wang%20and%20Lin%20Luo%20and%20Xi%20Chen%20and%20Mozheng%20Liao%20and%20Fangyun%20Wei%20and%20Yu%20Deng%20and%20Sicheng%20Xu%20and%20Yizhong%20Zhang%20and%20Xiaofan%20Wang%20and%20Bei%20Liu%20and%20Jianlong%20Fu%20and%20Jianmin%20Bao%20and%20Dong%20Chen%20and%20Yuanchun%20Shi%20and%20Jiaolong%20Yang%20and%20Baining%20Guo%0AAbstract%3A%20%20%20The%20advancement%20of%20large%20Vision-Language-Action%20%28VLA%29%20models%20has%0Asignificantly%20improved%20robotic%20manipulation%20in%20terms%20of%20language-guided%20task%0Aexecution%20and%20generalization%20to%20unseen%20scenarios.%20While%20existing%20VLAs%20adapted%0Afrom%20pretrained%20large%20Vision-Language-Models%20%28VLM%29%20have%20demonstrated%20promising%0Ageneralizability%2C%20their%20task%20performance%20is%20still%20unsatisfactory%20as%20indicated%0Aby%20the%20low%20tasks%20success%20rates%20in%20different%20environments.%20In%20this%20paper%2C%20we%0Apresent%20a%20new%20advanced%20VLA%20architecture%20derived%20from%20VLM.%20Unlike%20previous%20works%0Athat%20directly%20repurpose%20VLM%20for%20action%20prediction%20by%20simple%20action%0Aquantization%2C%20we%20propose%20a%20omponentized%20VLA%20architecture%20that%20has%20a%20specialized%0Aaction%20module%20conditioned%20on%20VLM%20output.%20We%20systematically%20study%20the%20design%20of%0Athe%20action%20module%20and%20demonstrates%20the%20strong%20performance%20enhancement%20with%0Adiffusion%20action%20transformers%20for%20action%20sequence%20modeling%2C%20as%20well%20as%20their%0Afavorable%20scaling%20behaviors.%20We%20also%20conduct%20comprehensive%20experiments%20and%0Aablation%20studies%20to%20evaluate%20the%20efficacy%20of%20our%20models%20with%20varied%20designs.%0AThe%20evaluation%20on%205%20robot%20embodiments%20in%20simulation%20and%20real%20work%20shows%20that%0Aour%20model%20not%20only%20significantly%20surpasses%20existing%20VLAs%20in%20task%20performance%0Aand%20but%20also%20exhibits%20remarkable%20adaptation%20to%20new%20robots%20and%20generalization%20to%0Aunseen%20objects%20and%20backgrounds.%20It%20exceeds%20the%20average%20success%20rates%20of%20OpenVLA%0Awhich%20has%20similar%20model%20size%20%287B%29%20with%20ours%20by%20over%2035%25%20in%20simulated%20evaluation%0Aand%2055%25%20in%20real%20robot%20experiments.%20It%20also%20outperforms%20the%20large%20RT-2-X%20model%0A%2855B%29%20by%2018%25%20absolute%20success%20rates%20in%20simulation.%20Code%20and%20models%20can%20be%20found%0Aon%20our%20project%20page%20%28https%3A//cogact.github.io/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogACT%253A%2520A%2520Foundational%2520Vision-Language-Action%2520Model%2520for%2520Synergizing%250A%2520%2520Cognition%2520and%2520Action%2520in%2520Robotic%2520Manipulation%26entry.906535625%3DQixiu%2520Li%2520and%2520Yaobo%2520Liang%2520and%2520Zeyu%2520Wang%2520and%2520Lin%2520Luo%2520and%2520Xi%2520Chen%2520and%2520Mozheng%2520Liao%2520and%2520Fangyun%2520Wei%2520and%2520Yu%2520Deng%2520and%2520Sicheng%2520Xu%2520and%2520Yizhong%2520Zhang%2520and%2520Xiaofan%2520Wang%2520and%2520Bei%2520Liu%2520and%2520Jianlong%2520Fu%2520and%2520Jianmin%2520Bao%2520and%2520Dong%2520Chen%2520and%2520Yuanchun%2520Shi%2520and%2520Jiaolong%2520Yang%2520and%2520Baining%2520Guo%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520large%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520has%250Asignificantly%2520improved%2520robotic%2520manipulation%2520in%2520terms%2520of%2520language-guided%2520task%250Aexecution%2520and%2520generalization%2520to%2520unseen%2520scenarios.%2520While%2520existing%2520VLAs%2520adapted%250Afrom%2520pretrained%2520large%2520Vision-Language-Models%2520%2528VLM%2529%2520have%2520demonstrated%2520promising%250Ageneralizability%252C%2520their%2520task%2520performance%2520is%2520still%2520unsatisfactory%2520as%2520indicated%250Aby%2520the%2520low%2520tasks%2520success%2520rates%2520in%2520different%2520environments.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520new%2520advanced%2520VLA%2520architecture%2520derived%2520from%2520VLM.%2520Unlike%2520previous%2520works%250Athat%2520directly%2520repurpose%2520VLM%2520for%2520action%2520prediction%2520by%2520simple%2520action%250Aquantization%252C%2520we%2520propose%2520a%2520omponentized%2520VLA%2520architecture%2520that%2520has%2520a%2520specialized%250Aaction%2520module%2520conditioned%2520on%2520VLM%2520output.%2520We%2520systematically%2520study%2520the%2520design%2520of%250Athe%2520action%2520module%2520and%2520demonstrates%2520the%2520strong%2520performance%2520enhancement%2520with%250Adiffusion%2520action%2520transformers%2520for%2520action%2520sequence%2520modeling%252C%2520as%2520well%2520as%2520their%250Afavorable%2520scaling%2520behaviors.%2520We%2520also%2520conduct%2520comprehensive%2520experiments%2520and%250Aablation%2520studies%2520to%2520evaluate%2520the%2520efficacy%2520of%2520our%2520models%2520with%2520varied%2520designs.%250AThe%2520evaluation%2520on%25205%2520robot%2520embodiments%2520in%2520simulation%2520and%2520real%2520work%2520shows%2520that%250Aour%2520model%2520not%2520only%2520significantly%2520surpasses%2520existing%2520VLAs%2520in%2520task%2520performance%250Aand%2520but%2520also%2520exhibits%2520remarkable%2520adaptation%2520to%2520new%2520robots%2520and%2520generalization%2520to%250Aunseen%2520objects%2520and%2520backgrounds.%2520It%2520exceeds%2520the%2520average%2520success%2520rates%2520of%2520OpenVLA%250Awhich%2520has%2520similar%2520model%2520size%2520%25287B%2529%2520with%2520ours%2520by%2520over%252035%2525%2520in%2520simulated%2520evaluation%250Aand%252055%2525%2520in%2520real%2520robot%2520experiments.%2520It%2520also%2520outperforms%2520the%2520large%2520RT-2-X%2520model%250A%252855B%2529%2520by%252018%2525%2520absolute%2520success%2520rates%2520in%2520simulation.%2520Code%2520and%2520models%2520can%2520be%2520found%250Aon%2520our%2520project%2520page%2520%2528https%253A//cogact.github.io/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogACT%3A%20A%20Foundational%20Vision-Language-Action%20Model%20for%20Synergizing%0A%20%20Cognition%20and%20Action%20in%20Robotic%20Manipulation&entry.906535625=Qixiu%20Li%20and%20Yaobo%20Liang%20and%20Zeyu%20Wang%20and%20Lin%20Luo%20and%20Xi%20Chen%20and%20Mozheng%20Liao%20and%20Fangyun%20Wei%20and%20Yu%20Deng%20and%20Sicheng%20Xu%20and%20Yizhong%20Zhang%20and%20Xiaofan%20Wang%20and%20Bei%20Liu%20and%20Jianlong%20Fu%20and%20Jianmin%20Bao%20and%20Dong%20Chen%20and%20Yuanchun%20Shi%20and%20Jiaolong%20Yang%20and%20Baining%20Guo&entry.1292438233=%20%20The%20advancement%20of%20large%20Vision-Language-Action%20%28VLA%29%20models%20has%0Asignificantly%20improved%20robotic%20manipulation%20in%20terms%20of%20language-guided%20task%0Aexecution%20and%20generalization%20to%20unseen%20scenarios.%20While%20existing%20VLAs%20adapted%0Afrom%20pretrained%20large%20Vision-Language-Models%20%28VLM%29%20have%20demonstrated%20promising%0Ageneralizability%2C%20their%20task%20performance%20is%20still%20unsatisfactory%20as%20indicated%0Aby%20the%20low%20tasks%20success%20rates%20in%20different%20environments.%20In%20this%20paper%2C%20we%0Apresent%20a%20new%20advanced%20VLA%20architecture%20derived%20from%20VLM.%20Unlike%20previous%20works%0Athat%20directly%20repurpose%20VLM%20for%20action%20prediction%20by%20simple%20action%0Aquantization%2C%20we%20propose%20a%20omponentized%20VLA%20architecture%20that%20has%20a%20specialized%0Aaction%20module%20conditioned%20on%20VLM%20output.%20We%20systematically%20study%20the%20design%20of%0Athe%20action%20module%20and%20demonstrates%20the%20strong%20performance%20enhancement%20with%0Adiffusion%20action%20transformers%20for%20action%20sequence%20modeling%2C%20as%20well%20as%20their%0Afavorable%20scaling%20behaviors.%20We%20also%20conduct%20comprehensive%20experiments%20and%0Aablation%20studies%20to%20evaluate%20the%20efficacy%20of%20our%20models%20with%20varied%20designs.%0AThe%20evaluation%20on%205%20robot%20embodiments%20in%20simulation%20and%20real%20work%20shows%20that%0Aour%20model%20not%20only%20significantly%20surpasses%20existing%20VLAs%20in%20task%20performance%0Aand%20but%20also%20exhibits%20remarkable%20adaptation%20to%20new%20robots%20and%20generalization%20to%0Aunseen%20objects%20and%20backgrounds.%20It%20exceeds%20the%20average%20success%20rates%20of%20OpenVLA%0Awhich%20has%20similar%20model%20size%20%287B%29%20with%20ours%20by%20over%2035%25%20in%20simulated%20evaluation%0Aand%2055%25%20in%20real%20robot%20experiments.%20It%20also%20outperforms%20the%20large%20RT-2-X%20model%0A%2855B%29%20by%2018%25%20absolute%20success%20rates%20in%20simulation.%20Code%20and%20models%20can%20be%20found%0Aon%20our%20project%20page%20%28https%3A//cogact.github.io/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19650v1&entry.124074799=Read"},
{"title": "SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical\n  VQA Tasks", "author": "Kim-Celine Kahl and Selen Erkan and Jeremias Traub and Carsten T. L\u00fcth and Klaus Maier-Hein and Lena Maier-Hein and Paul F. Jaeger", "abstract": "  Vision-Language Models (VLMs) have great potential in medical tasks, like\nVisual Question Answering (VQA), where they could act as interactive assistants\nfor both patients and clinicians. Yet their robustness to distribution shifts\non unseen data remains a critical concern for safe deployment. Evaluating such\nrobustness requires a controlled experimental setup that allows for systematic\ninsights into the model's behavior. However, we demonstrate that current setups\nfail to offer sufficiently thorough evaluations, limiting their ability to\naccurately assess model robustness. To address this gap, our work introduces a\nnovel framework, called SURE-VQA, centered around three key requirements to\novercome the current pitfalls and systematically analyze the robustness of\nVLMs: 1) Since robustness on synthetic shifts does not necessarily translate to\nreal-world shifts, robustness should be measured on real-world shifts that are\ninherent to the VQA data; 2) Traditional token-matching metrics often fail to\ncapture underlying semantics, necessitating the use of large language models\n(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks\ninterpretability due to missing sanity baselines, thus meaningful baselines\nshould be reported that allow assessing the multimodal impact on the VLM. To\ndemonstrate the relevance of this framework, we conduct a study on the\nrobustness of various fine-tuning methods across three medical datasets with\nfour different types of distribution shifts. Our study reveals several\nimportant findings: 1) Sanity baselines that do not utilize image data can\nperform surprisingly well; 2) We confirm LoRA as the best-performing PEFT\nmethod; 3) No PEFT method consistently outperforms others in terms of\nrobustness to shifts. Code is provided at https://github.com/IML-DKFZ/sure-vqa.\n", "link": "http://arxiv.org/abs/2411.19688v1", "date": "2024-11-29", "relevancy": 2.3186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks&body=Title%3A%20SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks%0AAuthor%3A%20Kim-Celine%20Kahl%20and%20Selen%20Erkan%20and%20Jeremias%20Traub%20and%20Carsten%20T.%20L%C3%BCth%20and%20Klaus%20Maier-Hein%20and%20Lena%20Maier-Hein%20and%20Paul%20F.%20Jaeger%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20great%20potential%20in%20medical%20tasks%2C%20like%0AVisual%20Question%20Answering%20%28VQA%29%2C%20where%20they%20could%20act%20as%20interactive%20assistants%0Afor%20both%20patients%20and%20clinicians.%20Yet%20their%20robustness%20to%20distribution%20shifts%0Aon%20unseen%20data%20remains%20a%20critical%20concern%20for%20safe%20deployment.%20Evaluating%20such%0Arobustness%20requires%20a%20controlled%20experimental%20setup%20that%20allows%20for%20systematic%0Ainsights%20into%20the%20model%27s%20behavior.%20However%2C%20we%20demonstrate%20that%20current%20setups%0Afail%20to%20offer%20sufficiently%20thorough%20evaluations%2C%20limiting%20their%20ability%20to%0Aaccurately%20assess%20model%20robustness.%20To%20address%20this%20gap%2C%20our%20work%20introduces%20a%0Anovel%20framework%2C%20called%20SURE-VQA%2C%20centered%20around%20three%20key%20requirements%20to%0Aovercome%20the%20current%20pitfalls%20and%20systematically%20analyze%20the%20robustness%20of%0AVLMs%3A%201%29%20Since%20robustness%20on%20synthetic%20shifts%20does%20not%20necessarily%20translate%20to%0Areal-world%20shifts%2C%20robustness%20should%20be%20measured%20on%20real-world%20shifts%20that%20are%0Ainherent%20to%20the%20VQA%20data%3B%202%29%20Traditional%20token-matching%20metrics%20often%20fail%20to%0Acapture%20underlying%20semantics%2C%20necessitating%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20more%20accurate%20semantic%20evaluation%3B%203%29%20Model%20performance%20often%20lacks%0Ainterpretability%20due%20to%20missing%20sanity%20baselines%2C%20thus%20meaningful%20baselines%0Ashould%20be%20reported%20that%20allow%20assessing%20the%20multimodal%20impact%20on%20the%20VLM.%20To%0Ademonstrate%20the%20relevance%20of%20this%20framework%2C%20we%20conduct%20a%20study%20on%20the%0Arobustness%20of%20various%20fine-tuning%20methods%20across%20three%20medical%20datasets%20with%0Afour%20different%20types%20of%20distribution%20shifts.%20Our%20study%20reveals%20several%0Aimportant%20findings%3A%201%29%20Sanity%20baselines%20that%20do%20not%20utilize%20image%20data%20can%0Aperform%20surprisingly%20well%3B%202%29%20We%20confirm%20LoRA%20as%20the%20best-performing%20PEFT%0Amethod%3B%203%29%20No%20PEFT%20method%20consistently%20outperforms%20others%20in%20terms%20of%0Arobustness%20to%20shifts.%20Code%20is%20provided%20at%20https%3A//github.com/IML-DKFZ/sure-vqa.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSURE-VQA%253A%2520Systematic%2520Understanding%2520of%2520Robustness%2520Evaluation%2520in%2520Medical%250A%2520%2520VQA%2520Tasks%26entry.906535625%3DKim-Celine%2520Kahl%2520and%2520Selen%2520Erkan%2520and%2520Jeremias%2520Traub%2520and%2520Carsten%2520T.%2520L%25C3%25BCth%2520and%2520Klaus%2520Maier-Hein%2520and%2520Lena%2520Maier-Hein%2520and%2520Paul%2520F.%2520Jaeger%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520great%2520potential%2520in%2520medical%2520tasks%252C%2520like%250AVisual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520where%2520they%2520could%2520act%2520as%2520interactive%2520assistants%250Afor%2520both%2520patients%2520and%2520clinicians.%2520Yet%2520their%2520robustness%2520to%2520distribution%2520shifts%250Aon%2520unseen%2520data%2520remains%2520a%2520critical%2520concern%2520for%2520safe%2520deployment.%2520Evaluating%2520such%250Arobustness%2520requires%2520a%2520controlled%2520experimental%2520setup%2520that%2520allows%2520for%2520systematic%250Ainsights%2520into%2520the%2520model%2527s%2520behavior.%2520However%252C%2520we%2520demonstrate%2520that%2520current%2520setups%250Afail%2520to%2520offer%2520sufficiently%2520thorough%2520evaluations%252C%2520limiting%2520their%2520ability%2520to%250Aaccurately%2520assess%2520model%2520robustness.%2520To%2520address%2520this%2520gap%252C%2520our%2520work%2520introduces%2520a%250Anovel%2520framework%252C%2520called%2520SURE-VQA%252C%2520centered%2520around%2520three%2520key%2520requirements%2520to%250Aovercome%2520the%2520current%2520pitfalls%2520and%2520systematically%2520analyze%2520the%2520robustness%2520of%250AVLMs%253A%25201%2529%2520Since%2520robustness%2520on%2520synthetic%2520shifts%2520does%2520not%2520necessarily%2520translate%2520to%250Areal-world%2520shifts%252C%2520robustness%2520should%2520be%2520measured%2520on%2520real-world%2520shifts%2520that%2520are%250Ainherent%2520to%2520the%2520VQA%2520data%253B%25202%2529%2520Traditional%2520token-matching%2520metrics%2520often%2520fail%2520to%250Acapture%2520underlying%2520semantics%252C%2520necessitating%2520the%2520use%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520for%2520more%2520accurate%2520semantic%2520evaluation%253B%25203%2529%2520Model%2520performance%2520often%2520lacks%250Ainterpretability%2520due%2520to%2520missing%2520sanity%2520baselines%252C%2520thus%2520meaningful%2520baselines%250Ashould%2520be%2520reported%2520that%2520allow%2520assessing%2520the%2520multimodal%2520impact%2520on%2520the%2520VLM.%2520To%250Ademonstrate%2520the%2520relevance%2520of%2520this%2520framework%252C%2520we%2520conduct%2520a%2520study%2520on%2520the%250Arobustness%2520of%2520various%2520fine-tuning%2520methods%2520across%2520three%2520medical%2520datasets%2520with%250Afour%2520different%2520types%2520of%2520distribution%2520shifts.%2520Our%2520study%2520reveals%2520several%250Aimportant%2520findings%253A%25201%2529%2520Sanity%2520baselines%2520that%2520do%2520not%2520utilize%2520image%2520data%2520can%250Aperform%2520surprisingly%2520well%253B%25202%2529%2520We%2520confirm%2520LoRA%2520as%2520the%2520best-performing%2520PEFT%250Amethod%253B%25203%2529%2520No%2520PEFT%2520method%2520consistently%2520outperforms%2520others%2520in%2520terms%2520of%250Arobustness%2520to%2520shifts.%2520Code%2520is%2520provided%2520at%2520https%253A//github.com/IML-DKFZ/sure-vqa.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SURE-VQA%3A%20Systematic%20Understanding%20of%20Robustness%20Evaluation%20in%20Medical%0A%20%20VQA%20Tasks&entry.906535625=Kim-Celine%20Kahl%20and%20Selen%20Erkan%20and%20Jeremias%20Traub%20and%20Carsten%20T.%20L%C3%BCth%20and%20Klaus%20Maier-Hein%20and%20Lena%20Maier-Hein%20and%20Paul%20F.%20Jaeger&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20great%20potential%20in%20medical%20tasks%2C%20like%0AVisual%20Question%20Answering%20%28VQA%29%2C%20where%20they%20could%20act%20as%20interactive%20assistants%0Afor%20both%20patients%20and%20clinicians.%20Yet%20their%20robustness%20to%20distribution%20shifts%0Aon%20unseen%20data%20remains%20a%20critical%20concern%20for%20safe%20deployment.%20Evaluating%20such%0Arobustness%20requires%20a%20controlled%20experimental%20setup%20that%20allows%20for%20systematic%0Ainsights%20into%20the%20model%27s%20behavior.%20However%2C%20we%20demonstrate%20that%20current%20setups%0Afail%20to%20offer%20sufficiently%20thorough%20evaluations%2C%20limiting%20their%20ability%20to%0Aaccurately%20assess%20model%20robustness.%20To%20address%20this%20gap%2C%20our%20work%20introduces%20a%0Anovel%20framework%2C%20called%20SURE-VQA%2C%20centered%20around%20three%20key%20requirements%20to%0Aovercome%20the%20current%20pitfalls%20and%20systematically%20analyze%20the%20robustness%20of%0AVLMs%3A%201%29%20Since%20robustness%20on%20synthetic%20shifts%20does%20not%20necessarily%20translate%20to%0Areal-world%20shifts%2C%20robustness%20should%20be%20measured%20on%20real-world%20shifts%20that%20are%0Ainherent%20to%20the%20VQA%20data%3B%202%29%20Traditional%20token-matching%20metrics%20often%20fail%20to%0Acapture%20underlying%20semantics%2C%20necessitating%20the%20use%20of%20large%20language%20models%0A%28LLMs%29%20for%20more%20accurate%20semantic%20evaluation%3B%203%29%20Model%20performance%20often%20lacks%0Ainterpretability%20due%20to%20missing%20sanity%20baselines%2C%20thus%20meaningful%20baselines%0Ashould%20be%20reported%20that%20allow%20assessing%20the%20multimodal%20impact%20on%20the%20VLM.%20To%0Ademonstrate%20the%20relevance%20of%20this%20framework%2C%20we%20conduct%20a%20study%20on%20the%0Arobustness%20of%20various%20fine-tuning%20methods%20across%20three%20medical%20datasets%20with%0Afour%20different%20types%20of%20distribution%20shifts.%20Our%20study%20reveals%20several%0Aimportant%20findings%3A%201%29%20Sanity%20baselines%20that%20do%20not%20utilize%20image%20data%20can%0Aperform%20surprisingly%20well%3B%202%29%20We%20confirm%20LoRA%20as%20the%20best-performing%20PEFT%0Amethod%3B%203%29%20No%20PEFT%20method%20consistently%20outperforms%20others%20in%20terms%20of%0Arobustness%20to%20shifts.%20Code%20is%20provided%20at%20https%3A//github.com/IML-DKFZ/sure-vqa.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19688v1&entry.124074799=Read"},
{"title": "Rethinking the initialization of Momentum in Federated Learning with\n  Heterogeneous Data", "author": "Chenguang Xiao and Shuo Wang", "abstract": "  Data Heterogeneity is a major challenge of Federated Learning performance.\nRecently, momentum based optimization techniques have beed proved to be\neffective in mitigating the heterogeneity issue. Along with the model updates,\nthe momentum updates are transmitted to the server side and aggregated.\nTherefore, the local training initialized with a global momentum is guided by\nthe global history of the gradients. However, we spot a problem in the\ntraditional cumulation of the momentum which is suboptimal in the Federated\nLearning systems. The momentum used to weight less on the historical gradients\nand more on the recent gradients. This however, will engage more biased local\ngradients in the end of the local training. In this work, we propose a new way\nto calculate the estimated momentum used in local initialization. The proposed\nmethod is named as Reversed Momentum Federated Learning (RMFL). The key idea is\nto assign exponentially decayed weights to the gradients with the time going\nforward, which is on the contrary to the traditional momentum cumulation. The\neffectiveness of RMFL is evaluated on three popular benchmark datasets with\ndifferent heterogeneity levels.\n", "link": "http://arxiv.org/abs/2411.19798v1", "date": "2024-11-29", "relevancy": 2.3178, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4664}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.465}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20initialization%20of%20Momentum%20in%20Federated%20Learning%20with%0A%20%20Heterogeneous%20Data&body=Title%3A%20Rethinking%20the%20initialization%20of%20Momentum%20in%20Federated%20Learning%20with%0A%20%20Heterogeneous%20Data%0AAuthor%3A%20Chenguang%20Xiao%20and%20Shuo%20Wang%0AAbstract%3A%20%20%20Data%20Heterogeneity%20is%20a%20major%20challenge%20of%20Federated%20Learning%20performance.%0ARecently%2C%20momentum%20based%20optimization%20techniques%20have%20beed%20proved%20to%20be%0Aeffective%20in%20mitigating%20the%20heterogeneity%20issue.%20Along%20with%20the%20model%20updates%2C%0Athe%20momentum%20updates%20are%20transmitted%20to%20the%20server%20side%20and%20aggregated.%0ATherefore%2C%20the%20local%20training%20initialized%20with%20a%20global%20momentum%20is%20guided%20by%0Athe%20global%20history%20of%20the%20gradients.%20However%2C%20we%20spot%20a%20problem%20in%20the%0Atraditional%20cumulation%20of%20the%20momentum%20which%20is%20suboptimal%20in%20the%20Federated%0ALearning%20systems.%20The%20momentum%20used%20to%20weight%20less%20on%20the%20historical%20gradients%0Aand%20more%20on%20the%20recent%20gradients.%20This%20however%2C%20will%20engage%20more%20biased%20local%0Agradients%20in%20the%20end%20of%20the%20local%20training.%20In%20this%20work%2C%20we%20propose%20a%20new%20way%0Ato%20calculate%20the%20estimated%20momentum%20used%20in%20local%20initialization.%20The%20proposed%0Amethod%20is%20named%20as%20Reversed%20Momentum%20Federated%20Learning%20%28RMFL%29.%20The%20key%20idea%20is%0Ato%20assign%20exponentially%20decayed%20weights%20to%20the%20gradients%20with%20the%20time%20going%0Aforward%2C%20which%20is%20on%20the%20contrary%20to%20the%20traditional%20momentum%20cumulation.%20The%0Aeffectiveness%20of%20RMFL%20is%20evaluated%20on%20three%20popular%20benchmark%20datasets%20with%0Adifferent%20heterogeneity%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520initialization%2520of%2520Momentum%2520in%2520Federated%2520Learning%2520with%250A%2520%2520Heterogeneous%2520Data%26entry.906535625%3DChenguang%2520Xiao%2520and%2520Shuo%2520Wang%26entry.1292438233%3D%2520%2520Data%2520Heterogeneity%2520is%2520a%2520major%2520challenge%2520of%2520Federated%2520Learning%2520performance.%250ARecently%252C%2520momentum%2520based%2520optimization%2520techniques%2520have%2520beed%2520proved%2520to%2520be%250Aeffective%2520in%2520mitigating%2520the%2520heterogeneity%2520issue.%2520Along%2520with%2520the%2520model%2520updates%252C%250Athe%2520momentum%2520updates%2520are%2520transmitted%2520to%2520the%2520server%2520side%2520and%2520aggregated.%250ATherefore%252C%2520the%2520local%2520training%2520initialized%2520with%2520a%2520global%2520momentum%2520is%2520guided%2520by%250Athe%2520global%2520history%2520of%2520the%2520gradients.%2520However%252C%2520we%2520spot%2520a%2520problem%2520in%2520the%250Atraditional%2520cumulation%2520of%2520the%2520momentum%2520which%2520is%2520suboptimal%2520in%2520the%2520Federated%250ALearning%2520systems.%2520The%2520momentum%2520used%2520to%2520weight%2520less%2520on%2520the%2520historical%2520gradients%250Aand%2520more%2520on%2520the%2520recent%2520gradients.%2520This%2520however%252C%2520will%2520engage%2520more%2520biased%2520local%250Agradients%2520in%2520the%2520end%2520of%2520the%2520local%2520training.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520way%250Ato%2520calculate%2520the%2520estimated%2520momentum%2520used%2520in%2520local%2520initialization.%2520The%2520proposed%250Amethod%2520is%2520named%2520as%2520Reversed%2520Momentum%2520Federated%2520Learning%2520%2528RMFL%2529.%2520The%2520key%2520idea%2520is%250Ato%2520assign%2520exponentially%2520decayed%2520weights%2520to%2520the%2520gradients%2520with%2520the%2520time%2520going%250Aforward%252C%2520which%2520is%2520on%2520the%2520contrary%2520to%2520the%2520traditional%2520momentum%2520cumulation.%2520The%250Aeffectiveness%2520of%2520RMFL%2520is%2520evaluated%2520on%2520three%2520popular%2520benchmark%2520datasets%2520with%250Adifferent%2520heterogeneity%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20initialization%20of%20Momentum%20in%20Federated%20Learning%20with%0A%20%20Heterogeneous%20Data&entry.906535625=Chenguang%20Xiao%20and%20Shuo%20Wang&entry.1292438233=%20%20Data%20Heterogeneity%20is%20a%20major%20challenge%20of%20Federated%20Learning%20performance.%0ARecently%2C%20momentum%20based%20optimization%20techniques%20have%20beed%20proved%20to%20be%0Aeffective%20in%20mitigating%20the%20heterogeneity%20issue.%20Along%20with%20the%20model%20updates%2C%0Athe%20momentum%20updates%20are%20transmitted%20to%20the%20server%20side%20and%20aggregated.%0ATherefore%2C%20the%20local%20training%20initialized%20with%20a%20global%20momentum%20is%20guided%20by%0Athe%20global%20history%20of%20the%20gradients.%20However%2C%20we%20spot%20a%20problem%20in%20the%0Atraditional%20cumulation%20of%20the%20momentum%20which%20is%20suboptimal%20in%20the%20Federated%0ALearning%20systems.%20The%20momentum%20used%20to%20weight%20less%20on%20the%20historical%20gradients%0Aand%20more%20on%20the%20recent%20gradients.%20This%20however%2C%20will%20engage%20more%20biased%20local%0Agradients%20in%20the%20end%20of%20the%20local%20training.%20In%20this%20work%2C%20we%20propose%20a%20new%20way%0Ato%20calculate%20the%20estimated%20momentum%20used%20in%20local%20initialization.%20The%20proposed%0Amethod%20is%20named%20as%20Reversed%20Momentum%20Federated%20Learning%20%28RMFL%29.%20The%20key%20idea%20is%0Ato%20assign%20exponentially%20decayed%20weights%20to%20the%20gradients%20with%20the%20time%20going%0Aforward%2C%20which%20is%20on%20the%20contrary%20to%20the%20traditional%20momentum%20cumulation.%20The%0Aeffectiveness%20of%20RMFL%20is%20evaluated%20on%20three%20popular%20benchmark%20datasets%20with%0Adifferent%20heterogeneity%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19798v1&entry.124074799=Read"},
{"title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset\n  Distillation", "author": "Zhiqiang Shen and Ammar Sherif and Zeyuan Yin and Shitong Shao", "abstract": "  Recent advances in dataset distillation have led to solutions in two main\ndirections. The conventional batch-to-batch matching mechanism is ideal for\nsmall-scale datasets and includes bi-level optimization methods on models and\nsyntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like\ndistribution matching, gradient matching, and weight trajectory matching.\nConversely, batch-to-global matching typifies decoupled methods, which are\nparticularly advantageous for large-scale datasets. This approach has garnered\nsubstantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD,\nand CDA. A primary challenge with the second approach is the lack of diversity\namong syntheses within each class since samples are optimized independently and\nthe same global supervision signals are reused across different synthetic\nimages. In this study, we propose a new Diversity-driven EarlyLate Training\n(DELT) scheme to enhance the diversity of images in batch-to-global matching\nwith less computation. Our approach is conceptually simple yet effective, it\npartitions predefined IPC samples into smaller subtasks and employs local\noptimizations to distill each subset into distributions from distinct phases,\nreducing the uniformity induced by the unified optimization process. These\ndistilled images from the subtasks demonstrate effective generalization when\napplied to the entire task. We conduct extensive experiments on CIFAR,\nTiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the\nprevious state-of-the-art by 2$\\sim$5% on average across different datasets and\nIPCs (images per class), increasing diversity per class by more than 5% while\nreducing synthesis time by up to 39.3% for enhancing the training efficiency.\nCode is available at: https://github.com/VILA-Lab/DELT.\n", "link": "http://arxiv.org/abs/2411.19946v1", "date": "2024-11-29", "relevancy": 2.2962, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6009}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5809}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DELT%3A%20A%20Simple%20Diversity-driven%20EarlyLate%20Training%20for%20Dataset%0A%20%20Distillation&body=Title%3A%20DELT%3A%20A%20Simple%20Diversity-driven%20EarlyLate%20Training%20for%20Dataset%0A%20%20Distillation%0AAuthor%3A%20Zhiqiang%20Shen%20and%20Ammar%20Sherif%20and%20Zeyuan%20Yin%20and%20Shitong%20Shao%0AAbstract%3A%20%20%20Recent%20advances%20in%20dataset%20distillation%20have%20led%20to%20solutions%20in%20two%20main%0Adirections.%20The%20conventional%20batch-to-batch%20matching%20mechanism%20is%20ideal%20for%0Asmall-scale%20datasets%20and%20includes%20bi-level%20optimization%20methods%20on%20models%20and%0Asyntheses%2C%20such%20as%20FRePo%2C%20RCIG%2C%20and%20RaT-BPTT%2C%20as%20well%20as%20other%20methods%20like%0Adistribution%20matching%2C%20gradient%20matching%2C%20and%20weight%20trajectory%20matching.%0AConversely%2C%20batch-to-global%20matching%20typifies%20decoupled%20methods%2C%20which%20are%0Aparticularly%20advantageous%20for%20large-scale%20datasets.%20This%20approach%20has%20garnered%0Asubstantial%20interest%20within%20the%20community%2C%20as%20seen%20in%20SRe%24%5E2%24L%2C%20G-VBSM%2C%20WMDD%2C%0Aand%20CDA.%20A%20primary%20challenge%20with%20the%20second%20approach%20is%20the%20lack%20of%20diversity%0Aamong%20syntheses%20within%20each%20class%20since%20samples%20are%20optimized%20independently%20and%0Athe%20same%20global%20supervision%20signals%20are%20reused%20across%20different%20synthetic%0Aimages.%20In%20this%20study%2C%20we%20propose%20a%20new%20Diversity-driven%20EarlyLate%20Training%0A%28DELT%29%20scheme%20to%20enhance%20the%20diversity%20of%20images%20in%20batch-to-global%20matching%0Awith%20less%20computation.%20Our%20approach%20is%20conceptually%20simple%20yet%20effective%2C%20it%0Apartitions%20predefined%20IPC%20samples%20into%20smaller%20subtasks%20and%20employs%20local%0Aoptimizations%20to%20distill%20each%20subset%20into%20distributions%20from%20distinct%20phases%2C%0Areducing%20the%20uniformity%20induced%20by%20the%20unified%20optimization%20process.%20These%0Adistilled%20images%20from%20the%20subtasks%20demonstrate%20effective%20generalization%20when%0Aapplied%20to%20the%20entire%20task.%20We%20conduct%20extensive%20experiments%20on%20CIFAR%2C%0ATiny-ImageNet%2C%20ImageNet-1K%2C%20and%20its%20sub-datasets.%20Our%20approach%20outperforms%20the%0Aprevious%20state-of-the-art%20by%202%24%5Csim%245%25%20on%20average%20across%20different%20datasets%20and%0AIPCs%20%28images%20per%20class%29%2C%20increasing%20diversity%20per%20class%20by%20more%20than%205%25%20while%0Areducing%20synthesis%20time%20by%20up%20to%2039.3%25%20for%20enhancing%20the%20training%20efficiency.%0ACode%20is%20available%20at%3A%20https%3A//github.com/VILA-Lab/DELT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDELT%253A%2520A%2520Simple%2520Diversity-driven%2520EarlyLate%2520Training%2520for%2520Dataset%250A%2520%2520Distillation%26entry.906535625%3DZhiqiang%2520Shen%2520and%2520Ammar%2520Sherif%2520and%2520Zeyuan%2520Yin%2520and%2520Shitong%2520Shao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520dataset%2520distillation%2520have%2520led%2520to%2520solutions%2520in%2520two%2520main%250Adirections.%2520The%2520conventional%2520batch-to-batch%2520matching%2520mechanism%2520is%2520ideal%2520for%250Asmall-scale%2520datasets%2520and%2520includes%2520bi-level%2520optimization%2520methods%2520on%2520models%2520and%250Asyntheses%252C%2520such%2520as%2520FRePo%252C%2520RCIG%252C%2520and%2520RaT-BPTT%252C%2520as%2520well%2520as%2520other%2520methods%2520like%250Adistribution%2520matching%252C%2520gradient%2520matching%252C%2520and%2520weight%2520trajectory%2520matching.%250AConversely%252C%2520batch-to-global%2520matching%2520typifies%2520decoupled%2520methods%252C%2520which%2520are%250Aparticularly%2520advantageous%2520for%2520large-scale%2520datasets.%2520This%2520approach%2520has%2520garnered%250Asubstantial%2520interest%2520within%2520the%2520community%252C%2520as%2520seen%2520in%2520SRe%2524%255E2%2524L%252C%2520G-VBSM%252C%2520WMDD%252C%250Aand%2520CDA.%2520A%2520primary%2520challenge%2520with%2520the%2520second%2520approach%2520is%2520the%2520lack%2520of%2520diversity%250Aamong%2520syntheses%2520within%2520each%2520class%2520since%2520samples%2520are%2520optimized%2520independently%2520and%250Athe%2520same%2520global%2520supervision%2520signals%2520are%2520reused%2520across%2520different%2520synthetic%250Aimages.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520new%2520Diversity-driven%2520EarlyLate%2520Training%250A%2528DELT%2529%2520scheme%2520to%2520enhance%2520the%2520diversity%2520of%2520images%2520in%2520batch-to-global%2520matching%250Awith%2520less%2520computation.%2520Our%2520approach%2520is%2520conceptually%2520simple%2520yet%2520effective%252C%2520it%250Apartitions%2520predefined%2520IPC%2520samples%2520into%2520smaller%2520subtasks%2520and%2520employs%2520local%250Aoptimizations%2520to%2520distill%2520each%2520subset%2520into%2520distributions%2520from%2520distinct%2520phases%252C%250Areducing%2520the%2520uniformity%2520induced%2520by%2520the%2520unified%2520optimization%2520process.%2520These%250Adistilled%2520images%2520from%2520the%2520subtasks%2520demonstrate%2520effective%2520generalization%2520when%250Aapplied%2520to%2520the%2520entire%2520task.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520CIFAR%252C%250ATiny-ImageNet%252C%2520ImageNet-1K%252C%2520and%2520its%2520sub-datasets.%2520Our%2520approach%2520outperforms%2520the%250Aprevious%2520state-of-the-art%2520by%25202%2524%255Csim%25245%2525%2520on%2520average%2520across%2520different%2520datasets%2520and%250AIPCs%2520%2528images%2520per%2520class%2529%252C%2520increasing%2520diversity%2520per%2520class%2520by%2520more%2520than%25205%2525%2520while%250Areducing%2520synthesis%2520time%2520by%2520up%2520to%252039.3%2525%2520for%2520enhancing%2520the%2520training%2520efficiency.%250ACode%2520is%2520available%2520at%253A%2520https%253A//github.com/VILA-Lab/DELT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELT%3A%20A%20Simple%20Diversity-driven%20EarlyLate%20Training%20for%20Dataset%0A%20%20Distillation&entry.906535625=Zhiqiang%20Shen%20and%20Ammar%20Sherif%20and%20Zeyuan%20Yin%20and%20Shitong%20Shao&entry.1292438233=%20%20Recent%20advances%20in%20dataset%20distillation%20have%20led%20to%20solutions%20in%20two%20main%0Adirections.%20The%20conventional%20batch-to-batch%20matching%20mechanism%20is%20ideal%20for%0Asmall-scale%20datasets%20and%20includes%20bi-level%20optimization%20methods%20on%20models%20and%0Asyntheses%2C%20such%20as%20FRePo%2C%20RCIG%2C%20and%20RaT-BPTT%2C%20as%20well%20as%20other%20methods%20like%0Adistribution%20matching%2C%20gradient%20matching%2C%20and%20weight%20trajectory%20matching.%0AConversely%2C%20batch-to-global%20matching%20typifies%20decoupled%20methods%2C%20which%20are%0Aparticularly%20advantageous%20for%20large-scale%20datasets.%20This%20approach%20has%20garnered%0Asubstantial%20interest%20within%20the%20community%2C%20as%20seen%20in%20SRe%24%5E2%24L%2C%20G-VBSM%2C%20WMDD%2C%0Aand%20CDA.%20A%20primary%20challenge%20with%20the%20second%20approach%20is%20the%20lack%20of%20diversity%0Aamong%20syntheses%20within%20each%20class%20since%20samples%20are%20optimized%20independently%20and%0Athe%20same%20global%20supervision%20signals%20are%20reused%20across%20different%20synthetic%0Aimages.%20In%20this%20study%2C%20we%20propose%20a%20new%20Diversity-driven%20EarlyLate%20Training%0A%28DELT%29%20scheme%20to%20enhance%20the%20diversity%20of%20images%20in%20batch-to-global%20matching%0Awith%20less%20computation.%20Our%20approach%20is%20conceptually%20simple%20yet%20effective%2C%20it%0Apartitions%20predefined%20IPC%20samples%20into%20smaller%20subtasks%20and%20employs%20local%0Aoptimizations%20to%20distill%20each%20subset%20into%20distributions%20from%20distinct%20phases%2C%0Areducing%20the%20uniformity%20induced%20by%20the%20unified%20optimization%20process.%20These%0Adistilled%20images%20from%20the%20subtasks%20demonstrate%20effective%20generalization%20when%0Aapplied%20to%20the%20entire%20task.%20We%20conduct%20extensive%20experiments%20on%20CIFAR%2C%0ATiny-ImageNet%2C%20ImageNet-1K%2C%20and%20its%20sub-datasets.%20Our%20approach%20outperforms%20the%0Aprevious%20state-of-the-art%20by%202%24%5Csim%245%25%20on%20average%20across%20different%20datasets%20and%0AIPCs%20%28images%20per%20class%29%2C%20increasing%20diversity%20per%20class%20by%20more%20than%205%25%20while%0Areducing%20synthesis%20time%20by%20up%20to%2039.3%25%20for%20enhancing%20the%20training%20efficiency.%0ACode%20is%20available%20at%3A%20https%3A//github.com/VILA-Lab/DELT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19946v1&entry.124074799=Read"},
{"title": "EarthMarker: A Visual Prompting Multi-modal Large Language Model for\n  Remote Sensing", "author": "Wei Zhang and Miaoxin Cai and Tong Zhang and Jun Li and Yin Zhuang and Xuerui Mao", "abstract": "  Recent advances in prompt learning have allowed users to interact with\nartificial intelligence (AI) tools in multi-turn dialogue, enabling an\ninteractive understanding of images. However, it is difficult and inefficient\nto deliver information in complicated remote sensing (RS) scenarios using plain\nlanguage instructions alone, which would severely hinder deep comprehension of\nthe latent content in imagery. Besides, existing prompting strategies in\nnatural scenes are hard to apply to interpret the RS data due to significant\ndomain differences. To address these challenges, the first visual\nprompting-based multi-modal large language model (MLLM) named EarthMarker is\nproposed in the RS domain. EarthMarker is capable of interpreting RS imagery at\nthe image, region, and point levels by levering visual prompts (i.e., boxes and\npoints). Specifically, a shared visual encoding method is developed to\nestablish the spatial pattern interpretation relationships between the\nmulti-scale representations of input images and various visual prompts.\nSubsequently, the mixed visual-spatial representations are associated with\nlanguage instructions to construct joint prompts, enabling the interpretation\nof intricate content of RS imagery. Furthermore, to bridge the domain gap\nbetween natural and RS data, and effectively transfer domain-level knowledge\nfrom natural scenes to the RS domain, a cross-domain learning strategy is\ndeveloped to facilitate the RS imagery understanding. In addition, to tackle\nthe lack of RS visual prompting data, a dataset named RSVP featuring\nmulti-modal multi-granularity visual prompts instruction-following is\nconstructed. Our code and dataset are available at\nhttps://github.com/wivizhang/EarthMarker.\n", "link": "http://arxiv.org/abs/2407.13596v3", "date": "2024-11-29", "relevancy": 2.2688, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5702}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5702}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EarthMarker%3A%20A%20Visual%20Prompting%20Multi-modal%20Large%20Language%20Model%20for%0A%20%20Remote%20Sensing&body=Title%3A%20EarthMarker%3A%20A%20Visual%20Prompting%20Multi-modal%20Large%20Language%20Model%20for%0A%20%20Remote%20Sensing%0AAuthor%3A%20Wei%20Zhang%20and%20Miaoxin%20Cai%20and%20Tong%20Zhang%20and%20Jun%20Li%20and%20Yin%20Zhuang%20and%20Xuerui%20Mao%0AAbstract%3A%20%20%20Recent%20advances%20in%20prompt%20learning%20have%20allowed%20users%20to%20interact%20with%0Aartificial%20intelligence%20%28AI%29%20tools%20in%20multi-turn%20dialogue%2C%20enabling%20an%0Ainteractive%20understanding%20of%20images.%20However%2C%20it%20is%20difficult%20and%20inefficient%0Ato%20deliver%20information%20in%20complicated%20remote%20sensing%20%28RS%29%20scenarios%20using%20plain%0Alanguage%20instructions%20alone%2C%20which%20would%20severely%20hinder%20deep%20comprehension%20of%0Athe%20latent%20content%20in%20imagery.%20Besides%2C%20existing%20prompting%20strategies%20in%0Anatural%20scenes%20are%20hard%20to%20apply%20to%20interpret%20the%20RS%20data%20due%20to%20significant%0Adomain%20differences.%20To%20address%20these%20challenges%2C%20the%20first%20visual%0Aprompting-based%20multi-modal%20large%20language%20model%20%28MLLM%29%20named%20EarthMarker%20is%0Aproposed%20in%20the%20RS%20domain.%20EarthMarker%20is%20capable%20of%20interpreting%20RS%20imagery%20at%0Athe%20image%2C%20region%2C%20and%20point%20levels%20by%20levering%20visual%20prompts%20%28i.e.%2C%20boxes%20and%0Apoints%29.%20Specifically%2C%20a%20shared%20visual%20encoding%20method%20is%20developed%20to%0Aestablish%20the%20spatial%20pattern%20interpretation%20relationships%20between%20the%0Amulti-scale%20representations%20of%20input%20images%20and%20various%20visual%20prompts.%0ASubsequently%2C%20the%20mixed%20visual-spatial%20representations%20are%20associated%20with%0Alanguage%20instructions%20to%20construct%20joint%20prompts%2C%20enabling%20the%20interpretation%0Aof%20intricate%20content%20of%20RS%20imagery.%20Furthermore%2C%20to%20bridge%20the%20domain%20gap%0Abetween%20natural%20and%20RS%20data%2C%20and%20effectively%20transfer%20domain-level%20knowledge%0Afrom%20natural%20scenes%20to%20the%20RS%20domain%2C%20a%20cross-domain%20learning%20strategy%20is%0Adeveloped%20to%20facilitate%20the%20RS%20imagery%20understanding.%20In%20addition%2C%20to%20tackle%0Athe%20lack%20of%20RS%20visual%20prompting%20data%2C%20a%20dataset%20named%20RSVP%20featuring%0Amulti-modal%20multi-granularity%20visual%20prompts%20instruction-following%20is%0Aconstructed.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/wivizhang/EarthMarker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13596v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarthMarker%253A%2520A%2520Visual%2520Prompting%2520Multi-modal%2520Large%2520Language%2520Model%2520for%250A%2520%2520Remote%2520Sensing%26entry.906535625%3DWei%2520Zhang%2520and%2520Miaoxin%2520Cai%2520and%2520Tong%2520Zhang%2520and%2520Jun%2520Li%2520and%2520Yin%2520Zhuang%2520and%2520Xuerui%2520Mao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520prompt%2520learning%2520have%2520allowed%2520users%2520to%2520interact%2520with%250Aartificial%2520intelligence%2520%2528AI%2529%2520tools%2520in%2520multi-turn%2520dialogue%252C%2520enabling%2520an%250Ainteractive%2520understanding%2520of%2520images.%2520However%252C%2520it%2520is%2520difficult%2520and%2520inefficient%250Ato%2520deliver%2520information%2520in%2520complicated%2520remote%2520sensing%2520%2528RS%2529%2520scenarios%2520using%2520plain%250Alanguage%2520instructions%2520alone%252C%2520which%2520would%2520severely%2520hinder%2520deep%2520comprehension%2520of%250Athe%2520latent%2520content%2520in%2520imagery.%2520Besides%252C%2520existing%2520prompting%2520strategies%2520in%250Anatural%2520scenes%2520are%2520hard%2520to%2520apply%2520to%2520interpret%2520the%2520RS%2520data%2520due%2520to%2520significant%250Adomain%2520differences.%2520To%2520address%2520these%2520challenges%252C%2520the%2520first%2520visual%250Aprompting-based%2520multi-modal%2520large%2520language%2520model%2520%2528MLLM%2529%2520named%2520EarthMarker%2520is%250Aproposed%2520in%2520the%2520RS%2520domain.%2520EarthMarker%2520is%2520capable%2520of%2520interpreting%2520RS%2520imagery%2520at%250Athe%2520image%252C%2520region%252C%2520and%2520point%2520levels%2520by%2520levering%2520visual%2520prompts%2520%2528i.e.%252C%2520boxes%2520and%250Apoints%2529.%2520Specifically%252C%2520a%2520shared%2520visual%2520encoding%2520method%2520is%2520developed%2520to%250Aestablish%2520the%2520spatial%2520pattern%2520interpretation%2520relationships%2520between%2520the%250Amulti-scale%2520representations%2520of%2520input%2520images%2520and%2520various%2520visual%2520prompts.%250ASubsequently%252C%2520the%2520mixed%2520visual-spatial%2520representations%2520are%2520associated%2520with%250Alanguage%2520instructions%2520to%2520construct%2520joint%2520prompts%252C%2520enabling%2520the%2520interpretation%250Aof%2520intricate%2520content%2520of%2520RS%2520imagery.%2520Furthermore%252C%2520to%2520bridge%2520the%2520domain%2520gap%250Abetween%2520natural%2520and%2520RS%2520data%252C%2520and%2520effectively%2520transfer%2520domain-level%2520knowledge%250Afrom%2520natural%2520scenes%2520to%2520the%2520RS%2520domain%252C%2520a%2520cross-domain%2520learning%2520strategy%2520is%250Adeveloped%2520to%2520facilitate%2520the%2520RS%2520imagery%2520understanding.%2520In%2520addition%252C%2520to%2520tackle%250Athe%2520lack%2520of%2520RS%2520visual%2520prompting%2520data%252C%2520a%2520dataset%2520named%2520RSVP%2520featuring%250Amulti-modal%2520multi-granularity%2520visual%2520prompts%2520instruction-following%2520is%250Aconstructed.%2520Our%2520code%2520and%2520dataset%2520are%2520available%2520at%250Ahttps%253A//github.com/wivizhang/EarthMarker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13596v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthMarker%3A%20A%20Visual%20Prompting%20Multi-modal%20Large%20Language%20Model%20for%0A%20%20Remote%20Sensing&entry.906535625=Wei%20Zhang%20and%20Miaoxin%20Cai%20and%20Tong%20Zhang%20and%20Jun%20Li%20and%20Yin%20Zhuang%20and%20Xuerui%20Mao&entry.1292438233=%20%20Recent%20advances%20in%20prompt%20learning%20have%20allowed%20users%20to%20interact%20with%0Aartificial%20intelligence%20%28AI%29%20tools%20in%20multi-turn%20dialogue%2C%20enabling%20an%0Ainteractive%20understanding%20of%20images.%20However%2C%20it%20is%20difficult%20and%20inefficient%0Ato%20deliver%20information%20in%20complicated%20remote%20sensing%20%28RS%29%20scenarios%20using%20plain%0Alanguage%20instructions%20alone%2C%20which%20would%20severely%20hinder%20deep%20comprehension%20of%0Athe%20latent%20content%20in%20imagery.%20Besides%2C%20existing%20prompting%20strategies%20in%0Anatural%20scenes%20are%20hard%20to%20apply%20to%20interpret%20the%20RS%20data%20due%20to%20significant%0Adomain%20differences.%20To%20address%20these%20challenges%2C%20the%20first%20visual%0Aprompting-based%20multi-modal%20large%20language%20model%20%28MLLM%29%20named%20EarthMarker%20is%0Aproposed%20in%20the%20RS%20domain.%20EarthMarker%20is%20capable%20of%20interpreting%20RS%20imagery%20at%0Athe%20image%2C%20region%2C%20and%20point%20levels%20by%20levering%20visual%20prompts%20%28i.e.%2C%20boxes%20and%0Apoints%29.%20Specifically%2C%20a%20shared%20visual%20encoding%20method%20is%20developed%20to%0Aestablish%20the%20spatial%20pattern%20interpretation%20relationships%20between%20the%0Amulti-scale%20representations%20of%20input%20images%20and%20various%20visual%20prompts.%0ASubsequently%2C%20the%20mixed%20visual-spatial%20representations%20are%20associated%20with%0Alanguage%20instructions%20to%20construct%20joint%20prompts%2C%20enabling%20the%20interpretation%0Aof%20intricate%20content%20of%20RS%20imagery.%20Furthermore%2C%20to%20bridge%20the%20domain%20gap%0Abetween%20natural%20and%20RS%20data%2C%20and%20effectively%20transfer%20domain-level%20knowledge%0Afrom%20natural%20scenes%20to%20the%20RS%20domain%2C%20a%20cross-domain%20learning%20strategy%20is%0Adeveloped%20to%20facilitate%20the%20RS%20imagery%20understanding.%20In%20addition%2C%20to%20tackle%0Athe%20lack%20of%20RS%20visual%20prompting%20data%2C%20a%20dataset%20named%20RSVP%20featuring%0Amulti-modal%20multi-granularity%20visual%20prompts%20instruction-following%20is%0Aconstructed.%20Our%20code%20and%20dataset%20are%20available%20at%0Ahttps%3A//github.com/wivizhang/EarthMarker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13596v3&entry.124074799=Read"},
{"title": "Riemannian Denoising Score Matching for Molecular Structure Optimization\n  with Accurate Energy", "author": "Jeheon Woo and Seonghwan Kim and Jun Hyeong Kim and Woo Youn Kim", "abstract": "  This study introduces a modified score matching method aimed at generating\nmolecular structures with high energy accuracy. The denoising process of score\nmatching or diffusion models mirrors molecular structure optimization, where\nscores act like physical force fields that guide particles toward equilibrium\nstates. To achieve energetically accurate structures, it can be advantageous to\nhave the score closely approximate the gradient of the actual potential energy\nsurface. Unlike conventional methods that simply design the target score based\non structural differences in Euclidean space, we propose a Riemannian score\nmatching approach. This method represents molecular structures on a manifold\ndefined by physics-informed internal coordinates to efficiently mimic the\nenergy landscape, and performs noising and denoising within this space. Our\nmethod has been evaluated by refining several types of starting structures on\nthe QM9 and GEOM datasets, demonstrating that the proposed Riemannian score\nmatching method significantly improves the accuracy of the generated molecular\nstructures, attaining chemical accuracy. The implications of this study extend\nto various applications in computational chemistry, offering a robust tool for\naccurate molecular structure prediction.\n", "link": "http://arxiv.org/abs/2411.19769v1", "date": "2024-11-29", "relevancy": 2.2523, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4634}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4614}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Denoising%20Score%20Matching%20for%20Molecular%20Structure%20Optimization%0A%20%20with%20Accurate%20Energy&body=Title%3A%20Riemannian%20Denoising%20Score%20Matching%20for%20Molecular%20Structure%20Optimization%0A%20%20with%20Accurate%20Energy%0AAuthor%3A%20Jeheon%20Woo%20and%20Seonghwan%20Kim%20and%20Jun%20Hyeong%20Kim%20and%20Woo%20Youn%20Kim%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20modified%20score%20matching%20method%20aimed%20at%20generating%0Amolecular%20structures%20with%20high%20energy%20accuracy.%20The%20denoising%20process%20of%20score%0Amatching%20or%20diffusion%20models%20mirrors%20molecular%20structure%20optimization%2C%20where%0Ascores%20act%20like%20physical%20force%20fields%20that%20guide%20particles%20toward%20equilibrium%0Astates.%20To%20achieve%20energetically%20accurate%20structures%2C%20it%20can%20be%20advantageous%20to%0Ahave%20the%20score%20closely%20approximate%20the%20gradient%20of%20the%20actual%20potential%20energy%0Asurface.%20Unlike%20conventional%20methods%20that%20simply%20design%20the%20target%20score%20based%0Aon%20structural%20differences%20in%20Euclidean%20space%2C%20we%20propose%20a%20Riemannian%20score%0Amatching%20approach.%20This%20method%20represents%20molecular%20structures%20on%20a%20manifold%0Adefined%20by%20physics-informed%20internal%20coordinates%20to%20efficiently%20mimic%20the%0Aenergy%20landscape%2C%20and%20performs%20noising%20and%20denoising%20within%20this%20space.%20Our%0Amethod%20has%20been%20evaluated%20by%20refining%20several%20types%20of%20starting%20structures%20on%0Athe%20QM9%20and%20GEOM%20datasets%2C%20demonstrating%20that%20the%20proposed%20Riemannian%20score%0Amatching%20method%20significantly%20improves%20the%20accuracy%20of%20the%20generated%20molecular%0Astructures%2C%20attaining%20chemical%20accuracy.%20The%20implications%20of%20this%20study%20extend%0Ato%20various%20applications%20in%20computational%20chemistry%2C%20offering%20a%20robust%20tool%20for%0Aaccurate%20molecular%20structure%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Denoising%2520Score%2520Matching%2520for%2520Molecular%2520Structure%2520Optimization%250A%2520%2520with%2520Accurate%2520Energy%26entry.906535625%3DJeheon%2520Woo%2520and%2520Seonghwan%2520Kim%2520and%2520Jun%2520Hyeong%2520Kim%2520and%2520Woo%2520Youn%2520Kim%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520modified%2520score%2520matching%2520method%2520aimed%2520at%2520generating%250Amolecular%2520structures%2520with%2520high%2520energy%2520accuracy.%2520The%2520denoising%2520process%2520of%2520score%250Amatching%2520or%2520diffusion%2520models%2520mirrors%2520molecular%2520structure%2520optimization%252C%2520where%250Ascores%2520act%2520like%2520physical%2520force%2520fields%2520that%2520guide%2520particles%2520toward%2520equilibrium%250Astates.%2520To%2520achieve%2520energetically%2520accurate%2520structures%252C%2520it%2520can%2520be%2520advantageous%2520to%250Ahave%2520the%2520score%2520closely%2520approximate%2520the%2520gradient%2520of%2520the%2520actual%2520potential%2520energy%250Asurface.%2520Unlike%2520conventional%2520methods%2520that%2520simply%2520design%2520the%2520target%2520score%2520based%250Aon%2520structural%2520differences%2520in%2520Euclidean%2520space%252C%2520we%2520propose%2520a%2520Riemannian%2520score%250Amatching%2520approach.%2520This%2520method%2520represents%2520molecular%2520structures%2520on%2520a%2520manifold%250Adefined%2520by%2520physics-informed%2520internal%2520coordinates%2520to%2520efficiently%2520mimic%2520the%250Aenergy%2520landscape%252C%2520and%2520performs%2520noising%2520and%2520denoising%2520within%2520this%2520space.%2520Our%250Amethod%2520has%2520been%2520evaluated%2520by%2520refining%2520several%2520types%2520of%2520starting%2520structures%2520on%250Athe%2520QM9%2520and%2520GEOM%2520datasets%252C%2520demonstrating%2520that%2520the%2520proposed%2520Riemannian%2520score%250Amatching%2520method%2520significantly%2520improves%2520the%2520accuracy%2520of%2520the%2520generated%2520molecular%250Astructures%252C%2520attaining%2520chemical%2520accuracy.%2520The%2520implications%2520of%2520this%2520study%2520extend%250Ato%2520various%2520applications%2520in%2520computational%2520chemistry%252C%2520offering%2520a%2520robust%2520tool%2520for%250Aaccurate%2520molecular%2520structure%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Denoising%20Score%20Matching%20for%20Molecular%20Structure%20Optimization%0A%20%20with%20Accurate%20Energy&entry.906535625=Jeheon%20Woo%20and%20Seonghwan%20Kim%20and%20Jun%20Hyeong%20Kim%20and%20Woo%20Youn%20Kim&entry.1292438233=%20%20This%20study%20introduces%20a%20modified%20score%20matching%20method%20aimed%20at%20generating%0Amolecular%20structures%20with%20high%20energy%20accuracy.%20The%20denoising%20process%20of%20score%0Amatching%20or%20diffusion%20models%20mirrors%20molecular%20structure%20optimization%2C%20where%0Ascores%20act%20like%20physical%20force%20fields%20that%20guide%20particles%20toward%20equilibrium%0Astates.%20To%20achieve%20energetically%20accurate%20structures%2C%20it%20can%20be%20advantageous%20to%0Ahave%20the%20score%20closely%20approximate%20the%20gradient%20of%20the%20actual%20potential%20energy%0Asurface.%20Unlike%20conventional%20methods%20that%20simply%20design%20the%20target%20score%20based%0Aon%20structural%20differences%20in%20Euclidean%20space%2C%20we%20propose%20a%20Riemannian%20score%0Amatching%20approach.%20This%20method%20represents%20molecular%20structures%20on%20a%20manifold%0Adefined%20by%20physics-informed%20internal%20coordinates%20to%20efficiently%20mimic%20the%0Aenergy%20landscape%2C%20and%20performs%20noising%20and%20denoising%20within%20this%20space.%20Our%0Amethod%20has%20been%20evaluated%20by%20refining%20several%20types%20of%20starting%20structures%20on%0Athe%20QM9%20and%20GEOM%20datasets%2C%20demonstrating%20that%20the%20proposed%20Riemannian%20score%0Amatching%20method%20significantly%20improves%20the%20accuracy%20of%20the%20generated%20molecular%0Astructures%2C%20attaining%20chemical%20accuracy.%20The%20implications%20of%20this%20study%20extend%0Ato%20various%20applications%20in%20computational%20chemistry%2C%20offering%20a%20robust%20tool%20for%0Aaccurate%20molecular%20structure%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19769v1&entry.124074799=Read"},
{"title": "Embedded Hierarchical MPC for Autonomous Navigation", "author": "Dennis Benders and Johannes K\u00f6hler and Thijs Niesten and Robert Babu\u0161ka and Javier Alonso-Mora and Laura Ferranti", "abstract": "  To efficiently deploy robotic systems in society, mobile robots need to\nautonomously and safely move through complex environments. Nonlinear model\npredictive control (MPC) methods provide a natural way to find a dynamically\nfeasible trajectory through the environment without colliding with nearby\nobstacles. However, the limited computation power available on typical embedded\nrobotic systems, such as quadrotors, poses a challenge to running MPC in\nreal-time, including its most expensive tasks: constraints generation and\noptimization. To address this problem, we propose a novel hierarchical MPC\nscheme that consists of a planning and a tracking layer. The planner constructs\na trajectory with a long prediction horizon at a slow rate, while the tracker\nensures trajectory tracking at a relatively fast rate. We prove that the\nproposed framework avoids collisions and is recursively feasible. Furthermore,\nwe demonstrate its effectiveness in simulations and lab experiments with a\nquadrotor that needs to reach a goal position in a complex static environment.\nThe code is efficiently implemented on the quadrotor's embedded computer to\nensure real-time feasibility. Compared to a state-of-the-art single-layer MPC\nformulation, this allows us to increase the planning horizon by a factor of 5,\nwhich results in significantly better performance.\n", "link": "http://arxiv.org/abs/2406.11506v2", "date": "2024-11-29", "relevancy": 2.2312, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5822}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5777}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation&body=Title%3A%20Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation%0AAuthor%3A%20Dennis%20Benders%20and%20Johannes%20K%C3%B6hler%20and%20Thijs%20Niesten%20and%20Robert%20Babu%C5%A1ka%20and%20Javier%20Alonso-Mora%20and%20Laura%20Ferranti%0AAbstract%3A%20%20%20To%20efficiently%20deploy%20robotic%20systems%20in%20society%2C%20mobile%20robots%20need%20to%0Aautonomously%20and%20safely%20move%20through%20complex%20environments.%20Nonlinear%20model%0Apredictive%20control%20%28MPC%29%20methods%20provide%20a%20natural%20way%20to%20find%20a%20dynamically%0Afeasible%20trajectory%20through%20the%20environment%20without%20colliding%20with%20nearby%0Aobstacles.%20However%2C%20the%20limited%20computation%20power%20available%20on%20typical%20embedded%0Arobotic%20systems%2C%20such%20as%20quadrotors%2C%20poses%20a%20challenge%20to%20running%20MPC%20in%0Areal-time%2C%20including%20its%20most%20expensive%20tasks%3A%20constraints%20generation%20and%0Aoptimization.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20hierarchical%20MPC%0Ascheme%20that%20consists%20of%20a%20planning%20and%20a%20tracking%20layer.%20The%20planner%20constructs%0Aa%20trajectory%20with%20a%20long%20prediction%20horizon%20at%20a%20slow%20rate%2C%20while%20the%20tracker%0Aensures%20trajectory%20tracking%20at%20a%20relatively%20fast%20rate.%20We%20prove%20that%20the%0Aproposed%20framework%20avoids%20collisions%20and%20is%20recursively%20feasible.%20Furthermore%2C%0Awe%20demonstrate%20its%20effectiveness%20in%20simulations%20and%20lab%20experiments%20with%20a%0Aquadrotor%20that%20needs%20to%20reach%20a%20goal%20position%20in%20a%20complex%20static%20environment.%0AThe%20code%20is%20efficiently%20implemented%20on%20the%20quadrotor%27s%20embedded%20computer%20to%0Aensure%20real-time%20feasibility.%20Compared%20to%20a%20state-of-the-art%20single-layer%20MPC%0Aformulation%2C%20this%20allows%20us%20to%20increase%20the%20planning%20horizon%20by%20a%20factor%20of%205%2C%0Awhich%20results%20in%20significantly%20better%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11506v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedded%2520Hierarchical%2520MPC%2520for%2520Autonomous%2520Navigation%26entry.906535625%3DDennis%2520Benders%2520and%2520Johannes%2520K%25C3%25B6hler%2520and%2520Thijs%2520Niesten%2520and%2520Robert%2520Babu%25C5%25A1ka%2520and%2520Javier%2520Alonso-Mora%2520and%2520Laura%2520Ferranti%26entry.1292438233%3D%2520%2520To%2520efficiently%2520deploy%2520robotic%2520systems%2520in%2520society%252C%2520mobile%2520robots%2520need%2520to%250Aautonomously%2520and%2520safely%2520move%2520through%2520complex%2520environments.%2520Nonlinear%2520model%250Apredictive%2520control%2520%2528MPC%2529%2520methods%2520provide%2520a%2520natural%2520way%2520to%2520find%2520a%2520dynamically%250Afeasible%2520trajectory%2520through%2520the%2520environment%2520without%2520colliding%2520with%2520nearby%250Aobstacles.%2520However%252C%2520the%2520limited%2520computation%2520power%2520available%2520on%2520typical%2520embedded%250Arobotic%2520systems%252C%2520such%2520as%2520quadrotors%252C%2520poses%2520a%2520challenge%2520to%2520running%2520MPC%2520in%250Areal-time%252C%2520including%2520its%2520most%2520expensive%2520tasks%253A%2520constraints%2520generation%2520and%250Aoptimization.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%2520MPC%250Ascheme%2520that%2520consists%2520of%2520a%2520planning%2520and%2520a%2520tracking%2520layer.%2520The%2520planner%2520constructs%250Aa%2520trajectory%2520with%2520a%2520long%2520prediction%2520horizon%2520at%2520a%2520slow%2520rate%252C%2520while%2520the%2520tracker%250Aensures%2520trajectory%2520tracking%2520at%2520a%2520relatively%2520fast%2520rate.%2520We%2520prove%2520that%2520the%250Aproposed%2520framework%2520avoids%2520collisions%2520and%2520is%2520recursively%2520feasible.%2520Furthermore%252C%250Awe%2520demonstrate%2520its%2520effectiveness%2520in%2520simulations%2520and%2520lab%2520experiments%2520with%2520a%250Aquadrotor%2520that%2520needs%2520to%2520reach%2520a%2520goal%2520position%2520in%2520a%2520complex%2520static%2520environment.%250AThe%2520code%2520is%2520efficiently%2520implemented%2520on%2520the%2520quadrotor%2527s%2520embedded%2520computer%2520to%250Aensure%2520real-time%2520feasibility.%2520Compared%2520to%2520a%2520state-of-the-art%2520single-layer%2520MPC%250Aformulation%252C%2520this%2520allows%2520us%2520to%2520increase%2520the%2520planning%2520horizon%2520by%2520a%2520factor%2520of%25205%252C%250Awhich%2520results%2520in%2520significantly%2520better%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11506v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation&entry.906535625=Dennis%20Benders%20and%20Johannes%20K%C3%B6hler%20and%20Thijs%20Niesten%20and%20Robert%20Babu%C5%A1ka%20and%20Javier%20Alonso-Mora%20and%20Laura%20Ferranti&entry.1292438233=%20%20To%20efficiently%20deploy%20robotic%20systems%20in%20society%2C%20mobile%20robots%20need%20to%0Aautonomously%20and%20safely%20move%20through%20complex%20environments.%20Nonlinear%20model%0Apredictive%20control%20%28MPC%29%20methods%20provide%20a%20natural%20way%20to%20find%20a%20dynamically%0Afeasible%20trajectory%20through%20the%20environment%20without%20colliding%20with%20nearby%0Aobstacles.%20However%2C%20the%20limited%20computation%20power%20available%20on%20typical%20embedded%0Arobotic%20systems%2C%20such%20as%20quadrotors%2C%20poses%20a%20challenge%20to%20running%20MPC%20in%0Areal-time%2C%20including%20its%20most%20expensive%20tasks%3A%20constraints%20generation%20and%0Aoptimization.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20hierarchical%20MPC%0Ascheme%20that%20consists%20of%20a%20planning%20and%20a%20tracking%20layer.%20The%20planner%20constructs%0Aa%20trajectory%20with%20a%20long%20prediction%20horizon%20at%20a%20slow%20rate%2C%20while%20the%20tracker%0Aensures%20trajectory%20tracking%20at%20a%20relatively%20fast%20rate.%20We%20prove%20that%20the%0Aproposed%20framework%20avoids%20collisions%20and%20is%20recursively%20feasible.%20Furthermore%2C%0Awe%20demonstrate%20its%20effectiveness%20in%20simulations%20and%20lab%20experiments%20with%20a%0Aquadrotor%20that%20needs%20to%20reach%20a%20goal%20position%20in%20a%20complex%20static%20environment.%0AThe%20code%20is%20efficiently%20implemented%20on%20the%20quadrotor%27s%20embedded%20computer%20to%0Aensure%20real-time%20feasibility.%20Compared%20to%20a%20state-of-the-art%20single-layer%20MPC%0Aformulation%2C%20this%20allows%20us%20to%20increase%20the%20planning%20horizon%20by%20a%20factor%20of%205%2C%0Awhich%20results%20in%20significantly%20better%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11506v2&entry.124074799=Read"},
{"title": "Dynamic EEG-fMRI mapping: Revealing the relationship between brain\n  connectivity and cognitive state", "author": "Guiran Liu and Binrong Zhu", "abstract": "  This study investigated the dynamic connectivity patterns between EEG and\nfMRI modalities, contributing to our understanding of brain network\ninteractions. By employing a comprehensive approach that integrated static and\ndynamic analyses of EEG-fMRI data, we were able to uncover distinct\nconnectivity states and characterize their temporal fluctuations. The results\nrevealed modular organization within the intrinsic connectivity networks (ICNs)\nof the brain, highlighting the significant roles of sensory systems and the\ndefault mode network. The use of a sliding window technique allowed us to\nassess how functional connectivity varies over time, further elucidating the\ntransient nature of brain connectivity. Additionally, our findings align with\nprevious literature, reinforcing the notion that cognitive states can be\neffectively identified through short-duration data, specifically within the\n30-60 second timeframe. The established relationships between connectivity\nstrength and cognitive processes, particularly during different visual states,\nunderscore the relevance of our approach for future research into brain\ndynamics. Overall, this study not only enhances our understanding of the\ninterplay between EEG and fMRI signals but also paves the way for further\nexploration into the neural correlates of cognitive functions and their\nimplications in clinical settings. Future research should focus on refining\nthese methodologies and exploring their applications in various cognitive and\nclinical contexts.\n", "link": "http://arxiv.org/abs/2411.19922v1", "date": "2024-11-29", "relevancy": 2.2233, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4451}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20EEG-fMRI%20mapping%3A%20Revealing%20the%20relationship%20between%20brain%0A%20%20connectivity%20and%20cognitive%20state&body=Title%3A%20Dynamic%20EEG-fMRI%20mapping%3A%20Revealing%20the%20relationship%20between%20brain%0A%20%20connectivity%20and%20cognitive%20state%0AAuthor%3A%20Guiran%20Liu%20and%20Binrong%20Zhu%0AAbstract%3A%20%20%20This%20study%20investigated%20the%20dynamic%20connectivity%20patterns%20between%20EEG%20and%0AfMRI%20modalities%2C%20contributing%20to%20our%20understanding%20of%20brain%20network%0Ainteractions.%20By%20employing%20a%20comprehensive%20approach%20that%20integrated%20static%20and%0Adynamic%20analyses%20of%20EEG-fMRI%20data%2C%20we%20were%20able%20to%20uncover%20distinct%0Aconnectivity%20states%20and%20characterize%20their%20temporal%20fluctuations.%20The%20results%0Arevealed%20modular%20organization%20within%20the%20intrinsic%20connectivity%20networks%20%28ICNs%29%0Aof%20the%20brain%2C%20highlighting%20the%20significant%20roles%20of%20sensory%20systems%20and%20the%0Adefault%20mode%20network.%20The%20use%20of%20a%20sliding%20window%20technique%20allowed%20us%20to%0Aassess%20how%20functional%20connectivity%20varies%20over%20time%2C%20further%20elucidating%20the%0Atransient%20nature%20of%20brain%20connectivity.%20Additionally%2C%20our%20findings%20align%20with%0Aprevious%20literature%2C%20reinforcing%20the%20notion%20that%20cognitive%20states%20can%20be%0Aeffectively%20identified%20through%20short-duration%20data%2C%20specifically%20within%20the%0A30-60%20second%20timeframe.%20The%20established%20relationships%20between%20connectivity%0Astrength%20and%20cognitive%20processes%2C%20particularly%20during%20different%20visual%20states%2C%0Aunderscore%20the%20relevance%20of%20our%20approach%20for%20future%20research%20into%20brain%0Adynamics.%20Overall%2C%20this%20study%20not%20only%20enhances%20our%20understanding%20of%20the%0Ainterplay%20between%20EEG%20and%20fMRI%20signals%20but%20also%20paves%20the%20way%20for%20further%0Aexploration%20into%20the%20neural%20correlates%20of%20cognitive%20functions%20and%20their%0Aimplications%20in%20clinical%20settings.%20Future%20research%20should%20focus%20on%20refining%0Athese%20methodologies%20and%20exploring%20their%20applications%20in%20various%20cognitive%20and%0Aclinical%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520EEG-fMRI%2520mapping%253A%2520Revealing%2520the%2520relationship%2520between%2520brain%250A%2520%2520connectivity%2520and%2520cognitive%2520state%26entry.906535625%3DGuiran%2520Liu%2520and%2520Binrong%2520Zhu%26entry.1292438233%3D%2520%2520This%2520study%2520investigated%2520the%2520dynamic%2520connectivity%2520patterns%2520between%2520EEG%2520and%250AfMRI%2520modalities%252C%2520contributing%2520to%2520our%2520understanding%2520of%2520brain%2520network%250Ainteractions.%2520By%2520employing%2520a%2520comprehensive%2520approach%2520that%2520integrated%2520static%2520and%250Adynamic%2520analyses%2520of%2520EEG-fMRI%2520data%252C%2520we%2520were%2520able%2520to%2520uncover%2520distinct%250Aconnectivity%2520states%2520and%2520characterize%2520their%2520temporal%2520fluctuations.%2520The%2520results%250Arevealed%2520modular%2520organization%2520within%2520the%2520intrinsic%2520connectivity%2520networks%2520%2528ICNs%2529%250Aof%2520the%2520brain%252C%2520highlighting%2520the%2520significant%2520roles%2520of%2520sensory%2520systems%2520and%2520the%250Adefault%2520mode%2520network.%2520The%2520use%2520of%2520a%2520sliding%2520window%2520technique%2520allowed%2520us%2520to%250Aassess%2520how%2520functional%2520connectivity%2520varies%2520over%2520time%252C%2520further%2520elucidating%2520the%250Atransient%2520nature%2520of%2520brain%2520connectivity.%2520Additionally%252C%2520our%2520findings%2520align%2520with%250Aprevious%2520literature%252C%2520reinforcing%2520the%2520notion%2520that%2520cognitive%2520states%2520can%2520be%250Aeffectively%2520identified%2520through%2520short-duration%2520data%252C%2520specifically%2520within%2520the%250A30-60%2520second%2520timeframe.%2520The%2520established%2520relationships%2520between%2520connectivity%250Astrength%2520and%2520cognitive%2520processes%252C%2520particularly%2520during%2520different%2520visual%2520states%252C%250Aunderscore%2520the%2520relevance%2520of%2520our%2520approach%2520for%2520future%2520research%2520into%2520brain%250Adynamics.%2520Overall%252C%2520this%2520study%2520not%2520only%2520enhances%2520our%2520understanding%2520of%2520the%250Ainterplay%2520between%2520EEG%2520and%2520fMRI%2520signals%2520but%2520also%2520paves%2520the%2520way%2520for%2520further%250Aexploration%2520into%2520the%2520neural%2520correlates%2520of%2520cognitive%2520functions%2520and%2520their%250Aimplications%2520in%2520clinical%2520settings.%2520Future%2520research%2520should%2520focus%2520on%2520refining%250Athese%2520methodologies%2520and%2520exploring%2520their%2520applications%2520in%2520various%2520cognitive%2520and%250Aclinical%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20EEG-fMRI%20mapping%3A%20Revealing%20the%20relationship%20between%20brain%0A%20%20connectivity%20and%20cognitive%20state&entry.906535625=Guiran%20Liu%20and%20Binrong%20Zhu&entry.1292438233=%20%20This%20study%20investigated%20the%20dynamic%20connectivity%20patterns%20between%20EEG%20and%0AfMRI%20modalities%2C%20contributing%20to%20our%20understanding%20of%20brain%20network%0Ainteractions.%20By%20employing%20a%20comprehensive%20approach%20that%20integrated%20static%20and%0Adynamic%20analyses%20of%20EEG-fMRI%20data%2C%20we%20were%20able%20to%20uncover%20distinct%0Aconnectivity%20states%20and%20characterize%20their%20temporal%20fluctuations.%20The%20results%0Arevealed%20modular%20organization%20within%20the%20intrinsic%20connectivity%20networks%20%28ICNs%29%0Aof%20the%20brain%2C%20highlighting%20the%20significant%20roles%20of%20sensory%20systems%20and%20the%0Adefault%20mode%20network.%20The%20use%20of%20a%20sliding%20window%20technique%20allowed%20us%20to%0Aassess%20how%20functional%20connectivity%20varies%20over%20time%2C%20further%20elucidating%20the%0Atransient%20nature%20of%20brain%20connectivity.%20Additionally%2C%20our%20findings%20align%20with%0Aprevious%20literature%2C%20reinforcing%20the%20notion%20that%20cognitive%20states%20can%20be%0Aeffectively%20identified%20through%20short-duration%20data%2C%20specifically%20within%20the%0A30-60%20second%20timeframe.%20The%20established%20relationships%20between%20connectivity%0Astrength%20and%20cognitive%20processes%2C%20particularly%20during%20different%20visual%20states%2C%0Aunderscore%20the%20relevance%20of%20our%20approach%20for%20future%20research%20into%20brain%0Adynamics.%20Overall%2C%20this%20study%20not%20only%20enhances%20our%20understanding%20of%20the%0Ainterplay%20between%20EEG%20and%20fMRI%20signals%20but%20also%20paves%20the%20way%20for%20further%0Aexploration%20into%20the%20neural%20correlates%20of%20cognitive%20functions%20and%20their%0Aimplications%20in%20clinical%20settings.%20Future%20research%20should%20focus%20on%20refining%0Athese%20methodologies%20and%20exploring%20their%20applications%20in%20various%20cognitive%20and%0Aclinical%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19922v1&entry.124074799=Read"},
{"title": "Relative Representations of Latent Spaces enable Efficient Semantic\n  Channel Equalization", "author": "Tom\u00e1s H\u00fcttebr\u00e4ucker and Simone Fiorellino and Mohamed Sana and Paolo Di Lorenzo and Emilio Calvanese Strinati", "abstract": "  In multi-user semantic communication, language mismatche poses a significant\nchallenge when independently trained agents interact. We present a novel\nsemantic equalization algorithm that enables communication between agents with\ndifferent languages without additional retraining. Our algorithm is based on\nrelative representations, a framework that enables different agents employing\ndifferent neural network models to have unified representation. It proceeds by\nprojecting the latent vectors of different models into a common space defined\nrelative to a set of data samples called \\textit{anchors}, whose number equals\nthe dimension of the resulting space. A communication between different agents\ntranslates to a communication of semantic symbols sampled from this relative\nspace. This approach, in addition to aligning the semantic representations of\ndifferent agents, allows compressing the amount of information being exchanged,\nby appropriately selecting the number of anchors. Eventually, we introduce a\nnovel anchor selection strategy, which advantageously determines prototypical\nanchors, capturing the most relevant information for the downstream task. Our\nnumerical results show the effectiveness of the proposed approach allowing\nseamless communication between agents with radically different models,\nincluding differences in terms of neural network architecture and datasets used\nfor initial training.\n", "link": "http://arxiv.org/abs/2411.19719v1", "date": "2024-11-29", "relevancy": 2.2195, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5696}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Representations%20of%20Latent%20Spaces%20enable%20Efficient%20Semantic%0A%20%20Channel%20Equalization&body=Title%3A%20Relative%20Representations%20of%20Latent%20Spaces%20enable%20Efficient%20Semantic%0A%20%20Channel%20Equalization%0AAuthor%3A%20Tom%C3%A1s%20H%C3%BCttebr%C3%A4ucker%20and%20Simone%20Fiorellino%20and%20Mohamed%20Sana%20and%20Paolo%20Di%20Lorenzo%20and%20Emilio%20Calvanese%20Strinati%0AAbstract%3A%20%20%20In%20multi-user%20semantic%20communication%2C%20language%20mismatche%20poses%20a%20significant%0Achallenge%20when%20independently%20trained%20agents%20interact.%20We%20present%20a%20novel%0Asemantic%20equalization%20algorithm%20that%20enables%20communication%20between%20agents%20with%0Adifferent%20languages%20without%20additional%20retraining.%20Our%20algorithm%20is%20based%20on%0Arelative%20representations%2C%20a%20framework%20that%20enables%20different%20agents%20employing%0Adifferent%20neural%20network%20models%20to%20have%20unified%20representation.%20It%20proceeds%20by%0Aprojecting%20the%20latent%20vectors%20of%20different%20models%20into%20a%20common%20space%20defined%0Arelative%20to%20a%20set%20of%20data%20samples%20called%20%5Ctextit%7Banchors%7D%2C%20whose%20number%20equals%0Athe%20dimension%20of%20the%20resulting%20space.%20A%20communication%20between%20different%20agents%0Atranslates%20to%20a%20communication%20of%20semantic%20symbols%20sampled%20from%20this%20relative%0Aspace.%20This%20approach%2C%20in%20addition%20to%20aligning%20the%20semantic%20representations%20of%0Adifferent%20agents%2C%20allows%20compressing%20the%20amount%20of%20information%20being%20exchanged%2C%0Aby%20appropriately%20selecting%20the%20number%20of%20anchors.%20Eventually%2C%20we%20introduce%20a%0Anovel%20anchor%20selection%20strategy%2C%20which%20advantageously%20determines%20prototypical%0Aanchors%2C%20capturing%20the%20most%20relevant%20information%20for%20the%20downstream%20task.%20Our%0Anumerical%20results%20show%20the%20effectiveness%20of%20the%20proposed%20approach%20allowing%0Aseamless%20communication%20between%20agents%20with%20radically%20different%20models%2C%0Aincluding%20differences%20in%20terms%20of%20neural%20network%20architecture%20and%20datasets%20used%0Afor%20initial%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Representations%2520of%2520Latent%2520Spaces%2520enable%2520Efficient%2520Semantic%250A%2520%2520Channel%2520Equalization%26entry.906535625%3DTom%25C3%25A1s%2520H%25C3%25BCttebr%25C3%25A4ucker%2520and%2520Simone%2520Fiorellino%2520and%2520Mohamed%2520Sana%2520and%2520Paolo%2520Di%2520Lorenzo%2520and%2520Emilio%2520Calvanese%2520Strinati%26entry.1292438233%3D%2520%2520In%2520multi-user%2520semantic%2520communication%252C%2520language%2520mismatche%2520poses%2520a%2520significant%250Achallenge%2520when%2520independently%2520trained%2520agents%2520interact.%2520We%2520present%2520a%2520novel%250Asemantic%2520equalization%2520algorithm%2520that%2520enables%2520communication%2520between%2520agents%2520with%250Adifferent%2520languages%2520without%2520additional%2520retraining.%2520Our%2520algorithm%2520is%2520based%2520on%250Arelative%2520representations%252C%2520a%2520framework%2520that%2520enables%2520different%2520agents%2520employing%250Adifferent%2520neural%2520network%2520models%2520to%2520have%2520unified%2520representation.%2520It%2520proceeds%2520by%250Aprojecting%2520the%2520latent%2520vectors%2520of%2520different%2520models%2520into%2520a%2520common%2520space%2520defined%250Arelative%2520to%2520a%2520set%2520of%2520data%2520samples%2520called%2520%255Ctextit%257Banchors%257D%252C%2520whose%2520number%2520equals%250Athe%2520dimension%2520of%2520the%2520resulting%2520space.%2520A%2520communication%2520between%2520different%2520agents%250Atranslates%2520to%2520a%2520communication%2520of%2520semantic%2520symbols%2520sampled%2520from%2520this%2520relative%250Aspace.%2520This%2520approach%252C%2520in%2520addition%2520to%2520aligning%2520the%2520semantic%2520representations%2520of%250Adifferent%2520agents%252C%2520allows%2520compressing%2520the%2520amount%2520of%2520information%2520being%2520exchanged%252C%250Aby%2520appropriately%2520selecting%2520the%2520number%2520of%2520anchors.%2520Eventually%252C%2520we%2520introduce%2520a%250Anovel%2520anchor%2520selection%2520strategy%252C%2520which%2520advantageously%2520determines%2520prototypical%250Aanchors%252C%2520capturing%2520the%2520most%2520relevant%2520information%2520for%2520the%2520downstream%2520task.%2520Our%250Anumerical%2520results%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520allowing%250Aseamless%2520communication%2520between%2520agents%2520with%2520radically%2520different%2520models%252C%250Aincluding%2520differences%2520in%2520terms%2520of%2520neural%2520network%2520architecture%2520and%2520datasets%2520used%250Afor%2520initial%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Representations%20of%20Latent%20Spaces%20enable%20Efficient%20Semantic%0A%20%20Channel%20Equalization&entry.906535625=Tom%C3%A1s%20H%C3%BCttebr%C3%A4ucker%20and%20Simone%20Fiorellino%20and%20Mohamed%20Sana%20and%20Paolo%20Di%20Lorenzo%20and%20Emilio%20Calvanese%20Strinati&entry.1292438233=%20%20In%20multi-user%20semantic%20communication%2C%20language%20mismatche%20poses%20a%20significant%0Achallenge%20when%20independently%20trained%20agents%20interact.%20We%20present%20a%20novel%0Asemantic%20equalization%20algorithm%20that%20enables%20communication%20between%20agents%20with%0Adifferent%20languages%20without%20additional%20retraining.%20Our%20algorithm%20is%20based%20on%0Arelative%20representations%2C%20a%20framework%20that%20enables%20different%20agents%20employing%0Adifferent%20neural%20network%20models%20to%20have%20unified%20representation.%20It%20proceeds%20by%0Aprojecting%20the%20latent%20vectors%20of%20different%20models%20into%20a%20common%20space%20defined%0Arelative%20to%20a%20set%20of%20data%20samples%20called%20%5Ctextit%7Banchors%7D%2C%20whose%20number%20equals%0Athe%20dimension%20of%20the%20resulting%20space.%20A%20communication%20between%20different%20agents%0Atranslates%20to%20a%20communication%20of%20semantic%20symbols%20sampled%20from%20this%20relative%0Aspace.%20This%20approach%2C%20in%20addition%20to%20aligning%20the%20semantic%20representations%20of%0Adifferent%20agents%2C%20allows%20compressing%20the%20amount%20of%20information%20being%20exchanged%2C%0Aby%20appropriately%20selecting%20the%20number%20of%20anchors.%20Eventually%2C%20we%20introduce%20a%0Anovel%20anchor%20selection%20strategy%2C%20which%20advantageously%20determines%20prototypical%0Aanchors%2C%20capturing%20the%20most%20relevant%20information%20for%20the%20downstream%20task.%20Our%0Anumerical%20results%20show%20the%20effectiveness%20of%20the%20proposed%20approach%20allowing%0Aseamless%20communication%20between%20agents%20with%20radically%20different%20models%2C%0Aincluding%20differences%20in%20terms%20of%20neural%20network%20architecture%20and%20datasets%20used%0Afor%20initial%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19719v1&entry.124074799=Read"},
{"title": "An Operator Splitting View of Federated Learning", "author": "Saber Malekmohammadi and Kiarash Shaloudegi and Zeou Hu and Yaoliang Yu", "abstract": "  Over the past few years, the federated learning ($\\texttt{FL}$) community has\nwitnessed a proliferation of new $\\texttt{FL}$ algorithms. However, our\nunderstating of the theory of $\\texttt{FL}$ is still fragmented, and a\nthorough, formal comparison of these algorithms remains elusive. Motivated by\nthis gap, we show that many of the existing $\\texttt{FL}$ algorithms can be\nunderstood from an operator splitting point of view. This unification allows us\nto compare different algorithms with ease, to refine previous convergence\nresults and to uncover new algorithmic variants. In particular, our analysis\nreveals the vital role played by the step size in $\\texttt{FL}$ algorithms. The\nunification also leads to a streamlined and economic way to accelerate\n$\\texttt{FL}$ algorithms, without incurring any communication overhead. We\nperform numerical experiments on both convex and nonconvex models to validate\nour findings.\n", "link": "http://arxiv.org/abs/2108.05974v2", "date": "2024-11-29", "relevancy": 2.2123, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4393}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Operator%20Splitting%20View%20of%20Federated%20Learning&body=Title%3A%20An%20Operator%20Splitting%20View%20of%20Federated%20Learning%0AAuthor%3A%20Saber%20Malekmohammadi%20and%20Kiarash%20Shaloudegi%20and%20Zeou%20Hu%20and%20Yaoliang%20Yu%0AAbstract%3A%20%20%20Over%20the%20past%20few%20years%2C%20the%20federated%20learning%20%28%24%5Ctexttt%7BFL%7D%24%29%20community%20has%0Awitnessed%20a%20proliferation%20of%20new%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20However%2C%20our%0Aunderstating%20of%20the%20theory%20of%20%24%5Ctexttt%7BFL%7D%24%20is%20still%20fragmented%2C%20and%20a%0Athorough%2C%20formal%20comparison%20of%20these%20algorithms%20remains%20elusive.%20Motivated%20by%0Athis%20gap%2C%20we%20show%20that%20many%20of%20the%20existing%20%24%5Ctexttt%7BFL%7D%24%20algorithms%20can%20be%0Aunderstood%20from%20an%20operator%20splitting%20point%20of%20view.%20This%20unification%20allows%20us%0Ato%20compare%20different%20algorithms%20with%20ease%2C%20to%20refine%20previous%20convergence%0Aresults%20and%20to%20uncover%20new%20algorithmic%20variants.%20In%20particular%2C%20our%20analysis%0Areveals%20the%20vital%20role%20played%20by%20the%20step%20size%20in%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20The%0Aunification%20also%20leads%20to%20a%20streamlined%20and%20economic%20way%20to%20accelerate%0A%24%5Ctexttt%7BFL%7D%24%20algorithms%2C%20without%20incurring%20any%20communication%20overhead.%20We%0Aperform%20numerical%20experiments%20on%20both%20convex%20and%20nonconvex%20models%20to%20validate%0Aour%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.05974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Operator%2520Splitting%2520View%2520of%2520Federated%2520Learning%26entry.906535625%3DSaber%2520Malekmohammadi%2520and%2520Kiarash%2520Shaloudegi%2520and%2520Zeou%2520Hu%2520and%2520Yaoliang%2520Yu%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520few%2520years%252C%2520the%2520federated%2520learning%2520%2528%2524%255Ctexttt%257BFL%257D%2524%2529%2520community%2520has%250Awitnessed%2520a%2520proliferation%2520of%2520new%2520%2524%255Ctexttt%257BFL%257D%2524%2520algorithms.%2520However%252C%2520our%250Aunderstating%2520of%2520the%2520theory%2520of%2520%2524%255Ctexttt%257BFL%257D%2524%2520is%2520still%2520fragmented%252C%2520and%2520a%250Athorough%252C%2520formal%2520comparison%2520of%2520these%2520algorithms%2520remains%2520elusive.%2520Motivated%2520by%250Athis%2520gap%252C%2520we%2520show%2520that%2520many%2520of%2520the%2520existing%2520%2524%255Ctexttt%257BFL%257D%2524%2520algorithms%2520can%2520be%250Aunderstood%2520from%2520an%2520operator%2520splitting%2520point%2520of%2520view.%2520This%2520unification%2520allows%2520us%250Ato%2520compare%2520different%2520algorithms%2520with%2520ease%252C%2520to%2520refine%2520previous%2520convergence%250Aresults%2520and%2520to%2520uncover%2520new%2520algorithmic%2520variants.%2520In%2520particular%252C%2520our%2520analysis%250Areveals%2520the%2520vital%2520role%2520played%2520by%2520the%2520step%2520size%2520in%2520%2524%255Ctexttt%257BFL%257D%2524%2520algorithms.%2520The%250Aunification%2520also%2520leads%2520to%2520a%2520streamlined%2520and%2520economic%2520way%2520to%2520accelerate%250A%2524%255Ctexttt%257BFL%257D%2524%2520algorithms%252C%2520without%2520incurring%2520any%2520communication%2520overhead.%2520We%250Aperform%2520numerical%2520experiments%2520on%2520both%2520convex%2520and%2520nonconvex%2520models%2520to%2520validate%250Aour%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.05974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Operator%20Splitting%20View%20of%20Federated%20Learning&entry.906535625=Saber%20Malekmohammadi%20and%20Kiarash%20Shaloudegi%20and%20Zeou%20Hu%20and%20Yaoliang%20Yu&entry.1292438233=%20%20Over%20the%20past%20few%20years%2C%20the%20federated%20learning%20%28%24%5Ctexttt%7BFL%7D%24%29%20community%20has%0Awitnessed%20a%20proliferation%20of%20new%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20However%2C%20our%0Aunderstating%20of%20the%20theory%20of%20%24%5Ctexttt%7BFL%7D%24%20is%20still%20fragmented%2C%20and%20a%0Athorough%2C%20formal%20comparison%20of%20these%20algorithms%20remains%20elusive.%20Motivated%20by%0Athis%20gap%2C%20we%20show%20that%20many%20of%20the%20existing%20%24%5Ctexttt%7BFL%7D%24%20algorithms%20can%20be%0Aunderstood%20from%20an%20operator%20splitting%20point%20of%20view.%20This%20unification%20allows%20us%0Ato%20compare%20different%20algorithms%20with%20ease%2C%20to%20refine%20previous%20convergence%0Aresults%20and%20to%20uncover%20new%20algorithmic%20variants.%20In%20particular%2C%20our%20analysis%0Areveals%20the%20vital%20role%20played%20by%20the%20step%20size%20in%20%24%5Ctexttt%7BFL%7D%24%20algorithms.%20The%0Aunification%20also%20leads%20to%20a%20streamlined%20and%20economic%20way%20to%20accelerate%0A%24%5Ctexttt%7BFL%7D%24%20algorithms%2C%20without%20incurring%20any%20communication%20overhead.%20We%0Aperform%20numerical%20experiments%20on%20both%20convex%20and%20nonconvex%20models%20to%20validate%0Aour%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.05974v2&entry.124074799=Read"},
{"title": "Aggregated Attributions for Explanatory Analysis of 3D Segmentation\n  Models", "author": "Maciej Chrabaszcz and Hubert Baniecki and Piotr Komorowski and Szymon P\u0142otka and Przemyslaw Biecek", "abstract": "  Analysis of 3D segmentation models, especially in the context of medical\nimaging, is often limited to segmentation performance metrics that overlook the\ncrucial aspect of explainability and bias. Currently, effectively explaining\nthese models with saliency maps is challenging due to the high dimensions of\ninput images multiplied by the ever-growing number of segmented class labels.\nTo this end, we introduce Agg^2Exp, a methodology for aggregating fine-grained\nvoxel attributions of the segmentation model's predictions. Unlike classical\nexplanation methods that primarily focus on the local feature attribution,\nAgg^2Exp enables a more comprehensive global view on the importance of\npredicted segments in 3D images. Our benchmarking experiments show that\ngradient-based voxel attributions are more faithful to the model's predictions\nthan perturbation-based explanations. As a concrete use-case, we apply Agg^2Exp\nto discover knowledge acquired by the Swin UNEt TRansformer model trained on\nthe TotalSegmentator v2 dataset for segmenting anatomical structures in\ncomputed tomography medical images. Agg^2Exp facilitates the explanatory\nanalysis of large segmentation models beyond their predictive performance. The\nsource code is publicly available at https://github.com/mi2datalab/agg2exp.\n", "link": "http://arxiv.org/abs/2407.16653v3", "date": "2024-11-29", "relevancy": 2.2087, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5592}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aggregated%20Attributions%20for%20Explanatory%20Analysis%20of%203D%20Segmentation%0A%20%20Models&body=Title%3A%20Aggregated%20Attributions%20for%20Explanatory%20Analysis%20of%203D%20Segmentation%0A%20%20Models%0AAuthor%3A%20Maciej%20Chrabaszcz%20and%20Hubert%20Baniecki%20and%20Piotr%20Komorowski%20and%20Szymon%20P%C5%82otka%20and%20Przemyslaw%20Biecek%0AAbstract%3A%20%20%20Analysis%20of%203D%20segmentation%20models%2C%20especially%20in%20the%20context%20of%20medical%0Aimaging%2C%20is%20often%20limited%20to%20segmentation%20performance%20metrics%20that%20overlook%20the%0Acrucial%20aspect%20of%20explainability%20and%20bias.%20Currently%2C%20effectively%20explaining%0Athese%20models%20with%20saliency%20maps%20is%20challenging%20due%20to%20the%20high%20dimensions%20of%0Ainput%20images%20multiplied%20by%20the%20ever-growing%20number%20of%20segmented%20class%20labels.%0ATo%20this%20end%2C%20we%20introduce%20Agg%5E2Exp%2C%20a%20methodology%20for%20aggregating%20fine-grained%0Avoxel%20attributions%20of%20the%20segmentation%20model%27s%20predictions.%20Unlike%20classical%0Aexplanation%20methods%20that%20primarily%20focus%20on%20the%20local%20feature%20attribution%2C%0AAgg%5E2Exp%20enables%20a%20more%20comprehensive%20global%20view%20on%20the%20importance%20of%0Apredicted%20segments%20in%203D%20images.%20Our%20benchmarking%20experiments%20show%20that%0Agradient-based%20voxel%20attributions%20are%20more%20faithful%20to%20the%20model%27s%20predictions%0Athan%20perturbation-based%20explanations.%20As%20a%20concrete%20use-case%2C%20we%20apply%20Agg%5E2Exp%0Ato%20discover%20knowledge%20acquired%20by%20the%20Swin%20UNEt%20TRansformer%20model%20trained%20on%0Athe%20TotalSegmentator%20v2%20dataset%20for%20segmenting%20anatomical%20structures%20in%0Acomputed%20tomography%20medical%20images.%20Agg%5E2Exp%20facilitates%20the%20explanatory%0Aanalysis%20of%20large%20segmentation%20models%20beyond%20their%20predictive%20performance.%20The%0Asource%20code%20is%20publicly%20available%20at%20https%3A//github.com/mi2datalab/agg2exp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16653v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAggregated%2520Attributions%2520for%2520Explanatory%2520Analysis%2520of%25203D%2520Segmentation%250A%2520%2520Models%26entry.906535625%3DMaciej%2520Chrabaszcz%2520and%2520Hubert%2520Baniecki%2520and%2520Piotr%2520Komorowski%2520and%2520Szymon%2520P%25C5%2582otka%2520and%2520Przemyslaw%2520Biecek%26entry.1292438233%3D%2520%2520Analysis%2520of%25203D%2520segmentation%2520models%252C%2520especially%2520in%2520the%2520context%2520of%2520medical%250Aimaging%252C%2520is%2520often%2520limited%2520to%2520segmentation%2520performance%2520metrics%2520that%2520overlook%2520the%250Acrucial%2520aspect%2520of%2520explainability%2520and%2520bias.%2520Currently%252C%2520effectively%2520explaining%250Athese%2520models%2520with%2520saliency%2520maps%2520is%2520challenging%2520due%2520to%2520the%2520high%2520dimensions%2520of%250Ainput%2520images%2520multiplied%2520by%2520the%2520ever-growing%2520number%2520of%2520segmented%2520class%2520labels.%250ATo%2520this%2520end%252C%2520we%2520introduce%2520Agg%255E2Exp%252C%2520a%2520methodology%2520for%2520aggregating%2520fine-grained%250Avoxel%2520attributions%2520of%2520the%2520segmentation%2520model%2527s%2520predictions.%2520Unlike%2520classical%250Aexplanation%2520methods%2520that%2520primarily%2520focus%2520on%2520the%2520local%2520feature%2520attribution%252C%250AAgg%255E2Exp%2520enables%2520a%2520more%2520comprehensive%2520global%2520view%2520on%2520the%2520importance%2520of%250Apredicted%2520segments%2520in%25203D%2520images.%2520Our%2520benchmarking%2520experiments%2520show%2520that%250Agradient-based%2520voxel%2520attributions%2520are%2520more%2520faithful%2520to%2520the%2520model%2527s%2520predictions%250Athan%2520perturbation-based%2520explanations.%2520As%2520a%2520concrete%2520use-case%252C%2520we%2520apply%2520Agg%255E2Exp%250Ato%2520discover%2520knowledge%2520acquired%2520by%2520the%2520Swin%2520UNEt%2520TRansformer%2520model%2520trained%2520on%250Athe%2520TotalSegmentator%2520v2%2520dataset%2520for%2520segmenting%2520anatomical%2520structures%2520in%250Acomputed%2520tomography%2520medical%2520images.%2520Agg%255E2Exp%2520facilitates%2520the%2520explanatory%250Aanalysis%2520of%2520large%2520segmentation%2520models%2520beyond%2520their%2520predictive%2520performance.%2520The%250Asource%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/mi2datalab/agg2exp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16653v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aggregated%20Attributions%20for%20Explanatory%20Analysis%20of%203D%20Segmentation%0A%20%20Models&entry.906535625=Maciej%20Chrabaszcz%20and%20Hubert%20Baniecki%20and%20Piotr%20Komorowski%20and%20Szymon%20P%C5%82otka%20and%20Przemyslaw%20Biecek&entry.1292438233=%20%20Analysis%20of%203D%20segmentation%20models%2C%20especially%20in%20the%20context%20of%20medical%0Aimaging%2C%20is%20often%20limited%20to%20segmentation%20performance%20metrics%20that%20overlook%20the%0Acrucial%20aspect%20of%20explainability%20and%20bias.%20Currently%2C%20effectively%20explaining%0Athese%20models%20with%20saliency%20maps%20is%20challenging%20due%20to%20the%20high%20dimensions%20of%0Ainput%20images%20multiplied%20by%20the%20ever-growing%20number%20of%20segmented%20class%20labels.%0ATo%20this%20end%2C%20we%20introduce%20Agg%5E2Exp%2C%20a%20methodology%20for%20aggregating%20fine-grained%0Avoxel%20attributions%20of%20the%20segmentation%20model%27s%20predictions.%20Unlike%20classical%0Aexplanation%20methods%20that%20primarily%20focus%20on%20the%20local%20feature%20attribution%2C%0AAgg%5E2Exp%20enables%20a%20more%20comprehensive%20global%20view%20on%20the%20importance%20of%0Apredicted%20segments%20in%203D%20images.%20Our%20benchmarking%20experiments%20show%20that%0Agradient-based%20voxel%20attributions%20are%20more%20faithful%20to%20the%20model%27s%20predictions%0Athan%20perturbation-based%20explanations.%20As%20a%20concrete%20use-case%2C%20we%20apply%20Agg%5E2Exp%0Ato%20discover%20knowledge%20acquired%20by%20the%20Swin%20UNEt%20TRansformer%20model%20trained%20on%0Athe%20TotalSegmentator%20v2%20dataset%20for%20segmenting%20anatomical%20structures%20in%0Acomputed%20tomography%20medical%20images.%20Agg%5E2Exp%20facilitates%20the%20explanatory%0Aanalysis%20of%20large%20segmentation%20models%20beyond%20their%20predictive%20performance.%20The%0Asource%20code%20is%20publicly%20available%20at%20https%3A//github.com/mi2datalab/agg2exp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16653v3&entry.124074799=Read"},
{"title": "Domain-Adaptive Pre-training of Self-Supervised Foundation Models for\n  Medical Image Classification in Gastrointestinal Endoscopy", "author": "Marcel Roth and Micha V. Nowak and Adrian Krenzer and Frank Puppe", "abstract": "  Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE)\ndiagnostics by offering a non-invasive method for capturing detailed images of\nthe gastrointestinal tract, enabling early disease detection. However, its\npotential is limited by the sheer volume of images generated during the imaging\nprocedure, which can take anywhere from 6-8 hours and often produce up to 1\nmillion images, necessitating automated analysis. Additionally, the variability\nof these images, combined with the need for expert annotations and the scarcity\nof large, high-quality labeled datasets, constrains the effectiveness of\ncurrent medical image analysis models. To address this, we introduce a novel\nlarge GIE dataset, called EndoExtend24, created by merging ten existing public\nand private datasets, ensuring patient integrity across splits. EndoExtend24\nincludes over 226,000 labeled images, as well as dynamic class mappings, which\nallow unified training across datasets with differing labeling granularity,\nsupporting up to 123 distinct pathological findings. Further, we propose to\nleverage domain adaptive pre-training of foundation models trained with\nself-supervision on generic image data, to adapt them to the task of GIE\nmedical image diagnosis. Specifically, the EVA-02 model, which is based on the\nViT architecture and trained on ImageNet-22k with masked image modeling (using\nEVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to\nachieve domain adaptation, and finally trained on the Capsule Endoscopy 2024\nChallenge dataset. Our model demonstrates robust performance, securing third\nplace in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762\nand a balanced accuracy of 37.1% on the test set. These results emphasize the\neffectiveness of our domain-adaptive pre-training approach and the enriched\nEndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics.\n", "link": "http://arxiv.org/abs/2410.21302v3", "date": "2024-11-29", "relevancy": 2.1725, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5594}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5494}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-Adaptive%20Pre-training%20of%20Self-Supervised%20Foundation%20Models%20for%0A%20%20Medical%20Image%20Classification%20in%20Gastrointestinal%20Endoscopy&body=Title%3A%20Domain-Adaptive%20Pre-training%20of%20Self-Supervised%20Foundation%20Models%20for%0A%20%20Medical%20Image%20Classification%20in%20Gastrointestinal%20Endoscopy%0AAuthor%3A%20Marcel%20Roth%20and%20Micha%20V.%20Nowak%20and%20Adrian%20Krenzer%20and%20Frank%20Puppe%0AAbstract%3A%20%20%20Video%20capsule%20endoscopy%20has%20transformed%20gastrointestinal%20endoscopy%20%28GIE%29%0Adiagnostics%20by%20offering%20a%20non-invasive%20method%20for%20capturing%20detailed%20images%20of%0Athe%20gastrointestinal%20tract%2C%20enabling%20early%20disease%20detection.%20However%2C%20its%0Apotential%20is%20limited%20by%20the%20sheer%20volume%20of%20images%20generated%20during%20the%20imaging%0Aprocedure%2C%20which%20can%20take%20anywhere%20from%206-8%20hours%20and%20often%20produce%20up%20to%201%0Amillion%20images%2C%20necessitating%20automated%20analysis.%20Additionally%2C%20the%20variability%0Aof%20these%20images%2C%20combined%20with%20the%20need%20for%20expert%20annotations%20and%20the%20scarcity%0Aof%20large%2C%20high-quality%20labeled%20datasets%2C%20constrains%20the%20effectiveness%20of%0Acurrent%20medical%20image%20analysis%20models.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Alarge%20GIE%20dataset%2C%20called%20EndoExtend24%2C%20created%20by%20merging%20ten%20existing%20public%0Aand%20private%20datasets%2C%20ensuring%20patient%20integrity%20across%20splits.%20EndoExtend24%0Aincludes%20over%20226%2C000%20labeled%20images%2C%20as%20well%20as%20dynamic%20class%20mappings%2C%20which%0Aallow%20unified%20training%20across%20datasets%20with%20differing%20labeling%20granularity%2C%0Asupporting%20up%20to%20123%20distinct%20pathological%20findings.%20Further%2C%20we%20propose%20to%0Aleverage%20domain%20adaptive%20pre-training%20of%20foundation%20models%20trained%20with%0Aself-supervision%20on%20generic%20image%20data%2C%20to%20adapt%20them%20to%20the%20task%20of%20GIE%0Amedical%20image%20diagnosis.%20Specifically%2C%20the%20EVA-02%20model%2C%20which%20is%20based%20on%20the%0AViT%20architecture%20and%20trained%20on%20ImageNet-22k%20with%20masked%20image%20modeling%20%28using%0AEVA-CLIP%20as%20a%20MIM%20teacher%29%2C%20is%20pre-trained%20on%20the%20EndoExtend24%20dataset%20to%0Aachieve%20domain%20adaptation%2C%20and%20finally%20trained%20on%20the%20Capsule%20Endoscopy%202024%0AChallenge%20dataset.%20Our%20model%20demonstrates%20robust%20performance%2C%20securing%20third%0Aplace%20in%20the%20Capsule%20Endoscopy%202024%20Challenge.%20We%20achieved%20a%20macro%20AUC%20of%200.762%0Aand%20a%20balanced%20accuracy%20of%2037.1%25%20on%20the%20test%20set.%20These%20results%20emphasize%20the%0Aeffectiveness%20of%20our%20domain-adaptive%20pre-training%20approach%20and%20the%20enriched%0AEndoExtend24%20dataset%20in%20advancing%20gastrointestinal%20endoscopy%20diagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21302v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-Adaptive%2520Pre-training%2520of%2520Self-Supervised%2520Foundation%2520Models%2520for%250A%2520%2520Medical%2520Image%2520Classification%2520in%2520Gastrointestinal%2520Endoscopy%26entry.906535625%3DMarcel%2520Roth%2520and%2520Micha%2520V.%2520Nowak%2520and%2520Adrian%2520Krenzer%2520and%2520Frank%2520Puppe%26entry.1292438233%3D%2520%2520Video%2520capsule%2520endoscopy%2520has%2520transformed%2520gastrointestinal%2520endoscopy%2520%2528GIE%2529%250Adiagnostics%2520by%2520offering%2520a%2520non-invasive%2520method%2520for%2520capturing%2520detailed%2520images%2520of%250Athe%2520gastrointestinal%2520tract%252C%2520enabling%2520early%2520disease%2520detection.%2520However%252C%2520its%250Apotential%2520is%2520limited%2520by%2520the%2520sheer%2520volume%2520of%2520images%2520generated%2520during%2520the%2520imaging%250Aprocedure%252C%2520which%2520can%2520take%2520anywhere%2520from%25206-8%2520hours%2520and%2520often%2520produce%2520up%2520to%25201%250Amillion%2520images%252C%2520necessitating%2520automated%2520analysis.%2520Additionally%252C%2520the%2520variability%250Aof%2520these%2520images%252C%2520combined%2520with%2520the%2520need%2520for%2520expert%2520annotations%2520and%2520the%2520scarcity%250Aof%2520large%252C%2520high-quality%2520labeled%2520datasets%252C%2520constrains%2520the%2520effectiveness%2520of%250Acurrent%2520medical%2520image%2520analysis%2520models.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%250Alarge%2520GIE%2520dataset%252C%2520called%2520EndoExtend24%252C%2520created%2520by%2520merging%2520ten%2520existing%2520public%250Aand%2520private%2520datasets%252C%2520ensuring%2520patient%2520integrity%2520across%2520splits.%2520EndoExtend24%250Aincludes%2520over%2520226%252C000%2520labeled%2520images%252C%2520as%2520well%2520as%2520dynamic%2520class%2520mappings%252C%2520which%250Aallow%2520unified%2520training%2520across%2520datasets%2520with%2520differing%2520labeling%2520granularity%252C%250Asupporting%2520up%2520to%2520123%2520distinct%2520pathological%2520findings.%2520Further%252C%2520we%2520propose%2520to%250Aleverage%2520domain%2520adaptive%2520pre-training%2520of%2520foundation%2520models%2520trained%2520with%250Aself-supervision%2520on%2520generic%2520image%2520data%252C%2520to%2520adapt%2520them%2520to%2520the%2520task%2520of%2520GIE%250Amedical%2520image%2520diagnosis.%2520Specifically%252C%2520the%2520EVA-02%2520model%252C%2520which%2520is%2520based%2520on%2520the%250AViT%2520architecture%2520and%2520trained%2520on%2520ImageNet-22k%2520with%2520masked%2520image%2520modeling%2520%2528using%250AEVA-CLIP%2520as%2520a%2520MIM%2520teacher%2529%252C%2520is%2520pre-trained%2520on%2520the%2520EndoExtend24%2520dataset%2520to%250Aachieve%2520domain%2520adaptation%252C%2520and%2520finally%2520trained%2520on%2520the%2520Capsule%2520Endoscopy%25202024%250AChallenge%2520dataset.%2520Our%2520model%2520demonstrates%2520robust%2520performance%252C%2520securing%2520third%250Aplace%2520in%2520the%2520Capsule%2520Endoscopy%25202024%2520Challenge.%2520We%2520achieved%2520a%2520macro%2520AUC%2520of%25200.762%250Aand%2520a%2520balanced%2520accuracy%2520of%252037.1%2525%2520on%2520the%2520test%2520set.%2520These%2520results%2520emphasize%2520the%250Aeffectiveness%2520of%2520our%2520domain-adaptive%2520pre-training%2520approach%2520and%2520the%2520enriched%250AEndoExtend24%2520dataset%2520in%2520advancing%2520gastrointestinal%2520endoscopy%2520diagnostics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21302v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Adaptive%20Pre-training%20of%20Self-Supervised%20Foundation%20Models%20for%0A%20%20Medical%20Image%20Classification%20in%20Gastrointestinal%20Endoscopy&entry.906535625=Marcel%20Roth%20and%20Micha%20V.%20Nowak%20and%20Adrian%20Krenzer%20and%20Frank%20Puppe&entry.1292438233=%20%20Video%20capsule%20endoscopy%20has%20transformed%20gastrointestinal%20endoscopy%20%28GIE%29%0Adiagnostics%20by%20offering%20a%20non-invasive%20method%20for%20capturing%20detailed%20images%20of%0Athe%20gastrointestinal%20tract%2C%20enabling%20early%20disease%20detection.%20However%2C%20its%0Apotential%20is%20limited%20by%20the%20sheer%20volume%20of%20images%20generated%20during%20the%20imaging%0Aprocedure%2C%20which%20can%20take%20anywhere%20from%206-8%20hours%20and%20often%20produce%20up%20to%201%0Amillion%20images%2C%20necessitating%20automated%20analysis.%20Additionally%2C%20the%20variability%0Aof%20these%20images%2C%20combined%20with%20the%20need%20for%20expert%20annotations%20and%20the%20scarcity%0Aof%20large%2C%20high-quality%20labeled%20datasets%2C%20constrains%20the%20effectiveness%20of%0Acurrent%20medical%20image%20analysis%20models.%20To%20address%20this%2C%20we%20introduce%20a%20novel%0Alarge%20GIE%20dataset%2C%20called%20EndoExtend24%2C%20created%20by%20merging%20ten%20existing%20public%0Aand%20private%20datasets%2C%20ensuring%20patient%20integrity%20across%20splits.%20EndoExtend24%0Aincludes%20over%20226%2C000%20labeled%20images%2C%20as%20well%20as%20dynamic%20class%20mappings%2C%20which%0Aallow%20unified%20training%20across%20datasets%20with%20differing%20labeling%20granularity%2C%0Asupporting%20up%20to%20123%20distinct%20pathological%20findings.%20Further%2C%20we%20propose%20to%0Aleverage%20domain%20adaptive%20pre-training%20of%20foundation%20models%20trained%20with%0Aself-supervision%20on%20generic%20image%20data%2C%20to%20adapt%20them%20to%20the%20task%20of%20GIE%0Amedical%20image%20diagnosis.%20Specifically%2C%20the%20EVA-02%20model%2C%20which%20is%20based%20on%20the%0AViT%20architecture%20and%20trained%20on%20ImageNet-22k%20with%20masked%20image%20modeling%20%28using%0AEVA-CLIP%20as%20a%20MIM%20teacher%29%2C%20is%20pre-trained%20on%20the%20EndoExtend24%20dataset%20to%0Aachieve%20domain%20adaptation%2C%20and%20finally%20trained%20on%20the%20Capsule%20Endoscopy%202024%0AChallenge%20dataset.%20Our%20model%20demonstrates%20robust%20performance%2C%20securing%20third%0Aplace%20in%20the%20Capsule%20Endoscopy%202024%20Challenge.%20We%20achieved%20a%20macro%20AUC%20of%200.762%0Aand%20a%20balanced%20accuracy%20of%2037.1%25%20on%20the%20test%20set.%20These%20results%20emphasize%20the%0Aeffectiveness%20of%20our%20domain-adaptive%20pre-training%20approach%20and%20the%20enriched%0AEndoExtend24%20dataset%20in%20advancing%20gastrointestinal%20endoscopy%20diagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21302v3&entry.124074799=Read"},
{"title": "Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning", "author": "Geri Skenderi and Luigi Capogrosso and Andrea Toaiari and Matteo Denitto and Franco Fummi and Simone Melzi and Marco Cristani", "abstract": "  Auxiliary tasks facilitate learning in situations when data is scarce or the\nprincipal task of focus is extremely complex. This idea is primarily inspired\nby the improved generalization capability induced by solving multiple tasks\nsimultaneously, which leads to a more robust shared representation.\nNevertheless, finding optimal auxiliary tasks is a crucial problem that often\nrequires hand-crafted solutions or expensive meta-learning approaches. In this\npaper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised\ndisentanglement procedure is used to discover a new unrelated auxiliary\nclassification task, which allows us to go from a Single-Task Learning (STL) to\na Multi-Task Learning (MTL) problem. The disentanglement procedure works at the\nrepresentation level, isolating the variation related to the principal task\ninto an isolated subspace and additionally producing an arbitrary number of\northogonal subspaces, each one of them encouraging high separability among the\nprojections. We generate the auxiliary classification task through a clustering\nprocedure on the most disentangled subspace, obtaining a discrete set of\nlabels. Subsequently, the original data, the labels associated with the\nprincipal task, and the newly discovered ones can be fed into any MTL\nframework. Experimental validation on both synthetic and real data, along with\nvarious ablation studies, demonstrate promising results, revealing the\npotential in what has been, so far, an unexplored connection between learning\ndisentangled representations and MTL. The source code will be made available\nupon acceptance.\n", "link": "http://arxiv.org/abs/2310.09278v2", "date": "2024-11-29", "relevancy": 2.1497, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5564}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5304}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Latent%20Spaces%20Facilitate%20Data-Driven%20Auxiliary%20Learning&body=Title%3A%20Disentangled%20Latent%20Spaces%20Facilitate%20Data-Driven%20Auxiliary%20Learning%0AAuthor%3A%20Geri%20Skenderi%20and%20Luigi%20Capogrosso%20and%20Andrea%20Toaiari%20and%20Matteo%20Denitto%20and%20Franco%20Fummi%20and%20Simone%20Melzi%20and%20Marco%20Cristani%0AAbstract%3A%20%20%20Auxiliary%20tasks%20facilitate%20learning%20in%20situations%20when%20data%20is%20scarce%20or%20the%0Aprincipal%20task%20of%20focus%20is%20extremely%20complex.%20This%20idea%20is%20primarily%20inspired%0Aby%20the%20improved%20generalization%20capability%20induced%20by%20solving%20multiple%20tasks%0Asimultaneously%2C%20which%20leads%20to%20a%20more%20robust%20shared%20representation.%0ANevertheless%2C%20finding%20optimal%20auxiliary%20tasks%20is%20a%20crucial%20problem%20that%20often%0Arequires%20hand-crafted%20solutions%20or%20expensive%20meta-learning%20approaches.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%2C%20dubbed%20Detaux%2C%20whereby%20a%20weakly%20supervised%0Adisentanglement%20procedure%20is%20used%20to%20discover%20a%20new%20unrelated%20auxiliary%0Aclassification%20task%2C%20which%20allows%20us%20to%20go%20from%20a%20Single-Task%20Learning%20%28STL%29%20to%0Aa%20Multi-Task%20Learning%20%28MTL%29%20problem.%20The%20disentanglement%20procedure%20works%20at%20the%0Arepresentation%20level%2C%20isolating%20the%20variation%20related%20to%20the%20principal%20task%0Ainto%20an%20isolated%20subspace%20and%20additionally%20producing%20an%20arbitrary%20number%20of%0Aorthogonal%20subspaces%2C%20each%20one%20of%20them%20encouraging%20high%20separability%20among%20the%0Aprojections.%20We%20generate%20the%20auxiliary%20classification%20task%20through%20a%20clustering%0Aprocedure%20on%20the%20most%20disentangled%20subspace%2C%20obtaining%20a%20discrete%20set%20of%0Alabels.%20Subsequently%2C%20the%20original%20data%2C%20the%20labels%20associated%20with%20the%0Aprincipal%20task%2C%20and%20the%20newly%20discovered%20ones%20can%20be%20fed%20into%20any%20MTL%0Aframework.%20Experimental%20validation%20on%20both%20synthetic%20and%20real%20data%2C%20along%20with%0Avarious%20ablation%20studies%2C%20demonstrate%20promising%20results%2C%20revealing%20the%0Apotential%20in%20what%20has%20been%2C%20so%20far%2C%20an%20unexplored%20connection%20between%20learning%0Adisentangled%20representations%20and%20MTL.%20The%20source%20code%20will%20be%20made%20available%0Aupon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Latent%2520Spaces%2520Facilitate%2520Data-Driven%2520Auxiliary%2520Learning%26entry.906535625%3DGeri%2520Skenderi%2520and%2520Luigi%2520Capogrosso%2520and%2520Andrea%2520Toaiari%2520and%2520Matteo%2520Denitto%2520and%2520Franco%2520Fummi%2520and%2520Simone%2520Melzi%2520and%2520Marco%2520Cristani%26entry.1292438233%3D%2520%2520Auxiliary%2520tasks%2520facilitate%2520learning%2520in%2520situations%2520when%2520data%2520is%2520scarce%2520or%2520the%250Aprincipal%2520task%2520of%2520focus%2520is%2520extremely%2520complex.%2520This%2520idea%2520is%2520primarily%2520inspired%250Aby%2520the%2520improved%2520generalization%2520capability%2520induced%2520by%2520solving%2520multiple%2520tasks%250Asimultaneously%252C%2520which%2520leads%2520to%2520a%2520more%2520robust%2520shared%2520representation.%250ANevertheless%252C%2520finding%2520optimal%2520auxiliary%2520tasks%2520is%2520a%2520crucial%2520problem%2520that%2520often%250Arequires%2520hand-crafted%2520solutions%2520or%2520expensive%2520meta-learning%2520approaches.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520dubbed%2520Detaux%252C%2520whereby%2520a%2520weakly%2520supervised%250Adisentanglement%2520procedure%2520is%2520used%2520to%2520discover%2520a%2520new%2520unrelated%2520auxiliary%250Aclassification%2520task%252C%2520which%2520allows%2520us%2520to%2520go%2520from%2520a%2520Single-Task%2520Learning%2520%2528STL%2529%2520to%250Aa%2520Multi-Task%2520Learning%2520%2528MTL%2529%2520problem.%2520The%2520disentanglement%2520procedure%2520works%2520at%2520the%250Arepresentation%2520level%252C%2520isolating%2520the%2520variation%2520related%2520to%2520the%2520principal%2520task%250Ainto%2520an%2520isolated%2520subspace%2520and%2520additionally%2520producing%2520an%2520arbitrary%2520number%2520of%250Aorthogonal%2520subspaces%252C%2520each%2520one%2520of%2520them%2520encouraging%2520high%2520separability%2520among%2520the%250Aprojections.%2520We%2520generate%2520the%2520auxiliary%2520classification%2520task%2520through%2520a%2520clustering%250Aprocedure%2520on%2520the%2520most%2520disentangled%2520subspace%252C%2520obtaining%2520a%2520discrete%2520set%2520of%250Alabels.%2520Subsequently%252C%2520the%2520original%2520data%252C%2520the%2520labels%2520associated%2520with%2520the%250Aprincipal%2520task%252C%2520and%2520the%2520newly%2520discovered%2520ones%2520can%2520be%2520fed%2520into%2520any%2520MTL%250Aframework.%2520Experimental%2520validation%2520on%2520both%2520synthetic%2520and%2520real%2520data%252C%2520along%2520with%250Avarious%2520ablation%2520studies%252C%2520demonstrate%2520promising%2520results%252C%2520revealing%2520the%250Apotential%2520in%2520what%2520has%2520been%252C%2520so%2520far%252C%2520an%2520unexplored%2520connection%2520between%2520learning%250Adisentangled%2520representations%2520and%2520MTL.%2520The%2520source%2520code%2520will%2520be%2520made%2520available%250Aupon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Latent%20Spaces%20Facilitate%20Data-Driven%20Auxiliary%20Learning&entry.906535625=Geri%20Skenderi%20and%20Luigi%20Capogrosso%20and%20Andrea%20Toaiari%20and%20Matteo%20Denitto%20and%20Franco%20Fummi%20and%20Simone%20Melzi%20and%20Marco%20Cristani&entry.1292438233=%20%20Auxiliary%20tasks%20facilitate%20learning%20in%20situations%20when%20data%20is%20scarce%20or%20the%0Aprincipal%20task%20of%20focus%20is%20extremely%20complex.%20This%20idea%20is%20primarily%20inspired%0Aby%20the%20improved%20generalization%20capability%20induced%20by%20solving%20multiple%20tasks%0Asimultaneously%2C%20which%20leads%20to%20a%20more%20robust%20shared%20representation.%0ANevertheless%2C%20finding%20optimal%20auxiliary%20tasks%20is%20a%20crucial%20problem%20that%20often%0Arequires%20hand-crafted%20solutions%20or%20expensive%20meta-learning%20approaches.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20framework%2C%20dubbed%20Detaux%2C%20whereby%20a%20weakly%20supervised%0Adisentanglement%20procedure%20is%20used%20to%20discover%20a%20new%20unrelated%20auxiliary%0Aclassification%20task%2C%20which%20allows%20us%20to%20go%20from%20a%20Single-Task%20Learning%20%28STL%29%20to%0Aa%20Multi-Task%20Learning%20%28MTL%29%20problem.%20The%20disentanglement%20procedure%20works%20at%20the%0Arepresentation%20level%2C%20isolating%20the%20variation%20related%20to%20the%20principal%20task%0Ainto%20an%20isolated%20subspace%20and%20additionally%20producing%20an%20arbitrary%20number%20of%0Aorthogonal%20subspaces%2C%20each%20one%20of%20them%20encouraging%20high%20separability%20among%20the%0Aprojections.%20We%20generate%20the%20auxiliary%20classification%20task%20through%20a%20clustering%0Aprocedure%20on%20the%20most%20disentangled%20subspace%2C%20obtaining%20a%20discrete%20set%20of%0Alabels.%20Subsequently%2C%20the%20original%20data%2C%20the%20labels%20associated%20with%20the%0Aprincipal%20task%2C%20and%20the%20newly%20discovered%20ones%20can%20be%20fed%20into%20any%20MTL%0Aframework.%20Experimental%20validation%20on%20both%20synthetic%20and%20real%20data%2C%20along%20with%0Avarious%20ablation%20studies%2C%20demonstrate%20promising%20results%2C%20revealing%20the%0Apotential%20in%20what%20has%20been%2C%20so%20far%2C%20an%20unexplored%20connection%20between%20learning%0Adisentangled%20representations%20and%20MTL.%20The%20source%20code%20will%20be%20made%20available%0Aupon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09278v2&entry.124074799=Read"},
{"title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety", "author": "Xuhao Hu and Dongrui Liu and Hao Li and Xuanjing Huang and Jing Shao", "abstract": "  Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counter-intuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs trained with\nimage-text pairs. To explain such a counter-intuitive phenomenon, we discover a\nvisual safety information leakage (VSIL) problem in existing multimodal safety\nbenchmarks, i.e., the potentially risky and sensitive content in the image has\nbeen revealed in the textual query. In this way, MLLMs can easily refuse these\nsensitive text-image queries according to textual queries. However, image-text\npairs without VSIL are common in real-world scenarios and are overlooked by\nexisting multimodal safety benchmarks. To this end, we construct multimodal\nvisual leakless safety benchmark (VLSBench) preventing visual safety leakage\nfrom image to textual query with 2.4k image-text pairs. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.\nThis study demonstrates that textual alignment is enough for multimodal safety\nscenarios with VSIL, while multimodal alignment is a more promising solution\nfor multimodal safety scenarios without VSIL. Please see our code and data at:\nhttp://hxhcreate.github.io/VLSBench\n", "link": "http://arxiv.org/abs/2411.19939v1", "date": "2024-11-29", "relevancy": 2.1347, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5413}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLSBench%3A%20Unveiling%20Visual%20Leakage%20in%20Multimodal%20Safety&body=Title%3A%20VLSBench%3A%20Unveiling%20Visual%20Leakage%20in%20Multimodal%20Safety%0AAuthor%3A%20Xuhao%20Hu%20and%20Dongrui%20Liu%20and%20Hao%20Li%20and%20Xuanjing%20Huang%20and%20Jing%20Shao%0AAbstract%3A%20%20%20Safety%20concerns%20of%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20gradually%0Abecome%20an%20important%20problem%20in%20various%20applications.%20Surprisingly%2C%20previous%0Aworks%20indicate%20a%20counter-intuitive%20phenomenon%20that%20using%20textual%20unlearning%20to%0Aalign%20MLLMs%20achieves%20comparable%20safety%20performances%20with%20MLLMs%20trained%20with%0Aimage-text%20pairs.%20To%20explain%20such%20a%20counter-intuitive%20phenomenon%2C%20we%20discover%20a%0Avisual%20safety%20information%20leakage%20%28VSIL%29%20problem%20in%20existing%20multimodal%20safety%0Abenchmarks%2C%20i.e.%2C%20the%20potentially%20risky%20and%20sensitive%20content%20in%20the%20image%20has%0Abeen%20revealed%20in%20the%20textual%20query.%20In%20this%20way%2C%20MLLMs%20can%20easily%20refuse%20these%0Asensitive%20text-image%20queries%20according%20to%20textual%20queries.%20However%2C%20image-text%0Apairs%20without%20VSIL%20are%20common%20in%20real-world%20scenarios%20and%20are%20overlooked%20by%0Aexisting%20multimodal%20safety%20benchmarks.%20To%20this%20end%2C%20we%20construct%20multimodal%0Avisual%20leakless%20safety%20benchmark%20%28VLSBench%29%20preventing%20visual%20safety%20leakage%0Afrom%20image%20to%20textual%20query%20with%202.4k%20image-text%20pairs.%20Experimental%20results%0Aindicate%20that%20VLSBench%20poses%20a%20significant%20challenge%20to%20both%20open-source%20and%0Aclose-source%20MLLMs%2C%20including%20LLaVA%2C%20Qwen2-VL%2C%20Llama3.2-Vision%2C%20and%20GPT-4o.%0AThis%20study%20demonstrates%20that%20textual%20alignment%20is%20enough%20for%20multimodal%20safety%0Ascenarios%20with%20VSIL%2C%20while%20multimodal%20alignment%20is%20a%20more%20promising%20solution%0Afor%20multimodal%20safety%20scenarios%20without%20VSIL.%20Please%20see%20our%20code%20and%20data%20at%3A%0Ahttp%3A//hxhcreate.github.io/VLSBench%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLSBench%253A%2520Unveiling%2520Visual%2520Leakage%2520in%2520Multimodal%2520Safety%26entry.906535625%3DXuhao%2520Hu%2520and%2520Dongrui%2520Liu%2520and%2520Hao%2520Li%2520and%2520Xuanjing%2520Huang%2520and%2520Jing%2520Shao%26entry.1292438233%3D%2520%2520Safety%2520concerns%2520of%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520gradually%250Abecome%2520an%2520important%2520problem%2520in%2520various%2520applications.%2520Surprisingly%252C%2520previous%250Aworks%2520indicate%2520a%2520counter-intuitive%2520phenomenon%2520that%2520using%2520textual%2520unlearning%2520to%250Aalign%2520MLLMs%2520achieves%2520comparable%2520safety%2520performances%2520with%2520MLLMs%2520trained%2520with%250Aimage-text%2520pairs.%2520To%2520explain%2520such%2520a%2520counter-intuitive%2520phenomenon%252C%2520we%2520discover%2520a%250Avisual%2520safety%2520information%2520leakage%2520%2528VSIL%2529%2520problem%2520in%2520existing%2520multimodal%2520safety%250Abenchmarks%252C%2520i.e.%252C%2520the%2520potentially%2520risky%2520and%2520sensitive%2520content%2520in%2520the%2520image%2520has%250Abeen%2520revealed%2520in%2520the%2520textual%2520query.%2520In%2520this%2520way%252C%2520MLLMs%2520can%2520easily%2520refuse%2520these%250Asensitive%2520text-image%2520queries%2520according%2520to%2520textual%2520queries.%2520However%252C%2520image-text%250Apairs%2520without%2520VSIL%2520are%2520common%2520in%2520real-world%2520scenarios%2520and%2520are%2520overlooked%2520by%250Aexisting%2520multimodal%2520safety%2520benchmarks.%2520To%2520this%2520end%252C%2520we%2520construct%2520multimodal%250Avisual%2520leakless%2520safety%2520benchmark%2520%2528VLSBench%2529%2520preventing%2520visual%2520safety%2520leakage%250Afrom%2520image%2520to%2520textual%2520query%2520with%25202.4k%2520image-text%2520pairs.%2520Experimental%2520results%250Aindicate%2520that%2520VLSBench%2520poses%2520a%2520significant%2520challenge%2520to%2520both%2520open-source%2520and%250Aclose-source%2520MLLMs%252C%2520including%2520LLaVA%252C%2520Qwen2-VL%252C%2520Llama3.2-Vision%252C%2520and%2520GPT-4o.%250AThis%2520study%2520demonstrates%2520that%2520textual%2520alignment%2520is%2520enough%2520for%2520multimodal%2520safety%250Ascenarios%2520with%2520VSIL%252C%2520while%2520multimodal%2520alignment%2520is%2520a%2520more%2520promising%2520solution%250Afor%2520multimodal%2520safety%2520scenarios%2520without%2520VSIL.%2520Please%2520see%2520our%2520code%2520and%2520data%2520at%253A%250Ahttp%253A//hxhcreate.github.io/VLSBench%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLSBench%3A%20Unveiling%20Visual%20Leakage%20in%20Multimodal%20Safety&entry.906535625=Xuhao%20Hu%20and%20Dongrui%20Liu%20and%20Hao%20Li%20and%20Xuanjing%20Huang%20and%20Jing%20Shao&entry.1292438233=%20%20Safety%20concerns%20of%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20gradually%0Abecome%20an%20important%20problem%20in%20various%20applications.%20Surprisingly%2C%20previous%0Aworks%20indicate%20a%20counter-intuitive%20phenomenon%20that%20using%20textual%20unlearning%20to%0Aalign%20MLLMs%20achieves%20comparable%20safety%20performances%20with%20MLLMs%20trained%20with%0Aimage-text%20pairs.%20To%20explain%20such%20a%20counter-intuitive%20phenomenon%2C%20we%20discover%20a%0Avisual%20safety%20information%20leakage%20%28VSIL%29%20problem%20in%20existing%20multimodal%20safety%0Abenchmarks%2C%20i.e.%2C%20the%20potentially%20risky%20and%20sensitive%20content%20in%20the%20image%20has%0Abeen%20revealed%20in%20the%20textual%20query.%20In%20this%20way%2C%20MLLMs%20can%20easily%20refuse%20these%0Asensitive%20text-image%20queries%20according%20to%20textual%20queries.%20However%2C%20image-text%0Apairs%20without%20VSIL%20are%20common%20in%20real-world%20scenarios%20and%20are%20overlooked%20by%0Aexisting%20multimodal%20safety%20benchmarks.%20To%20this%20end%2C%20we%20construct%20multimodal%0Avisual%20leakless%20safety%20benchmark%20%28VLSBench%29%20preventing%20visual%20safety%20leakage%0Afrom%20image%20to%20textual%20query%20with%202.4k%20image-text%20pairs.%20Experimental%20results%0Aindicate%20that%20VLSBench%20poses%20a%20significant%20challenge%20to%20both%20open-source%20and%0Aclose-source%20MLLMs%2C%20including%20LLaVA%2C%20Qwen2-VL%2C%20Llama3.2-Vision%2C%20and%20GPT-4o.%0AThis%20study%20demonstrates%20that%20textual%20alignment%20is%20enough%20for%20multimodal%20safety%0Ascenarios%20with%20VSIL%2C%20while%20multimodal%20alignment%20is%20a%20more%20promising%20solution%0Afor%20multimodal%20safety%20scenarios%20without%20VSIL.%20Please%20see%20our%20code%20and%20data%20at%3A%0Ahttp%3A//hxhcreate.github.io/VLSBench%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19939v1&entry.124074799=Read"},
{"title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics", "author": "Katharina Friedl and No\u00e9mie Jaquier and Jens Lundell and Tamim Asfour and Danica Kragic", "abstract": "  By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nnon-linear structure-preserving latent space and the associated low-dimensional\ndynamics. Our approach enables accurate long-term predictions of the\nhigh-dimensional dynamics of rigid and deformable systems with increased data\nefficiency by inferring interpretable and physically plausible reduced\nLagrangian models.\n", "link": "http://arxiv.org/abs/2410.18868v2", "date": "2024-11-29", "relevancy": 2.1323, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5422}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5265}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Riemannian%20Framework%20for%20Learning%20Reduced-order%20Lagrangian%20Dynamics&body=Title%3A%20A%20Riemannian%20Framework%20for%20Learning%20Reduced-order%20Lagrangian%20Dynamics%0AAuthor%3A%20Katharina%20Friedl%20and%20No%C3%A9mie%20Jaquier%20and%20Jens%20Lundell%20and%20Tamim%20Asfour%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20By%20incorporating%20physical%20consistency%20as%20inductive%20bias%2C%20deep%20neural%20networks%0Adisplay%20increased%20generalization%20capabilities%20and%20data%20efficiency%20in%20learning%0Anonlinear%20dynamic%20models.%20However%2C%20the%20complexity%20of%20these%20models%20generally%0Aincreases%20with%20the%20system%20dimensionality%2C%20requiring%20larger%20datasets%2C%20more%0Acomplex%20deep%20networks%2C%20and%20significant%20computational%20effort.%20We%20propose%20a%20novel%0Ageometric%20network%20architecture%20to%20learn%20physically-consistent%20reduced-order%0Adynamic%20parameters%20that%20accurately%20describe%20the%20original%20high-dimensional%0Asystem%20behavior.%20This%20is%20achieved%20by%20building%20on%20recent%20advances%20in%20model-order%0Areduction%20and%20by%20adopting%20a%20Riemannian%20perspective%20to%20jointly%20learn%20a%0Anon-linear%20structure-preserving%20latent%20space%20and%20the%20associated%20low-dimensional%0Adynamics.%20Our%20approach%20enables%20accurate%20long-term%20predictions%20of%20the%0Ahigh-dimensional%20dynamics%20of%20rigid%20and%20deformable%20systems%20with%20increased%20data%0Aefficiency%20by%20inferring%20interpretable%20and%20physically%20plausible%20reduced%0ALagrangian%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Riemannian%2520Framework%2520for%2520Learning%2520Reduced-order%2520Lagrangian%2520Dynamics%26entry.906535625%3DKatharina%2520Friedl%2520and%2520No%25C3%25A9mie%2520Jaquier%2520and%2520Jens%2520Lundell%2520and%2520Tamim%2520Asfour%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520By%2520incorporating%2520physical%2520consistency%2520as%2520inductive%2520bias%252C%2520deep%2520neural%2520networks%250Adisplay%2520increased%2520generalization%2520capabilities%2520and%2520data%2520efficiency%2520in%2520learning%250Anonlinear%2520dynamic%2520models.%2520However%252C%2520the%2520complexity%2520of%2520these%2520models%2520generally%250Aincreases%2520with%2520the%2520system%2520dimensionality%252C%2520requiring%2520larger%2520datasets%252C%2520more%250Acomplex%2520deep%2520networks%252C%2520and%2520significant%2520computational%2520effort.%2520We%2520propose%2520a%2520novel%250Ageometric%2520network%2520architecture%2520to%2520learn%2520physically-consistent%2520reduced-order%250Adynamic%2520parameters%2520that%2520accurately%2520describe%2520the%2520original%2520high-dimensional%250Asystem%2520behavior.%2520This%2520is%2520achieved%2520by%2520building%2520on%2520recent%2520advances%2520in%2520model-order%250Areduction%2520and%2520by%2520adopting%2520a%2520Riemannian%2520perspective%2520to%2520jointly%2520learn%2520a%250Anon-linear%2520structure-preserving%2520latent%2520space%2520and%2520the%2520associated%2520low-dimensional%250Adynamics.%2520Our%2520approach%2520enables%2520accurate%2520long-term%2520predictions%2520of%2520the%250Ahigh-dimensional%2520dynamics%2520of%2520rigid%2520and%2520deformable%2520systems%2520with%2520increased%2520data%250Aefficiency%2520by%2520inferring%2520interpretable%2520and%2520physically%2520plausible%2520reduced%250ALagrangian%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Riemannian%20Framework%20for%20Learning%20Reduced-order%20Lagrangian%20Dynamics&entry.906535625=Katharina%20Friedl%20and%20No%C3%A9mie%20Jaquier%20and%20Jens%20Lundell%20and%20Tamim%20Asfour%20and%20Danica%20Kragic&entry.1292438233=%20%20By%20incorporating%20physical%20consistency%20as%20inductive%20bias%2C%20deep%20neural%20networks%0Adisplay%20increased%20generalization%20capabilities%20and%20data%20efficiency%20in%20learning%0Anonlinear%20dynamic%20models.%20However%2C%20the%20complexity%20of%20these%20models%20generally%0Aincreases%20with%20the%20system%20dimensionality%2C%20requiring%20larger%20datasets%2C%20more%0Acomplex%20deep%20networks%2C%20and%20significant%20computational%20effort.%20We%20propose%20a%20novel%0Ageometric%20network%20architecture%20to%20learn%20physically-consistent%20reduced-order%0Adynamic%20parameters%20that%20accurately%20describe%20the%20original%20high-dimensional%0Asystem%20behavior.%20This%20is%20achieved%20by%20building%20on%20recent%20advances%20in%20model-order%0Areduction%20and%20by%20adopting%20a%20Riemannian%20perspective%20to%20jointly%20learn%20a%0Anon-linear%20structure-preserving%20latent%20space%20and%20the%20associated%20low-dimensional%0Adynamics.%20Our%20approach%20enables%20accurate%20long-term%20predictions%20of%20the%0Ahigh-dimensional%20dynamics%20of%20rigid%20and%20deformable%20systems%20with%20increased%20data%0Aefficiency%20by%20inferring%20interpretable%20and%20physically%20plausible%20reduced%0ALagrangian%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18868v2&entry.124074799=Read"},
{"title": "FairDD: Fair Dataset Distillation via Synchronized Matching", "author": "Qihang Zhou and Shenhao Fang and Shibo He and Wenchao Meng and Jiming Chen", "abstract": "  Condensing large datasets into smaller synthetic counterparts has\ndemonstrated its promise for image classification. However, previous research\nhas overlooked a crucial concern in image recognition: ensuring that models\ntrained on condensed datasets are unbiased towards protected attributes (PA),\nsuch as gender and race. Our investigation reveals that dataset distillation\n(DD) fails to alleviate the unfairness towards minority groups within original\ndatasets. Moreover, this bias typically worsens in the condensed datasets due\nto their smaller size. To bridge the research gap, we propose a novel fair\ndataset distillation (FDD) framework, namely FairDD, which can be seamlessly\napplied to diverse matching-based DD approaches, requiring no modifications to\ntheir original architectures. The key innovation of FairDD lies in\nsynchronously matching synthetic datasets to PA-wise groups of original\ndatasets, rather than indiscriminate alignment to the whole distributions in\nvanilla DDs, dominated by majority groups. This synchronized matching allows\nsynthetic datasets to avoid collapsing into majority groups and bootstrap their\nbalanced generation to all PA groups. Consequently, FairDD could effectively\nregularize vanilla DDs to favor biased generation toward minority groups while\nmaintaining the accuracy of target attributes. Theoretical analyses and\nextensive experimental evaluations demonstrate that FairDD significantly\nimproves fairness compared to vanilla DD methods, without sacrificing\nclassification accuracy. Its consistent superiority across diverse DDs,\nspanning Distribution and Gradient Matching, establishes it as a versatile FDD\napproach.\n", "link": "http://arxiv.org/abs/2411.19623v1", "date": "2024-11-29", "relevancy": 2.124, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5401}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5253}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairDD%3A%20Fair%20Dataset%20Distillation%20via%20Synchronized%20Matching&body=Title%3A%20FairDD%3A%20Fair%20Dataset%20Distillation%20via%20Synchronized%20Matching%0AAuthor%3A%20Qihang%20Zhou%20and%20Shenhao%20Fang%20and%20Shibo%20He%20and%20Wenchao%20Meng%20and%20Jiming%20Chen%0AAbstract%3A%20%20%20Condensing%20large%20datasets%20into%20smaller%20synthetic%20counterparts%20has%0Ademonstrated%20its%20promise%20for%20image%20classification.%20However%2C%20previous%20research%0Ahas%20overlooked%20a%20crucial%20concern%20in%20image%20recognition%3A%20ensuring%20that%20models%0Atrained%20on%20condensed%20datasets%20are%20unbiased%20towards%20protected%20attributes%20%28PA%29%2C%0Asuch%20as%20gender%20and%20race.%20Our%20investigation%20reveals%20that%20dataset%20distillation%0A%28DD%29%20fails%20to%20alleviate%20the%20unfairness%20towards%20minority%20groups%20within%20original%0Adatasets.%20Moreover%2C%20this%20bias%20typically%20worsens%20in%20the%20condensed%20datasets%20due%0Ato%20their%20smaller%20size.%20To%20bridge%20the%20research%20gap%2C%20we%20propose%20a%20novel%20fair%0Adataset%20distillation%20%28FDD%29%20framework%2C%20namely%20FairDD%2C%20which%20can%20be%20seamlessly%0Aapplied%20to%20diverse%20matching-based%20DD%20approaches%2C%20requiring%20no%20modifications%20to%0Atheir%20original%20architectures.%20The%20key%20innovation%20of%20FairDD%20lies%20in%0Asynchronously%20matching%20synthetic%20datasets%20to%20PA-wise%20groups%20of%20original%0Adatasets%2C%20rather%20than%20indiscriminate%20alignment%20to%20the%20whole%20distributions%20in%0Avanilla%20DDs%2C%20dominated%20by%20majority%20groups.%20This%20synchronized%20matching%20allows%0Asynthetic%20datasets%20to%20avoid%20collapsing%20into%20majority%20groups%20and%20bootstrap%20their%0Abalanced%20generation%20to%20all%20PA%20groups.%20Consequently%2C%20FairDD%20could%20effectively%0Aregularize%20vanilla%20DDs%20to%20favor%20biased%20generation%20toward%20minority%20groups%20while%0Amaintaining%20the%20accuracy%20of%20target%20attributes.%20Theoretical%20analyses%20and%0Aextensive%20experimental%20evaluations%20demonstrate%20that%20FairDD%20significantly%0Aimproves%20fairness%20compared%20to%20vanilla%20DD%20methods%2C%20without%20sacrificing%0Aclassification%20accuracy.%20Its%20consistent%20superiority%20across%20diverse%20DDs%2C%0Aspanning%20Distribution%20and%20Gradient%20Matching%2C%20establishes%20it%20as%20a%20versatile%20FDD%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairDD%253A%2520Fair%2520Dataset%2520Distillation%2520via%2520Synchronized%2520Matching%26entry.906535625%3DQihang%2520Zhou%2520and%2520Shenhao%2520Fang%2520and%2520Shibo%2520He%2520and%2520Wenchao%2520Meng%2520and%2520Jiming%2520Chen%26entry.1292438233%3D%2520%2520Condensing%2520large%2520datasets%2520into%2520smaller%2520synthetic%2520counterparts%2520has%250Ademonstrated%2520its%2520promise%2520for%2520image%2520classification.%2520However%252C%2520previous%2520research%250Ahas%2520overlooked%2520a%2520crucial%2520concern%2520in%2520image%2520recognition%253A%2520ensuring%2520that%2520models%250Atrained%2520on%2520condensed%2520datasets%2520are%2520unbiased%2520towards%2520protected%2520attributes%2520%2528PA%2529%252C%250Asuch%2520as%2520gender%2520and%2520race.%2520Our%2520investigation%2520reveals%2520that%2520dataset%2520distillation%250A%2528DD%2529%2520fails%2520to%2520alleviate%2520the%2520unfairness%2520towards%2520minority%2520groups%2520within%2520original%250Adatasets.%2520Moreover%252C%2520this%2520bias%2520typically%2520worsens%2520in%2520the%2520condensed%2520datasets%2520due%250Ato%2520their%2520smaller%2520size.%2520To%2520bridge%2520the%2520research%2520gap%252C%2520we%2520propose%2520a%2520novel%2520fair%250Adataset%2520distillation%2520%2528FDD%2529%2520framework%252C%2520namely%2520FairDD%252C%2520which%2520can%2520be%2520seamlessly%250Aapplied%2520to%2520diverse%2520matching-based%2520DD%2520approaches%252C%2520requiring%2520no%2520modifications%2520to%250Atheir%2520original%2520architectures.%2520The%2520key%2520innovation%2520of%2520FairDD%2520lies%2520in%250Asynchronously%2520matching%2520synthetic%2520datasets%2520to%2520PA-wise%2520groups%2520of%2520original%250Adatasets%252C%2520rather%2520than%2520indiscriminate%2520alignment%2520to%2520the%2520whole%2520distributions%2520in%250Avanilla%2520DDs%252C%2520dominated%2520by%2520majority%2520groups.%2520This%2520synchronized%2520matching%2520allows%250Asynthetic%2520datasets%2520to%2520avoid%2520collapsing%2520into%2520majority%2520groups%2520and%2520bootstrap%2520their%250Abalanced%2520generation%2520to%2520all%2520PA%2520groups.%2520Consequently%252C%2520FairDD%2520could%2520effectively%250Aregularize%2520vanilla%2520DDs%2520to%2520favor%2520biased%2520generation%2520toward%2520minority%2520groups%2520while%250Amaintaining%2520the%2520accuracy%2520of%2520target%2520attributes.%2520Theoretical%2520analyses%2520and%250Aextensive%2520experimental%2520evaluations%2520demonstrate%2520that%2520FairDD%2520significantly%250Aimproves%2520fairness%2520compared%2520to%2520vanilla%2520DD%2520methods%252C%2520without%2520sacrificing%250Aclassification%2520accuracy.%2520Its%2520consistent%2520superiority%2520across%2520diverse%2520DDs%252C%250Aspanning%2520Distribution%2520and%2520Gradient%2520Matching%252C%2520establishes%2520it%2520as%2520a%2520versatile%2520FDD%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairDD%3A%20Fair%20Dataset%20Distillation%20via%20Synchronized%20Matching&entry.906535625=Qihang%20Zhou%20and%20Shenhao%20Fang%20and%20Shibo%20He%20and%20Wenchao%20Meng%20and%20Jiming%20Chen&entry.1292438233=%20%20Condensing%20large%20datasets%20into%20smaller%20synthetic%20counterparts%20has%0Ademonstrated%20its%20promise%20for%20image%20classification.%20However%2C%20previous%20research%0Ahas%20overlooked%20a%20crucial%20concern%20in%20image%20recognition%3A%20ensuring%20that%20models%0Atrained%20on%20condensed%20datasets%20are%20unbiased%20towards%20protected%20attributes%20%28PA%29%2C%0Asuch%20as%20gender%20and%20race.%20Our%20investigation%20reveals%20that%20dataset%20distillation%0A%28DD%29%20fails%20to%20alleviate%20the%20unfairness%20towards%20minority%20groups%20within%20original%0Adatasets.%20Moreover%2C%20this%20bias%20typically%20worsens%20in%20the%20condensed%20datasets%20due%0Ato%20their%20smaller%20size.%20To%20bridge%20the%20research%20gap%2C%20we%20propose%20a%20novel%20fair%0Adataset%20distillation%20%28FDD%29%20framework%2C%20namely%20FairDD%2C%20which%20can%20be%20seamlessly%0Aapplied%20to%20diverse%20matching-based%20DD%20approaches%2C%20requiring%20no%20modifications%20to%0Atheir%20original%20architectures.%20The%20key%20innovation%20of%20FairDD%20lies%20in%0Asynchronously%20matching%20synthetic%20datasets%20to%20PA-wise%20groups%20of%20original%0Adatasets%2C%20rather%20than%20indiscriminate%20alignment%20to%20the%20whole%20distributions%20in%0Avanilla%20DDs%2C%20dominated%20by%20majority%20groups.%20This%20synchronized%20matching%20allows%0Asynthetic%20datasets%20to%20avoid%20collapsing%20into%20majority%20groups%20and%20bootstrap%20their%0Abalanced%20generation%20to%20all%20PA%20groups.%20Consequently%2C%20FairDD%20could%20effectively%0Aregularize%20vanilla%20DDs%20to%20favor%20biased%20generation%20toward%20minority%20groups%20while%0Amaintaining%20the%20accuracy%20of%20target%20attributes.%20Theoretical%20analyses%20and%0Aextensive%20experimental%20evaluations%20demonstrate%20that%20FairDD%20significantly%0Aimproves%20fairness%20compared%20to%20vanilla%20DD%20methods%2C%20without%20sacrificing%0Aclassification%20accuracy.%20Its%20consistent%20superiority%20across%20diverse%20DDs%2C%0Aspanning%20Distribution%20and%20Gradient%20Matching%2C%20establishes%20it%20as%20a%20versatile%20FDD%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19623v1&entry.124074799=Read"},
{"title": "HUPE: Heuristic Underwater Perceptual Enhancement with Semantic\n  Collaborative Learning", "author": "Zengxi Zhang and Zhiying Jiang and Long Ma and Jinyuan Liu and Xin Fan and Risheng Liu", "abstract": "  Underwater images are often affected by light refraction and absorption,\nreducing visibility and interfering with subsequent applications. Existing\nunderwater image enhancement methods primarily focus on improving visual\nquality while overlooking practical implications. To strike a balance between\nvisual quality and application, we propose a heuristic invertible network for\nunderwater perception enhancement, dubbed HUPE, which enhances visual quality\nand demonstrates flexibility in handling other downstream tasks. Specifically,\nwe introduced an information-preserving reversible transformation with embedded\nFourier transform to establish a bidirectional mapping between underwater\nimages and their clear images. Additionally, a heuristic prior is incorporated\ninto the enhancement process to better capture scene information. To further\nbridge the feature gap between vision-based enhancement images and\napplication-oriented images, a semantic collaborative learning module is\napplied in the joint optimization process of the visual enhancement task and\nthe downstream task, which guides the proposed enhancement model to extract\nmore task-oriented semantic features while obtaining visually pleasing images.\nExtensive experiments, both quantitative and qualitative, demonstrate the\nsuperiority of our HUPE over state-of-the-art methods. The source code is\navailable at https://github.com/ZengxiZhang/HUPE.\n", "link": "http://arxiv.org/abs/2411.18296v2", "date": "2024-11-29", "relevancy": 2.1229, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5544}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HUPE%3A%20Heuristic%20Underwater%20Perceptual%20Enhancement%20with%20Semantic%0A%20%20Collaborative%20Learning&body=Title%3A%20HUPE%3A%20Heuristic%20Underwater%20Perceptual%20Enhancement%20with%20Semantic%0A%20%20Collaborative%20Learning%0AAuthor%3A%20Zengxi%20Zhang%20and%20Zhiying%20Jiang%20and%20Long%20Ma%20and%20Jinyuan%20Liu%20and%20Xin%20Fan%20and%20Risheng%20Liu%0AAbstract%3A%20%20%20Underwater%20images%20are%20often%20affected%20by%20light%20refraction%20and%20absorption%2C%0Areducing%20visibility%20and%20interfering%20with%20subsequent%20applications.%20Existing%0Aunderwater%20image%20enhancement%20methods%20primarily%20focus%20on%20improving%20visual%0Aquality%20while%20overlooking%20practical%20implications.%20To%20strike%20a%20balance%20between%0Avisual%20quality%20and%20application%2C%20we%20propose%20a%20heuristic%20invertible%20network%20for%0Aunderwater%20perception%20enhancement%2C%20dubbed%20HUPE%2C%20which%20enhances%20visual%20quality%0Aand%20demonstrates%20flexibility%20in%20handling%20other%20downstream%20tasks.%20Specifically%2C%0Awe%20introduced%20an%20information-preserving%20reversible%20transformation%20with%20embedded%0AFourier%20transform%20to%20establish%20a%20bidirectional%20mapping%20between%20underwater%0Aimages%20and%20their%20clear%20images.%20Additionally%2C%20a%20heuristic%20prior%20is%20incorporated%0Ainto%20the%20enhancement%20process%20to%20better%20capture%20scene%20information.%20To%20further%0Abridge%20the%20feature%20gap%20between%20vision-based%20enhancement%20images%20and%0Aapplication-oriented%20images%2C%20a%20semantic%20collaborative%20learning%20module%20is%0Aapplied%20in%20the%20joint%20optimization%20process%20of%20the%20visual%20enhancement%20task%20and%0Athe%20downstream%20task%2C%20which%20guides%20the%20proposed%20enhancement%20model%20to%20extract%0Amore%20task-oriented%20semantic%20features%20while%20obtaining%20visually%20pleasing%20images.%0AExtensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20demonstrate%20the%0Asuperiority%20of%20our%20HUPE%20over%20state-of-the-art%20methods.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/ZengxiZhang/HUPE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18296v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHUPE%253A%2520Heuristic%2520Underwater%2520Perceptual%2520Enhancement%2520with%2520Semantic%250A%2520%2520Collaborative%2520Learning%26entry.906535625%3DZengxi%2520Zhang%2520and%2520Zhiying%2520Jiang%2520and%2520Long%2520Ma%2520and%2520Jinyuan%2520Liu%2520and%2520Xin%2520Fan%2520and%2520Risheng%2520Liu%26entry.1292438233%3D%2520%2520Underwater%2520images%2520are%2520often%2520affected%2520by%2520light%2520refraction%2520and%2520absorption%252C%250Areducing%2520visibility%2520and%2520interfering%2520with%2520subsequent%2520applications.%2520Existing%250Aunderwater%2520image%2520enhancement%2520methods%2520primarily%2520focus%2520on%2520improving%2520visual%250Aquality%2520while%2520overlooking%2520practical%2520implications.%2520To%2520strike%2520a%2520balance%2520between%250Avisual%2520quality%2520and%2520application%252C%2520we%2520propose%2520a%2520heuristic%2520invertible%2520network%2520for%250Aunderwater%2520perception%2520enhancement%252C%2520dubbed%2520HUPE%252C%2520which%2520enhances%2520visual%2520quality%250Aand%2520demonstrates%2520flexibility%2520in%2520handling%2520other%2520downstream%2520tasks.%2520Specifically%252C%250Awe%2520introduced%2520an%2520information-preserving%2520reversible%2520transformation%2520with%2520embedded%250AFourier%2520transform%2520to%2520establish%2520a%2520bidirectional%2520mapping%2520between%2520underwater%250Aimages%2520and%2520their%2520clear%2520images.%2520Additionally%252C%2520a%2520heuristic%2520prior%2520is%2520incorporated%250Ainto%2520the%2520enhancement%2520process%2520to%2520better%2520capture%2520scene%2520information.%2520To%2520further%250Abridge%2520the%2520feature%2520gap%2520between%2520vision-based%2520enhancement%2520images%2520and%250Aapplication-oriented%2520images%252C%2520a%2520semantic%2520collaborative%2520learning%2520module%2520is%250Aapplied%2520in%2520the%2520joint%2520optimization%2520process%2520of%2520the%2520visual%2520enhancement%2520task%2520and%250Athe%2520downstream%2520task%252C%2520which%2520guides%2520the%2520proposed%2520enhancement%2520model%2520to%2520extract%250Amore%2520task-oriented%2520semantic%2520features%2520while%2520obtaining%2520visually%2520pleasing%2520images.%250AExtensive%2520experiments%252C%2520both%2520quantitative%2520and%2520qualitative%252C%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520HUPE%2520over%2520state-of-the-art%2520methods.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/ZengxiZhang/HUPE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18296v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HUPE%3A%20Heuristic%20Underwater%20Perceptual%20Enhancement%20with%20Semantic%0A%20%20Collaborative%20Learning&entry.906535625=Zengxi%20Zhang%20and%20Zhiying%20Jiang%20and%20Long%20Ma%20and%20Jinyuan%20Liu%20and%20Xin%20Fan%20and%20Risheng%20Liu&entry.1292438233=%20%20Underwater%20images%20are%20often%20affected%20by%20light%20refraction%20and%20absorption%2C%0Areducing%20visibility%20and%20interfering%20with%20subsequent%20applications.%20Existing%0Aunderwater%20image%20enhancement%20methods%20primarily%20focus%20on%20improving%20visual%0Aquality%20while%20overlooking%20practical%20implications.%20To%20strike%20a%20balance%20between%0Avisual%20quality%20and%20application%2C%20we%20propose%20a%20heuristic%20invertible%20network%20for%0Aunderwater%20perception%20enhancement%2C%20dubbed%20HUPE%2C%20which%20enhances%20visual%20quality%0Aand%20demonstrates%20flexibility%20in%20handling%20other%20downstream%20tasks.%20Specifically%2C%0Awe%20introduced%20an%20information-preserving%20reversible%20transformation%20with%20embedded%0AFourier%20transform%20to%20establish%20a%20bidirectional%20mapping%20between%20underwater%0Aimages%20and%20their%20clear%20images.%20Additionally%2C%20a%20heuristic%20prior%20is%20incorporated%0Ainto%20the%20enhancement%20process%20to%20better%20capture%20scene%20information.%20To%20further%0Abridge%20the%20feature%20gap%20between%20vision-based%20enhancement%20images%20and%0Aapplication-oriented%20images%2C%20a%20semantic%20collaborative%20learning%20module%20is%0Aapplied%20in%20the%20joint%20optimization%20process%20of%20the%20visual%20enhancement%20task%20and%0Athe%20downstream%20task%2C%20which%20guides%20the%20proposed%20enhancement%20model%20to%20extract%0Amore%20task-oriented%20semantic%20features%20while%20obtaining%20visually%20pleasing%20images.%0AExtensive%20experiments%2C%20both%20quantitative%20and%20qualitative%2C%20demonstrate%20the%0Asuperiority%20of%20our%20HUPE%20over%20state-of-the-art%20methods.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/ZengxiZhang/HUPE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18296v2&entry.124074799=Read"},
{"title": "Feedback-driven object detection and iterative model improvement", "author": "S\u00f6nke Tenckhoff and Mario Koddenbrock and Erik Rodner", "abstract": "  Automated object detection has become increasingly valuable across diverse\napplications, yet efficient, high-quality annotation remains a persistent\nchallenge. In this paper, we present the development and evaluation of a\nplatform designed to interactively improve object detection models. The\nplatform allows uploading and annotating images as well as fine-tuning object\ndetection models. Users can then manually review and refine annotations,\nfurther creating improved snapshots that are used for automatic object\ndetection on subsequent image uploads - a process we refer to as semi-automatic\nannotation resulting in a significant gain in annotation efficiency.\n  Whereas iterative refinement of model results to speed up annotation has\nbecome common practice, we are the first to quantitatively evaluate its\nbenefits with respect to time, effort, and interaction savings. Our\nexperimental results show clear evidence for a significant time reduction of up\nto 53% for semi-automatic compared to manual annotation. Importantly, these\nefficiency gains did not compromise annotation quality, while matching or\noccasionally even exceeding the accuracy of manual annotations. These findings\ndemonstrate the potential of our lightweight annotation platform for creating\nhigh-quality object detection datasets and provide best practices to guide\nfuture development of annotation platforms.\n  The platform is open-source, with the frontend and backend repositories\navailable on GitHub.\n", "link": "http://arxiv.org/abs/2411.19835v1", "date": "2024-11-29", "relevancy": 2.1148, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5689}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5223}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedback-driven%20object%20detection%20and%20iterative%20model%20improvement&body=Title%3A%20Feedback-driven%20object%20detection%20and%20iterative%20model%20improvement%0AAuthor%3A%20S%C3%B6nke%20Tenckhoff%20and%20Mario%20Koddenbrock%20and%20Erik%20Rodner%0AAbstract%3A%20%20%20Automated%20object%20detection%20has%20become%20increasingly%20valuable%20across%20diverse%0Aapplications%2C%20yet%20efficient%2C%20high-quality%20annotation%20remains%20a%20persistent%0Achallenge.%20In%20this%20paper%2C%20we%20present%20the%20development%20and%20evaluation%20of%20a%0Aplatform%20designed%20to%20interactively%20improve%20object%20detection%20models.%20The%0Aplatform%20allows%20uploading%20and%20annotating%20images%20as%20well%20as%20fine-tuning%20object%0Adetection%20models.%20Users%20can%20then%20manually%20review%20and%20refine%20annotations%2C%0Afurther%20creating%20improved%20snapshots%20that%20are%20used%20for%20automatic%20object%0Adetection%20on%20subsequent%20image%20uploads%20-%20a%20process%20we%20refer%20to%20as%20semi-automatic%0Aannotation%20resulting%20in%20a%20significant%20gain%20in%20annotation%20efficiency.%0A%20%20Whereas%20iterative%20refinement%20of%20model%20results%20to%20speed%20up%20annotation%20has%0Abecome%20common%20practice%2C%20we%20are%20the%20first%20to%20quantitatively%20evaluate%20its%0Abenefits%20with%20respect%20to%20time%2C%20effort%2C%20and%20interaction%20savings.%20Our%0Aexperimental%20results%20show%20clear%20evidence%20for%20a%20significant%20time%20reduction%20of%20up%0Ato%2053%25%20for%20semi-automatic%20compared%20to%20manual%20annotation.%20Importantly%2C%20these%0Aefficiency%20gains%20did%20not%20compromise%20annotation%20quality%2C%20while%20matching%20or%0Aoccasionally%20even%20exceeding%20the%20accuracy%20of%20manual%20annotations.%20These%20findings%0Ademonstrate%20the%20potential%20of%20our%20lightweight%20annotation%20platform%20for%20creating%0Ahigh-quality%20object%20detection%20datasets%20and%20provide%20best%20practices%20to%20guide%0Afuture%20development%20of%20annotation%20platforms.%0A%20%20The%20platform%20is%20open-source%2C%20with%20the%20frontend%20and%20backend%20repositories%0Aavailable%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedback-driven%2520object%2520detection%2520and%2520iterative%2520model%2520improvement%26entry.906535625%3DS%25C3%25B6nke%2520Tenckhoff%2520and%2520Mario%2520Koddenbrock%2520and%2520Erik%2520Rodner%26entry.1292438233%3D%2520%2520Automated%2520object%2520detection%2520has%2520become%2520increasingly%2520valuable%2520across%2520diverse%250Aapplications%252C%2520yet%2520efficient%252C%2520high-quality%2520annotation%2520remains%2520a%2520persistent%250Achallenge.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520development%2520and%2520evaluation%2520of%2520a%250Aplatform%2520designed%2520to%2520interactively%2520improve%2520object%2520detection%2520models.%2520The%250Aplatform%2520allows%2520uploading%2520and%2520annotating%2520images%2520as%2520well%2520as%2520fine-tuning%2520object%250Adetection%2520models.%2520Users%2520can%2520then%2520manually%2520review%2520and%2520refine%2520annotations%252C%250Afurther%2520creating%2520improved%2520snapshots%2520that%2520are%2520used%2520for%2520automatic%2520object%250Adetection%2520on%2520subsequent%2520image%2520uploads%2520-%2520a%2520process%2520we%2520refer%2520to%2520as%2520semi-automatic%250Aannotation%2520resulting%2520in%2520a%2520significant%2520gain%2520in%2520annotation%2520efficiency.%250A%2520%2520Whereas%2520iterative%2520refinement%2520of%2520model%2520results%2520to%2520speed%2520up%2520annotation%2520has%250Abecome%2520common%2520practice%252C%2520we%2520are%2520the%2520first%2520to%2520quantitatively%2520evaluate%2520its%250Abenefits%2520with%2520respect%2520to%2520time%252C%2520effort%252C%2520and%2520interaction%2520savings.%2520Our%250Aexperimental%2520results%2520show%2520clear%2520evidence%2520for%2520a%2520significant%2520time%2520reduction%2520of%2520up%250Ato%252053%2525%2520for%2520semi-automatic%2520compared%2520to%2520manual%2520annotation.%2520Importantly%252C%2520these%250Aefficiency%2520gains%2520did%2520not%2520compromise%2520annotation%2520quality%252C%2520while%2520matching%2520or%250Aoccasionally%2520even%2520exceeding%2520the%2520accuracy%2520of%2520manual%2520annotations.%2520These%2520findings%250Ademonstrate%2520the%2520potential%2520of%2520our%2520lightweight%2520annotation%2520platform%2520for%2520creating%250Ahigh-quality%2520object%2520detection%2520datasets%2520and%2520provide%2520best%2520practices%2520to%2520guide%250Afuture%2520development%2520of%2520annotation%2520platforms.%250A%2520%2520The%2520platform%2520is%2520open-source%252C%2520with%2520the%2520frontend%2520and%2520backend%2520repositories%250Aavailable%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedback-driven%20object%20detection%20and%20iterative%20model%20improvement&entry.906535625=S%C3%B6nke%20Tenckhoff%20and%20Mario%20Koddenbrock%20and%20Erik%20Rodner&entry.1292438233=%20%20Automated%20object%20detection%20has%20become%20increasingly%20valuable%20across%20diverse%0Aapplications%2C%20yet%20efficient%2C%20high-quality%20annotation%20remains%20a%20persistent%0Achallenge.%20In%20this%20paper%2C%20we%20present%20the%20development%20and%20evaluation%20of%20a%0Aplatform%20designed%20to%20interactively%20improve%20object%20detection%20models.%20The%0Aplatform%20allows%20uploading%20and%20annotating%20images%20as%20well%20as%20fine-tuning%20object%0Adetection%20models.%20Users%20can%20then%20manually%20review%20and%20refine%20annotations%2C%0Afurther%20creating%20improved%20snapshots%20that%20are%20used%20for%20automatic%20object%0Adetection%20on%20subsequent%20image%20uploads%20-%20a%20process%20we%20refer%20to%20as%20semi-automatic%0Aannotation%20resulting%20in%20a%20significant%20gain%20in%20annotation%20efficiency.%0A%20%20Whereas%20iterative%20refinement%20of%20model%20results%20to%20speed%20up%20annotation%20has%0Abecome%20common%20practice%2C%20we%20are%20the%20first%20to%20quantitatively%20evaluate%20its%0Abenefits%20with%20respect%20to%20time%2C%20effort%2C%20and%20interaction%20savings.%20Our%0Aexperimental%20results%20show%20clear%20evidence%20for%20a%20significant%20time%20reduction%20of%20up%0Ato%2053%25%20for%20semi-automatic%20compared%20to%20manual%20annotation.%20Importantly%2C%20these%0Aefficiency%20gains%20did%20not%20compromise%20annotation%20quality%2C%20while%20matching%20or%0Aoccasionally%20even%20exceeding%20the%20accuracy%20of%20manual%20annotations.%20These%20findings%0Ademonstrate%20the%20potential%20of%20our%20lightweight%20annotation%20platform%20for%20creating%0Ahigh-quality%20object%20detection%20datasets%20and%20provide%20best%20practices%20to%20guide%0Afuture%20development%20of%20annotation%20platforms.%0A%20%20The%20platform%20is%20open-source%2C%20with%20the%20frontend%20and%20backend%20repositories%0Aavailable%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19835v1&entry.124074799=Read"},
{"title": "Analysing Multiscale Clusterings with Persistent Homology", "author": "Dominik J. Schindler and Mauricio Barahona", "abstract": "  In many applications in data clustering, it is desirable to find not just a\nsingle partition into clusters but a sequence of partitions describing the data\nat different scales (or levels of coarseness). A natural problem then is to\nanalyse and compare the (not necessarily hierarchical) sequences of partitions\nthat underpin multiscale descriptions of data. Here, we introduce the\nMultiscale Clustering Filtration (MCF), a well-defined and stable filtration of\nabstract simplicial complexes that encodes arbitrary patterns of cluster\nassignments across scales of increasing coarseness. We show that the\nzero-dimensional persistent homology of the MCF measures the degree of\nhierarchy in the sequence of partitions, and the higher-dimensional persistent\nhomology tracks the emergence and resolution of conflicts between cluster\nassignments across the sequence of partitions. To broaden the theoretical\nfoundations of the MCF, we also provide an equivalent construction via a nerve\ncomplex filtration, and we show that in the hierarchical case, the MCF reduces\nto a Vietoris-Rips filtration of an ultrametric space. We then use numerical\nexperiments to illustrate how the MCF can serve to characterise multiscale\nclusterings of synthetic data from stochastic block models.\n", "link": "http://arxiv.org/abs/2305.04281v3", "date": "2024-11-29", "relevancy": 2.1104, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4362}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysing%20Multiscale%20Clusterings%20with%20Persistent%20Homology&body=Title%3A%20Analysing%20Multiscale%20Clusterings%20with%20Persistent%20Homology%0AAuthor%3A%20Dominik%20J.%20Schindler%20and%20Mauricio%20Barahona%0AAbstract%3A%20%20%20In%20many%20applications%20in%20data%20clustering%2C%20it%20is%20desirable%20to%20find%20not%20just%20a%0Asingle%20partition%20into%20clusters%20but%20a%20sequence%20of%20partitions%20describing%20the%20data%0Aat%20different%20scales%20%28or%20levels%20of%20coarseness%29.%20A%20natural%20problem%20then%20is%20to%0Aanalyse%20and%20compare%20the%20%28not%20necessarily%20hierarchical%29%20sequences%20of%20partitions%0Athat%20underpin%20multiscale%20descriptions%20of%20data.%20Here%2C%20we%20introduce%20the%0AMultiscale%20Clustering%20Filtration%20%28MCF%29%2C%20a%20well-defined%20and%20stable%20filtration%20of%0Aabstract%20simplicial%20complexes%20that%20encodes%20arbitrary%20patterns%20of%20cluster%0Aassignments%20across%20scales%20of%20increasing%20coarseness.%20We%20show%20that%20the%0Azero-dimensional%20persistent%20homology%20of%20the%20MCF%20measures%20the%20degree%20of%0Ahierarchy%20in%20the%20sequence%20of%20partitions%2C%20and%20the%20higher-dimensional%20persistent%0Ahomology%20tracks%20the%20emergence%20and%20resolution%20of%20conflicts%20between%20cluster%0Aassignments%20across%20the%20sequence%20of%20partitions.%20To%20broaden%20the%20theoretical%0Afoundations%20of%20the%20MCF%2C%20we%20also%20provide%20an%20equivalent%20construction%20via%20a%20nerve%0Acomplex%20filtration%2C%20and%20we%20show%20that%20in%20the%20hierarchical%20case%2C%20the%20MCF%20reduces%0Ato%20a%20Vietoris-Rips%20filtration%20of%20an%20ultrametric%20space.%20We%20then%20use%20numerical%0Aexperiments%20to%20illustrate%20how%20the%20MCF%20can%20serve%20to%20characterise%20multiscale%0Aclusterings%20of%20synthetic%20data%20from%20stochastic%20block%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.04281v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysing%2520Multiscale%2520Clusterings%2520with%2520Persistent%2520Homology%26entry.906535625%3DDominik%2520J.%2520Schindler%2520and%2520Mauricio%2520Barahona%26entry.1292438233%3D%2520%2520In%2520many%2520applications%2520in%2520data%2520clustering%252C%2520it%2520is%2520desirable%2520to%2520find%2520not%2520just%2520a%250Asingle%2520partition%2520into%2520clusters%2520but%2520a%2520sequence%2520of%2520partitions%2520describing%2520the%2520data%250Aat%2520different%2520scales%2520%2528or%2520levels%2520of%2520coarseness%2529.%2520A%2520natural%2520problem%2520then%2520is%2520to%250Aanalyse%2520and%2520compare%2520the%2520%2528not%2520necessarily%2520hierarchical%2529%2520sequences%2520of%2520partitions%250Athat%2520underpin%2520multiscale%2520descriptions%2520of%2520data.%2520Here%252C%2520we%2520introduce%2520the%250AMultiscale%2520Clustering%2520Filtration%2520%2528MCF%2529%252C%2520a%2520well-defined%2520and%2520stable%2520filtration%2520of%250Aabstract%2520simplicial%2520complexes%2520that%2520encodes%2520arbitrary%2520patterns%2520of%2520cluster%250Aassignments%2520across%2520scales%2520of%2520increasing%2520coarseness.%2520We%2520show%2520that%2520the%250Azero-dimensional%2520persistent%2520homology%2520of%2520the%2520MCF%2520measures%2520the%2520degree%2520of%250Ahierarchy%2520in%2520the%2520sequence%2520of%2520partitions%252C%2520and%2520the%2520higher-dimensional%2520persistent%250Ahomology%2520tracks%2520the%2520emergence%2520and%2520resolution%2520of%2520conflicts%2520between%2520cluster%250Aassignments%2520across%2520the%2520sequence%2520of%2520partitions.%2520To%2520broaden%2520the%2520theoretical%250Afoundations%2520of%2520the%2520MCF%252C%2520we%2520also%2520provide%2520an%2520equivalent%2520construction%2520via%2520a%2520nerve%250Acomplex%2520filtration%252C%2520and%2520we%2520show%2520that%2520in%2520the%2520hierarchical%2520case%252C%2520the%2520MCF%2520reduces%250Ato%2520a%2520Vietoris-Rips%2520filtration%2520of%2520an%2520ultrametric%2520space.%2520We%2520then%2520use%2520numerical%250Aexperiments%2520to%2520illustrate%2520how%2520the%2520MCF%2520can%2520serve%2520to%2520characterise%2520multiscale%250Aclusterings%2520of%2520synthetic%2520data%2520from%2520stochastic%2520block%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.04281v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysing%20Multiscale%20Clusterings%20with%20Persistent%20Homology&entry.906535625=Dominik%20J.%20Schindler%20and%20Mauricio%20Barahona&entry.1292438233=%20%20In%20many%20applications%20in%20data%20clustering%2C%20it%20is%20desirable%20to%20find%20not%20just%20a%0Asingle%20partition%20into%20clusters%20but%20a%20sequence%20of%20partitions%20describing%20the%20data%0Aat%20different%20scales%20%28or%20levels%20of%20coarseness%29.%20A%20natural%20problem%20then%20is%20to%0Aanalyse%20and%20compare%20the%20%28not%20necessarily%20hierarchical%29%20sequences%20of%20partitions%0Athat%20underpin%20multiscale%20descriptions%20of%20data.%20Here%2C%20we%20introduce%20the%0AMultiscale%20Clustering%20Filtration%20%28MCF%29%2C%20a%20well-defined%20and%20stable%20filtration%20of%0Aabstract%20simplicial%20complexes%20that%20encodes%20arbitrary%20patterns%20of%20cluster%0Aassignments%20across%20scales%20of%20increasing%20coarseness.%20We%20show%20that%20the%0Azero-dimensional%20persistent%20homology%20of%20the%20MCF%20measures%20the%20degree%20of%0Ahierarchy%20in%20the%20sequence%20of%20partitions%2C%20and%20the%20higher-dimensional%20persistent%0Ahomology%20tracks%20the%20emergence%20and%20resolution%20of%20conflicts%20between%20cluster%0Aassignments%20across%20the%20sequence%20of%20partitions.%20To%20broaden%20the%20theoretical%0Afoundations%20of%20the%20MCF%2C%20we%20also%20provide%20an%20equivalent%20construction%20via%20a%20nerve%0Acomplex%20filtration%2C%20and%20we%20show%20that%20in%20the%20hierarchical%20case%2C%20the%20MCF%20reduces%0Ato%20a%20Vietoris-Rips%20filtration%20of%20an%20ultrametric%20space.%20We%20then%20use%20numerical%0Aexperiments%20to%20illustrate%20how%20the%20MCF%20can%20serve%20to%20characterise%20multiscale%0Aclusterings%20of%20synthetic%20data%20from%20stochastic%20block%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.04281v3&entry.124074799=Read"},
{"title": "CAREL: Instruction-guided reinforcement learning with cross-modal\n  auxiliary objectives", "author": "Armin Saghafian and Amirmohammad Izadi and Negin Hashemi Dijujin and Mahdieh Soleymani Baghshah", "abstract": "  Grounding the instruction in the environment is a key step in solving\nlanguage-guided goal-reaching reinforcement learning problems. In automated\nreinforcement learning, a key concern is to enhance the model's ability to\ngeneralize across various tasks and environments. In goal-reaching scenarios,\nthe agent must comprehend the different parts of the instructions within the\nenvironmental context in order to complete the overall task successfully. In\nthis work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a\nnew framework to solve this problem using auxiliary loss functions inspired by\nvideo-text retrieval literature and a novel method called instruction tracking,\nwhich automatically keeps track of progress in an environment. The results of\nour experiments suggest superior sample efficiency and systematic\ngeneralization for this framework in multi-modal reinforcement learning\nproblems. Our code base is available here.\n", "link": "http://arxiv.org/abs/2411.19787v1", "date": "2024-11-29", "relevancy": 2.1101, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5307}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAREL%3A%20Instruction-guided%20reinforcement%20learning%20with%20cross-modal%0A%20%20auxiliary%20objectives&body=Title%3A%20CAREL%3A%20Instruction-guided%20reinforcement%20learning%20with%20cross-modal%0A%20%20auxiliary%20objectives%0AAuthor%3A%20Armin%20Saghafian%20and%20Amirmohammad%20Izadi%20and%20Negin%20Hashemi%20Dijujin%20and%20Mahdieh%20Soleymani%20Baghshah%0AAbstract%3A%20%20%20Grounding%20the%20instruction%20in%20the%20environment%20is%20a%20key%20step%20in%20solving%0Alanguage-guided%20goal-reaching%20reinforcement%20learning%20problems.%20In%20automated%0Areinforcement%20learning%2C%20a%20key%20concern%20is%20to%20enhance%20the%20model%27s%20ability%20to%0Ageneralize%20across%20various%20tasks%20and%20environments.%20In%20goal-reaching%20scenarios%2C%0Athe%20agent%20must%20comprehend%20the%20different%20parts%20of%20the%20instructions%20within%20the%0Aenvironmental%20context%20in%20order%20to%20complete%20the%20overall%20task%20successfully.%20In%0Athis%20work%2C%20we%20propose%20CAREL%20%28Cross-modal%20Auxiliary%20REinforcement%20Learning%29%20as%20a%0Anew%20framework%20to%20solve%20this%20problem%20using%20auxiliary%20loss%20functions%20inspired%20by%0Avideo-text%20retrieval%20literature%20and%20a%20novel%20method%20called%20instruction%20tracking%2C%0Awhich%20automatically%20keeps%20track%20of%20progress%20in%20an%20environment.%20The%20results%20of%0Aour%20experiments%20suggest%20superior%20sample%20efficiency%20and%20systematic%0Ageneralization%20for%20this%20framework%20in%20multi-modal%20reinforcement%20learning%0Aproblems.%20Our%20code%20base%20is%20available%20here.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAREL%253A%2520Instruction-guided%2520reinforcement%2520learning%2520with%2520cross-modal%250A%2520%2520auxiliary%2520objectives%26entry.906535625%3DArmin%2520Saghafian%2520and%2520Amirmohammad%2520Izadi%2520and%2520Negin%2520Hashemi%2520Dijujin%2520and%2520Mahdieh%2520Soleymani%2520Baghshah%26entry.1292438233%3D%2520%2520Grounding%2520the%2520instruction%2520in%2520the%2520environment%2520is%2520a%2520key%2520step%2520in%2520solving%250Alanguage-guided%2520goal-reaching%2520reinforcement%2520learning%2520problems.%2520In%2520automated%250Areinforcement%2520learning%252C%2520a%2520key%2520concern%2520is%2520to%2520enhance%2520the%2520model%2527s%2520ability%2520to%250Ageneralize%2520across%2520various%2520tasks%2520and%2520environments.%2520In%2520goal-reaching%2520scenarios%252C%250Athe%2520agent%2520must%2520comprehend%2520the%2520different%2520parts%2520of%2520the%2520instructions%2520within%2520the%250Aenvironmental%2520context%2520in%2520order%2520to%2520complete%2520the%2520overall%2520task%2520successfully.%2520In%250Athis%2520work%252C%2520we%2520propose%2520CAREL%2520%2528Cross-modal%2520Auxiliary%2520REinforcement%2520Learning%2529%2520as%2520a%250Anew%2520framework%2520to%2520solve%2520this%2520problem%2520using%2520auxiliary%2520loss%2520functions%2520inspired%2520by%250Avideo-text%2520retrieval%2520literature%2520and%2520a%2520novel%2520method%2520called%2520instruction%2520tracking%252C%250Awhich%2520automatically%2520keeps%2520track%2520of%2520progress%2520in%2520an%2520environment.%2520The%2520results%2520of%250Aour%2520experiments%2520suggest%2520superior%2520sample%2520efficiency%2520and%2520systematic%250Ageneralization%2520for%2520this%2520framework%2520in%2520multi-modal%2520reinforcement%2520learning%250Aproblems.%2520Our%2520code%2520base%2520is%2520available%2520here.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAREL%3A%20Instruction-guided%20reinforcement%20learning%20with%20cross-modal%0A%20%20auxiliary%20objectives&entry.906535625=Armin%20Saghafian%20and%20Amirmohammad%20Izadi%20and%20Negin%20Hashemi%20Dijujin%20and%20Mahdieh%20Soleymani%20Baghshah&entry.1292438233=%20%20Grounding%20the%20instruction%20in%20the%20environment%20is%20a%20key%20step%20in%20solving%0Alanguage-guided%20goal-reaching%20reinforcement%20learning%20problems.%20In%20automated%0Areinforcement%20learning%2C%20a%20key%20concern%20is%20to%20enhance%20the%20model%27s%20ability%20to%0Ageneralize%20across%20various%20tasks%20and%20environments.%20In%20goal-reaching%20scenarios%2C%0Athe%20agent%20must%20comprehend%20the%20different%20parts%20of%20the%20instructions%20within%20the%0Aenvironmental%20context%20in%20order%20to%20complete%20the%20overall%20task%20successfully.%20In%0Athis%20work%2C%20we%20propose%20CAREL%20%28Cross-modal%20Auxiliary%20REinforcement%20Learning%29%20as%20a%0Anew%20framework%20to%20solve%20this%20problem%20using%20auxiliary%20loss%20functions%20inspired%20by%0Avideo-text%20retrieval%20literature%20and%20a%20novel%20method%20called%20instruction%20tracking%2C%0Awhich%20automatically%20keeps%20track%20of%20progress%20in%20an%20environment.%20The%20results%20of%0Aour%20experiments%20suggest%20superior%20sample%20efficiency%20and%20systematic%0Ageneralization%20for%20this%20framework%20in%20multi-modal%20reinforcement%20learning%0Aproblems.%20Our%20code%20base%20is%20available%20here.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19787v1&entry.124074799=Read"},
{"title": "AIDetx: a compression-based method for identification of\n  machine-learning generated text", "author": "Leonardo Almeida and Pedro Rodrigues and Diogo Magalh\u00e3es and Armando J. Pinho and Diogo Pratas", "abstract": "  This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx.\n", "link": "http://arxiv.org/abs/2411.19869v1", "date": "2024-11-29", "relevancy": 2.1091, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5406}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5249}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIDetx%3A%20a%20compression-based%20method%20for%20identification%20of%0A%20%20machine-learning%20generated%20text&body=Title%3A%20AIDetx%3A%20a%20compression-based%20method%20for%20identification%20of%0A%20%20machine-learning%20generated%20text%0AAuthor%3A%20Leonardo%20Almeida%20and%20Pedro%20Rodrigues%20and%20Diogo%20Magalh%C3%A3es%20and%20Armando%20J.%20Pinho%20and%20Diogo%20Pratas%0AAbstract%3A%20%20%20This%20paper%20introduces%20AIDetx%2C%20a%20novel%20method%20for%20detecting%20machine-generated%0Atext%20using%20data%20compression%20techniques.%20Traditional%20approaches%2C%20such%20as%20deep%0Alearning%20classifiers%2C%20often%20suffer%20from%20high%20computational%20costs%20and%20limited%0Ainterpretability.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20compression-based%0Aclassification%20framework%20that%20leverages%20finite-context%20models%20%28FCMs%29.%20AIDetx%0Aconstructs%20distinct%20compression%20models%20for%20human-written%20and%20AI-generated%20text%2C%0Aclassifying%20new%20inputs%20based%20on%20which%20model%20achieves%20a%20higher%20compression%0Aratio.%20We%20evaluated%20AIDetx%20on%20two%20benchmark%20datasets%2C%20achieving%20F1%20scores%0Aexceeding%2097%25%20and%2099%25%2C%20respectively%2C%20highlighting%20its%20high%20accuracy.%20Compared%0Ato%20current%20methods%2C%20such%20as%20large%20language%20models%20%28LLMs%29%2C%20AIDetx%20offers%20a%20more%0Ainterpretable%20and%20computationally%20efficient%20solution%2C%20significantly%20reducing%0Aboth%20training%20time%20and%20hardware%20requirements%20%28e.g.%2C%20no%20GPUs%20needed%29.%20The%20full%0Aimplementation%20is%20publicly%20available%20at%20https%3A//github.com/AIDetx/AIDetx.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIDetx%253A%2520a%2520compression-based%2520method%2520for%2520identification%2520of%250A%2520%2520machine-learning%2520generated%2520text%26entry.906535625%3DLeonardo%2520Almeida%2520and%2520Pedro%2520Rodrigues%2520and%2520Diogo%2520Magalh%25C3%25A3es%2520and%2520Armando%2520J.%2520Pinho%2520and%2520Diogo%2520Pratas%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520AIDetx%252C%2520a%2520novel%2520method%2520for%2520detecting%2520machine-generated%250Atext%2520using%2520data%2520compression%2520techniques.%2520Traditional%2520approaches%252C%2520such%2520as%2520deep%250Alearning%2520classifiers%252C%2520often%2520suffer%2520from%2520high%2520computational%2520costs%2520and%2520limited%250Ainterpretability.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520compression-based%250Aclassification%2520framework%2520that%2520leverages%2520finite-context%2520models%2520%2528FCMs%2529.%2520AIDetx%250Aconstructs%2520distinct%2520compression%2520models%2520for%2520human-written%2520and%2520AI-generated%2520text%252C%250Aclassifying%2520new%2520inputs%2520based%2520on%2520which%2520model%2520achieves%2520a%2520higher%2520compression%250Aratio.%2520We%2520evaluated%2520AIDetx%2520on%2520two%2520benchmark%2520datasets%252C%2520achieving%2520F1%2520scores%250Aexceeding%252097%2525%2520and%252099%2525%252C%2520respectively%252C%2520highlighting%2520its%2520high%2520accuracy.%2520Compared%250Ato%2520current%2520methods%252C%2520such%2520as%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520AIDetx%2520offers%2520a%2520more%250Ainterpretable%2520and%2520computationally%2520efficient%2520solution%252C%2520significantly%2520reducing%250Aboth%2520training%2520time%2520and%2520hardware%2520requirements%2520%2528e.g.%252C%2520no%2520GPUs%2520needed%2529.%2520The%2520full%250Aimplementation%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/AIDetx/AIDetx.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIDetx%3A%20a%20compression-based%20method%20for%20identification%20of%0A%20%20machine-learning%20generated%20text&entry.906535625=Leonardo%20Almeida%20and%20Pedro%20Rodrigues%20and%20Diogo%20Magalh%C3%A3es%20and%20Armando%20J.%20Pinho%20and%20Diogo%20Pratas&entry.1292438233=%20%20This%20paper%20introduces%20AIDetx%2C%20a%20novel%20method%20for%20detecting%20machine-generated%0Atext%20using%20data%20compression%20techniques.%20Traditional%20approaches%2C%20such%20as%20deep%0Alearning%20classifiers%2C%20often%20suffer%20from%20high%20computational%20costs%20and%20limited%0Ainterpretability.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20compression-based%0Aclassification%20framework%20that%20leverages%20finite-context%20models%20%28FCMs%29.%20AIDetx%0Aconstructs%20distinct%20compression%20models%20for%20human-written%20and%20AI-generated%20text%2C%0Aclassifying%20new%20inputs%20based%20on%20which%20model%20achieves%20a%20higher%20compression%0Aratio.%20We%20evaluated%20AIDetx%20on%20two%20benchmark%20datasets%2C%20achieving%20F1%20scores%0Aexceeding%2097%25%20and%2099%25%2C%20respectively%2C%20highlighting%20its%20high%20accuracy.%20Compared%0Ato%20current%20methods%2C%20such%20as%20large%20language%20models%20%28LLMs%29%2C%20AIDetx%20offers%20a%20more%0Ainterpretable%20and%20computationally%20efficient%20solution%2C%20significantly%20reducing%0Aboth%20training%20time%20and%20hardware%20requirements%20%28e.g.%2C%20no%20GPUs%20needed%29.%20The%20full%0Aimplementation%20is%20publicly%20available%20at%20https%3A//github.com/AIDetx/AIDetx.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19869v1&entry.124074799=Read"},
{"title": "Safe, Out-of-Distribution-Adaptive MPC with Conformalized Neural Network\n  Ensembles", "author": "Polo Contreras and Ola Shorinwa and Mac Schwager", "abstract": "  We present SODA-MPC, a Safe, Out-of-Distribution-Adaptive Model Predictive\nControl algorithm, which uses an ensemble of learned models for prediction,\nwith a runtime monitor to flag unreliable out-of-distribution (OOD)\npredictions. When an OOD situation is detected, SODA-MPC triggers a safe\nfallback control strategy based on reachability, yielding a control framework\nthat achieves the high performance of learning-based models while preserving\nthe safety of reachability-based control. We demonstrate the method in the\ncontext of an autonomous vehicle, driving among dynamic pedestrians, where\nSODA-MPC uses a neural network ensemble for pedestrian prediction. We calibrate\nthe OOD signal using conformal prediction to derive an OOD detector with\nprobabilistic guarantees on the false-positive rate, given a user-specified\nconfidence level. During in-distribution operation, the MPC controller avoids\ncollisions with a pedestrian based on the trajectory predicted by the mean of\nthe ensemble. When OOD conditions are detected, the MPC switches to a\nreachability-based controller to avoid collisions with the reachable set of the\npedestrian assuming a maximum pedestrian speed, to guarantee safety under the\nworst-case actions of the pedestrian. We verify SODA-MPC in extensive\nautonomous driving simulations in a pedestrian-crossing scenario. Our model\nensemble is trained and calibrated with real pedestrian data, showing that our\nOOD detector obtains the desired accuracy rate within a theoretically-predicted\nrange. We empirically show improved safety and improved task completion\ncompared with two state-of-the-art MPC methods that also use conformal\nprediction, but without OOD adaptation. Further, we demonstrate the\neffectiveness of our method with the large-scale multi-agent predictor\nTrajectron++, using large-scale traffic data from the nuScenes dataset for\ntraining and calibration.\n", "link": "http://arxiv.org/abs/2406.02436v2", "date": "2024-11-29", "relevancy": 2.1047, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.575}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5263}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%2C%20Out-of-Distribution-Adaptive%20MPC%20with%20Conformalized%20Neural%20Network%0A%20%20Ensembles&body=Title%3A%20Safe%2C%20Out-of-Distribution-Adaptive%20MPC%20with%20Conformalized%20Neural%20Network%0A%20%20Ensembles%0AAuthor%3A%20Polo%20Contreras%20and%20Ola%20Shorinwa%20and%20Mac%20Schwager%0AAbstract%3A%20%20%20We%20present%20SODA-MPC%2C%20a%20Safe%2C%20Out-of-Distribution-Adaptive%20Model%20Predictive%0AControl%20algorithm%2C%20which%20uses%20an%20ensemble%20of%20learned%20models%20for%20prediction%2C%0Awith%20a%20runtime%20monitor%20to%20flag%20unreliable%20out-of-distribution%20%28OOD%29%0Apredictions.%20When%20an%20OOD%20situation%20is%20detected%2C%20SODA-MPC%20triggers%20a%20safe%0Afallback%20control%20strategy%20based%20on%20reachability%2C%20yielding%20a%20control%20framework%0Athat%20achieves%20the%20high%20performance%20of%20learning-based%20models%20while%20preserving%0Athe%20safety%20of%20reachability-based%20control.%20We%20demonstrate%20the%20method%20in%20the%0Acontext%20of%20an%20autonomous%20vehicle%2C%20driving%20among%20dynamic%20pedestrians%2C%20where%0ASODA-MPC%20uses%20a%20neural%20network%20ensemble%20for%20pedestrian%20prediction.%20We%20calibrate%0Athe%20OOD%20signal%20using%20conformal%20prediction%20to%20derive%20an%20OOD%20detector%20with%0Aprobabilistic%20guarantees%20on%20the%20false-positive%20rate%2C%20given%20a%20user-specified%0Aconfidence%20level.%20During%20in-distribution%20operation%2C%20the%20MPC%20controller%20avoids%0Acollisions%20with%20a%20pedestrian%20based%20on%20the%20trajectory%20predicted%20by%20the%20mean%20of%0Athe%20ensemble.%20When%20OOD%20conditions%20are%20detected%2C%20the%20MPC%20switches%20to%20a%0Areachability-based%20controller%20to%20avoid%20collisions%20with%20the%20reachable%20set%20of%20the%0Apedestrian%20assuming%20a%20maximum%20pedestrian%20speed%2C%20to%20guarantee%20safety%20under%20the%0Aworst-case%20actions%20of%20the%20pedestrian.%20We%20verify%20SODA-MPC%20in%20extensive%0Aautonomous%20driving%20simulations%20in%20a%20pedestrian-crossing%20scenario.%20Our%20model%0Aensemble%20is%20trained%20and%20calibrated%20with%20real%20pedestrian%20data%2C%20showing%20that%20our%0AOOD%20detector%20obtains%20the%20desired%20accuracy%20rate%20within%20a%20theoretically-predicted%0Arange.%20We%20empirically%20show%20improved%20safety%20and%20improved%20task%20completion%0Acompared%20with%20two%20state-of-the-art%20MPC%20methods%20that%20also%20use%20conformal%0Aprediction%2C%20but%20without%20OOD%20adaptation.%20Further%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20with%20the%20large-scale%20multi-agent%20predictor%0ATrajectron%2B%2B%2C%20using%20large-scale%20traffic%20data%20from%20the%20nuScenes%20dataset%20for%0Atraining%20and%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%252C%2520Out-of-Distribution-Adaptive%2520MPC%2520with%2520Conformalized%2520Neural%2520Network%250A%2520%2520Ensembles%26entry.906535625%3DPolo%2520Contreras%2520and%2520Ola%2520Shorinwa%2520and%2520Mac%2520Schwager%26entry.1292438233%3D%2520%2520We%2520present%2520SODA-MPC%252C%2520a%2520Safe%252C%2520Out-of-Distribution-Adaptive%2520Model%2520Predictive%250AControl%2520algorithm%252C%2520which%2520uses%2520an%2520ensemble%2520of%2520learned%2520models%2520for%2520prediction%252C%250Awith%2520a%2520runtime%2520monitor%2520to%2520flag%2520unreliable%2520out-of-distribution%2520%2528OOD%2529%250Apredictions.%2520When%2520an%2520OOD%2520situation%2520is%2520detected%252C%2520SODA-MPC%2520triggers%2520a%2520safe%250Afallback%2520control%2520strategy%2520based%2520on%2520reachability%252C%2520yielding%2520a%2520control%2520framework%250Athat%2520achieves%2520the%2520high%2520performance%2520of%2520learning-based%2520models%2520while%2520preserving%250Athe%2520safety%2520of%2520reachability-based%2520control.%2520We%2520demonstrate%2520the%2520method%2520in%2520the%250Acontext%2520of%2520an%2520autonomous%2520vehicle%252C%2520driving%2520among%2520dynamic%2520pedestrians%252C%2520where%250ASODA-MPC%2520uses%2520a%2520neural%2520network%2520ensemble%2520for%2520pedestrian%2520prediction.%2520We%2520calibrate%250Athe%2520OOD%2520signal%2520using%2520conformal%2520prediction%2520to%2520derive%2520an%2520OOD%2520detector%2520with%250Aprobabilistic%2520guarantees%2520on%2520the%2520false-positive%2520rate%252C%2520given%2520a%2520user-specified%250Aconfidence%2520level.%2520During%2520in-distribution%2520operation%252C%2520the%2520MPC%2520controller%2520avoids%250Acollisions%2520with%2520a%2520pedestrian%2520based%2520on%2520the%2520trajectory%2520predicted%2520by%2520the%2520mean%2520of%250Athe%2520ensemble.%2520When%2520OOD%2520conditions%2520are%2520detected%252C%2520the%2520MPC%2520switches%2520to%2520a%250Areachability-based%2520controller%2520to%2520avoid%2520collisions%2520with%2520the%2520reachable%2520set%2520of%2520the%250Apedestrian%2520assuming%2520a%2520maximum%2520pedestrian%2520speed%252C%2520to%2520guarantee%2520safety%2520under%2520the%250Aworst-case%2520actions%2520of%2520the%2520pedestrian.%2520We%2520verify%2520SODA-MPC%2520in%2520extensive%250Aautonomous%2520driving%2520simulations%2520in%2520a%2520pedestrian-crossing%2520scenario.%2520Our%2520model%250Aensemble%2520is%2520trained%2520and%2520calibrated%2520with%2520real%2520pedestrian%2520data%252C%2520showing%2520that%2520our%250AOOD%2520detector%2520obtains%2520the%2520desired%2520accuracy%2520rate%2520within%2520a%2520theoretically-predicted%250Arange.%2520We%2520empirically%2520show%2520improved%2520safety%2520and%2520improved%2520task%2520completion%250Acompared%2520with%2520two%2520state-of-the-art%2520MPC%2520methods%2520that%2520also%2520use%2520conformal%250Aprediction%252C%2520but%2520without%2520OOD%2520adaptation.%2520Further%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520with%2520the%2520large-scale%2520multi-agent%2520predictor%250ATrajectron%252B%252B%252C%2520using%2520large-scale%2520traffic%2520data%2520from%2520the%2520nuScenes%2520dataset%2520for%250Atraining%2520and%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%2C%20Out-of-Distribution-Adaptive%20MPC%20with%20Conformalized%20Neural%20Network%0A%20%20Ensembles&entry.906535625=Polo%20Contreras%20and%20Ola%20Shorinwa%20and%20Mac%20Schwager&entry.1292438233=%20%20We%20present%20SODA-MPC%2C%20a%20Safe%2C%20Out-of-Distribution-Adaptive%20Model%20Predictive%0AControl%20algorithm%2C%20which%20uses%20an%20ensemble%20of%20learned%20models%20for%20prediction%2C%0Awith%20a%20runtime%20monitor%20to%20flag%20unreliable%20out-of-distribution%20%28OOD%29%0Apredictions.%20When%20an%20OOD%20situation%20is%20detected%2C%20SODA-MPC%20triggers%20a%20safe%0Afallback%20control%20strategy%20based%20on%20reachability%2C%20yielding%20a%20control%20framework%0Athat%20achieves%20the%20high%20performance%20of%20learning-based%20models%20while%20preserving%0Athe%20safety%20of%20reachability-based%20control.%20We%20demonstrate%20the%20method%20in%20the%0Acontext%20of%20an%20autonomous%20vehicle%2C%20driving%20among%20dynamic%20pedestrians%2C%20where%0ASODA-MPC%20uses%20a%20neural%20network%20ensemble%20for%20pedestrian%20prediction.%20We%20calibrate%0Athe%20OOD%20signal%20using%20conformal%20prediction%20to%20derive%20an%20OOD%20detector%20with%0Aprobabilistic%20guarantees%20on%20the%20false-positive%20rate%2C%20given%20a%20user-specified%0Aconfidence%20level.%20During%20in-distribution%20operation%2C%20the%20MPC%20controller%20avoids%0Acollisions%20with%20a%20pedestrian%20based%20on%20the%20trajectory%20predicted%20by%20the%20mean%20of%0Athe%20ensemble.%20When%20OOD%20conditions%20are%20detected%2C%20the%20MPC%20switches%20to%20a%0Areachability-based%20controller%20to%20avoid%20collisions%20with%20the%20reachable%20set%20of%20the%0Apedestrian%20assuming%20a%20maximum%20pedestrian%20speed%2C%20to%20guarantee%20safety%20under%20the%0Aworst-case%20actions%20of%20the%20pedestrian.%20We%20verify%20SODA-MPC%20in%20extensive%0Aautonomous%20driving%20simulations%20in%20a%20pedestrian-crossing%20scenario.%20Our%20model%0Aensemble%20is%20trained%20and%20calibrated%20with%20real%20pedestrian%20data%2C%20showing%20that%20our%0AOOD%20detector%20obtains%20the%20desired%20accuracy%20rate%20within%20a%20theoretically-predicted%0Arange.%20We%20empirically%20show%20improved%20safety%20and%20improved%20task%20completion%0Acompared%20with%20two%20state-of-the-art%20MPC%20methods%20that%20also%20use%20conformal%0Aprediction%2C%20but%20without%20OOD%20adaptation.%20Further%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20with%20the%20large-scale%20multi-agent%20predictor%0ATrajectron%2B%2B%2C%20using%20large-scale%20traffic%20data%20from%20the%20nuScenes%20dataset%20for%0Atraining%20and%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02436v2&entry.124074799=Read"},
{"title": "Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA\n  Benchmark", "author": "Joseph Heyward and Jo\u00e3o Carreira and Dima Damen and Andrew Zisserman and Viorica P\u0103tr\u0103ucean", "abstract": "  Following the successful 2023 edition, we organised the Second Perception\nTest challenge as a half-day workshop alongside the IEEE/CVF European\nConference on Computer Vision (ECCV) 2024, with the goal of benchmarking\nstate-of-the-art video models and measuring the progress since last year using\nthe Perception Test benchmark. This year, the challenge had seven tracks (up\nfrom six last year) and covered low-level and high-level tasks, with language\nand non-language interfaces, across video, audio, and text modalities; the\nadditional track covered hour-long video understanding and introduced a novel\nvideo QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks\nwere: object tracking, point tracking, temporal action localisation, temporal\nsound localisation, multiple-choice video question-answering, grounded video\nquestion-answering, and hour-long video question-answering. We summarise in\nthis report the challenge tasks and results, and introduce in detail the novel\nhour-long video QA benchmark 1h-walk VQA.\n", "link": "http://arxiv.org/abs/2411.19941v1", "date": "2024-11-29", "relevancy": 2.0955, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Test%202024%3A%20Challenge%20Summary%20and%20a%20Novel%20Hour-Long%20VideoQA%0A%20%20Benchmark&body=Title%3A%20Perception%20Test%202024%3A%20Challenge%20Summary%20and%20a%20Novel%20Hour-Long%20VideoQA%0A%20%20Benchmark%0AAuthor%3A%20Joseph%20Heyward%20and%20Jo%C3%A3o%20Carreira%20and%20Dima%20Damen%20and%20Andrew%20Zisserman%20and%20Viorica%20P%C4%83tr%C4%83ucean%0AAbstract%3A%20%20%20Following%20the%20successful%202023%20edition%2C%20we%20organised%20the%20Second%20Perception%0ATest%20challenge%20as%20a%20half-day%20workshop%20alongside%20the%20IEEE/CVF%20European%0AConference%20on%20Computer%20Vision%20%28ECCV%29%202024%2C%20with%20the%20goal%20of%20benchmarking%0Astate-of-the-art%20video%20models%20and%20measuring%20the%20progress%20since%20last%20year%20using%0Athe%20Perception%20Test%20benchmark.%20This%20year%2C%20the%20challenge%20had%20seven%20tracks%20%28up%0Afrom%20six%20last%20year%29%20and%20covered%20low-level%20and%20high-level%20tasks%2C%20with%20language%0Aand%20non-language%20interfaces%2C%20across%20video%2C%20audio%2C%20and%20text%20modalities%3B%20the%0Aadditional%20track%20covered%20hour-long%20video%20understanding%20and%20introduced%20a%20novel%0Avideo%20QA%20benchmark%201h-walk%20VQA.%20Overall%2C%20the%20tasks%20in%20the%20different%20tracks%0Awere%3A%20object%20tracking%2C%20point%20tracking%2C%20temporal%20action%20localisation%2C%20temporal%0Asound%20localisation%2C%20multiple-choice%20video%20question-answering%2C%20grounded%20video%0Aquestion-answering%2C%20and%20hour-long%20video%20question-answering.%20We%20summarise%20in%0Athis%20report%20the%20challenge%20tasks%20and%20results%2C%20and%20introduce%20in%20detail%20the%20novel%0Ahour-long%20video%20QA%20benchmark%201h-walk%20VQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Test%25202024%253A%2520Challenge%2520Summary%2520and%2520a%2520Novel%2520Hour-Long%2520VideoQA%250A%2520%2520Benchmark%26entry.906535625%3DJoseph%2520Heyward%2520and%2520Jo%25C3%25A3o%2520Carreira%2520and%2520Dima%2520Damen%2520and%2520Andrew%2520Zisserman%2520and%2520Viorica%2520P%25C4%2583tr%25C4%2583ucean%26entry.1292438233%3D%2520%2520Following%2520the%2520successful%25202023%2520edition%252C%2520we%2520organised%2520the%2520Second%2520Perception%250ATest%2520challenge%2520as%2520a%2520half-day%2520workshop%2520alongside%2520the%2520IEEE/CVF%2520European%250AConference%2520on%2520Computer%2520Vision%2520%2528ECCV%2529%25202024%252C%2520with%2520the%2520goal%2520of%2520benchmarking%250Astate-of-the-art%2520video%2520models%2520and%2520measuring%2520the%2520progress%2520since%2520last%2520year%2520using%250Athe%2520Perception%2520Test%2520benchmark.%2520This%2520year%252C%2520the%2520challenge%2520had%2520seven%2520tracks%2520%2528up%250Afrom%2520six%2520last%2520year%2529%2520and%2520covered%2520low-level%2520and%2520high-level%2520tasks%252C%2520with%2520language%250Aand%2520non-language%2520interfaces%252C%2520across%2520video%252C%2520audio%252C%2520and%2520text%2520modalities%253B%2520the%250Aadditional%2520track%2520covered%2520hour-long%2520video%2520understanding%2520and%2520introduced%2520a%2520novel%250Avideo%2520QA%2520benchmark%25201h-walk%2520VQA.%2520Overall%252C%2520the%2520tasks%2520in%2520the%2520different%2520tracks%250Awere%253A%2520object%2520tracking%252C%2520point%2520tracking%252C%2520temporal%2520action%2520localisation%252C%2520temporal%250Asound%2520localisation%252C%2520multiple-choice%2520video%2520question-answering%252C%2520grounded%2520video%250Aquestion-answering%252C%2520and%2520hour-long%2520video%2520question-answering.%2520We%2520summarise%2520in%250Athis%2520report%2520the%2520challenge%2520tasks%2520and%2520results%252C%2520and%2520introduce%2520in%2520detail%2520the%2520novel%250Ahour-long%2520video%2520QA%2520benchmark%25201h-walk%2520VQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Test%202024%3A%20Challenge%20Summary%20and%20a%20Novel%20Hour-Long%20VideoQA%0A%20%20Benchmark&entry.906535625=Joseph%20Heyward%20and%20Jo%C3%A3o%20Carreira%20and%20Dima%20Damen%20and%20Andrew%20Zisserman%20and%20Viorica%20P%C4%83tr%C4%83ucean&entry.1292438233=%20%20Following%20the%20successful%202023%20edition%2C%20we%20organised%20the%20Second%20Perception%0ATest%20challenge%20as%20a%20half-day%20workshop%20alongside%20the%20IEEE/CVF%20European%0AConference%20on%20Computer%20Vision%20%28ECCV%29%202024%2C%20with%20the%20goal%20of%20benchmarking%0Astate-of-the-art%20video%20models%20and%20measuring%20the%20progress%20since%20last%20year%20using%0Athe%20Perception%20Test%20benchmark.%20This%20year%2C%20the%20challenge%20had%20seven%20tracks%20%28up%0Afrom%20six%20last%20year%29%20and%20covered%20low-level%20and%20high-level%20tasks%2C%20with%20language%0Aand%20non-language%20interfaces%2C%20across%20video%2C%20audio%2C%20and%20text%20modalities%3B%20the%0Aadditional%20track%20covered%20hour-long%20video%20understanding%20and%20introduced%20a%20novel%0Avideo%20QA%20benchmark%201h-walk%20VQA.%20Overall%2C%20the%20tasks%20in%20the%20different%20tracks%0Awere%3A%20object%20tracking%2C%20point%20tracking%2C%20temporal%20action%20localisation%2C%20temporal%0Asound%20localisation%2C%20multiple-choice%20video%20question-answering%2C%20grounded%20video%0Aquestion-answering%2C%20and%20hour-long%20video%20question-answering.%20We%20summarise%20in%0Athis%20report%20the%20challenge%20tasks%20and%20results%2C%20and%20introduce%20in%20detail%20the%20novel%0Ahour-long%20video%20QA%20benchmark%201h-walk%20VQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19941v1&entry.124074799=Read"},
{"title": "DeMo: Decoupled Momentum Optimization", "author": "Bowen Peng and Jeffrey Quesnelle and Diederik P. Kingma", "abstract": "  Training large neural networks typically requires sharing gradients between\naccelerators through specialized high-speed interconnects. Drawing from the\nsignal processing principles of frequency decomposition and energy compaction,\nwe demonstrate that synchronizing full optimizer states and model parameters\nduring training is unnecessary. By decoupling momentum updates and allowing\ncontrolled divergence in optimizer states across accelerators, we achieve\nimproved convergence compared to state-of-the-art optimizers. We introduce\n{\\textbf{De}}coupled {\\textbf{Mo}}mentum (DeMo), a fused optimizer and data\nparallel algorithm that reduces inter-accelerator communication requirements by\nseveral orders of magnitude. This enables training of large neural networks\neven with limited network bandwidth and heterogeneous hardware. Our method is\ntopology-agnostic and architecture-independent and supports scalable\nclock-synchronous distributed training with negligible compute and memory\noverhead. Empirical results show that models trained with DeMo match or exceed\nthe performance of equivalent models trained with AdamW, while eliminating the\nneed for high-speed interconnects when pre-training large scale foundation\nmodels. An open source reference PyTorch implementation is published on GitHub\nat https://github.com/bloc97/DeMo\n", "link": "http://arxiv.org/abs/2411.19870v1", "date": "2024-11-29", "relevancy": 2.0916, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5437}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5113}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeMo%3A%20Decoupled%20Momentum%20Optimization&body=Title%3A%20DeMo%3A%20Decoupled%20Momentum%20Optimization%0AAuthor%3A%20Bowen%20Peng%20and%20Jeffrey%20Quesnelle%20and%20Diederik%20P.%20Kingma%0AAbstract%3A%20%20%20Training%20large%20neural%20networks%20typically%20requires%20sharing%20gradients%20between%0Aaccelerators%20through%20specialized%20high-speed%20interconnects.%20Drawing%20from%20the%0Asignal%20processing%20principles%20of%20frequency%20decomposition%20and%20energy%20compaction%2C%0Awe%20demonstrate%20that%20synchronizing%20full%20optimizer%20states%20and%20model%20parameters%0Aduring%20training%20is%20unnecessary.%20By%20decoupling%20momentum%20updates%20and%20allowing%0Acontrolled%20divergence%20in%20optimizer%20states%20across%20accelerators%2C%20we%20achieve%0Aimproved%20convergence%20compared%20to%20state-of-the-art%20optimizers.%20We%20introduce%0A%7B%5Ctextbf%7BDe%7D%7Dcoupled%20%7B%5Ctextbf%7BMo%7D%7Dmentum%20%28DeMo%29%2C%20a%20fused%20optimizer%20and%20data%0Aparallel%20algorithm%20that%20reduces%20inter-accelerator%20communication%20requirements%20by%0Aseveral%20orders%20of%20magnitude.%20This%20enables%20training%20of%20large%20neural%20networks%0Aeven%20with%20limited%20network%20bandwidth%20and%20heterogeneous%20hardware.%20Our%20method%20is%0Atopology-agnostic%20and%20architecture-independent%20and%20supports%20scalable%0Aclock-synchronous%20distributed%20training%20with%20negligible%20compute%20and%20memory%0Aoverhead.%20Empirical%20results%20show%20that%20models%20trained%20with%20DeMo%20match%20or%20exceed%0Athe%20performance%20of%20equivalent%20models%20trained%20with%20AdamW%2C%20while%20eliminating%20the%0Aneed%20for%20high-speed%20interconnects%20when%20pre-training%20large%20scale%20foundation%0Amodels.%20An%20open%20source%20reference%20PyTorch%20implementation%20is%20published%20on%20GitHub%0Aat%20https%3A//github.com/bloc97/DeMo%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeMo%253A%2520Decoupled%2520Momentum%2520Optimization%26entry.906535625%3DBowen%2520Peng%2520and%2520Jeffrey%2520Quesnelle%2520and%2520Diederik%2520P.%2520Kingma%26entry.1292438233%3D%2520%2520Training%2520large%2520neural%2520networks%2520typically%2520requires%2520sharing%2520gradients%2520between%250Aaccelerators%2520through%2520specialized%2520high-speed%2520interconnects.%2520Drawing%2520from%2520the%250Asignal%2520processing%2520principles%2520of%2520frequency%2520decomposition%2520and%2520energy%2520compaction%252C%250Awe%2520demonstrate%2520that%2520synchronizing%2520full%2520optimizer%2520states%2520and%2520model%2520parameters%250Aduring%2520training%2520is%2520unnecessary.%2520By%2520decoupling%2520momentum%2520updates%2520and%2520allowing%250Acontrolled%2520divergence%2520in%2520optimizer%2520states%2520across%2520accelerators%252C%2520we%2520achieve%250Aimproved%2520convergence%2520compared%2520to%2520state-of-the-art%2520optimizers.%2520We%2520introduce%250A%257B%255Ctextbf%257BDe%257D%257Dcoupled%2520%257B%255Ctextbf%257BMo%257D%257Dmentum%2520%2528DeMo%2529%252C%2520a%2520fused%2520optimizer%2520and%2520data%250Aparallel%2520algorithm%2520that%2520reduces%2520inter-accelerator%2520communication%2520requirements%2520by%250Aseveral%2520orders%2520of%2520magnitude.%2520This%2520enables%2520training%2520of%2520large%2520neural%2520networks%250Aeven%2520with%2520limited%2520network%2520bandwidth%2520and%2520heterogeneous%2520hardware.%2520Our%2520method%2520is%250Atopology-agnostic%2520and%2520architecture-independent%2520and%2520supports%2520scalable%250Aclock-synchronous%2520distributed%2520training%2520with%2520negligible%2520compute%2520and%2520memory%250Aoverhead.%2520Empirical%2520results%2520show%2520that%2520models%2520trained%2520with%2520DeMo%2520match%2520or%2520exceed%250Athe%2520performance%2520of%2520equivalent%2520models%2520trained%2520with%2520AdamW%252C%2520while%2520eliminating%2520the%250Aneed%2520for%2520high-speed%2520interconnects%2520when%2520pre-training%2520large%2520scale%2520foundation%250Amodels.%2520An%2520open%2520source%2520reference%2520PyTorch%2520implementation%2520is%2520published%2520on%2520GitHub%250Aat%2520https%253A//github.com/bloc97/DeMo%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeMo%3A%20Decoupled%20Momentum%20Optimization&entry.906535625=Bowen%20Peng%20and%20Jeffrey%20Quesnelle%20and%20Diederik%20P.%20Kingma&entry.1292438233=%20%20Training%20large%20neural%20networks%20typically%20requires%20sharing%20gradients%20between%0Aaccelerators%20through%20specialized%20high-speed%20interconnects.%20Drawing%20from%20the%0Asignal%20processing%20principles%20of%20frequency%20decomposition%20and%20energy%20compaction%2C%0Awe%20demonstrate%20that%20synchronizing%20full%20optimizer%20states%20and%20model%20parameters%0Aduring%20training%20is%20unnecessary.%20By%20decoupling%20momentum%20updates%20and%20allowing%0Acontrolled%20divergence%20in%20optimizer%20states%20across%20accelerators%2C%20we%20achieve%0Aimproved%20convergence%20compared%20to%20state-of-the-art%20optimizers.%20We%20introduce%0A%7B%5Ctextbf%7BDe%7D%7Dcoupled%20%7B%5Ctextbf%7BMo%7D%7Dmentum%20%28DeMo%29%2C%20a%20fused%20optimizer%20and%20data%0Aparallel%20algorithm%20that%20reduces%20inter-accelerator%20communication%20requirements%20by%0Aseveral%20orders%20of%20magnitude.%20This%20enables%20training%20of%20large%20neural%20networks%0Aeven%20with%20limited%20network%20bandwidth%20and%20heterogeneous%20hardware.%20Our%20method%20is%0Atopology-agnostic%20and%20architecture-independent%20and%20supports%20scalable%0Aclock-synchronous%20distributed%20training%20with%20negligible%20compute%20and%20memory%0Aoverhead.%20Empirical%20results%20show%20that%20models%20trained%20with%20DeMo%20match%20or%20exceed%0Athe%20performance%20of%20equivalent%20models%20trained%20with%20AdamW%2C%20while%20eliminating%20the%0Aneed%20for%20high-speed%20interconnects%20when%20pre-training%20large%20scale%20foundation%0Amodels.%20An%20open%20source%20reference%20PyTorch%20implementation%20is%20published%20on%20GitHub%0Aat%20https%3A//github.com/bloc97/DeMo%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19870v1&entry.124074799=Read"},
{"title": "Non-linear Equalization in 112 Gb/s PONs Using Kolmogorov-Arnold\n  Networks", "author": "Rodrigo Fischer and Patrick Matalla and Sebastian Randel and Laurent Schmalen", "abstract": "  We investigate Kolmogorov-Arnold networks (KANs) for non-linear equalization\nof 112 Gb/s PAM4 passive optical networks (PONs). Using pruning and extensive\nhyperparameter search, we outperform linear equalizers and convolutional neural\nnetworks at low computational complexity.\n", "link": "http://arxiv.org/abs/2411.19631v1", "date": "2024-11-29", "relevancy": 2.0765, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4244}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4177}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-linear%20Equalization%20in%20112%20Gb/s%20PONs%20Using%20Kolmogorov-Arnold%0A%20%20Networks&body=Title%3A%20Non-linear%20Equalization%20in%20112%20Gb/s%20PONs%20Using%20Kolmogorov-Arnold%0A%20%20Networks%0AAuthor%3A%20Rodrigo%20Fischer%20and%20Patrick%20Matalla%20and%20Sebastian%20Randel%20and%20Laurent%20Schmalen%0AAbstract%3A%20%20%20We%20investigate%20Kolmogorov-Arnold%20networks%20%28KANs%29%20for%20non-linear%20equalization%0Aof%20112%20Gb/s%20PAM4%20passive%20optical%20networks%20%28PONs%29.%20Using%20pruning%20and%20extensive%0Ahyperparameter%20search%2C%20we%20outperform%20linear%20equalizers%20and%20convolutional%20neural%0Anetworks%20at%20low%20computational%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-linear%2520Equalization%2520in%2520112%2520Gb/s%2520PONs%2520Using%2520Kolmogorov-Arnold%250A%2520%2520Networks%26entry.906535625%3DRodrigo%2520Fischer%2520and%2520Patrick%2520Matalla%2520and%2520Sebastian%2520Randel%2520and%2520Laurent%2520Schmalen%26entry.1292438233%3D%2520%2520We%2520investigate%2520Kolmogorov-Arnold%2520networks%2520%2528KANs%2529%2520for%2520non-linear%2520equalization%250Aof%2520112%2520Gb/s%2520PAM4%2520passive%2520optical%2520networks%2520%2528PONs%2529.%2520Using%2520pruning%2520and%2520extensive%250Ahyperparameter%2520search%252C%2520we%2520outperform%2520linear%2520equalizers%2520and%2520convolutional%2520neural%250Anetworks%2520at%2520low%2520computational%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-linear%20Equalization%20in%20112%20Gb/s%20PONs%20Using%20Kolmogorov-Arnold%0A%20%20Networks&entry.906535625=Rodrigo%20Fischer%20and%20Patrick%20Matalla%20and%20Sebastian%20Randel%20and%20Laurent%20Schmalen&entry.1292438233=%20%20We%20investigate%20Kolmogorov-Arnold%20networks%20%28KANs%29%20for%20non-linear%20equalization%0Aof%20112%20Gb/s%20PAM4%20passive%20optical%20networks%20%28PONs%29.%20Using%20pruning%20and%20extensive%0Ahyperparameter%20search%2C%20we%20outperform%20linear%20equalizers%20and%20convolutional%20neural%0Anetworks%20at%20low%20computational%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19631v1&entry.124074799=Read"},
{"title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding", "author": "Julian D Parker and Anton Smirnov and Jordi Pons and CJ Carr and Zack Zukowski and Zach Evans and Xubo Liu", "abstract": "  The tokenization of speech with neural audio codec models is a vital part of\nmodern AI pipelines for the generation or understanding of speech, alone or in\na multimodal context. Traditionally such tokenization models have concentrated\non low parameter-count architectures using only components with strong\ninductive biases. In this work we show that by scaling a transformer\narchitecture with large parameter count to this problem, and applying a\nflexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to\nreach state-of-the-art speech quality at extremely low bit-rates of $400$ or\n$700$ bits-per-second. The trained models strongly out-perform existing\nbaselines in both objective and subjective tests.\n", "link": "http://arxiv.org/abs/2411.19842v1", "date": "2024-11-29", "relevancy": 2.0711, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6175}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5303}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Transformers%20for%20Low-Bitrate%20High-Quality%20Speech%20Coding&body=Title%3A%20Scaling%20Transformers%20for%20Low-Bitrate%20High-Quality%20Speech%20Coding%0AAuthor%3A%20Julian%20D%20Parker%20and%20Anton%20Smirnov%20and%20Jordi%20Pons%20and%20CJ%20Carr%20and%20Zack%20Zukowski%20and%20Zach%20Evans%20and%20Xubo%20Liu%0AAbstract%3A%20%20%20The%20tokenization%20of%20speech%20with%20neural%20audio%20codec%20models%20is%20a%20vital%20part%20of%0Amodern%20AI%20pipelines%20for%20the%20generation%20or%20understanding%20of%20speech%2C%20alone%20or%20in%0Aa%20multimodal%20context.%20Traditionally%20such%20tokenization%20models%20have%20concentrated%0Aon%20low%20parameter-count%20architectures%20using%20only%20components%20with%20strong%0Ainductive%20biases.%20In%20this%20work%20we%20show%20that%20by%20scaling%20a%20transformer%0Aarchitecture%20with%20large%20parameter%20count%20to%20this%20problem%2C%20and%20applying%20a%0Aflexible%20Finite%20Scalar%20Quantization%20%28FSQ%29%20based%20bottleneck%2C%20it%20is%20possible%20to%0Areach%20state-of-the-art%20speech%20quality%20at%20extremely%20low%20bit-rates%20of%20%24400%24%20or%0A%24700%24%20bits-per-second.%20The%20trained%20models%20strongly%20out-perform%20existing%0Abaselines%20in%20both%20objective%20and%20subjective%20tests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Transformers%2520for%2520Low-Bitrate%2520High-Quality%2520Speech%2520Coding%26entry.906535625%3DJulian%2520D%2520Parker%2520and%2520Anton%2520Smirnov%2520and%2520Jordi%2520Pons%2520and%2520CJ%2520Carr%2520and%2520Zack%2520Zukowski%2520and%2520Zach%2520Evans%2520and%2520Xubo%2520Liu%26entry.1292438233%3D%2520%2520The%2520tokenization%2520of%2520speech%2520with%2520neural%2520audio%2520codec%2520models%2520is%2520a%2520vital%2520part%2520of%250Amodern%2520AI%2520pipelines%2520for%2520the%2520generation%2520or%2520understanding%2520of%2520speech%252C%2520alone%2520or%2520in%250Aa%2520multimodal%2520context.%2520Traditionally%2520such%2520tokenization%2520models%2520have%2520concentrated%250Aon%2520low%2520parameter-count%2520architectures%2520using%2520only%2520components%2520with%2520strong%250Ainductive%2520biases.%2520In%2520this%2520work%2520we%2520show%2520that%2520by%2520scaling%2520a%2520transformer%250Aarchitecture%2520with%2520large%2520parameter%2520count%2520to%2520this%2520problem%252C%2520and%2520applying%2520a%250Aflexible%2520Finite%2520Scalar%2520Quantization%2520%2528FSQ%2529%2520based%2520bottleneck%252C%2520it%2520is%2520possible%2520to%250Areach%2520state-of-the-art%2520speech%2520quality%2520at%2520extremely%2520low%2520bit-rates%2520of%2520%2524400%2524%2520or%250A%2524700%2524%2520bits-per-second.%2520The%2520trained%2520models%2520strongly%2520out-perform%2520existing%250Abaselines%2520in%2520both%2520objective%2520and%2520subjective%2520tests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Transformers%20for%20Low-Bitrate%20High-Quality%20Speech%20Coding&entry.906535625=Julian%20D%20Parker%20and%20Anton%20Smirnov%20and%20Jordi%20Pons%20and%20CJ%20Carr%20and%20Zack%20Zukowski%20and%20Zach%20Evans%20and%20Xubo%20Liu&entry.1292438233=%20%20The%20tokenization%20of%20speech%20with%20neural%20audio%20codec%20models%20is%20a%20vital%20part%20of%0Amodern%20AI%20pipelines%20for%20the%20generation%20or%20understanding%20of%20speech%2C%20alone%20or%20in%0Aa%20multimodal%20context.%20Traditionally%20such%20tokenization%20models%20have%20concentrated%0Aon%20low%20parameter-count%20architectures%20using%20only%20components%20with%20strong%0Ainductive%20biases.%20In%20this%20work%20we%20show%20that%20by%20scaling%20a%20transformer%0Aarchitecture%20with%20large%20parameter%20count%20to%20this%20problem%2C%20and%20applying%20a%0Aflexible%20Finite%20Scalar%20Quantization%20%28FSQ%29%20based%20bottleneck%2C%20it%20is%20possible%20to%0Areach%20state-of-the-art%20speech%20quality%20at%20extremely%20low%20bit-rates%20of%20%24400%24%20or%0A%24700%24%20bits-per-second.%20The%20trained%20models%20strongly%20out-perform%20existing%0Abaselines%20in%20both%20objective%20and%20subjective%20tests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19842v1&entry.124074799=Read"},
{"title": "FlowCLAS: Enhancing Normalizing Flow Via Contrastive Learning For\n  Anomaly Segmentation", "author": "Chang Won Lee and Selina Leveugle and Svetlana Stolpner and Chris Langley and Paul Grouchy and Jonathan Kelly and Steven L. Waslander", "abstract": "  Anomaly segmentation is a valuable computer vision task for safety-critical\napplications that need to be aware of unexpected events. Current\nstate-of-the-art (SOTA) scene-level anomaly segmentation approaches rely on\ndiverse inlier class labels during training, limiting their ability to leverage\nvast unlabeled datasets and pre-trained vision encoders. These methods may\nunderperform in domains with reduced color diversity and limited object\nclasses. Conversely, existing unsupervised methods struggle with anomaly\nsegmentation with the diverse scenes of less restricted domains. To address\nthese challenges, we introduce FlowCLAS, a novel self-supervised framework that\nutilizes vision foundation models to extract rich features and employs a\nnormalizing flow network to learn their density distribution. We enhance the\nmodel's discriminative power by incorporating Outlier Exposure and contrastive\nlearning in the latent space. FlowCLAS significantly outperforms all existing\nmethods on the ALLO anomaly segmentation benchmark for space robotics and\ndemonstrates competitive results on multiple road anomaly segmentation\nbenchmarks for autonomous driving, including Fishyscapes Lost&Found and Road\nAnomaly. These results highlight FlowCLAS's effectiveness in addressing the\nunique challenges of space anomaly segmentation while retaining SOTA\nperformance in the autonomous driving domain without reliance on inlier\nsegmentation labels.\n", "link": "http://arxiv.org/abs/2411.19888v1", "date": "2024-11-29", "relevancy": 2.0684, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.53}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowCLAS%3A%20Enhancing%20Normalizing%20Flow%20Via%20Contrastive%20Learning%20For%0A%20%20Anomaly%20Segmentation&body=Title%3A%20FlowCLAS%3A%20Enhancing%20Normalizing%20Flow%20Via%20Contrastive%20Learning%20For%0A%20%20Anomaly%20Segmentation%0AAuthor%3A%20Chang%20Won%20Lee%20and%20Selina%20Leveugle%20and%20Svetlana%20Stolpner%20and%20Chris%20Langley%20and%20Paul%20Grouchy%20and%20Jonathan%20Kelly%20and%20Steven%20L.%20Waslander%0AAbstract%3A%20%20%20Anomaly%20segmentation%20is%20a%20valuable%20computer%20vision%20task%20for%20safety-critical%0Aapplications%20that%20need%20to%20be%20aware%20of%20unexpected%20events.%20Current%0Astate-of-the-art%20%28SOTA%29%20scene-level%20anomaly%20segmentation%20approaches%20rely%20on%0Adiverse%20inlier%20class%20labels%20during%20training%2C%20limiting%20their%20ability%20to%20leverage%0Avast%20unlabeled%20datasets%20and%20pre-trained%20vision%20encoders.%20These%20methods%20may%0Aunderperform%20in%20domains%20with%20reduced%20color%20diversity%20and%20limited%20object%0Aclasses.%20Conversely%2C%20existing%20unsupervised%20methods%20struggle%20with%20anomaly%0Asegmentation%20with%20the%20diverse%20scenes%20of%20less%20restricted%20domains.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20FlowCLAS%2C%20a%20novel%20self-supervised%20framework%20that%0Autilizes%20vision%20foundation%20models%20to%20extract%20rich%20features%20and%20employs%20a%0Anormalizing%20flow%20network%20to%20learn%20their%20density%20distribution.%20We%20enhance%20the%0Amodel%27s%20discriminative%20power%20by%20incorporating%20Outlier%20Exposure%20and%20contrastive%0Alearning%20in%20the%20latent%20space.%20FlowCLAS%20significantly%20outperforms%20all%20existing%0Amethods%20on%20the%20ALLO%20anomaly%20segmentation%20benchmark%20for%20space%20robotics%20and%0Ademonstrates%20competitive%20results%20on%20multiple%20road%20anomaly%20segmentation%0Abenchmarks%20for%20autonomous%20driving%2C%20including%20Fishyscapes%20Lost%26Found%20and%20Road%0AAnomaly.%20These%20results%20highlight%20FlowCLAS%27s%20effectiveness%20in%20addressing%20the%0Aunique%20challenges%20of%20space%20anomaly%20segmentation%20while%20retaining%20SOTA%0Aperformance%20in%20the%20autonomous%20driving%20domain%20without%20reliance%20on%20inlier%0Asegmentation%20labels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19888v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowCLAS%253A%2520Enhancing%2520Normalizing%2520Flow%2520Via%2520Contrastive%2520Learning%2520For%250A%2520%2520Anomaly%2520Segmentation%26entry.906535625%3DChang%2520Won%2520Lee%2520and%2520Selina%2520Leveugle%2520and%2520Svetlana%2520Stolpner%2520and%2520Chris%2520Langley%2520and%2520Paul%2520Grouchy%2520and%2520Jonathan%2520Kelly%2520and%2520Steven%2520L.%2520Waslander%26entry.1292438233%3D%2520%2520Anomaly%2520segmentation%2520is%2520a%2520valuable%2520computer%2520vision%2520task%2520for%2520safety-critical%250Aapplications%2520that%2520need%2520to%2520be%2520aware%2520of%2520unexpected%2520events.%2520Current%250Astate-of-the-art%2520%2528SOTA%2529%2520scene-level%2520anomaly%2520segmentation%2520approaches%2520rely%2520on%250Adiverse%2520inlier%2520class%2520labels%2520during%2520training%252C%2520limiting%2520their%2520ability%2520to%2520leverage%250Avast%2520unlabeled%2520datasets%2520and%2520pre-trained%2520vision%2520encoders.%2520These%2520methods%2520may%250Aunderperform%2520in%2520domains%2520with%2520reduced%2520color%2520diversity%2520and%2520limited%2520object%250Aclasses.%2520Conversely%252C%2520existing%2520unsupervised%2520methods%2520struggle%2520with%2520anomaly%250Asegmentation%2520with%2520the%2520diverse%2520scenes%2520of%2520less%2520restricted%2520domains.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520FlowCLAS%252C%2520a%2520novel%2520self-supervised%2520framework%2520that%250Autilizes%2520vision%2520foundation%2520models%2520to%2520extract%2520rich%2520features%2520and%2520employs%2520a%250Anormalizing%2520flow%2520network%2520to%2520learn%2520their%2520density%2520distribution.%2520We%2520enhance%2520the%250Amodel%2527s%2520discriminative%2520power%2520by%2520incorporating%2520Outlier%2520Exposure%2520and%2520contrastive%250Alearning%2520in%2520the%2520latent%2520space.%2520FlowCLAS%2520significantly%2520outperforms%2520all%2520existing%250Amethods%2520on%2520the%2520ALLO%2520anomaly%2520segmentation%2520benchmark%2520for%2520space%2520robotics%2520and%250Ademonstrates%2520competitive%2520results%2520on%2520multiple%2520road%2520anomaly%2520segmentation%250Abenchmarks%2520for%2520autonomous%2520driving%252C%2520including%2520Fishyscapes%2520Lost%2526Found%2520and%2520Road%250AAnomaly.%2520These%2520results%2520highlight%2520FlowCLAS%2527s%2520effectiveness%2520in%2520addressing%2520the%250Aunique%2520challenges%2520of%2520space%2520anomaly%2520segmentation%2520while%2520retaining%2520SOTA%250Aperformance%2520in%2520the%2520autonomous%2520driving%2520domain%2520without%2520reliance%2520on%2520inlier%250Asegmentation%2520labels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19888v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowCLAS%3A%20Enhancing%20Normalizing%20Flow%20Via%20Contrastive%20Learning%20For%0A%20%20Anomaly%20Segmentation&entry.906535625=Chang%20Won%20Lee%20and%20Selina%20Leveugle%20and%20Svetlana%20Stolpner%20and%20Chris%20Langley%20and%20Paul%20Grouchy%20and%20Jonathan%20Kelly%20and%20Steven%20L.%20Waslander&entry.1292438233=%20%20Anomaly%20segmentation%20is%20a%20valuable%20computer%20vision%20task%20for%20safety-critical%0Aapplications%20that%20need%20to%20be%20aware%20of%20unexpected%20events.%20Current%0Astate-of-the-art%20%28SOTA%29%20scene-level%20anomaly%20segmentation%20approaches%20rely%20on%0Adiverse%20inlier%20class%20labels%20during%20training%2C%20limiting%20their%20ability%20to%20leverage%0Avast%20unlabeled%20datasets%20and%20pre-trained%20vision%20encoders.%20These%20methods%20may%0Aunderperform%20in%20domains%20with%20reduced%20color%20diversity%20and%20limited%20object%0Aclasses.%20Conversely%2C%20existing%20unsupervised%20methods%20struggle%20with%20anomaly%0Asegmentation%20with%20the%20diverse%20scenes%20of%20less%20restricted%20domains.%20To%20address%0Athese%20challenges%2C%20we%20introduce%20FlowCLAS%2C%20a%20novel%20self-supervised%20framework%20that%0Autilizes%20vision%20foundation%20models%20to%20extract%20rich%20features%20and%20employs%20a%0Anormalizing%20flow%20network%20to%20learn%20their%20density%20distribution.%20We%20enhance%20the%0Amodel%27s%20discriminative%20power%20by%20incorporating%20Outlier%20Exposure%20and%20contrastive%0Alearning%20in%20the%20latent%20space.%20FlowCLAS%20significantly%20outperforms%20all%20existing%0Amethods%20on%20the%20ALLO%20anomaly%20segmentation%20benchmark%20for%20space%20robotics%20and%0Ademonstrates%20competitive%20results%20on%20multiple%20road%20anomaly%20segmentation%0Abenchmarks%20for%20autonomous%20driving%2C%20including%20Fishyscapes%20Lost%26Found%20and%20Road%0AAnomaly.%20These%20results%20highlight%20FlowCLAS%27s%20effectiveness%20in%20addressing%20the%0Aunique%20challenges%20of%20space%20anomaly%20segmentation%20while%20retaining%20SOTA%0Aperformance%20in%20the%20autonomous%20driving%20domain%20without%20reliance%20on%20inlier%0Asegmentation%20labels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19888v1&entry.124074799=Read"},
{"title": "P2PFormer: A Primitive-to-polygon Method for Regular Building Contour\n  Extraction from Remote Sensing Images", "author": "Tao Zhang and Shiqing Wei and Yikang Zhou and Muying Luo and Wenling You and Shunping Ji", "abstract": "  Extracting building contours from remote sensing imagery is a significant\nchallenge due to buildings' complex and diverse shapes, occlusions, and noise.\nExisting methods often struggle with irregular contours, rounded corners, and\nredundancy points, necessitating extensive post-processing to produce regular\npolygonal building contours. To address these challenges, we introduce a novel,\nstreamlined pipeline that generates regular building contours without\npost-processing. Our approach begins with the segmentation of generic geometric\nprimitives (which can include vertices, lines, and corners), followed by the\nprediction of their sequence. This allows for the direct construction of\nregular building contours by sequentially connecting the segmented primitives.\nBuilding on this pipeline, we developed P2PFormer, which utilizes a\ntransformer-based architecture to segment geometric primitives and predict\ntheir order. To enhance the segmentation of primitives, we introduce a unique\nrepresentation called group queries. This representation comprises a set of\nqueries and a singular query position, which improve the focus on multiple\nmidpoints of primitives and their efficient linkage. Furthermore, we propose an\ninnovative implicit update strategy for the query position embedding aimed at\nsharpening the focus of queries on the correct positions and, consequently,\nenhancing the quality of primitive segmentation. Our experiments demonstrate\nthat P2PFormer achieves new state-of-the-art performance on the WHU, CrowdAI,\nand WHU-Mix datasets, surpassing the previous SOTA PolyWorld by a margin of 2.7\nAP and 6.5 AP75 on the largest CrowdAI dataset\n", "link": "http://arxiv.org/abs/2406.02930v2", "date": "2024-11-29", "relevancy": 2.0678, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P2PFormer%3A%20A%20Primitive-to-polygon%20Method%20for%20Regular%20Building%20Contour%0A%20%20Extraction%20from%20Remote%20Sensing%20Images&body=Title%3A%20P2PFormer%3A%20A%20Primitive-to-polygon%20Method%20for%20Regular%20Building%20Contour%0A%20%20Extraction%20from%20Remote%20Sensing%20Images%0AAuthor%3A%20Tao%20Zhang%20and%20Shiqing%20Wei%20and%20Yikang%20Zhou%20and%20Muying%20Luo%20and%20Wenling%20You%20and%20Shunping%20Ji%0AAbstract%3A%20%20%20Extracting%20building%20contours%20from%20remote%20sensing%20imagery%20is%20a%20significant%0Achallenge%20due%20to%20buildings%27%20complex%20and%20diverse%20shapes%2C%20occlusions%2C%20and%20noise.%0AExisting%20methods%20often%20struggle%20with%20irregular%20contours%2C%20rounded%20corners%2C%20and%0Aredundancy%20points%2C%20necessitating%20extensive%20post-processing%20to%20produce%20regular%0Apolygonal%20building%20contours.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%2C%0Astreamlined%20pipeline%20that%20generates%20regular%20building%20contours%20without%0Apost-processing.%20Our%20approach%20begins%20with%20the%20segmentation%20of%20generic%20geometric%0Aprimitives%20%28which%20can%20include%20vertices%2C%20lines%2C%20and%20corners%29%2C%20followed%20by%20the%0Aprediction%20of%20their%20sequence.%20This%20allows%20for%20the%20direct%20construction%20of%0Aregular%20building%20contours%20by%20sequentially%20connecting%20the%20segmented%20primitives.%0ABuilding%20on%20this%20pipeline%2C%20we%20developed%20P2PFormer%2C%20which%20utilizes%20a%0Atransformer-based%20architecture%20to%20segment%20geometric%20primitives%20and%20predict%0Atheir%20order.%20To%20enhance%20the%20segmentation%20of%20primitives%2C%20we%20introduce%20a%20unique%0Arepresentation%20called%20group%20queries.%20This%20representation%20comprises%20a%20set%20of%0Aqueries%20and%20a%20singular%20query%20position%2C%20which%20improve%20the%20focus%20on%20multiple%0Amidpoints%20of%20primitives%20and%20their%20efficient%20linkage.%20Furthermore%2C%20we%20propose%20an%0Ainnovative%20implicit%20update%20strategy%20for%20the%20query%20position%20embedding%20aimed%20at%0Asharpening%20the%20focus%20of%20queries%20on%20the%20correct%20positions%20and%2C%20consequently%2C%0Aenhancing%20the%20quality%20of%20primitive%20segmentation.%20Our%20experiments%20demonstrate%0Athat%20P2PFormer%20achieves%20new%20state-of-the-art%20performance%20on%20the%20WHU%2C%20CrowdAI%2C%0Aand%20WHU-Mix%20datasets%2C%20surpassing%20the%20previous%20SOTA%20PolyWorld%20by%20a%20margin%20of%202.7%0AAP%20and%206.5%20AP75%20on%20the%20largest%20CrowdAI%20dataset%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02930v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP2PFormer%253A%2520A%2520Primitive-to-polygon%2520Method%2520for%2520Regular%2520Building%2520Contour%250A%2520%2520Extraction%2520from%2520Remote%2520Sensing%2520Images%26entry.906535625%3DTao%2520Zhang%2520and%2520Shiqing%2520Wei%2520and%2520Yikang%2520Zhou%2520and%2520Muying%2520Luo%2520and%2520Wenling%2520You%2520and%2520Shunping%2520Ji%26entry.1292438233%3D%2520%2520Extracting%2520building%2520contours%2520from%2520remote%2520sensing%2520imagery%2520is%2520a%2520significant%250Achallenge%2520due%2520to%2520buildings%2527%2520complex%2520and%2520diverse%2520shapes%252C%2520occlusions%252C%2520and%2520noise.%250AExisting%2520methods%2520often%2520struggle%2520with%2520irregular%2520contours%252C%2520rounded%2520corners%252C%2520and%250Aredundancy%2520points%252C%2520necessitating%2520extensive%2520post-processing%2520to%2520produce%2520regular%250Apolygonal%2520building%2520contours.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%252C%250Astreamlined%2520pipeline%2520that%2520generates%2520regular%2520building%2520contours%2520without%250Apost-processing.%2520Our%2520approach%2520begins%2520with%2520the%2520segmentation%2520of%2520generic%2520geometric%250Aprimitives%2520%2528which%2520can%2520include%2520vertices%252C%2520lines%252C%2520and%2520corners%2529%252C%2520followed%2520by%2520the%250Aprediction%2520of%2520their%2520sequence.%2520This%2520allows%2520for%2520the%2520direct%2520construction%2520of%250Aregular%2520building%2520contours%2520by%2520sequentially%2520connecting%2520the%2520segmented%2520primitives.%250ABuilding%2520on%2520this%2520pipeline%252C%2520we%2520developed%2520P2PFormer%252C%2520which%2520utilizes%2520a%250Atransformer-based%2520architecture%2520to%2520segment%2520geometric%2520primitives%2520and%2520predict%250Atheir%2520order.%2520To%2520enhance%2520the%2520segmentation%2520of%2520primitives%252C%2520we%2520introduce%2520a%2520unique%250Arepresentation%2520called%2520group%2520queries.%2520This%2520representation%2520comprises%2520a%2520set%2520of%250Aqueries%2520and%2520a%2520singular%2520query%2520position%252C%2520which%2520improve%2520the%2520focus%2520on%2520multiple%250Amidpoints%2520of%2520primitives%2520and%2520their%2520efficient%2520linkage.%2520Furthermore%252C%2520we%2520propose%2520an%250Ainnovative%2520implicit%2520update%2520strategy%2520for%2520the%2520query%2520position%2520embedding%2520aimed%2520at%250Asharpening%2520the%2520focus%2520of%2520queries%2520on%2520the%2520correct%2520positions%2520and%252C%2520consequently%252C%250Aenhancing%2520the%2520quality%2520of%2520primitive%2520segmentation.%2520Our%2520experiments%2520demonstrate%250Athat%2520P2PFormer%2520achieves%2520new%2520state-of-the-art%2520performance%2520on%2520the%2520WHU%252C%2520CrowdAI%252C%250Aand%2520WHU-Mix%2520datasets%252C%2520surpassing%2520the%2520previous%2520SOTA%2520PolyWorld%2520by%2520a%2520margin%2520of%25202.7%250AAP%2520and%25206.5%2520AP75%2520on%2520the%2520largest%2520CrowdAI%2520dataset%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02930v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P2PFormer%3A%20A%20Primitive-to-polygon%20Method%20for%20Regular%20Building%20Contour%0A%20%20Extraction%20from%20Remote%20Sensing%20Images&entry.906535625=Tao%20Zhang%20and%20Shiqing%20Wei%20and%20Yikang%20Zhou%20and%20Muying%20Luo%20and%20Wenling%20You%20and%20Shunping%20Ji&entry.1292438233=%20%20Extracting%20building%20contours%20from%20remote%20sensing%20imagery%20is%20a%20significant%0Achallenge%20due%20to%20buildings%27%20complex%20and%20diverse%20shapes%2C%20occlusions%2C%20and%20noise.%0AExisting%20methods%20often%20struggle%20with%20irregular%20contours%2C%20rounded%20corners%2C%20and%0Aredundancy%20points%2C%20necessitating%20extensive%20post-processing%20to%20produce%20regular%0Apolygonal%20building%20contours.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%2C%0Astreamlined%20pipeline%20that%20generates%20regular%20building%20contours%20without%0Apost-processing.%20Our%20approach%20begins%20with%20the%20segmentation%20of%20generic%20geometric%0Aprimitives%20%28which%20can%20include%20vertices%2C%20lines%2C%20and%20corners%29%2C%20followed%20by%20the%0Aprediction%20of%20their%20sequence.%20This%20allows%20for%20the%20direct%20construction%20of%0Aregular%20building%20contours%20by%20sequentially%20connecting%20the%20segmented%20primitives.%0ABuilding%20on%20this%20pipeline%2C%20we%20developed%20P2PFormer%2C%20which%20utilizes%20a%0Atransformer-based%20architecture%20to%20segment%20geometric%20primitives%20and%20predict%0Atheir%20order.%20To%20enhance%20the%20segmentation%20of%20primitives%2C%20we%20introduce%20a%20unique%0Arepresentation%20called%20group%20queries.%20This%20representation%20comprises%20a%20set%20of%0Aqueries%20and%20a%20singular%20query%20position%2C%20which%20improve%20the%20focus%20on%20multiple%0Amidpoints%20of%20primitives%20and%20their%20efficient%20linkage.%20Furthermore%2C%20we%20propose%20an%0Ainnovative%20implicit%20update%20strategy%20for%20the%20query%20position%20embedding%20aimed%20at%0Asharpening%20the%20focus%20of%20queries%20on%20the%20correct%20positions%20and%2C%20consequently%2C%0Aenhancing%20the%20quality%20of%20primitive%20segmentation.%20Our%20experiments%20demonstrate%0Athat%20P2PFormer%20achieves%20new%20state-of-the-art%20performance%20on%20the%20WHU%2C%20CrowdAI%2C%0Aand%20WHU-Mix%20datasets%2C%20surpassing%20the%20previous%20SOTA%20PolyWorld%20by%20a%20margin%20of%202.7%0AAP%20and%206.5%20AP75%20on%20the%20largest%20CrowdAI%20dataset%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02930v2&entry.124074799=Read"},
{"title": "Towards Evaluating Generalist Agents: An Automated Benchmark in Open\n  World", "author": "Xinyue Zheng and Haowei Lin and Kaichen He and Zihao Wang and Zilong Zheng and Yitao Liang", "abstract": "  Evaluating generalist agents presents significant challenges due to their\nwide-ranging abilities and the limitations of current benchmarks in assessing\ntrue generalization. We introduce the Minecraft Universe (MCU), a fully\nautomated benchmarking framework set within the open-world game Minecraft. MCU\ndynamically generates and evaluates a broad spectrum of tasks, offering three\ncore components: 1) a task generation mechanism that provides high degrees of\nfreedom and variability, 2) an ever-expanding set of over 3K composable atomic\ntasks, and 3) a general evaluation framework that supports open-ended task\nassessment. By integrating large language models (LLMs), MCU dynamically\ncreates diverse environments for each evaluation, fostering agent\ngeneralization. The framework uses a vision-language model (VLM) to\nautomatically generate evaluation criteria, achieving over 90% agreement with\nhuman ratings across multi-dimensional assessments, which demonstrates that MCU\nis a scalable and explainable solution for evaluating generalist agents.\nAdditionally, we show that while state-of-the-art foundational models perform\nwell on specific tasks, they often struggle with increased task diversity and\ndifficulty.\n", "link": "http://arxiv.org/abs/2310.08367v2", "date": "2024-11-29", "relevancy": 2.0539, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5368}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Evaluating%20Generalist%20Agents%3A%20An%20Automated%20Benchmark%20in%20Open%0A%20%20World&body=Title%3A%20Towards%20Evaluating%20Generalist%20Agents%3A%20An%20Automated%20Benchmark%20in%20Open%0A%20%20World%0AAuthor%3A%20Xinyue%20Zheng%20and%20Haowei%20Lin%20and%20Kaichen%20He%20and%20Zihao%20Wang%20and%20Zilong%20Zheng%20and%20Yitao%20Liang%0AAbstract%3A%20%20%20Evaluating%20generalist%20agents%20presents%20significant%20challenges%20due%20to%20their%0Awide-ranging%20abilities%20and%20the%20limitations%20of%20current%20benchmarks%20in%20assessing%0Atrue%20generalization.%20We%20introduce%20the%20Minecraft%20Universe%20%28MCU%29%2C%20a%20fully%0Aautomated%20benchmarking%20framework%20set%20within%20the%20open-world%20game%20Minecraft.%20MCU%0Adynamically%20generates%20and%20evaluates%20a%20broad%20spectrum%20of%20tasks%2C%20offering%20three%0Acore%20components%3A%201%29%20a%20task%20generation%20mechanism%20that%20provides%20high%20degrees%20of%0Afreedom%20and%20variability%2C%202%29%20an%20ever-expanding%20set%20of%20over%203K%20composable%20atomic%0Atasks%2C%20and%203%29%20a%20general%20evaluation%20framework%20that%20supports%20open-ended%20task%0Aassessment.%20By%20integrating%20large%20language%20models%20%28LLMs%29%2C%20MCU%20dynamically%0Acreates%20diverse%20environments%20for%20each%20evaluation%2C%20fostering%20agent%0Ageneralization.%20The%20framework%20uses%20a%20vision-language%20model%20%28VLM%29%20to%0Aautomatically%20generate%20evaluation%20criteria%2C%20achieving%20over%2090%25%20agreement%20with%0Ahuman%20ratings%20across%20multi-dimensional%20assessments%2C%20which%20demonstrates%20that%20MCU%0Ais%20a%20scalable%20and%20explainable%20solution%20for%20evaluating%20generalist%20agents.%0AAdditionally%2C%20we%20show%20that%20while%20state-of-the-art%20foundational%20models%20perform%0Awell%20on%20specific%20tasks%2C%20they%20often%20struggle%20with%20increased%20task%20diversity%20and%0Adifficulty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Evaluating%2520Generalist%2520Agents%253A%2520An%2520Automated%2520Benchmark%2520in%2520Open%250A%2520%2520World%26entry.906535625%3DXinyue%2520Zheng%2520and%2520Haowei%2520Lin%2520and%2520Kaichen%2520He%2520and%2520Zihao%2520Wang%2520and%2520Zilong%2520Zheng%2520and%2520Yitao%2520Liang%26entry.1292438233%3D%2520%2520Evaluating%2520generalist%2520agents%2520presents%2520significant%2520challenges%2520due%2520to%2520their%250Awide-ranging%2520abilities%2520and%2520the%2520limitations%2520of%2520current%2520benchmarks%2520in%2520assessing%250Atrue%2520generalization.%2520We%2520introduce%2520the%2520Minecraft%2520Universe%2520%2528MCU%2529%252C%2520a%2520fully%250Aautomated%2520benchmarking%2520framework%2520set%2520within%2520the%2520open-world%2520game%2520Minecraft.%2520MCU%250Adynamically%2520generates%2520and%2520evaluates%2520a%2520broad%2520spectrum%2520of%2520tasks%252C%2520offering%2520three%250Acore%2520components%253A%25201%2529%2520a%2520task%2520generation%2520mechanism%2520that%2520provides%2520high%2520degrees%2520of%250Afreedom%2520and%2520variability%252C%25202%2529%2520an%2520ever-expanding%2520set%2520of%2520over%25203K%2520composable%2520atomic%250Atasks%252C%2520and%25203%2529%2520a%2520general%2520evaluation%2520framework%2520that%2520supports%2520open-ended%2520task%250Aassessment.%2520By%2520integrating%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520MCU%2520dynamically%250Acreates%2520diverse%2520environments%2520for%2520each%2520evaluation%252C%2520fostering%2520agent%250Ageneralization.%2520The%2520framework%2520uses%2520a%2520vision-language%2520model%2520%2528VLM%2529%2520to%250Aautomatically%2520generate%2520evaluation%2520criteria%252C%2520achieving%2520over%252090%2525%2520agreement%2520with%250Ahuman%2520ratings%2520across%2520multi-dimensional%2520assessments%252C%2520which%2520demonstrates%2520that%2520MCU%250Ais%2520a%2520scalable%2520and%2520explainable%2520solution%2520for%2520evaluating%2520generalist%2520agents.%250AAdditionally%252C%2520we%2520show%2520that%2520while%2520state-of-the-art%2520foundational%2520models%2520perform%250Awell%2520on%2520specific%2520tasks%252C%2520they%2520often%2520struggle%2520with%2520increased%2520task%2520diversity%2520and%250Adifficulty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Evaluating%20Generalist%20Agents%3A%20An%20Automated%20Benchmark%20in%20Open%0A%20%20World&entry.906535625=Xinyue%20Zheng%20and%20Haowei%20Lin%20and%20Kaichen%20He%20and%20Zihao%20Wang%20and%20Zilong%20Zheng%20and%20Yitao%20Liang&entry.1292438233=%20%20Evaluating%20generalist%20agents%20presents%20significant%20challenges%20due%20to%20their%0Awide-ranging%20abilities%20and%20the%20limitations%20of%20current%20benchmarks%20in%20assessing%0Atrue%20generalization.%20We%20introduce%20the%20Minecraft%20Universe%20%28MCU%29%2C%20a%20fully%0Aautomated%20benchmarking%20framework%20set%20within%20the%20open-world%20game%20Minecraft.%20MCU%0Adynamically%20generates%20and%20evaluates%20a%20broad%20spectrum%20of%20tasks%2C%20offering%20three%0Acore%20components%3A%201%29%20a%20task%20generation%20mechanism%20that%20provides%20high%20degrees%20of%0Afreedom%20and%20variability%2C%202%29%20an%20ever-expanding%20set%20of%20over%203K%20composable%20atomic%0Atasks%2C%20and%203%29%20a%20general%20evaluation%20framework%20that%20supports%20open-ended%20task%0Aassessment.%20By%20integrating%20large%20language%20models%20%28LLMs%29%2C%20MCU%20dynamically%0Acreates%20diverse%20environments%20for%20each%20evaluation%2C%20fostering%20agent%0Ageneralization.%20The%20framework%20uses%20a%20vision-language%20model%20%28VLM%29%20to%0Aautomatically%20generate%20evaluation%20criteria%2C%20achieving%20over%2090%25%20agreement%20with%0Ahuman%20ratings%20across%20multi-dimensional%20assessments%2C%20which%20demonstrates%20that%20MCU%0Ais%20a%20scalable%20and%20explainable%20solution%20for%20evaluating%20generalist%20agents.%0AAdditionally%2C%20we%20show%20that%20while%20state-of-the-art%20foundational%20models%20perform%0Awell%20on%20specific%20tasks%2C%20they%20often%20struggle%20with%20increased%20task%20diversity%20and%0Adifficulty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08367v2&entry.124074799=Read"},
{"title": "PACMANN: Point Adaptive Collocation Method for Artificial Neural\n  Networks", "author": "Coen Visser and Alexander Heinlein and Bianca Giovanardi", "abstract": "  Physics-Informed Neural Networks (PINNs) are an emerging tool for\napproximating the solution of Partial Differential Equations (PDEs) in both\nforward and inverse problems. PINNs minimize a loss function which includes the\nPDE residual determined for a set of collocation points. Previous work has\nshown that the number and distribution of these collocation points have a\nsignificant influence on the accuracy of the PINN solution. Therefore, the\neffective placement of these collocation points is an active area of research.\nSpecifically, adaptive collocation point sampling methods have been proposed,\nwhich have been reported to scale poorly to higher dimensions. In this work, we\naddress this issue and present the Point Adaptive Collocation Method for\nArtificial Neural Networks (PACMANN). Inspired by classic optimization\nproblems, this approach incrementally moves collocation points toward regions\nof higher residuals using gradient-based optimization algorithms guided by the\ngradient of the squared residual. We apply PACMANN for forward and inverse\nproblems, and demonstrate that this method matches the performance of\nstate-of-the-art methods in terms of the accuracy/efficiency tradeoff for the\nlow-dimensional problems, while outperforming available approaches for\nhigh-dimensional problems; the best performance is observed for the Adam\noptimizer. Key features of the method include its low computational cost and\nsimplicity of integration in existing physics-informed neural network\npipelines.\n", "link": "http://arxiv.org/abs/2411.19632v1", "date": "2024-11-29", "relevancy": 2.0518, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5441}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4911}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACMANN%3A%20Point%20Adaptive%20Collocation%20Method%20for%20Artificial%20Neural%0A%20%20Networks&body=Title%3A%20PACMANN%3A%20Point%20Adaptive%20Collocation%20Method%20for%20Artificial%20Neural%0A%20%20Networks%0AAuthor%3A%20Coen%20Visser%20and%20Alexander%20Heinlein%20and%20Bianca%20Giovanardi%0AAbstract%3A%20%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20an%20emerging%20tool%20for%0Aapproximating%20the%20solution%20of%20Partial%20Differential%20Equations%20%28PDEs%29%20in%20both%0Aforward%20and%20inverse%20problems.%20PINNs%20minimize%20a%20loss%20function%20which%20includes%20the%0APDE%20residual%20determined%20for%20a%20set%20of%20collocation%20points.%20Previous%20work%20has%0Ashown%20that%20the%20number%20and%20distribution%20of%20these%20collocation%20points%20have%20a%0Asignificant%20influence%20on%20the%20accuracy%20of%20the%20PINN%20solution.%20Therefore%2C%20the%0Aeffective%20placement%20of%20these%20collocation%20points%20is%20an%20active%20area%20of%20research.%0ASpecifically%2C%20adaptive%20collocation%20point%20sampling%20methods%20have%20been%20proposed%2C%0Awhich%20have%20been%20reported%20to%20scale%20poorly%20to%20higher%20dimensions.%20In%20this%20work%2C%20we%0Aaddress%20this%20issue%20and%20present%20the%20Point%20Adaptive%20Collocation%20Method%20for%0AArtificial%20Neural%20Networks%20%28PACMANN%29.%20Inspired%20by%20classic%20optimization%0Aproblems%2C%20this%20approach%20incrementally%20moves%20collocation%20points%20toward%20regions%0Aof%20higher%20residuals%20using%20gradient-based%20optimization%20algorithms%20guided%20by%20the%0Agradient%20of%20the%20squared%20residual.%20We%20apply%20PACMANN%20for%20forward%20and%20inverse%0Aproblems%2C%20and%20demonstrate%20that%20this%20method%20matches%20the%20performance%20of%0Astate-of-the-art%20methods%20in%20terms%20of%20the%20accuracy/efficiency%20tradeoff%20for%20the%0Alow-dimensional%20problems%2C%20while%20outperforming%20available%20approaches%20for%0Ahigh-dimensional%20problems%3B%20the%20best%20performance%20is%20observed%20for%20the%20Adam%0Aoptimizer.%20Key%20features%20of%20the%20method%20include%20its%20low%20computational%20cost%20and%0Asimplicity%20of%20integration%20in%20existing%20physics-informed%20neural%20network%0Apipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACMANN%253A%2520Point%2520Adaptive%2520Collocation%2520Method%2520for%2520Artificial%2520Neural%250A%2520%2520Networks%26entry.906535625%3DCoen%2520Visser%2520and%2520Alexander%2520Heinlein%2520and%2520Bianca%2520Giovanardi%26entry.1292438233%3D%2520%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%2520are%2520an%2520emerging%2520tool%2520for%250Aapproximating%2520the%2520solution%2520of%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529%2520in%2520both%250Aforward%2520and%2520inverse%2520problems.%2520PINNs%2520minimize%2520a%2520loss%2520function%2520which%2520includes%2520the%250APDE%2520residual%2520determined%2520for%2520a%2520set%2520of%2520collocation%2520points.%2520Previous%2520work%2520has%250Ashown%2520that%2520the%2520number%2520and%2520distribution%2520of%2520these%2520collocation%2520points%2520have%2520a%250Asignificant%2520influence%2520on%2520the%2520accuracy%2520of%2520the%2520PINN%2520solution.%2520Therefore%252C%2520the%250Aeffective%2520placement%2520of%2520these%2520collocation%2520points%2520is%2520an%2520active%2520area%2520of%2520research.%250ASpecifically%252C%2520adaptive%2520collocation%2520point%2520sampling%2520methods%2520have%2520been%2520proposed%252C%250Awhich%2520have%2520been%2520reported%2520to%2520scale%2520poorly%2520to%2520higher%2520dimensions.%2520In%2520this%2520work%252C%2520we%250Aaddress%2520this%2520issue%2520and%2520present%2520the%2520Point%2520Adaptive%2520Collocation%2520Method%2520for%250AArtificial%2520Neural%2520Networks%2520%2528PACMANN%2529.%2520Inspired%2520by%2520classic%2520optimization%250Aproblems%252C%2520this%2520approach%2520incrementally%2520moves%2520collocation%2520points%2520toward%2520regions%250Aof%2520higher%2520residuals%2520using%2520gradient-based%2520optimization%2520algorithms%2520guided%2520by%2520the%250Agradient%2520of%2520the%2520squared%2520residual.%2520We%2520apply%2520PACMANN%2520for%2520forward%2520and%2520inverse%250Aproblems%252C%2520and%2520demonstrate%2520that%2520this%2520method%2520matches%2520the%2520performance%2520of%250Astate-of-the-art%2520methods%2520in%2520terms%2520of%2520the%2520accuracy/efficiency%2520tradeoff%2520for%2520the%250Alow-dimensional%2520problems%252C%2520while%2520outperforming%2520available%2520approaches%2520for%250Ahigh-dimensional%2520problems%253B%2520the%2520best%2520performance%2520is%2520observed%2520for%2520the%2520Adam%250Aoptimizer.%2520Key%2520features%2520of%2520the%2520method%2520include%2520its%2520low%2520computational%2520cost%2520and%250Asimplicity%2520of%2520integration%2520in%2520existing%2520physics-informed%2520neural%2520network%250Apipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACMANN%3A%20Point%20Adaptive%20Collocation%20Method%20for%20Artificial%20Neural%0A%20%20Networks&entry.906535625=Coen%20Visser%20and%20Alexander%20Heinlein%20and%20Bianca%20Giovanardi&entry.1292438233=%20%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20are%20an%20emerging%20tool%20for%0Aapproximating%20the%20solution%20of%20Partial%20Differential%20Equations%20%28PDEs%29%20in%20both%0Aforward%20and%20inverse%20problems.%20PINNs%20minimize%20a%20loss%20function%20which%20includes%20the%0APDE%20residual%20determined%20for%20a%20set%20of%20collocation%20points.%20Previous%20work%20has%0Ashown%20that%20the%20number%20and%20distribution%20of%20these%20collocation%20points%20have%20a%0Asignificant%20influence%20on%20the%20accuracy%20of%20the%20PINN%20solution.%20Therefore%2C%20the%0Aeffective%20placement%20of%20these%20collocation%20points%20is%20an%20active%20area%20of%20research.%0ASpecifically%2C%20adaptive%20collocation%20point%20sampling%20methods%20have%20been%20proposed%2C%0Awhich%20have%20been%20reported%20to%20scale%20poorly%20to%20higher%20dimensions.%20In%20this%20work%2C%20we%0Aaddress%20this%20issue%20and%20present%20the%20Point%20Adaptive%20Collocation%20Method%20for%0AArtificial%20Neural%20Networks%20%28PACMANN%29.%20Inspired%20by%20classic%20optimization%0Aproblems%2C%20this%20approach%20incrementally%20moves%20collocation%20points%20toward%20regions%0Aof%20higher%20residuals%20using%20gradient-based%20optimization%20algorithms%20guided%20by%20the%0Agradient%20of%20the%20squared%20residual.%20We%20apply%20PACMANN%20for%20forward%20and%20inverse%0Aproblems%2C%20and%20demonstrate%20that%20this%20method%20matches%20the%20performance%20of%0Astate-of-the-art%20methods%20in%20terms%20of%20the%20accuracy/efficiency%20tradeoff%20for%20the%0Alow-dimensional%20problems%2C%20while%20outperforming%20available%20approaches%20for%0Ahigh-dimensional%20problems%3B%20the%20best%20performance%20is%20observed%20for%20the%20Adam%0Aoptimizer.%20Key%20features%20of%20the%20method%20include%20its%20low%20computational%20cost%20and%0Asimplicity%20of%20integration%20in%20existing%20physics-informed%20neural%20network%0Apipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19632v1&entry.124074799=Read"},
{"title": "Barrier-Enhanced Parallel Homotopic Trajectory Optimization for\n  Safety-Critical Autonomous Driving", "author": "Lei Zheng and Rui Yang and Michael Yu Wang and Jun Ma", "abstract": "  Enforcing safety while preventing overly conservative behaviors is essential\nfor autonomous vehicles to achieve high task performance. In this paper, we\npropose a barrier-enhanced parallel homotopic trajectory optimization (BPHTO)\napproach with the over-relaxed alternating direction method of multipliers\n(ADMM) for real-time integrated decision-making and planning. To facilitate\nsafety interactions between the ego vehicle (EV) and surrounding vehicles, a\nspatiotemporal safety module exhibiting bi-convexity is developed on the basis\nof barrier function. Varying barrier coefficients are adopted for different\ntime steps in a planning horizon to account for the motion uncertainties of\nsurrounding HVs and mitigate conservative behaviors. Additionally, we exploit\nthe discrete characteristics of driving maneuvers to initialize nominal\nbehavior-oriented free-end homotopic trajectories based on reachability\nanalysis, and each trajectory is locally constrained to a specific driving\nmaneuver while sharing the same task objectives. By leveraging the bi-convexity\nof the safety module and the kinematics of the EV, we formulate the BPHTO as a\nbi-convex optimization problem. Then constraint transcription and the\nover-relaxed ADMM are employed to streamline the optimization process, such\nthat multiple trajectories are generated in real time with feasibility\nguarantees. Through a series of experiments, the proposed development\ndemonstrates improved task accuracy, stability, and consistency in various\ntraffic scenarios using synthetic and real-world traffic datasets.\n", "link": "http://arxiv.org/abs/2402.10441v3", "date": "2024-11-29", "relevancy": 2.0518, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5329}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5289}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Barrier-Enhanced%20Parallel%20Homotopic%20Trajectory%20Optimization%20for%0A%20%20Safety-Critical%20Autonomous%20Driving&body=Title%3A%20Barrier-Enhanced%20Parallel%20Homotopic%20Trajectory%20Optimization%20for%0A%20%20Safety-Critical%20Autonomous%20Driving%0AAuthor%3A%20Lei%20Zheng%20and%20Rui%20Yang%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma%0AAbstract%3A%20%20%20Enforcing%20safety%20while%20preventing%20overly%20conservative%20behaviors%20is%20essential%0Afor%20autonomous%20vehicles%20to%20achieve%20high%20task%20performance.%20In%20this%20paper%2C%20we%0Apropose%20a%20barrier-enhanced%20parallel%20homotopic%20trajectory%20optimization%20%28BPHTO%29%0Aapproach%20with%20the%20over-relaxed%20alternating%20direction%20method%20of%20multipliers%0A%28ADMM%29%20for%20real-time%20integrated%20decision-making%20and%20planning.%20To%20facilitate%0Asafety%20interactions%20between%20the%20ego%20vehicle%20%28EV%29%20and%20surrounding%20vehicles%2C%20a%0Aspatiotemporal%20safety%20module%20exhibiting%20bi-convexity%20is%20developed%20on%20the%20basis%0Aof%20barrier%20function.%20Varying%20barrier%20coefficients%20are%20adopted%20for%20different%0Atime%20steps%20in%20a%20planning%20horizon%20to%20account%20for%20the%20motion%20uncertainties%20of%0Asurrounding%20HVs%20and%20mitigate%20conservative%20behaviors.%20Additionally%2C%20we%20exploit%0Athe%20discrete%20characteristics%20of%20driving%20maneuvers%20to%20initialize%20nominal%0Abehavior-oriented%20free-end%20homotopic%20trajectories%20based%20on%20reachability%0Aanalysis%2C%20and%20each%20trajectory%20is%20locally%20constrained%20to%20a%20specific%20driving%0Amaneuver%20while%20sharing%20the%20same%20task%20objectives.%20By%20leveraging%20the%20bi-convexity%0Aof%20the%20safety%20module%20and%20the%20kinematics%20of%20the%20EV%2C%20we%20formulate%20the%20BPHTO%20as%20a%0Abi-convex%20optimization%20problem.%20Then%20constraint%20transcription%20and%20the%0Aover-relaxed%20ADMM%20are%20employed%20to%20streamline%20the%20optimization%20process%2C%20such%0Athat%20multiple%20trajectories%20are%20generated%20in%20real%20time%20with%20feasibility%0Aguarantees.%20Through%20a%20series%20of%20experiments%2C%20the%20proposed%20development%0Ademonstrates%20improved%20task%20accuracy%2C%20stability%2C%20and%20consistency%20in%20various%0Atraffic%20scenarios%20using%20synthetic%20and%20real-world%20traffic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBarrier-Enhanced%2520Parallel%2520Homotopic%2520Trajectory%2520Optimization%2520for%250A%2520%2520Safety-Critical%2520Autonomous%2520Driving%26entry.906535625%3DLei%2520Zheng%2520and%2520Rui%2520Yang%2520and%2520Michael%2520Yu%2520Wang%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520Enforcing%2520safety%2520while%2520preventing%2520overly%2520conservative%2520behaviors%2520is%2520essential%250Afor%2520autonomous%2520vehicles%2520to%2520achieve%2520high%2520task%2520performance.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520barrier-enhanced%2520parallel%2520homotopic%2520trajectory%2520optimization%2520%2528BPHTO%2529%250Aapproach%2520with%2520the%2520over-relaxed%2520alternating%2520direction%2520method%2520of%2520multipliers%250A%2528ADMM%2529%2520for%2520real-time%2520integrated%2520decision-making%2520and%2520planning.%2520To%2520facilitate%250Asafety%2520interactions%2520between%2520the%2520ego%2520vehicle%2520%2528EV%2529%2520and%2520surrounding%2520vehicles%252C%2520a%250Aspatiotemporal%2520safety%2520module%2520exhibiting%2520bi-convexity%2520is%2520developed%2520on%2520the%2520basis%250Aof%2520barrier%2520function.%2520Varying%2520barrier%2520coefficients%2520are%2520adopted%2520for%2520different%250Atime%2520steps%2520in%2520a%2520planning%2520horizon%2520to%2520account%2520for%2520the%2520motion%2520uncertainties%2520of%250Asurrounding%2520HVs%2520and%2520mitigate%2520conservative%2520behaviors.%2520Additionally%252C%2520we%2520exploit%250Athe%2520discrete%2520characteristics%2520of%2520driving%2520maneuvers%2520to%2520initialize%2520nominal%250Abehavior-oriented%2520free-end%2520homotopic%2520trajectories%2520based%2520on%2520reachability%250Aanalysis%252C%2520and%2520each%2520trajectory%2520is%2520locally%2520constrained%2520to%2520a%2520specific%2520driving%250Amaneuver%2520while%2520sharing%2520the%2520same%2520task%2520objectives.%2520By%2520leveraging%2520the%2520bi-convexity%250Aof%2520the%2520safety%2520module%2520and%2520the%2520kinematics%2520of%2520the%2520EV%252C%2520we%2520formulate%2520the%2520BPHTO%2520as%2520a%250Abi-convex%2520optimization%2520problem.%2520Then%2520constraint%2520transcription%2520and%2520the%250Aover-relaxed%2520ADMM%2520are%2520employed%2520to%2520streamline%2520the%2520optimization%2520process%252C%2520such%250Athat%2520multiple%2520trajectories%2520are%2520generated%2520in%2520real%2520time%2520with%2520feasibility%250Aguarantees.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520the%2520proposed%2520development%250Ademonstrates%2520improved%2520task%2520accuracy%252C%2520stability%252C%2520and%2520consistency%2520in%2520various%250Atraffic%2520scenarios%2520using%2520synthetic%2520and%2520real-world%2520traffic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Barrier-Enhanced%20Parallel%20Homotopic%20Trajectory%20Optimization%20for%0A%20%20Safety-Critical%20Autonomous%20Driving&entry.906535625=Lei%20Zheng%20and%20Rui%20Yang%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma&entry.1292438233=%20%20Enforcing%20safety%20while%20preventing%20overly%20conservative%20behaviors%20is%20essential%0Afor%20autonomous%20vehicles%20to%20achieve%20high%20task%20performance.%20In%20this%20paper%2C%20we%0Apropose%20a%20barrier-enhanced%20parallel%20homotopic%20trajectory%20optimization%20%28BPHTO%29%0Aapproach%20with%20the%20over-relaxed%20alternating%20direction%20method%20of%20multipliers%0A%28ADMM%29%20for%20real-time%20integrated%20decision-making%20and%20planning.%20To%20facilitate%0Asafety%20interactions%20between%20the%20ego%20vehicle%20%28EV%29%20and%20surrounding%20vehicles%2C%20a%0Aspatiotemporal%20safety%20module%20exhibiting%20bi-convexity%20is%20developed%20on%20the%20basis%0Aof%20barrier%20function.%20Varying%20barrier%20coefficients%20are%20adopted%20for%20different%0Atime%20steps%20in%20a%20planning%20horizon%20to%20account%20for%20the%20motion%20uncertainties%20of%0Asurrounding%20HVs%20and%20mitigate%20conservative%20behaviors.%20Additionally%2C%20we%20exploit%0Athe%20discrete%20characteristics%20of%20driving%20maneuvers%20to%20initialize%20nominal%0Abehavior-oriented%20free-end%20homotopic%20trajectories%20based%20on%20reachability%0Aanalysis%2C%20and%20each%20trajectory%20is%20locally%20constrained%20to%20a%20specific%20driving%0Amaneuver%20while%20sharing%20the%20same%20task%20objectives.%20By%20leveraging%20the%20bi-convexity%0Aof%20the%20safety%20module%20and%20the%20kinematics%20of%20the%20EV%2C%20we%20formulate%20the%20BPHTO%20as%20a%0Abi-convex%20optimization%20problem.%20Then%20constraint%20transcription%20and%20the%0Aover-relaxed%20ADMM%20are%20employed%20to%20streamline%20the%20optimization%20process%2C%20such%0Athat%20multiple%20trajectories%20are%20generated%20in%20real%20time%20with%20feasibility%0Aguarantees.%20Through%20a%20series%20of%20experiments%2C%20the%20proposed%20development%0Ademonstrates%20improved%20task%20accuracy%2C%20stability%2C%20and%20consistency%20in%20various%0Atraffic%20scenarios%20using%20synthetic%20and%20real-world%20traffic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10441v3&entry.124074799=Read"},
{"title": "Forensics Adapter: Adapting CLIP for Generalizable Face Forgery\n  Detection", "author": "Xinjie Cui and Yuezun Li and Ao Luo and Jiaran Zhou and Junyu Dong", "abstract": "  We describe the Forensics Adapter, an adapter network designed to transform\nCLIP into an effective and generalizable face forgery detector. Although CLIP\nis highly versatile, adapting it for face forgery detection is non-trivial as\nforgery-related knowledge is entangled with a wide range of unrelated\nknowledge. Existing methods treat CLIP merely as a feature extractor, lacking\ntask-specific adaptation, which limits their effectiveness. To address this, we\nintroduce an adapter to learn face forgery traces -- the blending boundaries\nunique to forged faces, guided by task-specific objectives. Then we enhance the\nCLIP visual tokens with a dedicated interaction strategy that communicates\nknowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its\nversatility is highly retained, naturally ensuring strong generalizability in\nface forgery detection. With only $\\bm{5.7M}$ trainable parameters, our method\nachieves a significant performance boost, improving by approximately $\\bm{7\\%}$\non average across five standard datasets. We believe the proposed method can\nserve as a baseline for future CLIP-based face forgery detection methods.\n", "link": "http://arxiv.org/abs/2411.19715v1", "date": "2024-11-29", "relevancy": 2.0411, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5649}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4719}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forensics%20Adapter%3A%20Adapting%20CLIP%20for%20Generalizable%20Face%20Forgery%0A%20%20Detection&body=Title%3A%20Forensics%20Adapter%3A%20Adapting%20CLIP%20for%20Generalizable%20Face%20Forgery%0A%20%20Detection%0AAuthor%3A%20Xinjie%20Cui%20and%20Yuezun%20Li%20and%20Ao%20Luo%20and%20Jiaran%20Zhou%20and%20Junyu%20Dong%0AAbstract%3A%20%20%20We%20describe%20the%20Forensics%20Adapter%2C%20an%20adapter%20network%20designed%20to%20transform%0ACLIP%20into%20an%20effective%20and%20generalizable%20face%20forgery%20detector.%20Although%20CLIP%0Ais%20highly%20versatile%2C%20adapting%20it%20for%20face%20forgery%20detection%20is%20non-trivial%20as%0Aforgery-related%20knowledge%20is%20entangled%20with%20a%20wide%20range%20of%20unrelated%0Aknowledge.%20Existing%20methods%20treat%20CLIP%20merely%20as%20a%20feature%20extractor%2C%20lacking%0Atask-specific%20adaptation%2C%20which%20limits%20their%20effectiveness.%20To%20address%20this%2C%20we%0Aintroduce%20an%20adapter%20to%20learn%20face%20forgery%20traces%20--%20the%20blending%20boundaries%0Aunique%20to%20forged%20faces%2C%20guided%20by%20task-specific%20objectives.%20Then%20we%20enhance%20the%0ACLIP%20visual%20tokens%20with%20a%20dedicated%20interaction%20strategy%20that%20communicates%0Aknowledge%20across%20CLIP%20and%20the%20adapter.%20Since%20the%20adapter%20is%20alongside%20CLIP%2C%20its%0Aversatility%20is%20highly%20retained%2C%20naturally%20ensuring%20strong%20generalizability%20in%0Aface%20forgery%20detection.%20With%20only%20%24%5Cbm%7B5.7M%7D%24%20trainable%20parameters%2C%20our%20method%0Aachieves%20a%20significant%20performance%20boost%2C%20improving%20by%20approximately%20%24%5Cbm%7B7%5C%25%7D%24%0Aon%20average%20across%20five%20standard%20datasets.%20We%20believe%20the%20proposed%20method%20can%0Aserve%20as%20a%20baseline%20for%20future%20CLIP-based%20face%20forgery%20detection%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForensics%2520Adapter%253A%2520Adapting%2520CLIP%2520for%2520Generalizable%2520Face%2520Forgery%250A%2520%2520Detection%26entry.906535625%3DXinjie%2520Cui%2520and%2520Yuezun%2520Li%2520and%2520Ao%2520Luo%2520and%2520Jiaran%2520Zhou%2520and%2520Junyu%2520Dong%26entry.1292438233%3D%2520%2520We%2520describe%2520the%2520Forensics%2520Adapter%252C%2520an%2520adapter%2520network%2520designed%2520to%2520transform%250ACLIP%2520into%2520an%2520effective%2520and%2520generalizable%2520face%2520forgery%2520detector.%2520Although%2520CLIP%250Ais%2520highly%2520versatile%252C%2520adapting%2520it%2520for%2520face%2520forgery%2520detection%2520is%2520non-trivial%2520as%250Aforgery-related%2520knowledge%2520is%2520entangled%2520with%2520a%2520wide%2520range%2520of%2520unrelated%250Aknowledge.%2520Existing%2520methods%2520treat%2520CLIP%2520merely%2520as%2520a%2520feature%2520extractor%252C%2520lacking%250Atask-specific%2520adaptation%252C%2520which%2520limits%2520their%2520effectiveness.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520an%2520adapter%2520to%2520learn%2520face%2520forgery%2520traces%2520--%2520the%2520blending%2520boundaries%250Aunique%2520to%2520forged%2520faces%252C%2520guided%2520by%2520task-specific%2520objectives.%2520Then%2520we%2520enhance%2520the%250ACLIP%2520visual%2520tokens%2520with%2520a%2520dedicated%2520interaction%2520strategy%2520that%2520communicates%250Aknowledge%2520across%2520CLIP%2520and%2520the%2520adapter.%2520Since%2520the%2520adapter%2520is%2520alongside%2520CLIP%252C%2520its%250Aversatility%2520is%2520highly%2520retained%252C%2520naturally%2520ensuring%2520strong%2520generalizability%2520in%250Aface%2520forgery%2520detection.%2520With%2520only%2520%2524%255Cbm%257B5.7M%257D%2524%2520trainable%2520parameters%252C%2520our%2520method%250Aachieves%2520a%2520significant%2520performance%2520boost%252C%2520improving%2520by%2520approximately%2520%2524%255Cbm%257B7%255C%2525%257D%2524%250Aon%2520average%2520across%2520five%2520standard%2520datasets.%2520We%2520believe%2520the%2520proposed%2520method%2520can%250Aserve%2520as%2520a%2520baseline%2520for%2520future%2520CLIP-based%2520face%2520forgery%2520detection%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forensics%20Adapter%3A%20Adapting%20CLIP%20for%20Generalizable%20Face%20Forgery%0A%20%20Detection&entry.906535625=Xinjie%20Cui%20and%20Yuezun%20Li%20and%20Ao%20Luo%20and%20Jiaran%20Zhou%20and%20Junyu%20Dong&entry.1292438233=%20%20We%20describe%20the%20Forensics%20Adapter%2C%20an%20adapter%20network%20designed%20to%20transform%0ACLIP%20into%20an%20effective%20and%20generalizable%20face%20forgery%20detector.%20Although%20CLIP%0Ais%20highly%20versatile%2C%20adapting%20it%20for%20face%20forgery%20detection%20is%20non-trivial%20as%0Aforgery-related%20knowledge%20is%20entangled%20with%20a%20wide%20range%20of%20unrelated%0Aknowledge.%20Existing%20methods%20treat%20CLIP%20merely%20as%20a%20feature%20extractor%2C%20lacking%0Atask-specific%20adaptation%2C%20which%20limits%20their%20effectiveness.%20To%20address%20this%2C%20we%0Aintroduce%20an%20adapter%20to%20learn%20face%20forgery%20traces%20--%20the%20blending%20boundaries%0Aunique%20to%20forged%20faces%2C%20guided%20by%20task-specific%20objectives.%20Then%20we%20enhance%20the%0ACLIP%20visual%20tokens%20with%20a%20dedicated%20interaction%20strategy%20that%20communicates%0Aknowledge%20across%20CLIP%20and%20the%20adapter.%20Since%20the%20adapter%20is%20alongside%20CLIP%2C%20its%0Aversatility%20is%20highly%20retained%2C%20naturally%20ensuring%20strong%20generalizability%20in%0Aface%20forgery%20detection.%20With%20only%20%24%5Cbm%7B5.7M%7D%24%20trainable%20parameters%2C%20our%20method%0Aachieves%20a%20significant%20performance%20boost%2C%20improving%20by%20approximately%20%24%5Cbm%7B7%5C%25%7D%24%0Aon%20average%20across%20five%20standard%20datasets.%20We%20believe%20the%20proposed%20method%20can%0Aserve%20as%20a%20baseline%20for%20future%20CLIP-based%20face%20forgery%20detection%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19715v1&entry.124074799=Read"},
{"title": "Learning Local Control Barrier Functions for Hybrid Systems", "author": "Shuo Yang and Yu Chen and Xiang Yin and George J. Pappas and Rahul Mangharam", "abstract": "  Hybrid dynamical systems are ubiquitous as practical robotic applications\noften involve both continuous states and discrete switchings. Safety is a\nprimary concern for hybrid robotic systems. Existing safety-critical control\napproaches for hybrid systems are either computationally inefficient,\ndetrimental to system performance, or limited to small-scale systems. To amend\nthese drawbacks, in this paper, we propose a learning-enabled approach to\nconstruct local Control Barrier Functions (CBFs) to guarantee the safety of a\nwide class of nonlinear hybrid dynamical systems. The end result is a safe\nneural CBF-based switching controller. Our approach is computationally\nefficient, minimally invasive to any reference controller, and applicable to\nlarge-scale systems. We empirically evaluate our framework and demonstrate its\nefficacy and flexibility through two robotic examples including a\nhigh-dimensional autonomous racing case, against other CBF-based approaches and\nmodel predictive control.\n", "link": "http://arxiv.org/abs/2401.14907v2", "date": "2024-11-29", "relevancy": 2.0408, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5517}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5227}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Local%20Control%20Barrier%20Functions%20for%20Hybrid%20Systems&body=Title%3A%20Learning%20Local%20Control%20Barrier%20Functions%20for%20Hybrid%20Systems%0AAuthor%3A%20Shuo%20Yang%20and%20Yu%20Chen%20and%20Xiang%20Yin%20and%20George%20J.%20Pappas%20and%20Rahul%20Mangharam%0AAbstract%3A%20%20%20Hybrid%20dynamical%20systems%20are%20ubiquitous%20as%20practical%20robotic%20applications%0Aoften%20involve%20both%20continuous%20states%20and%20discrete%20switchings.%20Safety%20is%20a%0Aprimary%20concern%20for%20hybrid%20robotic%20systems.%20Existing%20safety-critical%20control%0Aapproaches%20for%20hybrid%20systems%20are%20either%20computationally%20inefficient%2C%0Adetrimental%20to%20system%20performance%2C%20or%20limited%20to%20small-scale%20systems.%20To%20amend%0Athese%20drawbacks%2C%20in%20this%20paper%2C%20we%20propose%20a%20learning-enabled%20approach%20to%0Aconstruct%20local%20Control%20Barrier%20Functions%20%28CBFs%29%20to%20guarantee%20the%20safety%20of%20a%0Awide%20class%20of%20nonlinear%20hybrid%20dynamical%20systems.%20The%20end%20result%20is%20a%20safe%0Aneural%20CBF-based%20switching%20controller.%20Our%20approach%20is%20computationally%0Aefficient%2C%20minimally%20invasive%20to%20any%20reference%20controller%2C%20and%20applicable%20to%0Alarge-scale%20systems.%20We%20empirically%20evaluate%20our%20framework%20and%20demonstrate%20its%0Aefficacy%20and%20flexibility%20through%20two%20robotic%20examples%20including%20a%0Ahigh-dimensional%20autonomous%20racing%20case%2C%20against%20other%20CBF-based%20approaches%20and%0Amodel%20predictive%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14907v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Local%2520Control%2520Barrier%2520Functions%2520for%2520Hybrid%2520Systems%26entry.906535625%3DShuo%2520Yang%2520and%2520Yu%2520Chen%2520and%2520Xiang%2520Yin%2520and%2520George%2520J.%2520Pappas%2520and%2520Rahul%2520Mangharam%26entry.1292438233%3D%2520%2520Hybrid%2520dynamical%2520systems%2520are%2520ubiquitous%2520as%2520practical%2520robotic%2520applications%250Aoften%2520involve%2520both%2520continuous%2520states%2520and%2520discrete%2520switchings.%2520Safety%2520is%2520a%250Aprimary%2520concern%2520for%2520hybrid%2520robotic%2520systems.%2520Existing%2520safety-critical%2520control%250Aapproaches%2520for%2520hybrid%2520systems%2520are%2520either%2520computationally%2520inefficient%252C%250Adetrimental%2520to%2520system%2520performance%252C%2520or%2520limited%2520to%2520small-scale%2520systems.%2520To%2520amend%250Athese%2520drawbacks%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520learning-enabled%2520approach%2520to%250Aconstruct%2520local%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529%2520to%2520guarantee%2520the%2520safety%2520of%2520a%250Awide%2520class%2520of%2520nonlinear%2520hybrid%2520dynamical%2520systems.%2520The%2520end%2520result%2520is%2520a%2520safe%250Aneural%2520CBF-based%2520switching%2520controller.%2520Our%2520approach%2520is%2520computationally%250Aefficient%252C%2520minimally%2520invasive%2520to%2520any%2520reference%2520controller%252C%2520and%2520applicable%2520to%250Alarge-scale%2520systems.%2520We%2520empirically%2520evaluate%2520our%2520framework%2520and%2520demonstrate%2520its%250Aefficacy%2520and%2520flexibility%2520through%2520two%2520robotic%2520examples%2520including%2520a%250Ahigh-dimensional%2520autonomous%2520racing%2520case%252C%2520against%2520other%2520CBF-based%2520approaches%2520and%250Amodel%2520predictive%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14907v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Local%20Control%20Barrier%20Functions%20for%20Hybrid%20Systems&entry.906535625=Shuo%20Yang%20and%20Yu%20Chen%20and%20Xiang%20Yin%20and%20George%20J.%20Pappas%20and%20Rahul%20Mangharam&entry.1292438233=%20%20Hybrid%20dynamical%20systems%20are%20ubiquitous%20as%20practical%20robotic%20applications%0Aoften%20involve%20both%20continuous%20states%20and%20discrete%20switchings.%20Safety%20is%20a%0Aprimary%20concern%20for%20hybrid%20robotic%20systems.%20Existing%20safety-critical%20control%0Aapproaches%20for%20hybrid%20systems%20are%20either%20computationally%20inefficient%2C%0Adetrimental%20to%20system%20performance%2C%20or%20limited%20to%20small-scale%20systems.%20To%20amend%0Athese%20drawbacks%2C%20in%20this%20paper%2C%20we%20propose%20a%20learning-enabled%20approach%20to%0Aconstruct%20local%20Control%20Barrier%20Functions%20%28CBFs%29%20to%20guarantee%20the%20safety%20of%20a%0Awide%20class%20of%20nonlinear%20hybrid%20dynamical%20systems.%20The%20end%20result%20is%20a%20safe%0Aneural%20CBF-based%20switching%20controller.%20Our%20approach%20is%20computationally%0Aefficient%2C%20minimally%20invasive%20to%20any%20reference%20controller%2C%20and%20applicable%20to%0Alarge-scale%20systems.%20We%20empirically%20evaluate%20our%20framework%20and%20demonstrate%20its%0Aefficacy%20and%20flexibility%20through%20two%20robotic%20examples%20including%20a%0Ahigh-dimensional%20autonomous%20racing%20case%2C%20against%20other%20CBF-based%20approaches%20and%0Amodel%20predictive%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14907v2&entry.124074799=Read"},
{"title": "Imagined Speech and Visual Imagery as Intuitive Paradigms for\n  Brain-Computer Interfaces", "author": "Seo-Hyun Lee and Ji-Ha Park and Deok-Seon Kim", "abstract": "  Brain-computer interfaces (BCIs) have shown promise in enabling communication\nfor individuals with motor impairments. Recent advancements like\nbrain-to-speech technology aim to reconstruct speech from neural activity.\nHowever, decoding communication-related paradigms, such as imagined speech and\nvisual imagery, using non-invasive techniques remains challenging. This study\nanalyzes brain dynamics in these two paradigms by examining neural\nsynchronization and functional connectivity through phase-locking values (PLV)\nin EEG data from 16 participants. Results show that visual imagery produces\nhigher PLV values in visual cortex, engaging spatial networks, while imagined\nspeech demonstrates consistent synchronization, primarily engaging\nlanguage-related regions. These findings suggest that imagined speech is\nsuitable for language-driven BCI applications, while visual imagery can\ncomplement BCI systems for users with speech impairments. Personalized\ncalibration is crucial for optimizing BCI performance.\n", "link": "http://arxiv.org/abs/2411.09400v2", "date": "2024-11-29", "relevancy": 2.0362, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5146}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imagined%20Speech%20and%20Visual%20Imagery%20as%20Intuitive%20Paradigms%20for%0A%20%20Brain-Computer%20Interfaces&body=Title%3A%20Imagined%20Speech%20and%20Visual%20Imagery%20as%20Intuitive%20Paradigms%20for%0A%20%20Brain-Computer%20Interfaces%0AAuthor%3A%20Seo-Hyun%20Lee%20and%20Ji-Ha%20Park%20and%20Deok-Seon%20Kim%0AAbstract%3A%20%20%20Brain-computer%20interfaces%20%28BCIs%29%20have%20shown%20promise%20in%20enabling%20communication%0Afor%20individuals%20with%20motor%20impairments.%20Recent%20advancements%20like%0Abrain-to-speech%20technology%20aim%20to%20reconstruct%20speech%20from%20neural%20activity.%0AHowever%2C%20decoding%20communication-related%20paradigms%2C%20such%20as%20imagined%20speech%20and%0Avisual%20imagery%2C%20using%20non-invasive%20techniques%20remains%20challenging.%20This%20study%0Aanalyzes%20brain%20dynamics%20in%20these%20two%20paradigms%20by%20examining%20neural%0Asynchronization%20and%20functional%20connectivity%20through%20phase-locking%20values%20%28PLV%29%0Ain%20EEG%20data%20from%2016%20participants.%20Results%20show%20that%20visual%20imagery%20produces%0Ahigher%20PLV%20values%20in%20visual%20cortex%2C%20engaging%20spatial%20networks%2C%20while%20imagined%0Aspeech%20demonstrates%20consistent%20synchronization%2C%20primarily%20engaging%0Alanguage-related%20regions.%20These%20findings%20suggest%20that%20imagined%20speech%20is%0Asuitable%20for%20language-driven%20BCI%20applications%2C%20while%20visual%20imagery%20can%0Acomplement%20BCI%20systems%20for%20users%20with%20speech%20impairments.%20Personalized%0Acalibration%20is%20crucial%20for%20optimizing%20BCI%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.09400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImagined%2520Speech%2520and%2520Visual%2520Imagery%2520as%2520Intuitive%2520Paradigms%2520for%250A%2520%2520Brain-Computer%2520Interfaces%26entry.906535625%3DSeo-Hyun%2520Lee%2520and%2520Ji-Ha%2520Park%2520and%2520Deok-Seon%2520Kim%26entry.1292438233%3D%2520%2520Brain-computer%2520interfaces%2520%2528BCIs%2529%2520have%2520shown%2520promise%2520in%2520enabling%2520communication%250Afor%2520individuals%2520with%2520motor%2520impairments.%2520Recent%2520advancements%2520like%250Abrain-to-speech%2520technology%2520aim%2520to%2520reconstruct%2520speech%2520from%2520neural%2520activity.%250AHowever%252C%2520decoding%2520communication-related%2520paradigms%252C%2520such%2520as%2520imagined%2520speech%2520and%250Avisual%2520imagery%252C%2520using%2520non-invasive%2520techniques%2520remains%2520challenging.%2520This%2520study%250Aanalyzes%2520brain%2520dynamics%2520in%2520these%2520two%2520paradigms%2520by%2520examining%2520neural%250Asynchronization%2520and%2520functional%2520connectivity%2520through%2520phase-locking%2520values%2520%2528PLV%2529%250Ain%2520EEG%2520data%2520from%252016%2520participants.%2520Results%2520show%2520that%2520visual%2520imagery%2520produces%250Ahigher%2520PLV%2520values%2520in%2520visual%2520cortex%252C%2520engaging%2520spatial%2520networks%252C%2520while%2520imagined%250Aspeech%2520demonstrates%2520consistent%2520synchronization%252C%2520primarily%2520engaging%250Alanguage-related%2520regions.%2520These%2520findings%2520suggest%2520that%2520imagined%2520speech%2520is%250Asuitable%2520for%2520language-driven%2520BCI%2520applications%252C%2520while%2520visual%2520imagery%2520can%250Acomplement%2520BCI%2520systems%2520for%2520users%2520with%2520speech%2520impairments.%2520Personalized%250Acalibration%2520is%2520crucial%2520for%2520optimizing%2520BCI%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.09400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imagined%20Speech%20and%20Visual%20Imagery%20as%20Intuitive%20Paradigms%20for%0A%20%20Brain-Computer%20Interfaces&entry.906535625=Seo-Hyun%20Lee%20and%20Ji-Ha%20Park%20and%20Deok-Seon%20Kim&entry.1292438233=%20%20Brain-computer%20interfaces%20%28BCIs%29%20have%20shown%20promise%20in%20enabling%20communication%0Afor%20individuals%20with%20motor%20impairments.%20Recent%20advancements%20like%0Abrain-to-speech%20technology%20aim%20to%20reconstruct%20speech%20from%20neural%20activity.%0AHowever%2C%20decoding%20communication-related%20paradigms%2C%20such%20as%20imagined%20speech%20and%0Avisual%20imagery%2C%20using%20non-invasive%20techniques%20remains%20challenging.%20This%20study%0Aanalyzes%20brain%20dynamics%20in%20these%20two%20paradigms%20by%20examining%20neural%0Asynchronization%20and%20functional%20connectivity%20through%20phase-locking%20values%20%28PLV%29%0Ain%20EEG%20data%20from%2016%20participants.%20Results%20show%20that%20visual%20imagery%20produces%0Ahigher%20PLV%20values%20in%20visual%20cortex%2C%20engaging%20spatial%20networks%2C%20while%20imagined%0Aspeech%20demonstrates%20consistent%20synchronization%2C%20primarily%20engaging%0Alanguage-related%20regions.%20These%20findings%20suggest%20that%20imagined%20speech%20is%0Asuitable%20for%20language-driven%20BCI%20applications%2C%20while%20visual%20imagery%20can%0Acomplement%20BCI%20systems%20for%20users%20with%20speech%20impairments.%20Personalized%0Acalibration%20is%20crucial%20for%20optimizing%20BCI%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.09400v2&entry.124074799=Read"},
{"title": "Risk-Averse Certification of Bayesian Neural Networks", "author": "Xiyue Zhang and Zifan Wang and Yulong Gao and Licio Romao and Alessandro Abate and Marta Kwiatkowska", "abstract": "  In light of the inherently complex and dynamic nature of real-world\nenvironments, incorporating risk measures is crucial for the robustness\nevaluation of deep learning models. In this work, we propose a Risk-Averse\nCertification framework for Bayesian neural networks called RAC-BNN. Our method\nleverages sampling and optimisation to compute a sound approximation of the\noutput set of a BNN, represented using a set of template polytopes. To enhance\nrobustness evaluation, we integrate a coherent distortion risk\nmeasure--Conditional Value at Risk (CVaR)--into the certification framework,\nproviding probabilistic guarantees based on empirical distributions obtained\nthrough sampling. We validate RAC-BNN on a range of regression and\nclassification benchmarks and compare its performance with a state-of-the-art\nmethod. The results show that RAC-BNN effectively quantifies robustness under\nworst-performing risky scenarios, and achieves tighter certified bounds and\nhigher efficiency in complex tasks.\n", "link": "http://arxiv.org/abs/2411.19729v1", "date": "2024-11-29", "relevancy": 2.0297, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5278}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5127}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk-Averse%20Certification%20of%20Bayesian%20Neural%20Networks&body=Title%3A%20Risk-Averse%20Certification%20of%20Bayesian%20Neural%20Networks%0AAuthor%3A%20Xiyue%20Zhang%20and%20Zifan%20Wang%20and%20Yulong%20Gao%20and%20Licio%20Romao%20and%20Alessandro%20Abate%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20In%20light%20of%20the%20inherently%20complex%20and%20dynamic%20nature%20of%20real-world%0Aenvironments%2C%20incorporating%20risk%20measures%20is%20crucial%20for%20the%20robustness%0Aevaluation%20of%20deep%20learning%20models.%20In%20this%20work%2C%20we%20propose%20a%20Risk-Averse%0ACertification%20framework%20for%20Bayesian%20neural%20networks%20called%20RAC-BNN.%20Our%20method%0Aleverages%20sampling%20and%20optimisation%20to%20compute%20a%20sound%20approximation%20of%20the%0Aoutput%20set%20of%20a%20BNN%2C%20represented%20using%20a%20set%20of%20template%20polytopes.%20To%20enhance%0Arobustness%20evaluation%2C%20we%20integrate%20a%20coherent%20distortion%20risk%0Ameasure--Conditional%20Value%20at%20Risk%20%28CVaR%29--into%20the%20certification%20framework%2C%0Aproviding%20probabilistic%20guarantees%20based%20on%20empirical%20distributions%20obtained%0Athrough%20sampling.%20We%20validate%20RAC-BNN%20on%20a%20range%20of%20regression%20and%0Aclassification%20benchmarks%20and%20compare%20its%20performance%20with%20a%20state-of-the-art%0Amethod.%20The%20results%20show%20that%20RAC-BNN%20effectively%20quantifies%20robustness%20under%0Aworst-performing%20risky%20scenarios%2C%20and%20achieves%20tighter%20certified%20bounds%20and%0Ahigher%20efficiency%20in%20complex%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk-Averse%2520Certification%2520of%2520Bayesian%2520Neural%2520Networks%26entry.906535625%3DXiyue%2520Zhang%2520and%2520Zifan%2520Wang%2520and%2520Yulong%2520Gao%2520and%2520Licio%2520Romao%2520and%2520Alessandro%2520Abate%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3D%2520%2520In%2520light%2520of%2520the%2520inherently%2520complex%2520and%2520dynamic%2520nature%2520of%2520real-world%250Aenvironments%252C%2520incorporating%2520risk%2520measures%2520is%2520crucial%2520for%2520the%2520robustness%250Aevaluation%2520of%2520deep%2520learning%2520models.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520Risk-Averse%250ACertification%2520framework%2520for%2520Bayesian%2520neural%2520networks%2520called%2520RAC-BNN.%2520Our%2520method%250Aleverages%2520sampling%2520and%2520optimisation%2520to%2520compute%2520a%2520sound%2520approximation%2520of%2520the%250Aoutput%2520set%2520of%2520a%2520BNN%252C%2520represented%2520using%2520a%2520set%2520of%2520template%2520polytopes.%2520To%2520enhance%250Arobustness%2520evaluation%252C%2520we%2520integrate%2520a%2520coherent%2520distortion%2520risk%250Ameasure--Conditional%2520Value%2520at%2520Risk%2520%2528CVaR%2529--into%2520the%2520certification%2520framework%252C%250Aproviding%2520probabilistic%2520guarantees%2520based%2520on%2520empirical%2520distributions%2520obtained%250Athrough%2520sampling.%2520We%2520validate%2520RAC-BNN%2520on%2520a%2520range%2520of%2520regression%2520and%250Aclassification%2520benchmarks%2520and%2520compare%2520its%2520performance%2520with%2520a%2520state-of-the-art%250Amethod.%2520The%2520results%2520show%2520that%2520RAC-BNN%2520effectively%2520quantifies%2520robustness%2520under%250Aworst-performing%2520risky%2520scenarios%252C%2520and%2520achieves%2520tighter%2520certified%2520bounds%2520and%250Ahigher%2520efficiency%2520in%2520complex%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk-Averse%20Certification%20of%20Bayesian%20Neural%20Networks&entry.906535625=Xiyue%20Zhang%20and%20Zifan%20Wang%20and%20Yulong%20Gao%20and%20Licio%20Romao%20and%20Alessandro%20Abate%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20In%20light%20of%20the%20inherently%20complex%20and%20dynamic%20nature%20of%20real-world%0Aenvironments%2C%20incorporating%20risk%20measures%20is%20crucial%20for%20the%20robustness%0Aevaluation%20of%20deep%20learning%20models.%20In%20this%20work%2C%20we%20propose%20a%20Risk-Averse%0ACertification%20framework%20for%20Bayesian%20neural%20networks%20called%20RAC-BNN.%20Our%20method%0Aleverages%20sampling%20and%20optimisation%20to%20compute%20a%20sound%20approximation%20of%20the%0Aoutput%20set%20of%20a%20BNN%2C%20represented%20using%20a%20set%20of%20template%20polytopes.%20To%20enhance%0Arobustness%20evaluation%2C%20we%20integrate%20a%20coherent%20distortion%20risk%0Ameasure--Conditional%20Value%20at%20Risk%20%28CVaR%29--into%20the%20certification%20framework%2C%0Aproviding%20probabilistic%20guarantees%20based%20on%20empirical%20distributions%20obtained%0Athrough%20sampling.%20We%20validate%20RAC-BNN%20on%20a%20range%20of%20regression%20and%0Aclassification%20benchmarks%20and%20compare%20its%20performance%20with%20a%20state-of-the-art%0Amethod.%20The%20results%20show%20that%20RAC-BNN%20effectively%20quantifies%20robustness%20under%0Aworst-performing%20risky%20scenarios%2C%20and%20achieves%20tighter%20certified%20bounds%20and%0Ahigher%20efficiency%20in%20complex%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19729v1&entry.124074799=Read"},
{"title": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering", "author": "Joris Postmus and Steven Abreu", "abstract": "  Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering.\n", "link": "http://arxiv.org/abs/2410.16314v2", "date": "2024-11-29", "relevancy": 2.0297, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5081}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20Large%20Language%20Models%20using%20Conceptors%3A%20Improving%0A%20%20Addition-Based%20Activation%20Engineering&body=Title%3A%20Steering%20Large%20Language%20Models%20using%20Conceptors%3A%20Improving%0A%20%20Addition-Based%20Activation%20Engineering%0AAuthor%3A%20Joris%20Postmus%20and%20Steven%20Abreu%0AAbstract%3A%20%20%20Large%20language%20models%20have%20transformed%20AI%2C%20yet%20reliably%20controlling%20their%0Aoutputs%20remains%20a%20challenge.%20This%20paper%20explores%20activation%20engineering%2C%20where%0Aoutputs%20of%20pre-trained%20LLMs%20are%20controlled%20by%20manipulating%20their%20activations%20at%0Ainference%20time.%20Unlike%20traditional%20methods%20using%20a%20single%20steering%20vector%2C%20we%0Aintroduce%20conceptors%20-%20mathematical%20constructs%20that%20represent%20sets%20of%0Aactivation%20vectors%20as%20ellipsoidal%20regions.%20Conceptors%20act%20as%20soft%20projection%0Amatrices%20and%20offer%20more%20precise%20control%20over%20complex%20activation%20patterns.%20Our%0Aexperiments%20demonstrate%20that%20conceptors%20outperform%20traditional%20methods%20across%0Amultiple%20steering%20tasks.%20We%20further%20use%20Boolean%20operations%20on%20conceptors%20for%0Acombined%20steering%20goals%20that%20empirically%20outperform%20additively%20combining%0Asteering%20vectors%20on%20a%20set%20of%20tasks.%20These%20results%20highlight%20conceptors%20as%20a%0Apromising%20tool%20for%20more%20effective%20steering%20of%20LLMs.%20Our%20code%20is%20available%20on%0Agithub.com/jorispos/conceptorsteering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520Large%2520Language%2520Models%2520using%2520Conceptors%253A%2520Improving%250A%2520%2520Addition-Based%2520Activation%2520Engineering%26entry.906535625%3DJoris%2520Postmus%2520and%2520Steven%2520Abreu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520transformed%2520AI%252C%2520yet%2520reliably%2520controlling%2520their%250Aoutputs%2520remains%2520a%2520challenge.%2520This%2520paper%2520explores%2520activation%2520engineering%252C%2520where%250Aoutputs%2520of%2520pre-trained%2520LLMs%2520are%2520controlled%2520by%2520manipulating%2520their%2520activations%2520at%250Ainference%2520time.%2520Unlike%2520traditional%2520methods%2520using%2520a%2520single%2520steering%2520vector%252C%2520we%250Aintroduce%2520conceptors%2520-%2520mathematical%2520constructs%2520that%2520represent%2520sets%2520of%250Aactivation%2520vectors%2520as%2520ellipsoidal%2520regions.%2520Conceptors%2520act%2520as%2520soft%2520projection%250Amatrices%2520and%2520offer%2520more%2520precise%2520control%2520over%2520complex%2520activation%2520patterns.%2520Our%250Aexperiments%2520demonstrate%2520that%2520conceptors%2520outperform%2520traditional%2520methods%2520across%250Amultiple%2520steering%2520tasks.%2520We%2520further%2520use%2520Boolean%2520operations%2520on%2520conceptors%2520for%250Acombined%2520steering%2520goals%2520that%2520empirically%2520outperform%2520additively%2520combining%250Asteering%2520vectors%2520on%2520a%2520set%2520of%2520tasks.%2520These%2520results%2520highlight%2520conceptors%2520as%2520a%250Apromising%2520tool%2520for%2520more%2520effective%2520steering%2520of%2520LLMs.%2520Our%2520code%2520is%2520available%2520on%250Agithub.com/jorispos/conceptorsteering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20Large%20Language%20Models%20using%20Conceptors%3A%20Improving%0A%20%20Addition-Based%20Activation%20Engineering&entry.906535625=Joris%20Postmus%20and%20Steven%20Abreu&entry.1292438233=%20%20Large%20language%20models%20have%20transformed%20AI%2C%20yet%20reliably%20controlling%20their%0Aoutputs%20remains%20a%20challenge.%20This%20paper%20explores%20activation%20engineering%2C%20where%0Aoutputs%20of%20pre-trained%20LLMs%20are%20controlled%20by%20manipulating%20their%20activations%20at%0Ainference%20time.%20Unlike%20traditional%20methods%20using%20a%20single%20steering%20vector%2C%20we%0Aintroduce%20conceptors%20-%20mathematical%20constructs%20that%20represent%20sets%20of%0Aactivation%20vectors%20as%20ellipsoidal%20regions.%20Conceptors%20act%20as%20soft%20projection%0Amatrices%20and%20offer%20more%20precise%20control%20over%20complex%20activation%20patterns.%20Our%0Aexperiments%20demonstrate%20that%20conceptors%20outperform%20traditional%20methods%20across%0Amultiple%20steering%20tasks.%20We%20further%20use%20Boolean%20operations%20on%20conceptors%20for%0Acombined%20steering%20goals%20that%20empirically%20outperform%20additively%20combining%0Asteering%20vectors%20on%20a%20set%20of%20tasks.%20These%20results%20highlight%20conceptors%20as%20a%0Apromising%20tool%20for%20more%20effective%20steering%20of%20LLMs.%20Our%20code%20is%20available%20on%0Agithub.com/jorispos/conceptorsteering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16314v2&entry.124074799=Read"},
{"title": "TEAM: Topological Evolution-aware Framework for Traffic\n  Forecasting--Extended Version", "author": "Duc Kieu and Tung Kieu and Peng Han and Bin Yang and Christian S. Jensen and Bac Le", "abstract": "  Due to the global trend towards urbanization, people increasingly move to and\nlive in cities that then continue to grow. Traffic forecasting plays an\nimportant role in the intelligent transportation systems of cities as well as\nin spatio-temporal data mining. State-of-the-art forecasting is achieved by\ndeep-learning approaches due to their ability to contend with complex\nspatio-temporal dynamics. However, existing methods assume the input is\nfixed-topology road networks and static traffic time series. These assumptions\nfail to align with urbanization, where time series are collected continuously\nand road networks evolve over time. In such settings, deep-learning models\nrequire frequent re-initialization and re-training, imposing high computational\ncosts. To enable much more efficient training without jeopardizing model\naccuracy, we propose the Topological Evolution-aware Framework (TEAM) for\ntraffic forecasting that incorporates convolution and attention. This\ncombination of mechanisms enables better adaptation to newly collected time\nseries, while being able to maintain learned knowledge from old time series.\nTEAM features a continual learning module based on the Wasserstein metric that\nacts as a buffer that can identify the most stable and the most changing\nnetwork nodes. Then, only data related to stable nodes is employed for\nre-training when consolidating a model. Further, only data of new nodes and\ntheir adjacent nodes as well as data pertaining to changing nodes are used to\nre-train the model. Empirical studies with two real-world traffic datasets\noffer evidence that TEAM is capable of much lower re-training costs than\nexisting methods are, without jeopardizing forecasting accuracy.\n", "link": "http://arxiv.org/abs/2410.19192v3", "date": "2024-11-29", "relevancy": 2.0211, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5186}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4986}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TEAM%3A%20Topological%20Evolution-aware%20Framework%20for%20Traffic%0A%20%20Forecasting--Extended%20Version&body=Title%3A%20TEAM%3A%20Topological%20Evolution-aware%20Framework%20for%20Traffic%0A%20%20Forecasting--Extended%20Version%0AAuthor%3A%20Duc%20Kieu%20and%20Tung%20Kieu%20and%20Peng%20Han%20and%20Bin%20Yang%20and%20Christian%20S.%20Jensen%20and%20Bac%20Le%0AAbstract%3A%20%20%20Due%20to%20the%20global%20trend%20towards%20urbanization%2C%20people%20increasingly%20move%20to%20and%0Alive%20in%20cities%20that%20then%20continue%20to%20grow.%20Traffic%20forecasting%20plays%20an%0Aimportant%20role%20in%20the%20intelligent%20transportation%20systems%20of%20cities%20as%20well%20as%0Ain%20spatio-temporal%20data%20mining.%20State-of-the-art%20forecasting%20is%20achieved%20by%0Adeep-learning%20approaches%20due%20to%20their%20ability%20to%20contend%20with%20complex%0Aspatio-temporal%20dynamics.%20However%2C%20existing%20methods%20assume%20the%20input%20is%0Afixed-topology%20road%20networks%20and%20static%20traffic%20time%20series.%20These%20assumptions%0Afail%20to%20align%20with%20urbanization%2C%20where%20time%20series%20are%20collected%20continuously%0Aand%20road%20networks%20evolve%20over%20time.%20In%20such%20settings%2C%20deep-learning%20models%0Arequire%20frequent%20re-initialization%20and%20re-training%2C%20imposing%20high%20computational%0Acosts.%20To%20enable%20much%20more%20efficient%20training%20without%20jeopardizing%20model%0Aaccuracy%2C%20we%20propose%20the%20Topological%20Evolution-aware%20Framework%20%28TEAM%29%20for%0Atraffic%20forecasting%20that%20incorporates%20convolution%20and%20attention.%20This%0Acombination%20of%20mechanisms%20enables%20better%20adaptation%20to%20newly%20collected%20time%0Aseries%2C%20while%20being%20able%20to%20maintain%20learned%20knowledge%20from%20old%20time%20series.%0ATEAM%20features%20a%20continual%20learning%20module%20based%20on%20the%20Wasserstein%20metric%20that%0Aacts%20as%20a%20buffer%20that%20can%20identify%20the%20most%20stable%20and%20the%20most%20changing%0Anetwork%20nodes.%20Then%2C%20only%20data%20related%20to%20stable%20nodes%20is%20employed%20for%0Are-training%20when%20consolidating%20a%20model.%20Further%2C%20only%20data%20of%20new%20nodes%20and%0Atheir%20adjacent%20nodes%20as%20well%20as%20data%20pertaining%20to%20changing%20nodes%20are%20used%20to%0Are-train%20the%20model.%20Empirical%20studies%20with%20two%20real-world%20traffic%20datasets%0Aoffer%20evidence%20that%20TEAM%20is%20capable%20of%20much%20lower%20re-training%20costs%20than%0Aexisting%20methods%20are%2C%20without%20jeopardizing%20forecasting%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19192v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTEAM%253A%2520Topological%2520Evolution-aware%2520Framework%2520for%2520Traffic%250A%2520%2520Forecasting--Extended%2520Version%26entry.906535625%3DDuc%2520Kieu%2520and%2520Tung%2520Kieu%2520and%2520Peng%2520Han%2520and%2520Bin%2520Yang%2520and%2520Christian%2520S.%2520Jensen%2520and%2520Bac%2520Le%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520global%2520trend%2520towards%2520urbanization%252C%2520people%2520increasingly%2520move%2520to%2520and%250Alive%2520in%2520cities%2520that%2520then%2520continue%2520to%2520grow.%2520Traffic%2520forecasting%2520plays%2520an%250Aimportant%2520role%2520in%2520the%2520intelligent%2520transportation%2520systems%2520of%2520cities%2520as%2520well%2520as%250Ain%2520spatio-temporal%2520data%2520mining.%2520State-of-the-art%2520forecasting%2520is%2520achieved%2520by%250Adeep-learning%2520approaches%2520due%2520to%2520their%2520ability%2520to%2520contend%2520with%2520complex%250Aspatio-temporal%2520dynamics.%2520However%252C%2520existing%2520methods%2520assume%2520the%2520input%2520is%250Afixed-topology%2520road%2520networks%2520and%2520static%2520traffic%2520time%2520series.%2520These%2520assumptions%250Afail%2520to%2520align%2520with%2520urbanization%252C%2520where%2520time%2520series%2520are%2520collected%2520continuously%250Aand%2520road%2520networks%2520evolve%2520over%2520time.%2520In%2520such%2520settings%252C%2520deep-learning%2520models%250Arequire%2520frequent%2520re-initialization%2520and%2520re-training%252C%2520imposing%2520high%2520computational%250Acosts.%2520To%2520enable%2520much%2520more%2520efficient%2520training%2520without%2520jeopardizing%2520model%250Aaccuracy%252C%2520we%2520propose%2520the%2520Topological%2520Evolution-aware%2520Framework%2520%2528TEAM%2529%2520for%250Atraffic%2520forecasting%2520that%2520incorporates%2520convolution%2520and%2520attention.%2520This%250Acombination%2520of%2520mechanisms%2520enables%2520better%2520adaptation%2520to%2520newly%2520collected%2520time%250Aseries%252C%2520while%2520being%2520able%2520to%2520maintain%2520learned%2520knowledge%2520from%2520old%2520time%2520series.%250ATEAM%2520features%2520a%2520continual%2520learning%2520module%2520based%2520on%2520the%2520Wasserstein%2520metric%2520that%250Aacts%2520as%2520a%2520buffer%2520that%2520can%2520identify%2520the%2520most%2520stable%2520and%2520the%2520most%2520changing%250Anetwork%2520nodes.%2520Then%252C%2520only%2520data%2520related%2520to%2520stable%2520nodes%2520is%2520employed%2520for%250Are-training%2520when%2520consolidating%2520a%2520model.%2520Further%252C%2520only%2520data%2520of%2520new%2520nodes%2520and%250Atheir%2520adjacent%2520nodes%2520as%2520well%2520as%2520data%2520pertaining%2520to%2520changing%2520nodes%2520are%2520used%2520to%250Are-train%2520the%2520model.%2520Empirical%2520studies%2520with%2520two%2520real-world%2520traffic%2520datasets%250Aoffer%2520evidence%2520that%2520TEAM%2520is%2520capable%2520of%2520much%2520lower%2520re-training%2520costs%2520than%250Aexisting%2520methods%2520are%252C%2520without%2520jeopardizing%2520forecasting%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19192v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TEAM%3A%20Topological%20Evolution-aware%20Framework%20for%20Traffic%0A%20%20Forecasting--Extended%20Version&entry.906535625=Duc%20Kieu%20and%20Tung%20Kieu%20and%20Peng%20Han%20and%20Bin%20Yang%20and%20Christian%20S.%20Jensen%20and%20Bac%20Le&entry.1292438233=%20%20Due%20to%20the%20global%20trend%20towards%20urbanization%2C%20people%20increasingly%20move%20to%20and%0Alive%20in%20cities%20that%20then%20continue%20to%20grow.%20Traffic%20forecasting%20plays%20an%0Aimportant%20role%20in%20the%20intelligent%20transportation%20systems%20of%20cities%20as%20well%20as%0Ain%20spatio-temporal%20data%20mining.%20State-of-the-art%20forecasting%20is%20achieved%20by%0Adeep-learning%20approaches%20due%20to%20their%20ability%20to%20contend%20with%20complex%0Aspatio-temporal%20dynamics.%20However%2C%20existing%20methods%20assume%20the%20input%20is%0Afixed-topology%20road%20networks%20and%20static%20traffic%20time%20series.%20These%20assumptions%0Afail%20to%20align%20with%20urbanization%2C%20where%20time%20series%20are%20collected%20continuously%0Aand%20road%20networks%20evolve%20over%20time.%20In%20such%20settings%2C%20deep-learning%20models%0Arequire%20frequent%20re-initialization%20and%20re-training%2C%20imposing%20high%20computational%0Acosts.%20To%20enable%20much%20more%20efficient%20training%20without%20jeopardizing%20model%0Aaccuracy%2C%20we%20propose%20the%20Topological%20Evolution-aware%20Framework%20%28TEAM%29%20for%0Atraffic%20forecasting%20that%20incorporates%20convolution%20and%20attention.%20This%0Acombination%20of%20mechanisms%20enables%20better%20adaptation%20to%20newly%20collected%20time%0Aseries%2C%20while%20being%20able%20to%20maintain%20learned%20knowledge%20from%20old%20time%20series.%0ATEAM%20features%20a%20continual%20learning%20module%20based%20on%20the%20Wasserstein%20metric%20that%0Aacts%20as%20a%20buffer%20that%20can%20identify%20the%20most%20stable%20and%20the%20most%20changing%0Anetwork%20nodes.%20Then%2C%20only%20data%20related%20to%20stable%20nodes%20is%20employed%20for%0Are-training%20when%20consolidating%20a%20model.%20Further%2C%20only%20data%20of%20new%20nodes%20and%0Atheir%20adjacent%20nodes%20as%20well%20as%20data%20pertaining%20to%20changing%20nodes%20are%20used%20to%0Are-train%20the%20model.%20Empirical%20studies%20with%20two%20real-world%20traffic%20datasets%0Aoffer%20evidence%20that%20TEAM%20is%20capable%20of%20much%20lower%20re-training%20costs%20than%0Aexisting%20methods%20are%2C%20without%20jeopardizing%20forecasting%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19192v3&entry.124074799=Read"},
{"title": "A data driven approach to classify descriptors based on their efficiency\n  in translating noisy trajectories into physically-relevant information", "author": "Simone Martino and Domiziano Doria and Chiara Lionello and Matteo Becchi and Giovanni M. Pavan", "abstract": "  Reconstructing the physical complexity of many-body dynamical systems can be\nchallenging. Starting from the trajectories of their constitutive units (raw\ndata), typical approaches require selecting appropriate descriptors to convert\nthem into time-series, which are then analyzed to extract interpretable\ninformation. However, identifying the most effective descriptor is often\nnon-trivial. Here, we report a data-driven approach to compare the efficiency\nof various descriptors in extracting information from noisy trajectories and\ntranslating it into physically relevant insights. As a prototypical system with\nnon-trivial internal complexity, we analyze molecular dynamics trajectories of\nan atomistic system where ice and water coexist in equilibrium near the\nsolid/liquid transition temperature. We compare general and specific\ndescriptors often used in aqueous systems: number of neighbors, molecular\nvelocities, Smooth Overlap of Atomic Positions (SOAP), Local Environments and\nNeighbors Shuffling (LENS), Orientational Tetrahedral Order, and distance from\nthe fifth neighbor ($d_5$). Using Onion Clustering -- an efficient unsupervised\nmethod for single-point time-series analysis -- we assess the maximum\nextractable information for each descriptor and rank them via a\nhigh-dimensional metric. Our results show that advanced descriptors like SOAP\nand LENS outperform classical ones due to higher signal-to-noise ratios.\nNonetheless, even simple descriptors can rival or exceed advanced ones after\nlocal signal denoising. For example, $d_5$, initially among the weakest,\nbecomes the most effective at resolving the system's non-local dynamical\ncomplexity after denoising. This work highlights the critical role of noise in\ninformation extraction from molecular trajectories and offers a data-driven\napproach to identify optimal descriptors for systems with characteristic\ninternal complexity.\n", "link": "http://arxiv.org/abs/2411.12570v2", "date": "2024-11-29", "relevancy": 2.0113, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20data%20driven%20approach%20to%20classify%20descriptors%20based%20on%20their%20efficiency%0A%20%20in%20translating%20noisy%20trajectories%20into%20physically-relevant%20information&body=Title%3A%20A%20data%20driven%20approach%20to%20classify%20descriptors%20based%20on%20their%20efficiency%0A%20%20in%20translating%20noisy%20trajectories%20into%20physically-relevant%20information%0AAuthor%3A%20Simone%20Martino%20and%20Domiziano%20Doria%20and%20Chiara%20Lionello%20and%20Matteo%20Becchi%20and%20Giovanni%20M.%20Pavan%0AAbstract%3A%20%20%20Reconstructing%20the%20physical%20complexity%20of%20many-body%20dynamical%20systems%20can%20be%0Achallenging.%20Starting%20from%20the%20trajectories%20of%20their%20constitutive%20units%20%28raw%0Adata%29%2C%20typical%20approaches%20require%20selecting%20appropriate%20descriptors%20to%20convert%0Athem%20into%20time-series%2C%20which%20are%20then%20analyzed%20to%20extract%20interpretable%0Ainformation.%20However%2C%20identifying%20the%20most%20effective%20descriptor%20is%20often%0Anon-trivial.%20Here%2C%20we%20report%20a%20data-driven%20approach%20to%20compare%20the%20efficiency%0Aof%20various%20descriptors%20in%20extracting%20information%20from%20noisy%20trajectories%20and%0Atranslating%20it%20into%20physically%20relevant%20insights.%20As%20a%20prototypical%20system%20with%0Anon-trivial%20internal%20complexity%2C%20we%20analyze%20molecular%20dynamics%20trajectories%20of%0Aan%20atomistic%20system%20where%20ice%20and%20water%20coexist%20in%20equilibrium%20near%20the%0Asolid/liquid%20transition%20temperature.%20We%20compare%20general%20and%20specific%0Adescriptors%20often%20used%20in%20aqueous%20systems%3A%20number%20of%20neighbors%2C%20molecular%0Avelocities%2C%20Smooth%20Overlap%20of%20Atomic%20Positions%20%28SOAP%29%2C%20Local%20Environments%20and%0ANeighbors%20Shuffling%20%28LENS%29%2C%20Orientational%20Tetrahedral%20Order%2C%20and%20distance%20from%0Athe%20fifth%20neighbor%20%28%24d_5%24%29.%20Using%20Onion%20Clustering%20--%20an%20efficient%20unsupervised%0Amethod%20for%20single-point%20time-series%20analysis%20--%20we%20assess%20the%20maximum%0Aextractable%20information%20for%20each%20descriptor%20and%20rank%20them%20via%20a%0Ahigh-dimensional%20metric.%20Our%20results%20show%20that%20advanced%20descriptors%20like%20SOAP%0Aand%20LENS%20outperform%20classical%20ones%20due%20to%20higher%20signal-to-noise%20ratios.%0ANonetheless%2C%20even%20simple%20descriptors%20can%20rival%20or%20exceed%20advanced%20ones%20after%0Alocal%20signal%20denoising.%20For%20example%2C%20%24d_5%24%2C%20initially%20among%20the%20weakest%2C%0Abecomes%20the%20most%20effective%20at%20resolving%20the%20system%27s%20non-local%20dynamical%0Acomplexity%20after%20denoising.%20This%20work%20highlights%20the%20critical%20role%20of%20noise%20in%0Ainformation%20extraction%20from%20molecular%20trajectories%20and%20offers%20a%20data-driven%0Aapproach%20to%20identify%20optimal%20descriptors%20for%20systems%20with%20characteristic%0Ainternal%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.12570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520data%2520driven%2520approach%2520to%2520classify%2520descriptors%2520based%2520on%2520their%2520efficiency%250A%2520%2520in%2520translating%2520noisy%2520trajectories%2520into%2520physically-relevant%2520information%26entry.906535625%3DSimone%2520Martino%2520and%2520Domiziano%2520Doria%2520and%2520Chiara%2520Lionello%2520and%2520Matteo%2520Becchi%2520and%2520Giovanni%2520M.%2520Pavan%26entry.1292438233%3D%2520%2520Reconstructing%2520the%2520physical%2520complexity%2520of%2520many-body%2520dynamical%2520systems%2520can%2520be%250Achallenging.%2520Starting%2520from%2520the%2520trajectories%2520of%2520their%2520constitutive%2520units%2520%2528raw%250Adata%2529%252C%2520typical%2520approaches%2520require%2520selecting%2520appropriate%2520descriptors%2520to%2520convert%250Athem%2520into%2520time-series%252C%2520which%2520are%2520then%2520analyzed%2520to%2520extract%2520interpretable%250Ainformation.%2520However%252C%2520identifying%2520the%2520most%2520effective%2520descriptor%2520is%2520often%250Anon-trivial.%2520Here%252C%2520we%2520report%2520a%2520data-driven%2520approach%2520to%2520compare%2520the%2520efficiency%250Aof%2520various%2520descriptors%2520in%2520extracting%2520information%2520from%2520noisy%2520trajectories%2520and%250Atranslating%2520it%2520into%2520physically%2520relevant%2520insights.%2520As%2520a%2520prototypical%2520system%2520with%250Anon-trivial%2520internal%2520complexity%252C%2520we%2520analyze%2520molecular%2520dynamics%2520trajectories%2520of%250Aan%2520atomistic%2520system%2520where%2520ice%2520and%2520water%2520coexist%2520in%2520equilibrium%2520near%2520the%250Asolid/liquid%2520transition%2520temperature.%2520We%2520compare%2520general%2520and%2520specific%250Adescriptors%2520often%2520used%2520in%2520aqueous%2520systems%253A%2520number%2520of%2520neighbors%252C%2520molecular%250Avelocities%252C%2520Smooth%2520Overlap%2520of%2520Atomic%2520Positions%2520%2528SOAP%2529%252C%2520Local%2520Environments%2520and%250ANeighbors%2520Shuffling%2520%2528LENS%2529%252C%2520Orientational%2520Tetrahedral%2520Order%252C%2520and%2520distance%2520from%250Athe%2520fifth%2520neighbor%2520%2528%2524d_5%2524%2529.%2520Using%2520Onion%2520Clustering%2520--%2520an%2520efficient%2520unsupervised%250Amethod%2520for%2520single-point%2520time-series%2520analysis%2520--%2520we%2520assess%2520the%2520maximum%250Aextractable%2520information%2520for%2520each%2520descriptor%2520and%2520rank%2520them%2520via%2520a%250Ahigh-dimensional%2520metric.%2520Our%2520results%2520show%2520that%2520advanced%2520descriptors%2520like%2520SOAP%250Aand%2520LENS%2520outperform%2520classical%2520ones%2520due%2520to%2520higher%2520signal-to-noise%2520ratios.%250ANonetheless%252C%2520even%2520simple%2520descriptors%2520can%2520rival%2520or%2520exceed%2520advanced%2520ones%2520after%250Alocal%2520signal%2520denoising.%2520For%2520example%252C%2520%2524d_5%2524%252C%2520initially%2520among%2520the%2520weakest%252C%250Abecomes%2520the%2520most%2520effective%2520at%2520resolving%2520the%2520system%2527s%2520non-local%2520dynamical%250Acomplexity%2520after%2520denoising.%2520This%2520work%2520highlights%2520the%2520critical%2520role%2520of%2520noise%2520in%250Ainformation%2520extraction%2520from%2520molecular%2520trajectories%2520and%2520offers%2520a%2520data-driven%250Aapproach%2520to%2520identify%2520optimal%2520descriptors%2520for%2520systems%2520with%2520characteristic%250Ainternal%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.12570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20data%20driven%20approach%20to%20classify%20descriptors%20based%20on%20their%20efficiency%0A%20%20in%20translating%20noisy%20trajectories%20into%20physically-relevant%20information&entry.906535625=Simone%20Martino%20and%20Domiziano%20Doria%20and%20Chiara%20Lionello%20and%20Matteo%20Becchi%20and%20Giovanni%20M.%20Pavan&entry.1292438233=%20%20Reconstructing%20the%20physical%20complexity%20of%20many-body%20dynamical%20systems%20can%20be%0Achallenging.%20Starting%20from%20the%20trajectories%20of%20their%20constitutive%20units%20%28raw%0Adata%29%2C%20typical%20approaches%20require%20selecting%20appropriate%20descriptors%20to%20convert%0Athem%20into%20time-series%2C%20which%20are%20then%20analyzed%20to%20extract%20interpretable%0Ainformation.%20However%2C%20identifying%20the%20most%20effective%20descriptor%20is%20often%0Anon-trivial.%20Here%2C%20we%20report%20a%20data-driven%20approach%20to%20compare%20the%20efficiency%0Aof%20various%20descriptors%20in%20extracting%20information%20from%20noisy%20trajectories%20and%0Atranslating%20it%20into%20physically%20relevant%20insights.%20As%20a%20prototypical%20system%20with%0Anon-trivial%20internal%20complexity%2C%20we%20analyze%20molecular%20dynamics%20trajectories%20of%0Aan%20atomistic%20system%20where%20ice%20and%20water%20coexist%20in%20equilibrium%20near%20the%0Asolid/liquid%20transition%20temperature.%20We%20compare%20general%20and%20specific%0Adescriptors%20often%20used%20in%20aqueous%20systems%3A%20number%20of%20neighbors%2C%20molecular%0Avelocities%2C%20Smooth%20Overlap%20of%20Atomic%20Positions%20%28SOAP%29%2C%20Local%20Environments%20and%0ANeighbors%20Shuffling%20%28LENS%29%2C%20Orientational%20Tetrahedral%20Order%2C%20and%20distance%20from%0Athe%20fifth%20neighbor%20%28%24d_5%24%29.%20Using%20Onion%20Clustering%20--%20an%20efficient%20unsupervised%0Amethod%20for%20single-point%20time-series%20analysis%20--%20we%20assess%20the%20maximum%0Aextractable%20information%20for%20each%20descriptor%20and%20rank%20them%20via%20a%0Ahigh-dimensional%20metric.%20Our%20results%20show%20that%20advanced%20descriptors%20like%20SOAP%0Aand%20LENS%20outperform%20classical%20ones%20due%20to%20higher%20signal-to-noise%20ratios.%0ANonetheless%2C%20even%20simple%20descriptors%20can%20rival%20or%20exceed%20advanced%20ones%20after%0Alocal%20signal%20denoising.%20For%20example%2C%20%24d_5%24%2C%20initially%20among%20the%20weakest%2C%0Abecomes%20the%20most%20effective%20at%20resolving%20the%20system%27s%20non-local%20dynamical%0Acomplexity%20after%20denoising.%20This%20work%20highlights%20the%20critical%20role%20of%20noise%20in%0Ainformation%20extraction%20from%20molecular%20trajectories%20and%20offers%20a%20data-driven%0Aapproach%20to%20identify%20optimal%20descriptors%20for%20systems%20with%20characteristic%0Ainternal%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.12570v2&entry.124074799=Read"},
{"title": "Robust Stochastically-Descending Unrolled Networks", "author": "Samar Hadou and Navid NaderiAlizadeh and Alejandro Ribeiro", "abstract": "  Deep unrolling, or unfolding, is an emerging learning-to-optimize method that\nunrolls a truncated iterative algorithm in the layers of a trainable neural\nnetwork. However, the convergence guarantees and generalizability of the\nunrolled networks are still open theoretical problems. To tackle these\nproblems, we provide deep unrolled architectures with a stochastic descent\nnature by imposing descending constraints during training. The descending\nconstraints are forced layer by layer to ensure that each unrolled layer takes,\non average, a descent step toward the optimum during training. We theoretically\nprove that the sequence constructed by the outputs of the unrolled layers is\nthen guaranteed to converge for unseen problems, assuming no distribution shift\nbetween training and test problems. We also show that standard unrolling is\nbrittle to perturbations, and our imposed constraints provide the unrolled\nnetworks with robustness to additive noise and perturbations. We numerically\nassess unrolled architectures trained under the proposed constraints in two\ndifferent applications, including the sparse coding using learnable iterative\nshrinkage and thresholding algorithm (LISTA) and image inpainting using\nproximal generative flow (GLOW-Prox), and demonstrate the performance and\nrobustness benefits of the proposed method.\n", "link": "http://arxiv.org/abs/2312.15788v2", "date": "2024-11-29", "relevancy": 2.0037, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5167}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4994}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Stochastically-Descending%20Unrolled%20Networks&body=Title%3A%20Robust%20Stochastically-Descending%20Unrolled%20Networks%0AAuthor%3A%20Samar%20Hadou%20and%20Navid%20NaderiAlizadeh%20and%20Alejandro%20Ribeiro%0AAbstract%3A%20%20%20Deep%20unrolling%2C%20or%20unfolding%2C%20is%20an%20emerging%20learning-to-optimize%20method%20that%0Aunrolls%20a%20truncated%20iterative%20algorithm%20in%20the%20layers%20of%20a%20trainable%20neural%0Anetwork.%20However%2C%20the%20convergence%20guarantees%20and%20generalizability%20of%20the%0Aunrolled%20networks%20are%20still%20open%20theoretical%20problems.%20To%20tackle%20these%0Aproblems%2C%20we%20provide%20deep%20unrolled%20architectures%20with%20a%20stochastic%20descent%0Anature%20by%20imposing%20descending%20constraints%20during%20training.%20The%20descending%0Aconstraints%20are%20forced%20layer%20by%20layer%20to%20ensure%20that%20each%20unrolled%20layer%20takes%2C%0Aon%20average%2C%20a%20descent%20step%20toward%20the%20optimum%20during%20training.%20We%20theoretically%0Aprove%20that%20the%20sequence%20constructed%20by%20the%20outputs%20of%20the%20unrolled%20layers%20is%0Athen%20guaranteed%20to%20converge%20for%20unseen%20problems%2C%20assuming%20no%20distribution%20shift%0Abetween%20training%20and%20test%20problems.%20We%20also%20show%20that%20standard%20unrolling%20is%0Abrittle%20to%20perturbations%2C%20and%20our%20imposed%20constraints%20provide%20the%20unrolled%0Anetworks%20with%20robustness%20to%20additive%20noise%20and%20perturbations.%20We%20numerically%0Aassess%20unrolled%20architectures%20trained%20under%20the%20proposed%20constraints%20in%20two%0Adifferent%20applications%2C%20including%20the%20sparse%20coding%20using%20learnable%20iterative%0Ashrinkage%20and%20thresholding%20algorithm%20%28LISTA%29%20and%20image%20inpainting%20using%0Aproximal%20generative%20flow%20%28GLOW-Prox%29%2C%20and%20demonstrate%20the%20performance%20and%0Arobustness%20benefits%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Stochastically-Descending%2520Unrolled%2520Networks%26entry.906535625%3DSamar%2520Hadou%2520and%2520Navid%2520NaderiAlizadeh%2520and%2520Alejandro%2520Ribeiro%26entry.1292438233%3D%2520%2520Deep%2520unrolling%252C%2520or%2520unfolding%252C%2520is%2520an%2520emerging%2520learning-to-optimize%2520method%2520that%250Aunrolls%2520a%2520truncated%2520iterative%2520algorithm%2520in%2520the%2520layers%2520of%2520a%2520trainable%2520neural%250Anetwork.%2520However%252C%2520the%2520convergence%2520guarantees%2520and%2520generalizability%2520of%2520the%250Aunrolled%2520networks%2520are%2520still%2520open%2520theoretical%2520problems.%2520To%2520tackle%2520these%250Aproblems%252C%2520we%2520provide%2520deep%2520unrolled%2520architectures%2520with%2520a%2520stochastic%2520descent%250Anature%2520by%2520imposing%2520descending%2520constraints%2520during%2520training.%2520The%2520descending%250Aconstraints%2520are%2520forced%2520layer%2520by%2520layer%2520to%2520ensure%2520that%2520each%2520unrolled%2520layer%2520takes%252C%250Aon%2520average%252C%2520a%2520descent%2520step%2520toward%2520the%2520optimum%2520during%2520training.%2520We%2520theoretically%250Aprove%2520that%2520the%2520sequence%2520constructed%2520by%2520the%2520outputs%2520of%2520the%2520unrolled%2520layers%2520is%250Athen%2520guaranteed%2520to%2520converge%2520for%2520unseen%2520problems%252C%2520assuming%2520no%2520distribution%2520shift%250Abetween%2520training%2520and%2520test%2520problems.%2520We%2520also%2520show%2520that%2520standard%2520unrolling%2520is%250Abrittle%2520to%2520perturbations%252C%2520and%2520our%2520imposed%2520constraints%2520provide%2520the%2520unrolled%250Anetworks%2520with%2520robustness%2520to%2520additive%2520noise%2520and%2520perturbations.%2520We%2520numerically%250Aassess%2520unrolled%2520architectures%2520trained%2520under%2520the%2520proposed%2520constraints%2520in%2520two%250Adifferent%2520applications%252C%2520including%2520the%2520sparse%2520coding%2520using%2520learnable%2520iterative%250Ashrinkage%2520and%2520thresholding%2520algorithm%2520%2528LISTA%2529%2520and%2520image%2520inpainting%2520using%250Aproximal%2520generative%2520flow%2520%2528GLOW-Prox%2529%252C%2520and%2520demonstrate%2520the%2520performance%2520and%250Arobustness%2520benefits%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Stochastically-Descending%20Unrolled%20Networks&entry.906535625=Samar%20Hadou%20and%20Navid%20NaderiAlizadeh%20and%20Alejandro%20Ribeiro&entry.1292438233=%20%20Deep%20unrolling%2C%20or%20unfolding%2C%20is%20an%20emerging%20learning-to-optimize%20method%20that%0Aunrolls%20a%20truncated%20iterative%20algorithm%20in%20the%20layers%20of%20a%20trainable%20neural%0Anetwork.%20However%2C%20the%20convergence%20guarantees%20and%20generalizability%20of%20the%0Aunrolled%20networks%20are%20still%20open%20theoretical%20problems.%20To%20tackle%20these%0Aproblems%2C%20we%20provide%20deep%20unrolled%20architectures%20with%20a%20stochastic%20descent%0Anature%20by%20imposing%20descending%20constraints%20during%20training.%20The%20descending%0Aconstraints%20are%20forced%20layer%20by%20layer%20to%20ensure%20that%20each%20unrolled%20layer%20takes%2C%0Aon%20average%2C%20a%20descent%20step%20toward%20the%20optimum%20during%20training.%20We%20theoretically%0Aprove%20that%20the%20sequence%20constructed%20by%20the%20outputs%20of%20the%20unrolled%20layers%20is%0Athen%20guaranteed%20to%20converge%20for%20unseen%20problems%2C%20assuming%20no%20distribution%20shift%0Abetween%20training%20and%20test%20problems.%20We%20also%20show%20that%20standard%20unrolling%20is%0Abrittle%20to%20perturbations%2C%20and%20our%20imposed%20constraints%20provide%20the%20unrolled%0Anetworks%20with%20robustness%20to%20additive%20noise%20and%20perturbations.%20We%20numerically%0Aassess%20unrolled%20architectures%20trained%20under%20the%20proposed%20constraints%20in%20two%0Adifferent%20applications%2C%20including%20the%20sparse%20coding%20using%20learnable%20iterative%0Ashrinkage%20and%20thresholding%20algorithm%20%28LISTA%29%20and%20image%20inpainting%20using%0Aproximal%20generative%20flow%20%28GLOW-Prox%29%2C%20and%20demonstrate%20the%20performance%20and%0Arobustness%20benefits%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15788v2&entry.124074799=Read"},
{"title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries", "author": "Jonathan F\u00fcrst and Catherine Kosten and Farhad Nooralahzadeh and Yi Zhang and Kurt Stockinger", "abstract": "  Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.\n", "link": "http://arxiv.org/abs/2402.08349v3", "date": "2024-11-29", "relevancy": 2.0024, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Data%20Model%20Robustness%20of%20Text-to-SQL%20Systems%20Based%20on%0A%20%20Real%20User%20Queries&body=Title%3A%20Evaluating%20the%20Data%20Model%20Robustness%20of%20Text-to-SQL%20Systems%20Based%20on%0A%20%20Real%20User%20Queries%0AAuthor%3A%20Jonathan%20F%C3%BCrst%20and%20Catherine%20Kosten%20and%20Farhad%20Nooralahzadeh%20and%20Yi%20Zhang%20and%20Kurt%20Stockinger%0AAbstract%3A%20%20%20Text-to-SQL%20systems%20%28also%20known%20as%20NL-to-SQL%20systems%29%20have%20become%20an%0Aincreasingly%20popular%20solution%20for%20bridging%20the%20gap%20between%20user%20capabilities%0Aand%20SQL-based%20data%20access.%20These%20systems%20translate%20user%20requests%20in%20natural%0Alanguage%20to%20valid%20SQL%20statements%20for%20a%20specific%20database.%20Recent%20Text-to-SQL%0Asystems%20have%20benefited%20from%20the%20rapid%20improvement%20of%20transformer-based%20language%0Amodels.%20However%2C%20while%20Text-to-SQL%20systems%20that%20incorporate%20such%20models%0Acontinuously%20reach%20new%20high%20scores%20on%20--%20often%20synthetic%20--%20benchmark%20datasets%2C%0Aa%20systematic%20exploration%20of%20their%20robustness%20towards%20different%20data%20models%20in%20a%0Areal-world%2C%20realistic%20scenario%20is%20notably%20missing.%20This%20paper%20provides%20the%0Afirst%20in-depth%20evaluation%20of%20the%20data%20model%20robustness%20of%20Text-to-SQL%20systems%0Ain%20practice%20based%20on%20a%20multi-year%20international%20project%20focused%20on%20Text-to-SQL%0Ainterfaces.%20Our%20evaluation%20is%20based%20on%20a%20real-world%20deployment%20of%20FootballDB%2C%20a%0Asystem%20that%20was%20deployed%20over%20a%209%20month%20period%20in%20the%20context%20of%20the%20FIFA%20World%0ACup%202022%2C%20during%20which%20about%206K%20natural%20language%20questions%20were%20asked%20and%0Aexecuted.%20All%20of%20our%20data%20is%20based%20on%20real%20user%20questions%20that%20were%20asked%20live%0Ato%20the%20system.%20We%20manually%20labeled%20and%20translated%20a%20subset%20of%20these%20questions%0Afor%20three%20different%20data%20models.%20For%20each%20data%20model%2C%20we%20explore%20the%0Aperformance%20of%20representative%20Text-to-SQL%20systems%20and%20language%20models.%20We%0Afurther%20quantify%20the%20impact%20of%20training%20data%20size%2C%20pre-%2C%20and%20post-processing%0Asteps%20as%20well%20as%20language%20model%20inference%20time.%20Our%20comprehensive%20evaluation%0Asheds%20light%20on%20the%20design%20choices%20of%20real-world%20Text-to-SQL%20systems%20and%20their%0Aimpact%20on%20moving%20from%20research%20prototypes%20to%20real%20deployments.%20Last%2C%20we%20provide%0Aa%20new%20benchmark%20dataset%20to%20the%20community%2C%20which%20is%20the%20first%20to%20enable%20the%0Aevaluation%20of%20different%20data%20models%20for%20the%20same%20dataset%20and%20is%20substantially%0Amore%20challenging%20than%20most%20previous%20datasets%20in%20terms%20of%20query%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08349v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Data%2520Model%2520Robustness%2520of%2520Text-to-SQL%2520Systems%2520Based%2520on%250A%2520%2520Real%2520User%2520Queries%26entry.906535625%3DJonathan%2520F%25C3%25BCrst%2520and%2520Catherine%2520Kosten%2520and%2520Farhad%2520Nooralahzadeh%2520and%2520Yi%2520Zhang%2520and%2520Kurt%2520Stockinger%26entry.1292438233%3D%2520%2520Text-to-SQL%2520systems%2520%2528also%2520known%2520as%2520NL-to-SQL%2520systems%2529%2520have%2520become%2520an%250Aincreasingly%2520popular%2520solution%2520for%2520bridging%2520the%2520gap%2520between%2520user%2520capabilities%250Aand%2520SQL-based%2520data%2520access.%2520These%2520systems%2520translate%2520user%2520requests%2520in%2520natural%250Alanguage%2520to%2520valid%2520SQL%2520statements%2520for%2520a%2520specific%2520database.%2520Recent%2520Text-to-SQL%250Asystems%2520have%2520benefited%2520from%2520the%2520rapid%2520improvement%2520of%2520transformer-based%2520language%250Amodels.%2520However%252C%2520while%2520Text-to-SQL%2520systems%2520that%2520incorporate%2520such%2520models%250Acontinuously%2520reach%2520new%2520high%2520scores%2520on%2520--%2520often%2520synthetic%2520--%2520benchmark%2520datasets%252C%250Aa%2520systematic%2520exploration%2520of%2520their%2520robustness%2520towards%2520different%2520data%2520models%2520in%2520a%250Areal-world%252C%2520realistic%2520scenario%2520is%2520notably%2520missing.%2520This%2520paper%2520provides%2520the%250Afirst%2520in-depth%2520evaluation%2520of%2520the%2520data%2520model%2520robustness%2520of%2520Text-to-SQL%2520systems%250Ain%2520practice%2520based%2520on%2520a%2520multi-year%2520international%2520project%2520focused%2520on%2520Text-to-SQL%250Ainterfaces.%2520Our%2520evaluation%2520is%2520based%2520on%2520a%2520real-world%2520deployment%2520of%2520FootballDB%252C%2520a%250Asystem%2520that%2520was%2520deployed%2520over%2520a%25209%2520month%2520period%2520in%2520the%2520context%2520of%2520the%2520FIFA%2520World%250ACup%25202022%252C%2520during%2520which%2520about%25206K%2520natural%2520language%2520questions%2520were%2520asked%2520and%250Aexecuted.%2520All%2520of%2520our%2520data%2520is%2520based%2520on%2520real%2520user%2520questions%2520that%2520were%2520asked%2520live%250Ato%2520the%2520system.%2520We%2520manually%2520labeled%2520and%2520translated%2520a%2520subset%2520of%2520these%2520questions%250Afor%2520three%2520different%2520data%2520models.%2520For%2520each%2520data%2520model%252C%2520we%2520explore%2520the%250Aperformance%2520of%2520representative%2520Text-to-SQL%2520systems%2520and%2520language%2520models.%2520We%250Afurther%2520quantify%2520the%2520impact%2520of%2520training%2520data%2520size%252C%2520pre-%252C%2520and%2520post-processing%250Asteps%2520as%2520well%2520as%2520language%2520model%2520inference%2520time.%2520Our%2520comprehensive%2520evaluation%250Asheds%2520light%2520on%2520the%2520design%2520choices%2520of%2520real-world%2520Text-to-SQL%2520systems%2520and%2520their%250Aimpact%2520on%2520moving%2520from%2520research%2520prototypes%2520to%2520real%2520deployments.%2520Last%252C%2520we%2520provide%250Aa%2520new%2520benchmark%2520dataset%2520to%2520the%2520community%252C%2520which%2520is%2520the%2520first%2520to%2520enable%2520the%250Aevaluation%2520of%2520different%2520data%2520models%2520for%2520the%2520same%2520dataset%2520and%2520is%2520substantially%250Amore%2520challenging%2520than%2520most%2520previous%2520datasets%2520in%2520terms%2520of%2520query%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08349v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Data%20Model%20Robustness%20of%20Text-to-SQL%20Systems%20Based%20on%0A%20%20Real%20User%20Queries&entry.906535625=Jonathan%20F%C3%BCrst%20and%20Catherine%20Kosten%20and%20Farhad%20Nooralahzadeh%20and%20Yi%20Zhang%20and%20Kurt%20Stockinger&entry.1292438233=%20%20Text-to-SQL%20systems%20%28also%20known%20as%20NL-to-SQL%20systems%29%20have%20become%20an%0Aincreasingly%20popular%20solution%20for%20bridging%20the%20gap%20between%20user%20capabilities%0Aand%20SQL-based%20data%20access.%20These%20systems%20translate%20user%20requests%20in%20natural%0Alanguage%20to%20valid%20SQL%20statements%20for%20a%20specific%20database.%20Recent%20Text-to-SQL%0Asystems%20have%20benefited%20from%20the%20rapid%20improvement%20of%20transformer-based%20language%0Amodels.%20However%2C%20while%20Text-to-SQL%20systems%20that%20incorporate%20such%20models%0Acontinuously%20reach%20new%20high%20scores%20on%20--%20often%20synthetic%20--%20benchmark%20datasets%2C%0Aa%20systematic%20exploration%20of%20their%20robustness%20towards%20different%20data%20models%20in%20a%0Areal-world%2C%20realistic%20scenario%20is%20notably%20missing.%20This%20paper%20provides%20the%0Afirst%20in-depth%20evaluation%20of%20the%20data%20model%20robustness%20of%20Text-to-SQL%20systems%0Ain%20practice%20based%20on%20a%20multi-year%20international%20project%20focused%20on%20Text-to-SQL%0Ainterfaces.%20Our%20evaluation%20is%20based%20on%20a%20real-world%20deployment%20of%20FootballDB%2C%20a%0Asystem%20that%20was%20deployed%20over%20a%209%20month%20period%20in%20the%20context%20of%20the%20FIFA%20World%0ACup%202022%2C%20during%20which%20about%206K%20natural%20language%20questions%20were%20asked%20and%0Aexecuted.%20All%20of%20our%20data%20is%20based%20on%20real%20user%20questions%20that%20were%20asked%20live%0Ato%20the%20system.%20We%20manually%20labeled%20and%20translated%20a%20subset%20of%20these%20questions%0Afor%20three%20different%20data%20models.%20For%20each%20data%20model%2C%20we%20explore%20the%0Aperformance%20of%20representative%20Text-to-SQL%20systems%20and%20language%20models.%20We%0Afurther%20quantify%20the%20impact%20of%20training%20data%20size%2C%20pre-%2C%20and%20post-processing%0Asteps%20as%20well%20as%20language%20model%20inference%20time.%20Our%20comprehensive%20evaluation%0Asheds%20light%20on%20the%20design%20choices%20of%20real-world%20Text-to-SQL%20systems%20and%20their%0Aimpact%20on%20moving%20from%20research%20prototypes%20to%20real%20deployments.%20Last%2C%20we%20provide%0Aa%20new%20benchmark%20dataset%20to%20the%20community%2C%20which%20is%20the%20first%20to%20enable%20the%0Aevaluation%20of%20different%20data%20models%20for%20the%20same%20dataset%20and%20is%20substantially%0Amore%20challenging%20than%20most%20previous%20datasets%20in%20terms%20of%20query%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08349v3&entry.124074799=Read"},
{"title": "CAdam: Confidence-Based Optimization for Online Learning", "author": "Shaowen Wang and Anan Liu and Jian Xiao and Huan Liu and Yuekui Yang and Cong Xu and Qianqian Pu and Suncong Zheng and Wei Zhang and Jian Li", "abstract": "  Modern recommendation systems frequently employ online learning to\ndynamically update their models with freshly collected data. The most commonly\nused optimizer for updating neural networks in these contexts is the Adam\noptimizer, which integrates momentum ($m_t$) and adaptive learning rate\n($v_t$). However, the volatile nature of online learning data, characterized by\nits frequent distribution shifts and presence of noises, poses significant\nchallenges to Adam's standard optimization process: (1) Adam may use outdated\nmomentum and the average of squared gradients, resulting in slower adaptation\nto distribution changes, and (2) Adam's performance is adversely affected by\ndata noise. To mitigate these issues, we introduce CAdam, a confidence-based\noptimization strategy that assesses the consistence between the momentum and\nthe gradient for each parameter dimension before deciding on updates. If\nmomentum and gradient are in sync, CAdam proceeds with parameter updates\naccording to Adam's original formulation; if not, it temporarily withholds\nupdates and monitors potential shifts in data distribution in subsequent\niterations. This method allows CAdam to distinguish between the true\ndistributional shifts and mere noise, and adapt more quickly to new data\ndistributions. Our experiments with both synthetic and real-world datasets\ndemonstrate that CAdam surpasses other well-known optimizers, including the\noriginal Adam, in efficiency and noise robustness. Furthermore, in large-scale\nA/B testing within a live recommendation system, CAdam significantly enhances\nmodel performance compared to Adam, leading to substantial increases in the\nsystem's gross merchandise volume (GMV).\n", "link": "http://arxiv.org/abs/2411.19647v1", "date": "2024-11-29", "relevancy": 1.9992, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5358}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5044}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAdam%3A%20Confidence-Based%20Optimization%20for%20Online%20Learning&body=Title%3A%20CAdam%3A%20Confidence-Based%20Optimization%20for%20Online%20Learning%0AAuthor%3A%20Shaowen%20Wang%20and%20Anan%20Liu%20and%20Jian%20Xiao%20and%20Huan%20Liu%20and%20Yuekui%20Yang%20and%20Cong%20Xu%20and%20Qianqian%20Pu%20and%20Suncong%20Zheng%20and%20Wei%20Zhang%20and%20Jian%20Li%0AAbstract%3A%20%20%20Modern%20recommendation%20systems%20frequently%20employ%20online%20learning%20to%0Adynamically%20update%20their%20models%20with%20freshly%20collected%20data.%20The%20most%20commonly%0Aused%20optimizer%20for%20updating%20neural%20networks%20in%20these%20contexts%20is%20the%20Adam%0Aoptimizer%2C%20which%20integrates%20momentum%20%28%24m_t%24%29%20and%20adaptive%20learning%20rate%0A%28%24v_t%24%29.%20However%2C%20the%20volatile%20nature%20of%20online%20learning%20data%2C%20characterized%20by%0Aits%20frequent%20distribution%20shifts%20and%20presence%20of%20noises%2C%20poses%20significant%0Achallenges%20to%20Adam%27s%20standard%20optimization%20process%3A%20%281%29%20Adam%20may%20use%20outdated%0Amomentum%20and%20the%20average%20of%20squared%20gradients%2C%20resulting%20in%20slower%20adaptation%0Ato%20distribution%20changes%2C%20and%20%282%29%20Adam%27s%20performance%20is%20adversely%20affected%20by%0Adata%20noise.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20CAdam%2C%20a%20confidence-based%0Aoptimization%20strategy%20that%20assesses%20the%20consistence%20between%20the%20momentum%20and%0Athe%20gradient%20for%20each%20parameter%20dimension%20before%20deciding%20on%20updates.%20If%0Amomentum%20and%20gradient%20are%20in%20sync%2C%20CAdam%20proceeds%20with%20parameter%20updates%0Aaccording%20to%20Adam%27s%20original%20formulation%3B%20if%20not%2C%20it%20temporarily%20withholds%0Aupdates%20and%20monitors%20potential%20shifts%20in%20data%20distribution%20in%20subsequent%0Aiterations.%20This%20method%20allows%20CAdam%20to%20distinguish%20between%20the%20true%0Adistributional%20shifts%20and%20mere%20noise%2C%20and%20adapt%20more%20quickly%20to%20new%20data%0Adistributions.%20Our%20experiments%20with%20both%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20that%20CAdam%20surpasses%20other%20well-known%20optimizers%2C%20including%20the%0Aoriginal%20Adam%2C%20in%20efficiency%20and%20noise%20robustness.%20Furthermore%2C%20in%20large-scale%0AA/B%20testing%20within%20a%20live%20recommendation%20system%2C%20CAdam%20significantly%20enhances%0Amodel%20performance%20compared%20to%20Adam%2C%20leading%20to%20substantial%20increases%20in%20the%0Asystem%27s%20gross%20merchandise%20volume%20%28GMV%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAdam%253A%2520Confidence-Based%2520Optimization%2520for%2520Online%2520Learning%26entry.906535625%3DShaowen%2520Wang%2520and%2520Anan%2520Liu%2520and%2520Jian%2520Xiao%2520and%2520Huan%2520Liu%2520and%2520Yuekui%2520Yang%2520and%2520Cong%2520Xu%2520and%2520Qianqian%2520Pu%2520and%2520Suncong%2520Zheng%2520and%2520Wei%2520Zhang%2520and%2520Jian%2520Li%26entry.1292438233%3D%2520%2520Modern%2520recommendation%2520systems%2520frequently%2520employ%2520online%2520learning%2520to%250Adynamically%2520update%2520their%2520models%2520with%2520freshly%2520collected%2520data.%2520The%2520most%2520commonly%250Aused%2520optimizer%2520for%2520updating%2520neural%2520networks%2520in%2520these%2520contexts%2520is%2520the%2520Adam%250Aoptimizer%252C%2520which%2520integrates%2520momentum%2520%2528%2524m_t%2524%2529%2520and%2520adaptive%2520learning%2520rate%250A%2528%2524v_t%2524%2529.%2520However%252C%2520the%2520volatile%2520nature%2520of%2520online%2520learning%2520data%252C%2520characterized%2520by%250Aits%2520frequent%2520distribution%2520shifts%2520and%2520presence%2520of%2520noises%252C%2520poses%2520significant%250Achallenges%2520to%2520Adam%2527s%2520standard%2520optimization%2520process%253A%2520%25281%2529%2520Adam%2520may%2520use%2520outdated%250Amomentum%2520and%2520the%2520average%2520of%2520squared%2520gradients%252C%2520resulting%2520in%2520slower%2520adaptation%250Ato%2520distribution%2520changes%252C%2520and%2520%25282%2529%2520Adam%2527s%2520performance%2520is%2520adversely%2520affected%2520by%250Adata%2520noise.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520introduce%2520CAdam%252C%2520a%2520confidence-based%250Aoptimization%2520strategy%2520that%2520assesses%2520the%2520consistence%2520between%2520the%2520momentum%2520and%250Athe%2520gradient%2520for%2520each%2520parameter%2520dimension%2520before%2520deciding%2520on%2520updates.%2520If%250Amomentum%2520and%2520gradient%2520are%2520in%2520sync%252C%2520CAdam%2520proceeds%2520with%2520parameter%2520updates%250Aaccording%2520to%2520Adam%2527s%2520original%2520formulation%253B%2520if%2520not%252C%2520it%2520temporarily%2520withholds%250Aupdates%2520and%2520monitors%2520potential%2520shifts%2520in%2520data%2520distribution%2520in%2520subsequent%250Aiterations.%2520This%2520method%2520allows%2520CAdam%2520to%2520distinguish%2520between%2520the%2520true%250Adistributional%2520shifts%2520and%2520mere%2520noise%252C%2520and%2520adapt%2520more%2520quickly%2520to%2520new%2520data%250Adistributions.%2520Our%2520experiments%2520with%2520both%2520synthetic%2520and%2520real-world%2520datasets%250Ademonstrate%2520that%2520CAdam%2520surpasses%2520other%2520well-known%2520optimizers%252C%2520including%2520the%250Aoriginal%2520Adam%252C%2520in%2520efficiency%2520and%2520noise%2520robustness.%2520Furthermore%252C%2520in%2520large-scale%250AA/B%2520testing%2520within%2520a%2520live%2520recommendation%2520system%252C%2520CAdam%2520significantly%2520enhances%250Amodel%2520performance%2520compared%2520to%2520Adam%252C%2520leading%2520to%2520substantial%2520increases%2520in%2520the%250Asystem%2527s%2520gross%2520merchandise%2520volume%2520%2528GMV%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAdam%3A%20Confidence-Based%20Optimization%20for%20Online%20Learning&entry.906535625=Shaowen%20Wang%20and%20Anan%20Liu%20and%20Jian%20Xiao%20and%20Huan%20Liu%20and%20Yuekui%20Yang%20and%20Cong%20Xu%20and%20Qianqian%20Pu%20and%20Suncong%20Zheng%20and%20Wei%20Zhang%20and%20Jian%20Li&entry.1292438233=%20%20Modern%20recommendation%20systems%20frequently%20employ%20online%20learning%20to%0Adynamically%20update%20their%20models%20with%20freshly%20collected%20data.%20The%20most%20commonly%0Aused%20optimizer%20for%20updating%20neural%20networks%20in%20these%20contexts%20is%20the%20Adam%0Aoptimizer%2C%20which%20integrates%20momentum%20%28%24m_t%24%29%20and%20adaptive%20learning%20rate%0A%28%24v_t%24%29.%20However%2C%20the%20volatile%20nature%20of%20online%20learning%20data%2C%20characterized%20by%0Aits%20frequent%20distribution%20shifts%20and%20presence%20of%20noises%2C%20poses%20significant%0Achallenges%20to%20Adam%27s%20standard%20optimization%20process%3A%20%281%29%20Adam%20may%20use%20outdated%0Amomentum%20and%20the%20average%20of%20squared%20gradients%2C%20resulting%20in%20slower%20adaptation%0Ato%20distribution%20changes%2C%20and%20%282%29%20Adam%27s%20performance%20is%20adversely%20affected%20by%0Adata%20noise.%20To%20mitigate%20these%20issues%2C%20we%20introduce%20CAdam%2C%20a%20confidence-based%0Aoptimization%20strategy%20that%20assesses%20the%20consistence%20between%20the%20momentum%20and%0Athe%20gradient%20for%20each%20parameter%20dimension%20before%20deciding%20on%20updates.%20If%0Amomentum%20and%20gradient%20are%20in%20sync%2C%20CAdam%20proceeds%20with%20parameter%20updates%0Aaccording%20to%20Adam%27s%20original%20formulation%3B%20if%20not%2C%20it%20temporarily%20withholds%0Aupdates%20and%20monitors%20potential%20shifts%20in%20data%20distribution%20in%20subsequent%0Aiterations.%20This%20method%20allows%20CAdam%20to%20distinguish%20between%20the%20true%0Adistributional%20shifts%20and%20mere%20noise%2C%20and%20adapt%20more%20quickly%20to%20new%20data%0Adistributions.%20Our%20experiments%20with%20both%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20that%20CAdam%20surpasses%20other%20well-known%20optimizers%2C%20including%20the%0Aoriginal%20Adam%2C%20in%20efficiency%20and%20noise%20robustness.%20Furthermore%2C%20in%20large-scale%0AA/B%20testing%20within%20a%20live%20recommendation%20system%2C%20CAdam%20significantly%20enhances%0Amodel%20performance%20compared%20to%20Adam%2C%20leading%20to%20substantial%20increases%20in%20the%0Asystem%27s%20gross%20merchandise%20volume%20%28GMV%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19647v1&entry.124074799=Read"},
{"title": "The brain versus AI: World-model-based versatile circuit computation\n  underlying diverse functions in the neocortex and cerebellum", "author": "Shogo Ohmae and Keiko Ohmae", "abstract": "  AI's significant recent advances using general-purpose circuit computations\noffer a potential window into how the neocortex and cerebellum of the brain are\nable to achieve a diverse range of functions across sensory, cognitive, and\nmotor domains, despite their uniform circuit structures. However, comparing the\nbrain and AI is challenging unless clear similarities exist, and past reviews\nhave been limited to comparison of brain-inspired vision AI and the visual\nneocortex. Here, to enable comparisons across diverse functional domains, we\nsubdivide circuit computation into three elements -- circuit structure,\ninput/outputs, and the learning algorithm -- and evaluate the similarities for\neach element. With this novel approach, we identify wide-ranging similarities\nand convergent evolution in the brain and AI, providing new insights into key\nconcepts in neuroscience. Furthermore, inspired by processing mechanisms of AI,\nwe propose a new theory that integrates established neuroscience theories,\nparticularly the theories of internal models and the mirror neuron system. Both\nthe neocortex and cerebellum predict future world events from past information\nand learn from prediction errors, thereby acquiring models of the world. These\nmodels enable three core processes: (1) Prediction -- generating future\ninformation, (2) Understanding -- interpreting the external world via\ncompressed and abstracted sensory information, and (3) Generation --\nrepurposing the future-information generation mechanism to produce other types\nof outputs. The universal application of these processes underlies the ability\nof the neocortex and cerebellum to accomplish diverse functions with uniform\ncircuits. Our systematic approach, insights, and theory promise groundbreaking\nadvances in understanding the brain.\n", "link": "http://arxiv.org/abs/2411.16075v2", "date": "2024-11-29", "relevancy": 1.9901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5008}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20brain%20versus%20AI%3A%20World-model-based%20versatile%20circuit%20computation%0A%20%20underlying%20diverse%20functions%20in%20the%20neocortex%20and%20cerebellum&body=Title%3A%20The%20brain%20versus%20AI%3A%20World-model-based%20versatile%20circuit%20computation%0A%20%20underlying%20diverse%20functions%20in%20the%20neocortex%20and%20cerebellum%0AAuthor%3A%20Shogo%20Ohmae%20and%20Keiko%20Ohmae%0AAbstract%3A%20%20%20AI%27s%20significant%20recent%20advances%20using%20general-purpose%20circuit%20computations%0Aoffer%20a%20potential%20window%20into%20how%20the%20neocortex%20and%20cerebellum%20of%20the%20brain%20are%0Aable%20to%20achieve%20a%20diverse%20range%20of%20functions%20across%20sensory%2C%20cognitive%2C%20and%0Amotor%20domains%2C%20despite%20their%20uniform%20circuit%20structures.%20However%2C%20comparing%20the%0Abrain%20and%20AI%20is%20challenging%20unless%20clear%20similarities%20exist%2C%20and%20past%20reviews%0Ahave%20been%20limited%20to%20comparison%20of%20brain-inspired%20vision%20AI%20and%20the%20visual%0Aneocortex.%20Here%2C%20to%20enable%20comparisons%20across%20diverse%20functional%20domains%2C%20we%0Asubdivide%20circuit%20computation%20into%20three%20elements%20--%20circuit%20structure%2C%0Ainput/outputs%2C%20and%20the%20learning%20algorithm%20--%20and%20evaluate%20the%20similarities%20for%0Aeach%20element.%20With%20this%20novel%20approach%2C%20we%20identify%20wide-ranging%20similarities%0Aand%20convergent%20evolution%20in%20the%20brain%20and%20AI%2C%20providing%20new%20insights%20into%20key%0Aconcepts%20in%20neuroscience.%20Furthermore%2C%20inspired%20by%20processing%20mechanisms%20of%20AI%2C%0Awe%20propose%20a%20new%20theory%20that%20integrates%20established%20neuroscience%20theories%2C%0Aparticularly%20the%20theories%20of%20internal%20models%20and%20the%20mirror%20neuron%20system.%20Both%0Athe%20neocortex%20and%20cerebellum%20predict%20future%20world%20events%20from%20past%20information%0Aand%20learn%20from%20prediction%20errors%2C%20thereby%20acquiring%20models%20of%20the%20world.%20These%0Amodels%20enable%20three%20core%20processes%3A%20%281%29%20Prediction%20--%20generating%20future%0Ainformation%2C%20%282%29%20Understanding%20--%20interpreting%20the%20external%20world%20via%0Acompressed%20and%20abstracted%20sensory%20information%2C%20and%20%283%29%20Generation%20--%0Arepurposing%20the%20future-information%20generation%20mechanism%20to%20produce%20other%20types%0Aof%20outputs.%20The%20universal%20application%20of%20these%20processes%20underlies%20the%20ability%0Aof%20the%20neocortex%20and%20cerebellum%20to%20accomplish%20diverse%20functions%20with%20uniform%0Acircuits.%20Our%20systematic%20approach%2C%20insights%2C%20and%20theory%20promise%20groundbreaking%0Aadvances%20in%20understanding%20the%20brain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520brain%2520versus%2520AI%253A%2520World-model-based%2520versatile%2520circuit%2520computation%250A%2520%2520underlying%2520diverse%2520functions%2520in%2520the%2520neocortex%2520and%2520cerebellum%26entry.906535625%3DShogo%2520Ohmae%2520and%2520Keiko%2520Ohmae%26entry.1292438233%3D%2520%2520AI%2527s%2520significant%2520recent%2520advances%2520using%2520general-purpose%2520circuit%2520computations%250Aoffer%2520a%2520potential%2520window%2520into%2520how%2520the%2520neocortex%2520and%2520cerebellum%2520of%2520the%2520brain%2520are%250Aable%2520to%2520achieve%2520a%2520diverse%2520range%2520of%2520functions%2520across%2520sensory%252C%2520cognitive%252C%2520and%250Amotor%2520domains%252C%2520despite%2520their%2520uniform%2520circuit%2520structures.%2520However%252C%2520comparing%2520the%250Abrain%2520and%2520AI%2520is%2520challenging%2520unless%2520clear%2520similarities%2520exist%252C%2520and%2520past%2520reviews%250Ahave%2520been%2520limited%2520to%2520comparison%2520of%2520brain-inspired%2520vision%2520AI%2520and%2520the%2520visual%250Aneocortex.%2520Here%252C%2520to%2520enable%2520comparisons%2520across%2520diverse%2520functional%2520domains%252C%2520we%250Asubdivide%2520circuit%2520computation%2520into%2520three%2520elements%2520--%2520circuit%2520structure%252C%250Ainput/outputs%252C%2520and%2520the%2520learning%2520algorithm%2520--%2520and%2520evaluate%2520the%2520similarities%2520for%250Aeach%2520element.%2520With%2520this%2520novel%2520approach%252C%2520we%2520identify%2520wide-ranging%2520similarities%250Aand%2520convergent%2520evolution%2520in%2520the%2520brain%2520and%2520AI%252C%2520providing%2520new%2520insights%2520into%2520key%250Aconcepts%2520in%2520neuroscience.%2520Furthermore%252C%2520inspired%2520by%2520processing%2520mechanisms%2520of%2520AI%252C%250Awe%2520propose%2520a%2520new%2520theory%2520that%2520integrates%2520established%2520neuroscience%2520theories%252C%250Aparticularly%2520the%2520theories%2520of%2520internal%2520models%2520and%2520the%2520mirror%2520neuron%2520system.%2520Both%250Athe%2520neocortex%2520and%2520cerebellum%2520predict%2520future%2520world%2520events%2520from%2520past%2520information%250Aand%2520learn%2520from%2520prediction%2520errors%252C%2520thereby%2520acquiring%2520models%2520of%2520the%2520world.%2520These%250Amodels%2520enable%2520three%2520core%2520processes%253A%2520%25281%2529%2520Prediction%2520--%2520generating%2520future%250Ainformation%252C%2520%25282%2529%2520Understanding%2520--%2520interpreting%2520the%2520external%2520world%2520via%250Acompressed%2520and%2520abstracted%2520sensory%2520information%252C%2520and%2520%25283%2529%2520Generation%2520--%250Arepurposing%2520the%2520future-information%2520generation%2520mechanism%2520to%2520produce%2520other%2520types%250Aof%2520outputs.%2520The%2520universal%2520application%2520of%2520these%2520processes%2520underlies%2520the%2520ability%250Aof%2520the%2520neocortex%2520and%2520cerebellum%2520to%2520accomplish%2520diverse%2520functions%2520with%2520uniform%250Acircuits.%2520Our%2520systematic%2520approach%252C%2520insights%252C%2520and%2520theory%2520promise%2520groundbreaking%250Aadvances%2520in%2520understanding%2520the%2520brain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20brain%20versus%20AI%3A%20World-model-based%20versatile%20circuit%20computation%0A%20%20underlying%20diverse%20functions%20in%20the%20neocortex%20and%20cerebellum&entry.906535625=Shogo%20Ohmae%20and%20Keiko%20Ohmae&entry.1292438233=%20%20AI%27s%20significant%20recent%20advances%20using%20general-purpose%20circuit%20computations%0Aoffer%20a%20potential%20window%20into%20how%20the%20neocortex%20and%20cerebellum%20of%20the%20brain%20are%0Aable%20to%20achieve%20a%20diverse%20range%20of%20functions%20across%20sensory%2C%20cognitive%2C%20and%0Amotor%20domains%2C%20despite%20their%20uniform%20circuit%20structures.%20However%2C%20comparing%20the%0Abrain%20and%20AI%20is%20challenging%20unless%20clear%20similarities%20exist%2C%20and%20past%20reviews%0Ahave%20been%20limited%20to%20comparison%20of%20brain-inspired%20vision%20AI%20and%20the%20visual%0Aneocortex.%20Here%2C%20to%20enable%20comparisons%20across%20diverse%20functional%20domains%2C%20we%0Asubdivide%20circuit%20computation%20into%20three%20elements%20--%20circuit%20structure%2C%0Ainput/outputs%2C%20and%20the%20learning%20algorithm%20--%20and%20evaluate%20the%20similarities%20for%0Aeach%20element.%20With%20this%20novel%20approach%2C%20we%20identify%20wide-ranging%20similarities%0Aand%20convergent%20evolution%20in%20the%20brain%20and%20AI%2C%20providing%20new%20insights%20into%20key%0Aconcepts%20in%20neuroscience.%20Furthermore%2C%20inspired%20by%20processing%20mechanisms%20of%20AI%2C%0Awe%20propose%20a%20new%20theory%20that%20integrates%20established%20neuroscience%20theories%2C%0Aparticularly%20the%20theories%20of%20internal%20models%20and%20the%20mirror%20neuron%20system.%20Both%0Athe%20neocortex%20and%20cerebellum%20predict%20future%20world%20events%20from%20past%20information%0Aand%20learn%20from%20prediction%20errors%2C%20thereby%20acquiring%20models%20of%20the%20world.%20These%0Amodels%20enable%20three%20core%20processes%3A%20%281%29%20Prediction%20--%20generating%20future%0Ainformation%2C%20%282%29%20Understanding%20--%20interpreting%20the%20external%20world%20via%0Acompressed%20and%20abstracted%20sensory%20information%2C%20and%20%283%29%20Generation%20--%0Arepurposing%20the%20future-information%20generation%20mechanism%20to%20produce%20other%20types%0Aof%20outputs.%20The%20universal%20application%20of%20these%20processes%20underlies%20the%20ability%0Aof%20the%20neocortex%20and%20cerebellum%20to%20accomplish%20diverse%20functions%20with%20uniform%0Acircuits.%20Our%20systematic%20approach%2C%20insights%2C%20and%20theory%20promise%20groundbreaking%0Aadvances%20in%20understanding%20the%20brain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16075v2&entry.124074799=Read"},
{"title": "Gated-Attention Feature-Fusion Based Framework for Poverty Prediction", "author": "Muhammad Umer Ramzan and Wahab Khaddim and Muhammad Ehsan Rana and Usman Ali and Manohar Ali and Fiaz ul Hassan and Fatima Mehmood", "abstract": "  This research paper addresses the significant challenge of accurately\nestimating poverty levels using deep learning, particularly in developing\nregions where traditional methods like household surveys are often costly,\ninfrequent, and quickly become outdated. To address these issues, we propose a\nstate-of-the-art Convolutional Neural Network (CNN) architecture, extending the\nResNet50 model by incorporating a Gated-Attention Feature-Fusion Module (GAFM).\nOur architecture is designed to improve the model's ability to capture and\ncombine both global and local features from satellite images, leading to more\naccurate poverty estimates. The model achieves a 75% R2 score, significantly\noutperforming existing leading methods in poverty mapping. This improvement is\ndue to the model's capacity to focus on and refine the most relevant features,\nfiltering out unnecessary data, which makes it a powerful tool for remote\nsensing and poverty estimation.\n", "link": "http://arxiv.org/abs/2411.19690v1", "date": "2024-11-29", "relevancy": 1.9882, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5103}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4938}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gated-Attention%20Feature-Fusion%20Based%20Framework%20for%20Poverty%20Prediction&body=Title%3A%20Gated-Attention%20Feature-Fusion%20Based%20Framework%20for%20Poverty%20Prediction%0AAuthor%3A%20Muhammad%20Umer%20Ramzan%20and%20Wahab%20Khaddim%20and%20Muhammad%20Ehsan%20Rana%20and%20Usman%20Ali%20and%20Manohar%20Ali%20and%20Fiaz%20ul%20Hassan%20and%20Fatima%20Mehmood%0AAbstract%3A%20%20%20This%20research%20paper%20addresses%20the%20significant%20challenge%20of%20accurately%0Aestimating%20poverty%20levels%20using%20deep%20learning%2C%20particularly%20in%20developing%0Aregions%20where%20traditional%20methods%20like%20household%20surveys%20are%20often%20costly%2C%0Ainfrequent%2C%20and%20quickly%20become%20outdated.%20To%20address%20these%20issues%2C%20we%20propose%20a%0Astate-of-the-art%20Convolutional%20Neural%20Network%20%28CNN%29%20architecture%2C%20extending%20the%0AResNet50%20model%20by%20incorporating%20a%20Gated-Attention%20Feature-Fusion%20Module%20%28GAFM%29.%0AOur%20architecture%20is%20designed%20to%20improve%20the%20model%27s%20ability%20to%20capture%20and%0Acombine%20both%20global%20and%20local%20features%20from%20satellite%20images%2C%20leading%20to%20more%0Aaccurate%20poverty%20estimates.%20The%20model%20achieves%20a%2075%25%20R2%20score%2C%20significantly%0Aoutperforming%20existing%20leading%20methods%20in%20poverty%20mapping.%20This%20improvement%20is%0Adue%20to%20the%20model%27s%20capacity%20to%20focus%20on%20and%20refine%20the%20most%20relevant%20features%2C%0Afiltering%20out%20unnecessary%20data%2C%20which%20makes%20it%20a%20powerful%20tool%20for%20remote%0Asensing%20and%20poverty%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGated-Attention%2520Feature-Fusion%2520Based%2520Framework%2520for%2520Poverty%2520Prediction%26entry.906535625%3DMuhammad%2520Umer%2520Ramzan%2520and%2520Wahab%2520Khaddim%2520and%2520Muhammad%2520Ehsan%2520Rana%2520and%2520Usman%2520Ali%2520and%2520Manohar%2520Ali%2520and%2520Fiaz%2520ul%2520Hassan%2520and%2520Fatima%2520Mehmood%26entry.1292438233%3D%2520%2520This%2520research%2520paper%2520addresses%2520the%2520significant%2520challenge%2520of%2520accurately%250Aestimating%2520poverty%2520levels%2520using%2520deep%2520learning%252C%2520particularly%2520in%2520developing%250Aregions%2520where%2520traditional%2520methods%2520like%2520household%2520surveys%2520are%2520often%2520costly%252C%250Ainfrequent%252C%2520and%2520quickly%2520become%2520outdated.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%250Astate-of-the-art%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520architecture%252C%2520extending%2520the%250AResNet50%2520model%2520by%2520incorporating%2520a%2520Gated-Attention%2520Feature-Fusion%2520Module%2520%2528GAFM%2529.%250AOur%2520architecture%2520is%2520designed%2520to%2520improve%2520the%2520model%2527s%2520ability%2520to%2520capture%2520and%250Acombine%2520both%2520global%2520and%2520local%2520features%2520from%2520satellite%2520images%252C%2520leading%2520to%2520more%250Aaccurate%2520poverty%2520estimates.%2520The%2520model%2520achieves%2520a%252075%2525%2520R2%2520score%252C%2520significantly%250Aoutperforming%2520existing%2520leading%2520methods%2520in%2520poverty%2520mapping.%2520This%2520improvement%2520is%250Adue%2520to%2520the%2520model%2527s%2520capacity%2520to%2520focus%2520on%2520and%2520refine%2520the%2520most%2520relevant%2520features%252C%250Afiltering%2520out%2520unnecessary%2520data%252C%2520which%2520makes%2520it%2520a%2520powerful%2520tool%2520for%2520remote%250Asensing%2520and%2520poverty%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gated-Attention%20Feature-Fusion%20Based%20Framework%20for%20Poverty%20Prediction&entry.906535625=Muhammad%20Umer%20Ramzan%20and%20Wahab%20Khaddim%20and%20Muhammad%20Ehsan%20Rana%20and%20Usman%20Ali%20and%20Manohar%20Ali%20and%20Fiaz%20ul%20Hassan%20and%20Fatima%20Mehmood&entry.1292438233=%20%20This%20research%20paper%20addresses%20the%20significant%20challenge%20of%20accurately%0Aestimating%20poverty%20levels%20using%20deep%20learning%2C%20particularly%20in%20developing%0Aregions%20where%20traditional%20methods%20like%20household%20surveys%20are%20often%20costly%2C%0Ainfrequent%2C%20and%20quickly%20become%20outdated.%20To%20address%20these%20issues%2C%20we%20propose%20a%0Astate-of-the-art%20Convolutional%20Neural%20Network%20%28CNN%29%20architecture%2C%20extending%20the%0AResNet50%20model%20by%20incorporating%20a%20Gated-Attention%20Feature-Fusion%20Module%20%28GAFM%29.%0AOur%20architecture%20is%20designed%20to%20improve%20the%20model%27s%20ability%20to%20capture%20and%0Acombine%20both%20global%20and%20local%20features%20from%20satellite%20images%2C%20leading%20to%20more%0Aaccurate%20poverty%20estimates.%20The%20model%20achieves%20a%2075%25%20R2%20score%2C%20significantly%0Aoutperforming%20existing%20leading%20methods%20in%20poverty%20mapping.%20This%20improvement%20is%0Adue%20to%20the%20model%27s%20capacity%20to%20focus%20on%20and%20refine%20the%20most%20relevant%20features%2C%0Afiltering%20out%20unnecessary%20data%2C%20which%20makes%20it%20a%20powerful%20tool%20for%20remote%0Asensing%20and%20poverty%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19690v1&entry.124074799=Read"},
{"title": "Hadamard Representations: Augmenting Hyperbolic Tangents in RL", "author": "Jacob E. Kooi and Mark Hoogendoorn and Vincent Fran\u00e7ois-Lavet", "abstract": "  Activation functions are one of the key components of a deep neural network.\nThe most commonly used activation functions can be classed into the category of\ncontinuously differentiable (e.g. tanh) and piece-wise linear functions (e.g.\nReLU), both having their own strengths and drawbacks with respect to downstream\nperformance and representation capacity through learning (e.g. measured by the\nnumber of dead neurons and the effective rank). In reinforcement learning, the\nperformance of continuously differentiable activations often falls short as\ncompared to piece-wise linear functions. We provide insights into the vanishing\ngradients associated with the former, and show that the dying neuron problem is\nnot exclusive to ReLU's. To alleviate vanishing gradients and the resulting\ndying neuron problem occurring with continuously differentiable activations, we\npropose a Hadamard representation. Using deep Q-networks, proximal policy\noptimization and parallelized Q-networks in the Atari domain, we show faster\nlearning, a reduction in dead neurons and increased effective rank.\n", "link": "http://arxiv.org/abs/2406.09079v3", "date": "2024-11-29", "relevancy": 1.9862, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5161}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4849}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hadamard%20Representations%3A%20Augmenting%20Hyperbolic%20Tangents%20in%20RL&body=Title%3A%20Hadamard%20Representations%3A%20Augmenting%20Hyperbolic%20Tangents%20in%20RL%0AAuthor%3A%20Jacob%20E.%20Kooi%20and%20Mark%20Hoogendoorn%20and%20Vincent%20Fran%C3%A7ois-Lavet%0AAbstract%3A%20%20%20Activation%20functions%20are%20one%20of%20the%20key%20components%20of%20a%20deep%20neural%20network.%0AThe%20most%20commonly%20used%20activation%20functions%20can%20be%20classed%20into%20the%20category%20of%0Acontinuously%20differentiable%20%28e.g.%20tanh%29%20and%20piece-wise%20linear%20functions%20%28e.g.%0AReLU%29%2C%20both%20having%20their%20own%20strengths%20and%20drawbacks%20with%20respect%20to%20downstream%0Aperformance%20and%20representation%20capacity%20through%20learning%20%28e.g.%20measured%20by%20the%0Anumber%20of%20dead%20neurons%20and%20the%20effective%20rank%29.%20In%20reinforcement%20learning%2C%20the%0Aperformance%20of%20continuously%20differentiable%20activations%20often%20falls%20short%20as%0Acompared%20to%20piece-wise%20linear%20functions.%20We%20provide%20insights%20into%20the%20vanishing%0Agradients%20associated%20with%20the%20former%2C%20and%20show%20that%20the%20dying%20neuron%20problem%20is%0Anot%20exclusive%20to%20ReLU%27s.%20To%20alleviate%20vanishing%20gradients%20and%20the%20resulting%0Adying%20neuron%20problem%20occurring%20with%20continuously%20differentiable%20activations%2C%20we%0Apropose%20a%20Hadamard%20representation.%20Using%20deep%20Q-networks%2C%20proximal%20policy%0Aoptimization%20and%20parallelized%20Q-networks%20in%20the%20Atari%20domain%2C%20we%20show%20faster%0Alearning%2C%20a%20reduction%20in%20dead%20neurons%20and%20increased%20effective%20rank.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09079v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHadamard%2520Representations%253A%2520Augmenting%2520Hyperbolic%2520Tangents%2520in%2520RL%26entry.906535625%3DJacob%2520E.%2520Kooi%2520and%2520Mark%2520Hoogendoorn%2520and%2520Vincent%2520Fran%25C3%25A7ois-Lavet%26entry.1292438233%3D%2520%2520Activation%2520functions%2520are%2520one%2520of%2520the%2520key%2520components%2520of%2520a%2520deep%2520neural%2520network.%250AThe%2520most%2520commonly%2520used%2520activation%2520functions%2520can%2520be%2520classed%2520into%2520the%2520category%2520of%250Acontinuously%2520differentiable%2520%2528e.g.%2520tanh%2529%2520and%2520piece-wise%2520linear%2520functions%2520%2528e.g.%250AReLU%2529%252C%2520both%2520having%2520their%2520own%2520strengths%2520and%2520drawbacks%2520with%2520respect%2520to%2520downstream%250Aperformance%2520and%2520representation%2520capacity%2520through%2520learning%2520%2528e.g.%2520measured%2520by%2520the%250Anumber%2520of%2520dead%2520neurons%2520and%2520the%2520effective%2520rank%2529.%2520In%2520reinforcement%2520learning%252C%2520the%250Aperformance%2520of%2520continuously%2520differentiable%2520activations%2520often%2520falls%2520short%2520as%250Acompared%2520to%2520piece-wise%2520linear%2520functions.%2520We%2520provide%2520insights%2520into%2520the%2520vanishing%250Agradients%2520associated%2520with%2520the%2520former%252C%2520and%2520show%2520that%2520the%2520dying%2520neuron%2520problem%2520is%250Anot%2520exclusive%2520to%2520ReLU%2527s.%2520To%2520alleviate%2520vanishing%2520gradients%2520and%2520the%2520resulting%250Adying%2520neuron%2520problem%2520occurring%2520with%2520continuously%2520differentiable%2520activations%252C%2520we%250Apropose%2520a%2520Hadamard%2520representation.%2520Using%2520deep%2520Q-networks%252C%2520proximal%2520policy%250Aoptimization%2520and%2520parallelized%2520Q-networks%2520in%2520the%2520Atari%2520domain%252C%2520we%2520show%2520faster%250Alearning%252C%2520a%2520reduction%2520in%2520dead%2520neurons%2520and%2520increased%2520effective%2520rank.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09079v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hadamard%20Representations%3A%20Augmenting%20Hyperbolic%20Tangents%20in%20RL&entry.906535625=Jacob%20E.%20Kooi%20and%20Mark%20Hoogendoorn%20and%20Vincent%20Fran%C3%A7ois-Lavet&entry.1292438233=%20%20Activation%20functions%20are%20one%20of%20the%20key%20components%20of%20a%20deep%20neural%20network.%0AThe%20most%20commonly%20used%20activation%20functions%20can%20be%20classed%20into%20the%20category%20of%0Acontinuously%20differentiable%20%28e.g.%20tanh%29%20and%20piece-wise%20linear%20functions%20%28e.g.%0AReLU%29%2C%20both%20having%20their%20own%20strengths%20and%20drawbacks%20with%20respect%20to%20downstream%0Aperformance%20and%20representation%20capacity%20through%20learning%20%28e.g.%20measured%20by%20the%0Anumber%20of%20dead%20neurons%20and%20the%20effective%20rank%29.%20In%20reinforcement%20learning%2C%20the%0Aperformance%20of%20continuously%20differentiable%20activations%20often%20falls%20short%20as%0Acompared%20to%20piece-wise%20linear%20functions.%20We%20provide%20insights%20into%20the%20vanishing%0Agradients%20associated%20with%20the%20former%2C%20and%20show%20that%20the%20dying%20neuron%20problem%20is%0Anot%20exclusive%20to%20ReLU%27s.%20To%20alleviate%20vanishing%20gradients%20and%20the%20resulting%0Adying%20neuron%20problem%20occurring%20with%20continuously%20differentiable%20activations%2C%20we%0Apropose%20a%20Hadamard%20representation.%20Using%20deep%20Q-networks%2C%20proximal%20policy%0Aoptimization%20and%20parallelized%20Q-networks%20in%20the%20Atari%20domain%2C%20we%20show%20faster%0Alearning%2C%20a%20reduction%20in%20dead%20neurons%20and%20increased%20effective%20rank.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09079v3&entry.124074799=Read"},
{"title": "Think Beyond Size: Adaptive Prompting for More Effective Reasoning", "author": "Kamesh R", "abstract": "  Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods.\n", "link": "http://arxiv.org/abs/2410.08130v2", "date": "2024-11-29", "relevancy": 1.9739, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Beyond%20Size%3A%20Adaptive%20Prompting%20for%20More%20Effective%20Reasoning&body=Title%3A%20Think%20Beyond%20Size%3A%20Adaptive%20Prompting%20for%20More%20Effective%20Reasoning%0AAuthor%3A%20Kamesh%20R%0AAbstract%3A%20%20%20Pretrained%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20utilized%20across%20a%0Awide%20range%20of%20natural%20language%20processing%20%28NLP%29%20tasks%20due%20to%20their%20impressive%0Acapabilities%20as%20few-shot%20learners.%20Recent%20techniques%2C%20such%20as%20chain-of-thought%0A%28CoT%29%20prompting%2C%20have%20significantly%20advanced%20multi-step%20reasoning%20by%0Aintroducing%20step-by-step%20decomposition%2C%20achieving%20state-of-the-art%20results%20on%0Acomplex%20reasoning%20benchmarks.%20However%2C%20these%20approaches%20often%20rely%20on%20static%0Aprompting%20templates%20that%20do%20not%20adapt%20to%20task%20complexity%20or%20errors%20during%20the%0Areasoning%20process.%20In%20this%20work%2C%20we%20introduce%20Adaptive%20Prompting%2C%20a%20dynamic%20and%0Aiterative%20framework%20designed%20to%20enhance%20reasoning%20by%20incorporating%20real-time%0Aadjustments%20to%20prompt%20structures%20and%20validation%20mechanisms.Experimental%20results%0Ademonstrate%20that%20Adaptive%20Prompting%20significantly%20improves%20performance%20on%0Adiverse%20reasoning%20benchmarks%2C%20including%20arithmetic%20reasoning%20%28GSM8K%2C%0AMultiArith%29%2C%20logical%20reasoning%20and%20commonsense%20tasks%2C%20achieving%20substantial%0Aaccuracy%20gains%20compared%20to%20static%20prompting%20baselines.%20By%20integrating%20guided%0Aprompts%2C%20intermediate%20validation%2C%20and%20self-corrective%20steps%2C%20our%20approach%0Aenables%20smaller%20models%20to%20achieve%20competitive%20performance%20with%20larger%0Acounterparts%2C%20such%20as%20GPT-4%2C%20while%20maintaining%20computational%20efficiency.%20The%0Aframework%20achieves%20this%20without%20requiring%20fine-tuning%20or%20task-specific%20training%0Adata%2C%20highlighting%20the%20untapped%20potential%20of%20iterative%20reasoning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08130v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Beyond%2520Size%253A%2520Adaptive%2520Prompting%2520for%2520More%2520Effective%2520Reasoning%26entry.906535625%3DKamesh%2520R%26entry.1292438233%3D%2520%2520Pretrained%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520utilized%2520across%2520a%250Awide%2520range%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520tasks%2520due%2520to%2520their%2520impressive%250Acapabilities%2520as%2520few-shot%2520learners.%2520Recent%2520techniques%252C%2520such%2520as%2520chain-of-thought%250A%2528CoT%2529%2520prompting%252C%2520have%2520significantly%2520advanced%2520multi-step%2520reasoning%2520by%250Aintroducing%2520step-by-step%2520decomposition%252C%2520achieving%2520state-of-the-art%2520results%2520on%250Acomplex%2520reasoning%2520benchmarks.%2520However%252C%2520these%2520approaches%2520often%2520rely%2520on%2520static%250Aprompting%2520templates%2520that%2520do%2520not%2520adapt%2520to%2520task%2520complexity%2520or%2520errors%2520during%2520the%250Areasoning%2520process.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Adaptive%2520Prompting%252C%2520a%2520dynamic%2520and%250Aiterative%2520framework%2520designed%2520to%2520enhance%2520reasoning%2520by%2520incorporating%2520real-time%250Aadjustments%2520to%2520prompt%2520structures%2520and%2520validation%2520mechanisms.Experimental%2520results%250Ademonstrate%2520that%2520Adaptive%2520Prompting%2520significantly%2520improves%2520performance%2520on%250Adiverse%2520reasoning%2520benchmarks%252C%2520including%2520arithmetic%2520reasoning%2520%2528GSM8K%252C%250AMultiArith%2529%252C%2520logical%2520reasoning%2520and%2520commonsense%2520tasks%252C%2520achieving%2520substantial%250Aaccuracy%2520gains%2520compared%2520to%2520static%2520prompting%2520baselines.%2520By%2520integrating%2520guided%250Aprompts%252C%2520intermediate%2520validation%252C%2520and%2520self-corrective%2520steps%252C%2520our%2520approach%250Aenables%2520smaller%2520models%2520to%2520achieve%2520competitive%2520performance%2520with%2520larger%250Acounterparts%252C%2520such%2520as%2520GPT-4%252C%2520while%2520maintaining%2520computational%2520efficiency.%2520The%250Aframework%2520achieves%2520this%2520without%2520requiring%2520fine-tuning%2520or%2520task-specific%2520training%250Adata%252C%2520highlighting%2520the%2520untapped%2520potential%2520of%2520iterative%2520reasoning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08130v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Beyond%20Size%3A%20Adaptive%20Prompting%20for%20More%20Effective%20Reasoning&entry.906535625=Kamesh%20R&entry.1292438233=%20%20Pretrained%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20utilized%20across%20a%0Awide%20range%20of%20natural%20language%20processing%20%28NLP%29%20tasks%20due%20to%20their%20impressive%0Acapabilities%20as%20few-shot%20learners.%20Recent%20techniques%2C%20such%20as%20chain-of-thought%0A%28CoT%29%20prompting%2C%20have%20significantly%20advanced%20multi-step%20reasoning%20by%0Aintroducing%20step-by-step%20decomposition%2C%20achieving%20state-of-the-art%20results%20on%0Acomplex%20reasoning%20benchmarks.%20However%2C%20these%20approaches%20often%20rely%20on%20static%0Aprompting%20templates%20that%20do%20not%20adapt%20to%20task%20complexity%20or%20errors%20during%20the%0Areasoning%20process.%20In%20this%20work%2C%20we%20introduce%20Adaptive%20Prompting%2C%20a%20dynamic%20and%0Aiterative%20framework%20designed%20to%20enhance%20reasoning%20by%20incorporating%20real-time%0Aadjustments%20to%20prompt%20structures%20and%20validation%20mechanisms.Experimental%20results%0Ademonstrate%20that%20Adaptive%20Prompting%20significantly%20improves%20performance%20on%0Adiverse%20reasoning%20benchmarks%2C%20including%20arithmetic%20reasoning%20%28GSM8K%2C%0AMultiArith%29%2C%20logical%20reasoning%20and%20commonsense%20tasks%2C%20achieving%20substantial%0Aaccuracy%20gains%20compared%20to%20static%20prompting%20baselines.%20By%20integrating%20guided%0Aprompts%2C%20intermediate%20validation%2C%20and%20self-corrective%20steps%2C%20our%20approach%0Aenables%20smaller%20models%20to%20achieve%20competitive%20performance%20with%20larger%0Acounterparts%2C%20such%20as%20GPT-4%2C%20while%20maintaining%20computational%20efficiency.%20The%0Aframework%20achieves%20this%20without%20requiring%20fine-tuning%20or%20task-specific%20training%0Adata%2C%20highlighting%20the%20untapped%20potential%20of%20iterative%20reasoning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08130v2&entry.124074799=Read"},
{"title": "A Comprehensive Content Verification System for ensuring Digital\n  Integrity in the Age of Deep Fakes", "author": "RaviKanth Kaja", "abstract": "  In an era marked by the widespread sharing of digital content, the need for a\nrobust content-integrity verification goes beyond the confines of individual\nsocial media platforms. While verified profiles (such as blue ticks on\nplatforms like Instagram and X) have become synonymous with credibility, the\ncontent they share often traverses a complex network of interconnected\nplatforms, by means of re-sharing, re-posting, etc., leaving a void in the\nauthentication process of the content itself. With the advent of easily\naccessible AI tools (like DALL-E, Sora, and the tools that are explicitly built\nfor generating deepfakes & face swaps), the risk of misinformation through\nsocial media platforms is growing exponentially. This paper discusses a\nsolution, a Content Verification System, designed to authenticate images and\nvideos shared as posts or stories across the digital landscape. Going beyond\nthe limitations of blue ticks, this system empowers individuals and influencers\nto validate the authenticity of their digital footprint, safeguarding their\nreputation in an interconnected world.\n", "link": "http://arxiv.org/abs/2411.19750v1", "date": "2024-11-29", "relevancy": 1.9685, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5302}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4736}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Content%20Verification%20System%20for%20ensuring%20Digital%0A%20%20Integrity%20in%20the%20Age%20of%20Deep%20Fakes&body=Title%3A%20A%20Comprehensive%20Content%20Verification%20System%20for%20ensuring%20Digital%0A%20%20Integrity%20in%20the%20Age%20of%20Deep%20Fakes%0AAuthor%3A%20RaviKanth%20Kaja%0AAbstract%3A%20%20%20In%20an%20era%20marked%20by%20the%20widespread%20sharing%20of%20digital%20content%2C%20the%20need%20for%20a%0Arobust%20content-integrity%20verification%20goes%20beyond%20the%20confines%20of%20individual%0Asocial%20media%20platforms.%20While%20verified%20profiles%20%28such%20as%20blue%20ticks%20on%0Aplatforms%20like%20Instagram%20and%20X%29%20have%20become%20synonymous%20with%20credibility%2C%20the%0Acontent%20they%20share%20often%20traverses%20a%20complex%20network%20of%20interconnected%0Aplatforms%2C%20by%20means%20of%20re-sharing%2C%20re-posting%2C%20etc.%2C%20leaving%20a%20void%20in%20the%0Aauthentication%20process%20of%20the%20content%20itself.%20With%20the%20advent%20of%20easily%0Aaccessible%20AI%20tools%20%28like%20DALL-E%2C%20Sora%2C%20and%20the%20tools%20that%20are%20explicitly%20built%0Afor%20generating%20deepfakes%20%26%20face%20swaps%29%2C%20the%20risk%20of%20misinformation%20through%0Asocial%20media%20platforms%20is%20growing%20exponentially.%20This%20paper%20discusses%20a%0Asolution%2C%20a%20Content%20Verification%20System%2C%20designed%20to%20authenticate%20images%20and%0Avideos%20shared%20as%20posts%20or%20stories%20across%20the%20digital%20landscape.%20Going%20beyond%0Athe%20limitations%20of%20blue%20ticks%2C%20this%20system%20empowers%20individuals%20and%20influencers%0Ato%20validate%20the%20authenticity%20of%20their%20digital%20footprint%2C%20safeguarding%20their%0Areputation%20in%20an%20interconnected%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Content%2520Verification%2520System%2520for%2520ensuring%2520Digital%250A%2520%2520Integrity%2520in%2520the%2520Age%2520of%2520Deep%2520Fakes%26entry.906535625%3DRaviKanth%2520Kaja%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520marked%2520by%2520the%2520widespread%2520sharing%2520of%2520digital%2520content%252C%2520the%2520need%2520for%2520a%250Arobust%2520content-integrity%2520verification%2520goes%2520beyond%2520the%2520confines%2520of%2520individual%250Asocial%2520media%2520platforms.%2520While%2520verified%2520profiles%2520%2528such%2520as%2520blue%2520ticks%2520on%250Aplatforms%2520like%2520Instagram%2520and%2520X%2529%2520have%2520become%2520synonymous%2520with%2520credibility%252C%2520the%250Acontent%2520they%2520share%2520often%2520traverses%2520a%2520complex%2520network%2520of%2520interconnected%250Aplatforms%252C%2520by%2520means%2520of%2520re-sharing%252C%2520re-posting%252C%2520etc.%252C%2520leaving%2520a%2520void%2520in%2520the%250Aauthentication%2520process%2520of%2520the%2520content%2520itself.%2520With%2520the%2520advent%2520of%2520easily%250Aaccessible%2520AI%2520tools%2520%2528like%2520DALL-E%252C%2520Sora%252C%2520and%2520the%2520tools%2520that%2520are%2520explicitly%2520built%250Afor%2520generating%2520deepfakes%2520%2526%2520face%2520swaps%2529%252C%2520the%2520risk%2520of%2520misinformation%2520through%250Asocial%2520media%2520platforms%2520is%2520growing%2520exponentially.%2520This%2520paper%2520discusses%2520a%250Asolution%252C%2520a%2520Content%2520Verification%2520System%252C%2520designed%2520to%2520authenticate%2520images%2520and%250Avideos%2520shared%2520as%2520posts%2520or%2520stories%2520across%2520the%2520digital%2520landscape.%2520Going%2520beyond%250Athe%2520limitations%2520of%2520blue%2520ticks%252C%2520this%2520system%2520empowers%2520individuals%2520and%2520influencers%250Ato%2520validate%2520the%2520authenticity%2520of%2520their%2520digital%2520footprint%252C%2520safeguarding%2520their%250Areputation%2520in%2520an%2520interconnected%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Content%20Verification%20System%20for%20ensuring%20Digital%0A%20%20Integrity%20in%20the%20Age%20of%20Deep%20Fakes&entry.906535625=RaviKanth%20Kaja&entry.1292438233=%20%20In%20an%20era%20marked%20by%20the%20widespread%20sharing%20of%20digital%20content%2C%20the%20need%20for%20a%0Arobust%20content-integrity%20verification%20goes%20beyond%20the%20confines%20of%20individual%0Asocial%20media%20platforms.%20While%20verified%20profiles%20%28such%20as%20blue%20ticks%20on%0Aplatforms%20like%20Instagram%20and%20X%29%20have%20become%20synonymous%20with%20credibility%2C%20the%0Acontent%20they%20share%20often%20traverses%20a%20complex%20network%20of%20interconnected%0Aplatforms%2C%20by%20means%20of%20re-sharing%2C%20re-posting%2C%20etc.%2C%20leaving%20a%20void%20in%20the%0Aauthentication%20process%20of%20the%20content%20itself.%20With%20the%20advent%20of%20easily%0Aaccessible%20AI%20tools%20%28like%20DALL-E%2C%20Sora%2C%20and%20the%20tools%20that%20are%20explicitly%20built%0Afor%20generating%20deepfakes%20%26%20face%20swaps%29%2C%20the%20risk%20of%20misinformation%20through%0Asocial%20media%20platforms%20is%20growing%20exponentially.%20This%20paper%20discusses%20a%0Asolution%2C%20a%20Content%20Verification%20System%2C%20designed%20to%20authenticate%20images%20and%0Avideos%20shared%20as%20posts%20or%20stories%20across%20the%20digital%20landscape.%20Going%20beyond%0Athe%20limitations%20of%20blue%20ticks%2C%20this%20system%20empowers%20individuals%20and%20influencers%0Ato%20validate%20the%20authenticity%20of%20their%20digital%20footprint%2C%20safeguarding%20their%0Areputation%20in%20an%20interconnected%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19750v1&entry.124074799=Read"},
{"title": "LLM-Rank: A Graph Theoretical Approach to Pruning Large Language Models", "author": "David Hoffmann and Kailash Budhathoki and Matthaeus Kleindessner", "abstract": "  The evolving capabilities of large language models are accompanied by growing\nsizes and deployment costs, necessitating effective inference optimisation\ntechniques. We propose a novel pruning method utilising centrality measures\nfrom graph theory, reducing both the computational requirements and the memory\nfootprint of these models. Specifically, we devise a method for creating a\nweighted directed acyclical graph representation of multilayer perceptrons to\nwhich we apply a modified version of the weighted PageRank centrality measure\nto compute node importance scores. In combination with uniform pruning this\nleads to structured sparsity. We call this pruning method MLPRank. Furthermore\nwe introduce an extension to decoder-only transformer models and call it\nLLMRank. For both variants we demonstrate a strong performance. With MLPRank on\naverage leading to 6.09 % higher accuracy retention than three popular\nbaselines and 13.42 % with LLMRank compared to two popular baselines. Code is\navailable at https://github.com/amazon-science/llm-rank-pruning.\n", "link": "http://arxiv.org/abs/2410.13299v2", "date": "2024-11-29", "relevancy": 1.9595, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4966}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4958}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Rank%3A%20A%20Graph%20Theoretical%20Approach%20to%20Pruning%20Large%20Language%20Models&body=Title%3A%20LLM-Rank%3A%20A%20Graph%20Theoretical%20Approach%20to%20Pruning%20Large%20Language%20Models%0AAuthor%3A%20David%20Hoffmann%20and%20Kailash%20Budhathoki%20and%20Matthaeus%20Kleindessner%0AAbstract%3A%20%20%20The%20evolving%20capabilities%20of%20large%20language%20models%20are%20accompanied%20by%20growing%0Asizes%20and%20deployment%20costs%2C%20necessitating%20effective%20inference%20optimisation%0Atechniques.%20We%20propose%20a%20novel%20pruning%20method%20utilising%20centrality%20measures%0Afrom%20graph%20theory%2C%20reducing%20both%20the%20computational%20requirements%20and%20the%20memory%0Afootprint%20of%20these%20models.%20Specifically%2C%20we%20devise%20a%20method%20for%20creating%20a%0Aweighted%20directed%20acyclical%20graph%20representation%20of%20multilayer%20perceptrons%20to%0Awhich%20we%20apply%20a%20modified%20version%20of%20the%20weighted%20PageRank%20centrality%20measure%0Ato%20compute%20node%20importance%20scores.%20In%20combination%20with%20uniform%20pruning%20this%0Aleads%20to%20structured%20sparsity.%20We%20call%20this%20pruning%20method%20MLPRank.%20Furthermore%0Awe%20introduce%20an%20extension%20to%20decoder-only%20transformer%20models%20and%20call%20it%0ALLMRank.%20For%20both%20variants%20we%20demonstrate%20a%20strong%20performance.%20With%20MLPRank%20on%0Aaverage%20leading%20to%206.09%20%25%20higher%20accuracy%20retention%20than%20three%20popular%0Abaselines%20and%2013.42%20%25%20with%20LLMRank%20compared%20to%20two%20popular%20baselines.%20Code%20is%0Aavailable%20at%20https%3A//github.com/amazon-science/llm-rank-pruning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13299v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Rank%253A%2520A%2520Graph%2520Theoretical%2520Approach%2520to%2520Pruning%2520Large%2520Language%2520Models%26entry.906535625%3DDavid%2520Hoffmann%2520and%2520Kailash%2520Budhathoki%2520and%2520Matthaeus%2520Kleindessner%26entry.1292438233%3D%2520%2520The%2520evolving%2520capabilities%2520of%2520large%2520language%2520models%2520are%2520accompanied%2520by%2520growing%250Asizes%2520and%2520deployment%2520costs%252C%2520necessitating%2520effective%2520inference%2520optimisation%250Atechniques.%2520We%2520propose%2520a%2520novel%2520pruning%2520method%2520utilising%2520centrality%2520measures%250Afrom%2520graph%2520theory%252C%2520reducing%2520both%2520the%2520computational%2520requirements%2520and%2520the%2520memory%250Afootprint%2520of%2520these%2520models.%2520Specifically%252C%2520we%2520devise%2520a%2520method%2520for%2520creating%2520a%250Aweighted%2520directed%2520acyclical%2520graph%2520representation%2520of%2520multilayer%2520perceptrons%2520to%250Awhich%2520we%2520apply%2520a%2520modified%2520version%2520of%2520the%2520weighted%2520PageRank%2520centrality%2520measure%250Ato%2520compute%2520node%2520importance%2520scores.%2520In%2520combination%2520with%2520uniform%2520pruning%2520this%250Aleads%2520to%2520structured%2520sparsity.%2520We%2520call%2520this%2520pruning%2520method%2520MLPRank.%2520Furthermore%250Awe%2520introduce%2520an%2520extension%2520to%2520decoder-only%2520transformer%2520models%2520and%2520call%2520it%250ALLMRank.%2520For%2520both%2520variants%2520we%2520demonstrate%2520a%2520strong%2520performance.%2520With%2520MLPRank%2520on%250Aaverage%2520leading%2520to%25206.09%2520%2525%2520higher%2520accuracy%2520retention%2520than%2520three%2520popular%250Abaselines%2520and%252013.42%2520%2525%2520with%2520LLMRank%2520compared%2520to%2520two%2520popular%2520baselines.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/amazon-science/llm-rank-pruning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13299v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Rank%3A%20A%20Graph%20Theoretical%20Approach%20to%20Pruning%20Large%20Language%20Models&entry.906535625=David%20Hoffmann%20and%20Kailash%20Budhathoki%20and%20Matthaeus%20Kleindessner&entry.1292438233=%20%20The%20evolving%20capabilities%20of%20large%20language%20models%20are%20accompanied%20by%20growing%0Asizes%20and%20deployment%20costs%2C%20necessitating%20effective%20inference%20optimisation%0Atechniques.%20We%20propose%20a%20novel%20pruning%20method%20utilising%20centrality%20measures%0Afrom%20graph%20theory%2C%20reducing%20both%20the%20computational%20requirements%20and%20the%20memory%0Afootprint%20of%20these%20models.%20Specifically%2C%20we%20devise%20a%20method%20for%20creating%20a%0Aweighted%20directed%20acyclical%20graph%20representation%20of%20multilayer%20perceptrons%20to%0Awhich%20we%20apply%20a%20modified%20version%20of%20the%20weighted%20PageRank%20centrality%20measure%0Ato%20compute%20node%20importance%20scores.%20In%20combination%20with%20uniform%20pruning%20this%0Aleads%20to%20structured%20sparsity.%20We%20call%20this%20pruning%20method%20MLPRank.%20Furthermore%0Awe%20introduce%20an%20extension%20to%20decoder-only%20transformer%20models%20and%20call%20it%0ALLMRank.%20For%20both%20variants%20we%20demonstrate%20a%20strong%20performance.%20With%20MLPRank%20on%0Aaverage%20leading%20to%206.09%20%25%20higher%20accuracy%20retention%20than%20three%20popular%0Abaselines%20and%2013.42%20%25%20with%20LLMRank%20compared%20to%20two%20popular%20baselines.%20Code%20is%0Aavailable%20at%20https%3A//github.com/amazon-science/llm-rank-pruning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13299v2&entry.124074799=Read"},
{"title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models", "author": "Yulei Qin and Yuncheng Yang and Pengcheng Guo and Gang Li and Hang Shao and Yuchen Shi and Zihan Xu and Yun Gu and Ke Li and Xing Sun", "abstract": "  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n", "link": "http://arxiv.org/abs/2408.02085v4", "date": "2024-11-29", "relevancy": 1.9552, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Data%20Tsunami%3A%20A%20Comprehensive%20Survey%20on%20Data%0A%20%20Assessment%20and%20Selection%20for%20Instruction%20Tuning%20of%20Language%20Models&body=Title%3A%20Unleashing%20the%20Power%20of%20Data%20Tsunami%3A%20A%20Comprehensive%20Survey%20on%20Data%0A%20%20Assessment%20and%20Selection%20for%20Instruction%20Tuning%20of%20Language%20Models%0AAuthor%3A%20Yulei%20Qin%20and%20Yuncheng%20Yang%20and%20Pengcheng%20Guo%20and%20Gang%20Li%20and%20Hang%20Shao%20and%20Yuchen%20Shi%20and%20Zihan%20Xu%20and%20Yun%20Gu%20and%20Ke%20Li%20and%20Xing%20Sun%0AAbstract%3A%20%20%20Instruction%20tuning%20plays%20a%20critical%20role%20in%20aligning%20large%20language%20models%0A%28LLMs%29%20with%20human%20preference.%20Despite%20the%20vast%20amount%20of%20open%20instruction%0Adatasets%2C%20naively%20training%20a%20LLM%20on%20all%20existing%20instructions%20may%20not%20be%0Aoptimal%20and%20practical.%20To%20pinpoint%20the%20most%20beneficial%20datapoints%2C%20data%0Aassessment%20and%20selection%20methods%20have%20been%20proposed%20in%20the%20fields%20of%20natural%0Alanguage%20processing%20%28NLP%29%20and%20deep%20learning.%20However%2C%20under%20the%20context%20of%0Ainstruction%20tuning%2C%20there%20still%20exists%20a%20gap%20in%20knowledge%20on%20what%20kind%20of%20data%0Aevaluation%20metrics%20can%20be%20employed%20and%20how%20they%20can%20be%20integrated%20into%20the%0Aselection%20mechanism.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20comprehensive%20review%20on%0Aexisting%20literature%20of%20data%20assessment%20and%20selection%20especially%20for%20instruction%0Atuning%20of%20LLMs.%20We%20systematically%20categorize%20all%20applicable%20methods%20into%0Aquality-based%2C%20diversity-based%2C%20and%20importance-based%20ones%20where%20a%20unified%2C%0Afine-grained%20taxonomy%20is%20structured.%20For%20each%20category%2C%20representative%20methods%0Aare%20elaborated%20to%20describe%20the%20landscape%20of%20relevant%20research.%20In%20addition%2C%0Acomparison%20between%20the%20latest%20methods%20is%20conducted%20on%20their%20officially%20reported%0Aresults%20to%20provide%20in-depth%20discussions%20on%20their%20limitations.%20Finally%2C%20we%0Asummarize%20the%20open%20challenges%20and%20propose%20the%20promosing%20avenues%20for%20future%0Astudies.%20All%20related%20contents%20are%20available%20at%0Ahttps%3A//github.com/yuleiqin/fantastic-data-engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02085v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Power%2520of%2520Data%2520Tsunami%253A%2520A%2520Comprehensive%2520Survey%2520on%2520Data%250A%2520%2520Assessment%2520and%2520Selection%2520for%2520Instruction%2520Tuning%2520of%2520Language%2520Models%26entry.906535625%3DYulei%2520Qin%2520and%2520Yuncheng%2520Yang%2520and%2520Pengcheng%2520Guo%2520and%2520Gang%2520Li%2520and%2520Hang%2520Shao%2520and%2520Yuchen%2520Shi%2520and%2520Zihan%2520Xu%2520and%2520Yun%2520Gu%2520and%2520Ke%2520Li%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520plays%2520a%2520critical%2520role%2520in%2520aligning%2520large%2520language%2520models%250A%2528LLMs%2529%2520with%2520human%2520preference.%2520Despite%2520the%2520vast%2520amount%2520of%2520open%2520instruction%250Adatasets%252C%2520naively%2520training%2520a%2520LLM%2520on%2520all%2520existing%2520instructions%2520may%2520not%2520be%250Aoptimal%2520and%2520practical.%2520To%2520pinpoint%2520the%2520most%2520beneficial%2520datapoints%252C%2520data%250Aassessment%2520and%2520selection%2520methods%2520have%2520been%2520proposed%2520in%2520the%2520fields%2520of%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%2520and%2520deep%2520learning.%2520However%252C%2520under%2520the%2520context%2520of%250Ainstruction%2520tuning%252C%2520there%2520still%2520exists%2520a%2520gap%2520in%2520knowledge%2520on%2520what%2520kind%2520of%2520data%250Aevaluation%2520metrics%2520can%2520be%2520employed%2520and%2520how%2520they%2520can%2520be%2520integrated%2520into%2520the%250Aselection%2520mechanism.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520a%2520comprehensive%2520review%2520on%250Aexisting%2520literature%2520of%2520data%2520assessment%2520and%2520selection%2520especially%2520for%2520instruction%250Atuning%2520of%2520LLMs.%2520We%2520systematically%2520categorize%2520all%2520applicable%2520methods%2520into%250Aquality-based%252C%2520diversity-based%252C%2520and%2520importance-based%2520ones%2520where%2520a%2520unified%252C%250Afine-grained%2520taxonomy%2520is%2520structured.%2520For%2520each%2520category%252C%2520representative%2520methods%250Aare%2520elaborated%2520to%2520describe%2520the%2520landscape%2520of%2520relevant%2520research.%2520In%2520addition%252C%250Acomparison%2520between%2520the%2520latest%2520methods%2520is%2520conducted%2520on%2520their%2520officially%2520reported%250Aresults%2520to%2520provide%2520in-depth%2520discussions%2520on%2520their%2520limitations.%2520Finally%252C%2520we%250Asummarize%2520the%2520open%2520challenges%2520and%2520propose%2520the%2520promosing%2520avenues%2520for%2520future%250Astudies.%2520All%2520related%2520contents%2520are%2520available%2520at%250Ahttps%253A//github.com/yuleiqin/fantastic-data-engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02085v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Data%20Tsunami%3A%20A%20Comprehensive%20Survey%20on%20Data%0A%20%20Assessment%20and%20Selection%20for%20Instruction%20Tuning%20of%20Language%20Models&entry.906535625=Yulei%20Qin%20and%20Yuncheng%20Yang%20and%20Pengcheng%20Guo%20and%20Gang%20Li%20and%20Hang%20Shao%20and%20Yuchen%20Shi%20and%20Zihan%20Xu%20and%20Yun%20Gu%20and%20Ke%20Li%20and%20Xing%20Sun&entry.1292438233=%20%20Instruction%20tuning%20plays%20a%20critical%20role%20in%20aligning%20large%20language%20models%0A%28LLMs%29%20with%20human%20preference.%20Despite%20the%20vast%20amount%20of%20open%20instruction%0Adatasets%2C%20naively%20training%20a%20LLM%20on%20all%20existing%20instructions%20may%20not%20be%0Aoptimal%20and%20practical.%20To%20pinpoint%20the%20most%20beneficial%20datapoints%2C%20data%0Aassessment%20and%20selection%20methods%20have%20been%20proposed%20in%20the%20fields%20of%20natural%0Alanguage%20processing%20%28NLP%29%20and%20deep%20learning.%20However%2C%20under%20the%20context%20of%0Ainstruction%20tuning%2C%20there%20still%20exists%20a%20gap%20in%20knowledge%20on%20what%20kind%20of%20data%0Aevaluation%20metrics%20can%20be%20employed%20and%20how%20they%20can%20be%20integrated%20into%20the%0Aselection%20mechanism.%20To%20bridge%20this%20gap%2C%20we%20present%20a%20comprehensive%20review%20on%0Aexisting%20literature%20of%20data%20assessment%20and%20selection%20especially%20for%20instruction%0Atuning%20of%20LLMs.%20We%20systematically%20categorize%20all%20applicable%20methods%20into%0Aquality-based%2C%20diversity-based%2C%20and%20importance-based%20ones%20where%20a%20unified%2C%0Afine-grained%20taxonomy%20is%20structured.%20For%20each%20category%2C%20representative%20methods%0Aare%20elaborated%20to%20describe%20the%20landscape%20of%20relevant%20research.%20In%20addition%2C%0Acomparison%20between%20the%20latest%20methods%20is%20conducted%20on%20their%20officially%20reported%0Aresults%20to%20provide%20in-depth%20discussions%20on%20their%20limitations.%20Finally%2C%20we%0Asummarize%20the%20open%20challenges%20and%20propose%20the%20promosing%20avenues%20for%20future%0Astudies.%20All%20related%20contents%20are%20available%20at%0Ahttps%3A//github.com/yuleiqin/fantastic-data-engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02085v4&entry.124074799=Read"},
{"title": "What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics", "author": "Jordan J. Bird", "abstract": "  The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.\n", "link": "http://arxiv.org/abs/2411.17593v2", "date": "2024-11-29", "relevancy": 1.9531, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5012}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Differentiates%20Educational%20Literature%3F%20A%20Multimodal%20Fusion%20Approach%0A%20%20of%20Transformers%20and%20Computational%20Linguistics&body=Title%3A%20What%20Differentiates%20Educational%20Literature%3F%20A%20Multimodal%20Fusion%20Approach%0A%20%20of%20Transformers%20and%20Computational%20Linguistics%0AAuthor%3A%20Jordan%20J.%20Bird%0AAbstract%3A%20%20%20The%20integration%20of%20new%20literature%20into%20the%20English%20curriculum%20remains%20a%0Achallenge%20since%20educators%20often%20lack%20scalable%20tools%20to%20rapidly%20evaluate%0Areadability%20and%20adapt%20texts%20for%20diverse%20classroom%20needs.%20This%20study%20proposes%20to%0Aaddress%20this%20gap%20through%20a%20multimodal%20approach%20that%20combines%20transformer-based%0Atext%20classification%20with%20linguistic%20feature%20analysis%20to%20align%20texts%20with%20UK%20Key%0AStages.%20Eight%20state-of-the-art%20Transformers%20were%20fine-tuned%20on%20segmented%20text%0Adata%2C%20with%20BERT%20achieving%20the%20highest%20unimodal%20F1%20score%20of%200.75.%20In%20parallel%2C%0A500%20deep%20neural%20network%20topologies%20were%20searched%20for%20the%20classification%20of%0Alinguistic%20characteristics%2C%20achieving%20an%20F1%20score%20of%200.392.%20The%20fusion%20of%20these%0Amodalities%20shows%20a%20significant%20improvement%2C%20with%20every%20multimodal%20approach%0Aoutperforming%20all%20unimodal%20models.%20In%20particular%2C%20the%20ELECTRA%20Transformer%20fused%0Awith%20the%20neural%20network%20achieved%20an%20F1%20score%20of%200.996.%20Unimodal%20and%20multimodal%0Aapproaches%20are%20shown%20to%20have%20statistically%20significant%20differences%20in%20all%0Avalidation%20metrics%20%28accuracy%2C%20precision%2C%20recall%2C%20F1%20score%29%20except%20for%20inference%0Atime.%20The%20proposed%20approach%20is%20finally%20encapsulated%20in%20a%20stakeholder-facing%20web%0Aapplication%2C%20providing%20non-technical%20stakeholder%20access%20to%20real-time%20insights%0Aon%20text%20complexity%2C%20reading%20difficulty%2C%20curriculum%20alignment%2C%20and%0Arecommendations%20for%20learning%20age%20range.%20The%20application%20empowers%20data-driven%0Adecision%20making%20and%20reduces%20manual%20workload%20by%20integrating%20AI-based%0Arecommendations%20into%20lesson%20planning%20for%20English%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Differentiates%2520Educational%2520Literature%253F%2520A%2520Multimodal%2520Fusion%2520Approach%250A%2520%2520of%2520Transformers%2520and%2520Computational%2520Linguistics%26entry.906535625%3DJordan%2520J.%2520Bird%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520new%2520literature%2520into%2520the%2520English%2520curriculum%2520remains%2520a%250Achallenge%2520since%2520educators%2520often%2520lack%2520scalable%2520tools%2520to%2520rapidly%2520evaluate%250Areadability%2520and%2520adapt%2520texts%2520for%2520diverse%2520classroom%2520needs.%2520This%2520study%2520proposes%2520to%250Aaddress%2520this%2520gap%2520through%2520a%2520multimodal%2520approach%2520that%2520combines%2520transformer-based%250Atext%2520classification%2520with%2520linguistic%2520feature%2520analysis%2520to%2520align%2520texts%2520with%2520UK%2520Key%250AStages.%2520Eight%2520state-of-the-art%2520Transformers%2520were%2520fine-tuned%2520on%2520segmented%2520text%250Adata%252C%2520with%2520BERT%2520achieving%2520the%2520highest%2520unimodal%2520F1%2520score%2520of%25200.75.%2520In%2520parallel%252C%250A500%2520deep%2520neural%2520network%2520topologies%2520were%2520searched%2520for%2520the%2520classification%2520of%250Alinguistic%2520characteristics%252C%2520achieving%2520an%2520F1%2520score%2520of%25200.392.%2520The%2520fusion%2520of%2520these%250Amodalities%2520shows%2520a%2520significant%2520improvement%252C%2520with%2520every%2520multimodal%2520approach%250Aoutperforming%2520all%2520unimodal%2520models.%2520In%2520particular%252C%2520the%2520ELECTRA%2520Transformer%2520fused%250Awith%2520the%2520neural%2520network%2520achieved%2520an%2520F1%2520score%2520of%25200.996.%2520Unimodal%2520and%2520multimodal%250Aapproaches%2520are%2520shown%2520to%2520have%2520statistically%2520significant%2520differences%2520in%2520all%250Avalidation%2520metrics%2520%2528accuracy%252C%2520precision%252C%2520recall%252C%2520F1%2520score%2529%2520except%2520for%2520inference%250Atime.%2520The%2520proposed%2520approach%2520is%2520finally%2520encapsulated%2520in%2520a%2520stakeholder-facing%2520web%250Aapplication%252C%2520providing%2520non-technical%2520stakeholder%2520access%2520to%2520real-time%2520insights%250Aon%2520text%2520complexity%252C%2520reading%2520difficulty%252C%2520curriculum%2520alignment%252C%2520and%250Arecommendations%2520for%2520learning%2520age%2520range.%2520The%2520application%2520empowers%2520data-driven%250Adecision%2520making%2520and%2520reduces%2520manual%2520workload%2520by%2520integrating%2520AI-based%250Arecommendations%2520into%2520lesson%2520planning%2520for%2520English%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Differentiates%20Educational%20Literature%3F%20A%20Multimodal%20Fusion%20Approach%0A%20%20of%20Transformers%20and%20Computational%20Linguistics&entry.906535625=Jordan%20J.%20Bird&entry.1292438233=%20%20The%20integration%20of%20new%20literature%20into%20the%20English%20curriculum%20remains%20a%0Achallenge%20since%20educators%20often%20lack%20scalable%20tools%20to%20rapidly%20evaluate%0Areadability%20and%20adapt%20texts%20for%20diverse%20classroom%20needs.%20This%20study%20proposes%20to%0Aaddress%20this%20gap%20through%20a%20multimodal%20approach%20that%20combines%20transformer-based%0Atext%20classification%20with%20linguistic%20feature%20analysis%20to%20align%20texts%20with%20UK%20Key%0AStages.%20Eight%20state-of-the-art%20Transformers%20were%20fine-tuned%20on%20segmented%20text%0Adata%2C%20with%20BERT%20achieving%20the%20highest%20unimodal%20F1%20score%20of%200.75.%20In%20parallel%2C%0A500%20deep%20neural%20network%20topologies%20were%20searched%20for%20the%20classification%20of%0Alinguistic%20characteristics%2C%20achieving%20an%20F1%20score%20of%200.392.%20The%20fusion%20of%20these%0Amodalities%20shows%20a%20significant%20improvement%2C%20with%20every%20multimodal%20approach%0Aoutperforming%20all%20unimodal%20models.%20In%20particular%2C%20the%20ELECTRA%20Transformer%20fused%0Awith%20the%20neural%20network%20achieved%20an%20F1%20score%20of%200.996.%20Unimodal%20and%20multimodal%0Aapproaches%20are%20shown%20to%20have%20statistically%20significant%20differences%20in%20all%0Avalidation%20metrics%20%28accuracy%2C%20precision%2C%20recall%2C%20F1%20score%29%20except%20for%20inference%0Atime.%20The%20proposed%20approach%20is%20finally%20encapsulated%20in%20a%20stakeholder-facing%20web%0Aapplication%2C%20providing%20non-technical%20stakeholder%20access%20to%20real-time%20insights%0Aon%20text%20complexity%2C%20reading%20difficulty%2C%20curriculum%20alignment%2C%20and%0Arecommendations%20for%20learning%20age%20range.%20The%20application%20empowers%20data-driven%0Adecision%20making%20and%20reduces%20manual%20workload%20by%20integrating%20AI-based%0Arecommendations%20into%20lesson%20planning%20for%20English%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17593v2&entry.124074799=Read"},
{"title": "Learned Random Label Predictions as a Neural Network Complexity Metric", "author": "Marlon Becker and Benjamin Risse", "abstract": "  We empirically investigate the impact of learning randomly generated labels\nin parallel to class labels in supervised learning on memorization, model\ncomplexity, and generalization in deep neural networks. To this end, we\nintroduce a multi-head network architecture as an extension of standard CNN\narchitectures. Inspired by methods used in fair AI, our approach allows for the\nunlearning of random labels, preventing the network from memorizing individual\nsamples. Based on the concept of Rademacher complexity, we first use our\nproposed method as a complexity metric to analyze the effects of common\nregularization techniques and challenge the traditional understanding of\nfeature extraction and classification in CNNs. Second, we propose a novel\nregularizer that effectively reduces sample memorization. However, contrary to\nthe predictions of classical statistical learning theory, we do not observe\nimprovements in generalization.\n", "link": "http://arxiv.org/abs/2411.19640v1", "date": "2024-11-29", "relevancy": 1.9351, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4881}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4845}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Random%20Label%20Predictions%20as%20a%20Neural%20Network%20Complexity%20Metric&body=Title%3A%20Learned%20Random%20Label%20Predictions%20as%20a%20Neural%20Network%20Complexity%20Metric%0AAuthor%3A%20Marlon%20Becker%20and%20Benjamin%20Risse%0AAbstract%3A%20%20%20We%20empirically%20investigate%20the%20impact%20of%20learning%20randomly%20generated%20labels%0Ain%20parallel%20to%20class%20labels%20in%20supervised%20learning%20on%20memorization%2C%20model%0Acomplexity%2C%20and%20generalization%20in%20deep%20neural%20networks.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multi-head%20network%20architecture%20as%20an%20extension%20of%20standard%20CNN%0Aarchitectures.%20Inspired%20by%20methods%20used%20in%20fair%20AI%2C%20our%20approach%20allows%20for%20the%0Aunlearning%20of%20random%20labels%2C%20preventing%20the%20network%20from%20memorizing%20individual%0Asamples.%20Based%20on%20the%20concept%20of%20Rademacher%20complexity%2C%20we%20first%20use%20our%0Aproposed%20method%20as%20a%20complexity%20metric%20to%20analyze%20the%20effects%20of%20common%0Aregularization%20techniques%20and%20challenge%20the%20traditional%20understanding%20of%0Afeature%20extraction%20and%20classification%20in%20CNNs.%20Second%2C%20we%20propose%20a%20novel%0Aregularizer%20that%20effectively%20reduces%20sample%20memorization.%20However%2C%20contrary%20to%0Athe%20predictions%20of%20classical%20statistical%20learning%20theory%2C%20we%20do%20not%20observe%0Aimprovements%20in%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Random%2520Label%2520Predictions%2520as%2520a%2520Neural%2520Network%2520Complexity%2520Metric%26entry.906535625%3DMarlon%2520Becker%2520and%2520Benjamin%2520Risse%26entry.1292438233%3D%2520%2520We%2520empirically%2520investigate%2520the%2520impact%2520of%2520learning%2520randomly%2520generated%2520labels%250Ain%2520parallel%2520to%2520class%2520labels%2520in%2520supervised%2520learning%2520on%2520memorization%252C%2520model%250Acomplexity%252C%2520and%2520generalization%2520in%2520deep%2520neural%2520networks.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520multi-head%2520network%2520architecture%2520as%2520an%2520extension%2520of%2520standard%2520CNN%250Aarchitectures.%2520Inspired%2520by%2520methods%2520used%2520in%2520fair%2520AI%252C%2520our%2520approach%2520allows%2520for%2520the%250Aunlearning%2520of%2520random%2520labels%252C%2520preventing%2520the%2520network%2520from%2520memorizing%2520individual%250Asamples.%2520Based%2520on%2520the%2520concept%2520of%2520Rademacher%2520complexity%252C%2520we%2520first%2520use%2520our%250Aproposed%2520method%2520as%2520a%2520complexity%2520metric%2520to%2520analyze%2520the%2520effects%2520of%2520common%250Aregularization%2520techniques%2520and%2520challenge%2520the%2520traditional%2520understanding%2520of%250Afeature%2520extraction%2520and%2520classification%2520in%2520CNNs.%2520Second%252C%2520we%2520propose%2520a%2520novel%250Aregularizer%2520that%2520effectively%2520reduces%2520sample%2520memorization.%2520However%252C%2520contrary%2520to%250Athe%2520predictions%2520of%2520classical%2520statistical%2520learning%2520theory%252C%2520we%2520do%2520not%2520observe%250Aimprovements%2520in%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Random%20Label%20Predictions%20as%20a%20Neural%20Network%20Complexity%20Metric&entry.906535625=Marlon%20Becker%20and%20Benjamin%20Risse&entry.1292438233=%20%20We%20empirically%20investigate%20the%20impact%20of%20learning%20randomly%20generated%20labels%0Ain%20parallel%20to%20class%20labels%20in%20supervised%20learning%20on%20memorization%2C%20model%0Acomplexity%2C%20and%20generalization%20in%20deep%20neural%20networks.%20To%20this%20end%2C%20we%0Aintroduce%20a%20multi-head%20network%20architecture%20as%20an%20extension%20of%20standard%20CNN%0Aarchitectures.%20Inspired%20by%20methods%20used%20in%20fair%20AI%2C%20our%20approach%20allows%20for%20the%0Aunlearning%20of%20random%20labels%2C%20preventing%20the%20network%20from%20memorizing%20individual%0Asamples.%20Based%20on%20the%20concept%20of%20Rademacher%20complexity%2C%20we%20first%20use%20our%0Aproposed%20method%20as%20a%20complexity%20metric%20to%20analyze%20the%20effects%20of%20common%0Aregularization%20techniques%20and%20challenge%20the%20traditional%20understanding%20of%0Afeature%20extraction%20and%20classification%20in%20CNNs.%20Second%2C%20we%20propose%20a%20novel%0Aregularizer%20that%20effectively%20reduces%20sample%20memorization.%20However%2C%20contrary%20to%0Athe%20predictions%20of%20classical%20statistical%20learning%20theory%2C%20we%20do%20not%20observe%0Aimprovements%20in%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19640v1&entry.124074799=Read"},
{"title": "Risk-Sensitive Reinforcement Learning with Exponential Criteria", "author": "Erfaun Noorani and Christos Mavridis and John Baras", "abstract": "  While reinforcement learning has shown experimental success in a number of\napplications, it is known to be sensitive to noise and perturbations in the\nparameters of the system, leading to high variance in the total reward amongst\ndifferent episodes in slightly different environments. To introduce robustness,\nas well as sample efficiency, risk-sensitive reinforcement learning methods are\nbeing thoroughly studied. In this work, we provide a definition of robust\nreinforcement learning policies and formulate a risk-sensitive reinforcement\nlearning problem to approximate them, by solving an optimization problem with\nrespect to a modified objective based on exponential criteria. In particular,\nwe study a model-free risk-sensitive variation of the widely-used Monte Carlo\nPolicy Gradient algorithm and introduce a novel risk-sensitive online\nActor-Critic algorithm based on solving a multiplicative Bellman equation using\nstochastic approximation updates. Analytical results suggest that the use of\nexponential criteria generalizes commonly used ad-hoc regularization\napproaches, improves sample efficiency, and introduces robustness with respect\nto perturbations in the model parameters and the environment. The\nimplementation, performance, and robustness properties of the proposed methods\nare evaluated in simulated experiments.\n", "link": "http://arxiv.org/abs/2212.09010v6", "date": "2024-11-29", "relevancy": 1.415, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.48}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4714}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk-Sensitive%20Reinforcement%20Learning%20with%20Exponential%20Criteria&body=Title%3A%20Risk-Sensitive%20Reinforcement%20Learning%20with%20Exponential%20Criteria%0AAuthor%3A%20Erfaun%20Noorani%20and%20Christos%20Mavridis%20and%20John%20Baras%0AAbstract%3A%20%20%20While%20reinforcement%20learning%20has%20shown%20experimental%20success%20in%20a%20number%20of%0Aapplications%2C%20it%20is%20known%20to%20be%20sensitive%20to%20noise%20and%20perturbations%20in%20the%0Aparameters%20of%20the%20system%2C%20leading%20to%20high%20variance%20in%20the%20total%20reward%20amongst%0Adifferent%20episodes%20in%20slightly%20different%20environments.%20To%20introduce%20robustness%2C%0Aas%20well%20as%20sample%20efficiency%2C%20risk-sensitive%20reinforcement%20learning%20methods%20are%0Abeing%20thoroughly%20studied.%20In%20this%20work%2C%20we%20provide%20a%20definition%20of%20robust%0Areinforcement%20learning%20policies%20and%20formulate%20a%20risk-sensitive%20reinforcement%0Alearning%20problem%20to%20approximate%20them%2C%20by%20solving%20an%20optimization%20problem%20with%0Arespect%20to%20a%20modified%20objective%20based%20on%20exponential%20criteria.%20In%20particular%2C%0Awe%20study%20a%20model-free%20risk-sensitive%20variation%20of%20the%20widely-used%20Monte%20Carlo%0APolicy%20Gradient%20algorithm%20and%20introduce%20a%20novel%20risk-sensitive%20online%0AActor-Critic%20algorithm%20based%20on%20solving%20a%20multiplicative%20Bellman%20equation%20using%0Astochastic%20approximation%20updates.%20Analytical%20results%20suggest%20that%20the%20use%20of%0Aexponential%20criteria%20generalizes%20commonly%20used%20ad-hoc%20regularization%0Aapproaches%2C%20improves%20sample%20efficiency%2C%20and%20introduces%20robustness%20with%20respect%0Ato%20perturbations%20in%20the%20model%20parameters%20and%20the%20environment.%20The%0Aimplementation%2C%20performance%2C%20and%20robustness%20properties%20of%20the%20proposed%20methods%0Aare%20evaluated%20in%20simulated%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.09010v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk-Sensitive%2520Reinforcement%2520Learning%2520with%2520Exponential%2520Criteria%26entry.906535625%3DErfaun%2520Noorani%2520and%2520Christos%2520Mavridis%2520and%2520John%2520Baras%26entry.1292438233%3D%2520%2520While%2520reinforcement%2520learning%2520has%2520shown%2520experimental%2520success%2520in%2520a%2520number%2520of%250Aapplications%252C%2520it%2520is%2520known%2520to%2520be%2520sensitive%2520to%2520noise%2520and%2520perturbations%2520in%2520the%250Aparameters%2520of%2520the%2520system%252C%2520leading%2520to%2520high%2520variance%2520in%2520the%2520total%2520reward%2520amongst%250Adifferent%2520episodes%2520in%2520slightly%2520different%2520environments.%2520To%2520introduce%2520robustness%252C%250Aas%2520well%2520as%2520sample%2520efficiency%252C%2520risk-sensitive%2520reinforcement%2520learning%2520methods%2520are%250Abeing%2520thoroughly%2520studied.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520definition%2520of%2520robust%250Areinforcement%2520learning%2520policies%2520and%2520formulate%2520a%2520risk-sensitive%2520reinforcement%250Alearning%2520problem%2520to%2520approximate%2520them%252C%2520by%2520solving%2520an%2520optimization%2520problem%2520with%250Arespect%2520to%2520a%2520modified%2520objective%2520based%2520on%2520exponential%2520criteria.%2520In%2520particular%252C%250Awe%2520study%2520a%2520model-free%2520risk-sensitive%2520variation%2520of%2520the%2520widely-used%2520Monte%2520Carlo%250APolicy%2520Gradient%2520algorithm%2520and%2520introduce%2520a%2520novel%2520risk-sensitive%2520online%250AActor-Critic%2520algorithm%2520based%2520on%2520solving%2520a%2520multiplicative%2520Bellman%2520equation%2520using%250Astochastic%2520approximation%2520updates.%2520Analytical%2520results%2520suggest%2520that%2520the%2520use%2520of%250Aexponential%2520criteria%2520generalizes%2520commonly%2520used%2520ad-hoc%2520regularization%250Aapproaches%252C%2520improves%2520sample%2520efficiency%252C%2520and%2520introduces%2520robustness%2520with%2520respect%250Ato%2520perturbations%2520in%2520the%2520model%2520parameters%2520and%2520the%2520environment.%2520The%250Aimplementation%252C%2520performance%252C%2520and%2520robustness%2520properties%2520of%2520the%2520proposed%2520methods%250Aare%2520evaluated%2520in%2520simulated%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.09010v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk-Sensitive%20Reinforcement%20Learning%20with%20Exponential%20Criteria&entry.906535625=Erfaun%20Noorani%20and%20Christos%20Mavridis%20and%20John%20Baras&entry.1292438233=%20%20While%20reinforcement%20learning%20has%20shown%20experimental%20success%20in%20a%20number%20of%0Aapplications%2C%20it%20is%20known%20to%20be%20sensitive%20to%20noise%20and%20perturbations%20in%20the%0Aparameters%20of%20the%20system%2C%20leading%20to%20high%20variance%20in%20the%20total%20reward%20amongst%0Adifferent%20episodes%20in%20slightly%20different%20environments.%20To%20introduce%20robustness%2C%0Aas%20well%20as%20sample%20efficiency%2C%20risk-sensitive%20reinforcement%20learning%20methods%20are%0Abeing%20thoroughly%20studied.%20In%20this%20work%2C%20we%20provide%20a%20definition%20of%20robust%0Areinforcement%20learning%20policies%20and%20formulate%20a%20risk-sensitive%20reinforcement%0Alearning%20problem%20to%20approximate%20them%2C%20by%20solving%20an%20optimization%20problem%20with%0Arespect%20to%20a%20modified%20objective%20based%20on%20exponential%20criteria.%20In%20particular%2C%0Awe%20study%20a%20model-free%20risk-sensitive%20variation%20of%20the%20widely-used%20Monte%20Carlo%0APolicy%20Gradient%20algorithm%20and%20introduce%20a%20novel%20risk-sensitive%20online%0AActor-Critic%20algorithm%20based%20on%20solving%20a%20multiplicative%20Bellman%20equation%20using%0Astochastic%20approximation%20updates.%20Analytical%20results%20suggest%20that%20the%20use%20of%0Aexponential%20criteria%20generalizes%20commonly%20used%20ad-hoc%20regularization%0Aapproaches%2C%20improves%20sample%20efficiency%2C%20and%20introduces%20robustness%20with%20respect%0Ato%20perturbations%20in%20the%20model%20parameters%20and%20the%20environment.%20The%0Aimplementation%2C%20performance%2C%20and%20robustness%20properties%20of%20the%20proposed%20methods%0Aare%20evaluated%20in%20simulated%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.09010v6&entry.124074799=Read"},
{"title": "Advanced System Integration: Analyzing OpenAPI Chunking for\n  Retrieval-Augmented Generation", "author": "Robin D. Pesl and Jerin G. Mathew and Massimo Mecella and Marco Aiello", "abstract": "  Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.\n", "link": "http://arxiv.org/abs/2411.19804v1", "date": "2024-11-29", "relevancy": 1.9337, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20System%20Integration%3A%20Analyzing%20OpenAPI%20Chunking%20for%0A%20%20Retrieval-Augmented%20Generation&body=Title%3A%20Advanced%20System%20Integration%3A%20Analyzing%20OpenAPI%20Chunking%20for%0A%20%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Robin%20D.%20Pesl%20and%20Jerin%20G.%20Mathew%20and%20Massimo%20Mecella%20and%20Marco%20Aiello%0AAbstract%3A%20%20%20Integrating%20multiple%20%28sub-%29systems%20is%20essential%20to%20create%20advanced%0AInformation%20Systems%20%28ISs%29.%20Difficulties%20mainly%20arise%20when%20integrating%20dynamic%0Aenvironments%20across%20the%20IS%20lifecycle.%20A%20traditional%20approach%20is%20a%20registry%20that%0Aprovides%20the%20API%20documentation%20of%20the%20systems%27%20endpoints.%20Large%20Language%20Models%0A%28LLMs%29%20have%20shown%20to%20be%20capable%20of%20automatically%20creating%20system%20integrations%0A%28e.g.%2C%20as%20service%20composition%29%20based%20on%20this%20documentation%20but%20require%20concise%0Ainput%20due%20to%20input%20token%20limitations%2C%20especially%20regarding%20comprehensive%20API%0Adescriptions.%20Currently%2C%20it%20is%20unknown%20how%20best%20to%20preprocess%20these%20API%0Adescriptions.%20Within%20this%20work%2C%20we%20%28i%29%20analyze%20the%20usage%20of%20Retrieval%20Augmented%0AGeneration%20%28RAG%29%20for%20endpoint%20discovery%20and%20the%20chunking%2C%20i.e.%2C%20preprocessing%2C%0Aof%20OpenAPIs%20to%20reduce%20the%20input%20token%20length%20while%20preserving%20the%20most%20relevant%0Ainformation.%20To%20further%20reduce%20the%20input%20token%20length%20for%20the%20composition%0Aprompt%20and%20improve%20endpoint%20retrieval%2C%20we%20propose%20%28ii%29%20a%20Discovery%20Agent%20that%0Aonly%20receives%20a%20summary%20of%20the%20most%20relevant%20endpoints%20and%20retrieves%20details%20on%0Ademand.%20We%20evaluate%20RAG%20for%20endpoint%20discovery%20using%20the%20RestBench%20benchmark%2C%0Afirst%2C%20for%20the%20different%20chunking%20possibilities%20and%20parameters%20measuring%20the%0Aendpoint%20retrieval%20recall%2C%20precision%2C%20and%20F1%20score.%20Then%2C%20we%20assess%20the%0ADiscovery%20Agent%20using%20the%20same%20test%20set.%20With%20our%20prototype%2C%20we%20demonstrate%20how%0Ato%20successfully%20employ%20RAG%20for%20endpoint%20discovery%20to%20reduce%20the%20token%20count.%0AWhile%20revealing%20high%20values%20for%20recall%2C%20precision%2C%20and%20F1%2C%20further%20research%20is%0Anecessary%20to%20retrieve%20all%20requisite%20endpoints.%20Our%20experiments%20show%20that%20for%0Apreprocessing%2C%20LLM-based%20and%20format-specific%20approaches%20outperform%20na%5C%22ive%0Achunking%20methods.%20Relying%20on%20an%20agent%20further%20enhances%20these%20results%20as%20the%0Aagent%20splits%20the%20tasks%20into%20multiple%20fine%20granular%20subtasks%2C%20improving%20the%0Aoverall%20RAG%20performance%20in%20the%20token%20count%2C%20precision%2C%20and%20F1%20score.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520System%2520Integration%253A%2520Analyzing%2520OpenAPI%2520Chunking%2520for%250A%2520%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DRobin%2520D.%2520Pesl%2520and%2520Jerin%2520G.%2520Mathew%2520and%2520Massimo%2520Mecella%2520and%2520Marco%2520Aiello%26entry.1292438233%3D%2520%2520Integrating%2520multiple%2520%2528sub-%2529systems%2520is%2520essential%2520to%2520create%2520advanced%250AInformation%2520Systems%2520%2528ISs%2529.%2520Difficulties%2520mainly%2520arise%2520when%2520integrating%2520dynamic%250Aenvironments%2520across%2520the%2520IS%2520lifecycle.%2520A%2520traditional%2520approach%2520is%2520a%2520registry%2520that%250Aprovides%2520the%2520API%2520documentation%2520of%2520the%2520systems%2527%2520endpoints.%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520have%2520shown%2520to%2520be%2520capable%2520of%2520automatically%2520creating%2520system%2520integrations%250A%2528e.g.%252C%2520as%2520service%2520composition%2529%2520based%2520on%2520this%2520documentation%2520but%2520require%2520concise%250Ainput%2520due%2520to%2520input%2520token%2520limitations%252C%2520especially%2520regarding%2520comprehensive%2520API%250Adescriptions.%2520Currently%252C%2520it%2520is%2520unknown%2520how%2520best%2520to%2520preprocess%2520these%2520API%250Adescriptions.%2520Within%2520this%2520work%252C%2520we%2520%2528i%2529%2520analyze%2520the%2520usage%2520of%2520Retrieval%2520Augmented%250AGeneration%2520%2528RAG%2529%2520for%2520endpoint%2520discovery%2520and%2520the%2520chunking%252C%2520i.e.%252C%2520preprocessing%252C%250Aof%2520OpenAPIs%2520to%2520reduce%2520the%2520input%2520token%2520length%2520while%2520preserving%2520the%2520most%2520relevant%250Ainformation.%2520To%2520further%2520reduce%2520the%2520input%2520token%2520length%2520for%2520the%2520composition%250Aprompt%2520and%2520improve%2520endpoint%2520retrieval%252C%2520we%2520propose%2520%2528ii%2529%2520a%2520Discovery%2520Agent%2520that%250Aonly%2520receives%2520a%2520summary%2520of%2520the%2520most%2520relevant%2520endpoints%2520and%2520retrieves%2520details%2520on%250Ademand.%2520We%2520evaluate%2520RAG%2520for%2520endpoint%2520discovery%2520using%2520the%2520RestBench%2520benchmark%252C%250Afirst%252C%2520for%2520the%2520different%2520chunking%2520possibilities%2520and%2520parameters%2520measuring%2520the%250Aendpoint%2520retrieval%2520recall%252C%2520precision%252C%2520and%2520F1%2520score.%2520Then%252C%2520we%2520assess%2520the%250ADiscovery%2520Agent%2520using%2520the%2520same%2520test%2520set.%2520With%2520our%2520prototype%252C%2520we%2520demonstrate%2520how%250Ato%2520successfully%2520employ%2520RAG%2520for%2520endpoint%2520discovery%2520to%2520reduce%2520the%2520token%2520count.%250AWhile%2520revealing%2520high%2520values%2520for%2520recall%252C%2520precision%252C%2520and%2520F1%252C%2520further%2520research%2520is%250Anecessary%2520to%2520retrieve%2520all%2520requisite%2520endpoints.%2520Our%2520experiments%2520show%2520that%2520for%250Apreprocessing%252C%2520LLM-based%2520and%2520format-specific%2520approaches%2520outperform%2520na%255C%2522ive%250Achunking%2520methods.%2520Relying%2520on%2520an%2520agent%2520further%2520enhances%2520these%2520results%2520as%2520the%250Aagent%2520splits%2520the%2520tasks%2520into%2520multiple%2520fine%2520granular%2520subtasks%252C%2520improving%2520the%250Aoverall%2520RAG%2520performance%2520in%2520the%2520token%2520count%252C%2520precision%252C%2520and%2520F1%2520score.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20System%20Integration%3A%20Analyzing%20OpenAPI%20Chunking%20for%0A%20%20Retrieval-Augmented%20Generation&entry.906535625=Robin%20D.%20Pesl%20and%20Jerin%20G.%20Mathew%20and%20Massimo%20Mecella%20and%20Marco%20Aiello&entry.1292438233=%20%20Integrating%20multiple%20%28sub-%29systems%20is%20essential%20to%20create%20advanced%0AInformation%20Systems%20%28ISs%29.%20Difficulties%20mainly%20arise%20when%20integrating%20dynamic%0Aenvironments%20across%20the%20IS%20lifecycle.%20A%20traditional%20approach%20is%20a%20registry%20that%0Aprovides%20the%20API%20documentation%20of%20the%20systems%27%20endpoints.%20Large%20Language%20Models%0A%28LLMs%29%20have%20shown%20to%20be%20capable%20of%20automatically%20creating%20system%20integrations%0A%28e.g.%2C%20as%20service%20composition%29%20based%20on%20this%20documentation%20but%20require%20concise%0Ainput%20due%20to%20input%20token%20limitations%2C%20especially%20regarding%20comprehensive%20API%0Adescriptions.%20Currently%2C%20it%20is%20unknown%20how%20best%20to%20preprocess%20these%20API%0Adescriptions.%20Within%20this%20work%2C%20we%20%28i%29%20analyze%20the%20usage%20of%20Retrieval%20Augmented%0AGeneration%20%28RAG%29%20for%20endpoint%20discovery%20and%20the%20chunking%2C%20i.e.%2C%20preprocessing%2C%0Aof%20OpenAPIs%20to%20reduce%20the%20input%20token%20length%20while%20preserving%20the%20most%20relevant%0Ainformation.%20To%20further%20reduce%20the%20input%20token%20length%20for%20the%20composition%0Aprompt%20and%20improve%20endpoint%20retrieval%2C%20we%20propose%20%28ii%29%20a%20Discovery%20Agent%20that%0Aonly%20receives%20a%20summary%20of%20the%20most%20relevant%20endpoints%20and%20retrieves%20details%20on%0Ademand.%20We%20evaluate%20RAG%20for%20endpoint%20discovery%20using%20the%20RestBench%20benchmark%2C%0Afirst%2C%20for%20the%20different%20chunking%20possibilities%20and%20parameters%20measuring%20the%0Aendpoint%20retrieval%20recall%2C%20precision%2C%20and%20F1%20score.%20Then%2C%20we%20assess%20the%0ADiscovery%20Agent%20using%20the%20same%20test%20set.%20With%20our%20prototype%2C%20we%20demonstrate%20how%0Ato%20successfully%20employ%20RAG%20for%20endpoint%20discovery%20to%20reduce%20the%20token%20count.%0AWhile%20revealing%20high%20values%20for%20recall%2C%20precision%2C%20and%20F1%2C%20further%20research%20is%0Anecessary%20to%20retrieve%20all%20requisite%20endpoints.%20Our%20experiments%20show%20that%20for%0Apreprocessing%2C%20LLM-based%20and%20format-specific%20approaches%20outperform%20na%5C%22ive%0Achunking%20methods.%20Relying%20on%20an%20agent%20further%20enhances%20these%20results%20as%20the%0Aagent%20splits%20the%20tasks%20into%20multiple%20fine%20granular%20subtasks%2C%20improving%20the%0Aoverall%20RAG%20performance%20in%20the%20token%20count%2C%20precision%2C%20and%20F1%20score.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19804v1&entry.124074799=Read"},
{"title": "An Interpretable Approach to Load Profile Forecasting in Power Grids\n  using Galerkin-Approximated Koopman Pseudospectra", "author": "Ali Tavasoli and Behnaz Moradijamei and Heman Shakeri", "abstract": "  This paper presents an interpretable machine learning approach that\ncharacterizes load dynamics within an operator-theoretic framework for\nelectricity load forecasting in power grids. We represent the dynamics of load\ndata using the Koopman operator, which provides a linear, infinite-dimensional\nrepresentation of the nonlinear dynamics, and approximate a finite version that\nremains robust against spectral pollutions due to truncation. By computing\n$\\epsilon$-approximate Koopman eigenfunctions using dynamics-adapted kernels in\ndelay coordinates, we decompose the load dynamics into coherent spatiotemporal\npatterns that evolve quasi-independently. Our approach captures temporal\ncoherent patterns due to seasonal changes and finer time scales, such as time\nof day and day of the week. This method allows for a more nuanced understanding\nof the complex interactions within power grids and their response to various\nexogenous factors. We assess our method using a large-scale dataset from a\nrenewable power system in the continental European electricity system. The\nresults indicate that our Koopman-based method surpasses a separately optimized\ndeep learning (LSTM) architecture in both accuracy and computational\nefficiency, while providing deeper insights into the underlying dynamics of the\npower grid\\footnote{The code is available at\n\\href{https://github.com/Shakeri-Lab/Power-Grids}{github.com/Shakeri-Lab/Power-Grids}.\n", "link": "http://arxiv.org/abs/2304.07832v2", "date": "2024-11-29", "relevancy": 1.2738, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4598}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4155}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Interpretable%20Approach%20to%20Load%20Profile%20Forecasting%20in%20Power%20Grids%0A%20%20using%20Galerkin-Approximated%20Koopman%20Pseudospectra&body=Title%3A%20An%20Interpretable%20Approach%20to%20Load%20Profile%20Forecasting%20in%20Power%20Grids%0A%20%20using%20Galerkin-Approximated%20Koopman%20Pseudospectra%0AAuthor%3A%20Ali%20Tavasoli%20and%20Behnaz%20Moradijamei%20and%20Heman%20Shakeri%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20interpretable%20machine%20learning%20approach%20that%0Acharacterizes%20load%20dynamics%20within%20an%20operator-theoretic%20framework%20for%0Aelectricity%20load%20forecasting%20in%20power%20grids.%20We%20represent%20the%20dynamics%20of%20load%0Adata%20using%20the%20Koopman%20operator%2C%20which%20provides%20a%20linear%2C%20infinite-dimensional%0Arepresentation%20of%20the%20nonlinear%20dynamics%2C%20and%20approximate%20a%20finite%20version%20that%0Aremains%20robust%20against%20spectral%20pollutions%20due%20to%20truncation.%20By%20computing%0A%24%5Cepsilon%24-approximate%20Koopman%20eigenfunctions%20using%20dynamics-adapted%20kernels%20in%0Adelay%20coordinates%2C%20we%20decompose%20the%20load%20dynamics%20into%20coherent%20spatiotemporal%0Apatterns%20that%20evolve%20quasi-independently.%20Our%20approach%20captures%20temporal%0Acoherent%20patterns%20due%20to%20seasonal%20changes%20and%20finer%20time%20scales%2C%20such%20as%20time%0Aof%20day%20and%20day%20of%20the%20week.%20This%20method%20allows%20for%20a%20more%20nuanced%20understanding%0Aof%20the%20complex%20interactions%20within%20power%20grids%20and%20their%20response%20to%20various%0Aexogenous%20factors.%20We%20assess%20our%20method%20using%20a%20large-scale%20dataset%20from%20a%0Arenewable%20power%20system%20in%20the%20continental%20European%20electricity%20system.%20The%0Aresults%20indicate%20that%20our%20Koopman-based%20method%20surpasses%20a%20separately%20optimized%0Adeep%20learning%20%28LSTM%29%20architecture%20in%20both%20accuracy%20and%20computational%0Aefficiency%2C%20while%20providing%20deeper%20insights%20into%20the%20underlying%20dynamics%20of%20the%0Apower%20grid%5Cfootnote%7BThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Shakeri-Lab/Power-Grids%7D%7Bgithub.com/Shakeri-Lab/Power-Grids%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.07832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Interpretable%2520Approach%2520to%2520Load%2520Profile%2520Forecasting%2520in%2520Power%2520Grids%250A%2520%2520using%2520Galerkin-Approximated%2520Koopman%2520Pseudospectra%26entry.906535625%3DAli%2520Tavasoli%2520and%2520Behnaz%2520Moradijamei%2520and%2520Heman%2520Shakeri%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520interpretable%2520machine%2520learning%2520approach%2520that%250Acharacterizes%2520load%2520dynamics%2520within%2520an%2520operator-theoretic%2520framework%2520for%250Aelectricity%2520load%2520forecasting%2520in%2520power%2520grids.%2520We%2520represent%2520the%2520dynamics%2520of%2520load%250Adata%2520using%2520the%2520Koopman%2520operator%252C%2520which%2520provides%2520a%2520linear%252C%2520infinite-dimensional%250Arepresentation%2520of%2520the%2520nonlinear%2520dynamics%252C%2520and%2520approximate%2520a%2520finite%2520version%2520that%250Aremains%2520robust%2520against%2520spectral%2520pollutions%2520due%2520to%2520truncation.%2520By%2520computing%250A%2524%255Cepsilon%2524-approximate%2520Koopman%2520eigenfunctions%2520using%2520dynamics-adapted%2520kernels%2520in%250Adelay%2520coordinates%252C%2520we%2520decompose%2520the%2520load%2520dynamics%2520into%2520coherent%2520spatiotemporal%250Apatterns%2520that%2520evolve%2520quasi-independently.%2520Our%2520approach%2520captures%2520temporal%250Acoherent%2520patterns%2520due%2520to%2520seasonal%2520changes%2520and%2520finer%2520time%2520scales%252C%2520such%2520as%2520time%250Aof%2520day%2520and%2520day%2520of%2520the%2520week.%2520This%2520method%2520allows%2520for%2520a%2520more%2520nuanced%2520understanding%250Aof%2520the%2520complex%2520interactions%2520within%2520power%2520grids%2520and%2520their%2520response%2520to%2520various%250Aexogenous%2520factors.%2520We%2520assess%2520our%2520method%2520using%2520a%2520large-scale%2520dataset%2520from%2520a%250Arenewable%2520power%2520system%2520in%2520the%2520continental%2520European%2520electricity%2520system.%2520The%250Aresults%2520indicate%2520that%2520our%2520Koopman-based%2520method%2520surpasses%2520a%2520separately%2520optimized%250Adeep%2520learning%2520%2528LSTM%2529%2520architecture%2520in%2520both%2520accuracy%2520and%2520computational%250Aefficiency%252C%2520while%2520providing%2520deeper%2520insights%2520into%2520the%2520underlying%2520dynamics%2520of%2520the%250Apower%2520grid%255Cfootnote%257BThe%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/Shakeri-Lab/Power-Grids%257D%257Bgithub.com/Shakeri-Lab/Power-Grids%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.07832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Interpretable%20Approach%20to%20Load%20Profile%20Forecasting%20in%20Power%20Grids%0A%20%20using%20Galerkin-Approximated%20Koopman%20Pseudospectra&entry.906535625=Ali%20Tavasoli%20and%20Behnaz%20Moradijamei%20and%20Heman%20Shakeri&entry.1292438233=%20%20This%20paper%20presents%20an%20interpretable%20machine%20learning%20approach%20that%0Acharacterizes%20load%20dynamics%20within%20an%20operator-theoretic%20framework%20for%0Aelectricity%20load%20forecasting%20in%20power%20grids.%20We%20represent%20the%20dynamics%20of%20load%0Adata%20using%20the%20Koopman%20operator%2C%20which%20provides%20a%20linear%2C%20infinite-dimensional%0Arepresentation%20of%20the%20nonlinear%20dynamics%2C%20and%20approximate%20a%20finite%20version%20that%0Aremains%20robust%20against%20spectral%20pollutions%20due%20to%20truncation.%20By%20computing%0A%24%5Cepsilon%24-approximate%20Koopman%20eigenfunctions%20using%20dynamics-adapted%20kernels%20in%0Adelay%20coordinates%2C%20we%20decompose%20the%20load%20dynamics%20into%20coherent%20spatiotemporal%0Apatterns%20that%20evolve%20quasi-independently.%20Our%20approach%20captures%20temporal%0Acoherent%20patterns%20due%20to%20seasonal%20changes%20and%20finer%20time%20scales%2C%20such%20as%20time%0Aof%20day%20and%20day%20of%20the%20week.%20This%20method%20allows%20for%20a%20more%20nuanced%20understanding%0Aof%20the%20complex%20interactions%20within%20power%20grids%20and%20their%20response%20to%20various%0Aexogenous%20factors.%20We%20assess%20our%20method%20using%20a%20large-scale%20dataset%20from%20a%0Arenewable%20power%20system%20in%20the%20continental%20European%20electricity%20system.%20The%0Aresults%20indicate%20that%20our%20Koopman-based%20method%20surpasses%20a%20separately%20optimized%0Adeep%20learning%20%28LSTM%29%20architecture%20in%20both%20accuracy%20and%20computational%0Aefficiency%2C%20while%20providing%20deeper%20insights%20into%20the%20underlying%20dynamics%20of%20the%0Apower%20grid%5Cfootnote%7BThe%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/Shakeri-Lab/Power-Grids%7D%7Bgithub.com/Shakeri-Lab/Power-Grids%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.07832v2&entry.124074799=Read"},
{"title": "What Is Fairness? On the Role of Protected Attributes and Fictitious\n  Worlds", "author": "Ludwig Bothmann and Kristina Peters and Bernd Bischl", "abstract": "  A growing body of literature in fairness-aware machine learning (fairML) aims\nto mitigate machine learning (ML)-related unfairness in automated\ndecision-making (ADM) by defining metrics that measure fairness of an ML model\nand by proposing methods to ensure that trained ML models achieve low scores on\nthese metrics. However, the underlying concept of fairness, i.e., the question\nof what fairness is, is rarely discussed, leaving a significant gap between\ncenturies of philosophical discussion and the recent adoption of the concept in\nthe ML community. In this work, we try to bridge this gap by formalizing a\nconsistent concept of fairness and by translating the philosophical\nconsiderations into a formal framework for the training and evaluation of ML\nmodels in ADM systems. We argue that fairness problems can arise even without\nthe presence of protected attributes (PAs), and point out that fairness and\npredictive performance are not irreconcilable opposites, but that the latter is\nnecessary to achieve the former. Furthermore, we argue why and how causal\nconsiderations are necessary when assessing fairness in the presence of PAs by\nproposing a fictitious, normatively desired (FiND) world in which PAs have no\ncausal effects. In practice, this FiND world must be approximated by a warped\nworld in which the causal effects of the PAs are removed from the real-world\ndata. Finally, we achieve greater linguistic clarity in the discussion of\nfairML. We outline algorithms for practical applications and present\nillustrative experiments on COMPAS data.\n", "link": "http://arxiv.org/abs/2205.09622v6", "date": "2024-11-29", "relevancy": 1.3522, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4403}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Is%20Fairness%3F%20On%20the%20Role%20of%20Protected%20Attributes%20and%20Fictitious%0A%20%20Worlds&body=Title%3A%20What%20Is%20Fairness%3F%20On%20the%20Role%20of%20Protected%20Attributes%20and%20Fictitious%0A%20%20Worlds%0AAuthor%3A%20Ludwig%20Bothmann%20and%20Kristina%20Peters%20and%20Bernd%20Bischl%0AAbstract%3A%20%20%20A%20growing%20body%20of%20literature%20in%20fairness-aware%20machine%20learning%20%28fairML%29%20aims%0Ato%20mitigate%20machine%20learning%20%28ML%29-related%20unfairness%20in%20automated%0Adecision-making%20%28ADM%29%20by%20defining%20metrics%20that%20measure%20fairness%20of%20an%20ML%20model%0Aand%20by%20proposing%20methods%20to%20ensure%20that%20trained%20ML%20models%20achieve%20low%20scores%20on%0Athese%20metrics.%20However%2C%20the%20underlying%20concept%20of%20fairness%2C%20i.e.%2C%20the%20question%0Aof%20what%20fairness%20is%2C%20is%20rarely%20discussed%2C%20leaving%20a%20significant%20gap%20between%0Acenturies%20of%20philosophical%20discussion%20and%20the%20recent%20adoption%20of%20the%20concept%20in%0Athe%20ML%20community.%20In%20this%20work%2C%20we%20try%20to%20bridge%20this%20gap%20by%20formalizing%20a%0Aconsistent%20concept%20of%20fairness%20and%20by%20translating%20the%20philosophical%0Aconsiderations%20into%20a%20formal%20framework%20for%20the%20training%20and%20evaluation%20of%20ML%0Amodels%20in%20ADM%20systems.%20We%20argue%20that%20fairness%20problems%20can%20arise%20even%20without%0Athe%20presence%20of%20protected%20attributes%20%28PAs%29%2C%20and%20point%20out%20that%20fairness%20and%0Apredictive%20performance%20are%20not%20irreconcilable%20opposites%2C%20but%20that%20the%20latter%20is%0Anecessary%20to%20achieve%20the%20former.%20Furthermore%2C%20we%20argue%20why%20and%20how%20causal%0Aconsiderations%20are%20necessary%20when%20assessing%20fairness%20in%20the%20presence%20of%20PAs%20by%0Aproposing%20a%20fictitious%2C%20normatively%20desired%20%28FiND%29%20world%20in%20which%20PAs%20have%20no%0Acausal%20effects.%20In%20practice%2C%20this%20FiND%20world%20must%20be%20approximated%20by%20a%20warped%0Aworld%20in%20which%20the%20causal%20effects%20of%20the%20PAs%20are%20removed%20from%20the%20real-world%0Adata.%20Finally%2C%20we%20achieve%20greater%20linguistic%20clarity%20in%20the%20discussion%20of%0AfairML.%20We%20outline%20algorithms%20for%20practical%20applications%20and%20present%0Aillustrative%20experiments%20on%20COMPAS%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.09622v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Is%2520Fairness%253F%2520On%2520the%2520Role%2520of%2520Protected%2520Attributes%2520and%2520Fictitious%250A%2520%2520Worlds%26entry.906535625%3DLudwig%2520Bothmann%2520and%2520Kristina%2520Peters%2520and%2520Bernd%2520Bischl%26entry.1292438233%3D%2520%2520A%2520growing%2520body%2520of%2520literature%2520in%2520fairness-aware%2520machine%2520learning%2520%2528fairML%2529%2520aims%250Ato%2520mitigate%2520machine%2520learning%2520%2528ML%2529-related%2520unfairness%2520in%2520automated%250Adecision-making%2520%2528ADM%2529%2520by%2520defining%2520metrics%2520that%2520measure%2520fairness%2520of%2520an%2520ML%2520model%250Aand%2520by%2520proposing%2520methods%2520to%2520ensure%2520that%2520trained%2520ML%2520models%2520achieve%2520low%2520scores%2520on%250Athese%2520metrics.%2520However%252C%2520the%2520underlying%2520concept%2520of%2520fairness%252C%2520i.e.%252C%2520the%2520question%250Aof%2520what%2520fairness%2520is%252C%2520is%2520rarely%2520discussed%252C%2520leaving%2520a%2520significant%2520gap%2520between%250Acenturies%2520of%2520philosophical%2520discussion%2520and%2520the%2520recent%2520adoption%2520of%2520the%2520concept%2520in%250Athe%2520ML%2520community.%2520In%2520this%2520work%252C%2520we%2520try%2520to%2520bridge%2520this%2520gap%2520by%2520formalizing%2520a%250Aconsistent%2520concept%2520of%2520fairness%2520and%2520by%2520translating%2520the%2520philosophical%250Aconsiderations%2520into%2520a%2520formal%2520framework%2520for%2520the%2520training%2520and%2520evaluation%2520of%2520ML%250Amodels%2520in%2520ADM%2520systems.%2520We%2520argue%2520that%2520fairness%2520problems%2520can%2520arise%2520even%2520without%250Athe%2520presence%2520of%2520protected%2520attributes%2520%2528PAs%2529%252C%2520and%2520point%2520out%2520that%2520fairness%2520and%250Apredictive%2520performance%2520are%2520not%2520irreconcilable%2520opposites%252C%2520but%2520that%2520the%2520latter%2520is%250Anecessary%2520to%2520achieve%2520the%2520former.%2520Furthermore%252C%2520we%2520argue%2520why%2520and%2520how%2520causal%250Aconsiderations%2520are%2520necessary%2520when%2520assessing%2520fairness%2520in%2520the%2520presence%2520of%2520PAs%2520by%250Aproposing%2520a%2520fictitious%252C%2520normatively%2520desired%2520%2528FiND%2529%2520world%2520in%2520which%2520PAs%2520have%2520no%250Acausal%2520effects.%2520In%2520practice%252C%2520this%2520FiND%2520world%2520must%2520be%2520approximated%2520by%2520a%2520warped%250Aworld%2520in%2520which%2520the%2520causal%2520effects%2520of%2520the%2520PAs%2520are%2520removed%2520from%2520the%2520real-world%250Adata.%2520Finally%252C%2520we%2520achieve%2520greater%2520linguistic%2520clarity%2520in%2520the%2520discussion%2520of%250AfairML.%2520We%2520outline%2520algorithms%2520for%2520practical%2520applications%2520and%2520present%250Aillustrative%2520experiments%2520on%2520COMPAS%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.09622v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Is%20Fairness%3F%20On%20the%20Role%20of%20Protected%20Attributes%20and%20Fictitious%0A%20%20Worlds&entry.906535625=Ludwig%20Bothmann%20and%20Kristina%20Peters%20and%20Bernd%20Bischl&entry.1292438233=%20%20A%20growing%20body%20of%20literature%20in%20fairness-aware%20machine%20learning%20%28fairML%29%20aims%0Ato%20mitigate%20machine%20learning%20%28ML%29-related%20unfairness%20in%20automated%0Adecision-making%20%28ADM%29%20by%20defining%20metrics%20that%20measure%20fairness%20of%20an%20ML%20model%0Aand%20by%20proposing%20methods%20to%20ensure%20that%20trained%20ML%20models%20achieve%20low%20scores%20on%0Athese%20metrics.%20However%2C%20the%20underlying%20concept%20of%20fairness%2C%20i.e.%2C%20the%20question%0Aof%20what%20fairness%20is%2C%20is%20rarely%20discussed%2C%20leaving%20a%20significant%20gap%20between%0Acenturies%20of%20philosophical%20discussion%20and%20the%20recent%20adoption%20of%20the%20concept%20in%0Athe%20ML%20community.%20In%20this%20work%2C%20we%20try%20to%20bridge%20this%20gap%20by%20formalizing%20a%0Aconsistent%20concept%20of%20fairness%20and%20by%20translating%20the%20philosophical%0Aconsiderations%20into%20a%20formal%20framework%20for%20the%20training%20and%20evaluation%20of%20ML%0Amodels%20in%20ADM%20systems.%20We%20argue%20that%20fairness%20problems%20can%20arise%20even%20without%0Athe%20presence%20of%20protected%20attributes%20%28PAs%29%2C%20and%20point%20out%20that%20fairness%20and%0Apredictive%20performance%20are%20not%20irreconcilable%20opposites%2C%20but%20that%20the%20latter%20is%0Anecessary%20to%20achieve%20the%20former.%20Furthermore%2C%20we%20argue%20why%20and%20how%20causal%0Aconsiderations%20are%20necessary%20when%20assessing%20fairness%20in%20the%20presence%20of%20PAs%20by%0Aproposing%20a%20fictitious%2C%20normatively%20desired%20%28FiND%29%20world%20in%20which%20PAs%20have%20no%0Acausal%20effects.%20In%20practice%2C%20this%20FiND%20world%20must%20be%20approximated%20by%20a%20warped%0Aworld%20in%20which%20the%20causal%20effects%20of%20the%20PAs%20are%20removed%20from%20the%20real-world%0Adata.%20Finally%2C%20we%20achieve%20greater%20linguistic%20clarity%20in%20the%20discussion%20of%0AfairML.%20We%20outline%20algorithms%20for%20practical%20applications%20and%20present%0Aillustrative%20experiments%20on%20COMPAS%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.09622v6&entry.124074799=Read"},
{"title": "Nonparametric Instrumental Regression via Kernel Methods is Minimax\n  Optimal", "author": "Dimitri Meunier and Zhu Li and Tim Christensen and Arthur Gretton", "abstract": "  We study the kernel instrumental variable algorithm of\n\\citet{singh2019kernel}, a nonparametric two-stage least squares (2SLS)\nprocedure which has demonstrated strong empirical performance. We provide a\nconvergence analysis that covers both the identified and unidentified settings:\nwhen the structural function cannot be identified, we show that the kernel NPIV\nestimator converges to the IV solution with minimum norm. Crucially, our\nconvergence is with respect to the strong $L_2$-norm, rather than a\npseudo-norm. Additionally, we characterize the smoothness of the target\nfunction without relying on the instrument, instead leveraging a new\ndescription of the projected subspace size (this being closely related to the\nlink condition in inverse learning literature). With the subspace size\ndescription and under standard kernel learning assumptions, we derive, for the\nfirst time, the minimax optimal learning rate for kernel NPIV in the strong\n$L_2$-norm. Our result demonstrates that the strength of the instrument is\nessential to achieve efficient learning. We also improve the original kernel\nNPIV algorithm by adopting a general spectral regularization in stage 1\nregression. The modified regularization can overcome the saturation effect of\nTikhonov regularization.\n", "link": "http://arxiv.org/abs/2411.19653v1", "date": "2024-11-29", "relevancy": 1.5227, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3872}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3843}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonparametric%20Instrumental%20Regression%20via%20Kernel%20Methods%20is%20Minimax%0A%20%20Optimal&body=Title%3A%20Nonparametric%20Instrumental%20Regression%20via%20Kernel%20Methods%20is%20Minimax%0A%20%20Optimal%0AAuthor%3A%20Dimitri%20Meunier%20and%20Zhu%20Li%20and%20Tim%20Christensen%20and%20Arthur%20Gretton%0AAbstract%3A%20%20%20We%20study%20the%20kernel%20instrumental%20variable%20algorithm%20of%0A%5Ccitet%7Bsingh2019kernel%7D%2C%20a%20nonparametric%20two-stage%20least%20squares%20%282SLS%29%0Aprocedure%20which%20has%20demonstrated%20strong%20empirical%20performance.%20We%20provide%20a%0Aconvergence%20analysis%20that%20covers%20both%20the%20identified%20and%20unidentified%20settings%3A%0Awhen%20the%20structural%20function%20cannot%20be%20identified%2C%20we%20show%20that%20the%20kernel%20NPIV%0Aestimator%20converges%20to%20the%20IV%20solution%20with%20minimum%20norm.%20Crucially%2C%20our%0Aconvergence%20is%20with%20respect%20to%20the%20strong%20%24L_2%24-norm%2C%20rather%20than%20a%0Apseudo-norm.%20Additionally%2C%20we%20characterize%20the%20smoothness%20of%20the%20target%0Afunction%20without%20relying%20on%20the%20instrument%2C%20instead%20leveraging%20a%20new%0Adescription%20of%20the%20projected%20subspace%20size%20%28this%20being%20closely%20related%20to%20the%0Alink%20condition%20in%20inverse%20learning%20literature%29.%20With%20the%20subspace%20size%0Adescription%20and%20under%20standard%20kernel%20learning%20assumptions%2C%20we%20derive%2C%20for%20the%0Afirst%20time%2C%20the%20minimax%20optimal%20learning%20rate%20for%20kernel%20NPIV%20in%20the%20strong%0A%24L_2%24-norm.%20Our%20result%20demonstrates%20that%20the%20strength%20of%20the%20instrument%20is%0Aessential%20to%20achieve%20efficient%20learning.%20We%20also%20improve%20the%20original%20kernel%0ANPIV%20algorithm%20by%20adopting%20a%20general%20spectral%20regularization%20in%20stage%201%0Aregression.%20The%20modified%20regularization%20can%20overcome%20the%20saturation%20effect%20of%0ATikhonov%20regularization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonparametric%2520Instrumental%2520Regression%2520via%2520Kernel%2520Methods%2520is%2520Minimax%250A%2520%2520Optimal%26entry.906535625%3DDimitri%2520Meunier%2520and%2520Zhu%2520Li%2520and%2520Tim%2520Christensen%2520and%2520Arthur%2520Gretton%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520kernel%2520instrumental%2520variable%2520algorithm%2520of%250A%255Ccitet%257Bsingh2019kernel%257D%252C%2520a%2520nonparametric%2520two-stage%2520least%2520squares%2520%25282SLS%2529%250Aprocedure%2520which%2520has%2520demonstrated%2520strong%2520empirical%2520performance.%2520We%2520provide%2520a%250Aconvergence%2520analysis%2520that%2520covers%2520both%2520the%2520identified%2520and%2520unidentified%2520settings%253A%250Awhen%2520the%2520structural%2520function%2520cannot%2520be%2520identified%252C%2520we%2520show%2520that%2520the%2520kernel%2520NPIV%250Aestimator%2520converges%2520to%2520the%2520IV%2520solution%2520with%2520minimum%2520norm.%2520Crucially%252C%2520our%250Aconvergence%2520is%2520with%2520respect%2520to%2520the%2520strong%2520%2524L_2%2524-norm%252C%2520rather%2520than%2520a%250Apseudo-norm.%2520Additionally%252C%2520we%2520characterize%2520the%2520smoothness%2520of%2520the%2520target%250Afunction%2520without%2520relying%2520on%2520the%2520instrument%252C%2520instead%2520leveraging%2520a%2520new%250Adescription%2520of%2520the%2520projected%2520subspace%2520size%2520%2528this%2520being%2520closely%2520related%2520to%2520the%250Alink%2520condition%2520in%2520inverse%2520learning%2520literature%2529.%2520With%2520the%2520subspace%2520size%250Adescription%2520and%2520under%2520standard%2520kernel%2520learning%2520assumptions%252C%2520we%2520derive%252C%2520for%2520the%250Afirst%2520time%252C%2520the%2520minimax%2520optimal%2520learning%2520rate%2520for%2520kernel%2520NPIV%2520in%2520the%2520strong%250A%2524L_2%2524-norm.%2520Our%2520result%2520demonstrates%2520that%2520the%2520strength%2520of%2520the%2520instrument%2520is%250Aessential%2520to%2520achieve%2520efficient%2520learning.%2520We%2520also%2520improve%2520the%2520original%2520kernel%250ANPIV%2520algorithm%2520by%2520adopting%2520a%2520general%2520spectral%2520regularization%2520in%2520stage%25201%250Aregression.%2520The%2520modified%2520regularization%2520can%2520overcome%2520the%2520saturation%2520effect%2520of%250ATikhonov%2520regularization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonparametric%20Instrumental%20Regression%20via%20Kernel%20Methods%20is%20Minimax%0A%20%20Optimal&entry.906535625=Dimitri%20Meunier%20and%20Zhu%20Li%20and%20Tim%20Christensen%20and%20Arthur%20Gretton&entry.1292438233=%20%20We%20study%20the%20kernel%20instrumental%20variable%20algorithm%20of%0A%5Ccitet%7Bsingh2019kernel%7D%2C%20a%20nonparametric%20two-stage%20least%20squares%20%282SLS%29%0Aprocedure%20which%20has%20demonstrated%20strong%20empirical%20performance.%20We%20provide%20a%0Aconvergence%20analysis%20that%20covers%20both%20the%20identified%20and%20unidentified%20settings%3A%0Awhen%20the%20structural%20function%20cannot%20be%20identified%2C%20we%20show%20that%20the%20kernel%20NPIV%0Aestimator%20converges%20to%20the%20IV%20solution%20with%20minimum%20norm.%20Crucially%2C%20our%0Aconvergence%20is%20with%20respect%20to%20the%20strong%20%24L_2%24-norm%2C%20rather%20than%20a%0Apseudo-norm.%20Additionally%2C%20we%20characterize%20the%20smoothness%20of%20the%20target%0Afunction%20without%20relying%20on%20the%20instrument%2C%20instead%20leveraging%20a%20new%0Adescription%20of%20the%20projected%20subspace%20size%20%28this%20being%20closely%20related%20to%20the%0Alink%20condition%20in%20inverse%20learning%20literature%29.%20With%20the%20subspace%20size%0Adescription%20and%20under%20standard%20kernel%20learning%20assumptions%2C%20we%20derive%2C%20for%20the%0Afirst%20time%2C%20the%20minimax%20optimal%20learning%20rate%20for%20kernel%20NPIV%20in%20the%20strong%0A%24L_2%24-norm.%20Our%20result%20demonstrates%20that%20the%20strength%20of%20the%20instrument%20is%0Aessential%20to%20achieve%20efficient%20learning.%20We%20also%20improve%20the%20original%20kernel%0ANPIV%20algorithm%20by%20adopting%20a%20general%20spectral%20regularization%20in%20stage%201%0Aregression.%20The%20modified%20regularization%20can%20overcome%20the%20saturation%20effect%20of%0ATikhonov%20regularization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19653v1&entry.124074799=Read"},
{"title": "Hybrid Architecture for Real-Time Video Anomaly Detection: Integrating\n  Spatial and Temporal Analysis", "author": "Fabien Poirier", "abstract": "  In this paper, we propose a new architecture for real-time anomaly detection\nin video data, inspired by human behavior combining spatial and temporal\nanalyses. This approach uses two distinct models: (i) for temporal analysis, a\nrecurrent convolutional network (CNN + RNN) is employed, associating VGG19 and\na GRU to process video sequences; (ii) regarding spatial analysis, it is\nperformed using YOLOv7 to analyze individual images. These two analyses can be\ncarried out either in parallel, with a final prediction that combines the\nresults of both analysis, or in series, where the spatial analysis enriches the\ndata before the temporal analysis. Some experimentations are been made to\ncompare these two architectural configurations with each other, and evaluate\nthe effectiveness of our hybrid approach in video anomaly detection.\n", "link": "http://arxiv.org/abs/2410.15909v3", "date": "2024-11-29", "relevancy": 1.6388, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5645}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5413}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Architecture%20for%20Real-Time%20Video%20Anomaly%20Detection%3A%20Integrating%0A%20%20Spatial%20and%20Temporal%20Analysis&body=Title%3A%20Hybrid%20Architecture%20for%20Real-Time%20Video%20Anomaly%20Detection%3A%20Integrating%0A%20%20Spatial%20and%20Temporal%20Analysis%0AAuthor%3A%20Fabien%20Poirier%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%20for%20real-time%20anomaly%20detection%0Ain%20video%20data%2C%20inspired%20by%20human%20behavior%20combining%20spatial%20and%20temporal%0Aanalyses.%20This%20approach%20uses%20two%20distinct%20models%3A%20%28i%29%20for%20temporal%20analysis%2C%20a%0Arecurrent%20convolutional%20network%20%28CNN%20%2B%20RNN%29%20is%20employed%2C%20associating%20VGG19%20and%0Aa%20GRU%20to%20process%20video%20sequences%3B%20%28ii%29%20regarding%20spatial%20analysis%2C%20it%20is%0Aperformed%20using%20YOLOv7%20to%20analyze%20individual%20images.%20These%20two%20analyses%20can%20be%0Acarried%20out%20either%20in%20parallel%2C%20with%20a%20final%20prediction%20that%20combines%20the%0Aresults%20of%20both%20analysis%2C%20or%20in%20series%2C%20where%20the%20spatial%20analysis%20enriches%20the%0Adata%20before%20the%20temporal%20analysis.%20Some%20experimentations%20are%20been%20made%20to%0Acompare%20these%20two%20architectural%20configurations%20with%20each%20other%2C%20and%20evaluate%0Athe%20effectiveness%20of%20our%20hybrid%20approach%20in%20video%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15909v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Architecture%2520for%2520Real-Time%2520Video%2520Anomaly%2520Detection%253A%2520Integrating%250A%2520%2520Spatial%2520and%2520Temporal%2520Analysis%26entry.906535625%3DFabien%2520Poirier%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520architecture%2520for%2520real-time%2520anomaly%2520detection%250Ain%2520video%2520data%252C%2520inspired%2520by%2520human%2520behavior%2520combining%2520spatial%2520and%2520temporal%250Aanalyses.%2520This%2520approach%2520uses%2520two%2520distinct%2520models%253A%2520%2528i%2529%2520for%2520temporal%2520analysis%252C%2520a%250Arecurrent%2520convolutional%2520network%2520%2528CNN%2520%252B%2520RNN%2529%2520is%2520employed%252C%2520associating%2520VGG19%2520and%250Aa%2520GRU%2520to%2520process%2520video%2520sequences%253B%2520%2528ii%2529%2520regarding%2520spatial%2520analysis%252C%2520it%2520is%250Aperformed%2520using%2520YOLOv7%2520to%2520analyze%2520individual%2520images.%2520These%2520two%2520analyses%2520can%2520be%250Acarried%2520out%2520either%2520in%2520parallel%252C%2520with%2520a%2520final%2520prediction%2520that%2520combines%2520the%250Aresults%2520of%2520both%2520analysis%252C%2520or%2520in%2520series%252C%2520where%2520the%2520spatial%2520analysis%2520enriches%2520the%250Adata%2520before%2520the%2520temporal%2520analysis.%2520Some%2520experimentations%2520are%2520been%2520made%2520to%250Acompare%2520these%2520two%2520architectural%2520configurations%2520with%2520each%2520other%252C%2520and%2520evaluate%250Athe%2520effectiveness%2520of%2520our%2520hybrid%2520approach%2520in%2520video%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15909v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Architecture%20for%20Real-Time%20Video%20Anomaly%20Detection%3A%20Integrating%0A%20%20Spatial%20and%20Temporal%20Analysis&entry.906535625=Fabien%20Poirier&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20architecture%20for%20real-time%20anomaly%20detection%0Ain%20video%20data%2C%20inspired%20by%20human%20behavior%20combining%20spatial%20and%20temporal%0Aanalyses.%20This%20approach%20uses%20two%20distinct%20models%3A%20%28i%29%20for%20temporal%20analysis%2C%20a%0Arecurrent%20convolutional%20network%20%28CNN%20%2B%20RNN%29%20is%20employed%2C%20associating%20VGG19%20and%0Aa%20GRU%20to%20process%20video%20sequences%3B%20%28ii%29%20regarding%20spatial%20analysis%2C%20it%20is%0Aperformed%20using%20YOLOv7%20to%20analyze%20individual%20images.%20These%20two%20analyses%20can%20be%0Acarried%20out%20either%20in%20parallel%2C%20with%20a%20final%20prediction%20that%20combines%20the%0Aresults%20of%20both%20analysis%2C%20or%20in%20series%2C%20where%20the%20spatial%20analysis%20enriches%20the%0Adata%20before%20the%20temporal%20analysis.%20Some%20experimentations%20are%20been%20made%20to%0Acompare%20these%20two%20architectural%20configurations%20with%20each%20other%2C%20and%20evaluate%0Athe%20effectiveness%20of%20our%20hybrid%20approach%20in%20video%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15909v3&entry.124074799=Read"},
{"title": "MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks", "author": "Yiming Wu and Wei Ji and Kecheng Zheng and Zicheng Wang and Dong Xu", "abstract": "  Recently, human motion analysis has experienced great improvement due to\ninspiring generative models such as the denoising diffusion model and large\nlanguage model. While the existing approaches mainly focus on generating\nmotions with textual descriptions and overlook the reciprocal task. In this\npaper, we present~\\textbf{MoTe}, a unified multi-modal model that could handle\ndiverse tasks by learning the marginal, conditional, and joint distributions of\nmotion and text simultaneously. MoTe enables us to handle the paired\ntext-motion generation, motion captioning, and text-driven motion generation by\nsimply modifying the input context. Specifically, MoTe is composed of three\ncomponents: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and\nMoti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for\nextracting latent embeddings, and subsequently reconstructing the motion\nsequences and textual descriptions from the extracted embeddings, respectively.\nMTDM, on the other hand, performs an iterative denoising process on the input\ncontext to handle diverse tasks. Experimental results on the benchmark datasets\ndemonstrate the superior performance of our proposed method on text-to-motion\ngeneration and competitive performance on motion captioning.\n", "link": "http://arxiv.org/abs/2411.19786v1", "date": "2024-11-29", "relevancy": 1.7846, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6406}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5855}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoTe%3A%20Learning%20Motion-Text%20Diffusion%20Model%20for%20Multiple%20Generation%20Tasks&body=Title%3A%20MoTe%3A%20Learning%20Motion-Text%20Diffusion%20Model%20for%20Multiple%20Generation%20Tasks%0AAuthor%3A%20Yiming%20Wu%20and%20Wei%20Ji%20and%20Kecheng%20Zheng%20and%20Zicheng%20Wang%20and%20Dong%20Xu%0AAbstract%3A%20%20%20Recently%2C%20human%20motion%20analysis%20has%20experienced%20great%20improvement%20due%20to%0Ainspiring%20generative%20models%20such%20as%20the%20denoising%20diffusion%20model%20and%20large%0Alanguage%20model.%20While%20the%20existing%20approaches%20mainly%20focus%20on%20generating%0Amotions%20with%20textual%20descriptions%20and%20overlook%20the%20reciprocal%20task.%20In%20this%0Apaper%2C%20we%20present~%5Ctextbf%7BMoTe%7D%2C%20a%20unified%20multi-modal%20model%20that%20could%20handle%0Adiverse%20tasks%20by%20learning%20the%20marginal%2C%20conditional%2C%20and%20joint%20distributions%20of%0Amotion%20and%20text%20simultaneously.%20MoTe%20enables%20us%20to%20handle%20the%20paired%0Atext-motion%20generation%2C%20motion%20captioning%2C%20and%20text-driven%20motion%20generation%20by%0Asimply%20modifying%20the%20input%20context.%20Specifically%2C%20MoTe%20is%20composed%20of%20three%0Acomponents%3A%20Motion%20Encoder-Decoder%20%28MED%29%2C%20Text%20Encoder-Decoder%20%28TED%29%2C%20and%0AMoti-on-Text%20Diffusion%20Model%20%28MTDM%29.%20In%20particular%2C%20MED%20and%20TED%20are%20trained%20for%0Aextracting%20latent%20embeddings%2C%20and%20subsequently%20reconstructing%20the%20motion%0Asequences%20and%20textual%20descriptions%20from%20the%20extracted%20embeddings%2C%20respectively.%0AMTDM%2C%20on%20the%20other%20hand%2C%20performs%20an%20iterative%20denoising%20process%20on%20the%20input%0Acontext%20to%20handle%20diverse%20tasks.%20Experimental%20results%20on%20the%20benchmark%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20method%20on%20text-to-motion%0Ageneration%20and%20competitive%20performance%20on%20motion%20captioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoTe%253A%2520Learning%2520Motion-Text%2520Diffusion%2520Model%2520for%2520Multiple%2520Generation%2520Tasks%26entry.906535625%3DYiming%2520Wu%2520and%2520Wei%2520Ji%2520and%2520Kecheng%2520Zheng%2520and%2520Zicheng%2520Wang%2520and%2520Dong%2520Xu%26entry.1292438233%3D%2520%2520Recently%252C%2520human%2520motion%2520analysis%2520has%2520experienced%2520great%2520improvement%2520due%2520to%250Ainspiring%2520generative%2520models%2520such%2520as%2520the%2520denoising%2520diffusion%2520model%2520and%2520large%250Alanguage%2520model.%2520While%2520the%2520existing%2520approaches%2520mainly%2520focus%2520on%2520generating%250Amotions%2520with%2520textual%2520descriptions%2520and%2520overlook%2520the%2520reciprocal%2520task.%2520In%2520this%250Apaper%252C%2520we%2520present~%255Ctextbf%257BMoTe%257D%252C%2520a%2520unified%2520multi-modal%2520model%2520that%2520could%2520handle%250Adiverse%2520tasks%2520by%2520learning%2520the%2520marginal%252C%2520conditional%252C%2520and%2520joint%2520distributions%2520of%250Amotion%2520and%2520text%2520simultaneously.%2520MoTe%2520enables%2520us%2520to%2520handle%2520the%2520paired%250Atext-motion%2520generation%252C%2520motion%2520captioning%252C%2520and%2520text-driven%2520motion%2520generation%2520by%250Asimply%2520modifying%2520the%2520input%2520context.%2520Specifically%252C%2520MoTe%2520is%2520composed%2520of%2520three%250Acomponents%253A%2520Motion%2520Encoder-Decoder%2520%2528MED%2529%252C%2520Text%2520Encoder-Decoder%2520%2528TED%2529%252C%2520and%250AMoti-on-Text%2520Diffusion%2520Model%2520%2528MTDM%2529.%2520In%2520particular%252C%2520MED%2520and%2520TED%2520are%2520trained%2520for%250Aextracting%2520latent%2520embeddings%252C%2520and%2520subsequently%2520reconstructing%2520the%2520motion%250Asequences%2520and%2520textual%2520descriptions%2520from%2520the%2520extracted%2520embeddings%252C%2520respectively.%250AMTDM%252C%2520on%2520the%2520other%2520hand%252C%2520performs%2520an%2520iterative%2520denoising%2520process%2520on%2520the%2520input%250Acontext%2520to%2520handle%2520diverse%2520tasks.%2520Experimental%2520results%2520on%2520the%2520benchmark%2520datasets%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520proposed%2520method%2520on%2520text-to-motion%250Ageneration%2520and%2520competitive%2520performance%2520on%2520motion%2520captioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoTe%3A%20Learning%20Motion-Text%20Diffusion%20Model%20for%20Multiple%20Generation%20Tasks&entry.906535625=Yiming%20Wu%20and%20Wei%20Ji%20and%20Kecheng%20Zheng%20and%20Zicheng%20Wang%20and%20Dong%20Xu&entry.1292438233=%20%20Recently%2C%20human%20motion%20analysis%20has%20experienced%20great%20improvement%20due%20to%0Ainspiring%20generative%20models%20such%20as%20the%20denoising%20diffusion%20model%20and%20large%0Alanguage%20model.%20While%20the%20existing%20approaches%20mainly%20focus%20on%20generating%0Amotions%20with%20textual%20descriptions%20and%20overlook%20the%20reciprocal%20task.%20In%20this%0Apaper%2C%20we%20present~%5Ctextbf%7BMoTe%7D%2C%20a%20unified%20multi-modal%20model%20that%20could%20handle%0Adiverse%20tasks%20by%20learning%20the%20marginal%2C%20conditional%2C%20and%20joint%20distributions%20of%0Amotion%20and%20text%20simultaneously.%20MoTe%20enables%20us%20to%20handle%20the%20paired%0Atext-motion%20generation%2C%20motion%20captioning%2C%20and%20text-driven%20motion%20generation%20by%0Asimply%20modifying%20the%20input%20context.%20Specifically%2C%20MoTe%20is%20composed%20of%20three%0Acomponents%3A%20Motion%20Encoder-Decoder%20%28MED%29%2C%20Text%20Encoder-Decoder%20%28TED%29%2C%20and%0AMoti-on-Text%20Diffusion%20Model%20%28MTDM%29.%20In%20particular%2C%20MED%20and%20TED%20are%20trained%20for%0Aextracting%20latent%20embeddings%2C%20and%20subsequently%20reconstructing%20the%20motion%0Asequences%20and%20textual%20descriptions%20from%20the%20extracted%20embeddings%2C%20respectively.%0AMTDM%2C%20on%20the%20other%20hand%2C%20performs%20an%20iterative%20denoising%20process%20on%20the%20input%0Acontext%20to%20handle%20diverse%20tasks.%20Experimental%20results%20on%20the%20benchmark%20datasets%0Ademonstrate%20the%20superior%20performance%20of%20our%20proposed%20method%20on%20text-to-motion%0Ageneration%20and%20competitive%20performance%20on%20motion%20captioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19786v1&entry.124074799=Read"},
{"title": "Reverse Thinking Makes LLMs Stronger Reasoners", "author": "Justin Chih-Yao Chen and Zifeng Wang and Hamid Palangi and Rujun Han and Sayna Ebrahimi and Long Le and Vincent Perot and Swaroop Mishra and Mohit Bansal and Chen-Yu Lee and Tomas Pfister", "abstract": "  Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.\n", "link": "http://arxiv.org/abs/2411.19865v1", "date": "2024-11-29", "relevancy": 1.9116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reverse%20Thinking%20Makes%20LLMs%20Stronger%20Reasoners&body=Title%3A%20Reverse%20Thinking%20Makes%20LLMs%20Stronger%20Reasoners%0AAuthor%3A%20Justin%20Chih-Yao%20Chen%20and%20Zifeng%20Wang%20and%20Hamid%20Palangi%20and%20Rujun%20Han%20and%20Sayna%20Ebrahimi%20and%20Long%20Le%20and%20Vincent%20Perot%20and%20Swaroop%20Mishra%20and%20Mohit%20Bansal%20and%20Chen-Yu%20Lee%20and%20Tomas%20Pfister%0AAbstract%3A%20%20%20Reverse%20thinking%20plays%20a%20crucial%20role%20in%20human%20reasoning.%20Humans%20can%20reason%0Anot%20only%20from%20a%20problem%20to%20a%20solution%20but%20also%20in%20reverse%2C%20i.e.%2C%20start%20from%20the%0Asolution%20and%20reason%20towards%20the%20problem.%20This%20often%20enhances%20overall%20reasoning%0Aperformance%20as%20it%20enables%20consistency%20checks%20between%20their%20forward%20and%20backward%0Athinking.%20To%20enable%20Large%20Language%20Models%20%28LLMs%29%20to%20perform%20reverse%20thinking%2C%0Awe%20introduce%20Reverse-Enhanced%20Thinking%20%28RevThink%29%2C%20a%20framework%20composed%20of%20data%0Aaugmentation%20and%20learning%20objectives.%20In%20RevThink%2C%20we%20augment%20the%20dataset%20by%0Acollecting%20structured%20forward-backward%20reasoning%20from%20a%20teacher%20model%2C%0Aconsisting%20of%3A%20%281%29%20the%20original%20question%2C%20%282%29%20forward%20reasoning%2C%20%283%29%20backward%0Aquestion%2C%20and%20%284%29%20backward%20reasoning.%20We%20then%20employ%20three%20objectives%20to%20train%0Aa%20smaller%20student%20model%20in%20a%20multi-task%20learning%20fashion%3A%20%28a%29%20generate%20forward%0Areasoning%20from%20a%20question%2C%20%28b%29%20generate%20a%20backward%20question%20from%20a%20question%2C%0Aand%20%28c%29%20generate%20backward%20reasoning%20from%20the%20backward%20question.%20Experiments%0Aacross%2012%20datasets%20covering%20commonsense%2C%20math%2C%20and%20logical%20reasoning%20show%20an%0Aaverage%2013.53%25%20improvement%20over%20the%20student%20model%27s%20zero-shot%20performance%20and%20a%0A6.84%25%20improvement%20over%20the%20strongest%20knowledge%20distillation%20baselines.%0AMoreover%2C%20our%20method%20demonstrates%20sample%20efficiency%20--%20using%20only%2010%25%20of%20the%0Acorrect%20forward%20reasoning%20from%20the%20training%20data%2C%20it%20outperforms%20a%20standard%0Afine-tuning%20method%20trained%20on%2010x%20more%20forward%20reasoning.%20RevThink%20also%0Aexhibits%20strong%20generalization%20to%20out-of-distribution%20held-out%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReverse%2520Thinking%2520Makes%2520LLMs%2520Stronger%2520Reasoners%26entry.906535625%3DJustin%2520Chih-Yao%2520Chen%2520and%2520Zifeng%2520Wang%2520and%2520Hamid%2520Palangi%2520and%2520Rujun%2520Han%2520and%2520Sayna%2520Ebrahimi%2520and%2520Long%2520Le%2520and%2520Vincent%2520Perot%2520and%2520Swaroop%2520Mishra%2520and%2520Mohit%2520Bansal%2520and%2520Chen-Yu%2520Lee%2520and%2520Tomas%2520Pfister%26entry.1292438233%3D%2520%2520Reverse%2520thinking%2520plays%2520a%2520crucial%2520role%2520in%2520human%2520reasoning.%2520Humans%2520can%2520reason%250Anot%2520only%2520from%2520a%2520problem%2520to%2520a%2520solution%2520but%2520also%2520in%2520reverse%252C%2520i.e.%252C%2520start%2520from%2520the%250Asolution%2520and%2520reason%2520towards%2520the%2520problem.%2520This%2520often%2520enhances%2520overall%2520reasoning%250Aperformance%2520as%2520it%2520enables%2520consistency%2520checks%2520between%2520their%2520forward%2520and%2520backward%250Athinking.%2520To%2520enable%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520perform%2520reverse%2520thinking%252C%250Awe%2520introduce%2520Reverse-Enhanced%2520Thinking%2520%2528RevThink%2529%252C%2520a%2520framework%2520composed%2520of%2520data%250Aaugmentation%2520and%2520learning%2520objectives.%2520In%2520RevThink%252C%2520we%2520augment%2520the%2520dataset%2520by%250Acollecting%2520structured%2520forward-backward%2520reasoning%2520from%2520a%2520teacher%2520model%252C%250Aconsisting%2520of%253A%2520%25281%2529%2520the%2520original%2520question%252C%2520%25282%2529%2520forward%2520reasoning%252C%2520%25283%2529%2520backward%250Aquestion%252C%2520and%2520%25284%2529%2520backward%2520reasoning.%2520We%2520then%2520employ%2520three%2520objectives%2520to%2520train%250Aa%2520smaller%2520student%2520model%2520in%2520a%2520multi-task%2520learning%2520fashion%253A%2520%2528a%2529%2520generate%2520forward%250Areasoning%2520from%2520a%2520question%252C%2520%2528b%2529%2520generate%2520a%2520backward%2520question%2520from%2520a%2520question%252C%250Aand%2520%2528c%2529%2520generate%2520backward%2520reasoning%2520from%2520the%2520backward%2520question.%2520Experiments%250Aacross%252012%2520datasets%2520covering%2520commonsense%252C%2520math%252C%2520and%2520logical%2520reasoning%2520show%2520an%250Aaverage%252013.53%2525%2520improvement%2520over%2520the%2520student%2520model%2527s%2520zero-shot%2520performance%2520and%2520a%250A6.84%2525%2520improvement%2520over%2520the%2520strongest%2520knowledge%2520distillation%2520baselines.%250AMoreover%252C%2520our%2520method%2520demonstrates%2520sample%2520efficiency%2520--%2520using%2520only%252010%2525%2520of%2520the%250Acorrect%2520forward%2520reasoning%2520from%2520the%2520training%2520data%252C%2520it%2520outperforms%2520a%2520standard%250Afine-tuning%2520method%2520trained%2520on%252010x%2520more%2520forward%2520reasoning.%2520RevThink%2520also%250Aexhibits%2520strong%2520generalization%2520to%2520out-of-distribution%2520held-out%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reverse%20Thinking%20Makes%20LLMs%20Stronger%20Reasoners&entry.906535625=Justin%20Chih-Yao%20Chen%20and%20Zifeng%20Wang%20and%20Hamid%20Palangi%20and%20Rujun%20Han%20and%20Sayna%20Ebrahimi%20and%20Long%20Le%20and%20Vincent%20Perot%20and%20Swaroop%20Mishra%20and%20Mohit%20Bansal%20and%20Chen-Yu%20Lee%20and%20Tomas%20Pfister&entry.1292438233=%20%20Reverse%20thinking%20plays%20a%20crucial%20role%20in%20human%20reasoning.%20Humans%20can%20reason%0Anot%20only%20from%20a%20problem%20to%20a%20solution%20but%20also%20in%20reverse%2C%20i.e.%2C%20start%20from%20the%0Asolution%20and%20reason%20towards%20the%20problem.%20This%20often%20enhances%20overall%20reasoning%0Aperformance%20as%20it%20enables%20consistency%20checks%20between%20their%20forward%20and%20backward%0Athinking.%20To%20enable%20Large%20Language%20Models%20%28LLMs%29%20to%20perform%20reverse%20thinking%2C%0Awe%20introduce%20Reverse-Enhanced%20Thinking%20%28RevThink%29%2C%20a%20framework%20composed%20of%20data%0Aaugmentation%20and%20learning%20objectives.%20In%20RevThink%2C%20we%20augment%20the%20dataset%20by%0Acollecting%20structured%20forward-backward%20reasoning%20from%20a%20teacher%20model%2C%0Aconsisting%20of%3A%20%281%29%20the%20original%20question%2C%20%282%29%20forward%20reasoning%2C%20%283%29%20backward%0Aquestion%2C%20and%20%284%29%20backward%20reasoning.%20We%20then%20employ%20three%20objectives%20to%20train%0Aa%20smaller%20student%20model%20in%20a%20multi-task%20learning%20fashion%3A%20%28a%29%20generate%20forward%0Areasoning%20from%20a%20question%2C%20%28b%29%20generate%20a%20backward%20question%20from%20a%20question%2C%0Aand%20%28c%29%20generate%20backward%20reasoning%20from%20the%20backward%20question.%20Experiments%0Aacross%2012%20datasets%20covering%20commonsense%2C%20math%2C%20and%20logical%20reasoning%20show%20an%0Aaverage%2013.53%25%20improvement%20over%20the%20student%20model%27s%20zero-shot%20performance%20and%20a%0A6.84%25%20improvement%20over%20the%20strongest%20knowledge%20distillation%20baselines.%0AMoreover%2C%20our%20method%20demonstrates%20sample%20efficiency%20--%20using%20only%2010%25%20of%20the%0Acorrect%20forward%20reasoning%20from%20the%20training%20data%2C%20it%20outperforms%20a%20standard%0Afine-tuning%20method%20trained%20on%2010x%20more%20forward%20reasoning.%20RevThink%20also%0Aexhibits%20strong%20generalization%20to%20out-of-distribution%20held-out%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19865v1&entry.124074799=Read"},
{"title": "Amplifying human performance in combinatorial competitive programming", "author": "Petar Veli\u010dkovi\u0107 and Alex Vitvitskyi and Larisa Markeeva and Borja Ibarz and Lars Buesing and Matej Balog and Alexander Novikov", "abstract": "  Recent years have seen a significant surge in complex AI systems for\ncompetitive programming, capable of performing at admirable levels against\nhuman competitors. While steady progress has been made, the highest percentiles\nstill remain out of reach for these methods on standard competition platforms\nsuch as Codeforces. Here we instead focus on combinatorial competitive\nprogramming, where the target is to find as-good-as-possible solutions to\notherwise computationally intractable problems, over specific given inputs. We\nhypothesise that this scenario offers a unique testbed for human-AI synergy, as\nhuman programmers can write a backbone of a heuristic solution, after which AI\ncan be used to optimise the scoring function used by the heuristic. We deploy\nour approach on previous iterations of Hash Code, a global team programming\ncompetition inspired by NP-hard software engineering problems at Google, and we\nleverage FunSearch to evolve our scoring functions. Our evolved solutions\nsignificantly improve the attained scores from their baseline, successfully\nbreaking into the top percentile on all previous Hash Code online qualification\nrounds, and outperforming the top human teams on several. Our method is also\nperformant on an optimisation problem that featured in a recent held-out\nAtCoder contest.\n", "link": "http://arxiv.org/abs/2411.19744v1", "date": "2024-11-29", "relevancy": 1.3303, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4616}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4435}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Amplifying%20human%20performance%20in%20combinatorial%20competitive%20programming&body=Title%3A%20Amplifying%20human%20performance%20in%20combinatorial%20competitive%20programming%0AAuthor%3A%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Alex%20Vitvitskyi%20and%20Larisa%20Markeeva%20and%20Borja%20Ibarz%20and%20Lars%20Buesing%20and%20Matej%20Balog%20and%20Alexander%20Novikov%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20a%20significant%20surge%20in%20complex%20AI%20systems%20for%0Acompetitive%20programming%2C%20capable%20of%20performing%20at%20admirable%20levels%20against%0Ahuman%20competitors.%20While%20steady%20progress%20has%20been%20made%2C%20the%20highest%20percentiles%0Astill%20remain%20out%20of%20reach%20for%20these%20methods%20on%20standard%20competition%20platforms%0Asuch%20as%20Codeforces.%20Here%20we%20instead%20focus%20on%20combinatorial%20competitive%0Aprogramming%2C%20where%20the%20target%20is%20to%20find%20as-good-as-possible%20solutions%20to%0Aotherwise%20computationally%20intractable%20problems%2C%20over%20specific%20given%20inputs.%20We%0Ahypothesise%20that%20this%20scenario%20offers%20a%20unique%20testbed%20for%20human-AI%20synergy%2C%20as%0Ahuman%20programmers%20can%20write%20a%20backbone%20of%20a%20heuristic%20solution%2C%20after%20which%20AI%0Acan%20be%20used%20to%20optimise%20the%20scoring%20function%20used%20by%20the%20heuristic.%20We%20deploy%0Aour%20approach%20on%20previous%20iterations%20of%20Hash%20Code%2C%20a%20global%20team%20programming%0Acompetition%20inspired%20by%20NP-hard%20software%20engineering%20problems%20at%20Google%2C%20and%20we%0Aleverage%20FunSearch%20to%20evolve%20our%20scoring%20functions.%20Our%20evolved%20solutions%0Asignificantly%20improve%20the%20attained%20scores%20from%20their%20baseline%2C%20successfully%0Abreaking%20into%20the%20top%20percentile%20on%20all%20previous%20Hash%20Code%20online%20qualification%0Arounds%2C%20and%20outperforming%20the%20top%20human%20teams%20on%20several.%20Our%20method%20is%20also%0Aperformant%20on%20an%20optimisation%20problem%20that%20featured%20in%20a%20recent%20held-out%0AAtCoder%20contest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmplifying%2520human%2520performance%2520in%2520combinatorial%2520competitive%2520programming%26entry.906535625%3DPetar%2520Veli%25C4%258Dkovi%25C4%2587%2520and%2520Alex%2520Vitvitskyi%2520and%2520Larisa%2520Markeeva%2520and%2520Borja%2520Ibarz%2520and%2520Lars%2520Buesing%2520and%2520Matej%2520Balog%2520and%2520Alexander%2520Novikov%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520a%2520significant%2520surge%2520in%2520complex%2520AI%2520systems%2520for%250Acompetitive%2520programming%252C%2520capable%2520of%2520performing%2520at%2520admirable%2520levels%2520against%250Ahuman%2520competitors.%2520While%2520steady%2520progress%2520has%2520been%2520made%252C%2520the%2520highest%2520percentiles%250Astill%2520remain%2520out%2520of%2520reach%2520for%2520these%2520methods%2520on%2520standard%2520competition%2520platforms%250Asuch%2520as%2520Codeforces.%2520Here%2520we%2520instead%2520focus%2520on%2520combinatorial%2520competitive%250Aprogramming%252C%2520where%2520the%2520target%2520is%2520to%2520find%2520as-good-as-possible%2520solutions%2520to%250Aotherwise%2520computationally%2520intractable%2520problems%252C%2520over%2520specific%2520given%2520inputs.%2520We%250Ahypothesise%2520that%2520this%2520scenario%2520offers%2520a%2520unique%2520testbed%2520for%2520human-AI%2520synergy%252C%2520as%250Ahuman%2520programmers%2520can%2520write%2520a%2520backbone%2520of%2520a%2520heuristic%2520solution%252C%2520after%2520which%2520AI%250Acan%2520be%2520used%2520to%2520optimise%2520the%2520scoring%2520function%2520used%2520by%2520the%2520heuristic.%2520We%2520deploy%250Aour%2520approach%2520on%2520previous%2520iterations%2520of%2520Hash%2520Code%252C%2520a%2520global%2520team%2520programming%250Acompetition%2520inspired%2520by%2520NP-hard%2520software%2520engineering%2520problems%2520at%2520Google%252C%2520and%2520we%250Aleverage%2520FunSearch%2520to%2520evolve%2520our%2520scoring%2520functions.%2520Our%2520evolved%2520solutions%250Asignificantly%2520improve%2520the%2520attained%2520scores%2520from%2520their%2520baseline%252C%2520successfully%250Abreaking%2520into%2520the%2520top%2520percentile%2520on%2520all%2520previous%2520Hash%2520Code%2520online%2520qualification%250Arounds%252C%2520and%2520outperforming%2520the%2520top%2520human%2520teams%2520on%2520several.%2520Our%2520method%2520is%2520also%250Aperformant%2520on%2520an%2520optimisation%2520problem%2520that%2520featured%2520in%2520a%2520recent%2520held-out%250AAtCoder%2520contest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amplifying%20human%20performance%20in%20combinatorial%20competitive%20programming&entry.906535625=Petar%20Veli%C4%8Dkovi%C4%87%20and%20Alex%20Vitvitskyi%20and%20Larisa%20Markeeva%20and%20Borja%20Ibarz%20and%20Lars%20Buesing%20and%20Matej%20Balog%20and%20Alexander%20Novikov&entry.1292438233=%20%20Recent%20years%20have%20seen%20a%20significant%20surge%20in%20complex%20AI%20systems%20for%0Acompetitive%20programming%2C%20capable%20of%20performing%20at%20admirable%20levels%20against%0Ahuman%20competitors.%20While%20steady%20progress%20has%20been%20made%2C%20the%20highest%20percentiles%0Astill%20remain%20out%20of%20reach%20for%20these%20methods%20on%20standard%20competition%20platforms%0Asuch%20as%20Codeforces.%20Here%20we%20instead%20focus%20on%20combinatorial%20competitive%0Aprogramming%2C%20where%20the%20target%20is%20to%20find%20as-good-as-possible%20solutions%20to%0Aotherwise%20computationally%20intractable%20problems%2C%20over%20specific%20given%20inputs.%20We%0Ahypothesise%20that%20this%20scenario%20offers%20a%20unique%20testbed%20for%20human-AI%20synergy%2C%20as%0Ahuman%20programmers%20can%20write%20a%20backbone%20of%20a%20heuristic%20solution%2C%20after%20which%20AI%0Acan%20be%20used%20to%20optimise%20the%20scoring%20function%20used%20by%20the%20heuristic.%20We%20deploy%0Aour%20approach%20on%20previous%20iterations%20of%20Hash%20Code%2C%20a%20global%20team%20programming%0Acompetition%20inspired%20by%20NP-hard%20software%20engineering%20problems%20at%20Google%2C%20and%20we%0Aleverage%20FunSearch%20to%20evolve%20our%20scoring%20functions.%20Our%20evolved%20solutions%0Asignificantly%20improve%20the%20attained%20scores%20from%20their%20baseline%2C%20successfully%0Abreaking%20into%20the%20top%20percentile%20on%20all%20previous%20Hash%20Code%20online%20qualification%0Arounds%2C%20and%20outperforming%20the%20top%20human%20teams%20on%20several.%20Our%20method%20is%20also%0Aperformant%20on%20an%20optimisation%20problem%20that%20featured%20in%20a%20recent%20held-out%0AAtCoder%20contest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19744v1&entry.124074799=Read"},
{"title": "The ATTUNE model for Artificial Trust Towards Human Operators", "author": "Giannis Petousakis and Angelo Cangelosi and Rustam Stolkin and Manolis Chiou", "abstract": "  This paper presents a novel method to quantify Trust in HRI. It proposes an\nHRI framework for estimating the Robot Trust towards the Human in the context\nof a narrow and specified task. The framework produces a real-time estimation\nof an AI agent's Artificial Trust towards a Human partner interacting with a\nmobile teleoperation robot. The approach for the framework is based on\nprinciples drawn from Theory of Mind, including information about the human\nstate, action, and intent. The framework creates the ATTUNE model for\nArtificial Trust Towards Human Operators. The model uses metrics on the\noperator's state of attention, navigational intent, actions, and performance to\nquantify the Trust towards them. The model is tested on a pre-existing dataset\nthat includes recordings (ROSbags) of a human trial in a simulated disaster\nresponse scenario. The performance of ATTUNE is evaluated through a qualitative\nand quantitative analysis. The results of the analyses provide insight into the\nnext stages of the research and help refine the proposed approach.\n", "link": "http://arxiv.org/abs/2411.19580v1", "date": "2024-11-29", "relevancy": 1.4303, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4925}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20ATTUNE%20model%20for%20Artificial%20Trust%20Towards%20Human%20Operators&body=Title%3A%20The%20ATTUNE%20model%20for%20Artificial%20Trust%20Towards%20Human%20Operators%0AAuthor%3A%20Giannis%20Petousakis%20and%20Angelo%20Cangelosi%20and%20Rustam%20Stolkin%20and%20Manolis%20Chiou%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20method%20to%20quantify%20Trust%20in%20HRI.%20It%20proposes%20an%0AHRI%20framework%20for%20estimating%20the%20Robot%20Trust%20towards%20the%20Human%20in%20the%20context%0Aof%20a%20narrow%20and%20specified%20task.%20The%20framework%20produces%20a%20real-time%20estimation%0Aof%20an%20AI%20agent%27s%20Artificial%20Trust%20towards%20a%20Human%20partner%20interacting%20with%20a%0Amobile%20teleoperation%20robot.%20The%20approach%20for%20the%20framework%20is%20based%20on%0Aprinciples%20drawn%20from%20Theory%20of%20Mind%2C%20including%20information%20about%20the%20human%0Astate%2C%20action%2C%20and%20intent.%20The%20framework%20creates%20the%20ATTUNE%20model%20for%0AArtificial%20Trust%20Towards%20Human%20Operators.%20The%20model%20uses%20metrics%20on%20the%0Aoperator%27s%20state%20of%20attention%2C%20navigational%20intent%2C%20actions%2C%20and%20performance%20to%0Aquantify%20the%20Trust%20towards%20them.%20The%20model%20is%20tested%20on%20a%20pre-existing%20dataset%0Athat%20includes%20recordings%20%28ROSbags%29%20of%20a%20human%20trial%20in%20a%20simulated%20disaster%0Aresponse%20scenario.%20The%20performance%20of%20ATTUNE%20is%20evaluated%20through%20a%20qualitative%0Aand%20quantitative%20analysis.%20The%20results%20of%20the%20analyses%20provide%20insight%20into%20the%0Anext%20stages%20of%20the%20research%20and%20help%20refine%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19580v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520ATTUNE%2520model%2520for%2520Artificial%2520Trust%2520Towards%2520Human%2520Operators%26entry.906535625%3DGiannis%2520Petousakis%2520and%2520Angelo%2520Cangelosi%2520and%2520Rustam%2520Stolkin%2520and%2520Manolis%2520Chiou%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520method%2520to%2520quantify%2520Trust%2520in%2520HRI.%2520It%2520proposes%2520an%250AHRI%2520framework%2520for%2520estimating%2520the%2520Robot%2520Trust%2520towards%2520the%2520Human%2520in%2520the%2520context%250Aof%2520a%2520narrow%2520and%2520specified%2520task.%2520The%2520framework%2520produces%2520a%2520real-time%2520estimation%250Aof%2520an%2520AI%2520agent%2527s%2520Artificial%2520Trust%2520towards%2520a%2520Human%2520partner%2520interacting%2520with%2520a%250Amobile%2520teleoperation%2520robot.%2520The%2520approach%2520for%2520the%2520framework%2520is%2520based%2520on%250Aprinciples%2520drawn%2520from%2520Theory%2520of%2520Mind%252C%2520including%2520information%2520about%2520the%2520human%250Astate%252C%2520action%252C%2520and%2520intent.%2520The%2520framework%2520creates%2520the%2520ATTUNE%2520model%2520for%250AArtificial%2520Trust%2520Towards%2520Human%2520Operators.%2520The%2520model%2520uses%2520metrics%2520on%2520the%250Aoperator%2527s%2520state%2520of%2520attention%252C%2520navigational%2520intent%252C%2520actions%252C%2520and%2520performance%2520to%250Aquantify%2520the%2520Trust%2520towards%2520them.%2520The%2520model%2520is%2520tested%2520on%2520a%2520pre-existing%2520dataset%250Athat%2520includes%2520recordings%2520%2528ROSbags%2529%2520of%2520a%2520human%2520trial%2520in%2520a%2520simulated%2520disaster%250Aresponse%2520scenario.%2520The%2520performance%2520of%2520ATTUNE%2520is%2520evaluated%2520through%2520a%2520qualitative%250Aand%2520quantitative%2520analysis.%2520The%2520results%2520of%2520the%2520analyses%2520provide%2520insight%2520into%2520the%250Anext%2520stages%2520of%2520the%2520research%2520and%2520help%2520refine%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19580v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20ATTUNE%20model%20for%20Artificial%20Trust%20Towards%20Human%20Operators&entry.906535625=Giannis%20Petousakis%20and%20Angelo%20Cangelosi%20and%20Rustam%20Stolkin%20and%20Manolis%20Chiou&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20method%20to%20quantify%20Trust%20in%20HRI.%20It%20proposes%20an%0AHRI%20framework%20for%20estimating%20the%20Robot%20Trust%20towards%20the%20Human%20in%20the%20context%0Aof%20a%20narrow%20and%20specified%20task.%20The%20framework%20produces%20a%20real-time%20estimation%0Aof%20an%20AI%20agent%27s%20Artificial%20Trust%20towards%20a%20Human%20partner%20interacting%20with%20a%0Amobile%20teleoperation%20robot.%20The%20approach%20for%20the%20framework%20is%20based%20on%0Aprinciples%20drawn%20from%20Theory%20of%20Mind%2C%20including%20information%20about%20the%20human%0Astate%2C%20action%2C%20and%20intent.%20The%20framework%20creates%20the%20ATTUNE%20model%20for%0AArtificial%20Trust%20Towards%20Human%20Operators.%20The%20model%20uses%20metrics%20on%20the%0Aoperator%27s%20state%20of%20attention%2C%20navigational%20intent%2C%20actions%2C%20and%20performance%20to%0Aquantify%20the%20Trust%20towards%20them.%20The%20model%20is%20tested%20on%20a%20pre-existing%20dataset%0Athat%20includes%20recordings%20%28ROSbags%29%20of%20a%20human%20trial%20in%20a%20simulated%20disaster%0Aresponse%20scenario.%20The%20performance%20of%20ATTUNE%20is%20evaluated%20through%20a%20qualitative%0Aand%20quantitative%20analysis.%20The%20results%20of%20the%20analyses%20provide%20insight%20into%20the%0Anext%20stages%20of%20the%20research%20and%20help%20refine%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19580v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


