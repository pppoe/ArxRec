<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250519.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation", "author": "Seungjun Oh and Younggeun Lee and Hyejin Jeon and Eunbyung Park", "abstract": "  Recent advancements in dynamic 3D scene reconstruction have shown promising\nresults, enabling high-fidelity 3D novel view synthesis with improved temporal\nconsistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an\nappealing approach due to its ability to model high-fidelity spatial and\ntemporal variations. However, existing methods suffer from substantial\ncomputational and memory overhead due to the redundant allocation of 4D\nGaussians to static regions, which can also degrade image quality. In this\nwork, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework\nthat adaptively represents static regions with 3D Gaussians while reserving 4D\nGaussians for dynamic elements. Our method begins with a fully 4D Gaussian\nrepresentation and iteratively converts temporally invariant Gaussians into 3D,\nsignificantly reducing the number of parameters and improving computational\nefficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,\ncapturing complex motions with high fidelity. Our approach achieves\nsignificantly faster training times compared to baseline 4D Gaussian Splatting\nmethods while maintaining or improving the visual quality.\n", "link": "http://arxiv.org/abs/2505.13215v1", "date": "2025-05-19", "relevancy": 3.6231, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7427}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7294}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%203D-4D%20Gaussian%20Splatting%20for%20Fast%20Dynamic%20Scene%20Representation&body=Title%3A%20Hybrid%203D-4D%20Gaussian%20Splatting%20for%20Fast%20Dynamic%20Scene%20Representation%0AAuthor%3A%20Seungjun%20Oh%20and%20Younggeun%20Lee%20and%20Hyejin%20Jeon%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%20Recent%20advancements%20in%20dynamic%203D%20scene%20reconstruction%20have%20shown%20promising%0Aresults%2C%20enabling%20high-fidelity%203D%20novel%20view%20synthesis%20with%20improved%20temporal%0Aconsistency.%20Among%20these%2C%204D%20Gaussian%20Splatting%20%284DGS%29%20has%20emerged%20as%20an%0Aappealing%20approach%20due%20to%20its%20ability%20to%20model%20high-fidelity%20spatial%20and%0Atemporal%20variations.%20However%2C%20existing%20methods%20suffer%20from%20substantial%0Acomputational%20and%20memory%20overhead%20due%20to%20the%20redundant%20allocation%20of%204D%0AGaussians%20to%20static%20regions%2C%20which%20can%20also%20degrade%20image%20quality.%20In%20this%0Awork%2C%20we%20introduce%20hybrid%203D-4D%20Gaussian%20Splatting%20%283D-4DGS%29%2C%20a%20novel%20framework%0Athat%20adaptively%20represents%20static%20regions%20with%203D%20Gaussians%20while%20reserving%204D%0AGaussians%20for%20dynamic%20elements.%20Our%20method%20begins%20with%20a%20fully%204D%20Gaussian%0Arepresentation%20and%20iteratively%20converts%20temporally%20invariant%20Gaussians%20into%203D%2C%0Asignificantly%20reducing%20the%20number%20of%20parameters%20and%20improving%20computational%0Aefficiency.%20Meanwhile%2C%20dynamic%20Gaussians%20retain%20their%20full%204D%20representation%2C%0Acapturing%20complex%20motions%20with%20high%20fidelity.%20Our%20approach%20achieves%0Asignificantly%20faster%20training%20times%20compared%20to%20baseline%204D%20Gaussian%20Splatting%0Amethods%20while%20maintaining%20or%20improving%20the%20visual%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13215v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%25203D-4D%2520Gaussian%2520Splatting%2520for%2520Fast%2520Dynamic%2520Scene%2520Representation%26entry.906535625%3DSeungjun%2520Oh%2520and%2520Younggeun%2520Lee%2520and%2520Hyejin%2520Jeon%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520dynamic%25203D%2520scene%2520reconstruction%2520have%2520shown%2520promising%250Aresults%252C%2520enabling%2520high-fidelity%25203D%2520novel%2520view%2520synthesis%2520with%2520improved%2520temporal%250Aconsistency.%2520Among%2520these%252C%25204D%2520Gaussian%2520Splatting%2520%25284DGS%2529%2520has%2520emerged%2520as%2520an%250Aappealing%2520approach%2520due%2520to%2520its%2520ability%2520to%2520model%2520high-fidelity%2520spatial%2520and%250Atemporal%2520variations.%2520However%252C%2520existing%2520methods%2520suffer%2520from%2520substantial%250Acomputational%2520and%2520memory%2520overhead%2520due%2520to%2520the%2520redundant%2520allocation%2520of%25204D%250AGaussians%2520to%2520static%2520regions%252C%2520which%2520can%2520also%2520degrade%2520image%2520quality.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520hybrid%25203D-4D%2520Gaussian%2520Splatting%2520%25283D-4DGS%2529%252C%2520a%2520novel%2520framework%250Athat%2520adaptively%2520represents%2520static%2520regions%2520with%25203D%2520Gaussians%2520while%2520reserving%25204D%250AGaussians%2520for%2520dynamic%2520elements.%2520Our%2520method%2520begins%2520with%2520a%2520fully%25204D%2520Gaussian%250Arepresentation%2520and%2520iteratively%2520converts%2520temporally%2520invariant%2520Gaussians%2520into%25203D%252C%250Asignificantly%2520reducing%2520the%2520number%2520of%2520parameters%2520and%2520improving%2520computational%250Aefficiency.%2520Meanwhile%252C%2520dynamic%2520Gaussians%2520retain%2520their%2520full%25204D%2520representation%252C%250Acapturing%2520complex%2520motions%2520with%2520high%2520fidelity.%2520Our%2520approach%2520achieves%250Asignificantly%2520faster%2520training%2520times%2520compared%2520to%2520baseline%25204D%2520Gaussian%2520Splatting%250Amethods%2520while%2520maintaining%2520or%2520improving%2520the%2520visual%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13215v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%203D-4D%20Gaussian%20Splatting%20for%20Fast%20Dynamic%20Scene%20Representation&entry.906535625=Seungjun%20Oh%20and%20Younggeun%20Lee%20and%20Hyejin%20Jeon%20and%20Eunbyung%20Park&entry.1292438233=%20%20Recent%20advancements%20in%20dynamic%203D%20scene%20reconstruction%20have%20shown%20promising%0Aresults%2C%20enabling%20high-fidelity%203D%20novel%20view%20synthesis%20with%20improved%20temporal%0Aconsistency.%20Among%20these%2C%204D%20Gaussian%20Splatting%20%284DGS%29%20has%20emerged%20as%20an%0Aappealing%20approach%20due%20to%20its%20ability%20to%20model%20high-fidelity%20spatial%20and%0Atemporal%20variations.%20However%2C%20existing%20methods%20suffer%20from%20substantial%0Acomputational%20and%20memory%20overhead%20due%20to%20the%20redundant%20allocation%20of%204D%0AGaussians%20to%20static%20regions%2C%20which%20can%20also%20degrade%20image%20quality.%20In%20this%0Awork%2C%20we%20introduce%20hybrid%203D-4D%20Gaussian%20Splatting%20%283D-4DGS%29%2C%20a%20novel%20framework%0Athat%20adaptively%20represents%20static%20regions%20with%203D%20Gaussians%20while%20reserving%204D%0AGaussians%20for%20dynamic%20elements.%20Our%20method%20begins%20with%20a%20fully%204D%20Gaussian%0Arepresentation%20and%20iteratively%20converts%20temporally%20invariant%20Gaussians%20into%203D%2C%0Asignificantly%20reducing%20the%20number%20of%20parameters%20and%20improving%20computational%0Aefficiency.%20Meanwhile%2C%20dynamic%20Gaussians%20retain%20their%20full%204D%20representation%2C%0Acapturing%20complex%20motions%20with%20high%20fidelity.%20Our%20approach%20achieves%0Asignificantly%20faster%20training%20times%20compared%20to%20baseline%204D%20Gaussian%20Splatting%0Amethods%20while%20maintaining%20or%20improving%20the%20visual%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13215v1&entry.124074799=Read"},
{"title": "FinePhys: Fine-grained Human Action Generation by Explicitly\n  Incorporating Physical Laws for Effective Skeletal Guidance", "author": "Dian Shao and Mingfei Shi and Shengda Xu and Haodong Chen and Yongle Huang and Binglu Wang", "abstract": "  Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions.\n", "link": "http://arxiv.org/abs/2505.13437v1", "date": "2025-05-19", "relevancy": 3.2488, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6873}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6821}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.58}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinePhys%3A%20Fine-grained%20Human%20Action%20Generation%20by%20Explicitly%0A%20%20Incorporating%20Physical%20Laws%20for%20Effective%20Skeletal%20Guidance&body=Title%3A%20FinePhys%3A%20Fine-grained%20Human%20Action%20Generation%20by%20Explicitly%0A%20%20Incorporating%20Physical%20Laws%20for%20Effective%20Skeletal%20Guidance%0AAuthor%3A%20Dian%20Shao%20and%20Mingfei%20Shi%20and%20Shengda%20Xu%20and%20Haodong%20Chen%20and%20Yongle%20Huang%20and%20Binglu%20Wang%0AAbstract%3A%20%20%20Despite%20significant%20advances%20in%20video%20generation%2C%20synthesizing%20physically%0Aplausible%20human%20actions%20remains%20a%20persistent%20challenge%2C%20particularly%20in%0Amodeling%20fine-grained%20semantics%20and%20complex%20temporal%20dynamics.%20For%20instance%2C%0Agenerating%20gymnastics%20routines%20such%20as%20%22switch%20leap%20with%200.5%20turn%22%20poses%0Asubstantial%20difficulties%20for%20current%20methods%2C%20often%20yielding%20unsatisfactory%0Aresults.%20To%20bridge%20this%20gap%2C%20we%20propose%20FinePhys%2C%20a%20Fine-grained%20human%20action%0Ageneration%20framework%20that%20incorporates%20Physics%20to%20obtain%20effective%20skeletal%0Aguidance.%20Specifically%2C%20FinePhys%20first%20estimates%202D%20poses%20in%20an%20online%20manner%0Aand%20then%20performs%202D-to-3D%20dimension%20lifting%20via%20in-context%20learning.%20To%0Amitigate%20the%20instability%20and%20limited%20interpretability%20of%20purely%20data-driven%203D%0Aposes%2C%20we%20further%20introduce%20a%20physics-based%20motion%20re-estimation%20module%0Agoverned%20by%20Euler-Lagrange%20equations%2C%20calculating%20joint%20accelerations%20via%0Abidirectional%20temporal%20updating.%20The%20physically%20predicted%203D%20poses%20are%20then%0Afused%20with%20data-driven%20ones%2C%20offering%20multi-scale%202D%20heatmap%20guidance%20for%20the%0Adiffusion%20process.%20Evaluated%20on%20three%20fine-grained%20action%20subsets%20from%20FineGym%0A%28FX-JUMP%2C%20FX-TURN%2C%20and%20FX-SALTO%29%2C%20FinePhys%20significantly%20outperforms%0Acompetitive%20baselines.%20Comprehensive%20qualitative%20results%20further%20demonstrate%0AFinePhys%27s%20ability%20to%20generate%20more%20natural%20and%20plausible%20fine-grained%20human%0Aactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinePhys%253A%2520Fine-grained%2520Human%2520Action%2520Generation%2520by%2520Explicitly%250A%2520%2520Incorporating%2520Physical%2520Laws%2520for%2520Effective%2520Skeletal%2520Guidance%26entry.906535625%3DDian%2520Shao%2520and%2520Mingfei%2520Shi%2520and%2520Shengda%2520Xu%2520and%2520Haodong%2520Chen%2520and%2520Yongle%2520Huang%2520and%2520Binglu%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advances%2520in%2520video%2520generation%252C%2520synthesizing%2520physically%250Aplausible%2520human%2520actions%2520remains%2520a%2520persistent%2520challenge%252C%2520particularly%2520in%250Amodeling%2520fine-grained%2520semantics%2520and%2520complex%2520temporal%2520dynamics.%2520For%2520instance%252C%250Agenerating%2520gymnastics%2520routines%2520such%2520as%2520%2522switch%2520leap%2520with%25200.5%2520turn%2522%2520poses%250Asubstantial%2520difficulties%2520for%2520current%2520methods%252C%2520often%2520yielding%2520unsatisfactory%250Aresults.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520FinePhys%252C%2520a%2520Fine-grained%2520human%2520action%250Ageneration%2520framework%2520that%2520incorporates%2520Physics%2520to%2520obtain%2520effective%2520skeletal%250Aguidance.%2520Specifically%252C%2520FinePhys%2520first%2520estimates%25202D%2520poses%2520in%2520an%2520online%2520manner%250Aand%2520then%2520performs%25202D-to-3D%2520dimension%2520lifting%2520via%2520in-context%2520learning.%2520To%250Amitigate%2520the%2520instability%2520and%2520limited%2520interpretability%2520of%2520purely%2520data-driven%25203D%250Aposes%252C%2520we%2520further%2520introduce%2520a%2520physics-based%2520motion%2520re-estimation%2520module%250Agoverned%2520by%2520Euler-Lagrange%2520equations%252C%2520calculating%2520joint%2520accelerations%2520via%250Abidirectional%2520temporal%2520updating.%2520The%2520physically%2520predicted%25203D%2520poses%2520are%2520then%250Afused%2520with%2520data-driven%2520ones%252C%2520offering%2520multi-scale%25202D%2520heatmap%2520guidance%2520for%2520the%250Adiffusion%2520process.%2520Evaluated%2520on%2520three%2520fine-grained%2520action%2520subsets%2520from%2520FineGym%250A%2528FX-JUMP%252C%2520FX-TURN%252C%2520and%2520FX-SALTO%2529%252C%2520FinePhys%2520significantly%2520outperforms%250Acompetitive%2520baselines.%2520Comprehensive%2520qualitative%2520results%2520further%2520demonstrate%250AFinePhys%2527s%2520ability%2520to%2520generate%2520more%2520natural%2520and%2520plausible%2520fine-grained%2520human%250Aactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinePhys%3A%20Fine-grained%20Human%20Action%20Generation%20by%20Explicitly%0A%20%20Incorporating%20Physical%20Laws%20for%20Effective%20Skeletal%20Guidance&entry.906535625=Dian%20Shao%20and%20Mingfei%20Shi%20and%20Shengda%20Xu%20and%20Haodong%20Chen%20and%20Yongle%20Huang%20and%20Binglu%20Wang&entry.1292438233=%20%20Despite%20significant%20advances%20in%20video%20generation%2C%20synthesizing%20physically%0Aplausible%20human%20actions%20remains%20a%20persistent%20challenge%2C%20particularly%20in%0Amodeling%20fine-grained%20semantics%20and%20complex%20temporal%20dynamics.%20For%20instance%2C%0Agenerating%20gymnastics%20routines%20such%20as%20%22switch%20leap%20with%200.5%20turn%22%20poses%0Asubstantial%20difficulties%20for%20current%20methods%2C%20often%20yielding%20unsatisfactory%0Aresults.%20To%20bridge%20this%20gap%2C%20we%20propose%20FinePhys%2C%20a%20Fine-grained%20human%20action%0Ageneration%20framework%20that%20incorporates%20Physics%20to%20obtain%20effective%20skeletal%0Aguidance.%20Specifically%2C%20FinePhys%20first%20estimates%202D%20poses%20in%20an%20online%20manner%0Aand%20then%20performs%202D-to-3D%20dimension%20lifting%20via%20in-context%20learning.%20To%0Amitigate%20the%20instability%20and%20limited%20interpretability%20of%20purely%20data-driven%203D%0Aposes%2C%20we%20further%20introduce%20a%20physics-based%20motion%20re-estimation%20module%0Agoverned%20by%20Euler-Lagrange%20equations%2C%20calculating%20joint%20accelerations%20via%0Abidirectional%20temporal%20updating.%20The%20physically%20predicted%203D%20poses%20are%20then%0Afused%20with%20data-driven%20ones%2C%20offering%20multi-scale%202D%20heatmap%20guidance%20for%20the%0Adiffusion%20process.%20Evaluated%20on%20three%20fine-grained%20action%20subsets%20from%20FineGym%0A%28FX-JUMP%2C%20FX-TURN%2C%20and%20FX-SALTO%29%2C%20FinePhys%20significantly%20outperforms%0Acompetitive%20baselines.%20Comprehensive%20qualitative%20results%20further%20demonstrate%0AFinePhys%27s%20ability%20to%20generate%20more%20natural%20and%20plausible%20fine-grained%20human%0Aactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13437v1&entry.124074799=Read"},
{"title": "MAGI-1: Autoregressive Video Generation at Scale", "author": "Sand. ai and Hansi Teng and Hongyu Jia and Lei Sun and Lingzhi Li and Maolin Li and Mingqiu Tang and Shuai Han and Tianning Zhang and W. Q. Zhang and Weifeng Luo and Xiaoyang Kang and Yuchen Sun and Yue Cao and Yunpeng Huang and Yutong Lin and Yuxin Fang and Zewei Tao and Zheng Zhang and Zhongshu Wang and Zixun Liu and Dai Shi and Guoli Su and Hanwen Sun and Hong Pan and Jie Wang and Jiexin Sheng and Min Cui and Min Hu and Ming Yan and Shucheng Yin and Siran Zhang and Tingting Liu and Xianping Yin and Xiaoyu Yang and Xin Song and Xuan Hu and Yankai Zhang and Yuqiao Li", "abstract": "  We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai.\n", "link": "http://arxiv.org/abs/2505.13211v1", "date": "2025-05-19", "relevancy": 3.1901, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6695}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.643}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGI-1%3A%20Autoregressive%20Video%20Generation%20at%20Scale&body=Title%3A%20MAGI-1%3A%20Autoregressive%20Video%20Generation%20at%20Scale%0AAuthor%3A%20Sand.%20ai%20and%20Hansi%20Teng%20and%20Hongyu%20Jia%20and%20Lei%20Sun%20and%20Lingzhi%20Li%20and%20Maolin%20Li%20and%20Mingqiu%20Tang%20and%20Shuai%20Han%20and%20Tianning%20Zhang%20and%20W.%20Q.%20Zhang%20and%20Weifeng%20Luo%20and%20Xiaoyang%20Kang%20and%20Yuchen%20Sun%20and%20Yue%20Cao%20and%20Yunpeng%20Huang%20and%20Yutong%20Lin%20and%20Yuxin%20Fang%20and%20Zewei%20Tao%20and%20Zheng%20Zhang%20and%20Zhongshu%20Wang%20and%20Zixun%20Liu%20and%20Dai%20Shi%20and%20Guoli%20Su%20and%20Hanwen%20Sun%20and%20Hong%20Pan%20and%20Jie%20Wang%20and%20Jiexin%20Sheng%20and%20Min%20Cui%20and%20Min%20Hu%20and%20Ming%20Yan%20and%20Shucheng%20Yin%20and%20Siran%20Zhang%20and%20Tingting%20Liu%20and%20Xianping%20Yin%20and%20Xiaoyu%20Yang%20and%20Xin%20Song%20and%20Xuan%20Hu%20and%20Yankai%20Zhang%20and%20Yuqiao%20Li%0AAbstract%3A%20%20%20We%20present%20MAGI-1%2C%20a%20world%20model%20that%20generates%20videos%20by%20autoregressively%0Apredicting%20a%20sequence%20of%20video%20chunks%2C%20defined%20as%20fixed-length%20segments%20of%0Aconsecutive%20frames.%20Trained%20to%20denoise%20per-chunk%20noise%20that%20increases%0Amonotonically%20over%20time%2C%20MAGI-1%20enables%20causal%20temporal%20modeling%20and%20naturally%0Asupports%20streaming%20generation.%20It%20achieves%20strong%20performance%20on%20image-to-video%0A%28I2V%29%20tasks%20conditioned%20on%20text%20instructions%2C%20providing%20high%20temporal%0Aconsistency%20and%20scalability%2C%20which%20are%20made%20possible%20by%20several%20algorithmic%0Ainnovations%20and%20a%20dedicated%20infrastructure%20stack.%20MAGI-1%20facilitates%0Acontrollable%20generation%20via%20chunk-wise%20prompting%20and%20supports%20real-time%2C%0Amemory-efficient%20deployment%20by%20maintaining%20constant%20peak%20inference%20cost%2C%0Aregardless%20of%20video%20length.%20The%20largest%20variant%20of%20MAGI-1%20comprises%2024%20billion%0Aparameters%20and%20supports%20context%20lengths%20of%20up%20to%204%20million%20tokens%2C%0Ademonstrating%20the%20scalability%20and%20robustness%20of%20our%20approach.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/SandAI-org/MAGI-1%20and%0Ahttps%3A//github.com/SandAI-org/MagiAttention.%20The%20product%20can%20be%20accessed%20at%0Ahttps%3A//sand.ai.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGI-1%253A%2520Autoregressive%2520Video%2520Generation%2520at%2520Scale%26entry.906535625%3DSand.%2520ai%2520and%2520Hansi%2520Teng%2520and%2520Hongyu%2520Jia%2520and%2520Lei%2520Sun%2520and%2520Lingzhi%2520Li%2520and%2520Maolin%2520Li%2520and%2520Mingqiu%2520Tang%2520and%2520Shuai%2520Han%2520and%2520Tianning%2520Zhang%2520and%2520W.%2520Q.%2520Zhang%2520and%2520Weifeng%2520Luo%2520and%2520Xiaoyang%2520Kang%2520and%2520Yuchen%2520Sun%2520and%2520Yue%2520Cao%2520and%2520Yunpeng%2520Huang%2520and%2520Yutong%2520Lin%2520and%2520Yuxin%2520Fang%2520and%2520Zewei%2520Tao%2520and%2520Zheng%2520Zhang%2520and%2520Zhongshu%2520Wang%2520and%2520Zixun%2520Liu%2520and%2520Dai%2520Shi%2520and%2520Guoli%2520Su%2520and%2520Hanwen%2520Sun%2520and%2520Hong%2520Pan%2520and%2520Jie%2520Wang%2520and%2520Jiexin%2520Sheng%2520and%2520Min%2520Cui%2520and%2520Min%2520Hu%2520and%2520Ming%2520Yan%2520and%2520Shucheng%2520Yin%2520and%2520Siran%2520Zhang%2520and%2520Tingting%2520Liu%2520and%2520Xianping%2520Yin%2520and%2520Xiaoyu%2520Yang%2520and%2520Xin%2520Song%2520and%2520Xuan%2520Hu%2520and%2520Yankai%2520Zhang%2520and%2520Yuqiao%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520MAGI-1%252C%2520a%2520world%2520model%2520that%2520generates%2520videos%2520by%2520autoregressively%250Apredicting%2520a%2520sequence%2520of%2520video%2520chunks%252C%2520defined%2520as%2520fixed-length%2520segments%2520of%250Aconsecutive%2520frames.%2520Trained%2520to%2520denoise%2520per-chunk%2520noise%2520that%2520increases%250Amonotonically%2520over%2520time%252C%2520MAGI-1%2520enables%2520causal%2520temporal%2520modeling%2520and%2520naturally%250Asupports%2520streaming%2520generation.%2520It%2520achieves%2520strong%2520performance%2520on%2520image-to-video%250A%2528I2V%2529%2520tasks%2520conditioned%2520on%2520text%2520instructions%252C%2520providing%2520high%2520temporal%250Aconsistency%2520and%2520scalability%252C%2520which%2520are%2520made%2520possible%2520by%2520several%2520algorithmic%250Ainnovations%2520and%2520a%2520dedicated%2520infrastructure%2520stack.%2520MAGI-1%2520facilitates%250Acontrollable%2520generation%2520via%2520chunk-wise%2520prompting%2520and%2520supports%2520real-time%252C%250Amemory-efficient%2520deployment%2520by%2520maintaining%2520constant%2520peak%2520inference%2520cost%252C%250Aregardless%2520of%2520video%2520length.%2520The%2520largest%2520variant%2520of%2520MAGI-1%2520comprises%252024%2520billion%250Aparameters%2520and%2520supports%2520context%2520lengths%2520of%2520up%2520to%25204%2520million%2520tokens%252C%250Ademonstrating%2520the%2520scalability%2520and%2520robustness%2520of%2520our%2520approach.%2520The%2520code%2520and%250Amodels%2520are%2520available%2520at%2520https%253A//github.com/SandAI-org/MAGI-1%2520and%250Ahttps%253A//github.com/SandAI-org/MagiAttention.%2520The%2520product%2520can%2520be%2520accessed%2520at%250Ahttps%253A//sand.ai.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGI-1%3A%20Autoregressive%20Video%20Generation%20at%20Scale&entry.906535625=Sand.%20ai%20and%20Hansi%20Teng%20and%20Hongyu%20Jia%20and%20Lei%20Sun%20and%20Lingzhi%20Li%20and%20Maolin%20Li%20and%20Mingqiu%20Tang%20and%20Shuai%20Han%20and%20Tianning%20Zhang%20and%20W.%20Q.%20Zhang%20and%20Weifeng%20Luo%20and%20Xiaoyang%20Kang%20and%20Yuchen%20Sun%20and%20Yue%20Cao%20and%20Yunpeng%20Huang%20and%20Yutong%20Lin%20and%20Yuxin%20Fang%20and%20Zewei%20Tao%20and%20Zheng%20Zhang%20and%20Zhongshu%20Wang%20and%20Zixun%20Liu%20and%20Dai%20Shi%20and%20Guoli%20Su%20and%20Hanwen%20Sun%20and%20Hong%20Pan%20and%20Jie%20Wang%20and%20Jiexin%20Sheng%20and%20Min%20Cui%20and%20Min%20Hu%20and%20Ming%20Yan%20and%20Shucheng%20Yin%20and%20Siran%20Zhang%20and%20Tingting%20Liu%20and%20Xianping%20Yin%20and%20Xiaoyu%20Yang%20and%20Xin%20Song%20and%20Xuan%20Hu%20and%20Yankai%20Zhang%20and%20Yuqiao%20Li&entry.1292438233=%20%20We%20present%20MAGI-1%2C%20a%20world%20model%20that%20generates%20videos%20by%20autoregressively%0Apredicting%20a%20sequence%20of%20video%20chunks%2C%20defined%20as%20fixed-length%20segments%20of%0Aconsecutive%20frames.%20Trained%20to%20denoise%20per-chunk%20noise%20that%20increases%0Amonotonically%20over%20time%2C%20MAGI-1%20enables%20causal%20temporal%20modeling%20and%20naturally%0Asupports%20streaming%20generation.%20It%20achieves%20strong%20performance%20on%20image-to-video%0A%28I2V%29%20tasks%20conditioned%20on%20text%20instructions%2C%20providing%20high%20temporal%0Aconsistency%20and%20scalability%2C%20which%20are%20made%20possible%20by%20several%20algorithmic%0Ainnovations%20and%20a%20dedicated%20infrastructure%20stack.%20MAGI-1%20facilitates%0Acontrollable%20generation%20via%20chunk-wise%20prompting%20and%20supports%20real-time%2C%0Amemory-efficient%20deployment%20by%20maintaining%20constant%20peak%20inference%20cost%2C%0Aregardless%20of%20video%20length.%20The%20largest%20variant%20of%20MAGI-1%20comprises%2024%20billion%0Aparameters%20and%20supports%20context%20lengths%20of%20up%20to%204%20million%20tokens%2C%0Ademonstrating%20the%20scalability%20and%20robustness%20of%20our%20approach.%20The%20code%20and%0Amodels%20are%20available%20at%20https%3A//github.com/SandAI-org/MAGI-1%20and%0Ahttps%3A//github.com/SandAI-org/MagiAttention.%20The%20product%20can%20be%20accessed%20at%0Ahttps%3A//sand.ai.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13211v1&entry.124074799=Read"},
{"title": "Multi-Faceted Multimodal Monosemanticity", "author": "Hanqi Yan and Xiangxiang Cui and Lu Yin and Paul Pu Liang and Yulan He and Yifei Wang", "abstract": "  Humans experience the world through multiple modalities, such as, vision,\nlanguage, and speech, making it natural to explore the commonality and\ndistinctions among them. In this work, we take a data-driven approach to\naddress this question by analyzing interpretable, monosemantic features\nextracted from deep multimodal models. Specifically, we investigate CLIP, a\nprominent visual-language representation model trained on massive image-text\npairs. Building on prior research in single-modal interpretability, we develop\na set of multi-modal interpretability tools and measures designed to\ndisentangle and analyze features learned from CLIP. Specifically, we introduce\nthe Modality Dominance Score (MDS) to attribute each CLIP feature to a specific\nmodality. We then map CLIP features into a more interpretable space, enabling\nus to categorize them into three distinct classes: vision features\n(single-modal), language features (single-modal), and visual-language features\n(cross-modal). Interestingly, this data-driven categorization closely aligns\nwith human intuitive understandings of different modalities. We further show\nthat this modality decomposition can benefit multiple downstream tasks,\nincluding reducing bias in gender detection, generating cross-modal adversarial\nexamples, and enabling modal-specific feature control in text-to-image\ngeneration. These results indicate that large-scale multimodal models, when\nequipped with task-agnostic interpretability tools, can offer valuable insights\ninto the relationships between different data modalities.\n", "link": "http://arxiv.org/abs/2502.14888v2", "date": "2025-05-19", "relevancy": 3.1151, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Faceted%20Multimodal%20Monosemanticity&body=Title%3A%20Multi-Faceted%20Multimodal%20Monosemanticity%0AAuthor%3A%20Hanqi%20Yan%20and%20Xiangxiang%20Cui%20and%20Lu%20Yin%20and%20Paul%20Pu%20Liang%20and%20Yulan%20He%20and%20Yifei%20Wang%0AAbstract%3A%20%20%20Humans%20experience%20the%20world%20through%20multiple%20modalities%2C%20such%20as%2C%20vision%2C%0Alanguage%2C%20and%20speech%2C%20making%20it%20natural%20to%20explore%20the%20commonality%20and%0Adistinctions%20among%20them.%20In%20this%20work%2C%20we%20take%20a%20data-driven%20approach%20to%0Aaddress%20this%20question%20by%20analyzing%20interpretable%2C%20monosemantic%20features%0Aextracted%20from%20deep%20multimodal%20models.%20Specifically%2C%20we%20investigate%20CLIP%2C%20a%0Aprominent%20visual-language%20representation%20model%20trained%20on%20massive%20image-text%0Apairs.%20Building%20on%20prior%20research%20in%20single-modal%20interpretability%2C%20we%20develop%0Aa%20set%20of%20multi-modal%20interpretability%20tools%20and%20measures%20designed%20to%0Adisentangle%20and%20analyze%20features%20learned%20from%20CLIP.%20Specifically%2C%20we%20introduce%0Athe%20Modality%20Dominance%20Score%20%28MDS%29%20to%20attribute%20each%20CLIP%20feature%20to%20a%20specific%0Amodality.%20We%20then%20map%20CLIP%20features%20into%20a%20more%20interpretable%20space%2C%20enabling%0Aus%20to%20categorize%20them%20into%20three%20distinct%20classes%3A%20vision%20features%0A%28single-modal%29%2C%20language%20features%20%28single-modal%29%2C%20and%20visual-language%20features%0A%28cross-modal%29.%20Interestingly%2C%20this%20data-driven%20categorization%20closely%20aligns%0Awith%20human%20intuitive%20understandings%20of%20different%20modalities.%20We%20further%20show%0Athat%20this%20modality%20decomposition%20can%20benefit%20multiple%20downstream%20tasks%2C%0Aincluding%20reducing%20bias%20in%20gender%20detection%2C%20generating%20cross-modal%20adversarial%0Aexamples%2C%20and%20enabling%20modal-specific%20feature%20control%20in%20text-to-image%0Ageneration.%20These%20results%20indicate%20that%20large-scale%20multimodal%20models%2C%20when%0Aequipped%20with%20task-agnostic%20interpretability%20tools%2C%20can%20offer%20valuable%20insights%0Ainto%20the%20relationships%20between%20different%20data%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Faceted%2520Multimodal%2520Monosemanticity%26entry.906535625%3DHanqi%2520Yan%2520and%2520Xiangxiang%2520Cui%2520and%2520Lu%2520Yin%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Yulan%2520He%2520and%2520Yifei%2520Wang%26entry.1292438233%3D%2520%2520Humans%2520experience%2520the%2520world%2520through%2520multiple%2520modalities%252C%2520such%2520as%252C%2520vision%252C%250Alanguage%252C%2520and%2520speech%252C%2520making%2520it%2520natural%2520to%2520explore%2520the%2520commonality%2520and%250Adistinctions%2520among%2520them.%2520In%2520this%2520work%252C%2520we%2520take%2520a%2520data-driven%2520approach%2520to%250Aaddress%2520this%2520question%2520by%2520analyzing%2520interpretable%252C%2520monosemantic%2520features%250Aextracted%2520from%2520deep%2520multimodal%2520models.%2520Specifically%252C%2520we%2520investigate%2520CLIP%252C%2520a%250Aprominent%2520visual-language%2520representation%2520model%2520trained%2520on%2520massive%2520image-text%250Apairs.%2520Building%2520on%2520prior%2520research%2520in%2520single-modal%2520interpretability%252C%2520we%2520develop%250Aa%2520set%2520of%2520multi-modal%2520interpretability%2520tools%2520and%2520measures%2520designed%2520to%250Adisentangle%2520and%2520analyze%2520features%2520learned%2520from%2520CLIP.%2520Specifically%252C%2520we%2520introduce%250Athe%2520Modality%2520Dominance%2520Score%2520%2528MDS%2529%2520to%2520attribute%2520each%2520CLIP%2520feature%2520to%2520a%2520specific%250Amodality.%2520We%2520then%2520map%2520CLIP%2520features%2520into%2520a%2520more%2520interpretable%2520space%252C%2520enabling%250Aus%2520to%2520categorize%2520them%2520into%2520three%2520distinct%2520classes%253A%2520vision%2520features%250A%2528single-modal%2529%252C%2520language%2520features%2520%2528single-modal%2529%252C%2520and%2520visual-language%2520features%250A%2528cross-modal%2529.%2520Interestingly%252C%2520this%2520data-driven%2520categorization%2520closely%2520aligns%250Awith%2520human%2520intuitive%2520understandings%2520of%2520different%2520modalities.%2520We%2520further%2520show%250Athat%2520this%2520modality%2520decomposition%2520can%2520benefit%2520multiple%2520downstream%2520tasks%252C%250Aincluding%2520reducing%2520bias%2520in%2520gender%2520detection%252C%2520generating%2520cross-modal%2520adversarial%250Aexamples%252C%2520and%2520enabling%2520modal-specific%2520feature%2520control%2520in%2520text-to-image%250Ageneration.%2520These%2520results%2520indicate%2520that%2520large-scale%2520multimodal%2520models%252C%2520when%250Aequipped%2520with%2520task-agnostic%2520interpretability%2520tools%252C%2520can%2520offer%2520valuable%2520insights%250Ainto%2520the%2520relationships%2520between%2520different%2520data%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Faceted%20Multimodal%20Monosemanticity&entry.906535625=Hanqi%20Yan%20and%20Xiangxiang%20Cui%20and%20Lu%20Yin%20and%20Paul%20Pu%20Liang%20and%20Yulan%20He%20and%20Yifei%20Wang&entry.1292438233=%20%20Humans%20experience%20the%20world%20through%20multiple%20modalities%2C%20such%20as%2C%20vision%2C%0Alanguage%2C%20and%20speech%2C%20making%20it%20natural%20to%20explore%20the%20commonality%20and%0Adistinctions%20among%20them.%20In%20this%20work%2C%20we%20take%20a%20data-driven%20approach%20to%0Aaddress%20this%20question%20by%20analyzing%20interpretable%2C%20monosemantic%20features%0Aextracted%20from%20deep%20multimodal%20models.%20Specifically%2C%20we%20investigate%20CLIP%2C%20a%0Aprominent%20visual-language%20representation%20model%20trained%20on%20massive%20image-text%0Apairs.%20Building%20on%20prior%20research%20in%20single-modal%20interpretability%2C%20we%20develop%0Aa%20set%20of%20multi-modal%20interpretability%20tools%20and%20measures%20designed%20to%0Adisentangle%20and%20analyze%20features%20learned%20from%20CLIP.%20Specifically%2C%20we%20introduce%0Athe%20Modality%20Dominance%20Score%20%28MDS%29%20to%20attribute%20each%20CLIP%20feature%20to%20a%20specific%0Amodality.%20We%20then%20map%20CLIP%20features%20into%20a%20more%20interpretable%20space%2C%20enabling%0Aus%20to%20categorize%20them%20into%20three%20distinct%20classes%3A%20vision%20features%0A%28single-modal%29%2C%20language%20features%20%28single-modal%29%2C%20and%20visual-language%20features%0A%28cross-modal%29.%20Interestingly%2C%20this%20data-driven%20categorization%20closely%20aligns%0Awith%20human%20intuitive%20understandings%20of%20different%20modalities.%20We%20further%20show%0Athat%20this%20modality%20decomposition%20can%20benefit%20multiple%20downstream%20tasks%2C%0Aincluding%20reducing%20bias%20in%20gender%20detection%2C%20generating%20cross-modal%20adversarial%0Aexamples%2C%20and%20enabling%20modal-specific%20feature%20control%20in%20text-to-image%0Ageneration.%20These%20results%20indicate%20that%20large-scale%20multimodal%20models%2C%20when%0Aequipped%20with%20task-agnostic%20interpretability%20tools%2C%20can%20offer%20valuable%20insights%0Ainto%20the%20relationships%20between%20different%20data%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14888v2&entry.124074799=Read"},
{"title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with\n  Vision-Language Instruction Fine-Tuning", "author": "Run Luo and Renke Shan and Longze Chen and Ziqiang Liu and Lu Wang and Min Yang and Xiaobo Xia", "abstract": "  Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM.\n", "link": "http://arxiv.org/abs/2504.19627v2", "date": "2025-05-19", "relevancy": 3.0422, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCM%3A%20Vision%20Concept%20Modeling%20Based%20on%20Implicit%20Contrastive%20Learning%20with%0A%20%20Vision-Language%20Instruction%20Fine-Tuning&body=Title%3A%20VCM%3A%20Vision%20Concept%20Modeling%20Based%20on%20Implicit%20Contrastive%20Learning%20with%0A%20%20Vision-Language%20Instruction%20Fine-Tuning%0AAuthor%3A%20Run%20Luo%20and%20Renke%20Shan%20and%20Longze%20Chen%20and%20Ziqiang%20Liu%20and%20Lu%20Wang%20and%20Min%20Yang%20and%20Xiaobo%20Xia%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20pivotal%20for%20real-world%20AI%20tasks%20like%0Aembodied%20intelligence%20due%20to%20their%20strong%20vision-language%20reasoning%20abilities.%0AHowever%2C%20current%20LVLMs%20process%20entire%20images%20at%20the%20token%20level%2C%20which%20is%0Ainefficient%20compared%20to%20humans%20who%20analyze%20information%20and%20generate%20content%20at%0Athe%20conceptual%20level%2C%20extracting%20relevant%20visual%20concepts%20with%20minimal%20effort.%0AThis%20inefficiency%2C%20stemming%20from%20the%20lack%20of%20a%20visual%20concept%20model%2C%20limits%0ALVLMs%27%20usability%20in%20real-world%20applications.%20To%20address%20this%2C%20we%20propose%20VCM%2C%0Aan%20end-to-end%20self-supervised%20visual%20concept%20modeling%20framework.%20VCM%20leverages%0Aimplicit%20contrastive%20learning%20across%20multiple%20sampled%20instances%20and%0Avision-language%20fine-tuning%20to%20construct%20a%20visual%20concept%20model%20without%0Arequiring%20costly%20concept-level%20annotations.%20Our%20results%20show%20that%20VCM%0Asignificantly%20reduces%20computational%20costs%20%28e.g.%2C%2085%5C%25%20fewer%20FLOPs%20for%0ALLaVA-1.5-7B%29%20while%20maintaining%20strong%20performance%20across%20diverse%20image%0Aunderstanding%20tasks.%20Moreover%2C%20VCM%20enhances%20visual%20encoders%27%20capabilities%20in%0Aclassic%20visual%20concept%20perception%20tasks.%20Extensive%20quantitative%20and%20qualitative%0Aexperiments%20validate%20the%20effectiveness%20and%20efficiency%20of%20VCM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19627v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCM%253A%2520Vision%2520Concept%2520Modeling%2520Based%2520on%2520Implicit%2520Contrastive%2520Learning%2520with%250A%2520%2520Vision-Language%2520Instruction%2520Fine-Tuning%26entry.906535625%3DRun%2520Luo%2520and%2520Renke%2520Shan%2520and%2520Longze%2520Chen%2520and%2520Ziqiang%2520Liu%2520and%2520Lu%2520Wang%2520and%2520Min%2520Yang%2520and%2520Xiaobo%2520Xia%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520are%2520pivotal%2520for%2520real-world%2520AI%2520tasks%2520like%250Aembodied%2520intelligence%2520due%2520to%2520their%2520strong%2520vision-language%2520reasoning%2520abilities.%250AHowever%252C%2520current%2520LVLMs%2520process%2520entire%2520images%2520at%2520the%2520token%2520level%252C%2520which%2520is%250Ainefficient%2520compared%2520to%2520humans%2520who%2520analyze%2520information%2520and%2520generate%2520content%2520at%250Athe%2520conceptual%2520level%252C%2520extracting%2520relevant%2520visual%2520concepts%2520with%2520minimal%2520effort.%250AThis%2520inefficiency%252C%2520stemming%2520from%2520the%2520lack%2520of%2520a%2520visual%2520concept%2520model%252C%2520limits%250ALVLMs%2527%2520usability%2520in%2520real-world%2520applications.%2520To%2520address%2520this%252C%2520we%2520propose%2520VCM%252C%250Aan%2520end-to-end%2520self-supervised%2520visual%2520concept%2520modeling%2520framework.%2520VCM%2520leverages%250Aimplicit%2520contrastive%2520learning%2520across%2520multiple%2520sampled%2520instances%2520and%250Avision-language%2520fine-tuning%2520to%2520construct%2520a%2520visual%2520concept%2520model%2520without%250Arequiring%2520costly%2520concept-level%2520annotations.%2520Our%2520results%2520show%2520that%2520VCM%250Asignificantly%2520reduces%2520computational%2520costs%2520%2528e.g.%252C%252085%255C%2525%2520fewer%2520FLOPs%2520for%250ALLaVA-1.5-7B%2529%2520while%2520maintaining%2520strong%2520performance%2520across%2520diverse%2520image%250Aunderstanding%2520tasks.%2520Moreover%252C%2520VCM%2520enhances%2520visual%2520encoders%2527%2520capabilities%2520in%250Aclassic%2520visual%2520concept%2520perception%2520tasks.%2520Extensive%2520quantitative%2520and%2520qualitative%250Aexperiments%2520validate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520VCM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19627v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCM%3A%20Vision%20Concept%20Modeling%20Based%20on%20Implicit%20Contrastive%20Learning%20with%0A%20%20Vision-Language%20Instruction%20Fine-Tuning&entry.906535625=Run%20Luo%20and%20Renke%20Shan%20and%20Longze%20Chen%20and%20Ziqiang%20Liu%20and%20Lu%20Wang%20and%20Min%20Yang%20and%20Xiaobo%20Xia&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20are%20pivotal%20for%20real-world%20AI%20tasks%20like%0Aembodied%20intelligence%20due%20to%20their%20strong%20vision-language%20reasoning%20abilities.%0AHowever%2C%20current%20LVLMs%20process%20entire%20images%20at%20the%20token%20level%2C%20which%20is%0Ainefficient%20compared%20to%20humans%20who%20analyze%20information%20and%20generate%20content%20at%0Athe%20conceptual%20level%2C%20extracting%20relevant%20visual%20concepts%20with%20minimal%20effort.%0AThis%20inefficiency%2C%20stemming%20from%20the%20lack%20of%20a%20visual%20concept%20model%2C%20limits%0ALVLMs%27%20usability%20in%20real-world%20applications.%20To%20address%20this%2C%20we%20propose%20VCM%2C%0Aan%20end-to-end%20self-supervised%20visual%20concept%20modeling%20framework.%20VCM%20leverages%0Aimplicit%20contrastive%20learning%20across%20multiple%20sampled%20instances%20and%0Avision-language%20fine-tuning%20to%20construct%20a%20visual%20concept%20model%20without%0Arequiring%20costly%20concept-level%20annotations.%20Our%20results%20show%20that%20VCM%0Asignificantly%20reduces%20computational%20costs%20%28e.g.%2C%2085%5C%25%20fewer%20FLOPs%20for%0ALLaVA-1.5-7B%29%20while%20maintaining%20strong%20performance%20across%20diverse%20image%0Aunderstanding%20tasks.%20Moreover%2C%20VCM%20enhances%20visual%20encoders%27%20capabilities%20in%0Aclassic%20visual%20concept%20perception%20tasks.%20Extensive%20quantitative%20and%20qualitative%0Aexperiments%20validate%20the%20effectiveness%20and%20efficiency%20of%20VCM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19627v2&entry.124074799=Read"},
{"title": "M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human\n  Motion Synthesis", "author": "Zhizhuo Yin and Yuk Hang Tsui and Pan Hui", "abstract": "  Generating full-body human gestures encompassing face, body, hands, and\nglobal movements from audio is a valuable yet challenging task in virtual\navatar creation. Previous systems focused on tokenizing the human gestures\nframewisely and predicting the tokens of each frame from the input audio.\nHowever, one observation is that the number of frames required for a complete\nexpressive human gesture, defined as granularity, varies among different human\ngesture patterns. Existing systems fail to model these gesture patterns due to\nthe fixed granularity of their gesture tokens. To solve this problem, we\npropose a novel framework named Multi-Granular Gesture Generator (M3G) for\naudio-driven holistic gesture generation. In M3G, we propose a novel\nMulti-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct\nmotion sequences from different temporal granularities. Subsequently, we\nproposed a multi-granular token predictor that extracts multi-granular\ninformation from audio and predicts the corresponding motion tokens. Then M3G\nreconstructs the human gestures from the predicted tokens using the MGVQ-VAE.\nBoth objective and subjective experiments demonstrate that our proposed M3G\nframework outperforms the state-of-the-art methods in terms of generating\nnatural and expressive full-body human gestures.\n", "link": "http://arxiv.org/abs/2505.08293v2", "date": "2025-05-19", "relevancy": 2.9989, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6378}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5808}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3G%3A%20Multi-Granular%20Gesture%20Generator%20for%20Audio-Driven%20Full-Body%20Human%0A%20%20Motion%20Synthesis&body=Title%3A%20M3G%3A%20Multi-Granular%20Gesture%20Generator%20for%20Audio-Driven%20Full-Body%20Human%0A%20%20Motion%20Synthesis%0AAuthor%3A%20Zhizhuo%20Yin%20and%20Yuk%20Hang%20Tsui%20and%20Pan%20Hui%0AAbstract%3A%20%20%20Generating%20full-body%20human%20gestures%20encompassing%20face%2C%20body%2C%20hands%2C%20and%0Aglobal%20movements%20from%20audio%20is%20a%20valuable%20yet%20challenging%20task%20in%20virtual%0Aavatar%20creation.%20Previous%20systems%20focused%20on%20tokenizing%20the%20human%20gestures%0Aframewisely%20and%20predicting%20the%20tokens%20of%20each%20frame%20from%20the%20input%20audio.%0AHowever%2C%20one%20observation%20is%20that%20the%20number%20of%20frames%20required%20for%20a%20complete%0Aexpressive%20human%20gesture%2C%20defined%20as%20granularity%2C%20varies%20among%20different%20human%0Agesture%20patterns.%20Existing%20systems%20fail%20to%20model%20these%20gesture%20patterns%20due%20to%0Athe%20fixed%20granularity%20of%20their%20gesture%20tokens.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20novel%20framework%20named%20Multi-Granular%20Gesture%20Generator%20%28M3G%29%20for%0Aaudio-driven%20holistic%20gesture%20generation.%20In%20M3G%2C%20we%20propose%20a%20novel%0AMulti-Granular%20VQ-VAE%20%28MGVQ-VAE%29%20to%20tokenize%20motion%20patterns%20and%20reconstruct%0Amotion%20sequences%20from%20different%20temporal%20granularities.%20Subsequently%2C%20we%0Aproposed%20a%20multi-granular%20token%20predictor%20that%20extracts%20multi-granular%0Ainformation%20from%20audio%20and%20predicts%20the%20corresponding%20motion%20tokens.%20Then%20M3G%0Areconstructs%20the%20human%20gestures%20from%20the%20predicted%20tokens%20using%20the%20MGVQ-VAE.%0ABoth%20objective%20and%20subjective%20experiments%20demonstrate%20that%20our%20proposed%20M3G%0Aframework%20outperforms%20the%20state-of-the-art%20methods%20in%20terms%20of%20generating%0Anatural%20and%20expressive%20full-body%20human%20gestures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3G%253A%2520Multi-Granular%2520Gesture%2520Generator%2520for%2520Audio-Driven%2520Full-Body%2520Human%250A%2520%2520Motion%2520Synthesis%26entry.906535625%3DZhizhuo%2520Yin%2520and%2520Yuk%2520Hang%2520Tsui%2520and%2520Pan%2520Hui%26entry.1292438233%3D%2520%2520Generating%2520full-body%2520human%2520gestures%2520encompassing%2520face%252C%2520body%252C%2520hands%252C%2520and%250Aglobal%2520movements%2520from%2520audio%2520is%2520a%2520valuable%2520yet%2520challenging%2520task%2520in%2520virtual%250Aavatar%2520creation.%2520Previous%2520systems%2520focused%2520on%2520tokenizing%2520the%2520human%2520gestures%250Aframewisely%2520and%2520predicting%2520the%2520tokens%2520of%2520each%2520frame%2520from%2520the%2520input%2520audio.%250AHowever%252C%2520one%2520observation%2520is%2520that%2520the%2520number%2520of%2520frames%2520required%2520for%2520a%2520complete%250Aexpressive%2520human%2520gesture%252C%2520defined%2520as%2520granularity%252C%2520varies%2520among%2520different%2520human%250Agesture%2520patterns.%2520Existing%2520systems%2520fail%2520to%2520model%2520these%2520gesture%2520patterns%2520due%2520to%250Athe%2520fixed%2520granularity%2520of%2520their%2520gesture%2520tokens.%2520To%2520solve%2520this%2520problem%252C%2520we%250Apropose%2520a%2520novel%2520framework%2520named%2520Multi-Granular%2520Gesture%2520Generator%2520%2528M3G%2529%2520for%250Aaudio-driven%2520holistic%2520gesture%2520generation.%2520In%2520M3G%252C%2520we%2520propose%2520a%2520novel%250AMulti-Granular%2520VQ-VAE%2520%2528MGVQ-VAE%2529%2520to%2520tokenize%2520motion%2520patterns%2520and%2520reconstruct%250Amotion%2520sequences%2520from%2520different%2520temporal%2520granularities.%2520Subsequently%252C%2520we%250Aproposed%2520a%2520multi-granular%2520token%2520predictor%2520that%2520extracts%2520multi-granular%250Ainformation%2520from%2520audio%2520and%2520predicts%2520the%2520corresponding%2520motion%2520tokens.%2520Then%2520M3G%250Areconstructs%2520the%2520human%2520gestures%2520from%2520the%2520predicted%2520tokens%2520using%2520the%2520MGVQ-VAE.%250ABoth%2520objective%2520and%2520subjective%2520experiments%2520demonstrate%2520that%2520our%2520proposed%2520M3G%250Aframework%2520outperforms%2520the%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520generating%250Anatural%2520and%2520expressive%2520full-body%2520human%2520gestures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3G%3A%20Multi-Granular%20Gesture%20Generator%20for%20Audio-Driven%20Full-Body%20Human%0A%20%20Motion%20Synthesis&entry.906535625=Zhizhuo%20Yin%20and%20Yuk%20Hang%20Tsui%20and%20Pan%20Hui&entry.1292438233=%20%20Generating%20full-body%20human%20gestures%20encompassing%20face%2C%20body%2C%20hands%2C%20and%0Aglobal%20movements%20from%20audio%20is%20a%20valuable%20yet%20challenging%20task%20in%20virtual%0Aavatar%20creation.%20Previous%20systems%20focused%20on%20tokenizing%20the%20human%20gestures%0Aframewisely%20and%20predicting%20the%20tokens%20of%20each%20frame%20from%20the%20input%20audio.%0AHowever%2C%20one%20observation%20is%20that%20the%20number%20of%20frames%20required%20for%20a%20complete%0Aexpressive%20human%20gesture%2C%20defined%20as%20granularity%2C%20varies%20among%20different%20human%0Agesture%20patterns.%20Existing%20systems%20fail%20to%20model%20these%20gesture%20patterns%20due%20to%0Athe%20fixed%20granularity%20of%20their%20gesture%20tokens.%20To%20solve%20this%20problem%2C%20we%0Apropose%20a%20novel%20framework%20named%20Multi-Granular%20Gesture%20Generator%20%28M3G%29%20for%0Aaudio-driven%20holistic%20gesture%20generation.%20In%20M3G%2C%20we%20propose%20a%20novel%0AMulti-Granular%20VQ-VAE%20%28MGVQ-VAE%29%20to%20tokenize%20motion%20patterns%20and%20reconstruct%0Amotion%20sequences%20from%20different%20temporal%20granularities.%20Subsequently%2C%20we%0Aproposed%20a%20multi-granular%20token%20predictor%20that%20extracts%20multi-granular%0Ainformation%20from%20audio%20and%20predicts%20the%20corresponding%20motion%20tokens.%20Then%20M3G%0Areconstructs%20the%20human%20gestures%20from%20the%20predicted%20tokens%20using%20the%20MGVQ-VAE.%0ABoth%20objective%20and%20subjective%20experiments%20demonstrate%20that%20our%20proposed%20M3G%0Aframework%20outperforms%20the%20state-of-the-art%20methods%20in%20terms%20of%20generating%0Anatural%20and%20expressive%20full-body%20human%20gestures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08293v2&entry.124074799=Read"},
{"title": "From Local Details to Global Context: Advancing Vision-Language Models\n  with Attention-Based Selection", "author": "Lincan Cai and Jingxuan Kang and Shuang Li and Wenxuan Ma and Binhui Xie and Zhida Qin and Jian Liang", "abstract": "  Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.\n", "link": "http://arxiv.org/abs/2505.13233v1", "date": "2025-05-19", "relevancy": 2.93, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Local%20Details%20to%20Global%20Context%3A%20Advancing%20Vision-Language%20Models%0A%20%20with%20Attention-Based%20Selection&body=Title%3A%20From%20Local%20Details%20to%20Global%20Context%3A%20Advancing%20Vision-Language%20Models%0A%20%20with%20Attention-Based%20Selection%0AAuthor%3A%20Lincan%20Cai%20and%20Jingxuan%20Kang%20and%20Shuang%20Li%20and%20Wenxuan%20Ma%20and%20Binhui%20Xie%20and%20Zhida%20Qin%20and%20Jian%20Liang%0AAbstract%3A%20%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20e.g.%2C%20CLIP%2C%20demonstrate%20impressive%0Azero-shot%20capabilities%20on%20downstream%20tasks.%20Prior%20research%20highlights%20the%0Acrucial%20role%20of%20visual%20augmentation%20techniques%2C%20like%20random%20cropping%2C%20in%0Aalignment%20with%20fine-grained%20class%20descriptions%20generated%20by%20large%20language%0Amodels%20%28LLMs%29%2C%20significantly%20enhancing%20zero-shot%20performance%20by%20incorporating%0Amulti-view%20information.%20However%2C%20the%20inherent%20randomness%20of%20these%20augmentations%0Acan%20inevitably%20introduce%20background%20artifacts%20and%20cause%20models%20to%20overly%20focus%0Aon%20local%20details%2C%20compromising%20global%20semantic%20understanding.%20To%20address%20these%0Aissues%2C%20we%20propose%20an%20%5Ctextbf%7BA%7Dttention-%5Ctextbf%7BB%7Dased%20%5Ctextbf%7BS%7Delection%0A%28%5Ctextbf%7BABS%7D%29%20method%20from%20local%20details%20to%20global%20context%2C%20which%20applies%0Aattention-guided%20cropping%20in%20both%20raw%20images%20and%20feature%20space%2C%20supplement%0Aglobal%20semantic%20information%20through%20strategic%20feature%20selection.%20Additionally%2C%0Awe%20introduce%20a%20soft%20matching%20technique%20to%20effectively%20filter%20LLM%20descriptions%0Afor%20better%20alignment.%20%5Ctextbf%7BABS%7D%20achieves%20state-of-the-art%20performance%20on%0Aout-of-distribution%20generalization%20and%20zero-shot%20classification%20tasks.%20Notably%2C%0A%5Ctextbf%7BABS%7D%20is%20training-free%20and%20even%20rivals%20few-shot%20and%20test-time%20adaptation%0Amethods.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/BIT-DA/ABS%7D%7B%5Ctextcolor%7Bdarkgreen%7D%7Bhttps%3A//github.com/BIT-DA/ABS%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Local%2520Details%2520to%2520Global%2520Context%253A%2520Advancing%2520Vision-Language%2520Models%250A%2520%2520with%2520Attention-Based%2520Selection%26entry.906535625%3DLincan%2520Cai%2520and%2520Jingxuan%2520Kang%2520and%2520Shuang%2520Li%2520and%2520Wenxuan%2520Ma%2520and%2520Binhui%2520Xie%2520and%2520Zhida%2520Qin%2520and%2520Jian%2520Liang%26entry.1292438233%3D%2520%2520Pretrained%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520e.g.%252C%2520CLIP%252C%2520demonstrate%2520impressive%250Azero-shot%2520capabilities%2520on%2520downstream%2520tasks.%2520Prior%2520research%2520highlights%2520the%250Acrucial%2520role%2520of%2520visual%2520augmentation%2520techniques%252C%2520like%2520random%2520cropping%252C%2520in%250Aalignment%2520with%2520fine-grained%2520class%2520descriptions%2520generated%2520by%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520significantly%2520enhancing%2520zero-shot%2520performance%2520by%2520incorporating%250Amulti-view%2520information.%2520However%252C%2520the%2520inherent%2520randomness%2520of%2520these%2520augmentations%250Acan%2520inevitably%2520introduce%2520background%2520artifacts%2520and%2520cause%2520models%2520to%2520overly%2520focus%250Aon%2520local%2520details%252C%2520compromising%2520global%2520semantic%2520understanding.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520an%2520%255Ctextbf%257BA%257Dttention-%255Ctextbf%257BB%257Dased%2520%255Ctextbf%257BS%257Delection%250A%2528%255Ctextbf%257BABS%257D%2529%2520method%2520from%2520local%2520details%2520to%2520global%2520context%252C%2520which%2520applies%250Aattention-guided%2520cropping%2520in%2520both%2520raw%2520images%2520and%2520feature%2520space%252C%2520supplement%250Aglobal%2520semantic%2520information%2520through%2520strategic%2520feature%2520selection.%2520Additionally%252C%250Awe%2520introduce%2520a%2520soft%2520matching%2520technique%2520to%2520effectively%2520filter%2520LLM%2520descriptions%250Afor%2520better%2520alignment.%2520%255Ctextbf%257BABS%257D%2520achieves%2520state-of-the-art%2520performance%2520on%250Aout-of-distribution%2520generalization%2520and%2520zero-shot%2520classification%2520tasks.%2520Notably%252C%250A%255Ctextbf%257BABS%257D%2520is%2520training-free%2520and%2520even%2520rivals%2520few-shot%2520and%2520test-time%2520adaptation%250Amethods.%2520Our%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/BIT-DA/ABS%257D%257B%255Ctextcolor%257Bdarkgreen%257D%257Bhttps%253A//github.com/BIT-DA/ABS%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Local%20Details%20to%20Global%20Context%3A%20Advancing%20Vision-Language%20Models%0A%20%20with%20Attention-Based%20Selection&entry.906535625=Lincan%20Cai%20and%20Jingxuan%20Kang%20and%20Shuang%20Li%20and%20Wenxuan%20Ma%20and%20Binhui%20Xie%20and%20Zhida%20Qin%20and%20Jian%20Liang&entry.1292438233=%20%20Pretrained%20vision-language%20models%20%28VLMs%29%2C%20e.g.%2C%20CLIP%2C%20demonstrate%20impressive%0Azero-shot%20capabilities%20on%20downstream%20tasks.%20Prior%20research%20highlights%20the%0Acrucial%20role%20of%20visual%20augmentation%20techniques%2C%20like%20random%20cropping%2C%20in%0Aalignment%20with%20fine-grained%20class%20descriptions%20generated%20by%20large%20language%0Amodels%20%28LLMs%29%2C%20significantly%20enhancing%20zero-shot%20performance%20by%20incorporating%0Amulti-view%20information.%20However%2C%20the%20inherent%20randomness%20of%20these%20augmentations%0Acan%20inevitably%20introduce%20background%20artifacts%20and%20cause%20models%20to%20overly%20focus%0Aon%20local%20details%2C%20compromising%20global%20semantic%20understanding.%20To%20address%20these%0Aissues%2C%20we%20propose%20an%20%5Ctextbf%7BA%7Dttention-%5Ctextbf%7BB%7Dased%20%5Ctextbf%7BS%7Delection%0A%28%5Ctextbf%7BABS%7D%29%20method%20from%20local%20details%20to%20global%20context%2C%20which%20applies%0Aattention-guided%20cropping%20in%20both%20raw%20images%20and%20feature%20space%2C%20supplement%0Aglobal%20semantic%20information%20through%20strategic%20feature%20selection.%20Additionally%2C%0Awe%20introduce%20a%20soft%20matching%20technique%20to%20effectively%20filter%20LLM%20descriptions%0Afor%20better%20alignment.%20%5Ctextbf%7BABS%7D%20achieves%20state-of-the-art%20performance%20on%0Aout-of-distribution%20generalization%20and%20zero-shot%20classification%20tasks.%20Notably%2C%0A%5Ctextbf%7BABS%7D%20is%20training-free%20and%20even%20rivals%20few-shot%20and%20test-time%20adaptation%0Amethods.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/BIT-DA/ABS%7D%7B%5Ctextcolor%7Bdarkgreen%7D%7Bhttps%3A//github.com/BIT-DA/ABS%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13233v1&entry.124074799=Read"},
{"title": "Computer Vision Models Show Human-Like Sensitivity to Geometric and\n  Topological Concepts", "author": "Zekun Wang and Sashank Varma", "abstract": "  With the rapid improvement of machine learning (ML) models, cognitive\nscientists are increasingly asking about their alignment with how humans think.\nHere, we ask this question for computer vision models and human sensitivity to\ngeometric and topological (GT) concepts. Under the core knowledge account,\nthese concepts are innate and supported by dedicated neural circuitry. In this\nwork, we investigate an alternative explanation, that GT concepts are learned\n``for free'' through everyday interaction with the environment. We do so using\ncomputer visions models, which are trained on large image datasets. We build on\nprior studies to investigate the overall performance and human alignment of\nthree classes of models -- convolutional neural networks (CNNs),\ntransformer-based models, and vision-language models -- on an odd-one-out task\ntesting 43 GT concepts spanning seven classes. Transformer-based models achieve\nthe highest overall accuracy, surpassing that of young children. They also show\nstrong alignment with children's performance, finding the same classes of\nconcepts easy vs. difficult. By contrast, vision-language models underperform\ntheir vision-only counterparts and deviate further from human profiles,\nindicating that na\\\"ive multimodality might compromise abstract geometric\nsensitivity. These findings support the use of computer vision models to\nevaluate the sufficiency of the learning account for explaining human\nsensitivity to GT concepts, while also suggesting that integrating linguistic\nand visual representations might have unpredicted deleterious consequences.\n", "link": "http://arxiv.org/abs/2505.13281v1", "date": "2025-05-19", "relevancy": 2.8998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6027}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer%20Vision%20Models%20Show%20Human-Like%20Sensitivity%20to%20Geometric%20and%0A%20%20Topological%20Concepts&body=Title%3A%20Computer%20Vision%20Models%20Show%20Human-Like%20Sensitivity%20to%20Geometric%20and%0A%20%20Topological%20Concepts%0AAuthor%3A%20Zekun%20Wang%20and%20Sashank%20Varma%0AAbstract%3A%20%20%20With%20the%20rapid%20improvement%20of%20machine%20learning%20%28ML%29%20models%2C%20cognitive%0Ascientists%20are%20increasingly%20asking%20about%20their%20alignment%20with%20how%20humans%20think.%0AHere%2C%20we%20ask%20this%20question%20for%20computer%20vision%20models%20and%20human%20sensitivity%20to%0Ageometric%20and%20topological%20%28GT%29%20concepts.%20Under%20the%20core%20knowledge%20account%2C%0Athese%20concepts%20are%20innate%20and%20supported%20by%20dedicated%20neural%20circuitry.%20In%20this%0Awork%2C%20we%20investigate%20an%20alternative%20explanation%2C%20that%20GT%20concepts%20are%20learned%0A%60%60for%20free%27%27%20through%20everyday%20interaction%20with%20the%20environment.%20We%20do%20so%20using%0Acomputer%20visions%20models%2C%20which%20are%20trained%20on%20large%20image%20datasets.%20We%20build%20on%0Aprior%20studies%20to%20investigate%20the%20overall%20performance%20and%20human%20alignment%20of%0Athree%20classes%20of%20models%20--%20convolutional%20neural%20networks%20%28CNNs%29%2C%0Atransformer-based%20models%2C%20and%20vision-language%20models%20--%20on%20an%20odd-one-out%20task%0Atesting%2043%20GT%20concepts%20spanning%20seven%20classes.%20Transformer-based%20models%20achieve%0Athe%20highest%20overall%20accuracy%2C%20surpassing%20that%20of%20young%20children.%20They%20also%20show%0Astrong%20alignment%20with%20children%27s%20performance%2C%20finding%20the%20same%20classes%20of%0Aconcepts%20easy%20vs.%20difficult.%20By%20contrast%2C%20vision-language%20models%20underperform%0Atheir%20vision-only%20counterparts%20and%20deviate%20further%20from%20human%20profiles%2C%0Aindicating%20that%20na%5C%22ive%20multimodality%20might%20compromise%20abstract%20geometric%0Asensitivity.%20These%20findings%20support%20the%20use%20of%20computer%20vision%20models%20to%0Aevaluate%20the%20sufficiency%20of%20the%20learning%20account%20for%20explaining%20human%0Asensitivity%20to%20GT%20concepts%2C%20while%20also%20suggesting%20that%20integrating%20linguistic%0Aand%20visual%20representations%20might%20have%20unpredicted%20deleterious%20consequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer%2520Vision%2520Models%2520Show%2520Human-Like%2520Sensitivity%2520to%2520Geometric%2520and%250A%2520%2520Topological%2520Concepts%26entry.906535625%3DZekun%2520Wang%2520and%2520Sashank%2520Varma%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520improvement%2520of%2520machine%2520learning%2520%2528ML%2529%2520models%252C%2520cognitive%250Ascientists%2520are%2520increasingly%2520asking%2520about%2520their%2520alignment%2520with%2520how%2520humans%2520think.%250AHere%252C%2520we%2520ask%2520this%2520question%2520for%2520computer%2520vision%2520models%2520and%2520human%2520sensitivity%2520to%250Ageometric%2520and%2520topological%2520%2528GT%2529%2520concepts.%2520Under%2520the%2520core%2520knowledge%2520account%252C%250Athese%2520concepts%2520are%2520innate%2520and%2520supported%2520by%2520dedicated%2520neural%2520circuitry.%2520In%2520this%250Awork%252C%2520we%2520investigate%2520an%2520alternative%2520explanation%252C%2520that%2520GT%2520concepts%2520are%2520learned%250A%2560%2560for%2520free%2527%2527%2520through%2520everyday%2520interaction%2520with%2520the%2520environment.%2520We%2520do%2520so%2520using%250Acomputer%2520visions%2520models%252C%2520which%2520are%2520trained%2520on%2520large%2520image%2520datasets.%2520We%2520build%2520on%250Aprior%2520studies%2520to%2520investigate%2520the%2520overall%2520performance%2520and%2520human%2520alignment%2520of%250Athree%2520classes%2520of%2520models%2520--%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%250Atransformer-based%2520models%252C%2520and%2520vision-language%2520models%2520--%2520on%2520an%2520odd-one-out%2520task%250Atesting%252043%2520GT%2520concepts%2520spanning%2520seven%2520classes.%2520Transformer-based%2520models%2520achieve%250Athe%2520highest%2520overall%2520accuracy%252C%2520surpassing%2520that%2520of%2520young%2520children.%2520They%2520also%2520show%250Astrong%2520alignment%2520with%2520children%2527s%2520performance%252C%2520finding%2520the%2520same%2520classes%2520of%250Aconcepts%2520easy%2520vs.%2520difficult.%2520By%2520contrast%252C%2520vision-language%2520models%2520underperform%250Atheir%2520vision-only%2520counterparts%2520and%2520deviate%2520further%2520from%2520human%2520profiles%252C%250Aindicating%2520that%2520na%255C%2522ive%2520multimodality%2520might%2520compromise%2520abstract%2520geometric%250Asensitivity.%2520These%2520findings%2520support%2520the%2520use%2520of%2520computer%2520vision%2520models%2520to%250Aevaluate%2520the%2520sufficiency%2520of%2520the%2520learning%2520account%2520for%2520explaining%2520human%250Asensitivity%2520to%2520GT%2520concepts%252C%2520while%2520also%2520suggesting%2520that%2520integrating%2520linguistic%250Aand%2520visual%2520representations%2520might%2520have%2520unpredicted%2520deleterious%2520consequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer%20Vision%20Models%20Show%20Human-Like%20Sensitivity%20to%20Geometric%20and%0A%20%20Topological%20Concepts&entry.906535625=Zekun%20Wang%20and%20Sashank%20Varma&entry.1292438233=%20%20With%20the%20rapid%20improvement%20of%20machine%20learning%20%28ML%29%20models%2C%20cognitive%0Ascientists%20are%20increasingly%20asking%20about%20their%20alignment%20with%20how%20humans%20think.%0AHere%2C%20we%20ask%20this%20question%20for%20computer%20vision%20models%20and%20human%20sensitivity%20to%0Ageometric%20and%20topological%20%28GT%29%20concepts.%20Under%20the%20core%20knowledge%20account%2C%0Athese%20concepts%20are%20innate%20and%20supported%20by%20dedicated%20neural%20circuitry.%20In%20this%0Awork%2C%20we%20investigate%20an%20alternative%20explanation%2C%20that%20GT%20concepts%20are%20learned%0A%60%60for%20free%27%27%20through%20everyday%20interaction%20with%20the%20environment.%20We%20do%20so%20using%0Acomputer%20visions%20models%2C%20which%20are%20trained%20on%20large%20image%20datasets.%20We%20build%20on%0Aprior%20studies%20to%20investigate%20the%20overall%20performance%20and%20human%20alignment%20of%0Athree%20classes%20of%20models%20--%20convolutional%20neural%20networks%20%28CNNs%29%2C%0Atransformer-based%20models%2C%20and%20vision-language%20models%20--%20on%20an%20odd-one-out%20task%0Atesting%2043%20GT%20concepts%20spanning%20seven%20classes.%20Transformer-based%20models%20achieve%0Athe%20highest%20overall%20accuracy%2C%20surpassing%20that%20of%20young%20children.%20They%20also%20show%0Astrong%20alignment%20with%20children%27s%20performance%2C%20finding%20the%20same%20classes%20of%0Aconcepts%20easy%20vs.%20difficult.%20By%20contrast%2C%20vision-language%20models%20underperform%0Atheir%20vision-only%20counterparts%20and%20deviate%20further%20from%20human%20profiles%2C%0Aindicating%20that%20na%5C%22ive%20multimodality%20might%20compromise%20abstract%20geometric%0Asensitivity.%20These%20findings%20support%20the%20use%20of%20computer%20vision%20models%20to%0Aevaluate%20the%20sufficiency%20of%20the%20learning%20account%20for%20explaining%20human%0Asensitivity%20to%20GT%20concepts%2C%20while%20also%20suggesting%20that%20integrating%20linguistic%0Aand%20visual%20representations%20might%20have%20unpredicted%20deleterious%20consequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13281v1&entry.124074799=Read"},
{"title": "Feedback-Driven Vision-Language Alignment with Minimal Human Supervision", "author": "Giorgio Giannone and Ruoteng Li and Qianli Feng and Evgeny Perevodchikov and Rui Chen and Aleix Martinez", "abstract": "  Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Sampling-based Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on manually curated text-image pairs or preference annotation. SVP\nleverages a small set of manually selected images, self-captioning and a\npre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14 %\naverage improvement in captioning tasks, up to 12 % increase in object recall,\nand significantly reduced hallucinations, while maintaining question-answering\ncapabilities. Using SVP, a small VLM achieves hallucination reductions similar\nto a model five times larger, while a VLM with initially poor referring\ncapabilities more than doubles its performance, approaching parity with a model\ntwice its size.\n", "link": "http://arxiv.org/abs/2501.04568v2", "date": "2025-05-19", "relevancy": 2.8704, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5845}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedback-Driven%20Vision-Language%20Alignment%20with%20Minimal%20Human%20Supervision&body=Title%3A%20Feedback-Driven%20Vision-Language%20Alignment%20with%20Minimal%20Human%20Supervision%0AAuthor%3A%20Giorgio%20Giannone%20and%20Ruoteng%20Li%20and%20Qianli%20Feng%20and%20Evgeny%20Perevodchikov%20and%20Rui%20Chen%20and%20Aleix%20Martinez%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Aintegrating%20visual%20and%20linguistic%20information%2C%20but%20their%20performance%20is%20often%0Aconstrained%20by%20the%20need%20for%20extensive%2C%20high-quality%20image-text%20training%20data.%0ACuration%20of%20these%20image-text%20pairs%20is%20both%20time-consuming%20and%20computationally%0Aexpensive.%20To%20address%20this%20challenge%2C%20we%20introduce%20SVP%20%28Sampling-based%20Visual%0AProjection%29%2C%20a%20novel%20framework%20that%20enhances%20vision-language%20alignment%20without%0Arelying%20on%20manually%20curated%20text-image%20pairs%20or%20preference%20annotation.%20SVP%0Aleverages%20a%20small%20set%20of%20manually%20selected%20images%2C%20self-captioning%20and%20a%0Apre-trained%20grounding%20model%20as%20a%20feedback%20mechanism%20to%20elicit%20latent%0Ainformation%20in%20VLMs.%20We%20evaluate%20our%20approach%20across%20six%20key%20areas%3A%20captioning%2C%0Areferring%2C%20visual%20question%20answering%2C%20multitasking%2C%20hallucination%20control%2C%20and%0Aobject%20recall.%20Results%20demonstrate%20significant%20improvements%2C%20including%20a%2014%20%25%0Aaverage%20improvement%20in%20captioning%20tasks%2C%20up%20to%2012%20%25%20increase%20in%20object%20recall%2C%0Aand%20significantly%20reduced%20hallucinations%2C%20while%20maintaining%20question-answering%0Acapabilities.%20Using%20SVP%2C%20a%20small%20VLM%20achieves%20hallucination%20reductions%20similar%0Ato%20a%20model%20five%20times%20larger%2C%20while%20a%20VLM%20with%20initially%20poor%20referring%0Acapabilities%20more%20than%20doubles%20its%20performance%2C%20approaching%20parity%20with%20a%20model%0Atwice%20its%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedback-Driven%2520Vision-Language%2520Alignment%2520with%2520Minimal%2520Human%2520Supervision%26entry.906535625%3DGiorgio%2520Giannone%2520and%2520Ruoteng%2520Li%2520and%2520Qianli%2520Feng%2520and%2520Evgeny%2520Perevodchikov%2520and%2520Rui%2520Chen%2520and%2520Aleix%2520Martinez%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520remarkable%2520potential%2520in%250Aintegrating%2520visual%2520and%2520linguistic%2520information%252C%2520but%2520their%2520performance%2520is%2520often%250Aconstrained%2520by%2520the%2520need%2520for%2520extensive%252C%2520high-quality%2520image-text%2520training%2520data.%250ACuration%2520of%2520these%2520image-text%2520pairs%2520is%2520both%2520time-consuming%2520and%2520computationally%250Aexpensive.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520SVP%2520%2528Sampling-based%2520Visual%250AProjection%2529%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520vision-language%2520alignment%2520without%250Arelying%2520on%2520manually%2520curated%2520text-image%2520pairs%2520or%2520preference%2520annotation.%2520SVP%250Aleverages%2520a%2520small%2520set%2520of%2520manually%2520selected%2520images%252C%2520self-captioning%2520and%2520a%250Apre-trained%2520grounding%2520model%2520as%2520a%2520feedback%2520mechanism%2520to%2520elicit%2520latent%250Ainformation%2520in%2520VLMs.%2520We%2520evaluate%2520our%2520approach%2520across%2520six%2520key%2520areas%253A%2520captioning%252C%250Areferring%252C%2520visual%2520question%2520answering%252C%2520multitasking%252C%2520hallucination%2520control%252C%2520and%250Aobject%2520recall.%2520Results%2520demonstrate%2520significant%2520improvements%252C%2520including%2520a%252014%2520%2525%250Aaverage%2520improvement%2520in%2520captioning%2520tasks%252C%2520up%2520to%252012%2520%2525%2520increase%2520in%2520object%2520recall%252C%250Aand%2520significantly%2520reduced%2520hallucinations%252C%2520while%2520maintaining%2520question-answering%250Acapabilities.%2520Using%2520SVP%252C%2520a%2520small%2520VLM%2520achieves%2520hallucination%2520reductions%2520similar%250Ato%2520a%2520model%2520five%2520times%2520larger%252C%2520while%2520a%2520VLM%2520with%2520initially%2520poor%2520referring%250Acapabilities%2520more%2520than%2520doubles%2520its%2520performance%252C%2520approaching%2520parity%2520with%2520a%2520model%250Atwice%2520its%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedback-Driven%20Vision-Language%20Alignment%20with%20Minimal%20Human%20Supervision&entry.906535625=Giorgio%20Giannone%20and%20Ruoteng%20Li%20and%20Qianli%20Feng%20and%20Evgeny%20Perevodchikov%20and%20Rui%20Chen%20and%20Aleix%20Martinez&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20remarkable%20potential%20in%0Aintegrating%20visual%20and%20linguistic%20information%2C%20but%20their%20performance%20is%20often%0Aconstrained%20by%20the%20need%20for%20extensive%2C%20high-quality%20image-text%20training%20data.%0ACuration%20of%20these%20image-text%20pairs%20is%20both%20time-consuming%20and%20computationally%0Aexpensive.%20To%20address%20this%20challenge%2C%20we%20introduce%20SVP%20%28Sampling-based%20Visual%0AProjection%29%2C%20a%20novel%20framework%20that%20enhances%20vision-language%20alignment%20without%0Arelying%20on%20manually%20curated%20text-image%20pairs%20or%20preference%20annotation.%20SVP%0Aleverages%20a%20small%20set%20of%20manually%20selected%20images%2C%20self-captioning%20and%20a%0Apre-trained%20grounding%20model%20as%20a%20feedback%20mechanism%20to%20elicit%20latent%0Ainformation%20in%20VLMs.%20We%20evaluate%20our%20approach%20across%20six%20key%20areas%3A%20captioning%2C%0Areferring%2C%20visual%20question%20answering%2C%20multitasking%2C%20hallucination%20control%2C%20and%0Aobject%20recall.%20Results%20demonstrate%20significant%20improvements%2C%20including%20a%2014%20%25%0Aaverage%20improvement%20in%20captioning%20tasks%2C%20up%20to%2012%20%25%20increase%20in%20object%20recall%2C%0Aand%20significantly%20reduced%20hallucinations%2C%20while%20maintaining%20question-answering%0Acapabilities.%20Using%20SVP%2C%20a%20small%20VLM%20achieves%20hallucination%20reductions%20similar%0Ato%20a%20model%20five%20times%20larger%2C%20while%20a%20VLM%20with%20initially%20poor%20referring%0Acapabilities%20more%20than%20doubles%20its%20performance%2C%20approaching%20parity%20with%20a%20model%0Atwice%20its%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04568v2&entry.124074799=Read"},
{"title": "3D Visual Illusion Depth Estimation", "author": "CHengtang Yao and Zhidan Liu and Jiaxi Zeng and Lidong Yu and Yuwei Wu and Yunde Jia", "abstract": "  3D visual illusion is a perceptual phenomenon where a two-dimensional plane\nis manipulated to simulate three-dimensional spatial relationships, making a\nflat artwork or object look three-dimensional in the human visual system. In\nthis paper, we reveal that the machine visual system is also seriously fooled\nby 3D visual illusions, including monocular and binocular depth estimation. In\norder to explore and analyze the impact of 3D visual illusion on depth\nestimation, we collect a large dataset containing almost 3k scenes and 200k\nimages to train and evaluate SOTA monocular and binocular depth estimation\nmethods. We also propose a robust depth estimation framework that uses common\nsense from a vision-language model to adaptively select reliable depth from\nbinocular disparity and monocular depth. Experiments show that SOTA monocular,\nbinocular, and multi-view depth estimation approaches are all fooled by various\n3D visual illusions, while our method achieves SOTA performance.\n", "link": "http://arxiv.org/abs/2505.13061v1", "date": "2025-05-19", "relevancy": 2.8488, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Visual%20Illusion%20Depth%20Estimation&body=Title%3A%203D%20Visual%20Illusion%20Depth%20Estimation%0AAuthor%3A%20CHengtang%20Yao%20and%20Zhidan%20Liu%20and%20Jiaxi%20Zeng%20and%20Lidong%20Yu%20and%20Yuwei%20Wu%20and%20Yunde%20Jia%0AAbstract%3A%20%20%203D%20visual%20illusion%20is%20a%20perceptual%20phenomenon%20where%20a%20two-dimensional%20plane%0Ais%20manipulated%20to%20simulate%20three-dimensional%20spatial%20relationships%2C%20making%20a%0Aflat%20artwork%20or%20object%20look%20three-dimensional%20in%20the%20human%20visual%20system.%20In%0Athis%20paper%2C%20we%20reveal%20that%20the%20machine%20visual%20system%20is%20also%20seriously%20fooled%0Aby%203D%20visual%20illusions%2C%20including%20monocular%20and%20binocular%20depth%20estimation.%20In%0Aorder%20to%20explore%20and%20analyze%20the%20impact%20of%203D%20visual%20illusion%20on%20depth%0Aestimation%2C%20we%20collect%20a%20large%20dataset%20containing%20almost%203k%20scenes%20and%20200k%0Aimages%20to%20train%20and%20evaluate%20SOTA%20monocular%20and%20binocular%20depth%20estimation%0Amethods.%20We%20also%20propose%20a%20robust%20depth%20estimation%20framework%20that%20uses%20common%0Asense%20from%20a%20vision-language%20model%20to%20adaptively%20select%20reliable%20depth%20from%0Abinocular%20disparity%20and%20monocular%20depth.%20Experiments%20show%20that%20SOTA%20monocular%2C%0Abinocular%2C%20and%20multi-view%20depth%20estimation%20approaches%20are%20all%20fooled%20by%20various%0A3D%20visual%20illusions%2C%20while%20our%20method%20achieves%20SOTA%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Visual%2520Illusion%2520Depth%2520Estimation%26entry.906535625%3DCHengtang%2520Yao%2520and%2520Zhidan%2520Liu%2520and%2520Jiaxi%2520Zeng%2520and%2520Lidong%2520Yu%2520and%2520Yuwei%2520Wu%2520and%2520Yunde%2520Jia%26entry.1292438233%3D%2520%25203D%2520visual%2520illusion%2520is%2520a%2520perceptual%2520phenomenon%2520where%2520a%2520two-dimensional%2520plane%250Ais%2520manipulated%2520to%2520simulate%2520three-dimensional%2520spatial%2520relationships%252C%2520making%2520a%250Aflat%2520artwork%2520or%2520object%2520look%2520three-dimensional%2520in%2520the%2520human%2520visual%2520system.%2520In%250Athis%2520paper%252C%2520we%2520reveal%2520that%2520the%2520machine%2520visual%2520system%2520is%2520also%2520seriously%2520fooled%250Aby%25203D%2520visual%2520illusions%252C%2520including%2520monocular%2520and%2520binocular%2520depth%2520estimation.%2520In%250Aorder%2520to%2520explore%2520and%2520analyze%2520the%2520impact%2520of%25203D%2520visual%2520illusion%2520on%2520depth%250Aestimation%252C%2520we%2520collect%2520a%2520large%2520dataset%2520containing%2520almost%25203k%2520scenes%2520and%2520200k%250Aimages%2520to%2520train%2520and%2520evaluate%2520SOTA%2520monocular%2520and%2520binocular%2520depth%2520estimation%250Amethods.%2520We%2520also%2520propose%2520a%2520robust%2520depth%2520estimation%2520framework%2520that%2520uses%2520common%250Asense%2520from%2520a%2520vision-language%2520model%2520to%2520adaptively%2520select%2520reliable%2520depth%2520from%250Abinocular%2520disparity%2520and%2520monocular%2520depth.%2520Experiments%2520show%2520that%2520SOTA%2520monocular%252C%250Abinocular%252C%2520and%2520multi-view%2520depth%2520estimation%2520approaches%2520are%2520all%2520fooled%2520by%2520various%250A3D%2520visual%2520illusions%252C%2520while%2520our%2520method%2520achieves%2520SOTA%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Visual%20Illusion%20Depth%20Estimation&entry.906535625=CHengtang%20Yao%20and%20Zhidan%20Liu%20and%20Jiaxi%20Zeng%20and%20Lidong%20Yu%20and%20Yuwei%20Wu%20and%20Yunde%20Jia&entry.1292438233=%20%203D%20visual%20illusion%20is%20a%20perceptual%20phenomenon%20where%20a%20two-dimensional%20plane%0Ais%20manipulated%20to%20simulate%20three-dimensional%20spatial%20relationships%2C%20making%20a%0Aflat%20artwork%20or%20object%20look%20three-dimensional%20in%20the%20human%20visual%20system.%20In%0Athis%20paper%2C%20we%20reveal%20that%20the%20machine%20visual%20system%20is%20also%20seriously%20fooled%0Aby%203D%20visual%20illusions%2C%20including%20monocular%20and%20binocular%20depth%20estimation.%20In%0Aorder%20to%20explore%20and%20analyze%20the%20impact%20of%203D%20visual%20illusion%20on%20depth%0Aestimation%2C%20we%20collect%20a%20large%20dataset%20containing%20almost%203k%20scenes%20and%20200k%0Aimages%20to%20train%20and%20evaluate%20SOTA%20monocular%20and%20binocular%20depth%20estimation%0Amethods.%20We%20also%20propose%20a%20robust%20depth%20estimation%20framework%20that%20uses%20common%0Asense%20from%20a%20vision-language%20model%20to%20adaptively%20select%20reliable%20depth%20from%0Abinocular%20disparity%20and%20monocular%20depth.%20Experiments%20show%20that%20SOTA%20monocular%2C%0Abinocular%2C%20and%20multi-view%20depth%20estimation%20approaches%20are%20all%20fooled%20by%20various%0A3D%20visual%20illusions%2C%20while%20our%20method%20achieves%20SOTA%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13061v1&entry.124074799=Read"},
{"title": "UniTok: A Unified Tokenizer for Visual Generation and Understanding", "author": "Chuofan Ma and Yi Jiang and Junfeng Wu and Jihan Yang and Xin Yu and Zehuan Yuan and Bingyue Peng and Xiaojuan Qi", "abstract": "  Visual generative and understanding models typically rely on distinct\ntokenizers to process images, presenting a key challenge for unifying them\nwithin a single framework. Recent studies attempt to address this by connecting\nthe training of VQVAE (for autoregressive generation) and CLIP (for\nunderstanding) to build a unified tokenizer. However, directly combining these\ntraining objectives has been observed to cause severe loss conflicts. In this\npaper, we show that reconstruction and semantic supervision do not inherently\nconflict. Instead, the underlying bottleneck stems from limited\nrepresentational capacity of discrete token space. Building on these insights,\nwe introduce UniTok, a unified tokenizer featuring a novel multi-codebook\nquantization mechanism that effectively scales up the vocabulary size and\nbottleneck dimension. In terms of final performance, UniTok sets a new record\nof 0.38 rFID and 78.6% zero-shot accuracy on ImageNet. Besides, UniTok can be\nseamlessly integrated into MLLMs to unlock native visual generation capability,\nwithout compromising the understanding performance. Additionally, we show that\nUniTok favors cfg-free generation, reducing gFID from 14.6 to 2.5 on ImageNet\n256$\\times$256 benchmark. GitHub: https://github.com/FoundationVision/UniTok.\n", "link": "http://arxiv.org/abs/2502.20321v2", "date": "2025-05-19", "relevancy": 2.8468, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6272}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniTok%3A%20A%20Unified%20Tokenizer%20for%20Visual%20Generation%20and%20Understanding&body=Title%3A%20UniTok%3A%20A%20Unified%20Tokenizer%20for%20Visual%20Generation%20and%20Understanding%0AAuthor%3A%20Chuofan%20Ma%20and%20Yi%20Jiang%20and%20Junfeng%20Wu%20and%20Jihan%20Yang%20and%20Xin%20Yu%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Visual%20generative%20and%20understanding%20models%20typically%20rely%20on%20distinct%0Atokenizers%20to%20process%20images%2C%20presenting%20a%20key%20challenge%20for%20unifying%20them%0Awithin%20a%20single%20framework.%20Recent%20studies%20attempt%20to%20address%20this%20by%20connecting%0Athe%20training%20of%20VQVAE%20%28for%20autoregressive%20generation%29%20and%20CLIP%20%28for%0Aunderstanding%29%20to%20build%20a%20unified%20tokenizer.%20However%2C%20directly%20combining%20these%0Atraining%20objectives%20has%20been%20observed%20to%20cause%20severe%20loss%20conflicts.%20In%20this%0Apaper%2C%20we%20show%20that%20reconstruction%20and%20semantic%20supervision%20do%20not%20inherently%0Aconflict.%20Instead%2C%20the%20underlying%20bottleneck%20stems%20from%20limited%0Arepresentational%20capacity%20of%20discrete%20token%20space.%20Building%20on%20these%20insights%2C%0Awe%20introduce%20UniTok%2C%20a%20unified%20tokenizer%20featuring%20a%20novel%20multi-codebook%0Aquantization%20mechanism%20that%20effectively%20scales%20up%20the%20vocabulary%20size%20and%0Abottleneck%20dimension.%20In%20terms%20of%20final%20performance%2C%20UniTok%20sets%20a%20new%20record%0Aof%200.38%20rFID%20and%2078.6%25%20zero-shot%20accuracy%20on%20ImageNet.%20Besides%2C%20UniTok%20can%20be%0Aseamlessly%20integrated%20into%20MLLMs%20to%20unlock%20native%20visual%20generation%20capability%2C%0Awithout%20compromising%20the%20understanding%20performance.%20Additionally%2C%20we%20show%20that%0AUniTok%20favors%20cfg-free%20generation%2C%20reducing%20gFID%20from%2014.6%20to%202.5%20on%20ImageNet%0A256%24%5Ctimes%24256%20benchmark.%20GitHub%3A%20https%3A//github.com/FoundationVision/UniTok.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20321v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniTok%253A%2520A%2520Unified%2520Tokenizer%2520for%2520Visual%2520Generation%2520and%2520Understanding%26entry.906535625%3DChuofan%2520Ma%2520and%2520Yi%2520Jiang%2520and%2520Junfeng%2520Wu%2520and%2520Jihan%2520Yang%2520and%2520Xin%2520Yu%2520and%2520Zehuan%2520Yuan%2520and%2520Bingyue%2520Peng%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Visual%2520generative%2520and%2520understanding%2520models%2520typically%2520rely%2520on%2520distinct%250Atokenizers%2520to%2520process%2520images%252C%2520presenting%2520a%2520key%2520challenge%2520for%2520unifying%2520them%250Awithin%2520a%2520single%2520framework.%2520Recent%2520studies%2520attempt%2520to%2520address%2520this%2520by%2520connecting%250Athe%2520training%2520of%2520VQVAE%2520%2528for%2520autoregressive%2520generation%2529%2520and%2520CLIP%2520%2528for%250Aunderstanding%2529%2520to%2520build%2520a%2520unified%2520tokenizer.%2520However%252C%2520directly%2520combining%2520these%250Atraining%2520objectives%2520has%2520been%2520observed%2520to%2520cause%2520severe%2520loss%2520conflicts.%2520In%2520this%250Apaper%252C%2520we%2520show%2520that%2520reconstruction%2520and%2520semantic%2520supervision%2520do%2520not%2520inherently%250Aconflict.%2520Instead%252C%2520the%2520underlying%2520bottleneck%2520stems%2520from%2520limited%250Arepresentational%2520capacity%2520of%2520discrete%2520token%2520space.%2520Building%2520on%2520these%2520insights%252C%250Awe%2520introduce%2520UniTok%252C%2520a%2520unified%2520tokenizer%2520featuring%2520a%2520novel%2520multi-codebook%250Aquantization%2520mechanism%2520that%2520effectively%2520scales%2520up%2520the%2520vocabulary%2520size%2520and%250Abottleneck%2520dimension.%2520In%2520terms%2520of%2520final%2520performance%252C%2520UniTok%2520sets%2520a%2520new%2520record%250Aof%25200.38%2520rFID%2520and%252078.6%2525%2520zero-shot%2520accuracy%2520on%2520ImageNet.%2520Besides%252C%2520UniTok%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520MLLMs%2520to%2520unlock%2520native%2520visual%2520generation%2520capability%252C%250Awithout%2520compromising%2520the%2520understanding%2520performance.%2520Additionally%252C%2520we%2520show%2520that%250AUniTok%2520favors%2520cfg-free%2520generation%252C%2520reducing%2520gFID%2520from%252014.6%2520to%25202.5%2520on%2520ImageNet%250A256%2524%255Ctimes%2524256%2520benchmark.%2520GitHub%253A%2520https%253A//github.com/FoundationVision/UniTok.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20321v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniTok%3A%20A%20Unified%20Tokenizer%20for%20Visual%20Generation%20and%20Understanding&entry.906535625=Chuofan%20Ma%20and%20Yi%20Jiang%20and%20Junfeng%20Wu%20and%20Jihan%20Yang%20and%20Xin%20Yu%20and%20Zehuan%20Yuan%20and%20Bingyue%20Peng%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Visual%20generative%20and%20understanding%20models%20typically%20rely%20on%20distinct%0Atokenizers%20to%20process%20images%2C%20presenting%20a%20key%20challenge%20for%20unifying%20them%0Awithin%20a%20single%20framework.%20Recent%20studies%20attempt%20to%20address%20this%20by%20connecting%0Athe%20training%20of%20VQVAE%20%28for%20autoregressive%20generation%29%20and%20CLIP%20%28for%0Aunderstanding%29%20to%20build%20a%20unified%20tokenizer.%20However%2C%20directly%20combining%20these%0Atraining%20objectives%20has%20been%20observed%20to%20cause%20severe%20loss%20conflicts.%20In%20this%0Apaper%2C%20we%20show%20that%20reconstruction%20and%20semantic%20supervision%20do%20not%20inherently%0Aconflict.%20Instead%2C%20the%20underlying%20bottleneck%20stems%20from%20limited%0Arepresentational%20capacity%20of%20discrete%20token%20space.%20Building%20on%20these%20insights%2C%0Awe%20introduce%20UniTok%2C%20a%20unified%20tokenizer%20featuring%20a%20novel%20multi-codebook%0Aquantization%20mechanism%20that%20effectively%20scales%20up%20the%20vocabulary%20size%20and%0Abottleneck%20dimension.%20In%20terms%20of%20final%20performance%2C%20UniTok%20sets%20a%20new%20record%0Aof%200.38%20rFID%20and%2078.6%25%20zero-shot%20accuracy%20on%20ImageNet.%20Besides%2C%20UniTok%20can%20be%0Aseamlessly%20integrated%20into%20MLLMs%20to%20unlock%20native%20visual%20generation%20capability%2C%0Awithout%20compromising%20the%20understanding%20performance.%20Additionally%2C%20we%20show%20that%0AUniTok%20favors%20cfg-free%20generation%2C%20reducing%20gFID%20from%2014.6%20to%202.5%20on%20ImageNet%0A256%24%5Ctimes%24256%20benchmark.%20GitHub%3A%20https%3A//github.com/FoundationVision/UniTok.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20321v2&entry.124074799=Read"},
{"title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding", "author": "Jinghui Lu and Haiyang Yu and Yanjie Wang and Yongjie Ye and Jingqun Tang and Ziwei Yang and Binghong Wu and Qi Liu and Hao Feng and Han Wang and Hao Liu and Can Huang", "abstract": "  Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. LayTextLLM\nprojects each bounding box to a single embedding and interleaves it with text,\nefficiently avoiding long sequence issues while leveraging autoregressive\ntraits of LLMs. LayTextLLM not only streamlines the interaction of layout and\ntextual data but also shows enhanced performance in KIE and VQA. Comprehensive\nbenchmark evaluations reveal significant improvements of LayTextLLM, with a\n15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA\nOCR-based LLMs. All resources are available at\nhttps://github.com/LayTextLLM/LayTextLLM.\n", "link": "http://arxiv.org/abs/2407.01976v3", "date": "2025-05-19", "relevancy": 2.8381, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.582}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Bounding%20Box%20is%20Worth%20One%20Token%3A%20Interleaving%20Layout%20and%20Text%20in%20a%0A%20%20Large%20Language%20Model%20for%20Document%20Understanding&body=Title%3A%20A%20Bounding%20Box%20is%20Worth%20One%20Token%3A%20Interleaving%20Layout%20and%20Text%20in%20a%0A%20%20Large%20Language%20Model%20for%20Document%20Understanding%0AAuthor%3A%20Jinghui%20Lu%20and%20Haiyang%20Yu%20and%20Yanjie%20Wang%20and%20Yongjie%20Ye%20and%20Jingqun%20Tang%20and%20Ziwei%20Yang%20and%20Binghong%20Wu%20and%20Qi%20Liu%20and%20Hao%20Feng%20and%20Han%20Wang%20and%20Hao%20Liu%20and%20Can%20Huang%0AAbstract%3A%20%20%20Recently%2C%20many%20studies%20have%20demonstrated%20that%20exclusively%20incorporating%0AOCR-derived%20text%20and%20spatial%20layouts%20with%20large%20language%20models%20%28LLMs%29%20can%20be%0Ahighly%20effective%20for%20document%20understanding%20tasks.%20However%2C%20existing%20methods%0Athat%20integrate%20spatial%20layouts%20with%20text%20have%20limitations%2C%20such%20as%20producing%0Aoverly%20long%20text%20sequences%20or%20failing%20to%20fully%20leverage%20the%20autoregressive%0Atraits%20of%20LLMs.%20In%20this%20work%2C%20we%20introduce%20Interleaving%20Layout%20and%20Text%20in%20a%0ALarge%20Language%20Model%20%28LayTextLLM%29%7D%20for%20document%20understanding.%20LayTextLLM%0Aprojects%20each%20bounding%20box%20to%20a%20single%20embedding%20and%20interleaves%20it%20with%20text%2C%0Aefficiently%20avoiding%20long%20sequence%20issues%20while%20leveraging%20autoregressive%0Atraits%20of%20LLMs.%20LayTextLLM%20not%20only%20streamlines%20the%20interaction%20of%20layout%20and%0Atextual%20data%20but%20also%20shows%20enhanced%20performance%20in%20KIE%20and%20VQA.%20Comprehensive%0Abenchmark%20evaluations%20reveal%20significant%20improvements%20of%20LayTextLLM%2C%20with%20a%0A15.2%25%20increase%20on%20KIE%20tasks%20and%2010.7%25%20on%20VQA%20tasks%20compared%20to%20previous%20SOTA%0AOCR-based%20LLMs.%20All%20resources%20are%20available%20at%0Ahttps%3A//github.com/LayTextLLM/LayTextLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01976v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Bounding%2520Box%2520is%2520Worth%2520One%2520Token%253A%2520Interleaving%2520Layout%2520and%2520Text%2520in%2520a%250A%2520%2520Large%2520Language%2520Model%2520for%2520Document%2520Understanding%26entry.906535625%3DJinghui%2520Lu%2520and%2520Haiyang%2520Yu%2520and%2520Yanjie%2520Wang%2520and%2520Yongjie%2520Ye%2520and%2520Jingqun%2520Tang%2520and%2520Ziwei%2520Yang%2520and%2520Binghong%2520Wu%2520and%2520Qi%2520Liu%2520and%2520Hao%2520Feng%2520and%2520Han%2520Wang%2520and%2520Hao%2520Liu%2520and%2520Can%2520Huang%26entry.1292438233%3D%2520%2520Recently%252C%2520many%2520studies%2520have%2520demonstrated%2520that%2520exclusively%2520incorporating%250AOCR-derived%2520text%2520and%2520spatial%2520layouts%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520be%250Ahighly%2520effective%2520for%2520document%2520understanding%2520tasks.%2520However%252C%2520existing%2520methods%250Athat%2520integrate%2520spatial%2520layouts%2520with%2520text%2520have%2520limitations%252C%2520such%2520as%2520producing%250Aoverly%2520long%2520text%2520sequences%2520or%2520failing%2520to%2520fully%2520leverage%2520the%2520autoregressive%250Atraits%2520of%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Interleaving%2520Layout%2520and%2520Text%2520in%2520a%250ALarge%2520Language%2520Model%2520%2528LayTextLLM%2529%257D%2520for%2520document%2520understanding.%2520LayTextLLM%250Aprojects%2520each%2520bounding%2520box%2520to%2520a%2520single%2520embedding%2520and%2520interleaves%2520it%2520with%2520text%252C%250Aefficiently%2520avoiding%2520long%2520sequence%2520issues%2520while%2520leveraging%2520autoregressive%250Atraits%2520of%2520LLMs.%2520LayTextLLM%2520not%2520only%2520streamlines%2520the%2520interaction%2520of%2520layout%2520and%250Atextual%2520data%2520but%2520also%2520shows%2520enhanced%2520performance%2520in%2520KIE%2520and%2520VQA.%2520Comprehensive%250Abenchmark%2520evaluations%2520reveal%2520significant%2520improvements%2520of%2520LayTextLLM%252C%2520with%2520a%250A15.2%2525%2520increase%2520on%2520KIE%2520tasks%2520and%252010.7%2525%2520on%2520VQA%2520tasks%2520compared%2520to%2520previous%2520SOTA%250AOCR-based%2520LLMs.%2520All%2520resources%2520are%2520available%2520at%250Ahttps%253A//github.com/LayTextLLM/LayTextLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01976v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Bounding%20Box%20is%20Worth%20One%20Token%3A%20Interleaving%20Layout%20and%20Text%20in%20a%0A%20%20Large%20Language%20Model%20for%20Document%20Understanding&entry.906535625=Jinghui%20Lu%20and%20Haiyang%20Yu%20and%20Yanjie%20Wang%20and%20Yongjie%20Ye%20and%20Jingqun%20Tang%20and%20Ziwei%20Yang%20and%20Binghong%20Wu%20and%20Qi%20Liu%20and%20Hao%20Feng%20and%20Han%20Wang%20and%20Hao%20Liu%20and%20Can%20Huang&entry.1292438233=%20%20Recently%2C%20many%20studies%20have%20demonstrated%20that%20exclusively%20incorporating%0AOCR-derived%20text%20and%20spatial%20layouts%20with%20large%20language%20models%20%28LLMs%29%20can%20be%0Ahighly%20effective%20for%20document%20understanding%20tasks.%20However%2C%20existing%20methods%0Athat%20integrate%20spatial%20layouts%20with%20text%20have%20limitations%2C%20such%20as%20producing%0Aoverly%20long%20text%20sequences%20or%20failing%20to%20fully%20leverage%20the%20autoregressive%0Atraits%20of%20LLMs.%20In%20this%20work%2C%20we%20introduce%20Interleaving%20Layout%20and%20Text%20in%20a%0ALarge%20Language%20Model%20%28LayTextLLM%29%7D%20for%20document%20understanding.%20LayTextLLM%0Aprojects%20each%20bounding%20box%20to%20a%20single%20embedding%20and%20interleaves%20it%20with%20text%2C%0Aefficiently%20avoiding%20long%20sequence%20issues%20while%20leveraging%20autoregressive%0Atraits%20of%20LLMs.%20LayTextLLM%20not%20only%20streamlines%20the%20interaction%20of%20layout%20and%0Atextual%20data%20but%20also%20shows%20enhanced%20performance%20in%20KIE%20and%20VQA.%20Comprehensive%0Abenchmark%20evaluations%20reveal%20significant%20improvements%20of%20LayTextLLM%2C%20with%20a%0A15.2%25%20increase%20on%20KIE%20tasks%20and%2010.7%25%20on%20VQA%20tasks%20compared%20to%20previous%20SOTA%0AOCR-based%20LLMs.%20All%20resources%20are%20available%20at%0Ahttps%3A//github.com/LayTextLLM/LayTextLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01976v3&entry.124074799=Read"},
{"title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning", "author": "Liang Chen and Hongcheng Gao and Tianyu Liu and Zhiqi Huang and Flood Sung and Xinyu Zhou and Yuxin Wu and Baobao Chang", "abstract": "  Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents.\n", "link": "http://arxiv.org/abs/2505.13426v1", "date": "2025-05-19", "relevancy": 2.8377, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5683}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G1%3A%20Bootstrapping%20Perception%20and%20Reasoning%20Abilities%20of%20Vision-Language%0A%20%20Model%20via%20Reinforcement%20Learning&body=Title%3A%20G1%3A%20Bootstrapping%20Perception%20and%20Reasoning%20Abilities%20of%20Vision-Language%0A%20%20Model%20via%20Reinforcement%20Learning%0AAuthor%3A%20Liang%20Chen%20and%20Hongcheng%20Gao%20and%20Tianyu%20Liu%20and%20Zhiqi%20Huang%20and%20Flood%20Sung%20and%20Xinyu%20Zhou%20and%20Yuxin%20Wu%20and%20Baobao%20Chang%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20excel%20in%20many%20direct%20multimodal%20tasks%20but%0Astruggle%20to%20translate%20this%20prowess%20into%20effective%20decision-making%20within%0Ainteractive%2C%20visually%20rich%20environments%20like%20games.%20This%20%60%60knowing-doing%27%27%20gap%0Asignificantly%20limits%20their%20potential%20as%20autonomous%20agents%2C%20as%20leading%20VLMs%0Aoften%20performing%20badly%20in%20simple%20games.%20To%20address%20this%2C%20we%20introduce%20VLM-Gym%2C%0Aa%20curated%20reinforcement%20learning%20%28RL%29%20environment%20featuring%20diverse%20visual%0Agames%20with%20unified%20interfaces%20and%20adjustable%2C%20compositional%20difficulty%2C%0Aspecifically%20designed%20for%20scalable%20multi-game%20parallel%20training.%20Leveraging%0AVLM-Gym%2C%20we%20train%20G0%20models%20using%20pure%20RL-driven%20self-evolution%2C%20which%0Ademonstrate%20emergent%20perception%20and%20reasoning%20patterns.%20To%20further%20mitigate%0Achallenges%20arising%20from%20game%20diversity%2C%20we%20develop%20G1%20models.%20G1%20incorporates%20a%0Aperception-enhanced%20cold%20start%20prior%20to%20RL%20fine-tuning.%20Our%20resulting%20G1%20models%0Aconsistently%20surpass%20their%20teacher%20across%20all%20games%20and%20outperform%20leading%0Aproprietary%20models%20like%20Claude-3.7-Sonnet-Thinking.%20Systematic%20analysis%20reveals%0Aan%20intriguing%20finding%3A%20perception%20and%20reasoning%20abilities%20mutually%20bootstrap%0Aeach%20other%20throughout%20the%20RL%20training%20process.%20Source%20code%20including%20VLM-Gym%0Aand%20RL%20training%20are%20released%20at%20https%3A//github.com/chenllliang/G1%20to%20foster%0Afuture%20research%20in%20advancing%20VLMs%20as%20capable%20interactive%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG1%253A%2520Bootstrapping%2520Perception%2520and%2520Reasoning%2520Abilities%2520of%2520Vision-Language%250A%2520%2520Model%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DLiang%2520Chen%2520and%2520Hongcheng%2520Gao%2520and%2520Tianyu%2520Liu%2520and%2520Zhiqi%2520Huang%2520and%2520Flood%2520Sung%2520and%2520Xinyu%2520Zhou%2520and%2520Yuxin%2520Wu%2520and%2520Baobao%2520Chang%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520in%2520many%2520direct%2520multimodal%2520tasks%2520but%250Astruggle%2520to%2520translate%2520this%2520prowess%2520into%2520effective%2520decision-making%2520within%250Ainteractive%252C%2520visually%2520rich%2520environments%2520like%2520games.%2520This%2520%2560%2560knowing-doing%2527%2527%2520gap%250Asignificantly%2520limits%2520their%2520potential%2520as%2520autonomous%2520agents%252C%2520as%2520leading%2520VLMs%250Aoften%2520performing%2520badly%2520in%2520simple%2520games.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VLM-Gym%252C%250Aa%2520curated%2520reinforcement%2520learning%2520%2528RL%2529%2520environment%2520featuring%2520diverse%2520visual%250Agames%2520with%2520unified%2520interfaces%2520and%2520adjustable%252C%2520compositional%2520difficulty%252C%250Aspecifically%2520designed%2520for%2520scalable%2520multi-game%2520parallel%2520training.%2520Leveraging%250AVLM-Gym%252C%2520we%2520train%2520G0%2520models%2520using%2520pure%2520RL-driven%2520self-evolution%252C%2520which%250Ademonstrate%2520emergent%2520perception%2520and%2520reasoning%2520patterns.%2520To%2520further%2520mitigate%250Achallenges%2520arising%2520from%2520game%2520diversity%252C%2520we%2520develop%2520G1%2520models.%2520G1%2520incorporates%2520a%250Aperception-enhanced%2520cold%2520start%2520prior%2520to%2520RL%2520fine-tuning.%2520Our%2520resulting%2520G1%2520models%250Aconsistently%2520surpass%2520their%2520teacher%2520across%2520all%2520games%2520and%2520outperform%2520leading%250Aproprietary%2520models%2520like%2520Claude-3.7-Sonnet-Thinking.%2520Systematic%2520analysis%2520reveals%250Aan%2520intriguing%2520finding%253A%2520perception%2520and%2520reasoning%2520abilities%2520mutually%2520bootstrap%250Aeach%2520other%2520throughout%2520the%2520RL%2520training%2520process.%2520Source%2520code%2520including%2520VLM-Gym%250Aand%2520RL%2520training%2520are%2520released%2520at%2520https%253A//github.com/chenllliang/G1%2520to%2520foster%250Afuture%2520research%2520in%2520advancing%2520VLMs%2520as%2520capable%2520interactive%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G1%3A%20Bootstrapping%20Perception%20and%20Reasoning%20Abilities%20of%20Vision-Language%0A%20%20Model%20via%20Reinforcement%20Learning&entry.906535625=Liang%20Chen%20and%20Hongcheng%20Gao%20and%20Tianyu%20Liu%20and%20Zhiqi%20Huang%20and%20Flood%20Sung%20and%20Xinyu%20Zhou%20and%20Yuxin%20Wu%20and%20Baobao%20Chang&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20excel%20in%20many%20direct%20multimodal%20tasks%20but%0Astruggle%20to%20translate%20this%20prowess%20into%20effective%20decision-making%20within%0Ainteractive%2C%20visually%20rich%20environments%20like%20games.%20This%20%60%60knowing-doing%27%27%20gap%0Asignificantly%20limits%20their%20potential%20as%20autonomous%20agents%2C%20as%20leading%20VLMs%0Aoften%20performing%20badly%20in%20simple%20games.%20To%20address%20this%2C%20we%20introduce%20VLM-Gym%2C%0Aa%20curated%20reinforcement%20learning%20%28RL%29%20environment%20featuring%20diverse%20visual%0Agames%20with%20unified%20interfaces%20and%20adjustable%2C%20compositional%20difficulty%2C%0Aspecifically%20designed%20for%20scalable%20multi-game%20parallel%20training.%20Leveraging%0AVLM-Gym%2C%20we%20train%20G0%20models%20using%20pure%20RL-driven%20self-evolution%2C%20which%0Ademonstrate%20emergent%20perception%20and%20reasoning%20patterns.%20To%20further%20mitigate%0Achallenges%20arising%20from%20game%20diversity%2C%20we%20develop%20G1%20models.%20G1%20incorporates%20a%0Aperception-enhanced%20cold%20start%20prior%20to%20RL%20fine-tuning.%20Our%20resulting%20G1%20models%0Aconsistently%20surpass%20their%20teacher%20across%20all%20games%20and%20outperform%20leading%0Aproprietary%20models%20like%20Claude-3.7-Sonnet-Thinking.%20Systematic%20analysis%20reveals%0Aan%20intriguing%20finding%3A%20perception%20and%20reasoning%20abilities%20mutually%20bootstrap%0Aeach%20other%20throughout%20the%20RL%20training%20process.%20Source%20code%20including%20VLM-Gym%0Aand%20RL%20training%20are%20released%20at%20https%3A//github.com/chenllliang/G1%20to%20foster%0Afuture%20research%20in%20advancing%20VLMs%20as%20capable%20interactive%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13426v1&entry.124074799=Read"},
{"title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models", "author": "Wenhao Wang and Adam Dziedzic and Grace C. Kim and Michael Backes and Franziska Boenisch", "abstract": "  Multi-modal models, such as CLIP, have demonstrated strong performance in\naligning visual and textual representations, excelling in tasks like image\nretrieval and zero-shot classification. Despite this success, the mechanisms by\nwhich these models utilize training data, particularly the role of\nmemorization, remain unclear. In uni-modal models, both supervised and\nself-supervised, memorization has been shown to be essential for\ngeneralization. However, it is not well understood how these findings would\napply to CLIP, which incorporates elements from both supervised learning via\ncaptions that provide a supervisory signal similar to labels, and from\nself-supervised learning via the contrastive objective. To bridge this gap in\nunderstanding, we propose a formal definition of memorization in CLIP (CLIPMem)\nand use it to quantify memorization in CLIP models. Our results indicate that\nCLIP's memorization behavior falls between the supervised and self-supervised\nparadigms, with \"mis-captioned\" samples exhibiting highest levels of\nmemorization. Additionally, we find that the text encoder contributes more to\nmemorization than the image encoder, suggesting that mitigation strategies\nshould focus on the text domain. Building on these insights, we propose\nmultiple strategies to reduce memorization while at the same time improving\nutility--something that had not been shown before for traditional learning\nparadigms where reducing memorization typically results in utility decrease.\n", "link": "http://arxiv.org/abs/2502.07830v2", "date": "2025-05-19", "relevancy": 2.8336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Captured%20by%20Captions%3A%20On%20Memorization%20and%20its%20Mitigation%20in%20CLIP%20Models&body=Title%3A%20Captured%20by%20Captions%3A%20On%20Memorization%20and%20its%20Mitigation%20in%20CLIP%20Models%0AAuthor%3A%20Wenhao%20Wang%20and%20Adam%20Dziedzic%20and%20Grace%20C.%20Kim%20and%20Michael%20Backes%20and%20Franziska%20Boenisch%0AAbstract%3A%20%20%20Multi-modal%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%20strong%20performance%20in%0Aaligning%20visual%20and%20textual%20representations%2C%20excelling%20in%20tasks%20like%20image%0Aretrieval%20and%20zero-shot%20classification.%20Despite%20this%20success%2C%20the%20mechanisms%20by%0Awhich%20these%20models%20utilize%20training%20data%2C%20particularly%20the%20role%20of%0Amemorization%2C%20remain%20unclear.%20In%20uni-modal%20models%2C%20both%20supervised%20and%0Aself-supervised%2C%20memorization%20has%20been%20shown%20to%20be%20essential%20for%0Ageneralization.%20However%2C%20it%20is%20not%20well%20understood%20how%20these%20findings%20would%0Aapply%20to%20CLIP%2C%20which%20incorporates%20elements%20from%20both%20supervised%20learning%20via%0Acaptions%20that%20provide%20a%20supervisory%20signal%20similar%20to%20labels%2C%20and%20from%0Aself-supervised%20learning%20via%20the%20contrastive%20objective.%20To%20bridge%20this%20gap%20in%0Aunderstanding%2C%20we%20propose%20a%20formal%20definition%20of%20memorization%20in%20CLIP%20%28CLIPMem%29%0Aand%20use%20it%20to%20quantify%20memorization%20in%20CLIP%20models.%20Our%20results%20indicate%20that%0ACLIP%27s%20memorization%20behavior%20falls%20between%20the%20supervised%20and%20self-supervised%0Aparadigms%2C%20with%20%22mis-captioned%22%20samples%20exhibiting%20highest%20levels%20of%0Amemorization.%20Additionally%2C%20we%20find%20that%20the%20text%20encoder%20contributes%20more%20to%0Amemorization%20than%20the%20image%20encoder%2C%20suggesting%20that%20mitigation%20strategies%0Ashould%20focus%20on%20the%20text%20domain.%20Building%20on%20these%20insights%2C%20we%20propose%0Amultiple%20strategies%20to%20reduce%20memorization%20while%20at%20the%20same%20time%20improving%0Autility--something%20that%20had%20not%20been%20shown%20before%20for%20traditional%20learning%0Aparadigms%20where%20reducing%20memorization%20typically%20results%20in%20utility%20decrease.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07830v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaptured%2520by%2520Captions%253A%2520On%2520Memorization%2520and%2520its%2520Mitigation%2520in%2520CLIP%2520Models%26entry.906535625%3DWenhao%2520Wang%2520and%2520Adam%2520Dziedzic%2520and%2520Grace%2520C.%2520Kim%2520and%2520Michael%2520Backes%2520and%2520Franziska%2520Boenisch%26entry.1292438233%3D%2520%2520Multi-modal%2520models%252C%2520such%2520as%2520CLIP%252C%2520have%2520demonstrated%2520strong%2520performance%2520in%250Aaligning%2520visual%2520and%2520textual%2520representations%252C%2520excelling%2520in%2520tasks%2520like%2520image%250Aretrieval%2520and%2520zero-shot%2520classification.%2520Despite%2520this%2520success%252C%2520the%2520mechanisms%2520by%250Awhich%2520these%2520models%2520utilize%2520training%2520data%252C%2520particularly%2520the%2520role%2520of%250Amemorization%252C%2520remain%2520unclear.%2520In%2520uni-modal%2520models%252C%2520both%2520supervised%2520and%250Aself-supervised%252C%2520memorization%2520has%2520been%2520shown%2520to%2520be%2520essential%2520for%250Ageneralization.%2520However%252C%2520it%2520is%2520not%2520well%2520understood%2520how%2520these%2520findings%2520would%250Aapply%2520to%2520CLIP%252C%2520which%2520incorporates%2520elements%2520from%2520both%2520supervised%2520learning%2520via%250Acaptions%2520that%2520provide%2520a%2520supervisory%2520signal%2520similar%2520to%2520labels%252C%2520and%2520from%250Aself-supervised%2520learning%2520via%2520the%2520contrastive%2520objective.%2520To%2520bridge%2520this%2520gap%2520in%250Aunderstanding%252C%2520we%2520propose%2520a%2520formal%2520definition%2520of%2520memorization%2520in%2520CLIP%2520%2528CLIPMem%2529%250Aand%2520use%2520it%2520to%2520quantify%2520memorization%2520in%2520CLIP%2520models.%2520Our%2520results%2520indicate%2520that%250ACLIP%2527s%2520memorization%2520behavior%2520falls%2520between%2520the%2520supervised%2520and%2520self-supervised%250Aparadigms%252C%2520with%2520%2522mis-captioned%2522%2520samples%2520exhibiting%2520highest%2520levels%2520of%250Amemorization.%2520Additionally%252C%2520we%2520find%2520that%2520the%2520text%2520encoder%2520contributes%2520more%2520to%250Amemorization%2520than%2520the%2520image%2520encoder%252C%2520suggesting%2520that%2520mitigation%2520strategies%250Ashould%2520focus%2520on%2520the%2520text%2520domain.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%250Amultiple%2520strategies%2520to%2520reduce%2520memorization%2520while%2520at%2520the%2520same%2520time%2520improving%250Autility--something%2520that%2520had%2520not%2520been%2520shown%2520before%2520for%2520traditional%2520learning%250Aparadigms%2520where%2520reducing%2520memorization%2520typically%2520results%2520in%2520utility%2520decrease.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07830v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Captured%20by%20Captions%3A%20On%20Memorization%20and%20its%20Mitigation%20in%20CLIP%20Models&entry.906535625=Wenhao%20Wang%20and%20Adam%20Dziedzic%20and%20Grace%20C.%20Kim%20and%20Michael%20Backes%20and%20Franziska%20Boenisch&entry.1292438233=%20%20Multi-modal%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%20strong%20performance%20in%0Aaligning%20visual%20and%20textual%20representations%2C%20excelling%20in%20tasks%20like%20image%0Aretrieval%20and%20zero-shot%20classification.%20Despite%20this%20success%2C%20the%20mechanisms%20by%0Awhich%20these%20models%20utilize%20training%20data%2C%20particularly%20the%20role%20of%0Amemorization%2C%20remain%20unclear.%20In%20uni-modal%20models%2C%20both%20supervised%20and%0Aself-supervised%2C%20memorization%20has%20been%20shown%20to%20be%20essential%20for%0Ageneralization.%20However%2C%20it%20is%20not%20well%20understood%20how%20these%20findings%20would%0Aapply%20to%20CLIP%2C%20which%20incorporates%20elements%20from%20both%20supervised%20learning%20via%0Acaptions%20that%20provide%20a%20supervisory%20signal%20similar%20to%20labels%2C%20and%20from%0Aself-supervised%20learning%20via%20the%20contrastive%20objective.%20To%20bridge%20this%20gap%20in%0Aunderstanding%2C%20we%20propose%20a%20formal%20definition%20of%20memorization%20in%20CLIP%20%28CLIPMem%29%0Aand%20use%20it%20to%20quantify%20memorization%20in%20CLIP%20models.%20Our%20results%20indicate%20that%0ACLIP%27s%20memorization%20behavior%20falls%20between%20the%20supervised%20and%20self-supervised%0Aparadigms%2C%20with%20%22mis-captioned%22%20samples%20exhibiting%20highest%20levels%20of%0Amemorization.%20Additionally%2C%20we%20find%20that%20the%20text%20encoder%20contributes%20more%20to%0Amemorization%20than%20the%20image%20encoder%2C%20suggesting%20that%20mitigation%20strategies%0Ashould%20focus%20on%20the%20text%20domain.%20Building%20on%20these%20insights%2C%20we%20propose%0Amultiple%20strategies%20to%20reduce%20memorization%20while%20at%20the%20same%20time%20improving%0Autility--something%20that%20had%20not%20been%20shown%20before%20for%20traditional%20learning%0Aparadigms%20where%20reducing%20memorization%20typically%20results%20in%20utility%20decrease.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07830v2&entry.124074799=Read"},
{"title": "Cross-modal feature fusion for robust point cloud registration with\n  ambiguous geometry", "author": "Zhaoyi Wang and Shengyu Huang and Jemil Avers Butt and Yuanzhou Cai and Matej Varga and Andreas Wieser", "abstract": "  Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)\n", "link": "http://arxiv.org/abs/2505.13088v1", "date": "2025-05-19", "relevancy": 2.8006, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6006}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5417}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20feature%20fusion%20for%20robust%20point%20cloud%20registration%20with%0A%20%20ambiguous%20geometry&body=Title%3A%20Cross-modal%20feature%20fusion%20for%20robust%20point%20cloud%20registration%20with%0A%20%20ambiguous%20geometry%0AAuthor%3A%20Zhaoyi%20Wang%20and%20Shengyu%20Huang%20and%20Jemil%20Avers%20Butt%20and%20Yuanzhou%20Cai%20and%20Matej%20Varga%20and%20Andreas%20Wieser%0AAbstract%3A%20%20%20Point%20cloud%20registration%20has%20seen%20significant%20advancements%20with%20the%0Aapplication%20of%20deep%20learning%20techniques.%20However%2C%20existing%20approaches%20often%0Aoverlook%20the%20potential%20of%20integrating%20radiometric%20information%20from%20RGB%20images.%0AThis%20limitation%20reduces%20their%20effectiveness%20in%20aligning%20point%20clouds%20pairs%2C%0Aespecially%20in%20regions%20where%20geometric%20data%20alone%20is%20insufficient.%20When%20used%0Aeffectively%2C%20radiometric%20information%20can%20enhance%20the%20registration%20process%20by%0Aproviding%20context%20that%20is%20missing%20from%20purely%20geometric%20data.%20In%20this%20paper%2C%20we%0Apropose%20CoFF%2C%20a%20novel%20Cross-modal%20Feature%20Fusion%20method%20that%20utilizes%20both%0Apoint%20cloud%20geometry%20and%20RGB%20images%20for%20pairwise%20point%20cloud%20registration.%0AAssuming%20that%20the%20co-registration%20between%20point%20clouds%20and%20RGB%20images%20is%0Aavailable%2C%20CoFF%20explicitly%20addresses%20the%20challenges%20where%20geometric%20information%0Aalone%20is%20unclear%2C%20such%20as%20in%20regions%20with%20symmetric%20similarity%20or%20planar%0Astructures%2C%20through%20a%20two-stage%20fusion%20of%203D%20point%20cloud%20features%20and%202D%20image%0Afeatures.%20It%20incorporates%20a%20cross-modal%20feature%20fusion%20module%20that%20assigns%0Apixel-wise%20image%20features%20to%203D%20input%20point%20clouds%20to%20enhance%20learned%203D%20point%0Afeatures%2C%20and%20integrates%20patch-wise%20image%20features%20with%20superpoint%20features%20to%0Aimprove%20the%20quality%20of%20coarse%20matching.%20This%20is%20followed%20by%20a%20coarse-to-fine%0Amatching%20module%20that%20accurately%20establishes%20correspondences%20using%20the%20fused%0Afeatures.%20We%20extensively%20evaluate%20CoFF%20on%20four%20common%20datasets%3A%203DMatch%2C%0A3DLoMatch%2C%20IndoorLRS%2C%20and%20the%20recently%20released%20ScanNet%2B%2B%20datasets.%20In%0Aaddition%2C%20we%20assess%20CoFF%20on%20specific%20subset%20datasets%20containing%20geometrically%0Aambiguous%20cases.%20Our%20experimental%20results%20demonstrate%20that%20CoFF%20achieves%0Astate-of-the-art%20registration%20performance%20across%20all%20benchmarks%2C%20including%0Aremarkable%20registration%20recalls%20of%2095.9%25%20and%2081.6%25%20on%20the%20widely-used%203DMatch%0Aand%203DLoMatch%20datasets%2C%20respectively...%28Truncated%20to%20fit%20arXiv%20abstract%20length%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-modal%2520feature%2520fusion%2520for%2520robust%2520point%2520cloud%2520registration%2520with%250A%2520%2520ambiguous%2520geometry%26entry.906535625%3DZhaoyi%2520Wang%2520and%2520Shengyu%2520Huang%2520and%2520Jemil%2520Avers%2520Butt%2520and%2520Yuanzhou%2520Cai%2520and%2520Matej%2520Varga%2520and%2520Andreas%2520Wieser%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520has%2520seen%2520significant%2520advancements%2520with%2520the%250Aapplication%2520of%2520deep%2520learning%2520techniques.%2520However%252C%2520existing%2520approaches%2520often%250Aoverlook%2520the%2520potential%2520of%2520integrating%2520radiometric%2520information%2520from%2520RGB%2520images.%250AThis%2520limitation%2520reduces%2520their%2520effectiveness%2520in%2520aligning%2520point%2520clouds%2520pairs%252C%250Aespecially%2520in%2520regions%2520where%2520geometric%2520data%2520alone%2520is%2520insufficient.%2520When%2520used%250Aeffectively%252C%2520radiometric%2520information%2520can%2520enhance%2520the%2520registration%2520process%2520by%250Aproviding%2520context%2520that%2520is%2520missing%2520from%2520purely%2520geometric%2520data.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520CoFF%252C%2520a%2520novel%2520Cross-modal%2520Feature%2520Fusion%2520method%2520that%2520utilizes%2520both%250Apoint%2520cloud%2520geometry%2520and%2520RGB%2520images%2520for%2520pairwise%2520point%2520cloud%2520registration.%250AAssuming%2520that%2520the%2520co-registration%2520between%2520point%2520clouds%2520and%2520RGB%2520images%2520is%250Aavailable%252C%2520CoFF%2520explicitly%2520addresses%2520the%2520challenges%2520where%2520geometric%2520information%250Aalone%2520is%2520unclear%252C%2520such%2520as%2520in%2520regions%2520with%2520symmetric%2520similarity%2520or%2520planar%250Astructures%252C%2520through%2520a%2520two-stage%2520fusion%2520of%25203D%2520point%2520cloud%2520features%2520and%25202D%2520image%250Afeatures.%2520It%2520incorporates%2520a%2520cross-modal%2520feature%2520fusion%2520module%2520that%2520assigns%250Apixel-wise%2520image%2520features%2520to%25203D%2520input%2520point%2520clouds%2520to%2520enhance%2520learned%25203D%2520point%250Afeatures%252C%2520and%2520integrates%2520patch-wise%2520image%2520features%2520with%2520superpoint%2520features%2520to%250Aimprove%2520the%2520quality%2520of%2520coarse%2520matching.%2520This%2520is%2520followed%2520by%2520a%2520coarse-to-fine%250Amatching%2520module%2520that%2520accurately%2520establishes%2520correspondences%2520using%2520the%2520fused%250Afeatures.%2520We%2520extensively%2520evaluate%2520CoFF%2520on%2520four%2520common%2520datasets%253A%25203DMatch%252C%250A3DLoMatch%252C%2520IndoorLRS%252C%2520and%2520the%2520recently%2520released%2520ScanNet%252B%252B%2520datasets.%2520In%250Aaddition%252C%2520we%2520assess%2520CoFF%2520on%2520specific%2520subset%2520datasets%2520containing%2520geometrically%250Aambiguous%2520cases.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520CoFF%2520achieves%250Astate-of-the-art%2520registration%2520performance%2520across%2520all%2520benchmarks%252C%2520including%250Aremarkable%2520registration%2520recalls%2520of%252095.9%2525%2520and%252081.6%2525%2520on%2520the%2520widely-used%25203DMatch%250Aand%25203DLoMatch%2520datasets%252C%2520respectively...%2528Truncated%2520to%2520fit%2520arXiv%2520abstract%2520length%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20feature%20fusion%20for%20robust%20point%20cloud%20registration%20with%0A%20%20ambiguous%20geometry&entry.906535625=Zhaoyi%20Wang%20and%20Shengyu%20Huang%20and%20Jemil%20Avers%20Butt%20and%20Yuanzhou%20Cai%20and%20Matej%20Varga%20and%20Andreas%20Wieser&entry.1292438233=%20%20Point%20cloud%20registration%20has%20seen%20significant%20advancements%20with%20the%0Aapplication%20of%20deep%20learning%20techniques.%20However%2C%20existing%20approaches%20often%0Aoverlook%20the%20potential%20of%20integrating%20radiometric%20information%20from%20RGB%20images.%0AThis%20limitation%20reduces%20their%20effectiveness%20in%20aligning%20point%20clouds%20pairs%2C%0Aespecially%20in%20regions%20where%20geometric%20data%20alone%20is%20insufficient.%20When%20used%0Aeffectively%2C%20radiometric%20information%20can%20enhance%20the%20registration%20process%20by%0Aproviding%20context%20that%20is%20missing%20from%20purely%20geometric%20data.%20In%20this%20paper%2C%20we%0Apropose%20CoFF%2C%20a%20novel%20Cross-modal%20Feature%20Fusion%20method%20that%20utilizes%20both%0Apoint%20cloud%20geometry%20and%20RGB%20images%20for%20pairwise%20point%20cloud%20registration.%0AAssuming%20that%20the%20co-registration%20between%20point%20clouds%20and%20RGB%20images%20is%0Aavailable%2C%20CoFF%20explicitly%20addresses%20the%20challenges%20where%20geometric%20information%0Aalone%20is%20unclear%2C%20such%20as%20in%20regions%20with%20symmetric%20similarity%20or%20planar%0Astructures%2C%20through%20a%20two-stage%20fusion%20of%203D%20point%20cloud%20features%20and%202D%20image%0Afeatures.%20It%20incorporates%20a%20cross-modal%20feature%20fusion%20module%20that%20assigns%0Apixel-wise%20image%20features%20to%203D%20input%20point%20clouds%20to%20enhance%20learned%203D%20point%0Afeatures%2C%20and%20integrates%20patch-wise%20image%20features%20with%20superpoint%20features%20to%0Aimprove%20the%20quality%20of%20coarse%20matching.%20This%20is%20followed%20by%20a%20coarse-to-fine%0Amatching%20module%20that%20accurately%20establishes%20correspondences%20using%20the%20fused%0Afeatures.%20We%20extensively%20evaluate%20CoFF%20on%20four%20common%20datasets%3A%203DMatch%2C%0A3DLoMatch%2C%20IndoorLRS%2C%20and%20the%20recently%20released%20ScanNet%2B%2B%20datasets.%20In%0Aaddition%2C%20we%20assess%20CoFF%20on%20specific%20subset%20datasets%20containing%20geometrically%0Aambiguous%20cases.%20Our%20experimental%20results%20demonstrate%20that%20CoFF%20achieves%0Astate-of-the-art%20registration%20performance%20across%20all%20benchmarks%2C%20including%0Aremarkable%20registration%20recalls%20of%2095.9%25%20and%2081.6%25%20on%20the%20widely-used%203DMatch%0Aand%203DLoMatch%20datasets%2C%20respectively...%28Truncated%20to%20fit%20arXiv%20abstract%20length%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13088v1&entry.124074799=Read"},
{"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large\n  Vision-Language Models", "author": "Liyan Tang and Grace Kim and Xinyu Zhao and Thom Lake and Wenxuan Ding and Fangcong Yin and Prasann Singhal and Manya Wadhwa and Zeyu Leo Liu and Zayne Sprague and Ramya Namuduri and Bodun Hu and Juan Diego Rodriguez and Puyuan Peng and Greg Durrett", "abstract": "  Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.\n", "link": "http://arxiv.org/abs/2505.13444v1", "date": "2025-05-19", "relevancy": 2.7727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartMuseum%3A%20Testing%20Visual%20Reasoning%20Capabilities%20of%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20ChartMuseum%3A%20Testing%20Visual%20Reasoning%20Capabilities%20of%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Liyan%20Tang%20and%20Grace%20Kim%20and%20Xinyu%20Zhao%20and%20Thom%20Lake%20and%20Wenxuan%20Ding%20and%20Fangcong%20Yin%20and%20Prasann%20Singhal%20and%20Manya%20Wadhwa%20and%20Zeyu%20Leo%20Liu%20and%20Zayne%20Sprague%20and%20Ramya%20Namuduri%20and%20Bodun%20Hu%20and%20Juan%20Diego%20Rodriguez%20and%20Puyuan%20Peng%20and%20Greg%20Durrett%0AAbstract%3A%20%20%20Chart%20understanding%20presents%20a%20unique%20challenge%20for%20large%20vision-language%0Amodels%20%28LVLMs%29%2C%20as%20it%20requires%20the%20integration%20of%20sophisticated%20textual%20and%0Avisual%20reasoning%20capabilities.%20However%2C%20current%20LVLMs%20exhibit%20a%20notable%0Aimbalance%20between%20these%20skills%2C%20falling%20short%20on%20visual%20reasoning%20that%20is%0Adifficult%20to%20perform%20in%20text.%20We%20conduct%20a%20case%20study%20using%20a%20synthetic%20dataset%0Asolvable%20only%20through%20visual%20reasoning%20and%20show%20that%20model%20performance%20degrades%0Asignificantly%20with%20increasing%20visual%20complexity%2C%20while%20human%20performance%0Aremains%20robust.%20We%20then%20introduce%20ChartMuseum%2C%20a%20new%20Chart%20Question%20Answering%0A%28QA%29%20benchmark%20containing%201%2C162%20expert-annotated%20questions%20spanning%20multiple%0Areasoning%20types%2C%20curated%20from%20real-world%20charts%20across%20184%20sources%2C%0Aspecifically%20built%20to%20evaluate%20complex%20visual%20and%20textual%20reasoning.%20Unlike%0Aprior%20chart%20understanding%20benchmarks%20--%20where%20frontier%20models%20perform%20similarly%0Aand%20near%20saturation%20--%20our%20benchmark%20exposes%20a%20substantial%20gap%20between%20model%0Aand%20human%20performance%2C%20while%20effectively%20differentiating%20model%20capabilities%3A%0Aalthough%20humans%20achieve%2093%25%20accuracy%2C%20the%20best-performing%20model%20Gemini-2.5-Pro%0Aattains%20only%2063.0%25%2C%20and%20the%20leading%20open-source%20LVLM%20Qwen2.5-VL-72B-Instruct%0Aachieves%20only%2038.5%25.%20Moreover%2C%20on%20questions%20requiring%20primarily%20visual%0Areasoning%2C%20all%20models%20experience%20a%2035%25-55%25%20performance%20drop%20from%0Atext-reasoning-heavy%20question%20performance.%20Lastly%2C%20our%20qualitative%20error%0Aanalysis%20reveals%20specific%20categories%20of%20visual%20reasoning%20that%20are%20challenging%0Afor%20current%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartMuseum%253A%2520Testing%2520Visual%2520Reasoning%2520Capabilities%2520of%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DLiyan%2520Tang%2520and%2520Grace%2520Kim%2520and%2520Xinyu%2520Zhao%2520and%2520Thom%2520Lake%2520and%2520Wenxuan%2520Ding%2520and%2520Fangcong%2520Yin%2520and%2520Prasann%2520Singhal%2520and%2520Manya%2520Wadhwa%2520and%2520Zeyu%2520Leo%2520Liu%2520and%2520Zayne%2520Sprague%2520and%2520Ramya%2520Namuduri%2520and%2520Bodun%2520Hu%2520and%2520Juan%2520Diego%2520Rodriguez%2520and%2520Puyuan%2520Peng%2520and%2520Greg%2520Durrett%26entry.1292438233%3D%2520%2520Chart%2520understanding%2520presents%2520a%2520unique%2520challenge%2520for%2520large%2520vision-language%250Amodels%2520%2528LVLMs%2529%252C%2520as%2520it%2520requires%2520the%2520integration%2520of%2520sophisticated%2520textual%2520and%250Avisual%2520reasoning%2520capabilities.%2520However%252C%2520current%2520LVLMs%2520exhibit%2520a%2520notable%250Aimbalance%2520between%2520these%2520skills%252C%2520falling%2520short%2520on%2520visual%2520reasoning%2520that%2520is%250Adifficult%2520to%2520perform%2520in%2520text.%2520We%2520conduct%2520a%2520case%2520study%2520using%2520a%2520synthetic%2520dataset%250Asolvable%2520only%2520through%2520visual%2520reasoning%2520and%2520show%2520that%2520model%2520performance%2520degrades%250Asignificantly%2520with%2520increasing%2520visual%2520complexity%252C%2520while%2520human%2520performance%250Aremains%2520robust.%2520We%2520then%2520introduce%2520ChartMuseum%252C%2520a%2520new%2520Chart%2520Question%2520Answering%250A%2528QA%2529%2520benchmark%2520containing%25201%252C162%2520expert-annotated%2520questions%2520spanning%2520multiple%250Areasoning%2520types%252C%2520curated%2520from%2520real-world%2520charts%2520across%2520184%2520sources%252C%250Aspecifically%2520built%2520to%2520evaluate%2520complex%2520visual%2520and%2520textual%2520reasoning.%2520Unlike%250Aprior%2520chart%2520understanding%2520benchmarks%2520--%2520where%2520frontier%2520models%2520perform%2520similarly%250Aand%2520near%2520saturation%2520--%2520our%2520benchmark%2520exposes%2520a%2520substantial%2520gap%2520between%2520model%250Aand%2520human%2520performance%252C%2520while%2520effectively%2520differentiating%2520model%2520capabilities%253A%250Aalthough%2520humans%2520achieve%252093%2525%2520accuracy%252C%2520the%2520best-performing%2520model%2520Gemini-2.5-Pro%250Aattains%2520only%252063.0%2525%252C%2520and%2520the%2520leading%2520open-source%2520LVLM%2520Qwen2.5-VL-72B-Instruct%250Aachieves%2520only%252038.5%2525.%2520Moreover%252C%2520on%2520questions%2520requiring%2520primarily%2520visual%250Areasoning%252C%2520all%2520models%2520experience%2520a%252035%2525-55%2525%2520performance%2520drop%2520from%250Atext-reasoning-heavy%2520question%2520performance.%2520Lastly%252C%2520our%2520qualitative%2520error%250Aanalysis%2520reveals%2520specific%2520categories%2520of%2520visual%2520reasoning%2520that%2520are%2520challenging%250Afor%2520current%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartMuseum%3A%20Testing%20Visual%20Reasoning%20Capabilities%20of%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Liyan%20Tang%20and%20Grace%20Kim%20and%20Xinyu%20Zhao%20and%20Thom%20Lake%20and%20Wenxuan%20Ding%20and%20Fangcong%20Yin%20and%20Prasann%20Singhal%20and%20Manya%20Wadhwa%20and%20Zeyu%20Leo%20Liu%20and%20Zayne%20Sprague%20and%20Ramya%20Namuduri%20and%20Bodun%20Hu%20and%20Juan%20Diego%20Rodriguez%20and%20Puyuan%20Peng%20and%20Greg%20Durrett&entry.1292438233=%20%20Chart%20understanding%20presents%20a%20unique%20challenge%20for%20large%20vision-language%0Amodels%20%28LVLMs%29%2C%20as%20it%20requires%20the%20integration%20of%20sophisticated%20textual%20and%0Avisual%20reasoning%20capabilities.%20However%2C%20current%20LVLMs%20exhibit%20a%20notable%0Aimbalance%20between%20these%20skills%2C%20falling%20short%20on%20visual%20reasoning%20that%20is%0Adifficult%20to%20perform%20in%20text.%20We%20conduct%20a%20case%20study%20using%20a%20synthetic%20dataset%0Asolvable%20only%20through%20visual%20reasoning%20and%20show%20that%20model%20performance%20degrades%0Asignificantly%20with%20increasing%20visual%20complexity%2C%20while%20human%20performance%0Aremains%20robust.%20We%20then%20introduce%20ChartMuseum%2C%20a%20new%20Chart%20Question%20Answering%0A%28QA%29%20benchmark%20containing%201%2C162%20expert-annotated%20questions%20spanning%20multiple%0Areasoning%20types%2C%20curated%20from%20real-world%20charts%20across%20184%20sources%2C%0Aspecifically%20built%20to%20evaluate%20complex%20visual%20and%20textual%20reasoning.%20Unlike%0Aprior%20chart%20understanding%20benchmarks%20--%20where%20frontier%20models%20perform%20similarly%0Aand%20near%20saturation%20--%20our%20benchmark%20exposes%20a%20substantial%20gap%20between%20model%0Aand%20human%20performance%2C%20while%20effectively%20differentiating%20model%20capabilities%3A%0Aalthough%20humans%20achieve%2093%25%20accuracy%2C%20the%20best-performing%20model%20Gemini-2.5-Pro%0Aattains%20only%2063.0%25%2C%20and%20the%20leading%20open-source%20LVLM%20Qwen2.5-VL-72B-Instruct%0Aachieves%20only%2038.5%25.%20Moreover%2C%20on%20questions%20requiring%20primarily%20visual%0Areasoning%2C%20all%20models%20experience%20a%2035%25-55%25%20performance%20drop%20from%0Atext-reasoning-heavy%20question%20performance.%20Lastly%2C%20our%20qualitative%20error%0Aanalysis%20reveals%20specific%20categories%20of%20visual%20reasoning%20that%20are%20challenging%0Afor%20current%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13444v1&entry.124074799=Read"},
{"title": "GMM-Based Comprehensive Feature Extraction and Relative Distance\n  Preservation For Few-Shot Cross-Modal Retrieval", "author": "Chengsong Sun and Weiping Li and Xiang Li and Yuankun Liu and Lianlei Shan", "abstract": "  Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.13306v1", "date": "2025-05-19", "relevancy": 2.7574, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.577}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.55}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMM-Based%20Comprehensive%20Feature%20Extraction%20and%20Relative%20Distance%0A%20%20Preservation%20For%20Few-Shot%20Cross-Modal%20Retrieval&body=Title%3A%20GMM-Based%20Comprehensive%20Feature%20Extraction%20and%20Relative%20Distance%0A%20%20Preservation%20For%20Few-Shot%20Cross-Modal%20Retrieval%0AAuthor%3A%20Chengsong%20Sun%20and%20Weiping%20Li%20and%20Xiang%20Li%20and%20Yuankun%20Liu%20and%20Lianlei%20Shan%0AAbstract%3A%20%20%20Few-shot%20cross-modal%20retrieval%20focuses%20on%20learning%20cross-modal%0Arepresentations%20with%20limited%20training%20samples%2C%20enabling%20the%20model%20to%20handle%0Aunseen%20classes%20during%20inference.%20Unlike%20traditional%20cross-modal%20retrieval%0Atasks%2C%20which%20assume%20that%20both%20training%20and%20testing%20data%20share%20the%20same%20class%0Adistribution%2C%20few-shot%20retrieval%20involves%20data%20with%20sparse%20representations%0Aacross%20modalities.%20Existing%20methods%20often%20fail%20to%20adequately%20model%20the%0Amulti-peak%20distribution%20of%20few-shot%20cross-modal%20data%2C%20resulting%20in%20two%20main%0Abiases%20in%20the%20latent%20semantic%20space%3A%20intra-modal%20bias%2C%20where%20sparse%20samples%0Afail%20to%20capture%20intra-class%20diversity%2C%20and%20inter-modal%20bias%2C%20where%0Amisalignments%20between%20image%20and%20text%20distributions%20exacerbate%20the%20semantic%20gap.%0AThese%20biases%20hinder%20retrieval%20accuracy.%20To%20address%20these%20issues%2C%20we%20propose%20a%0Anovel%20method%2C%20GCRDP%2C%20for%20few-shot%20cross-modal%20retrieval.%20This%20approach%0Aeffectively%20captures%20the%20complex%20multi-peak%20distribution%20of%20data%20using%20a%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20incorporates%20a%20multi-positive%20sample%0Acontrastive%20learning%20mechanism%20for%20comprehensive%20feature%20modeling.%0AAdditionally%2C%20we%20introduce%20a%20new%20strategy%20for%20cross-modal%20semantic%20alignment%2C%0Awhich%20constrains%20the%20relative%20distances%20between%20image%20and%20text%20feature%0Adistributions%2C%20thereby%20improving%20the%20accuracy%20of%20cross-modal%20representations.%0AWe%20validate%20our%20approach%20through%20extensive%20experiments%20on%20four%20benchmark%0Adatasets%2C%20demonstrating%20superior%20performance%20over%20six%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMM-Based%2520Comprehensive%2520Feature%2520Extraction%2520and%2520Relative%2520Distance%250A%2520%2520Preservation%2520For%2520Few-Shot%2520Cross-Modal%2520Retrieval%26entry.906535625%3DChengsong%2520Sun%2520and%2520Weiping%2520Li%2520and%2520Xiang%2520Li%2520and%2520Yuankun%2520Liu%2520and%2520Lianlei%2520Shan%26entry.1292438233%3D%2520%2520Few-shot%2520cross-modal%2520retrieval%2520focuses%2520on%2520learning%2520cross-modal%250Arepresentations%2520with%2520limited%2520training%2520samples%252C%2520enabling%2520the%2520model%2520to%2520handle%250Aunseen%2520classes%2520during%2520inference.%2520Unlike%2520traditional%2520cross-modal%2520retrieval%250Atasks%252C%2520which%2520assume%2520that%2520both%2520training%2520and%2520testing%2520data%2520share%2520the%2520same%2520class%250Adistribution%252C%2520few-shot%2520retrieval%2520involves%2520data%2520with%2520sparse%2520representations%250Aacross%2520modalities.%2520Existing%2520methods%2520often%2520fail%2520to%2520adequately%2520model%2520the%250Amulti-peak%2520distribution%2520of%2520few-shot%2520cross-modal%2520data%252C%2520resulting%2520in%2520two%2520main%250Abiases%2520in%2520the%2520latent%2520semantic%2520space%253A%2520intra-modal%2520bias%252C%2520where%2520sparse%2520samples%250Afail%2520to%2520capture%2520intra-class%2520diversity%252C%2520and%2520inter-modal%2520bias%252C%2520where%250Amisalignments%2520between%2520image%2520and%2520text%2520distributions%2520exacerbate%2520the%2520semantic%2520gap.%250AThese%2520biases%2520hinder%2520retrieval%2520accuracy.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%250Anovel%2520method%252C%2520GCRDP%252C%2520for%2520few-shot%2520cross-modal%2520retrieval.%2520This%2520approach%250Aeffectively%2520captures%2520the%2520complex%2520multi-peak%2520distribution%2520of%2520data%2520using%2520a%250AGaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520and%2520incorporates%2520a%2520multi-positive%2520sample%250Acontrastive%2520learning%2520mechanism%2520for%2520comprehensive%2520feature%2520modeling.%250AAdditionally%252C%2520we%2520introduce%2520a%2520new%2520strategy%2520for%2520cross-modal%2520semantic%2520alignment%252C%250Awhich%2520constrains%2520the%2520relative%2520distances%2520between%2520image%2520and%2520text%2520feature%250Adistributions%252C%2520thereby%2520improving%2520the%2520accuracy%2520of%2520cross-modal%2520representations.%250AWe%2520validate%2520our%2520approach%2520through%2520extensive%2520experiments%2520on%2520four%2520benchmark%250Adatasets%252C%2520demonstrating%2520superior%2520performance%2520over%2520six%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMM-Based%20Comprehensive%20Feature%20Extraction%20and%20Relative%20Distance%0A%20%20Preservation%20For%20Few-Shot%20Cross-Modal%20Retrieval&entry.906535625=Chengsong%20Sun%20and%20Weiping%20Li%20and%20Xiang%20Li%20and%20Yuankun%20Liu%20and%20Lianlei%20Shan&entry.1292438233=%20%20Few-shot%20cross-modal%20retrieval%20focuses%20on%20learning%20cross-modal%0Arepresentations%20with%20limited%20training%20samples%2C%20enabling%20the%20model%20to%20handle%0Aunseen%20classes%20during%20inference.%20Unlike%20traditional%20cross-modal%20retrieval%0Atasks%2C%20which%20assume%20that%20both%20training%20and%20testing%20data%20share%20the%20same%20class%0Adistribution%2C%20few-shot%20retrieval%20involves%20data%20with%20sparse%20representations%0Aacross%20modalities.%20Existing%20methods%20often%20fail%20to%20adequately%20model%20the%0Amulti-peak%20distribution%20of%20few-shot%20cross-modal%20data%2C%20resulting%20in%20two%20main%0Abiases%20in%20the%20latent%20semantic%20space%3A%20intra-modal%20bias%2C%20where%20sparse%20samples%0Afail%20to%20capture%20intra-class%20diversity%2C%20and%20inter-modal%20bias%2C%20where%0Amisalignments%20between%20image%20and%20text%20distributions%20exacerbate%20the%20semantic%20gap.%0AThese%20biases%20hinder%20retrieval%20accuracy.%20To%20address%20these%20issues%2C%20we%20propose%20a%0Anovel%20method%2C%20GCRDP%2C%20for%20few-shot%20cross-modal%20retrieval.%20This%20approach%0Aeffectively%20captures%20the%20complex%20multi-peak%20distribution%20of%20data%20using%20a%0AGaussian%20Mixture%20Model%20%28GMM%29%20and%20incorporates%20a%20multi-positive%20sample%0Acontrastive%20learning%20mechanism%20for%20comprehensive%20feature%20modeling.%0AAdditionally%2C%20we%20introduce%20a%20new%20strategy%20for%20cross-modal%20semantic%20alignment%2C%0Awhich%20constrains%20the%20relative%20distances%20between%20image%20and%20text%20feature%0Adistributions%2C%20thereby%20improving%20the%20accuracy%20of%20cross-modal%20representations.%0AWe%20validate%20our%20approach%20through%20extensive%20experiments%20on%20four%20benchmark%0Adatasets%2C%20demonstrating%20superior%20performance%20over%20six%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13306v1&entry.124074799=Read"},
{"title": "FIOVA: A Multi-Annotator Benchmark for Human-Aligned Video Captioning", "author": "Shiyu Hu and Xuchen Li and Xuzhao Li and Jing Zhang and Yipei Wang and Xin Zhao and Kang Hao Cheong", "abstract": "  Despite rapid progress in large vision-language models (LVLMs), existing\nvideo caption benchmarks remain limited in evaluating their alignment with\nhuman understanding. Most rely on a single annotation per video and lexical\nsimilarity-based metrics, failing to capture the variability in human\nperception and the cognitive importance of events. These limitations hinder\naccurate diagnosis of model capabilities in producing coherent, complete, and\nhuman-aligned descriptions. To address this, we introduce FIOVA (Five-In-One\nVideo Annotations), a human-centric benchmark tailored for evaluation. It\ncomprises 3,002 real-world videos (about 33.6s each), each annotated\nindependently by five annotators. This design enables modeling of semantic\ndiversity and inter-subjective agreement, offering a richer foundation for\nmeasuring human-machine alignment. We further propose FIOVA-DQ, an event-level\nevaluation metric that incorporates cognitive weights derived from annotator\nconsensus, providing fine-grained assessment of event relevance and semantic\ncoverage. Leveraging FIOVA, we conduct a comprehensive evaluation of nine\nrepresentative LVLMs and introduce a complexity-aware analysis framework based\non inter-annotator variation (CV). This reveals consistency gaps across\ndifficulty levels and identifies structural issues such as event\nunder-description and template convergence. Our results highlight FIOVA's\ndiagnostic value for understanding LVLM behavior under varying complexity,\nsetting a new standard for cognitively aligned evaluation in long-video\ncaptioning. The benchmark, annotations, metric, and model outputs are publicly\nreleased to support future evaluation-driven research in video understanding.\nMore detailed information can be found at https://huuuuusy.github.io/fiova/.\n", "link": "http://arxiv.org/abs/2410.15270v2", "date": "2025-05-19", "relevancy": 2.7491, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5575}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FIOVA%3A%20A%20Multi-Annotator%20Benchmark%20for%20Human-Aligned%20Video%20Captioning&body=Title%3A%20FIOVA%3A%20A%20Multi-Annotator%20Benchmark%20for%20Human-Aligned%20Video%20Captioning%0AAuthor%3A%20Shiyu%20Hu%20and%20Xuchen%20Li%20and%20Xuzhao%20Li%20and%20Jing%20Zhang%20and%20Yipei%20Wang%20and%20Xin%20Zhao%20and%20Kang%20Hao%20Cheong%0AAbstract%3A%20%20%20Despite%20rapid%20progress%20in%20large%20vision-language%20models%20%28LVLMs%29%2C%20existing%0Avideo%20caption%20benchmarks%20remain%20limited%20in%20evaluating%20their%20alignment%20with%0Ahuman%20understanding.%20Most%20rely%20on%20a%20single%20annotation%20per%20video%20and%20lexical%0Asimilarity-based%20metrics%2C%20failing%20to%20capture%20the%20variability%20in%20human%0Aperception%20and%20the%20cognitive%20importance%20of%20events.%20These%20limitations%20hinder%0Aaccurate%20diagnosis%20of%20model%20capabilities%20in%20producing%20coherent%2C%20complete%2C%20and%0Ahuman-aligned%20descriptions.%20To%20address%20this%2C%20we%20introduce%20FIOVA%20%28Five-In-One%0AVideo%20Annotations%29%2C%20a%20human-centric%20benchmark%20tailored%20for%20evaluation.%20It%0Acomprises%203%2C002%20real-world%20videos%20%28about%2033.6s%20each%29%2C%20each%20annotated%0Aindependently%20by%20five%20annotators.%20This%20design%20enables%20modeling%20of%20semantic%0Adiversity%20and%20inter-subjective%20agreement%2C%20offering%20a%20richer%20foundation%20for%0Ameasuring%20human-machine%20alignment.%20We%20further%20propose%20FIOVA-DQ%2C%20an%20event-level%0Aevaluation%20metric%20that%20incorporates%20cognitive%20weights%20derived%20from%20annotator%0Aconsensus%2C%20providing%20fine-grained%20assessment%20of%20event%20relevance%20and%20semantic%0Acoverage.%20Leveraging%20FIOVA%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20nine%0Arepresentative%20LVLMs%20and%20introduce%20a%20complexity-aware%20analysis%20framework%20based%0Aon%20inter-annotator%20variation%20%28CV%29.%20This%20reveals%20consistency%20gaps%20across%0Adifficulty%20levels%20and%20identifies%20structural%20issues%20such%20as%20event%0Aunder-description%20and%20template%20convergence.%20Our%20results%20highlight%20FIOVA%27s%0Adiagnostic%20value%20for%20understanding%20LVLM%20behavior%20under%20varying%20complexity%2C%0Asetting%20a%20new%20standard%20for%20cognitively%20aligned%20evaluation%20in%20long-video%0Acaptioning.%20The%20benchmark%2C%20annotations%2C%20metric%2C%20and%20model%20outputs%20are%20publicly%0Areleased%20to%20support%20future%20evaluation-driven%20research%20in%20video%20understanding.%0AMore%20detailed%20information%20can%20be%20found%20at%20https%3A//huuuuusy.github.io/fiova/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFIOVA%253A%2520A%2520Multi-Annotator%2520Benchmark%2520for%2520Human-Aligned%2520Video%2520Captioning%26entry.906535625%3DShiyu%2520Hu%2520and%2520Xuchen%2520Li%2520and%2520Xuzhao%2520Li%2520and%2520Jing%2520Zhang%2520and%2520Yipei%2520Wang%2520and%2520Xin%2520Zhao%2520and%2520Kang%2520Hao%2520Cheong%26entry.1292438233%3D%2520%2520Despite%2520rapid%2520progress%2520in%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%252C%2520existing%250Avideo%2520caption%2520benchmarks%2520remain%2520limited%2520in%2520evaluating%2520their%2520alignment%2520with%250Ahuman%2520understanding.%2520Most%2520rely%2520on%2520a%2520single%2520annotation%2520per%2520video%2520and%2520lexical%250Asimilarity-based%2520metrics%252C%2520failing%2520to%2520capture%2520the%2520variability%2520in%2520human%250Aperception%2520and%2520the%2520cognitive%2520importance%2520of%2520events.%2520These%2520limitations%2520hinder%250Aaccurate%2520diagnosis%2520of%2520model%2520capabilities%2520in%2520producing%2520coherent%252C%2520complete%252C%2520and%250Ahuman-aligned%2520descriptions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520FIOVA%2520%2528Five-In-One%250AVideo%2520Annotations%2529%252C%2520a%2520human-centric%2520benchmark%2520tailored%2520for%2520evaluation.%2520It%250Acomprises%25203%252C002%2520real-world%2520videos%2520%2528about%252033.6s%2520each%2529%252C%2520each%2520annotated%250Aindependently%2520by%2520five%2520annotators.%2520This%2520design%2520enables%2520modeling%2520of%2520semantic%250Adiversity%2520and%2520inter-subjective%2520agreement%252C%2520offering%2520a%2520richer%2520foundation%2520for%250Ameasuring%2520human-machine%2520alignment.%2520We%2520further%2520propose%2520FIOVA-DQ%252C%2520an%2520event-level%250Aevaluation%2520metric%2520that%2520incorporates%2520cognitive%2520weights%2520derived%2520from%2520annotator%250Aconsensus%252C%2520providing%2520fine-grained%2520assessment%2520of%2520event%2520relevance%2520and%2520semantic%250Acoverage.%2520Leveraging%2520FIOVA%252C%2520we%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520nine%250Arepresentative%2520LVLMs%2520and%2520introduce%2520a%2520complexity-aware%2520analysis%2520framework%2520based%250Aon%2520inter-annotator%2520variation%2520%2528CV%2529.%2520This%2520reveals%2520consistency%2520gaps%2520across%250Adifficulty%2520levels%2520and%2520identifies%2520structural%2520issues%2520such%2520as%2520event%250Aunder-description%2520and%2520template%2520convergence.%2520Our%2520results%2520highlight%2520FIOVA%2527s%250Adiagnostic%2520value%2520for%2520understanding%2520LVLM%2520behavior%2520under%2520varying%2520complexity%252C%250Asetting%2520a%2520new%2520standard%2520for%2520cognitively%2520aligned%2520evaluation%2520in%2520long-video%250Acaptioning.%2520The%2520benchmark%252C%2520annotations%252C%2520metric%252C%2520and%2520model%2520outputs%2520are%2520publicly%250Areleased%2520to%2520support%2520future%2520evaluation-driven%2520research%2520in%2520video%2520understanding.%250AMore%2520detailed%2520information%2520can%2520be%2520found%2520at%2520https%253A//huuuuusy.github.io/fiova/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIOVA%3A%20A%20Multi-Annotator%20Benchmark%20for%20Human-Aligned%20Video%20Captioning&entry.906535625=Shiyu%20Hu%20and%20Xuchen%20Li%20and%20Xuzhao%20Li%20and%20Jing%20Zhang%20and%20Yipei%20Wang%20and%20Xin%20Zhao%20and%20Kang%20Hao%20Cheong&entry.1292438233=%20%20Despite%20rapid%20progress%20in%20large%20vision-language%20models%20%28LVLMs%29%2C%20existing%0Avideo%20caption%20benchmarks%20remain%20limited%20in%20evaluating%20their%20alignment%20with%0Ahuman%20understanding.%20Most%20rely%20on%20a%20single%20annotation%20per%20video%20and%20lexical%0Asimilarity-based%20metrics%2C%20failing%20to%20capture%20the%20variability%20in%20human%0Aperception%20and%20the%20cognitive%20importance%20of%20events.%20These%20limitations%20hinder%0Aaccurate%20diagnosis%20of%20model%20capabilities%20in%20producing%20coherent%2C%20complete%2C%20and%0Ahuman-aligned%20descriptions.%20To%20address%20this%2C%20we%20introduce%20FIOVA%20%28Five-In-One%0AVideo%20Annotations%29%2C%20a%20human-centric%20benchmark%20tailored%20for%20evaluation.%20It%0Acomprises%203%2C002%20real-world%20videos%20%28about%2033.6s%20each%29%2C%20each%20annotated%0Aindependently%20by%20five%20annotators.%20This%20design%20enables%20modeling%20of%20semantic%0Adiversity%20and%20inter-subjective%20agreement%2C%20offering%20a%20richer%20foundation%20for%0Ameasuring%20human-machine%20alignment.%20We%20further%20propose%20FIOVA-DQ%2C%20an%20event-level%0Aevaluation%20metric%20that%20incorporates%20cognitive%20weights%20derived%20from%20annotator%0Aconsensus%2C%20providing%20fine-grained%20assessment%20of%20event%20relevance%20and%20semantic%0Acoverage.%20Leveraging%20FIOVA%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20nine%0Arepresentative%20LVLMs%20and%20introduce%20a%20complexity-aware%20analysis%20framework%20based%0Aon%20inter-annotator%20variation%20%28CV%29.%20This%20reveals%20consistency%20gaps%20across%0Adifficulty%20levels%20and%20identifies%20structural%20issues%20such%20as%20event%0Aunder-description%20and%20template%20convergence.%20Our%20results%20highlight%20FIOVA%27s%0Adiagnostic%20value%20for%20understanding%20LVLM%20behavior%20under%20varying%20complexity%2C%0Asetting%20a%20new%20standard%20for%20cognitively%20aligned%20evaluation%20in%20long-video%0Acaptioning.%20The%20benchmark%2C%20annotations%2C%20metric%2C%20and%20model%20outputs%20are%20publicly%0Areleased%20to%20support%20future%20evaluation-driven%20research%20in%20video%20understanding.%0AMore%20detailed%20information%20can%20be%20found%20at%20https%3A//huuuuusy.github.io/fiova/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15270v2&entry.124074799=Read"},
{"title": "A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused\n  Ultrasound Simulation", "author": "Vinkle Srivastav and Juliette Puel and Jonathan Vappou and Elijah Van Houten and Paolo Cabras and Nicolas Padoy", "abstract": "  Transcranial focused ultrasound (tFUS) is an emerging modality for\nnon-invasive brain stimulation and therapeutic intervention, offering\nmillimeter-scale spatial precision and the ability to target deep brain\nstructures. However, the heterogeneous and anisotropic nature of the human\nskull introduces significant distortions to the propagating ultrasound\nwavefront, which require time-consuming patient-specific planning and\ncorrections using numerical solvers for accurate targeting. To enable\ndata-driven approaches in this domain, we introduce TFUScapes, the first\nlarge-scale, high-resolution dataset of tFUS simulations through anatomically\nrealistic human skulls derived from T1-weighted MRI images. We have developed a\nscalable simulation engine pipeline using the k-Wave pseudo-spectral solver,\nwhere each simulation returns a steady-state pressure field generated by a\nfocused ultrasound transducer placed at realistic scalp locations. In addition\nto the dataset, we present DeepTFUS, a deep learning model that estimates\nnormalized pressure fields directly from input 3D CT volumes and transducer\nposition. The model extends a U-Net backbone with transducer-aware\nconditioning, incorporating Fourier-encoded position embeddings and MLP layers\nto create global transducer embeddings. These embeddings are fused with U-Net\nencoder features via feature-wise modulation, dynamic convolutions, and\ncross-attention mechanisms. The model is trained using a combination of\nspatially weighted and gradient-sensitive loss functions, enabling it to\napproximate high-fidelity wavefields. The TFUScapes dataset is publicly\nreleased to accelerate research at the intersection of computational acoustics,\nneurotechnology, and deep learning. The project page is available at\nhttps://github.com/CAMMA-public/TFUScapes.\n", "link": "http://arxiv.org/abs/2505.12998v1", "date": "2025-05-19", "relevancy": 2.7329, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.551}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.551}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Skull-Adaptive%20Framework%20for%20AI-Based%203D%20Transcranial%20Focused%0A%20%20Ultrasound%20Simulation&body=Title%3A%20A%20Skull-Adaptive%20Framework%20for%20AI-Based%203D%20Transcranial%20Focused%0A%20%20Ultrasound%20Simulation%0AAuthor%3A%20Vinkle%20Srivastav%20and%20Juliette%20Puel%20and%20Jonathan%20Vappou%20and%20Elijah%20Van%20Houten%20and%20Paolo%20Cabras%20and%20Nicolas%20Padoy%0AAbstract%3A%20%20%20Transcranial%20focused%20ultrasound%20%28tFUS%29%20is%20an%20emerging%20modality%20for%0Anon-invasive%20brain%20stimulation%20and%20therapeutic%20intervention%2C%20offering%0Amillimeter-scale%20spatial%20precision%20and%20the%20ability%20to%20target%20deep%20brain%0Astructures.%20However%2C%20the%20heterogeneous%20and%20anisotropic%20nature%20of%20the%20human%0Askull%20introduces%20significant%20distortions%20to%20the%20propagating%20ultrasound%0Awavefront%2C%20which%20require%20time-consuming%20patient-specific%20planning%20and%0Acorrections%20using%20numerical%20solvers%20for%20accurate%20targeting.%20To%20enable%0Adata-driven%20approaches%20in%20this%20domain%2C%20we%20introduce%20TFUScapes%2C%20the%20first%0Alarge-scale%2C%20high-resolution%20dataset%20of%20tFUS%20simulations%20through%20anatomically%0Arealistic%20human%20skulls%20derived%20from%20T1-weighted%20MRI%20images.%20We%20have%20developed%20a%0Ascalable%20simulation%20engine%20pipeline%20using%20the%20k-Wave%20pseudo-spectral%20solver%2C%0Awhere%20each%20simulation%20returns%20a%20steady-state%20pressure%20field%20generated%20by%20a%0Afocused%20ultrasound%20transducer%20placed%20at%20realistic%20scalp%20locations.%20In%20addition%0Ato%20the%20dataset%2C%20we%20present%20DeepTFUS%2C%20a%20deep%20learning%20model%20that%20estimates%0Anormalized%20pressure%20fields%20directly%20from%20input%203D%20CT%20volumes%20and%20transducer%0Aposition.%20The%20model%20extends%20a%20U-Net%20backbone%20with%20transducer-aware%0Aconditioning%2C%20incorporating%20Fourier-encoded%20position%20embeddings%20and%20MLP%20layers%0Ato%20create%20global%20transducer%20embeddings.%20These%20embeddings%20are%20fused%20with%20U-Net%0Aencoder%20features%20via%20feature-wise%20modulation%2C%20dynamic%20convolutions%2C%20and%0Across-attention%20mechanisms.%20The%20model%20is%20trained%20using%20a%20combination%20of%0Aspatially%20weighted%20and%20gradient-sensitive%20loss%20functions%2C%20enabling%20it%20to%0Aapproximate%20high-fidelity%20wavefields.%20The%20TFUScapes%20dataset%20is%20publicly%0Areleased%20to%20accelerate%20research%20at%20the%20intersection%20of%20computational%20acoustics%2C%0Aneurotechnology%2C%20and%20deep%20learning.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//github.com/CAMMA-public/TFUScapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Skull-Adaptive%2520Framework%2520for%2520AI-Based%25203D%2520Transcranial%2520Focused%250A%2520%2520Ultrasound%2520Simulation%26entry.906535625%3DVinkle%2520Srivastav%2520and%2520Juliette%2520Puel%2520and%2520Jonathan%2520Vappou%2520and%2520Elijah%2520Van%2520Houten%2520and%2520Paolo%2520Cabras%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3D%2520%2520Transcranial%2520focused%2520ultrasound%2520%2528tFUS%2529%2520is%2520an%2520emerging%2520modality%2520for%250Anon-invasive%2520brain%2520stimulation%2520and%2520therapeutic%2520intervention%252C%2520offering%250Amillimeter-scale%2520spatial%2520precision%2520and%2520the%2520ability%2520to%2520target%2520deep%2520brain%250Astructures.%2520However%252C%2520the%2520heterogeneous%2520and%2520anisotropic%2520nature%2520of%2520the%2520human%250Askull%2520introduces%2520significant%2520distortions%2520to%2520the%2520propagating%2520ultrasound%250Awavefront%252C%2520which%2520require%2520time-consuming%2520patient-specific%2520planning%2520and%250Acorrections%2520using%2520numerical%2520solvers%2520for%2520accurate%2520targeting.%2520To%2520enable%250Adata-driven%2520approaches%2520in%2520this%2520domain%252C%2520we%2520introduce%2520TFUScapes%252C%2520the%2520first%250Alarge-scale%252C%2520high-resolution%2520dataset%2520of%2520tFUS%2520simulations%2520through%2520anatomically%250Arealistic%2520human%2520skulls%2520derived%2520from%2520T1-weighted%2520MRI%2520images.%2520We%2520have%2520developed%2520a%250Ascalable%2520simulation%2520engine%2520pipeline%2520using%2520the%2520k-Wave%2520pseudo-spectral%2520solver%252C%250Awhere%2520each%2520simulation%2520returns%2520a%2520steady-state%2520pressure%2520field%2520generated%2520by%2520a%250Afocused%2520ultrasound%2520transducer%2520placed%2520at%2520realistic%2520scalp%2520locations.%2520In%2520addition%250Ato%2520the%2520dataset%252C%2520we%2520present%2520DeepTFUS%252C%2520a%2520deep%2520learning%2520model%2520that%2520estimates%250Anormalized%2520pressure%2520fields%2520directly%2520from%2520input%25203D%2520CT%2520volumes%2520and%2520transducer%250Aposition.%2520The%2520model%2520extends%2520a%2520U-Net%2520backbone%2520with%2520transducer-aware%250Aconditioning%252C%2520incorporating%2520Fourier-encoded%2520position%2520embeddings%2520and%2520MLP%2520layers%250Ato%2520create%2520global%2520transducer%2520embeddings.%2520These%2520embeddings%2520are%2520fused%2520with%2520U-Net%250Aencoder%2520features%2520via%2520feature-wise%2520modulation%252C%2520dynamic%2520convolutions%252C%2520and%250Across-attention%2520mechanisms.%2520The%2520model%2520is%2520trained%2520using%2520a%2520combination%2520of%250Aspatially%2520weighted%2520and%2520gradient-sensitive%2520loss%2520functions%252C%2520enabling%2520it%2520to%250Aapproximate%2520high-fidelity%2520wavefields.%2520The%2520TFUScapes%2520dataset%2520is%2520publicly%250Areleased%2520to%2520accelerate%2520research%2520at%2520the%2520intersection%2520of%2520computational%2520acoustics%252C%250Aneurotechnology%252C%2520and%2520deep%2520learning.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//github.com/CAMMA-public/TFUScapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Skull-Adaptive%20Framework%20for%20AI-Based%203D%20Transcranial%20Focused%0A%20%20Ultrasound%20Simulation&entry.906535625=Vinkle%20Srivastav%20and%20Juliette%20Puel%20and%20Jonathan%20Vappou%20and%20Elijah%20Van%20Houten%20and%20Paolo%20Cabras%20and%20Nicolas%20Padoy&entry.1292438233=%20%20Transcranial%20focused%20ultrasound%20%28tFUS%29%20is%20an%20emerging%20modality%20for%0Anon-invasive%20brain%20stimulation%20and%20therapeutic%20intervention%2C%20offering%0Amillimeter-scale%20spatial%20precision%20and%20the%20ability%20to%20target%20deep%20brain%0Astructures.%20However%2C%20the%20heterogeneous%20and%20anisotropic%20nature%20of%20the%20human%0Askull%20introduces%20significant%20distortions%20to%20the%20propagating%20ultrasound%0Awavefront%2C%20which%20require%20time-consuming%20patient-specific%20planning%20and%0Acorrections%20using%20numerical%20solvers%20for%20accurate%20targeting.%20To%20enable%0Adata-driven%20approaches%20in%20this%20domain%2C%20we%20introduce%20TFUScapes%2C%20the%20first%0Alarge-scale%2C%20high-resolution%20dataset%20of%20tFUS%20simulations%20through%20anatomically%0Arealistic%20human%20skulls%20derived%20from%20T1-weighted%20MRI%20images.%20We%20have%20developed%20a%0Ascalable%20simulation%20engine%20pipeline%20using%20the%20k-Wave%20pseudo-spectral%20solver%2C%0Awhere%20each%20simulation%20returns%20a%20steady-state%20pressure%20field%20generated%20by%20a%0Afocused%20ultrasound%20transducer%20placed%20at%20realistic%20scalp%20locations.%20In%20addition%0Ato%20the%20dataset%2C%20we%20present%20DeepTFUS%2C%20a%20deep%20learning%20model%20that%20estimates%0Anormalized%20pressure%20fields%20directly%20from%20input%203D%20CT%20volumes%20and%20transducer%0Aposition.%20The%20model%20extends%20a%20U-Net%20backbone%20with%20transducer-aware%0Aconditioning%2C%20incorporating%20Fourier-encoded%20position%20embeddings%20and%20MLP%20layers%0Ato%20create%20global%20transducer%20embeddings.%20These%20embeddings%20are%20fused%20with%20U-Net%0Aencoder%20features%20via%20feature-wise%20modulation%2C%20dynamic%20convolutions%2C%20and%0Across-attention%20mechanisms.%20The%20model%20is%20trained%20using%20a%20combination%20of%0Aspatially%20weighted%20and%20gradient-sensitive%20loss%20functions%2C%20enabling%20it%20to%0Aapproximate%20high-fidelity%20wavefields.%20The%20TFUScapes%20dataset%20is%20publicly%0Areleased%20to%20accelerate%20research%20at%20the%20intersection%20of%20computational%20acoustics%2C%0Aneurotechnology%2C%20and%20deep%20learning.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//github.com/CAMMA-public/TFUScapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12998v1&entry.124074799=Read"},
{"title": "Emergence of Fixational and Saccadic Movements in a Multi-Level\n  Recurrent Attention Model for Vision", "author": "Pengcheng Pan and Yonekura Shogo and Yasuo Kuniyoshi", "abstract": "  Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks.\n", "link": "http://arxiv.org/abs/2505.13191v1", "date": "2025-05-19", "relevancy": 2.6912, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5401}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Fixational%20and%20Saccadic%20Movements%20in%20a%20Multi-Level%0A%20%20Recurrent%20Attention%20Model%20for%20Vision&body=Title%3A%20Emergence%20of%20Fixational%20and%20Saccadic%20Movements%20in%20a%20Multi-Level%0A%20%20Recurrent%20Attention%20Model%20for%20Vision%0AAuthor%3A%20Pengcheng%20Pan%20and%20Yonekura%20Shogo%20and%20Yasuo%20Kuniyoshi%0AAbstract%3A%20%20%20Inspired%20by%20foveal%20vision%2C%20hard%20attention%20models%20promise%20interpretability%20and%0Aparameter%20economy.%20However%2C%20existing%20models%20like%20the%20Recurrent%20Model%20of%20Visual%0AAttention%20%28RAM%29%20and%20Deep%20Recurrent%20Attention%20Model%20%28DRAM%29%20failed%20to%20model%20the%0Ahierarchy%20of%20human%20vision%20system%2C%20that%20compromise%20on%20the%20visual%20exploration%0Adynamics.%20As%20a%20result%2C%20they%20tend%20to%20produce%20attention%20that%20are%20either%20overly%0Afixational%20or%20excessively%20saccadic%2C%20diverging%20from%20human%20eye%20movement%20behavior.%0AIn%20this%20paper%2C%20we%20propose%20a%20Multi-Level%20Recurrent%20Attention%20Model%20%28MRAM%29%2C%20a%0Anovel%20hard%20attention%20framework%20that%20explicitly%20models%20the%20neural%20hierarchy%20of%0Ahuman%20visual%20processing.%20By%20decoupling%20the%20function%20of%20glimpse%20location%0Ageneration%20and%20task%20execution%20in%20two%20recurrent%20layers%2C%20MRAM%20emergent%20a%20balanced%0Abehavior%20between%20fixation%20and%20saccadic%20movement.%20Our%20results%20show%20that%20MRAM%20not%0Aonly%20achieves%20more%20human-like%20attention%20dynamics%2C%20but%20also%20consistently%0Aoutperforms%20CNN%2C%20RAM%20and%20DRAM%20baselines%20on%20standard%20image%20classification%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13191v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520Fixational%2520and%2520Saccadic%2520Movements%2520in%2520a%2520Multi-Level%250A%2520%2520Recurrent%2520Attention%2520Model%2520for%2520Vision%26entry.906535625%3DPengcheng%2520Pan%2520and%2520Yonekura%2520Shogo%2520and%2520Yasuo%2520Kuniyoshi%26entry.1292438233%3D%2520%2520Inspired%2520by%2520foveal%2520vision%252C%2520hard%2520attention%2520models%2520promise%2520interpretability%2520and%250Aparameter%2520economy.%2520However%252C%2520existing%2520models%2520like%2520the%2520Recurrent%2520Model%2520of%2520Visual%250AAttention%2520%2528RAM%2529%2520and%2520Deep%2520Recurrent%2520Attention%2520Model%2520%2528DRAM%2529%2520failed%2520to%2520model%2520the%250Ahierarchy%2520of%2520human%2520vision%2520system%252C%2520that%2520compromise%2520on%2520the%2520visual%2520exploration%250Adynamics.%2520As%2520a%2520result%252C%2520they%2520tend%2520to%2520produce%2520attention%2520that%2520are%2520either%2520overly%250Afixational%2520or%2520excessively%2520saccadic%252C%2520diverging%2520from%2520human%2520eye%2520movement%2520behavior.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520Multi-Level%2520Recurrent%2520Attention%2520Model%2520%2528MRAM%2529%252C%2520a%250Anovel%2520hard%2520attention%2520framework%2520that%2520explicitly%2520models%2520the%2520neural%2520hierarchy%2520of%250Ahuman%2520visual%2520processing.%2520By%2520decoupling%2520the%2520function%2520of%2520glimpse%2520location%250Ageneration%2520and%2520task%2520execution%2520in%2520two%2520recurrent%2520layers%252C%2520MRAM%2520emergent%2520a%2520balanced%250Abehavior%2520between%2520fixation%2520and%2520saccadic%2520movement.%2520Our%2520results%2520show%2520that%2520MRAM%2520not%250Aonly%2520achieves%2520more%2520human-like%2520attention%2520dynamics%252C%2520but%2520also%2520consistently%250Aoutperforms%2520CNN%252C%2520RAM%2520and%2520DRAM%2520baselines%2520on%2520standard%2520image%2520classification%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13191v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Fixational%20and%20Saccadic%20Movements%20in%20a%20Multi-Level%0A%20%20Recurrent%20Attention%20Model%20for%20Vision&entry.906535625=Pengcheng%20Pan%20and%20Yonekura%20Shogo%20and%20Yasuo%20Kuniyoshi&entry.1292438233=%20%20Inspired%20by%20foveal%20vision%2C%20hard%20attention%20models%20promise%20interpretability%20and%0Aparameter%20economy.%20However%2C%20existing%20models%20like%20the%20Recurrent%20Model%20of%20Visual%0AAttention%20%28RAM%29%20and%20Deep%20Recurrent%20Attention%20Model%20%28DRAM%29%20failed%20to%20model%20the%0Ahierarchy%20of%20human%20vision%20system%2C%20that%20compromise%20on%20the%20visual%20exploration%0Adynamics.%20As%20a%20result%2C%20they%20tend%20to%20produce%20attention%20that%20are%20either%20overly%0Afixational%20or%20excessively%20saccadic%2C%20diverging%20from%20human%20eye%20movement%20behavior.%0AIn%20this%20paper%2C%20we%20propose%20a%20Multi-Level%20Recurrent%20Attention%20Model%20%28MRAM%29%2C%20a%0Anovel%20hard%20attention%20framework%20that%20explicitly%20models%20the%20neural%20hierarchy%20of%0Ahuman%20visual%20processing.%20By%20decoupling%20the%20function%20of%20glimpse%20location%0Ageneration%20and%20task%20execution%20in%20two%20recurrent%20layers%2C%20MRAM%20emergent%20a%20balanced%0Abehavior%20between%20fixation%20and%20saccadic%20movement.%20Our%20results%20show%20that%20MRAM%20not%0Aonly%20achieves%20more%20human-like%20attention%20dynamics%2C%20but%20also%20consistently%0Aoutperforms%20CNN%2C%20RAM%20and%20DRAM%20baselines%20on%20standard%20image%20classification%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13191v1&entry.124074799=Read"},
{"title": "Industry-focused Synthetic Segmentation Pre-training", "author": "Shinichi Mae and Ryosuke Yamada and Hirokatsu Kataoka", "abstract": "  Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications.\n", "link": "http://arxiv.org/abs/2505.13099v1", "date": "2025-05-19", "relevancy": 2.6906, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5551}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Industry-focused%20Synthetic%20Segmentation%20Pre-training&body=Title%3A%20Industry-focused%20Synthetic%20Segmentation%20Pre-training%0AAuthor%3A%20Shinichi%20Mae%20and%20Ryosuke%20Yamada%20and%20Hirokatsu%20Kataoka%0AAbstract%3A%20%20%20Pre-training%20on%20real-image%20datasets%20has%20been%20widely%20proven%20effective%20for%0Aimproving%20instance%20segmentation.%20However%2C%20industrial%20applications%20face%20two%20key%0Achallenges%3A%20%281%29%20legal%20and%20ethical%20restrictions%2C%20such%20as%20ImageNet%27s%20prohibition%0Aof%20commercial%20use%2C%20and%20%282%29%20limited%20transferability%20due%20to%20the%20domain%20gap%0Abetween%20web%20images%20and%20industrial%20imagery.%20Even%20recent%20vision%20foundation%0Amodels%2C%20including%20the%20segment%20anything%20model%20%28SAM%29%2C%20show%20notable%20performance%0Adegradation%20in%20industrial%20settings.%20These%20challenges%20raise%20critical%20questions%3A%0ACan%20we%20build%20a%20vision%20foundation%20model%20for%20industrial%20applications%20without%0Arelying%20on%20real%20images%20or%20manual%20annotations%3F%20And%20can%20such%20models%20outperform%0Aeven%20fine-tuned%20SAM%20on%20industrial%20datasets%3F%20To%20address%20these%20questions%2C%20we%0Apropose%20the%20Instance%20Core%20Segmentation%20Dataset%20%28InsCore%29%2C%20a%20synthetic%0Apre-training%20dataset%20based%20on%20formula-driven%20supervised%20learning%20%28FDSL%29.%0AInsCore%20generates%20fully%20annotated%20instance%20segmentation%20images%20that%20reflect%20key%0Acharacteristics%20of%20industrial%20data%2C%20including%20complex%20occlusions%2C%20dense%0Ahierarchical%20masks%2C%20and%20diverse%20non-rigid%20shapes%2C%20distinct%20from%20typical%20web%0Aimagery.%20Unlike%20previous%20methods%2C%20InsCore%20requires%20neither%20real%20images%20nor%0Ahuman%20annotations.%20Experiments%20on%20five%20industrial%20datasets%20show%20that%20models%0Apre-trained%20with%20InsCore%20outperform%20those%20trained%20on%20COCO%20and%20ImageNet-21k%2C%20as%0Awell%20as%20fine-tuned%20SAM%2C%20achieving%20an%20average%20improvement%20of%206.2%20points%20in%0Ainstance%20segmentation%20performance.%20This%20result%20is%20achieved%20using%20only%20100k%0Asynthetic%20images%2C%20more%20than%20100%20times%20fewer%20than%20the%2011%20million%20images%20in%20SAM%27s%0ASA-1B%20dataset%2C%20demonstrating%20the%20data%20efficiency%20of%20our%20approach.%20These%0Afindings%20position%20InsCore%20as%20a%20practical%20and%20license-free%20vision%20foundation%0Amodel%20for%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndustry-focused%2520Synthetic%2520Segmentation%2520Pre-training%26entry.906535625%3DShinichi%2520Mae%2520and%2520Ryosuke%2520Yamada%2520and%2520Hirokatsu%2520Kataoka%26entry.1292438233%3D%2520%2520Pre-training%2520on%2520real-image%2520datasets%2520has%2520been%2520widely%2520proven%2520effective%2520for%250Aimproving%2520instance%2520segmentation.%2520However%252C%2520industrial%2520applications%2520face%2520two%2520key%250Achallenges%253A%2520%25281%2529%2520legal%2520and%2520ethical%2520restrictions%252C%2520such%2520as%2520ImageNet%2527s%2520prohibition%250Aof%2520commercial%2520use%252C%2520and%2520%25282%2529%2520limited%2520transferability%2520due%2520to%2520the%2520domain%2520gap%250Abetween%2520web%2520images%2520and%2520industrial%2520imagery.%2520Even%2520recent%2520vision%2520foundation%250Amodels%252C%2520including%2520the%2520segment%2520anything%2520model%2520%2528SAM%2529%252C%2520show%2520notable%2520performance%250Adegradation%2520in%2520industrial%2520settings.%2520These%2520challenges%2520raise%2520critical%2520questions%253A%250ACan%2520we%2520build%2520a%2520vision%2520foundation%2520model%2520for%2520industrial%2520applications%2520without%250Arelying%2520on%2520real%2520images%2520or%2520manual%2520annotations%253F%2520And%2520can%2520such%2520models%2520outperform%250Aeven%2520fine-tuned%2520SAM%2520on%2520industrial%2520datasets%253F%2520To%2520address%2520these%2520questions%252C%2520we%250Apropose%2520the%2520Instance%2520Core%2520Segmentation%2520Dataset%2520%2528InsCore%2529%252C%2520a%2520synthetic%250Apre-training%2520dataset%2520based%2520on%2520formula-driven%2520supervised%2520learning%2520%2528FDSL%2529.%250AInsCore%2520generates%2520fully%2520annotated%2520instance%2520segmentation%2520images%2520that%2520reflect%2520key%250Acharacteristics%2520of%2520industrial%2520data%252C%2520including%2520complex%2520occlusions%252C%2520dense%250Ahierarchical%2520masks%252C%2520and%2520diverse%2520non-rigid%2520shapes%252C%2520distinct%2520from%2520typical%2520web%250Aimagery.%2520Unlike%2520previous%2520methods%252C%2520InsCore%2520requires%2520neither%2520real%2520images%2520nor%250Ahuman%2520annotations.%2520Experiments%2520on%2520five%2520industrial%2520datasets%2520show%2520that%2520models%250Apre-trained%2520with%2520InsCore%2520outperform%2520those%2520trained%2520on%2520COCO%2520and%2520ImageNet-21k%252C%2520as%250Awell%2520as%2520fine-tuned%2520SAM%252C%2520achieving%2520an%2520average%2520improvement%2520of%25206.2%2520points%2520in%250Ainstance%2520segmentation%2520performance.%2520This%2520result%2520is%2520achieved%2520using%2520only%2520100k%250Asynthetic%2520images%252C%2520more%2520than%2520100%2520times%2520fewer%2520than%2520the%252011%2520million%2520images%2520in%2520SAM%2527s%250ASA-1B%2520dataset%252C%2520demonstrating%2520the%2520data%2520efficiency%2520of%2520our%2520approach.%2520These%250Afindings%2520position%2520InsCore%2520as%2520a%2520practical%2520and%2520license-free%2520vision%2520foundation%250Amodel%2520for%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Industry-focused%20Synthetic%20Segmentation%20Pre-training&entry.906535625=Shinichi%20Mae%20and%20Ryosuke%20Yamada%20and%20Hirokatsu%20Kataoka&entry.1292438233=%20%20Pre-training%20on%20real-image%20datasets%20has%20been%20widely%20proven%20effective%20for%0Aimproving%20instance%20segmentation.%20However%2C%20industrial%20applications%20face%20two%20key%0Achallenges%3A%20%281%29%20legal%20and%20ethical%20restrictions%2C%20such%20as%20ImageNet%27s%20prohibition%0Aof%20commercial%20use%2C%20and%20%282%29%20limited%20transferability%20due%20to%20the%20domain%20gap%0Abetween%20web%20images%20and%20industrial%20imagery.%20Even%20recent%20vision%20foundation%0Amodels%2C%20including%20the%20segment%20anything%20model%20%28SAM%29%2C%20show%20notable%20performance%0Adegradation%20in%20industrial%20settings.%20These%20challenges%20raise%20critical%20questions%3A%0ACan%20we%20build%20a%20vision%20foundation%20model%20for%20industrial%20applications%20without%0Arelying%20on%20real%20images%20or%20manual%20annotations%3F%20And%20can%20such%20models%20outperform%0Aeven%20fine-tuned%20SAM%20on%20industrial%20datasets%3F%20To%20address%20these%20questions%2C%20we%0Apropose%20the%20Instance%20Core%20Segmentation%20Dataset%20%28InsCore%29%2C%20a%20synthetic%0Apre-training%20dataset%20based%20on%20formula-driven%20supervised%20learning%20%28FDSL%29.%0AInsCore%20generates%20fully%20annotated%20instance%20segmentation%20images%20that%20reflect%20key%0Acharacteristics%20of%20industrial%20data%2C%20including%20complex%20occlusions%2C%20dense%0Ahierarchical%20masks%2C%20and%20diverse%20non-rigid%20shapes%2C%20distinct%20from%20typical%20web%0Aimagery.%20Unlike%20previous%20methods%2C%20InsCore%20requires%20neither%20real%20images%20nor%0Ahuman%20annotations.%20Experiments%20on%20five%20industrial%20datasets%20show%20that%20models%0Apre-trained%20with%20InsCore%20outperform%20those%20trained%20on%20COCO%20and%20ImageNet-21k%2C%20as%0Awell%20as%20fine-tuned%20SAM%2C%20achieving%20an%20average%20improvement%20of%206.2%20points%20in%0Ainstance%20segmentation%20performance.%20This%20result%20is%20achieved%20using%20only%20100k%0Asynthetic%20images%2C%20more%20than%20100%20times%20fewer%20than%20the%2011%20million%20images%20in%20SAM%27s%0ASA-1B%20dataset%2C%20demonstrating%20the%20data%20efficiency%20of%20our%20approach.%20These%0Afindings%20position%20InsCore%20as%20a%20practical%20and%20license-free%20vision%20foundation%0Amodel%20for%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13099v1&entry.124074799=Read"},
{"title": "Understanding Complexity in VideoQA via Visual Program Generation", "author": "Cristobal Eyzaguirre and Igor Vasiljevic and Achal Dave and Jiajun Wu and Rares Andrei Ambrus and Thomas Kollar and Juan Carlos Niebles and Pavel Tokmakov", "abstract": "  We propose a data-driven approach to analyzing query complexity in Video\nQuestion Answering (VideoQA). Previous efforts in benchmark design have relied\non human expertise to design challenging questions, yet we experimentally show\nthat humans struggle to predict which questions are difficult for machine\nlearning models. Our automatic approach leverages recent advances in code\ngeneration for visual question answering, using the complexity of generated\ncode as a proxy for question difficulty. We demonstrate that this measure\ncorrelates significantly better with model performance than human estimates. To\noperationalize this insight, we propose an algorithm for estimating question\ncomplexity from code. It identifies fine-grained primitives that correlate with\nthe hardest questions for any given set of models, making it easy to scale to\nnew approaches in the future. Finally, to further illustrate the utility of our\nmethod, we extend it to automatically generate complex questions, constructing\na new benchmark that is 1.9 times harder than the popular NExT-QA.\n", "link": "http://arxiv.org/abs/2505.13429v1", "date": "2025-05-19", "relevancy": 2.6901, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5602}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Complexity%20in%20VideoQA%20via%20Visual%20Program%20Generation&body=Title%3A%20Understanding%20Complexity%20in%20VideoQA%20via%20Visual%20Program%20Generation%0AAuthor%3A%20Cristobal%20Eyzaguirre%20and%20Igor%20Vasiljevic%20and%20Achal%20Dave%20and%20Jiajun%20Wu%20and%20Rares%20Andrei%20Ambrus%20and%20Thomas%20Kollar%20and%20Juan%20Carlos%20Niebles%20and%20Pavel%20Tokmakov%0AAbstract%3A%20%20%20We%20propose%20a%20data-driven%20approach%20to%20analyzing%20query%20complexity%20in%20Video%0AQuestion%20Answering%20%28VideoQA%29.%20Previous%20efforts%20in%20benchmark%20design%20have%20relied%0Aon%20human%20expertise%20to%20design%20challenging%20questions%2C%20yet%20we%20experimentally%20show%0Athat%20humans%20struggle%20to%20predict%20which%20questions%20are%20difficult%20for%20machine%0Alearning%20models.%20Our%20automatic%20approach%20leverages%20recent%20advances%20in%20code%0Ageneration%20for%20visual%20question%20answering%2C%20using%20the%20complexity%20of%20generated%0Acode%20as%20a%20proxy%20for%20question%20difficulty.%20We%20demonstrate%20that%20this%20measure%0Acorrelates%20significantly%20better%20with%20model%20performance%20than%20human%20estimates.%20To%0Aoperationalize%20this%20insight%2C%20we%20propose%20an%20algorithm%20for%20estimating%20question%0Acomplexity%20from%20code.%20It%20identifies%20fine-grained%20primitives%20that%20correlate%20with%0Athe%20hardest%20questions%20for%20any%20given%20set%20of%20models%2C%20making%20it%20easy%20to%20scale%20to%0Anew%20approaches%20in%20the%20future.%20Finally%2C%20to%20further%20illustrate%20the%20utility%20of%20our%0Amethod%2C%20we%20extend%20it%20to%20automatically%20generate%20complex%20questions%2C%20constructing%0Aa%20new%20benchmark%20that%20is%201.9%20times%20harder%20than%20the%20popular%20NExT-QA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Complexity%2520in%2520VideoQA%2520via%2520Visual%2520Program%2520Generation%26entry.906535625%3DCristobal%2520Eyzaguirre%2520and%2520Igor%2520Vasiljevic%2520and%2520Achal%2520Dave%2520and%2520Jiajun%2520Wu%2520and%2520Rares%2520Andrei%2520Ambrus%2520and%2520Thomas%2520Kollar%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520Pavel%2520Tokmakov%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520data-driven%2520approach%2520to%2520analyzing%2520query%2520complexity%2520in%2520Video%250AQuestion%2520Answering%2520%2528VideoQA%2529.%2520Previous%2520efforts%2520in%2520benchmark%2520design%2520have%2520relied%250Aon%2520human%2520expertise%2520to%2520design%2520challenging%2520questions%252C%2520yet%2520we%2520experimentally%2520show%250Athat%2520humans%2520struggle%2520to%2520predict%2520which%2520questions%2520are%2520difficult%2520for%2520machine%250Alearning%2520models.%2520Our%2520automatic%2520approach%2520leverages%2520recent%2520advances%2520in%2520code%250Ageneration%2520for%2520visual%2520question%2520answering%252C%2520using%2520the%2520complexity%2520of%2520generated%250Acode%2520as%2520a%2520proxy%2520for%2520question%2520difficulty.%2520We%2520demonstrate%2520that%2520this%2520measure%250Acorrelates%2520significantly%2520better%2520with%2520model%2520performance%2520than%2520human%2520estimates.%2520To%250Aoperationalize%2520this%2520insight%252C%2520we%2520propose%2520an%2520algorithm%2520for%2520estimating%2520question%250Acomplexity%2520from%2520code.%2520It%2520identifies%2520fine-grained%2520primitives%2520that%2520correlate%2520with%250Athe%2520hardest%2520questions%2520for%2520any%2520given%2520set%2520of%2520models%252C%2520making%2520it%2520easy%2520to%2520scale%2520to%250Anew%2520approaches%2520in%2520the%2520future.%2520Finally%252C%2520to%2520further%2520illustrate%2520the%2520utility%2520of%2520our%250Amethod%252C%2520we%2520extend%2520it%2520to%2520automatically%2520generate%2520complex%2520questions%252C%2520constructing%250Aa%2520new%2520benchmark%2520that%2520is%25201.9%2520times%2520harder%2520than%2520the%2520popular%2520NExT-QA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Complexity%20in%20VideoQA%20via%20Visual%20Program%20Generation&entry.906535625=Cristobal%20Eyzaguirre%20and%20Igor%20Vasiljevic%20and%20Achal%20Dave%20and%20Jiajun%20Wu%20and%20Rares%20Andrei%20Ambrus%20and%20Thomas%20Kollar%20and%20Juan%20Carlos%20Niebles%20and%20Pavel%20Tokmakov&entry.1292438233=%20%20We%20propose%20a%20data-driven%20approach%20to%20analyzing%20query%20complexity%20in%20Video%0AQuestion%20Answering%20%28VideoQA%29.%20Previous%20efforts%20in%20benchmark%20design%20have%20relied%0Aon%20human%20expertise%20to%20design%20challenging%20questions%2C%20yet%20we%20experimentally%20show%0Athat%20humans%20struggle%20to%20predict%20which%20questions%20are%20difficult%20for%20machine%0Alearning%20models.%20Our%20automatic%20approach%20leverages%20recent%20advances%20in%20code%0Ageneration%20for%20visual%20question%20answering%2C%20using%20the%20complexity%20of%20generated%0Acode%20as%20a%20proxy%20for%20question%20difficulty.%20We%20demonstrate%20that%20this%20measure%0Acorrelates%20significantly%20better%20with%20model%20performance%20than%20human%20estimates.%20To%0Aoperationalize%20this%20insight%2C%20we%20propose%20an%20algorithm%20for%20estimating%20question%0Acomplexity%20from%20code.%20It%20identifies%20fine-grained%20primitives%20that%20correlate%20with%0Athe%20hardest%20questions%20for%20any%20given%20set%20of%20models%2C%20making%20it%20easy%20to%20scale%20to%0Anew%20approaches%20in%20the%20future.%20Finally%2C%20to%20further%20illustrate%20the%20utility%20of%20our%0Amethod%2C%20we%20extend%20it%20to%20automatically%20generate%20complex%20questions%2C%20constructing%0Aa%20new%20benchmark%20that%20is%201.9%20times%20harder%20than%20the%20popular%20NExT-QA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13429v1&entry.124074799=Read"},
{"title": "Unveiling and Steering Connectome Organization with Interpretable Latent\n  Variables", "author": "Yubin Li and Xingyu Liu and Guozhang Chen", "abstract": "  The brain's intricate connectome, a blueprint for its function, presents\nimmense complexity, yet it arises from a compact genetic code, hinting at\nunderlying low-dimensional organizational principles. This work bridges\nconnectomics and representation learning to uncover these principles. We\npropose a framework that combines subgraph extraction from the Drosophila\nconnectome, FlyWire, with a generative model to derive interpretable\nlow-dimensional representations of neural circuitry. Crucially, an\nexplainability module links these latent dimensions to specific structural\nfeatures, offering insights into their functional relevance. We validate our\napproach by demonstrating effective graph reconstruction and, significantly,\nthe ability to manipulate these latent codes to controllably generate\nconnectome subgraphs with predefined properties. This research offers a novel\ntool for understanding brain architecture and a potential avenue for designing\nbio-inspired artificial neural networks.\n", "link": "http://arxiv.org/abs/2505.13011v1", "date": "2025-05-19", "relevancy": 2.6658, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20and%20Steering%20Connectome%20Organization%20with%20Interpretable%20Latent%0A%20%20Variables&body=Title%3A%20Unveiling%20and%20Steering%20Connectome%20Organization%20with%20Interpretable%20Latent%0A%20%20Variables%0AAuthor%3A%20Yubin%20Li%20and%20Xingyu%20Liu%20and%20Guozhang%20Chen%0AAbstract%3A%20%20%20The%20brain%27s%20intricate%20connectome%2C%20a%20blueprint%20for%20its%20function%2C%20presents%0Aimmense%20complexity%2C%20yet%20it%20arises%20from%20a%20compact%20genetic%20code%2C%20hinting%20at%0Aunderlying%20low-dimensional%20organizational%20principles.%20This%20work%20bridges%0Aconnectomics%20and%20representation%20learning%20to%20uncover%20these%20principles.%20We%0Apropose%20a%20framework%20that%20combines%20subgraph%20extraction%20from%20the%20Drosophila%0Aconnectome%2C%20FlyWire%2C%20with%20a%20generative%20model%20to%20derive%20interpretable%0Alow-dimensional%20representations%20of%20neural%20circuitry.%20Crucially%2C%20an%0Aexplainability%20module%20links%20these%20latent%20dimensions%20to%20specific%20structural%0Afeatures%2C%20offering%20insights%20into%20their%20functional%20relevance.%20We%20validate%20our%0Aapproach%20by%20demonstrating%20effective%20graph%20reconstruction%20and%2C%20significantly%2C%0Athe%20ability%20to%20manipulate%20these%20latent%20codes%20to%20controllably%20generate%0Aconnectome%20subgraphs%20with%20predefined%20properties.%20This%20research%20offers%20a%20novel%0Atool%20for%20understanding%20brain%20architecture%20and%20a%20potential%20avenue%20for%20designing%0Abio-inspired%20artificial%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520and%2520Steering%2520Connectome%2520Organization%2520with%2520Interpretable%2520Latent%250A%2520%2520Variables%26entry.906535625%3DYubin%2520Li%2520and%2520Xingyu%2520Liu%2520and%2520Guozhang%2520Chen%26entry.1292438233%3D%2520%2520The%2520brain%2527s%2520intricate%2520connectome%252C%2520a%2520blueprint%2520for%2520its%2520function%252C%2520presents%250Aimmense%2520complexity%252C%2520yet%2520it%2520arises%2520from%2520a%2520compact%2520genetic%2520code%252C%2520hinting%2520at%250Aunderlying%2520low-dimensional%2520organizational%2520principles.%2520This%2520work%2520bridges%250Aconnectomics%2520and%2520representation%2520learning%2520to%2520uncover%2520these%2520principles.%2520We%250Apropose%2520a%2520framework%2520that%2520combines%2520subgraph%2520extraction%2520from%2520the%2520Drosophila%250Aconnectome%252C%2520FlyWire%252C%2520with%2520a%2520generative%2520model%2520to%2520derive%2520interpretable%250Alow-dimensional%2520representations%2520of%2520neural%2520circuitry.%2520Crucially%252C%2520an%250Aexplainability%2520module%2520links%2520these%2520latent%2520dimensions%2520to%2520specific%2520structural%250Afeatures%252C%2520offering%2520insights%2520into%2520their%2520functional%2520relevance.%2520We%2520validate%2520our%250Aapproach%2520by%2520demonstrating%2520effective%2520graph%2520reconstruction%2520and%252C%2520significantly%252C%250Athe%2520ability%2520to%2520manipulate%2520these%2520latent%2520codes%2520to%2520controllably%2520generate%250Aconnectome%2520subgraphs%2520with%2520predefined%2520properties.%2520This%2520research%2520offers%2520a%2520novel%250Atool%2520for%2520understanding%2520brain%2520architecture%2520and%2520a%2520potential%2520avenue%2520for%2520designing%250Abio-inspired%2520artificial%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20and%20Steering%20Connectome%20Organization%20with%20Interpretable%20Latent%0A%20%20Variables&entry.906535625=Yubin%20Li%20and%20Xingyu%20Liu%20and%20Guozhang%20Chen&entry.1292438233=%20%20The%20brain%27s%20intricate%20connectome%2C%20a%20blueprint%20for%20its%20function%2C%20presents%0Aimmense%20complexity%2C%20yet%20it%20arises%20from%20a%20compact%20genetic%20code%2C%20hinting%20at%0Aunderlying%20low-dimensional%20organizational%20principles.%20This%20work%20bridges%0Aconnectomics%20and%20representation%20learning%20to%20uncover%20these%20principles.%20We%0Apropose%20a%20framework%20that%20combines%20subgraph%20extraction%20from%20the%20Drosophila%0Aconnectome%2C%20FlyWire%2C%20with%20a%20generative%20model%20to%20derive%20interpretable%0Alow-dimensional%20representations%20of%20neural%20circuitry.%20Crucially%2C%20an%0Aexplainability%20module%20links%20these%20latent%20dimensions%20to%20specific%20structural%0Afeatures%2C%20offering%20insights%20into%20their%20functional%20relevance.%20We%20validate%20our%0Aapproach%20by%20demonstrating%20effective%20graph%20reconstruction%20and%2C%20significantly%2C%0Athe%20ability%20to%20manipulate%20these%20latent%20codes%20to%20controllably%20generate%0Aconnectome%20subgraphs%20with%20predefined%20properties.%20This%20research%20offers%20a%20novel%0Atool%20for%20understanding%20brain%20architecture%20and%20a%20potential%20avenue%20for%20designing%0Abio-inspired%20artificial%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13011v1&entry.124074799=Read"},
{"title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM\n  Perceptions for Early Awareness", "author": "Lotem Peled-Cohen and Maya Zadok and Nitay Calderon and Hila Gonen and Roi Reichart", "abstract": "  Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.\n", "link": "http://arxiv.org/abs/2505.13418v1", "date": "2025-05-19", "relevancy": 2.6652, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dementia%20Through%20Different%20Eyes%3A%20Explainable%20Modeling%20of%20Human%20and%20LLM%0A%20%20Perceptions%20for%20Early%20Awareness&body=Title%3A%20Dementia%20Through%20Different%20Eyes%3A%20Explainable%20Modeling%20of%20Human%20and%20LLM%0A%20%20Perceptions%20for%20Early%20Awareness%0AAuthor%3A%20Lotem%20Peled-Cohen%20and%20Maya%20Zadok%20and%20Nitay%20Calderon%20and%20Hila%20Gonen%20and%20Roi%20Reichart%0AAbstract%3A%20%20%20Cognitive%20decline%20often%20surfaces%20in%20language%20years%20before%20diagnosis.%20It%20is%0Afrequently%20non-experts%2C%20such%20as%20those%20closest%20to%20the%20patient%2C%20who%20first%20sense%20a%0Achange%20and%20raise%20concern.%20As%20LLMs%20become%20integrated%20into%20daily%20communication%0Aand%20used%20over%20prolonged%20periods%2C%20it%20may%20even%20be%20an%20LLM%20that%20notices%20something%0Ais%20off.%20But%20what%20exactly%20do%20they%20notice--and%20should%20be%20noticing--when%20making%0Athat%20judgment%3F%20This%20paper%20investigates%20how%20dementia%20is%20perceived%20through%0Alanguage%20by%20non-experts.%20We%20presented%20transcribed%20picture%20descriptions%20to%0Anon-expert%20humans%20and%20LLMs%2C%20asking%20them%20to%20intuitively%20judge%20whether%20each%20text%0Awas%20produced%20by%20someone%20healthy%20or%20with%20dementia.%20We%20introduce%20an%20explainable%0Amethod%20that%20uses%20LLMs%20to%20extract%20high-level%2C%20expert-guided%20features%0Arepresenting%20these%20picture%20descriptions%2C%20and%20use%20logistic%20regression%20to%20model%0Ahuman%20and%20LLM%20perceptions%20and%20compare%20with%20clinical%20diagnoses.%20Our%20analysis%0Areveals%20that%20human%20perception%20of%20dementia%20is%20inconsistent%20and%20relies%20on%20a%0Anarrow%2C%20and%20sometimes%20misleading%2C%20set%20of%20cues.%20LLMs%2C%20by%20contrast%2C%20draw%20on%20a%0Aricher%2C%20more%20nuanced%20feature%20set%20that%20aligns%20more%20closely%20with%20clinical%0Apatterns.%20Still%2C%20both%20groups%20show%20a%20tendency%20toward%20false%20negatives%2C%20frequently%0Aoverlooking%20dementia%20cases.%20Through%20our%20interpretable%20framework%20and%20the%0Ainsights%20it%20provides%2C%20we%20hope%20to%20help%20non-experts%20better%20recognize%20the%0Alinguistic%20signs%20that%20matter.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDementia%2520Through%2520Different%2520Eyes%253A%2520Explainable%2520Modeling%2520of%2520Human%2520and%2520LLM%250A%2520%2520Perceptions%2520for%2520Early%2520Awareness%26entry.906535625%3DLotem%2520Peled-Cohen%2520and%2520Maya%2520Zadok%2520and%2520Nitay%2520Calderon%2520and%2520Hila%2520Gonen%2520and%2520Roi%2520Reichart%26entry.1292438233%3D%2520%2520Cognitive%2520decline%2520often%2520surfaces%2520in%2520language%2520years%2520before%2520diagnosis.%2520It%2520is%250Afrequently%2520non-experts%252C%2520such%2520as%2520those%2520closest%2520to%2520the%2520patient%252C%2520who%2520first%2520sense%2520a%250Achange%2520and%2520raise%2520concern.%2520As%2520LLMs%2520become%2520integrated%2520into%2520daily%2520communication%250Aand%2520used%2520over%2520prolonged%2520periods%252C%2520it%2520may%2520even%2520be%2520an%2520LLM%2520that%2520notices%2520something%250Ais%2520off.%2520But%2520what%2520exactly%2520do%2520they%2520notice--and%2520should%2520be%2520noticing--when%2520making%250Athat%2520judgment%253F%2520This%2520paper%2520investigates%2520how%2520dementia%2520is%2520perceived%2520through%250Alanguage%2520by%2520non-experts.%2520We%2520presented%2520transcribed%2520picture%2520descriptions%2520to%250Anon-expert%2520humans%2520and%2520LLMs%252C%2520asking%2520them%2520to%2520intuitively%2520judge%2520whether%2520each%2520text%250Awas%2520produced%2520by%2520someone%2520healthy%2520or%2520with%2520dementia.%2520We%2520introduce%2520an%2520explainable%250Amethod%2520that%2520uses%2520LLMs%2520to%2520extract%2520high-level%252C%2520expert-guided%2520features%250Arepresenting%2520these%2520picture%2520descriptions%252C%2520and%2520use%2520logistic%2520regression%2520to%2520model%250Ahuman%2520and%2520LLM%2520perceptions%2520and%2520compare%2520with%2520clinical%2520diagnoses.%2520Our%2520analysis%250Areveals%2520that%2520human%2520perception%2520of%2520dementia%2520is%2520inconsistent%2520and%2520relies%2520on%2520a%250Anarrow%252C%2520and%2520sometimes%2520misleading%252C%2520set%2520of%2520cues.%2520LLMs%252C%2520by%2520contrast%252C%2520draw%2520on%2520a%250Aricher%252C%2520more%2520nuanced%2520feature%2520set%2520that%2520aligns%2520more%2520closely%2520with%2520clinical%250Apatterns.%2520Still%252C%2520both%2520groups%2520show%2520a%2520tendency%2520toward%2520false%2520negatives%252C%2520frequently%250Aoverlooking%2520dementia%2520cases.%2520Through%2520our%2520interpretable%2520framework%2520and%2520the%250Ainsights%2520it%2520provides%252C%2520we%2520hope%2520to%2520help%2520non-experts%2520better%2520recognize%2520the%250Alinguistic%2520signs%2520that%2520matter.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dementia%20Through%20Different%20Eyes%3A%20Explainable%20Modeling%20of%20Human%20and%20LLM%0A%20%20Perceptions%20for%20Early%20Awareness&entry.906535625=Lotem%20Peled-Cohen%20and%20Maya%20Zadok%20and%20Nitay%20Calderon%20and%20Hila%20Gonen%20and%20Roi%20Reichart&entry.1292438233=%20%20Cognitive%20decline%20often%20surfaces%20in%20language%20years%20before%20diagnosis.%20It%20is%0Afrequently%20non-experts%2C%20such%20as%20those%20closest%20to%20the%20patient%2C%20who%20first%20sense%20a%0Achange%20and%20raise%20concern.%20As%20LLMs%20become%20integrated%20into%20daily%20communication%0Aand%20used%20over%20prolonged%20periods%2C%20it%20may%20even%20be%20an%20LLM%20that%20notices%20something%0Ais%20off.%20But%20what%20exactly%20do%20they%20notice--and%20should%20be%20noticing--when%20making%0Athat%20judgment%3F%20This%20paper%20investigates%20how%20dementia%20is%20perceived%20through%0Alanguage%20by%20non-experts.%20We%20presented%20transcribed%20picture%20descriptions%20to%0Anon-expert%20humans%20and%20LLMs%2C%20asking%20them%20to%20intuitively%20judge%20whether%20each%20text%0Awas%20produced%20by%20someone%20healthy%20or%20with%20dementia.%20We%20introduce%20an%20explainable%0Amethod%20that%20uses%20LLMs%20to%20extract%20high-level%2C%20expert-guided%20features%0Arepresenting%20these%20picture%20descriptions%2C%20and%20use%20logistic%20regression%20to%20model%0Ahuman%20and%20LLM%20perceptions%20and%20compare%20with%20clinical%20diagnoses.%20Our%20analysis%0Areveals%20that%20human%20perception%20of%20dementia%20is%20inconsistent%20and%20relies%20on%20a%0Anarrow%2C%20and%20sometimes%20misleading%2C%20set%20of%20cues.%20LLMs%2C%20by%20contrast%2C%20draw%20on%20a%0Aricher%2C%20more%20nuanced%20feature%20set%20that%20aligns%20more%20closely%20with%20clinical%0Apatterns.%20Still%2C%20both%20groups%20show%20a%20tendency%20toward%20false%20negatives%2C%20frequently%0Aoverlooking%20dementia%20cases.%20Through%20our%20interpretable%20framework%20and%20the%0Ainsights%20it%20provides%2C%20we%20hope%20to%20help%20non-experts%20better%20recognize%20the%0Alinguistic%20signs%20that%20matter.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13418v1&entry.124074799=Read"},
{"title": "Function Encoders: A Principled Approach to Transfer Learning in Hilbert\n  Spaces", "author": "Tyler Ingebrand and Adam J. Thorpe and Ufuk Topcu", "abstract": "  A central challenge in transfer learning is designing algorithms that can\nquickly adapt and generalize to new tasks without retraining. Yet, the\nconditions of when and how algorithms can effectively transfer to new tasks is\npoorly characterized. We introduce a geometric characterization of transfer in\nHilbert spaces and define three types of inductive transfer: interpolation\nwithin the convex hull, extrapolation to the linear span, and extrapolation\noutside the span. We propose a method grounded in the theory of function\nencoders to achieve all three types of transfer. Specifically, we introduce a\nnovel training scheme for function encoders using least-squares optimization,\nprove a universal approximation theorem for function encoders, and provide a\ncomprehensive comparison with existing approaches such as transformers and\nmeta-learning on four diverse benchmarks. Our experiments demonstrate that the\nfunction encoder outperforms state-of-the-art methods on four benchmark tasks\nand on all three types of transfer.\n", "link": "http://arxiv.org/abs/2501.18373v2", "date": "2025-05-19", "relevancy": 2.6503, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5387}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Function%20Encoders%3A%20A%20Principled%20Approach%20to%20Transfer%20Learning%20in%20Hilbert%0A%20%20Spaces&body=Title%3A%20Function%20Encoders%3A%20A%20Principled%20Approach%20to%20Transfer%20Learning%20in%20Hilbert%0A%20%20Spaces%0AAuthor%3A%20Tyler%20Ingebrand%20and%20Adam%20J.%20Thorpe%20and%20Ufuk%20Topcu%0AAbstract%3A%20%20%20A%20central%20challenge%20in%20transfer%20learning%20is%20designing%20algorithms%20that%20can%0Aquickly%20adapt%20and%20generalize%20to%20new%20tasks%20without%20retraining.%20Yet%2C%20the%0Aconditions%20of%20when%20and%20how%20algorithms%20can%20effectively%20transfer%20to%20new%20tasks%20is%0Apoorly%20characterized.%20We%20introduce%20a%20geometric%20characterization%20of%20transfer%20in%0AHilbert%20spaces%20and%20define%20three%20types%20of%20inductive%20transfer%3A%20interpolation%0Awithin%20the%20convex%20hull%2C%20extrapolation%20to%20the%20linear%20span%2C%20and%20extrapolation%0Aoutside%20the%20span.%20We%20propose%20a%20method%20grounded%20in%20the%20theory%20of%20function%0Aencoders%20to%20achieve%20all%20three%20types%20of%20transfer.%20Specifically%2C%20we%20introduce%20a%0Anovel%20training%20scheme%20for%20function%20encoders%20using%20least-squares%20optimization%2C%0Aprove%20a%20universal%20approximation%20theorem%20for%20function%20encoders%2C%20and%20provide%20a%0Acomprehensive%20comparison%20with%20existing%20approaches%20such%20as%20transformers%20and%0Ameta-learning%20on%20four%20diverse%20benchmarks.%20Our%20experiments%20demonstrate%20that%20the%0Afunction%20encoder%20outperforms%20state-of-the-art%20methods%20on%20four%20benchmark%20tasks%0Aand%20on%20all%20three%20types%20of%20transfer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18373v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunction%2520Encoders%253A%2520A%2520Principled%2520Approach%2520to%2520Transfer%2520Learning%2520in%2520Hilbert%250A%2520%2520Spaces%26entry.906535625%3DTyler%2520Ingebrand%2520and%2520Adam%2520J.%2520Thorpe%2520and%2520Ufuk%2520Topcu%26entry.1292438233%3D%2520%2520A%2520central%2520challenge%2520in%2520transfer%2520learning%2520is%2520designing%2520algorithms%2520that%2520can%250Aquickly%2520adapt%2520and%2520generalize%2520to%2520new%2520tasks%2520without%2520retraining.%2520Yet%252C%2520the%250Aconditions%2520of%2520when%2520and%2520how%2520algorithms%2520can%2520effectively%2520transfer%2520to%2520new%2520tasks%2520is%250Apoorly%2520characterized.%2520We%2520introduce%2520a%2520geometric%2520characterization%2520of%2520transfer%2520in%250AHilbert%2520spaces%2520and%2520define%2520three%2520types%2520of%2520inductive%2520transfer%253A%2520interpolation%250Awithin%2520the%2520convex%2520hull%252C%2520extrapolation%2520to%2520the%2520linear%2520span%252C%2520and%2520extrapolation%250Aoutside%2520the%2520span.%2520We%2520propose%2520a%2520method%2520grounded%2520in%2520the%2520theory%2520of%2520function%250Aencoders%2520to%2520achieve%2520all%2520three%2520types%2520of%2520transfer.%2520Specifically%252C%2520we%2520introduce%2520a%250Anovel%2520training%2520scheme%2520for%2520function%2520encoders%2520using%2520least-squares%2520optimization%252C%250Aprove%2520a%2520universal%2520approximation%2520theorem%2520for%2520function%2520encoders%252C%2520and%2520provide%2520a%250Acomprehensive%2520comparison%2520with%2520existing%2520approaches%2520such%2520as%2520transformers%2520and%250Ameta-learning%2520on%2520four%2520diverse%2520benchmarks.%2520Our%2520experiments%2520demonstrate%2520that%2520the%250Afunction%2520encoder%2520outperforms%2520state-of-the-art%2520methods%2520on%2520four%2520benchmark%2520tasks%250Aand%2520on%2520all%2520three%2520types%2520of%2520transfer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18373v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Function%20Encoders%3A%20A%20Principled%20Approach%20to%20Transfer%20Learning%20in%20Hilbert%0A%20%20Spaces&entry.906535625=Tyler%20Ingebrand%20and%20Adam%20J.%20Thorpe%20and%20Ufuk%20Topcu&entry.1292438233=%20%20A%20central%20challenge%20in%20transfer%20learning%20is%20designing%20algorithms%20that%20can%0Aquickly%20adapt%20and%20generalize%20to%20new%20tasks%20without%20retraining.%20Yet%2C%20the%0Aconditions%20of%20when%20and%20how%20algorithms%20can%20effectively%20transfer%20to%20new%20tasks%20is%0Apoorly%20characterized.%20We%20introduce%20a%20geometric%20characterization%20of%20transfer%20in%0AHilbert%20spaces%20and%20define%20three%20types%20of%20inductive%20transfer%3A%20interpolation%0Awithin%20the%20convex%20hull%2C%20extrapolation%20to%20the%20linear%20span%2C%20and%20extrapolation%0Aoutside%20the%20span.%20We%20propose%20a%20method%20grounded%20in%20the%20theory%20of%20function%0Aencoders%20to%20achieve%20all%20three%20types%20of%20transfer.%20Specifically%2C%20we%20introduce%20a%0Anovel%20training%20scheme%20for%20function%20encoders%20using%20least-squares%20optimization%2C%0Aprove%20a%20universal%20approximation%20theorem%20for%20function%20encoders%2C%20and%20provide%20a%0Acomprehensive%20comparison%20with%20existing%20approaches%20such%20as%20transformers%20and%0Ameta-learning%20on%20four%20diverse%20benchmarks.%20Our%20experiments%20demonstrate%20that%20the%0Afunction%20encoder%20outperforms%20state-of-the-art%20methods%20on%20four%20benchmark%20tasks%0Aand%20on%20all%20three%20types%20of%20transfer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18373v2&entry.124074799=Read"},
{"title": "GuidedMorph: Two-Stage Deformable Registration for Breast MRI", "author": "Yaqian Chen and Hanxue Gu and Haoyu Dong and Qihang Li and Yuwen Chen and Nicholas Konz and Lin Li and Maciej A. Mazurowski", "abstract": "  Accurately registering breast MR images from different time points enables\nthe alignment of anatomical structures and tracking of tumor progression,\nsupporting more effective breast cancer detection, diagnosis, and treatment\nplanning. However, the complexity of dense tissue and its highly non-rigid\nnature pose challenges for conventional registration methods, which primarily\nfocus on aligning general structures while overlooking intricate internal\ndetails. To address this, we propose \\textbf{GuidedMorph}, a novel two-stage\nregistration framework designed to better align dense tissue. In addition to a\nsingle-scale network for global structure alignment, we introduce a framework\nthat utilizes dense tissue information to track breast movement. The learned\ntransformation fields are fused by introducing the Dual Spatial Transformer\nNetwork (DSTN), improving overall alignment accuracy. A novel warping method\nbased on the Euclidean distance transform (EDT) is also proposed to accurately\nwarp the registered dense tissue and breast masks, preserving fine structural\ndetails during deformation. The framework supports paradigms that require\nexternal segmentation models and with image data only. It also operates\neffectively with the VoxelMorph and TransMorph backbones, offering a versatile\nsolution for breast registration. We validate our method on ISPY2 and internal\ndataset, demonstrating superior performance in dense tissue, overall breast\nalignment, and breast structural similarity index measure (SSIM), with notable\nimprovements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and\n1.21% in breast SSIM compared to the best learning-based baseline.\n", "link": "http://arxiv.org/abs/2505.13414v1", "date": "2025-05-19", "relevancy": 2.6186, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5363}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5207}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GuidedMorph%3A%20Two-Stage%20Deformable%20Registration%20for%20Breast%20MRI&body=Title%3A%20GuidedMorph%3A%20Two-Stage%20Deformable%20Registration%20for%20Breast%20MRI%0AAuthor%3A%20Yaqian%20Chen%20and%20Hanxue%20Gu%20and%20Haoyu%20Dong%20and%20Qihang%20Li%20and%20Yuwen%20Chen%20and%20Nicholas%20Konz%20and%20Lin%20Li%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Accurately%20registering%20breast%20MR%20images%20from%20different%20time%20points%20enables%0Athe%20alignment%20of%20anatomical%20structures%20and%20tracking%20of%20tumor%20progression%2C%0Asupporting%20more%20effective%20breast%20cancer%20detection%2C%20diagnosis%2C%20and%20treatment%0Aplanning.%20However%2C%20the%20complexity%20of%20dense%20tissue%20and%20its%20highly%20non-rigid%0Anature%20pose%20challenges%20for%20conventional%20registration%20methods%2C%20which%20primarily%0Afocus%20on%20aligning%20general%20structures%20while%20overlooking%20intricate%20internal%0Adetails.%20To%20address%20this%2C%20we%20propose%20%5Ctextbf%7BGuidedMorph%7D%2C%20a%20novel%20two-stage%0Aregistration%20framework%20designed%20to%20better%20align%20dense%20tissue.%20In%20addition%20to%20a%0Asingle-scale%20network%20for%20global%20structure%20alignment%2C%20we%20introduce%20a%20framework%0Athat%20utilizes%20dense%20tissue%20information%20to%20track%20breast%20movement.%20The%20learned%0Atransformation%20fields%20are%20fused%20by%20introducing%20the%20Dual%20Spatial%20Transformer%0ANetwork%20%28DSTN%29%2C%20improving%20overall%20alignment%20accuracy.%20A%20novel%20warping%20method%0Abased%20on%20the%20Euclidean%20distance%20transform%20%28EDT%29%20is%20also%20proposed%20to%20accurately%0Awarp%20the%20registered%20dense%20tissue%20and%20breast%20masks%2C%20preserving%20fine%20structural%0Adetails%20during%20deformation.%20The%20framework%20supports%20paradigms%20that%20require%0Aexternal%20segmentation%20models%20and%20with%20image%20data%20only.%20It%20also%20operates%0Aeffectively%20with%20the%20VoxelMorph%20and%20TransMorph%20backbones%2C%20offering%20a%20versatile%0Asolution%20for%20breast%20registration.%20We%20validate%20our%20method%20on%20ISPY2%20and%20internal%0Adataset%2C%20demonstrating%20superior%20performance%20in%20dense%20tissue%2C%20overall%20breast%0Aalignment%2C%20and%20breast%20structural%20similarity%20index%20measure%20%28SSIM%29%2C%20with%20notable%0Aimprovements%20by%20over%2013.01%25%20in%20dense%20tissue%20Dice%2C%203.13%25%20in%20breast%20Dice%2C%20and%0A1.21%25%20in%20breast%20SSIM%20compared%20to%20the%20best%20learning-based%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuidedMorph%253A%2520Two-Stage%2520Deformable%2520Registration%2520for%2520Breast%2520MRI%26entry.906535625%3DYaqian%2520Chen%2520and%2520Hanxue%2520Gu%2520and%2520Haoyu%2520Dong%2520and%2520Qihang%2520Li%2520and%2520Yuwen%2520Chen%2520and%2520Nicholas%2520Konz%2520and%2520Lin%2520Li%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520Accurately%2520registering%2520breast%2520MR%2520images%2520from%2520different%2520time%2520points%2520enables%250Athe%2520alignment%2520of%2520anatomical%2520structures%2520and%2520tracking%2520of%2520tumor%2520progression%252C%250Asupporting%2520more%2520effective%2520breast%2520cancer%2520detection%252C%2520diagnosis%252C%2520and%2520treatment%250Aplanning.%2520However%252C%2520the%2520complexity%2520of%2520dense%2520tissue%2520and%2520its%2520highly%2520non-rigid%250Anature%2520pose%2520challenges%2520for%2520conventional%2520registration%2520methods%252C%2520which%2520primarily%250Afocus%2520on%2520aligning%2520general%2520structures%2520while%2520overlooking%2520intricate%2520internal%250Adetails.%2520To%2520address%2520this%252C%2520we%2520propose%2520%255Ctextbf%257BGuidedMorph%257D%252C%2520a%2520novel%2520two-stage%250Aregistration%2520framework%2520designed%2520to%2520better%2520align%2520dense%2520tissue.%2520In%2520addition%2520to%2520a%250Asingle-scale%2520network%2520for%2520global%2520structure%2520alignment%252C%2520we%2520introduce%2520a%2520framework%250Athat%2520utilizes%2520dense%2520tissue%2520information%2520to%2520track%2520breast%2520movement.%2520The%2520learned%250Atransformation%2520fields%2520are%2520fused%2520by%2520introducing%2520the%2520Dual%2520Spatial%2520Transformer%250ANetwork%2520%2528DSTN%2529%252C%2520improving%2520overall%2520alignment%2520accuracy.%2520A%2520novel%2520warping%2520method%250Abased%2520on%2520the%2520Euclidean%2520distance%2520transform%2520%2528EDT%2529%2520is%2520also%2520proposed%2520to%2520accurately%250Awarp%2520the%2520registered%2520dense%2520tissue%2520and%2520breast%2520masks%252C%2520preserving%2520fine%2520structural%250Adetails%2520during%2520deformation.%2520The%2520framework%2520supports%2520paradigms%2520that%2520require%250Aexternal%2520segmentation%2520models%2520and%2520with%2520image%2520data%2520only.%2520It%2520also%2520operates%250Aeffectively%2520with%2520the%2520VoxelMorph%2520and%2520TransMorph%2520backbones%252C%2520offering%2520a%2520versatile%250Asolution%2520for%2520breast%2520registration.%2520We%2520validate%2520our%2520method%2520on%2520ISPY2%2520and%2520internal%250Adataset%252C%2520demonstrating%2520superior%2520performance%2520in%2520dense%2520tissue%252C%2520overall%2520breast%250Aalignment%252C%2520and%2520breast%2520structural%2520similarity%2520index%2520measure%2520%2528SSIM%2529%252C%2520with%2520notable%250Aimprovements%2520by%2520over%252013.01%2525%2520in%2520dense%2520tissue%2520Dice%252C%25203.13%2525%2520in%2520breast%2520Dice%252C%2520and%250A1.21%2525%2520in%2520breast%2520SSIM%2520compared%2520to%2520the%2520best%2520learning-based%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GuidedMorph%3A%20Two-Stage%20Deformable%20Registration%20for%20Breast%20MRI&entry.906535625=Yaqian%20Chen%20and%20Hanxue%20Gu%20and%20Haoyu%20Dong%20and%20Qihang%20Li%20and%20Yuwen%20Chen%20and%20Nicholas%20Konz%20and%20Lin%20Li%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Accurately%20registering%20breast%20MR%20images%20from%20different%20time%20points%20enables%0Athe%20alignment%20of%20anatomical%20structures%20and%20tracking%20of%20tumor%20progression%2C%0Asupporting%20more%20effective%20breast%20cancer%20detection%2C%20diagnosis%2C%20and%20treatment%0Aplanning.%20However%2C%20the%20complexity%20of%20dense%20tissue%20and%20its%20highly%20non-rigid%0Anature%20pose%20challenges%20for%20conventional%20registration%20methods%2C%20which%20primarily%0Afocus%20on%20aligning%20general%20structures%20while%20overlooking%20intricate%20internal%0Adetails.%20To%20address%20this%2C%20we%20propose%20%5Ctextbf%7BGuidedMorph%7D%2C%20a%20novel%20two-stage%0Aregistration%20framework%20designed%20to%20better%20align%20dense%20tissue.%20In%20addition%20to%20a%0Asingle-scale%20network%20for%20global%20structure%20alignment%2C%20we%20introduce%20a%20framework%0Athat%20utilizes%20dense%20tissue%20information%20to%20track%20breast%20movement.%20The%20learned%0Atransformation%20fields%20are%20fused%20by%20introducing%20the%20Dual%20Spatial%20Transformer%0ANetwork%20%28DSTN%29%2C%20improving%20overall%20alignment%20accuracy.%20A%20novel%20warping%20method%0Abased%20on%20the%20Euclidean%20distance%20transform%20%28EDT%29%20is%20also%20proposed%20to%20accurately%0Awarp%20the%20registered%20dense%20tissue%20and%20breast%20masks%2C%20preserving%20fine%20structural%0Adetails%20during%20deformation.%20The%20framework%20supports%20paradigms%20that%20require%0Aexternal%20segmentation%20models%20and%20with%20image%20data%20only.%20It%20also%20operates%0Aeffectively%20with%20the%20VoxelMorph%20and%20TransMorph%20backbones%2C%20offering%20a%20versatile%0Asolution%20for%20breast%20registration.%20We%20validate%20our%20method%20on%20ISPY2%20and%20internal%0Adataset%2C%20demonstrating%20superior%20performance%20in%20dense%20tissue%2C%20overall%20breast%0Aalignment%2C%20and%20breast%20structural%20similarity%20index%20measure%20%28SSIM%29%2C%20with%20notable%0Aimprovements%20by%20over%2013.01%25%20in%20dense%20tissue%20Dice%2C%203.13%25%20in%20breast%20Dice%2C%20and%0A1.21%25%20in%20breast%20SSIM%20compared%20to%20the%20best%20learning-based%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13414v1&entry.124074799=Read"},
{"title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided\n  Cross-Modal Alignment", "author": "Siming Sun and Kai Zhang and Xuejun Jiang and Wenchao Meng and Qinmin Yang", "abstract": "  The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting.\n", "link": "http://arxiv.org/abs/2505.13175v1", "date": "2025-05-19", "relevancy": 2.6024, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5782}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20LLMs%20for%20Time%20Series%20Forecasting%20via%20Structure-Guided%0A%20%20Cross-Modal%20Alignment&body=Title%3A%20Enhancing%20LLMs%20for%20Time%20Series%20Forecasting%20via%20Structure-Guided%0A%20%20Cross-Modal%20Alignment%0AAuthor%3A%20Siming%20Sun%20and%20Kai%20Zhang%20and%20Xuejun%20Jiang%20and%20Wenchao%20Meng%20and%20Qinmin%20Yang%0AAbstract%3A%20%20%20The%20emerging%20paradigm%20of%20leveraging%20pretrained%20large%20language%20models%20%28LLMs%29%0Afor%20time%20series%20forecasting%20has%20predominantly%20employed%20linguistic-temporal%0Amodality%20alignment%20strategies%20through%20token-level%20or%20layer-wise%20feature%0Amapping.%20However%2C%20these%20approaches%20fundamentally%20neglect%20a%20critical%20insight%3A%0Athe%20core%20competency%20of%20LLMs%20resides%20not%20merely%20in%20processing%20localized%20token%0Afeatures%20but%20in%20their%20inherent%20capacity%20to%20model%20holistic%20sequence%20structures.%0AThis%20paper%20posits%20that%20effective%20cross-modal%20alignment%20necessitates%20structural%0Aconsistency%20at%20the%20sequence%20level.%20We%20propose%20the%20Structure-Guided%20Cross-Modal%0AAlignment%20%28SGCMA%29%2C%20a%20framework%20that%20fully%20exploits%20and%20aligns%20the%0Astate-transition%20graph%20structures%20shared%20by%20time-series%20and%20linguistic%20data%20as%0Asequential%20modalities%2C%20thereby%20endowing%20time%20series%20with%20language-like%0Aproperties%20and%20delivering%20stronger%20generalization%20after%20modality%20alignment.%0ASGCMA%20consists%20of%20two%20key%20components%2C%20namely%20Structure%20Alignment%20and%20Semantic%0AAlignment.%20In%20Structure%20Alignment%2C%20a%20state%20transition%20matrix%20is%20learned%20from%0Atext%20data%20through%20Hidden%20Markov%20Models%20%28HMMs%29%2C%20and%20a%20shallow%20transformer-based%0AMaximum%20Entropy%20Markov%20Model%20%28MEMM%29%20receives%20the%20hot-start%20transition%20matrix%0Aand%20annotates%20each%20temporal%20patch%20into%20state%20probability%2C%20ensuring%20that%20the%0Atemporal%20representation%20sequence%20inherits%20language-like%20sequential%20dynamics.%20In%0ASemantic%20Alignment%2C%20cross-attention%20is%20applied%20between%20temporal%20patches%20and%20the%0Atop-k%20tokens%20within%20each%20state%2C%20and%20the%20ultimate%20temporal%20embeddings%20are%0Aderived%20by%20the%20expected%20value%20of%20these%20embeddings%20using%20a%20weighted%20average%0Abased%20on%20state%20probabilities.%20Experiments%20on%20multiple%20benchmarks%20demonstrate%0Athat%20SGCMA%20achieves%20state-of-the-art%20performance%2C%20offering%20a%20novel%20approach%20to%0Across-modal%20alignment%20in%20time%20series%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520LLMs%2520for%2520Time%2520Series%2520Forecasting%2520via%2520Structure-Guided%250A%2520%2520Cross-Modal%2520Alignment%26entry.906535625%3DSiming%2520Sun%2520and%2520Kai%2520Zhang%2520and%2520Xuejun%2520Jiang%2520and%2520Wenchao%2520Meng%2520and%2520Qinmin%2520Yang%26entry.1292438233%3D%2520%2520The%2520emerging%2520paradigm%2520of%2520leveraging%2520pretrained%2520large%2520language%2520models%2520%2528LLMs%2529%250Afor%2520time%2520series%2520forecasting%2520has%2520predominantly%2520employed%2520linguistic-temporal%250Amodality%2520alignment%2520strategies%2520through%2520token-level%2520or%2520layer-wise%2520feature%250Amapping.%2520However%252C%2520these%2520approaches%2520fundamentally%2520neglect%2520a%2520critical%2520insight%253A%250Athe%2520core%2520competency%2520of%2520LLMs%2520resides%2520not%2520merely%2520in%2520processing%2520localized%2520token%250Afeatures%2520but%2520in%2520their%2520inherent%2520capacity%2520to%2520model%2520holistic%2520sequence%2520structures.%250AThis%2520paper%2520posits%2520that%2520effective%2520cross-modal%2520alignment%2520necessitates%2520structural%250Aconsistency%2520at%2520the%2520sequence%2520level.%2520We%2520propose%2520the%2520Structure-Guided%2520Cross-Modal%250AAlignment%2520%2528SGCMA%2529%252C%2520a%2520framework%2520that%2520fully%2520exploits%2520and%2520aligns%2520the%250Astate-transition%2520graph%2520structures%2520shared%2520by%2520time-series%2520and%2520linguistic%2520data%2520as%250Asequential%2520modalities%252C%2520thereby%2520endowing%2520time%2520series%2520with%2520language-like%250Aproperties%2520and%2520delivering%2520stronger%2520generalization%2520after%2520modality%2520alignment.%250ASGCMA%2520consists%2520of%2520two%2520key%2520components%252C%2520namely%2520Structure%2520Alignment%2520and%2520Semantic%250AAlignment.%2520In%2520Structure%2520Alignment%252C%2520a%2520state%2520transition%2520matrix%2520is%2520learned%2520from%250Atext%2520data%2520through%2520Hidden%2520Markov%2520Models%2520%2528HMMs%2529%252C%2520and%2520a%2520shallow%2520transformer-based%250AMaximum%2520Entropy%2520Markov%2520Model%2520%2528MEMM%2529%2520receives%2520the%2520hot-start%2520transition%2520matrix%250Aand%2520annotates%2520each%2520temporal%2520patch%2520into%2520state%2520probability%252C%2520ensuring%2520that%2520the%250Atemporal%2520representation%2520sequence%2520inherits%2520language-like%2520sequential%2520dynamics.%2520In%250ASemantic%2520Alignment%252C%2520cross-attention%2520is%2520applied%2520between%2520temporal%2520patches%2520and%2520the%250Atop-k%2520tokens%2520within%2520each%2520state%252C%2520and%2520the%2520ultimate%2520temporal%2520embeddings%2520are%250Aderived%2520by%2520the%2520expected%2520value%2520of%2520these%2520embeddings%2520using%2520a%2520weighted%2520average%250Abased%2520on%2520state%2520probabilities.%2520Experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%250Athat%2520SGCMA%2520achieves%2520state-of-the-art%2520performance%252C%2520offering%2520a%2520novel%2520approach%2520to%250Across-modal%2520alignment%2520in%2520time%2520series%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20LLMs%20for%20Time%20Series%20Forecasting%20via%20Structure-Guided%0A%20%20Cross-Modal%20Alignment&entry.906535625=Siming%20Sun%20and%20Kai%20Zhang%20and%20Xuejun%20Jiang%20and%20Wenchao%20Meng%20and%20Qinmin%20Yang&entry.1292438233=%20%20The%20emerging%20paradigm%20of%20leveraging%20pretrained%20large%20language%20models%20%28LLMs%29%0Afor%20time%20series%20forecasting%20has%20predominantly%20employed%20linguistic-temporal%0Amodality%20alignment%20strategies%20through%20token-level%20or%20layer-wise%20feature%0Amapping.%20However%2C%20these%20approaches%20fundamentally%20neglect%20a%20critical%20insight%3A%0Athe%20core%20competency%20of%20LLMs%20resides%20not%20merely%20in%20processing%20localized%20token%0Afeatures%20but%20in%20their%20inherent%20capacity%20to%20model%20holistic%20sequence%20structures.%0AThis%20paper%20posits%20that%20effective%20cross-modal%20alignment%20necessitates%20structural%0Aconsistency%20at%20the%20sequence%20level.%20We%20propose%20the%20Structure-Guided%20Cross-Modal%0AAlignment%20%28SGCMA%29%2C%20a%20framework%20that%20fully%20exploits%20and%20aligns%20the%0Astate-transition%20graph%20structures%20shared%20by%20time-series%20and%20linguistic%20data%20as%0Asequential%20modalities%2C%20thereby%20endowing%20time%20series%20with%20language-like%0Aproperties%20and%20delivering%20stronger%20generalization%20after%20modality%20alignment.%0ASGCMA%20consists%20of%20two%20key%20components%2C%20namely%20Structure%20Alignment%20and%20Semantic%0AAlignment.%20In%20Structure%20Alignment%2C%20a%20state%20transition%20matrix%20is%20learned%20from%0Atext%20data%20through%20Hidden%20Markov%20Models%20%28HMMs%29%2C%20and%20a%20shallow%20transformer-based%0AMaximum%20Entropy%20Markov%20Model%20%28MEMM%29%20receives%20the%20hot-start%20transition%20matrix%0Aand%20annotates%20each%20temporal%20patch%20into%20state%20probability%2C%20ensuring%20that%20the%0Atemporal%20representation%20sequence%20inherits%20language-like%20sequential%20dynamics.%20In%0ASemantic%20Alignment%2C%20cross-attention%20is%20applied%20between%20temporal%20patches%20and%20the%0Atop-k%20tokens%20within%20each%20state%2C%20and%20the%20ultimate%20temporal%20embeddings%20are%0Aderived%20by%20the%20expected%20value%20of%20these%20embeddings%20using%20a%20weighted%20average%0Abased%20on%20state%20probabilities.%20Experiments%20on%20multiple%20benchmarks%20demonstrate%0Athat%20SGCMA%20achieves%20state-of-the-art%20performance%2C%20offering%20a%20novel%20approach%20to%0Across-modal%20alignment%20in%20time%20series%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13175v1&entry.124074799=Read"},
{"title": "Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation", "author": "Tiansheng Wen and Yifei Wang and Zequn Zeng and Zhong Peng and Yudi Su and Xinyang Liu and Bo Chen and Hongwei Liu and Stefanie Jegelka and Chenyu You", "abstract": "  Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep\n", "link": "http://arxiv.org/abs/2503.01776v4", "date": "2025-05-19", "relevancy": 2.5948, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5281}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5202}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Matryoshka%3A%20Revisiting%20Sparse%20Coding%20for%20Adaptive%20Representation&body=Title%3A%20Beyond%20Matryoshka%3A%20Revisiting%20Sparse%20Coding%20for%20Adaptive%20Representation%0AAuthor%3A%20Tiansheng%20Wen%20and%20Yifei%20Wang%20and%20Zequn%20Zeng%20and%20Zhong%20Peng%20and%20Yudi%20Su%20and%20Xinyang%20Liu%20and%20Bo%20Chen%20and%20Hongwei%20Liu%20and%20Stefanie%20Jegelka%20and%20Chenyu%20You%0AAbstract%3A%20%20%20Many%20large-scale%20systems%20rely%20on%20high-quality%20deep%20representations%0A%28embeddings%29%20to%20facilitate%20tasks%20like%20retrieval%2C%20search%2C%20and%20generative%0Amodeling.%20Matryoshka%20Representation%20Learning%20%28MRL%29%20recently%20emerged%20as%20a%0Asolution%20for%20adaptive%20embedding%20lengths%2C%20but%20it%20requires%20full%20model%20retraining%0Aand%20suffers%20from%20noticeable%20performance%20degradations%20at%20short%20lengths.%20In%20this%0Apaper%2C%20we%20show%20that%20sparse%20coding%20offers%20a%20compelling%20alternative%20for%20achieving%0Aadaptive%20representation%20with%20minimal%20overhead%20and%20higher%20fidelity.%20We%20propose%0AContrastive%20Sparse%20Representation%20%28CSR%29%2C%20a%20method%20that%20sparsifies%20pre-trained%0Aembeddings%20into%20a%20high-dimensional%20but%20selectively%20activated%20feature%20space.%20By%0Aleveraging%20lightweight%20autoencoding%20and%20task-aware%20contrastive%20objectives%2C%20CSR%0Apreserves%20semantic%20quality%20while%20allowing%20flexible%2C%20cost-effective%20inference%20at%0Adifferent%20sparsity%20levels.%20Extensive%20experiments%20on%20image%2C%20text%2C%20and%20multimodal%0Abenchmarks%20demonstrate%20that%20CSR%20consistently%20outperforms%20MRL%20in%20terms%20of%20both%0Aaccuracy%20and%20retrieval%20speed-often%20by%20large%20margins-while%20also%20cutting%20training%0Atime%20to%20a%20fraction%20of%20that%20required%20by%20MRL.%20Our%20results%20establish%20sparse%20coding%0Aas%20a%20powerful%20paradigm%20for%20adaptive%20representation%20learning%20in%20real-world%0Aapplications%20where%20efficiency%20and%20fidelity%20are%20both%20paramount.%20Code%20is%0Aavailable%20at%20https%3A//github.com/neilwen987/CSR_Adaptive_Rep%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01776v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Matryoshka%253A%2520Revisiting%2520Sparse%2520Coding%2520for%2520Adaptive%2520Representation%26entry.906535625%3DTiansheng%2520Wen%2520and%2520Yifei%2520Wang%2520and%2520Zequn%2520Zeng%2520and%2520Zhong%2520Peng%2520and%2520Yudi%2520Su%2520and%2520Xinyang%2520Liu%2520and%2520Bo%2520Chen%2520and%2520Hongwei%2520Liu%2520and%2520Stefanie%2520Jegelka%2520and%2520Chenyu%2520You%26entry.1292438233%3D%2520%2520Many%2520large-scale%2520systems%2520rely%2520on%2520high-quality%2520deep%2520representations%250A%2528embeddings%2529%2520to%2520facilitate%2520tasks%2520like%2520retrieval%252C%2520search%252C%2520and%2520generative%250Amodeling.%2520Matryoshka%2520Representation%2520Learning%2520%2528MRL%2529%2520recently%2520emerged%2520as%2520a%250Asolution%2520for%2520adaptive%2520embedding%2520lengths%252C%2520but%2520it%2520requires%2520full%2520model%2520retraining%250Aand%2520suffers%2520from%2520noticeable%2520performance%2520degradations%2520at%2520short%2520lengths.%2520In%2520this%250Apaper%252C%2520we%2520show%2520that%2520sparse%2520coding%2520offers%2520a%2520compelling%2520alternative%2520for%2520achieving%250Aadaptive%2520representation%2520with%2520minimal%2520overhead%2520and%2520higher%2520fidelity.%2520We%2520propose%250AContrastive%2520Sparse%2520Representation%2520%2528CSR%2529%252C%2520a%2520method%2520that%2520sparsifies%2520pre-trained%250Aembeddings%2520into%2520a%2520high-dimensional%2520but%2520selectively%2520activated%2520feature%2520space.%2520By%250Aleveraging%2520lightweight%2520autoencoding%2520and%2520task-aware%2520contrastive%2520objectives%252C%2520CSR%250Apreserves%2520semantic%2520quality%2520while%2520allowing%2520flexible%252C%2520cost-effective%2520inference%2520at%250Adifferent%2520sparsity%2520levels.%2520Extensive%2520experiments%2520on%2520image%252C%2520text%252C%2520and%2520multimodal%250Abenchmarks%2520demonstrate%2520that%2520CSR%2520consistently%2520outperforms%2520MRL%2520in%2520terms%2520of%2520both%250Aaccuracy%2520and%2520retrieval%2520speed-often%2520by%2520large%2520margins-while%2520also%2520cutting%2520training%250Atime%2520to%2520a%2520fraction%2520of%2520that%2520required%2520by%2520MRL.%2520Our%2520results%2520establish%2520sparse%2520coding%250Aas%2520a%2520powerful%2520paradigm%2520for%2520adaptive%2520representation%2520learning%2520in%2520real-world%250Aapplications%2520where%2520efficiency%2520and%2520fidelity%2520are%2520both%2520paramount.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/neilwen987/CSR_Adaptive_Rep%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01776v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Matryoshka%3A%20Revisiting%20Sparse%20Coding%20for%20Adaptive%20Representation&entry.906535625=Tiansheng%20Wen%20and%20Yifei%20Wang%20and%20Zequn%20Zeng%20and%20Zhong%20Peng%20and%20Yudi%20Su%20and%20Xinyang%20Liu%20and%20Bo%20Chen%20and%20Hongwei%20Liu%20and%20Stefanie%20Jegelka%20and%20Chenyu%20You&entry.1292438233=%20%20Many%20large-scale%20systems%20rely%20on%20high-quality%20deep%20representations%0A%28embeddings%29%20to%20facilitate%20tasks%20like%20retrieval%2C%20search%2C%20and%20generative%0Amodeling.%20Matryoshka%20Representation%20Learning%20%28MRL%29%20recently%20emerged%20as%20a%0Asolution%20for%20adaptive%20embedding%20lengths%2C%20but%20it%20requires%20full%20model%20retraining%0Aand%20suffers%20from%20noticeable%20performance%20degradations%20at%20short%20lengths.%20In%20this%0Apaper%2C%20we%20show%20that%20sparse%20coding%20offers%20a%20compelling%20alternative%20for%20achieving%0Aadaptive%20representation%20with%20minimal%20overhead%20and%20higher%20fidelity.%20We%20propose%0AContrastive%20Sparse%20Representation%20%28CSR%29%2C%20a%20method%20that%20sparsifies%20pre-trained%0Aembeddings%20into%20a%20high-dimensional%20but%20selectively%20activated%20feature%20space.%20By%0Aleveraging%20lightweight%20autoencoding%20and%20task-aware%20contrastive%20objectives%2C%20CSR%0Apreserves%20semantic%20quality%20while%20allowing%20flexible%2C%20cost-effective%20inference%20at%0Adifferent%20sparsity%20levels.%20Extensive%20experiments%20on%20image%2C%20text%2C%20and%20multimodal%0Abenchmarks%20demonstrate%20that%20CSR%20consistently%20outperforms%20MRL%20in%20terms%20of%20both%0Aaccuracy%20and%20retrieval%20speed-often%20by%20large%20margins-while%20also%20cutting%20training%0Atime%20to%20a%20fraction%20of%20that%20required%20by%20MRL.%20Our%20results%20establish%20sparse%20coding%0Aas%20a%20powerful%20paradigm%20for%20adaptive%20representation%20learning%20in%20real-world%0Aapplications%20where%20efficiency%20and%20fidelity%20are%20both%20paramount.%20Code%20is%0Aavailable%20at%20https%3A//github.com/neilwen987/CSR_Adaptive_Rep%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01776v4&entry.124074799=Read"},
{"title": "Learnware of Language Models: Specialized Small Language Models Can Do\n  Big", "author": "Zhi-Hao Tan and Zi-Chen Zhao and Hao-Yu Shi and Xin-Yu Zhang and Peng Tan and Yang Yu and Zhi-Hua Zhou", "abstract": "  The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks.\n", "link": "http://arxiv.org/abs/2505.13425v1", "date": "2025-05-19", "relevancy": 2.5944, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnware%20of%20Language%20Models%3A%20Specialized%20Small%20Language%20Models%20Can%20Do%0A%20%20Big&body=Title%3A%20Learnware%20of%20Language%20Models%3A%20Specialized%20Small%20Language%20Models%20Can%20Do%0A%20%20Big%0AAuthor%3A%20Zhi-Hao%20Tan%20and%20Zi-Chen%20Zhao%20and%20Hao-Yu%20Shi%20and%20Xin-Yu%20Zhang%20and%20Peng%20Tan%20and%20Yang%20Yu%20and%20Zhi-Hua%20Zhou%0AAbstract%3A%20%20%20The%20learnware%20paradigm%20offers%20a%20novel%20approach%20to%20machine%20learning%20by%0Aenabling%20users%20to%20reuse%20a%20set%20of%20well-trained%20models%20for%20tasks%20beyond%20the%0Amodels%27%20original%20purposes.%20It%20eliminates%20the%20need%20to%20build%20models%20from%20scratch%2C%0Ainstead%20relying%20on%20specifications%20%28representations%20of%20a%20model%27s%20capabilities%29%0Ato%20identify%20and%20leverage%20the%20most%20suitable%20models%20for%20new%20tasks.%20While%0Alearnware%20has%20proven%20effective%20in%20many%20scenarios%2C%20its%20application%20to%20language%0Amodels%20has%20remained%20largely%20unexplored.%20At%20the%20same%20time%2C%20large%20language%20models%0A%28LLMs%29%20have%20demonstrated%20remarkable%20universal%20question-answering%20abilities%2C%20yet%0Athey%20face%20challenges%20in%20specialized%20scenarios%20due%20to%20data%20scarcity%2C%20privacy%0Aconcerns%2C%20and%20high%20computational%20costs%2C%20thus%20more%20and%20more%20specialized%20small%0Alanguage%20models%20%28SLMs%29%20are%20being%20trained%20for%20specific%20domains.%20To%20address%20these%0Alimitations%20systematically%2C%20the%20learnware%20paradigm%20provides%20a%20promising%0Asolution%20by%20enabling%20maximum%20utilization%20of%20specialized%20SLMs%2C%20and%20allowing%0Ausers%20to%20identify%20and%20reuse%20them%20in%20a%20collaborative%20and%20privacy-preserving%0Amanner.%0A%20%20This%20paper%20presents%20a%20preliminary%20attempt%20to%20apply%20the%20learnware%20paradigm%20to%0Alanguage%20models.%20We%20simulated%20a%20learnware%20system%20comprising%20approximately%20100%0Alearnwares%20of%20specialized%20SLMs%20with%208B%20parameters%2C%20fine-tuned%20across%20finance%2C%0Ahealthcare%2C%20and%20mathematics%20domains.%20Each%20learnware%20contains%20an%20SLM%20and%20a%0Aspecification%2C%20which%20enables%20users%20to%20identify%20the%20most%20relevant%20models%20without%0Aexposing%20their%20own%20data.%20Experimental%20results%20demonstrate%20promising%0Aperformance%3A%20by%20selecting%20one%20suitable%20learnware%20for%20each%20task-specific%0Ainference%2C%20the%20system%20outperforms%20the%20base%20SLMs%20on%20all%20benchmarks.%20Compared%20to%0ALLMs%2C%20the%20system%20outperforms%20Qwen1.5-110B%2C%20Qwen2.5-72B%2C%20and%0ALlama3.1-70B-Instruct%20by%20at%20least%2014%25%20in%20finance%20domain%20tasks%2C%20and%20surpasses%0AFlan-PaLM-540B%20%28ranked%207th%20on%20the%20Open%20Medical%20LLM%20Leaderboard%29%20in%20medical%0Adomain%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnware%2520of%2520Language%2520Models%253A%2520Specialized%2520Small%2520Language%2520Models%2520Can%2520Do%250A%2520%2520Big%26entry.906535625%3DZhi-Hao%2520Tan%2520and%2520Zi-Chen%2520Zhao%2520and%2520Hao-Yu%2520Shi%2520and%2520Xin-Yu%2520Zhang%2520and%2520Peng%2520Tan%2520and%2520Yang%2520Yu%2520and%2520Zhi-Hua%2520Zhou%26entry.1292438233%3D%2520%2520The%2520learnware%2520paradigm%2520offers%2520a%2520novel%2520approach%2520to%2520machine%2520learning%2520by%250Aenabling%2520users%2520to%2520reuse%2520a%2520set%2520of%2520well-trained%2520models%2520for%2520tasks%2520beyond%2520the%250Amodels%2527%2520original%2520purposes.%2520It%2520eliminates%2520the%2520need%2520to%2520build%2520models%2520from%2520scratch%252C%250Ainstead%2520relying%2520on%2520specifications%2520%2528representations%2520of%2520a%2520model%2527s%2520capabilities%2529%250Ato%2520identify%2520and%2520leverage%2520the%2520most%2520suitable%2520models%2520for%2520new%2520tasks.%2520While%250Alearnware%2520has%2520proven%2520effective%2520in%2520many%2520scenarios%252C%2520its%2520application%2520to%2520language%250Amodels%2520has%2520remained%2520largely%2520unexplored.%2520At%2520the%2520same%2520time%252C%2520large%2520language%2520models%250A%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520universal%2520question-answering%2520abilities%252C%2520yet%250Athey%2520face%2520challenges%2520in%2520specialized%2520scenarios%2520due%2520to%2520data%2520scarcity%252C%2520privacy%250Aconcerns%252C%2520and%2520high%2520computational%2520costs%252C%2520thus%2520more%2520and%2520more%2520specialized%2520small%250Alanguage%2520models%2520%2528SLMs%2529%2520are%2520being%2520trained%2520for%2520specific%2520domains.%2520To%2520address%2520these%250Alimitations%2520systematically%252C%2520the%2520learnware%2520paradigm%2520provides%2520a%2520promising%250Asolution%2520by%2520enabling%2520maximum%2520utilization%2520of%2520specialized%2520SLMs%252C%2520and%2520allowing%250Ausers%2520to%2520identify%2520and%2520reuse%2520them%2520in%2520a%2520collaborative%2520and%2520privacy-preserving%250Amanner.%250A%2520%2520This%2520paper%2520presents%2520a%2520preliminary%2520attempt%2520to%2520apply%2520the%2520learnware%2520paradigm%2520to%250Alanguage%2520models.%2520We%2520simulated%2520a%2520learnware%2520system%2520comprising%2520approximately%2520100%250Alearnwares%2520of%2520specialized%2520SLMs%2520with%25208B%2520parameters%252C%2520fine-tuned%2520across%2520finance%252C%250Ahealthcare%252C%2520and%2520mathematics%2520domains.%2520Each%2520learnware%2520contains%2520an%2520SLM%2520and%2520a%250Aspecification%252C%2520which%2520enables%2520users%2520to%2520identify%2520the%2520most%2520relevant%2520models%2520without%250Aexposing%2520their%2520own%2520data.%2520Experimental%2520results%2520demonstrate%2520promising%250Aperformance%253A%2520by%2520selecting%2520one%2520suitable%2520learnware%2520for%2520each%2520task-specific%250Ainference%252C%2520the%2520system%2520outperforms%2520the%2520base%2520SLMs%2520on%2520all%2520benchmarks.%2520Compared%2520to%250ALLMs%252C%2520the%2520system%2520outperforms%2520Qwen1.5-110B%252C%2520Qwen2.5-72B%252C%2520and%250ALlama3.1-70B-Instruct%2520by%2520at%2520least%252014%2525%2520in%2520finance%2520domain%2520tasks%252C%2520and%2520surpasses%250AFlan-PaLM-540B%2520%2528ranked%25207th%2520on%2520the%2520Open%2520Medical%2520LLM%2520Leaderboard%2529%2520in%2520medical%250Adomain%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnware%20of%20Language%20Models%3A%20Specialized%20Small%20Language%20Models%20Can%20Do%0A%20%20Big&entry.906535625=Zhi-Hao%20Tan%20and%20Zi-Chen%20Zhao%20and%20Hao-Yu%20Shi%20and%20Xin-Yu%20Zhang%20and%20Peng%20Tan%20and%20Yang%20Yu%20and%20Zhi-Hua%20Zhou&entry.1292438233=%20%20The%20learnware%20paradigm%20offers%20a%20novel%20approach%20to%20machine%20learning%20by%0Aenabling%20users%20to%20reuse%20a%20set%20of%20well-trained%20models%20for%20tasks%20beyond%20the%0Amodels%27%20original%20purposes.%20It%20eliminates%20the%20need%20to%20build%20models%20from%20scratch%2C%0Ainstead%20relying%20on%20specifications%20%28representations%20of%20a%20model%27s%20capabilities%29%0Ato%20identify%20and%20leverage%20the%20most%20suitable%20models%20for%20new%20tasks.%20While%0Alearnware%20has%20proven%20effective%20in%20many%20scenarios%2C%20its%20application%20to%20language%0Amodels%20has%20remained%20largely%20unexplored.%20At%20the%20same%20time%2C%20large%20language%20models%0A%28LLMs%29%20have%20demonstrated%20remarkable%20universal%20question-answering%20abilities%2C%20yet%0Athey%20face%20challenges%20in%20specialized%20scenarios%20due%20to%20data%20scarcity%2C%20privacy%0Aconcerns%2C%20and%20high%20computational%20costs%2C%20thus%20more%20and%20more%20specialized%20small%0Alanguage%20models%20%28SLMs%29%20are%20being%20trained%20for%20specific%20domains.%20To%20address%20these%0Alimitations%20systematically%2C%20the%20learnware%20paradigm%20provides%20a%20promising%0Asolution%20by%20enabling%20maximum%20utilization%20of%20specialized%20SLMs%2C%20and%20allowing%0Ausers%20to%20identify%20and%20reuse%20them%20in%20a%20collaborative%20and%20privacy-preserving%0Amanner.%0A%20%20This%20paper%20presents%20a%20preliminary%20attempt%20to%20apply%20the%20learnware%20paradigm%20to%0Alanguage%20models.%20We%20simulated%20a%20learnware%20system%20comprising%20approximately%20100%0Alearnwares%20of%20specialized%20SLMs%20with%208B%20parameters%2C%20fine-tuned%20across%20finance%2C%0Ahealthcare%2C%20and%20mathematics%20domains.%20Each%20learnware%20contains%20an%20SLM%20and%20a%0Aspecification%2C%20which%20enables%20users%20to%20identify%20the%20most%20relevant%20models%20without%0Aexposing%20their%20own%20data.%20Experimental%20results%20demonstrate%20promising%0Aperformance%3A%20by%20selecting%20one%20suitable%20learnware%20for%20each%20task-specific%0Ainference%2C%20the%20system%20outperforms%20the%20base%20SLMs%20on%20all%20benchmarks.%20Compared%20to%0ALLMs%2C%20the%20system%20outperforms%20Qwen1.5-110B%2C%20Qwen2.5-72B%2C%20and%0ALlama3.1-70B-Instruct%20by%20at%20least%2014%25%20in%20finance%20domain%20tasks%2C%20and%20surpasses%0AFlan-PaLM-540B%20%28ranked%207th%20on%20the%20Open%20Medical%20LLM%20Leaderboard%29%20in%20medical%0Adomain%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13425v1&entry.124074799=Read"},
{"title": "An Empirical Study of Many-to-Many Summarization with Large Language\n  Models", "author": "Jiaan Wang and Fandong Meng and Zengkui Sun and Yunlong Liang and Yuxuan Cao and Jiarong Xu and Haoxiang Shi and Jie Zhou", "abstract": "  Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.\n", "link": "http://arxiv.org/abs/2505.12983v1", "date": "2025-05-19", "relevancy": 2.5536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Study%20of%20Many-to-Many%20Summarization%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20An%20Empirical%20Study%20of%20Many-to-Many%20Summarization%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Zengkui%20Sun%20and%20Yunlong%20Liang%20and%20Yuxuan%20Cao%20and%20Jiarong%20Xu%20and%20Haoxiang%20Shi%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Many-to-many%20summarization%20%28M2MS%29%20aims%20to%20process%20documents%20in%20any%20language%0Aand%20generate%20the%20corresponding%20summaries%20also%20in%20any%20language.%20Recently%2C%20large%0Alanguage%20models%20%28LLMs%29%20have%20shown%20strong%20multi-lingual%20abilities%2C%20giving%20them%0Athe%20potential%20to%20perform%20M2MS%20in%20real%20applications.%20This%20work%20presents%20a%0Asystematic%20empirical%20study%20on%20LLMs%27%20M2MS%20ability.%20Specifically%2C%20we%20first%0Areorganize%20M2MS%20data%20based%20on%20eight%20previous%20domain-specific%20datasets.%20The%0Areorganized%20data%20contains%2047.8K%20samples%20spanning%20five%20domains%20and%20six%0Alanguages%2C%20which%20could%20be%20used%20to%20train%20and%20evaluate%20LLMs.%20Then%2C%20we%20benchmark%0A18%20LLMs%20in%20a%20zero-shot%20manner%20and%20an%20instruction-tuning%20manner.%20Fine-tuned%0Atraditional%20models%20%28e.g.%2C%20mBART%29%20are%20also%20conducted%20for%20comparisons.%20Our%0Aexperiments%20reveal%20that%2C%20zero-shot%20LLMs%20achieve%20competitive%20results%20with%0Afine-tuned%20traditional%20models.%20After%20instruct-tuning%2C%20open-source%20LLMs%20can%0Asignificantly%20improve%20their%20M2MS%20ability%2C%20and%20outperform%20zero-shot%20LLMs%0A%28including%20GPT-4%29%20in%20terms%20of%20automatic%20evaluations.%20In%20addition%2C%20we%0Ademonstrate%20that%20this%20task-specific%20improvement%20does%20not%20sacrifice%20the%20LLMs%27%0Ageneral%20task-solving%20abilities.%20However%2C%20as%20revealed%20by%20our%20human%20evaluation%2C%0ALLMs%20still%20face%20the%20factuality%20issue%2C%20and%20the%20instruction%20tuning%20might%0Aintensify%20the%20issue.%20Thus%2C%20how%20to%20control%20factual%20errors%20becomes%20the%20key%20when%0Abuilding%20LLM%20summarizers%20in%20real%20applications%2C%20and%20is%20worth%20noting%20in%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Study%2520of%2520Many-to-Many%2520Summarization%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DJiaan%2520Wang%2520and%2520Fandong%2520Meng%2520and%2520Zengkui%2520Sun%2520and%2520Yunlong%2520Liang%2520and%2520Yuxuan%2520Cao%2520and%2520Jiarong%2520Xu%2520and%2520Haoxiang%2520Shi%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Many-to-many%2520summarization%2520%2528M2MS%2529%2520aims%2520to%2520process%2520documents%2520in%2520any%2520language%250Aand%2520generate%2520the%2520corresponding%2520summaries%2520also%2520in%2520any%2520language.%2520Recently%252C%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520shown%2520strong%2520multi-lingual%2520abilities%252C%2520giving%2520them%250Athe%2520potential%2520to%2520perform%2520M2MS%2520in%2520real%2520applications.%2520This%2520work%2520presents%2520a%250Asystematic%2520empirical%2520study%2520on%2520LLMs%2527%2520M2MS%2520ability.%2520Specifically%252C%2520we%2520first%250Areorganize%2520M2MS%2520data%2520based%2520on%2520eight%2520previous%2520domain-specific%2520datasets.%2520The%250Areorganized%2520data%2520contains%252047.8K%2520samples%2520spanning%2520five%2520domains%2520and%2520six%250Alanguages%252C%2520which%2520could%2520be%2520used%2520to%2520train%2520and%2520evaluate%2520LLMs.%2520Then%252C%2520we%2520benchmark%250A18%2520LLMs%2520in%2520a%2520zero-shot%2520manner%2520and%2520an%2520instruction-tuning%2520manner.%2520Fine-tuned%250Atraditional%2520models%2520%2528e.g.%252C%2520mBART%2529%2520are%2520also%2520conducted%2520for%2520comparisons.%2520Our%250Aexperiments%2520reveal%2520that%252C%2520zero-shot%2520LLMs%2520achieve%2520competitive%2520results%2520with%250Afine-tuned%2520traditional%2520models.%2520After%2520instruct-tuning%252C%2520open-source%2520LLMs%2520can%250Asignificantly%2520improve%2520their%2520M2MS%2520ability%252C%2520and%2520outperform%2520zero-shot%2520LLMs%250A%2528including%2520GPT-4%2529%2520in%2520terms%2520of%2520automatic%2520evaluations.%2520In%2520addition%252C%2520we%250Ademonstrate%2520that%2520this%2520task-specific%2520improvement%2520does%2520not%2520sacrifice%2520the%2520LLMs%2527%250Ageneral%2520task-solving%2520abilities.%2520However%252C%2520as%2520revealed%2520by%2520our%2520human%2520evaluation%252C%250ALLMs%2520still%2520face%2520the%2520factuality%2520issue%252C%2520and%2520the%2520instruction%2520tuning%2520might%250Aintensify%2520the%2520issue.%2520Thus%252C%2520how%2520to%2520control%2520factual%2520errors%2520becomes%2520the%2520key%2520when%250Abuilding%2520LLM%2520summarizers%2520in%2520real%2520applications%252C%2520and%2520is%2520worth%2520noting%2520in%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Study%20of%20Many-to-Many%20Summarization%20with%20Large%20Language%0A%20%20Models&entry.906535625=Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Zengkui%20Sun%20and%20Yunlong%20Liang%20and%20Yuxuan%20Cao%20and%20Jiarong%20Xu%20and%20Haoxiang%20Shi%20and%20Jie%20Zhou&entry.1292438233=%20%20Many-to-many%20summarization%20%28M2MS%29%20aims%20to%20process%20documents%20in%20any%20language%0Aand%20generate%20the%20corresponding%20summaries%20also%20in%20any%20language.%20Recently%2C%20large%0Alanguage%20models%20%28LLMs%29%20have%20shown%20strong%20multi-lingual%20abilities%2C%20giving%20them%0Athe%20potential%20to%20perform%20M2MS%20in%20real%20applications.%20This%20work%20presents%20a%0Asystematic%20empirical%20study%20on%20LLMs%27%20M2MS%20ability.%20Specifically%2C%20we%20first%0Areorganize%20M2MS%20data%20based%20on%20eight%20previous%20domain-specific%20datasets.%20The%0Areorganized%20data%20contains%2047.8K%20samples%20spanning%20five%20domains%20and%20six%0Alanguages%2C%20which%20could%20be%20used%20to%20train%20and%20evaluate%20LLMs.%20Then%2C%20we%20benchmark%0A18%20LLMs%20in%20a%20zero-shot%20manner%20and%20an%20instruction-tuning%20manner.%20Fine-tuned%0Atraditional%20models%20%28e.g.%2C%20mBART%29%20are%20also%20conducted%20for%20comparisons.%20Our%0Aexperiments%20reveal%20that%2C%20zero-shot%20LLMs%20achieve%20competitive%20results%20with%0Afine-tuned%20traditional%20models.%20After%20instruct-tuning%2C%20open-source%20LLMs%20can%0Asignificantly%20improve%20their%20M2MS%20ability%2C%20and%20outperform%20zero-shot%20LLMs%0A%28including%20GPT-4%29%20in%20terms%20of%20automatic%20evaluations.%20In%20addition%2C%20we%0Ademonstrate%20that%20this%20task-specific%20improvement%20does%20not%20sacrifice%20the%20LLMs%27%0Ageneral%20task-solving%20abilities.%20However%2C%20as%20revealed%20by%20our%20human%20evaluation%2C%0ALLMs%20still%20face%20the%20factuality%20issue%2C%20and%20the%20instruction%20tuning%20might%0Aintensify%20the%20issue.%20Thus%2C%20how%20to%20control%20factual%20errors%20becomes%20the%20key%20when%0Abuilding%20LLM%20summarizers%20in%20real%20applications%2C%20and%20is%20worth%20noting%20in%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12983v1&entry.124074799=Read"},
{"title": "Unpacking Positional Encoding in Transformers: A Spectral Analysis of\n  Content-Position Coupling", "author": "Zihan Gu and Han Zhang and Ruoyu Chen and Yue Hu and Hua Zhang", "abstract": "  Positional encoding (PE) is essential for enabling Transformers to model\nsequential structure. However, the mechanisms by which different PE schemes\ncouple token content and positional information-and how these mechanisms\ninfluence model dynamics-remain theoretically underexplored. In this work, we\npresent a unified framework that analyzes PE through the spectral properties of\nToeplitz and related matrices derived from attention logits. We show that\nmultiplicative content-position coupling-exemplified by Rotary Positional\nEncoding (RoPE) via a Hadamard product with a Toeplitz matrix-induces spectral\ncontraction, which theoretically improves optimization stability and\nefficiency. Guided by this theory, we construct synthetic tasks that contrast\ncontent-position dependent and content-position independent settings, and\nevaluate a range of PE methods. Our experiments reveal strong alignment with\ntheory: RoPE consistently outperforms other methods on position-sensitive tasks\nand induces \"single-head deposit\" patterns in early layers, indicating\nlocalized positional processing. Further analyses show that modifying the\nmethod and timing of PE coupling, such as MLA in Deepseek-V3, can effectively\nmitigate this concentration. These results establish explicit content-relative\nmixing with relative-position Toeplitz signals as a key principle for effective\nPE design and provide new insight into how positional structure is integrated\nin Transformer architectures.\n", "link": "http://arxiv.org/abs/2505.13027v1", "date": "2025-05-19", "relevancy": 2.5365, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unpacking%20Positional%20Encoding%20in%20Transformers%3A%20A%20Spectral%20Analysis%20of%0A%20%20Content-Position%20Coupling&body=Title%3A%20Unpacking%20Positional%20Encoding%20in%20Transformers%3A%20A%20Spectral%20Analysis%20of%0A%20%20Content-Position%20Coupling%0AAuthor%3A%20Zihan%20Gu%20and%20Han%20Zhang%20and%20Ruoyu%20Chen%20and%20Yue%20Hu%20and%20Hua%20Zhang%0AAbstract%3A%20%20%20Positional%20encoding%20%28PE%29%20is%20essential%20for%20enabling%20Transformers%20to%20model%0Asequential%20structure.%20However%2C%20the%20mechanisms%20by%20which%20different%20PE%20schemes%0Acouple%20token%20content%20and%20positional%20information-and%20how%20these%20mechanisms%0Ainfluence%20model%20dynamics-remain%20theoretically%20underexplored.%20In%20this%20work%2C%20we%0Apresent%20a%20unified%20framework%20that%20analyzes%20PE%20through%20the%20spectral%20properties%20of%0AToeplitz%20and%20related%20matrices%20derived%20from%20attention%20logits.%20We%20show%20that%0Amultiplicative%20content-position%20coupling-exemplified%20by%20Rotary%20Positional%0AEncoding%20%28RoPE%29%20via%20a%20Hadamard%20product%20with%20a%20Toeplitz%20matrix-induces%20spectral%0Acontraction%2C%20which%20theoretically%20improves%20optimization%20stability%20and%0Aefficiency.%20Guided%20by%20this%20theory%2C%20we%20construct%20synthetic%20tasks%20that%20contrast%0Acontent-position%20dependent%20and%20content-position%20independent%20settings%2C%20and%0Aevaluate%20a%20range%20of%20PE%20methods.%20Our%20experiments%20reveal%20strong%20alignment%20with%0Atheory%3A%20RoPE%20consistently%20outperforms%20other%20methods%20on%20position-sensitive%20tasks%0Aand%20induces%20%22single-head%20deposit%22%20patterns%20in%20early%20layers%2C%20indicating%0Alocalized%20positional%20processing.%20Further%20analyses%20show%20that%20modifying%20the%0Amethod%20and%20timing%20of%20PE%20coupling%2C%20such%20as%20MLA%20in%20Deepseek-V3%2C%20can%20effectively%0Amitigate%20this%20concentration.%20These%20results%20establish%20explicit%20content-relative%0Amixing%20with%20relative-position%20Toeplitz%20signals%20as%20a%20key%20principle%20for%20effective%0APE%20design%20and%20provide%20new%20insight%20into%20how%20positional%20structure%20is%20integrated%0Ain%20Transformer%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnpacking%2520Positional%2520Encoding%2520in%2520Transformers%253A%2520A%2520Spectral%2520Analysis%2520of%250A%2520%2520Content-Position%2520Coupling%26entry.906535625%3DZihan%2520Gu%2520and%2520Han%2520Zhang%2520and%2520Ruoyu%2520Chen%2520and%2520Yue%2520Hu%2520and%2520Hua%2520Zhang%26entry.1292438233%3D%2520%2520Positional%2520encoding%2520%2528PE%2529%2520is%2520essential%2520for%2520enabling%2520Transformers%2520to%2520model%250Asequential%2520structure.%2520However%252C%2520the%2520mechanisms%2520by%2520which%2520different%2520PE%2520schemes%250Acouple%2520token%2520content%2520and%2520positional%2520information-and%2520how%2520these%2520mechanisms%250Ainfluence%2520model%2520dynamics-remain%2520theoretically%2520underexplored.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520unified%2520framework%2520that%2520analyzes%2520PE%2520through%2520the%2520spectral%2520properties%2520of%250AToeplitz%2520and%2520related%2520matrices%2520derived%2520from%2520attention%2520logits.%2520We%2520show%2520that%250Amultiplicative%2520content-position%2520coupling-exemplified%2520by%2520Rotary%2520Positional%250AEncoding%2520%2528RoPE%2529%2520via%2520a%2520Hadamard%2520product%2520with%2520a%2520Toeplitz%2520matrix-induces%2520spectral%250Acontraction%252C%2520which%2520theoretically%2520improves%2520optimization%2520stability%2520and%250Aefficiency.%2520Guided%2520by%2520this%2520theory%252C%2520we%2520construct%2520synthetic%2520tasks%2520that%2520contrast%250Acontent-position%2520dependent%2520and%2520content-position%2520independent%2520settings%252C%2520and%250Aevaluate%2520a%2520range%2520of%2520PE%2520methods.%2520Our%2520experiments%2520reveal%2520strong%2520alignment%2520with%250Atheory%253A%2520RoPE%2520consistently%2520outperforms%2520other%2520methods%2520on%2520position-sensitive%2520tasks%250Aand%2520induces%2520%2522single-head%2520deposit%2522%2520patterns%2520in%2520early%2520layers%252C%2520indicating%250Alocalized%2520positional%2520processing.%2520Further%2520analyses%2520show%2520that%2520modifying%2520the%250Amethod%2520and%2520timing%2520of%2520PE%2520coupling%252C%2520such%2520as%2520MLA%2520in%2520Deepseek-V3%252C%2520can%2520effectively%250Amitigate%2520this%2520concentration.%2520These%2520results%2520establish%2520explicit%2520content-relative%250Amixing%2520with%2520relative-position%2520Toeplitz%2520signals%2520as%2520a%2520key%2520principle%2520for%2520effective%250APE%2520design%2520and%2520provide%2520new%2520insight%2520into%2520how%2520positional%2520structure%2520is%2520integrated%250Ain%2520Transformer%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unpacking%20Positional%20Encoding%20in%20Transformers%3A%20A%20Spectral%20Analysis%20of%0A%20%20Content-Position%20Coupling&entry.906535625=Zihan%20Gu%20and%20Han%20Zhang%20and%20Ruoyu%20Chen%20and%20Yue%20Hu%20and%20Hua%20Zhang&entry.1292438233=%20%20Positional%20encoding%20%28PE%29%20is%20essential%20for%20enabling%20Transformers%20to%20model%0Asequential%20structure.%20However%2C%20the%20mechanisms%20by%20which%20different%20PE%20schemes%0Acouple%20token%20content%20and%20positional%20information-and%20how%20these%20mechanisms%0Ainfluence%20model%20dynamics-remain%20theoretically%20underexplored.%20In%20this%20work%2C%20we%0Apresent%20a%20unified%20framework%20that%20analyzes%20PE%20through%20the%20spectral%20properties%20of%0AToeplitz%20and%20related%20matrices%20derived%20from%20attention%20logits.%20We%20show%20that%0Amultiplicative%20content-position%20coupling-exemplified%20by%20Rotary%20Positional%0AEncoding%20%28RoPE%29%20via%20a%20Hadamard%20product%20with%20a%20Toeplitz%20matrix-induces%20spectral%0Acontraction%2C%20which%20theoretically%20improves%20optimization%20stability%20and%0Aefficiency.%20Guided%20by%20this%20theory%2C%20we%20construct%20synthetic%20tasks%20that%20contrast%0Acontent-position%20dependent%20and%20content-position%20independent%20settings%2C%20and%0Aevaluate%20a%20range%20of%20PE%20methods.%20Our%20experiments%20reveal%20strong%20alignment%20with%0Atheory%3A%20RoPE%20consistently%20outperforms%20other%20methods%20on%20position-sensitive%20tasks%0Aand%20induces%20%22single-head%20deposit%22%20patterns%20in%20early%20layers%2C%20indicating%0Alocalized%20positional%20processing.%20Further%20analyses%20show%20that%20modifying%20the%0Amethod%20and%20timing%20of%20PE%20coupling%2C%20such%20as%20MLA%20in%20Deepseek-V3%2C%20can%20effectively%0Amitigate%20this%20concentration.%20These%20results%20establish%20explicit%20content-relative%0Amixing%20with%20relative-position%20Toeplitz%20signals%20as%20a%20key%20principle%20for%20effective%0APE%20design%20and%20provide%20new%20insight%20into%20how%20positional%20structure%20is%20integrated%0Ain%20Transformer%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13027v1&entry.124074799=Read"},
{"title": "Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning\n  and Pretrain-Finetuning", "author": "Song-Lin Li and Rui Zhu and Yu-Feng Li and Lan-Zhe Guo", "abstract": "  Semi-supervised learning (SSL) alleviates the cost of data labeling process\nby exploiting unlabeled data, and has achieved promising results on various\ntasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm\nhas garnered significant attention in recent years, and exploiting pre-trained\nmodels could also reduce the requirement of labeled data in downstream tasks.\nTherefore, a question naturally occurs: \\emph{When the labeled data is scarce\nin the target tasks, should we exploit unlabeled data or pre-trained models?}\nTo answer this question, we select pre-trained Vision-Language Models (VLMs) as\nrepresentative pretrain-finetuning instances and propose \\textit{Few-shot SSL}\n-- a framework that enables fair comparison between these two paradigms by\ncontrolling the amount of labeled data used. Extensive experiments across\nvarious settings demonstrate that pre-trained VLMs generally outperform SSL\nmethods in nearly all cases, except when the data has low resolution or lacks\nclear semantic structure. Therefore, we encourage future SSL research to\ncompare with pre-trained models and explore deeper integration, such as using\npre-trained knowledge to enhance pseudo-labeling. To support future research,\nwe release our unified reproduction and evaluation framework. Codes are\navailable at\nhttps://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566\n", "link": "http://arxiv.org/abs/2505.13317v1", "date": "2025-05-19", "relevancy": 2.5352, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlabeled%20Data%20or%20Pre-trained%20Model%3A%20Rethinking%20Semi-Supervised%20Learning%0A%20%20and%20Pretrain-Finetuning&body=Title%3A%20Unlabeled%20Data%20or%20Pre-trained%20Model%3A%20Rethinking%20Semi-Supervised%20Learning%0A%20%20and%20Pretrain-Finetuning%0AAuthor%3A%20Song-Lin%20Li%20and%20Rui%20Zhu%20and%20Yu-Feng%20Li%20and%20Lan-Zhe%20Guo%0AAbstract%3A%20%20%20Semi-supervised%20learning%20%28SSL%29%20alleviates%20the%20cost%20of%20data%20labeling%20process%0Aby%20exploiting%20unlabeled%20data%2C%20and%20has%20achieved%20promising%20results%20on%20various%0Atasks%20such%20as%20image%20classification.%20Meanwhile%2C%20the%20Pretrain-Finetuning%20paradigm%0Ahas%20garnered%20significant%20attention%20in%20recent%20years%2C%20and%20exploiting%20pre-trained%0Amodels%20could%20also%20reduce%20the%20requirement%20of%20labeled%20data%20in%20downstream%20tasks.%0ATherefore%2C%20a%20question%20naturally%20occurs%3A%20%5Cemph%7BWhen%20the%20labeled%20data%20is%20scarce%0Ain%20the%20target%20tasks%2C%20should%20we%20exploit%20unlabeled%20data%20or%20pre-trained%20models%3F%7D%0ATo%20answer%20this%20question%2C%20we%20select%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20as%0Arepresentative%20pretrain-finetuning%20instances%20and%20propose%20%5Ctextit%7BFew-shot%20SSL%7D%0A--%20a%20framework%20that%20enables%20fair%20comparison%20between%20these%20two%20paradigms%20by%0Acontrolling%20the%20amount%20of%20labeled%20data%20used.%20Extensive%20experiments%20across%0Avarious%20settings%20demonstrate%20that%20pre-trained%20VLMs%20generally%20outperform%20SSL%0Amethods%20in%20nearly%20all%20cases%2C%20except%20when%20the%20data%20has%20low%20resolution%20or%20lacks%0Aclear%20semantic%20structure.%20Therefore%2C%20we%20encourage%20future%20SSL%20research%20to%0Acompare%20with%20pre-trained%20models%20and%20explore%20deeper%20integration%2C%20such%20as%20using%0Apre-trained%20knowledge%20to%20enhance%20pseudo-labeling.%20To%20support%20future%20research%2C%0Awe%20release%20our%20unified%20reproduction%20and%20evaluation%20framework.%20Codes%20are%0Aavailable%20at%0Ahttps%3A//anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlabeled%2520Data%2520or%2520Pre-trained%2520Model%253A%2520Rethinking%2520Semi-Supervised%2520Learning%250A%2520%2520and%2520Pretrain-Finetuning%26entry.906535625%3DSong-Lin%2520Li%2520and%2520Rui%2520Zhu%2520and%2520Yu-Feng%2520Li%2520and%2520Lan-Zhe%2520Guo%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%2520%2528SSL%2529%2520alleviates%2520the%2520cost%2520of%2520data%2520labeling%2520process%250Aby%2520exploiting%2520unlabeled%2520data%252C%2520and%2520has%2520achieved%2520promising%2520results%2520on%2520various%250Atasks%2520such%2520as%2520image%2520classification.%2520Meanwhile%252C%2520the%2520Pretrain-Finetuning%2520paradigm%250Ahas%2520garnered%2520significant%2520attention%2520in%2520recent%2520years%252C%2520and%2520exploiting%2520pre-trained%250Amodels%2520could%2520also%2520reduce%2520the%2520requirement%2520of%2520labeled%2520data%2520in%2520downstream%2520tasks.%250ATherefore%252C%2520a%2520question%2520naturally%2520occurs%253A%2520%255Cemph%257BWhen%2520the%2520labeled%2520data%2520is%2520scarce%250Ain%2520the%2520target%2520tasks%252C%2520should%2520we%2520exploit%2520unlabeled%2520data%2520or%2520pre-trained%2520models%253F%257D%250ATo%2520answer%2520this%2520question%252C%2520we%2520select%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520as%250Arepresentative%2520pretrain-finetuning%2520instances%2520and%2520propose%2520%255Ctextit%257BFew-shot%2520SSL%257D%250A--%2520a%2520framework%2520that%2520enables%2520fair%2520comparison%2520between%2520these%2520two%2520paradigms%2520by%250Acontrolling%2520the%2520amount%2520of%2520labeled%2520data%2520used.%2520Extensive%2520experiments%2520across%250Avarious%2520settings%2520demonstrate%2520that%2520pre-trained%2520VLMs%2520generally%2520outperform%2520SSL%250Amethods%2520in%2520nearly%2520all%2520cases%252C%2520except%2520when%2520the%2520data%2520has%2520low%2520resolution%2520or%2520lacks%250Aclear%2520semantic%2520structure.%2520Therefore%252C%2520we%2520encourage%2520future%2520SSL%2520research%2520to%250Acompare%2520with%2520pre-trained%2520models%2520and%2520explore%2520deeper%2520integration%252C%2520such%2520as%2520using%250Apre-trained%2520knowledge%2520to%2520enhance%2520pseudo-labeling.%2520To%2520support%2520future%2520research%252C%250Awe%2520release%2520our%2520unified%2520reproduction%2520and%2520evaluation%2520framework.%2520Codes%2520are%250Aavailable%2520at%250Ahttps%253A//anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlabeled%20Data%20or%20Pre-trained%20Model%3A%20Rethinking%20Semi-Supervised%20Learning%0A%20%20and%20Pretrain-Finetuning&entry.906535625=Song-Lin%20Li%20and%20Rui%20Zhu%20and%20Yu-Feng%20Li%20and%20Lan-Zhe%20Guo&entry.1292438233=%20%20Semi-supervised%20learning%20%28SSL%29%20alleviates%20the%20cost%20of%20data%20labeling%20process%0Aby%20exploiting%20unlabeled%20data%2C%20and%20has%20achieved%20promising%20results%20on%20various%0Atasks%20such%20as%20image%20classification.%20Meanwhile%2C%20the%20Pretrain-Finetuning%20paradigm%0Ahas%20garnered%20significant%20attention%20in%20recent%20years%2C%20and%20exploiting%20pre-trained%0Amodels%20could%20also%20reduce%20the%20requirement%20of%20labeled%20data%20in%20downstream%20tasks.%0ATherefore%2C%20a%20question%20naturally%20occurs%3A%20%5Cemph%7BWhen%20the%20labeled%20data%20is%20scarce%0Ain%20the%20target%20tasks%2C%20should%20we%20exploit%20unlabeled%20data%20or%20pre-trained%20models%3F%7D%0ATo%20answer%20this%20question%2C%20we%20select%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20as%0Arepresentative%20pretrain-finetuning%20instances%20and%20propose%20%5Ctextit%7BFew-shot%20SSL%7D%0A--%20a%20framework%20that%20enables%20fair%20comparison%20between%20these%20two%20paradigms%20by%0Acontrolling%20the%20amount%20of%20labeled%20data%20used.%20Extensive%20experiments%20across%0Avarious%20settings%20demonstrate%20that%20pre-trained%20VLMs%20generally%20outperform%20SSL%0Amethods%20in%20nearly%20all%20cases%2C%20except%20when%20the%20data%20has%20low%20resolution%20or%20lacks%0Aclear%20semantic%20structure.%20Therefore%2C%20we%20encourage%20future%20SSL%20research%20to%0Acompare%20with%20pre-trained%20models%20and%20explore%20deeper%20integration%2C%20such%20as%20using%0Apre-trained%20knowledge%20to%20enhance%20pseudo-labeling.%20To%20support%20future%20research%2C%0Awe%20release%20our%20unified%20reproduction%20and%20evaluation%20framework.%20Codes%20are%0Aavailable%20at%0Ahttps%3A//anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13317v1&entry.124074799=Read"},
{"title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid,\n  Resource-Efficient Scientific Computation", "author": "Reza T. Batley and Sourav Saha", "abstract": "  Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond.\n", "link": "http://arxiv.org/abs/2505.13315v1", "date": "2025-05-19", "relevancy": 2.525, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.517}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.505}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KHRONOS%3A%20a%20Kernel-Based%20Neural%20Architecture%20for%20Rapid%2C%0A%20%20Resource-Efficient%20Scientific%20Computation&body=Title%3A%20KHRONOS%3A%20a%20Kernel-Based%20Neural%20Architecture%20for%20Rapid%2C%0A%20%20Resource-Efficient%20Scientific%20Computation%0AAuthor%3A%20Reza%20T.%20Batley%20and%20Sourav%20Saha%0AAbstract%3A%20%20%20Contemporary%20models%20of%20high%20dimensional%20physical%20systems%20are%20constrained%20by%0Athe%20curse%20of%20dimensionality%20and%20a%20reliance%20on%20dense%20data.%20We%20introduce%20KHRONOS%0A%28Kernel%20Expansion%20Hierarchy%20for%20Reduced%20Order%2C%20Neural%20Optimized%20Surrogates%29%2C%20an%0AAI%20framework%20for%20model%20based%2C%20model%20free%20and%20model%20inversion%20tasks.%20KHRONOS%0Aconstructs%20continuously%20differentiable%20target%20fields%20with%20a%20hierarchical%0Acomposition%20of%20per-dimension%20kernel%20expansions%2C%20which%20are%20tensorized%20into%20modes%0Aand%20then%20superposed.%20We%20evaluate%20KHRONOS%20on%20a%20canonical%202D%2C%20Poisson%20equation%0Abenchmark%3A%20across%2016%20to%20512%20degrees%20of%20freedom%20%28DoFs%29%2C%20it%20obtained%20L2%20square%0Aerrors%20of%205e-4%20down%20to%206e-10.%20This%20represents%20a%20100%20time%20gain%20over%20Kolmogorov%0AArnold%20Networks%20%28which%20itself%20reports%20a%20100%20times%20improvement%20on%20MLPs/PINNs%0Awith%20100%20times%20fewer%20parameters%29%20when%20controlling%20for%20the%20number%20of%20parameters.%0AThis%20also%20represents%20a%201e4%20times%20improvement%20in%20L2%20square%20error%20compared%20to%0Astandard%20linear%20FEM%20at%20comparable%20DoFs.%20Inference%20complexity%20is%20dominated%20by%0Ainner%20products%2C%20yielding%20sub-millisecond%20full-field%20predictions%20that%20scale%20to%0Aan%20arbitrary%20resolution.%20For%20inverse%20problems%2C%20KHRONOS%20facilitates%20rapid%2C%0Aiterative%20level%20set%20recovery%20in%20only%20a%20few%20forward%20evaluations%2C%20with%0Asub-microsecond%20per%20sample%20latency.%20KHRONOS%20scalability%2C%20expressivity%2C%20and%0Ainterpretability%20open%20new%20avenues%20in%20constrained%20edge%20computing%2C%20online%0Acontrol%2C%20computer%20vision%2C%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKHRONOS%253A%2520a%2520Kernel-Based%2520Neural%2520Architecture%2520for%2520Rapid%252C%250A%2520%2520Resource-Efficient%2520Scientific%2520Computation%26entry.906535625%3DReza%2520T.%2520Batley%2520and%2520Sourav%2520Saha%26entry.1292438233%3D%2520%2520Contemporary%2520models%2520of%2520high%2520dimensional%2520physical%2520systems%2520are%2520constrained%2520by%250Athe%2520curse%2520of%2520dimensionality%2520and%2520a%2520reliance%2520on%2520dense%2520data.%2520We%2520introduce%2520KHRONOS%250A%2528Kernel%2520Expansion%2520Hierarchy%2520for%2520Reduced%2520Order%252C%2520Neural%2520Optimized%2520Surrogates%2529%252C%2520an%250AAI%2520framework%2520for%2520model%2520based%252C%2520model%2520free%2520and%2520model%2520inversion%2520tasks.%2520KHRONOS%250Aconstructs%2520continuously%2520differentiable%2520target%2520fields%2520with%2520a%2520hierarchical%250Acomposition%2520of%2520per-dimension%2520kernel%2520expansions%252C%2520which%2520are%2520tensorized%2520into%2520modes%250Aand%2520then%2520superposed.%2520We%2520evaluate%2520KHRONOS%2520on%2520a%2520canonical%25202D%252C%2520Poisson%2520equation%250Abenchmark%253A%2520across%252016%2520to%2520512%2520degrees%2520of%2520freedom%2520%2528DoFs%2529%252C%2520it%2520obtained%2520L2%2520square%250Aerrors%2520of%25205e-4%2520down%2520to%25206e-10.%2520This%2520represents%2520a%2520100%2520time%2520gain%2520over%2520Kolmogorov%250AArnold%2520Networks%2520%2528which%2520itself%2520reports%2520a%2520100%2520times%2520improvement%2520on%2520MLPs/PINNs%250Awith%2520100%2520times%2520fewer%2520parameters%2529%2520when%2520controlling%2520for%2520the%2520number%2520of%2520parameters.%250AThis%2520also%2520represents%2520a%25201e4%2520times%2520improvement%2520in%2520L2%2520square%2520error%2520compared%2520to%250Astandard%2520linear%2520FEM%2520at%2520comparable%2520DoFs.%2520Inference%2520complexity%2520is%2520dominated%2520by%250Ainner%2520products%252C%2520yielding%2520sub-millisecond%2520full-field%2520predictions%2520that%2520scale%2520to%250Aan%2520arbitrary%2520resolution.%2520For%2520inverse%2520problems%252C%2520KHRONOS%2520facilitates%2520rapid%252C%250Aiterative%2520level%2520set%2520recovery%2520in%2520only%2520a%2520few%2520forward%2520evaluations%252C%2520with%250Asub-microsecond%2520per%2520sample%2520latency.%2520KHRONOS%2520scalability%252C%2520expressivity%252C%2520and%250Ainterpretability%2520open%2520new%2520avenues%2520in%2520constrained%2520edge%2520computing%252C%2520online%250Acontrol%252C%2520computer%2520vision%252C%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KHRONOS%3A%20a%20Kernel-Based%20Neural%20Architecture%20for%20Rapid%2C%0A%20%20Resource-Efficient%20Scientific%20Computation&entry.906535625=Reza%20T.%20Batley%20and%20Sourav%20Saha&entry.1292438233=%20%20Contemporary%20models%20of%20high%20dimensional%20physical%20systems%20are%20constrained%20by%0Athe%20curse%20of%20dimensionality%20and%20a%20reliance%20on%20dense%20data.%20We%20introduce%20KHRONOS%0A%28Kernel%20Expansion%20Hierarchy%20for%20Reduced%20Order%2C%20Neural%20Optimized%20Surrogates%29%2C%20an%0AAI%20framework%20for%20model%20based%2C%20model%20free%20and%20model%20inversion%20tasks.%20KHRONOS%0Aconstructs%20continuously%20differentiable%20target%20fields%20with%20a%20hierarchical%0Acomposition%20of%20per-dimension%20kernel%20expansions%2C%20which%20are%20tensorized%20into%20modes%0Aand%20then%20superposed.%20We%20evaluate%20KHRONOS%20on%20a%20canonical%202D%2C%20Poisson%20equation%0Abenchmark%3A%20across%2016%20to%20512%20degrees%20of%20freedom%20%28DoFs%29%2C%20it%20obtained%20L2%20square%0Aerrors%20of%205e-4%20down%20to%206e-10.%20This%20represents%20a%20100%20time%20gain%20over%20Kolmogorov%0AArnold%20Networks%20%28which%20itself%20reports%20a%20100%20times%20improvement%20on%20MLPs/PINNs%0Awith%20100%20times%20fewer%20parameters%29%20when%20controlling%20for%20the%20number%20of%20parameters.%0AThis%20also%20represents%20a%201e4%20times%20improvement%20in%20L2%20square%20error%20compared%20to%0Astandard%20linear%20FEM%20at%20comparable%20DoFs.%20Inference%20complexity%20is%20dominated%20by%0Ainner%20products%2C%20yielding%20sub-millisecond%20full-field%20predictions%20that%20scale%20to%0Aan%20arbitrary%20resolution.%20For%20inverse%20problems%2C%20KHRONOS%20facilitates%20rapid%2C%0Aiterative%20level%20set%20recovery%20in%20only%20a%20few%20forward%20evaluations%2C%20with%0Asub-microsecond%20per%20sample%20latency.%20KHRONOS%20scalability%2C%20expressivity%2C%20and%0Ainterpretability%20open%20new%20avenues%20in%20constrained%20edge%20computing%2C%20online%0Acontrol%2C%20computer%20vision%2C%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13315v1&entry.124074799=Read"},
{"title": "DICE: Device-level Integrated Circuits Encoder with Graph Contrastive\n  Pretraining", "author": "Sungyoung Lee and Ziyi Wang and Seunggeun Kim and Taekyun Lee and Yao Lai and David Z. Pan", "abstract": "  Pretraining models with unsupervised graph representation learning has led to\nsignificant advancements in domains such as social network analysis, molecular\ndesign, and electronic design automation (EDA). However, prior work in EDA has\nmainly focused on pretraining models for digital circuits, overlooking analog\nand mixed-signal circuits. To bridge this gap, we introduce DICE, a\nDevice-level Integrated Circuits Encoder, which is the first graph neural\nnetwork (GNN) pretrained via self-supervised learning specifically tailored for\ngraph-level prediction tasks in both analog and digital circuits. DICE adopts a\nsimulation-free pretraining approach based on graph contrastive learning,\nleveraging two novel graph augmentation techniques. Experimental results\ndemonstrate substantial performance improvements across three downstream tasks,\nhighlighting the effectiveness of DICE for both analog and digital circuits.\nThe code is available at github.com/brianlsy98/DICE.\n", "link": "http://arxiv.org/abs/2502.08949v2", "date": "2025-05-19", "relevancy": 2.5143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DICE%3A%20Device-level%20Integrated%20Circuits%20Encoder%20with%20Graph%20Contrastive%0A%20%20Pretraining&body=Title%3A%20DICE%3A%20Device-level%20Integrated%20Circuits%20Encoder%20with%20Graph%20Contrastive%0A%20%20Pretraining%0AAuthor%3A%20Sungyoung%20Lee%20and%20Ziyi%20Wang%20and%20Seunggeun%20Kim%20and%20Taekyun%20Lee%20and%20Yao%20Lai%20and%20David%20Z.%20Pan%0AAbstract%3A%20%20%20Pretraining%20models%20with%20unsupervised%20graph%20representation%20learning%20has%20led%20to%0Asignificant%20advancements%20in%20domains%20such%20as%20social%20network%20analysis%2C%20molecular%0Adesign%2C%20and%20electronic%20design%20automation%20%28EDA%29.%20However%2C%20prior%20work%20in%20EDA%20has%0Amainly%20focused%20on%20pretraining%20models%20for%20digital%20circuits%2C%20overlooking%20analog%0Aand%20mixed-signal%20circuits.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DICE%2C%20a%0ADevice-level%20Integrated%20Circuits%20Encoder%2C%20which%20is%20the%20first%20graph%20neural%0Anetwork%20%28GNN%29%20pretrained%20via%20self-supervised%20learning%20specifically%20tailored%20for%0Agraph-level%20prediction%20tasks%20in%20both%20analog%20and%20digital%20circuits.%20DICE%20adopts%20a%0Asimulation-free%20pretraining%20approach%20based%20on%20graph%20contrastive%20learning%2C%0Aleveraging%20two%20novel%20graph%20augmentation%20techniques.%20Experimental%20results%0Ademonstrate%20substantial%20performance%20improvements%20across%20three%20downstream%20tasks%2C%0Ahighlighting%20the%20effectiveness%20of%20DICE%20for%20both%20analog%20and%20digital%20circuits.%0AThe%20code%20is%20available%20at%20github.com/brianlsy98/DICE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08949v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDICE%253A%2520Device-level%2520Integrated%2520Circuits%2520Encoder%2520with%2520Graph%2520Contrastive%250A%2520%2520Pretraining%26entry.906535625%3DSungyoung%2520Lee%2520and%2520Ziyi%2520Wang%2520and%2520Seunggeun%2520Kim%2520and%2520Taekyun%2520Lee%2520and%2520Yao%2520Lai%2520and%2520David%2520Z.%2520Pan%26entry.1292438233%3D%2520%2520Pretraining%2520models%2520with%2520unsupervised%2520graph%2520representation%2520learning%2520has%2520led%2520to%250Asignificant%2520advancements%2520in%2520domains%2520such%2520as%2520social%2520network%2520analysis%252C%2520molecular%250Adesign%252C%2520and%2520electronic%2520design%2520automation%2520%2528EDA%2529.%2520However%252C%2520prior%2520work%2520in%2520EDA%2520has%250Amainly%2520focused%2520on%2520pretraining%2520models%2520for%2520digital%2520circuits%252C%2520overlooking%2520analog%250Aand%2520mixed-signal%2520circuits.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520DICE%252C%2520a%250ADevice-level%2520Integrated%2520Circuits%2520Encoder%252C%2520which%2520is%2520the%2520first%2520graph%2520neural%250Anetwork%2520%2528GNN%2529%2520pretrained%2520via%2520self-supervised%2520learning%2520specifically%2520tailored%2520for%250Agraph-level%2520prediction%2520tasks%2520in%2520both%2520analog%2520and%2520digital%2520circuits.%2520DICE%2520adopts%2520a%250Asimulation-free%2520pretraining%2520approach%2520based%2520on%2520graph%2520contrastive%2520learning%252C%250Aleveraging%2520two%2520novel%2520graph%2520augmentation%2520techniques.%2520Experimental%2520results%250Ademonstrate%2520substantial%2520performance%2520improvements%2520across%2520three%2520downstream%2520tasks%252C%250Ahighlighting%2520the%2520effectiveness%2520of%2520DICE%2520for%2520both%2520analog%2520and%2520digital%2520circuits.%250AThe%2520code%2520is%2520available%2520at%2520github.com/brianlsy98/DICE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08949v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DICE%3A%20Device-level%20Integrated%20Circuits%20Encoder%20with%20Graph%20Contrastive%0A%20%20Pretraining&entry.906535625=Sungyoung%20Lee%20and%20Ziyi%20Wang%20and%20Seunggeun%20Kim%20and%20Taekyun%20Lee%20and%20Yao%20Lai%20and%20David%20Z.%20Pan&entry.1292438233=%20%20Pretraining%20models%20with%20unsupervised%20graph%20representation%20learning%20has%20led%20to%0Asignificant%20advancements%20in%20domains%20such%20as%20social%20network%20analysis%2C%20molecular%0Adesign%2C%20and%20electronic%20design%20automation%20%28EDA%29.%20However%2C%20prior%20work%20in%20EDA%20has%0Amainly%20focused%20on%20pretraining%20models%20for%20digital%20circuits%2C%20overlooking%20analog%0Aand%20mixed-signal%20circuits.%20To%20bridge%20this%20gap%2C%20we%20introduce%20DICE%2C%20a%0ADevice-level%20Integrated%20Circuits%20Encoder%2C%20which%20is%20the%20first%20graph%20neural%0Anetwork%20%28GNN%29%20pretrained%20via%20self-supervised%20learning%20specifically%20tailored%20for%0Agraph-level%20prediction%20tasks%20in%20both%20analog%20and%20digital%20circuits.%20DICE%20adopts%20a%0Asimulation-free%20pretraining%20approach%20based%20on%20graph%20contrastive%20learning%2C%0Aleveraging%20two%20novel%20graph%20augmentation%20techniques.%20Experimental%20results%0Ademonstrate%20substantial%20performance%20improvements%20across%20three%20downstream%20tasks%2C%0Ahighlighting%20the%20effectiveness%20of%20DICE%20for%20both%20analog%20and%20digital%20circuits.%0AThe%20code%20is%20available%20at%20github.com/brianlsy98/DICE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08949v2&entry.124074799=Read"},
{"title": "Automatic Complementary Separation Pruning Toward Lightweight CNNs", "author": "David Levin and Gonen Singer", "abstract": "  In this paper, we present Automatic Complementary Separation Pruning (ACSP),\na novel and fully automated pruning method for convolutional neural networks.\nACSP integrates the strengths of both structured pruning and activation-based\npruning, enabling the efficient removal of entire components such as neurons\nand channels while leveraging activations to identify and retain the most\nrelevant components. Our approach is designed specifically for supervised\nlearning tasks, where we construct a graph space that encodes the separation\ncapabilities of each component with respect to all class pairs. By employing\ncomplementary selection principles and utilizing a clustering algorithm, ACSP\nensures that the selected components maintain diverse and complementary\nseparation capabilities, reducing redundancy and maintaining high network\nperformance. The method automatically determines the optimal subset of\ncomponents in each layer, utilizing a knee-finding algorithm to select the\nminimal subset that preserves performance without requiring user-defined\npruning volumes. Extensive experiments on multiple architectures, including\nVGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,\nand ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared\nto other methods while significantly reducing computational costs. This fully\nautomated approach not only enhances scalability but also makes ACSP especially\npractical for real-world deployment by eliminating the need for manually\ndefining the pruning volume.\n", "link": "http://arxiv.org/abs/2505.13225v1", "date": "2025-05-19", "relevancy": 2.5143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4959}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Complementary%20Separation%20Pruning%20Toward%20Lightweight%20CNNs&body=Title%3A%20Automatic%20Complementary%20Separation%20Pruning%20Toward%20Lightweight%20CNNs%0AAuthor%3A%20David%20Levin%20and%20Gonen%20Singer%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20Automatic%20Complementary%20Separation%20Pruning%20%28ACSP%29%2C%0Aa%20novel%20and%20fully%20automated%20pruning%20method%20for%20convolutional%20neural%20networks.%0AACSP%20integrates%20the%20strengths%20of%20both%20structured%20pruning%20and%20activation-based%0Apruning%2C%20enabling%20the%20efficient%20removal%20of%20entire%20components%20such%20as%20neurons%0Aand%20channels%20while%20leveraging%20activations%20to%20identify%20and%20retain%20the%20most%0Arelevant%20components.%20Our%20approach%20is%20designed%20specifically%20for%20supervised%0Alearning%20tasks%2C%20where%20we%20construct%20a%20graph%20space%20that%20encodes%20the%20separation%0Acapabilities%20of%20each%20component%20with%20respect%20to%20all%20class%20pairs.%20By%20employing%0Acomplementary%20selection%20principles%20and%20utilizing%20a%20clustering%20algorithm%2C%20ACSP%0Aensures%20that%20the%20selected%20components%20maintain%20diverse%20and%20complementary%0Aseparation%20capabilities%2C%20reducing%20redundancy%20and%20maintaining%20high%20network%0Aperformance.%20The%20method%20automatically%20determines%20the%20optimal%20subset%20of%0Acomponents%20in%20each%20layer%2C%20utilizing%20a%20knee-finding%20algorithm%20to%20select%20the%0Aminimal%20subset%20that%20preserves%20performance%20without%20requiring%20user-defined%0Apruning%20volumes.%20Extensive%20experiments%20on%20multiple%20architectures%2C%20including%0AVGG-16%2C%20ResNet-50%2C%20and%20MobileNet-V2%2C%20across%20datasets%20like%20CIFAR-10%2C%20CIFAR-100%2C%0Aand%20ImageNet-1K%2C%20demonstrate%20that%20ACSP%20achieves%20competitive%20accuracy%20compared%0Ato%20other%20methods%20while%20significantly%20reducing%20computational%20costs.%20This%20fully%0Aautomated%20approach%20not%20only%20enhances%20scalability%20but%20also%20makes%20ACSP%20especially%0Apractical%20for%20real-world%20deployment%20by%20eliminating%20the%20need%20for%20manually%0Adefining%20the%20pruning%20volume.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Complementary%2520Separation%2520Pruning%2520Toward%2520Lightweight%2520CNNs%26entry.906535625%3DDavid%2520Levin%2520and%2520Gonen%2520Singer%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520Automatic%2520Complementary%2520Separation%2520Pruning%2520%2528ACSP%2529%252C%250Aa%2520novel%2520and%2520fully%2520automated%2520pruning%2520method%2520for%2520convolutional%2520neural%2520networks.%250AACSP%2520integrates%2520the%2520strengths%2520of%2520both%2520structured%2520pruning%2520and%2520activation-based%250Apruning%252C%2520enabling%2520the%2520efficient%2520removal%2520of%2520entire%2520components%2520such%2520as%2520neurons%250Aand%2520channels%2520while%2520leveraging%2520activations%2520to%2520identify%2520and%2520retain%2520the%2520most%250Arelevant%2520components.%2520Our%2520approach%2520is%2520designed%2520specifically%2520for%2520supervised%250Alearning%2520tasks%252C%2520where%2520we%2520construct%2520a%2520graph%2520space%2520that%2520encodes%2520the%2520separation%250Acapabilities%2520of%2520each%2520component%2520with%2520respect%2520to%2520all%2520class%2520pairs.%2520By%2520employing%250Acomplementary%2520selection%2520principles%2520and%2520utilizing%2520a%2520clustering%2520algorithm%252C%2520ACSP%250Aensures%2520that%2520the%2520selected%2520components%2520maintain%2520diverse%2520and%2520complementary%250Aseparation%2520capabilities%252C%2520reducing%2520redundancy%2520and%2520maintaining%2520high%2520network%250Aperformance.%2520The%2520method%2520automatically%2520determines%2520the%2520optimal%2520subset%2520of%250Acomponents%2520in%2520each%2520layer%252C%2520utilizing%2520a%2520knee-finding%2520algorithm%2520to%2520select%2520the%250Aminimal%2520subset%2520that%2520preserves%2520performance%2520without%2520requiring%2520user-defined%250Apruning%2520volumes.%2520Extensive%2520experiments%2520on%2520multiple%2520architectures%252C%2520including%250AVGG-16%252C%2520ResNet-50%252C%2520and%2520MobileNet-V2%252C%2520across%2520datasets%2520like%2520CIFAR-10%252C%2520CIFAR-100%252C%250Aand%2520ImageNet-1K%252C%2520demonstrate%2520that%2520ACSP%2520achieves%2520competitive%2520accuracy%2520compared%250Ato%2520other%2520methods%2520while%2520significantly%2520reducing%2520computational%2520costs.%2520This%2520fully%250Aautomated%2520approach%2520not%2520only%2520enhances%2520scalability%2520but%2520also%2520makes%2520ACSP%2520especially%250Apractical%2520for%2520real-world%2520deployment%2520by%2520eliminating%2520the%2520need%2520for%2520manually%250Adefining%2520the%2520pruning%2520volume.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Complementary%20Separation%20Pruning%20Toward%20Lightweight%20CNNs&entry.906535625=David%20Levin%20and%20Gonen%20Singer&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20Automatic%20Complementary%20Separation%20Pruning%20%28ACSP%29%2C%0Aa%20novel%20and%20fully%20automated%20pruning%20method%20for%20convolutional%20neural%20networks.%0AACSP%20integrates%20the%20strengths%20of%20both%20structured%20pruning%20and%20activation-based%0Apruning%2C%20enabling%20the%20efficient%20removal%20of%20entire%20components%20such%20as%20neurons%0Aand%20channels%20while%20leveraging%20activations%20to%20identify%20and%20retain%20the%20most%0Arelevant%20components.%20Our%20approach%20is%20designed%20specifically%20for%20supervised%0Alearning%20tasks%2C%20where%20we%20construct%20a%20graph%20space%20that%20encodes%20the%20separation%0Acapabilities%20of%20each%20component%20with%20respect%20to%20all%20class%20pairs.%20By%20employing%0Acomplementary%20selection%20principles%20and%20utilizing%20a%20clustering%20algorithm%2C%20ACSP%0Aensures%20that%20the%20selected%20components%20maintain%20diverse%20and%20complementary%0Aseparation%20capabilities%2C%20reducing%20redundancy%20and%20maintaining%20high%20network%0Aperformance.%20The%20method%20automatically%20determines%20the%20optimal%20subset%20of%0Acomponents%20in%20each%20layer%2C%20utilizing%20a%20knee-finding%20algorithm%20to%20select%20the%0Aminimal%20subset%20that%20preserves%20performance%20without%20requiring%20user-defined%0Apruning%20volumes.%20Extensive%20experiments%20on%20multiple%20architectures%2C%20including%0AVGG-16%2C%20ResNet-50%2C%20and%20MobileNet-V2%2C%20across%20datasets%20like%20CIFAR-10%2C%20CIFAR-100%2C%0Aand%20ImageNet-1K%2C%20demonstrate%20that%20ACSP%20achieves%20competitive%20accuracy%20compared%0Ato%20other%20methods%20while%20significantly%20reducing%20computational%20costs.%20This%20fully%0Aautomated%20approach%20not%20only%20enhances%20scalability%20but%20also%20makes%20ACSP%20especially%0Apractical%20for%20real-world%20deployment%20by%20eliminating%20the%20need%20for%20manually%0Adefining%20the%20pruning%20volume.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13225v1&entry.124074799=Read"},
{"title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "author": "Anton Ehrmanntraut and Julia Wunderle and Jan Pfister and Fotis Jannidis and Andreas Hotho", "abstract": "  Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.\n", "link": "http://arxiv.org/abs/2505.13136v1", "date": "2025-05-19", "relevancy": 2.5115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ModernGBERT%3A%20German-only%201B%20Encoder%20Model%20Trained%20from%20Scratch&body=Title%3A%20ModernGBERT%3A%20German-only%201B%20Encoder%20Model%20Trained%20from%20Scratch%0AAuthor%3A%20Anton%20Ehrmanntraut%20and%20Julia%20Wunderle%20and%20Jan%20Pfister%20and%20Fotis%20Jannidis%20and%20Andreas%20Hotho%0AAbstract%3A%20%20%20Despite%20the%20prominence%20of%20decoder-only%20language%20models%2C%20encoders%20remain%0Acrucial%20for%20resource-constrained%20applications.%20We%20introduce%20ModernGBERT%20%28134M%2C%0A1B%29%2C%20a%20fully%20transparent%20family%20of%20German%20encoder%20models%20trained%20from%20scratch%2C%0Aincorporating%20architectural%20innovations%20from%20ModernBERT.%20To%20evaluate%20the%0Apractical%20trade-offs%20of%20training%20encoders%20from%20scratch%2C%20we%20also%20present%0ALL%5C%22aMmlein2Vec%20%28120M%2C%201B%2C%207B%29%2C%20a%20family%20of%20encoders%20derived%20from%20German%0Adecoder-only%20models%20via%20LLM2Vec.%20We%20benchmark%20all%20models%20on%20natural%20language%0Aunderstanding%2C%20text%20embedding%2C%20and%20long-context%20reasoning%20tasks%2C%20enabling%20a%0Acontrolled%20comparison%20between%20dedicated%20encoders%20and%20converted%20decoders.%20Our%0Aresults%20show%20that%20ModernGBERT%201B%20outperforms%20prior%20state-of-the-art%20German%0Aencoders%20as%20well%20as%20encoders%20adapted%20via%20LLM2Vec%2C%20with%20regard%20to%20performance%0Aand%20parameter-efficiency.%20All%20models%2C%20training%20data%2C%20checkpoints%20and%20code%20are%0Apublicly%20available%2C%20advancing%20the%20German%20NLP%20ecosystem%20with%20transparent%2C%0Ahigh-performance%20encoder%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModernGBERT%253A%2520German-only%25201B%2520Encoder%2520Model%2520Trained%2520from%2520Scratch%26entry.906535625%3DAnton%2520Ehrmanntraut%2520and%2520Julia%2520Wunderle%2520and%2520Jan%2520Pfister%2520and%2520Fotis%2520Jannidis%2520and%2520Andreas%2520Hotho%26entry.1292438233%3D%2520%2520Despite%2520the%2520prominence%2520of%2520decoder-only%2520language%2520models%252C%2520encoders%2520remain%250Acrucial%2520for%2520resource-constrained%2520applications.%2520We%2520introduce%2520ModernGBERT%2520%2528134M%252C%250A1B%2529%252C%2520a%2520fully%2520transparent%2520family%2520of%2520German%2520encoder%2520models%2520trained%2520from%2520scratch%252C%250Aincorporating%2520architectural%2520innovations%2520from%2520ModernBERT.%2520To%2520evaluate%2520the%250Apractical%2520trade-offs%2520of%2520training%2520encoders%2520from%2520scratch%252C%2520we%2520also%2520present%250ALL%255C%2522aMmlein2Vec%2520%2528120M%252C%25201B%252C%25207B%2529%252C%2520a%2520family%2520of%2520encoders%2520derived%2520from%2520German%250Adecoder-only%2520models%2520via%2520LLM2Vec.%2520We%2520benchmark%2520all%2520models%2520on%2520natural%2520language%250Aunderstanding%252C%2520text%2520embedding%252C%2520and%2520long-context%2520reasoning%2520tasks%252C%2520enabling%2520a%250Acontrolled%2520comparison%2520between%2520dedicated%2520encoders%2520and%2520converted%2520decoders.%2520Our%250Aresults%2520show%2520that%2520ModernGBERT%25201B%2520outperforms%2520prior%2520state-of-the-art%2520German%250Aencoders%2520as%2520well%2520as%2520encoders%2520adapted%2520via%2520LLM2Vec%252C%2520with%2520regard%2520to%2520performance%250Aand%2520parameter-efficiency.%2520All%2520models%252C%2520training%2520data%252C%2520checkpoints%2520and%2520code%2520are%250Apublicly%2520available%252C%2520advancing%2520the%2520German%2520NLP%2520ecosystem%2520with%2520transparent%252C%250Ahigh-performance%2520encoder%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ModernGBERT%3A%20German-only%201B%20Encoder%20Model%20Trained%20from%20Scratch&entry.906535625=Anton%20Ehrmanntraut%20and%20Julia%20Wunderle%20and%20Jan%20Pfister%20and%20Fotis%20Jannidis%20and%20Andreas%20Hotho&entry.1292438233=%20%20Despite%20the%20prominence%20of%20decoder-only%20language%20models%2C%20encoders%20remain%0Acrucial%20for%20resource-constrained%20applications.%20We%20introduce%20ModernGBERT%20%28134M%2C%0A1B%29%2C%20a%20fully%20transparent%20family%20of%20German%20encoder%20models%20trained%20from%20scratch%2C%0Aincorporating%20architectural%20innovations%20from%20ModernBERT.%20To%20evaluate%20the%0Apractical%20trade-offs%20of%20training%20encoders%20from%20scratch%2C%20we%20also%20present%0ALL%5C%22aMmlein2Vec%20%28120M%2C%201B%2C%207B%29%2C%20a%20family%20of%20encoders%20derived%20from%20German%0Adecoder-only%20models%20via%20LLM2Vec.%20We%20benchmark%20all%20models%20on%20natural%20language%0Aunderstanding%2C%20text%20embedding%2C%20and%20long-context%20reasoning%20tasks%2C%20enabling%20a%0Acontrolled%20comparison%20between%20dedicated%20encoders%20and%20converted%20decoders.%20Our%0Aresults%20show%20that%20ModernGBERT%201B%20outperforms%20prior%20state-of-the-art%20German%0Aencoders%20as%20well%20as%20encoders%20adapted%20via%20LLM2Vec%2C%20with%20regard%20to%20performance%0Aand%20parameter-efficiency.%20All%20models%2C%20training%20data%2C%20checkpoints%20and%20code%20are%0Apublicly%20available%2C%20advancing%20the%20German%20NLP%20ecosystem%20with%20transparent%2C%0Ahigh-performance%20encoder%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13136v1&entry.124074799=Read"},
{"title": "BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological\n  Condition Identification", "author": "Jiaxing Xu and Kai He and Yue Tang and Wei Li and Mengcheng Lan and Xia Dong and Yiping Ke and Mengling Feng", "abstract": "  Neurological conditions, such as Alzheimer's Disease, are challenging to\ndiagnose, particularly in the early stages where symptoms closely resemble\nhealthy controls. Existing brain network analysis methods primarily focus on\ngraph-based models that rely solely on imaging data, which may overlook\nimportant non-imaging factors and limit the model's predictive power and\ninterpretability. In this paper, we present BrainPrompt, an innovative\nframework that enhances Graph Neural Networks (GNNs) by integrating Large\nLanguage Models (LLMs) with knowledge-driven prompts, enabling more effective\ncapture of complex, non-imaging information and external knowledge for\nneurological disease identification. BrainPrompt integrates three types of\nknowledge-driven prompts: (1) ROI-level prompts to encode the identity and\nfunction of each brain region, (2) subject-level prompts that incorporate\ndemographic information, and (3) disease-level prompts to capture the temporal\nprogression of disease. By leveraging these multi-level prompts, BrainPrompt\neffectively harnesses knowledge-enhanced multi-modal information from LLMs,\nenhancing the model's capability to predict neurological disease stages and\nmeanwhile offers more interpretable results. We evaluate BrainPrompt on two\nresting-state functional Magnetic Resonance Imaging (fMRI) datasets from\nneurological disorders, showing its superiority over state-of-the-art methods.\nAdditionally, a biomarker study demonstrates the framework's ability to extract\nvaluable and interpretable information aligned with domain knowledge in\nneuroscience. The code is available at\nhttps://github.com/AngusMonroe/BrainPrompt\n", "link": "http://arxiv.org/abs/2504.16096v2", "date": "2025-05-19", "relevancy": 2.4954, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrainPrompt%3A%20Multi-Level%20Brain%20Prompt%20Enhancement%20for%20Neurological%0A%20%20Condition%20Identification&body=Title%3A%20BrainPrompt%3A%20Multi-Level%20Brain%20Prompt%20Enhancement%20for%20Neurological%0A%20%20Condition%20Identification%0AAuthor%3A%20Jiaxing%20Xu%20and%20Kai%20He%20and%20Yue%20Tang%20and%20Wei%20Li%20and%20Mengcheng%20Lan%20and%20Xia%20Dong%20and%20Yiping%20Ke%20and%20Mengling%20Feng%0AAbstract%3A%20%20%20Neurological%20conditions%2C%20such%20as%20Alzheimer%27s%20Disease%2C%20are%20challenging%20to%0Adiagnose%2C%20particularly%20in%20the%20early%20stages%20where%20symptoms%20closely%20resemble%0Ahealthy%20controls.%20Existing%20brain%20network%20analysis%20methods%20primarily%20focus%20on%0Agraph-based%20models%20that%20rely%20solely%20on%20imaging%20data%2C%20which%20may%20overlook%0Aimportant%20non-imaging%20factors%20and%20limit%20the%20model%27s%20predictive%20power%20and%0Ainterpretability.%20In%20this%20paper%2C%20we%20present%20BrainPrompt%2C%20an%20innovative%0Aframework%20that%20enhances%20Graph%20Neural%20Networks%20%28GNNs%29%20by%20integrating%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20knowledge-driven%20prompts%2C%20enabling%20more%20effective%0Acapture%20of%20complex%2C%20non-imaging%20information%20and%20external%20knowledge%20for%0Aneurological%20disease%20identification.%20BrainPrompt%20integrates%20three%20types%20of%0Aknowledge-driven%20prompts%3A%20%281%29%20ROI-level%20prompts%20to%20encode%20the%20identity%20and%0Afunction%20of%20each%20brain%20region%2C%20%282%29%20subject-level%20prompts%20that%20incorporate%0Ademographic%20information%2C%20and%20%283%29%20disease-level%20prompts%20to%20capture%20the%20temporal%0Aprogression%20of%20disease.%20By%20leveraging%20these%20multi-level%20prompts%2C%20BrainPrompt%0Aeffectively%20harnesses%20knowledge-enhanced%20multi-modal%20information%20from%20LLMs%2C%0Aenhancing%20the%20model%27s%20capability%20to%20predict%20neurological%20disease%20stages%20and%0Ameanwhile%20offers%20more%20interpretable%20results.%20We%20evaluate%20BrainPrompt%20on%20two%0Aresting-state%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20datasets%20from%0Aneurological%20disorders%2C%20showing%20its%20superiority%20over%20state-of-the-art%20methods.%0AAdditionally%2C%20a%20biomarker%20study%20demonstrates%20the%20framework%27s%20ability%20to%20extract%0Avaluable%20and%20interpretable%20information%20aligned%20with%20domain%20knowledge%20in%0Aneuroscience.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AngusMonroe/BrainPrompt%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainPrompt%253A%2520Multi-Level%2520Brain%2520Prompt%2520Enhancement%2520for%2520Neurological%250A%2520%2520Condition%2520Identification%26entry.906535625%3DJiaxing%2520Xu%2520and%2520Kai%2520He%2520and%2520Yue%2520Tang%2520and%2520Wei%2520Li%2520and%2520Mengcheng%2520Lan%2520and%2520Xia%2520Dong%2520and%2520Yiping%2520Ke%2520and%2520Mengling%2520Feng%26entry.1292438233%3D%2520%2520Neurological%2520conditions%252C%2520such%2520as%2520Alzheimer%2527s%2520Disease%252C%2520are%2520challenging%2520to%250Adiagnose%252C%2520particularly%2520in%2520the%2520early%2520stages%2520where%2520symptoms%2520closely%2520resemble%250Ahealthy%2520controls.%2520Existing%2520brain%2520network%2520analysis%2520methods%2520primarily%2520focus%2520on%250Agraph-based%2520models%2520that%2520rely%2520solely%2520on%2520imaging%2520data%252C%2520which%2520may%2520overlook%250Aimportant%2520non-imaging%2520factors%2520and%2520limit%2520the%2520model%2527s%2520predictive%2520power%2520and%250Ainterpretability.%2520In%2520this%2520paper%252C%2520we%2520present%2520BrainPrompt%252C%2520an%2520innovative%250Aframework%2520that%2520enhances%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520by%2520integrating%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520with%2520knowledge-driven%2520prompts%252C%2520enabling%2520more%2520effective%250Acapture%2520of%2520complex%252C%2520non-imaging%2520information%2520and%2520external%2520knowledge%2520for%250Aneurological%2520disease%2520identification.%2520BrainPrompt%2520integrates%2520three%2520types%2520of%250Aknowledge-driven%2520prompts%253A%2520%25281%2529%2520ROI-level%2520prompts%2520to%2520encode%2520the%2520identity%2520and%250Afunction%2520of%2520each%2520brain%2520region%252C%2520%25282%2529%2520subject-level%2520prompts%2520that%2520incorporate%250Ademographic%2520information%252C%2520and%2520%25283%2529%2520disease-level%2520prompts%2520to%2520capture%2520the%2520temporal%250Aprogression%2520of%2520disease.%2520By%2520leveraging%2520these%2520multi-level%2520prompts%252C%2520BrainPrompt%250Aeffectively%2520harnesses%2520knowledge-enhanced%2520multi-modal%2520information%2520from%2520LLMs%252C%250Aenhancing%2520the%2520model%2527s%2520capability%2520to%2520predict%2520neurological%2520disease%2520stages%2520and%250Ameanwhile%2520offers%2520more%2520interpretable%2520results.%2520We%2520evaluate%2520BrainPrompt%2520on%2520two%250Aresting-state%2520functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%2520datasets%2520from%250Aneurological%2520disorders%252C%2520showing%2520its%2520superiority%2520over%2520state-of-the-art%2520methods.%250AAdditionally%252C%2520a%2520biomarker%2520study%2520demonstrates%2520the%2520framework%2527s%2520ability%2520to%2520extract%250Avaluable%2520and%2520interpretable%2520information%2520aligned%2520with%2520domain%2520knowledge%2520in%250Aneuroscience.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AngusMonroe/BrainPrompt%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrainPrompt%3A%20Multi-Level%20Brain%20Prompt%20Enhancement%20for%20Neurological%0A%20%20Condition%20Identification&entry.906535625=Jiaxing%20Xu%20and%20Kai%20He%20and%20Yue%20Tang%20and%20Wei%20Li%20and%20Mengcheng%20Lan%20and%20Xia%20Dong%20and%20Yiping%20Ke%20and%20Mengling%20Feng&entry.1292438233=%20%20Neurological%20conditions%2C%20such%20as%20Alzheimer%27s%20Disease%2C%20are%20challenging%20to%0Adiagnose%2C%20particularly%20in%20the%20early%20stages%20where%20symptoms%20closely%20resemble%0Ahealthy%20controls.%20Existing%20brain%20network%20analysis%20methods%20primarily%20focus%20on%0Agraph-based%20models%20that%20rely%20solely%20on%20imaging%20data%2C%20which%20may%20overlook%0Aimportant%20non-imaging%20factors%20and%20limit%20the%20model%27s%20predictive%20power%20and%0Ainterpretability.%20In%20this%20paper%2C%20we%20present%20BrainPrompt%2C%20an%20innovative%0Aframework%20that%20enhances%20Graph%20Neural%20Networks%20%28GNNs%29%20by%20integrating%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20knowledge-driven%20prompts%2C%20enabling%20more%20effective%0Acapture%20of%20complex%2C%20non-imaging%20information%20and%20external%20knowledge%20for%0Aneurological%20disease%20identification.%20BrainPrompt%20integrates%20three%20types%20of%0Aknowledge-driven%20prompts%3A%20%281%29%20ROI-level%20prompts%20to%20encode%20the%20identity%20and%0Afunction%20of%20each%20brain%20region%2C%20%282%29%20subject-level%20prompts%20that%20incorporate%0Ademographic%20information%2C%20and%20%283%29%20disease-level%20prompts%20to%20capture%20the%20temporal%0Aprogression%20of%20disease.%20By%20leveraging%20these%20multi-level%20prompts%2C%20BrainPrompt%0Aeffectively%20harnesses%20knowledge-enhanced%20multi-modal%20information%20from%20LLMs%2C%0Aenhancing%20the%20model%27s%20capability%20to%20predict%20neurological%20disease%20stages%20and%0Ameanwhile%20offers%20more%20interpretable%20results.%20We%20evaluate%20BrainPrompt%20on%20two%0Aresting-state%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20datasets%20from%0Aneurological%20disorders%2C%20showing%20its%20superiority%20over%20state-of-the-art%20methods.%0AAdditionally%2C%20a%20biomarker%20study%20demonstrates%20the%20framework%27s%20ability%20to%20extract%0Avaluable%20and%20interpretable%20information%20aligned%20with%20domain%20knowledge%20in%0Aneuroscience.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/AngusMonroe/BrainPrompt%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16096v2&entry.124074799=Read"},
{"title": "Recollection from Pensieve: Novel View Synthesis via Learning from\n  Uncalibrated Videos", "author": "Ruoyu Wang and Yi Ma and Shenghua Gao", "abstract": "  Currently almost all state-of-the-art novel view synthesis and reconstruction\nmodels rely on calibrated cameras or additional geometric priors for training.\nThese prerequisites significantly limit their applicability to massive\nuncalibrated data. To alleviate this requirement and unlock the potential for\nself-supervised training on large-scale uncalibrated videos, we propose a novel\ntwo-stage strategy to train a view synthesis model from only raw video frames\nor multi-view images, without providing camera parameters or other priors. In\nthe first stage, we learn to reconstruct the scene implicitly in a latent space\nwithout relying on any explicit 3D representation. Specifically, we predict\nper-frame latent camera and scene context features, and employ a view synthesis\nmodel as a proxy for explicit rendering. This pretraining stage substantially\nreduces the optimization complexity and encourages the network to learn the\nunderlying 3D consistency in a self-supervised manner. The learned latent\ncamera and implicit scene representation have a large gap compared with the\nreal 3D world. To reduce this gap, we introduce the second stage training by\nexplicitly predicting 3D Gaussian primitives. We additionally apply explicit\nGaussian Splatting rendering loss and depth projection loss to align the\nlearned latent representations with physically grounded 3D geometry. In this\nway, Stage 1 provides a strong initialization and Stage 2 enforces 3D\nconsistency - the two stages are complementary and mutually beneficial.\nExtensive experiments demonstrate the effectiveness of our approach, achieving\nhigh-quality novel view synthesis and accurate camera pose estimation, compared\nto methods that employ supervision with calibration, pose, or depth\ninformation. The code is available at https://github.com/Dwawayu/Pensieve.\n", "link": "http://arxiv.org/abs/2505.13440v1", "date": "2025-05-19", "relevancy": 2.4948, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6649}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6204}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recollection%20from%20Pensieve%3A%20Novel%20View%20Synthesis%20via%20Learning%20from%0A%20%20Uncalibrated%20Videos&body=Title%3A%20Recollection%20from%20Pensieve%3A%20Novel%20View%20Synthesis%20via%20Learning%20from%0A%20%20Uncalibrated%20Videos%0AAuthor%3A%20Ruoyu%20Wang%20and%20Yi%20Ma%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20Currently%20almost%20all%20state-of-the-art%20novel%20view%20synthesis%20and%20reconstruction%0Amodels%20rely%20on%20calibrated%20cameras%20or%20additional%20geometric%20priors%20for%20training.%0AThese%20prerequisites%20significantly%20limit%20their%20applicability%20to%20massive%0Auncalibrated%20data.%20To%20alleviate%20this%20requirement%20and%20unlock%20the%20potential%20for%0Aself-supervised%20training%20on%20large-scale%20uncalibrated%20videos%2C%20we%20propose%20a%20novel%0Atwo-stage%20strategy%20to%20train%20a%20view%20synthesis%20model%20from%20only%20raw%20video%20frames%0Aor%20multi-view%20images%2C%20without%20providing%20camera%20parameters%20or%20other%20priors.%20In%0Athe%20first%20stage%2C%20we%20learn%20to%20reconstruct%20the%20scene%20implicitly%20in%20a%20latent%20space%0Awithout%20relying%20on%20any%20explicit%203D%20representation.%20Specifically%2C%20we%20predict%0Aper-frame%20latent%20camera%20and%20scene%20context%20features%2C%20and%20employ%20a%20view%20synthesis%0Amodel%20as%20a%20proxy%20for%20explicit%20rendering.%20This%20pretraining%20stage%20substantially%0Areduces%20the%20optimization%20complexity%20and%20encourages%20the%20network%20to%20learn%20the%0Aunderlying%203D%20consistency%20in%20a%20self-supervised%20manner.%20The%20learned%20latent%0Acamera%20and%20implicit%20scene%20representation%20have%20a%20large%20gap%20compared%20with%20the%0Areal%203D%20world.%20To%20reduce%20this%20gap%2C%20we%20introduce%20the%20second%20stage%20training%20by%0Aexplicitly%20predicting%203D%20Gaussian%20primitives.%20We%20additionally%20apply%20explicit%0AGaussian%20Splatting%20rendering%20loss%20and%20depth%20projection%20loss%20to%20align%20the%0Alearned%20latent%20representations%20with%20physically%20grounded%203D%20geometry.%20In%20this%0Away%2C%20Stage%201%20provides%20a%20strong%20initialization%20and%20Stage%202%20enforces%203D%0Aconsistency%20-%20the%20two%20stages%20are%20complementary%20and%20mutually%20beneficial.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0Ahigh-quality%20novel%20view%20synthesis%20and%20accurate%20camera%20pose%20estimation%2C%20compared%0Ato%20methods%20that%20employ%20supervision%20with%20calibration%2C%20pose%2C%20or%20depth%0Ainformation.%20The%20code%20is%20available%20at%20https%3A//github.com/Dwawayu/Pensieve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecollection%2520from%2520Pensieve%253A%2520Novel%2520View%2520Synthesis%2520via%2520Learning%2520from%250A%2520%2520Uncalibrated%2520Videos%26entry.906535625%3DRuoyu%2520Wang%2520and%2520Yi%2520Ma%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520Currently%2520almost%2520all%2520state-of-the-art%2520novel%2520view%2520synthesis%2520and%2520reconstruction%250Amodels%2520rely%2520on%2520calibrated%2520cameras%2520or%2520additional%2520geometric%2520priors%2520for%2520training.%250AThese%2520prerequisites%2520significantly%2520limit%2520their%2520applicability%2520to%2520massive%250Auncalibrated%2520data.%2520To%2520alleviate%2520this%2520requirement%2520and%2520unlock%2520the%2520potential%2520for%250Aself-supervised%2520training%2520on%2520large-scale%2520uncalibrated%2520videos%252C%2520we%2520propose%2520a%2520novel%250Atwo-stage%2520strategy%2520to%2520train%2520a%2520view%2520synthesis%2520model%2520from%2520only%2520raw%2520video%2520frames%250Aor%2520multi-view%2520images%252C%2520without%2520providing%2520camera%2520parameters%2520or%2520other%2520priors.%2520In%250Athe%2520first%2520stage%252C%2520we%2520learn%2520to%2520reconstruct%2520the%2520scene%2520implicitly%2520in%2520a%2520latent%2520space%250Awithout%2520relying%2520on%2520any%2520explicit%25203D%2520representation.%2520Specifically%252C%2520we%2520predict%250Aper-frame%2520latent%2520camera%2520and%2520scene%2520context%2520features%252C%2520and%2520employ%2520a%2520view%2520synthesis%250Amodel%2520as%2520a%2520proxy%2520for%2520explicit%2520rendering.%2520This%2520pretraining%2520stage%2520substantially%250Areduces%2520the%2520optimization%2520complexity%2520and%2520encourages%2520the%2520network%2520to%2520learn%2520the%250Aunderlying%25203D%2520consistency%2520in%2520a%2520self-supervised%2520manner.%2520The%2520learned%2520latent%250Acamera%2520and%2520implicit%2520scene%2520representation%2520have%2520a%2520large%2520gap%2520compared%2520with%2520the%250Areal%25203D%2520world.%2520To%2520reduce%2520this%2520gap%252C%2520we%2520introduce%2520the%2520second%2520stage%2520training%2520by%250Aexplicitly%2520predicting%25203D%2520Gaussian%2520primitives.%2520We%2520additionally%2520apply%2520explicit%250AGaussian%2520Splatting%2520rendering%2520loss%2520and%2520depth%2520projection%2520loss%2520to%2520align%2520the%250Alearned%2520latent%2520representations%2520with%2520physically%2520grounded%25203D%2520geometry.%2520In%2520this%250Away%252C%2520Stage%25201%2520provides%2520a%2520strong%2520initialization%2520and%2520Stage%25202%2520enforces%25203D%250Aconsistency%2520-%2520the%2520two%2520stages%2520are%2520complementary%2520and%2520mutually%2520beneficial.%250AExtensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520achieving%250Ahigh-quality%2520novel%2520view%2520synthesis%2520and%2520accurate%2520camera%2520pose%2520estimation%252C%2520compared%250Ato%2520methods%2520that%2520employ%2520supervision%2520with%2520calibration%252C%2520pose%252C%2520or%2520depth%250Ainformation.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Dwawayu/Pensieve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recollection%20from%20Pensieve%3A%20Novel%20View%20Synthesis%20via%20Learning%20from%0A%20%20Uncalibrated%20Videos&entry.906535625=Ruoyu%20Wang%20and%20Yi%20Ma%20and%20Shenghua%20Gao&entry.1292438233=%20%20Currently%20almost%20all%20state-of-the-art%20novel%20view%20synthesis%20and%20reconstruction%0Amodels%20rely%20on%20calibrated%20cameras%20or%20additional%20geometric%20priors%20for%20training.%0AThese%20prerequisites%20significantly%20limit%20their%20applicability%20to%20massive%0Auncalibrated%20data.%20To%20alleviate%20this%20requirement%20and%20unlock%20the%20potential%20for%0Aself-supervised%20training%20on%20large-scale%20uncalibrated%20videos%2C%20we%20propose%20a%20novel%0Atwo-stage%20strategy%20to%20train%20a%20view%20synthesis%20model%20from%20only%20raw%20video%20frames%0Aor%20multi-view%20images%2C%20without%20providing%20camera%20parameters%20or%20other%20priors.%20In%0Athe%20first%20stage%2C%20we%20learn%20to%20reconstruct%20the%20scene%20implicitly%20in%20a%20latent%20space%0Awithout%20relying%20on%20any%20explicit%203D%20representation.%20Specifically%2C%20we%20predict%0Aper-frame%20latent%20camera%20and%20scene%20context%20features%2C%20and%20employ%20a%20view%20synthesis%0Amodel%20as%20a%20proxy%20for%20explicit%20rendering.%20This%20pretraining%20stage%20substantially%0Areduces%20the%20optimization%20complexity%20and%20encourages%20the%20network%20to%20learn%20the%0Aunderlying%203D%20consistency%20in%20a%20self-supervised%20manner.%20The%20learned%20latent%0Acamera%20and%20implicit%20scene%20representation%20have%20a%20large%20gap%20compared%20with%20the%0Areal%203D%20world.%20To%20reduce%20this%20gap%2C%20we%20introduce%20the%20second%20stage%20training%20by%0Aexplicitly%20predicting%203D%20Gaussian%20primitives.%20We%20additionally%20apply%20explicit%0AGaussian%20Splatting%20rendering%20loss%20and%20depth%20projection%20loss%20to%20align%20the%0Alearned%20latent%20representations%20with%20physically%20grounded%203D%20geometry.%20In%20this%0Away%2C%20Stage%201%20provides%20a%20strong%20initialization%20and%20Stage%202%20enforces%203D%0Aconsistency%20-%20the%20two%20stages%20are%20complementary%20and%20mutually%20beneficial.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0Ahigh-quality%20novel%20view%20synthesis%20and%20accurate%20camera%20pose%20estimation%2C%20compared%0Ato%20methods%20that%20employ%20supervision%20with%20calibration%2C%20pose%2C%20or%20depth%0Ainformation.%20The%20code%20is%20available%20at%20https%3A//github.com/Dwawayu/Pensieve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13440v1&entry.124074799=Read"},
{"title": "Leakage and Interpretability in Concept-Based Models", "author": "Enrico Parisini and Tapabrata Chakraborti and Chris Harbron and Ben D. MacArthur and Christopher R. S. Banerji", "abstract": "  Concept Bottleneck Models aim to improve interpretability by predicting\nhigh-level intermediate concepts, representing a promising approach for\ndeployment in high-risk scenarios. However, they are known to suffer from\ninformation leakage, whereby models exploit unintended information encoded\nwithin the learned concepts. We introduce an information-theoretic framework to\nrigorously characterise and quantify leakage, and define two complementary\nmeasures: the concepts-task leakage (CTL) and interconcept leakage (ICL)\nscores. We show that these measures are strongly predictive of model behaviour\nunder interventions and outperform existing alternatives in robustness and\nreliability. Using this framework, we identify the primary causes of leakage\nand provide strong evidence that Concept Embedding Models exhibit substantial\nleakage regardless of the hyperparameters choice. Finally, we propose practical\nguidelines for designing concept-based models to reduce leakage and ensure\ninterpretability.\n", "link": "http://arxiv.org/abs/2504.14094v2", "date": "2025-05-19", "relevancy": 2.4839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leakage%20and%20Interpretability%20in%20Concept-Based%20Models&body=Title%3A%20Leakage%20and%20Interpretability%20in%20Concept-Based%20Models%0AAuthor%3A%20Enrico%20Parisini%20and%20Tapabrata%20Chakraborti%20and%20Chris%20Harbron%20and%20Ben%20D.%20MacArthur%20and%20Christopher%20R.%20S.%20Banerji%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20aim%20to%20improve%20interpretability%20by%20predicting%0Ahigh-level%20intermediate%20concepts%2C%20representing%20a%20promising%20approach%20for%0Adeployment%20in%20high-risk%20scenarios.%20However%2C%20they%20are%20known%20to%20suffer%20from%0Ainformation%20leakage%2C%20whereby%20models%20exploit%20unintended%20information%20encoded%0Awithin%20the%20learned%20concepts.%20We%20introduce%20an%20information-theoretic%20framework%20to%0Arigorously%20characterise%20and%20quantify%20leakage%2C%20and%20define%20two%20complementary%0Ameasures%3A%20the%20concepts-task%20leakage%20%28CTL%29%20and%20interconcept%20leakage%20%28ICL%29%0Ascores.%20We%20show%20that%20these%20measures%20are%20strongly%20predictive%20of%20model%20behaviour%0Aunder%20interventions%20and%20outperform%20existing%20alternatives%20in%20robustness%20and%0Areliability.%20Using%20this%20framework%2C%20we%20identify%20the%20primary%20causes%20of%20leakage%0Aand%20provide%20strong%20evidence%20that%20Concept%20Embedding%20Models%20exhibit%20substantial%0Aleakage%20regardless%20of%20the%20hyperparameters%20choice.%20Finally%2C%20we%20propose%20practical%0Aguidelines%20for%20designing%20concept-based%20models%20to%20reduce%20leakage%20and%20ensure%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeakage%2520and%2520Interpretability%2520in%2520Concept-Based%2520Models%26entry.906535625%3DEnrico%2520Parisini%2520and%2520Tapabrata%2520Chakraborti%2520and%2520Chris%2520Harbron%2520and%2520Ben%2520D.%2520MacArthur%2520and%2520Christopher%2520R.%2520S.%2520Banerji%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520aim%2520to%2520improve%2520interpretability%2520by%2520predicting%250Ahigh-level%2520intermediate%2520concepts%252C%2520representing%2520a%2520promising%2520approach%2520for%250Adeployment%2520in%2520high-risk%2520scenarios.%2520However%252C%2520they%2520are%2520known%2520to%2520suffer%2520from%250Ainformation%2520leakage%252C%2520whereby%2520models%2520exploit%2520unintended%2520information%2520encoded%250Awithin%2520the%2520learned%2520concepts.%2520We%2520introduce%2520an%2520information-theoretic%2520framework%2520to%250Arigorously%2520characterise%2520and%2520quantify%2520leakage%252C%2520and%2520define%2520two%2520complementary%250Ameasures%253A%2520the%2520concepts-task%2520leakage%2520%2528CTL%2529%2520and%2520interconcept%2520leakage%2520%2528ICL%2529%250Ascores.%2520We%2520show%2520that%2520these%2520measures%2520are%2520strongly%2520predictive%2520of%2520model%2520behaviour%250Aunder%2520interventions%2520and%2520outperform%2520existing%2520alternatives%2520in%2520robustness%2520and%250Areliability.%2520Using%2520this%2520framework%252C%2520we%2520identify%2520the%2520primary%2520causes%2520of%2520leakage%250Aand%2520provide%2520strong%2520evidence%2520that%2520Concept%2520Embedding%2520Models%2520exhibit%2520substantial%250Aleakage%2520regardless%2520of%2520the%2520hyperparameters%2520choice.%2520Finally%252C%2520we%2520propose%2520practical%250Aguidelines%2520for%2520designing%2520concept-based%2520models%2520to%2520reduce%2520leakage%2520and%2520ensure%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leakage%20and%20Interpretability%20in%20Concept-Based%20Models&entry.906535625=Enrico%20Parisini%20and%20Tapabrata%20Chakraborti%20and%20Chris%20Harbron%20and%20Ben%20D.%20MacArthur%20and%20Christopher%20R.%20S.%20Banerji&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20aim%20to%20improve%20interpretability%20by%20predicting%0Ahigh-level%20intermediate%20concepts%2C%20representing%20a%20promising%20approach%20for%0Adeployment%20in%20high-risk%20scenarios.%20However%2C%20they%20are%20known%20to%20suffer%20from%0Ainformation%20leakage%2C%20whereby%20models%20exploit%20unintended%20information%20encoded%0Awithin%20the%20learned%20concepts.%20We%20introduce%20an%20information-theoretic%20framework%20to%0Arigorously%20characterise%20and%20quantify%20leakage%2C%20and%20define%20two%20complementary%0Ameasures%3A%20the%20concepts-task%20leakage%20%28CTL%29%20and%20interconcept%20leakage%20%28ICL%29%0Ascores.%20We%20show%20that%20these%20measures%20are%20strongly%20predictive%20of%20model%20behaviour%0Aunder%20interventions%20and%20outperform%20existing%20alternatives%20in%20robustness%20and%0Areliability.%20Using%20this%20framework%2C%20we%20identify%20the%20primary%20causes%20of%20leakage%0Aand%20provide%20strong%20evidence%20that%20Concept%20Embedding%20Models%20exhibit%20substantial%0Aleakage%20regardless%20of%20the%20hyperparameters%20choice.%20Finally%2C%20we%20propose%20practical%0Aguidelines%20for%20designing%20concept-based%20models%20to%20reduce%20leakage%20and%20ensure%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14094v2&entry.124074799=Read"},
{"title": "RGNMR: A Gauss-Newton method for robust matrix completion with\n  theoretical guarantees", "author": "Eilon Vaknin Laufer and Boaz Nadler", "abstract": "  Recovering a low rank matrix from a subset of its entries, some of which may\nbe corrupted, is known as the robust matrix completion (RMC) problem. Existing\nRMC methods have several limitations: they require a relatively large number of\nobserved entries; they may fail under overparametrization, when their assumed\nrank is higher than the correct one; and many of them fail to recover even\nmildly ill-conditioned matrices. In this paper we propose a novel RMC method,\ndenoted $\\texttt{RGNMR}$, which overcomes these limitations. $\\texttt{RGNMR}$\nis a simple factorization-based iterative algorithm, which combines a\nGauss-Newton linearization with removal of entries suspected to be outliers. On\nthe theoretical front, we prove that under suitable assumptions,\n$\\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank\nmatrix. Our theoretical results improve upon the best currently known for\nfactorization-based methods. On the empirical front, we show via several\nsimulations the advantages of $\\texttt{RGNMR}$ over existing RMC methods, and\nin particular its ability to handle a small number of observed entries,\noverparameterization of the rank and ill-conditioned matrices.\n", "link": "http://arxiv.org/abs/2505.12919v1", "date": "2025-05-19", "relevancy": 2.461, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5165}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5125}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGNMR%3A%20A%20Gauss-Newton%20method%20for%20robust%20matrix%20completion%20with%0A%20%20theoretical%20guarantees&body=Title%3A%20RGNMR%3A%20A%20Gauss-Newton%20method%20for%20robust%20matrix%20completion%20with%0A%20%20theoretical%20guarantees%0AAuthor%3A%20Eilon%20Vaknin%20Laufer%20and%20Boaz%20Nadler%0AAbstract%3A%20%20%20Recovering%20a%20low%20rank%20matrix%20from%20a%20subset%20of%20its%20entries%2C%20some%20of%20which%20may%0Abe%20corrupted%2C%20is%20known%20as%20the%20robust%20matrix%20completion%20%28RMC%29%20problem.%20Existing%0ARMC%20methods%20have%20several%20limitations%3A%20they%20require%20a%20relatively%20large%20number%20of%0Aobserved%20entries%3B%20they%20may%20fail%20under%20overparametrization%2C%20when%20their%20assumed%0Arank%20is%20higher%20than%20the%20correct%20one%3B%20and%20many%20of%20them%20fail%20to%20recover%20even%0Amildly%20ill-conditioned%20matrices.%20In%20this%20paper%20we%20propose%20a%20novel%20RMC%20method%2C%0Adenoted%20%24%5Ctexttt%7BRGNMR%7D%24%2C%20which%20overcomes%20these%20limitations.%20%24%5Ctexttt%7BRGNMR%7D%24%0Ais%20a%20simple%20factorization-based%20iterative%20algorithm%2C%20which%20combines%20a%0AGauss-Newton%20linearization%20with%20removal%20of%20entries%20suspected%20to%20be%20outliers.%20On%0Athe%20theoretical%20front%2C%20we%20prove%20that%20under%20suitable%20assumptions%2C%0A%24%5Ctexttt%7BRGNMR%7D%24%20is%20guaranteed%20exact%20recovery%20of%20the%20underlying%20low%20rank%0Amatrix.%20Our%20theoretical%20results%20improve%20upon%20the%20best%20currently%20known%20for%0Afactorization-based%20methods.%20On%20the%20empirical%20front%2C%20we%20show%20via%20several%0Asimulations%20the%20advantages%20of%20%24%5Ctexttt%7BRGNMR%7D%24%20over%20existing%20RMC%20methods%2C%20and%0Ain%20particular%20its%20ability%20to%20handle%20a%20small%20number%20of%20observed%20entries%2C%0Aoverparameterization%20of%20the%20rank%20and%20ill-conditioned%20matrices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGNMR%253A%2520A%2520Gauss-Newton%2520method%2520for%2520robust%2520matrix%2520completion%2520with%250A%2520%2520theoretical%2520guarantees%26entry.906535625%3DEilon%2520Vaknin%2520Laufer%2520and%2520Boaz%2520Nadler%26entry.1292438233%3D%2520%2520Recovering%2520a%2520low%2520rank%2520matrix%2520from%2520a%2520subset%2520of%2520its%2520entries%252C%2520some%2520of%2520which%2520may%250Abe%2520corrupted%252C%2520is%2520known%2520as%2520the%2520robust%2520matrix%2520completion%2520%2528RMC%2529%2520problem.%2520Existing%250ARMC%2520methods%2520have%2520several%2520limitations%253A%2520they%2520require%2520a%2520relatively%2520large%2520number%2520of%250Aobserved%2520entries%253B%2520they%2520may%2520fail%2520under%2520overparametrization%252C%2520when%2520their%2520assumed%250Arank%2520is%2520higher%2520than%2520the%2520correct%2520one%253B%2520and%2520many%2520of%2520them%2520fail%2520to%2520recover%2520even%250Amildly%2520ill-conditioned%2520matrices.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520novel%2520RMC%2520method%252C%250Adenoted%2520%2524%255Ctexttt%257BRGNMR%257D%2524%252C%2520which%2520overcomes%2520these%2520limitations.%2520%2524%255Ctexttt%257BRGNMR%257D%2524%250Ais%2520a%2520simple%2520factorization-based%2520iterative%2520algorithm%252C%2520which%2520combines%2520a%250AGauss-Newton%2520linearization%2520with%2520removal%2520of%2520entries%2520suspected%2520to%2520be%2520outliers.%2520On%250Athe%2520theoretical%2520front%252C%2520we%2520prove%2520that%2520under%2520suitable%2520assumptions%252C%250A%2524%255Ctexttt%257BRGNMR%257D%2524%2520is%2520guaranteed%2520exact%2520recovery%2520of%2520the%2520underlying%2520low%2520rank%250Amatrix.%2520Our%2520theoretical%2520results%2520improve%2520upon%2520the%2520best%2520currently%2520known%2520for%250Afactorization-based%2520methods.%2520On%2520the%2520empirical%2520front%252C%2520we%2520show%2520via%2520several%250Asimulations%2520the%2520advantages%2520of%2520%2524%255Ctexttt%257BRGNMR%257D%2524%2520over%2520existing%2520RMC%2520methods%252C%2520and%250Ain%2520particular%2520its%2520ability%2520to%2520handle%2520a%2520small%2520number%2520of%2520observed%2520entries%252C%250Aoverparameterization%2520of%2520the%2520rank%2520and%2520ill-conditioned%2520matrices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGNMR%3A%20A%20Gauss-Newton%20method%20for%20robust%20matrix%20completion%20with%0A%20%20theoretical%20guarantees&entry.906535625=Eilon%20Vaknin%20Laufer%20and%20Boaz%20Nadler&entry.1292438233=%20%20Recovering%20a%20low%20rank%20matrix%20from%20a%20subset%20of%20its%20entries%2C%20some%20of%20which%20may%0Abe%20corrupted%2C%20is%20known%20as%20the%20robust%20matrix%20completion%20%28RMC%29%20problem.%20Existing%0ARMC%20methods%20have%20several%20limitations%3A%20they%20require%20a%20relatively%20large%20number%20of%0Aobserved%20entries%3B%20they%20may%20fail%20under%20overparametrization%2C%20when%20their%20assumed%0Arank%20is%20higher%20than%20the%20correct%20one%3B%20and%20many%20of%20them%20fail%20to%20recover%20even%0Amildly%20ill-conditioned%20matrices.%20In%20this%20paper%20we%20propose%20a%20novel%20RMC%20method%2C%0Adenoted%20%24%5Ctexttt%7BRGNMR%7D%24%2C%20which%20overcomes%20these%20limitations.%20%24%5Ctexttt%7BRGNMR%7D%24%0Ais%20a%20simple%20factorization-based%20iterative%20algorithm%2C%20which%20combines%20a%0AGauss-Newton%20linearization%20with%20removal%20of%20entries%20suspected%20to%20be%20outliers.%20On%0Athe%20theoretical%20front%2C%20we%20prove%20that%20under%20suitable%20assumptions%2C%0A%24%5Ctexttt%7BRGNMR%7D%24%20is%20guaranteed%20exact%20recovery%20of%20the%20underlying%20low%20rank%0Amatrix.%20Our%20theoretical%20results%20improve%20upon%20the%20best%20currently%20known%20for%0Afactorization-based%20methods.%20On%20the%20empirical%20front%2C%20we%20show%20via%20several%0Asimulations%20the%20advantages%20of%20%24%5Ctexttt%7BRGNMR%7D%24%20over%20existing%20RMC%20methods%2C%20and%0Ain%20particular%20its%20ability%20to%20handle%20a%20small%20number%20of%20observed%20entries%2C%0Aoverparameterization%20of%20the%20rank%20and%20ill-conditioned%20matrices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12919v1&entry.124074799=Read"},
{"title": "Make Still Further Progress: Chain of Thoughts for Tabular Data\n  Leaderboard", "author": "Si-Yang Liu and Qile Zhou and Han-Jia Ye", "abstract": "  Tabular data, a fundamental data format in machine learning, is predominantly\nutilized in competitions and real-world applications. The performance of\ntabular models--such as gradient boosted decision trees and neural\nnetworks--can vary significantly across datasets due to differences in feature\ndistributions and task characteristics. Achieving top performance on each\ndataset often requires specialized expert knowledge. To address this\nvariability, practitioners often aggregate the predictions of multiple models.\nHowever, conventional aggregation strategies typically rely on static\ncombination rules and lack instance-level adaptability. In this work, we\npropose an in-context ensemble framework for tabular prediction that leverages\nlarge language models (LLMs) to perform dynamic, instance-specific integration\nof external model predictions. Without access to raw tabular features or\nsemantic information, our method constructs a context around each test instance\nusing its nearest neighbors and the predictions from a pool of external models.\nWithin this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$),\na prompting strategy that guides LLMs through multi-step, interpretable\nreasoning, making still further progress toward expert-level decision-making.\nExperimental results show that our method outperforms well-tuned baselines and\nstandard ensemble techniques across a wide range of tabular datasets.\n", "link": "http://arxiv.org/abs/2505.13421v1", "date": "2025-05-19", "relevancy": 2.451, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Make%20Still%20Further%20Progress%3A%20Chain%20of%20Thoughts%20for%20Tabular%20Data%0A%20%20Leaderboard&body=Title%3A%20Make%20Still%20Further%20Progress%3A%20Chain%20of%20Thoughts%20for%20Tabular%20Data%0A%20%20Leaderboard%0AAuthor%3A%20Si-Yang%20Liu%20and%20Qile%20Zhou%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Tabular%20data%2C%20a%20fundamental%20data%20format%20in%20machine%20learning%2C%20is%20predominantly%0Autilized%20in%20competitions%20and%20real-world%20applications.%20The%20performance%20of%0Atabular%20models--such%20as%20gradient%20boosted%20decision%20trees%20and%20neural%0Anetworks--can%20vary%20significantly%20across%20datasets%20due%20to%20differences%20in%20feature%0Adistributions%20and%20task%20characteristics.%20Achieving%20top%20performance%20on%20each%0Adataset%20often%20requires%20specialized%20expert%20knowledge.%20To%20address%20this%0Avariability%2C%20practitioners%20often%20aggregate%20the%20predictions%20of%20multiple%20models.%0AHowever%2C%20conventional%20aggregation%20strategies%20typically%20rely%20on%20static%0Acombination%20rules%20and%20lack%20instance-level%20adaptability.%20In%20this%20work%2C%20we%0Apropose%20an%20in-context%20ensemble%20framework%20for%20tabular%20prediction%20that%20leverages%0Alarge%20language%20models%20%28LLMs%29%20to%20perform%20dynamic%2C%20instance-specific%20integration%0Aof%20external%20model%20predictions.%20Without%20access%20to%20raw%20tabular%20features%20or%0Asemantic%20information%2C%20our%20method%20constructs%20a%20context%20around%20each%20test%20instance%0Ausing%20its%20nearest%20neighbors%20and%20the%20predictions%20from%20a%20pool%20of%20external%20models.%0AWithin%20this%20enriched%20context%2C%20we%20introduce%20Chain%20of%20Tabular%20Thoughts%20%28CoT%24%5E2%24%29%2C%0Aa%20prompting%20strategy%20that%20guides%20LLMs%20through%20multi-step%2C%20interpretable%0Areasoning%2C%20making%20still%20further%20progress%20toward%20expert-level%20decision-making.%0AExperimental%20results%20show%20that%20our%20method%20outperforms%20well-tuned%20baselines%20and%0Astandard%20ensemble%20techniques%20across%20a%20wide%20range%20of%20tabular%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMake%2520Still%2520Further%2520Progress%253A%2520Chain%2520of%2520Thoughts%2520for%2520Tabular%2520Data%250A%2520%2520Leaderboard%26entry.906535625%3DSi-Yang%2520Liu%2520and%2520Qile%2520Zhou%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520Tabular%2520data%252C%2520a%2520fundamental%2520data%2520format%2520in%2520machine%2520learning%252C%2520is%2520predominantly%250Autilized%2520in%2520competitions%2520and%2520real-world%2520applications.%2520The%2520performance%2520of%250Atabular%2520models--such%2520as%2520gradient%2520boosted%2520decision%2520trees%2520and%2520neural%250Anetworks--can%2520vary%2520significantly%2520across%2520datasets%2520due%2520to%2520differences%2520in%2520feature%250Adistributions%2520and%2520task%2520characteristics.%2520Achieving%2520top%2520performance%2520on%2520each%250Adataset%2520often%2520requires%2520specialized%2520expert%2520knowledge.%2520To%2520address%2520this%250Avariability%252C%2520practitioners%2520often%2520aggregate%2520the%2520predictions%2520of%2520multiple%2520models.%250AHowever%252C%2520conventional%2520aggregation%2520strategies%2520typically%2520rely%2520on%2520static%250Acombination%2520rules%2520and%2520lack%2520instance-level%2520adaptability.%2520In%2520this%2520work%252C%2520we%250Apropose%2520an%2520in-context%2520ensemble%2520framework%2520for%2520tabular%2520prediction%2520that%2520leverages%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520to%2520perform%2520dynamic%252C%2520instance-specific%2520integration%250Aof%2520external%2520model%2520predictions.%2520Without%2520access%2520to%2520raw%2520tabular%2520features%2520or%250Asemantic%2520information%252C%2520our%2520method%2520constructs%2520a%2520context%2520around%2520each%2520test%2520instance%250Ausing%2520its%2520nearest%2520neighbors%2520and%2520the%2520predictions%2520from%2520a%2520pool%2520of%2520external%2520models.%250AWithin%2520this%2520enriched%2520context%252C%2520we%2520introduce%2520Chain%2520of%2520Tabular%2520Thoughts%2520%2528CoT%2524%255E2%2524%2529%252C%250Aa%2520prompting%2520strategy%2520that%2520guides%2520LLMs%2520through%2520multi-step%252C%2520interpretable%250Areasoning%252C%2520making%2520still%2520further%2520progress%2520toward%2520expert-level%2520decision-making.%250AExperimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520well-tuned%2520baselines%2520and%250Astandard%2520ensemble%2520techniques%2520across%2520a%2520wide%2520range%2520of%2520tabular%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make%20Still%20Further%20Progress%3A%20Chain%20of%20Thoughts%20for%20Tabular%20Data%0A%20%20Leaderboard&entry.906535625=Si-Yang%20Liu%20and%20Qile%20Zhou%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Tabular%20data%2C%20a%20fundamental%20data%20format%20in%20machine%20learning%2C%20is%20predominantly%0Autilized%20in%20competitions%20and%20real-world%20applications.%20The%20performance%20of%0Atabular%20models--such%20as%20gradient%20boosted%20decision%20trees%20and%20neural%0Anetworks--can%20vary%20significantly%20across%20datasets%20due%20to%20differences%20in%20feature%0Adistributions%20and%20task%20characteristics.%20Achieving%20top%20performance%20on%20each%0Adataset%20often%20requires%20specialized%20expert%20knowledge.%20To%20address%20this%0Avariability%2C%20practitioners%20often%20aggregate%20the%20predictions%20of%20multiple%20models.%0AHowever%2C%20conventional%20aggregation%20strategies%20typically%20rely%20on%20static%0Acombination%20rules%20and%20lack%20instance-level%20adaptability.%20In%20this%20work%2C%20we%0Apropose%20an%20in-context%20ensemble%20framework%20for%20tabular%20prediction%20that%20leverages%0Alarge%20language%20models%20%28LLMs%29%20to%20perform%20dynamic%2C%20instance-specific%20integration%0Aof%20external%20model%20predictions.%20Without%20access%20to%20raw%20tabular%20features%20or%0Asemantic%20information%2C%20our%20method%20constructs%20a%20context%20around%20each%20test%20instance%0Ausing%20its%20nearest%20neighbors%20and%20the%20predictions%20from%20a%20pool%20of%20external%20models.%0AWithin%20this%20enriched%20context%2C%20we%20introduce%20Chain%20of%20Tabular%20Thoughts%20%28CoT%24%5E2%24%29%2C%0Aa%20prompting%20strategy%20that%20guides%20LLMs%20through%20multi-step%2C%20interpretable%0Areasoning%2C%20making%20still%20further%20progress%20toward%20expert-level%20decision-making.%0AExperimental%20results%20show%20that%20our%20method%20outperforms%20well-tuned%20baselines%20and%0Astandard%20ensemble%20techniques%20across%20a%20wide%20range%20of%20tabular%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13421v1&entry.124074799=Read"},
{"title": "Generative AI and Large Language Models in Language Preservation:\n  Opportunities and Challenges", "author": "Vincent Koc", "abstract": "  The global crisis of language endangerment meets a technological turning\npoint as Generative AI (GenAI) and Large Language Models (LLMs) unlock new\nfrontiers in automating corpus creation, transcription, translation, and\ntutoring. However, this promise is imperiled by fragmented practices and the\ncritical lack of a methodology to navigate the fraught balance between LLM\ncapabilities and the profound risks of data scarcity, cultural\nmisappropriation, and ethical missteps. This paper introduces a novel\nanalytical framework that systematically evaluates GenAI applications against\nlanguage-specific needs, embedding community governance and ethical safeguards\nas foundational pillars. We demonstrate its efficacy through the Te Reo M\\=aori\nrevitalization, where it illuminates successes, such as community-led Automatic\nSpeech Recognition achieving 92% accuracy, while critically surfacing\npersistent challenges in data sovereignty and model bias for digital archives\nand educational tools. Our findings underscore that GenAI can indeed\nrevolutionize language preservation, but only when interventions are rigorously\nanchored in community-centric data stewardship, continuous evaluation, and\ntransparent risk management. Ultimately, this framework provides an\nindispensable toolkit for researchers, language communities, and policymakers,\naiming to catalyze the ethical and high-impact deployment of LLMs to safeguard\nthe world's linguistic heritage.\n", "link": "http://arxiv.org/abs/2501.11496v2", "date": "2025-05-19", "relevancy": 2.4457, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20and%20Large%20Language%20Models%20in%20Language%20Preservation%3A%0A%20%20Opportunities%20and%20Challenges&body=Title%3A%20Generative%20AI%20and%20Large%20Language%20Models%20in%20Language%20Preservation%3A%0A%20%20Opportunities%20and%20Challenges%0AAuthor%3A%20Vincent%20Koc%0AAbstract%3A%20%20%20The%20global%20crisis%20of%20language%20endangerment%20meets%20a%20technological%20turning%0Apoint%20as%20Generative%20AI%20%28GenAI%29%20and%20Large%20Language%20Models%20%28LLMs%29%20unlock%20new%0Afrontiers%20in%20automating%20corpus%20creation%2C%20transcription%2C%20translation%2C%20and%0Atutoring.%20However%2C%20this%20promise%20is%20imperiled%20by%20fragmented%20practices%20and%20the%0Acritical%20lack%20of%20a%20methodology%20to%20navigate%20the%20fraught%20balance%20between%20LLM%0Acapabilities%20and%20the%20profound%20risks%20of%20data%20scarcity%2C%20cultural%0Amisappropriation%2C%20and%20ethical%20missteps.%20This%20paper%20introduces%20a%20novel%0Aanalytical%20framework%20that%20systematically%20evaluates%20GenAI%20applications%20against%0Alanguage-specific%20needs%2C%20embedding%20community%20governance%20and%20ethical%20safeguards%0Aas%20foundational%20pillars.%20We%20demonstrate%20its%20efficacy%20through%20the%20Te%20Reo%20M%5C%3Daori%0Arevitalization%2C%20where%20it%20illuminates%20successes%2C%20such%20as%20community-led%20Automatic%0ASpeech%20Recognition%20achieving%2092%25%20accuracy%2C%20while%20critically%20surfacing%0Apersistent%20challenges%20in%20data%20sovereignty%20and%20model%20bias%20for%20digital%20archives%0Aand%20educational%20tools.%20Our%20findings%20underscore%20that%20GenAI%20can%20indeed%0Arevolutionize%20language%20preservation%2C%20but%20only%20when%20interventions%20are%20rigorously%0Aanchored%20in%20community-centric%20data%20stewardship%2C%20continuous%20evaluation%2C%20and%0Atransparent%20risk%20management.%20Ultimately%2C%20this%20framework%20provides%20an%0Aindispensable%20toolkit%20for%20researchers%2C%20language%20communities%2C%20and%20policymakers%2C%0Aaiming%20to%20catalyze%20the%20ethical%20and%20high-impact%20deployment%20of%20LLMs%20to%20safeguard%0Athe%20world%27s%20linguistic%20heritage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520and%2520Large%2520Language%2520Models%2520in%2520Language%2520Preservation%253A%250A%2520%2520Opportunities%2520and%2520Challenges%26entry.906535625%3DVincent%2520Koc%26entry.1292438233%3D%2520%2520The%2520global%2520crisis%2520of%2520language%2520endangerment%2520meets%2520a%2520technological%2520turning%250Apoint%2520as%2520Generative%2520AI%2520%2528GenAI%2529%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520unlock%2520new%250Afrontiers%2520in%2520automating%2520corpus%2520creation%252C%2520transcription%252C%2520translation%252C%2520and%250Atutoring.%2520However%252C%2520this%2520promise%2520is%2520imperiled%2520by%2520fragmented%2520practices%2520and%2520the%250Acritical%2520lack%2520of%2520a%2520methodology%2520to%2520navigate%2520the%2520fraught%2520balance%2520between%2520LLM%250Acapabilities%2520and%2520the%2520profound%2520risks%2520of%2520data%2520scarcity%252C%2520cultural%250Amisappropriation%252C%2520and%2520ethical%2520missteps.%2520This%2520paper%2520introduces%2520a%2520novel%250Aanalytical%2520framework%2520that%2520systematically%2520evaluates%2520GenAI%2520applications%2520against%250Alanguage-specific%2520needs%252C%2520embedding%2520community%2520governance%2520and%2520ethical%2520safeguards%250Aas%2520foundational%2520pillars.%2520We%2520demonstrate%2520its%2520efficacy%2520through%2520the%2520Te%2520Reo%2520M%255C%253Daori%250Arevitalization%252C%2520where%2520it%2520illuminates%2520successes%252C%2520such%2520as%2520community-led%2520Automatic%250ASpeech%2520Recognition%2520achieving%252092%2525%2520accuracy%252C%2520while%2520critically%2520surfacing%250Apersistent%2520challenges%2520in%2520data%2520sovereignty%2520and%2520model%2520bias%2520for%2520digital%2520archives%250Aand%2520educational%2520tools.%2520Our%2520findings%2520underscore%2520that%2520GenAI%2520can%2520indeed%250Arevolutionize%2520language%2520preservation%252C%2520but%2520only%2520when%2520interventions%2520are%2520rigorously%250Aanchored%2520in%2520community-centric%2520data%2520stewardship%252C%2520continuous%2520evaluation%252C%2520and%250Atransparent%2520risk%2520management.%2520Ultimately%252C%2520this%2520framework%2520provides%2520an%250Aindispensable%2520toolkit%2520for%2520researchers%252C%2520language%2520communities%252C%2520and%2520policymakers%252C%250Aaiming%2520to%2520catalyze%2520the%2520ethical%2520and%2520high-impact%2520deployment%2520of%2520LLMs%2520to%2520safeguard%250Athe%2520world%2527s%2520linguistic%2520heritage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20and%20Large%20Language%20Models%20in%20Language%20Preservation%3A%0A%20%20Opportunities%20and%20Challenges&entry.906535625=Vincent%20Koc&entry.1292438233=%20%20The%20global%20crisis%20of%20language%20endangerment%20meets%20a%20technological%20turning%0Apoint%20as%20Generative%20AI%20%28GenAI%29%20and%20Large%20Language%20Models%20%28LLMs%29%20unlock%20new%0Afrontiers%20in%20automating%20corpus%20creation%2C%20transcription%2C%20translation%2C%20and%0Atutoring.%20However%2C%20this%20promise%20is%20imperiled%20by%20fragmented%20practices%20and%20the%0Acritical%20lack%20of%20a%20methodology%20to%20navigate%20the%20fraught%20balance%20between%20LLM%0Acapabilities%20and%20the%20profound%20risks%20of%20data%20scarcity%2C%20cultural%0Amisappropriation%2C%20and%20ethical%20missteps.%20This%20paper%20introduces%20a%20novel%0Aanalytical%20framework%20that%20systematically%20evaluates%20GenAI%20applications%20against%0Alanguage-specific%20needs%2C%20embedding%20community%20governance%20and%20ethical%20safeguards%0Aas%20foundational%20pillars.%20We%20demonstrate%20its%20efficacy%20through%20the%20Te%20Reo%20M%5C%3Daori%0Arevitalization%2C%20where%20it%20illuminates%20successes%2C%20such%20as%20community-led%20Automatic%0ASpeech%20Recognition%20achieving%2092%25%20accuracy%2C%20while%20critically%20surfacing%0Apersistent%20challenges%20in%20data%20sovereignty%20and%20model%20bias%20for%20digital%20archives%0Aand%20educational%20tools.%20Our%20findings%20underscore%20that%20GenAI%20can%20indeed%0Arevolutionize%20language%20preservation%2C%20but%20only%20when%20interventions%20are%20rigorously%0Aanchored%20in%20community-centric%20data%20stewardship%2C%20continuous%20evaluation%2C%20and%0Atransparent%20risk%20management.%20Ultimately%2C%20this%20framework%20provides%20an%0Aindispensable%20toolkit%20for%20researchers%2C%20language%20communities%2C%20and%20policymakers%2C%0Aaiming%20to%20catalyze%20the%20ethical%20and%20high-impact%20deployment%20of%20LLMs%20to%20safeguard%0Athe%20world%27s%20linguistic%20heritage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11496v2&entry.124074799=Read"},
{"title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models", "author": "Zihao Cheng and Hongru Wang and Zeming Liu and Yuhang Guo and Yuanfang Guo and Yunhong Wang and Haifeng Wang", "abstract": "  While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.\n", "link": "http://arxiv.org/abs/2505.13176v1", "date": "2025-05-19", "relevancy": 2.4402, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4908}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToolSpectrum%20%3A%20Towards%20Personalized%20Tool%20Utilization%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20ToolSpectrum%20%3A%20Towards%20Personalized%20Tool%20Utilization%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Zihao%20Cheng%20and%20Hongru%20Wang%20and%20Zeming%20Liu%20and%20Yuhang%20Guo%20and%20Yuanfang%20Guo%20and%20Yunhong%20Wang%20and%20Haifeng%20Wang%0AAbstract%3A%20%20%20While%20integrating%20external%20tools%20into%20large%20language%20models%20%28LLMs%29%20enhances%0Atheir%20ability%20to%20access%20real-time%20information%20and%20domain-specific%20services%2C%0Aexisting%20approaches%20focus%20narrowly%20on%20functional%20tool%20selection%20following%20user%0Ainstructions%2C%20overlooking%20the%20context-aware%20personalization%20in%20tool%20selection.%0AThis%20oversight%20leads%20to%20suboptimal%20user%20satisfaction%20and%20inefficient%20tool%0Autilization%2C%20particularly%20when%20overlapping%20toolsets%20require%20nuanced%20selection%0Abased%20on%20contextual%20factors.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ToolSpectrum%2C%20a%0Abenchmark%20designed%20to%20evaluate%20LLMs%27%20capabilities%20in%20personalized%20tool%0Autilization.%20Specifically%2C%20we%20formalize%20two%20key%20dimensions%20of%20personalization%2C%0Auser%20profile%20and%20environmental%20factors%2C%20and%20analyze%20their%20individual%20and%0Asynergistic%20impacts%20on%20tool%20utilization.%20Through%20extensive%20experiments%20on%0AToolSpectrum%2C%20we%20demonstrate%20that%20personalized%20tool%20utilization%20significantly%0Aimproves%20user%20experience%20across%20diverse%20scenarios.%20However%2C%20even%0Astate-of-the-art%20LLMs%20exhibit%20the%20limited%20ability%20to%20reason%20jointly%20about%20user%0Aprofiles%20and%20environmental%20factors%2C%20often%20prioritizing%20one%20dimension%20at%20the%0Aexpense%20of%20the%20other.%20Our%20findings%20underscore%20the%20necessity%20of%20context-aware%0Apersonalization%20in%20tool-augmented%20LLMs%20and%20reveal%20critical%20limitations%20for%0Acurrent%20models.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Chengziha0/ToolSpectrum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToolSpectrum%2520%253A%2520Towards%2520Personalized%2520Tool%2520Utilization%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZihao%2520Cheng%2520and%2520Hongru%2520Wang%2520and%2520Zeming%2520Liu%2520and%2520Yuhang%2520Guo%2520and%2520Yuanfang%2520Guo%2520and%2520Yunhong%2520Wang%2520and%2520Haifeng%2520Wang%26entry.1292438233%3D%2520%2520While%2520integrating%2520external%2520tools%2520into%2520large%2520language%2520models%2520%2528LLMs%2529%2520enhances%250Atheir%2520ability%2520to%2520access%2520real-time%2520information%2520and%2520domain-specific%2520services%252C%250Aexisting%2520approaches%2520focus%2520narrowly%2520on%2520functional%2520tool%2520selection%2520following%2520user%250Ainstructions%252C%2520overlooking%2520the%2520context-aware%2520personalization%2520in%2520tool%2520selection.%250AThis%2520oversight%2520leads%2520to%2520suboptimal%2520user%2520satisfaction%2520and%2520inefficient%2520tool%250Autilization%252C%2520particularly%2520when%2520overlapping%2520toolsets%2520require%2520nuanced%2520selection%250Abased%2520on%2520contextual%2520factors.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520ToolSpectrum%252C%2520a%250Abenchmark%2520designed%2520to%2520evaluate%2520LLMs%2527%2520capabilities%2520in%2520personalized%2520tool%250Autilization.%2520Specifically%252C%2520we%2520formalize%2520two%2520key%2520dimensions%2520of%2520personalization%252C%250Auser%2520profile%2520and%2520environmental%2520factors%252C%2520and%2520analyze%2520their%2520individual%2520and%250Asynergistic%2520impacts%2520on%2520tool%2520utilization.%2520Through%2520extensive%2520experiments%2520on%250AToolSpectrum%252C%2520we%2520demonstrate%2520that%2520personalized%2520tool%2520utilization%2520significantly%250Aimproves%2520user%2520experience%2520across%2520diverse%2520scenarios.%2520However%252C%2520even%250Astate-of-the-art%2520LLMs%2520exhibit%2520the%2520limited%2520ability%2520to%2520reason%2520jointly%2520about%2520user%250Aprofiles%2520and%2520environmental%2520factors%252C%2520often%2520prioritizing%2520one%2520dimension%2520at%2520the%250Aexpense%2520of%2520the%2520other.%2520Our%2520findings%2520underscore%2520the%2520necessity%2520of%2520context-aware%250Apersonalization%2520in%2520tool-augmented%2520LLMs%2520and%2520reveal%2520critical%2520limitations%2520for%250Acurrent%2520models.%2520Our%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/Chengziha0/ToolSpectrum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToolSpectrum%20%3A%20Towards%20Personalized%20Tool%20Utilization%20for%20Large%20Language%0A%20%20Models&entry.906535625=Zihao%20Cheng%20and%20Hongru%20Wang%20and%20Zeming%20Liu%20and%20Yuhang%20Guo%20and%20Yuanfang%20Guo%20and%20Yunhong%20Wang%20and%20Haifeng%20Wang&entry.1292438233=%20%20While%20integrating%20external%20tools%20into%20large%20language%20models%20%28LLMs%29%20enhances%0Atheir%20ability%20to%20access%20real-time%20information%20and%20domain-specific%20services%2C%0Aexisting%20approaches%20focus%20narrowly%20on%20functional%20tool%20selection%20following%20user%0Ainstructions%2C%20overlooking%20the%20context-aware%20personalization%20in%20tool%20selection.%0AThis%20oversight%20leads%20to%20suboptimal%20user%20satisfaction%20and%20inefficient%20tool%0Autilization%2C%20particularly%20when%20overlapping%20toolsets%20require%20nuanced%20selection%0Abased%20on%20contextual%20factors.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ToolSpectrum%2C%20a%0Abenchmark%20designed%20to%20evaluate%20LLMs%27%20capabilities%20in%20personalized%20tool%0Autilization.%20Specifically%2C%20we%20formalize%20two%20key%20dimensions%20of%20personalization%2C%0Auser%20profile%20and%20environmental%20factors%2C%20and%20analyze%20their%20individual%20and%0Asynergistic%20impacts%20on%20tool%20utilization.%20Through%20extensive%20experiments%20on%0AToolSpectrum%2C%20we%20demonstrate%20that%20personalized%20tool%20utilization%20significantly%0Aimproves%20user%20experience%20across%20diverse%20scenarios.%20However%2C%20even%0Astate-of-the-art%20LLMs%20exhibit%20the%20limited%20ability%20to%20reason%20jointly%20about%20user%0Aprofiles%20and%20environmental%20factors%2C%20often%20prioritizing%20one%20dimension%20at%20the%0Aexpense%20of%20the%20other.%20Our%20findings%20underscore%20the%20necessity%20of%20context-aware%0Apersonalization%20in%20tool-augmented%20LLMs%20and%20reveal%20critical%20limitations%20for%0Acurrent%20models.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/Chengziha0/ToolSpectrum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13176v1&entry.124074799=Read"},
{"title": "$S^3$ -- Semantic Signal Separation", "author": "M\u00e1rton Kardos and Jan Kostkan and Arnault-Quentin Vermillet and Kristoffer Nielbo and Kenneth Enevoldsen and Roberta Rocca", "abstract": "  Topic models are useful tools for discovering latent semantic structures in\nlarge textual corpora. Recent efforts have been oriented at incorporating\ncontextual representations in topic modeling and have been shown to outperform\nclassical topic models. These approaches are typically slow, volatile, and\nrequire heavy preprocessing for optimal results. We present Semantic Signal\nSeparation ($S^3$), a theory-driven topic modeling approach in neural embedding\nspaces. $S^3$ conceptualizes topics as independent axes of semantic space and\nuncovers these by decomposing contextualized document embeddings using\nIndependent Component Analysis. Our approach provides diverse and highly\ncoherent topics, requires no preprocessing, and is demonstrated to be the\nfastest contextual topic model, being, on average, 4.5x faster than the\nrunner-up BERTopic. We offer an implementation of $S^3$, and all contextual\nbaselines, in the Turftopic Python package.\n", "link": "http://arxiv.org/abs/2406.09556v3", "date": "2025-05-19", "relevancy": 2.429, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5013}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24S%5E3%24%20--%20Semantic%20Signal%20Separation&body=Title%3A%20%24S%5E3%24%20--%20Semantic%20Signal%20Separation%0AAuthor%3A%20M%C3%A1rton%20Kardos%20and%20Jan%20Kostkan%20and%20Arnault-Quentin%20Vermillet%20and%20Kristoffer%20Nielbo%20and%20Kenneth%20Enevoldsen%20and%20Roberta%20Rocca%0AAbstract%3A%20%20%20Topic%20models%20are%20useful%20tools%20for%20discovering%20latent%20semantic%20structures%20in%0Alarge%20textual%20corpora.%20Recent%20efforts%20have%20been%20oriented%20at%20incorporating%0Acontextual%20representations%20in%20topic%20modeling%20and%20have%20been%20shown%20to%20outperform%0Aclassical%20topic%20models.%20These%20approaches%20are%20typically%20slow%2C%20volatile%2C%20and%0Arequire%20heavy%20preprocessing%20for%20optimal%20results.%20We%20present%20Semantic%20Signal%0ASeparation%20%28%24S%5E3%24%29%2C%20a%20theory-driven%20topic%20modeling%20approach%20in%20neural%20embedding%0Aspaces.%20%24S%5E3%24%20conceptualizes%20topics%20as%20independent%20axes%20of%20semantic%20space%20and%0Auncovers%20these%20by%20decomposing%20contextualized%20document%20embeddings%20using%0AIndependent%20Component%20Analysis.%20Our%20approach%20provides%20diverse%20and%20highly%0Acoherent%20topics%2C%20requires%20no%20preprocessing%2C%20and%20is%20demonstrated%20to%20be%20the%0Afastest%20contextual%20topic%20model%2C%20being%2C%20on%20average%2C%204.5x%20faster%20than%20the%0Arunner-up%20BERTopic.%20We%20offer%20an%20implementation%20of%20%24S%5E3%24%2C%20and%20all%20contextual%0Abaselines%2C%20in%20the%20Turftopic%20Python%20package.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09556v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524S%255E3%2524%2520--%2520Semantic%2520Signal%2520Separation%26entry.906535625%3DM%25C3%25A1rton%2520Kardos%2520and%2520Jan%2520Kostkan%2520and%2520Arnault-Quentin%2520Vermillet%2520and%2520Kristoffer%2520Nielbo%2520and%2520Kenneth%2520Enevoldsen%2520and%2520Roberta%2520Rocca%26entry.1292438233%3D%2520%2520Topic%2520models%2520are%2520useful%2520tools%2520for%2520discovering%2520latent%2520semantic%2520structures%2520in%250Alarge%2520textual%2520corpora.%2520Recent%2520efforts%2520have%2520been%2520oriented%2520at%2520incorporating%250Acontextual%2520representations%2520in%2520topic%2520modeling%2520and%2520have%2520been%2520shown%2520to%2520outperform%250Aclassical%2520topic%2520models.%2520These%2520approaches%2520are%2520typically%2520slow%252C%2520volatile%252C%2520and%250Arequire%2520heavy%2520preprocessing%2520for%2520optimal%2520results.%2520We%2520present%2520Semantic%2520Signal%250ASeparation%2520%2528%2524S%255E3%2524%2529%252C%2520a%2520theory-driven%2520topic%2520modeling%2520approach%2520in%2520neural%2520embedding%250Aspaces.%2520%2524S%255E3%2524%2520conceptualizes%2520topics%2520as%2520independent%2520axes%2520of%2520semantic%2520space%2520and%250Auncovers%2520these%2520by%2520decomposing%2520contextualized%2520document%2520embeddings%2520using%250AIndependent%2520Component%2520Analysis.%2520Our%2520approach%2520provides%2520diverse%2520and%2520highly%250Acoherent%2520topics%252C%2520requires%2520no%2520preprocessing%252C%2520and%2520is%2520demonstrated%2520to%2520be%2520the%250Afastest%2520contextual%2520topic%2520model%252C%2520being%252C%2520on%2520average%252C%25204.5x%2520faster%2520than%2520the%250Arunner-up%2520BERTopic.%2520We%2520offer%2520an%2520implementation%2520of%2520%2524S%255E3%2524%252C%2520and%2520all%2520contextual%250Abaselines%252C%2520in%2520the%2520Turftopic%2520Python%2520package.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09556v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24S%5E3%24%20--%20Semantic%20Signal%20Separation&entry.906535625=M%C3%A1rton%20Kardos%20and%20Jan%20Kostkan%20and%20Arnault-Quentin%20Vermillet%20and%20Kristoffer%20Nielbo%20and%20Kenneth%20Enevoldsen%20and%20Roberta%20Rocca&entry.1292438233=%20%20Topic%20models%20are%20useful%20tools%20for%20discovering%20latent%20semantic%20structures%20in%0Alarge%20textual%20corpora.%20Recent%20efforts%20have%20been%20oriented%20at%20incorporating%0Acontextual%20representations%20in%20topic%20modeling%20and%20have%20been%20shown%20to%20outperform%0Aclassical%20topic%20models.%20These%20approaches%20are%20typically%20slow%2C%20volatile%2C%20and%0Arequire%20heavy%20preprocessing%20for%20optimal%20results.%20We%20present%20Semantic%20Signal%0ASeparation%20%28%24S%5E3%24%29%2C%20a%20theory-driven%20topic%20modeling%20approach%20in%20neural%20embedding%0Aspaces.%20%24S%5E3%24%20conceptualizes%20topics%20as%20independent%20axes%20of%20semantic%20space%20and%0Auncovers%20these%20by%20decomposing%20contextualized%20document%20embeddings%20using%0AIndependent%20Component%20Analysis.%20Our%20approach%20provides%20diverse%20and%20highly%0Acoherent%20topics%2C%20requires%20no%20preprocessing%2C%20and%20is%20demonstrated%20to%20be%20the%0Afastest%20contextual%20topic%20model%2C%20being%2C%20on%20average%2C%204.5x%20faster%20than%20the%0Arunner-up%20BERTopic.%20We%20offer%20an%20implementation%20of%20%24S%5E3%24%2C%20and%20all%20contextual%0Abaselines%2C%20in%20the%20Turftopic%20Python%20package.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09556v3&entry.124074799=Read"},
{"title": "Temporal Query Network for Efficient Multivariate Time Series\n  Forecasting", "author": "Shengsheng Lin and Haojun Chen and Haijie Wu and Chunyun Qiu and Weiwei Lin", "abstract": "  Sufficiently modeling the correlations among variables (aka channels) is\ncrucial for achieving accurate multivariate time series forecasting (MTSF). In\nthis paper, we propose a novel technique called Temporal Query (TQ) to more\neffectively capture multivariate correlations, thereby improving model\nperformance in MTSF tasks. Technically, the TQ technique employs periodically\nshifted learnable vectors as queries in the attention mechanism to capture\nglobal inter-variable patterns, while the keys and values are derived from the\nraw input data to encode local, sample-level correlations. Building upon the TQ\ntechnique, we develop a simple yet efficient model named Temporal Query Network\n(TQNet), which employs only a single-layer attention mechanism and a\nlightweight multi-layer perceptron (MLP). Extensive experiments demonstrate\nthat TQNet learns more robust multivariate correlations, achieving\nstate-of-the-art forecasting accuracy across 12 challenging real-world\ndatasets. Furthermore, TQNet achieves high efficiency comparable to\nlinear-based methods even on high-dimensional datasets, balancing performance\nand computational cost. The code is available at:\nhttps://github.com/ACAT-SCUT/TQNet.\n", "link": "http://arxiv.org/abs/2505.12917v1", "date": "2025-05-19", "relevancy": 2.4256, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4864}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4864}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Query%20Network%20for%20Efficient%20Multivariate%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Temporal%20Query%20Network%20for%20Efficient%20Multivariate%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Shengsheng%20Lin%20and%20Haojun%20Chen%20and%20Haijie%20Wu%20and%20Chunyun%20Qiu%20and%20Weiwei%20Lin%0AAbstract%3A%20%20%20Sufficiently%20modeling%20the%20correlations%20among%20variables%20%28aka%20channels%29%20is%0Acrucial%20for%20achieving%20accurate%20multivariate%20time%20series%20forecasting%20%28MTSF%29.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20technique%20called%20Temporal%20Query%20%28TQ%29%20to%20more%0Aeffectively%20capture%20multivariate%20correlations%2C%20thereby%20improving%20model%0Aperformance%20in%20MTSF%20tasks.%20Technically%2C%20the%20TQ%20technique%20employs%20periodically%0Ashifted%20learnable%20vectors%20as%20queries%20in%20the%20attention%20mechanism%20to%20capture%0Aglobal%20inter-variable%20patterns%2C%20while%20the%20keys%20and%20values%20are%20derived%20from%20the%0Araw%20input%20data%20to%20encode%20local%2C%20sample-level%20correlations.%20Building%20upon%20the%20TQ%0Atechnique%2C%20we%20develop%20a%20simple%20yet%20efficient%20model%20named%20Temporal%20Query%20Network%0A%28TQNet%29%2C%20which%20employs%20only%20a%20single-layer%20attention%20mechanism%20and%20a%0Alightweight%20multi-layer%20perceptron%20%28MLP%29.%20Extensive%20experiments%20demonstrate%0Athat%20TQNet%20learns%20more%20robust%20multivariate%20correlations%2C%20achieving%0Astate-of-the-art%20forecasting%20accuracy%20across%2012%20challenging%20real-world%0Adatasets.%20Furthermore%2C%20TQNet%20achieves%20high%20efficiency%20comparable%20to%0Alinear-based%20methods%20even%20on%20high-dimensional%20datasets%2C%20balancing%20performance%0Aand%20computational%20cost.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ACAT-SCUT/TQNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Query%2520Network%2520for%2520Efficient%2520Multivariate%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DShengsheng%2520Lin%2520and%2520Haojun%2520Chen%2520and%2520Haijie%2520Wu%2520and%2520Chunyun%2520Qiu%2520and%2520Weiwei%2520Lin%26entry.1292438233%3D%2520%2520Sufficiently%2520modeling%2520the%2520correlations%2520among%2520variables%2520%2528aka%2520channels%2529%2520is%250Acrucial%2520for%2520achieving%2520accurate%2520multivariate%2520time%2520series%2520forecasting%2520%2528MTSF%2529.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520novel%2520technique%2520called%2520Temporal%2520Query%2520%2528TQ%2529%2520to%2520more%250Aeffectively%2520capture%2520multivariate%2520correlations%252C%2520thereby%2520improving%2520model%250Aperformance%2520in%2520MTSF%2520tasks.%2520Technically%252C%2520the%2520TQ%2520technique%2520employs%2520periodically%250Ashifted%2520learnable%2520vectors%2520as%2520queries%2520in%2520the%2520attention%2520mechanism%2520to%2520capture%250Aglobal%2520inter-variable%2520patterns%252C%2520while%2520the%2520keys%2520and%2520values%2520are%2520derived%2520from%2520the%250Araw%2520input%2520data%2520to%2520encode%2520local%252C%2520sample-level%2520correlations.%2520Building%2520upon%2520the%2520TQ%250Atechnique%252C%2520we%2520develop%2520a%2520simple%2520yet%2520efficient%2520model%2520named%2520Temporal%2520Query%2520Network%250A%2528TQNet%2529%252C%2520which%2520employs%2520only%2520a%2520single-layer%2520attention%2520mechanism%2520and%2520a%250Alightweight%2520multi-layer%2520perceptron%2520%2528MLP%2529.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520TQNet%2520learns%2520more%2520robust%2520multivariate%2520correlations%252C%2520achieving%250Astate-of-the-art%2520forecasting%2520accuracy%2520across%252012%2520challenging%2520real-world%250Adatasets.%2520Furthermore%252C%2520TQNet%2520achieves%2520high%2520efficiency%2520comparable%2520to%250Alinear-based%2520methods%2520even%2520on%2520high-dimensional%2520datasets%252C%2520balancing%2520performance%250Aand%2520computational%2520cost.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ACAT-SCUT/TQNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Query%20Network%20for%20Efficient%20Multivariate%20Time%20Series%0A%20%20Forecasting&entry.906535625=Shengsheng%20Lin%20and%20Haojun%20Chen%20and%20Haijie%20Wu%20and%20Chunyun%20Qiu%20and%20Weiwei%20Lin&entry.1292438233=%20%20Sufficiently%20modeling%20the%20correlations%20among%20variables%20%28aka%20channels%29%20is%0Acrucial%20for%20achieving%20accurate%20multivariate%20time%20series%20forecasting%20%28MTSF%29.%20In%0Athis%20paper%2C%20we%20propose%20a%20novel%20technique%20called%20Temporal%20Query%20%28TQ%29%20to%20more%0Aeffectively%20capture%20multivariate%20correlations%2C%20thereby%20improving%20model%0Aperformance%20in%20MTSF%20tasks.%20Technically%2C%20the%20TQ%20technique%20employs%20periodically%0Ashifted%20learnable%20vectors%20as%20queries%20in%20the%20attention%20mechanism%20to%20capture%0Aglobal%20inter-variable%20patterns%2C%20while%20the%20keys%20and%20values%20are%20derived%20from%20the%0Araw%20input%20data%20to%20encode%20local%2C%20sample-level%20correlations.%20Building%20upon%20the%20TQ%0Atechnique%2C%20we%20develop%20a%20simple%20yet%20efficient%20model%20named%20Temporal%20Query%20Network%0A%28TQNet%29%2C%20which%20employs%20only%20a%20single-layer%20attention%20mechanism%20and%20a%0Alightweight%20multi-layer%20perceptron%20%28MLP%29.%20Extensive%20experiments%20demonstrate%0Athat%20TQNet%20learns%20more%20robust%20multivariate%20correlations%2C%20achieving%0Astate-of-the-art%20forecasting%20accuracy%20across%2012%20challenging%20real-world%0Adatasets.%20Furthermore%2C%20TQNet%20achieves%20high%20efficiency%20comparable%20to%0Alinear-based%20methods%20even%20on%20high-dimensional%20datasets%2C%20balancing%20performance%0Aand%20computational%20cost.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ACAT-SCUT/TQNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12917v1&entry.124074799=Read"},
{"title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "author": "Xian Wu and Chang Liu", "abstract": "  Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency.\n", "link": "http://arxiv.org/abs/2504.15661v3", "date": "2025-05-19", "relevancy": 2.414, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6162}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6026}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiTPainter%3A%20Efficient%20Video%20Inpainting%20with%20Diffusion%20Transformers&body=Title%3A%20DiTPainter%3A%20Efficient%20Video%20Inpainting%20with%20Diffusion%20Transformers%0AAuthor%3A%20Xian%20Wu%20and%20Chang%20Liu%0AAbstract%3A%20%20%20Many%20existing%20video%20inpainting%20algorithms%20utilize%20optical%20flows%20to%20construct%0Athe%20corresponding%20maps%20and%20then%20propagate%20pixels%20from%20adjacent%20frames%20to%0Amissing%20areas%20by%20mapping.%20Despite%20the%20effectiveness%20of%20the%20propagation%0Amechanism%2C%20they%20might%20encounter%20blurry%20and%20inconsistencies%20when%20dealing%20with%0Ainaccurate%20optical%20flows%20or%20large%20masks.%20Recently%2C%20Diffusion%20Transformer%20%28DiT%29%0Ahas%20emerged%20as%20a%20revolutionary%20technique%20for%20video%20generation%20tasks.%20However%2C%0Apretrained%20DiT%20models%20for%20video%20generation%20all%20contain%20a%20large%20amount%20of%0Aparameters%2C%20which%20makes%20it%20very%20time%20consuming%20to%20apply%20to%20video%20inpainting%0Atasks.%20In%20this%20paper%2C%20we%20present%20DiTPainter%2C%20an%20end-to-end%20video%20inpainting%0Amodel%20based%20on%20Diffusion%20Transformer%20%28DiT%29.%20DiTPainter%20uses%20an%20efficient%0Atransformer%20network%20designed%20for%20video%20inpainting%2C%20which%20is%20trained%20from%0Ascratch%20instead%20of%20initializing%20from%20any%20large%20pretrained%20models.%20DiTPainter%0Acan%20address%20videos%20with%20arbitrary%20lengths%20and%20can%20be%20applied%20to%20video%0Adecaptioning%20and%20video%20completion%20tasks%20with%20an%20acceptable%20time%20cost.%0AExperiments%20show%20that%20DiTPainter%20outperforms%20existing%20video%20inpainting%0Aalgorithms%20with%20higher%20quality%20and%20better%20spatial-temporal%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15661v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiTPainter%253A%2520Efficient%2520Video%2520Inpainting%2520with%2520Diffusion%2520Transformers%26entry.906535625%3DXian%2520Wu%2520and%2520Chang%2520Liu%26entry.1292438233%3D%2520%2520Many%2520existing%2520video%2520inpainting%2520algorithms%2520utilize%2520optical%2520flows%2520to%2520construct%250Athe%2520corresponding%2520maps%2520and%2520then%2520propagate%2520pixels%2520from%2520adjacent%2520frames%2520to%250Amissing%2520areas%2520by%2520mapping.%2520Despite%2520the%2520effectiveness%2520of%2520the%2520propagation%250Amechanism%252C%2520they%2520might%2520encounter%2520blurry%2520and%2520inconsistencies%2520when%2520dealing%2520with%250Ainaccurate%2520optical%2520flows%2520or%2520large%2520masks.%2520Recently%252C%2520Diffusion%2520Transformer%2520%2528DiT%2529%250Ahas%2520emerged%2520as%2520a%2520revolutionary%2520technique%2520for%2520video%2520generation%2520tasks.%2520However%252C%250Apretrained%2520DiT%2520models%2520for%2520video%2520generation%2520all%2520contain%2520a%2520large%2520amount%2520of%250Aparameters%252C%2520which%2520makes%2520it%2520very%2520time%2520consuming%2520to%2520apply%2520to%2520video%2520inpainting%250Atasks.%2520In%2520this%2520paper%252C%2520we%2520present%2520DiTPainter%252C%2520an%2520end-to-end%2520video%2520inpainting%250Amodel%2520based%2520on%2520Diffusion%2520Transformer%2520%2528DiT%2529.%2520DiTPainter%2520uses%2520an%2520efficient%250Atransformer%2520network%2520designed%2520for%2520video%2520inpainting%252C%2520which%2520is%2520trained%2520from%250Ascratch%2520instead%2520of%2520initializing%2520from%2520any%2520large%2520pretrained%2520models.%2520DiTPainter%250Acan%2520address%2520videos%2520with%2520arbitrary%2520lengths%2520and%2520can%2520be%2520applied%2520to%2520video%250Adecaptioning%2520and%2520video%2520completion%2520tasks%2520with%2520an%2520acceptable%2520time%2520cost.%250AExperiments%2520show%2520that%2520DiTPainter%2520outperforms%2520existing%2520video%2520inpainting%250Aalgorithms%2520with%2520higher%2520quality%2520and%2520better%2520spatial-temporal%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15661v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiTPainter%3A%20Efficient%20Video%20Inpainting%20with%20Diffusion%20Transformers&entry.906535625=Xian%20Wu%20and%20Chang%20Liu&entry.1292438233=%20%20Many%20existing%20video%20inpainting%20algorithms%20utilize%20optical%20flows%20to%20construct%0Athe%20corresponding%20maps%20and%20then%20propagate%20pixels%20from%20adjacent%20frames%20to%0Amissing%20areas%20by%20mapping.%20Despite%20the%20effectiveness%20of%20the%20propagation%0Amechanism%2C%20they%20might%20encounter%20blurry%20and%20inconsistencies%20when%20dealing%20with%0Ainaccurate%20optical%20flows%20or%20large%20masks.%20Recently%2C%20Diffusion%20Transformer%20%28DiT%29%0Ahas%20emerged%20as%20a%20revolutionary%20technique%20for%20video%20generation%20tasks.%20However%2C%0Apretrained%20DiT%20models%20for%20video%20generation%20all%20contain%20a%20large%20amount%20of%0Aparameters%2C%20which%20makes%20it%20very%20time%20consuming%20to%20apply%20to%20video%20inpainting%0Atasks.%20In%20this%20paper%2C%20we%20present%20DiTPainter%2C%20an%20end-to-end%20video%20inpainting%0Amodel%20based%20on%20Diffusion%20Transformer%20%28DiT%29.%20DiTPainter%20uses%20an%20efficient%0Atransformer%20network%20designed%20for%20video%20inpainting%2C%20which%20is%20trained%20from%0Ascratch%20instead%20of%20initializing%20from%20any%20large%20pretrained%20models.%20DiTPainter%0Acan%20address%20videos%20with%20arbitrary%20lengths%20and%20can%20be%20applied%20to%20video%0Adecaptioning%20and%20video%20completion%20tasks%20with%20an%20acceptable%20time%20cost.%0AExperiments%20show%20that%20DiTPainter%20outperforms%20existing%20video%20inpainting%0Aalgorithms%20with%20higher%20quality%20and%20better%20spatial-temporal%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15661v3&entry.124074799=Read"},
{"title": "The role of data partitioning on the performance of EEG-based deep\n  learning models in supervised cross-subject analysis: a preliminary study", "author": "Federico Del Pup and Andrea Zanola and Louis Fabrice Tshimanga and Alessandra Bertoldo and Livio Finos and Manfredo Atzori", "abstract": "  Deep learning is significantly advancing the analysis of\nelectroencephalography (EEG) data by effectively discovering highly nonlinear\npatterns within the signals. Data partitioning and cross-validation are crucial\nfor assessing model performance and ensuring study comparability, as they can\nproduce varied results and data leakage due to specific signal properties\n(e.g., biometric). Such variability leads to incomparable studies and,\nincreasingly, overestimated performance claims, which are detrimental to the\nfield. Nevertheless, no comprehensive guidelines for proper data partitioning\nand cross-validation exist in the domain, nor is there a quantitative\nevaluation of their impact on model accuracy, reliability, and\ngeneralizability. To assist researchers in identifying optimal experimental\nstrategies, this paper thoroughly investigates the role of data partitioning\nand cross-validation in evaluating EEG deep learning models. Five\ncross-validation settings are compared across three supervised cross-subject\nclassification tasks (BCI, Parkinson's, and Alzheimer's disease detection) and\nfour established architectures of increasing complexity (ShallowConvNet,\nEEGNet, DeepConvNet, and Temporal-based ResNet). The comparison of over 100,000\ntrained models underscores, first, the importance of using subject-based\ncross-validation strategies for evaluating EEG deep learning models, except\nwhen within-subject analyses are acceptable (e.g., BCI). Second, it highlights\nthe greater reliability of nested approaches (N-LNSO) compared to non-nested\ncounterparts, which are prone to data leakage and favor larger models\noverfitting to validation data. In conclusion, this work provides EEG deep\nlearning researchers with an analysis of data partitioning and cross-validation\nand offers guidelines to avoid data leakage, currently undermining the domain\nwith potentially overestimated performance claims.\n", "link": "http://arxiv.org/abs/2505.13021v1", "date": "2025-05-19", "relevancy": 2.4136, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20role%20of%20data%20partitioning%20on%20the%20performance%20of%20EEG-based%20deep%0A%20%20learning%20models%20in%20supervised%20cross-subject%20analysis%3A%20a%20preliminary%20study&body=Title%3A%20The%20role%20of%20data%20partitioning%20on%20the%20performance%20of%20EEG-based%20deep%0A%20%20learning%20models%20in%20supervised%20cross-subject%20analysis%3A%20a%20preliminary%20study%0AAuthor%3A%20Federico%20Del%20Pup%20and%20Andrea%20Zanola%20and%20Louis%20Fabrice%20Tshimanga%20and%20Alessandra%20Bertoldo%20and%20Livio%20Finos%20and%20Manfredo%20Atzori%0AAbstract%3A%20%20%20Deep%20learning%20is%20significantly%20advancing%20the%20analysis%20of%0Aelectroencephalography%20%28EEG%29%20data%20by%20effectively%20discovering%20highly%20nonlinear%0Apatterns%20within%20the%20signals.%20Data%20partitioning%20and%20cross-validation%20are%20crucial%0Afor%20assessing%20model%20performance%20and%20ensuring%20study%20comparability%2C%20as%20they%20can%0Aproduce%20varied%20results%20and%20data%20leakage%20due%20to%20specific%20signal%20properties%0A%28e.g.%2C%20biometric%29.%20Such%20variability%20leads%20to%20incomparable%20studies%20and%2C%0Aincreasingly%2C%20overestimated%20performance%20claims%2C%20which%20are%20detrimental%20to%20the%0Afield.%20Nevertheless%2C%20no%20comprehensive%20guidelines%20for%20proper%20data%20partitioning%0Aand%20cross-validation%20exist%20in%20the%20domain%2C%20nor%20is%20there%20a%20quantitative%0Aevaluation%20of%20their%20impact%20on%20model%20accuracy%2C%20reliability%2C%20and%0Ageneralizability.%20To%20assist%20researchers%20in%20identifying%20optimal%20experimental%0Astrategies%2C%20this%20paper%20thoroughly%20investigates%20the%20role%20of%20data%20partitioning%0Aand%20cross-validation%20in%20evaluating%20EEG%20deep%20learning%20models.%20Five%0Across-validation%20settings%20are%20compared%20across%20three%20supervised%20cross-subject%0Aclassification%20tasks%20%28BCI%2C%20Parkinson%27s%2C%20and%20Alzheimer%27s%20disease%20detection%29%20and%0Afour%20established%20architectures%20of%20increasing%20complexity%20%28ShallowConvNet%2C%0AEEGNet%2C%20DeepConvNet%2C%20and%20Temporal-based%20ResNet%29.%20The%20comparison%20of%20over%20100%2C000%0Atrained%20models%20underscores%2C%20first%2C%20the%20importance%20of%20using%20subject-based%0Across-validation%20strategies%20for%20evaluating%20EEG%20deep%20learning%20models%2C%20except%0Awhen%20within-subject%20analyses%20are%20acceptable%20%28e.g.%2C%20BCI%29.%20Second%2C%20it%20highlights%0Athe%20greater%20reliability%20of%20nested%20approaches%20%28N-LNSO%29%20compared%20to%20non-nested%0Acounterparts%2C%20which%20are%20prone%20to%20data%20leakage%20and%20favor%20larger%20models%0Aoverfitting%20to%20validation%20data.%20In%20conclusion%2C%20this%20work%20provides%20EEG%20deep%0Alearning%20researchers%20with%20an%20analysis%20of%20data%20partitioning%20and%20cross-validation%0Aand%20offers%20guidelines%20to%20avoid%20data%20leakage%2C%20currently%20undermining%20the%20domain%0Awith%20potentially%20overestimated%20performance%20claims.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520role%2520of%2520data%2520partitioning%2520on%2520the%2520performance%2520of%2520EEG-based%2520deep%250A%2520%2520learning%2520models%2520in%2520supervised%2520cross-subject%2520analysis%253A%2520a%2520preliminary%2520study%26entry.906535625%3DFederico%2520Del%2520Pup%2520and%2520Andrea%2520Zanola%2520and%2520Louis%2520Fabrice%2520Tshimanga%2520and%2520Alessandra%2520Bertoldo%2520and%2520Livio%2520Finos%2520and%2520Manfredo%2520Atzori%26entry.1292438233%3D%2520%2520Deep%2520learning%2520is%2520significantly%2520advancing%2520the%2520analysis%2520of%250Aelectroencephalography%2520%2528EEG%2529%2520data%2520by%2520effectively%2520discovering%2520highly%2520nonlinear%250Apatterns%2520within%2520the%2520signals.%2520Data%2520partitioning%2520and%2520cross-validation%2520are%2520crucial%250Afor%2520assessing%2520model%2520performance%2520and%2520ensuring%2520study%2520comparability%252C%2520as%2520they%2520can%250Aproduce%2520varied%2520results%2520and%2520data%2520leakage%2520due%2520to%2520specific%2520signal%2520properties%250A%2528e.g.%252C%2520biometric%2529.%2520Such%2520variability%2520leads%2520to%2520incomparable%2520studies%2520and%252C%250Aincreasingly%252C%2520overestimated%2520performance%2520claims%252C%2520which%2520are%2520detrimental%2520to%2520the%250Afield.%2520Nevertheless%252C%2520no%2520comprehensive%2520guidelines%2520for%2520proper%2520data%2520partitioning%250Aand%2520cross-validation%2520exist%2520in%2520the%2520domain%252C%2520nor%2520is%2520there%2520a%2520quantitative%250Aevaluation%2520of%2520their%2520impact%2520on%2520model%2520accuracy%252C%2520reliability%252C%2520and%250Ageneralizability.%2520To%2520assist%2520researchers%2520in%2520identifying%2520optimal%2520experimental%250Astrategies%252C%2520this%2520paper%2520thoroughly%2520investigates%2520the%2520role%2520of%2520data%2520partitioning%250Aand%2520cross-validation%2520in%2520evaluating%2520EEG%2520deep%2520learning%2520models.%2520Five%250Across-validation%2520settings%2520are%2520compared%2520across%2520three%2520supervised%2520cross-subject%250Aclassification%2520tasks%2520%2528BCI%252C%2520Parkinson%2527s%252C%2520and%2520Alzheimer%2527s%2520disease%2520detection%2529%2520and%250Afour%2520established%2520architectures%2520of%2520increasing%2520complexity%2520%2528ShallowConvNet%252C%250AEEGNet%252C%2520DeepConvNet%252C%2520and%2520Temporal-based%2520ResNet%2529.%2520The%2520comparison%2520of%2520over%2520100%252C000%250Atrained%2520models%2520underscores%252C%2520first%252C%2520the%2520importance%2520of%2520using%2520subject-based%250Across-validation%2520strategies%2520for%2520evaluating%2520EEG%2520deep%2520learning%2520models%252C%2520except%250Awhen%2520within-subject%2520analyses%2520are%2520acceptable%2520%2528e.g.%252C%2520BCI%2529.%2520Second%252C%2520it%2520highlights%250Athe%2520greater%2520reliability%2520of%2520nested%2520approaches%2520%2528N-LNSO%2529%2520compared%2520to%2520non-nested%250Acounterparts%252C%2520which%2520are%2520prone%2520to%2520data%2520leakage%2520and%2520favor%2520larger%2520models%250Aoverfitting%2520to%2520validation%2520data.%2520In%2520conclusion%252C%2520this%2520work%2520provides%2520EEG%2520deep%250Alearning%2520researchers%2520with%2520an%2520analysis%2520of%2520data%2520partitioning%2520and%2520cross-validation%250Aand%2520offers%2520guidelines%2520to%2520avoid%2520data%2520leakage%252C%2520currently%2520undermining%2520the%2520domain%250Awith%2520potentially%2520overestimated%2520performance%2520claims.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20role%20of%20data%20partitioning%20on%20the%20performance%20of%20EEG-based%20deep%0A%20%20learning%20models%20in%20supervised%20cross-subject%20analysis%3A%20a%20preliminary%20study&entry.906535625=Federico%20Del%20Pup%20and%20Andrea%20Zanola%20and%20Louis%20Fabrice%20Tshimanga%20and%20Alessandra%20Bertoldo%20and%20Livio%20Finos%20and%20Manfredo%20Atzori&entry.1292438233=%20%20Deep%20learning%20is%20significantly%20advancing%20the%20analysis%20of%0Aelectroencephalography%20%28EEG%29%20data%20by%20effectively%20discovering%20highly%20nonlinear%0Apatterns%20within%20the%20signals.%20Data%20partitioning%20and%20cross-validation%20are%20crucial%0Afor%20assessing%20model%20performance%20and%20ensuring%20study%20comparability%2C%20as%20they%20can%0Aproduce%20varied%20results%20and%20data%20leakage%20due%20to%20specific%20signal%20properties%0A%28e.g.%2C%20biometric%29.%20Such%20variability%20leads%20to%20incomparable%20studies%20and%2C%0Aincreasingly%2C%20overestimated%20performance%20claims%2C%20which%20are%20detrimental%20to%20the%0Afield.%20Nevertheless%2C%20no%20comprehensive%20guidelines%20for%20proper%20data%20partitioning%0Aand%20cross-validation%20exist%20in%20the%20domain%2C%20nor%20is%20there%20a%20quantitative%0Aevaluation%20of%20their%20impact%20on%20model%20accuracy%2C%20reliability%2C%20and%0Ageneralizability.%20To%20assist%20researchers%20in%20identifying%20optimal%20experimental%0Astrategies%2C%20this%20paper%20thoroughly%20investigates%20the%20role%20of%20data%20partitioning%0Aand%20cross-validation%20in%20evaluating%20EEG%20deep%20learning%20models.%20Five%0Across-validation%20settings%20are%20compared%20across%20three%20supervised%20cross-subject%0Aclassification%20tasks%20%28BCI%2C%20Parkinson%27s%2C%20and%20Alzheimer%27s%20disease%20detection%29%20and%0Afour%20established%20architectures%20of%20increasing%20complexity%20%28ShallowConvNet%2C%0AEEGNet%2C%20DeepConvNet%2C%20and%20Temporal-based%20ResNet%29.%20The%20comparison%20of%20over%20100%2C000%0Atrained%20models%20underscores%2C%20first%2C%20the%20importance%20of%20using%20subject-based%0Across-validation%20strategies%20for%20evaluating%20EEG%20deep%20learning%20models%2C%20except%0Awhen%20within-subject%20analyses%20are%20acceptable%20%28e.g.%2C%20BCI%29.%20Second%2C%20it%20highlights%0Athe%20greater%20reliability%20of%20nested%20approaches%20%28N-LNSO%29%20compared%20to%20non-nested%0Acounterparts%2C%20which%20are%20prone%20to%20data%20leakage%20and%20favor%20larger%20models%0Aoverfitting%20to%20validation%20data.%20In%20conclusion%2C%20this%20work%20provides%20EEG%20deep%0Alearning%20researchers%20with%20an%20analysis%20of%20data%20partitioning%20and%20cross-validation%0Aand%20offers%20guidelines%20to%20avoid%20data%20leakage%2C%20currently%20undermining%20the%20domain%0Awith%20potentially%20overestimated%20performance%20claims.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13021v1&entry.124074799=Read"},
{"title": "Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields\n  in Efficient CNNs for Fair Medical Image Classification", "author": "Xiao Wu and Xiaoqing Zhang and Zunjie Xiao and Lingxi Hu and Risa Higashita and Jiang Liu", "abstract": "  Efficient convolutional neural network (CNN) architecture designs have\nattracted growing research interests. However, they usually apply single\nreceptive field (RF), small asymmetric RFs, or pyramid RFs to learn different\nfeature representations, still encountering two significant challenges in\nmedical image classification tasks: 1) They have limitations in capturing\ndiverse lesion characteristics efficiently, e.g., tiny, coordination, small and\nsalient, which have unique roles on results, especially imbalanced medical\nimage classification. 2) The predictions generated by those CNNs are often\nunfair/biased, bringing a high risk by employing them to real-world medical\ndiagnosis conditions. To tackle these issues, we develop a new concept,\nExpert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields\n(ERoHPRF), to simultaneously boost medical image classification performance and\nfairness. This concept aims to mimic the multi-expert consultation mode by\napplying the well-designed heterogeneous pyramid RF bags to capture different\nlesion characteristics effectively via convolution operations with multiple\nheterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like\nstructural reparameterization technique to merge its parameters with the\ntwo-stage strategy, ensuring competitive computation cost and inference speed\nthrough comparisons to a single RF. To manifest the effectiveness and\ngeneralization ability of ERoHPRF, we incorporate it into mainstream efficient\nCNN architectures. The extensive experiments show that our method maintains a\nbetter trade-off than state-of-the-art methods in terms of medical image\nclassification, fairness, and computation overhead. The codes of this paper\nwill be released soon.\n", "link": "http://arxiv.org/abs/2505.13039v1", "date": "2025-05-19", "relevancy": 2.4133, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4937}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4851}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expert-Like%20Reparameterization%20of%20Heterogeneous%20Pyramid%20Receptive%20Fields%0A%20%20in%20Efficient%20CNNs%20for%20Fair%20Medical%20Image%20Classification&body=Title%3A%20Expert-Like%20Reparameterization%20of%20Heterogeneous%20Pyramid%20Receptive%20Fields%0A%20%20in%20Efficient%20CNNs%20for%20Fair%20Medical%20Image%20Classification%0AAuthor%3A%20Xiao%20Wu%20and%20Xiaoqing%20Zhang%20and%20Zunjie%20Xiao%20and%20Lingxi%20Hu%20and%20Risa%20Higashita%20and%20Jiang%20Liu%0AAbstract%3A%20%20%20Efficient%20convolutional%20neural%20network%20%28CNN%29%20architecture%20designs%20have%0Aattracted%20growing%20research%20interests.%20However%2C%20they%20usually%20apply%20single%0Areceptive%20field%20%28RF%29%2C%20small%20asymmetric%20RFs%2C%20or%20pyramid%20RFs%20to%20learn%20different%0Afeature%20representations%2C%20still%20encountering%20two%20significant%20challenges%20in%0Amedical%20image%20classification%20tasks%3A%201%29%20They%20have%20limitations%20in%20capturing%0Adiverse%20lesion%20characteristics%20efficiently%2C%20e.g.%2C%20tiny%2C%20coordination%2C%20small%20and%0Asalient%2C%20which%20have%20unique%20roles%20on%20results%2C%20especially%20imbalanced%20medical%0Aimage%20classification.%202%29%20The%20predictions%20generated%20by%20those%20CNNs%20are%20often%0Aunfair/biased%2C%20bringing%20a%20high%20risk%20by%20employing%20them%20to%20real-world%20medical%0Adiagnosis%20conditions.%20To%20tackle%20these%20issues%2C%20we%20develop%20a%20new%20concept%2C%0AExpert-Like%20Reparameterization%20of%20Heterogeneous%20Pyramid%20Receptive%20Fields%0A%28ERoHPRF%29%2C%20to%20simultaneously%20boost%20medical%20image%20classification%20performance%20and%0Afairness.%20This%20concept%20aims%20to%20mimic%20the%20multi-expert%20consultation%20mode%20by%0Aapplying%20the%20well-designed%20heterogeneous%20pyramid%20RF%20bags%20to%20capture%20different%0Alesion%20characteristics%20effectively%20via%20convolution%20operations%20with%20multiple%0Aheterogeneous%20kernel%20sizes.%20Additionally%2C%20ERoHPRF%20introduces%20an%20expert-like%0Astructural%20reparameterization%20technique%20to%20merge%20its%20parameters%20with%20the%0Atwo-stage%20strategy%2C%20ensuring%20competitive%20computation%20cost%20and%20inference%20speed%0Athrough%20comparisons%20to%20a%20single%20RF.%20To%20manifest%20the%20effectiveness%20and%0Ageneralization%20ability%20of%20ERoHPRF%2C%20we%20incorporate%20it%20into%20mainstream%20efficient%0ACNN%20architectures.%20The%20extensive%20experiments%20show%20that%20our%20method%20maintains%20a%0Abetter%20trade-off%20than%20state-of-the-art%20methods%20in%20terms%20of%20medical%20image%0Aclassification%2C%20fairness%2C%20and%20computation%20overhead.%20The%20codes%20of%20this%20paper%0Awill%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpert-Like%2520Reparameterization%2520of%2520Heterogeneous%2520Pyramid%2520Receptive%2520Fields%250A%2520%2520in%2520Efficient%2520CNNs%2520for%2520Fair%2520Medical%2520Image%2520Classification%26entry.906535625%3DXiao%2520Wu%2520and%2520Xiaoqing%2520Zhang%2520and%2520Zunjie%2520Xiao%2520and%2520Lingxi%2520Hu%2520and%2520Risa%2520Higashita%2520and%2520Jiang%2520Liu%26entry.1292438233%3D%2520%2520Efficient%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520architecture%2520designs%2520have%250Aattracted%2520growing%2520research%2520interests.%2520However%252C%2520they%2520usually%2520apply%2520single%250Areceptive%2520field%2520%2528RF%2529%252C%2520small%2520asymmetric%2520RFs%252C%2520or%2520pyramid%2520RFs%2520to%2520learn%2520different%250Afeature%2520representations%252C%2520still%2520encountering%2520two%2520significant%2520challenges%2520in%250Amedical%2520image%2520classification%2520tasks%253A%25201%2529%2520They%2520have%2520limitations%2520in%2520capturing%250Adiverse%2520lesion%2520characteristics%2520efficiently%252C%2520e.g.%252C%2520tiny%252C%2520coordination%252C%2520small%2520and%250Asalient%252C%2520which%2520have%2520unique%2520roles%2520on%2520results%252C%2520especially%2520imbalanced%2520medical%250Aimage%2520classification.%25202%2529%2520The%2520predictions%2520generated%2520by%2520those%2520CNNs%2520are%2520often%250Aunfair/biased%252C%2520bringing%2520a%2520high%2520risk%2520by%2520employing%2520them%2520to%2520real-world%2520medical%250Adiagnosis%2520conditions.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520develop%2520a%2520new%2520concept%252C%250AExpert-Like%2520Reparameterization%2520of%2520Heterogeneous%2520Pyramid%2520Receptive%2520Fields%250A%2528ERoHPRF%2529%252C%2520to%2520simultaneously%2520boost%2520medical%2520image%2520classification%2520performance%2520and%250Afairness.%2520This%2520concept%2520aims%2520to%2520mimic%2520the%2520multi-expert%2520consultation%2520mode%2520by%250Aapplying%2520the%2520well-designed%2520heterogeneous%2520pyramid%2520RF%2520bags%2520to%2520capture%2520different%250Alesion%2520characteristics%2520effectively%2520via%2520convolution%2520operations%2520with%2520multiple%250Aheterogeneous%2520kernel%2520sizes.%2520Additionally%252C%2520ERoHPRF%2520introduces%2520an%2520expert-like%250Astructural%2520reparameterization%2520technique%2520to%2520merge%2520its%2520parameters%2520with%2520the%250Atwo-stage%2520strategy%252C%2520ensuring%2520competitive%2520computation%2520cost%2520and%2520inference%2520speed%250Athrough%2520comparisons%2520to%2520a%2520single%2520RF.%2520To%2520manifest%2520the%2520effectiveness%2520and%250Ageneralization%2520ability%2520of%2520ERoHPRF%252C%2520we%2520incorporate%2520it%2520into%2520mainstream%2520efficient%250ACNN%2520architectures.%2520The%2520extensive%2520experiments%2520show%2520that%2520our%2520method%2520maintains%2520a%250Abetter%2520trade-off%2520than%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520medical%2520image%250Aclassification%252C%2520fairness%252C%2520and%2520computation%2520overhead.%2520The%2520codes%2520of%2520this%2520paper%250Awill%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expert-Like%20Reparameterization%20of%20Heterogeneous%20Pyramid%20Receptive%20Fields%0A%20%20in%20Efficient%20CNNs%20for%20Fair%20Medical%20Image%20Classification&entry.906535625=Xiao%20Wu%20and%20Xiaoqing%20Zhang%20and%20Zunjie%20Xiao%20and%20Lingxi%20Hu%20and%20Risa%20Higashita%20and%20Jiang%20Liu&entry.1292438233=%20%20Efficient%20convolutional%20neural%20network%20%28CNN%29%20architecture%20designs%20have%0Aattracted%20growing%20research%20interests.%20However%2C%20they%20usually%20apply%20single%0Areceptive%20field%20%28RF%29%2C%20small%20asymmetric%20RFs%2C%20or%20pyramid%20RFs%20to%20learn%20different%0Afeature%20representations%2C%20still%20encountering%20two%20significant%20challenges%20in%0Amedical%20image%20classification%20tasks%3A%201%29%20They%20have%20limitations%20in%20capturing%0Adiverse%20lesion%20characteristics%20efficiently%2C%20e.g.%2C%20tiny%2C%20coordination%2C%20small%20and%0Asalient%2C%20which%20have%20unique%20roles%20on%20results%2C%20especially%20imbalanced%20medical%0Aimage%20classification.%202%29%20The%20predictions%20generated%20by%20those%20CNNs%20are%20often%0Aunfair/biased%2C%20bringing%20a%20high%20risk%20by%20employing%20them%20to%20real-world%20medical%0Adiagnosis%20conditions.%20To%20tackle%20these%20issues%2C%20we%20develop%20a%20new%20concept%2C%0AExpert-Like%20Reparameterization%20of%20Heterogeneous%20Pyramid%20Receptive%20Fields%0A%28ERoHPRF%29%2C%20to%20simultaneously%20boost%20medical%20image%20classification%20performance%20and%0Afairness.%20This%20concept%20aims%20to%20mimic%20the%20multi-expert%20consultation%20mode%20by%0Aapplying%20the%20well-designed%20heterogeneous%20pyramid%20RF%20bags%20to%20capture%20different%0Alesion%20characteristics%20effectively%20via%20convolution%20operations%20with%20multiple%0Aheterogeneous%20kernel%20sizes.%20Additionally%2C%20ERoHPRF%20introduces%20an%20expert-like%0Astructural%20reparameterization%20technique%20to%20merge%20its%20parameters%20with%20the%0Atwo-stage%20strategy%2C%20ensuring%20competitive%20computation%20cost%20and%20inference%20speed%0Athrough%20comparisons%20to%20a%20single%20RF.%20To%20manifest%20the%20effectiveness%20and%0Ageneralization%20ability%20of%20ERoHPRF%2C%20we%20incorporate%20it%20into%20mainstream%20efficient%0ACNN%20architectures.%20The%20extensive%20experiments%20show%20that%20our%20method%20maintains%20a%0Abetter%20trade-off%20than%20state-of-the-art%20methods%20in%20terms%20of%20medical%20image%0Aclassification%2C%20fairness%2C%20and%20computation%20overhead.%20The%20codes%20of%20this%20paper%0Awill%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13039v1&entry.124074799=Read"},
{"title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and\n  Reconstruction", "author": "Yuanbo Wang and Zhaoxuan Zhang and Jiajin Qiu and Dilong Sun and Zhengyu Meng and Xiaopeng Wei and Xin Yang", "abstract": "  Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance.\n", "link": "http://arxiv.org/abs/2505.13091v1", "date": "2025-05-19", "relevancy": 2.4046, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6019}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6019}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Touch2Shape%3A%20Touch-Conditioned%203D%20Diffusion%20for%20Shape%20Exploration%20and%0A%20%20Reconstruction&body=Title%3A%20Touch2Shape%3A%20Touch-Conditioned%203D%20Diffusion%20for%20Shape%20Exploration%20and%0A%20%20Reconstruction%0AAuthor%3A%20Yuanbo%20Wang%20and%20Zhaoxuan%20Zhang%20and%20Jiajin%20Qiu%20and%20Dilong%20Sun%20and%20Zhengyu%20Meng%20and%20Xiaopeng%20Wei%20and%20Xin%20Yang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20made%20breakthroughs%20in%203D%20generation%20tasks.%20Current%203D%0Adiffusion%20models%20focus%20on%20reconstructing%20target%20shape%20from%20images%20or%20a%20set%20of%0Apartial%20observations.%20While%20excelling%20in%20global%20context%20understanding%2C%20they%0Astruggle%20to%20capture%20the%20local%20details%20of%20complex%20shapes%20and%20limited%20to%20the%0Aocclusion%20and%20lighting%20conditions.%20To%20overcome%20these%20limitations%2C%20we%20utilize%0Atactile%20images%20to%20capture%20the%20local%203D%20information%20and%20propose%20a%20Touch2Shape%0Amodel%2C%20which%20leverages%20a%20touch-conditioned%20diffusion%20model%20to%20explore%20and%0Areconstruct%20the%20target%20shape%20from%20touch.%20For%20shape%20reconstruction%2C%20we%20have%0Adeveloped%20a%20touch%20embedding%20module%20to%20condition%20the%20diffusion%20model%20in%20creating%0Aa%20compact%20representation%20and%20a%20touch%20shape%20fusion%20module%20to%20refine%20the%0Areconstructed%20shape.%20For%20shape%20exploration%2C%20we%20combine%20the%20diffusion%20model%20with%0Areinforcement%20learning%20to%20train%20a%20policy.%20This%20involves%20using%20the%20generated%0Alatent%20vector%20from%20the%20diffusion%20model%20to%20guide%20the%20touch%20exploration%20policy%0Atraining%20through%20a%20novel%20reward%20design.%20Experiments%20validate%20the%20reconstruction%0Aquality%20thorough%20both%20qualitatively%20and%20quantitative%20analysis%2C%20and%20our%20touch%0Aexploration%20policy%20further%20boosts%20reconstruction%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTouch2Shape%253A%2520Touch-Conditioned%25203D%2520Diffusion%2520for%2520Shape%2520Exploration%2520and%250A%2520%2520Reconstruction%26entry.906535625%3DYuanbo%2520Wang%2520and%2520Zhaoxuan%2520Zhang%2520and%2520Jiajin%2520Qiu%2520and%2520Dilong%2520Sun%2520and%2520Zhengyu%2520Meng%2520and%2520Xiaopeng%2520Wei%2520and%2520Xin%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520made%2520breakthroughs%2520in%25203D%2520generation%2520tasks.%2520Current%25203D%250Adiffusion%2520models%2520focus%2520on%2520reconstructing%2520target%2520shape%2520from%2520images%2520or%2520a%2520set%2520of%250Apartial%2520observations.%2520While%2520excelling%2520in%2520global%2520context%2520understanding%252C%2520they%250Astruggle%2520to%2520capture%2520the%2520local%2520details%2520of%2520complex%2520shapes%2520and%2520limited%2520to%2520the%250Aocclusion%2520and%2520lighting%2520conditions.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520utilize%250Atactile%2520images%2520to%2520capture%2520the%2520local%25203D%2520information%2520and%2520propose%2520a%2520Touch2Shape%250Amodel%252C%2520which%2520leverages%2520a%2520touch-conditioned%2520diffusion%2520model%2520to%2520explore%2520and%250Areconstruct%2520the%2520target%2520shape%2520from%2520touch.%2520For%2520shape%2520reconstruction%252C%2520we%2520have%250Adeveloped%2520a%2520touch%2520embedding%2520module%2520to%2520condition%2520the%2520diffusion%2520model%2520in%2520creating%250Aa%2520compact%2520representation%2520and%2520a%2520touch%2520shape%2520fusion%2520module%2520to%2520refine%2520the%250Areconstructed%2520shape.%2520For%2520shape%2520exploration%252C%2520we%2520combine%2520the%2520diffusion%2520model%2520with%250Areinforcement%2520learning%2520to%2520train%2520a%2520policy.%2520This%2520involves%2520using%2520the%2520generated%250Alatent%2520vector%2520from%2520the%2520diffusion%2520model%2520to%2520guide%2520the%2520touch%2520exploration%2520policy%250Atraining%2520through%2520a%2520novel%2520reward%2520design.%2520Experiments%2520validate%2520the%2520reconstruction%250Aquality%2520thorough%2520both%2520qualitatively%2520and%2520quantitative%2520analysis%252C%2520and%2520our%2520touch%250Aexploration%2520policy%2520further%2520boosts%2520reconstruction%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Touch2Shape%3A%20Touch-Conditioned%203D%20Diffusion%20for%20Shape%20Exploration%20and%0A%20%20Reconstruction&entry.906535625=Yuanbo%20Wang%20and%20Zhaoxuan%20Zhang%20and%20Jiajin%20Qiu%20and%20Dilong%20Sun%20and%20Zhengyu%20Meng%20and%20Xiaopeng%20Wei%20and%20Xin%20Yang&entry.1292438233=%20%20Diffusion%20models%20have%20made%20breakthroughs%20in%203D%20generation%20tasks.%20Current%203D%0Adiffusion%20models%20focus%20on%20reconstructing%20target%20shape%20from%20images%20or%20a%20set%20of%0Apartial%20observations.%20While%20excelling%20in%20global%20context%20understanding%2C%20they%0Astruggle%20to%20capture%20the%20local%20details%20of%20complex%20shapes%20and%20limited%20to%20the%0Aocclusion%20and%20lighting%20conditions.%20To%20overcome%20these%20limitations%2C%20we%20utilize%0Atactile%20images%20to%20capture%20the%20local%203D%20information%20and%20propose%20a%20Touch2Shape%0Amodel%2C%20which%20leverages%20a%20touch-conditioned%20diffusion%20model%20to%20explore%20and%0Areconstruct%20the%20target%20shape%20from%20touch.%20For%20shape%20reconstruction%2C%20we%20have%0Adeveloped%20a%20touch%20embedding%20module%20to%20condition%20the%20diffusion%20model%20in%20creating%0Aa%20compact%20representation%20and%20a%20touch%20shape%20fusion%20module%20to%20refine%20the%0Areconstructed%20shape.%20For%20shape%20exploration%2C%20we%20combine%20the%20diffusion%20model%20with%0Areinforcement%20learning%20to%20train%20a%20policy.%20This%20involves%20using%20the%20generated%0Alatent%20vector%20from%20the%20diffusion%20model%20to%20guide%20the%20touch%20exploration%20policy%0Atraining%20through%20a%20novel%20reward%20design.%20Experiments%20validate%20the%20reconstruction%0Aquality%20thorough%20both%20qualitatively%20and%20quantitative%20analysis%2C%20and%20our%20touch%0Aexploration%20policy%20further%20boosts%20reconstruction%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13091v1&entry.124074799=Read"},
{"title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and\n  Voices of Multiple Speakers", "author": "Kyeongman Park and Seongho Joo and Kyomin Jung", "abstract": "  We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies.\n", "link": "http://arxiv.org/abs/2505.13082v1", "date": "2025-05-19", "relevancy": 2.3973, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiActor-Audiobook%3A%20Zero-Shot%20Audiobook%20Generation%20with%20Faces%20and%0A%20%20Voices%20of%20Multiple%20Speakers&body=Title%3A%20MultiActor-Audiobook%3A%20Zero-Shot%20Audiobook%20Generation%20with%20Faces%20and%0A%20%20Voices%20of%20Multiple%20Speakers%0AAuthor%3A%20Kyeongman%20Park%20and%20Seongho%20Joo%20and%20Kyomin%20Jung%0AAbstract%3A%20%20%20We%20introduce%20MultiActor-Audiobook%2C%20a%20zero-shot%20approach%20for%20generating%0Aaudiobooks%20that%20automatically%20produces%20consistent%2C%20expressive%2C%20and%0Aspeaker-appropriate%20prosody%2C%20including%20intonation%20and%20emotion.%20Previous%0Aaudiobook%20systems%20have%20several%20limitations%3A%20they%20require%20users%20to%20manually%0Aconfigure%20the%20speaker%27s%20prosody%2C%20read%20each%20sentence%20with%20a%20monotonic%20tone%0Acompared%20to%20voice%20actors%2C%20or%20rely%20on%20costly%20training.%20However%2C%20our%0AMultiActor-Audiobook%20addresses%20these%20issues%20by%20introducing%20two%20novel%20processes%3A%0A%281%29%20MSP%20%28%2A%2AMultimodal%20Speaker%20Persona%20Generation%2A%2A%29%20and%20%282%29%20LSI%20%28%2A%2ALLM-based%0AScript%20Instruction%20Generation%2A%2A%29.%20With%20these%20two%20processes%2C%0AMultiActor-Audiobook%20can%20generate%20more%20emotionally%20expressive%20audiobooks%20with%20a%0Aconsistent%20speaker%20prosody%20without%20additional%20training.%20We%20compare%20our%20system%0Awith%20commercial%20products%2C%20through%20human%20and%20MLLM%20evaluations%2C%20achieving%0Acompetitive%20results.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20MSP%20and%0ALSI%20through%20ablation%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiActor-Audiobook%253A%2520Zero-Shot%2520Audiobook%2520Generation%2520with%2520Faces%2520and%250A%2520%2520Voices%2520of%2520Multiple%2520Speakers%26entry.906535625%3DKyeongman%2520Park%2520and%2520Seongho%2520Joo%2520and%2520Kyomin%2520Jung%26entry.1292438233%3D%2520%2520We%2520introduce%2520MultiActor-Audiobook%252C%2520a%2520zero-shot%2520approach%2520for%2520generating%250Aaudiobooks%2520that%2520automatically%2520produces%2520consistent%252C%2520expressive%252C%2520and%250Aspeaker-appropriate%2520prosody%252C%2520including%2520intonation%2520and%2520emotion.%2520Previous%250Aaudiobook%2520systems%2520have%2520several%2520limitations%253A%2520they%2520require%2520users%2520to%2520manually%250Aconfigure%2520the%2520speaker%2527s%2520prosody%252C%2520read%2520each%2520sentence%2520with%2520a%2520monotonic%2520tone%250Acompared%2520to%2520voice%2520actors%252C%2520or%2520rely%2520on%2520costly%2520training.%2520However%252C%2520our%250AMultiActor-Audiobook%2520addresses%2520these%2520issues%2520by%2520introducing%2520two%2520novel%2520processes%253A%250A%25281%2529%2520MSP%2520%2528%252A%252AMultimodal%2520Speaker%2520Persona%2520Generation%252A%252A%2529%2520and%2520%25282%2529%2520LSI%2520%2528%252A%252ALLM-based%250AScript%2520Instruction%2520Generation%252A%252A%2529.%2520With%2520these%2520two%2520processes%252C%250AMultiActor-Audiobook%2520can%2520generate%2520more%2520emotionally%2520expressive%2520audiobooks%2520with%2520a%250Aconsistent%2520speaker%2520prosody%2520without%2520additional%2520training.%2520We%2520compare%2520our%2520system%250Awith%2520commercial%2520products%252C%2520through%2520human%2520and%2520MLLM%2520evaluations%252C%2520achieving%250Acompetitive%2520results.%2520Furthermore%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520MSP%2520and%250ALSI%2520through%2520ablation%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiActor-Audiobook%3A%20Zero-Shot%20Audiobook%20Generation%20with%20Faces%20and%0A%20%20Voices%20of%20Multiple%20Speakers&entry.906535625=Kyeongman%20Park%20and%20Seongho%20Joo%20and%20Kyomin%20Jung&entry.1292438233=%20%20We%20introduce%20MultiActor-Audiobook%2C%20a%20zero-shot%20approach%20for%20generating%0Aaudiobooks%20that%20automatically%20produces%20consistent%2C%20expressive%2C%20and%0Aspeaker-appropriate%20prosody%2C%20including%20intonation%20and%20emotion.%20Previous%0Aaudiobook%20systems%20have%20several%20limitations%3A%20they%20require%20users%20to%20manually%0Aconfigure%20the%20speaker%27s%20prosody%2C%20read%20each%20sentence%20with%20a%20monotonic%20tone%0Acompared%20to%20voice%20actors%2C%20or%20rely%20on%20costly%20training.%20However%2C%20our%0AMultiActor-Audiobook%20addresses%20these%20issues%20by%20introducing%20two%20novel%20processes%3A%0A%281%29%20MSP%20%28%2A%2AMultimodal%20Speaker%20Persona%20Generation%2A%2A%29%20and%20%282%29%20LSI%20%28%2A%2ALLM-based%0AScript%20Instruction%20Generation%2A%2A%29.%20With%20these%20two%20processes%2C%0AMultiActor-Audiobook%20can%20generate%20more%20emotionally%20expressive%20audiobooks%20with%20a%0Aconsistent%20speaker%20prosody%20without%20additional%20training.%20We%20compare%20our%20system%0Awith%20commercial%20products%2C%20through%20human%20and%20MLLM%20evaluations%2C%20achieving%0Acompetitive%20results.%20Furthermore%2C%20we%20demonstrate%20the%20effectiveness%20of%20MSP%20and%0ALSI%20through%20ablation%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13082v1&entry.124074799=Read"},
{"title": "A generalisable head MRI defacing pipeline: Evaluation on 2,566\n  meningioma scans", "author": "Lorena Garcia-Foncillas Macias and Aaron Kujawa and Aya Elshalakany and Jonathan Shapey and Tom Vercauteren", "abstract": "  Reliable MRI defacing techniques to safeguard patient privacy while\npreserving brain anatomy are critical for research collaboration. Existing\nmethods often struggle with incomplete defacing or degradation of brain tissue\nregions. We present a robust, generalisable defacing pipeline for\nhigh-resolution MRI that integrates atlas-based registration with brain\nmasking. Our method was evaluated on 2,566 heterogeneous clinical scans for\nmeningioma and achieved a 99.92 per cent success rate (2,564/2,566) upon visual\ninspection. Excellent anatomical preservation is demonstrated with a Dice\nsimilarity coefficient of 0.9975 plus or minus 0.0023 between brain masks\nautomatically extracted from the original and defaced volumes. Source code is\navailable at https://github.com/cai4cai/defacing_pipeline.\n", "link": "http://arxiv.org/abs/2505.12999v1", "date": "2025-05-19", "relevancy": 2.3837, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5028}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4652}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20generalisable%20head%20MRI%20defacing%20pipeline%3A%20Evaluation%20on%202%2C566%0A%20%20meningioma%20scans&body=Title%3A%20A%20generalisable%20head%20MRI%20defacing%20pipeline%3A%20Evaluation%20on%202%2C566%0A%20%20meningioma%20scans%0AAuthor%3A%20Lorena%20Garcia-Foncillas%20Macias%20and%20Aaron%20Kujawa%20and%20Aya%20Elshalakany%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren%0AAbstract%3A%20%20%20Reliable%20MRI%20defacing%20techniques%20to%20safeguard%20patient%20privacy%20while%0Apreserving%20brain%20anatomy%20are%20critical%20for%20research%20collaboration.%20Existing%0Amethods%20often%20struggle%20with%20incomplete%20defacing%20or%20degradation%20of%20brain%20tissue%0Aregions.%20We%20present%20a%20robust%2C%20generalisable%20defacing%20pipeline%20for%0Ahigh-resolution%20MRI%20that%20integrates%20atlas-based%20registration%20with%20brain%0Amasking.%20Our%20method%20was%20evaluated%20on%202%2C566%20heterogeneous%20clinical%20scans%20for%0Ameningioma%20and%20achieved%20a%2099.92%20per%20cent%20success%20rate%20%282%2C564/2%2C566%29%20upon%20visual%0Ainspection.%20Excellent%20anatomical%20preservation%20is%20demonstrated%20with%20a%20Dice%0Asimilarity%20coefficient%20of%200.9975%20plus%20or%20minus%200.0023%20between%20brain%20masks%0Aautomatically%20extracted%20from%20the%20original%20and%20defaced%20volumes.%20Source%20code%20is%0Aavailable%20at%20https%3A//github.com/cai4cai/defacing_pipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520generalisable%2520head%2520MRI%2520defacing%2520pipeline%253A%2520Evaluation%2520on%25202%252C566%250A%2520%2520meningioma%2520scans%26entry.906535625%3DLorena%2520Garcia-Foncillas%2520Macias%2520and%2520Aaron%2520Kujawa%2520and%2520Aya%2520Elshalakany%2520and%2520Jonathan%2520Shapey%2520and%2520Tom%2520Vercauteren%26entry.1292438233%3D%2520%2520Reliable%2520MRI%2520defacing%2520techniques%2520to%2520safeguard%2520patient%2520privacy%2520while%250Apreserving%2520brain%2520anatomy%2520are%2520critical%2520for%2520research%2520collaboration.%2520Existing%250Amethods%2520often%2520struggle%2520with%2520incomplete%2520defacing%2520or%2520degradation%2520of%2520brain%2520tissue%250Aregions.%2520We%2520present%2520a%2520robust%252C%2520generalisable%2520defacing%2520pipeline%2520for%250Ahigh-resolution%2520MRI%2520that%2520integrates%2520atlas-based%2520registration%2520with%2520brain%250Amasking.%2520Our%2520method%2520was%2520evaluated%2520on%25202%252C566%2520heterogeneous%2520clinical%2520scans%2520for%250Ameningioma%2520and%2520achieved%2520a%252099.92%2520per%2520cent%2520success%2520rate%2520%25282%252C564/2%252C566%2529%2520upon%2520visual%250Ainspection.%2520Excellent%2520anatomical%2520preservation%2520is%2520demonstrated%2520with%2520a%2520Dice%250Asimilarity%2520coefficient%2520of%25200.9975%2520plus%2520or%2520minus%25200.0023%2520between%2520brain%2520masks%250Aautomatically%2520extracted%2520from%2520the%2520original%2520and%2520defaced%2520volumes.%2520Source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/cai4cai/defacing_pipeline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20generalisable%20head%20MRI%20defacing%20pipeline%3A%20Evaluation%20on%202%2C566%0A%20%20meningioma%20scans&entry.906535625=Lorena%20Garcia-Foncillas%20Macias%20and%20Aaron%20Kujawa%20and%20Aya%20Elshalakany%20and%20Jonathan%20Shapey%20and%20Tom%20Vercauteren&entry.1292438233=%20%20Reliable%20MRI%20defacing%20techniques%20to%20safeguard%20patient%20privacy%20while%0Apreserving%20brain%20anatomy%20are%20critical%20for%20research%20collaboration.%20Existing%0Amethods%20often%20struggle%20with%20incomplete%20defacing%20or%20degradation%20of%20brain%20tissue%0Aregions.%20We%20present%20a%20robust%2C%20generalisable%20defacing%20pipeline%20for%0Ahigh-resolution%20MRI%20that%20integrates%20atlas-based%20registration%20with%20brain%0Amasking.%20Our%20method%20was%20evaluated%20on%202%2C566%20heterogeneous%20clinical%20scans%20for%0Ameningioma%20and%20achieved%20a%2099.92%20per%20cent%20success%20rate%20%282%2C564/2%2C566%29%20upon%20visual%0Ainspection.%20Excellent%20anatomical%20preservation%20is%20demonstrated%20with%20a%20Dice%0Asimilarity%20coefficient%20of%200.9975%20plus%20or%20minus%200.0023%20between%20brain%20masks%0Aautomatically%20extracted%20from%20the%20original%20and%20defaced%20volumes.%20Source%20code%20is%0Aavailable%20at%20https%3A//github.com/cai4cai/defacing_pipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12999v1&entry.124074799=Read"},
{"title": "Greed is Good: A Unifying Perspective on Guided Generation", "author": "Zander W. Blasingame and Chen Liu", "abstract": "  Training-free guided generation is a widely used and powerful technique that\nallows the end user to exert further control over the generative process of\nflow/diffusion models. Generally speaking, two families of techniques have\nemerged for solving this problem for gradient-based guidance: namely, posterior\nguidance (i.e., guidance via projecting the current sample to the target\ndistribution via the target prediction model) and end-to-end guidance (i.e.,\nguidance by performing backpropagation throughout the entire ODE solve). In\nthis work, we show that these two seemingly separate families can actually be\nunified by looking at posterior guidance as a greedy strategy of end-to-end\nguidance. We explore the theoretical connections between these two families and\nprovide an in-depth theoretical of these two techniques relative to the\ncontinuous ideal gradients. Motivated by this analysis we then show a method\nfor interpolating between these two families enabling a trade-off between\ncompute and accuracy of the guidance gradients. We then validate this work on\nseveral inverse image problems and property-guided molecular generation.\n", "link": "http://arxiv.org/abs/2502.08006v2", "date": "2025-05-19", "relevancy": 2.3817, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6159}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5812}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Greed%20is%20Good%3A%20A%20Unifying%20Perspective%20on%20Guided%20Generation&body=Title%3A%20Greed%20is%20Good%3A%20A%20Unifying%20Perspective%20on%20Guided%20Generation%0AAuthor%3A%20Zander%20W.%20Blasingame%20and%20Chen%20Liu%0AAbstract%3A%20%20%20Training-free%20guided%20generation%20is%20a%20widely%20used%20and%20powerful%20technique%20that%0Aallows%20the%20end%20user%20to%20exert%20further%20control%20over%20the%20generative%20process%20of%0Aflow/diffusion%20models.%20Generally%20speaking%2C%20two%20families%20of%20techniques%20have%0Aemerged%20for%20solving%20this%20problem%20for%20gradient-based%20guidance%3A%20namely%2C%20posterior%0Aguidance%20%28i.e.%2C%20guidance%20via%20projecting%20the%20current%20sample%20to%20the%20target%0Adistribution%20via%20the%20target%20prediction%20model%29%20and%20end-to-end%20guidance%20%28i.e.%2C%0Aguidance%20by%20performing%20backpropagation%20throughout%20the%20entire%20ODE%20solve%29.%20In%0Athis%20work%2C%20we%20show%20that%20these%20two%20seemingly%20separate%20families%20can%20actually%20be%0Aunified%20by%20looking%20at%20posterior%20guidance%20as%20a%20greedy%20strategy%20of%20end-to-end%0Aguidance.%20We%20explore%20the%20theoretical%20connections%20between%20these%20two%20families%20and%0Aprovide%20an%20in-depth%20theoretical%20of%20these%20two%20techniques%20relative%20to%20the%0Acontinuous%20ideal%20gradients.%20Motivated%20by%20this%20analysis%20we%20then%20show%20a%20method%0Afor%20interpolating%20between%20these%20two%20families%20enabling%20a%20trade-off%20between%0Acompute%20and%20accuracy%20of%20the%20guidance%20gradients.%20We%20then%20validate%20this%20work%20on%0Aseveral%20inverse%20image%20problems%20and%20property-guided%20molecular%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGreed%2520is%2520Good%253A%2520A%2520Unifying%2520Perspective%2520on%2520Guided%2520Generation%26entry.906535625%3DZander%2520W.%2520Blasingame%2520and%2520Chen%2520Liu%26entry.1292438233%3D%2520%2520Training-free%2520guided%2520generation%2520is%2520a%2520widely%2520used%2520and%2520powerful%2520technique%2520that%250Aallows%2520the%2520end%2520user%2520to%2520exert%2520further%2520control%2520over%2520the%2520generative%2520process%2520of%250Aflow/diffusion%2520models.%2520Generally%2520speaking%252C%2520two%2520families%2520of%2520techniques%2520have%250Aemerged%2520for%2520solving%2520this%2520problem%2520for%2520gradient-based%2520guidance%253A%2520namely%252C%2520posterior%250Aguidance%2520%2528i.e.%252C%2520guidance%2520via%2520projecting%2520the%2520current%2520sample%2520to%2520the%2520target%250Adistribution%2520via%2520the%2520target%2520prediction%2520model%2529%2520and%2520end-to-end%2520guidance%2520%2528i.e.%252C%250Aguidance%2520by%2520performing%2520backpropagation%2520throughout%2520the%2520entire%2520ODE%2520solve%2529.%2520In%250Athis%2520work%252C%2520we%2520show%2520that%2520these%2520two%2520seemingly%2520separate%2520families%2520can%2520actually%2520be%250Aunified%2520by%2520looking%2520at%2520posterior%2520guidance%2520as%2520a%2520greedy%2520strategy%2520of%2520end-to-end%250Aguidance.%2520We%2520explore%2520the%2520theoretical%2520connections%2520between%2520these%2520two%2520families%2520and%250Aprovide%2520an%2520in-depth%2520theoretical%2520of%2520these%2520two%2520techniques%2520relative%2520to%2520the%250Acontinuous%2520ideal%2520gradients.%2520Motivated%2520by%2520this%2520analysis%2520we%2520then%2520show%2520a%2520method%250Afor%2520interpolating%2520between%2520these%2520two%2520families%2520enabling%2520a%2520trade-off%2520between%250Acompute%2520and%2520accuracy%2520of%2520the%2520guidance%2520gradients.%2520We%2520then%2520validate%2520this%2520work%2520on%250Aseveral%2520inverse%2520image%2520problems%2520and%2520property-guided%2520molecular%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Greed%20is%20Good%3A%20A%20Unifying%20Perspective%20on%20Guided%20Generation&entry.906535625=Zander%20W.%20Blasingame%20and%20Chen%20Liu&entry.1292438233=%20%20Training-free%20guided%20generation%20is%20a%20widely%20used%20and%20powerful%20technique%20that%0Aallows%20the%20end%20user%20to%20exert%20further%20control%20over%20the%20generative%20process%20of%0Aflow/diffusion%20models.%20Generally%20speaking%2C%20two%20families%20of%20techniques%20have%0Aemerged%20for%20solving%20this%20problem%20for%20gradient-based%20guidance%3A%20namely%2C%20posterior%0Aguidance%20%28i.e.%2C%20guidance%20via%20projecting%20the%20current%20sample%20to%20the%20target%0Adistribution%20via%20the%20target%20prediction%20model%29%20and%20end-to-end%20guidance%20%28i.e.%2C%0Aguidance%20by%20performing%20backpropagation%20throughout%20the%20entire%20ODE%20solve%29.%20In%0Athis%20work%2C%20we%20show%20that%20these%20two%20seemingly%20separate%20families%20can%20actually%20be%0Aunified%20by%20looking%20at%20posterior%20guidance%20as%20a%20greedy%20strategy%20of%20end-to-end%0Aguidance.%20We%20explore%20the%20theoretical%20connections%20between%20these%20two%20families%20and%0Aprovide%20an%20in-depth%20theoretical%20of%20these%20two%20techniques%20relative%20to%20the%0Acontinuous%20ideal%20gradients.%20Motivated%20by%20this%20analysis%20we%20then%20show%20a%20method%0Afor%20interpolating%20between%20these%20two%20families%20enabling%20a%20trade-off%20between%0Acompute%20and%20accuracy%20of%20the%20guidance%20gradients.%20We%20then%20validate%20this%20work%20on%0Aseveral%20inverse%20image%20problems%20and%20property-guided%20molecular%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08006v2&entry.124074799=Read"},
{"title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "author": "Guangda Liu and Chengwei Li and Zhenyu Ning and Jing Lin and Yiwu Yao and Danning Ke and Minyi Guo and Jieru Zhao", "abstract": "  Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.\n", "link": "http://arxiv.org/abs/2505.13109v1", "date": "2025-05-19", "relevancy": 2.3787, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeKV%3A%20Boosting%20KV%20Cache%20Retrieval%20for%20Efficient%20LLM%20Inference&body=Title%3A%20FreeKV%3A%20Boosting%20KV%20Cache%20Retrieval%20for%20Efficient%20LLM%20Inference%0AAuthor%3A%20Guangda%20Liu%20and%20Chengwei%20Li%20and%20Zhenyu%20Ning%20and%20Jing%20Lin%20and%20Yiwu%20Yao%20and%20Danning%20Ke%20and%20Minyi%20Guo%20and%20Jieru%20Zhao%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20deployed%20with%20rapidly%20expanding%0Acontext%20windows%20to%20support%20increasingly%20demanding%20applications.%20However%2C%20long%0Acontexts%20pose%20significant%20deployment%20challenges%2C%20primarily%20due%20to%20the%20KV%20cache%0Awhose%20size%20grows%20proportionally%20with%20context%20length.%20While%20KV%20cache%20compression%0Amethods%20are%20proposed%20to%20address%20this%20issue%2C%20KV%20dropping%20methods%20incur%0Aconsiderable%20accuracy%20loss%2C%20and%20KV%20retrieval%20methods%20suffer%20from%20significant%0Aefficiency%20bottlenecks.%20We%20propose%20FreeKV%2C%20an%20algorithm-system%20co-optimization%0Aframework%20to%20enhance%20KV%20retrieval%20efficiency%20while%20preserving%20accuracy.%20On%20the%0Aalgorithm%20side%2C%20FreeKV%20introduces%20speculative%20retrieval%20to%20shift%20the%20KV%0Aselection%20and%20recall%20processes%20out%20of%20the%20critical%20path%2C%20combined%20with%0Afine-grained%20correction%20to%20ensure%20accuracy.%20On%20the%20system%20side%2C%20FreeKV%20employs%0Ahybrid%20KV%20layouts%20across%20CPU%20and%20GPU%20memory%20to%20eliminate%20fragmented%20data%0Atransfers%2C%20and%20leverages%20double-buffered%20streamed%20recall%20to%20further%20improve%0Aefficiency.%20Experiments%20demonstrate%20that%20FreeKV%20achieves%20near-lossless%20accuracy%0Aacross%20various%20scenarios%20and%20models%2C%20delivering%20up%20to%2013%24%5Ctimes%24%20speedup%0Acompared%20to%20SOTA%20KV%20retrieval%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeKV%253A%2520Boosting%2520KV%2520Cache%2520Retrieval%2520for%2520Efficient%2520LLM%2520Inference%26entry.906535625%3DGuangda%2520Liu%2520and%2520Chengwei%2520Li%2520and%2520Zhenyu%2520Ning%2520and%2520Jing%2520Lin%2520and%2520Yiwu%2520Yao%2520and%2520Danning%2520Ke%2520and%2520Minyi%2520Guo%2520and%2520Jieru%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520deployed%2520with%2520rapidly%2520expanding%250Acontext%2520windows%2520to%2520support%2520increasingly%2520demanding%2520applications.%2520However%252C%2520long%250Acontexts%2520pose%2520significant%2520deployment%2520challenges%252C%2520primarily%2520due%2520to%2520the%2520KV%2520cache%250Awhose%2520size%2520grows%2520proportionally%2520with%2520context%2520length.%2520While%2520KV%2520cache%2520compression%250Amethods%2520are%2520proposed%2520to%2520address%2520this%2520issue%252C%2520KV%2520dropping%2520methods%2520incur%250Aconsiderable%2520accuracy%2520loss%252C%2520and%2520KV%2520retrieval%2520methods%2520suffer%2520from%2520significant%250Aefficiency%2520bottlenecks.%2520We%2520propose%2520FreeKV%252C%2520an%2520algorithm-system%2520co-optimization%250Aframework%2520to%2520enhance%2520KV%2520retrieval%2520efficiency%2520while%2520preserving%2520accuracy.%2520On%2520the%250Aalgorithm%2520side%252C%2520FreeKV%2520introduces%2520speculative%2520retrieval%2520to%2520shift%2520the%2520KV%250Aselection%2520and%2520recall%2520processes%2520out%2520of%2520the%2520critical%2520path%252C%2520combined%2520with%250Afine-grained%2520correction%2520to%2520ensure%2520accuracy.%2520On%2520the%2520system%2520side%252C%2520FreeKV%2520employs%250Ahybrid%2520KV%2520layouts%2520across%2520CPU%2520and%2520GPU%2520memory%2520to%2520eliminate%2520fragmented%2520data%250Atransfers%252C%2520and%2520leverages%2520double-buffered%2520streamed%2520recall%2520to%2520further%2520improve%250Aefficiency.%2520Experiments%2520demonstrate%2520that%2520FreeKV%2520achieves%2520near-lossless%2520accuracy%250Aacross%2520various%2520scenarios%2520and%2520models%252C%2520delivering%2520up%2520to%252013%2524%255Ctimes%2524%2520speedup%250Acompared%2520to%2520SOTA%2520KV%2520retrieval%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeKV%3A%20Boosting%20KV%20Cache%20Retrieval%20for%20Efficient%20LLM%20Inference&entry.906535625=Guangda%20Liu%20and%20Chengwei%20Li%20and%20Zhenyu%20Ning%20and%20Jing%20Lin%20and%20Yiwu%20Yao%20and%20Danning%20Ke%20and%20Minyi%20Guo%20and%20Jieru%20Zhao&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20been%20widely%20deployed%20with%20rapidly%20expanding%0Acontext%20windows%20to%20support%20increasingly%20demanding%20applications.%20However%2C%20long%0Acontexts%20pose%20significant%20deployment%20challenges%2C%20primarily%20due%20to%20the%20KV%20cache%0Awhose%20size%20grows%20proportionally%20with%20context%20length.%20While%20KV%20cache%20compression%0Amethods%20are%20proposed%20to%20address%20this%20issue%2C%20KV%20dropping%20methods%20incur%0Aconsiderable%20accuracy%20loss%2C%20and%20KV%20retrieval%20methods%20suffer%20from%20significant%0Aefficiency%20bottlenecks.%20We%20propose%20FreeKV%2C%20an%20algorithm-system%20co-optimization%0Aframework%20to%20enhance%20KV%20retrieval%20efficiency%20while%20preserving%20accuracy.%20On%20the%0Aalgorithm%20side%2C%20FreeKV%20introduces%20speculative%20retrieval%20to%20shift%20the%20KV%0Aselection%20and%20recall%20processes%20out%20of%20the%20critical%20path%2C%20combined%20with%0Afine-grained%20correction%20to%20ensure%20accuracy.%20On%20the%20system%20side%2C%20FreeKV%20employs%0Ahybrid%20KV%20layouts%20across%20CPU%20and%20GPU%20memory%20to%20eliminate%20fragmented%20data%0Atransfers%2C%20and%20leverages%20double-buffered%20streamed%20recall%20to%20further%20improve%0Aefficiency.%20Experiments%20demonstrate%20that%20FreeKV%20achieves%20near-lossless%20accuracy%0Aacross%20various%20scenarios%20and%20models%2C%20delivering%20up%20to%2013%24%5Ctimes%24%20speedup%0Acompared%20to%20SOTA%20KV%20retrieval%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13109v1&entry.124074799=Read"},
{"title": "Event-Driven Dynamic Scene Depth Completion", "author": "Zhiqiang Yan and Jianhao Jiao and Zhengxue Wang and Gim Hee Lee", "abstract": "  Depth completion in dynamic scenes poses significant challenges due to rapid\nego-motion and object motion, which can severely degrade the quality of input\nmodalities such as RGB images and LiDAR measurements. Conventional RGB-D\nsensors often struggle to align precisely and capture reliable depth under such\nconditions. In contrast, event cameras with their high temporal resolution and\nsensitivity to motion at the pixel level provide complementary cues that are\n%particularly beneficial in dynamic environments.To this end, we propose\nEventDC, the first event-driven depth completion framework. It consists of two\nkey components: Event-Modulated Alignment (EMA) and Local Depth Filtering\n(LDF). Both modules adaptively learn the two fundamental components of\nconvolution operations: offsets and weights conditioned on motion-sensitive\nevent streams. In the encoder, EMA leverages events to modulate the sampling\npositions of RGB-D features to achieve pixel redistribution for improved\nalignment and fusion. In the decoder, LDF refines depth estimations around\nmoving objects by learning motion-aware masks from events. Additionally,\nEventDC incorporates two loss terms to further benefit global alignment and\nenhance local depth recovery. Moreover, we establish the first benchmark for\nevent-based depth completion comprising one real-world and two synthetic\ndatasets to facilitate future research. Extensive experiments on this benchmark\ndemonstrate the superiority of our EventDC.\n", "link": "http://arxiv.org/abs/2505.13279v1", "date": "2025-05-19", "relevancy": 2.3776, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-Driven%20Dynamic%20Scene%20Depth%20Completion&body=Title%3A%20Event-Driven%20Dynamic%20Scene%20Depth%20Completion%0AAuthor%3A%20Zhiqiang%20Yan%20and%20Jianhao%20Jiao%20and%20Zhengxue%20Wang%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20Depth%20completion%20in%20dynamic%20scenes%20poses%20significant%20challenges%20due%20to%20rapid%0Aego-motion%20and%20object%20motion%2C%20which%20can%20severely%20degrade%20the%20quality%20of%20input%0Amodalities%20such%20as%20RGB%20images%20and%20LiDAR%20measurements.%20Conventional%20RGB-D%0Asensors%20often%20struggle%20to%20align%20precisely%20and%20capture%20reliable%20depth%20under%20such%0Aconditions.%20In%20contrast%2C%20event%20cameras%20with%20their%20high%20temporal%20resolution%20and%0Asensitivity%20to%20motion%20at%20the%20pixel%20level%20provide%20complementary%20cues%20that%20are%0A%25particularly%20beneficial%20in%20dynamic%20environments.To%20this%20end%2C%20we%20propose%0AEventDC%2C%20the%20first%20event-driven%20depth%20completion%20framework.%20It%20consists%20of%20two%0Akey%20components%3A%20Event-Modulated%20Alignment%20%28EMA%29%20and%20Local%20Depth%20Filtering%0A%28LDF%29.%20Both%20modules%20adaptively%20learn%20the%20two%20fundamental%20components%20of%0Aconvolution%20operations%3A%20offsets%20and%20weights%20conditioned%20on%20motion-sensitive%0Aevent%20streams.%20In%20the%20encoder%2C%20EMA%20leverages%20events%20to%20modulate%20the%20sampling%0Apositions%20of%20RGB-D%20features%20to%20achieve%20pixel%20redistribution%20for%20improved%0Aalignment%20and%20fusion.%20In%20the%20decoder%2C%20LDF%20refines%20depth%20estimations%20around%0Amoving%20objects%20by%20learning%20motion-aware%20masks%20from%20events.%20Additionally%2C%0AEventDC%20incorporates%20two%20loss%20terms%20to%20further%20benefit%20global%20alignment%20and%0Aenhance%20local%20depth%20recovery.%20Moreover%2C%20we%20establish%20the%20first%20benchmark%20for%0Aevent-based%20depth%20completion%20comprising%20one%20real-world%20and%20two%20synthetic%0Adatasets%20to%20facilitate%20future%20research.%20Extensive%20experiments%20on%20this%20benchmark%0Ademonstrate%20the%20superiority%20of%20our%20EventDC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-Driven%2520Dynamic%2520Scene%2520Depth%2520Completion%26entry.906535625%3DZhiqiang%2520Yan%2520and%2520Jianhao%2520Jiao%2520and%2520Zhengxue%2520Wang%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520Depth%2520completion%2520in%2520dynamic%2520scenes%2520poses%2520significant%2520challenges%2520due%2520to%2520rapid%250Aego-motion%2520and%2520object%2520motion%252C%2520which%2520can%2520severely%2520degrade%2520the%2520quality%2520of%2520input%250Amodalities%2520such%2520as%2520RGB%2520images%2520and%2520LiDAR%2520measurements.%2520Conventional%2520RGB-D%250Asensors%2520often%2520struggle%2520to%2520align%2520precisely%2520and%2520capture%2520reliable%2520depth%2520under%2520such%250Aconditions.%2520In%2520contrast%252C%2520event%2520cameras%2520with%2520their%2520high%2520temporal%2520resolution%2520and%250Asensitivity%2520to%2520motion%2520at%2520the%2520pixel%2520level%2520provide%2520complementary%2520cues%2520that%2520are%250A%2525particularly%2520beneficial%2520in%2520dynamic%2520environments.To%2520this%2520end%252C%2520we%2520propose%250AEventDC%252C%2520the%2520first%2520event-driven%2520depth%2520completion%2520framework.%2520It%2520consists%2520of%2520two%250Akey%2520components%253A%2520Event-Modulated%2520Alignment%2520%2528EMA%2529%2520and%2520Local%2520Depth%2520Filtering%250A%2528LDF%2529.%2520Both%2520modules%2520adaptively%2520learn%2520the%2520two%2520fundamental%2520components%2520of%250Aconvolution%2520operations%253A%2520offsets%2520and%2520weights%2520conditioned%2520on%2520motion-sensitive%250Aevent%2520streams.%2520In%2520the%2520encoder%252C%2520EMA%2520leverages%2520events%2520to%2520modulate%2520the%2520sampling%250Apositions%2520of%2520RGB-D%2520features%2520to%2520achieve%2520pixel%2520redistribution%2520for%2520improved%250Aalignment%2520and%2520fusion.%2520In%2520the%2520decoder%252C%2520LDF%2520refines%2520depth%2520estimations%2520around%250Amoving%2520objects%2520by%2520learning%2520motion-aware%2520masks%2520from%2520events.%2520Additionally%252C%250AEventDC%2520incorporates%2520two%2520loss%2520terms%2520to%2520further%2520benefit%2520global%2520alignment%2520and%250Aenhance%2520local%2520depth%2520recovery.%2520Moreover%252C%2520we%2520establish%2520the%2520first%2520benchmark%2520for%250Aevent-based%2520depth%2520completion%2520comprising%2520one%2520real-world%2520and%2520two%2520synthetic%250Adatasets%2520to%2520facilitate%2520future%2520research.%2520Extensive%2520experiments%2520on%2520this%2520benchmark%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520EventDC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-Driven%20Dynamic%20Scene%20Depth%20Completion&entry.906535625=Zhiqiang%20Yan%20and%20Jianhao%20Jiao%20and%20Zhengxue%20Wang%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20Depth%20completion%20in%20dynamic%20scenes%20poses%20significant%20challenges%20due%20to%20rapid%0Aego-motion%20and%20object%20motion%2C%20which%20can%20severely%20degrade%20the%20quality%20of%20input%0Amodalities%20such%20as%20RGB%20images%20and%20LiDAR%20measurements.%20Conventional%20RGB-D%0Asensors%20often%20struggle%20to%20align%20precisely%20and%20capture%20reliable%20depth%20under%20such%0Aconditions.%20In%20contrast%2C%20event%20cameras%20with%20their%20high%20temporal%20resolution%20and%0Asensitivity%20to%20motion%20at%20the%20pixel%20level%20provide%20complementary%20cues%20that%20are%0A%25particularly%20beneficial%20in%20dynamic%20environments.To%20this%20end%2C%20we%20propose%0AEventDC%2C%20the%20first%20event-driven%20depth%20completion%20framework.%20It%20consists%20of%20two%0Akey%20components%3A%20Event-Modulated%20Alignment%20%28EMA%29%20and%20Local%20Depth%20Filtering%0A%28LDF%29.%20Both%20modules%20adaptively%20learn%20the%20two%20fundamental%20components%20of%0Aconvolution%20operations%3A%20offsets%20and%20weights%20conditioned%20on%20motion-sensitive%0Aevent%20streams.%20In%20the%20encoder%2C%20EMA%20leverages%20events%20to%20modulate%20the%20sampling%0Apositions%20of%20RGB-D%20features%20to%20achieve%20pixel%20redistribution%20for%20improved%0Aalignment%20and%20fusion.%20In%20the%20decoder%2C%20LDF%20refines%20depth%20estimations%20around%0Amoving%20objects%20by%20learning%20motion-aware%20masks%20from%20events.%20Additionally%2C%0AEventDC%20incorporates%20two%20loss%20terms%20to%20further%20benefit%20global%20alignment%20and%0Aenhance%20local%20depth%20recovery.%20Moreover%2C%20we%20establish%20the%20first%20benchmark%20for%0Aevent-based%20depth%20completion%20comprising%20one%20real-world%20and%20two%20synthetic%0Adatasets%20to%20facilitate%20future%20research.%20Extensive%20experiments%20on%20this%20benchmark%0Ademonstrate%20the%20superiority%20of%20our%20EventDC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13279v1&entry.124074799=Read"},
{"title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and\n  Vision-Language Models", "author": "Matteo Merler and Nicola Dainese and Minttu Alakuijala and Giovanni Bonetta and Pietro Ferrazzi and Yu Tian and Bernardo Magnini and Pekka Marttinen", "abstract": "  Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning.\n", "link": "http://arxiv.org/abs/2505.13180v1", "date": "2025-05-19", "relevancy": 2.3674, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.606}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViPlan%3A%20A%20Benchmark%20for%20Visual%20Planning%20with%20Symbolic%20Predicates%20and%0A%20%20Vision-Language%20Models&body=Title%3A%20ViPlan%3A%20A%20Benchmark%20for%20Visual%20Planning%20with%20Symbolic%20Predicates%20and%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Matteo%20Merler%20and%20Nicola%20Dainese%20and%20Minttu%20Alakuijala%20and%20Giovanni%20Bonetta%20and%20Pietro%20Ferrazzi%20and%20Yu%20Tian%20and%20Bernardo%20Magnini%20and%20Pekka%20Marttinen%0AAbstract%3A%20%20%20Integrating%20Large%20Language%20Models%20with%20symbolic%20planners%20is%20a%20promising%0Adirection%20for%20obtaining%20verifiable%20and%20grounded%20plans%20compared%20to%20planning%20in%0Anatural%20language%2C%20with%20recent%20works%20extending%20this%20idea%20to%20visual%20domains%20using%0AVision-Language%20Models%20%28VLMs%29.%20However%2C%20rigorous%20comparison%20between%0AVLM-grounded%20symbolic%20approaches%20and%20methods%20that%20plan%20directly%20with%20a%20VLM%20has%0Abeen%20hindered%20by%20a%20lack%20of%20common%20environments%2C%20evaluation%20protocols%20and%20model%0Acoverage.%20We%20introduce%20ViPlan%2C%20the%20first%20open-source%20benchmark%20for%20Visual%0APlanning%20with%20symbolic%20predicates%20and%20VLMs.%20ViPlan%20features%20a%20series%20of%0Aincreasingly%20challenging%20tasks%20in%20two%20domains%3A%20a%20visual%20variant%20of%20the%20classic%0ABlocksworld%20planning%20problem%20and%20a%20simulated%20household%20robotics%20environment.%20We%0Abenchmark%20nine%20open-source%20VLM%20families%20across%20multiple%20sizes%2C%20along%20with%0Aselected%20closed%20models%2C%20evaluating%20both%20VLM-grounded%20symbolic%20planning%20and%0Ausing%20the%20models%20directly%20to%20propose%20actions.%20We%20find%20symbolic%20planning%20to%0Aoutperform%20direct%20VLM%20planning%20in%20Blocksworld%2C%20where%20accurate%20image%20grounding%0Ais%20crucial%2C%20whereas%20the%20opposite%20is%20true%20in%20the%20household%20robotics%20tasks%2C%20where%0Acommonsense%20knowledge%20and%20the%20ability%20to%20recover%20from%20errors%20are%20beneficial.%0AFinally%2C%20we%20show%20that%20across%20most%20models%20and%20methods%2C%20there%20is%20no%20significant%0Abenefit%20to%20using%20Chain-of-Thought%20prompting%2C%20suggesting%20that%20current%20VLMs%20still%0Astruggle%20with%20visual%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViPlan%253A%2520A%2520Benchmark%2520for%2520Visual%2520Planning%2520with%2520Symbolic%2520Predicates%2520and%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DMatteo%2520Merler%2520and%2520Nicola%2520Dainese%2520and%2520Minttu%2520Alakuijala%2520and%2520Giovanni%2520Bonetta%2520and%2520Pietro%2520Ferrazzi%2520and%2520Yu%2520Tian%2520and%2520Bernardo%2520Magnini%2520and%2520Pekka%2520Marttinen%26entry.1292438233%3D%2520%2520Integrating%2520Large%2520Language%2520Models%2520with%2520symbolic%2520planners%2520is%2520a%2520promising%250Adirection%2520for%2520obtaining%2520verifiable%2520and%2520grounded%2520plans%2520compared%2520to%2520planning%2520in%250Anatural%2520language%252C%2520with%2520recent%2520works%2520extending%2520this%2520idea%2520to%2520visual%2520domains%2520using%250AVision-Language%2520Models%2520%2528VLMs%2529.%2520However%252C%2520rigorous%2520comparison%2520between%250AVLM-grounded%2520symbolic%2520approaches%2520and%2520methods%2520that%2520plan%2520directly%2520with%2520a%2520VLM%2520has%250Abeen%2520hindered%2520by%2520a%2520lack%2520of%2520common%2520environments%252C%2520evaluation%2520protocols%2520and%2520model%250Acoverage.%2520We%2520introduce%2520ViPlan%252C%2520the%2520first%2520open-source%2520benchmark%2520for%2520Visual%250APlanning%2520with%2520symbolic%2520predicates%2520and%2520VLMs.%2520ViPlan%2520features%2520a%2520series%2520of%250Aincreasingly%2520challenging%2520tasks%2520in%2520two%2520domains%253A%2520a%2520visual%2520variant%2520of%2520the%2520classic%250ABlocksworld%2520planning%2520problem%2520and%2520a%2520simulated%2520household%2520robotics%2520environment.%2520We%250Abenchmark%2520nine%2520open-source%2520VLM%2520families%2520across%2520multiple%2520sizes%252C%2520along%2520with%250Aselected%2520closed%2520models%252C%2520evaluating%2520both%2520VLM-grounded%2520symbolic%2520planning%2520and%250Ausing%2520the%2520models%2520directly%2520to%2520propose%2520actions.%2520We%2520find%2520symbolic%2520planning%2520to%250Aoutperform%2520direct%2520VLM%2520planning%2520in%2520Blocksworld%252C%2520where%2520accurate%2520image%2520grounding%250Ais%2520crucial%252C%2520whereas%2520the%2520opposite%2520is%2520true%2520in%2520the%2520household%2520robotics%2520tasks%252C%2520where%250Acommonsense%2520knowledge%2520and%2520the%2520ability%2520to%2520recover%2520from%2520errors%2520are%2520beneficial.%250AFinally%252C%2520we%2520show%2520that%2520across%2520most%2520models%2520and%2520methods%252C%2520there%2520is%2520no%2520significant%250Abenefit%2520to%2520using%2520Chain-of-Thought%2520prompting%252C%2520suggesting%2520that%2520current%2520VLMs%2520still%250Astruggle%2520with%2520visual%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViPlan%3A%20A%20Benchmark%20for%20Visual%20Planning%20with%20Symbolic%20Predicates%20and%0A%20%20Vision-Language%20Models&entry.906535625=Matteo%20Merler%20and%20Nicola%20Dainese%20and%20Minttu%20Alakuijala%20and%20Giovanni%20Bonetta%20and%20Pietro%20Ferrazzi%20and%20Yu%20Tian%20and%20Bernardo%20Magnini%20and%20Pekka%20Marttinen&entry.1292438233=%20%20Integrating%20Large%20Language%20Models%20with%20symbolic%20planners%20is%20a%20promising%0Adirection%20for%20obtaining%20verifiable%20and%20grounded%20plans%20compared%20to%20planning%20in%0Anatural%20language%2C%20with%20recent%20works%20extending%20this%20idea%20to%20visual%20domains%20using%0AVision-Language%20Models%20%28VLMs%29.%20However%2C%20rigorous%20comparison%20between%0AVLM-grounded%20symbolic%20approaches%20and%20methods%20that%20plan%20directly%20with%20a%20VLM%20has%0Abeen%20hindered%20by%20a%20lack%20of%20common%20environments%2C%20evaluation%20protocols%20and%20model%0Acoverage.%20We%20introduce%20ViPlan%2C%20the%20first%20open-source%20benchmark%20for%20Visual%0APlanning%20with%20symbolic%20predicates%20and%20VLMs.%20ViPlan%20features%20a%20series%20of%0Aincreasingly%20challenging%20tasks%20in%20two%20domains%3A%20a%20visual%20variant%20of%20the%20classic%0ABlocksworld%20planning%20problem%20and%20a%20simulated%20household%20robotics%20environment.%20We%0Abenchmark%20nine%20open-source%20VLM%20families%20across%20multiple%20sizes%2C%20along%20with%0Aselected%20closed%20models%2C%20evaluating%20both%20VLM-grounded%20symbolic%20planning%20and%0Ausing%20the%20models%20directly%20to%20propose%20actions.%20We%20find%20symbolic%20planning%20to%0Aoutperform%20direct%20VLM%20planning%20in%20Blocksworld%2C%20where%20accurate%20image%20grounding%0Ais%20crucial%2C%20whereas%20the%20opposite%20is%20true%20in%20the%20household%20robotics%20tasks%2C%20where%0Acommonsense%20knowledge%20and%20the%20ability%20to%20recover%20from%20errors%20are%20beneficial.%0AFinally%2C%20we%20show%20that%20across%20most%20models%20and%20methods%2C%20there%20is%20no%20significant%0Abenefit%20to%20using%20Chain-of-Thought%20prompting%2C%20suggesting%20that%20current%20VLMs%20still%0Astruggle%20with%20visual%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13180v1&entry.124074799=Read"},
{"title": "Simplicity is Key: An Unsupervised Pretraining Approach for Sparse Radio\n  Channels", "author": "Jonathan Ott and Maximilian Stahlke and Tobias Feigl and Bjoern M. Eskofier and Christopher Mutschler", "abstract": "  We introduce the Sparse pretrained Radio Transformer (SpaRTran), an\nunsupervised representation learning approach based on the concept of\ncompressed sensing for radio channels. Our approach learns embeddings that\nfocus on the physical properties of radio propagation, to create the optimal\nbasis for fine-tuning on radio-based downstream tasks. SpaRTran uses a sparse\ngated autoencoder that induces a simplicity bias to the learned\nrepresentations, resembling the sparse nature of radio propagation. For signal\nreconstruction, it learns a dictionary that holds atomic features, which\nincreases flexibility across signal waveforms and spatiotemporal signal\npatterns. Our experiments show that SpaRTran reduces errors by up to 85 %\ncompared to state-of-the-art methods when fine-tuned on radio fingerprinting, a\nchallenging downstream task. In addition, our method requires less pretraining\neffort and offers greater flexibility, as we train it solely on individual\nradio signals. SpaRTran serves as an excellent base model that can be\nfine-tuned for various radio-based downstream tasks, effectively reducing the\ncost for labeling. In addition, it is significantly more versatile than\nexisting methods and demonstrates superior generalization.\n", "link": "http://arxiv.org/abs/2505.13055v1", "date": "2025-05-19", "relevancy": 2.3671, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4889}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4708}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplicity%20is%20Key%3A%20An%20Unsupervised%20Pretraining%20Approach%20for%20Sparse%20Radio%0A%20%20Channels&body=Title%3A%20Simplicity%20is%20Key%3A%20An%20Unsupervised%20Pretraining%20Approach%20for%20Sparse%20Radio%0A%20%20Channels%0AAuthor%3A%20Jonathan%20Ott%20and%20Maximilian%20Stahlke%20and%20Tobias%20Feigl%20and%20Bjoern%20M.%20Eskofier%20and%20Christopher%20Mutschler%0AAbstract%3A%20%20%20We%20introduce%20the%20Sparse%20pretrained%20Radio%20Transformer%20%28SpaRTran%29%2C%20an%0Aunsupervised%20representation%20learning%20approach%20based%20on%20the%20concept%20of%0Acompressed%20sensing%20for%20radio%20channels.%20Our%20approach%20learns%20embeddings%20that%0Afocus%20on%20the%20physical%20properties%20of%20radio%20propagation%2C%20to%20create%20the%20optimal%0Abasis%20for%20fine-tuning%20on%20radio-based%20downstream%20tasks.%20SpaRTran%20uses%20a%20sparse%0Agated%20autoencoder%20that%20induces%20a%20simplicity%20bias%20to%20the%20learned%0Arepresentations%2C%20resembling%20the%20sparse%20nature%20of%20radio%20propagation.%20For%20signal%0Areconstruction%2C%20it%20learns%20a%20dictionary%20that%20holds%20atomic%20features%2C%20which%0Aincreases%20flexibility%20across%20signal%20waveforms%20and%20spatiotemporal%20signal%0Apatterns.%20Our%20experiments%20show%20that%20SpaRTran%20reduces%20errors%20by%20up%20to%2085%20%25%0Acompared%20to%20state-of-the-art%20methods%20when%20fine-tuned%20on%20radio%20fingerprinting%2C%20a%0Achallenging%20downstream%20task.%20In%20addition%2C%20our%20method%20requires%20less%20pretraining%0Aeffort%20and%20offers%20greater%20flexibility%2C%20as%20we%20train%20it%20solely%20on%20individual%0Aradio%20signals.%20SpaRTran%20serves%20as%20an%20excellent%20base%20model%20that%20can%20be%0Afine-tuned%20for%20various%20radio-based%20downstream%20tasks%2C%20effectively%20reducing%20the%0Acost%20for%20labeling.%20In%20addition%2C%20it%20is%20significantly%20more%20versatile%20than%0Aexisting%20methods%20and%20demonstrates%20superior%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplicity%2520is%2520Key%253A%2520An%2520Unsupervised%2520Pretraining%2520Approach%2520for%2520Sparse%2520Radio%250A%2520%2520Channels%26entry.906535625%3DJonathan%2520Ott%2520and%2520Maximilian%2520Stahlke%2520and%2520Tobias%2520Feigl%2520and%2520Bjoern%2520M.%2520Eskofier%2520and%2520Christopher%2520Mutschler%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Sparse%2520pretrained%2520Radio%2520Transformer%2520%2528SpaRTran%2529%252C%2520an%250Aunsupervised%2520representation%2520learning%2520approach%2520based%2520on%2520the%2520concept%2520of%250Acompressed%2520sensing%2520for%2520radio%2520channels.%2520Our%2520approach%2520learns%2520embeddings%2520that%250Afocus%2520on%2520the%2520physical%2520properties%2520of%2520radio%2520propagation%252C%2520to%2520create%2520the%2520optimal%250Abasis%2520for%2520fine-tuning%2520on%2520radio-based%2520downstream%2520tasks.%2520SpaRTran%2520uses%2520a%2520sparse%250Agated%2520autoencoder%2520that%2520induces%2520a%2520simplicity%2520bias%2520to%2520the%2520learned%250Arepresentations%252C%2520resembling%2520the%2520sparse%2520nature%2520of%2520radio%2520propagation.%2520For%2520signal%250Areconstruction%252C%2520it%2520learns%2520a%2520dictionary%2520that%2520holds%2520atomic%2520features%252C%2520which%250Aincreases%2520flexibility%2520across%2520signal%2520waveforms%2520and%2520spatiotemporal%2520signal%250Apatterns.%2520Our%2520experiments%2520show%2520that%2520SpaRTran%2520reduces%2520errors%2520by%2520up%2520to%252085%2520%2525%250Acompared%2520to%2520state-of-the-art%2520methods%2520when%2520fine-tuned%2520on%2520radio%2520fingerprinting%252C%2520a%250Achallenging%2520downstream%2520task.%2520In%2520addition%252C%2520our%2520method%2520requires%2520less%2520pretraining%250Aeffort%2520and%2520offers%2520greater%2520flexibility%252C%2520as%2520we%2520train%2520it%2520solely%2520on%2520individual%250Aradio%2520signals.%2520SpaRTran%2520serves%2520as%2520an%2520excellent%2520base%2520model%2520that%2520can%2520be%250Afine-tuned%2520for%2520various%2520radio-based%2520downstream%2520tasks%252C%2520effectively%2520reducing%2520the%250Acost%2520for%2520labeling.%2520In%2520addition%252C%2520it%2520is%2520significantly%2520more%2520versatile%2520than%250Aexisting%2520methods%2520and%2520demonstrates%2520superior%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplicity%20is%20Key%3A%20An%20Unsupervised%20Pretraining%20Approach%20for%20Sparse%20Radio%0A%20%20Channels&entry.906535625=Jonathan%20Ott%20and%20Maximilian%20Stahlke%20and%20Tobias%20Feigl%20and%20Bjoern%20M.%20Eskofier%20and%20Christopher%20Mutschler&entry.1292438233=%20%20We%20introduce%20the%20Sparse%20pretrained%20Radio%20Transformer%20%28SpaRTran%29%2C%20an%0Aunsupervised%20representation%20learning%20approach%20based%20on%20the%20concept%20of%0Acompressed%20sensing%20for%20radio%20channels.%20Our%20approach%20learns%20embeddings%20that%0Afocus%20on%20the%20physical%20properties%20of%20radio%20propagation%2C%20to%20create%20the%20optimal%0Abasis%20for%20fine-tuning%20on%20radio-based%20downstream%20tasks.%20SpaRTran%20uses%20a%20sparse%0Agated%20autoencoder%20that%20induces%20a%20simplicity%20bias%20to%20the%20learned%0Arepresentations%2C%20resembling%20the%20sparse%20nature%20of%20radio%20propagation.%20For%20signal%0Areconstruction%2C%20it%20learns%20a%20dictionary%20that%20holds%20atomic%20features%2C%20which%0Aincreases%20flexibility%20across%20signal%20waveforms%20and%20spatiotemporal%20signal%0Apatterns.%20Our%20experiments%20show%20that%20SpaRTran%20reduces%20errors%20by%20up%20to%2085%20%25%0Acompared%20to%20state-of-the-art%20methods%20when%20fine-tuned%20on%20radio%20fingerprinting%2C%20a%0Achallenging%20downstream%20task.%20In%20addition%2C%20our%20method%20requires%20less%20pretraining%0Aeffort%20and%20offers%20greater%20flexibility%2C%20as%20we%20train%20it%20solely%20on%20individual%0Aradio%20signals.%20SpaRTran%20serves%20as%20an%20excellent%20base%20model%20that%20can%20be%0Afine-tuned%20for%20various%20radio-based%20downstream%20tasks%2C%20effectively%20reducing%20the%0Acost%20for%20labeling.%20In%20addition%2C%20it%20is%20significantly%20more%20versatile%20than%0Aexisting%20methods%20and%20demonstrates%20superior%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13055v1&entry.124074799=Read"},
{"title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text", "author": "Michael Tschannen and Andr\u00e9 Susano Pinto and Alexander Kolesnikov", "abstract": "  Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.\n", "link": "http://arxiv.org/abs/2411.19722v2", "date": "2025-05-19", "relevancy": 2.3596, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6799}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5816}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JetFormer%3A%20An%20Autoregressive%20Generative%20Model%20of%20Raw%20Images%20and%20Text&body=Title%3A%20JetFormer%3A%20An%20Autoregressive%20Generative%20Model%20of%20Raw%20Images%20and%20Text%0AAuthor%3A%20Michael%20Tschannen%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov%0AAbstract%3A%20%20%20Removing%20modeling%20constraints%20and%20unifying%20architectures%20across%20domains%20has%0Abeen%20a%20key%20driver%20of%20the%20recent%20progress%20in%20training%20large%20multimodal%20models.%0AHowever%2C%20most%20of%20these%20models%20still%20rely%20on%20many%20separately%20trained%20components%0Asuch%20as%20modality-specific%20encoders%20and%20decoders.%20In%20this%20work%2C%20we%20further%0Astreamline%20joint%20generative%20modeling%20of%20images%20and%20text.%20We%20propose%20an%0Aautoregressive%20decoder-only%20transformer%20-%20JetFormer%20-%20which%20is%20trained%20to%0Adirectly%20maximize%20the%20likelihood%20of%20raw%20data%2C%20without%20relying%20on%20any%20separately%0Apretrained%20components%2C%20and%20can%20understand%20and%20generate%20both%20text%20and%20images.%0ASpecifically%2C%20we%20leverage%20a%20normalizing%20flow%20model%20to%20obtain%20a%20soft-token%20image%0Arepresentation%20that%20is%20jointly%20trained%20with%20an%20autoregressive%20multimodal%0Atransformer.%20The%20normalizing%20flow%20model%20serves%20as%20both%20an%20image%20encoder%20for%0Aperception%20tasks%20and%20an%20image%20decoder%20for%20image%20generation%20tasks%20during%0Ainference.%20JetFormer%20achieves%20text-to-image%20generation%20quality%20competitive%20with%0Arecent%20VQ-VAE-%20and%20VAE-based%20baselines.%20These%20baselines%20rely%20on%20pretrained%0Aimage%20autoencoders%2C%20which%20are%20trained%20with%20a%20complex%20mixture%20of%20losses%2C%0Aincluding%20perceptual%20ones.%20At%20the%20same%20time%2C%20JetFormer%20demonstrates%20robust%0Aimage%20understanding%20capabilities.%20To%20the%20best%20of%20our%20knowledge%2C%20JetFormer%20is%0Athe%20first%20model%20that%20is%20capable%20of%20generating%20high-fidelity%20images%20and%0Aproducing%20strong%20log-likelihood%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJetFormer%253A%2520An%2520Autoregressive%2520Generative%2520Model%2520of%2520Raw%2520Images%2520and%2520Text%26entry.906535625%3DMichael%2520Tschannen%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Alexander%2520Kolesnikov%26entry.1292438233%3D%2520%2520Removing%2520modeling%2520constraints%2520and%2520unifying%2520architectures%2520across%2520domains%2520has%250Abeen%2520a%2520key%2520driver%2520of%2520the%2520recent%2520progress%2520in%2520training%2520large%2520multimodal%2520models.%250AHowever%252C%2520most%2520of%2520these%2520models%2520still%2520rely%2520on%2520many%2520separately%2520trained%2520components%250Asuch%2520as%2520modality-specific%2520encoders%2520and%2520decoders.%2520In%2520this%2520work%252C%2520we%2520further%250Astreamline%2520joint%2520generative%2520modeling%2520of%2520images%2520and%2520text.%2520We%2520propose%2520an%250Aautoregressive%2520decoder-only%2520transformer%2520-%2520JetFormer%2520-%2520which%2520is%2520trained%2520to%250Adirectly%2520maximize%2520the%2520likelihood%2520of%2520raw%2520data%252C%2520without%2520relying%2520on%2520any%2520separately%250Apretrained%2520components%252C%2520and%2520can%2520understand%2520and%2520generate%2520both%2520text%2520and%2520images.%250ASpecifically%252C%2520we%2520leverage%2520a%2520normalizing%2520flow%2520model%2520to%2520obtain%2520a%2520soft-token%2520image%250Arepresentation%2520that%2520is%2520jointly%2520trained%2520with%2520an%2520autoregressive%2520multimodal%250Atransformer.%2520The%2520normalizing%2520flow%2520model%2520serves%2520as%2520both%2520an%2520image%2520encoder%2520for%250Aperception%2520tasks%2520and%2520an%2520image%2520decoder%2520for%2520image%2520generation%2520tasks%2520during%250Ainference.%2520JetFormer%2520achieves%2520text-to-image%2520generation%2520quality%2520competitive%2520with%250Arecent%2520VQ-VAE-%2520and%2520VAE-based%2520baselines.%2520These%2520baselines%2520rely%2520on%2520pretrained%250Aimage%2520autoencoders%252C%2520which%2520are%2520trained%2520with%2520a%2520complex%2520mixture%2520of%2520losses%252C%250Aincluding%2520perceptual%2520ones.%2520At%2520the%2520same%2520time%252C%2520JetFormer%2520demonstrates%2520robust%250Aimage%2520understanding%2520capabilities.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520JetFormer%2520is%250Athe%2520first%2520model%2520that%2520is%2520capable%2520of%2520generating%2520high-fidelity%2520images%2520and%250Aproducing%2520strong%2520log-likelihood%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JetFormer%3A%20An%20Autoregressive%20Generative%20Model%20of%20Raw%20Images%20and%20Text&entry.906535625=Michael%20Tschannen%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Alexander%20Kolesnikov&entry.1292438233=%20%20Removing%20modeling%20constraints%20and%20unifying%20architectures%20across%20domains%20has%0Abeen%20a%20key%20driver%20of%20the%20recent%20progress%20in%20training%20large%20multimodal%20models.%0AHowever%2C%20most%20of%20these%20models%20still%20rely%20on%20many%20separately%20trained%20components%0Asuch%20as%20modality-specific%20encoders%20and%20decoders.%20In%20this%20work%2C%20we%20further%0Astreamline%20joint%20generative%20modeling%20of%20images%20and%20text.%20We%20propose%20an%0Aautoregressive%20decoder-only%20transformer%20-%20JetFormer%20-%20which%20is%20trained%20to%0Adirectly%20maximize%20the%20likelihood%20of%20raw%20data%2C%20without%20relying%20on%20any%20separately%0Apretrained%20components%2C%20and%20can%20understand%20and%20generate%20both%20text%20and%20images.%0ASpecifically%2C%20we%20leverage%20a%20normalizing%20flow%20model%20to%20obtain%20a%20soft-token%20image%0Arepresentation%20that%20is%20jointly%20trained%20with%20an%20autoregressive%20multimodal%0Atransformer.%20The%20normalizing%20flow%20model%20serves%20as%20both%20an%20image%20encoder%20for%0Aperception%20tasks%20and%20an%20image%20decoder%20for%20image%20generation%20tasks%20during%0Ainference.%20JetFormer%20achieves%20text-to-image%20generation%20quality%20competitive%20with%0Arecent%20VQ-VAE-%20and%20VAE-based%20baselines.%20These%20baselines%20rely%20on%20pretrained%0Aimage%20autoencoders%2C%20which%20are%20trained%20with%20a%20complex%20mixture%20of%20losses%2C%0Aincluding%20perceptual%20ones.%20At%20the%20same%20time%2C%20JetFormer%20demonstrates%20robust%0Aimage%20understanding%20capabilities.%20To%20the%20best%20of%20our%20knowledge%2C%20JetFormer%20is%0Athe%20first%20model%20that%20is%20capable%20of%20generating%20high-fidelity%20images%20and%0Aproducing%20strong%20log-likelihood%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19722v2&entry.124074799=Read"},
{"title": "Grokking at the Edge of Numerical Stability", "author": "Lucas Prieto and Melih Barsbey and Pedro A. M. Mediano and Tolga Birdal", "abstract": "  Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.\n", "link": "http://arxiv.org/abs/2501.04697v2", "date": "2025-05-19", "relevancy": 2.3512, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4932}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4615}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grokking%20at%20the%20Edge%20of%20Numerical%20Stability&body=Title%3A%20Grokking%20at%20the%20Edge%20of%20Numerical%20Stability%0AAuthor%3A%20Lucas%20Prieto%20and%20Melih%20Barsbey%20and%20Pedro%20A.%20M.%20Mediano%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Grokking%2C%20the%20sudden%20generalization%20that%20occurs%20after%20prolonged%20overfitting%2C%0Ais%20a%20surprising%20phenomenon%20challenging%20our%20understanding%20of%20deep%20learning.%0AAlthough%20significant%20progress%20has%20been%20made%20in%20understanding%20grokking%2C%20the%0Areasons%20behind%20the%20delayed%20generalization%20and%20its%20dependence%20on%20regularization%0Aremain%20unclear.%20In%20this%20work%2C%20we%20argue%20that%20without%20regularization%2C%20grokking%0Atasks%20push%20models%20to%20the%20edge%20of%20numerical%20stability%2C%20introducing%20floating%0Apoint%20errors%20in%20the%20Softmax%20function%2C%20which%20we%20refer%20to%20as%20Softmax%20Collapse%0A%28SC%29.%20We%20demonstrate%20that%20SC%20prevents%20grokking%20and%20that%20mitigating%20SC%20enables%0Agrokking%20without%20regularization.%20Investigating%20the%20root%20cause%20of%20SC%2C%20we%20find%0Athat%20beyond%20the%20point%20of%20overfitting%2C%20the%20gradients%20strongly%20align%20with%20what%20we%0Acall%20the%20na%5C%22ive%20loss%20minimization%20%28NLM%29%20direction.%20This%20component%20of%20the%0Agradient%20does%20not%20alter%20the%20model%27s%20predictions%20but%20decreases%20the%20loss%20by%0Ascaling%20the%20logits%2C%20typically%20by%20scaling%20the%20weights%20along%20their%20current%0Adirection.%20We%20show%20that%20this%20scaling%20of%20the%20logits%20explains%20the%20delay%20in%0Ageneralization%20characteristic%20of%20grokking%20and%20eventually%20leads%20to%20SC%2C%20halting%0Afurther%20learning.%20To%20validate%20our%20hypotheses%2C%20we%20introduce%20two%20key%0Acontributions%20that%20address%20the%20challenges%20in%20grokking%20tasks%3A%20StableMax%2C%20a%20new%0Aactivation%20function%20that%20prevents%20SC%20and%20enables%20grokking%20without%0Aregularization%2C%20and%20%24%5Cperp%24Grad%2C%20a%20training%20algorithm%20that%20promotes%20quick%0Ageneralization%20in%20grokking%20tasks%20by%20preventing%20NLM%20altogether.%20These%0Acontributions%20provide%20new%20insights%20into%20grokking%2C%20elucidating%20its%20delayed%0Ageneralization%2C%20reliance%20on%20regularization%2C%20and%20the%20effectiveness%20of%20existing%0Agrokking-inducing%20methods.%20Code%20for%20this%20paper%20is%20available%20at%0Ahttps%3A//github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrokking%2520at%2520the%2520Edge%2520of%2520Numerical%2520Stability%26entry.906535625%3DLucas%2520Prieto%2520and%2520Melih%2520Barsbey%2520and%2520Pedro%2520A.%2520M.%2520Mediano%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Grokking%252C%2520the%2520sudden%2520generalization%2520that%2520occurs%2520after%2520prolonged%2520overfitting%252C%250Ais%2520a%2520surprising%2520phenomenon%2520challenging%2520our%2520understanding%2520of%2520deep%2520learning.%250AAlthough%2520significant%2520progress%2520has%2520been%2520made%2520in%2520understanding%2520grokking%252C%2520the%250Areasons%2520behind%2520the%2520delayed%2520generalization%2520and%2520its%2520dependence%2520on%2520regularization%250Aremain%2520unclear.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520without%2520regularization%252C%2520grokking%250Atasks%2520push%2520models%2520to%2520the%2520edge%2520of%2520numerical%2520stability%252C%2520introducing%2520floating%250Apoint%2520errors%2520in%2520the%2520Softmax%2520function%252C%2520which%2520we%2520refer%2520to%2520as%2520Softmax%2520Collapse%250A%2528SC%2529.%2520We%2520demonstrate%2520that%2520SC%2520prevents%2520grokking%2520and%2520that%2520mitigating%2520SC%2520enables%250Agrokking%2520without%2520regularization.%2520Investigating%2520the%2520root%2520cause%2520of%2520SC%252C%2520we%2520find%250Athat%2520beyond%2520the%2520point%2520of%2520overfitting%252C%2520the%2520gradients%2520strongly%2520align%2520with%2520what%2520we%250Acall%2520the%2520na%255C%2522ive%2520loss%2520minimization%2520%2528NLM%2529%2520direction.%2520This%2520component%2520of%2520the%250Agradient%2520does%2520not%2520alter%2520the%2520model%2527s%2520predictions%2520but%2520decreases%2520the%2520loss%2520by%250Ascaling%2520the%2520logits%252C%2520typically%2520by%2520scaling%2520the%2520weights%2520along%2520their%2520current%250Adirection.%2520We%2520show%2520that%2520this%2520scaling%2520of%2520the%2520logits%2520explains%2520the%2520delay%2520in%250Ageneralization%2520characteristic%2520of%2520grokking%2520and%2520eventually%2520leads%2520to%2520SC%252C%2520halting%250Afurther%2520learning.%2520To%2520validate%2520our%2520hypotheses%252C%2520we%2520introduce%2520two%2520key%250Acontributions%2520that%2520address%2520the%2520challenges%2520in%2520grokking%2520tasks%253A%2520StableMax%252C%2520a%2520new%250Aactivation%2520function%2520that%2520prevents%2520SC%2520and%2520enables%2520grokking%2520without%250Aregularization%252C%2520and%2520%2524%255Cperp%2524Grad%252C%2520a%2520training%2520algorithm%2520that%2520promotes%2520quick%250Ageneralization%2520in%2520grokking%2520tasks%2520by%2520preventing%2520NLM%2520altogether.%2520These%250Acontributions%2520provide%2520new%2520insights%2520into%2520grokking%252C%2520elucidating%2520its%2520delayed%250Ageneralization%252C%2520reliance%2520on%2520regularization%252C%2520and%2520the%2520effectiveness%2520of%2520existing%250Agrokking-inducing%2520methods.%2520Code%2520for%2520this%2520paper%2520is%2520available%2520at%250Ahttps%253A//github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grokking%20at%20the%20Edge%20of%20Numerical%20Stability&entry.906535625=Lucas%20Prieto%20and%20Melih%20Barsbey%20and%20Pedro%20A.%20M.%20Mediano%20and%20Tolga%20Birdal&entry.1292438233=%20%20Grokking%2C%20the%20sudden%20generalization%20that%20occurs%20after%20prolonged%20overfitting%2C%0Ais%20a%20surprising%20phenomenon%20challenging%20our%20understanding%20of%20deep%20learning.%0AAlthough%20significant%20progress%20has%20been%20made%20in%20understanding%20grokking%2C%20the%0Areasons%20behind%20the%20delayed%20generalization%20and%20its%20dependence%20on%20regularization%0Aremain%20unclear.%20In%20this%20work%2C%20we%20argue%20that%20without%20regularization%2C%20grokking%0Atasks%20push%20models%20to%20the%20edge%20of%20numerical%20stability%2C%20introducing%20floating%0Apoint%20errors%20in%20the%20Softmax%20function%2C%20which%20we%20refer%20to%20as%20Softmax%20Collapse%0A%28SC%29.%20We%20demonstrate%20that%20SC%20prevents%20grokking%20and%20that%20mitigating%20SC%20enables%0Agrokking%20without%20regularization.%20Investigating%20the%20root%20cause%20of%20SC%2C%20we%20find%0Athat%20beyond%20the%20point%20of%20overfitting%2C%20the%20gradients%20strongly%20align%20with%20what%20we%0Acall%20the%20na%5C%22ive%20loss%20minimization%20%28NLM%29%20direction.%20This%20component%20of%20the%0Agradient%20does%20not%20alter%20the%20model%27s%20predictions%20but%20decreases%20the%20loss%20by%0Ascaling%20the%20logits%2C%20typically%20by%20scaling%20the%20weights%20along%20their%20current%0Adirection.%20We%20show%20that%20this%20scaling%20of%20the%20logits%20explains%20the%20delay%20in%0Ageneralization%20characteristic%20of%20grokking%20and%20eventually%20leads%20to%20SC%2C%20halting%0Afurther%20learning.%20To%20validate%20our%20hypotheses%2C%20we%20introduce%20two%20key%0Acontributions%20that%20address%20the%20challenges%20in%20grokking%20tasks%3A%20StableMax%2C%20a%20new%0Aactivation%20function%20that%20prevents%20SC%20and%20enables%20grokking%20without%0Aregularization%2C%20and%20%24%5Cperp%24Grad%2C%20a%20training%20algorithm%20that%20promotes%20quick%0Ageneralization%20in%20grokking%20tasks%20by%20preventing%20NLM%20altogether.%20These%0Acontributions%20provide%20new%20insights%20into%20grokking%2C%20elucidating%20its%20delayed%0Ageneralization%2C%20reliance%20on%20regularization%2C%20and%20the%20effectiveness%20of%20existing%0Agrokking-inducing%20methods.%20Code%20for%20this%20paper%20is%20available%20at%0Ahttps%3A//github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04697v2&entry.124074799=Read"},
{"title": "Thinkless: LLM Learns When to Think", "author": "Gongfan Fang and Xinyin Ma and Xinchao Wang", "abstract": "  Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless\n", "link": "http://arxiv.org/abs/2505.13379v1", "date": "2025-05-19", "relevancy": 2.3321, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.468}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinkless%3A%20LLM%20Learns%20When%20to%20Think&body=Title%3A%20Thinkless%3A%20LLM%20Learns%20When%20to%20Think%0AAuthor%3A%20Gongfan%20Fang%20and%20Xinyin%20Ma%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Reasoning%20Language%20Models%2C%20capable%20of%20extended%20chain-of-thought%20reasoning%2C%0Ahave%20demonstrated%20remarkable%20performance%20on%20tasks%20requiring%20complex%20logical%0Ainference.%20However%2C%20applying%20elaborate%20reasoning%20for%20all%20queries%20often%20results%0Ain%20substantial%20computational%20inefficiencies%2C%20particularly%20when%20many%20problems%0Aadmit%20straightforward%20solutions.%20This%20motivates%20an%20open%20question%3A%20Can%20LLMs%0Alearn%20when%20to%20think%3F%20To%20answer%20this%2C%20we%20propose%20Thinkless%2C%20a%20learnable%0Aframework%20that%20empowers%20an%20LLM%20to%20adaptively%20select%20between%20short-form%20and%0Along-form%20reasoning%2C%20based%20on%20both%20task%20complexity%20and%20the%20model%27s%20ability.%0AThinkless%20is%20trained%20under%20a%20reinforcement%20learning%20paradigm%20and%20employs%20two%0Acontrol%20tokens%2C%20%3Cshort%3E%20for%20concise%20responses%20and%20%3Cthink%3E%20for%20detailed%0Areasoning.%20At%20the%20core%20of%20our%20method%20is%20a%20Decoupled%20Group%20Relative%20Policy%0AOptimization%20%28DeGRPO%29%20algorithm%2C%20which%20decomposes%20the%20learning%20objective%20of%0Ahybrid%20reasoning%20into%20two%20components%3A%20%281%29%20a%20control%20token%20loss%20that%20governs%20the%0Aselection%20of%20the%20reasoning%20mode%2C%20and%20%282%29%20a%20response%20loss%20that%20improves%20the%0Aaccuracy%20of%20the%20generated%20answers.%20This%20decoupled%20formulation%20enables%0Afine-grained%20control%20over%20the%20contributions%20of%20each%20objective%2C%20stabilizing%0Atraining%20and%20effectively%20preventing%20collapse%20observed%20in%20vanilla%20GRPO.%0AEmpirically%2C%20on%20several%20benchmarks%20such%20as%20Minerva%20Algebra%2C%20MATH-500%2C%20and%0AGSM8K%2C%20Thinkless%20is%20able%20to%20reduce%20the%20usage%20of%20long-chain%20thinking%20by%2050%25%20-%0A90%25%2C%20significantly%20improving%20the%20efficiency%20of%20Reasoning%20Language%20Models.%20The%0Acode%20is%20available%20at%20https%3A//github.com/VainF/Thinkless%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinkless%253A%2520LLM%2520Learns%2520When%2520to%2520Think%26entry.906535625%3DGongfan%2520Fang%2520and%2520Xinyin%2520Ma%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Reasoning%2520Language%2520Models%252C%2520capable%2520of%2520extended%2520chain-of-thought%2520reasoning%252C%250Ahave%2520demonstrated%2520remarkable%2520performance%2520on%2520tasks%2520requiring%2520complex%2520logical%250Ainference.%2520However%252C%2520applying%2520elaborate%2520reasoning%2520for%2520all%2520queries%2520often%2520results%250Ain%2520substantial%2520computational%2520inefficiencies%252C%2520particularly%2520when%2520many%2520problems%250Aadmit%2520straightforward%2520solutions.%2520This%2520motivates%2520an%2520open%2520question%253A%2520Can%2520LLMs%250Alearn%2520when%2520to%2520think%253F%2520To%2520answer%2520this%252C%2520we%2520propose%2520Thinkless%252C%2520a%2520learnable%250Aframework%2520that%2520empowers%2520an%2520LLM%2520to%2520adaptively%2520select%2520between%2520short-form%2520and%250Along-form%2520reasoning%252C%2520based%2520on%2520both%2520task%2520complexity%2520and%2520the%2520model%2527s%2520ability.%250AThinkless%2520is%2520trained%2520under%2520a%2520reinforcement%2520learning%2520paradigm%2520and%2520employs%2520two%250Acontrol%2520tokens%252C%2520%253Cshort%253E%2520for%2520concise%2520responses%2520and%2520%253Cthink%253E%2520for%2520detailed%250Areasoning.%2520At%2520the%2520core%2520of%2520our%2520method%2520is%2520a%2520Decoupled%2520Group%2520Relative%2520Policy%250AOptimization%2520%2528DeGRPO%2529%2520algorithm%252C%2520which%2520decomposes%2520the%2520learning%2520objective%2520of%250Ahybrid%2520reasoning%2520into%2520two%2520components%253A%2520%25281%2529%2520a%2520control%2520token%2520loss%2520that%2520governs%2520the%250Aselection%2520of%2520the%2520reasoning%2520mode%252C%2520and%2520%25282%2529%2520a%2520response%2520loss%2520that%2520improves%2520the%250Aaccuracy%2520of%2520the%2520generated%2520answers.%2520This%2520decoupled%2520formulation%2520enables%250Afine-grained%2520control%2520over%2520the%2520contributions%2520of%2520each%2520objective%252C%2520stabilizing%250Atraining%2520and%2520effectively%2520preventing%2520collapse%2520observed%2520in%2520vanilla%2520GRPO.%250AEmpirically%252C%2520on%2520several%2520benchmarks%2520such%2520as%2520Minerva%2520Algebra%252C%2520MATH-500%252C%2520and%250AGSM8K%252C%2520Thinkless%2520is%2520able%2520to%2520reduce%2520the%2520usage%2520of%2520long-chain%2520thinking%2520by%252050%2525%2520-%250A90%2525%252C%2520significantly%2520improving%2520the%2520efficiency%2520of%2520Reasoning%2520Language%2520Models.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/VainF/Thinkless%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinkless%3A%20LLM%20Learns%20When%20to%20Think&entry.906535625=Gongfan%20Fang%20and%20Xinyin%20Ma%20and%20Xinchao%20Wang&entry.1292438233=%20%20Reasoning%20Language%20Models%2C%20capable%20of%20extended%20chain-of-thought%20reasoning%2C%0Ahave%20demonstrated%20remarkable%20performance%20on%20tasks%20requiring%20complex%20logical%0Ainference.%20However%2C%20applying%20elaborate%20reasoning%20for%20all%20queries%20often%20results%0Ain%20substantial%20computational%20inefficiencies%2C%20particularly%20when%20many%20problems%0Aadmit%20straightforward%20solutions.%20This%20motivates%20an%20open%20question%3A%20Can%20LLMs%0Alearn%20when%20to%20think%3F%20To%20answer%20this%2C%20we%20propose%20Thinkless%2C%20a%20learnable%0Aframework%20that%20empowers%20an%20LLM%20to%20adaptively%20select%20between%20short-form%20and%0Along-form%20reasoning%2C%20based%20on%20both%20task%20complexity%20and%20the%20model%27s%20ability.%0AThinkless%20is%20trained%20under%20a%20reinforcement%20learning%20paradigm%20and%20employs%20two%0Acontrol%20tokens%2C%20%3Cshort%3E%20for%20concise%20responses%20and%20%3Cthink%3E%20for%20detailed%0Areasoning.%20At%20the%20core%20of%20our%20method%20is%20a%20Decoupled%20Group%20Relative%20Policy%0AOptimization%20%28DeGRPO%29%20algorithm%2C%20which%20decomposes%20the%20learning%20objective%20of%0Ahybrid%20reasoning%20into%20two%20components%3A%20%281%29%20a%20control%20token%20loss%20that%20governs%20the%0Aselection%20of%20the%20reasoning%20mode%2C%20and%20%282%29%20a%20response%20loss%20that%20improves%20the%0Aaccuracy%20of%20the%20generated%20answers.%20This%20decoupled%20formulation%20enables%0Afine-grained%20control%20over%20the%20contributions%20of%20each%20objective%2C%20stabilizing%0Atraining%20and%20effectively%20preventing%20collapse%20observed%20in%20vanilla%20GRPO.%0AEmpirically%2C%20on%20several%20benchmarks%20such%20as%20Minerva%20Algebra%2C%20MATH-500%2C%20and%0AGSM8K%2C%20Thinkless%20is%20able%20to%20reduce%20the%20usage%20of%20long-chain%20thinking%20by%2050%25%20-%0A90%25%2C%20significantly%20improving%20the%20efficiency%20of%20Reasoning%20Language%20Models.%20The%0Acode%20is%20available%20at%20https%3A//github.com/VainF/Thinkless%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13379v1&entry.124074799=Read"},
{"title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "author": "Xiang Fei and Jinghui Lu and Qi Sun and Hao Feng and Yanjie Wang and Wei Shi and An-Lan Wang and Jingqun Tang and Can Huang", "abstract": "  Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.\n", "link": "http://arxiv.org/abs/2505.13077v1", "date": "2025-05-19", "relevancy": 2.3294, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4658}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Sequential%20Numerical%20Prediction%20in%20Autoregressive%20Models&body=Title%3A%20Advancing%20Sequential%20Numerical%20Prediction%20in%20Autoregressive%20Models%0AAuthor%3A%20Xiang%20Fei%20and%20Jinghui%20Lu%20and%20Qi%20Sun%20and%20Hao%20Feng%20and%20Yanjie%20Wang%20and%20Wei%20Shi%20and%20An-Lan%20Wang%20and%20Jingqun%20Tang%20and%20Can%20Huang%0AAbstract%3A%20%20%20Autoregressive%20models%20have%20become%20the%20de%20facto%20choice%20for%20sequence%20generation%0Atasks%2C%20but%20standard%20approaches%20treat%20digits%20as%20independent%20tokens%20and%20apply%0Across-entropy%20loss%2C%20overlooking%20the%20coherent%20structure%20of%20numerical%20sequences.%0AThis%20paper%20introduces%20Numerical%20Token%20Integrity%20Loss%20%28NTIL%29%20to%20address%20this%0Agap.%20NTIL%20operates%20at%20two%20levels%3A%20%281%29%20token-level%2C%20where%20it%20extends%20the%20Earth%0AMover%27s%20Distance%20%28EMD%29%20to%20preserve%20ordinal%20relationships%20between%20numerical%0Avalues%2C%20and%20%282%29%20sequence-level%2C%20where%20it%20penalizes%20the%20overall%20discrepancy%0Abetween%20the%20predicted%20and%20actual%20sequences.%20This%20dual%20approach%20improves%0Anumerical%20prediction%20and%20integrates%20effectively%20with%20LLMs/MLLMs.%20Extensive%0Aexperiments%20show%20significant%20performance%20improvements%20with%20NTIL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Sequential%2520Numerical%2520Prediction%2520in%2520Autoregressive%2520Models%26entry.906535625%3DXiang%2520Fei%2520and%2520Jinghui%2520Lu%2520and%2520Qi%2520Sun%2520and%2520Hao%2520Feng%2520and%2520Yanjie%2520Wang%2520and%2520Wei%2520Shi%2520and%2520An-Lan%2520Wang%2520and%2520Jingqun%2520Tang%2520and%2520Can%2520Huang%26entry.1292438233%3D%2520%2520Autoregressive%2520models%2520have%2520become%2520the%2520de%2520facto%2520choice%2520for%2520sequence%2520generation%250Atasks%252C%2520but%2520standard%2520approaches%2520treat%2520digits%2520as%2520independent%2520tokens%2520and%2520apply%250Across-entropy%2520loss%252C%2520overlooking%2520the%2520coherent%2520structure%2520of%2520numerical%2520sequences.%250AThis%2520paper%2520introduces%2520Numerical%2520Token%2520Integrity%2520Loss%2520%2528NTIL%2529%2520to%2520address%2520this%250Agap.%2520NTIL%2520operates%2520at%2520two%2520levels%253A%2520%25281%2529%2520token-level%252C%2520where%2520it%2520extends%2520the%2520Earth%250AMover%2527s%2520Distance%2520%2528EMD%2529%2520to%2520preserve%2520ordinal%2520relationships%2520between%2520numerical%250Avalues%252C%2520and%2520%25282%2529%2520sequence-level%252C%2520where%2520it%2520penalizes%2520the%2520overall%2520discrepancy%250Abetween%2520the%2520predicted%2520and%2520actual%2520sequences.%2520This%2520dual%2520approach%2520improves%250Anumerical%2520prediction%2520and%2520integrates%2520effectively%2520with%2520LLMs/MLLMs.%2520Extensive%250Aexperiments%2520show%2520significant%2520performance%2520improvements%2520with%2520NTIL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Sequential%20Numerical%20Prediction%20in%20Autoregressive%20Models&entry.906535625=Xiang%20Fei%20and%20Jinghui%20Lu%20and%20Qi%20Sun%20and%20Hao%20Feng%20and%20Yanjie%20Wang%20and%20Wei%20Shi%20and%20An-Lan%20Wang%20and%20Jingqun%20Tang%20and%20Can%20Huang&entry.1292438233=%20%20Autoregressive%20models%20have%20become%20the%20de%20facto%20choice%20for%20sequence%20generation%0Atasks%2C%20but%20standard%20approaches%20treat%20digits%20as%20independent%20tokens%20and%20apply%0Across-entropy%20loss%2C%20overlooking%20the%20coherent%20structure%20of%20numerical%20sequences.%0AThis%20paper%20introduces%20Numerical%20Token%20Integrity%20Loss%20%28NTIL%29%20to%20address%20this%0Agap.%20NTIL%20operates%20at%20two%20levels%3A%20%281%29%20token-level%2C%20where%20it%20extends%20the%20Earth%0AMover%27s%20Distance%20%28EMD%29%20to%20preserve%20ordinal%20relationships%20between%20numerical%0Avalues%2C%20and%20%282%29%20sequence-level%2C%20where%20it%20penalizes%20the%20overall%20discrepancy%0Abetween%20the%20predicted%20and%20actual%20sequences.%20This%20dual%20approach%20improves%0Anumerical%20prediction%20and%20integrates%20effectively%20with%20LLMs/MLLMs.%20Extensive%0Aexperiments%20show%20significant%20performance%20improvements%20with%20NTIL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13077v1&entry.124074799=Read"},
{"title": "A Minimum Description Length Approach to Regularization in Neural\n  Networks", "author": "Matan Abudy and Orr Well and Emmanuel Chemla and Roni Katzir and Nur Lan", "abstract": "  State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization.\n", "link": "http://arxiv.org/abs/2505.13398v1", "date": "2025-05-19", "relevancy": 2.3201, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Minimum%20Description%20Length%20Approach%20to%20Regularization%20in%20Neural%0A%20%20Networks&body=Title%3A%20A%20Minimum%20Description%20Length%20Approach%20to%20Regularization%20in%20Neural%0A%20%20Networks%0AAuthor%3A%20Matan%20Abudy%20and%20Orr%20Well%20and%20Emmanuel%20Chemla%20and%20Roni%20Katzir%20and%20Nur%20Lan%0AAbstract%3A%20%20%20State-of-the-art%20neural%20networks%20can%20be%20trained%20to%20become%20remarkable%0Asolutions%20to%20many%20problems.%20But%20while%20these%20architectures%20can%20express%20symbolic%2C%0Aperfect%20solutions%2C%20trained%20models%20often%20arrive%20at%20approximations%20instead.%20We%0Ashow%20that%20the%20choice%20of%20regularization%20method%20plays%20a%20crucial%20role%3A%20when%0Atrained%20on%20formal%20languages%20with%20standard%20regularization%20%28%24L_1%24%2C%20%24L_2%24%2C%20or%0Anone%29%2C%20expressive%20architectures%20not%20only%20fail%20to%20converge%20to%20correct%20solutions%0Abut%20are%20actively%20pushed%20away%20from%20perfect%20initializations.%20In%20contrast%2C%0Aapplying%20the%20Minimum%20Description%20Length%20%28MDL%29%20principle%20to%20balance%20model%0Acomplexity%20with%20data%20fit%20provides%20a%20theoretically%20grounded%20regularization%0Amethod.%20Using%20MDL%2C%20perfect%20solutions%20are%20selected%20over%20approximations%2C%0Aindependently%20of%20the%20optimization%20algorithm.%20We%20propose%20that%20unlike%20existing%0Aregularization%20techniques%2C%20MDL%20introduces%20the%20appropriate%20inductive%20bias%20to%0Aeffectively%20counteract%20overfitting%20and%20promote%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Minimum%2520Description%2520Length%2520Approach%2520to%2520Regularization%2520in%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMatan%2520Abudy%2520and%2520Orr%2520Well%2520and%2520Emmanuel%2520Chemla%2520and%2520Roni%2520Katzir%2520and%2520Nur%2520Lan%26entry.1292438233%3D%2520%2520State-of-the-art%2520neural%2520networks%2520can%2520be%2520trained%2520to%2520become%2520remarkable%250Asolutions%2520to%2520many%2520problems.%2520But%2520while%2520these%2520architectures%2520can%2520express%2520symbolic%252C%250Aperfect%2520solutions%252C%2520trained%2520models%2520often%2520arrive%2520at%2520approximations%2520instead.%2520We%250Ashow%2520that%2520the%2520choice%2520of%2520regularization%2520method%2520plays%2520a%2520crucial%2520role%253A%2520when%250Atrained%2520on%2520formal%2520languages%2520with%2520standard%2520regularization%2520%2528%2524L_1%2524%252C%2520%2524L_2%2524%252C%2520or%250Anone%2529%252C%2520expressive%2520architectures%2520not%2520only%2520fail%2520to%2520converge%2520to%2520correct%2520solutions%250Abut%2520are%2520actively%2520pushed%2520away%2520from%2520perfect%2520initializations.%2520In%2520contrast%252C%250Aapplying%2520the%2520Minimum%2520Description%2520Length%2520%2528MDL%2529%2520principle%2520to%2520balance%2520model%250Acomplexity%2520with%2520data%2520fit%2520provides%2520a%2520theoretically%2520grounded%2520regularization%250Amethod.%2520Using%2520MDL%252C%2520perfect%2520solutions%2520are%2520selected%2520over%2520approximations%252C%250Aindependently%2520of%2520the%2520optimization%2520algorithm.%2520We%2520propose%2520that%2520unlike%2520existing%250Aregularization%2520techniques%252C%2520MDL%2520introduces%2520the%2520appropriate%2520inductive%2520bias%2520to%250Aeffectively%2520counteract%2520overfitting%2520and%2520promote%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Minimum%20Description%20Length%20Approach%20to%20Regularization%20in%20Neural%0A%20%20Networks&entry.906535625=Matan%20Abudy%20and%20Orr%20Well%20and%20Emmanuel%20Chemla%20and%20Roni%20Katzir%20and%20Nur%20Lan&entry.1292438233=%20%20State-of-the-art%20neural%20networks%20can%20be%20trained%20to%20become%20remarkable%0Asolutions%20to%20many%20problems.%20But%20while%20these%20architectures%20can%20express%20symbolic%2C%0Aperfect%20solutions%2C%20trained%20models%20often%20arrive%20at%20approximations%20instead.%20We%0Ashow%20that%20the%20choice%20of%20regularization%20method%20plays%20a%20crucial%20role%3A%20when%0Atrained%20on%20formal%20languages%20with%20standard%20regularization%20%28%24L_1%24%2C%20%24L_2%24%2C%20or%0Anone%29%2C%20expressive%20architectures%20not%20only%20fail%20to%20converge%20to%20correct%20solutions%0Abut%20are%20actively%20pushed%20away%20from%20perfect%20initializations.%20In%20contrast%2C%0Aapplying%20the%20Minimum%20Description%20Length%20%28MDL%29%20principle%20to%20balance%20model%0Acomplexity%20with%20data%20fit%20provides%20a%20theoretically%20grounded%20regularization%0Amethod.%20Using%20MDL%2C%20perfect%20solutions%20are%20selected%20over%20approximations%2C%0Aindependently%20of%20the%20optimization%20algorithm.%20We%20propose%20that%20unlike%20existing%0Aregularization%20techniques%2C%20MDL%20introduces%20the%20appropriate%20inductive%20bias%20to%0Aeffectively%20counteract%20overfitting%20and%20promote%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13398v1&entry.124074799=Read"},
{"title": "Infinigen-Sim: Procedural Generation of Articulated Simulation Assets", "author": "Abhishek Joshi and Beining Han and Jack Nugent and Yiming Zuo and Jonathan Liu and Hongyu Wen and Stamatis Alexandropoulos and Tao Sun and Alexander Raistrick and Gaowen Liu and Yi Shao and Jia Deng", "abstract": "  We introduce Infinigen-Sim, a toolkit which enables users to create diverse\nand realistic articulated object procedural generators. These tools are\ncomposed of high-level utilities for use creating articulated assets in\nBlender, as well as an export pipeline to integrate the resulting assets into\ncommon robotics simulators. We demonstrate our system by creating procedural\ngenerators for 5 common articulated object categories. Experiments show that\nassets sampled from these generators are useful for movable object\nsegmentation, training generalizable reinforcement learning policies, and\nsim-to-real transfer of imitation learning policies.\n", "link": "http://arxiv.org/abs/2505.10755v2", "date": "2025-05-19", "relevancy": 2.3155, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5944}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5723}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinigen-Sim%3A%20Procedural%20Generation%20of%20Articulated%20Simulation%20Assets&body=Title%3A%20Infinigen-Sim%3A%20Procedural%20Generation%20of%20Articulated%20Simulation%20Assets%0AAuthor%3A%20Abhishek%20Joshi%20and%20Beining%20Han%20and%20Jack%20Nugent%20and%20Yiming%20Zuo%20and%20Jonathan%20Liu%20and%20Hongyu%20Wen%20and%20Stamatis%20Alexandropoulos%20and%20Tao%20Sun%20and%20Alexander%20Raistrick%20and%20Gaowen%20Liu%20and%20Yi%20Shao%20and%20Jia%20Deng%0AAbstract%3A%20%20%20We%20introduce%20Infinigen-Sim%2C%20a%20toolkit%20which%20enables%20users%20to%20create%20diverse%0Aand%20realistic%20articulated%20object%20procedural%20generators.%20These%20tools%20are%0Acomposed%20of%20high-level%20utilities%20for%20use%20creating%20articulated%20assets%20in%0ABlender%2C%20as%20well%20as%20an%20export%20pipeline%20to%20integrate%20the%20resulting%20assets%20into%0Acommon%20robotics%20simulators.%20We%20demonstrate%20our%20system%20by%20creating%20procedural%0Agenerators%20for%205%20common%20articulated%20object%20categories.%20Experiments%20show%20that%0Aassets%20sampled%20from%20these%20generators%20are%20useful%20for%20movable%20object%0Asegmentation%2C%20training%20generalizable%20reinforcement%20learning%20policies%2C%20and%0Asim-to-real%20transfer%20of%20imitation%20learning%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinigen-Sim%253A%2520Procedural%2520Generation%2520of%2520Articulated%2520Simulation%2520Assets%26entry.906535625%3DAbhishek%2520Joshi%2520and%2520Beining%2520Han%2520and%2520Jack%2520Nugent%2520and%2520Yiming%2520Zuo%2520and%2520Jonathan%2520Liu%2520and%2520Hongyu%2520Wen%2520and%2520Stamatis%2520Alexandropoulos%2520and%2520Tao%2520Sun%2520and%2520Alexander%2520Raistrick%2520and%2520Gaowen%2520Liu%2520and%2520Yi%2520Shao%2520and%2520Jia%2520Deng%26entry.1292438233%3D%2520%2520We%2520introduce%2520Infinigen-Sim%252C%2520a%2520toolkit%2520which%2520enables%2520users%2520to%2520create%2520diverse%250Aand%2520realistic%2520articulated%2520object%2520procedural%2520generators.%2520These%2520tools%2520are%250Acomposed%2520of%2520high-level%2520utilities%2520for%2520use%2520creating%2520articulated%2520assets%2520in%250ABlender%252C%2520as%2520well%2520as%2520an%2520export%2520pipeline%2520to%2520integrate%2520the%2520resulting%2520assets%2520into%250Acommon%2520robotics%2520simulators.%2520We%2520demonstrate%2520our%2520system%2520by%2520creating%2520procedural%250Agenerators%2520for%25205%2520common%2520articulated%2520object%2520categories.%2520Experiments%2520show%2520that%250Aassets%2520sampled%2520from%2520these%2520generators%2520are%2520useful%2520for%2520movable%2520object%250Asegmentation%252C%2520training%2520generalizable%2520reinforcement%2520learning%2520policies%252C%2520and%250Asim-to-real%2520transfer%2520of%2520imitation%2520learning%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinigen-Sim%3A%20Procedural%20Generation%20of%20Articulated%20Simulation%20Assets&entry.906535625=Abhishek%20Joshi%20and%20Beining%20Han%20and%20Jack%20Nugent%20and%20Yiming%20Zuo%20and%20Jonathan%20Liu%20and%20Hongyu%20Wen%20and%20Stamatis%20Alexandropoulos%20and%20Tao%20Sun%20and%20Alexander%20Raistrick%20and%20Gaowen%20Liu%20and%20Yi%20Shao%20and%20Jia%20Deng&entry.1292438233=%20%20We%20introduce%20Infinigen-Sim%2C%20a%20toolkit%20which%20enables%20users%20to%20create%20diverse%0Aand%20realistic%20articulated%20object%20procedural%20generators.%20These%20tools%20are%0Acomposed%20of%20high-level%20utilities%20for%20use%20creating%20articulated%20assets%20in%0ABlender%2C%20as%20well%20as%20an%20export%20pipeline%20to%20integrate%20the%20resulting%20assets%20into%0Acommon%20robotics%20simulators.%20We%20demonstrate%20our%20system%20by%20creating%20procedural%0Agenerators%20for%205%20common%20articulated%20object%20categories.%20Experiments%20show%20that%0Aassets%20sampled%20from%20these%20generators%20are%20useful%20for%20movable%20object%0Asegmentation%2C%20training%20generalizable%20reinforcement%20learning%20policies%2C%20and%0Asim-to-real%20transfer%20of%20imitation%20learning%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10755v2&entry.124074799=Read"},
{"title": "High Dynamic Range Novel View Synthesis with Single Exposure", "author": "Kaixuan Zhang and Hu Wang and Minxian Li and Mingwu Ren and Mao Ye and Xiatian Zhu", "abstract": "  High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D\nscene HDR model from Low Dynamic Range (LDR) imagery. Typically,\nmultiple-exposure LDR images are employed to capture a wider range of\nbrightness levels in a scene, as a single LDR image cannot represent both the\nbrightest and darkest regions simultaneously. While effective, this\nmultiple-exposure HDR-NVS approach has significant limitations, including\nsusceptibility to motion artifacts (e.g., ghosting and blurring), high capture\nand storage costs. To overcome these challenges, we introduce, for the first\ntime, the single-exposure HDR-NVS problem, where only single exposure LDR\nimages are available during training. We further introduce a novel approach,\nMono-HDR-3D, featuring two dedicated modules formulated by the LDR image\nformation principles, one for converting LDR colors to HDR counterparts, and\nthe other for transforming HDR images to LDR format so that unsupervised\nlearning is enabled in a closed loop. Designed as a meta-algorithm, our\napproach can be seamlessly integrated with existing NVS models. Extensive\nexperiments show that Mono-HDR-3D significantly outperforms previous methods.\nSource code will be released.\n", "link": "http://arxiv.org/abs/2505.01212v2", "date": "2025-05-19", "relevancy": 2.3089, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5848}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5763}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20with%20Single%20Exposure&body=Title%3A%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20with%20Single%20Exposure%0AAuthor%3A%20Kaixuan%20Zhang%20and%20Hu%20Wang%20and%20Minxian%20Li%20and%20Mingwu%20Ren%20and%20Mao%20Ye%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20%28HDR-NVS%29%20aims%20to%20establish%20a%203D%0Ascene%20HDR%20model%20from%20Low%20Dynamic%20Range%20%28LDR%29%20imagery.%20Typically%2C%0Amultiple-exposure%20LDR%20images%20are%20employed%20to%20capture%20a%20wider%20range%20of%0Abrightness%20levels%20in%20a%20scene%2C%20as%20a%20single%20LDR%20image%20cannot%20represent%20both%20the%0Abrightest%20and%20darkest%20regions%20simultaneously.%20While%20effective%2C%20this%0Amultiple-exposure%20HDR-NVS%20approach%20has%20significant%20limitations%2C%20including%0Asusceptibility%20to%20motion%20artifacts%20%28e.g.%2C%20ghosting%20and%20blurring%29%2C%20high%20capture%0Aand%20storage%20costs.%20To%20overcome%20these%20challenges%2C%20we%20introduce%2C%20for%20the%20first%0Atime%2C%20the%20single-exposure%20HDR-NVS%20problem%2C%20where%20only%20single%20exposure%20LDR%0Aimages%20are%20available%20during%20training.%20We%20further%20introduce%20a%20novel%20approach%2C%0AMono-HDR-3D%2C%20featuring%20two%20dedicated%20modules%20formulated%20by%20the%20LDR%20image%0Aformation%20principles%2C%20one%20for%20converting%20LDR%20colors%20to%20HDR%20counterparts%2C%20and%0Athe%20other%20for%20transforming%20HDR%20images%20to%20LDR%20format%20so%20that%20unsupervised%0Alearning%20is%20enabled%20in%20a%20closed%20loop.%20Designed%20as%20a%20meta-algorithm%2C%20our%0Aapproach%20can%20be%20seamlessly%20integrated%20with%20existing%20NVS%20models.%20Extensive%0Aexperiments%20show%20that%20Mono-HDR-3D%20significantly%20outperforms%20previous%20methods.%0ASource%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh%2520Dynamic%2520Range%2520Novel%2520View%2520Synthesis%2520with%2520Single%2520Exposure%26entry.906535625%3DKaixuan%2520Zhang%2520and%2520Hu%2520Wang%2520and%2520Minxian%2520Li%2520and%2520Mingwu%2520Ren%2520and%2520Mao%2520Ye%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520High%2520Dynamic%2520Range%2520Novel%2520View%2520Synthesis%2520%2528HDR-NVS%2529%2520aims%2520to%2520establish%2520a%25203D%250Ascene%2520HDR%2520model%2520from%2520Low%2520Dynamic%2520Range%2520%2528LDR%2529%2520imagery.%2520Typically%252C%250Amultiple-exposure%2520LDR%2520images%2520are%2520employed%2520to%2520capture%2520a%2520wider%2520range%2520of%250Abrightness%2520levels%2520in%2520a%2520scene%252C%2520as%2520a%2520single%2520LDR%2520image%2520cannot%2520represent%2520both%2520the%250Abrightest%2520and%2520darkest%2520regions%2520simultaneously.%2520While%2520effective%252C%2520this%250Amultiple-exposure%2520HDR-NVS%2520approach%2520has%2520significant%2520limitations%252C%2520including%250Asusceptibility%2520to%2520motion%2520artifacts%2520%2528e.g.%252C%2520ghosting%2520and%2520blurring%2529%252C%2520high%2520capture%250Aand%2520storage%2520costs.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%252C%2520for%2520the%2520first%250Atime%252C%2520the%2520single-exposure%2520HDR-NVS%2520problem%252C%2520where%2520only%2520single%2520exposure%2520LDR%250Aimages%2520are%2520available%2520during%2520training.%2520We%2520further%2520introduce%2520a%2520novel%2520approach%252C%250AMono-HDR-3D%252C%2520featuring%2520two%2520dedicated%2520modules%2520formulated%2520by%2520the%2520LDR%2520image%250Aformation%2520principles%252C%2520one%2520for%2520converting%2520LDR%2520colors%2520to%2520HDR%2520counterparts%252C%2520and%250Athe%2520other%2520for%2520transforming%2520HDR%2520images%2520to%2520LDR%2520format%2520so%2520that%2520unsupervised%250Alearning%2520is%2520enabled%2520in%2520a%2520closed%2520loop.%2520Designed%2520as%2520a%2520meta-algorithm%252C%2520our%250Aapproach%2520can%2520be%2520seamlessly%2520integrated%2520with%2520existing%2520NVS%2520models.%2520Extensive%250Aexperiments%2520show%2520that%2520Mono-HDR-3D%2520significantly%2520outperforms%2520previous%2520methods.%250ASource%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High%20Dynamic%20Range%20Novel%20View%20Synthesis%20with%20Single%20Exposure&entry.906535625=Kaixuan%20Zhang%20and%20Hu%20Wang%20and%20Minxian%20Li%20and%20Mingwu%20Ren%20and%20Mao%20Ye%20and%20Xiatian%20Zhu&entry.1292438233=%20%20High%20Dynamic%20Range%20Novel%20View%20Synthesis%20%28HDR-NVS%29%20aims%20to%20establish%20a%203D%0Ascene%20HDR%20model%20from%20Low%20Dynamic%20Range%20%28LDR%29%20imagery.%20Typically%2C%0Amultiple-exposure%20LDR%20images%20are%20employed%20to%20capture%20a%20wider%20range%20of%0Abrightness%20levels%20in%20a%20scene%2C%20as%20a%20single%20LDR%20image%20cannot%20represent%20both%20the%0Abrightest%20and%20darkest%20regions%20simultaneously.%20While%20effective%2C%20this%0Amultiple-exposure%20HDR-NVS%20approach%20has%20significant%20limitations%2C%20including%0Asusceptibility%20to%20motion%20artifacts%20%28e.g.%2C%20ghosting%20and%20blurring%29%2C%20high%20capture%0Aand%20storage%20costs.%20To%20overcome%20these%20challenges%2C%20we%20introduce%2C%20for%20the%20first%0Atime%2C%20the%20single-exposure%20HDR-NVS%20problem%2C%20where%20only%20single%20exposure%20LDR%0Aimages%20are%20available%20during%20training.%20We%20further%20introduce%20a%20novel%20approach%2C%0AMono-HDR-3D%2C%20featuring%20two%20dedicated%20modules%20formulated%20by%20the%20LDR%20image%0Aformation%20principles%2C%20one%20for%20converting%20LDR%20colors%20to%20HDR%20counterparts%2C%20and%0Athe%20other%20for%20transforming%20HDR%20images%20to%20LDR%20format%20so%20that%20unsupervised%0Alearning%20is%20enabled%20in%20a%20closed%20loop.%20Designed%20as%20a%20meta-algorithm%2C%20our%0Aapproach%20can%20be%20seamlessly%20integrated%20with%20existing%20NVS%20models.%20Extensive%0Aexperiments%20show%20that%20Mono-HDR-3D%20significantly%20outperforms%20previous%20methods.%0ASource%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01212v2&entry.124074799=Read"},
{"title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "author": "Colin Krawchuk and Nikhil Khatri and Neil John Ortega and Dimitri Kartsaklis", "abstract": "  Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.\n", "link": "http://arxiv.org/abs/2505.13208v1", "date": "2025-05-19", "relevancy": 2.3045, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Generation%20of%20Parameterised%20Quantum%20Circuits%20from%20Large%20Texts&body=Title%3A%20Efficient%20Generation%20of%20Parameterised%20Quantum%20Circuits%20from%20Large%20Texts%0AAuthor%3A%20Colin%20Krawchuk%20and%20Nikhil%20Khatri%20and%20Neil%20John%20Ortega%20and%20Dimitri%20Kartsaklis%0AAbstract%3A%20%20%20Quantum%20approaches%20to%20natural%20language%20processing%20%28NLP%29%20are%20redefining%20how%0Alinguistic%20information%20is%20represented%20and%20processed.%20While%20traditional%20hybrid%0Aquantum-classical%20models%20rely%20heavily%20on%20classical%20neural%20networks%2C%20recent%0Aadvancements%20propose%20a%20novel%20framework%2C%20DisCoCirc%2C%20capable%20of%20directly%20encoding%0Aentire%20documents%20as%20parameterised%20quantum%20circuits%20%28PQCs%29%2C%20besides%20enjoying%0Asome%20additional%20interpretability%20and%20compositionality%20benefits.%20Following%20these%0Aideas%2C%20this%20paper%20introduces%20an%20efficient%20methodology%20for%20converting%0Alarge-scale%20texts%20into%20quantum%20circuits%20using%20tree-like%20representations%20of%0Apregroup%20diagrams.%20Exploiting%20the%20compositional%20parallels%20between%20language%20and%0Aquantum%20mechanics%2C%20grounded%20in%20symmetric%20monoidal%20categories%2C%20our%20approach%0Aenables%20faithful%20and%20efficient%20encoding%20of%20syntactic%20and%20discourse%0Arelationships%20in%20long%20and%20complex%20texts%20%28up%20to%206410%20words%20in%20our%20experiments%29%0Ato%20quantum%20circuits.%20The%20developed%20system%20is%20provided%20to%20the%20community%20as%20part%0Aof%20the%20augmented%20open-source%20quantum%20NLP%20package%20lambeq%20Gen%20II.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Generation%2520of%2520Parameterised%2520Quantum%2520Circuits%2520from%2520Large%2520Texts%26entry.906535625%3DColin%2520Krawchuk%2520and%2520Nikhil%2520Khatri%2520and%2520Neil%2520John%2520Ortega%2520and%2520Dimitri%2520Kartsaklis%26entry.1292438233%3D%2520%2520Quantum%2520approaches%2520to%2520natural%2520language%2520processing%2520%2528NLP%2529%2520are%2520redefining%2520how%250Alinguistic%2520information%2520is%2520represented%2520and%2520processed.%2520While%2520traditional%2520hybrid%250Aquantum-classical%2520models%2520rely%2520heavily%2520on%2520classical%2520neural%2520networks%252C%2520recent%250Aadvancements%2520propose%2520a%2520novel%2520framework%252C%2520DisCoCirc%252C%2520capable%2520of%2520directly%2520encoding%250Aentire%2520documents%2520as%2520parameterised%2520quantum%2520circuits%2520%2528PQCs%2529%252C%2520besides%2520enjoying%250Asome%2520additional%2520interpretability%2520and%2520compositionality%2520benefits.%2520Following%2520these%250Aideas%252C%2520this%2520paper%2520introduces%2520an%2520efficient%2520methodology%2520for%2520converting%250Alarge-scale%2520texts%2520into%2520quantum%2520circuits%2520using%2520tree-like%2520representations%2520of%250Apregroup%2520diagrams.%2520Exploiting%2520the%2520compositional%2520parallels%2520between%2520language%2520and%250Aquantum%2520mechanics%252C%2520grounded%2520in%2520symmetric%2520monoidal%2520categories%252C%2520our%2520approach%250Aenables%2520faithful%2520and%2520efficient%2520encoding%2520of%2520syntactic%2520and%2520discourse%250Arelationships%2520in%2520long%2520and%2520complex%2520texts%2520%2528up%2520to%25206410%2520words%2520in%2520our%2520experiments%2529%250Ato%2520quantum%2520circuits.%2520The%2520developed%2520system%2520is%2520provided%2520to%2520the%2520community%2520as%2520part%250Aof%2520the%2520augmented%2520open-source%2520quantum%2520NLP%2520package%2520lambeq%2520Gen%2520II.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Generation%20of%20Parameterised%20Quantum%20Circuits%20from%20Large%20Texts&entry.906535625=Colin%20Krawchuk%20and%20Nikhil%20Khatri%20and%20Neil%20John%20Ortega%20and%20Dimitri%20Kartsaklis&entry.1292438233=%20%20Quantum%20approaches%20to%20natural%20language%20processing%20%28NLP%29%20are%20redefining%20how%0Alinguistic%20information%20is%20represented%20and%20processed.%20While%20traditional%20hybrid%0Aquantum-classical%20models%20rely%20heavily%20on%20classical%20neural%20networks%2C%20recent%0Aadvancements%20propose%20a%20novel%20framework%2C%20DisCoCirc%2C%20capable%20of%20directly%20encoding%0Aentire%20documents%20as%20parameterised%20quantum%20circuits%20%28PQCs%29%2C%20besides%20enjoying%0Asome%20additional%20interpretability%20and%20compositionality%20benefits.%20Following%20these%0Aideas%2C%20this%20paper%20introduces%20an%20efficient%20methodology%20for%20converting%0Alarge-scale%20texts%20into%20quantum%20circuits%20using%20tree-like%20representations%20of%0Apregroup%20diagrams.%20Exploiting%20the%20compositional%20parallels%20between%20language%20and%0Aquantum%20mechanics%2C%20grounded%20in%20symmetric%20monoidal%20categories%2C%20our%20approach%0Aenables%20faithful%20and%20efficient%20encoding%20of%20syntactic%20and%20discourse%0Arelationships%20in%20long%20and%20complex%20texts%20%28up%20to%206410%20words%20in%20our%20experiments%29%0Ato%20quantum%20circuits.%20The%20developed%20system%20is%20provided%20to%20the%20community%20as%20part%0Aof%20the%20augmented%20open-source%20quantum%20NLP%20package%20lambeq%20Gen%20II.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13208v1&entry.124074799=Read"},
{"title": "Policy Contrastive Decoding for Robotic Foundation Models", "author": "Shihan Wu and Ji Zhang and Xu Luo and Junlin Xie and Jingkuan Song and Heng Tao Shen and Lianli Gao", "abstract": "  Robotic foundation models, or generalist robot policies, hold immense\npotential to enable flexible, general-purpose and dexterous robotic systems.\nDespite their advancements, our empirical experiments reveal that existing\nrobot policies are prone to learning spurious correlations from pre-training\ntrajectories, adversely affecting their generalization capabilities beyond the\ntraining data. To tackle this, we propose a novel Policy Contrastive Decoding\n(PCD) approach, which redirects the robot policy's focus toward object-relevant\nvisual clues by contrasting action probability distributions derived from\noriginal and object-masked visual inputs. As a training-free method, our PCD\ncan be used as a plugin to improve different types of robot policies without\nneeding to finetune or access model weights. We conduct extensive experiments\non top of three open-source robot policies, including the autoregressive policy\nOpenVLA and the diffusion-based policies Octo and $\\pi_0$. The obtained results\nin both simulation and real-world environments prove PCD's flexibility and\neffectiveness, e.g., PCD enhances the state-of-the-art policy $\\pi_0$ by 8% in\nthe simulation environment and by 108% in the real-world environment. Code and\ndemos are publicly available at: https://Koorye.github.io/proj/PCD.\n", "link": "http://arxiv.org/abs/2505.13255v1", "date": "2025-05-19", "relevancy": 2.2894, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Policy%20Contrastive%20Decoding%20for%20Robotic%20Foundation%20Models&body=Title%3A%20Policy%20Contrastive%20Decoding%20for%20Robotic%20Foundation%20Models%0AAuthor%3A%20Shihan%20Wu%20and%20Ji%20Zhang%20and%20Xu%20Luo%20and%20Junlin%20Xie%20and%20Jingkuan%20Song%20and%20Heng%20Tao%20Shen%20and%20Lianli%20Gao%0AAbstract%3A%20%20%20Robotic%20foundation%20models%2C%20or%20generalist%20robot%20policies%2C%20hold%20immense%0Apotential%20to%20enable%20flexible%2C%20general-purpose%20and%20dexterous%20robotic%20systems.%0ADespite%20their%20advancements%2C%20our%20empirical%20experiments%20reveal%20that%20existing%0Arobot%20policies%20are%20prone%20to%20learning%20spurious%20correlations%20from%20pre-training%0Atrajectories%2C%20adversely%20affecting%20their%20generalization%20capabilities%20beyond%20the%0Atraining%20data.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20Policy%20Contrastive%20Decoding%0A%28PCD%29%20approach%2C%20which%20redirects%20the%20robot%20policy%27s%20focus%20toward%20object-relevant%0Avisual%20clues%20by%20contrasting%20action%20probability%20distributions%20derived%20from%0Aoriginal%20and%20object-masked%20visual%20inputs.%20As%20a%20training-free%20method%2C%20our%20PCD%0Acan%20be%20used%20as%20a%20plugin%20to%20improve%20different%20types%20of%20robot%20policies%20without%0Aneeding%20to%20finetune%20or%20access%20model%20weights.%20We%20conduct%20extensive%20experiments%0Aon%20top%20of%20three%20open-source%20robot%20policies%2C%20including%20the%20autoregressive%20policy%0AOpenVLA%20and%20the%20diffusion-based%20policies%20Octo%20and%20%24%5Cpi_0%24.%20The%20obtained%20results%0Ain%20both%20simulation%20and%20real-world%20environments%20prove%20PCD%27s%20flexibility%20and%0Aeffectiveness%2C%20e.g.%2C%20PCD%20enhances%20the%20state-of-the-art%20policy%20%24%5Cpi_0%24%20by%208%25%20in%0Athe%20simulation%20environment%20and%20by%20108%25%20in%20the%20real-world%20environment.%20Code%20and%0Ademos%20are%20publicly%20available%20at%3A%20https%3A//Koorye.github.io/proj/PCD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolicy%2520Contrastive%2520Decoding%2520for%2520Robotic%2520Foundation%2520Models%26entry.906535625%3DShihan%2520Wu%2520and%2520Ji%2520Zhang%2520and%2520Xu%2520Luo%2520and%2520Junlin%2520Xie%2520and%2520Jingkuan%2520Song%2520and%2520Heng%2520Tao%2520Shen%2520and%2520Lianli%2520Gao%26entry.1292438233%3D%2520%2520Robotic%2520foundation%2520models%252C%2520or%2520generalist%2520robot%2520policies%252C%2520hold%2520immense%250Apotential%2520to%2520enable%2520flexible%252C%2520general-purpose%2520and%2520dexterous%2520robotic%2520systems.%250ADespite%2520their%2520advancements%252C%2520our%2520empirical%2520experiments%2520reveal%2520that%2520existing%250Arobot%2520policies%2520are%2520prone%2520to%2520learning%2520spurious%2520correlations%2520from%2520pre-training%250Atrajectories%252C%2520adversely%2520affecting%2520their%2520generalization%2520capabilities%2520beyond%2520the%250Atraining%2520data.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%2520novel%2520Policy%2520Contrastive%2520Decoding%250A%2528PCD%2529%2520approach%252C%2520which%2520redirects%2520the%2520robot%2520policy%2527s%2520focus%2520toward%2520object-relevant%250Avisual%2520clues%2520by%2520contrasting%2520action%2520probability%2520distributions%2520derived%2520from%250Aoriginal%2520and%2520object-masked%2520visual%2520inputs.%2520As%2520a%2520training-free%2520method%252C%2520our%2520PCD%250Acan%2520be%2520used%2520as%2520a%2520plugin%2520to%2520improve%2520different%2520types%2520of%2520robot%2520policies%2520without%250Aneeding%2520to%2520finetune%2520or%2520access%2520model%2520weights.%2520We%2520conduct%2520extensive%2520experiments%250Aon%2520top%2520of%2520three%2520open-source%2520robot%2520policies%252C%2520including%2520the%2520autoregressive%2520policy%250AOpenVLA%2520and%2520the%2520diffusion-based%2520policies%2520Octo%2520and%2520%2524%255Cpi_0%2524.%2520The%2520obtained%2520results%250Ain%2520both%2520simulation%2520and%2520real-world%2520environments%2520prove%2520PCD%2527s%2520flexibility%2520and%250Aeffectiveness%252C%2520e.g.%252C%2520PCD%2520enhances%2520the%2520state-of-the-art%2520policy%2520%2524%255Cpi_0%2524%2520by%25208%2525%2520in%250Athe%2520simulation%2520environment%2520and%2520by%2520108%2525%2520in%2520the%2520real-world%2520environment.%2520Code%2520and%250Ademos%2520are%2520publicly%2520available%2520at%253A%2520https%253A//Koorye.github.io/proj/PCD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Policy%20Contrastive%20Decoding%20for%20Robotic%20Foundation%20Models&entry.906535625=Shihan%20Wu%20and%20Ji%20Zhang%20and%20Xu%20Luo%20and%20Junlin%20Xie%20and%20Jingkuan%20Song%20and%20Heng%20Tao%20Shen%20and%20Lianli%20Gao&entry.1292438233=%20%20Robotic%20foundation%20models%2C%20or%20generalist%20robot%20policies%2C%20hold%20immense%0Apotential%20to%20enable%20flexible%2C%20general-purpose%20and%20dexterous%20robotic%20systems.%0ADespite%20their%20advancements%2C%20our%20empirical%20experiments%20reveal%20that%20existing%0Arobot%20policies%20are%20prone%20to%20learning%20spurious%20correlations%20from%20pre-training%0Atrajectories%2C%20adversely%20affecting%20their%20generalization%20capabilities%20beyond%20the%0Atraining%20data.%20To%20tackle%20this%2C%20we%20propose%20a%20novel%20Policy%20Contrastive%20Decoding%0A%28PCD%29%20approach%2C%20which%20redirects%20the%20robot%20policy%27s%20focus%20toward%20object-relevant%0Avisual%20clues%20by%20contrasting%20action%20probability%20distributions%20derived%20from%0Aoriginal%20and%20object-masked%20visual%20inputs.%20As%20a%20training-free%20method%2C%20our%20PCD%0Acan%20be%20used%20as%20a%20plugin%20to%20improve%20different%20types%20of%20robot%20policies%20without%0Aneeding%20to%20finetune%20or%20access%20model%20weights.%20We%20conduct%20extensive%20experiments%0Aon%20top%20of%20three%20open-source%20robot%20policies%2C%20including%20the%20autoregressive%20policy%0AOpenVLA%20and%20the%20diffusion-based%20policies%20Octo%20and%20%24%5Cpi_0%24.%20The%20obtained%20results%0Ain%20both%20simulation%20and%20real-world%20environments%20prove%20PCD%27s%20flexibility%20and%0Aeffectiveness%2C%20e.g.%2C%20PCD%20enhances%20the%20state-of-the-art%20policy%20%24%5Cpi_0%24%20by%208%25%20in%0Athe%20simulation%20environment%20and%20by%20108%25%20in%20the%20real-world%20environment.%20Code%20and%0Ademos%20are%20publicly%20available%20at%3A%20https%3A//Koorye.github.io/proj/PCD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13255v1&entry.124074799=Read"},
{"title": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training", "author": "Adel Nabli and Louis Fournier and Pierre Erbacher and Louis Serrano and Eugene Belilovsky and Edouard Oyallon", "abstract": "  Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (\\acco), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, \\acco~reduces GPU\nidle time and supports heterogeneous hardware. To mitigate the convergence\nissues caused by delayed updates, we introduce a novel technique ensuring\ntraining dynamics align with standard distributed optimization. Compared to\nZeRO-1, our approach is significantly faster and scales effectively across\nheterogeneous hardware.\n", "link": "http://arxiv.org/abs/2406.02613v2", "date": "2025-05-19", "relevancy": 2.2894, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4603}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACCO%3A%20Accumulate%20While%20You%20Communicate%20for%20Communication-Overlapped%0A%20%20Sharded%20LLM%20Training&body=Title%3A%20ACCO%3A%20Accumulate%20While%20You%20Communicate%20for%20Communication-Overlapped%0A%20%20Sharded%20LLM%20Training%0AAuthor%3A%20Adel%20Nabli%20and%20Louis%20Fournier%20and%20Pierre%20Erbacher%20and%20Louis%20Serrano%20and%20Eugene%20Belilovsky%20and%20Edouard%20Oyallon%0AAbstract%3A%20%20%20Training%20LLMs%20relies%20on%20distributed%20implementations%20using%20multiple%20GPUs%20to%0Acompute%20gradients%20in%20parallel%20with%20sharded%20optimizers.%20However%2C%20synchronizing%0Agradients%20in%20data%20parallel%20setups%20introduces%20communication%20overhead%20that%20grows%0Awith%20the%20number%20of%20workers%2C%20limiting%20parallelization%20efficiency.%20Local%0Aoptimization%20algorithms%20reduce%20communications%20but%20incur%20high%20memory%20costs%20as%0Athey%20prevent%20optimizer%20state%20sharding%2C%20hindering%20scalability.%20To%20address%20this%2C%0Awe%20propose%20%5Ctextbf%7BAC%7Dcumulate%20while%20%5Ctextbf%7BCO%7Dmmunicate%20%28%5Cacco%29%2C%20a%0Amemory-efficient%20optimization%20algorithm%20for%20distributed%20LLM%20training.%20By%0Asynchronizing%20delayed%20gradients%20while%20computing%20new%20ones%2C%20%5Cacco~reduces%20GPU%0Aidle%20time%20and%20supports%20heterogeneous%20hardware.%20To%20mitigate%20the%20convergence%0Aissues%20caused%20by%20delayed%20updates%2C%20we%20introduce%20a%20novel%20technique%20ensuring%0Atraining%20dynamics%20align%20with%20standard%20distributed%20optimization.%20Compared%20to%0AZeRO-1%2C%20our%20approach%20is%20significantly%20faster%20and%20scales%20effectively%20across%0Aheterogeneous%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACCO%253A%2520Accumulate%2520While%2520You%2520Communicate%2520for%2520Communication-Overlapped%250A%2520%2520Sharded%2520LLM%2520Training%26entry.906535625%3DAdel%2520Nabli%2520and%2520Louis%2520Fournier%2520and%2520Pierre%2520Erbacher%2520and%2520Louis%2520Serrano%2520and%2520Eugene%2520Belilovsky%2520and%2520Edouard%2520Oyallon%26entry.1292438233%3D%2520%2520Training%2520LLMs%2520relies%2520on%2520distributed%2520implementations%2520using%2520multiple%2520GPUs%2520to%250Acompute%2520gradients%2520in%2520parallel%2520with%2520sharded%2520optimizers.%2520However%252C%2520synchronizing%250Agradients%2520in%2520data%2520parallel%2520setups%2520introduces%2520communication%2520overhead%2520that%2520grows%250Awith%2520the%2520number%2520of%2520workers%252C%2520limiting%2520parallelization%2520efficiency.%2520Local%250Aoptimization%2520algorithms%2520reduce%2520communications%2520but%2520incur%2520high%2520memory%2520costs%2520as%250Athey%2520prevent%2520optimizer%2520state%2520sharding%252C%2520hindering%2520scalability.%2520To%2520address%2520this%252C%250Awe%2520propose%2520%255Ctextbf%257BAC%257Dcumulate%2520while%2520%255Ctextbf%257BCO%257Dmmunicate%2520%2528%255Cacco%2529%252C%2520a%250Amemory-efficient%2520optimization%2520algorithm%2520for%2520distributed%2520LLM%2520training.%2520By%250Asynchronizing%2520delayed%2520gradients%2520while%2520computing%2520new%2520ones%252C%2520%255Cacco~reduces%2520GPU%250Aidle%2520time%2520and%2520supports%2520heterogeneous%2520hardware.%2520To%2520mitigate%2520the%2520convergence%250Aissues%2520caused%2520by%2520delayed%2520updates%252C%2520we%2520introduce%2520a%2520novel%2520technique%2520ensuring%250Atraining%2520dynamics%2520align%2520with%2520standard%2520distributed%2520optimization.%2520Compared%2520to%250AZeRO-1%252C%2520our%2520approach%2520is%2520significantly%2520faster%2520and%2520scales%2520effectively%2520across%250Aheterogeneous%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACCO%3A%20Accumulate%20While%20You%20Communicate%20for%20Communication-Overlapped%0A%20%20Sharded%20LLM%20Training&entry.906535625=Adel%20Nabli%20and%20Louis%20Fournier%20and%20Pierre%20Erbacher%20and%20Louis%20Serrano%20and%20Eugene%20Belilovsky%20and%20Edouard%20Oyallon&entry.1292438233=%20%20Training%20LLMs%20relies%20on%20distributed%20implementations%20using%20multiple%20GPUs%20to%0Acompute%20gradients%20in%20parallel%20with%20sharded%20optimizers.%20However%2C%20synchronizing%0Agradients%20in%20data%20parallel%20setups%20introduces%20communication%20overhead%20that%20grows%0Awith%20the%20number%20of%20workers%2C%20limiting%20parallelization%20efficiency.%20Local%0Aoptimization%20algorithms%20reduce%20communications%20but%20incur%20high%20memory%20costs%20as%0Athey%20prevent%20optimizer%20state%20sharding%2C%20hindering%20scalability.%20To%20address%20this%2C%0Awe%20propose%20%5Ctextbf%7BAC%7Dcumulate%20while%20%5Ctextbf%7BCO%7Dmmunicate%20%28%5Cacco%29%2C%20a%0Amemory-efficient%20optimization%20algorithm%20for%20distributed%20LLM%20training.%20By%0Asynchronizing%20delayed%20gradients%20while%20computing%20new%20ones%2C%20%5Cacco~reduces%20GPU%0Aidle%20time%20and%20supports%20heterogeneous%20hardware.%20To%20mitigate%20the%20convergence%0Aissues%20caused%20by%20delayed%20updates%2C%20we%20introduce%20a%20novel%20technique%20ensuring%0Atraining%20dynamics%20align%20with%20standard%20distributed%20optimization.%20Compared%20to%0AZeRO-1%2C%20our%20approach%20is%20significantly%20faster%20and%20scales%20effectively%20across%0Aheterogeneous%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02613v2&entry.124074799=Read"},
{"title": "Comparing Specialised Small and General Large Language Models on Text\n  Classification: 100 Labelled Samples to Achieve Break-Even Performance", "author": "Branislav Pecher and Ivan Srba and Maria Bielikova", "abstract": "  When solving NLP tasks with limited labelled data, researchers typically\neither use a general large language model without further update, or use a\nsmall number of labelled samples to tune a specialised smaller model. In this\nwork, we answer an important question -- how many labelled samples are required\nfor the specialised small models to outperform general large models, while\ntaking the performance variance into consideration. By observing the behaviour\nof fine-tuning, instruction-tuning, prompting and in-context learning on 8\nlanguage models, we identify such performance break-even points across 8\nrepresentative text classification tasks of varying characteristics. We show\nthat the specialised models often need only few samples (on average $100$) to\nbe on par or better than the general ones. At the same time, the number of\nrequired labels strongly depends on the dataset or task characteristics, with\nfine-tuning on binary datasets requiring significantly more samples. When\nperformance variance is taken into consideration, the number of required labels\nincreases on average by $100 - 200\\%$. Finally, larger models do not\nconsistently lead to better performance and lower variance, with 4-bit\nquantisation having negligible impact.\n", "link": "http://arxiv.org/abs/2402.12819v3", "date": "2025-05-19", "relevancy": 2.2883, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20Specialised%20Small%20and%20General%20Large%20Language%20Models%20on%20Text%0A%20%20Classification%3A%20100%20Labelled%20Samples%20to%20Achieve%20Break-Even%20Performance&body=Title%3A%20Comparing%20Specialised%20Small%20and%20General%20Large%20Language%20Models%20on%20Text%0A%20%20Classification%3A%20100%20Labelled%20Samples%20to%20Achieve%20Break-Even%20Performance%0AAuthor%3A%20Branislav%20Pecher%20and%20Ivan%20Srba%20and%20Maria%20Bielikova%0AAbstract%3A%20%20%20When%20solving%20NLP%20tasks%20with%20limited%20labelled%20data%2C%20researchers%20typically%0Aeither%20use%20a%20general%20large%20language%20model%20without%20further%20update%2C%20or%20use%20a%0Asmall%20number%20of%20labelled%20samples%20to%20tune%20a%20specialised%20smaller%20model.%20In%20this%0Awork%2C%20we%20answer%20an%20important%20question%20--%20how%20many%20labelled%20samples%20are%20required%0Afor%20the%20specialised%20small%20models%20to%20outperform%20general%20large%20models%2C%20while%0Ataking%20the%20performance%20variance%20into%20consideration.%20By%20observing%20the%20behaviour%0Aof%20fine-tuning%2C%20instruction-tuning%2C%20prompting%20and%20in-context%20learning%20on%208%0Alanguage%20models%2C%20we%20identify%20such%20performance%20break-even%20points%20across%208%0Arepresentative%20text%20classification%20tasks%20of%20varying%20characteristics.%20We%20show%0Athat%20the%20specialised%20models%20often%20need%20only%20few%20samples%20%28on%20average%20%24100%24%29%20to%0Abe%20on%20par%20or%20better%20than%20the%20general%20ones.%20At%20the%20same%20time%2C%20the%20number%20of%0Arequired%20labels%20strongly%20depends%20on%20the%20dataset%20or%20task%20characteristics%2C%20with%0Afine-tuning%20on%20binary%20datasets%20requiring%20significantly%20more%20samples.%20When%0Aperformance%20variance%20is%20taken%20into%20consideration%2C%20the%20number%20of%20required%20labels%0Aincreases%20on%20average%20by%20%24100%20-%20200%5C%25%24.%20Finally%2C%20larger%20models%20do%20not%0Aconsistently%20lead%20to%20better%20performance%20and%20lower%20variance%2C%20with%204-bit%0Aquantisation%20having%20negligible%20impact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12819v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520Specialised%2520Small%2520and%2520General%2520Large%2520Language%2520Models%2520on%2520Text%250A%2520%2520Classification%253A%2520100%2520Labelled%2520Samples%2520to%2520Achieve%2520Break-Even%2520Performance%26entry.906535625%3DBranislav%2520Pecher%2520and%2520Ivan%2520Srba%2520and%2520Maria%2520Bielikova%26entry.1292438233%3D%2520%2520When%2520solving%2520NLP%2520tasks%2520with%2520limited%2520labelled%2520data%252C%2520researchers%2520typically%250Aeither%2520use%2520a%2520general%2520large%2520language%2520model%2520without%2520further%2520update%252C%2520or%2520use%2520a%250Asmall%2520number%2520of%2520labelled%2520samples%2520to%2520tune%2520a%2520specialised%2520smaller%2520model.%2520In%2520this%250Awork%252C%2520we%2520answer%2520an%2520important%2520question%2520--%2520how%2520many%2520labelled%2520samples%2520are%2520required%250Afor%2520the%2520specialised%2520small%2520models%2520to%2520outperform%2520general%2520large%2520models%252C%2520while%250Ataking%2520the%2520performance%2520variance%2520into%2520consideration.%2520By%2520observing%2520the%2520behaviour%250Aof%2520fine-tuning%252C%2520instruction-tuning%252C%2520prompting%2520and%2520in-context%2520learning%2520on%25208%250Alanguage%2520models%252C%2520we%2520identify%2520such%2520performance%2520break-even%2520points%2520across%25208%250Arepresentative%2520text%2520classification%2520tasks%2520of%2520varying%2520characteristics.%2520We%2520show%250Athat%2520the%2520specialised%2520models%2520often%2520need%2520only%2520few%2520samples%2520%2528on%2520average%2520%2524100%2524%2529%2520to%250Abe%2520on%2520par%2520or%2520better%2520than%2520the%2520general%2520ones.%2520At%2520the%2520same%2520time%252C%2520the%2520number%2520of%250Arequired%2520labels%2520strongly%2520depends%2520on%2520the%2520dataset%2520or%2520task%2520characteristics%252C%2520with%250Afine-tuning%2520on%2520binary%2520datasets%2520requiring%2520significantly%2520more%2520samples.%2520When%250Aperformance%2520variance%2520is%2520taken%2520into%2520consideration%252C%2520the%2520number%2520of%2520required%2520labels%250Aincreases%2520on%2520average%2520by%2520%2524100%2520-%2520200%255C%2525%2524.%2520Finally%252C%2520larger%2520models%2520do%2520not%250Aconsistently%2520lead%2520to%2520better%2520performance%2520and%2520lower%2520variance%252C%2520with%25204-bit%250Aquantisation%2520having%2520negligible%2520impact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12819v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Specialised%20Small%20and%20General%20Large%20Language%20Models%20on%20Text%0A%20%20Classification%3A%20100%20Labelled%20Samples%20to%20Achieve%20Break-Even%20Performance&entry.906535625=Branislav%20Pecher%20and%20Ivan%20Srba%20and%20Maria%20Bielikova&entry.1292438233=%20%20When%20solving%20NLP%20tasks%20with%20limited%20labelled%20data%2C%20researchers%20typically%0Aeither%20use%20a%20general%20large%20language%20model%20without%20further%20update%2C%20or%20use%20a%0Asmall%20number%20of%20labelled%20samples%20to%20tune%20a%20specialised%20smaller%20model.%20In%20this%0Awork%2C%20we%20answer%20an%20important%20question%20--%20how%20many%20labelled%20samples%20are%20required%0Afor%20the%20specialised%20small%20models%20to%20outperform%20general%20large%20models%2C%20while%0Ataking%20the%20performance%20variance%20into%20consideration.%20By%20observing%20the%20behaviour%0Aof%20fine-tuning%2C%20instruction-tuning%2C%20prompting%20and%20in-context%20learning%20on%208%0Alanguage%20models%2C%20we%20identify%20such%20performance%20break-even%20points%20across%208%0Arepresentative%20text%20classification%20tasks%20of%20varying%20characteristics.%20We%20show%0Athat%20the%20specialised%20models%20often%20need%20only%20few%20samples%20%28on%20average%20%24100%24%29%20to%0Abe%20on%20par%20or%20better%20than%20the%20general%20ones.%20At%20the%20same%20time%2C%20the%20number%20of%0Arequired%20labels%20strongly%20depends%20on%20the%20dataset%20or%20task%20characteristics%2C%20with%0Afine-tuning%20on%20binary%20datasets%20requiring%20significantly%20more%20samples.%20When%0Aperformance%20variance%20is%20taken%20into%20consideration%2C%20the%20number%20of%20required%20labels%0Aincreases%20on%20average%20by%20%24100%20-%20200%5C%25%24.%20Finally%2C%20larger%20models%20do%20not%0Aconsistently%20lead%20to%20better%20performance%20and%20lower%20variance%2C%20with%204-bit%0Aquantisation%20having%20negligible%20impact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12819v3&entry.124074799=Read"},
{"title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with\n  RGPO", "author": "Yicheng Xiao and Lin Song and Yukang Chen and Yingmin Luo and Yuxin Chen and Yukang Gan and Wei Huang and Xiu Li and Xiaojuan Qi and Ying Shan", "abstract": "  Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\n\\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}.\n", "link": "http://arxiv.org/abs/2505.13031v1", "date": "2025-05-19", "relevancy": 2.287, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5738}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MindOmni%3A%20Unleashing%20Reasoning%20Generation%20in%20Vision%20Language%20Models%20with%0A%20%20RGPO&body=Title%3A%20MindOmni%3A%20Unleashing%20Reasoning%20Generation%20in%20Vision%20Language%20Models%20with%0A%20%20RGPO%0AAuthor%3A%20Yicheng%20Xiao%20and%20Lin%20Song%20and%20Yukang%20Chen%20and%20Yingmin%20Luo%20and%20Yuxin%20Chen%20and%20Yukang%20Gan%20and%20Wei%20Huang%20and%20Xiu%20Li%20and%20Xiaojuan%20Qi%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Recent%20text-to-image%20systems%20face%20limitations%20in%20handling%20multimodal%20inputs%0Aand%20complex%20reasoning%20tasks.%20We%20introduce%20MindOmni%2C%20a%20unified%20multimodal%20large%0Alanguage%20model%20that%20addresses%20these%20challenges%20by%20incorporating%20reasoning%0Ageneration%20through%20reinforcement%20learning.%20MindOmni%20leverages%20a%20three-phase%0Atraining%20strategy%3A%20i%29%20design%20of%20a%20unified%20vision%20language%20model%20with%20a%0Adecoder-only%20diffusion%20module%2C%20ii%29%20supervised%20fine-tuning%20with%20Chain-of-Thought%0A%28CoT%29%20instruction%20data%2C%20and%20iii%29%20our%20proposed%20Reasoning%20Generation%20Policy%0AOptimization%20%28RGPO%29%20algorithm%2C%20utilizing%20multimodal%20feedback%20to%20effectively%0Aguide%20policy%20updates.%20Experimental%20results%20demonstrate%20that%20MindOmni%0Aoutperforms%20existing%20models%2C%20achieving%20impressive%20performance%20on%20both%0Aunderstanding%20and%20generation%20benchmarks%2C%20meanwhile%20showcasing%20advanced%0Afine-grained%20reasoning%20generation%20capabilities%2C%20especially%20with%20mathematical%0Areasoning%20instruction.%20All%20codes%20will%20be%20made%20public%20at%0A%5Chref%7Bhttps%3A//github.com/EasonXiao-888/MindOmni%7D%7Bhttps%3A//github.com/EasonXiao-888/MindOmni%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMindOmni%253A%2520Unleashing%2520Reasoning%2520Generation%2520in%2520Vision%2520Language%2520Models%2520with%250A%2520%2520RGPO%26entry.906535625%3DYicheng%2520Xiao%2520and%2520Lin%2520Song%2520and%2520Yukang%2520Chen%2520and%2520Yingmin%2520Luo%2520and%2520Yuxin%2520Chen%2520and%2520Yukang%2520Gan%2520and%2520Wei%2520Huang%2520and%2520Xiu%2520Li%2520and%2520Xiaojuan%2520Qi%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520text-to-image%2520systems%2520face%2520limitations%2520in%2520handling%2520multimodal%2520inputs%250Aand%2520complex%2520reasoning%2520tasks.%2520We%2520introduce%2520MindOmni%252C%2520a%2520unified%2520multimodal%2520large%250Alanguage%2520model%2520that%2520addresses%2520these%2520challenges%2520by%2520incorporating%2520reasoning%250Ageneration%2520through%2520reinforcement%2520learning.%2520MindOmni%2520leverages%2520a%2520three-phase%250Atraining%2520strategy%253A%2520i%2529%2520design%2520of%2520a%2520unified%2520vision%2520language%2520model%2520with%2520a%250Adecoder-only%2520diffusion%2520module%252C%2520ii%2529%2520supervised%2520fine-tuning%2520with%2520Chain-of-Thought%250A%2528CoT%2529%2520instruction%2520data%252C%2520and%2520iii%2529%2520our%2520proposed%2520Reasoning%2520Generation%2520Policy%250AOptimization%2520%2528RGPO%2529%2520algorithm%252C%2520utilizing%2520multimodal%2520feedback%2520to%2520effectively%250Aguide%2520policy%2520updates.%2520Experimental%2520results%2520demonstrate%2520that%2520MindOmni%250Aoutperforms%2520existing%2520models%252C%2520achieving%2520impressive%2520performance%2520on%2520both%250Aunderstanding%2520and%2520generation%2520benchmarks%252C%2520meanwhile%2520showcasing%2520advanced%250Afine-grained%2520reasoning%2520generation%2520capabilities%252C%2520especially%2520with%2520mathematical%250Areasoning%2520instruction.%2520All%2520codes%2520will%2520be%2520made%2520public%2520at%250A%255Chref%257Bhttps%253A//github.com/EasonXiao-888/MindOmni%257D%257Bhttps%253A//github.com/EasonXiao-888/MindOmni%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MindOmni%3A%20Unleashing%20Reasoning%20Generation%20in%20Vision%20Language%20Models%20with%0A%20%20RGPO&entry.906535625=Yicheng%20Xiao%20and%20Lin%20Song%20and%20Yukang%20Chen%20and%20Yingmin%20Luo%20and%20Yuxin%20Chen%20and%20Yukang%20Gan%20and%20Wei%20Huang%20and%20Xiu%20Li%20and%20Xiaojuan%20Qi%20and%20Ying%20Shan&entry.1292438233=%20%20Recent%20text-to-image%20systems%20face%20limitations%20in%20handling%20multimodal%20inputs%0Aand%20complex%20reasoning%20tasks.%20We%20introduce%20MindOmni%2C%20a%20unified%20multimodal%20large%0Alanguage%20model%20that%20addresses%20these%20challenges%20by%20incorporating%20reasoning%0Ageneration%20through%20reinforcement%20learning.%20MindOmni%20leverages%20a%20three-phase%0Atraining%20strategy%3A%20i%29%20design%20of%20a%20unified%20vision%20language%20model%20with%20a%0Adecoder-only%20diffusion%20module%2C%20ii%29%20supervised%20fine-tuning%20with%20Chain-of-Thought%0A%28CoT%29%20instruction%20data%2C%20and%20iii%29%20our%20proposed%20Reasoning%20Generation%20Policy%0AOptimization%20%28RGPO%29%20algorithm%2C%20utilizing%20multimodal%20feedback%20to%20effectively%0Aguide%20policy%20updates.%20Experimental%20results%20demonstrate%20that%20MindOmni%0Aoutperforms%20existing%20models%2C%20achieving%20impressive%20performance%20on%20both%0Aunderstanding%20and%20generation%20benchmarks%2C%20meanwhile%20showcasing%20advanced%0Afine-grained%20reasoning%20generation%20capabilities%2C%20especially%20with%20mathematical%0Areasoning%20instruction.%20All%20codes%20will%20be%20made%20public%20at%0A%5Chref%7Bhttps%3A//github.com/EasonXiao-888/MindOmni%7D%7Bhttps%3A//github.com/EasonXiao-888/MindOmni%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13031v1&entry.124074799=Read"},
{"title": "Smoothed SGD for quantiles: Bahadur representation and Gaussian\n  approximation", "author": "Likai Chen and Georg Keilbar and Wei Biao Wu", "abstract": "  This paper considers the estimation of quantiles via a smoothed version of\nthe stochastic gradient descent (SGD) algorithm. By smoothing the score\nfunction in the conventional SGD quantile algorithm, we achieve monotonicity in\nthe quantile level in that the estimated quantile curves do not cross. We\nderive non-asymptotic tail probability bounds for the smoothed SGD quantile\nestimate both for the case with and without Polyak-Ruppert averaging. For the\nlatter, we also provide a uniform Bahadur representation and a resulting\nGaussian approximation result. Numerical studies show good finite sample\nbehavior for our theoretical results.\n", "link": "http://arxiv.org/abs/2505.13299v1", "date": "2025-05-19", "relevancy": 2.286, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4743}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4525}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smoothed%20SGD%20for%20quantiles%3A%20Bahadur%20representation%20and%20Gaussian%0A%20%20approximation&body=Title%3A%20Smoothed%20SGD%20for%20quantiles%3A%20Bahadur%20representation%20and%20Gaussian%0A%20%20approximation%0AAuthor%3A%20Likai%20Chen%20and%20Georg%20Keilbar%20and%20Wei%20Biao%20Wu%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20estimation%20of%20quantiles%20via%20a%20smoothed%20version%20of%0Athe%20stochastic%20gradient%20descent%20%28SGD%29%20algorithm.%20By%20smoothing%20the%20score%0Afunction%20in%20the%20conventional%20SGD%20quantile%20algorithm%2C%20we%20achieve%20monotonicity%20in%0Athe%20quantile%20level%20in%20that%20the%20estimated%20quantile%20curves%20do%20not%20cross.%20We%0Aderive%20non-asymptotic%20tail%20probability%20bounds%20for%20the%20smoothed%20SGD%20quantile%0Aestimate%20both%20for%20the%20case%20with%20and%20without%20Polyak-Ruppert%20averaging.%20For%20the%0Alatter%2C%20we%20also%20provide%20a%20uniform%20Bahadur%20representation%20and%20a%20resulting%0AGaussian%20approximation%20result.%20Numerical%20studies%20show%20good%20finite%20sample%0Abehavior%20for%20our%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmoothed%2520SGD%2520for%2520quantiles%253A%2520Bahadur%2520representation%2520and%2520Gaussian%250A%2520%2520approximation%26entry.906535625%3DLikai%2520Chen%2520and%2520Georg%2520Keilbar%2520and%2520Wei%2520Biao%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520estimation%2520of%2520quantiles%2520via%2520a%2520smoothed%2520version%2520of%250Athe%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%2520algorithm.%2520By%2520smoothing%2520the%2520score%250Afunction%2520in%2520the%2520conventional%2520SGD%2520quantile%2520algorithm%252C%2520we%2520achieve%2520monotonicity%2520in%250Athe%2520quantile%2520level%2520in%2520that%2520the%2520estimated%2520quantile%2520curves%2520do%2520not%2520cross.%2520We%250Aderive%2520non-asymptotic%2520tail%2520probability%2520bounds%2520for%2520the%2520smoothed%2520SGD%2520quantile%250Aestimate%2520both%2520for%2520the%2520case%2520with%2520and%2520without%2520Polyak-Ruppert%2520averaging.%2520For%2520the%250Alatter%252C%2520we%2520also%2520provide%2520a%2520uniform%2520Bahadur%2520representation%2520and%2520a%2520resulting%250AGaussian%2520approximation%2520result.%2520Numerical%2520studies%2520show%2520good%2520finite%2520sample%250Abehavior%2520for%2520our%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smoothed%20SGD%20for%20quantiles%3A%20Bahadur%20representation%20and%20Gaussian%0A%20%20approximation&entry.906535625=Likai%20Chen%20and%20Georg%20Keilbar%20and%20Wei%20Biao%20Wu&entry.1292438233=%20%20This%20paper%20considers%20the%20estimation%20of%20quantiles%20via%20a%20smoothed%20version%20of%0Athe%20stochastic%20gradient%20descent%20%28SGD%29%20algorithm.%20By%20smoothing%20the%20score%0Afunction%20in%20the%20conventional%20SGD%20quantile%20algorithm%2C%20we%20achieve%20monotonicity%20in%0Athe%20quantile%20level%20in%20that%20the%20estimated%20quantile%20curves%20do%20not%20cross.%20We%0Aderive%20non-asymptotic%20tail%20probability%20bounds%20for%20the%20smoothed%20SGD%20quantile%0Aestimate%20both%20for%20the%20case%20with%20and%20without%20Polyak-Ruppert%20averaging.%20For%20the%0Alatter%2C%20we%20also%20provide%20a%20uniform%20Bahadur%20representation%20and%20a%20resulting%0AGaussian%20approximation%20result.%20Numerical%20studies%20show%20good%20finite%20sample%0Abehavior%20for%20our%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13299v1&entry.124074799=Read"},
{"title": "EndoMetric: Near-Light Monocular Metric Scale Estimation in Endoscopy", "author": "Ra\u00fal Iranzo and V\u00edctor M. Batlle and Juan D. Tard\u00f3s and Jos\u00e9 M. M. Montiel", "abstract": "  Geometric reconstruction and SLAM with endoscopic images have advanced\nsignificantly in recent years. In most medical fields, monocular endoscopes are\nemployed, and the algorithms used are typically adaptations of those designed\nfor external environments, resulting in 3D reconstructions with an unknown\nscale factor.\n  For the first time, we propose a method to estimate the real metric scale of\na 3D reconstruction from standard monocular endoscopic images without relying\non application-specific learned priors. Our fully model-based approach\nleverages the near-light sources embedded in endoscopes, positioned at a small\nbut nonzero baseline from the camera, in combination with the inverse-square\nlaw of light attenuation, to accurately recover the metric scale from scratch.\nThis enables the transformation of any endoscope into a metric device, which is\ncrucial for applications such as measuring polyps, stenosis, or assessing the\nextent of diseased tissue.\n", "link": "http://arxiv.org/abs/2410.15065v2", "date": "2025-05-19", "relevancy": 2.2751, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5772}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5735}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EndoMetric%3A%20Near-Light%20Monocular%20Metric%20Scale%20Estimation%20in%20Endoscopy&body=Title%3A%20EndoMetric%3A%20Near-Light%20Monocular%20Metric%20Scale%20Estimation%20in%20Endoscopy%0AAuthor%3A%20Ra%C3%BAl%20Iranzo%20and%20V%C3%ADctor%20M.%20Batlle%20and%20Juan%20D.%20Tard%C3%B3s%20and%20Jos%C3%A9%20M.%20M.%20Montiel%0AAbstract%3A%20%20%20Geometric%20reconstruction%20and%20SLAM%20with%20endoscopic%20images%20have%20advanced%0Asignificantly%20in%20recent%20years.%20In%20most%20medical%20fields%2C%20monocular%20endoscopes%20are%0Aemployed%2C%20and%20the%20algorithms%20used%20are%20typically%20adaptations%20of%20those%20designed%0Afor%20external%20environments%2C%20resulting%20in%203D%20reconstructions%20with%20an%20unknown%0Ascale%20factor.%0A%20%20For%20the%20first%20time%2C%20we%20propose%20a%20method%20to%20estimate%20the%20real%20metric%20scale%20of%0Aa%203D%20reconstruction%20from%20standard%20monocular%20endoscopic%20images%20without%20relying%0Aon%20application-specific%20learned%20priors.%20Our%20fully%20model-based%20approach%0Aleverages%20the%20near-light%20sources%20embedded%20in%20endoscopes%2C%20positioned%20at%20a%20small%0Abut%20nonzero%20baseline%20from%20the%20camera%2C%20in%20combination%20with%20the%20inverse-square%0Alaw%20of%20light%20attenuation%2C%20to%20accurately%20recover%20the%20metric%20scale%20from%20scratch.%0AThis%20enables%20the%20transformation%20of%20any%20endoscope%20into%20a%20metric%20device%2C%20which%20is%0Acrucial%20for%20applications%20such%20as%20measuring%20polyps%2C%20stenosis%2C%20or%20assessing%20the%0Aextent%20of%20diseased%20tissue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15065v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndoMetric%253A%2520Near-Light%2520Monocular%2520Metric%2520Scale%2520Estimation%2520in%2520Endoscopy%26entry.906535625%3DRa%25C3%25BAl%2520Iranzo%2520and%2520V%25C3%25ADctor%2520M.%2520Batlle%2520and%2520Juan%2520D.%2520Tard%25C3%25B3s%2520and%2520Jos%25C3%25A9%2520M.%2520M.%2520Montiel%26entry.1292438233%3D%2520%2520Geometric%2520reconstruction%2520and%2520SLAM%2520with%2520endoscopic%2520images%2520have%2520advanced%250Asignificantly%2520in%2520recent%2520years.%2520In%2520most%2520medical%2520fields%252C%2520monocular%2520endoscopes%2520are%250Aemployed%252C%2520and%2520the%2520algorithms%2520used%2520are%2520typically%2520adaptations%2520of%2520those%2520designed%250Afor%2520external%2520environments%252C%2520resulting%2520in%25203D%2520reconstructions%2520with%2520an%2520unknown%250Ascale%2520factor.%250A%2520%2520For%2520the%2520first%2520time%252C%2520we%2520propose%2520a%2520method%2520to%2520estimate%2520the%2520real%2520metric%2520scale%2520of%250Aa%25203D%2520reconstruction%2520from%2520standard%2520monocular%2520endoscopic%2520images%2520without%2520relying%250Aon%2520application-specific%2520learned%2520priors.%2520Our%2520fully%2520model-based%2520approach%250Aleverages%2520the%2520near-light%2520sources%2520embedded%2520in%2520endoscopes%252C%2520positioned%2520at%2520a%2520small%250Abut%2520nonzero%2520baseline%2520from%2520the%2520camera%252C%2520in%2520combination%2520with%2520the%2520inverse-square%250Alaw%2520of%2520light%2520attenuation%252C%2520to%2520accurately%2520recover%2520the%2520metric%2520scale%2520from%2520scratch.%250AThis%2520enables%2520the%2520transformation%2520of%2520any%2520endoscope%2520into%2520a%2520metric%2520device%252C%2520which%2520is%250Acrucial%2520for%2520applications%2520such%2520as%2520measuring%2520polyps%252C%2520stenosis%252C%2520or%2520assessing%2520the%250Aextent%2520of%2520diseased%2520tissue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15065v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EndoMetric%3A%20Near-Light%20Monocular%20Metric%20Scale%20Estimation%20in%20Endoscopy&entry.906535625=Ra%C3%BAl%20Iranzo%20and%20V%C3%ADctor%20M.%20Batlle%20and%20Juan%20D.%20Tard%C3%B3s%20and%20Jos%C3%A9%20M.%20M.%20Montiel&entry.1292438233=%20%20Geometric%20reconstruction%20and%20SLAM%20with%20endoscopic%20images%20have%20advanced%0Asignificantly%20in%20recent%20years.%20In%20most%20medical%20fields%2C%20monocular%20endoscopes%20are%0Aemployed%2C%20and%20the%20algorithms%20used%20are%20typically%20adaptations%20of%20those%20designed%0Afor%20external%20environments%2C%20resulting%20in%203D%20reconstructions%20with%20an%20unknown%0Ascale%20factor.%0A%20%20For%20the%20first%20time%2C%20we%20propose%20a%20method%20to%20estimate%20the%20real%20metric%20scale%20of%0Aa%203D%20reconstruction%20from%20standard%20monocular%20endoscopic%20images%20without%20relying%0Aon%20application-specific%20learned%20priors.%20Our%20fully%20model-based%20approach%0Aleverages%20the%20near-light%20sources%20embedded%20in%20endoscopes%2C%20positioned%20at%20a%20small%0Abut%20nonzero%20baseline%20from%20the%20camera%2C%20in%20combination%20with%20the%20inverse-square%0Alaw%20of%20light%20attenuation%2C%20to%20accurately%20recover%20the%20metric%20scale%20from%20scratch.%0AThis%20enables%20the%20transformation%20of%20any%20endoscope%20into%20a%20metric%20device%2C%20which%20is%0Acrucial%20for%20applications%20such%20as%20measuring%20polyps%2C%20stenosis%2C%20or%20assessing%20the%0Aextent%20of%20diseased%20tissue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15065v2&entry.124074799=Read"},
{"title": "OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance\n  Reconstruction", "author": "Jie Yan and Xin Liu and Zhong-Yuan Zhang", "abstract": "  Federated clustering (FC) aims to discover global cluster structures across\ndecentralized clients without sharing raw data, making privacy preservation a\nfundamental requirement. There are two critical challenges: (1) privacy leakage\nduring collaboration, and (2) robustness degradation due to aggregation of\nproxy information from non-independent and identically distributed (Non-IID)\nlocal data, leading to inaccurate or inconsistent global clustering. Existing\nsolutions typically rely on model-specific local proxies, which are sensitive\nto data heterogeneity and inherit inductive biases from their centralized\ncounterparts, thus limiting robustness and generality. We propose Omni\nFederated Clustering (OmniFC), a unified and model-agnostic framework.\nLeveraging Lagrange coded computing, our method enables clients to share only\nencoded data, allowing exact reconstruction of the global distance matrix--a\nfundamental representation of sample relationships--without leaking private\ninformation, even under client collusion. This construction is naturally\nresilient to Non-IID data distributions. This approach decouples FC from\nmodel-specific proxies, providing a unified extension mechanism applicable to\ndiverse centralized clustering methods. Theoretical analysis confirms both\nreconstruction fidelity and privacy guarantees, while comprehensive experiments\ndemonstrate OmniFC's superior robustness, effectiveness, and generality across\nvarious benchmarks compared to state-of-the-art methods. Code will be released.\n", "link": "http://arxiv.org/abs/2505.13071v1", "date": "2025-05-19", "relevancy": 2.2751, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4821}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4536}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniFC%3A%20Rethinking%20Federated%20Clustering%20via%20Lossless%20and%20Secure%20Distance%0A%20%20Reconstruction&body=Title%3A%20OmniFC%3A%20Rethinking%20Federated%20Clustering%20via%20Lossless%20and%20Secure%20Distance%0A%20%20Reconstruction%0AAuthor%3A%20Jie%20Yan%20and%20Xin%20Liu%20and%20Zhong-Yuan%20Zhang%0AAbstract%3A%20%20%20Federated%20clustering%20%28FC%29%20aims%20to%20discover%20global%20cluster%20structures%20across%0Adecentralized%20clients%20without%20sharing%20raw%20data%2C%20making%20privacy%20preservation%20a%0Afundamental%20requirement.%20There%20are%20two%20critical%20challenges%3A%20%281%29%20privacy%20leakage%0Aduring%20collaboration%2C%20and%20%282%29%20robustness%20degradation%20due%20to%20aggregation%20of%0Aproxy%20information%20from%20non-independent%20and%20identically%20distributed%20%28Non-IID%29%0Alocal%20data%2C%20leading%20to%20inaccurate%20or%20inconsistent%20global%20clustering.%20Existing%0Asolutions%20typically%20rely%20on%20model-specific%20local%20proxies%2C%20which%20are%20sensitive%0Ato%20data%20heterogeneity%20and%20inherit%20inductive%20biases%20from%20their%20centralized%0Acounterparts%2C%20thus%20limiting%20robustness%20and%20generality.%20We%20propose%20Omni%0AFederated%20Clustering%20%28OmniFC%29%2C%20a%20unified%20and%20model-agnostic%20framework.%0ALeveraging%20Lagrange%20coded%20computing%2C%20our%20method%20enables%20clients%20to%20share%20only%0Aencoded%20data%2C%20allowing%20exact%20reconstruction%20of%20the%20global%20distance%20matrix--a%0Afundamental%20representation%20of%20sample%20relationships--without%20leaking%20private%0Ainformation%2C%20even%20under%20client%20collusion.%20This%20construction%20is%20naturally%0Aresilient%20to%20Non-IID%20data%20distributions.%20This%20approach%20decouples%20FC%20from%0Amodel-specific%20proxies%2C%20providing%20a%20unified%20extension%20mechanism%20applicable%20to%0Adiverse%20centralized%20clustering%20methods.%20Theoretical%20analysis%20confirms%20both%0Areconstruction%20fidelity%20and%20privacy%20guarantees%2C%20while%20comprehensive%20experiments%0Ademonstrate%20OmniFC%27s%20superior%20robustness%2C%20effectiveness%2C%20and%20generality%20across%0Avarious%20benchmarks%20compared%20to%20state-of-the-art%20methods.%20Code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniFC%253A%2520Rethinking%2520Federated%2520Clustering%2520via%2520Lossless%2520and%2520Secure%2520Distance%250A%2520%2520Reconstruction%26entry.906535625%3DJie%2520Yan%2520and%2520Xin%2520Liu%2520and%2520Zhong-Yuan%2520Zhang%26entry.1292438233%3D%2520%2520Federated%2520clustering%2520%2528FC%2529%2520aims%2520to%2520discover%2520global%2520cluster%2520structures%2520across%250Adecentralized%2520clients%2520without%2520sharing%2520raw%2520data%252C%2520making%2520privacy%2520preservation%2520a%250Afundamental%2520requirement.%2520There%2520are%2520two%2520critical%2520challenges%253A%2520%25281%2529%2520privacy%2520leakage%250Aduring%2520collaboration%252C%2520and%2520%25282%2529%2520robustness%2520degradation%2520due%2520to%2520aggregation%2520of%250Aproxy%2520information%2520from%2520non-independent%2520and%2520identically%2520distributed%2520%2528Non-IID%2529%250Alocal%2520data%252C%2520leading%2520to%2520inaccurate%2520or%2520inconsistent%2520global%2520clustering.%2520Existing%250Asolutions%2520typically%2520rely%2520on%2520model-specific%2520local%2520proxies%252C%2520which%2520are%2520sensitive%250Ato%2520data%2520heterogeneity%2520and%2520inherit%2520inductive%2520biases%2520from%2520their%2520centralized%250Acounterparts%252C%2520thus%2520limiting%2520robustness%2520and%2520generality.%2520We%2520propose%2520Omni%250AFederated%2520Clustering%2520%2528OmniFC%2529%252C%2520a%2520unified%2520and%2520model-agnostic%2520framework.%250ALeveraging%2520Lagrange%2520coded%2520computing%252C%2520our%2520method%2520enables%2520clients%2520to%2520share%2520only%250Aencoded%2520data%252C%2520allowing%2520exact%2520reconstruction%2520of%2520the%2520global%2520distance%2520matrix--a%250Afundamental%2520representation%2520of%2520sample%2520relationships--without%2520leaking%2520private%250Ainformation%252C%2520even%2520under%2520client%2520collusion.%2520This%2520construction%2520is%2520naturally%250Aresilient%2520to%2520Non-IID%2520data%2520distributions.%2520This%2520approach%2520decouples%2520FC%2520from%250Amodel-specific%2520proxies%252C%2520providing%2520a%2520unified%2520extension%2520mechanism%2520applicable%2520to%250Adiverse%2520centralized%2520clustering%2520methods.%2520Theoretical%2520analysis%2520confirms%2520both%250Areconstruction%2520fidelity%2520and%2520privacy%2520guarantees%252C%2520while%2520comprehensive%2520experiments%250Ademonstrate%2520OmniFC%2527s%2520superior%2520robustness%252C%2520effectiveness%252C%2520and%2520generality%2520across%250Avarious%2520benchmarks%2520compared%2520to%2520state-of-the-art%2520methods.%2520Code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniFC%3A%20Rethinking%20Federated%20Clustering%20via%20Lossless%20and%20Secure%20Distance%0A%20%20Reconstruction&entry.906535625=Jie%20Yan%20and%20Xin%20Liu%20and%20Zhong-Yuan%20Zhang&entry.1292438233=%20%20Federated%20clustering%20%28FC%29%20aims%20to%20discover%20global%20cluster%20structures%20across%0Adecentralized%20clients%20without%20sharing%20raw%20data%2C%20making%20privacy%20preservation%20a%0Afundamental%20requirement.%20There%20are%20two%20critical%20challenges%3A%20%281%29%20privacy%20leakage%0Aduring%20collaboration%2C%20and%20%282%29%20robustness%20degradation%20due%20to%20aggregation%20of%0Aproxy%20information%20from%20non-independent%20and%20identically%20distributed%20%28Non-IID%29%0Alocal%20data%2C%20leading%20to%20inaccurate%20or%20inconsistent%20global%20clustering.%20Existing%0Asolutions%20typically%20rely%20on%20model-specific%20local%20proxies%2C%20which%20are%20sensitive%0Ato%20data%20heterogeneity%20and%20inherit%20inductive%20biases%20from%20their%20centralized%0Acounterparts%2C%20thus%20limiting%20robustness%20and%20generality.%20We%20propose%20Omni%0AFederated%20Clustering%20%28OmniFC%29%2C%20a%20unified%20and%20model-agnostic%20framework.%0ALeveraging%20Lagrange%20coded%20computing%2C%20our%20method%20enables%20clients%20to%20share%20only%0Aencoded%20data%2C%20allowing%20exact%20reconstruction%20of%20the%20global%20distance%20matrix--a%0Afundamental%20representation%20of%20sample%20relationships--without%20leaking%20private%0Ainformation%2C%20even%20under%20client%20collusion.%20This%20construction%20is%20naturally%0Aresilient%20to%20Non-IID%20data%20distributions.%20This%20approach%20decouples%20FC%20from%0Amodel-specific%20proxies%2C%20providing%20a%20unified%20extension%20mechanism%20applicable%20to%0Adiverse%20centralized%20clustering%20methods.%20Theoretical%20analysis%20confirms%20both%0Areconstruction%20fidelity%20and%20privacy%20guarantees%2C%20while%20comprehensive%20experiments%0Ademonstrate%20OmniFC%27s%20superior%20robustness%2C%20effectiveness%2C%20and%20generality%20across%0Avarious%20benchmarks%20compared%20to%20state-of-the-art%20methods.%20Code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13071v1&entry.124074799=Read"},
{"title": "FlightBench: Benchmarking Learning-based Methods for Ego-vision-based\n  Quadrotors Navigation", "author": "Shu-Ang Yu and Chao Yu and Feng Gao and Yi Wu and Yu Wang", "abstract": "  Ego-vision-based navigation in cluttered environments is crucial for mobile\nsystems, particularly agile quadrotors. While learning-based methods have shown\npromise recently, head-to-head comparisons with cutting-edge optimization-based\napproaches are scarce, leaving open the question of where and to what extent\nthey truly excel. In this paper, we introduce FlightBench, the first\ncomprehensive benchmark that implements various learning-based methods for\nego-vision-based navigation and evaluates them against mainstream\noptimization-based baselines using a broad set of performance metrics. More\nimportantly, we develop a suite of criteria to assess scenario difficulty and\ndesign test cases that span different levels of difficulty based on these\ncriteria. Our results show that while learning-based methods excel in\nhigh-speed flight and faster inference, they struggle with challenging\nscenarios like sharp corners or view occlusion. Analytical experiments validate\nthe correlation between our difficulty criteria and flight performance.\nMoreover, we verify the trend in flight performance within real-world\nenvironments through full-pipeline and hardware-in-the-loop experiments. We\nhope this benchmark and these criteria will drive future advancements in\nlearning-based navigation for ego-vision quadrotors. Code and documentation are\navailable at https://github.com/thu-uav/FlightBench.\n", "link": "http://arxiv.org/abs/2406.05687v3", "date": "2025-05-19", "relevancy": 2.2749, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6072}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5449}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlightBench%3A%20Benchmarking%20Learning-based%20Methods%20for%20Ego-vision-based%0A%20%20Quadrotors%20Navigation&body=Title%3A%20FlightBench%3A%20Benchmarking%20Learning-based%20Methods%20for%20Ego-vision-based%0A%20%20Quadrotors%20Navigation%0AAuthor%3A%20Shu-Ang%20Yu%20and%20Chao%20Yu%20and%20Feng%20Gao%20and%20Yi%20Wu%20and%20Yu%20Wang%0AAbstract%3A%20%20%20Ego-vision-based%20navigation%20in%20cluttered%20environments%20is%20crucial%20for%20mobile%0Asystems%2C%20particularly%20agile%20quadrotors.%20While%20learning-based%20methods%20have%20shown%0Apromise%20recently%2C%20head-to-head%20comparisons%20with%20cutting-edge%20optimization-based%0Aapproaches%20are%20scarce%2C%20leaving%20open%20the%20question%20of%20where%20and%20to%20what%20extent%0Athey%20truly%20excel.%20In%20this%20paper%2C%20we%20introduce%20FlightBench%2C%20the%20first%0Acomprehensive%20benchmark%20that%20implements%20various%20learning-based%20methods%20for%0Aego-vision-based%20navigation%20and%20evaluates%20them%20against%20mainstream%0Aoptimization-based%20baselines%20using%20a%20broad%20set%20of%20performance%20metrics.%20More%0Aimportantly%2C%20we%20develop%20a%20suite%20of%20criteria%20to%20assess%20scenario%20difficulty%20and%0Adesign%20test%20cases%20that%20span%20different%20levels%20of%20difficulty%20based%20on%20these%0Acriteria.%20Our%20results%20show%20that%20while%20learning-based%20methods%20excel%20in%0Ahigh-speed%20flight%20and%20faster%20inference%2C%20they%20struggle%20with%20challenging%0Ascenarios%20like%20sharp%20corners%20or%20view%20occlusion.%20Analytical%20experiments%20validate%0Athe%20correlation%20between%20our%20difficulty%20criteria%20and%20flight%20performance.%0AMoreover%2C%20we%20verify%20the%20trend%20in%20flight%20performance%20within%20real-world%0Aenvironments%20through%20full-pipeline%20and%20hardware-in-the-loop%20experiments.%20We%0Ahope%20this%20benchmark%20and%20these%20criteria%20will%20drive%20future%20advancements%20in%0Alearning-based%20navigation%20for%20ego-vision%20quadrotors.%20Code%20and%20documentation%20are%0Aavailable%20at%20https%3A//github.com/thu-uav/FlightBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05687v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlightBench%253A%2520Benchmarking%2520Learning-based%2520Methods%2520for%2520Ego-vision-based%250A%2520%2520Quadrotors%2520Navigation%26entry.906535625%3DShu-Ang%2520Yu%2520and%2520Chao%2520Yu%2520and%2520Feng%2520Gao%2520and%2520Yi%2520Wu%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520Ego-vision-based%2520navigation%2520in%2520cluttered%2520environments%2520is%2520crucial%2520for%2520mobile%250Asystems%252C%2520particularly%2520agile%2520quadrotors.%2520While%2520learning-based%2520methods%2520have%2520shown%250Apromise%2520recently%252C%2520head-to-head%2520comparisons%2520with%2520cutting-edge%2520optimization-based%250Aapproaches%2520are%2520scarce%252C%2520leaving%2520open%2520the%2520question%2520of%2520where%2520and%2520to%2520what%2520extent%250Athey%2520truly%2520excel.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520FlightBench%252C%2520the%2520first%250Acomprehensive%2520benchmark%2520that%2520implements%2520various%2520learning-based%2520methods%2520for%250Aego-vision-based%2520navigation%2520and%2520evaluates%2520them%2520against%2520mainstream%250Aoptimization-based%2520baselines%2520using%2520a%2520broad%2520set%2520of%2520performance%2520metrics.%2520More%250Aimportantly%252C%2520we%2520develop%2520a%2520suite%2520of%2520criteria%2520to%2520assess%2520scenario%2520difficulty%2520and%250Adesign%2520test%2520cases%2520that%2520span%2520different%2520levels%2520of%2520difficulty%2520based%2520on%2520these%250Acriteria.%2520Our%2520results%2520show%2520that%2520while%2520learning-based%2520methods%2520excel%2520in%250Ahigh-speed%2520flight%2520and%2520faster%2520inference%252C%2520they%2520struggle%2520with%2520challenging%250Ascenarios%2520like%2520sharp%2520corners%2520or%2520view%2520occlusion.%2520Analytical%2520experiments%2520validate%250Athe%2520correlation%2520between%2520our%2520difficulty%2520criteria%2520and%2520flight%2520performance.%250AMoreover%252C%2520we%2520verify%2520the%2520trend%2520in%2520flight%2520performance%2520within%2520real-world%250Aenvironments%2520through%2520full-pipeline%2520and%2520hardware-in-the-loop%2520experiments.%2520We%250Ahope%2520this%2520benchmark%2520and%2520these%2520criteria%2520will%2520drive%2520future%2520advancements%2520in%250Alearning-based%2520navigation%2520for%2520ego-vision%2520quadrotors.%2520Code%2520and%2520documentation%2520are%250Aavailable%2520at%2520https%253A//github.com/thu-uav/FlightBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05687v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlightBench%3A%20Benchmarking%20Learning-based%20Methods%20for%20Ego-vision-based%0A%20%20Quadrotors%20Navigation&entry.906535625=Shu-Ang%20Yu%20and%20Chao%20Yu%20and%20Feng%20Gao%20and%20Yi%20Wu%20and%20Yu%20Wang&entry.1292438233=%20%20Ego-vision-based%20navigation%20in%20cluttered%20environments%20is%20crucial%20for%20mobile%0Asystems%2C%20particularly%20agile%20quadrotors.%20While%20learning-based%20methods%20have%20shown%0Apromise%20recently%2C%20head-to-head%20comparisons%20with%20cutting-edge%20optimization-based%0Aapproaches%20are%20scarce%2C%20leaving%20open%20the%20question%20of%20where%20and%20to%20what%20extent%0Athey%20truly%20excel.%20In%20this%20paper%2C%20we%20introduce%20FlightBench%2C%20the%20first%0Acomprehensive%20benchmark%20that%20implements%20various%20learning-based%20methods%20for%0Aego-vision-based%20navigation%20and%20evaluates%20them%20against%20mainstream%0Aoptimization-based%20baselines%20using%20a%20broad%20set%20of%20performance%20metrics.%20More%0Aimportantly%2C%20we%20develop%20a%20suite%20of%20criteria%20to%20assess%20scenario%20difficulty%20and%0Adesign%20test%20cases%20that%20span%20different%20levels%20of%20difficulty%20based%20on%20these%0Acriteria.%20Our%20results%20show%20that%20while%20learning-based%20methods%20excel%20in%0Ahigh-speed%20flight%20and%20faster%20inference%2C%20they%20struggle%20with%20challenging%0Ascenarios%20like%20sharp%20corners%20or%20view%20occlusion.%20Analytical%20experiments%20validate%0Athe%20correlation%20between%20our%20difficulty%20criteria%20and%20flight%20performance.%0AMoreover%2C%20we%20verify%20the%20trend%20in%20flight%20performance%20within%20real-world%0Aenvironments%20through%20full-pipeline%20and%20hardware-in-the-loop%20experiments.%20We%0Ahope%20this%20benchmark%20and%20these%20criteria%20will%20drive%20future%20advancements%20in%0Alearning-based%20navigation%20for%20ego-vision%20quadrotors.%20Code%20and%20documentation%20are%0Aavailable%20at%20https%3A//github.com/thu-uav/FlightBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05687v3&entry.124074799=Read"},
{"title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning\n  Tasks", "author": "Miko\u0142aj Ma\u0142ki\u0144ski and Jacek Ma\u0144dziuk", "abstract": "  The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods.\n", "link": "http://arxiv.org/abs/2505.13391v1", "date": "2025-05-19", "relevancy": 2.2643, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Generalization%20Across%20a%20Variety%20of%20Abstract%20Visual%20Reasoning%0A%20%20Tasks&body=Title%3A%20Advancing%20Generalization%20Across%20a%20Variety%20of%20Abstract%20Visual%20Reasoning%0A%20%20Tasks%0AAuthor%3A%20Miko%C5%82aj%20Ma%C5%82ki%C5%84ski%20and%20Jacek%20Ma%C5%84dziuk%0AAbstract%3A%20%20%20The%20abstract%20visual%20reasoning%20%28AVR%29%20domain%20presents%20a%20diverse%20suite%20of%0Aanalogy-based%20tasks%20devoted%20to%20studying%20model%20generalization.%20Recent%20years%20have%0Abrought%20dynamic%20progress%20in%20the%20field%2C%20particularly%20in%20i.i.d.%20scenarios%2C%20in%0Awhich%20models%20are%20trained%20and%20evaluated%20on%20the%20same%20data%20distributions.%0ANevertheless%2C%20o.o.d.%20setups%20that%20assess%20model%20generalization%20to%20new%20test%0Adistributions%20remain%20challenging%20even%20for%20the%20most%20recent%20models.%20To%20advance%0Ageneralization%20in%20AVR%20tasks%2C%20we%20present%20the%20Pathways%20of%20Normalized%20Group%0AConvolution%20model%20%28PoNG%29%2C%20a%20novel%20neural%20architecture%20that%20features%20group%0Aconvolution%2C%20normalization%2C%20and%20a%20parallel%20design.%20We%20consider%20a%20wide%20set%20of%0AAVR%20benchmarks%2C%20including%20Raven%27s%20Progressive%20Matrices%20and%20visual%20analogy%0Aproblems%20with%20both%20synthetic%20and%20real-world%20images.%20The%20experiments%20demonstrate%0Astrong%20generalization%20capabilities%20of%20the%20proposed%20model%2C%20which%20in%20several%0Asettings%20outperforms%20the%20existing%20literature%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Generalization%2520Across%2520a%2520Variety%2520of%2520Abstract%2520Visual%2520Reasoning%250A%2520%2520Tasks%26entry.906535625%3DMiko%25C5%2582aj%2520Ma%25C5%2582ki%25C5%2584ski%2520and%2520Jacek%2520Ma%25C5%2584dziuk%26entry.1292438233%3D%2520%2520The%2520abstract%2520visual%2520reasoning%2520%2528AVR%2529%2520domain%2520presents%2520a%2520diverse%2520suite%2520of%250Aanalogy-based%2520tasks%2520devoted%2520to%2520studying%2520model%2520generalization.%2520Recent%2520years%2520have%250Abrought%2520dynamic%2520progress%2520in%2520the%2520field%252C%2520particularly%2520in%2520i.i.d.%2520scenarios%252C%2520in%250Awhich%2520models%2520are%2520trained%2520and%2520evaluated%2520on%2520the%2520same%2520data%2520distributions.%250ANevertheless%252C%2520o.o.d.%2520setups%2520that%2520assess%2520model%2520generalization%2520to%2520new%2520test%250Adistributions%2520remain%2520challenging%2520even%2520for%2520the%2520most%2520recent%2520models.%2520To%2520advance%250Ageneralization%2520in%2520AVR%2520tasks%252C%2520we%2520present%2520the%2520Pathways%2520of%2520Normalized%2520Group%250AConvolution%2520model%2520%2528PoNG%2529%252C%2520a%2520novel%2520neural%2520architecture%2520that%2520features%2520group%250Aconvolution%252C%2520normalization%252C%2520and%2520a%2520parallel%2520design.%2520We%2520consider%2520a%2520wide%2520set%2520of%250AAVR%2520benchmarks%252C%2520including%2520Raven%2527s%2520Progressive%2520Matrices%2520and%2520visual%2520analogy%250Aproblems%2520with%2520both%2520synthetic%2520and%2520real-world%2520images.%2520The%2520experiments%2520demonstrate%250Astrong%2520generalization%2520capabilities%2520of%2520the%2520proposed%2520model%252C%2520which%2520in%2520several%250Asettings%2520outperforms%2520the%2520existing%2520literature%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Generalization%20Across%20a%20Variety%20of%20Abstract%20Visual%20Reasoning%0A%20%20Tasks&entry.906535625=Miko%C5%82aj%20Ma%C5%82ki%C5%84ski%20and%20Jacek%20Ma%C5%84dziuk&entry.1292438233=%20%20The%20abstract%20visual%20reasoning%20%28AVR%29%20domain%20presents%20a%20diverse%20suite%20of%0Aanalogy-based%20tasks%20devoted%20to%20studying%20model%20generalization.%20Recent%20years%20have%0Abrought%20dynamic%20progress%20in%20the%20field%2C%20particularly%20in%20i.i.d.%20scenarios%2C%20in%0Awhich%20models%20are%20trained%20and%20evaluated%20on%20the%20same%20data%20distributions.%0ANevertheless%2C%20o.o.d.%20setups%20that%20assess%20model%20generalization%20to%20new%20test%0Adistributions%20remain%20challenging%20even%20for%20the%20most%20recent%20models.%20To%20advance%0Ageneralization%20in%20AVR%20tasks%2C%20we%20present%20the%20Pathways%20of%20Normalized%20Group%0AConvolution%20model%20%28PoNG%29%2C%20a%20novel%20neural%20architecture%20that%20features%20group%0Aconvolution%2C%20normalization%2C%20and%20a%20parallel%20design.%20We%20consider%20a%20wide%20set%20of%0AAVR%20benchmarks%2C%20including%20Raven%27s%20Progressive%20Matrices%20and%20visual%20analogy%0Aproblems%20with%20both%20synthetic%20and%20real-world%20images.%20The%20experiments%20demonstrate%0Astrong%20generalization%20capabilities%20of%20the%20proposed%20model%2C%20which%20in%20several%0Asettings%20outperforms%20the%20existing%20literature%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13391v1&entry.124074799=Read"},
{"title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at\n  Low Bit-Rates", "author": "Gabriele Spadaro and Alberto Presta and Jhony H. Giraldo and Marco Grangetto and Wei Hu and Giuseppe Valenzise and Attilio Fiandrotti and Enzo Tartaglione", "abstract": "  Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC.\n", "link": "http://arxiv.org/abs/2505.13316v1", "date": "2025-05-19", "relevancy": 2.2429, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6008}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5549}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20Diffusion%20Probabilistic%20Model%20for%20Point%20Cloud%20Compression%20at%0A%20%20Low%20Bit-Rates&body=Title%3A%20Denoising%20Diffusion%20Probabilistic%20Model%20for%20Point%20Cloud%20Compression%20at%0A%20%20Low%20Bit-Rates%0AAuthor%3A%20Gabriele%20Spadaro%20and%20Alberto%20Presta%20and%20Jhony%20H.%20Giraldo%20and%20Marco%20Grangetto%20and%20Wei%20Hu%20and%20Giuseppe%20Valenzise%20and%20Attilio%20Fiandrotti%20and%20Enzo%20Tartaglione%0AAbstract%3A%20%20%20Efficient%20compression%20of%20low-bit-rate%20point%20clouds%20is%20critical%20for%0Abandwidth-constrained%20applications.%20However%2C%20existing%20techniques%20mainly%20focus%0Aon%20high-fidelity%20reconstruction%2C%20requiring%20many%20bits%20for%20compression.%20This%0Apaper%20proposes%20a%20%22Denoising%20Diffusion%20Probabilistic%20Model%22%20%28DDPM%29%20architecture%0Afor%20point%20cloud%20compression%20%28DDPM-PCC%29%20at%20low%20bit-rates.%20A%20PointNet%20encoder%0Aproduces%20the%20condition%20vector%20for%20the%20generation%2C%20which%20is%20then%20quantized%20via%20a%0Alearnable%20vector%20quantizer.%20This%20configuration%20allows%20to%20achieve%20a%20low%20bitrates%0Awhile%20preserving%20quality.%20Experiments%20on%20ShapeNet%20and%20ModelNet40%20show%20improved%0Arate-distortion%20at%20low%20rates%20compared%20to%20standardized%20and%20state-of-the-art%0Aapproaches.%20We%20publicly%20released%20the%20code%20at%0Ahttps%3A//github.com/EIDOSLAB/DDPM-PCC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520Diffusion%2520Probabilistic%2520Model%2520for%2520Point%2520Cloud%2520Compression%2520at%250A%2520%2520Low%2520Bit-Rates%26entry.906535625%3DGabriele%2520Spadaro%2520and%2520Alberto%2520Presta%2520and%2520Jhony%2520H.%2520Giraldo%2520and%2520Marco%2520Grangetto%2520and%2520Wei%2520Hu%2520and%2520Giuseppe%2520Valenzise%2520and%2520Attilio%2520Fiandrotti%2520and%2520Enzo%2520Tartaglione%26entry.1292438233%3D%2520%2520Efficient%2520compression%2520of%2520low-bit-rate%2520point%2520clouds%2520is%2520critical%2520for%250Abandwidth-constrained%2520applications.%2520However%252C%2520existing%2520techniques%2520mainly%2520focus%250Aon%2520high-fidelity%2520reconstruction%252C%2520requiring%2520many%2520bits%2520for%2520compression.%2520This%250Apaper%2520proposes%2520a%2520%2522Denoising%2520Diffusion%2520Probabilistic%2520Model%2522%2520%2528DDPM%2529%2520architecture%250Afor%2520point%2520cloud%2520compression%2520%2528DDPM-PCC%2529%2520at%2520low%2520bit-rates.%2520A%2520PointNet%2520encoder%250Aproduces%2520the%2520condition%2520vector%2520for%2520the%2520generation%252C%2520which%2520is%2520then%2520quantized%2520via%2520a%250Alearnable%2520vector%2520quantizer.%2520This%2520configuration%2520allows%2520to%2520achieve%2520a%2520low%2520bitrates%250Awhile%2520preserving%2520quality.%2520Experiments%2520on%2520ShapeNet%2520and%2520ModelNet40%2520show%2520improved%250Arate-distortion%2520at%2520low%2520rates%2520compared%2520to%2520standardized%2520and%2520state-of-the-art%250Aapproaches.%2520We%2520publicly%2520released%2520the%2520code%2520at%250Ahttps%253A//github.com/EIDOSLAB/DDPM-PCC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20Diffusion%20Probabilistic%20Model%20for%20Point%20Cloud%20Compression%20at%0A%20%20Low%20Bit-Rates&entry.906535625=Gabriele%20Spadaro%20and%20Alberto%20Presta%20and%20Jhony%20H.%20Giraldo%20and%20Marco%20Grangetto%20and%20Wei%20Hu%20and%20Giuseppe%20Valenzise%20and%20Attilio%20Fiandrotti%20and%20Enzo%20Tartaglione&entry.1292438233=%20%20Efficient%20compression%20of%20low-bit-rate%20point%20clouds%20is%20critical%20for%0Abandwidth-constrained%20applications.%20However%2C%20existing%20techniques%20mainly%20focus%0Aon%20high-fidelity%20reconstruction%2C%20requiring%20many%20bits%20for%20compression.%20This%0Apaper%20proposes%20a%20%22Denoising%20Diffusion%20Probabilistic%20Model%22%20%28DDPM%29%20architecture%0Afor%20point%20cloud%20compression%20%28DDPM-PCC%29%20at%20low%20bit-rates.%20A%20PointNet%20encoder%0Aproduces%20the%20condition%20vector%20for%20the%20generation%2C%20which%20is%20then%20quantized%20via%20a%0Alearnable%20vector%20quantizer.%20This%20configuration%20allows%20to%20achieve%20a%20low%20bitrates%0Awhile%20preserving%20quality.%20Experiments%20on%20ShapeNet%20and%20ModelNet40%20show%20improved%0Arate-distortion%20at%20low%20rates%20compared%20to%20standardized%20and%20state-of-the-art%0Aapproaches.%20We%20publicly%20released%20the%20code%20at%0Ahttps%3A//github.com/EIDOSLAB/DDPM-PCC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13316v1&entry.124074799=Read"},
{"title": "KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical\n  Models Enables Precise Replication of Able-Bodied and Impaired Movement from\n  Markerless Motion Capture", "author": "R. James Cotton", "abstract": "  Broader access to high-quality movement analysis could greatly benefit\nmovement science and rehabilitation, such as allowing more detailed\ncharacterization of movement impairments and responses to interventions, or\neven enabling early detection of new neurological conditions or fall risk.\nWhile emerging technologies are making it easier to capture kinematics with\nbiomechanical models, or how joint angles change over time, inferring the\nunderlying physics that give rise to these movements, including ground reaction\nforces, joint torques, or even muscle activations, is still challenging. Here\nwe explore whether imitation learning applied to a biomechanical model from a\nlarge dataset of movements from able-bodied and impaired individuals can learn\nto compute these inverse dynamics. Although imitation learning in human pose\nestimation has seen great interest in recent years, our work differences in\nseveral ways: we focus on using an accurate biomechanical model instead of\nmodels adopted for computer vision, we test it on a dataset that contains\nparticipants with impaired movements, we reported detailed tracking metrics\nrelevant for the clinical measurement of movement including joint angles and\nground contact events, and finally we apply imitation learning to a\nmuscle-driven neuromusculoskeletal model. We show that our imitation learning\npolicy, KinTwin, can accurately replicate the kinematics of a wide range of\nmovements, including those with assistive devices or therapist assistance, and\nthat it can infer clinically meaningful differences in joint torques and muscle\nactivations. Our work demonstrates the potential for using imitation learning\nto enable high-quality movement analysis in clinical practice.\n", "link": "http://arxiv.org/abs/2505.13436v1", "date": "2025-05-19", "relevancy": 2.2374, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5902}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5756}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KinTwin%3A%20Imitation%20Learning%20with%20Torque%20and%20Muscle%20Driven%20Biomechanical%0A%20%20Models%20Enables%20Precise%20Replication%20of%20Able-Bodied%20and%20Impaired%20Movement%20from%0A%20%20Markerless%20Motion%20Capture&body=Title%3A%20KinTwin%3A%20Imitation%20Learning%20with%20Torque%20and%20Muscle%20Driven%20Biomechanical%0A%20%20Models%20Enables%20Precise%20Replication%20of%20Able-Bodied%20and%20Impaired%20Movement%20from%0A%20%20Markerless%20Motion%20Capture%0AAuthor%3A%20R.%20James%20Cotton%0AAbstract%3A%20%20%20Broader%20access%20to%20high-quality%20movement%20analysis%20could%20greatly%20benefit%0Amovement%20science%20and%20rehabilitation%2C%20such%20as%20allowing%20more%20detailed%0Acharacterization%20of%20movement%20impairments%20and%20responses%20to%20interventions%2C%20or%0Aeven%20enabling%20early%20detection%20of%20new%20neurological%20conditions%20or%20fall%20risk.%0AWhile%20emerging%20technologies%20are%20making%20it%20easier%20to%20capture%20kinematics%20with%0Abiomechanical%20models%2C%20or%20how%20joint%20angles%20change%20over%20time%2C%20inferring%20the%0Aunderlying%20physics%20that%20give%20rise%20to%20these%20movements%2C%20including%20ground%20reaction%0Aforces%2C%20joint%20torques%2C%20or%20even%20muscle%20activations%2C%20is%20still%20challenging.%20Here%0Awe%20explore%20whether%20imitation%20learning%20applied%20to%20a%20biomechanical%20model%20from%20a%0Alarge%20dataset%20of%20movements%20from%20able-bodied%20and%20impaired%20individuals%20can%20learn%0Ato%20compute%20these%20inverse%20dynamics.%20Although%20imitation%20learning%20in%20human%20pose%0Aestimation%20has%20seen%20great%20interest%20in%20recent%20years%2C%20our%20work%20differences%20in%0Aseveral%20ways%3A%20we%20focus%20on%20using%20an%20accurate%20biomechanical%20model%20instead%20of%0Amodels%20adopted%20for%20computer%20vision%2C%20we%20test%20it%20on%20a%20dataset%20that%20contains%0Aparticipants%20with%20impaired%20movements%2C%20we%20reported%20detailed%20tracking%20metrics%0Arelevant%20for%20the%20clinical%20measurement%20of%20movement%20including%20joint%20angles%20and%0Aground%20contact%20events%2C%20and%20finally%20we%20apply%20imitation%20learning%20to%20a%0Amuscle-driven%20neuromusculoskeletal%20model.%20We%20show%20that%20our%20imitation%20learning%0Apolicy%2C%20KinTwin%2C%20can%20accurately%20replicate%20the%20kinematics%20of%20a%20wide%20range%20of%0Amovements%2C%20including%20those%20with%20assistive%20devices%20or%20therapist%20assistance%2C%20and%0Athat%20it%20can%20infer%20clinically%20meaningful%20differences%20in%20joint%20torques%20and%20muscle%0Aactivations.%20Our%20work%20demonstrates%20the%20potential%20for%20using%20imitation%20learning%0Ato%20enable%20high-quality%20movement%20analysis%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinTwin%253A%2520Imitation%2520Learning%2520with%2520Torque%2520and%2520Muscle%2520Driven%2520Biomechanical%250A%2520%2520Models%2520Enables%2520Precise%2520Replication%2520of%2520Able-Bodied%2520and%2520Impaired%2520Movement%2520from%250A%2520%2520Markerless%2520Motion%2520Capture%26entry.906535625%3DR.%2520James%2520Cotton%26entry.1292438233%3D%2520%2520Broader%2520access%2520to%2520high-quality%2520movement%2520analysis%2520could%2520greatly%2520benefit%250Amovement%2520science%2520and%2520rehabilitation%252C%2520such%2520as%2520allowing%2520more%2520detailed%250Acharacterization%2520of%2520movement%2520impairments%2520and%2520responses%2520to%2520interventions%252C%2520or%250Aeven%2520enabling%2520early%2520detection%2520of%2520new%2520neurological%2520conditions%2520or%2520fall%2520risk.%250AWhile%2520emerging%2520technologies%2520are%2520making%2520it%2520easier%2520to%2520capture%2520kinematics%2520with%250Abiomechanical%2520models%252C%2520or%2520how%2520joint%2520angles%2520change%2520over%2520time%252C%2520inferring%2520the%250Aunderlying%2520physics%2520that%2520give%2520rise%2520to%2520these%2520movements%252C%2520including%2520ground%2520reaction%250Aforces%252C%2520joint%2520torques%252C%2520or%2520even%2520muscle%2520activations%252C%2520is%2520still%2520challenging.%2520Here%250Awe%2520explore%2520whether%2520imitation%2520learning%2520applied%2520to%2520a%2520biomechanical%2520model%2520from%2520a%250Alarge%2520dataset%2520of%2520movements%2520from%2520able-bodied%2520and%2520impaired%2520individuals%2520can%2520learn%250Ato%2520compute%2520these%2520inverse%2520dynamics.%2520Although%2520imitation%2520learning%2520in%2520human%2520pose%250Aestimation%2520has%2520seen%2520great%2520interest%2520in%2520recent%2520years%252C%2520our%2520work%2520differences%2520in%250Aseveral%2520ways%253A%2520we%2520focus%2520on%2520using%2520an%2520accurate%2520biomechanical%2520model%2520instead%2520of%250Amodels%2520adopted%2520for%2520computer%2520vision%252C%2520we%2520test%2520it%2520on%2520a%2520dataset%2520that%2520contains%250Aparticipants%2520with%2520impaired%2520movements%252C%2520we%2520reported%2520detailed%2520tracking%2520metrics%250Arelevant%2520for%2520the%2520clinical%2520measurement%2520of%2520movement%2520including%2520joint%2520angles%2520and%250Aground%2520contact%2520events%252C%2520and%2520finally%2520we%2520apply%2520imitation%2520learning%2520to%2520a%250Amuscle-driven%2520neuromusculoskeletal%2520model.%2520We%2520show%2520that%2520our%2520imitation%2520learning%250Apolicy%252C%2520KinTwin%252C%2520can%2520accurately%2520replicate%2520the%2520kinematics%2520of%2520a%2520wide%2520range%2520of%250Amovements%252C%2520including%2520those%2520with%2520assistive%2520devices%2520or%2520therapist%2520assistance%252C%2520and%250Athat%2520it%2520can%2520infer%2520clinically%2520meaningful%2520differences%2520in%2520joint%2520torques%2520and%2520muscle%250Aactivations.%2520Our%2520work%2520demonstrates%2520the%2520potential%2520for%2520using%2520imitation%2520learning%250Ato%2520enable%2520high-quality%2520movement%2520analysis%2520in%2520clinical%2520practice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KinTwin%3A%20Imitation%20Learning%20with%20Torque%20and%20Muscle%20Driven%20Biomechanical%0A%20%20Models%20Enables%20Precise%20Replication%20of%20Able-Bodied%20and%20Impaired%20Movement%20from%0A%20%20Markerless%20Motion%20Capture&entry.906535625=R.%20James%20Cotton&entry.1292438233=%20%20Broader%20access%20to%20high-quality%20movement%20analysis%20could%20greatly%20benefit%0Amovement%20science%20and%20rehabilitation%2C%20such%20as%20allowing%20more%20detailed%0Acharacterization%20of%20movement%20impairments%20and%20responses%20to%20interventions%2C%20or%0Aeven%20enabling%20early%20detection%20of%20new%20neurological%20conditions%20or%20fall%20risk.%0AWhile%20emerging%20technologies%20are%20making%20it%20easier%20to%20capture%20kinematics%20with%0Abiomechanical%20models%2C%20or%20how%20joint%20angles%20change%20over%20time%2C%20inferring%20the%0Aunderlying%20physics%20that%20give%20rise%20to%20these%20movements%2C%20including%20ground%20reaction%0Aforces%2C%20joint%20torques%2C%20or%20even%20muscle%20activations%2C%20is%20still%20challenging.%20Here%0Awe%20explore%20whether%20imitation%20learning%20applied%20to%20a%20biomechanical%20model%20from%20a%0Alarge%20dataset%20of%20movements%20from%20able-bodied%20and%20impaired%20individuals%20can%20learn%0Ato%20compute%20these%20inverse%20dynamics.%20Although%20imitation%20learning%20in%20human%20pose%0Aestimation%20has%20seen%20great%20interest%20in%20recent%20years%2C%20our%20work%20differences%20in%0Aseveral%20ways%3A%20we%20focus%20on%20using%20an%20accurate%20biomechanical%20model%20instead%20of%0Amodels%20adopted%20for%20computer%20vision%2C%20we%20test%20it%20on%20a%20dataset%20that%20contains%0Aparticipants%20with%20impaired%20movements%2C%20we%20reported%20detailed%20tracking%20metrics%0Arelevant%20for%20the%20clinical%20measurement%20of%20movement%20including%20joint%20angles%20and%0Aground%20contact%20events%2C%20and%20finally%20we%20apply%20imitation%20learning%20to%20a%0Amuscle-driven%20neuromusculoskeletal%20model.%20We%20show%20that%20our%20imitation%20learning%0Apolicy%2C%20KinTwin%2C%20can%20accurately%20replicate%20the%20kinematics%20of%20a%20wide%20range%20of%0Amovements%2C%20including%20those%20with%20assistive%20devices%20or%20therapist%20assistance%2C%20and%0Athat%20it%20can%20infer%20clinically%20meaningful%20differences%20in%20joint%20torques%20and%20muscle%0Aactivations.%20Our%20work%20demonstrates%20the%20potential%20for%20using%20imitation%20learning%0Ato%20enable%20high-quality%20movement%20analysis%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13436v1&entry.124074799=Read"},
{"title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "author": "Jeffrey T. H. Wong and Cheng Zhang and Xinye Cao and Pedro Gimenes and George A. Constantinides and Wayne Luk and Yiren Zhao", "abstract": "  Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.\n", "link": "http://arxiv.org/abs/2505.12942v1", "date": "2025-05-19", "relevancy": 2.2292, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5638}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A3%20%3A%20an%20Analytical%20Low-Rank%20Approximation%20Framework%20for%20Attention&body=Title%3A%20A3%20%3A%20an%20Analytical%20Low-Rank%20Approximation%20Framework%20for%20Attention%0AAuthor%3A%20Jeffrey%20T.%20H.%20Wong%20and%20Cheng%20Zhang%20and%20Xinye%20Cao%20and%20Pedro%20Gimenes%20and%20George%20A.%20Constantinides%20and%20Wayne%20Luk%20and%20Yiren%20Zhao%0AAbstract%3A%20%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20performance%3B%20however%2C%0Atheir%20massive%20parameter%20counts%20make%20deployment%20highly%20expensive.%20Low-rank%0Aapproximation%20offers%20a%20promising%20compression%20solution%2C%20yet%20existing%20approaches%0Ahave%20two%20main%20limitations%3A%20%281%29%20They%20focus%20on%20minimizing%20the%20output%20error%20of%0Aindividual%20linear%20layers%2C%20without%20considering%20the%20architectural%20characteristics%0Aof%20Transformers%2C%20and%20%282%29%20they%20decompose%20a%20large%20weight%20matrix%20into%20two%20small%0Alow-rank%20matrices.%20Consequently%2C%20these%20methods%20often%20fall%20short%20compared%20to%0Aother%20compression%20techniques%20like%20pruning%20and%20quantization%2C%20and%20introduce%0Aruntime%20overhead%20such%20as%20the%20extra%20GEMM%20kernel%20launches%20for%20decomposed%20small%0Amatrices.%20To%20address%20these%20limitations%2C%20we%20propose%20%24%5Ctt%20A%5E%5Ctt%203%24%2C%20a%0Apost-training%20low-rank%20approximation%20framework.%20%24%5Ctt%20A%5E%5Ctt%203%24%20splits%20a%0ATransformer%20layer%20into%20three%20functional%20components%2C%20namely%20%24%5Ctt%20QK%24%2C%20%24%5Ctt%20OV%24%2C%0Aand%20%24%5Ctt%20MLP%24.%20For%20each%20component%2C%20%24%5Ctt%20A%5E%5Ctt%203%24%20provides%20an%20analytical%0Asolution%20that%20reduces%20the%20hidden%20dimension%20size%20inside%20each%20component%20while%0Aminimizing%20the%20component%27s%20functional%20loss%20%28%24%5Cit%20i.e.%24%2C%20error%20in%20attention%0Ascores%2C%20attention%20outputs%2C%20and%20MLP%20outputs%29.%20This%20approach%20directly%20reduces%0Amodel%20sizes%2C%20KV%20cache%20sizes%2C%20and%20FLOPs%20without%20introducing%20any%20runtime%0Aoverheads.%20In%20addition%2C%20it%20provides%20a%20new%20narrative%20in%20advancing%20the%0Aoptimization%20problem%20from%20singular%20linear%20layer%20loss%20optimization%20toward%0Aimproved%20end-to-end%20performance.%20Through%20extensive%20experiments%2C%20we%20show%20that%0A%24%5Ctt%20A%5E%5Ctt%203%24%20maintains%20superior%20performance%20compared%20to%20SoTAs.%20For%20example%2C%0Aunder%20the%20same%20reduction%20budget%20in%20computation%20and%20memory%2C%20our%20low-rank%0Aapproximated%20LLaMA%203.1-70B%20achieves%20a%20perplexity%20of%204.69%20on%20WikiText-2%2C%0Aoutperforming%20the%20previous%20SoTA%27s%207.87%20by%203.18.%20We%20also%20demonstrate%20the%0Aversatility%20of%20%24%5Ctt%20A%5E%5Ctt%203%24%2C%20including%20KV%20cache%20compression%2C%20quantization%2C%20and%0Amixed-rank%20assignments%20for%20enhanced%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA3%2520%253A%2520an%2520Analytical%2520Low-Rank%2520Approximation%2520Framework%2520for%2520Attention%26entry.906535625%3DJeffrey%2520T.%2520H.%2520Wong%2520and%2520Cheng%2520Zhang%2520and%2520Xinye%2520Cao%2520and%2520Pedro%2520Gimenes%2520and%2520George%2520A.%2520Constantinides%2520and%2520Wayne%2520Luk%2520and%2520Yiren%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520demonstrated%2520remarkable%2520performance%253B%2520however%252C%250Atheir%2520massive%2520parameter%2520counts%2520make%2520deployment%2520highly%2520expensive.%2520Low-rank%250Aapproximation%2520offers%2520a%2520promising%2520compression%2520solution%252C%2520yet%2520existing%2520approaches%250Ahave%2520two%2520main%2520limitations%253A%2520%25281%2529%2520They%2520focus%2520on%2520minimizing%2520the%2520output%2520error%2520of%250Aindividual%2520linear%2520layers%252C%2520without%2520considering%2520the%2520architectural%2520characteristics%250Aof%2520Transformers%252C%2520and%2520%25282%2529%2520they%2520decompose%2520a%2520large%2520weight%2520matrix%2520into%2520two%2520small%250Alow-rank%2520matrices.%2520Consequently%252C%2520these%2520methods%2520often%2520fall%2520short%2520compared%2520to%250Aother%2520compression%2520techniques%2520like%2520pruning%2520and%2520quantization%252C%2520and%2520introduce%250Aruntime%2520overhead%2520such%2520as%2520the%2520extra%2520GEMM%2520kernel%2520launches%2520for%2520decomposed%2520small%250Amatrices.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520%2524%255Ctt%2520A%255E%255Ctt%25203%2524%252C%2520a%250Apost-training%2520low-rank%2520approximation%2520framework.%2520%2524%255Ctt%2520A%255E%255Ctt%25203%2524%2520splits%2520a%250ATransformer%2520layer%2520into%2520three%2520functional%2520components%252C%2520namely%2520%2524%255Ctt%2520QK%2524%252C%2520%2524%255Ctt%2520OV%2524%252C%250Aand%2520%2524%255Ctt%2520MLP%2524.%2520For%2520each%2520component%252C%2520%2524%255Ctt%2520A%255E%255Ctt%25203%2524%2520provides%2520an%2520analytical%250Asolution%2520that%2520reduces%2520the%2520hidden%2520dimension%2520size%2520inside%2520each%2520component%2520while%250Aminimizing%2520the%2520component%2527s%2520functional%2520loss%2520%2528%2524%255Cit%2520i.e.%2524%252C%2520error%2520in%2520attention%250Ascores%252C%2520attention%2520outputs%252C%2520and%2520MLP%2520outputs%2529.%2520This%2520approach%2520directly%2520reduces%250Amodel%2520sizes%252C%2520KV%2520cache%2520sizes%252C%2520and%2520FLOPs%2520without%2520introducing%2520any%2520runtime%250Aoverheads.%2520In%2520addition%252C%2520it%2520provides%2520a%2520new%2520narrative%2520in%2520advancing%2520the%250Aoptimization%2520problem%2520from%2520singular%2520linear%2520layer%2520loss%2520optimization%2520toward%250Aimproved%2520end-to-end%2520performance.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%250A%2524%255Ctt%2520A%255E%255Ctt%25203%2524%2520maintains%2520superior%2520performance%2520compared%2520to%2520SoTAs.%2520For%2520example%252C%250Aunder%2520the%2520same%2520reduction%2520budget%2520in%2520computation%2520and%2520memory%252C%2520our%2520low-rank%250Aapproximated%2520LLaMA%25203.1-70B%2520achieves%2520a%2520perplexity%2520of%25204.69%2520on%2520WikiText-2%252C%250Aoutperforming%2520the%2520previous%2520SoTA%2527s%25207.87%2520by%25203.18.%2520We%2520also%2520demonstrate%2520the%250Aversatility%2520of%2520%2524%255Ctt%2520A%255E%255Ctt%25203%2524%252C%2520including%2520KV%2520cache%2520compression%252C%2520quantization%252C%2520and%250Amixed-rank%2520assignments%2520for%2520enhanced%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A3%20%3A%20an%20Analytical%20Low-Rank%20Approximation%20Framework%20for%20Attention&entry.906535625=Jeffrey%20T.%20H.%20Wong%20and%20Cheng%20Zhang%20and%20Xinye%20Cao%20and%20Pedro%20Gimenes%20and%20George%20A.%20Constantinides%20and%20Wayne%20Luk%20and%20Yiren%20Zhao&entry.1292438233=%20%20Large%20language%20models%20have%20demonstrated%20remarkable%20performance%3B%20however%2C%0Atheir%20massive%20parameter%20counts%20make%20deployment%20highly%20expensive.%20Low-rank%0Aapproximation%20offers%20a%20promising%20compression%20solution%2C%20yet%20existing%20approaches%0Ahave%20two%20main%20limitations%3A%20%281%29%20They%20focus%20on%20minimizing%20the%20output%20error%20of%0Aindividual%20linear%20layers%2C%20without%20considering%20the%20architectural%20characteristics%0Aof%20Transformers%2C%20and%20%282%29%20they%20decompose%20a%20large%20weight%20matrix%20into%20two%20small%0Alow-rank%20matrices.%20Consequently%2C%20these%20methods%20often%20fall%20short%20compared%20to%0Aother%20compression%20techniques%20like%20pruning%20and%20quantization%2C%20and%20introduce%0Aruntime%20overhead%20such%20as%20the%20extra%20GEMM%20kernel%20launches%20for%20decomposed%20small%0Amatrices.%20To%20address%20these%20limitations%2C%20we%20propose%20%24%5Ctt%20A%5E%5Ctt%203%24%2C%20a%0Apost-training%20low-rank%20approximation%20framework.%20%24%5Ctt%20A%5E%5Ctt%203%24%20splits%20a%0ATransformer%20layer%20into%20three%20functional%20components%2C%20namely%20%24%5Ctt%20QK%24%2C%20%24%5Ctt%20OV%24%2C%0Aand%20%24%5Ctt%20MLP%24.%20For%20each%20component%2C%20%24%5Ctt%20A%5E%5Ctt%203%24%20provides%20an%20analytical%0Asolution%20that%20reduces%20the%20hidden%20dimension%20size%20inside%20each%20component%20while%0Aminimizing%20the%20component%27s%20functional%20loss%20%28%24%5Cit%20i.e.%24%2C%20error%20in%20attention%0Ascores%2C%20attention%20outputs%2C%20and%20MLP%20outputs%29.%20This%20approach%20directly%20reduces%0Amodel%20sizes%2C%20KV%20cache%20sizes%2C%20and%20FLOPs%20without%20introducing%20any%20runtime%0Aoverheads.%20In%20addition%2C%20it%20provides%20a%20new%20narrative%20in%20advancing%20the%0Aoptimization%20problem%20from%20singular%20linear%20layer%20loss%20optimization%20toward%0Aimproved%20end-to-end%20performance.%20Through%20extensive%20experiments%2C%20we%20show%20that%0A%24%5Ctt%20A%5E%5Ctt%203%24%20maintains%20superior%20performance%20compared%20to%20SoTAs.%20For%20example%2C%0Aunder%20the%20same%20reduction%20budget%20in%20computation%20and%20memory%2C%20our%20low-rank%0Aapproximated%20LLaMA%203.1-70B%20achieves%20a%20perplexity%20of%204.69%20on%20WikiText-2%2C%0Aoutperforming%20the%20previous%20SoTA%27s%207.87%20by%203.18.%20We%20also%20demonstrate%20the%0Aversatility%20of%20%24%5Ctt%20A%5E%5Ctt%203%24%2C%20including%20KV%20cache%20compression%2C%20quantization%2C%20and%0Amixed-rank%20assignments%20for%20enhanced%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12942v1&entry.124074799=Read"},
{"title": "Joint Depth and Reflectivity Estimation using Single-Photon LiDAR", "author": "Hashan K. Weerasooriya and Prateek Chennuri and Weijian Zhang and Istvan Gyongy and Stanley H. Chan", "abstract": "  Single-Photon Light Detection and Ranging (SP-LiDAR is emerging as a leading\ntechnology for long-range, high-precision 3D vision tasks. In SP-LiDAR,\ntimestamps encode two complementary pieces of information: pulse travel time\n(depth) and the number of photons reflected by the object (reflectivity).\nExisting SP-LiDAR reconstruction methods typically recover depth and\nreflectivity separately or sequentially use one modality to estimate the other.\nMoreover, the conventional 3D histogram construction is effective mainly for\nslow-moving or stationary scenes. In dynamic scenes, however, it is more\nefficient and effective to directly process the timestamps. In this paper, we\nintroduce an estimation method to simultaneously recover both depth and\nreflectivity in fast-moving scenes. We offer two contributions: (1) A\ntheoretical analysis demonstrating the mutual correlation between depth and\nreflectivity and the conditions under which joint estimation becomes\nbeneficial. (2) A novel reconstruction method, \"SPLiDER\", which exploits the\nshared information to enhance signal recovery. On both synthetic and real\nSP-LiDAR data, our method outperforms existing approaches, achieving superior\njoint reconstruction quality.\n", "link": "http://arxiv.org/abs/2505.13250v1", "date": "2025-05-19", "relevancy": 2.2255, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5885}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Depth%20and%20Reflectivity%20Estimation%20using%20Single-Photon%20LiDAR&body=Title%3A%20Joint%20Depth%20and%20Reflectivity%20Estimation%20using%20Single-Photon%20LiDAR%0AAuthor%3A%20Hashan%20K.%20Weerasooriya%20and%20Prateek%20Chennuri%20and%20Weijian%20Zhang%20and%20Istvan%20Gyongy%20and%20Stanley%20H.%20Chan%0AAbstract%3A%20%20%20Single-Photon%20Light%20Detection%20and%20Ranging%20%28SP-LiDAR%20is%20emerging%20as%20a%20leading%0Atechnology%20for%20long-range%2C%20high-precision%203D%20vision%20tasks.%20In%20SP-LiDAR%2C%0Atimestamps%20encode%20two%20complementary%20pieces%20of%20information%3A%20pulse%20travel%20time%0A%28depth%29%20and%20the%20number%20of%20photons%20reflected%20by%20the%20object%20%28reflectivity%29.%0AExisting%20SP-LiDAR%20reconstruction%20methods%20typically%20recover%20depth%20and%0Areflectivity%20separately%20or%20sequentially%20use%20one%20modality%20to%20estimate%20the%20other.%0AMoreover%2C%20the%20conventional%203D%20histogram%20construction%20is%20effective%20mainly%20for%0Aslow-moving%20or%20stationary%20scenes.%20In%20dynamic%20scenes%2C%20however%2C%20it%20is%20more%0Aefficient%20and%20effective%20to%20directly%20process%20the%20timestamps.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20estimation%20method%20to%20simultaneously%20recover%20both%20depth%20and%0Areflectivity%20in%20fast-moving%20scenes.%20We%20offer%20two%20contributions%3A%20%281%29%20A%0Atheoretical%20analysis%20demonstrating%20the%20mutual%20correlation%20between%20depth%20and%0Areflectivity%20and%20the%20conditions%20under%20which%20joint%20estimation%20becomes%0Abeneficial.%20%282%29%20A%20novel%20reconstruction%20method%2C%20%22SPLiDER%22%2C%20which%20exploits%20the%0Ashared%20information%20to%20enhance%20signal%20recovery.%20On%20both%20synthetic%20and%20real%0ASP-LiDAR%20data%2C%20our%20method%20outperforms%20existing%20approaches%2C%20achieving%20superior%0Ajoint%20reconstruction%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Depth%2520and%2520Reflectivity%2520Estimation%2520using%2520Single-Photon%2520LiDAR%26entry.906535625%3DHashan%2520K.%2520Weerasooriya%2520and%2520Prateek%2520Chennuri%2520and%2520Weijian%2520Zhang%2520and%2520Istvan%2520Gyongy%2520and%2520Stanley%2520H.%2520Chan%26entry.1292438233%3D%2520%2520Single-Photon%2520Light%2520Detection%2520and%2520Ranging%2520%2528SP-LiDAR%2520is%2520emerging%2520as%2520a%2520leading%250Atechnology%2520for%2520long-range%252C%2520high-precision%25203D%2520vision%2520tasks.%2520In%2520SP-LiDAR%252C%250Atimestamps%2520encode%2520two%2520complementary%2520pieces%2520of%2520information%253A%2520pulse%2520travel%2520time%250A%2528depth%2529%2520and%2520the%2520number%2520of%2520photons%2520reflected%2520by%2520the%2520object%2520%2528reflectivity%2529.%250AExisting%2520SP-LiDAR%2520reconstruction%2520methods%2520typically%2520recover%2520depth%2520and%250Areflectivity%2520separately%2520or%2520sequentially%2520use%2520one%2520modality%2520to%2520estimate%2520the%2520other.%250AMoreover%252C%2520the%2520conventional%25203D%2520histogram%2520construction%2520is%2520effective%2520mainly%2520for%250Aslow-moving%2520or%2520stationary%2520scenes.%2520In%2520dynamic%2520scenes%252C%2520however%252C%2520it%2520is%2520more%250Aefficient%2520and%2520effective%2520to%2520directly%2520process%2520the%2520timestamps.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520an%2520estimation%2520method%2520to%2520simultaneously%2520recover%2520both%2520depth%2520and%250Areflectivity%2520in%2520fast-moving%2520scenes.%2520We%2520offer%2520two%2520contributions%253A%2520%25281%2529%2520A%250Atheoretical%2520analysis%2520demonstrating%2520the%2520mutual%2520correlation%2520between%2520depth%2520and%250Areflectivity%2520and%2520the%2520conditions%2520under%2520which%2520joint%2520estimation%2520becomes%250Abeneficial.%2520%25282%2529%2520A%2520novel%2520reconstruction%2520method%252C%2520%2522SPLiDER%2522%252C%2520which%2520exploits%2520the%250Ashared%2520information%2520to%2520enhance%2520signal%2520recovery.%2520On%2520both%2520synthetic%2520and%2520real%250ASP-LiDAR%2520data%252C%2520our%2520method%2520outperforms%2520existing%2520approaches%252C%2520achieving%2520superior%250Ajoint%2520reconstruction%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Depth%20and%20Reflectivity%20Estimation%20using%20Single-Photon%20LiDAR&entry.906535625=Hashan%20K.%20Weerasooriya%20and%20Prateek%20Chennuri%20and%20Weijian%20Zhang%20and%20Istvan%20Gyongy%20and%20Stanley%20H.%20Chan&entry.1292438233=%20%20Single-Photon%20Light%20Detection%20and%20Ranging%20%28SP-LiDAR%20is%20emerging%20as%20a%20leading%0Atechnology%20for%20long-range%2C%20high-precision%203D%20vision%20tasks.%20In%20SP-LiDAR%2C%0Atimestamps%20encode%20two%20complementary%20pieces%20of%20information%3A%20pulse%20travel%20time%0A%28depth%29%20and%20the%20number%20of%20photons%20reflected%20by%20the%20object%20%28reflectivity%29.%0AExisting%20SP-LiDAR%20reconstruction%20methods%20typically%20recover%20depth%20and%0Areflectivity%20separately%20or%20sequentially%20use%20one%20modality%20to%20estimate%20the%20other.%0AMoreover%2C%20the%20conventional%203D%20histogram%20construction%20is%20effective%20mainly%20for%0Aslow-moving%20or%20stationary%20scenes.%20In%20dynamic%20scenes%2C%20however%2C%20it%20is%20more%0Aefficient%20and%20effective%20to%20directly%20process%20the%20timestamps.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20estimation%20method%20to%20simultaneously%20recover%20both%20depth%20and%0Areflectivity%20in%20fast-moving%20scenes.%20We%20offer%20two%20contributions%3A%20%281%29%20A%0Atheoretical%20analysis%20demonstrating%20the%20mutual%20correlation%20between%20depth%20and%0Areflectivity%20and%20the%20conditions%20under%20which%20joint%20estimation%20becomes%0Abeneficial.%20%282%29%20A%20novel%20reconstruction%20method%2C%20%22SPLiDER%22%2C%20which%20exploits%20the%0Ashared%20information%20to%20enhance%20signal%20recovery.%20On%20both%20synthetic%20and%20real%0ASP-LiDAR%20data%2C%20our%20method%20outperforms%20existing%20approaches%2C%20achieving%20superior%0Ajoint%20reconstruction%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13250v1&entry.124074799=Read"},
{"title": "Learning to Adapt to Position Bias in Vision Transformer Classifiers", "author": "Robert-Jan Bruintjes and Jan van Gemert", "abstract": "  How discriminative position information is for image classification depends\non the data. On the one hand, the camera position is arbitrary and objects can\nappear anywhere in the image, arguing for translation invariance. At the same\ntime, position information is key for exploiting capture/center bias, and scene\nlayout, e.g.: the sky is up. We show that position bias, the level to which a\ndataset is more easily solved when positional information on input features is\nused, plays a crucial role in the performance of Vision Transformers image\nclassifiers. To investigate, we propose Position-SHAP, a direct measure of\nposition bias by extending SHAP to work with position embeddings. We show\nvarious levels of position bias in different datasets, and find that the\noptimal choice of position embedding depends on the position bias apparent in\nthe dataset. We therefore propose Auto-PE, a single-parameter position\nembedding extension, which allows the position embedding to modulate its norm,\nenabling the unlearning of position information. Auto-PE combines with existing\nPEs to match or improve accuracy on classification datasets.\n", "link": "http://arxiv.org/abs/2505.13137v1", "date": "2025-05-19", "relevancy": 2.2185, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5734}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Adapt%20to%20Position%20Bias%20in%20Vision%20Transformer%20Classifiers&body=Title%3A%20Learning%20to%20Adapt%20to%20Position%20Bias%20in%20Vision%20Transformer%20Classifiers%0AAuthor%3A%20Robert-Jan%20Bruintjes%20and%20Jan%20van%20Gemert%0AAbstract%3A%20%20%20How%20discriminative%20position%20information%20is%20for%20image%20classification%20depends%0Aon%20the%20data.%20On%20the%20one%20hand%2C%20the%20camera%20position%20is%20arbitrary%20and%20objects%20can%0Aappear%20anywhere%20in%20the%20image%2C%20arguing%20for%20translation%20invariance.%20At%20the%20same%0Atime%2C%20position%20information%20is%20key%20for%20exploiting%20capture/center%20bias%2C%20and%20scene%0Alayout%2C%20e.g.%3A%20the%20sky%20is%20up.%20We%20show%20that%20position%20bias%2C%20the%20level%20to%20which%20a%0Adataset%20is%20more%20easily%20solved%20when%20positional%20information%20on%20input%20features%20is%0Aused%2C%20plays%20a%20crucial%20role%20in%20the%20performance%20of%20Vision%20Transformers%20image%0Aclassifiers.%20To%20investigate%2C%20we%20propose%20Position-SHAP%2C%20a%20direct%20measure%20of%0Aposition%20bias%20by%20extending%20SHAP%20to%20work%20with%20position%20embeddings.%20We%20show%0Avarious%20levels%20of%20position%20bias%20in%20different%20datasets%2C%20and%20find%20that%20the%0Aoptimal%20choice%20of%20position%20embedding%20depends%20on%20the%20position%20bias%20apparent%20in%0Athe%20dataset.%20We%20therefore%20propose%20Auto-PE%2C%20a%20single-parameter%20position%0Aembedding%20extension%2C%20which%20allows%20the%20position%20embedding%20to%20modulate%20its%20norm%2C%0Aenabling%20the%20unlearning%20of%20position%20information.%20Auto-PE%20combines%20with%20existing%0APEs%20to%20match%20or%20improve%20accuracy%20on%20classification%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Adapt%2520to%2520Position%2520Bias%2520in%2520Vision%2520Transformer%2520Classifiers%26entry.906535625%3DRobert-Jan%2520Bruintjes%2520and%2520Jan%2520van%2520Gemert%26entry.1292438233%3D%2520%2520How%2520discriminative%2520position%2520information%2520is%2520for%2520image%2520classification%2520depends%250Aon%2520the%2520data.%2520On%2520the%2520one%2520hand%252C%2520the%2520camera%2520position%2520is%2520arbitrary%2520and%2520objects%2520can%250Aappear%2520anywhere%2520in%2520the%2520image%252C%2520arguing%2520for%2520translation%2520invariance.%2520At%2520the%2520same%250Atime%252C%2520position%2520information%2520is%2520key%2520for%2520exploiting%2520capture/center%2520bias%252C%2520and%2520scene%250Alayout%252C%2520e.g.%253A%2520the%2520sky%2520is%2520up.%2520We%2520show%2520that%2520position%2520bias%252C%2520the%2520level%2520to%2520which%2520a%250Adataset%2520is%2520more%2520easily%2520solved%2520when%2520positional%2520information%2520on%2520input%2520features%2520is%250Aused%252C%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520performance%2520of%2520Vision%2520Transformers%2520image%250Aclassifiers.%2520To%2520investigate%252C%2520we%2520propose%2520Position-SHAP%252C%2520a%2520direct%2520measure%2520of%250Aposition%2520bias%2520by%2520extending%2520SHAP%2520to%2520work%2520with%2520position%2520embeddings.%2520We%2520show%250Avarious%2520levels%2520of%2520position%2520bias%2520in%2520different%2520datasets%252C%2520and%2520find%2520that%2520the%250Aoptimal%2520choice%2520of%2520position%2520embedding%2520depends%2520on%2520the%2520position%2520bias%2520apparent%2520in%250Athe%2520dataset.%2520We%2520therefore%2520propose%2520Auto-PE%252C%2520a%2520single-parameter%2520position%250Aembedding%2520extension%252C%2520which%2520allows%2520the%2520position%2520embedding%2520to%2520modulate%2520its%2520norm%252C%250Aenabling%2520the%2520unlearning%2520of%2520position%2520information.%2520Auto-PE%2520combines%2520with%2520existing%250APEs%2520to%2520match%2520or%2520improve%2520accuracy%2520on%2520classification%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Adapt%20to%20Position%20Bias%20in%20Vision%20Transformer%20Classifiers&entry.906535625=Robert-Jan%20Bruintjes%20and%20Jan%20van%20Gemert&entry.1292438233=%20%20How%20discriminative%20position%20information%20is%20for%20image%20classification%20depends%0Aon%20the%20data.%20On%20the%20one%20hand%2C%20the%20camera%20position%20is%20arbitrary%20and%20objects%20can%0Aappear%20anywhere%20in%20the%20image%2C%20arguing%20for%20translation%20invariance.%20At%20the%20same%0Atime%2C%20position%20information%20is%20key%20for%20exploiting%20capture/center%20bias%2C%20and%20scene%0Alayout%2C%20e.g.%3A%20the%20sky%20is%20up.%20We%20show%20that%20position%20bias%2C%20the%20level%20to%20which%20a%0Adataset%20is%20more%20easily%20solved%20when%20positional%20information%20on%20input%20features%20is%0Aused%2C%20plays%20a%20crucial%20role%20in%20the%20performance%20of%20Vision%20Transformers%20image%0Aclassifiers.%20To%20investigate%2C%20we%20propose%20Position-SHAP%2C%20a%20direct%20measure%20of%0Aposition%20bias%20by%20extending%20SHAP%20to%20work%20with%20position%20embeddings.%20We%20show%0Avarious%20levels%20of%20position%20bias%20in%20different%20datasets%2C%20and%20find%20that%20the%0Aoptimal%20choice%20of%20position%20embedding%20depends%20on%20the%20position%20bias%20apparent%20in%0Athe%20dataset.%20We%20therefore%20propose%20Auto-PE%2C%20a%20single-parameter%20position%0Aembedding%20extension%2C%20which%20allows%20the%20position%20embedding%20to%20modulate%20its%20norm%2C%0Aenabling%20the%20unlearning%20of%20position%20information.%20Auto-PE%20combines%20with%20existing%0APEs%20to%20match%20or%20improve%20accuracy%20on%20classification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13137v1&entry.124074799=Read"},
{"title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image\n  Generation", "author": "Huawei Lin and Tong Geng and Zhaozhuo Xu and Weijie Zhao", "abstract": "  Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs.\n", "link": "http://arxiv.org/abs/2505.13439v1", "date": "2025-05-19", "relevancy": 2.2184, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5787}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTBench%3A%20Evaluating%20Visual%20Tokenizers%20for%20Autoregressive%20Image%0A%20%20Generation&body=Title%3A%20VTBench%3A%20Evaluating%20Visual%20Tokenizers%20for%20Autoregressive%20Image%0A%20%20Generation%0AAuthor%3A%20Huawei%20Lin%20and%20Tong%20Geng%20and%20Zhaozhuo%20Xu%20and%20Weijie%20Zhao%0AAbstract%3A%20%20%20Autoregressive%20%28AR%29%20models%20have%20recently%20shown%20strong%20performance%20in%20image%0Ageneration%2C%20where%20a%20critical%20component%20is%20the%20visual%20tokenizer%20%28VT%29%20that%20maps%0Acontinuous%20pixel%20inputs%20to%20discrete%20token%20sequences.%20The%20quality%20of%20the%20VT%0Alargely%20defines%20the%20upper%20bound%20of%20AR%20model%20performance.%20However%2C%20current%0Adiscrete%20VTs%20fall%20significantly%20behind%20continuous%20variational%20autoencoders%0A%28VAEs%29%2C%20leading%20to%20degraded%20image%20reconstructions%20and%20poor%20preservation%20of%0Adetails%20and%20text.%20Existing%20benchmarks%20focus%20on%20end-to-end%20generation%20quality%2C%0Awithout%20isolating%20VT%20performance.%20To%20address%20this%20gap%2C%20we%20introduce%20VTBench%2C%20a%0Acomprehensive%20benchmark%20that%20systematically%20evaluates%20VTs%20across%20three%20core%0Atasks%3A%20Image%20Reconstruction%2C%20Detail%20Preservation%2C%20and%20Text%20Preservation%2C%20and%0Acovers%20a%20diverse%20range%20of%20evaluation%20scenarios.%20We%20systematically%20assess%0Astate-of-the-art%20VTs%20using%20a%20set%20of%20metrics%20to%20evaluate%20the%20quality%20of%0Areconstructed%20images.%20Our%20findings%20reveal%20that%20continuous%20VAEs%20produce%20superior%0Avisual%20representations%20compared%20to%20discrete%20VTs%2C%20particularly%20in%20retaining%0Aspatial%20structure%20and%20semantic%20detail.%20In%20contrast%2C%20the%20degraded%0Arepresentations%20produced%20by%20discrete%20VTs%20often%20lead%20to%20distorted%0Areconstructions%2C%20loss%20of%20fine-grained%20textures%2C%20and%20failures%20in%20preserving%20text%0Aand%20object%20integrity.%20Furthermore%2C%20we%20conduct%20experiments%20on%20GPT-4o%20image%0Ageneration%20and%20discuss%20its%20potential%20AR%20nature%2C%20offering%20new%20insights%20into%20the%0Arole%20of%20visual%20tokenization.%20We%20release%20our%20benchmark%20and%20codebase%20publicly%20to%0Asupport%20further%20research%20and%20call%20on%20the%20community%20to%20develop%20strong%2C%0Ageneral-purpose%20open-source%20VTs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTBench%253A%2520Evaluating%2520Visual%2520Tokenizers%2520for%2520Autoregressive%2520Image%250A%2520%2520Generation%26entry.906535625%3DHuawei%2520Lin%2520and%2520Tong%2520Geng%2520and%2520Zhaozhuo%2520Xu%2520and%2520Weijie%2520Zhao%26entry.1292438233%3D%2520%2520Autoregressive%2520%2528AR%2529%2520models%2520have%2520recently%2520shown%2520strong%2520performance%2520in%2520image%250Ageneration%252C%2520where%2520a%2520critical%2520component%2520is%2520the%2520visual%2520tokenizer%2520%2528VT%2529%2520that%2520maps%250Acontinuous%2520pixel%2520inputs%2520to%2520discrete%2520token%2520sequences.%2520The%2520quality%2520of%2520the%2520VT%250Alargely%2520defines%2520the%2520upper%2520bound%2520of%2520AR%2520model%2520performance.%2520However%252C%2520current%250Adiscrete%2520VTs%2520fall%2520significantly%2520behind%2520continuous%2520variational%2520autoencoders%250A%2528VAEs%2529%252C%2520leading%2520to%2520degraded%2520image%2520reconstructions%2520and%2520poor%2520preservation%2520of%250Adetails%2520and%2520text.%2520Existing%2520benchmarks%2520focus%2520on%2520end-to-end%2520generation%2520quality%252C%250Awithout%2520isolating%2520VT%2520performance.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520VTBench%252C%2520a%250Acomprehensive%2520benchmark%2520that%2520systematically%2520evaluates%2520VTs%2520across%2520three%2520core%250Atasks%253A%2520Image%2520Reconstruction%252C%2520Detail%2520Preservation%252C%2520and%2520Text%2520Preservation%252C%2520and%250Acovers%2520a%2520diverse%2520range%2520of%2520evaluation%2520scenarios.%2520We%2520systematically%2520assess%250Astate-of-the-art%2520VTs%2520using%2520a%2520set%2520of%2520metrics%2520to%2520evaluate%2520the%2520quality%2520of%250Areconstructed%2520images.%2520Our%2520findings%2520reveal%2520that%2520continuous%2520VAEs%2520produce%2520superior%250Avisual%2520representations%2520compared%2520to%2520discrete%2520VTs%252C%2520particularly%2520in%2520retaining%250Aspatial%2520structure%2520and%2520semantic%2520detail.%2520In%2520contrast%252C%2520the%2520degraded%250Arepresentations%2520produced%2520by%2520discrete%2520VTs%2520often%2520lead%2520to%2520distorted%250Areconstructions%252C%2520loss%2520of%2520fine-grained%2520textures%252C%2520and%2520failures%2520in%2520preserving%2520text%250Aand%2520object%2520integrity.%2520Furthermore%252C%2520we%2520conduct%2520experiments%2520on%2520GPT-4o%2520image%250Ageneration%2520and%2520discuss%2520its%2520potential%2520AR%2520nature%252C%2520offering%2520new%2520insights%2520into%2520the%250Arole%2520of%2520visual%2520tokenization.%2520We%2520release%2520our%2520benchmark%2520and%2520codebase%2520publicly%2520to%250Asupport%2520further%2520research%2520and%2520call%2520on%2520the%2520community%2520to%2520develop%2520strong%252C%250Ageneral-purpose%2520open-source%2520VTs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTBench%3A%20Evaluating%20Visual%20Tokenizers%20for%20Autoregressive%20Image%0A%20%20Generation&entry.906535625=Huawei%20Lin%20and%20Tong%20Geng%20and%20Zhaozhuo%20Xu%20and%20Weijie%20Zhao&entry.1292438233=%20%20Autoregressive%20%28AR%29%20models%20have%20recently%20shown%20strong%20performance%20in%20image%0Ageneration%2C%20where%20a%20critical%20component%20is%20the%20visual%20tokenizer%20%28VT%29%20that%20maps%0Acontinuous%20pixel%20inputs%20to%20discrete%20token%20sequences.%20The%20quality%20of%20the%20VT%0Alargely%20defines%20the%20upper%20bound%20of%20AR%20model%20performance.%20However%2C%20current%0Adiscrete%20VTs%20fall%20significantly%20behind%20continuous%20variational%20autoencoders%0A%28VAEs%29%2C%20leading%20to%20degraded%20image%20reconstructions%20and%20poor%20preservation%20of%0Adetails%20and%20text.%20Existing%20benchmarks%20focus%20on%20end-to-end%20generation%20quality%2C%0Awithout%20isolating%20VT%20performance.%20To%20address%20this%20gap%2C%20we%20introduce%20VTBench%2C%20a%0Acomprehensive%20benchmark%20that%20systematically%20evaluates%20VTs%20across%20three%20core%0Atasks%3A%20Image%20Reconstruction%2C%20Detail%20Preservation%2C%20and%20Text%20Preservation%2C%20and%0Acovers%20a%20diverse%20range%20of%20evaluation%20scenarios.%20We%20systematically%20assess%0Astate-of-the-art%20VTs%20using%20a%20set%20of%20metrics%20to%20evaluate%20the%20quality%20of%0Areconstructed%20images.%20Our%20findings%20reveal%20that%20continuous%20VAEs%20produce%20superior%0Avisual%20representations%20compared%20to%20discrete%20VTs%2C%20particularly%20in%20retaining%0Aspatial%20structure%20and%20semantic%20detail.%20In%20contrast%2C%20the%20degraded%0Arepresentations%20produced%20by%20discrete%20VTs%20often%20lead%20to%20distorted%0Areconstructions%2C%20loss%20of%20fine-grained%20textures%2C%20and%20failures%20in%20preserving%20text%0Aand%20object%20integrity.%20Furthermore%2C%20we%20conduct%20experiments%20on%20GPT-4o%20image%0Ageneration%20and%20discuss%20its%20potential%20AR%20nature%2C%20offering%20new%20insights%20into%20the%0Arole%20of%20visual%20tokenization.%20We%20release%20our%20benchmark%20and%20codebase%20publicly%20to%0Asupport%20further%20research%20and%20call%20on%20the%20community%20to%20develop%20strong%2C%0Ageneral-purpose%20open-source%20VTs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13439v1&entry.124074799=Read"},
{"title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the\n  Ocean of LLMs", "author": "Lars-Peter Meyer and Johannes Frey and Desiree Heim and Felix Brei and Claus Stadler and Kurt Junghanns and Michael Martin", "abstract": "  Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.\n", "link": "http://arxiv.org/abs/2505.13098v1", "date": "2025-05-19", "relevancy": 2.214, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-KG-Bench%203.0%3A%20A%20Compass%20for%20SemanticTechnology%20Capabilities%20in%20the%0A%20%20Ocean%20of%20LLMs&body=Title%3A%20LLM-KG-Bench%203.0%3A%20A%20Compass%20for%20SemanticTechnology%20Capabilities%20in%20the%0A%20%20Ocean%20of%20LLMs%0AAuthor%3A%20Lars-Peter%20Meyer%20and%20Johannes%20Frey%20and%20Desiree%20Heim%20and%20Felix%20Brei%20and%20Claus%20Stadler%20and%20Kurt%20Junghanns%20and%20Michael%20Martin%0AAbstract%3A%20%20%20Current%20Large%20Language%20Models%20%28LLMs%29%20can%20assist%20developing%20program%20code%0Abeside%20many%20other%20things%2C%20but%20can%20they%20support%20working%20with%20Knowledge%20Graphs%0A%28KGs%29%20as%20well%3F%20Which%20LLM%20is%20offering%20the%20best%20capabilities%20in%20the%20field%20of%0ASemantic%20Web%20and%20Knowledge%20Graph%20Engineering%20%28KGE%29%3F%20Is%20this%20possible%20to%0Adetermine%20without%20checking%20many%20answers%20manually%3F%20The%20LLM-KG-Bench%20framework%20in%0AVersion%203.0%20is%20designed%20to%20answer%20these%20questions.%20It%20consists%20of%20an%20extensible%0Aset%20of%20tasks%20for%20automated%20evaluation%20of%20LLM%20answers%20and%20covers%20different%0Aaspects%20of%20working%20with%20semantic%20technologies.%20In%20this%20paper%20the%20LLM-KG-Bench%0Aframework%20is%20presented%20in%20Version%203%20along%20with%20a%20dataset%20of%20prompts%2C%20answers%0Aand%20evaluations%20generated%20with%20it%20and%20several%20state-of-the-art%20LLMs.%0ASignificant%20enhancements%20have%20been%20made%20to%20the%20framework%20since%20its%20initial%0Arelease%2C%20including%20an%20updated%20task%20API%20that%20offers%20greater%20flexibility%20in%0Ahandling%20evaluation%20tasks%2C%20revised%20tasks%2C%20and%20extended%20support%20for%20various%20open%0Amodels%20through%20the%20vllm%20library%2C%20among%20other%20improvements.%20A%20comprehensive%0Adataset%20has%20been%20generated%20using%20more%20than%2030%20contemporary%20open%20and%20proprietary%0ALLMs%2C%20enabling%20the%20creation%20of%20exemplary%20model%20cards%20that%20demonstrate%20the%0Amodels%27%20capabilities%20in%20working%20with%20RDF%20and%20SPARQL%2C%20as%20well%20as%20comparing%20their%0Aperformance%20on%20Turtle%20and%20JSON-LD%20RDF%20serialization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13098v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-KG-Bench%25203.0%253A%2520A%2520Compass%2520for%2520SemanticTechnology%2520Capabilities%2520in%2520the%250A%2520%2520Ocean%2520of%2520LLMs%26entry.906535625%3DLars-Peter%2520Meyer%2520and%2520Johannes%2520Frey%2520and%2520Desiree%2520Heim%2520and%2520Felix%2520Brei%2520and%2520Claus%2520Stadler%2520and%2520Kurt%2520Junghanns%2520and%2520Michael%2520Martin%26entry.1292438233%3D%2520%2520Current%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520can%2520assist%2520developing%2520program%2520code%250Abeside%2520many%2520other%2520things%252C%2520but%2520can%2520they%2520support%2520working%2520with%2520Knowledge%2520Graphs%250A%2528KGs%2529%2520as%2520well%253F%2520Which%2520LLM%2520is%2520offering%2520the%2520best%2520capabilities%2520in%2520the%2520field%2520of%250ASemantic%2520Web%2520and%2520Knowledge%2520Graph%2520Engineering%2520%2528KGE%2529%253F%2520Is%2520this%2520possible%2520to%250Adetermine%2520without%2520checking%2520many%2520answers%2520manually%253F%2520The%2520LLM-KG-Bench%2520framework%2520in%250AVersion%25203.0%2520is%2520designed%2520to%2520answer%2520these%2520questions.%2520It%2520consists%2520of%2520an%2520extensible%250Aset%2520of%2520tasks%2520for%2520automated%2520evaluation%2520of%2520LLM%2520answers%2520and%2520covers%2520different%250Aaspects%2520of%2520working%2520with%2520semantic%2520technologies.%2520In%2520this%2520paper%2520the%2520LLM-KG-Bench%250Aframework%2520is%2520presented%2520in%2520Version%25203%2520along%2520with%2520a%2520dataset%2520of%2520prompts%252C%2520answers%250Aand%2520evaluations%2520generated%2520with%2520it%2520and%2520several%2520state-of-the-art%2520LLMs.%250ASignificant%2520enhancements%2520have%2520been%2520made%2520to%2520the%2520framework%2520since%2520its%2520initial%250Arelease%252C%2520including%2520an%2520updated%2520task%2520API%2520that%2520offers%2520greater%2520flexibility%2520in%250Ahandling%2520evaluation%2520tasks%252C%2520revised%2520tasks%252C%2520and%2520extended%2520support%2520for%2520various%2520open%250Amodels%2520through%2520the%2520vllm%2520library%252C%2520among%2520other%2520improvements.%2520A%2520comprehensive%250Adataset%2520has%2520been%2520generated%2520using%2520more%2520than%252030%2520contemporary%2520open%2520and%2520proprietary%250ALLMs%252C%2520enabling%2520the%2520creation%2520of%2520exemplary%2520model%2520cards%2520that%2520demonstrate%2520the%250Amodels%2527%2520capabilities%2520in%2520working%2520with%2520RDF%2520and%2520SPARQL%252C%2520as%2520well%2520as%2520comparing%2520their%250Aperformance%2520on%2520Turtle%2520and%2520JSON-LD%2520RDF%2520serialization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13098v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-KG-Bench%203.0%3A%20A%20Compass%20for%20SemanticTechnology%20Capabilities%20in%20the%0A%20%20Ocean%20of%20LLMs&entry.906535625=Lars-Peter%20Meyer%20and%20Johannes%20Frey%20and%20Desiree%20Heim%20and%20Felix%20Brei%20and%20Claus%20Stadler%20and%20Kurt%20Junghanns%20and%20Michael%20Martin&entry.1292438233=%20%20Current%20Large%20Language%20Models%20%28LLMs%29%20can%20assist%20developing%20program%20code%0Abeside%20many%20other%20things%2C%20but%20can%20they%20support%20working%20with%20Knowledge%20Graphs%0A%28KGs%29%20as%20well%3F%20Which%20LLM%20is%20offering%20the%20best%20capabilities%20in%20the%20field%20of%0ASemantic%20Web%20and%20Knowledge%20Graph%20Engineering%20%28KGE%29%3F%20Is%20this%20possible%20to%0Adetermine%20without%20checking%20many%20answers%20manually%3F%20The%20LLM-KG-Bench%20framework%20in%0AVersion%203.0%20is%20designed%20to%20answer%20these%20questions.%20It%20consists%20of%20an%20extensible%0Aset%20of%20tasks%20for%20automated%20evaluation%20of%20LLM%20answers%20and%20covers%20different%0Aaspects%20of%20working%20with%20semantic%20technologies.%20In%20this%20paper%20the%20LLM-KG-Bench%0Aframework%20is%20presented%20in%20Version%203%20along%20with%20a%20dataset%20of%20prompts%2C%20answers%0Aand%20evaluations%20generated%20with%20it%20and%20several%20state-of-the-art%20LLMs.%0ASignificant%20enhancements%20have%20been%20made%20to%20the%20framework%20since%20its%20initial%0Arelease%2C%20including%20an%20updated%20task%20API%20that%20offers%20greater%20flexibility%20in%0Ahandling%20evaluation%20tasks%2C%20revised%20tasks%2C%20and%20extended%20support%20for%20various%20open%0Amodels%20through%20the%20vllm%20library%2C%20among%20other%20improvements.%20A%20comprehensive%0Adataset%20has%20been%20generated%20using%20more%20than%2030%20contemporary%20open%20and%20proprietary%0ALLMs%2C%20enabling%20the%20creation%20of%20exemplary%20model%20cards%20that%20demonstrate%20the%0Amodels%27%20capabilities%20in%20working%20with%20RDF%20and%20SPARQL%2C%20as%20well%20as%20comparing%20their%0Aperformance%20on%20Turtle%20and%20JSON-LD%20RDF%20serialization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13098v1&entry.124074799=Read"},
{"title": "Quantization-Free Autoregressive Action Transformer", "author": "Ziyad Sheebaelhamd and Michael Tschannen and Michael Muehlebach and Claire Vernade", "abstract": "  Current transformer-based imitation learning approaches introduce discrete\naction representations and train an autoregressive transformer decoder on the\nresulting latent code. However, the initial quantization breaks the continuous\nstructure of the action space thereby limiting the capabilities of the\ngenerative model. We propose a quantization-free method instead that leverages\nGenerative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous\npolicy parametrization for autoregressive transformers. This simplifies the\nimitation learning pipeline while achieving state-of-the-art performance on a\nvariety of popular simulated robotics tasks. We enhance our policy roll-outs by\ncarefully studying sampling algorithms, further improving the results.\n", "link": "http://arxiv.org/abs/2503.14259v2", "date": "2025-05-19", "relevancy": 2.2118, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5847}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.56}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization-Free%20Autoregressive%20Action%20Transformer&body=Title%3A%20Quantization-Free%20Autoregressive%20Action%20Transformer%0AAuthor%3A%20Ziyad%20Sheebaelhamd%20and%20Michael%20Tschannen%20and%20Michael%20Muehlebach%20and%20Claire%20Vernade%0AAbstract%3A%20%20%20Current%20transformer-based%20imitation%20learning%20approaches%20introduce%20discrete%0Aaction%20representations%20and%20train%20an%20autoregressive%20transformer%20decoder%20on%20the%0Aresulting%20latent%20code.%20However%2C%20the%20initial%20quantization%20breaks%20the%20continuous%0Astructure%20of%20the%20action%20space%20thereby%20limiting%20the%20capabilities%20of%20the%0Agenerative%20model.%20We%20propose%20a%20quantization-free%20method%20instead%20that%20leverages%0AGenerative%20Infinite-Vocabulary%20Transformers%20%28GIVT%29%20as%20a%20direct%2C%20continuous%0Apolicy%20parametrization%20for%20autoregressive%20transformers.%20This%20simplifies%20the%0Aimitation%20learning%20pipeline%20while%20achieving%20state-of-the-art%20performance%20on%20a%0Avariety%20of%20popular%20simulated%20robotics%20tasks.%20We%20enhance%20our%20policy%20roll-outs%20by%0Acarefully%20studying%20sampling%20algorithms%2C%20further%20improving%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.14259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization-Free%2520Autoregressive%2520Action%2520Transformer%26entry.906535625%3DZiyad%2520Sheebaelhamd%2520and%2520Michael%2520Tschannen%2520and%2520Michael%2520Muehlebach%2520and%2520Claire%2520Vernade%26entry.1292438233%3D%2520%2520Current%2520transformer-based%2520imitation%2520learning%2520approaches%2520introduce%2520discrete%250Aaction%2520representations%2520and%2520train%2520an%2520autoregressive%2520transformer%2520decoder%2520on%2520the%250Aresulting%2520latent%2520code.%2520However%252C%2520the%2520initial%2520quantization%2520breaks%2520the%2520continuous%250Astructure%2520of%2520the%2520action%2520space%2520thereby%2520limiting%2520the%2520capabilities%2520of%2520the%250Agenerative%2520model.%2520We%2520propose%2520a%2520quantization-free%2520method%2520instead%2520that%2520leverages%250AGenerative%2520Infinite-Vocabulary%2520Transformers%2520%2528GIVT%2529%2520as%2520a%2520direct%252C%2520continuous%250Apolicy%2520parametrization%2520for%2520autoregressive%2520transformers.%2520This%2520simplifies%2520the%250Aimitation%2520learning%2520pipeline%2520while%2520achieving%2520state-of-the-art%2520performance%2520on%2520a%250Avariety%2520of%2520popular%2520simulated%2520robotics%2520tasks.%2520We%2520enhance%2520our%2520policy%2520roll-outs%2520by%250Acarefully%2520studying%2520sampling%2520algorithms%252C%2520further%2520improving%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.14259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization-Free%20Autoregressive%20Action%20Transformer&entry.906535625=Ziyad%20Sheebaelhamd%20and%20Michael%20Tschannen%20and%20Michael%20Muehlebach%20and%20Claire%20Vernade&entry.1292438233=%20%20Current%20transformer-based%20imitation%20learning%20approaches%20introduce%20discrete%0Aaction%20representations%20and%20train%20an%20autoregressive%20transformer%20decoder%20on%20the%0Aresulting%20latent%20code.%20However%2C%20the%20initial%20quantization%20breaks%20the%20continuous%0Astructure%20of%20the%20action%20space%20thereby%20limiting%20the%20capabilities%20of%20the%0Agenerative%20model.%20We%20propose%20a%20quantization-free%20method%20instead%20that%20leverages%0AGenerative%20Infinite-Vocabulary%20Transformers%20%28GIVT%29%20as%20a%20direct%2C%20continuous%0Apolicy%20parametrization%20for%20autoregressive%20transformers.%20This%20simplifies%20the%0Aimitation%20learning%20pipeline%20while%20achieving%20state-of-the-art%20performance%20on%20a%0Avariety%20of%20popular%20simulated%20robotics%20tasks.%20We%20enhance%20our%20policy%20roll-outs%20by%0Acarefully%20studying%20sampling%20algorithms%2C%20further%20improving%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.14259v2&entry.124074799=Read"},
{"title": "Physics of Language Models: Part 1, Learning Hierarchical Language\n  Structures", "author": "Zeyuan Allen-Zhu and Yuanzhi Li", "abstract": "  Transformer-based language models are effective but complex, and\nunderstanding their inner workings and reasoning mechanisms is a significant\nchallenge. Previous research has primarily explored how these models handle\nsimple tasks like name copying or selection, and we extend this by\ninvestigating how these models perform recursive language structure reasoning\ndefined by context-free grammars (CFGs). We introduce a family of synthetic\nCFGs that produce hierarchical rules, capable of generating lengthy sentences\n(e.g., hundreds of tokens) that are locally ambiguous and require dynamic\nprogramming to parse. Despite this complexity, we demonstrate that generative\nmodels like GPT can accurately learn and reason over CFG-defined hierarchies\nand generate sentences based on it. We explore the model's internals, revealing\nthat its hidden states precisely capture the structure of CFGs, and its\nattention patterns resemble the information passing in a dynamic programming\nalgorithm.\n  This paper also presents several corollaries, including showing why absolute\npositional embeddings is inferior to relative and rotary embeddings; uniform\nattention alone is surprisingly effective (motivating our follow-up work on\nCanon layers); encoder-only models (e.g., BERT, DeBERTa) struggle with deep\nstructure reasoning on CFGs compared to autoregressive models (e.g., GPT); and\ninjecting structural or syntactic noise into pretraining data markedly improves\nrobustness to corrupted language prompts.\n", "link": "http://arxiv.org/abs/2305.13673v4", "date": "2025-05-19", "relevancy": 2.2091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5547}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics%20of%20Language%20Models%3A%20Part%201%2C%20Learning%20Hierarchical%20Language%0A%20%20Structures&body=Title%3A%20Physics%20of%20Language%20Models%3A%20Part%201%2C%20Learning%20Hierarchical%20Language%0A%20%20Structures%0AAuthor%3A%20Zeyuan%20Allen-Zhu%20and%20Yuanzhi%20Li%0AAbstract%3A%20%20%20Transformer-based%20language%20models%20are%20effective%20but%20complex%2C%20and%0Aunderstanding%20their%20inner%20workings%20and%20reasoning%20mechanisms%20is%20a%20significant%0Achallenge.%20Previous%20research%20has%20primarily%20explored%20how%20these%20models%20handle%0Asimple%20tasks%20like%20name%20copying%20or%20selection%2C%20and%20we%20extend%20this%20by%0Ainvestigating%20how%20these%20models%20perform%20recursive%20language%20structure%20reasoning%0Adefined%20by%20context-free%20grammars%20%28CFGs%29.%20We%20introduce%20a%20family%20of%20synthetic%0ACFGs%20that%20produce%20hierarchical%20rules%2C%20capable%20of%20generating%20lengthy%20sentences%0A%28e.g.%2C%20hundreds%20of%20tokens%29%20that%20are%20locally%20ambiguous%20and%20require%20dynamic%0Aprogramming%20to%20parse.%20Despite%20this%20complexity%2C%20we%20demonstrate%20that%20generative%0Amodels%20like%20GPT%20can%20accurately%20learn%20and%20reason%20over%20CFG-defined%20hierarchies%0Aand%20generate%20sentences%20based%20on%20it.%20We%20explore%20the%20model%27s%20internals%2C%20revealing%0Athat%20its%20hidden%20states%20precisely%20capture%20the%20structure%20of%20CFGs%2C%20and%20its%0Aattention%20patterns%20resemble%20the%20information%20passing%20in%20a%20dynamic%20programming%0Aalgorithm.%0A%20%20This%20paper%20also%20presents%20several%20corollaries%2C%20including%20showing%20why%20absolute%0Apositional%20embeddings%20is%20inferior%20to%20relative%20and%20rotary%20embeddings%3B%20uniform%0Aattention%20alone%20is%20surprisingly%20effective%20%28motivating%20our%20follow-up%20work%20on%0ACanon%20layers%29%3B%20encoder-only%20models%20%28e.g.%2C%20BERT%2C%20DeBERTa%29%20struggle%20with%20deep%0Astructure%20reasoning%20on%20CFGs%20compared%20to%20autoregressive%20models%20%28e.g.%2C%20GPT%29%3B%20and%0Ainjecting%20structural%20or%20syntactic%20noise%20into%20pretraining%20data%20markedly%20improves%0Arobustness%20to%20corrupted%20language%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.13673v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics%2520of%2520Language%2520Models%253A%2520Part%25201%252C%2520Learning%2520Hierarchical%2520Language%250A%2520%2520Structures%26entry.906535625%3DZeyuan%2520Allen-Zhu%2520and%2520Yuanzhi%2520Li%26entry.1292438233%3D%2520%2520Transformer-based%2520language%2520models%2520are%2520effective%2520but%2520complex%252C%2520and%250Aunderstanding%2520their%2520inner%2520workings%2520and%2520reasoning%2520mechanisms%2520is%2520a%2520significant%250Achallenge.%2520Previous%2520research%2520has%2520primarily%2520explored%2520how%2520these%2520models%2520handle%250Asimple%2520tasks%2520like%2520name%2520copying%2520or%2520selection%252C%2520and%2520we%2520extend%2520this%2520by%250Ainvestigating%2520how%2520these%2520models%2520perform%2520recursive%2520language%2520structure%2520reasoning%250Adefined%2520by%2520context-free%2520grammars%2520%2528CFGs%2529.%2520We%2520introduce%2520a%2520family%2520of%2520synthetic%250ACFGs%2520that%2520produce%2520hierarchical%2520rules%252C%2520capable%2520of%2520generating%2520lengthy%2520sentences%250A%2528e.g.%252C%2520hundreds%2520of%2520tokens%2529%2520that%2520are%2520locally%2520ambiguous%2520and%2520require%2520dynamic%250Aprogramming%2520to%2520parse.%2520Despite%2520this%2520complexity%252C%2520we%2520demonstrate%2520that%2520generative%250Amodels%2520like%2520GPT%2520can%2520accurately%2520learn%2520and%2520reason%2520over%2520CFG-defined%2520hierarchies%250Aand%2520generate%2520sentences%2520based%2520on%2520it.%2520We%2520explore%2520the%2520model%2527s%2520internals%252C%2520revealing%250Athat%2520its%2520hidden%2520states%2520precisely%2520capture%2520the%2520structure%2520of%2520CFGs%252C%2520and%2520its%250Aattention%2520patterns%2520resemble%2520the%2520information%2520passing%2520in%2520a%2520dynamic%2520programming%250Aalgorithm.%250A%2520%2520This%2520paper%2520also%2520presents%2520several%2520corollaries%252C%2520including%2520showing%2520why%2520absolute%250Apositional%2520embeddings%2520is%2520inferior%2520to%2520relative%2520and%2520rotary%2520embeddings%253B%2520uniform%250Aattention%2520alone%2520is%2520surprisingly%2520effective%2520%2528motivating%2520our%2520follow-up%2520work%2520on%250ACanon%2520layers%2529%253B%2520encoder-only%2520models%2520%2528e.g.%252C%2520BERT%252C%2520DeBERTa%2529%2520struggle%2520with%2520deep%250Astructure%2520reasoning%2520on%2520CFGs%2520compared%2520to%2520autoregressive%2520models%2520%2528e.g.%252C%2520GPT%2529%253B%2520and%250Ainjecting%2520structural%2520or%2520syntactic%2520noise%2520into%2520pretraining%2520data%2520markedly%2520improves%250Arobustness%2520to%2520corrupted%2520language%2520prompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.13673v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics%20of%20Language%20Models%3A%20Part%201%2C%20Learning%20Hierarchical%20Language%0A%20%20Structures&entry.906535625=Zeyuan%20Allen-Zhu%20and%20Yuanzhi%20Li&entry.1292438233=%20%20Transformer-based%20language%20models%20are%20effective%20but%20complex%2C%20and%0Aunderstanding%20their%20inner%20workings%20and%20reasoning%20mechanisms%20is%20a%20significant%0Achallenge.%20Previous%20research%20has%20primarily%20explored%20how%20these%20models%20handle%0Asimple%20tasks%20like%20name%20copying%20or%20selection%2C%20and%20we%20extend%20this%20by%0Ainvestigating%20how%20these%20models%20perform%20recursive%20language%20structure%20reasoning%0Adefined%20by%20context-free%20grammars%20%28CFGs%29.%20We%20introduce%20a%20family%20of%20synthetic%0ACFGs%20that%20produce%20hierarchical%20rules%2C%20capable%20of%20generating%20lengthy%20sentences%0A%28e.g.%2C%20hundreds%20of%20tokens%29%20that%20are%20locally%20ambiguous%20and%20require%20dynamic%0Aprogramming%20to%20parse.%20Despite%20this%20complexity%2C%20we%20demonstrate%20that%20generative%0Amodels%20like%20GPT%20can%20accurately%20learn%20and%20reason%20over%20CFG-defined%20hierarchies%0Aand%20generate%20sentences%20based%20on%20it.%20We%20explore%20the%20model%27s%20internals%2C%20revealing%0Athat%20its%20hidden%20states%20precisely%20capture%20the%20structure%20of%20CFGs%2C%20and%20its%0Aattention%20patterns%20resemble%20the%20information%20passing%20in%20a%20dynamic%20programming%0Aalgorithm.%0A%20%20This%20paper%20also%20presents%20several%20corollaries%2C%20including%20showing%20why%20absolute%0Apositional%20embeddings%20is%20inferior%20to%20relative%20and%20rotary%20embeddings%3B%20uniform%0Aattention%20alone%20is%20surprisingly%20effective%20%28motivating%20our%20follow-up%20work%20on%0ACanon%20layers%29%3B%20encoder-only%20models%20%28e.g.%2C%20BERT%2C%20DeBERTa%29%20struggle%20with%20deep%0Astructure%20reasoning%20on%20CFGs%20compared%20to%20autoregressive%20models%20%28e.g.%2C%20GPT%29%3B%20and%0Ainjecting%20structural%20or%20syntactic%20noise%20into%20pretraining%20data%20markedly%20improves%0Arobustness%20to%20corrupted%20language%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13673v4&entry.124074799=Read"},
{"title": "Evaluating Mathematical Reasoning Across Large Language Models: A\n  Fine-Grained Approach", "author": "Afrar Jahin and Arif Hassan Zidan and Wei Zhang and Yu Bao and Tianming Liu", "abstract": "  With the rapid advancement of Artificial Intelligence (AI), Large Language\nModels (LLMs) have significantly impacted a wide array of domains, including\nhealthcare, engineering, science, education, and mathematical reasoning. Among\nthese, mathematical reasoning remains a particularly challenging capability,\noften requiring multi-step logic and abstract generalization. While prior work\nhas explored LLM performance on reasoning tasks, comprehensive evaluations that\nspan both depth and breadth across model families remain limited. In this\nstudy, we present a systematic evaluation of mathematical reasoning abilities\nacross eight leading LLMs, including two recent DeepSeek models, using three\nindependent benchmark datasets. Our analyses reveal several key findings: (1)\nDeepSeek-R1 performs competitively with o1 across most domains and achieves the\nhighest accuracy on the MMLU Formal Logic benchmark; (2) distilled variants,\nsuch as DeepSeek-1.5B, exhibit substantial performance degradation; and (3)\nGemini 2.0 Flash achieves the lowest response latency. Beyond quantitative\nmetrics, we explore how architectural choices, training paradigms, and\noptimization strategies contribute to variation in reasoning performance. These\nfindings provide new insights into the capabilities and limitations of current\nLLMs in mathematical domains, and offer guidance for the development of future\nmodels better aligned with rigorous reasoning demands.\n", "link": "http://arxiv.org/abs/2503.10573v2", "date": "2025-05-19", "relevancy": 2.2019, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5608}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Mathematical%20Reasoning%20Across%20Large%20Language%20Models%3A%20A%0A%20%20Fine-Grained%20Approach&body=Title%3A%20Evaluating%20Mathematical%20Reasoning%20Across%20Large%20Language%20Models%3A%20A%0A%20%20Fine-Grained%20Approach%0AAuthor%3A%20Afrar%20Jahin%20and%20Arif%20Hassan%20Zidan%20and%20Wei%20Zhang%20and%20Yu%20Bao%20and%20Tianming%20Liu%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20Artificial%20Intelligence%20%28AI%29%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20significantly%20impacted%20a%20wide%20array%20of%20domains%2C%20including%0Ahealthcare%2C%20engineering%2C%20science%2C%20education%2C%20and%20mathematical%20reasoning.%20Among%0Athese%2C%20mathematical%20reasoning%20remains%20a%20particularly%20challenging%20capability%2C%0Aoften%20requiring%20multi-step%20logic%20and%20abstract%20generalization.%20While%20prior%20work%0Ahas%20explored%20LLM%20performance%20on%20reasoning%20tasks%2C%20comprehensive%20evaluations%20that%0Aspan%20both%20depth%20and%20breadth%20across%20model%20families%20remain%20limited.%20In%20this%0Astudy%2C%20we%20present%20a%20systematic%20evaluation%20of%20mathematical%20reasoning%20abilities%0Aacross%20eight%20leading%20LLMs%2C%20including%20two%20recent%20DeepSeek%20models%2C%20using%20three%0Aindependent%20benchmark%20datasets.%20Our%20analyses%20reveal%20several%20key%20findings%3A%20%281%29%0ADeepSeek-R1%20performs%20competitively%20with%20o1%20across%20most%20domains%20and%20achieves%20the%0Ahighest%20accuracy%20on%20the%20MMLU%20Formal%20Logic%20benchmark%3B%20%282%29%20distilled%20variants%2C%0Asuch%20as%20DeepSeek-1.5B%2C%20exhibit%20substantial%20performance%20degradation%3B%20and%20%283%29%0AGemini%202.0%20Flash%20achieves%20the%20lowest%20response%20latency.%20Beyond%20quantitative%0Ametrics%2C%20we%20explore%20how%20architectural%20choices%2C%20training%20paradigms%2C%20and%0Aoptimization%20strategies%20contribute%20to%20variation%20in%20reasoning%20performance.%20These%0Afindings%20provide%20new%20insights%20into%20the%20capabilities%20and%20limitations%20of%20current%0ALLMs%20in%20mathematical%20domains%2C%20and%20offer%20guidance%20for%20the%20development%20of%20future%0Amodels%20better%20aligned%20with%20rigorous%20reasoning%20demands.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Mathematical%2520Reasoning%2520Across%2520Large%2520Language%2520Models%253A%2520A%250A%2520%2520Fine-Grained%2520Approach%26entry.906535625%3DAfrar%2520Jahin%2520and%2520Arif%2520Hassan%2520Zidan%2520and%2520Wei%2520Zhang%2520and%2520Yu%2520Bao%2520and%2520Tianming%2520Liu%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%252C%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520significantly%2520impacted%2520a%2520wide%2520array%2520of%2520domains%252C%2520including%250Ahealthcare%252C%2520engineering%252C%2520science%252C%2520education%252C%2520and%2520mathematical%2520reasoning.%2520Among%250Athese%252C%2520mathematical%2520reasoning%2520remains%2520a%2520particularly%2520challenging%2520capability%252C%250Aoften%2520requiring%2520multi-step%2520logic%2520and%2520abstract%2520generalization.%2520While%2520prior%2520work%250Ahas%2520explored%2520LLM%2520performance%2520on%2520reasoning%2520tasks%252C%2520comprehensive%2520evaluations%2520that%250Aspan%2520both%2520depth%2520and%2520breadth%2520across%2520model%2520families%2520remain%2520limited.%2520In%2520this%250Astudy%252C%2520we%2520present%2520a%2520systematic%2520evaluation%2520of%2520mathematical%2520reasoning%2520abilities%250Aacross%2520eight%2520leading%2520LLMs%252C%2520including%2520two%2520recent%2520DeepSeek%2520models%252C%2520using%2520three%250Aindependent%2520benchmark%2520datasets.%2520Our%2520analyses%2520reveal%2520several%2520key%2520findings%253A%2520%25281%2529%250ADeepSeek-R1%2520performs%2520competitively%2520with%2520o1%2520across%2520most%2520domains%2520and%2520achieves%2520the%250Ahighest%2520accuracy%2520on%2520the%2520MMLU%2520Formal%2520Logic%2520benchmark%253B%2520%25282%2529%2520distilled%2520variants%252C%250Asuch%2520as%2520DeepSeek-1.5B%252C%2520exhibit%2520substantial%2520performance%2520degradation%253B%2520and%2520%25283%2529%250AGemini%25202.0%2520Flash%2520achieves%2520the%2520lowest%2520response%2520latency.%2520Beyond%2520quantitative%250Ametrics%252C%2520we%2520explore%2520how%2520architectural%2520choices%252C%2520training%2520paradigms%252C%2520and%250Aoptimization%2520strategies%2520contribute%2520to%2520variation%2520in%2520reasoning%2520performance.%2520These%250Afindings%2520provide%2520new%2520insights%2520into%2520the%2520capabilities%2520and%2520limitations%2520of%2520current%250ALLMs%2520in%2520mathematical%2520domains%252C%2520and%2520offer%2520guidance%2520for%2520the%2520development%2520of%2520future%250Amodels%2520better%2520aligned%2520with%2520rigorous%2520reasoning%2520demands.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Mathematical%20Reasoning%20Across%20Large%20Language%20Models%3A%20A%0A%20%20Fine-Grained%20Approach&entry.906535625=Afrar%20Jahin%20and%20Arif%20Hassan%20Zidan%20and%20Wei%20Zhang%20and%20Yu%20Bao%20and%20Tianming%20Liu&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20Artificial%20Intelligence%20%28AI%29%2C%20Large%20Language%0AModels%20%28LLMs%29%20have%20significantly%20impacted%20a%20wide%20array%20of%20domains%2C%20including%0Ahealthcare%2C%20engineering%2C%20science%2C%20education%2C%20and%20mathematical%20reasoning.%20Among%0Athese%2C%20mathematical%20reasoning%20remains%20a%20particularly%20challenging%20capability%2C%0Aoften%20requiring%20multi-step%20logic%20and%20abstract%20generalization.%20While%20prior%20work%0Ahas%20explored%20LLM%20performance%20on%20reasoning%20tasks%2C%20comprehensive%20evaluations%20that%0Aspan%20both%20depth%20and%20breadth%20across%20model%20families%20remain%20limited.%20In%20this%0Astudy%2C%20we%20present%20a%20systematic%20evaluation%20of%20mathematical%20reasoning%20abilities%0Aacross%20eight%20leading%20LLMs%2C%20including%20two%20recent%20DeepSeek%20models%2C%20using%20three%0Aindependent%20benchmark%20datasets.%20Our%20analyses%20reveal%20several%20key%20findings%3A%20%281%29%0ADeepSeek-R1%20performs%20competitively%20with%20o1%20across%20most%20domains%20and%20achieves%20the%0Ahighest%20accuracy%20on%20the%20MMLU%20Formal%20Logic%20benchmark%3B%20%282%29%20distilled%20variants%2C%0Asuch%20as%20DeepSeek-1.5B%2C%20exhibit%20substantial%20performance%20degradation%3B%20and%20%283%29%0AGemini%202.0%20Flash%20achieves%20the%20lowest%20response%20latency.%20Beyond%20quantitative%0Ametrics%2C%20we%20explore%20how%20architectural%20choices%2C%20training%20paradigms%2C%20and%0Aoptimization%20strategies%20contribute%20to%20variation%20in%20reasoning%20performance.%20These%0Afindings%20provide%20new%20insights%20into%20the%20capabilities%20and%20limitations%20of%20current%0ALLMs%20in%20mathematical%20domains%2C%20and%20offer%20guidance%20for%20the%20development%20of%20future%0Amodels%20better%20aligned%20with%20rigorous%20reasoning%20demands.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10573v2&entry.124074799=Read"},
{"title": "What are the Essential Factors in Crafting Effective Long Context\n  Multi-Hop Instruction Datasets? Insights and Best Practices", "author": "Zhi Chen and Qiguang Chen and Libo Qin and Qipeng Guo and Haijun Lv and Yicheng Zou and Wanxiang Che and Hang Yan and Kai Chen and Dahua Lin", "abstract": "  Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction,\nquestion answering, and complex planning scenarios. In order to achieve success\nin long context tasks, a large amount of work has been done to enhance the long\ncontext capabilities of the model through synthetic data. Existing methods\ntypically utilize the Self-Instruct framework to generate instruction tuning\ndata for better long context capability improvement. However, our preliminary\nexperiments indicate that less than 35% of generated samples are multi-hop, and\nmore than 40% exhibit poor quality, limiting comprehensive understanding and\nfurther research. To improve the quality of synthetic data, we propose the\nMulti-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a\nQuality Verification Agent, a Single-hop Question Generation Agent, a Multiple\nQuestion Sampling Strategy, and a Multi-hop Question Merger Agent. This\nframework improves the data quality, with the proportion of high-quality,\nmulti-hop, and diverse data exceeding 85%. Furthermore, we systematically\ninvestigate strategies for document selection, question merging, and validation\ntechniques through extensive experiments across various models. Our findings\nshow that our synthetic high-quality long-context instruction data\nsignificantly enhances model performance, even surpassing models trained on\nlarger amounts of human-annotated data. Our code is available at:\nhttps://github.com/WowCZ/LongMIT.\n", "link": "http://arxiv.org/abs/2409.01893v2", "date": "2025-05-19", "relevancy": 2.2011, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5578}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20are%20the%20Essential%20Factors%20in%20Crafting%20Effective%20Long%20Context%0A%20%20Multi-Hop%20Instruction%20Datasets%3F%20Insights%20and%20Best%20Practices&body=Title%3A%20What%20are%20the%20Essential%20Factors%20in%20Crafting%20Effective%20Long%20Context%0A%20%20Multi-Hop%20Instruction%20Datasets%3F%20Insights%20and%20Best%20Practices%0AAuthor%3A%20Zhi%20Chen%20and%20Qiguang%20Chen%20and%20Libo%20Qin%20and%20Qipeng%20Guo%20and%20Haijun%20Lv%20and%20Yicheng%20Zou%20and%20Wanxiang%20Che%20and%20Hang%20Yan%20and%20Kai%20Chen%20and%20Dahua%20Lin%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20with%20extended%20context%0Awindows%20have%20significantly%20improved%20tasks%20such%20as%20information%20extraction%2C%0Aquestion%20answering%2C%20and%20complex%20planning%20scenarios.%20In%20order%20to%20achieve%20success%0Ain%20long%20context%20tasks%2C%20a%20large%20amount%20of%20work%20has%20been%20done%20to%20enhance%20the%20long%0Acontext%20capabilities%20of%20the%20model%20through%20synthetic%20data.%20Existing%20methods%0Atypically%20utilize%20the%20Self-Instruct%20framework%20to%20generate%20instruction%20tuning%0Adata%20for%20better%20long%20context%20capability%20improvement.%20However%2C%20our%20preliminary%0Aexperiments%20indicate%20that%20less%20than%2035%25%20of%20generated%20samples%20are%20multi-hop%2C%20and%0Amore%20than%2040%25%20exhibit%20poor%20quality%2C%20limiting%20comprehensive%20understanding%20and%0Afurther%20research.%20To%20improve%20the%20quality%20of%20synthetic%20data%2C%20we%20propose%20the%0AMulti-agent%20Interactive%20Multi-hop%20Generation%20%28MIMG%29%20framework%2C%20incorporating%20a%0AQuality%20Verification%20Agent%2C%20a%20Single-hop%20Question%20Generation%20Agent%2C%20a%20Multiple%0AQuestion%20Sampling%20Strategy%2C%20and%20a%20Multi-hop%20Question%20Merger%20Agent.%20This%0Aframework%20improves%20the%20data%20quality%2C%20with%20the%20proportion%20of%20high-quality%2C%0Amulti-hop%2C%20and%20diverse%20data%20exceeding%2085%25.%20Furthermore%2C%20we%20systematically%0Ainvestigate%20strategies%20for%20document%20selection%2C%20question%20merging%2C%20and%20validation%0Atechniques%20through%20extensive%20experiments%20across%20various%20models.%20Our%20findings%0Ashow%20that%20our%20synthetic%20high-quality%20long-context%20instruction%20data%0Asignificantly%20enhances%20model%20performance%2C%20even%20surpassing%20models%20trained%20on%0Alarger%20amounts%20of%20human-annotated%20data.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/WowCZ/LongMIT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01893v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520are%2520the%2520Essential%2520Factors%2520in%2520Crafting%2520Effective%2520Long%2520Context%250A%2520%2520Multi-Hop%2520Instruction%2520Datasets%253F%2520Insights%2520and%2520Best%2520Practices%26entry.906535625%3DZhi%2520Chen%2520and%2520Qiguang%2520Chen%2520and%2520Libo%2520Qin%2520and%2520Qipeng%2520Guo%2520and%2520Haijun%2520Lv%2520and%2520Yicheng%2520Zou%2520and%2520Wanxiang%2520Che%2520and%2520Hang%2520Yan%2520and%2520Kai%2520Chen%2520and%2520Dahua%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520extended%2520context%250Awindows%2520have%2520significantly%2520improved%2520tasks%2520such%2520as%2520information%2520extraction%252C%250Aquestion%2520answering%252C%2520and%2520complex%2520planning%2520scenarios.%2520In%2520order%2520to%2520achieve%2520success%250Ain%2520long%2520context%2520tasks%252C%2520a%2520large%2520amount%2520of%2520work%2520has%2520been%2520done%2520to%2520enhance%2520the%2520long%250Acontext%2520capabilities%2520of%2520the%2520model%2520through%2520synthetic%2520data.%2520Existing%2520methods%250Atypically%2520utilize%2520the%2520Self-Instruct%2520framework%2520to%2520generate%2520instruction%2520tuning%250Adata%2520for%2520better%2520long%2520context%2520capability%2520improvement.%2520However%252C%2520our%2520preliminary%250Aexperiments%2520indicate%2520that%2520less%2520than%252035%2525%2520of%2520generated%2520samples%2520are%2520multi-hop%252C%2520and%250Amore%2520than%252040%2525%2520exhibit%2520poor%2520quality%252C%2520limiting%2520comprehensive%2520understanding%2520and%250Afurther%2520research.%2520To%2520improve%2520the%2520quality%2520of%2520synthetic%2520data%252C%2520we%2520propose%2520the%250AMulti-agent%2520Interactive%2520Multi-hop%2520Generation%2520%2528MIMG%2529%2520framework%252C%2520incorporating%2520a%250AQuality%2520Verification%2520Agent%252C%2520a%2520Single-hop%2520Question%2520Generation%2520Agent%252C%2520a%2520Multiple%250AQuestion%2520Sampling%2520Strategy%252C%2520and%2520a%2520Multi-hop%2520Question%2520Merger%2520Agent.%2520This%250Aframework%2520improves%2520the%2520data%2520quality%252C%2520with%2520the%2520proportion%2520of%2520high-quality%252C%250Amulti-hop%252C%2520and%2520diverse%2520data%2520exceeding%252085%2525.%2520Furthermore%252C%2520we%2520systematically%250Ainvestigate%2520strategies%2520for%2520document%2520selection%252C%2520question%2520merging%252C%2520and%2520validation%250Atechniques%2520through%2520extensive%2520experiments%2520across%2520various%2520models.%2520Our%2520findings%250Ashow%2520that%2520our%2520synthetic%2520high-quality%2520long-context%2520instruction%2520data%250Asignificantly%2520enhances%2520model%2520performance%252C%2520even%2520surpassing%2520models%2520trained%2520on%250Alarger%2520amounts%2520of%2520human-annotated%2520data.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/WowCZ/LongMIT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01893v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20are%20the%20Essential%20Factors%20in%20Crafting%20Effective%20Long%20Context%0A%20%20Multi-Hop%20Instruction%20Datasets%3F%20Insights%20and%20Best%20Practices&entry.906535625=Zhi%20Chen%20and%20Qiguang%20Chen%20and%20Libo%20Qin%20and%20Qipeng%20Guo%20and%20Haijun%20Lv%20and%20Yicheng%20Zou%20and%20Wanxiang%20Che%20and%20Hang%20Yan%20and%20Kai%20Chen%20and%20Dahua%20Lin&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20with%20extended%20context%0Awindows%20have%20significantly%20improved%20tasks%20such%20as%20information%20extraction%2C%0Aquestion%20answering%2C%20and%20complex%20planning%20scenarios.%20In%20order%20to%20achieve%20success%0Ain%20long%20context%20tasks%2C%20a%20large%20amount%20of%20work%20has%20been%20done%20to%20enhance%20the%20long%0Acontext%20capabilities%20of%20the%20model%20through%20synthetic%20data.%20Existing%20methods%0Atypically%20utilize%20the%20Self-Instruct%20framework%20to%20generate%20instruction%20tuning%0Adata%20for%20better%20long%20context%20capability%20improvement.%20However%2C%20our%20preliminary%0Aexperiments%20indicate%20that%20less%20than%2035%25%20of%20generated%20samples%20are%20multi-hop%2C%20and%0Amore%20than%2040%25%20exhibit%20poor%20quality%2C%20limiting%20comprehensive%20understanding%20and%0Afurther%20research.%20To%20improve%20the%20quality%20of%20synthetic%20data%2C%20we%20propose%20the%0AMulti-agent%20Interactive%20Multi-hop%20Generation%20%28MIMG%29%20framework%2C%20incorporating%20a%0AQuality%20Verification%20Agent%2C%20a%20Single-hop%20Question%20Generation%20Agent%2C%20a%20Multiple%0AQuestion%20Sampling%20Strategy%2C%20and%20a%20Multi-hop%20Question%20Merger%20Agent.%20This%0Aframework%20improves%20the%20data%20quality%2C%20with%20the%20proportion%20of%20high-quality%2C%0Amulti-hop%2C%20and%20diverse%20data%20exceeding%2085%25.%20Furthermore%2C%20we%20systematically%0Ainvestigate%20strategies%20for%20document%20selection%2C%20question%20merging%2C%20and%20validation%0Atechniques%20through%20extensive%20experiments%20across%20various%20models.%20Our%20findings%0Ashow%20that%20our%20synthetic%20high-quality%20long-context%20instruction%20data%0Asignificantly%20enhances%20model%20performance%2C%20even%20surpassing%20models%20trained%20on%0Alarger%20amounts%20of%20human-annotated%20data.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/WowCZ/LongMIT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01893v2&entry.124074799=Read"},
{"title": "VisDiff: SDF-Guided Polygon Generation for Visibility Reconstruction and\n  Recognition", "author": "Rahul Moorthy and Volkan Isler", "abstract": "  The ability to capture rich representations of combinatorial structures has\nenabled the application of machine learning to tasks such as analysis and\ngeneration of floorplans, terrains, images, and animations. Recent work has\nprimarily focused on understanding structures with well-defined features,\nneighborhoods, or underlying distance metrics, while those lacking such\ncharacteristics remain largely unstudied. Examples of these combinatorial\nstructures can be found in polygons, where a small change in the vertex\nlocations causes a significant rearrangement of the combinatorial structure,\nexpressed as a visibility or triangulation graphs. Current representation\nlearning approaches fail to capture structures without well-defined features\nand distance metrics. In this paper, we study the open problem of Visibility\nReconstruction: Given a visibility graph $G$, construct a polygon $P$ whose\nvisibility graph is $G$. We introduce VisDiff, a novel diffusion-based approach\nto generate polygon $P$ from the input visibility graph $G$. The main novelty\nof our approach is that, rather than generating the polygon's vertex set\ndirectly, we first estimate the signed distance function (SDF) associated with\nthe polygon. The SDF is then used to extract the vertex location representing\nthe final polygon. We show that going through the SDF allows VisDiff to learn\nthe visibility relationship much more effectively than generating vertex\nlocations directly. In order to train VisDiff, we create a carefully curated\ndataset. We use this dataset to benchmark our method and achieve 26%\nimprovement in F1-Score over standard methods as well as state of the art\napproaches.\n", "link": "http://arxiv.org/abs/2410.05530v2", "date": "2025-05-19", "relevancy": 2.1961, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5496}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5496}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisDiff%3A%20SDF-Guided%20Polygon%20Generation%20for%20Visibility%20Reconstruction%20and%0A%20%20Recognition&body=Title%3A%20VisDiff%3A%20SDF-Guided%20Polygon%20Generation%20for%20Visibility%20Reconstruction%20and%0A%20%20Recognition%0AAuthor%3A%20Rahul%20Moorthy%20and%20Volkan%20Isler%0AAbstract%3A%20%20%20The%20ability%20to%20capture%20rich%20representations%20of%20combinatorial%20structures%20has%0Aenabled%20the%20application%20of%20machine%20learning%20to%20tasks%20such%20as%20analysis%20and%0Ageneration%20of%20floorplans%2C%20terrains%2C%20images%2C%20and%20animations.%20Recent%20work%20has%0Aprimarily%20focused%20on%20understanding%20structures%20with%20well-defined%20features%2C%0Aneighborhoods%2C%20or%20underlying%20distance%20metrics%2C%20while%20those%20lacking%20such%0Acharacteristics%20remain%20largely%20unstudied.%20Examples%20of%20these%20combinatorial%0Astructures%20can%20be%20found%20in%20polygons%2C%20where%20a%20small%20change%20in%20the%20vertex%0Alocations%20causes%20a%20significant%20rearrangement%20of%20the%20combinatorial%20structure%2C%0Aexpressed%20as%20a%20visibility%20or%20triangulation%20graphs.%20Current%20representation%0Alearning%20approaches%20fail%20to%20capture%20structures%20without%20well-defined%20features%0Aand%20distance%20metrics.%20In%20this%20paper%2C%20we%20study%20the%20open%20problem%20of%20Visibility%0AReconstruction%3A%20Given%20a%20visibility%20graph%20%24G%24%2C%20construct%20a%20polygon%20%24P%24%20whose%0Avisibility%20graph%20is%20%24G%24.%20We%20introduce%20VisDiff%2C%20a%20novel%20diffusion-based%20approach%0Ato%20generate%20polygon%20%24P%24%20from%20the%20input%20visibility%20graph%20%24G%24.%20The%20main%20novelty%0Aof%20our%20approach%20is%20that%2C%20rather%20than%20generating%20the%20polygon%27s%20vertex%20set%0Adirectly%2C%20we%20first%20estimate%20the%20signed%20distance%20function%20%28SDF%29%20associated%20with%0Athe%20polygon.%20The%20SDF%20is%20then%20used%20to%20extract%20the%20vertex%20location%20representing%0Athe%20final%20polygon.%20We%20show%20that%20going%20through%20the%20SDF%20allows%20VisDiff%20to%20learn%0Athe%20visibility%20relationship%20much%20more%20effectively%20than%20generating%20vertex%0Alocations%20directly.%20In%20order%20to%20train%20VisDiff%2C%20we%20create%20a%20carefully%20curated%0Adataset.%20We%20use%20this%20dataset%20to%20benchmark%20our%20method%20and%20achieve%2026%25%0Aimprovement%20in%20F1-Score%20over%20standard%20methods%20as%20well%20as%20state%20of%20the%20art%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05530v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisDiff%253A%2520SDF-Guided%2520Polygon%2520Generation%2520for%2520Visibility%2520Reconstruction%2520and%250A%2520%2520Recognition%26entry.906535625%3DRahul%2520Moorthy%2520and%2520Volkan%2520Isler%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520capture%2520rich%2520representations%2520of%2520combinatorial%2520structures%2520has%250Aenabled%2520the%2520application%2520of%2520machine%2520learning%2520to%2520tasks%2520such%2520as%2520analysis%2520and%250Ageneration%2520of%2520floorplans%252C%2520terrains%252C%2520images%252C%2520and%2520animations.%2520Recent%2520work%2520has%250Aprimarily%2520focused%2520on%2520understanding%2520structures%2520with%2520well-defined%2520features%252C%250Aneighborhoods%252C%2520or%2520underlying%2520distance%2520metrics%252C%2520while%2520those%2520lacking%2520such%250Acharacteristics%2520remain%2520largely%2520unstudied.%2520Examples%2520of%2520these%2520combinatorial%250Astructures%2520can%2520be%2520found%2520in%2520polygons%252C%2520where%2520a%2520small%2520change%2520in%2520the%2520vertex%250Alocations%2520causes%2520a%2520significant%2520rearrangement%2520of%2520the%2520combinatorial%2520structure%252C%250Aexpressed%2520as%2520a%2520visibility%2520or%2520triangulation%2520graphs.%2520Current%2520representation%250Alearning%2520approaches%2520fail%2520to%2520capture%2520structures%2520without%2520well-defined%2520features%250Aand%2520distance%2520metrics.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520open%2520problem%2520of%2520Visibility%250AReconstruction%253A%2520Given%2520a%2520visibility%2520graph%2520%2524G%2524%252C%2520construct%2520a%2520polygon%2520%2524P%2524%2520whose%250Avisibility%2520graph%2520is%2520%2524G%2524.%2520We%2520introduce%2520VisDiff%252C%2520a%2520novel%2520diffusion-based%2520approach%250Ato%2520generate%2520polygon%2520%2524P%2524%2520from%2520the%2520input%2520visibility%2520graph%2520%2524G%2524.%2520The%2520main%2520novelty%250Aof%2520our%2520approach%2520is%2520that%252C%2520rather%2520than%2520generating%2520the%2520polygon%2527s%2520vertex%2520set%250Adirectly%252C%2520we%2520first%2520estimate%2520the%2520signed%2520distance%2520function%2520%2528SDF%2529%2520associated%2520with%250Athe%2520polygon.%2520The%2520SDF%2520is%2520then%2520used%2520to%2520extract%2520the%2520vertex%2520location%2520representing%250Athe%2520final%2520polygon.%2520We%2520show%2520that%2520going%2520through%2520the%2520SDF%2520allows%2520VisDiff%2520to%2520learn%250Athe%2520visibility%2520relationship%2520much%2520more%2520effectively%2520than%2520generating%2520vertex%250Alocations%2520directly.%2520In%2520order%2520to%2520train%2520VisDiff%252C%2520we%2520create%2520a%2520carefully%2520curated%250Adataset.%2520We%2520use%2520this%2520dataset%2520to%2520benchmark%2520our%2520method%2520and%2520achieve%252026%2525%250Aimprovement%2520in%2520F1-Score%2520over%2520standard%2520methods%2520as%2520well%2520as%2520state%2520of%2520the%2520art%250Aapproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05530v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisDiff%3A%20SDF-Guided%20Polygon%20Generation%20for%20Visibility%20Reconstruction%20and%0A%20%20Recognition&entry.906535625=Rahul%20Moorthy%20and%20Volkan%20Isler&entry.1292438233=%20%20The%20ability%20to%20capture%20rich%20representations%20of%20combinatorial%20structures%20has%0Aenabled%20the%20application%20of%20machine%20learning%20to%20tasks%20such%20as%20analysis%20and%0Ageneration%20of%20floorplans%2C%20terrains%2C%20images%2C%20and%20animations.%20Recent%20work%20has%0Aprimarily%20focused%20on%20understanding%20structures%20with%20well-defined%20features%2C%0Aneighborhoods%2C%20or%20underlying%20distance%20metrics%2C%20while%20those%20lacking%20such%0Acharacteristics%20remain%20largely%20unstudied.%20Examples%20of%20these%20combinatorial%0Astructures%20can%20be%20found%20in%20polygons%2C%20where%20a%20small%20change%20in%20the%20vertex%0Alocations%20causes%20a%20significant%20rearrangement%20of%20the%20combinatorial%20structure%2C%0Aexpressed%20as%20a%20visibility%20or%20triangulation%20graphs.%20Current%20representation%0Alearning%20approaches%20fail%20to%20capture%20structures%20without%20well-defined%20features%0Aand%20distance%20metrics.%20In%20this%20paper%2C%20we%20study%20the%20open%20problem%20of%20Visibility%0AReconstruction%3A%20Given%20a%20visibility%20graph%20%24G%24%2C%20construct%20a%20polygon%20%24P%24%20whose%0Avisibility%20graph%20is%20%24G%24.%20We%20introduce%20VisDiff%2C%20a%20novel%20diffusion-based%20approach%0Ato%20generate%20polygon%20%24P%24%20from%20the%20input%20visibility%20graph%20%24G%24.%20The%20main%20novelty%0Aof%20our%20approach%20is%20that%2C%20rather%20than%20generating%20the%20polygon%27s%20vertex%20set%0Adirectly%2C%20we%20first%20estimate%20the%20signed%20distance%20function%20%28SDF%29%20associated%20with%0Athe%20polygon.%20The%20SDF%20is%20then%20used%20to%20extract%20the%20vertex%20location%20representing%0Athe%20final%20polygon.%20We%20show%20that%20going%20through%20the%20SDF%20allows%20VisDiff%20to%20learn%0Athe%20visibility%20relationship%20much%20more%20effectively%20than%20generating%20vertex%0Alocations%20directly.%20In%20order%20to%20train%20VisDiff%2C%20we%20create%20a%20carefully%20curated%0Adataset.%20We%20use%20this%20dataset%20to%20benchmark%20our%20method%20and%20achieve%2026%25%0Aimprovement%20in%20F1-Score%20over%20standard%20methods%20as%20well%20as%20state%20of%20the%20art%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05530v2&entry.124074799=Read"},
{"title": "KIT's Offline Speech Translation and Instruction Following Submission\n  for IWSLT 2025", "author": "Sai Koneru and Maike Z\u00fcfle and Thai-Binh Nguyen and Seymanur Akti and Jan Niehues and Alexander Waibel", "abstract": "  The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.\n", "link": "http://arxiv.org/abs/2505.13036v1", "date": "2025-05-19", "relevancy": 2.177, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4358}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KIT%27s%20Offline%20Speech%20Translation%20and%20Instruction%20Following%20Submission%0A%20%20for%20IWSLT%202025&body=Title%3A%20KIT%27s%20Offline%20Speech%20Translation%20and%20Instruction%20Following%20Submission%0A%20%20for%20IWSLT%202025%0AAuthor%3A%20Sai%20Koneru%20and%20Maike%20Z%C3%BCfle%20and%20Thai-Binh%20Nguyen%20and%20Seymanur%20Akti%20and%20Jan%20Niehues%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20The%20scope%20of%20the%20International%20Workshop%20on%20Spoken%20Language%20Translation%0A%28IWSLT%29%20has%20recently%20broadened%20beyond%20traditional%20Speech%20Translation%20%28ST%29%20to%0Aencompass%20a%20wider%20array%20of%20tasks%2C%20including%20Speech%20Question%20Answering%20and%0ASummarization.%20This%20shift%20is%20partly%20driven%20by%20the%20growing%20capabilities%20of%0Amodern%20systems%2C%20particularly%20with%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29.%0AIn%20this%20paper%2C%20we%20present%20the%20Karlsruhe%20Institute%20of%20Technology%27s%20submissions%0Afor%20the%20Offline%20ST%20and%20Instruction%20Following%20%28IF%29%20tracks%2C%20where%20we%20leverage%0ALLMs%20to%20enhance%20performance%20across%20all%20tasks.%20For%20the%20Offline%20ST%20track%2C%20we%0Apropose%20a%20pipeline%20that%20employs%20multiple%20automatic%20speech%20recognition%20systems%2C%0Awhose%20outputs%20are%20fused%20using%20an%20LLM%20with%20document-level%20context.%20This%20is%0Afollowed%20by%20a%20two-step%20translation%20process%2C%20incorporating%20additional%20refinement%0Astep%20to%20improve%20translation%20quality.%20For%20the%20IF%20track%2C%20we%20develop%20an%20end-to-end%0Amodel%20that%20integrates%20a%20speech%20encoder%20with%20an%20LLM%20to%20perform%20a%20wide%20range%20of%0Ainstruction-following%20tasks.%20We%20complement%20it%20with%20a%20final%20document-level%0Arefinement%20stage%20to%20further%20enhance%20output%20quality%20by%20using%20contextual%0Ainformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKIT%2527s%2520Offline%2520Speech%2520Translation%2520and%2520Instruction%2520Following%2520Submission%250A%2520%2520for%2520IWSLT%25202025%26entry.906535625%3DSai%2520Koneru%2520and%2520Maike%2520Z%25C3%25BCfle%2520and%2520Thai-Binh%2520Nguyen%2520and%2520Seymanur%2520Akti%2520and%2520Jan%2520Niehues%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520The%2520scope%2520of%2520the%2520International%2520Workshop%2520on%2520Spoken%2520Language%2520Translation%250A%2528IWSLT%2529%2520has%2520recently%2520broadened%2520beyond%2520traditional%2520Speech%2520Translation%2520%2528ST%2529%2520to%250Aencompass%2520a%2520wider%2520array%2520of%2520tasks%252C%2520including%2520Speech%2520Question%2520Answering%2520and%250ASummarization.%2520This%2520shift%2520is%2520partly%2520driven%2520by%2520the%2520growing%2520capabilities%2520of%250Amodern%2520systems%252C%2520particularly%2520with%2520the%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250AIn%2520this%2520paper%252C%2520we%2520present%2520the%2520Karlsruhe%2520Institute%2520of%2520Technology%2527s%2520submissions%250Afor%2520the%2520Offline%2520ST%2520and%2520Instruction%2520Following%2520%2528IF%2529%2520tracks%252C%2520where%2520we%2520leverage%250ALLMs%2520to%2520enhance%2520performance%2520across%2520all%2520tasks.%2520For%2520the%2520Offline%2520ST%2520track%252C%2520we%250Apropose%2520a%2520pipeline%2520that%2520employs%2520multiple%2520automatic%2520speech%2520recognition%2520systems%252C%250Awhose%2520outputs%2520are%2520fused%2520using%2520an%2520LLM%2520with%2520document-level%2520context.%2520This%2520is%250Afollowed%2520by%2520a%2520two-step%2520translation%2520process%252C%2520incorporating%2520additional%2520refinement%250Astep%2520to%2520improve%2520translation%2520quality.%2520For%2520the%2520IF%2520track%252C%2520we%2520develop%2520an%2520end-to-end%250Amodel%2520that%2520integrates%2520a%2520speech%2520encoder%2520with%2520an%2520LLM%2520to%2520perform%2520a%2520wide%2520range%2520of%250Ainstruction-following%2520tasks.%2520We%2520complement%2520it%2520with%2520a%2520final%2520document-level%250Arefinement%2520stage%2520to%2520further%2520enhance%2520output%2520quality%2520by%2520using%2520contextual%250Ainformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KIT%27s%20Offline%20Speech%20Translation%20and%20Instruction%20Following%20Submission%0A%20%20for%20IWSLT%202025&entry.906535625=Sai%20Koneru%20and%20Maike%20Z%C3%BCfle%20and%20Thai-Binh%20Nguyen%20and%20Seymanur%20Akti%20and%20Jan%20Niehues%20and%20Alexander%20Waibel&entry.1292438233=%20%20The%20scope%20of%20the%20International%20Workshop%20on%20Spoken%20Language%20Translation%0A%28IWSLT%29%20has%20recently%20broadened%20beyond%20traditional%20Speech%20Translation%20%28ST%29%20to%0Aencompass%20a%20wider%20array%20of%20tasks%2C%20including%20Speech%20Question%20Answering%20and%0ASummarization.%20This%20shift%20is%20partly%20driven%20by%20the%20growing%20capabilities%20of%0Amodern%20systems%2C%20particularly%20with%20the%20success%20of%20Large%20Language%20Models%20%28LLMs%29.%0AIn%20this%20paper%2C%20we%20present%20the%20Karlsruhe%20Institute%20of%20Technology%27s%20submissions%0Afor%20the%20Offline%20ST%20and%20Instruction%20Following%20%28IF%29%20tracks%2C%20where%20we%20leverage%0ALLMs%20to%20enhance%20performance%20across%20all%20tasks.%20For%20the%20Offline%20ST%20track%2C%20we%0Apropose%20a%20pipeline%20that%20employs%20multiple%20automatic%20speech%20recognition%20systems%2C%0Awhose%20outputs%20are%20fused%20using%20an%20LLM%20with%20document-level%20context.%20This%20is%0Afollowed%20by%20a%20two-step%20translation%20process%2C%20incorporating%20additional%20refinement%0Astep%20to%20improve%20translation%20quality.%20For%20the%20IF%20track%2C%20we%20develop%20an%20end-to-end%0Amodel%20that%20integrates%20a%20speech%20encoder%20with%20an%20LLM%20to%20perform%20a%20wide%20range%20of%0Ainstruction-following%20tasks.%20We%20complement%20it%20with%20a%20final%20document-level%0Arefinement%20stage%20to%20further%20enhance%20output%20quality%20by%20using%20contextual%0Ainformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13036v1&entry.124074799=Read"},
{"title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake\n  Detection", "author": "Zihan Xiong and Xiaohua Wu and Lei Chen and Fangqi Lou", "abstract": "  Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets.\n", "link": "http://arxiv.org/abs/2505.12966v1", "date": "2025-05-19", "relevancy": 2.1738, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5631}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiscale%20Adaptive%20Conflict-Balancing%20Model%20For%20Multimedia%20Deepfake%0A%20%20Detection&body=Title%3A%20Multiscale%20Adaptive%20Conflict-Balancing%20Model%20For%20Multimedia%20Deepfake%0A%20%20Detection%0AAuthor%3A%20Zihan%20Xiong%20and%20Xiaohua%20Wu%20and%20Lei%20Chen%20and%20Fangqi%20Lou%0AAbstract%3A%20%20%20Advances%20in%20computer%20vision%20and%20deep%20learning%20have%20blurred%20the%20line%20between%0Adeepfakes%20and%20authentic%20media%2C%20undermining%20multimedia%20credibility%20through%0Aaudio-visual%20forgery.%20Current%20multimodal%20detection%20methods%20remain%20limited%20by%0Aunbalanced%20learning%20between%20modalities.%20To%20tackle%20this%20issue%2C%20we%20propose%20an%0AAudio-Visual%20Joint%20Learning%20Method%20%28MACB-DF%29%20to%20better%20mitigate%20modality%0Aconflicts%20and%20neglect%20by%20leveraging%20contrastive%20learning%20to%20assist%20in%0Amulti-level%20and%20cross-modal%20fusion%2C%20thereby%20fully%20balancing%20and%20exploiting%0Ainformation%20from%20each%20modality.%20Additionally%2C%20we%20designed%20an%0Aorthogonalization-multimodal%20pareto%20module%20that%20preserves%20unimodal%20information%0Awhile%20addressing%20gradient%20conflicts%20in%20audio-video%20encoders%20caused%20by%20differing%0Aoptimization%20targets%20of%20the%20loss%20functions.%20Extensive%20experiments%20and%20ablation%0Astudies%20conducted%20on%20mainstream%20deepfake%20datasets%20demonstrate%20consistent%0Aperformance%20gains%20of%20our%20model%20across%20key%20evaluation%20metrics%2C%20achieving%20an%0Aaverage%20accuracy%20of%2095.5%25%20across%20multiple%20datasets.%20Notably%2C%20our%20method%0Aexhibits%20superior%20cross-dataset%20generalization%20capabilities%2C%20with%20absolute%0Aimprovements%20of%208.0%25%20and%207.7%25%20in%20ACC%20scores%20over%20the%20previous%20best-performing%0Aapproach%20when%20trained%20on%20DFDC%20and%20tested%20on%20DefakeAVMiT%20and%20FakeAVCeleb%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiscale%2520Adaptive%2520Conflict-Balancing%2520Model%2520For%2520Multimedia%2520Deepfake%250A%2520%2520Detection%26entry.906535625%3DZihan%2520Xiong%2520and%2520Xiaohua%2520Wu%2520and%2520Lei%2520Chen%2520and%2520Fangqi%2520Lou%26entry.1292438233%3D%2520%2520Advances%2520in%2520computer%2520vision%2520and%2520deep%2520learning%2520have%2520blurred%2520the%2520line%2520between%250Adeepfakes%2520and%2520authentic%2520media%252C%2520undermining%2520multimedia%2520credibility%2520through%250Aaudio-visual%2520forgery.%2520Current%2520multimodal%2520detection%2520methods%2520remain%2520limited%2520by%250Aunbalanced%2520learning%2520between%2520modalities.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520an%250AAudio-Visual%2520Joint%2520Learning%2520Method%2520%2528MACB-DF%2529%2520to%2520better%2520mitigate%2520modality%250Aconflicts%2520and%2520neglect%2520by%2520leveraging%2520contrastive%2520learning%2520to%2520assist%2520in%250Amulti-level%2520and%2520cross-modal%2520fusion%252C%2520thereby%2520fully%2520balancing%2520and%2520exploiting%250Ainformation%2520from%2520each%2520modality.%2520Additionally%252C%2520we%2520designed%2520an%250Aorthogonalization-multimodal%2520pareto%2520module%2520that%2520preserves%2520unimodal%2520information%250Awhile%2520addressing%2520gradient%2520conflicts%2520in%2520audio-video%2520encoders%2520caused%2520by%2520differing%250Aoptimization%2520targets%2520of%2520the%2520loss%2520functions.%2520Extensive%2520experiments%2520and%2520ablation%250Astudies%2520conducted%2520on%2520mainstream%2520deepfake%2520datasets%2520demonstrate%2520consistent%250Aperformance%2520gains%2520of%2520our%2520model%2520across%2520key%2520evaluation%2520metrics%252C%2520achieving%2520an%250Aaverage%2520accuracy%2520of%252095.5%2525%2520across%2520multiple%2520datasets.%2520Notably%252C%2520our%2520method%250Aexhibits%2520superior%2520cross-dataset%2520generalization%2520capabilities%252C%2520with%2520absolute%250Aimprovements%2520of%25208.0%2525%2520and%25207.7%2525%2520in%2520ACC%2520scores%2520over%2520the%2520previous%2520best-performing%250Aapproach%2520when%2520trained%2520on%2520DFDC%2520and%2520tested%2520on%2520DefakeAVMiT%2520and%2520FakeAVCeleb%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiscale%20Adaptive%20Conflict-Balancing%20Model%20For%20Multimedia%20Deepfake%0A%20%20Detection&entry.906535625=Zihan%20Xiong%20and%20Xiaohua%20Wu%20and%20Lei%20Chen%20and%20Fangqi%20Lou&entry.1292438233=%20%20Advances%20in%20computer%20vision%20and%20deep%20learning%20have%20blurred%20the%20line%20between%0Adeepfakes%20and%20authentic%20media%2C%20undermining%20multimedia%20credibility%20through%0Aaudio-visual%20forgery.%20Current%20multimodal%20detection%20methods%20remain%20limited%20by%0Aunbalanced%20learning%20between%20modalities.%20To%20tackle%20this%20issue%2C%20we%20propose%20an%0AAudio-Visual%20Joint%20Learning%20Method%20%28MACB-DF%29%20to%20better%20mitigate%20modality%0Aconflicts%20and%20neglect%20by%20leveraging%20contrastive%20learning%20to%20assist%20in%0Amulti-level%20and%20cross-modal%20fusion%2C%20thereby%20fully%20balancing%20and%20exploiting%0Ainformation%20from%20each%20modality.%20Additionally%2C%20we%20designed%20an%0Aorthogonalization-multimodal%20pareto%20module%20that%20preserves%20unimodal%20information%0Awhile%20addressing%20gradient%20conflicts%20in%20audio-video%20encoders%20caused%20by%20differing%0Aoptimization%20targets%20of%20the%20loss%20functions.%20Extensive%20experiments%20and%20ablation%0Astudies%20conducted%20on%20mainstream%20deepfake%20datasets%20demonstrate%20consistent%0Aperformance%20gains%20of%20our%20model%20across%20key%20evaluation%20metrics%2C%20achieving%20an%0Aaverage%20accuracy%20of%2095.5%25%20across%20multiple%20datasets.%20Notably%2C%20our%20method%0Aexhibits%20superior%20cross-dataset%20generalization%20capabilities%2C%20with%20absolute%0Aimprovements%20of%208.0%25%20and%207.7%25%20in%20ACC%20scores%20over%20the%20previous%20best-performing%0Aapproach%20when%20trained%20on%20DFDC%20and%20tested%20on%20DefakeAVMiT%20and%20FakeAVCeleb%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12966v1&entry.124074799=Read"},
{"title": "$\\infty$-Video: A Training-Free Approach to Long Video Understanding via\n  Continuous-Time Memory Consolidation", "author": "Saul Santos and Ant\u00f3nio Farinhas and Daniel C. McNamee and Andr\u00e9 F. T. Martins", "abstract": "  Current video-language models struggle with long-video understanding due to\nlimited context lengths and reliance on sparse frame subsampling, often leading\nto information loss. This paper introduces $\\infty$-Video, which can process\narbitrarily long videos through a continuous-time long-term memory (LTM)\nconsolidation mechanism. Our framework augments video Q-formers by allowing\nthem to process unbounded video contexts efficiently and without requiring\nadditional training. Through continuous attention, our approach dynamically\nallocates higher granularity to the most relevant video segments, forming\n\"sticky\" memories that evolve over time. Experiments with Video-LLaMA and\nVideoChat2 demonstrate improved performance in video question-answering tasks,\nshowcasing the potential of continuous-time LTM mechanisms to enable scalable\nand training-free comprehension of long videos.\n", "link": "http://arxiv.org/abs/2501.19098v2", "date": "2025-05-19", "relevancy": 2.1734, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5438}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Cinfty%24-Video%3A%20A%20Training-Free%20Approach%20to%20Long%20Video%20Understanding%20via%0A%20%20Continuous-Time%20Memory%20Consolidation&body=Title%3A%20%24%5Cinfty%24-Video%3A%20A%20Training-Free%20Approach%20to%20Long%20Video%20Understanding%20via%0A%20%20Continuous-Time%20Memory%20Consolidation%0AAuthor%3A%20Saul%20Santos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Daniel%20C.%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Current%20video-language%20models%20struggle%20with%20long-video%20understanding%20due%20to%0Alimited%20context%20lengths%20and%20reliance%20on%20sparse%20frame%20subsampling%2C%20often%20leading%0Ato%20information%20loss.%20This%20paper%20introduces%20%24%5Cinfty%24-Video%2C%20which%20can%20process%0Aarbitrarily%20long%20videos%20through%20a%20continuous-time%20long-term%20memory%20%28LTM%29%0Aconsolidation%20mechanism.%20Our%20framework%20augments%20video%20Q-formers%20by%20allowing%0Athem%20to%20process%20unbounded%20video%20contexts%20efficiently%20and%20without%20requiring%0Aadditional%20training.%20Through%20continuous%20attention%2C%20our%20approach%20dynamically%0Aallocates%20higher%20granularity%20to%20the%20most%20relevant%20video%20segments%2C%20forming%0A%22sticky%22%20memories%20that%20evolve%20over%20time.%20Experiments%20with%20Video-LLaMA%20and%0AVideoChat2%20demonstrate%20improved%20performance%20in%20video%20question-answering%20tasks%2C%0Ashowcasing%20the%20potential%20of%20continuous-time%20LTM%20mechanisms%20to%20enable%20scalable%0Aand%20training-free%20comprehension%20of%20long%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Cinfty%2524-Video%253A%2520A%2520Training-Free%2520Approach%2520to%2520Long%2520Video%2520Understanding%2520via%250A%2520%2520Continuous-Time%2520Memory%2520Consolidation%26entry.906535625%3DSaul%2520Santos%2520and%2520Ant%25C3%25B3nio%2520Farinhas%2520and%2520Daniel%2520C.%2520McNamee%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Current%2520video-language%2520models%2520struggle%2520with%2520long-video%2520understanding%2520due%2520to%250Alimited%2520context%2520lengths%2520and%2520reliance%2520on%2520sparse%2520frame%2520subsampling%252C%2520often%2520leading%250Ato%2520information%2520loss.%2520This%2520paper%2520introduces%2520%2524%255Cinfty%2524-Video%252C%2520which%2520can%2520process%250Aarbitrarily%2520long%2520videos%2520through%2520a%2520continuous-time%2520long-term%2520memory%2520%2528LTM%2529%250Aconsolidation%2520mechanism.%2520Our%2520framework%2520augments%2520video%2520Q-formers%2520by%2520allowing%250Athem%2520to%2520process%2520unbounded%2520video%2520contexts%2520efficiently%2520and%2520without%2520requiring%250Aadditional%2520training.%2520Through%2520continuous%2520attention%252C%2520our%2520approach%2520dynamically%250Aallocates%2520higher%2520granularity%2520to%2520the%2520most%2520relevant%2520video%2520segments%252C%2520forming%250A%2522sticky%2522%2520memories%2520that%2520evolve%2520over%2520time.%2520Experiments%2520with%2520Video-LLaMA%2520and%250AVideoChat2%2520demonstrate%2520improved%2520performance%2520in%2520video%2520question-answering%2520tasks%252C%250Ashowcasing%2520the%2520potential%2520of%2520continuous-time%2520LTM%2520mechanisms%2520to%2520enable%2520scalable%250Aand%2520training-free%2520comprehension%2520of%2520long%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cinfty%24-Video%3A%20A%20Training-Free%20Approach%20to%20Long%20Video%20Understanding%20via%0A%20%20Continuous-Time%20Memory%20Consolidation&entry.906535625=Saul%20Santos%20and%20Ant%C3%B3nio%20Farinhas%20and%20Daniel%20C.%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Current%20video-language%20models%20struggle%20with%20long-video%20understanding%20due%20to%0Alimited%20context%20lengths%20and%20reliance%20on%20sparse%20frame%20subsampling%2C%20often%20leading%0Ato%20information%20loss.%20This%20paper%20introduces%20%24%5Cinfty%24-Video%2C%20which%20can%20process%0Aarbitrarily%20long%20videos%20through%20a%20continuous-time%20long-term%20memory%20%28LTM%29%0Aconsolidation%20mechanism.%20Our%20framework%20augments%20video%20Q-formers%20by%20allowing%0Athem%20to%20process%20unbounded%20video%20contexts%20efficiently%20and%20without%20requiring%0Aadditional%20training.%20Through%20continuous%20attention%2C%20our%20approach%20dynamically%0Aallocates%20higher%20granularity%20to%20the%20most%20relevant%20video%20segments%2C%20forming%0A%22sticky%22%20memories%20that%20evolve%20over%20time.%20Experiments%20with%20Video-LLaMA%20and%0AVideoChat2%20demonstrate%20improved%20performance%20in%20video%20question-answering%20tasks%2C%0Ashowcasing%20the%20potential%20of%20continuous-time%20LTM%20mechanisms%20to%20enable%20scalable%0Aand%20training-free%20comprehension%20of%20long%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19098v2&entry.124074799=Read"},
{"title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language\n  Models with Emotional Synergy and Reasoning", "author": "Zhuozhao Hu and Kaishen Yuan and Xin Liu and Zitong Yu and Yuan Zong and Jingang Shi and Huanjing Yue and Jingyu Yang", "abstract": "  Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM.\n", "link": "http://arxiv.org/abs/2505.13419v1", "date": "2025-05-19", "relevancy": 2.1721, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FEALLM%3A%20Advancing%20Facial%20Emotion%20Analysis%20in%20Multimodal%20Large%20Language%0A%20%20Models%20with%20Emotional%20Synergy%20and%20Reasoning&body=Title%3A%20FEALLM%3A%20Advancing%20Facial%20Emotion%20Analysis%20in%20Multimodal%20Large%20Language%0A%20%20Models%20with%20Emotional%20Synergy%20and%20Reasoning%0AAuthor%3A%20Zhuozhao%20Hu%20and%20Kaishen%20Yuan%20and%20Xin%20Liu%20and%20Zitong%20Yu%20and%20Yuan%20Zong%20and%20Jingang%20Shi%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang%0AAbstract%3A%20%20%20Facial%20Emotion%20Analysis%20%28FEA%29%20plays%20a%20crucial%20role%20in%20visual%20affective%0Acomputing%2C%20aiming%20to%20infer%20a%20person%27s%20emotional%20state%20based%20on%20facial%20data.%0AScientifically%2C%20facial%20expressions%20%28FEs%29%20result%20from%20the%20coordinated%20movement%0Aof%20facial%20muscles%2C%20which%20can%20be%20decomposed%20into%20specific%20action%20units%20%28AUs%29%0Athat%20provide%20detailed%20emotional%20insights.%20However%2C%20traditional%20methods%20often%0Astruggle%20with%20limited%20interpretability%2C%20constrained%20generalization%20and%0Areasoning%20abilities.%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Ashown%20exceptional%20performance%20in%20various%20visual%20tasks%2C%20while%20they%20still%20face%0Asignificant%20challenges%20in%20FEA%20due%20to%20the%20lack%20of%20specialized%20datasets%20and%20their%0Ainability%20to%20capture%20the%20intricate%20relationships%20between%20FEs%20and%20AUs.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20a%20novel%20FEA%20Instruction%20Dataset%20that%0Aprovides%20accurate%20and%20aligned%20FE%20and%20AU%20descriptions%20and%20establishes%20causal%0Areasoning%20relationships%20between%20them%2C%20followed%20by%20constructing%20a%20new%20benchmark%2C%0AFEABench.%20Moreover%2C%20we%20propose%20FEALLM%2C%20a%20novel%20MLLM%20architecture%20designed%20to%0Acapture%20more%20detailed%20facial%20information%2C%20enhancing%20its%20capability%20in%20FEA%0Atasks.%20Our%20model%20demonstrates%20strong%20performance%20on%20FEABench%20and%20impressive%0Ageneralization%20capability%20through%20zero-shot%20evaluation%20on%20various%20datasets%2C%0Aincluding%20RAF-DB%2C%20AffectNet%2C%20BP4D%2C%20and%20DISFA%2C%20showcasing%20its%20robustness%20and%0Aeffectiveness%20in%20FEA%20tasks.%20The%20dataset%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/953206211/FEALLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFEALLM%253A%2520Advancing%2520Facial%2520Emotion%2520Analysis%2520in%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%2520with%2520Emotional%2520Synergy%2520and%2520Reasoning%26entry.906535625%3DZhuozhao%2520Hu%2520and%2520Kaishen%2520Yuan%2520and%2520Xin%2520Liu%2520and%2520Zitong%2520Yu%2520and%2520Yuan%2520Zong%2520and%2520Jingang%2520Shi%2520and%2520Huanjing%2520Yue%2520and%2520Jingyu%2520Yang%26entry.1292438233%3D%2520%2520Facial%2520Emotion%2520Analysis%2520%2528FEA%2529%2520plays%2520a%2520crucial%2520role%2520in%2520visual%2520affective%250Acomputing%252C%2520aiming%2520to%2520infer%2520a%2520person%2527s%2520emotional%2520state%2520based%2520on%2520facial%2520data.%250AScientifically%252C%2520facial%2520expressions%2520%2528FEs%2529%2520result%2520from%2520the%2520coordinated%2520movement%250Aof%2520facial%2520muscles%252C%2520which%2520can%2520be%2520decomposed%2520into%2520specific%2520action%2520units%2520%2528AUs%2529%250Athat%2520provide%2520detailed%2520emotional%2520insights.%2520However%252C%2520traditional%2520methods%2520often%250Astruggle%2520with%2520limited%2520interpretability%252C%2520constrained%2520generalization%2520and%250Areasoning%2520abilities.%2520Recently%252C%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%250Ashown%2520exceptional%2520performance%2520in%2520various%2520visual%2520tasks%252C%2520while%2520they%2520still%2520face%250Asignificant%2520challenges%2520in%2520FEA%2520due%2520to%2520the%2520lack%2520of%2520specialized%2520datasets%2520and%2520their%250Ainability%2520to%2520capture%2520the%2520intricate%2520relationships%2520between%2520FEs%2520and%2520AUs.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%2520FEA%2520Instruction%2520Dataset%2520that%250Aprovides%2520accurate%2520and%2520aligned%2520FE%2520and%2520AU%2520descriptions%2520and%2520establishes%2520causal%250Areasoning%2520relationships%2520between%2520them%252C%2520followed%2520by%2520constructing%2520a%2520new%2520benchmark%252C%250AFEABench.%2520Moreover%252C%2520we%2520propose%2520FEALLM%252C%2520a%2520novel%2520MLLM%2520architecture%2520designed%2520to%250Acapture%2520more%2520detailed%2520facial%2520information%252C%2520enhancing%2520its%2520capability%2520in%2520FEA%250Atasks.%2520Our%2520model%2520demonstrates%2520strong%2520performance%2520on%2520FEABench%2520and%2520impressive%250Ageneralization%2520capability%2520through%2520zero-shot%2520evaluation%2520on%2520various%2520datasets%252C%250Aincluding%2520RAF-DB%252C%2520AffectNet%252C%2520BP4D%252C%2520and%2520DISFA%252C%2520showcasing%2520its%2520robustness%2520and%250Aeffectiveness%2520in%2520FEA%2520tasks.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/953206211/FEALLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FEALLM%3A%20Advancing%20Facial%20Emotion%20Analysis%20in%20Multimodal%20Large%20Language%0A%20%20Models%20with%20Emotional%20Synergy%20and%20Reasoning&entry.906535625=Zhuozhao%20Hu%20and%20Kaishen%20Yuan%20and%20Xin%20Liu%20and%20Zitong%20Yu%20and%20Yuan%20Zong%20and%20Jingang%20Shi%20and%20Huanjing%20Yue%20and%20Jingyu%20Yang&entry.1292438233=%20%20Facial%20Emotion%20Analysis%20%28FEA%29%20plays%20a%20crucial%20role%20in%20visual%20affective%0Acomputing%2C%20aiming%20to%20infer%20a%20person%27s%20emotional%20state%20based%20on%20facial%20data.%0AScientifically%2C%20facial%20expressions%20%28FEs%29%20result%20from%20the%20coordinated%20movement%0Aof%20facial%20muscles%2C%20which%20can%20be%20decomposed%20into%20specific%20action%20units%20%28AUs%29%0Athat%20provide%20detailed%20emotional%20insights.%20However%2C%20traditional%20methods%20often%0Astruggle%20with%20limited%20interpretability%2C%20constrained%20generalization%20and%0Areasoning%20abilities.%20Recently%2C%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%0Ashown%20exceptional%20performance%20in%20various%20visual%20tasks%2C%20while%20they%20still%20face%0Asignificant%20challenges%20in%20FEA%20due%20to%20the%20lack%20of%20specialized%20datasets%20and%20their%0Ainability%20to%20capture%20the%20intricate%20relationships%20between%20FEs%20and%20AUs.%20To%0Aaddress%20these%20issues%2C%20we%20introduce%20a%20novel%20FEA%20Instruction%20Dataset%20that%0Aprovides%20accurate%20and%20aligned%20FE%20and%20AU%20descriptions%20and%20establishes%20causal%0Areasoning%20relationships%20between%20them%2C%20followed%20by%20constructing%20a%20new%20benchmark%2C%0AFEABench.%20Moreover%2C%20we%20propose%20FEALLM%2C%20a%20novel%20MLLM%20architecture%20designed%20to%0Acapture%20more%20detailed%20facial%20information%2C%20enhancing%20its%20capability%20in%20FEA%0Atasks.%20Our%20model%20demonstrates%20strong%20performance%20on%20FEABench%20and%20impressive%0Ageneralization%20capability%20through%20zero-shot%20evaluation%20on%20various%20datasets%2C%0Aincluding%20RAF-DB%2C%20AffectNet%2C%20BP4D%2C%20and%20DISFA%2C%20showcasing%20its%20robustness%20and%0Aeffectiveness%20in%20FEA%20tasks.%20The%20dataset%20and%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/953206211/FEALLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13419v1&entry.124074799=Read"},
{"title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "author": "Shaowu Wu and Liting Zeng and Wei Lu and Xiangyang Luo", "abstract": "  With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks.\n", "link": "http://arxiv.org/abs/2505.13101v1", "date": "2025-05-19", "relevancy": 2.1709, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5791}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5616}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARIW-Framework%3A%20Adaptive%20Robust%20Iterative%20Watermarking%20Framework&body=Title%3A%20ARIW-Framework%3A%20Adaptive%20Robust%20Iterative%20Watermarking%20Framework%0AAuthor%3A%20Shaowu%20Wu%20and%20Liting%20Zeng%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%0AAbstract%3A%20%20%20With%20the%20rapid%20rise%20of%20large%20models%2C%20copyright%20protection%20for%20generated%20image%0Acontent%20has%20become%20a%20critical%20security%20challenge.%20Although%20deep%20learning%0Awatermarking%20techniques%20offer%20an%20effective%20solution%20for%20digital%20image%20copyright%0Aprotection%2C%20they%20still%20face%20limitations%20in%20terms%20of%20visual%20quality%2C%20robustness%0Aand%20generalization.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20an%20adaptive%0Arobust%20iterative%20watermarking%20framework%20%28ARIW-Framework%29%20that%20achieves%0Ahigh-quality%20watermarked%20images%20while%20maintaining%20exceptional%20robustness%20and%0Ageneralization%20performance.%20Specifically%2C%20we%20introduce%20an%20iterative%20approach%20to%0Aoptimize%20the%20encoder%20for%20generating%20robust%20residuals.%20The%20encoder%20incorporates%0Anoise%20layers%20and%20a%20decoder%20to%20compute%20robustness%20weights%20for%20residuals%20under%0Avarious%20noise%20attacks.%20By%20employing%20a%20parallel%20optimization%20strategy%2C%20the%0Aframework%20enhances%20robustness%20against%20multiple%20types%20of%20noise%20attacks.%0AFurthermore%2C%20we%20leverage%20image%20gradients%20to%20determine%20the%20embedding%20strength%20at%0Aeach%20pixel%20location%2C%20significantly%20improving%20the%20visual%20quality%20of%20the%0Awatermarked%20images.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20superior%20visual%20quality%20while%20exhibiting%20remarkable%20robustness%20and%0Ageneralization%20against%20noise%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARIW-Framework%253A%2520Adaptive%2520Robust%2520Iterative%2520Watermarking%2520Framework%26entry.906535625%3DShaowu%2520Wu%2520and%2520Liting%2520Zeng%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520rise%2520of%2520large%2520models%252C%2520copyright%2520protection%2520for%2520generated%2520image%250Acontent%2520has%2520become%2520a%2520critical%2520security%2520challenge.%2520Although%2520deep%2520learning%250Awatermarking%2520techniques%2520offer%2520an%2520effective%2520solution%2520for%2520digital%2520image%2520copyright%250Aprotection%252C%2520they%2520still%2520face%2520limitations%2520in%2520terms%2520of%2520visual%2520quality%252C%2520robustness%250Aand%2520generalization.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520proposes%2520an%2520adaptive%250Arobust%2520iterative%2520watermarking%2520framework%2520%2528ARIW-Framework%2529%2520that%2520achieves%250Ahigh-quality%2520watermarked%2520images%2520while%2520maintaining%2520exceptional%2520robustness%2520and%250Ageneralization%2520performance.%2520Specifically%252C%2520we%2520introduce%2520an%2520iterative%2520approach%2520to%250Aoptimize%2520the%2520encoder%2520for%2520generating%2520robust%2520residuals.%2520The%2520encoder%2520incorporates%250Anoise%2520layers%2520and%2520a%2520decoder%2520to%2520compute%2520robustness%2520weights%2520for%2520residuals%2520under%250Avarious%2520noise%2520attacks.%2520By%2520employing%2520a%2520parallel%2520optimization%2520strategy%252C%2520the%250Aframework%2520enhances%2520robustness%2520against%2520multiple%2520types%2520of%2520noise%2520attacks.%250AFurthermore%252C%2520we%2520leverage%2520image%2520gradients%2520to%2520determine%2520the%2520embedding%2520strength%2520at%250Aeach%2520pixel%2520location%252C%2520significantly%2520improving%2520the%2520visual%2520quality%2520of%2520the%250Awatermarked%2520images.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aachieves%2520superior%2520visual%2520quality%2520while%2520exhibiting%2520remarkable%2520robustness%2520and%250Ageneralization%2520against%2520noise%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARIW-Framework%3A%20Adaptive%20Robust%20Iterative%20Watermarking%20Framework&entry.906535625=Shaowu%20Wu%20and%20Liting%20Zeng%20and%20Wei%20Lu%20and%20Xiangyang%20Luo&entry.1292438233=%20%20With%20the%20rapid%20rise%20of%20large%20models%2C%20copyright%20protection%20for%20generated%20image%0Acontent%20has%20become%20a%20critical%20security%20challenge.%20Although%20deep%20learning%0Awatermarking%20techniques%20offer%20an%20effective%20solution%20for%20digital%20image%20copyright%0Aprotection%2C%20they%20still%20face%20limitations%20in%20terms%20of%20visual%20quality%2C%20robustness%0Aand%20generalization.%20To%20address%20these%20issues%2C%20this%20paper%20proposes%20an%20adaptive%0Arobust%20iterative%20watermarking%20framework%20%28ARIW-Framework%29%20that%20achieves%0Ahigh-quality%20watermarked%20images%20while%20maintaining%20exceptional%20robustness%20and%0Ageneralization%20performance.%20Specifically%2C%20we%20introduce%20an%20iterative%20approach%20to%0Aoptimize%20the%20encoder%20for%20generating%20robust%20residuals.%20The%20encoder%20incorporates%0Anoise%20layers%20and%20a%20decoder%20to%20compute%20robustness%20weights%20for%20residuals%20under%0Avarious%20noise%20attacks.%20By%20employing%20a%20parallel%20optimization%20strategy%2C%20the%0Aframework%20enhances%20robustness%20against%20multiple%20types%20of%20noise%20attacks.%0AFurthermore%2C%20we%20leverage%20image%20gradients%20to%20determine%20the%20embedding%20strength%20at%0Aeach%20pixel%20location%2C%20significantly%20improving%20the%20visual%20quality%20of%20the%0Awatermarked%20images.%20Extensive%20experiments%20demonstrate%20that%20the%20proposed%20method%0Aachieves%20superior%20visual%20quality%20while%20exhibiting%20remarkable%20robustness%20and%0Ageneralization%20against%20noise%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13101v1&entry.124074799=Read"},
{"title": "Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards", "author": "Xiaoyuan Liu and Tian Liang and Zhiwei He and Jiahao Xu and Wenxuan Wang and Pinjia He and Zhaopeng Tu and Haitao Mi and Dong Yu", "abstract": "  Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.\n", "link": "http://arxiv.org/abs/2505.13445v1", "date": "2025-05-19", "relevancy": 1.9586, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5005}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4914}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trust%2C%20But%20Verify%3A%20A%20Self-Verification%20Approach%20to%20Reinforcement%0A%20%20Learning%20with%20Verifiable%20Rewards&body=Title%3A%20Trust%2C%20But%20Verify%3A%20A%20Self-Verification%20Approach%20to%20Reinforcement%0A%20%20Learning%20with%20Verifiable%20Rewards%0AAuthor%3A%20Xiaoyuan%20Liu%20and%20Tian%20Liang%20and%20Zhiwei%20He%20and%20Jiahao%20Xu%20and%20Wenxuan%20Wang%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu%20and%20Haitao%20Mi%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20show%20great%20promise%20in%20complex%20reasoning%2C%20with%0AReinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20being%20a%20key%20enhancement%0Astrategy.%20However%2C%20a%20prevalent%20issue%20is%20%60%60superficial%20self-reflection%27%27%2C%20where%0Amodels%20fail%20to%20robustly%20verify%20their%20own%20outputs.%20We%20introduce%20RISE%0A%28Reinforcing%20Reasoning%20with%20Self-Verification%29%2C%20a%20novel%20online%20RL%20framework%0Adesigned%20to%20tackle%20this.%20RISE%20explicitly%20and%20simultaneously%20trains%20an%20LLM%20to%0Aimprove%20both%20its%20problem-solving%20and%20self-verification%20abilities%20within%20a%0Asingle%2C%20integrated%20RL%20process.%20The%20core%20mechanism%20involves%20leveraging%0Averifiable%20rewards%20from%20an%20outcome%20verifier%20to%20provide%20on-the-fly%20feedback%20for%0Aboth%20solution%20generation%20and%20self-verification%20tasks.%20In%20each%20iteration%2C%20the%0Amodel%20generates%20solutions%2C%20then%20critiques%20its%20own%20on-policy%20generated%0Asolutions%2C%20with%20both%20trajectories%20contributing%20to%20the%20policy%20update.%20Extensive%0Aexperiments%20on%20diverse%20mathematical%20reasoning%20benchmarks%20show%20that%20RISE%0Aconsistently%20improves%20model%27s%20problem-solving%20accuracy%20while%20concurrently%0Afostering%20strong%20self-verification%20skills.%20Our%20analyses%20highlight%20the%0Aadvantages%20of%20online%20verification%20and%20the%20benefits%20of%20increased%20verification%0Acompute.%20Additionally%2C%20RISE%20models%20exhibit%20more%20frequent%20and%20accurate%0Aself-verification%20behaviors%20during%20reasoning.%20These%20advantages%20reinforce%20RISE%0Aas%20a%20flexible%20and%20effective%20path%20towards%20developing%20more%20robust%20and%20self-aware%0Areasoners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrust%252C%2520But%2520Verify%253A%2520A%2520Self-Verification%2520Approach%2520to%2520Reinforcement%250A%2520%2520Learning%2520with%2520Verifiable%2520Rewards%26entry.906535625%3DXiaoyuan%2520Liu%2520and%2520Tian%2520Liang%2520and%2520Zhiwei%2520He%2520and%2520Jiahao%2520Xu%2520and%2520Wenxuan%2520Wang%2520and%2520Pinjia%2520He%2520and%2520Zhaopeng%2520Tu%2520and%2520Haitao%2520Mi%2520and%2520Dong%2520Yu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520great%2520promise%2520in%2520complex%2520reasoning%252C%2520with%250AReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520being%2520a%2520key%2520enhancement%250Astrategy.%2520However%252C%2520a%2520prevalent%2520issue%2520is%2520%2560%2560superficial%2520self-reflection%2527%2527%252C%2520where%250Amodels%2520fail%2520to%2520robustly%2520verify%2520their%2520own%2520outputs.%2520We%2520introduce%2520RISE%250A%2528Reinforcing%2520Reasoning%2520with%2520Self-Verification%2529%252C%2520a%2520novel%2520online%2520RL%2520framework%250Adesigned%2520to%2520tackle%2520this.%2520RISE%2520explicitly%2520and%2520simultaneously%2520trains%2520an%2520LLM%2520to%250Aimprove%2520both%2520its%2520problem-solving%2520and%2520self-verification%2520abilities%2520within%2520a%250Asingle%252C%2520integrated%2520RL%2520process.%2520The%2520core%2520mechanism%2520involves%2520leveraging%250Averifiable%2520rewards%2520from%2520an%2520outcome%2520verifier%2520to%2520provide%2520on-the-fly%2520feedback%2520for%250Aboth%2520solution%2520generation%2520and%2520self-verification%2520tasks.%2520In%2520each%2520iteration%252C%2520the%250Amodel%2520generates%2520solutions%252C%2520then%2520critiques%2520its%2520own%2520on-policy%2520generated%250Asolutions%252C%2520with%2520both%2520trajectories%2520contributing%2520to%2520the%2520policy%2520update.%2520Extensive%250Aexperiments%2520on%2520diverse%2520mathematical%2520reasoning%2520benchmarks%2520show%2520that%2520RISE%250Aconsistently%2520improves%2520model%2527s%2520problem-solving%2520accuracy%2520while%2520concurrently%250Afostering%2520strong%2520self-verification%2520skills.%2520Our%2520analyses%2520highlight%2520the%250Aadvantages%2520of%2520online%2520verification%2520and%2520the%2520benefits%2520of%2520increased%2520verification%250Acompute.%2520Additionally%252C%2520RISE%2520models%2520exhibit%2520more%2520frequent%2520and%2520accurate%250Aself-verification%2520behaviors%2520during%2520reasoning.%2520These%2520advantages%2520reinforce%2520RISE%250Aas%2520a%2520flexible%2520and%2520effective%2520path%2520towards%2520developing%2520more%2520robust%2520and%2520self-aware%250Areasoners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trust%2C%20But%20Verify%3A%20A%20Self-Verification%20Approach%20to%20Reinforcement%0A%20%20Learning%20with%20Verifiable%20Rewards&entry.906535625=Xiaoyuan%20Liu%20and%20Tian%20Liang%20and%20Zhiwei%20He%20and%20Jiahao%20Xu%20and%20Wenxuan%20Wang%20and%20Pinjia%20He%20and%20Zhaopeng%20Tu%20and%20Haitao%20Mi%20and%20Dong%20Yu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20show%20great%20promise%20in%20complex%20reasoning%2C%20with%0AReinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20being%20a%20key%20enhancement%0Astrategy.%20However%2C%20a%20prevalent%20issue%20is%20%60%60superficial%20self-reflection%27%27%2C%20where%0Amodels%20fail%20to%20robustly%20verify%20their%20own%20outputs.%20We%20introduce%20RISE%0A%28Reinforcing%20Reasoning%20with%20Self-Verification%29%2C%20a%20novel%20online%20RL%20framework%0Adesigned%20to%20tackle%20this.%20RISE%20explicitly%20and%20simultaneously%20trains%20an%20LLM%20to%0Aimprove%20both%20its%20problem-solving%20and%20self-verification%20abilities%20within%20a%0Asingle%2C%20integrated%20RL%20process.%20The%20core%20mechanism%20involves%20leveraging%0Averifiable%20rewards%20from%20an%20outcome%20verifier%20to%20provide%20on-the-fly%20feedback%20for%0Aboth%20solution%20generation%20and%20self-verification%20tasks.%20In%20each%20iteration%2C%20the%0Amodel%20generates%20solutions%2C%20then%20critiques%20its%20own%20on-policy%20generated%0Asolutions%2C%20with%20both%20trajectories%20contributing%20to%20the%20policy%20update.%20Extensive%0Aexperiments%20on%20diverse%20mathematical%20reasoning%20benchmarks%20show%20that%20RISE%0Aconsistently%20improves%20model%27s%20problem-solving%20accuracy%20while%20concurrently%0Afostering%20strong%20self-verification%20skills.%20Our%20analyses%20highlight%20the%0Aadvantages%20of%20online%20verification%20and%20the%20benefits%20of%20increased%20verification%0Acompute.%20Additionally%2C%20RISE%20models%20exhibit%20more%20frequent%20and%20accurate%0Aself-verification%20behaviors%20during%20reasoning.%20These%20advantages%20reinforce%20RISE%0Aas%20a%20flexible%20and%20effective%20path%20towards%20developing%20more%20robust%20and%20self-aware%0Areasoners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13445v1&entry.124074799=Read"},
{"title": "Diffusion Models with Double Guidance: Generate with aggregated datasets", "author": "Yanfeng Yang and Kenji Fukumizu", "abstract": "  Creating large-scale datasets for training high-performance generative models\nis often prohibitively expensive, especially when associated attributes or\nannotations must be provided. As a result, merging existing datasets has become\na common strategy. However, the sets of attributes across datasets are often\ninconsistent, and their naive concatenation typically leads to block-wise\nmissing conditions. This presents a significant challenge for conditional\ngenerative modeling when the multiple attributes are used jointly as\nconditions, thereby limiting the model's controllability and applicability. To\naddress this issue, we propose a novel generative approach, Diffusion Model\nwith Double Guidance, which enables precise conditional generation even when no\ntraining samples contain all conditions simultaneously. Our method maintains\nrigorous control over multiple conditions without requiring joint annotations.\nWe demonstrate its effectiveness in molecular and image generation tasks, where\nit outperforms existing baselines both in alignment with target conditional\ndistributions and in controllability under missing condition settings.\n", "link": "http://arxiv.org/abs/2505.13213v1", "date": "2025-05-19", "relevancy": 1.2337, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.641}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6223}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Models%20with%20Double%20Guidance%3A%20Generate%20with%20aggregated%20datasets&body=Title%3A%20Diffusion%20Models%20with%20Double%20Guidance%3A%20Generate%20with%20aggregated%20datasets%0AAuthor%3A%20Yanfeng%20Yang%20and%20Kenji%20Fukumizu%0AAbstract%3A%20%20%20Creating%20large-scale%20datasets%20for%20training%20high-performance%20generative%20models%0Ais%20often%20prohibitively%20expensive%2C%20especially%20when%20associated%20attributes%20or%0Aannotations%20must%20be%20provided.%20As%20a%20result%2C%20merging%20existing%20datasets%20has%20become%0Aa%20common%20strategy.%20However%2C%20the%20sets%20of%20attributes%20across%20datasets%20are%20often%0Ainconsistent%2C%20and%20their%20naive%20concatenation%20typically%20leads%20to%20block-wise%0Amissing%20conditions.%20This%20presents%20a%20significant%20challenge%20for%20conditional%0Agenerative%20modeling%20when%20the%20multiple%20attributes%20are%20used%20jointly%20as%0Aconditions%2C%20thereby%20limiting%20the%20model%27s%20controllability%20and%20applicability.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20generative%20approach%2C%20Diffusion%20Model%0Awith%20Double%20Guidance%2C%20which%20enables%20precise%20conditional%20generation%20even%20when%20no%0Atraining%20samples%20contain%20all%20conditions%20simultaneously.%20Our%20method%20maintains%0Arigorous%20control%20over%20multiple%20conditions%20without%20requiring%20joint%20annotations.%0AWe%20demonstrate%20its%20effectiveness%20in%20molecular%20and%20image%20generation%20tasks%2C%20where%0Ait%20outperforms%20existing%20baselines%20both%20in%20alignment%20with%20target%20conditional%0Adistributions%20and%20in%20controllability%20under%20missing%20condition%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Models%2520with%2520Double%2520Guidance%253A%2520Generate%2520with%2520aggregated%2520datasets%26entry.906535625%3DYanfeng%2520Yang%2520and%2520Kenji%2520Fukumizu%26entry.1292438233%3D%2520%2520Creating%2520large-scale%2520datasets%2520for%2520training%2520high-performance%2520generative%2520models%250Ais%2520often%2520prohibitively%2520expensive%252C%2520especially%2520when%2520associated%2520attributes%2520or%250Aannotations%2520must%2520be%2520provided.%2520As%2520a%2520result%252C%2520merging%2520existing%2520datasets%2520has%2520become%250Aa%2520common%2520strategy.%2520However%252C%2520the%2520sets%2520of%2520attributes%2520across%2520datasets%2520are%2520often%250Ainconsistent%252C%2520and%2520their%2520naive%2520concatenation%2520typically%2520leads%2520to%2520block-wise%250Amissing%2520conditions.%2520This%2520presents%2520a%2520significant%2520challenge%2520for%2520conditional%250Agenerative%2520modeling%2520when%2520the%2520multiple%2520attributes%2520are%2520used%2520jointly%2520as%250Aconditions%252C%2520thereby%2520limiting%2520the%2520model%2527s%2520controllability%2520and%2520applicability.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520generative%2520approach%252C%2520Diffusion%2520Model%250Awith%2520Double%2520Guidance%252C%2520which%2520enables%2520precise%2520conditional%2520generation%2520even%2520when%2520no%250Atraining%2520samples%2520contain%2520all%2520conditions%2520simultaneously.%2520Our%2520method%2520maintains%250Arigorous%2520control%2520over%2520multiple%2520conditions%2520without%2520requiring%2520joint%2520annotations.%250AWe%2520demonstrate%2520its%2520effectiveness%2520in%2520molecular%2520and%2520image%2520generation%2520tasks%252C%2520where%250Ait%2520outperforms%2520existing%2520baselines%2520both%2520in%2520alignment%2520with%2520target%2520conditional%250Adistributions%2520and%2520in%2520controllability%2520under%2520missing%2520condition%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Models%20with%20Double%20Guidance%3A%20Generate%20with%20aggregated%20datasets&entry.906535625=Yanfeng%20Yang%20and%20Kenji%20Fukumizu&entry.1292438233=%20%20Creating%20large-scale%20datasets%20for%20training%20high-performance%20generative%20models%0Ais%20often%20prohibitively%20expensive%2C%20especially%20when%20associated%20attributes%20or%0Aannotations%20must%20be%20provided.%20As%20a%20result%2C%20merging%20existing%20datasets%20has%20become%0Aa%20common%20strategy.%20However%2C%20the%20sets%20of%20attributes%20across%20datasets%20are%20often%0Ainconsistent%2C%20and%20their%20naive%20concatenation%20typically%20leads%20to%20block-wise%0Amissing%20conditions.%20This%20presents%20a%20significant%20challenge%20for%20conditional%0Agenerative%20modeling%20when%20the%20multiple%20attributes%20are%20used%20jointly%20as%0Aconditions%2C%20thereby%20limiting%20the%20model%27s%20controllability%20and%20applicability.%20To%0Aaddress%20this%20issue%2C%20we%20propose%20a%20novel%20generative%20approach%2C%20Diffusion%20Model%0Awith%20Double%20Guidance%2C%20which%20enables%20precise%20conditional%20generation%20even%20when%20no%0Atraining%20samples%20contain%20all%20conditions%20simultaneously.%20Our%20method%20maintains%0Arigorous%20control%20over%20multiple%20conditions%20without%20requiring%20joint%20annotations.%0AWe%20demonstrate%20its%20effectiveness%20in%20molecular%20and%20image%20generation%20tasks%2C%20where%0Ait%20outperforms%20existing%20baselines%20both%20in%20alignment%20with%20target%20conditional%0Adistributions%20and%20in%20controllability%20under%20missing%20condition%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13213v1&entry.124074799=Read"},
{"title": "Superhuman performance of a large language model on the reasoning tasks\n  of a physician", "author": "Peter G. Brodeur and Thomas A. Buckley and Zahir Kanjee and Ethan Goh and Evelyn Bin Ling and Priyank Jain and Stephanie Cabral and Raja-Elie Abdulnour and Adrian D. Haimovich and Jason A. Freed and Andrew Olson and Daniel J. Morgan and Jason Hom and Robert Gallo and Liam G. McCoy and Haadi Mombini and Christopher Lucas and Misha Fotoohi and Matthew Gwiazdon and Daniele Restifo and Daniel Restrepo and Eric Horvitz and Jonathan Chen and Arjun K. Manrai and Adam Rodman", "abstract": "  A seminal paper published by Ledley and Lusted in 1959 introduced complex\nclinical diagnostic reasoning cases as the gold standard for the evaluation of\nexpert medical computing systems, a standard that has held ever since. Here, we\nreport the results of a physician evaluation of a large language model (LLM) on\nchallenging clinical cases against a baseline of hundreds of physicians. We\nconduct five experiments to measure clinical reasoning across differential\ndiagnosis generation, display of diagnostic reasoning, triage differential\ndiagnosis, probabilistic reasoning, and management reasoning, all adjudicated\nby physician experts with validated psychometrics. We then report a real-world\nstudy comparing human expert and AI second opinions in randomly-selected\npatients in the emergency room of a major tertiary academic medical center in\nBoston, MA. We compared LLMs and board-certified physicians at three predefined\ndiagnostic touchpoints: triage in the emergency room, initial evaluation by a\nphysician, and admission to the hospital or intensive care unit. In all\nexperiments--both vignettes and emergency room second opinions--the LLM\ndisplayed superhuman diagnostic and reasoning abilities, as well as continued\nimprovement from prior generations of AI clinical decision support. Our study\nsuggests that LLMs have achieved superhuman performance on general medical\ndiagnostic and management reasoning, fulfilling the vision put forth by Ledley\nand Lusted, and motivating the urgent need for prospective trials.\n", "link": "http://arxiv.org/abs/2412.10849v2", "date": "2025-05-19", "relevancy": 1.9845, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superhuman%20performance%20of%20a%20large%20language%20model%20on%20the%20reasoning%20tasks%0A%20%20of%20a%20physician&body=Title%3A%20Superhuman%20performance%20of%20a%20large%20language%20model%20on%20the%20reasoning%20tasks%0A%20%20of%20a%20physician%0AAuthor%3A%20Peter%20G.%20Brodeur%20and%20Thomas%20A.%20Buckley%20and%20Zahir%20Kanjee%20and%20Ethan%20Goh%20and%20Evelyn%20Bin%20Ling%20and%20Priyank%20Jain%20and%20Stephanie%20Cabral%20and%20Raja-Elie%20Abdulnour%20and%20Adrian%20D.%20Haimovich%20and%20Jason%20A.%20Freed%20and%20Andrew%20Olson%20and%20Daniel%20J.%20Morgan%20and%20Jason%20Hom%20and%20Robert%20Gallo%20and%20Liam%20G.%20McCoy%20and%20Haadi%20Mombini%20and%20Christopher%20Lucas%20and%20Misha%20Fotoohi%20and%20Matthew%20Gwiazdon%20and%20Daniele%20Restifo%20and%20Daniel%20Restrepo%20and%20Eric%20Horvitz%20and%20Jonathan%20Chen%20and%20Arjun%20K.%20Manrai%20and%20Adam%20Rodman%0AAbstract%3A%20%20%20A%20seminal%20paper%20published%20by%20Ledley%20and%20Lusted%20in%201959%20introduced%20complex%0Aclinical%20diagnostic%20reasoning%20cases%20as%20the%20gold%20standard%20for%20the%20evaluation%20of%0Aexpert%20medical%20computing%20systems%2C%20a%20standard%20that%20has%20held%20ever%20since.%20Here%2C%20we%0Areport%20the%20results%20of%20a%20physician%20evaluation%20of%20a%20large%20language%20model%20%28LLM%29%20on%0Achallenging%20clinical%20cases%20against%20a%20baseline%20of%20hundreds%20of%20physicians.%20We%0Aconduct%20five%20experiments%20to%20measure%20clinical%20reasoning%20across%20differential%0Adiagnosis%20generation%2C%20display%20of%20diagnostic%20reasoning%2C%20triage%20differential%0Adiagnosis%2C%20probabilistic%20reasoning%2C%20and%20management%20reasoning%2C%20all%20adjudicated%0Aby%20physician%20experts%20with%20validated%20psychometrics.%20We%20then%20report%20a%20real-world%0Astudy%20comparing%20human%20expert%20and%20AI%20second%20opinions%20in%20randomly-selected%0Apatients%20in%20the%20emergency%20room%20of%20a%20major%20tertiary%20academic%20medical%20center%20in%0ABoston%2C%20MA.%20We%20compared%20LLMs%20and%20board-certified%20physicians%20at%20three%20predefined%0Adiagnostic%20touchpoints%3A%20triage%20in%20the%20emergency%20room%2C%20initial%20evaluation%20by%20a%0Aphysician%2C%20and%20admission%20to%20the%20hospital%20or%20intensive%20care%20unit.%20In%20all%0Aexperiments--both%20vignettes%20and%20emergency%20room%20second%20opinions--the%20LLM%0Adisplayed%20superhuman%20diagnostic%20and%20reasoning%20abilities%2C%20as%20well%20as%20continued%0Aimprovement%20from%20prior%20generations%20of%20AI%20clinical%20decision%20support.%20Our%20study%0Asuggests%20that%20LLMs%20have%20achieved%20superhuman%20performance%20on%20general%20medical%0Adiagnostic%20and%20management%20reasoning%2C%20fulfilling%20the%20vision%20put%20forth%20by%20Ledley%0Aand%20Lusted%2C%20and%20motivating%20the%20urgent%20need%20for%20prospective%20trials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperhuman%2520performance%2520of%2520a%2520large%2520language%2520model%2520on%2520the%2520reasoning%2520tasks%250A%2520%2520of%2520a%2520physician%26entry.906535625%3DPeter%2520G.%2520Brodeur%2520and%2520Thomas%2520A.%2520Buckley%2520and%2520Zahir%2520Kanjee%2520and%2520Ethan%2520Goh%2520and%2520Evelyn%2520Bin%2520Ling%2520and%2520Priyank%2520Jain%2520and%2520Stephanie%2520Cabral%2520and%2520Raja-Elie%2520Abdulnour%2520and%2520Adrian%2520D.%2520Haimovich%2520and%2520Jason%2520A.%2520Freed%2520and%2520Andrew%2520Olson%2520and%2520Daniel%2520J.%2520Morgan%2520and%2520Jason%2520Hom%2520and%2520Robert%2520Gallo%2520and%2520Liam%2520G.%2520McCoy%2520and%2520Haadi%2520Mombini%2520and%2520Christopher%2520Lucas%2520and%2520Misha%2520Fotoohi%2520and%2520Matthew%2520Gwiazdon%2520and%2520Daniele%2520Restifo%2520and%2520Daniel%2520Restrepo%2520and%2520Eric%2520Horvitz%2520and%2520Jonathan%2520Chen%2520and%2520Arjun%2520K.%2520Manrai%2520and%2520Adam%2520Rodman%26entry.1292438233%3D%2520%2520A%2520seminal%2520paper%2520published%2520by%2520Ledley%2520and%2520Lusted%2520in%25201959%2520introduced%2520complex%250Aclinical%2520diagnostic%2520reasoning%2520cases%2520as%2520the%2520gold%2520standard%2520for%2520the%2520evaluation%2520of%250Aexpert%2520medical%2520computing%2520systems%252C%2520a%2520standard%2520that%2520has%2520held%2520ever%2520since.%2520Here%252C%2520we%250Areport%2520the%2520results%2520of%2520a%2520physician%2520evaluation%2520of%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520on%250Achallenging%2520clinical%2520cases%2520against%2520a%2520baseline%2520of%2520hundreds%2520of%2520physicians.%2520We%250Aconduct%2520five%2520experiments%2520to%2520measure%2520clinical%2520reasoning%2520across%2520differential%250Adiagnosis%2520generation%252C%2520display%2520of%2520diagnostic%2520reasoning%252C%2520triage%2520differential%250Adiagnosis%252C%2520probabilistic%2520reasoning%252C%2520and%2520management%2520reasoning%252C%2520all%2520adjudicated%250Aby%2520physician%2520experts%2520with%2520validated%2520psychometrics.%2520We%2520then%2520report%2520a%2520real-world%250Astudy%2520comparing%2520human%2520expert%2520and%2520AI%2520second%2520opinions%2520in%2520randomly-selected%250Apatients%2520in%2520the%2520emergency%2520room%2520of%2520a%2520major%2520tertiary%2520academic%2520medical%2520center%2520in%250ABoston%252C%2520MA.%2520We%2520compared%2520LLMs%2520and%2520board-certified%2520physicians%2520at%2520three%2520predefined%250Adiagnostic%2520touchpoints%253A%2520triage%2520in%2520the%2520emergency%2520room%252C%2520initial%2520evaluation%2520by%2520a%250Aphysician%252C%2520and%2520admission%2520to%2520the%2520hospital%2520or%2520intensive%2520care%2520unit.%2520In%2520all%250Aexperiments--both%2520vignettes%2520and%2520emergency%2520room%2520second%2520opinions--the%2520LLM%250Adisplayed%2520superhuman%2520diagnostic%2520and%2520reasoning%2520abilities%252C%2520as%2520well%2520as%2520continued%250Aimprovement%2520from%2520prior%2520generations%2520of%2520AI%2520clinical%2520decision%2520support.%2520Our%2520study%250Asuggests%2520that%2520LLMs%2520have%2520achieved%2520superhuman%2520performance%2520on%2520general%2520medical%250Adiagnostic%2520and%2520management%2520reasoning%252C%2520fulfilling%2520the%2520vision%2520put%2520forth%2520by%2520Ledley%250Aand%2520Lusted%252C%2520and%2520motivating%2520the%2520urgent%2520need%2520for%2520prospective%2520trials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superhuman%20performance%20of%20a%20large%20language%20model%20on%20the%20reasoning%20tasks%0A%20%20of%20a%20physician&entry.906535625=Peter%20G.%20Brodeur%20and%20Thomas%20A.%20Buckley%20and%20Zahir%20Kanjee%20and%20Ethan%20Goh%20and%20Evelyn%20Bin%20Ling%20and%20Priyank%20Jain%20and%20Stephanie%20Cabral%20and%20Raja-Elie%20Abdulnour%20and%20Adrian%20D.%20Haimovich%20and%20Jason%20A.%20Freed%20and%20Andrew%20Olson%20and%20Daniel%20J.%20Morgan%20and%20Jason%20Hom%20and%20Robert%20Gallo%20and%20Liam%20G.%20McCoy%20and%20Haadi%20Mombini%20and%20Christopher%20Lucas%20and%20Misha%20Fotoohi%20and%20Matthew%20Gwiazdon%20and%20Daniele%20Restifo%20and%20Daniel%20Restrepo%20and%20Eric%20Horvitz%20and%20Jonathan%20Chen%20and%20Arjun%20K.%20Manrai%20and%20Adam%20Rodman&entry.1292438233=%20%20A%20seminal%20paper%20published%20by%20Ledley%20and%20Lusted%20in%201959%20introduced%20complex%0Aclinical%20diagnostic%20reasoning%20cases%20as%20the%20gold%20standard%20for%20the%20evaluation%20of%0Aexpert%20medical%20computing%20systems%2C%20a%20standard%20that%20has%20held%20ever%20since.%20Here%2C%20we%0Areport%20the%20results%20of%20a%20physician%20evaluation%20of%20a%20large%20language%20model%20%28LLM%29%20on%0Achallenging%20clinical%20cases%20against%20a%20baseline%20of%20hundreds%20of%20physicians.%20We%0Aconduct%20five%20experiments%20to%20measure%20clinical%20reasoning%20across%20differential%0Adiagnosis%20generation%2C%20display%20of%20diagnostic%20reasoning%2C%20triage%20differential%0Adiagnosis%2C%20probabilistic%20reasoning%2C%20and%20management%20reasoning%2C%20all%20adjudicated%0Aby%20physician%20experts%20with%20validated%20psychometrics.%20We%20then%20report%20a%20real-world%0Astudy%20comparing%20human%20expert%20and%20AI%20second%20opinions%20in%20randomly-selected%0Apatients%20in%20the%20emergency%20room%20of%20a%20major%20tertiary%20academic%20medical%20center%20in%0ABoston%2C%20MA.%20We%20compared%20LLMs%20and%20board-certified%20physicians%20at%20three%20predefined%0Adiagnostic%20touchpoints%3A%20triage%20in%20the%20emergency%20room%2C%20initial%20evaluation%20by%20a%0Aphysician%2C%20and%20admission%20to%20the%20hospital%20or%20intensive%20care%20unit.%20In%20all%0Aexperiments--both%20vignettes%20and%20emergency%20room%20second%20opinions--the%20LLM%0Adisplayed%20superhuman%20diagnostic%20and%20reasoning%20abilities%2C%20as%20well%20as%20continued%0Aimprovement%20from%20prior%20generations%20of%20AI%20clinical%20decision%20support.%20Our%20study%0Asuggests%20that%20LLMs%20have%20achieved%20superhuman%20performance%20on%20general%20medical%0Adiagnostic%20and%20management%20reasoning%2C%20fulfilling%20the%20vision%20put%20forth%20by%20Ledley%0Aand%20Lusted%2C%20and%20motivating%20the%20urgent%20need%20for%20prospective%20trials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10849v2&entry.124074799=Read"},
{"title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation\n  Normalization", "author": "Alonso Urbano and David W. Romero and Max Zimmer and Sebastian Pokutta", "abstract": "  Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling.\n", "link": "http://arxiv.org/abs/2505.13289v1", "date": "2025-05-19", "relevancy": 2.0772, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5204}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5191}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RECON%3A%20Robust%20symmetry%20discovery%20via%20Explicit%20Canonical%20Orientation%0A%20%20Normalization&body=Title%3A%20RECON%3A%20Robust%20symmetry%20discovery%20via%20Explicit%20Canonical%20Orientation%0A%20%20Normalization%0AAuthor%3A%20Alonso%20Urbano%20and%20David%20W.%20Romero%20and%20Max%20Zimmer%20and%20Sebastian%20Pokutta%0AAbstract%3A%20%20%20Real-world%20data%20often%20exhibits%20unknown%20or%20approximate%20symmetries%2C%20yet%0Aexisting%20equivariant%20networks%20must%20commit%20to%20a%20fixed%20transformation%20group%20prior%0Ato%20training%2C%20e.g.%2C%20continuous%20%24SO%282%29%24%20rotations.%20This%20mismatch%20degrades%0Aperformance%20when%20the%20actual%20data%20symmetries%20differ%20from%20those%20in%20the%0Atransformation%20group.%20We%20introduce%20RECON%2C%20a%20framework%20to%20discover%20each%20input%27s%0Aintrinsic%20symmetry%20distribution%20from%20unlabeled%20data.%20RECON%20leverages%20class-pose%0Adecompositions%20and%20applies%20a%20data-driven%20normalization%20to%20align%20arbitrary%0Areference%20frames%20into%20a%20common%20natural%20pose%2C%20yielding%20directly%20comparable%20and%0Ainterpretable%20symmetry%20descriptors.%20We%20demonstrate%20effective%20symmetry%20discovery%0Aon%202D%20image%20benchmarks%20and%20--%20for%20the%20first%20time%20--%20extend%20it%20to%203D%0Atransformation%20groups%2C%20paving%20the%20way%20towards%20more%20flexible%20equivariant%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRECON%253A%2520Robust%2520symmetry%2520discovery%2520via%2520Explicit%2520Canonical%2520Orientation%250A%2520%2520Normalization%26entry.906535625%3DAlonso%2520Urbano%2520and%2520David%2520W.%2520Romero%2520and%2520Max%2520Zimmer%2520and%2520Sebastian%2520Pokutta%26entry.1292438233%3D%2520%2520Real-world%2520data%2520often%2520exhibits%2520unknown%2520or%2520approximate%2520symmetries%252C%2520yet%250Aexisting%2520equivariant%2520networks%2520must%2520commit%2520to%2520a%2520fixed%2520transformation%2520group%2520prior%250Ato%2520training%252C%2520e.g.%252C%2520continuous%2520%2524SO%25282%2529%2524%2520rotations.%2520This%2520mismatch%2520degrades%250Aperformance%2520when%2520the%2520actual%2520data%2520symmetries%2520differ%2520from%2520those%2520in%2520the%250Atransformation%2520group.%2520We%2520introduce%2520RECON%252C%2520a%2520framework%2520to%2520discover%2520each%2520input%2527s%250Aintrinsic%2520symmetry%2520distribution%2520from%2520unlabeled%2520data.%2520RECON%2520leverages%2520class-pose%250Adecompositions%2520and%2520applies%2520a%2520data-driven%2520normalization%2520to%2520align%2520arbitrary%250Areference%2520frames%2520into%2520a%2520common%2520natural%2520pose%252C%2520yielding%2520directly%2520comparable%2520and%250Ainterpretable%2520symmetry%2520descriptors.%2520We%2520demonstrate%2520effective%2520symmetry%2520discovery%250Aon%25202D%2520image%2520benchmarks%2520and%2520--%2520for%2520the%2520first%2520time%2520--%2520extend%2520it%2520to%25203D%250Atransformation%2520groups%252C%2520paving%2520the%2520way%2520towards%2520more%2520flexible%2520equivariant%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECON%3A%20Robust%20symmetry%20discovery%20via%20Explicit%20Canonical%20Orientation%0A%20%20Normalization&entry.906535625=Alonso%20Urbano%20and%20David%20W.%20Romero%20and%20Max%20Zimmer%20and%20Sebastian%20Pokutta&entry.1292438233=%20%20Real-world%20data%20often%20exhibits%20unknown%20or%20approximate%20symmetries%2C%20yet%0Aexisting%20equivariant%20networks%20must%20commit%20to%20a%20fixed%20transformation%20group%20prior%0Ato%20training%2C%20e.g.%2C%20continuous%20%24SO%282%29%24%20rotations.%20This%20mismatch%20degrades%0Aperformance%20when%20the%20actual%20data%20symmetries%20differ%20from%20those%20in%20the%0Atransformation%20group.%20We%20introduce%20RECON%2C%20a%20framework%20to%20discover%20each%20input%27s%0Aintrinsic%20symmetry%20distribution%20from%20unlabeled%20data.%20RECON%20leverages%20class-pose%0Adecompositions%20and%20applies%20a%20data-driven%20normalization%20to%20align%20arbitrary%0Areference%20frames%20into%20a%20common%20natural%20pose%2C%20yielding%20directly%20comparable%20and%0Ainterpretable%20symmetry%20descriptors.%20We%20demonstrate%20effective%20symmetry%20discovery%0Aon%202D%20image%20benchmarks%20and%20--%20for%20the%20first%20time%20--%20extend%20it%20to%203D%0Atransformation%20groups%2C%20paving%20the%20way%20towards%20more%20flexible%20equivariant%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13289v1&entry.124074799=Read"},
{"title": "Asymptotic Performance of Time-Varying Bayesian Optimization", "author": "Anthony Bardou and Patrick Thiran", "abstract": "  Time-Varying Bayesian Optimization (TVBO) is the go-to framework for\noptimizing a time-varying black-box objective function that may be noisy and\nexpensive to evaluate. Is it possible for the instantaneous regret of a TVBO\nalgorithm to vanish asymptotically, and if so, when? We answer this question of\ngreat theoretical importance by providing algorithm-independent lower regret\nbounds and upper regret bounds for TVBO algorithms, from which we derive\nsufficient conditions for a TVBO algorithm to have the no-regret property. Our\nanalysis covers all major classes of stationary kernel functions.\n", "link": "http://arxiv.org/abs/2505.13012v1", "date": "2025-05-19", "relevancy": 1.2655, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4422}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4181}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymptotic%20Performance%20of%20Time-Varying%20Bayesian%20Optimization&body=Title%3A%20Asymptotic%20Performance%20of%20Time-Varying%20Bayesian%20Optimization%0AAuthor%3A%20Anthony%20Bardou%20and%20Patrick%20Thiran%0AAbstract%3A%20%20%20Time-Varying%20Bayesian%20Optimization%20%28TVBO%29%20is%20the%20go-to%20framework%20for%0Aoptimizing%20a%20time-varying%20black-box%20objective%20function%20that%20may%20be%20noisy%20and%0Aexpensive%20to%20evaluate.%20Is%20it%20possible%20for%20the%20instantaneous%20regret%20of%20a%20TVBO%0Aalgorithm%20to%20vanish%20asymptotically%2C%20and%20if%20so%2C%20when%3F%20We%20answer%20this%20question%20of%0Agreat%20theoretical%20importance%20by%20providing%20algorithm-independent%20lower%20regret%0Abounds%20and%20upper%20regret%20bounds%20for%20TVBO%20algorithms%2C%20from%20which%20we%20derive%0Asufficient%20conditions%20for%20a%20TVBO%20algorithm%20to%20have%20the%20no-regret%20property.%20Our%0Aanalysis%20covers%20all%20major%20classes%20of%20stationary%20kernel%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymptotic%2520Performance%2520of%2520Time-Varying%2520Bayesian%2520Optimization%26entry.906535625%3DAnthony%2520Bardou%2520and%2520Patrick%2520Thiran%26entry.1292438233%3D%2520%2520Time-Varying%2520Bayesian%2520Optimization%2520%2528TVBO%2529%2520is%2520the%2520go-to%2520framework%2520for%250Aoptimizing%2520a%2520time-varying%2520black-box%2520objective%2520function%2520that%2520may%2520be%2520noisy%2520and%250Aexpensive%2520to%2520evaluate.%2520Is%2520it%2520possible%2520for%2520the%2520instantaneous%2520regret%2520of%2520a%2520TVBO%250Aalgorithm%2520to%2520vanish%2520asymptotically%252C%2520and%2520if%2520so%252C%2520when%253F%2520We%2520answer%2520this%2520question%2520of%250Agreat%2520theoretical%2520importance%2520by%2520providing%2520algorithm-independent%2520lower%2520regret%250Abounds%2520and%2520upper%2520regret%2520bounds%2520for%2520TVBO%2520algorithms%252C%2520from%2520which%2520we%2520derive%250Asufficient%2520conditions%2520for%2520a%2520TVBO%2520algorithm%2520to%2520have%2520the%2520no-regret%2520property.%2520Our%250Aanalysis%2520covers%2520all%2520major%2520classes%2520of%2520stationary%2520kernel%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymptotic%20Performance%20of%20Time-Varying%20Bayesian%20Optimization&entry.906535625=Anthony%20Bardou%20and%20Patrick%20Thiran&entry.1292438233=%20%20Time-Varying%20Bayesian%20Optimization%20%28TVBO%29%20is%20the%20go-to%20framework%20for%0Aoptimizing%20a%20time-varying%20black-box%20objective%20function%20that%20may%20be%20noisy%20and%0Aexpensive%20to%20evaluate.%20Is%20it%20possible%20for%20the%20instantaneous%20regret%20of%20a%20TVBO%0Aalgorithm%20to%20vanish%20asymptotically%2C%20and%20if%20so%2C%20when%3F%20We%20answer%20this%20question%20of%0Agreat%20theoretical%20importance%20by%20providing%20algorithm-independent%20lower%20regret%0Abounds%20and%20upper%20regret%20bounds%20for%20TVBO%20algorithms%2C%20from%20which%20we%20derive%0Asufficient%20conditions%20for%20a%20TVBO%20algorithm%20to%20have%20the%20no-regret%20property.%20Our%0Aanalysis%20covers%20all%20major%20classes%20of%20stationary%20kernel%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13012v1&entry.124074799=Read"},
{"title": "RevCD -- Reversed Conditional Diffusion for Generalized Zero-Shot\n  Learning", "author": "William Heyden and Habib Ullah and M. Salman Siddiqui and Fadi Al Machot", "abstract": "  In Generalized Zero-Shot Learning (GZSL), we aim to recognize both seen and\nunseen categories using a model trained only on seen categories. In computer\nvision, this translates into a classification problem, where knowledge from\nseen categories is transferred to unseen categories by exploiting the\nrelationships between visual features and available semantic information, such\nas text corpora or manual annotations. However, learning this joint\ndistribution is costly and requires one-to-one training with corresponding\nsemantic information. We present a reversed conditional Diffusion-based model\n(RevCD) that mitigates this issue by generating semantic features synthesized\nfrom visual inputs by leveraging Diffusion models' conditional mechanisms. Our\nRevCD model consists of a cross Hadamard-Addition embedding of a sinusoidal\ntime schedule and a multi-headed visual transformer for attention-guided\nembeddings. The proposed approach introduces three key innovations. First, we\nreverse the process of generating semantic space based on visual data,\nintroducing a novel loss function that facilitates more efficient knowledge\ntransfer. Second, we apply Diffusion models to zero-shot learning - a novel\napproach that exploits their strengths in capturing data complexity. Third, we\ndemonstrate our model's performance through a comprehensive cross-dataset\nevaluation. The complete code will be available on GitHub.\n", "link": "http://arxiv.org/abs/2409.00511v2", "date": "2025-05-19", "relevancy": 1.7333, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6249}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5651}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RevCD%20--%20Reversed%20Conditional%20Diffusion%20for%20Generalized%20Zero-Shot%0A%20%20Learning&body=Title%3A%20RevCD%20--%20Reversed%20Conditional%20Diffusion%20for%20Generalized%20Zero-Shot%0A%20%20Learning%0AAuthor%3A%20William%20Heyden%20and%20Habib%20Ullah%20and%20M.%20Salman%20Siddiqui%20and%20Fadi%20Al%20Machot%0AAbstract%3A%20%20%20In%20Generalized%20Zero-Shot%20Learning%20%28GZSL%29%2C%20we%20aim%20to%20recognize%20both%20seen%20and%0Aunseen%20categories%20using%20a%20model%20trained%20only%20on%20seen%20categories.%20In%20computer%0Avision%2C%20this%20translates%20into%20a%20classification%20problem%2C%20where%20knowledge%20from%0Aseen%20categories%20is%20transferred%20to%20unseen%20categories%20by%20exploiting%20the%0Arelationships%20between%20visual%20features%20and%20available%20semantic%20information%2C%20such%0Aas%20text%20corpora%20or%20manual%20annotations.%20However%2C%20learning%20this%20joint%0Adistribution%20is%20costly%20and%20requires%20one-to-one%20training%20with%20corresponding%0Asemantic%20information.%20We%20present%20a%20reversed%20conditional%20Diffusion-based%20model%0A%28RevCD%29%20that%20mitigates%20this%20issue%20by%20generating%20semantic%20features%20synthesized%0Afrom%20visual%20inputs%20by%20leveraging%20Diffusion%20models%27%20conditional%20mechanisms.%20Our%0ARevCD%20model%20consists%20of%20a%20cross%20Hadamard-Addition%20embedding%20of%20a%20sinusoidal%0Atime%20schedule%20and%20a%20multi-headed%20visual%20transformer%20for%20attention-guided%0Aembeddings.%20The%20proposed%20approach%20introduces%20three%20key%20innovations.%20First%2C%20we%0Areverse%20the%20process%20of%20generating%20semantic%20space%20based%20on%20visual%20data%2C%0Aintroducing%20a%20novel%20loss%20function%20that%20facilitates%20more%20efficient%20knowledge%0Atransfer.%20Second%2C%20we%20apply%20Diffusion%20models%20to%20zero-shot%20learning%20-%20a%20novel%0Aapproach%20that%20exploits%20their%20strengths%20in%20capturing%20data%20complexity.%20Third%2C%20we%0Ademonstrate%20our%20model%27s%20performance%20through%20a%20comprehensive%20cross-dataset%0Aevaluation.%20The%20complete%20code%20will%20be%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00511v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevCD%2520--%2520Reversed%2520Conditional%2520Diffusion%2520for%2520Generalized%2520Zero-Shot%250A%2520%2520Learning%26entry.906535625%3DWilliam%2520Heyden%2520and%2520Habib%2520Ullah%2520and%2520M.%2520Salman%2520Siddiqui%2520and%2520Fadi%2520Al%2520Machot%26entry.1292438233%3D%2520%2520In%2520Generalized%2520Zero-Shot%2520Learning%2520%2528GZSL%2529%252C%2520we%2520aim%2520to%2520recognize%2520both%2520seen%2520and%250Aunseen%2520categories%2520using%2520a%2520model%2520trained%2520only%2520on%2520seen%2520categories.%2520In%2520computer%250Avision%252C%2520this%2520translates%2520into%2520a%2520classification%2520problem%252C%2520where%2520knowledge%2520from%250Aseen%2520categories%2520is%2520transferred%2520to%2520unseen%2520categories%2520by%2520exploiting%2520the%250Arelationships%2520between%2520visual%2520features%2520and%2520available%2520semantic%2520information%252C%2520such%250Aas%2520text%2520corpora%2520or%2520manual%2520annotations.%2520However%252C%2520learning%2520this%2520joint%250Adistribution%2520is%2520costly%2520and%2520requires%2520one-to-one%2520training%2520with%2520corresponding%250Asemantic%2520information.%2520We%2520present%2520a%2520reversed%2520conditional%2520Diffusion-based%2520model%250A%2528RevCD%2529%2520that%2520mitigates%2520this%2520issue%2520by%2520generating%2520semantic%2520features%2520synthesized%250Afrom%2520visual%2520inputs%2520by%2520leveraging%2520Diffusion%2520models%2527%2520conditional%2520mechanisms.%2520Our%250ARevCD%2520model%2520consists%2520of%2520a%2520cross%2520Hadamard-Addition%2520embedding%2520of%2520a%2520sinusoidal%250Atime%2520schedule%2520and%2520a%2520multi-headed%2520visual%2520transformer%2520for%2520attention-guided%250Aembeddings.%2520The%2520proposed%2520approach%2520introduces%2520three%2520key%2520innovations.%2520First%252C%2520we%250Areverse%2520the%2520process%2520of%2520generating%2520semantic%2520space%2520based%2520on%2520visual%2520data%252C%250Aintroducing%2520a%2520novel%2520loss%2520function%2520that%2520facilitates%2520more%2520efficient%2520knowledge%250Atransfer.%2520Second%252C%2520we%2520apply%2520Diffusion%2520models%2520to%2520zero-shot%2520learning%2520-%2520a%2520novel%250Aapproach%2520that%2520exploits%2520their%2520strengths%2520in%2520capturing%2520data%2520complexity.%2520Third%252C%2520we%250Ademonstrate%2520our%2520model%2527s%2520performance%2520through%2520a%2520comprehensive%2520cross-dataset%250Aevaluation.%2520The%2520complete%2520code%2520will%2520be%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00511v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RevCD%20--%20Reversed%20Conditional%20Diffusion%20for%20Generalized%20Zero-Shot%0A%20%20Learning&entry.906535625=William%20Heyden%20and%20Habib%20Ullah%20and%20M.%20Salman%20Siddiqui%20and%20Fadi%20Al%20Machot&entry.1292438233=%20%20In%20Generalized%20Zero-Shot%20Learning%20%28GZSL%29%2C%20we%20aim%20to%20recognize%20both%20seen%20and%0Aunseen%20categories%20using%20a%20model%20trained%20only%20on%20seen%20categories.%20In%20computer%0Avision%2C%20this%20translates%20into%20a%20classification%20problem%2C%20where%20knowledge%20from%0Aseen%20categories%20is%20transferred%20to%20unseen%20categories%20by%20exploiting%20the%0Arelationships%20between%20visual%20features%20and%20available%20semantic%20information%2C%20such%0Aas%20text%20corpora%20or%20manual%20annotations.%20However%2C%20learning%20this%20joint%0Adistribution%20is%20costly%20and%20requires%20one-to-one%20training%20with%20corresponding%0Asemantic%20information.%20We%20present%20a%20reversed%20conditional%20Diffusion-based%20model%0A%28RevCD%29%20that%20mitigates%20this%20issue%20by%20generating%20semantic%20features%20synthesized%0Afrom%20visual%20inputs%20by%20leveraging%20Diffusion%20models%27%20conditional%20mechanisms.%20Our%0ARevCD%20model%20consists%20of%20a%20cross%20Hadamard-Addition%20embedding%20of%20a%20sinusoidal%0Atime%20schedule%20and%20a%20multi-headed%20visual%20transformer%20for%20attention-guided%0Aembeddings.%20The%20proposed%20approach%20introduces%20three%20key%20innovations.%20First%2C%20we%0Areverse%20the%20process%20of%20generating%20semantic%20space%20based%20on%20visual%20data%2C%0Aintroducing%20a%20novel%20loss%20function%20that%20facilitates%20more%20efficient%20knowledge%0Atransfer.%20Second%2C%20we%20apply%20Diffusion%20models%20to%20zero-shot%20learning%20-%20a%20novel%0Aapproach%20that%20exploits%20their%20strengths%20in%20capturing%20data%20complexity.%20Third%2C%20we%0Ademonstrate%20our%20model%27s%20performance%20through%20a%20comprehensive%20cross-dataset%0Aevaluation.%20The%20complete%20code%20will%20be%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00511v2&entry.124074799=Read"},
{"title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark\n  Dataset", "author": "Sayon Palit and Daniel Woods", "abstract": "  Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.\n", "link": "http://arxiv.org/abs/2505.13028v1", "date": "2025-05-19", "relevancy": 1.8492, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5065}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4588}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluatiing%20the%20efficacy%20of%20LLM%20Safety%20Solutions%20%3A%20The%20Palit%20Benchmark%0A%20%20Dataset&body=Title%3A%20Evaluatiing%20the%20efficacy%20of%20LLM%20Safety%20Solutions%20%3A%20The%20Palit%20Benchmark%0A%20%20Dataset%0AAuthor%3A%20Sayon%20Palit%20and%20Daniel%20Woods%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20integrated%20into%20critical%0Asystems%20in%20industries%20like%20healthcare%20and%20finance.%20Users%20can%20often%20submit%0Aqueries%20to%20LLM-enabled%20chatbots%2C%20some%20of%20which%20can%20enrich%20responses%20with%0Ainformation%20retrieved%20from%20internal%20databases%20storing%20sensitive%20data.%20This%0Agives%20rise%20to%20a%20range%20of%20attacks%20in%20which%20a%20user%20submits%20a%20malicious%20query%20and%0Athe%20LLM-system%20outputs%20a%20response%20that%20creates%20harm%20to%20the%20owner%2C%20such%20as%0Aleaking%20internal%20data%20or%20creating%20legal%20liability%20by%20harming%20a%20third-party.%0AWhile%20security%20tools%20are%20being%20developed%20to%20counter%20these%20threats%2C%20there%20is%0Alittle%20formal%20evaluation%20of%20their%20effectiveness%20and%20usability.%20This%20study%0Aaddresses%20this%20gap%20by%20conducting%20a%20thorough%20comparative%20analysis%20of%20LLM%0Asecurity%20tools.%20We%20identified%2013%20solutions%20%289%20closed-source%2C%204%20open-source%29%2C%0Abut%20only%207%20were%20evaluated%20due%20to%20a%20lack%20of%20participation%20by%20proprietary%20model%0Aowners.To%20evaluate%2C%20we%20built%20a%20benchmark%20dataset%20of%20malicious%20prompts%2C%20and%0Aevaluate%20these%20tools%20performance%20against%20a%20baseline%20LLM%20model%0A%28ChatGPT-3.5-Turbo%29.%20Our%20results%20show%20that%20the%20baseline%20model%20has%20too%20many%0Afalse%20positives%20to%20be%20used%20for%20this%20task.%20Lakera%20Guard%20and%20ProtectAI%20LLM%20Guard%0Aemerged%20as%20the%20best%20overall%20tools%20showcasing%20the%20tradeoff%20between%20usability%20and%0Aperformance.%20The%20study%20concluded%20with%20recommendations%20for%20greater%20transparency%0Aamong%20closed%20source%20providers%2C%20improved%20context-aware%20detections%2C%20enhanced%0Aopen-source%20engagement%2C%20increased%20user%20awareness%2C%20and%20the%20adoption%20of%20more%0Arepresentative%20performance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluatiing%2520the%2520efficacy%2520of%2520LLM%2520Safety%2520Solutions%2520%253A%2520The%2520Palit%2520Benchmark%250A%2520%2520Dataset%26entry.906535625%3DSayon%2520Palit%2520and%2520Daniel%2520Woods%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520integrated%2520into%2520critical%250Asystems%2520in%2520industries%2520like%2520healthcare%2520and%2520finance.%2520Users%2520can%2520often%2520submit%250Aqueries%2520to%2520LLM-enabled%2520chatbots%252C%2520some%2520of%2520which%2520can%2520enrich%2520responses%2520with%250Ainformation%2520retrieved%2520from%2520internal%2520databases%2520storing%2520sensitive%2520data.%2520This%250Agives%2520rise%2520to%2520a%2520range%2520of%2520attacks%2520in%2520which%2520a%2520user%2520submits%2520a%2520malicious%2520query%2520and%250Athe%2520LLM-system%2520outputs%2520a%2520response%2520that%2520creates%2520harm%2520to%2520the%2520owner%252C%2520such%2520as%250Aleaking%2520internal%2520data%2520or%2520creating%2520legal%2520liability%2520by%2520harming%2520a%2520third-party.%250AWhile%2520security%2520tools%2520are%2520being%2520developed%2520to%2520counter%2520these%2520threats%252C%2520there%2520is%250Alittle%2520formal%2520evaluation%2520of%2520their%2520effectiveness%2520and%2520usability.%2520This%2520study%250Aaddresses%2520this%2520gap%2520by%2520conducting%2520a%2520thorough%2520comparative%2520analysis%2520of%2520LLM%250Asecurity%2520tools.%2520We%2520identified%252013%2520solutions%2520%25289%2520closed-source%252C%25204%2520open-source%2529%252C%250Abut%2520only%25207%2520were%2520evaluated%2520due%2520to%2520a%2520lack%2520of%2520participation%2520by%2520proprietary%2520model%250Aowners.To%2520evaluate%252C%2520we%2520built%2520a%2520benchmark%2520dataset%2520of%2520malicious%2520prompts%252C%2520and%250Aevaluate%2520these%2520tools%2520performance%2520against%2520a%2520baseline%2520LLM%2520model%250A%2528ChatGPT-3.5-Turbo%2529.%2520Our%2520results%2520show%2520that%2520the%2520baseline%2520model%2520has%2520too%2520many%250Afalse%2520positives%2520to%2520be%2520used%2520for%2520this%2520task.%2520Lakera%2520Guard%2520and%2520ProtectAI%2520LLM%2520Guard%250Aemerged%2520as%2520the%2520best%2520overall%2520tools%2520showcasing%2520the%2520tradeoff%2520between%2520usability%2520and%250Aperformance.%2520The%2520study%2520concluded%2520with%2520recommendations%2520for%2520greater%2520transparency%250Aamong%2520closed%2520source%2520providers%252C%2520improved%2520context-aware%2520detections%252C%2520enhanced%250Aopen-source%2520engagement%252C%2520increased%2520user%2520awareness%252C%2520and%2520the%2520adoption%2520of%2520more%250Arepresentative%2520performance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluatiing%20the%20efficacy%20of%20LLM%20Safety%20Solutions%20%3A%20The%20Palit%20Benchmark%0A%20%20Dataset&entry.906535625=Sayon%20Palit%20and%20Daniel%20Woods&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20integrated%20into%20critical%0Asystems%20in%20industries%20like%20healthcare%20and%20finance.%20Users%20can%20often%20submit%0Aqueries%20to%20LLM-enabled%20chatbots%2C%20some%20of%20which%20can%20enrich%20responses%20with%0Ainformation%20retrieved%20from%20internal%20databases%20storing%20sensitive%20data.%20This%0Agives%20rise%20to%20a%20range%20of%20attacks%20in%20which%20a%20user%20submits%20a%20malicious%20query%20and%0Athe%20LLM-system%20outputs%20a%20response%20that%20creates%20harm%20to%20the%20owner%2C%20such%20as%0Aleaking%20internal%20data%20or%20creating%20legal%20liability%20by%20harming%20a%20third-party.%0AWhile%20security%20tools%20are%20being%20developed%20to%20counter%20these%20threats%2C%20there%20is%0Alittle%20formal%20evaluation%20of%20their%20effectiveness%20and%20usability.%20This%20study%0Aaddresses%20this%20gap%20by%20conducting%20a%20thorough%20comparative%20analysis%20of%20LLM%0Asecurity%20tools.%20We%20identified%2013%20solutions%20%289%20closed-source%2C%204%20open-source%29%2C%0Abut%20only%207%20were%20evaluated%20due%20to%20a%20lack%20of%20participation%20by%20proprietary%20model%0Aowners.To%20evaluate%2C%20we%20built%20a%20benchmark%20dataset%20of%20malicious%20prompts%2C%20and%0Aevaluate%20these%20tools%20performance%20against%20a%20baseline%20LLM%20model%0A%28ChatGPT-3.5-Turbo%29.%20Our%20results%20show%20that%20the%20baseline%20model%20has%20too%20many%0Afalse%20positives%20to%20be%20used%20for%20this%20task.%20Lakera%20Guard%20and%20ProtectAI%20LLM%20Guard%0Aemerged%20as%20the%20best%20overall%20tools%20showcasing%20the%20tradeoff%20between%20usability%20and%0Aperformance.%20The%20study%20concluded%20with%20recommendations%20for%20greater%20transparency%0Aamong%20closed%20source%20providers%2C%20improved%20context-aware%20detections%2C%20enhanced%0Aopen-source%20engagement%2C%20increased%20user%20awareness%2C%20and%20the%20adoption%20of%20more%0Arepresentative%20performance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13028v1&entry.124074799=Read"},
{"title": "The Traitors: Deception and Trust in Multi-Agent Language Model\n  Simulations", "author": "Pedro M. P. Curvo", "abstract": "  As AI systems increasingly assume roles where trust and alignment with human\nvalues are essential, understanding when and why they engage in deception has\nbecome a critical research priority. We introduce The Traitors, a multi-agent\nsimulation framework inspired by social deduction games, designed to probe\ndeception, trust formation, and strategic communication among large language\nmodel (LLM) agents under asymmetric information. A minority of agents the\ntraitors seek to mislead the majority, while the faithful must infer hidden\nidentities through dialogue and reasoning. Our contributions are: (1) we ground\nthe environment in formal frameworks from game theory, behavioral economics,\nand social cognition; (2) we develop a suite of evaluation metrics capturing\ndeception success, trust dynamics, and collective inference quality; (3) we\nimplement a fully autonomous simulation platform where LLMs reason over\npersistent memory and evolving social dynamics, with support for heterogeneous\nagent populations, specialized traits, and adaptive behaviors. Our initial\nexperiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model)\nreveal a notable asymmetry: advanced models like GPT-4o demonstrate superior\ndeceptive capabilities yet exhibit disproportionate vulnerability to others'\nfalsehoods. This suggests deception skills may scale faster than detection\nabilities. Overall, The Traitors provides a focused, configurable testbed for\ninvestigating LLM behavior in socially nuanced interactions. We position this\nwork as a contribution toward more rigorous research on deception mechanisms,\nalignment challenges, and the broader social reliability of AI systems.\n", "link": "http://arxiv.org/abs/2505.12923v1", "date": "2025-05-19", "relevancy": 1.9207, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4803}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Traitors%3A%20Deception%20and%20Trust%20in%20Multi-Agent%20Language%20Model%0A%20%20Simulations&body=Title%3A%20The%20Traitors%3A%20Deception%20and%20Trust%20in%20Multi-Agent%20Language%20Model%0A%20%20Simulations%0AAuthor%3A%20Pedro%20M.%20P.%20Curvo%0AAbstract%3A%20%20%20As%20AI%20systems%20increasingly%20assume%20roles%20where%20trust%20and%20alignment%20with%20human%0Avalues%20are%20essential%2C%20understanding%20when%20and%20why%20they%20engage%20in%20deception%20has%0Abecome%20a%20critical%20research%20priority.%20We%20introduce%20The%20Traitors%2C%20a%20multi-agent%0Asimulation%20framework%20inspired%20by%20social%20deduction%20games%2C%20designed%20to%20probe%0Adeception%2C%20trust%20formation%2C%20and%20strategic%20communication%20among%20large%20language%0Amodel%20%28LLM%29%20agents%20under%20asymmetric%20information.%20A%20minority%20of%20agents%20the%0Atraitors%20seek%20to%20mislead%20the%20majority%2C%20while%20the%20faithful%20must%20infer%20hidden%0Aidentities%20through%20dialogue%20and%20reasoning.%20Our%20contributions%20are%3A%20%281%29%20we%20ground%0Athe%20environment%20in%20formal%20frameworks%20from%20game%20theory%2C%20behavioral%20economics%2C%0Aand%20social%20cognition%3B%20%282%29%20we%20develop%20a%20suite%20of%20evaluation%20metrics%20capturing%0Adeception%20success%2C%20trust%20dynamics%2C%20and%20collective%20inference%20quality%3B%20%283%29%20we%0Aimplement%20a%20fully%20autonomous%20simulation%20platform%20where%20LLMs%20reason%20over%0Apersistent%20memory%20and%20evolving%20social%20dynamics%2C%20with%20support%20for%20heterogeneous%0Aagent%20populations%2C%20specialized%20traits%2C%20and%20adaptive%20behaviors.%20Our%20initial%0Aexperiments%20across%20DeepSeek-V3%2C%20GPT-4o-mini%2C%20and%20GPT-4o%20%2810%20runs%20per%20model%29%0Areveal%20a%20notable%20asymmetry%3A%20advanced%20models%20like%20GPT-4o%20demonstrate%20superior%0Adeceptive%20capabilities%20yet%20exhibit%20disproportionate%20vulnerability%20to%20others%27%0Afalsehoods.%20This%20suggests%20deception%20skills%20may%20scale%20faster%20than%20detection%0Aabilities.%20Overall%2C%20The%20Traitors%20provides%20a%20focused%2C%20configurable%20testbed%20for%0Ainvestigating%20LLM%20behavior%20in%20socially%20nuanced%20interactions.%20We%20position%20this%0Awork%20as%20a%20contribution%20toward%20more%20rigorous%20research%20on%20deception%20mechanisms%2C%0Aalignment%20challenges%2C%20and%20the%20broader%20social%20reliability%20of%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Traitors%253A%2520Deception%2520and%2520Trust%2520in%2520Multi-Agent%2520Language%2520Model%250A%2520%2520Simulations%26entry.906535625%3DPedro%2520M.%2520P.%2520Curvo%26entry.1292438233%3D%2520%2520As%2520AI%2520systems%2520increasingly%2520assume%2520roles%2520where%2520trust%2520and%2520alignment%2520with%2520human%250Avalues%2520are%2520essential%252C%2520understanding%2520when%2520and%2520why%2520they%2520engage%2520in%2520deception%2520has%250Abecome%2520a%2520critical%2520research%2520priority.%2520We%2520introduce%2520The%2520Traitors%252C%2520a%2520multi-agent%250Asimulation%2520framework%2520inspired%2520by%2520social%2520deduction%2520games%252C%2520designed%2520to%2520probe%250Adeception%252C%2520trust%2520formation%252C%2520and%2520strategic%2520communication%2520among%2520large%2520language%250Amodel%2520%2528LLM%2529%2520agents%2520under%2520asymmetric%2520information.%2520A%2520minority%2520of%2520agents%2520the%250Atraitors%2520seek%2520to%2520mislead%2520the%2520majority%252C%2520while%2520the%2520faithful%2520must%2520infer%2520hidden%250Aidentities%2520through%2520dialogue%2520and%2520reasoning.%2520Our%2520contributions%2520are%253A%2520%25281%2529%2520we%2520ground%250Athe%2520environment%2520in%2520formal%2520frameworks%2520from%2520game%2520theory%252C%2520behavioral%2520economics%252C%250Aand%2520social%2520cognition%253B%2520%25282%2529%2520we%2520develop%2520a%2520suite%2520of%2520evaluation%2520metrics%2520capturing%250Adeception%2520success%252C%2520trust%2520dynamics%252C%2520and%2520collective%2520inference%2520quality%253B%2520%25283%2529%2520we%250Aimplement%2520a%2520fully%2520autonomous%2520simulation%2520platform%2520where%2520LLMs%2520reason%2520over%250Apersistent%2520memory%2520and%2520evolving%2520social%2520dynamics%252C%2520with%2520support%2520for%2520heterogeneous%250Aagent%2520populations%252C%2520specialized%2520traits%252C%2520and%2520adaptive%2520behaviors.%2520Our%2520initial%250Aexperiments%2520across%2520DeepSeek-V3%252C%2520GPT-4o-mini%252C%2520and%2520GPT-4o%2520%252810%2520runs%2520per%2520model%2529%250Areveal%2520a%2520notable%2520asymmetry%253A%2520advanced%2520models%2520like%2520GPT-4o%2520demonstrate%2520superior%250Adeceptive%2520capabilities%2520yet%2520exhibit%2520disproportionate%2520vulnerability%2520to%2520others%2527%250Afalsehoods.%2520This%2520suggests%2520deception%2520skills%2520may%2520scale%2520faster%2520than%2520detection%250Aabilities.%2520Overall%252C%2520The%2520Traitors%2520provides%2520a%2520focused%252C%2520configurable%2520testbed%2520for%250Ainvestigating%2520LLM%2520behavior%2520in%2520socially%2520nuanced%2520interactions.%2520We%2520position%2520this%250Awork%2520as%2520a%2520contribution%2520toward%2520more%2520rigorous%2520research%2520on%2520deception%2520mechanisms%252C%250Aalignment%2520challenges%252C%2520and%2520the%2520broader%2520social%2520reliability%2520of%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Traitors%3A%20Deception%20and%20Trust%20in%20Multi-Agent%20Language%20Model%0A%20%20Simulations&entry.906535625=Pedro%20M.%20P.%20Curvo&entry.1292438233=%20%20As%20AI%20systems%20increasingly%20assume%20roles%20where%20trust%20and%20alignment%20with%20human%0Avalues%20are%20essential%2C%20understanding%20when%20and%20why%20they%20engage%20in%20deception%20has%0Abecome%20a%20critical%20research%20priority.%20We%20introduce%20The%20Traitors%2C%20a%20multi-agent%0Asimulation%20framework%20inspired%20by%20social%20deduction%20games%2C%20designed%20to%20probe%0Adeception%2C%20trust%20formation%2C%20and%20strategic%20communication%20among%20large%20language%0Amodel%20%28LLM%29%20agents%20under%20asymmetric%20information.%20A%20minority%20of%20agents%20the%0Atraitors%20seek%20to%20mislead%20the%20majority%2C%20while%20the%20faithful%20must%20infer%20hidden%0Aidentities%20through%20dialogue%20and%20reasoning.%20Our%20contributions%20are%3A%20%281%29%20we%20ground%0Athe%20environment%20in%20formal%20frameworks%20from%20game%20theory%2C%20behavioral%20economics%2C%0Aand%20social%20cognition%3B%20%282%29%20we%20develop%20a%20suite%20of%20evaluation%20metrics%20capturing%0Adeception%20success%2C%20trust%20dynamics%2C%20and%20collective%20inference%20quality%3B%20%283%29%20we%0Aimplement%20a%20fully%20autonomous%20simulation%20platform%20where%20LLMs%20reason%20over%0Apersistent%20memory%20and%20evolving%20social%20dynamics%2C%20with%20support%20for%20heterogeneous%0Aagent%20populations%2C%20specialized%20traits%2C%20and%20adaptive%20behaviors.%20Our%20initial%0Aexperiments%20across%20DeepSeek-V3%2C%20GPT-4o-mini%2C%20and%20GPT-4o%20%2810%20runs%20per%20model%29%0Areveal%20a%20notable%20asymmetry%3A%20advanced%20models%20like%20GPT-4o%20demonstrate%20superior%0Adeceptive%20capabilities%20yet%20exhibit%20disproportionate%20vulnerability%20to%20others%27%0Afalsehoods.%20This%20suggests%20deception%20skills%20may%20scale%20faster%20than%20detection%0Aabilities.%20Overall%2C%20The%20Traitors%20provides%20a%20focused%2C%20configurable%20testbed%20for%0Ainvestigating%20LLM%20behavior%20in%20socially%20nuanced%20interactions.%20We%20position%20this%0Awork%20as%20a%20contribution%20toward%20more%20rigorous%20research%20on%20deception%20mechanisms%2C%0Aalignment%20challenges%2C%20and%20the%20broader%20social%20reliability%20of%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12923v1&entry.124074799=Read"},
{"title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on\n  Optimal Transport for ASR", "author": "Xugang Lu and Peng Shen and Yu Tsao and Hisashi Kawai", "abstract": "  Transferring linguistic knowledge from a pretrained language model (PLM) to\nacoustic feature learning has proven effective in enhancing end-to-end\nautomatic speech recognition (E2E-ASR). However, aligning representations\nbetween linguistic and acoustic modalities remains a challenge due to inherent\nmodality gaps. Optimal transport (OT) has shown promise in mitigating these\ngaps by minimizing the Wasserstein distance (WD) between linguistic and\nacoustic feature distributions. However, previous OT-based methods overlook\nstructural relationships, treating feature vectors as unordered sets. To\naddress this, we propose Graph Matching Optimal Transport (GM-OT), which models\nlinguistic and acoustic sequences as structured graphs. Nodes represent feature\nembeddings, while edges capture temporal and sequential relationships. GM-OT\nminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)\n(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)\nformulation. This enables structured alignment and more efficient knowledge\ntransfer compared to existing OT-based approaches. Theoretical analysis further\nshows that prior OT-based methods in linguistic knowledge transfer can be\nviewed as a special case within our GM-OT framework. We evaluate GM-OT on\nMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge\ntransfer. Experimental results demonstrate significant performance gains over\nstate-of-the-art models, validating the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2505.13079v1", "date": "2025-05-19", "relevancy": 1.925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4569}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-modal%20Knowledge%20Transfer%20Learning%20as%20Graph%20Matching%20Based%20on%0A%20%20Optimal%20Transport%20for%20ASR&body=Title%3A%20Cross-modal%20Knowledge%20Transfer%20Learning%20as%20Graph%20Matching%20Based%20on%0A%20%20Optimal%20Transport%20for%20ASR%0AAuthor%3A%20Xugang%20Lu%20and%20Peng%20Shen%20and%20Yu%20Tsao%20and%20Hisashi%20Kawai%0AAbstract%3A%20%20%20Transferring%20linguistic%20knowledge%20from%20a%20pretrained%20language%20model%20%28PLM%29%20to%0Aacoustic%20feature%20learning%20has%20proven%20effective%20in%20enhancing%20end-to-end%0Aautomatic%20speech%20recognition%20%28E2E-ASR%29.%20However%2C%20aligning%20representations%0Abetween%20linguistic%20and%20acoustic%20modalities%20remains%20a%20challenge%20due%20to%20inherent%0Amodality%20gaps.%20Optimal%20transport%20%28OT%29%20has%20shown%20promise%20in%20mitigating%20these%0Agaps%20by%20minimizing%20the%20Wasserstein%20distance%20%28WD%29%20between%20linguistic%20and%0Aacoustic%20feature%20distributions.%20However%2C%20previous%20OT-based%20methods%20overlook%0Astructural%20relationships%2C%20treating%20feature%20vectors%20as%20unordered%20sets.%20To%0Aaddress%20this%2C%20we%20propose%20Graph%20Matching%20Optimal%20Transport%20%28GM-OT%29%2C%20which%20models%0Alinguistic%20and%20acoustic%20sequences%20as%20structured%20graphs.%20Nodes%20represent%20feature%0Aembeddings%2C%20while%20edges%20capture%20temporal%20and%20sequential%20relationships.%20GM-OT%0Aminimizes%20both%20WD%20%28between%20nodes%29%20and%20Gromov-Wasserstein%20distance%20%28GWD%29%0A%28between%20edges%29%2C%20leading%20to%20a%20fused%20Gromov-Wasserstein%20distance%20%28FGWD%29%0Aformulation.%20This%20enables%20structured%20alignment%20and%20more%20efficient%20knowledge%0Atransfer%20compared%20to%20existing%20OT-based%20approaches.%20Theoretical%20analysis%20further%0Ashows%20that%20prior%20OT-based%20methods%20in%20linguistic%20knowledge%20transfer%20can%20be%0Aviewed%20as%20a%20special%20case%20within%20our%20GM-OT%20framework.%20We%20evaluate%20GM-OT%20on%0AMandarin%20ASR%20using%20a%20CTC-based%20E2E-ASR%20system%20with%20a%20PLM%20for%20knowledge%0Atransfer.%20Experimental%20results%20demonstrate%20significant%20performance%20gains%20over%0Astate-of-the-art%20models%2C%20validating%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-modal%2520Knowledge%2520Transfer%2520Learning%2520as%2520Graph%2520Matching%2520Based%2520on%250A%2520%2520Optimal%2520Transport%2520for%2520ASR%26entry.906535625%3DXugang%2520Lu%2520and%2520Peng%2520Shen%2520and%2520Yu%2520Tsao%2520and%2520Hisashi%2520Kawai%26entry.1292438233%3D%2520%2520Transferring%2520linguistic%2520knowledge%2520from%2520a%2520pretrained%2520language%2520model%2520%2528PLM%2529%2520to%250Aacoustic%2520feature%2520learning%2520has%2520proven%2520effective%2520in%2520enhancing%2520end-to-end%250Aautomatic%2520speech%2520recognition%2520%2528E2E-ASR%2529.%2520However%252C%2520aligning%2520representations%250Abetween%2520linguistic%2520and%2520acoustic%2520modalities%2520remains%2520a%2520challenge%2520due%2520to%2520inherent%250Amodality%2520gaps.%2520Optimal%2520transport%2520%2528OT%2529%2520has%2520shown%2520promise%2520in%2520mitigating%2520these%250Agaps%2520by%2520minimizing%2520the%2520Wasserstein%2520distance%2520%2528WD%2529%2520between%2520linguistic%2520and%250Aacoustic%2520feature%2520distributions.%2520However%252C%2520previous%2520OT-based%2520methods%2520overlook%250Astructural%2520relationships%252C%2520treating%2520feature%2520vectors%2520as%2520unordered%2520sets.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520Graph%2520Matching%2520Optimal%2520Transport%2520%2528GM-OT%2529%252C%2520which%2520models%250Alinguistic%2520and%2520acoustic%2520sequences%2520as%2520structured%2520graphs.%2520Nodes%2520represent%2520feature%250Aembeddings%252C%2520while%2520edges%2520capture%2520temporal%2520and%2520sequential%2520relationships.%2520GM-OT%250Aminimizes%2520both%2520WD%2520%2528between%2520nodes%2529%2520and%2520Gromov-Wasserstein%2520distance%2520%2528GWD%2529%250A%2528between%2520edges%2529%252C%2520leading%2520to%2520a%2520fused%2520Gromov-Wasserstein%2520distance%2520%2528FGWD%2529%250Aformulation.%2520This%2520enables%2520structured%2520alignment%2520and%2520more%2520efficient%2520knowledge%250Atransfer%2520compared%2520to%2520existing%2520OT-based%2520approaches.%2520Theoretical%2520analysis%2520further%250Ashows%2520that%2520prior%2520OT-based%2520methods%2520in%2520linguistic%2520knowledge%2520transfer%2520can%2520be%250Aviewed%2520as%2520a%2520special%2520case%2520within%2520our%2520GM-OT%2520framework.%2520We%2520evaluate%2520GM-OT%2520on%250AMandarin%2520ASR%2520using%2520a%2520CTC-based%2520E2E-ASR%2520system%2520with%2520a%2520PLM%2520for%2520knowledge%250Atransfer.%2520Experimental%2520results%2520demonstrate%2520significant%2520performance%2520gains%2520over%250Astate-of-the-art%2520models%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-modal%20Knowledge%20Transfer%20Learning%20as%20Graph%20Matching%20Based%20on%0A%20%20Optimal%20Transport%20for%20ASR&entry.906535625=Xugang%20Lu%20and%20Peng%20Shen%20and%20Yu%20Tsao%20and%20Hisashi%20Kawai&entry.1292438233=%20%20Transferring%20linguistic%20knowledge%20from%20a%20pretrained%20language%20model%20%28PLM%29%20to%0Aacoustic%20feature%20learning%20has%20proven%20effective%20in%20enhancing%20end-to-end%0Aautomatic%20speech%20recognition%20%28E2E-ASR%29.%20However%2C%20aligning%20representations%0Abetween%20linguistic%20and%20acoustic%20modalities%20remains%20a%20challenge%20due%20to%20inherent%0Amodality%20gaps.%20Optimal%20transport%20%28OT%29%20has%20shown%20promise%20in%20mitigating%20these%0Agaps%20by%20minimizing%20the%20Wasserstein%20distance%20%28WD%29%20between%20linguistic%20and%0Aacoustic%20feature%20distributions.%20However%2C%20previous%20OT-based%20methods%20overlook%0Astructural%20relationships%2C%20treating%20feature%20vectors%20as%20unordered%20sets.%20To%0Aaddress%20this%2C%20we%20propose%20Graph%20Matching%20Optimal%20Transport%20%28GM-OT%29%2C%20which%20models%0Alinguistic%20and%20acoustic%20sequences%20as%20structured%20graphs.%20Nodes%20represent%20feature%0Aembeddings%2C%20while%20edges%20capture%20temporal%20and%20sequential%20relationships.%20GM-OT%0Aminimizes%20both%20WD%20%28between%20nodes%29%20and%20Gromov-Wasserstein%20distance%20%28GWD%29%0A%28between%20edges%29%2C%20leading%20to%20a%20fused%20Gromov-Wasserstein%20distance%20%28FGWD%29%0Aformulation.%20This%20enables%20structured%20alignment%20and%20more%20efficient%20knowledge%0Atransfer%20compared%20to%20existing%20OT-based%20approaches.%20Theoretical%20analysis%20further%0Ashows%20that%20prior%20OT-based%20methods%20in%20linguistic%20knowledge%20transfer%20can%20be%0Aviewed%20as%20a%20special%20case%20within%20our%20GM-OT%20framework.%20We%20evaluate%20GM-OT%20on%0AMandarin%20ASR%20using%20a%20CTC-based%20E2E-ASR%20system%20with%20a%20PLM%20for%20knowledge%0Atransfer.%20Experimental%20results%20demonstrate%20significant%20performance%20gains%20over%0Astate-of-the-art%20models%2C%20validating%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13079v1&entry.124074799=Read"},
{"title": "Robin: A multi-agent system for automating scientific discovery", "author": "Ali Essam Ghareeb and Benjamin Chang and Ludovico Mitchener and Angela Yiu and Caralyn J. Szostkiewicz and Jon M. Laurent and Muhammed T. Razzak and Andrew D. White and Michaela M. Hinks and Samuel G. Rodriques", "abstract": "  Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.\n", "link": "http://arxiv.org/abs/2505.13400v1", "date": "2025-05-19", "relevancy": 1.451, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5335}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4914}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robin%3A%20A%20multi-agent%20system%20for%20automating%20scientific%20discovery&body=Title%3A%20Robin%3A%20A%20multi-agent%20system%20for%20automating%20scientific%20discovery%0AAuthor%3A%20Ali%20Essam%20Ghareeb%20and%20Benjamin%20Chang%20and%20Ludovico%20Mitchener%20and%20Angela%20Yiu%20and%20Caralyn%20J.%20Szostkiewicz%20and%20Jon%20M.%20Laurent%20and%20Muhammed%20T.%20Razzak%20and%20Andrew%20D.%20White%20and%20Michaela%20M.%20Hinks%20and%20Samuel%20G.%20Rodriques%0AAbstract%3A%20%20%20Scientific%20discovery%20is%20driven%20by%20the%20iterative%20process%20of%20background%0Aresearch%2C%20hypothesis%20generation%2C%20experimentation%2C%20and%20data%20analysis.%20Despite%0Arecent%20advancements%20in%20applying%20artificial%20intelligence%20to%20scientific%0Adiscovery%2C%20no%20system%20has%20yet%20automated%20all%20of%20these%20stages%20in%20a%20single%0Aworkflow.%20Here%2C%20we%20introduce%20Robin%2C%20the%20first%20multi-agent%20system%20capable%20of%0Afully%20automating%20the%20key%20intellectual%20steps%20of%20the%20scientific%20process.%20By%0Aintegrating%20literature%20search%20agents%20with%20data%20analysis%20agents%2C%20Robin%20can%0Agenerate%20hypotheses%2C%20propose%20experiments%2C%20interpret%20experimental%20results%2C%20and%0Agenerate%20updated%20hypotheses%2C%20achieving%20a%20semi-autonomous%20approach%20to%20scientific%0Adiscovery.%20By%20applying%20this%20system%2C%20we%20were%20able%20to%20identify%20a%20novel%20treatment%0Afor%20dry%20age-related%20macular%20degeneration%20%28dAMD%29%2C%20the%20major%20cause%20of%20blindness%0Ain%20the%20developed%20world.%20Robin%20proposed%20enhancing%20retinal%20pigment%20epithelium%0Aphagocytosis%20as%20a%20therapeutic%20strategy%2C%20and%20identified%20and%20validated%20a%0Apromising%20therapeutic%20candidate%2C%20ripasudil.%20Ripasudil%20is%20a%20clinically-used%20rho%0Akinase%20%28ROCK%29%20inhibitor%20that%20has%20never%20previously%20been%20proposed%20for%20treating%0AdAMD.%20To%20elucidate%20the%20mechanism%20of%20ripasudil-induced%20upregulation%20of%0Aphagocytosis%2C%20Robin%20then%20proposed%20and%20analyzed%20a%20follow-up%20RNA-seq%20experiment%2C%0Awhich%20revealed%20upregulation%20of%20ABCA1%2C%20a%20critical%20lipid%20efflux%20pump%20and%20possible%0Anovel%20target.%20All%20hypotheses%2C%20experimental%20plans%2C%20data%20analyses%2C%20and%20data%0Afigures%20in%20the%20main%20text%20of%20this%20report%20were%20produced%20by%20Robin.%20As%20the%20first%20AI%0Asystem%20to%20autonomously%20discover%20and%20validate%20a%20novel%20therapeutic%20candidate%0Awithin%20an%20iterative%20lab-in-the-loop%20framework%2C%20Robin%20establishes%20a%20new%20paradigm%0Afor%20AI-driven%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobin%253A%2520A%2520multi-agent%2520system%2520for%2520automating%2520scientific%2520discovery%26entry.906535625%3DAli%2520Essam%2520Ghareeb%2520and%2520Benjamin%2520Chang%2520and%2520Ludovico%2520Mitchener%2520and%2520Angela%2520Yiu%2520and%2520Caralyn%2520J.%2520Szostkiewicz%2520and%2520Jon%2520M.%2520Laurent%2520and%2520Muhammed%2520T.%2520Razzak%2520and%2520Andrew%2520D.%2520White%2520and%2520Michaela%2520M.%2520Hinks%2520and%2520Samuel%2520G.%2520Rodriques%26entry.1292438233%3D%2520%2520Scientific%2520discovery%2520is%2520driven%2520by%2520the%2520iterative%2520process%2520of%2520background%250Aresearch%252C%2520hypothesis%2520generation%252C%2520experimentation%252C%2520and%2520data%2520analysis.%2520Despite%250Arecent%2520advancements%2520in%2520applying%2520artificial%2520intelligence%2520to%2520scientific%250Adiscovery%252C%2520no%2520system%2520has%2520yet%2520automated%2520all%2520of%2520these%2520stages%2520in%2520a%2520single%250Aworkflow.%2520Here%252C%2520we%2520introduce%2520Robin%252C%2520the%2520first%2520multi-agent%2520system%2520capable%2520of%250Afully%2520automating%2520the%2520key%2520intellectual%2520steps%2520of%2520the%2520scientific%2520process.%2520By%250Aintegrating%2520literature%2520search%2520agents%2520with%2520data%2520analysis%2520agents%252C%2520Robin%2520can%250Agenerate%2520hypotheses%252C%2520propose%2520experiments%252C%2520interpret%2520experimental%2520results%252C%2520and%250Agenerate%2520updated%2520hypotheses%252C%2520achieving%2520a%2520semi-autonomous%2520approach%2520to%2520scientific%250Adiscovery.%2520By%2520applying%2520this%2520system%252C%2520we%2520were%2520able%2520to%2520identify%2520a%2520novel%2520treatment%250Afor%2520dry%2520age-related%2520macular%2520degeneration%2520%2528dAMD%2529%252C%2520the%2520major%2520cause%2520of%2520blindness%250Ain%2520the%2520developed%2520world.%2520Robin%2520proposed%2520enhancing%2520retinal%2520pigment%2520epithelium%250Aphagocytosis%2520as%2520a%2520therapeutic%2520strategy%252C%2520and%2520identified%2520and%2520validated%2520a%250Apromising%2520therapeutic%2520candidate%252C%2520ripasudil.%2520Ripasudil%2520is%2520a%2520clinically-used%2520rho%250Akinase%2520%2528ROCK%2529%2520inhibitor%2520that%2520has%2520never%2520previously%2520been%2520proposed%2520for%2520treating%250AdAMD.%2520To%2520elucidate%2520the%2520mechanism%2520of%2520ripasudil-induced%2520upregulation%2520of%250Aphagocytosis%252C%2520Robin%2520then%2520proposed%2520and%2520analyzed%2520a%2520follow-up%2520RNA-seq%2520experiment%252C%250Awhich%2520revealed%2520upregulation%2520of%2520ABCA1%252C%2520a%2520critical%2520lipid%2520efflux%2520pump%2520and%2520possible%250Anovel%2520target.%2520All%2520hypotheses%252C%2520experimental%2520plans%252C%2520data%2520analyses%252C%2520and%2520data%250Afigures%2520in%2520the%2520main%2520text%2520of%2520this%2520report%2520were%2520produced%2520by%2520Robin.%2520As%2520the%2520first%2520AI%250Asystem%2520to%2520autonomously%2520discover%2520and%2520validate%2520a%2520novel%2520therapeutic%2520candidate%250Awithin%2520an%2520iterative%2520lab-in-the-loop%2520framework%252C%2520Robin%2520establishes%2520a%2520new%2520paradigm%250Afor%2520AI-driven%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robin%3A%20A%20multi-agent%20system%20for%20automating%20scientific%20discovery&entry.906535625=Ali%20Essam%20Ghareeb%20and%20Benjamin%20Chang%20and%20Ludovico%20Mitchener%20and%20Angela%20Yiu%20and%20Caralyn%20J.%20Szostkiewicz%20and%20Jon%20M.%20Laurent%20and%20Muhammed%20T.%20Razzak%20and%20Andrew%20D.%20White%20and%20Michaela%20M.%20Hinks%20and%20Samuel%20G.%20Rodriques&entry.1292438233=%20%20Scientific%20discovery%20is%20driven%20by%20the%20iterative%20process%20of%20background%0Aresearch%2C%20hypothesis%20generation%2C%20experimentation%2C%20and%20data%20analysis.%20Despite%0Arecent%20advancements%20in%20applying%20artificial%20intelligence%20to%20scientific%0Adiscovery%2C%20no%20system%20has%20yet%20automated%20all%20of%20these%20stages%20in%20a%20single%0Aworkflow.%20Here%2C%20we%20introduce%20Robin%2C%20the%20first%20multi-agent%20system%20capable%20of%0Afully%20automating%20the%20key%20intellectual%20steps%20of%20the%20scientific%20process.%20By%0Aintegrating%20literature%20search%20agents%20with%20data%20analysis%20agents%2C%20Robin%20can%0Agenerate%20hypotheses%2C%20propose%20experiments%2C%20interpret%20experimental%20results%2C%20and%0Agenerate%20updated%20hypotheses%2C%20achieving%20a%20semi-autonomous%20approach%20to%20scientific%0Adiscovery.%20By%20applying%20this%20system%2C%20we%20were%20able%20to%20identify%20a%20novel%20treatment%0Afor%20dry%20age-related%20macular%20degeneration%20%28dAMD%29%2C%20the%20major%20cause%20of%20blindness%0Ain%20the%20developed%20world.%20Robin%20proposed%20enhancing%20retinal%20pigment%20epithelium%0Aphagocytosis%20as%20a%20therapeutic%20strategy%2C%20and%20identified%20and%20validated%20a%0Apromising%20therapeutic%20candidate%2C%20ripasudil.%20Ripasudil%20is%20a%20clinically-used%20rho%0Akinase%20%28ROCK%29%20inhibitor%20that%20has%20never%20previously%20been%20proposed%20for%20treating%0AdAMD.%20To%20elucidate%20the%20mechanism%20of%20ripasudil-induced%20upregulation%20of%0Aphagocytosis%2C%20Robin%20then%20proposed%20and%20analyzed%20a%20follow-up%20RNA-seq%20experiment%2C%0Awhich%20revealed%20upregulation%20of%20ABCA1%2C%20a%20critical%20lipid%20efflux%20pump%20and%20possible%0Anovel%20target.%20All%20hypotheses%2C%20experimental%20plans%2C%20data%20analyses%2C%20and%20data%0Afigures%20in%20the%20main%20text%20of%20this%20report%20were%20produced%20by%20Robin.%20As%20the%20first%20AI%0Asystem%20to%20autonomously%20discover%20and%20validate%20a%20novel%20therapeutic%20candidate%0Awithin%20an%20iterative%20lab-in-the-loop%20framework%2C%20Robin%20establishes%20a%20new%20paradigm%0Afor%20AI-driven%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13400v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


